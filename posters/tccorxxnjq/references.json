{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces the Low-Rank Adaptation (LoRA) technique, a core method that the current paper builds upon for efficient and federated fine-tuning of LLMs."}, {"fullname_first_author": "Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This is a foundational paper in federated learning (FL), providing the algorithm (FedAvg) that the current paper adapts and improves upon for the context of LLM fine-tuning."}, {"fullname_first_author": "Jianyi Zhang", "paper_title": "Towards building the federated gpt: Federated instruction tuning", "publication_date": "2023-05-05", "reason": "This paper proposes FedIT, a direct predecessor to the current work, integrating LoRA with FedAvg for federated LLM fine-tuning; the current paper addresses FedIT's limitations."}, {"fullname_first_author": "Xiang Li", "paper_title": "On the convergence of FedAvg on non-iid data", "publication_date": "2019-07-02", "reason": "This paper provides key theoretical analysis of federated averaging (FedAvg) convergence, particularly addressing the challenges of non-IID data distributions relevant to the current paper's setting."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces the Llama family of LLMs which is one of the base models used in the current paper's experiments, demonstrating the model's characteristics and performance."}]}