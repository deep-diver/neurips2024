[{"figure_path": "TcCorXxNJQ/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of FLORA with baselines on MMLU and MT-bench. \"Homo\" represents the settings with homogeneous LoRA ranks, and \"Heter\" denotes those with heterogeneous LoRA ranks.", "description": "This table presents the results of experiments comparing FLORA's performance against several baseline methods on two downstream tasks: MMLU and MT-bench.  It shows the performance of different models (TinyLlama and Llama) using various fine-tuning algorithms (FLORA, FedIT, Zero-Padding, Centralized LoRA) under both homogeneous (Homo) and heterogeneous (Heter) LoRA rank settings.  The results are presented as scores for each task and model, illustrating FLORA's superior performance in various scenarios.", "section": "4.2 Experiment Results"}, {"figure_path": "TcCorXxNJQ/tables/tables_11_1.jpg", "caption": "Table 2: The communication rounds and local epochs on each experiment setting. The Rounds column represents the number of communication rounds and the Epochs column represents the number of local fine-tuning epochs in each round.", "description": "This table shows the hyperparameters used in the experiments. Specifically, it details the number of communication rounds and the number of local epochs for each combination of foundation model (TinyLlama, Llama, Llama2) and dataset (Dolly, Alpaca, Wizard, ShareGPT).  These settings were chosen to balance computational resource constraints with the need for sufficient data to observe meaningful results.", "section": "A.2 Hyperparameter Details"}, {"figure_path": "TcCorXxNJQ/tables/tables_12_1.jpg", "caption": "Table 3: The performance of FLORA + AdaLoRA. AdaLoRA can reduce the rank while preserving the fine-tuning effectiveness.", "description": "This table presents the results of experiments comparing the performance of FLORA alone and FLORA combined with AdaLoRA.  AdaLoRA is a technique that dynamically adjusts the rank of LoRA adapters during fine-tuning. The table shows that using AdaLoRA with FLORA reduces the sum of local ranks (indicating reduced model size) while maintaining comparable or even slightly improved performance on the MT-bench benchmark across different foundation models (TinyLlama, Llama, and Llama2). This demonstrates the effectiveness of AdaLoRA in enhancing FLORA's efficiency.", "section": "4.2 Experiment Results"}, {"figure_path": "TcCorXxNJQ/tables/tables_12_2.jpg", "caption": "Table 4: Compare FLORA with baselines in Llama2.", "description": "This table compares the performance of FLORA against three baseline methods (Centralized LoRA, FedIT, and Zero-Padding) when fine-tuning the Llama2 model on two downstream tasks (Wizard and ShareGPT).  The results show FLORA's performance compared to others under both homogeneous (all clients use same LoRA rank) and heterogeneous (clients use different LoRA ranks) settings.  It highlights FLORA's ability to achieve better results, especially in the heterogeneous setting, where other methods struggle.", "section": "4.2 Experiment Results"}]