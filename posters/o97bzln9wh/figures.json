[{"figure_path": "O97BzlN9Wh/figures/figures_1_1.jpg", "caption": "Figure 1: (a) We report the label distribution of the training set retained by InfoBatch at pruning ratios of 50% in the {0, 100, 200, 300}-th epochs. The gray, light blue and dark blue represent pruned, minority, and majority samples, respectively. (b) Performance comparison between InfoBatch and our GDeR when introducing outliers (following [36]) into {0%, 10%, 20%} of the training set.", "description": "The figure shows the effect of imbalanced and biased data on the InfoBatch model, a state-of-the-art data pruning method, compared to the proposed GDeR method. In (a), the label distribution of the training set retained by InfoBatch at different epochs with a pruning ratio of 50% is shown, highlighting the imbalance issue. (b) compares the performance of InfoBatch and GDeR under different noise levels (0%, 10%, 20%) added to the training set, demonstrating GDeR's improved robustness.", "section": "1 Introduction"}, {"figure_path": "O97BzlN9Wh/figures/figures_3_1.jpg", "caption": "Figure 2: The overview of our proposed GDeR. GDeR comprises hypersphere projection, embedding space modeling, sampling distribution formatting, and the final dynamic sampling. We present the dynamic sample selection process of GDeR within one epoch.", "description": "This figure provides a detailed overview of the proposed GDeR method. It's a dynamic soft-pruning method designed for efficient and robust graph training. The figure breaks down the GDeR process into four stages:\n\n1. **Hyperspherical Projection:** Graph samples are projected onto a hypersphere for better embedding space modeling.\n2. **Embedding Space Modeling:** Trainable prototypes are used to shape the embedding space, ensuring inter-class separation and intra-class compactness. \n3. **Sampling Distribution Formatting:** A sampling distribution is created based on outlier risk, model familiarity, and class balance, prioritizing samples that are representative, robust, and unbiased. \n4. **Dynamic Sampling:**  The method dynamically selects training samples during the training process based on the sampling distribution. This dynamic selection ensures that both efficiency (by pruning unnecessary samples) and robustness (by handling imbalance and noise) are achieved. The figure visualizes each step and how the process iterates across epochs.", "section": "3.2 Overview of the Proposed Method"}, {"figure_path": "O97BzlN9Wh/figures/figures_6_1.jpg", "caption": "Figure 3: The trade-off between per epoch time and ROC-AUC (%) of data pruning methods. Specifically, we report the test performance when pruning methods achieve per epoch times of {90%, 70%, 50%, 40%, 30%} of the full dataset training time. \"Vanilla\" denotes the original GNN backbone without any data pruning.", "description": "This figure demonstrates the trade-off between training efficiency and model performance for various data pruning methods on the OGB-MOLHIV dataset.  The x-axis represents the percentage of the original training time per epoch achieved by each method, while the y-axis shows the corresponding ROC-AUC score.  It highlights how GDeR achieves comparable or superior performance to other methods while significantly reducing training time.", "section": "4.2 GDeR makes GNN training way faster"}, {"figure_path": "O97BzlN9Wh/figures/figures_7_1.jpg", "caption": "Figure 4: Performance comparison of different pruning methods across various imbalance ratios. We utilize MUTAG and DHFR datasets with GCN, and reported the metrics when adjusting the imbalance ratios among {1:9, 3:7, 15:57:3, 9:1}. \u201cNo Pruning\u201d denotes training GCN without dataset pruning.", "description": "This figure compares the performance of various data pruning methods, including GDeR, across different imbalance ratios in the MUTAG and DHFR datasets using GCN.  The x-axis represents the imbalance ratio (minority:majority), while the y-axis shows the F1-macro score.  The \"No Pruning\" line represents the performance without any pruning. The different lines depict performance with different pruning techniques, highlighting GDeR's effectiveness in maintaining performance despite significant data imbalance.", "section": "4.3 GDeR Mitigates Graph Imbalance"}, {"figure_path": "O97BzlN9Wh/figures/figures_8_1.jpg", "caption": "Figure 5: (Left) We report the performance of several top-performing pruning methods when perturbation noise is added to 10% of the training set of MUTAG. The black dashed line represents the original GNN performance without pruning. (Right) We compare GDeR with DropEdge and GRAND under different noise settings, utilizing GDeR with pruning ratios of 10% and 30%.", "description": "The left graph shows the test accuracy of different pruning methods on MUTAG dataset with 10% noise added to the training data. The right graph compares the robustness of GDeR against DropEdge and GRAND under different noise levels (5%, 10%, 20%, and 30%).", "section": "4.4 GDeR Aids in GNN Robustness"}]