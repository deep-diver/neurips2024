[{"figure_path": "O97BzlN9Wh/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison to state-of-the-art dataset pruning methods when remaining {20%, 30%, 50%, 70%} of the full set. All methods are trained using PNA, and the reported metrics represent the average of five runs.", "description": "This table presents a comparison of the performance of various dataset pruning methods, including GDeR, against the full dataset.  The methods are tested at different pruning ratios (20%, 30%, 50%, and 70%), with the results averaged over five runs. The performance metric used is ROC-AUC for OGBG-MOLHIV and Average Precision (AP) for OGBG-MOLPCBA.  The table shows GDeR's performance relative to other static and dynamic pruning techniques.", "section": "4.2 GDeR makes GNN training way faster"}, {"figure_path": "O97BzlN9Wh/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparison to state-of-the-art dataset pruning methods when remaining {20%, 30%, 50%, 70%} of the full set. All methods are trained using PNA, and the reported metrics represent the average of five runs.", "description": "This table compares the performance of GDeR against 14 static and 3 dynamic data pruning methods across four different remaining ratios (20%, 30%, 50%, 70%) on the OGBG-MOLHIV and OGBG-MOLPCBA datasets. The performance metric used is ROC-AUC for OGBG-MOLHIV and AP for OGBG-MOLPCBA.  All methods are trained using the PNA backbone. The results are averages over five runs.", "section": "4.2 GDeR makes GNN training way faster"}, {"figure_path": "O97BzlN9Wh/tables/tables_8_1.jpg", "caption": "Table 3: Ablation study on GDeR and its three variants. \"Imbalance\" refers to setting the imbalance ratio to be {1 : 9}, and \u201cNoisy\u201d refers to adding 5% noise to the training set. All metrics are reported under 30% pruning ratio.", "description": "This table presents the ablation study results for the proposed GDeR method. It shows the performance of GDeR under normal, imbalanced, and noisy training scenarios when different components (outlier risk assessment, sample familiarity metric, and sample balancing score) are removed.  The results demonstrate the contribution of each component to the overall performance of GDeR.", "section": "4.5 Ablation & Sensitivity Study"}, {"figure_path": "O97BzlN9Wh/tables/tables_8_2.jpg", "caption": "Table 2: Performance comparison to state-of-the-art dataset pruning methods. All methods are trained using GraphGPS, and the reported metrics represent the average of five runs.", "description": "This table compares the performance of GDeR against other state-of-the-art dataset pruning methods using the GraphGPS model.  It shows the ROC-AUC and AP scores for different pruning ratios (20%, 30%, 50%, 70%) across two datasets (OGBG-MOLHIV and OGBG-MOLPCBA).  The results highlight GDeR's performance relative to both static and dynamic pruning methods under various data reduction levels.", "section": "4.2 GDeR makes GNN training way faster"}, {"figure_path": "O97BzlN9Wh/tables/tables_16_1.jpg", "caption": "Table 1: Performance comparison to state-of-the-art dataset pruning methods when remaining {20%, 30%, 50%, 70%} of the full set. All methods are trained using PNA, and the reported metrics represent the average of five runs.", "description": "This table compares the performance of GDeR against other state-of-the-art dataset pruning methods on two datasets (OGBG-MOLHIV and OGBG-MOLPCBA) using the PNA backbone.  The table shows the performance (ROC-AUC for OGBG-MOLHIV and Average Precision for OGBG-MOLPCBA) achieved by each method when only a subset of the original data (20%, 30%, 50%, or 70%) is used for training.  The results highlight GDeR's performance relative to other methods, both static and dynamic.", "section": "4.2 GDeR makes GNN training way faster"}, {"figure_path": "O97BzlN9Wh/tables/tables_16_2.jpg", "caption": "Table 1: Performance comparison to state-of-the-art dataset pruning methods when remaining {20%, 30%, 50%, 70%} of the full set. All methods are trained using PNA, and the reported metrics represent the average of five runs.", "description": "This table compares the performance of GDeR against other state-of-the-art dataset pruning methods across four different pruning ratios (20%, 30%, 50%, and 70%).  The performance is measured using metrics appropriate for each dataset (ROC-AUC and AP for OGBG datasets, Accuracy and F1-macro for MUTAG and DHFR). The results showcase GDeR's effectiveness in maintaining or improving performance even with significant data reduction.", "section": "4.2 GDeR makes GNN training way faster"}, {"figure_path": "O97BzlN9Wh/tables/tables_19_1.jpg", "caption": "Table 7: Graph pre-training performance of GDeR on GraphMAE [47]+ZINC15 [104]. Following [47], the model is first pre-trained in 2 million unlabeled molecules sampled from the ZINC15, and then finetuned in 3 classification benchmark datasets contained in MoleculeNet [105].", "description": "This table shows the performance of GDeR on the GraphMAE model pre-trained with ZINC15 dataset.  It presents the results of three benchmark datasets (BBBP, ToxCast, BACE) with different remaining ratios of the data (30%, 50%, 70%). For each dataset and remaining ratio, the original performance and the performance after applying GDeR are shown, along with the training time and speedup achieved by GDeR. The table demonstrates that GDeR is capable of achieving higher accuracy or comparable accuracy while significantly reducing training time compared to the original training without pruning.", "section": "4 Experiments"}, {"figure_path": "O97BzlN9Wh/tables/tables_19_2.jpg", "caption": "Table 1: Performance comparison to state-of-the-art dataset pruning methods when remaining {20%, 30%, 50%, 70%} of the full set. All methods are trained using PNA, and the reported metrics represent the average of five runs.", "description": "This table presents a comparison of GDeR's performance against other state-of-the-art static and dynamic dataset pruning methods.  The comparison uses the PNA architecture and evaluates performance across four different remaining ratios of the full dataset (20%, 30%, 50%, and 70%).  Reported metrics are averages across five independent runs.  The table allows assessment of GDeR's efficiency and performance relative to existing techniques.", "section": "4.2 GDeR makes GNN training way faster"}]