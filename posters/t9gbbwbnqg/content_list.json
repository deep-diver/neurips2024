[{"type": "text", "text": "Layer-Adaptive State Pruning for Deep State Space Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minseon Gwakt, Seongrok Moont, Joohwan $\\mathbf{Ko^{\\ddagger}}$ , PooGyeon Parkt\\* ", "page_idx": 0}, {"type": "text", "text": "1 Department of Electrical Engineering, POSTECH Department of Computer Science, University of Massachusetts Amherst {minseon25, srmoon,ppg}@postech.ac.kr, joohwanko@cs.umass.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Due to the lack of state dimension optimization methods, deep state space models (SSMs) have sacrificed model capacity, training search space, or stability to alleviate computational costs caused by high state dimensions. In this work, we provide a structured pruning method for SSMs, Layer-Adaptive STate pruning (LAST), which reduces the state dimension of each layer in minimizing model-level energy loss by extending modal truncation for a single system. LAST scores are evaluated using $\\mathcal{H}_{\\infty}$ norms of subsystems for each state and layer-wise energy normalization. The scores serve as global pruning criteria, enabling cross-layer comparison of states and layer-adaptive pruning. Across various sequence benchmarks, LAST optimizes previous SSMs, revealing the redundancy and compressibility of their state spaces. Notably, we demonstrate that, on average, pruning $33\\%$ of states still maintains performance with $0.52\\%$ accuracy loss in multi-input multi-output SSMs without retraining. Code is available at https : / / github . Com/msgwak /LAST. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep state space models (SSMs) have proven effective in modeling sequential data by optimally compressing input history to internal states [Gu et al., 2020, 2021, 2022b, Gu and Dao, 2023, Zhang et al., 2023, Parnichkun et al., 2024]. Given their modeling capabilities, ensuring the feasibility and stability of SSMs during training has become a crucial research focus for achieving efficient learning without divergence. Leveraging the knowledge founded in linear system theory [Kailath, 1980], various advancements have emerged, including stability-guaranteeing parameterization [Gu et al., 2022a], general system architecture [Smith et al., 2023], and efficiency improvements via frequency-domain operations, utilizing the fast Fourier transform and the transfer functions of systems [Gu et al., 2022b,a, Zhang et al., 2023, Parnichkun et al., 2024]. ", "page_idx": 0}, {"type": "text", "text": "One of the main computation and memory contributors of SSMs is the state dimension $n$ .Since the initial proposal of SSMs, a multiple single-input single-output (multi-SISO) architecture has been employed for scalable and efficient training Gu et al. [2022b,a], Gu and Dao [2023], Zhang et al. [2023], Parnichkun et al. [2024]. In this architecture, rather than directly learning an $n$ -dimensional system, smaller-dimensional SIsO systems are trained in parallel and then integrated through a channel-mixing layer. Within this structure, Gupta et al. [2022] presented that diagonal systems can achieve matching performance to nondiagonal systems. Gu et al. [2022a] introduced a stabilityguaranteed model, where the diagonal systems are trained to satisfy the necessary and sufficient stability condition. ", "page_idx": 0}, {"type": "text", "text": "Instead of utilizing multiple SISO systems in parallel, Smith et al. [2023] adopted a multi-input multi-output (MIMO) architecture, where the enhanced information usage through a MIMO system. ", "page_idx": 0}, {"type": "image", "img_path": "T9GbbWbNQG/tmp/69352bfd5f80b75e7d9a05100b56d41760f2d20d94fc662f2e4ec5f65ec3de94.jpg", "img_caption": ["Figure 1: Mlustration of LAST for two layers. Matrices are divided by lines on a per-state basis, and subsystems are sorted in descending order by their $\\mathcal{H}_{\\infty}$ norms. LAST scores are obtained by normalizing each $\\mathcal{H}_{\\infty}$ norm by the sum of all $\\mathcal{H}_{\\infty}$ norms in a layer when the states with lower $\\mathcal{H}_{\\infty}$ norms are excluded. Since LAST scores correlate with model-level output energy loss, we prune all parameters corresponding to states with low LAST scores. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "This architecture provides high performance with much smaller state dimensions than equivalent block systems in multi-SISO layers. For instance, in the Pat h-X task that involves the longest tested sequences, this architecture showed state-of-the-art performance [Smith et al., 2023, Parnichkun et al., 2024]. However, both architectures lack optimization methods for state dimensions, leading to inefficiencies when the model is over-parameterized for the task. ", "page_idx": 1}, {"type": "text", "text": "Recently, Parnichkun et al. [2024] parameterized the transfer functions of SIsO systems and proposed a state-free inference. However, this approach indirectly trains the poles of the transfer functions, resulting in a restrictive search space or stability being guaranteed only at initialization. ", "page_idx": 1}, {"type": "text", "text": "Focusing on the stability-guaranteed diagonal SSMs, we develop and verify a layer-adaptive model order reduction (MOR) method for SSMs to identify the least significant states or subsystems in terms of their impact on task performance. Inspired by layer-adaptive neural network pruning [Evci et al., 2020, Lee et al., 2021, Xu et al., 2023] and extending the traditional MOR for a single system [Green and Limebeer, 2012], we propose Layer-Adaptive STate pruning (LAST), where importance scores for learned states are evaluated and used as global pruning criteria. LAST scores measure the relative maximum frequency-domain gain of each subsystem when subsystems with lower scores are excluded, as illustrated in Figure 1. LAST prunes insignificant subsystems to achieve a desired compression level, reducing unnecessary computational and memory costs while bounding the output distortionby the $\\mathcal{H}_{\\infty}$ norms of the pruned subsystems. ", "page_idx": 1}, {"type": "text", "text": "We validate the insignificant state identification performance of LAST on long-range sequences, including Long Range Arena (LRA) [Tay et al., 2021] and Speech Command [Warden, 2018] benchmarks. Our results present that previous SSMs have great compressibility, demonstrating that pruning $33\\%$ $(26.25\\%)$ of the trained states resulted in only $0.52\\%$ $(0.32\\%)$ of accuracy loss in MIMO models (in multi-SISO models) on average, including the non-compressible cases. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1  Stability of state space models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "A DT SSM is stable if all poles, roots of a denominator, of its transfer function lie within the unit circle. However, it is challenging to train systems to ensure stability at every step. One approach for this issue is to confine the search space to sufficient stable region [Zhang et al., 2023], as illustrated in Figure 5 for a second-order linear time-invariant (LTI) system. Due to the restricted search space, training under this condition can limit model performance [Parnichkun et al., 2024]. Another approach is initializing the system at the center of the stable region, as marked in Figure 5, referred to as zero initialization in [Parnichkun et al., 2024]. While this approach mitigates the performance limitation, the stability is guaranteed only at initialization. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In contrast, diagonal SSMs [Gupta et al., 2022, Gu et al., 2022a, Smith et al., 2023] directly parameterize the system poles, enabling the model to explore all expressible systems that possess stability-satisfying poles. Thus, all our derivations are based on the diagonal SSMs to leverage the guaranteed stability, which allows for the application of various system analysis techniques. Detailed explanations on the stability regions are provided in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "2.2  Diagonal state space models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Architectures.  Diagonal SSMs consist of an encoder that increases the number of input channels to $h$ $L$ SSM layers, and a decoder for the downstream task. Each SSM layer can be designed with either a multi-SISO or MIMO architecture. In the multi-SISO architecture [Gu et al., 2022a], independent systems are trained for each input channel, with a total of $h$ $n_{s}$ th-order SISO systems being learned in a layer. A fully connected layer is then used to mix features from different channels. In contrast, the MIMO architecture [Smith et al., 2023] employs an $n_{m}$ th-order MIMO system within each layer, handling $h$ -dimensional input and output signals. As noted in Smith et al. [2023], $h$ SISO systems in a layer can be represented as one MIMO system, where specific states are assigned to each input channel. Therefore, we describe SSM layers using MIMO expressions, defining the effective total state dimension for an SSM layer by $n$ , where $n=n_{s}h$ for a multi-SISO layer and $n=n_{m}$ for a MIMO layer. ", "page_idx": 2}, {"type": "text", "text": "Parameterization. The learnable parameters in a diagonal SSM layer with state dimension $n$ are CT system matrices $\\mathbf{A}\\in\\mathbb{C}^{n\\times n}$ \uff01\uff0c $\\mathbf{B}\\in\\mathbb{C}^{n\\times h}$ \uff0c $\\mathbf{C}\\in\\breve{\\mathbb{C}}^{h\\times n}$ $\\mathbf{D}\\in\\mathbb{R}^{\\check{h}\\times h}$ , where $\\mathbf{A}\\in\\mathbb{C}^{n\\times n}$ is a diagonal matrix and complex-valued matrices consist of elements that form conjugate pairs to handle real-valued signals [Gu et al., 2022a]. In the diagonal structure, each subsystem is discretized by applying different timescales from $\\Delta\\in\\mathbb{R}^{n}$ to process discrete sequences. By zero-order hold (ZOH) discretization [Chen, 1984], a discretized LTI diagonal system $\\Sigma:\\mathbf{u}\\mapsto\\mathbf{y}$ in a layer $f_{\\sigma}({\\mathbf{u}}_{k};\\Sigma)$ can be represented as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf x}_{k+1}=\\overline{{\\bf{A}}}{\\bf x}_{k}+\\overline{{\\bf{B}}}{\\bf u}_{k},\\qquad{\\bf y}_{k}={\\bf C}{\\bf x}_{k}+{\\bf D}{\\bf u}_{k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\overline{{\\Lambda}}=e^{\\Lambda\\Delta}$ and $\\overline{{\\mathbf{B}}}=\\mathbf{A}^{-1}(\\overline{{\\mathbf{A}}}-\\mathbf{I}_{n})\\mathbf{B}$ are the discretized system matrices, $\\mathbf{x}_{k}\\in\\mathbb{C}^{n}$ is a state vector, $\\mathbf{u}_{k}\\in\\mathbb{R}^{h}$ is an input signal, and $\\mathbf{y}_{k}\\in\\mathbb{R}^{h}$ is an output signal. The stability of the discretized system can be achieved by ensuring the stability of the CT parameters with Hurwitz parameterization [Gu et al., 2022a], as derived in Appendix A.2. Finally, a nonlinear activation function $\\sigma(\\cdot)$ is applied to the output of the linear system, i.e., $f_{\\sigma}(\\mathbf{u}_{k};\\Sigma)=\\bar{\\sigma}(\\Sigma(\\mathbf{u}_{k}))$ , introducing nonlinearity to the SSM. ", "page_idx": 2}, {"type": "text", "text": "2.3 $\\mathcal{H}_{\\infty}$ norms of systems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In robust control, the $\\mathcal{H}_{\\infty}$ norm is widely used to minimize the worst-case gain from disturbances to outputs, ensuring stability and performance under system uncertainty [Qin and Sun, 2023, Zheng et al., 2023]. In this work, we use the $\\mathcal{H}_{\\infty}$ norm to measure the divergence between the original and approximatedsystems. ", "page_idx": 2}, {"type": "text", "text": "For a DT LTI system $\\Sigma:\\mathbf{u}\\mapsto\\mathbf{y}$ with the transfer function matrix (TFM) $\\mathbf{G}$ , the $\\mathcal{H}_{\\infty}$ norm of the system is defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\mathbf{G}\\|_{\\infty}:=\\operatorname*{sup}_{\\theta\\in[0,2\\pi]}\\overline{{\\sigma}}(\\mathbf{G}(e^{j\\theta})),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\overline{{\\sigma}}$ denotes the maximum singular value of a matrix. In robust control design, the $\\mathcal{H}_{\\infty}$ norm is frequently minimized to design controllers that ensure the system performs optimally under disturbance. ", "page_idx": 2}, {"type": "text", "text": "In this work, we utilize the following important property of the $\\mathcal{H}_{\\infty}$ norm, that is, the energy of the output signal $\\|\\mathbf{y}\\|_{2}^{2}$ can be bounded with the squared $\\mathcal{H}_{\\infty}$ norm and the energy of the input signal $\\|\\mathbf{u}\\|_{2}^{2}$ ,i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|\\mathbf{y}\\|_{2}^{2}\\leq\\|\\mathbf{G}\\|_{\\infty}^{2}\\|\\mathbf{u}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "(See Appendix B.1 for derivation). In other words, the $\\mathcal{H}_{\\infty}$ norm of a system measures the maximum gain of the system, which is useful in assessing the energy loss caused by pruning. ", "page_idx": 2}, {"type": "text", "text": "3  LAST: Layer-adaptive state pruning for SSMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose a structured pruning for SSMs with per-state pruning granularity, where all parameters associated with the identified insignificant state are pruned. Although pruning is implemented by masking, we represent pruned systems with their effective remaining states and parameters. ", "page_idx": 3}, {"type": "text", "text": "In Section 3.1, we derive a local pruning criterion by evaluating the layer-level energy loss for a single SSM layer, which consists of a MIMO system followed by nonlinear activation. In Section 3.2, we extend this to a global pruning criterion by assessing the model-level energy loss when considering multiple stacked SSM layers. ", "page_idx": 3}, {"type": "text", "text": "3.1 $\\mathcal{H}_{\\infty}$ scores as local pruning criteria ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "From a DT system $\\Sigma:(\\overline{{\\Lambda}},\\overline{{\\mathbf{B}}},\\mathbf{C})$ , suppose we prune the ith subsystem $\\Sigma_{i}:(\\overline{{\\mathbf{\\Lambda}}}_{i},\\overline{{\\mathbf{B}}}_{i},\\mathbf{C}_{i})$ corresponding to the $i$ th state $\\mathbf{x}_{i}$ , leaving the $i$ th state-pruned system $\\Sigma_{-i}$ . Specifically, the state-pruned system can be written as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{-i}:\\left(\\begin{array}{l}{\\overline{{\\mathbf{A}}}_{-i}=\\mathrm{diag}(\\overline{{\\lambda}}_{1},\\cdot\\cdot\\cdot\\cdot\\mathbf{\\sigma},\\overline{{\\lambda}}_{i-1},\\overline{{\\lambda}}_{i+1},\\cdot\\cdot\\cdot\\mathbf{\\sigma},\\overline{{\\lambda}}_{n}),}\\\\ {\\overline{{\\mathbf{B}}}_{-i}^{\\top}=[\\begin{array}{l l l l l l}{\\overline{{\\mathbf{B}}}_{1}^{\\top}}&{\\cdot\\cdot}&{\\overline{{\\mathbf{B}}}_{i-1}^{\\top}}&{\\overline{{\\mathbf{B}}}_{i+1}^{\\top}}&{\\cdot\\cdot\\cdot}&{\\overline{{\\mathbf{B}}}_{n}^{\\top}}\\end{array}],\\right),}\\\\ &{\\mathbf{C}_{-i}=[\\begin{array}{l l l l l l}{\\mathbf{C}_{1}}&{\\cdot\\cdot}&{\\mathbf{C}_{i-1}}&{\\mathbf{C}_{i+1}}&{\\cdot\\cdot}&{\\mathbf{C}_{n}}\\end{array}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\overline{{\\boldsymbol{\\Lambda}}}=\\mathrm{diag}(\\overline{{\\boldsymbol{\\lambda}}}_{1},\\cdot\\cdot\\cdot\\ ,\\overline{{\\boldsymbol{\\lambda}}}_{n}),\\overline{{\\mathbf{B}}}^{\\top}=\\left[\\begin{array}{l l l}{\\overline{{\\mathbf{B}}}_{1}^{\\top}}&{\\cdot\\cdot}&{\\overline{{\\mathbf{B}}}_{n}^{\\top}}\\end{array}\\right]}\\end{array}$ , with $\\mathbf{C}={\\left[\\begin{array}{l l l}{\\mathbf{C}_{1}}&{\\cdots}&{\\mathbf{C}_{n}}\\end{array}\\right]}$ for $\\overline{{\\mathbf{B}}}_{i}\\in$ ", "page_idx": 3}, {"type": "text", "text": "$\\mathbb{C}^{1\\times h}$ and $\\mathbb{C}^{h\\times1}$ . Our objective is to minimize the layer-level output energy loss, defined as the squared $\\ell_{2}$ distortion in the output signal, incurred by the system approximation through state pruning. The optimization is formalized by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathcal{P}\\subset\\mathcal{S}}{\\mathrm{minimize}}}&{{}\\left\\|f_{\\sigma}(\\mathbf{u};\\Sigma)-f_{\\sigma}(\\mathbf{u};\\Sigma_{-\\mathcal{P}})\\right\\|_{2}^{2}}\\\\ {\\mathrm{subject\\,}\\mathrm{to}}&{{}|\\mathcal{P}|\\leq r,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $S=\\{1,\\cdot\\cdot\\cdot,n\\}$ is the set of state indices in the full system, $\\mathcal{P}$ is the set of pruned state indices, and $r$ is the desired state dimension. ", "page_idx": 3}, {"type": "text", "text": "Using the properties of diagonal systems and the $\\mathcal{H}_{\\infty}$ norm in Equation (2), the energy loss can be bounded as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\boldsymbol{f}_{\\sigma}(\\mathbf{u};\\Sigma)-{f}_{\\sigma}(\\mathbf{u};\\Sigma_{-\\mathcal{P}})\\right\\|_{2}^{2}\\leq\\sum_{i\\in\\mathcal{P}}\\left\\|\\mathbf{G}_{i}\\right\\|_{\\infty}^{2}\\left\\|\\mathbf{u}\\right\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $-{\\mathcal{P}}:=S\\setminus{\\mathcal{P}}$ and $\\mathbf{G}_{i}$ is the TFM of $\\Sigma_{i}$ (See Appendix B.2 for proof). Therefore, we can reduce a system by pruning subsystems with small $\\mathcal{H}_{\\infty}$ norms, minimizing the upper bound in Equation (3). This result shows that, even in the presence of nonlinearity, pruning for a single layer can be performed similarly to modal truncation [Green and Limebeer, 2012]. As the stability is guaranteed by Hurwitz parameterization [Gu et al., 2022a], the $\\mathcal{H}_{\\infty}$ norm of a subsystem is evaluated as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{G}_{i}\\right\\|_{\\infty}=\\frac{\\left\\|\\mathbf{C}_{i}\\overline{{\\mathbf{B}}}_{i}\\right\\|}{1-\\left|\\overline{{\\boldsymbol{\\lambda}}}_{i}\\right|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, the importance of $\\mathbf{x}_{i}$ can be defined by the squared $\\mathcal{H}_{\\infty}$ norm of $\\Sigma_{i}$ with a minor optimization of computational efficiency for rank-1 matrix $\\mathbf{C}_{i}\\overline{{\\mathbf{B}}}_{i}$ as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{H}_{\\infty}\\big(\\mathbf{x}_{i};\\boldsymbol{\\Sigma}\\big)=\\frac{\\left\\lVert\\mathbf{C}_{i}\\right\\rVert^{2}\\left\\lVert\\overline{{\\mathbf{B}}}_{i}\\right\\rVert^{2}}{\\left(1-\\left|\\overline{{\\lambda}}_{i}\\right|\\right)^{2}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{H}_{\\infty}(\\mathbf{x}_{i};\\Sigma)$ refers to the $\\mathcal{H}_{\\infty}$ score of $\\mathbf{x}_{i}$ , and we prioritize pruning states with lower scores. This can also be simplified with $\\|\\overline{{\\mathbf{B}}}_{i}\\|^{2}=1$ when $\\overline{{\\bf B}}$ is fixed while $\\mathbf{C}$ is trained. Moreover, when two $\\mathbf{C}$ matrices are used for bidirectional SSMs, $\\|\\mathbf{C}_{i}\\|^{2}$ can be substituted as the average for the two matrices, i.e., $\\|\\mathbf{C}_{i}\\|^{2}=(\\|\\mathbf{C}_{i}^{f}\\|^{2}+\\|\\mathbf{C}_{i}^{b}\\|^{2})/2$ , where $\\|\\mathbf{C}_{i}^{f}\\|$ is for forward direction and $\\|\\mathbf{C}_{i}^{b}\\|$ is for backward direction. ", "page_idx": 3}, {"type": "text", "text": "The $\\mathcal{H}_{\\infty}$ score can be used as a local pruning criterion once the target pruning ratio for each layer is determined. However, this approach has limitations, as it requires a heuristic to determine the pruning ratio for each layer and applies the same amount of pruning without considering layer-specific characteristics. ", "page_idx": 3}, {"type": "text", "text": "3.2  LAST scores as global pruning criteria ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To extend the local pruning criterion to a global pruning criterion, we now consider the model-level output energy loss incurred by pruning $L$ layers. In the following description, superscripts indicate layer indices from 1 to $L$ for signals, systems, and state index sets. ", "page_idx": 4}, {"type": "text", "text": "Following the notation in Lee et al. [2021], $\\mathrm{\\DeltaXu}$ et al. [2023], the output of $k$ th layer is obtained by recursively applying the preceding systems and activation functions as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{\\sigma}(\\mathbf{u}^{(1)};\\Sigma^{(1:k)})=\\sigma(\\Sigma^{(k)}(f_{\\sigma}(\\mathbf{u}^{(1)};\\Sigma^{(1:k-1)}))).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our objective is to minimize the model-level output energy loss as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathcal{P}^{(l)}\\subset\\mathcal{S}^{(l)}}{\\mathrm{minimize}}}&{\\|f_{\\sigma}(\\mathbf{u}^{(1)};\\Sigma^{(1:L)})-f_{\\sigma}(\\mathbf{u}^{(1)};\\widehat{\\Sigma}^{(1:L)})\\|_{2}^{2}}\\\\ {\\mathrm{subject\\,to}}&{\\sum_{l=1}^{L}|\\mathcal{P}^{(l)}|\\le R,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where () = and $R$ is the sum of desired state dimensions in all layers. Similar to Lee et al. [2021], we consider a greedy iterative optimization, where we decide the next pruning state $x_{i}^{(l)}$ by optimizing the following problem for every step: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{minimize}_{l\\in\\{1,\\cdots,L\\},\\;i\\in S_{t}^{(l)}}\\,J_{l}\\big(i;\\,\\widetilde{\\Sigma}_{t}^{(1:L)}\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $J_{l}\\big(i;\\widetilde\\Sigma_{t}^{(1:L)}\\big):=\\big\\|f_{\\sigma}\\big(\\mathbf{u}^{(1)};\\widetilde\\Sigma_{t}^{(1:L)}\\big)-f_{\\sigma}\\big(\\mathbf{u}^{(1)};\\widetilde\\Sigma_{t}^{(1:l-1)},\\Sigma_{S_{t}^{(l)}\\setminus\\{i\\}}^{(l)},\\widetilde\\Sigma_{t}^{(l+1:L)}\\big)\\big\\|_{2}^{2},$ ,(+1:L)l2, denotes the step index, and ) := \u2265 With $S_{t}^{(l)}\\subset S^{(l)}$ indicatinn st $t$ step. The objective function in Equation (7) represents the model-level output energy loss when pruning a single subsystem in one layer among layers pruned to different extents. By the proof provided in the Appendix B.3, the objective function can be upper-bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ_{l}\\big(i;\\,\\widetilde{\\Sigma}_{t}^{(1:L)}\\big)\\leq\\frac{\\big\\|\\mathbf{G}_{i}^{(l)}\\big\\|_{\\infty}^{2}}{\\big\\|\\mathbf{G}_{S_{t}^{(l)}}^{(l)}\\big\\|_{\\infty}^{2}}\\prod_{k=1}^{L}\\Big\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\big\\|_{\\infty}^{2}\\big\\|\\mathbf{u}^{(1)}\\big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, the upper bound for a subsystem correlates with the ratio of the squared $\\mathcal{H}_{\\infty}$ norm of the subsystem to the squared $\\mathcal{H}_{\\infty}$ norm of the remaining system for layer $l$ . The other terms, except for the ratio, in Equation (8) are common across all layers and can, therefore, be excluded from the cross-layer importance score calculation. ", "page_idx": 4}, {"type": "text", "text": "Although we initially considered an iterative optimization to determine the next pruning state, the important scores for all states in all layers can be computed with a few steps, as each score is independently determined based on the trained parameters. For efficient evaluation of the scores, we sort the subsystems in each layer in descending order with their $\\mathcal{H}_{\\infty}$ norms in advance, s.t., $\\mathcal{H}_{\\infty}\\big(\\mathbf{x}_{i}^{(l)};\\Sigma^{(l)}\\big)>\\mathcal{H}_{\\infty}\\big(\\mathbf{x}_{j}^{(l)};\\Sigma^{(l)}\\big)$ for $i<j$ Finally, wedefine the LAST score for $\\mathbf{x}_{i}^{(l)}$ as folows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{L A S T}\\left(\\mathbf{x}_{i}^{(l)};\\Sigma^{(l)}\\right)=\\frac{\\mathcal{H}_{\\infty}\\left(\\mathbf{x}_{i}^{(l)};\\Sigma^{(l)}\\right)}{\\sum_{j\\leq i}\\mathcal{H}_{\\infty}\\left(\\mathbf{x}_{j}^{(l)};\\Sigma^{(l)}\\right)}}\\\\ {=\\left(\\frac{\\left\\|\\mathbf{C}_{i}^{(l)}\\right\\|^{2}\\left\\|\\overline{{\\mathbf{B}}}_{i}^{(l)}\\right\\|^{2}}{\\left(1-\\left|\\overline{{\\lambda}}_{i}^{(l)}\\right|\\right)^{2}}\\right)\\bigg/\\sum_{j\\leq i}\\left(\\frac{\\left\\|\\mathbf{C}_{j}^{(l)}\\right\\|^{2}\\left\\|\\overline{{\\mathbf{B}}}_{j}^{(l)}\\right\\|^{2}}{\\left(1-\\left|\\overline{{\\lambda}}_{j}^{(l)}\\right|\\right)^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similar to the local pruning criterion, Equation (10) can be modified for the case of using fixed $\\overline{{\\mathbf{B}}}^{(l)}$ or bidirectional SSMs. The LAST score for each state reveals the contribution of the subsystem to the model output by assessing the relative gain within the remaining system in a layer, thereby indicating the significance of the subsystem. We refer to this relative metric calculation as energy normalization, which adjusts the state importance from different layers to a comparable scale, enabling a cross-layer comparison of the states from different layers. In this way, states with lower LAST scores are selected from the overall states, and layer-adaptive pruning is performed according to the desired model-level compressionrate. ", "page_idx": 4}, {"type": "text", "text": "4 Related works ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Model order reduction. In linear system theory, MOR methods have been extensively researched to approximate high-dimensional systems in engineering applications, such as VLSI [Antoulas and Sorensen, 2001], power systems [Li and White, 1999], and various systems that employ spatial discretization [Jones and Kerrigan, 2010, Curtain and Zwart, 2012, Penzl, 2006]. Using the $\\mathcal{H}_{\\infty}$ norm to characterize a stable system, modal truncation [Green and Limebeer, 2012] removes states from a diagonal realization for minimal $\\mathcal{H}_{\\infty}$ norm distortion of the system. Balanced truncation [Khalil et al., 1996, Safonov and Chiang, 1988] transforms a given system into a form, not necessarily diagonal, where all states are controllable and observable, then truncates the transformed system. Due to its superior approximation quality, balanced truncation has been developed for various systems and conditions [Petreczky et al., 2013, Besselink et al., 2014, Cheng et al., 2019]. However, transformation into non-diagonal systems is not applicable to current diagonal SSMs, where diagonal parameterization is necessary for computational efficiency and stability. In our work, per-state pruning granularity operates similarly to modal truncation. Compared to traditional MOR, which is reducing a single linear system, we extend it to multi-system reduction, where nonlinear functions are also involved, which have not been addressed in traditional system theory. ", "page_idx": 5}, {"type": "text", "text": "Layer-adaptive neural network pruning.  Using magnitude as a pruning criterion, previous works have demonstrated the superiority of layer-adaptive pruning, where layers have different pruning ratios [Morcos et al., 2019, Han et al., 2015, Mocanu et al., 2018, Evci et al., 2020, Lee et al., 2021, Xu et al., 2023], compared to uniform pruning [Zhu and Gupta, 2017, Gale et al., 2019]. In Han et al. [2015], Mocanu et al. [2018], Evci et al. [2020], layer-adaptive pruning was achieved by setting a specific magnitude threshold or target pruning ratio for each layer. In Morcos et al. [2019], Lee et al. [2021], layer-adaptive pruning was performed using a global pruning criterion and simultaneously comparing scores from different layers under the target pruning ratio. Specifically, Lee et al. [2021] proposed a global pruning criterion designed from the Frobenius norm-based upper bound of the worst-case $\\ell_{2}$ distortion caused by pruning one layer while fixing the other. Xu et al. [2023] advanced this approach into joint optimization for the sum of filtered layer-wise worst-case $\\ell_{2}$ distortionover pruning ratios. Inspired by Lee et al. [2021], we provide the first global pruning criterion for SSMs, where a non-magnitude-based criterion is essential due to the different transfer functions of SSMs compared to other neural networks. Lastly, we provide a missing design motivation for the squaring operation in score evaluation in Lee et al. [2021] by offering a clear rationale based on signal energy. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Table 1 presents the average performance of pruning methods for 10 tasks and 2 models. Further experimental details are explained below. ", "page_idx": 5}, {"type": "text", "text": "Models and tasks. Experiments were conducted with a single A6000 48GB or RTX 3090 24GB GPU. We verify our method on S4D (S4DLegS) [Gu et al., 2022a] and S5 [Smith et al., 2023] models, which are multi-SISO and MIMO SSMs, respectively. Although our main motivation was to reduce the state dimension of MIMO models, we also investigated the compressibility of multi-SISO models and the applicability of LAST to them. ", "page_idx": 5}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/b64293ba11b63f1274dd6fc3f511087b7c98c1662d86fd49c58962d9b1c44d09.jpg", "table_caption": ["Table 1: Average pruning ratio and accuracy loss for all tasks. Values in parentheses are evaluated by excluding non-compressible cases. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "The models were reproduced with three seeds according to the reported configurations [Gu et al., 2022a, Smith et al., 2023] for the six tasks in LRA benchmark [Tay et al., 2021], the raw speech classification task using Speech Commands dataset [Warden, 2018], and pixel-level image classification tasks using MNIST and CIFAR10 datasets. We evaluated the performance of the full (unpruned) and one-shot pruned models while freezing other parameters not involved with SSM layers. See Appendix C for more experimental details. ", "page_idx": 5}, {"type": "text", "text": "Baselines._ The unique transfer functions of SSMs require the state pruning granularity and $\\mathcal{H}_{\\infty}$ norm-based pruning criteria, not simple magnitude-based pruning criteria, as validated in Appendix D. ", "page_idx": 5}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/aafa87e9d155a48d6115763e2eb66df6c5a4e353f66e694328f427b934ab1fc3.jpg", "table_caption": ["Table 2: Accuracy of pruned models on LRA tasks. LAST is evaluated at the maximum tested pruning ratio with less than $1\\%$ accuracy loss, and other methods were evaluated for the same pruning ratios. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Here, we compare LAST with two pruning methods: Uniform $\\mathcal{H}_{\\infty}$ and Global $\\mathcal{H}_{\\infty}$ . Uniform $\\mathcal{H}_{\\infty}$ utilizes the local pruning criterion, $\\mathcal{H}_{\\infty}$ score, and applies the same pruning ratio to each layer. Global $\\mathcal{H}_{\\infty}$ employs $\\mathcal{H}_{\\infty}$ score as a global criterion, serving as the ablation of the energy normalization used in LAST. Moreover, we present random state pruning results to demonstrate the effectiveness of developed local and global pruning criteria in identifying insignificant states. After one-shot pruning, we evaluate whether these methods appropriately identify significant and insignificant states by measuring accuracy without retraining. ", "page_idx": 6}, {"type": "text", "text": "Pruning ratios. For models pruned by Global $\\mathcal{H}_{\\infty}$ or LAST, which apply layer-adaptive pruning ratios, the reported pruning ratios indicate the average pruning ratios across all layers. We compare Uniform $\\mathcal{H}_{\\infty}$ and layer-adaptive pruning methods, Global $\\mathcal{H}_{\\infty}$ and LAST, by setting the same desired compression rate. The tested pruning ratios were $10\\%$ \uff0c $20\\%$ .. $90\\%$ , and $100\\%$ , where a pruning ratio of $100\\%$ indicates the extreme case leaving only one pair of complex-conjugate subsystems in each layer. ", "page_idx": 6}, {"type": "text", "text": "5.1   Long range arena ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The LRA benchmark [Tay et al., 2021] has been used to evaluate the ability to capture long-range context from sequences with lengths ranging from 1,024 to 16,384. Table 2 shows the accuracy of models when each model is compressed to the maximum pruning ratio at which LAST achieved an accuracylossbelow $1\\%$ for LRAtasks. ", "page_idx": 6}, {"type": "text", "text": "Without any retraining, LAST outperformed other methods, achieving the average accuracy loss of $0.56\\%$ $(0.67\\%)$ for the average compression rate of $33.3\\%$ $(40.0\\%)$ , with (without) the noncompressible cases. Since the complexity and state dimension differ across tasks, achievable compression rate varied: for the most compressible case Text, $80\\%$ compression on S4D resulted in less than $1\\%$ loss in accuracy, while for the least compressible case Li st Ops, where the state dimension was initially set to 16 for S5 layers, even $10\\%$ compression led to large performance degradation. ", "page_idx": 6}, {"type": "text", "text": "Figure 2 shows the accuracies of S5 models at different pruning ratios, including randomly pruned models. For Pat hfinder and Path-X tasks, LAST consistently outperformed Uniform $\\mathcal{H}_{\\infty}$ and Global $\\mathcal{H}_{\\infty}$ , and the accuracy of Global $\\mathcal{H}_{\\infty}$ significantly dropped at high pruning ratios in these cases. ", "page_idx": 6}, {"type": "text", "text": "Moreover, we observed that the performance of Uniform $\\mathcal{H}_{\\infty}$ was comparable to LAST at low pruning ratios, whereas at high pruning ratios, its performance became inferior to LAST. We hypothesize that this was because the number of significant states in a layer was considerably lower than the number of original states. That is, if Uniform $\\mathcal{H}_{\\infty}$ pruning begins pruning beyond the lowest proportion of insignificant states in any layer, it can subsequently cause great accuracy degradation. See Appendix E.2 for full results in LRA tasks. ", "page_idx": 6}, {"type": "text", "text": "5.2 Raw speech classification ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The inductive bias and CT parameterization of SSMs enable 1) encoding raw speech without requiring feature engineering using methods such as short-time Fourier transform and 2) adapting to changes in sampling rate [Gu et al., 2022b, Goel et al., 2022, Gu et al., 2022a, Smith et al., 2023]. ", "page_idx": 6}, {"type": "image", "img_path": "T9GbbWbNQG/tmp/f1768bd8663730cc5ec6f6412340d827f70ea7994a1b90e7639be82ccdff46cf.jpg", "img_caption": ["Figure 2: Efficiency-accuracy trade-off curves of pruned S5 models for tasks in LRA benchmark. LAST maintained accuracy better than other methods, Uniform $\\mathcal{H}_{\\infty}$ andGlobal $\\mathcal{H}_{\\infty}$ (LASTwithout energy normalization), demonstrating its superior ability to identify insignificant states. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 10 presents that these properties remained consistent after pruning, as pruned models maintained their performance on raw speech and flexibly processed to sequences at different sampling rates by adjusting the learned timescales according to the sampling shifts, similarly to Gu et al. [2022b]. See Figure 8 for more results for Speech Command task. ", "page_idx": 7}, {"type": "text", "text": "5.3  Pixel-level image classification ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We applied pruning to tasks that classify sequenced images, including sequential MNIST (sMNI S T), permuted sequential MNIST (psMNI ST), and sequential CIFAR (sCIFAR), where sCIFAR is the colored version of Image task in LRA. LAST consistently exhibited the smallest accuracy loss on average. See Appendix E.1 for results on the pixel-level classification tasks. ", "page_idx": 7}, {"type": "text", "text": "5.4  Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "5.4.1 Ablation study on energy normalization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct an ablation study on the energy normalization of LAsT by Global $\\mathcal{H}_{\\infty}$ ,which is LAST without using energy normalization. LAST normalizes the differences in layer-wise signal amplification, enabling the cross-layer comparison of states on a common scale. Figure 3 shows the effect of the normalization in S5 models for Pat h-X task. In Layers 5 and 6, the overall $\\mathcal{H}_{\\infty}$ scores were relatively lower than other layers except Layer 1. Global $\\mathcal{H}_{\\infty}$ directly used $\\mathcal{H}_{\\infty}$ scores, resulting in excessive pruning in Layers 5 and 6 from the pruning ratio of $40\\%$ . However, LAST adjusted the scores by accounting for the low total energy transmission of the layers, making their states less prioritized in pruning. This led to different accuracy loss of methods as shown in Figure 2. ", "page_idx": 7}, {"type": "text", "text": "Moreover, energy normalization, which excludes pruned subsystems and normalizes accordingly, expands the range of high scores compared to normalizing without exclusions. In the case of Layer 1, this effect results in greater differences between LAST scores, making the scores distinguishable and pruning decisions easier. As a result, Layer 1 was identified to have more insignificant scores compared to other layers, leading to the removal of a large number of states. In conclusion, energy normalization was critical in the pruning process, ensuring a robust cross-layer comparison and preserving the model performance. ", "page_idx": 7}, {"type": "image", "img_path": "T9GbbWbNQG/tmp/df974ee4babad17c687be971610f327b7c3bbb39f9c957f068ac510ee16edcfd.jpg", "img_caption": ["Figure 3: (Top) Evaluated state importance score and (Bottom)  Figure 4: Remaining poles in remaining state dimensions in an S5 model for Pat h-X task. A(6) of an S5 model for Path-X The state indices are sorted by $\\mathcal{H}_{\\infty}$ scores, evaluated once for  task. each conjugate pair. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.4.2 Compressibility of models ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We considered S4D, a multi-SISO model, as the equivalent block diagonal MIMO model and applied the same pruning methods. While per-state structured pruning can completely remove state parameters, we implemented masking following the common practice in neural network pruning experiments. This approach allowed us to prune without compromising the parallelism of the multi-SIsO model. ", "page_idx": 8}, {"type": "text", "text": "We observed that although the effective state dimension is larger in multi-SISO models, the average pruning ratio that does not result in severe accuracy loss was smaller in multi-SIsO models $(25\\%)$ comparedtoMIMO $(33\\%)$ models. This is likely because, in multi-SISO, specific states are assigned to specific channels, meaning that each state is given a certain role. Consequently, pruning a single state can result in a greater loss. Additionally, this characteristic resulted in each subsystem exhibiting a significantlylow $\\mathcal{H}_{\\infty}$ norm. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Toward efficient training with guaranteed stability. Relying on guaranteed stability, previous diagonal SSMs have used as many state dimensions as possible due to the challenge of optimizing state dimensions for the task. Although excessive states are initially used, efficient and stable learning can be achieved if insignificant states are pruned under a well-planned pruning schedule ", "page_idx": 8}, {"type": "text", "text": "Given this objective, we first proposed an SSM pruning method that adaptively reduces the order of multiple systems within a deep layered structure with nonlinearity. We derived a local pruning criterion considering the nonlinearity and layer-level output energy loss, applying the criterion in theUniform $\\mathcal{H}_{\\infty}$ andGlobal $\\mathcal{H}_{\\infty}$ methods, which can also be viewed as independently applying traditional MOR to systems in each layer. However, we empirically verified that our method can be more robust than locally applying MOR, particularly when there are significant differences in the $\\mathcal{H}_{\\infty}$ norm scale or the proportion of important states across layers. As demonstrated in our application of the proposed method in multi-SIsO models, this approach can be applied alongside parallelism. ", "page_idx": 8}, {"type": "text", "text": "Which states are pruned? Lessons for future work. We investigated the pruned states, which have been judged insignificant for the task, presenting some insightful observations for future work. It is known that ${\\sf R e}\\{\\lambda_{i}\\}$ controls the decay rate and $\\operatorname{Im}\\{\\lambda_{i}\\}$ controls the oscillating frequencies of dynamics [Gu et al., 2022a, Chen, 1984]. As the $\\mathcal{H}_{\\infty}$ norm is computed with these values, large $\\left|\\mathrm{Re}\\{\\lambda_{i}\\}\\right|$ (fast decaying mode) and large $\\left|{\\mathrm{Im}}\\{\\lambda_{i}\\}\\right|$ (high-frequency dynamics) were more prone to be pruned, as shown in Figure 4. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Based on the insignificant pole characteristics, future work might explore new training strategies for SSMs, e.g., making poles constrained to avoid having the insignificant characteristics. Moreover, this provides a conjecture for the empirical effectiveness of the block-diagonal initialization in S5 [Smith et al., 2023], suggesting that the initialization performed well because it resulted in fewer large $|{\\mathrm{Im}}\\{\\lambda_{i}\\}|$ . Even using the block-diagonal initialization, we found that previous models tend to haveverylarge $\\left|{\\mathrm{Im}}\\{\\lambda_{i}\\}\\right|$ , e.g., over 1,0o0, which also could be addressed in future work. ", "page_idx": 9}, {"type": "text", "text": "Limitations. This paper has the following limitations. Although we explored the pruning criterion for SSMs, questions about when and how often to prune SSMs remained unresolved. Additionally, our proposed method was verified on a specific set of tasks, where both mult-SISO and MIMO models have been evaluated in previous work, and the adaptation to other tasks remains to be investigated. However, we believe that our work opens opportunities to utilize the full capacity of MIMO SSMs by making them as compact as possible, not sacrificing their capacity, search space, or stability. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT, and Future Planning (2020R1A2C2005709). This work was supported by Samsung Electronics Co., Ltd. (IO201211- 08100-01). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "A. C. Antoulas and D. C. Sorensen. Approximation of large-scale dynamical systems: An overview. International Journal of Applied Mathematics and Computer Science, 11(5):1093-1121, 2001.   \nB. Besselink, N. van de Wouw, J. M. Scherpen, and H. Nijmeijer. Model reduction for nonlinear systems by incremental balanced truncation. IEEE Transactions on Automatic Control, 59(10): 2739-2753,2014.   \nJ. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al. Jax: composable transformations of python $^{+}$ numpy programs. 2018.   \nC.-T. Chen. Linear system theory and design. Saunders college publishing, 1984.   \nH. Cheng, M. Zhang, and J. Q. Shi. A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \nX. Cheng, J. M. Scherpen, and B. Besselink. Balanced truncation of networked linear passive systems. Automatica, 104:17-25, 2019.   \nR. F. Curtain and H. Zwart. An introduction to infinite-dimensional linear systems theory, volume 21. Springer Science & Business Media, 2012.   \nU. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen. Rigging the lottery: Making all tickets winners. In International conference on machine learning, pages 2943-2952. PMLR, 2020.   \nT. Gale, E. Elsen, and S. Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.   \nK. Goel, A. Gu, C. Donahue, and C. R\u00e9. It's raw! audio generation with state-space models. In International Conference on Machine Learning, pages 7616-7633. PMLR, 2022.   \nM. Green and D. J. Limebeer. Linear robust control. Courier Corporation, 2012.   \nA. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \nA. Gu, T. Dao, S. Ermon, A. Rudra, and C. Re. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:1474-1487, 2020.   \nA. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572-585, 2021.   \nA. Gu, K. Goel, A. Gupta, and C. Re. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022a.   \nA. Gu, K.Goel, and C. Re. Effciently modeling lng sequences with sructured state space. In T International Conference on Learning Representations, 2022b.   \nA. Gupta, A. Gu, and J. Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-22994, 2022.   \nS. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.   \nY. He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pages 1389-1397, 2017.   \nD. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv: 1606.08415, 2016.   \nR. A. Horn and C. R. Johnson. Matrix analysis. Cambridge university press, 2012.   \nB. L. Jones and E. C. Kerrigan. When is the discretization of a spatially distributed system good enough for control? Automatica, 46(9):1462-1468, 2010.   \nT. Kailath. Linear systems, volume 156. Prentice-Hall Englewood Cliffs, NJ, 1980.   \nI. Khalil, J. Doyle, and K. Glover. Robust and optimal control. Prentice hall, 1996.   \nA. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \nJ. Lee, S. Park, S. Mo, S. An, and J. Shin. Layer-adaptive sparsity for the magnitue-based pruing. In The International Conference on Learning Representations, 2021.   \nJ.-R.Li and JWhiteEfcient model redction of interconect viaaproximat systm gramian In 1999 IEEE/ACM International Conference on Computer-Aided Design. Digest of Technical Papers (Cat. No. 99CH37051), pages 380-383. IEEE, 1999.   \nD. Linsley, J. Kim, V. Veerabadran, C. Windolf, and T. Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. Advances in neural information processing systems, 31, 2018.   \nA. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142-150, 2011.   \nD. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and A. Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):2383, 2018.   \nA. Morcos, H. Yu, M. Paganini, and Y. Tian. One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. Advances in neural information processing systems, 32,2019.   \nN. Nangia and S. R. Bowman. Listops: A diagnostic dataset for latent tre learning. arXiv preprint arXiv: 1804.06028, 2018.   \nB. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In Conference on learning theory, pages 1376-1401. PMLR, 2015.   \nR. N. Parnichkun, S. Massaroli, A. Moro, J. T. H. Smith, R. Hasani, M. Lechner, Q. An, C. R\u00e9, H. Asama, S. Ermon, T. Suzuki, A. Yamashita, and M. Poli. State-free inference of state-space models: The transfer function approach, 2024.   \nT. Penzl. Algorithms for model reduction of large dynamical systems. Linear algebra and its applications, 415(2-3):322-343, 2006.   \nM. Petreczky, R. Wisniewski, and J. Leth. Balanced truncation for linear switched systems. Nonlinear Analysis: Hybrid Systems, 10:4-20, 2013.   \nY. Qin and Z. Sun. Observer-based asynchronous event-triggered robust $H_{\\infty}$ adaptive switching control for nonlinear industrial cyber physical systems under data injection attacks. International Journal of Control, Automation and Systems, 21(7):2175-2182, 2023.   \nD. R. Radev, P. Muthukrishnan, and V. Qazvinian. The ACL Anthology network corpus. In M.-Y. Kan and S. Teufel, editors, Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 54-61, Suntec City, Singapore, Aug. 2009. Association for Computational Linguistics. URL https : / /aclanthology . Org/W09-3607.   \nS. Rush and S. Karamcheti. The annotated s4. In Blog Track at ICLR, 2022.   \nM. Safonov and R. Chiang. A schur method for balanced model reduction. In 1988 American Control Conference, pages 1036-1040. IEEE, 1988.   \nJ. T. Smith, A. Warrington, and S. Linderman. Simplified state space layers for sequence modeling. In The International Conference on Learning Representations, 2023.   \nY. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena : A benchmark for efficient transformers. In The International Conference on Learning Representations, 2021.   \nP. Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209, 2018.   \nK. Xu, Z. Wang, X. Geng, M. Wu, X. Li, and W. Lin. Efficient joint optimization of layer-adaptive weight pruning in deep neural networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17447-17457, 2023.   \nM. Zhang, K. K. Saab, M. Poli, T. Dao, K. Goel, and C. R\u00e9. Effectively modeling time series with simple discrete state spaces. The International Conference on Learning Representations, 2023.   \nQ. Zheng, W. Shi, K. Wu, and S. Jiang. Robust $H_{\\infty}$ and guaranteed cost filtering for ts fuzzy systems with multipath quantizations. International Journal of Control, Automation and Systems, 21(2): 671-683,2023.   \nM. Zhu and S. Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1 Introduction ", "page_idx": 12}, {"type": "text", "text": "2Background 2 ", "page_idx": 12}, {"type": "text", "text": "2.1 Stability of state space models 2   \n2.2 Diagonal state space models 3   \n2.3 $\\mathcal{H}_{\\infty}$ norms of systems . 3 ", "page_idx": 12}, {"type": "text", "text": "3  LAST: Layer-adaptive state pruning for SSMs 4 ", "page_idx": 12}, {"type": "text", "text": "3.1 $\\mathcal{H}_{\\infty}$ scores as local pruning criteria 4   \n3.2 LAST scores as global pruning criteria 5 ", "page_idx": 12}, {"type": "text", "text": "4 Related works 6 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "5 Experiments 6 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "5.1 Long range arena 7   \n5.2 Raw speech classification 7   \n5.3 Pixel-level image classification 8   \n5.4 Analysis .. 8   \n5.4.1 Ablation study on energy normalization 8   \n5.4.2 Compressibility of models 9 ", "page_idx": 12}, {"type": "text", "text": "6Discussion 9 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Stability of state space models 15 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1  Indirect pole training 15   \nA.1.1 Sufficient stability condition 15   \nA.1.2Stability guaranteed only at initialization 15   \nA.2  Direct pole training 16   \nProofs 16   \nB.1 System norm property . 16   \nB.2  Bounded layer-level output energy loss . . 16   \nB.3  Bounded model-level output energy loss 17   \nC  Experimental details 18   \nC.1  Tasks 19 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "C.2 Hyperparameters 20 ", "page_idx": 13}, {"type": "text", "text": "D   Validation of pruning granularity and criterion 20 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 State pruning granularity 20   \nD.2  Comparison with magnitude pruning 21 ", "page_idx": 13}, {"type": "text", "text": "E Full results 22 ", "page_idx": 13}, {"type": "text", "text": "E.1 Pixel-level image classification 22   \nE.2 Long range arena 23   \nE.3 Speech command 26 ", "page_idx": 13}, {"type": "text", "text": "A  Stability of state space models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1  Indirect pole training ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The rational transfer function of an $n$ th-order system can be defined by: ", "page_idx": 14}, {"type": "equation", "text": "$$\nH(z)=h_{0}+{\\frac{b_{1}z^{-1}+\\cdot\\cdot\\cdot+b_{n}z^{-n}}{1+a_{1}z^{-1}+\\cdot\\cdot\\cdot+a_{n}z^{-n}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "following the notation in Parnichkun et al. [2024]. For a second-order system, the characteristic function that determines stability is ", "page_idx": 14}, {"type": "equation", "text": "$$\na(z)=z^{2}+a_{1}z+a_{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For a DT system to be stable, the poles of the transfer function should be within the unit circle. This can be checked through the Schur-Cohn test [Kailath, 1980], which provides the stable region for the second-order system with the characteristic function $a(z)$ as: ", "page_idx": 14}, {"type": "equation", "text": "$$\na_{2}^{2}<1\\quad\\mathrm{and}\\quad(1+a_{2})^{2}-a_{1}^{2}>0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as shown in Figure 5. ", "page_idx": 14}, {"type": "text", "text": "A.1.1 Sufficient stability condition ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In models where the poles are trained indirectly, stability can be ensured by applying sufficient constraints for stable poles during training. Montel's constraint [Horn and Johnson, 2012] serves as a sufficient stability condition by restricting the coefficients as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left|a_{i}\\right|\\leq1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the second-order case, Montel's constraint is ", "page_idx": 14}, {"type": "equation", "text": "$$\n|a_{1}|+|a_{2}|\\leq1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This defines a sufficient stable region shown in Figure 5. However, as highlighted in Parnichkun et al.   \n[2024], this search space restriction can confine the model performance. ", "page_idx": 14}, {"type": "text", "text": "A.1.2 Stability guaranteed only at initialization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As an alternative, zero initialization [Parnichkun et al., 2024] initializes the system at the center point of the stable region. Thus, the initial coefficients of zero initialization for a second-order system are ", "page_idx": 14}, {"type": "equation", "text": "$$\na_{1}=0\\quad{\\mathrm{and}}\\quad a_{2}=0,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as marked in Figure 5. However, this does not guarantee stability in subsequent training, which potentially causes states to diverge and makes training infeasible. ", "page_idx": 14}, {"type": "image", "img_path": "T9GbbWbNQG/tmp/e74dfe91668a4c6729c34c1feb70636984ae5a0bb06183460c8eb7cc30d2df41.jpg", "img_caption": ["Figure 5: Search space in the two-dimensional coefficient space for stability. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.2  Direct pole training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For models like diagonal SSMs that train poles as parameters, it is possible to directly control them to satisfy stability conditions. For example, in the case of CT SSMs, ensuring that the system is Hurwitz, i.e., ${\\mathrm{Re}}(\\lambda_{i})<0$ for $i\\in S$ , guarantees stability. If a CT SSM is stable, the ZOH-discretized SSM is also stable, i.e., $|\\overline{{\\lambda}}_{i}|<1$ for $i\\in S$ ,since ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\overline{{\\lambda}}_{i}|=|e^{\\lambda_{i}\\Delta_{i}}|}\\\\ &{\\quad\\quad=|e^{\\mathrm{Re}(\\lambda_{i}\\Delta_{i})}e^{j\\mathrm{Im}(\\lambda_{i}\\Delta_{i})}|}\\\\ &{\\quad\\quad=|e^{\\mathrm{Re}(\\lambda_{i}\\Delta_{i})}||e^{j\\mathrm{Im}(\\lambda_{i}\\Delta_{i})}|}\\\\ &{\\quad\\quad=e^{\\mathrm{Re}(\\lambda_{i}\\Delta_{i})}}\\\\ &{\\quad\\quad<1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the inequality holds since $\\mathbf{Re}(\\lambda_{i})<0$ and $\\Delta_{i}>0$ for a stable CT SSM. ", "page_idx": 15}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 System norm property ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The transfer function matrix $\\mathbf{G}$ of a system $\\Sigma:\\mathbf{u}\\mapsto\\mathbf{y}$ is defined by $\\mathbf{Y}=\\mathbf{G}\\mathbf{U}$ , where $\\mathbf{U}$ and $\\mathbf{Y}$ are the ${\\cal Z}_{}$ -transforms of $\\mathbf{u}$ and $\\mathbf{y}$ . The energy of the output signal $\\mathbf{y}$ is bounded with the $\\mathcal{H}_{\\infty}$ norm of the system as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{y}\\|_{2}^{2}=\\|\\mathbf{Y}\\|_{2}^{2}}\\\\ &{\\qquad=\\|\\mathbf{G}\\mathbf{U}\\|_{2}^{2}}\\\\ &{\\qquad=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\|\\mathbf{G}(e^{j\\theta})\\mathbf{U}(e^{j\\theta})\\|^{2}d\\theta}\\\\ &{\\qquad\\le\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\|\\mathbf{G}(e^{j\\theta})\\|^{2}\\|\\mathbf{U}(e^{j\\theta})\\|^{2}d\\theta}\\\\ &{\\qquad\\le\\underset{\\theta\\in[0,2\\pi]}{\\operatorname*{sup}}\\ \\overline{{\\sigma^{2}(\\mathbf{G}(e^{j\\theta}))}}\\left(\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\|\\mathbf{U}(e^{j\\theta})\\|^{2}d\\theta\\right)}\\\\ &{\\qquad=\\|\\mathbf{G}\\|_{2}^{2}\\|\\mathbf{u}\\|_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we use Parseval's theorem, i.e., $\\lVert\\mathbf{v}\\rVert_{2}^{2}=\\lVert\\boldsymbol{\\mathcal{Z}}(\\mathbf{v})\\rVert_{2}^{2}$ for a signal $\\mathbf{v}$ and the ${{Z}}$ -transform $\\mathcal{Z}$ , in (12) and (13). ", "page_idx": 15}, {"type": "text", "text": "B.2  Bounded layer-level output energy loss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first show that the TFM of a diagonal system is the sum of the TFMs of subsystems. By applying the ${{Z}}$ -transform to (1) (For simplicity, the feed-forward matrix $\\mathbf{D}$ is excluded.), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nz\\mathbf{X}(z)=\\overline{{\\mathbf{A}}}\\mathbf{X}(z)+\\overline{{\\mathbf{B}}}\\mathbf{U}(z),\\qquad\\mathbf{Y}(z)=\\mathbf{C}\\mathbf{X}(z),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where X, U, and $\\mathbf{Y}$ are the ${\\cal Z}_{}$ -transforms of $\\mathbf{x},\\mathbf{u}.$ and $\\mathbf{y}$ , respectively. We can combine the equations by $\\mathbf{Y}(z)=\\mathbf{G}(z)\\mathbf{X}(z)$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf G}(z)={\\bf C}(z{\\bf I}_{n}-\\overline{{{\\bf A}}})^{-1}{\\bf\\overline{{B}}}}\\ ~}\\\\ {{\\displaystyle~~~~~~~~=\\sum_{i=1}^{n}\\frac{{\\bf C}_{i}\\overline{{{\\bf B}}}_{i}}{z-\\overline{{{\\bf\\lambda}}}_{i}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and the decomposition in (15) holds since the considered system is a diagonal system. ", "page_idx": 15}, {"type": "text", "text": "Similarly, by applying the $Z_{\\cdot}$ -transform to subsystem $\\Sigma_{i}$ , we can derive its TFM $\\mathbf{G}_{i}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{G}_{i}(z)=\\frac{\\mathbf{C}_{i}\\overline{{\\mathbf{B}}}_{i}}{z-\\overline{{\\lambda}}_{i}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Substituting (15) with (16) shows that the TFM of the diagonal system is the sum of TFMs of all subsystems as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{G}(z)=\\sum_{i=1}^{n}\\mathbf{G}_{i}(z).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, we consider the layer-layer output energy loss caused by pruning states in $\\mathcal{P}$ , i.e., reducing $\\Sigma$ into $\\Sigma_{-\\mathcal{P}}$ . In previous SSMs, GELU [Hendrycks and Gimpel, 2016] has been widely used as the activation function. Using the 1-Lipschitzness of GELU, TFM decomposition (17), and $\\mathcal{H}_{\\infty}$ norm property (2), the layer-level energy loss is upper bounded as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert f_{\\sigma}(\\mathbf{u};\\Sigma)-f_{\\sigma}(\\mathbf{u};\\Sigma_{-}p)\\Vert_{2}^{2}=\\Vert\\sigma(\\Sigma(\\mathbf{u}))-\\sigma(\\Sigma_{-}p(\\mathbf{u}))\\Vert_{2}^{2}}&{}\\\\ {\\leq\\Vert\\Sigma(\\mathbf{u})-\\Sigma_{-}p(\\mathbf{u})\\Vert_{2}^{2}}&{}\\\\ {=\\Vert\\mathbf{G}\\mathbf{U}-\\mathbf{G}_{-}p\\mathbf{U}\\Vert_{2}^{2}}&{}\\\\ {=\\Vert\\sum_{i\\in\\mathcal{C}}\\mathbf{G}_{i}\\Vert_{2}^{2}}&{}\\\\ {\\leq\\underset{i\\in\\mathcal{C}}{\\leq}\\Vert\\mathbf{G}_{i}\\mathbf{U}\\Vert_{2}^{2}}&{}\\\\ {=\\underset{i\\in\\mathcal{C}}{\\overset{\\Sigma}{\\sum}}\\Vert\\Sigma_{i}(\\mathbf{u})\\Vert_{2}^{2}}&{}\\\\ {\\leq\\underset{i\\in\\mathcal{C}}{\\overset{\\Sigma}{\\sum}}\\Vert\\mathbf{G}_{i}\\Vert_{\\infty}^{2}\\Vert\\mathbf{u}\\Vert_{2}^{2}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This inequality builds upon the approach in Neyshabur et al. [2015], Lee et al. [2021], adapting it for diagonal SSMs and deriving bounds using signal and system norms. In the above derivation, we can analyze on a subsystem basis by utilizing the diagonal structure. Parseval's theorem [Green and Limebeer, 2012] allows us to switch between the time and frequency domains with energy equivalence. Even in the presence of nonlinear functions and signal distortion analysis, we achieved a result similar to modal truncation [Green and Limebeer, 2012], where the $\\mathcal{H}_{\\infty}$ norm distortion of an LTI system is bounded by the sum of the $\\mathcal{H}_{\\infty}$ norms of the truncated LTI systems. ", "page_idx": 16}, {"type": "text", "text": "B.3  Bounded model-level output energy loss ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We show that the model-level output energy loss in Equation (8) is bounded with $\\mathcal{H}_{\\infty}$ norms of subsystems: ", "page_idx": 16}, {"type": "equation", "text": "$$\nJ_{l}\\big(i;\\,\\widetilde{\\Sigma}_{t}^{(1:L)}\\big)\\leq\\frac{\\big\\|\\mathbf{G}_{i}^{(l)}\\big\\|_{\\infty}^{2}}{\\big\\|\\mathbf{G}_{S_{t}^{(l)}}^{(l)}\\big\\|_{\\infty}^{2}}\\prod_{k=1}^{L}\\Big\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\big\\|_{\\infty}^{2}\\big\\|\\mathbf{u}^{(1)}\\big\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{J_{l}\\big(i;\\,\\widetilde\\Sigma_{t}^{(1:L)}\\big):=\\bigg\\|f_{\\sigma}\\big(\\mathbf{u}^{(1)};\\widetilde\\Sigma_{t}^{(1:L)}\\big)-f_{\\sigma}\\big(\\mathbf{u}^{(1)};\\widetilde\\Sigma_{t}^{(1:l-1)},\\Sigma_{S_{t}^{(l)}\\setminus\\{i\\}}^{(l)},\\widetilde\\Sigma_{t}^{(l+1:L)}\\big)\\bigg\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From $L$ th layer to $l+1$ th layer, we can keep bounding the output energy of formal layers using the 1-Lipschitzness of $\\sigma(\\cdot)$ and $\\mathcal{H}_{\\infty}$ norm property in Equation (2) as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\int_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:L)})-f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)},\\Sigma_{s_{t}^{(l)}\\setminus\\{i\\}}^{(l)},\\widetilde{\\Sigma}_{t}^{(l+1:L)})\\right|_{2}^{2}}\\\\ &{=\\left|\\left|\\sigma(\\Sigma_{s_{t}^{(L)}}^{(L)}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:L-1)}))-\\sigma(\\Sigma_{s_{t}^{(L)}}^{(L)}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)},\\Sigma_{s_{t}^{(l)}\\setminus\\{i\\}}^{(l)},\\widetilde{\\Sigma}_{t}^{(l+1:L-1)}))\\right|\\right|_{2}^{2}}\\\\ &{\\leq\\left|\\left|\\Sigma_{s_{t}^{(L)}}^{(L)}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:L-1)})-\\Sigma_{s_{t}^{(L)}}^{(L)}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)},\\Sigma_{s_{t}^{(l)}\\setminus\\{i\\}}^{(l)},\\widetilde{\\Sigma}_{t}^{(l+1:L-1)})\\right|\\right|_{2}^{2}}\\\\ &{\\leq\\left\\|\\mathbf{G}_{s_{t}^{(L)}}^{(L)}\\right\\|_{\\infty}^{2}\\left\\|f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:L-1)})-f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)},\\Sigma_{s_{t}^{(l)}\\setminus\\{i\\}}^{(l)},\\widetilde{\\Sigma}_{t}^{(l+1:L-1)})\\right\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\leq\\prod_{k=l+1}^{L}\\left\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\widetilde{\\Sigma}_{t}^{(l)}\\left(f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)})\\right)-\\Sigma_{S_{t}^{(l)}\\setminus\\{i\\}}^{(l)}\\left(f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)})\\right)\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\widetilde\\Sigma_{t}^{(l)}:=\\Sigma_{S_{t}^{(l)}}^{(l)}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\prod_{k=l+1}^{L}\\left\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\widetilde{\\Sigma}_{t}^{(l)}\\left(f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)})\\right)-\\Sigma_{S_{t}^{(l)}\\setminus\\{i\\}}^{(l)}\\left(f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)})\\right)\\right\\|_{2}^{2}}\\\\ &{=\\displaystyle\\prod_{k=l+1}^{L}\\left\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\Sigma_{S_{t}^{(l)}}^{(l)}\\left(f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)})\\right)-\\Sigma_{S_{t}^{(l)}\\setminus\\{i\\}}^{(l)}\\left(f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)})\\right)\\right\\|_{2}^{2}}\\\\ &{\\leq\\left\\|\\mathbf{G}_{i}^{(l)}\\right\\|_{\\infty}^{2}\\displaystyle\\prod_{k=l+1}^{L}\\left\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)})\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we can add an auxiliary term $\\sigma(0)$ and use the 1-Lipschitzness property since $\\sigma(0)=0$ holds for GELU activation $\\sigma$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{G}_{i}^{(l)}\\right\\|_{\\infty}^{2}\\displaystyle\\prod_{k=l+1}^{L}\\left\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-1)})\\right\\|_{2}^{2}}\\\\ &{=\\left\\|\\mathbf{G}_{i}^{(l)}\\right\\|_{\\infty}^{2}\\displaystyle\\prod_{k=l+1}^{L}\\left\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\sigma(\\widetilde{\\Sigma}_{t}^{(l-1)}(f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-2)})))-\\sigma(0)\\right\\|_{2}^{2}}\\\\ &{\\leq\\left\\|\\mathbf{G}_{i}^{(l)}\\right\\|_{\\infty}^{2}\\displaystyle\\prod_{k=l+1}^{L}\\left\\|\\mathbf{G}_{S_{t}^{(k)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\widetilde{\\Sigma}_{t}^{(l-1)}(f_{\\sigma}(\\mathbf{u}^{(1)};\\widetilde{\\Sigma}_{t}^{(1:l-2)}))\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Again, the output of $l-1$ th layer can be bounded using the $\\mathcal{H}_{\\infty}$ norm property, and keeping the procedures to the first layer proves the statement as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{G}_{i}^{(t)}\\right\\|_{\\infty}^{2}\\underset{k=t+1}{\\overset{L}{\\prod}}\\left\\|\\mathbf{G}_{s_{i}^{(t)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\widehat{\\mathbf{z}}_{t}^{(l-1)}(f_{o}(\\mathbf{u}^{(1)};\\widehat{\\mathbf{z}}_{t}^{(1,l-2)}))\\right\\|_{2}^{2}}\\\\ &{\\leq\\left\\|\\mathbf{G}_{i}^{(t)}\\right\\|_{\\infty}^{2}\\underset{k=t+1}{\\overset{L}{\\prod}}\\left\\|\\mathbf{G}_{s_{i}^{(t)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\mathbf{G}_{s_{i}^{(t-1)}}^{(l-1)}\\right\\|_{\\infty}^{2}\\left\\|f_{o}(\\mathbf{u}^{(1)};\\widehat{\\mathbf{z}}_{t}^{(1,l-2)})\\right\\|_{2}^{2}}\\\\ &{\\vdots}\\\\ &{\\leq\\left\\|\\mathbf{G}_{i}^{(t)}\\right\\|_{\\infty}^{2}\\underset{k=t+1}{\\overset{L}{\\prod}}\\left\\|\\mathbf{G}_{s_{i}^{(t)}}^{(k)}\\right\\|_{\\infty}^{2}\\underset{k=t}{\\overset{L}{\\prod}}\\left\\|\\mathbf{G}_{s_{i}^{(t)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\mathbf{u}^{(1)}\\right\\|_{\\infty}^{2}}\\\\ &{=\\left\\|\\mathbf{G}_{s_{i}^{(t)}}^{(l)}\\right\\|_{\\infty}^{2}\\underset{k=t+1}{\\overset{L}{\\prod}}\\left\\|\\mathbf{G}_{s_{i}^{(t)}}^{(k)}\\right\\|_{\\infty}^{2}\\left\\|\\mathbf{u}^{(1)}\\right\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C Experimental details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our experiments were conducted with JAX [Bradbury et al., 2018] on a single A6000 48GB GPU or RTX 3090 24GB GPU. We reproduced S4D models using the implementations from Rush and Karamcheti $[2022]^{2}$ and S5 models from Smith et al. [2023]3. We used bidirectional SSMs for all ", "page_idx": 17}, {"type": "text", "text": "tasks except sMN IST and p sMNIST tasks. Following S5, we implemented bidirectional S4D models tohave $\\dot{\\mathbf{C}^{b}}$ matrices for reverse convolution. For inference, we used Vandermonde product and convolution kernel schemes for S4D and parallel scans for S5 models. ", "page_idx": 18}, {"type": "text", "text": "C.1 Tasks ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The following lists ten tasks where we tested our proposed method, along with the specified resources and the time taken for model training for each task. ", "page_idx": 18}, {"type": "text", "text": "\u00b7 sMNIST: 10-way classification task with flattened MNIST images, each having a sequence length of 784. Original images are for handwritten digits. It took 30 minutes to train an S5 model with an RTX 3090 24GB GPU.   \n\u00b7 psMNIST: 10-way classification task with flattened and fixed-order permuted MNIST images, each having a sequence length of 784. Original images are for handwritten digits. It took 1 hour to train an S5 model with an RTX 3090 24GB GPU.   \n\u00b7 sCIFAR: 10-way classification task with fattened CIFAR-10 images [Krizhevsky et al., 2009], each having a sequence length of 1,024 for each R, G, B channel. The dataset includes 45,000 training, 5,000 validation, and 10,000 test sequences. It took 7 hours to train an S4D model or an S5 model with an A6000 48GB GPU.   \n\u00b7 Li st Ops: 10-way classification task with longer variations of ListOps data [Nangia and Bowman, 2018], each having a maximum sequence length of 2048 for a single channel. The task is solving nested mathematical operations applied to numbers in the range of O-9 to derive a final result. One-hot vectors for 17 values, including operators, enclosers of operators, and numbers, are concatenated. The dataset includes 96,000 training, 2,000 validation, and 2,000 test sequences. It took 2 hours to train an S4D model or an S5 model with an RTX 3090 24GB GPU.   \n\u00b7 Text: 2-way byte-level text classification task with IMDB review data [Maas et al., 2011], each having a maximum sequence length of 4,096 for a single channel. The task is classifying the sentiment of a review. One-hot vectors for 129 characters are concatenated. The dataset includes 25,000 training and 25,000 test sequences. It took 2.5 hours to train an S4D model and 1.5 hours to train an S5 model with an RTX 3090 24GB GPU.   \n\u00b7 Ret rieval: 2-way byte-level document retrieval task with ACL Anthology Network document data [Radev et al., 2009], each having a maximum sequence length of 4,000 for a single channel. The task is classifying if two documents are linked by equivalent citations. One-hot vectors for 97 characters are concatenated for each document. The dataset includes 147,086 training, 18,090 validation, and 17,437 test sequence pairs. It took 15.5 hours to train an S4D model with an A6000 48GB GPU and 6 hours to train an S5 model with an RTX 3090 24GB GPU.   \n\u00b7 Image: 10-way classification task with flattened CIFAR-10 images [Krizhevsky et al., 2009], each having a sequence length of 1,024 for a single channel. It took 9.5 hours to train an S4D model and 7.5 hours to train an S5 model with an RTX 3090 24GB GPU.   \n\u00b7 Pat hfinder: 2-way classification task with flattened Pathfinder challenge images [Linsley et al., 2018], each having a sequence length of 1,024 for a single channel. Original images are for points with connecting or distracting paths. The dataset includes 160,000 training, 20,000 validation, and 20.000 test sequences. It took 14 hours to train an S4D model and 11 hours to train an S5 model with an RTX 3090 24GB GPU.   \n\u00b7 Pat h-X: 2-way classification task with flattened scaled Pathfinder challenge images [Linsley et al., 2018], each having a sequence length of 16,384 for a single channel. Original images are for points with connecting or distracting paths. Original images are for points and connecting or distracting paths. It took 3 days to train an S4D model and 1 day to train an S5 model with an A6000 48GB GPU.   \n\u00b7 Speech Command: 35-way classification task with 1-second word-speaking audio recording data [Warden, 2018], each having a sequence length of 16,000 for a single channel. For the varying sampling frequency tests, the data was downsampled from 16kHz to 8kHz. The dataset includes 24,482 training, 5,246 validation, and 5,247 test sequences. It took 21 hours to train an S4D model and 8 hours to train an S5 model with an RTX 3090 24GB GPU. ", "page_idx": 18}, {"type": "text", "text": "C.2  Hyperparameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We followed the hyperparameters in [Gu et al., 2022a, Smith et al., 2023]. For Pat h-X task, it was challenging to train S4D models with the original learning rate of 0.0005, thus we changed it to 0.001. ", "page_idx": 19}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/30db520efcbb9225570de9a1a37f6f39bd5cc4f2c64f1ac662434d3ef3ab3a93.jpg", "table_caption": ["Table 3: Training configurations of S4D models for all tested tasks. $n_{s}$ : state dimension of each SISo system. LN: layer normalization, BN: batch normalization, Pre: pre-normalization. D: dropout. LR: learning rate. B: batch size. E: epochs. WD: weight decay. f: The value is changed from the original release [Gu et al., 2022a] for training feasibility. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/79b5dd316eb07646b31fda06c7d0268c9976122c4b4e1aad7b69626f17bc2f48.jpg", "table_caption": ["Table 4: Training configurations of S5 models for all tested tasks. All models used batch normalization, pre-normalization, and $\\Delta_{m a x}=0.1$ $n_{m}$ : state dimension of a MIMO system. $J$ : number of blocks for block initialization of $\\Lambda$ . D: dropout. LR: learning rate. SSM LR: learning rate for SSM parameters, B: batch size. E: epochs. WD: weight decay. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D  Validation of pruning granularity and criterion ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1  State pruning granularity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As in channel pruning [He et al., 2017], state pruning is named based on its granularity of pruning, that is, all parameters associated with insignificant states are pruned at once. For instance, the parameters $\\lambda_{i}$ from $\\Lambda$ , the row vector $\\mathbf{B}_{i}$ from $\\mathbf{B}$ , and the column vector $\\mathbf{C}_{i}$ from $\\mathbf{C}$ are pruned when the state $i$ is identified as an insignificant state. ", "page_idx": 19}, {"type": "text", "text": "To explicitly demonstrate the necessity of state pruning in SSMs, we compared the performance of unstructured random pruning and structured random state pruning using S5 models. For unstructured random pruning, we pruned randomly selected elements from the system matrices, obtaining the results in Table 5. ", "page_idx": 19}, {"type": "text", "text": "Despite the similar number of parameters being pruned, the model suffered a significant performance degradation, with an average accuracy loss of $59.92\\%$ , in the case of unstructured random pruning. ", "page_idx": 19}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/e19572cb9675f68f9b2258a229c7be509ab69931413dc67af48bf1553c5cbdb7.jpg", "table_caption": ["Table 5: Average pruning ratio and accuracy loss for all tasks. Values in parentheses are evaluated by excluding non-compressible cases. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "This is because unstructured pruning can alter the learned dynamics in all subsystems. In contrast, state pruning maintains the functionality of unpruned subsystems, leading to less performance degradation. This highlights the importance of considering the structure and mechanism of the model when applying pruning techniques. ", "page_idx": 20}, {"type": "text", "text": "D.2  Comparison with magnitude pruning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Magnitudes and $L_{p}$ norms of parameters are simple but effective pruning criteria to obtain efficient neural networks [Cheng et al., 2024]. Given the necessity of state pruning granularity in SSMs, we set the pruning granularity to state pruning and then compared the significant state identification abilities of the magnitude and $\\mathcal{H}_{\\infty}$ pruning methods. To extend Table 1, we define magnitude state pruning methods as follows: ", "page_idx": 20}, {"type": "text", "text": "\u00b7 Uniform Magnitude. Every layer is uniformly pruned to have the same pruning ratio with the importance of each state $i$ as $|\\dot{\\overline{{\\lambda}}}_{i}|||\\overline{{\\mathbf{B}}}_{i}|||\\mathbf{C}_{i}||$ . While any $L_{p}$ norm can be used, we present the results using the $L_{2}$ norm as an example. ", "page_idx": 20}, {"type": "text", "text": "\u00b7 Global Magnitude. The same state importance criterion as in Uniform Magnitude is used, but the comparison group is extended from intra-layer to inter-layer, ensuring that the pruning ratio is met globally for the entire network. ", "page_idx": 20}, {"type": "text", "text": "\u00b7 LAMP. This method employs a criterion of $\\frac{|\\overline{{\\lambda}}_{i}|^{2}\\|\\overline{{\\mathbf{B}}}_{i}\\|^{2}\\|\\mathbf{C}_{i}\\|^{2}}{\\sum_{-j\\leq i|\\overline{{\\lambda}}_{j}|^{2}\\|\\overline{{\\mathbf{B}}}_{j}\\|^{2}\\|\\mathbf{C}_{j}\\|^{2}}$ adapted from Lee et al. [2021], which originally used $\\frac{W_{i}^{2}}{\\sum{j\\le i W_{j}^{2}}}$ as a criterion for a real-valued weight parameter $W$ . The state indices in the denominator are assumed to be ordered based on their evaluation using the basic magnitude criterion similar to LAST. ", "page_idx": 20}, {"type": "text", "text": "Extending the S5 model part in Table 1, Table 6 reports that, at the same pruning ratio, LAST and other $\\mathcal{H}_{\\infty}$ pruning methods significantly outperform magnitude pruning methods by showing less accuracy loss, which implies that $\\mathcal{H}_{\\infty}$ pruning methods can better distinguish significant and insignificant states. This performance gap and suitability can be explained with the unique transfer function of SSMs, which is defined in the frequency domain as in Equation (14) for the whole system and Equation (16) for a subsystem. Specifically, the importance of $\\overline{{\\lambda}}_{i}$ is evaluated by $(1-|\\overline{{\\lambda}}_{i}|)^{-1}$ in $\\mathcal{H}_{\\infty}$ pruning methods, while magnitude pruning methods evaluate $|\\overline{{\\lambda}}_{i}|$ ", "page_idx": 20}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/218f7205239d1f59bfdcd3428453bbd52633e31ebe043ca9b68ee576355870b9.jpg", "table_caption": ["Table 6: Average pruning ratio and accuracy loss in S5 models for all tasks. Values in parentheses are evaluated by excluding non-compressible cases. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "E Full results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1   Pixel-level image classification ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 7 highlights the results evaluated at the maximum pruning ratio where the accuracy loss of LAST was less than $1\\%$ . Figure 6 shows the accuracy at all tested pruning ratios. ", "page_idx": 21}, {"type": "text", "text": "As shown in Table 7 and Figure 6, both S4D and S5 had great compressibility. In Table 7, the state dimension of S4D indicates the average $n_{s}$ of SISO systems, while in Figure 6, it refers to the average effective state dimension $n$ acrosslayers. ", "page_idx": 21}, {"type": "image", "img_path": "T9GbbWbNQG/tmp/bfa7951abaf7ad94385a7290355d450956152de11ad65cf5527f31b1c2c604f9.jpg", "img_caption": ["Figure 6: Efficiency-accuracy trade-off curves of pruned (Upper) S4D (Lower) S5 models for pixellevel image classification tasks. LAST obtained more efficient models that maintain performance comparedtoUniform $\\mathcal{H}_{\\infty}$ , which was observed more stably and consistently than Global $\\mathcal{H}_{\\infty}$ (LAST w/o score normalization). "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/7849abfef6663ed781f4e38b8526f4523e8f2bc1961b83dae0a6b5ceee652328.jpg", "table_caption": ["Table 7: Accuracy of pruned models on pixel-level image classification tasks. LAST is evaluated at the maximum tested pruning ratio with less than $1\\%$ accuracy loss, and other methods were evaluated for the same pruning ratios. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "E.2  Long range arena ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To evaluate the practical efficiency resulting from LAST, we implemented pruning by removal, in addition to pruning by masking implementation, by transferring selected significant parameters to a smaller-dimensional model. Table 8 presents the average evaluation step speed and peak GPU memory usage of pruned S5 models for an NVIDIA RTX 3090 GPU. Reducing the state dimension improved efficiency in both computational and memory costs, with the degree of efficiency depending on the channel size per task. ", "page_idx": 22}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/33b935d9fb30c95b1dd21b17d53e18e79322586b5c90b77dfaa8cd05dc3a99f5.jpg", "table_caption": ["Table 8: Efficiency improvement in computational and memory costs in S5 models. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 9 highlights the results evaluated at the maximum pruning ratio where the accuracy loss of LAST was less than $1\\%$ . Figure 7 shows the accuracy at all tested pruning ratios. In Table 9, the state dimension of S4D indicates the average $n_{s}$ of SISO systems, while in Figure 7, it refers to the average effective state dimension $n$ across layers. ", "page_idx": 22}, {"type": "text", "text": "In List Ops task, where the initial state dimension was small, the S5 models were uncompressible. For Text task, both S4D and S5 models showed the highest compressibility among all tasks, followed by Retrieval task. ", "page_idx": 22}, {"type": "text", "text": "In Image task, S4D models were uncompressible since the $\\mathcal{H}_{\\infty}$ scores were significantly low and fell below the precision threshold of the foating-point representation, making the comparison in local pruning and sorting required for LAST score calculation impossible. ", "page_idx": 22}, {"type": "image", "img_path": "T9GbbWbNQG/tmp/31ce7df905cdd07a87d122db95124680d9b91842e7855b9a4407e0ba55785bfe.jpg", "img_caption": ["Figure 7: Efficiency-accuracy trade-off curves of pruned S4D models for LRA tasks. LAST obtained more efficient models that maintain performance compared to Uniform $\\mathcal{H}_{\\infty}$ ,whichwasobserved more stably and consistently than Global $\\mathcal{H}_{\\infty}$ (LAST w/o score normalization). "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Notably, the state dimensions of S5 models were able to reduce by $30\\%$ in both the Pathfinder and Pat h-X tasks. The ability to maintain performance in P at h-X highlights the effectiveness of theMIMOstructure ofS5. ", "page_idx": 23}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/bab3d60f20bd8566b13b4889b94c13b7fa300ff35b259fdc60cf51d8440099fc.jpg", "table_caption": ["Table 9: Accuracy of pruned models for LRA tasks. LAST is evaluated at the maximum tested pruning ratio with less than $1\\%$ accuracy loss, and other methods were evaluated for the same pruning ratios. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.3  Speech command ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table 10 highlights the results evaluated at the maximum pruning ratio where the accuracy loss of LAST was less than $1\\%$ . Figure 8 shows the accuracy at all tested pruning ratios. In Table 10, the state dimension of S4D indicates the average $n_{s}$ of SISO systems, while in Figure 8, it refers to the average effective state dimension $n$ across layers. ", "page_idx": 25}, {"type": "table", "img_path": "T9GbbWbNQG/tmp/f349aa4d8817f0cb403fadef6f4ce06dc42866c3cd19f158fea928d6066d2e00.jpg", "table_caption": ["Table 10: Accuracy of pruned models on Speech Command task. LAST is evaluated at the maximum tested pruning ratio with less than $1\\%$ accuracy loss, and other methods were evaluated for the same pruning ratios. "], "table_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "T9GbbWbNQG/tmp/f5044058347f155109d59de03d2497dfc42a3950b028c40b6169d10aa6991594.jpg", "img_caption": ["Figure 8: Efficiency-accuracy tradeoff curves of pruned (Left) S4D (Right) S5 models for Spee ch Command task. LAST obtained more efficient models that maintain performance compared to Uniform $\\mathcal{H}_{\\infty}$ , which was observed more stably and consistently than Global $\\mathcal{H}_{\\infty}$ (LASTw/oscore normalization). "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The abstract and introduction reflect the contribution of extending modal truncation to a multi-system approximation for deep state space models. In addition, it accurately presents the average result values from all datasets used in the experiment. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The limitation of this paper is discussed in Section 6. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper clearly states the assumption of a stable diagonal system before derivations, and this assumption is satisfied in the experiments through Hurwitz parameterization. The proof for the upper bound of the objective function in the proposed method is provided in Appendix B. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We disclose the final numerical form of our proposed scores in Equation (6) and Equation (10), and provide the simulation codes by external link. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper clearly specifies the sources of the existing data and code used, and the external link includes the environment settings necessary for using the data and code. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https : / /nips . CC / public/guides /CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (ht tps : / /nips.cc/public/guides/CodeSubmissionPolicy)formore details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The full training details are provided in Appendix C. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: For all experiments, the standard deviation of test accuracies for the three seeds is shown in Appendix E. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The computer resources used for model reproduction and the corresponding training time for each task are provided in Appendix C. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPs Code ofEthics https: / /neurips.Cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. The research conducted in the paper conforms, in every respect, with the NeurIPs Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper properly credits the datasets and two backbone codes used. The licenses of the datasets (Apache-2.0 license for LRA dataset) and codes (MIT licenses for backbones) are properly respected. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/ dataset s has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper makes the used code available as an asset through the external link. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]