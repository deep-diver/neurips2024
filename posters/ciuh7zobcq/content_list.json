[{"type": "text", "text": "Coherence-free Entrywise Estimation of Eigenvectors in Low-rank Signal-plus-noise Matrix Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hao Yan ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Keith Levin ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics University of Wisconsin-Madison Madison, WI 53706 United States of America hyan84@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics University of Wisconsin-Madison Madison,WI 53706 United States of America kdlevin@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Spectral methods are widely used to estimate eigenvectors of a low-rank signal matrix subject to noise. These methods use the leading eigenspace of an observed matrix to estimate this low-rank signal. Typically, the entrywise estimation error of these methods depends on the coherence of the low-rank signal matrix with respect to the standard basis. In this work, we present a novel method for eigenvector estimation that avoids this dependence on coherence. Assuming a rank-one signal matrix, under mild technical conditions, the entrywise estimation error of our method provably has no dependence on the coherence under Gaussian noise (i.e., in the spiked Wigner model), and achieves the optimal estimation rate up to logarithmic factors. Simulations demonstrate that our method performs well under non-Gaussian noise and that an extension of our method to the case of a rank- $^r$ signal matrix has little to no dependence on the coherence. In addition, we derive new metric entropy bounds for rank- $r$ singular subspaces under $\\ell_{2,\\infty}$ distance, which may be of independent interest. We use these new bounds to improve the best known lower bound for rank- ${\\bf\\nabla}r$ eigenspace estimation under $\\ell_{2,\\infty}$ distance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Spectral methods are extensively used in contemporary data science and engineering [27]. The fundamental idea underlying these methods is that the eigenspace or singular subspace of an observed matrix reflects important structure present in the data from which it is derived. Spectral methods have been deployed successfully in a variety of tasks, including low-rank matrix denoising [30, 10], factor analysis [21, 32, 3, 61, 11, 62], community detection [49, 52, 1, 40, 50], pairwise ranking [43, 25] and matrix completion [48, 24, 7]. The widespread use of spectral methods has driven extensive research into the theoretical properties of eigenspaces and singular subspaces, yielding normal approximation results [10, 11, 33, 58, 5] as well as perturbation bounds [60, 20, 22, 45]. For a more comprehensive recent review of spectral methods, see [27]. ", "page_idx": 0}, {"type": "text", "text": "1.1  Eigenspace estimation in low-rank matrix models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Consider an unknown symmetric matrix $M^{\\star}\\,=\\,U^{\\star}\\Lambda^{\\star}U^{\\star^{\\top}}\\,\\in\\,\\mathbb{R}^{n\\times n}$ ,where $U^{\\star}\\,\\in\\,\\mathbb{R}^{n\\times r}$ has orthonormal columns and $\\mathbf{A}^{\\star}\\,\\in\\,\\mathbb{R}^{r\\times r}$ is diagonal, containing the nonzero eigenvalues of $M^{\\star}$ ordered so that $|\\lambda_{1}^{\\star}|\\geq|\\lambda_{2}^{\\star}|\\geq\\cdot\\cdot:\\geq|\\lambda_{r}^{\\star}|>0$ . Our goal is to estimate $U^{\\star}$ from a noisy observation ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\pmb{Y}=\\pmb{M}^{\\star}+\\pmb{W}\\in\\mathbb{R}^{n\\times n},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Where $W=[W_{i j}]_{1\\leq i,j\\leq n}$ is a symmetric random noise matrix with mean zero. We restrict our attention here to the symmetric case for the sake of simplicity, but we expect that our results can be extended to the asymmetric case using standard dilation arguments [53]. ", "page_idx": 1}, {"type": "text", "text": "Throughout this paper, we assume that the entries of $W$ are subgaussian. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1. The entries of. $W$ on and above the diagonal are independent and symmetric about zerowith common variance $\\bar{\\sigma}^{2}$ and common subgaussian parameter $\\nu_{W}$ ", "page_idx": 1}, {"type": "text", "text": "We remind the reader that the subgaussian parameter $\\nu_{W}$ serves as a \u201cproxy\u201d for the variance. Indeed, in the Gaussian case, we have $\\sigma^{2}=\\nu_{W}$ ,while $\\sigma^{2}\\leq\\nu_{W}$ more generally [54, 56]. ", "page_idx": 1}, {"type": "text", "text": "Spectral methods often estimate $U^{\\star}$ directly using the $r$ leading eigenvectors $U$ of $\\mathbf{\\deltaY}$ . As a result, the entrywise and row-wise behavior of $U$ has attracted considerable attention [31, 22, 41, 2, 4, 14]. Given an estimator $\\widehat{\\b{U}}\\in\\mathbb{R}^{n\\times r}$ , the estimation error is measured in terms of the $\\ell_{2,\\infty}$ distance ", "page_idx": 1}, {"type": "equation", "text": "$$\nd_{2,\\infty}(\\widehat{\\pmb{U}},\\pmb{U}^{\\star})=\\operatorname*{min}_{\\mathbf{\\epsilon}_{\\Gamma\\in\\mathbb{O}_{r}}}\\|\\pmb{U}^{\\star}-\\widehat{\\pmb{U}}\\mathbf{\\Gamma}\\|_{2,\\infty},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the presence of $\\mathbf{T}$ is to resolve rotational non-identifiability. In the rank-one case, this reduces to the $\\ell_{\\infty}$ distance, ", "page_idx": 1}, {"type": "equation", "text": "$$\nd_{\\infty}(\\widehat{\\pmb{u}},\\pmb{u}^{\\star})=\\operatorname*{min}\\left\\lbrace\\|\\pmb{u}^{\\star}-\\widehat{\\pmb{u}}\\|_{\\infty},\\|\\pmb{u}^{\\star}+\\widehat{\\pmb{u}}\\|_{\\infty}\\right\\rbrace\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "for $\\pmb{u}^{\\star}\\in\\mathbb{R}^{n}$ and a given estimator $\\widehat{\\pmb{u}}\\in\\mathbb{R}^{n}$ . Estimation error bounds in $\\ell_{\\infty}$ or $\\ell_{2,\\infty}$ distance typically rely on the incoherence parameter $\\mu$ of $U^{\\star}$ , defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mu=\\frac{n}{r}\\left\\|\\boldsymbol{U}^{\\star}\\right\\|_{2,\\infty}^{2}\\in[1,n/r].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In the rank-one case with Gaussian noise, if the leading eigenvalue $\\lambda^{\\star}$ of $M^{\\star}$ satisfies $|\\lambda^{\\star}|\\;=$ $\\Omega(\\sigma{\\sqrt{n}})$ , Theorem 4.1 of [27] shows that with probability at least $1\\!-\\!O(n^{-8})$ , the leading eigenvector $\\textbf{\\em u}$ of $\\mathbf{Y}$ satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\nd_{\\infty}(\\pmb{u},\\pmb{u}^{\\star})\\lesssim\\frac{\\sigma\\left(\\sqrt{\\log n}+\\sqrt{n}\\|\\pmb{u}^{\\star}\\|_{\\infty}\\right)}{\\left|\\lambda^{\\star}\\right|}=\\frac{\\sigma\\sqrt{\\log n}+\\sigma\\sqrt{\\mu}}{\\left|\\lambda^{\\star}\\right|}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "When $\\sigma{\\sqrt{n}}\\lesssim|\\lambda^{\\star}|\\lesssim\\sigma{\\sqrt{n\\log n}}$ , a regime of most interest (no ploynomial-time algorithm is known When $|\\lambda^{\\star}|\\ll\\sigma\\sqrt{n}$ [6], and estimation is easy when $|\\lambda^{\\star}|\\gg\\sigma\\sqrt{n\\log n})$ , we show in Lemma 1 that Equation (4) is not improvable up to log-factors , as a result of the Baik-Ben Arous-P\u00e9ch\u00e9 (BBP) phase transition [8] (see [12, 38, 34] for BBP-style phase transitions in the setting of this paper). ", "page_idx": 1}, {"type": "text", "text": "Lemma 1. Under Equation (1) with Gaussian noise, let $M^{\\star}\\;=\\;\\lambda^{\\star}{\\pmb u}^{\\star}{\\pmb u}^{\\star\\top}$ . If both limits $\\operatorname*{lim}_{n\\to\\infty}\\lambda^{\\star}/(\\sigma{\\sqrt{n}})>\\bar{1}$ and $\\scriptstyle\\operatorname*{lim}_{n\\to\\infty}\\mu/n$ exist, then for any $\\mu\\!\\in\\![1,n].$ there exists $\\pmb{u}^{\\star}\\in\\mathbb{S}^{n-1}$ such that almost surely, ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{inf}_{\\mathbf{\\pi}}d_{\\infty}(\\pmb{u},\\pmb{u}^{\\star})\\geq\\operatorname*{lim}_{n\\to\\infty}\\frac{\\sigma^{2}\\sqrt{n\\mu}}{2\\sqrt{2}|\\lambda^{\\star}|^{2}}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Equation (5) shows that the spectral estimate $\\textbf{\\em u}$ has an intrinsic dependence on $\\mu$ , with especially bad performance when $\\mu$ is large and $|\\lambda^{\\star}|=\\Theta(\\sigma\\sqrt{n\\log n})$ . In this large- $\\mu$ regime, beyond lowrankedness, $M^{\\star}$ exhibits additional structure (e.g., sparsity) that is not fully utilized by the spectral estimator. This suggests that the dependence on $\\mu$ in Equations (4) and (5) is a shortcoming of the spectral estimator. In Algorithm 1, we present a new estimator designed to remove this dependence on $\\mu$ . Theorem 1 shows that up to log-factors, it matches the minimax lower bound discussed below (see Equation (9) in Section 1.2). Experiments in Section 5 further support our theoretical results. ", "page_idx": 1}, {"type": "text", "text": "In the rank- $^r$ case with Gaussian noise, Theorem 4.2 in [27] shows that when $|\\lambda_{r}^{\\star}|\\gtrsim\\sigma\\sqrt{n\\log n}$ ", "page_idx": 1}, {"type": "equation", "text": "$$\nd_{2,\\infty}(\\pmb{U},\\pmb{U}^{\\star})\\lesssim\\frac{\\sigma\\left(\\kappa\\sqrt{\\mu r}+\\sqrt{r\\log n}\\right)}{\\left|\\lambda_{r}^{\\star}\\right|}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with probability at least $1-O(n^{-8})$ , where $\\kappa=|\\lambda_{1}^{\\star}|/|\\lambda_{r}^{\\star}|$ is the condition number of $M^{\\star}$ . Here again, the estimation error depends on $\\mu$ , and we conjecture that this dependence is also sub-optimal. Algorithm 2 extends our rank-1 estimation algorithm to this more general rank- $^r$ case. Experiments in Section 5 show that Algorithm 2 outperforms the naive spectral method in the general rank- $^r$ case, with little to no sensitivity to the coherence $\\mu$ . We note in passing that the dependence on $\\kappa$ in Equation (6) can likely be removed [3, 62, 57], though we do not pursue this here. ", "page_idx": 1}, {"type": "text", "text": "1.2 Minimax lower bounds for subspace estimation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Minimax lower bounds have been established for a variety of subspace estimation problems, including sparse PCA [21, 55], matrix denoising [20], structural matrix estimation [19], network estimation [35, 63] and estimating linear functions of eigenvectors [42, 28]. Most of these studies focus on minimax lower bounds under the Frobenius or operator norm, derived using the packing numbers of Grassmann manifolds [46, 13]. In the matrix denoising literature, it is well-known that for $r\\geq1$ ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{r}\\in\\mathbb{O}_{r}}\\left\\|\\widehat{U}\\mathbf{r}-U^{\\star}\\right\\|_{\\mathrm{F}}\\gtrsim\\operatorname*{min}\\left\\{\\frac{\\sigma\\sqrt{n r}}{\\vert\\lambda_{r}^{\\star}\\vert},\\sqrt{r}\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "holds in a minimax sense (see Theorem 3 in [28] or Theorem 4 in [63]). Far fewer papers have considered lower bounds under $d_{2,\\infty}$ [18, 4], and these results are derived via the trivial lower bound ", "page_idx": 2}, {"type": "equation", "text": "$$\nd_{2,\\infty}(\\widehat{U},U^{\\star})\\geq\\frac{1}{\\sqrt{n}}\\operatorname*{min}_{\\Gamma\\in\\mathbb{Q}_{r}}\\left\\|\\widehat{U}\\Gamma-U^{\\star}\\right\\|_{\\mathrm{F}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which holds for any $\\widehat{\\boldsymbol{U}},\\boldsymbol{U}^{\\star}\\in\\mathbb{R}^{n\\times r}$ . Applying the lower bounds in Equations (7) and (8), we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{U^{\\star}\\in\\mathbb R^{n\\times r}:U^{\\star}\\overset{\\textstyle\\uparrow}{\\boldsymbol{U}^{\\star}}=I_{r}}d_{2,\\infty}(\\widehat{U},U^{\\star})\\gtrsim\\operatorname*{min}\\left\\{\\frac{\\sigma\\sqrt{r}}{|\\lambda_{r}^{\\star}|},\\sqrt{\\frac{r}{n}}\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "holds for any estimator $\\widehat{U}\\in\\mathbb{R}^{n\\times r}$ . When $U^{\\star}$ is incoherent, meaning that $\\mu=O(1)$ , this lower bound is achieved by the spectral estimation rate in Equation (6) when $|\\lambda_{r}^{\\star}|=\\Omega(\\sigma\\sqrt{n\\log n})$ , and is achieved trivially by $\\widehat{\\boldsymbol{U}}=\\mathbf{0}_{n,r}$ when $|\\lambda_{r}^{\\star}|=o(\\sigma\\sqrt{n})$ . On the other hand, when $U^{\\star}$ is coherent, in the sense that $\\mu=\\omega(1)$ , and $|\\lambda_{r}^{\\star}|=\\Omega(\\sigma\\sqrt{n\\log n})$ our discussion above in Section (1.1) (including our new results in Theorem 1) suggests that the rate in Equation (9) can be achieved up to log-factors. ", "page_idx": 2}, {"type": "text", "text": "This leaves open the question of the minimax rate when $\\mu=\\omega(1)$ and $|\\lambda_{r}^{\\star}|=o(\\sigma\\sqrt{n})$ . In this case, the lower bound in Equation (9) cannot exceed $\\sqrt{r/n}$ . This seems suboptimal, as we expect some dependence on $\\|U^{\\star}\\|_{2,\\infty}$ (for example, consider the extreme case when $\\lambda^{\\star}$ is very near zero). This suboptimality arises from the naive lower bound in Equation (8). In Theorem 2, we improve this lower bound, removing the $\\sqrt{r/n}$ dependence in Equation (9). This improved lower bound makes use of novel metric entropy bounds for singular subspaces, which may be of independent interest. ", "page_idx": 2}, {"type": "text", "text": "1.3  Notation and roadmap ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We use $C$ to denote a constant whose precise values may change from line to line. For a positive integer $n$ , we write $[n]=\\{1,2,\\ldots,n\\}$ $|{\\mathcal{A}}|$ denotes the cardinality of a set $\\boldsymbol{\\mathcal{A}}$ . For real numbers $a$ and $b$ , we write $a\\vee b=\\operatorname*{max}\\{a,b\\}$ and $a\\wedge b=\\operatorname*{min}\\{a,b\\}$ . For a vector $\\mathbf{\\boldsymbol{v}}=\\left(v_{1},v_{2},\\ldots,v_{n}\\right)^{\\top}\\in\\mathbb{R}^{n}$ we use the norms $\\|\\pmb{v}\\|_{2}\\,=\\,\\sqrt{\\sum_{i=1}^{n}v_{i}^{2}}$ and $\\|\\pmb{v}\\|_{\\infty}\\,=\\,\\operatorname*{max}_{i}\\,|v_{i}|$ We let $\\pmb{e}_{i}\\,\\in\\,\\mathbb{R}^{n},i\\,\\in\\,[n]$ denote the standard basis vectors of $\\mathbb{R}^{n}$ $\\mathbb{S}^{n-1}\\,=\\,\\{{\\pmb u}\\in\\mathbb{R}^{n}:\\|{\\pmb u}\\|_{2}=1\\}$ denotes the unit sphere. For a matrix $M\\in\\mathbb{R}^{n\\times n}$ \uff0c $M_{i}$ . denotes its $i$ -th row as a row vector, $\\lVert M\\rVert$ denotes its operator norm and $\\|M\\|_{2,\\infty}=\\operatorname*{max}_{i\\in[n]}\\|M_{i,\\cdot}\\|_{2}$ indicates the maximum row-wise $\\ell_{2}$ norm. $\\pmb{I}_{n}\\in\\mathbb{R}^{n}$ denotes the $n$ by- $\\cdot n$ identity matrix. $\\mathbb{O}_{r}$ denotes the $r$ -dimensional orthogonal group. We use both standard Landau notation and asymptotic notation: for positive functions $f(n)$ and $g(n)$ , we write $f(n)\\gg g(n)$ $f(n)\\,=\\,\\omega(g(n))$ or $~g(n)\\,=\\,o(f(n))$ if $f(n)/g(n)\\,\\to\\,\\infty$ as $n\\,\\rightarrow\\,\\infty$ . We write $f(n)\\,\\gtrsim\\,g(n)$ $f(n)=\\Omega(g(n))$ or $g(n)=O(f(n))$ if for some constant $C>0$ , we have $f(n)/g(n)\\stackrel{.}{\\geq}C$ for all sufficiently large $n$ .We write $f(n)=\\Theta(g(n))$ if both $f(n)=O(g(n))$ and $g(n)=O(f(n))$ ", "page_idx": 2}, {"type": "text", "text": "The remainder of the paper is organized as follows. In Section 2, we study the eigenspace estimation problem for rank-one matrices and propose a new algorithm that achieves the minimax optimal error rate up to logarithmic factors in the growth regime where $|\\lambda^{\\star}|=\\Omega(\\sigma\\sqrt{n\\log n})$ (Theorem 1). In Section 3, we extend this algorithm to rank- $^r$ eigenspace estimation. In Section 4, we present theoretical results for the metric entropy of subspaces under $d_{\\infty}$ and $d_{2,\\infty}$ and improve Equation (9) under the growth regime where $|\\lambda^{\\star}|\\ ={\\cal O}(\\sigma\\sqrt{n})$ (Theorem 2). Numerical results are provided in Section 5. We conclude in Section 6 with a discussion of the limitations of our study and directions for future work. Detailed proofs of all lemmas and theorems can be found in the appendix. ", "page_idx": 2}, {"type": "text", "text": "2   Rank-one matrix eigenspace estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we study the model in Equation (1) when $M^{\\star}$ is rank-one with eigendecomposition $M^{\\star}\\!=\\!\\lambda^{\\star}{\\boldsymbol u}^{\\star}{\\boldsymbol u}^{\\star}^{\\intercal}$ ;where $\\lambda^{\\star}\\in\\mathbb{R}$ and $\\pmb{u}^{\\star}\\in\\mathbb{S}^{n-1}$ . As discussed in Section 1, spectral methods may have sub-optimal dependence on $\\mu$ compared to the lower bound in Equation (9). We show that a better estimator is possible by working with a carefully selected subset of entries of $\\mathbf{Y}$ .We start with a key observation in Lemma 2, which states that any unit vector contains a subset of large entries, and this subset has a sufficiently large cardinality. This subset is the key to our new estimator. ", "page_idx": 3}, {"type": "text", "text": "Lemma 2. Let ${\\mathcal{A}}\\subset[0,1]$ be the set ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A=\\left\\{\\log^{-\\frac{1}{2}}n,\\dots,\\log^{-\\frac{\\lceil L\\rceil-1}{2}}n,\\log^{-\\frac{L}{2}}n\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $L$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nL={\\frac{\\log(2n)}{\\log\\log n}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For n sufficiently large, for every $\\pmb{v}\\in\\mathbb{S}^{n-1}$ ,there exists $\\alpha_{0}\\in A$ suchthat ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{1}{\\alpha_{0}^{2}}\\geq|\\{i:|v_{i}|\\geq\\alpha_{0}\\}|>\\frac{1}{\\alpha_{0}^{2}\\log^{2}n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To motivate Algorithm 1, suppose $\\pmb{u}^{\\star}$ is entrywise positive. For $\\alpha_{0}~\\in~A$ , denote the set in Equation (12) by $I_{\\alpha_{0}}$ . By Equation (12), the sum of entries $M_{i j}^{\\star}$ with $i,j\\ \\in\\ I_{\\alpha_{0}}$ grows as $\\Omega(|\\lambda^{\\star}||I_{\\alpha_{0}}|\\log^{-2}n)$ , while the sum of the corresponding entries of $W$ grows as $O(\\sigma|I_{\\alpha_{0}}|\\sqrt{\\log n})$ That is, when $\\left|\\lambda^{\\star}\\right|$ is sufficiently large, the signal contained in the entries $i,j\\in I_{\\alpha_{0}}$ dominates the noise. If we knew $I_{\\alpha_{0}}$ , utilizing the entries in $I_{\\alpha_{0}}$ would reduce the estimation error incurred by small entries of $\\pmb{u}^{\\star}$ . In practice, we do not know $I_{\\alpha_{0}}$ and must estimate such a subset. To ensure that this is possible, we impose a technical assumption on $\\pmb{u}^{\\star}$ . We discuss this assumption below in Remark 2. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. There exists an $\\alpha_{0}\\in A$ satisfying Equation (12) and a constant $0<\\epsilon_{0}\\le1$ such that for all sufficiently large $n$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\{i:|u_{i}^{\\star}|\\in[(1-\\epsilon_{0})\\alpha_{0},(1+\\epsilon_{0})\\alpha_{0}]\\}=\\emptyset.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We pause to give a few examples to illustrate Assumption 2. First, consider $\\pmb{u}^{\\star}=c_{1}\\pmb{e}_{1}+c_{2}n^{-1/2}\\mathbf{1}_{n}$ \uff0c with $c_{1},c_{2}=\\Theta(1)$ chosen so that $\\lVert\\boldsymbol{u}^{\\star}\\rVert_{2}=1$ . We note that $c_{1},c_{2}$ both depend on $n$ , but are bounded away from zero as $n$ grows, and one can verify that Assumption 2 holds with $\\alpha_{0}\\,=\\,\\log^{-1/2}n$ and $\\epsilon_{0}=1/2$ .As another example, consider $\\dot{\\pmb{u}^{\\star}}=n^{-1/2}\\bar{\\bf1}_{n}\\bar{\\in\\mathbb{R}^{n}}$ . One may verify that taking $\\alpha_{0}=(\\log n)^{-L/2}=1/\\sqrt{2n}$ and $\\epsilon_{0}=0.4$ satisfies the conditions in Assumption 2. ", "page_idx": 3}, {"type": "text", "text": "As an example of a setting that violates Assumption 2, consider $\\pmb{u}^{\\star}$ obtained by renormalizing a vector of i.i.d. Gaussians. This results in $\\pmb{u}^{\\star}$ being Haar-distributed on $\\mathbb{S}^{n-1}$ and Assumption 2 is violated with high probability. To see this, note that $\\pmb{u}^{\\star}\\approx\\pmb{g}/\\sqrt{n}$ where $\\pmb{g}\\sim N(0,\\pmb{I}_{n})$ (see Theorem 3.4.6 in [54]). Since with high probability $\\|g\\|_{\\infty}=O({\\sqrt{\\log n}})$ , Equation (i2) holds only when $\\alpha_{0}\\approx1/{\\sqrt{n}}$ . For Assumption 2 to hold, there must be a gap of $\\Theta(1)$ between the entries of $\\textbf{\\textit{g}}$ , which fails with high probability. ", "page_idx": 3}, {"type": "text", "text": "It is tempting to conclude from the counter-example just given that renormalizing a vector of i.i.d. entries must necessarily result in a $\\pmb{u}^{\\star}$ that violates Assumption 2, but this is not always the case. If the entrywise distribution has suitable structure, $\\pmb{u}^{\\star}$ may still obey Assumption 2. As an illustration, suppose that $\\pmb{u}^{\\star}$ is obtained by renormalizing a vector $\\mathbf{\\bar{g}}=(g_{1},g_{2},\\bar{\\,}...\\,,g_{n})^{\\intercal}\\in\\mathbb{R}^{n}$ with i.i.d. entries from a distribution with variance 1, so that $\\pmb{u}^{\\star}\\approx\\pmb{g}/\\sqrt{n}$ . If the $g_{i}$ are drawn by taking $g_{i}=a$ with probability $p$ and $g_{i}=b$ with probability $1-p$ , then each entry of $\\pmb{u}^{\\star}$ is either approximately $a p/\\sqrt{n}$ or approximately $\\bar{b}(1-p)/\\sqrt{n}$ . Choosing $a,b$ and $p$ appropriately, we can ensure a gap between the entries of $\\pmb{u}^{\\star}$ of size $O(n^{-1/2})$ , and we can take $\\alpha_{0}=(\\log n)^{-L}\\approx n^{-1/2}$ ", "page_idx": 3}, {"type": "text", "text": "Our new estimator of $\\pmb{u}^{\\star}$ is a refinement based on the leading eigenvector and eigenvalue of $\\mathbf{\\deltaY}$ .For the spectral estimator to provide a useful initialization, we make Assumption 3 on $\\lambda^{\\star}$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 3. There exists a constant $C_{1}>2400/\\epsilon_{0}$ where $\\epsilon_{0}$ is as in Assumption 2, such that the leading eigenvalue $\\lambda^{\\star}$ of $M^{\\star}$ satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n|\\lambda^{\\star}|\\geq C_{1}\\sqrt{\\nu_{W}n\\log n}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark1.The dependence on $\\epsilon_{0}$ in Assumption 3 is for technical reasons discussed in Remark 2. In our proofs, we do not optimize the dependence on $C_{1}$ and assume that $C_{1}\\;>\\;2400/\\epsilon_{0}$ .As demonstrated in our experiments in Section 5, $|\\lambda^{\\star}|\\,\\geq\\,\\sqrt{\\nu_{W}n\\log n}$ appears sufficient in practice. When $|\\lambda^{\\star}|\\,\\leq\\,\\sqrt{\\nu_{W}n}$ . the spectral estimator fails to provide any useful initial estimate and it is believed that no polynomial-time algorithm can succeed. We provide more discussion on this matter inRemark $6$ ", "page_idx": 4}, {"type": "text", "text": "The final ingredient required for Algorithm 1 is a leading eigenvalue estimate X that recovers the true signal eigenvalue $\\lambda^{\\star}$ suitablywell. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4. Under Assumption 3, $\\widehat{\\lambda}$ is such that with probability at least $1-O(n^{-8}\\log n)$ \uff0c ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\leq C_{2}\\sqrt{\\nu_{W}}\\log^{5/2}n.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 4 seems stringent at first. The top eigenvalue $\\lambda$ of $\\mathbf{Y}$ achieves only a $O(\\sqrt{\\nu_{W}n})$ error rate (see Lemma 2.2 and Equation (3.12) in [27]). This is because $\\lambda\\!=\\!\\lambda^{\\star}\\!+\\!n\\sigma^{2}/\\lambda^{\\star}\\!+\\!O(\\sqrt{\\nu_{W}\\log n})$ (see [47] or Theorem 2.3 in [23]; see also [26, 51, 17]). Luckily, in our setting, the bias-corrected estimate ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\lambda}_{\\mathrm{c}}=\\frac{1}{2}\\left(\\lambda+\\sqrt{\\lambda^{2}-4n\\sigma^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "does satisfy Assumption 4 [26]. Another estimator, which falls naturally out of Algorithm 1, also satisfies Assumption 4. We find that it performs similarly to the debiased estimator $\\widehat{\\lambda}_{\\mathrm{c}}$ empirically, and so we do not explore it here. Theoretical results for this estimator are in the appendix. ", "page_idx": 4}, {"type": "text", "text": "With the above assumptions in hand, we propose a new method given in Algorithm 1. We note that the main computational bottleneck of Algorithm 1 is to find the leading eigenvector $\\textbf{\\em u}$ of $\\mathbf{Y}$ ,and thus the runtime is essentially the same as for standard spectral methods (see, e.g., [37]). ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Coherence-free eigenvector estimation algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Input: Observed matrix $\\mathbf{Y}\\in\\mathbb{R}^{n\\times n}$ ; leading eigenvalue estimate $\\widehat{\\lambda}$ ; parameter $\\beta>0$ Output: $\\widehat{\\pmb{u}}\\in\\mathbb{R}^{n}$ ", "page_idx": 4}, {"type": "text", "text": "1:If $\\widehat{\\lambda}<0$ set $Y=-Y$ . Obtain the top eigenvector $\\pmb{u}\\in\\mathbb{S}^{n-1}$ of $\\mathbf{\\deltaY}$   \n2:Pick any $\\widehat{\\alpha}\\in A$ such that the set $\\hat{I}=\\{i:|u_{i}|\\geq\\widehat{\\alpha}\\}$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n|\\hat{I}|\\geq{\\frac{1}{{\\widehat{\\alpha}}^{2}\\log^{2}n}},\\quad\\mathrm{and}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\{i:(1-\\beta){\\widehat{\\alpha}}<|u_{i}|<(1+\\beta){\\widehat{\\alpha}}\\}=\\emptyset,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3: Let $Q\\in\\mathbb{R}^{n\\times n}$ be diagonal with $Q_{k k}={\\left\\{\\!\\!\\begin{array}{l l}{\\operatorname{sgn}\\left(u_{k}\\right)}&{{\\mathrm{~if~}}k\\in{\\hat{I}}}\\\\ {1}&{{\\mathrm{~if~}}k\\in{\\hat{I}}^{c}}\\end{array}\\right.}$ and let $\\widetilde{Y}=Q Y Q$ ", "page_idx": 4}, {"type": "text", "text": "4: Set $\\begin{array}{r}{\\widehat{S}=\\sqrt{\\sum_{j,k\\in\\widehat{I}}\\widetilde{Y}_{j k}}}\\end{array}$ and et $\\begin{array}{r}{\\widehat{v}_{j}=\\left(\\sum_{k\\in\\widehat{I}}\\widetilde{Y}_{j k}\\right)\\Big/\\left(\\widehat{S}\\sqrt{\\widehat{\\lambda}}\\right)}\\end{array}$ for $j\\in[n]$   \n5: For each $j\\in[n]$ , set $\\widehat{u}_{j}=u_{j}$ $|u_{j}|\\leq\\left(\\sigma/\\widehat{\\lambda}\\right)\\log n$ , and $\\widehat{u}_{j}=Q_{j j}\\widehat{v}_{j}$ otherwise. ", "page_idx": 4}, {"type": "text", "text": "Remark 2. In our proofs, we set $\\beta=\\epsilon_{0}/2$ .Equation (16), Assumption 2 and the $\\epsilon_{0}$ -dependence in Assumption 3 are technical requirements to ensure that with high probability, $\\hat{I}$ is one of a few deterministicsets,avoidingthecomplicateddependencebetween $\\hat{I}$ and $W$ Empirically,Algorithm 1 works well even without these technical conditions. We conjecture that Assumption 2 as well as the $\\epsilon_{0}$ -dependence in Assumption 3 can be removed. See Section 5 for further discussion. ", "page_idx": 4}, {"type": "text", "text": "As alluded to above, the intuition behind Algorithm 1 is that we aim to concentrate our efforts on estimating the large entries of $\\pmb{u}^{\\star}$ . Consider an entry of $\\mathbf{Y}$ given by $\\lambda^{\\star}u_{i}^{\\star}u_{j}^{\\star}+W_{i j}$ . Intuitively, locations corresponding to small entries of $\\pmb{u}^{\\star}$ produce small $u_{i}^{\\star}u_{j}^{\\star}$ . These entries of $\\mathbf{\\deltaY}$ have a small signal to noise ratio compared to those arising from products of large entries of $\\pmb{u}^{\\star}$ . If we knew the locations of the large entries of $\\pmb{u}^{\\star}$ , we could use them to obtain more accurate estimates of $\\pmb{u}^{\\star}$ Essentially, both Algorithm 1 above and Algorithm 2 presented below consist of two parts: finding the large locations, and using those locations to improve our initial spectral estimate of $\\pmb{u}^{\\star}$ ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 assumes that the entrywise variance $\\sigma^{2}$ of $W$ is known. Of course, in practice, this is not the case, and we must estimate $\\bar{\\sigma}^{2}$ . There are several well-established methods for this estimation task. For example, when $W$ is asymmetric, [36] introduces an estimator based on the median singular value of $\\mathbf{Y}$ . In our case, it sufces to estimate $\\sigma^{2}$ using a simple plug-in estimator ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\sigma}^{2}=\\frac{2}{n(n+1)}\\sum_{1\\leq i\\leq j\\leq n}\\left(Y_{i j}-\\widehat{M}_{i j}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\widehat{\\pmb{M}}=\\lambda\\pmb{u}\\pmb{u}^{\\top}\\in\\mathbb{R}^{n\\times n}$ In general, if $M^{\\star}$ has rank $r$ , then we set $\\widehat{\\pmb{M}}=\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{\\top}$ , where $\\Lambda$ is the leading $r$ eigenvalues of $\\mathbf{Y}$ (sorted by non-increasing magnitude) and $U\\in\\mathbb{R}^{n\\times r}$ contains the corresponding $r$ leading orthonormal eigenvectors as its columns. Lemma 3 controls the estimation error of the plug-in estimator ${\\widehat{\\sigma}}^{2}$ for a general rank- $^r$ signal matrix. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3. Under the model given in Equation (1), let $\\boldsymbol{M}^{\\star}=\\boldsymbol{U}^{\\star}\\boldsymbol{\\Lambda}^{\\star}\\boldsymbol{U}^{\\star\\top}$ be a rank- $^r$ matrix with $r\\geq1$ where A\\* = diag $(\\lambda_{1}^{\\star},\\lambda_{2}^{\\star},\\ldots,\\lambda_{r}^{\\star})$ such that $|\\lambda_{1}^{\\star}|\\geq\\cdots\\geq|\\lambda_{r}^{\\star}|$ .Suppose that Assumption $^{\\,l}$ holds and that $|\\lambda_{r}^{\\star}|\\,\\geq\\,20\\bar{\\sqrt{\\nu_{W}n}}$ then the estimator ${\\widehat{\\sigma}}^{2}$ given in Equation (17) is such that with probability at least $1-O(n^{-8})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left|\\widehat\\sigma^{2}-\\sigma^{2}\\right|\\leq\\frac{400\\nu_{W}r}{n}+\\frac{4\\nu_{W}\\sqrt{\\log n}}{c n}+\\frac{200\\nu_{W}r\\sqrt{\\log n}}{n^{3/2}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $c>0$ is a universal constant. ", "page_idx": 5}, {"type": "text", "text": "Remark 3. We use the plug-in estimator $\\widehat{\\sigma}$ in thedebiased estimator $\\widehat{\\lambda}_{\\mathrm{c}}$ in Equation (14) and to construct $\\widehat{\\pmb{u}}$ in Step 5 of Algorithm 1. Lemma 3 shows that this only introduces an extra log-factor to the error bound, which does not affect the estimation error rate of either $\\lambda^{\\star}$ or $\\pmb{u}^{\\star}$ ", "page_idx": 5}, {"type": "text", "text": "Our main result, Theorem 1, controls the estimation error of Algorithm 1, as measured under $\\ell_{\\infty}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Under the model in Equation (1), suppose that Assumptions $^{\\,l}$ , 2, 3 and 4 hold. Then for $n$ sufficiently large, the estimate $\\widehat{\\pmb{u}}\\in\\mathbb{R}^{n}$ producedby Algorithm $^{\\,l}$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\nd_{\\infty}\\left(\\widehat{\\pmb{u}},\\pmb{u}^{\\star}\\right)\\leq\\frac{C\\sqrt{\\nu_{W}}(\\log n)^{5/2}}{\\left|\\lambda^{\\star}\\right|}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with probability at least $1-O(n^{-8}\\log n)$ where $C>0$ is a universal constant. ", "page_idx": 5}, {"type": "text", "text": "Remark 4. Under Gaussian noise, in the regime $|\\lambda^{\\star}|=\\Omega(\\sigma\\sqrt{n\\log n})$ ,our upper bound is minimax rate-optimal up to log-factors compared to the lower bound in Equation (9).In particular, the rate obtainedinTheorem $^{\\,l}$ does not depend on the coherence parameter $\\mu$ A more careful analysis might be able to remove some of the log-factors inTheorem $^{\\,l}$ , but we leave this matter for future work. ", "page_idx": 5}, {"type": "text", "text": "3Rank- $r$ matrix eigenspace estimation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To handle the more general case in which the signal matrix $M^{\\star}$ is rank $r$ , we propose Algorithm 2, which yields an estimate of $U^{\\star}$ . This is achieved by estimating the $r$ leading eigenvectors separately, then combining them into an estimate $\\hat{\\boldsymbol{U}}\\,\\in\\,\\mathbb{R}^{\\dot{n}\\times{\\boldsymbol{r}}}$ .We explore the empirical performance of Algorithm 2 via simulation in Section 5.2 and leave its theoretical analysis to future work. Algorithm 2 requires the observed matrix $\\mathbf{Y}$ and an estimate of the $k$ -th leading eigenvalue $\\lambda_{k}^{\\star}$ of $M^{\\star}$ as input. Similar to the rank-one case, the top- $-r$ leading eigenvalues $\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{r}$ of $\\mathbf{\\deltaY}$ are biased [47]. We again use a debiased estimator $\\widehat{\\lambda}_{k,c}$ for $k\\in[r]$ as input to Algorithm 2, given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\lambda}_{k,c}=\\frac{1}{2}\\left(\\lambda_{k}+\\sqrt{\\lambda_{k}^{2}-4n\\sigma^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 is a natural extension of Algorithm 1. Essentially, it converts the problem of estimating $\\pmb{u}_{k}^{\\star}$ into an eigenvector estimation problem under a rank-one signal-plus-noise model given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{Y}=\\lambda_{k}^{\\star}\\pmb{u}_{k}^{\\star}\\pmb{u}_{k}^{\\star\\top}+(\\pmb{M}_{-k}^{\\star}+\\pmb{W})=\\lambda_{k}^{\\star}\\pmb{u}_{k}^{\\star}\\pmb{u}_{k}^{\\star\\top}+\\left(\\pmb{M}^{\\star}-\\lambda_{k}^{\\star}\\pmb{u}_{k}^{\\star}\\pmb{u}_{k}^{\\star\\top}+\\pmb{W}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Where $M_{\\mathrm{-}k}^{\\star}=M^{\\star}-\\lambda_{k}^{\\star}{\\pmb u}_{k}^{\\star}{\\pmb u}_{k}^{\\star\\top}$ . There are two differences between Algorithms 1 and 2. First, we remove Equation (16) from Algorithm 1, as this is mainly a technical requirement (see Remark 2). ", "page_idx": 5}, {"type": "text", "text": "More importantly, in Algorithm 2, we conjugate $\\mathbf{\\deltaY}$ by a random orthogonal matrix $\\pmb{H}\\in\\mathbb{O}_{n}$ The $(r-1)$ leading eigenvectors of $H M_{\\mathrm{-}k}^{\\star}\\bar{H}^{\\top}$ form a random subspace of $\\mathbb{R}^{n\\times(r-1)}$ , which allows us to treat $H M_{\\mathrm{-}k}^{\\star}H^{\\top}$ as a noise matrix. By way of illustration, consider the rank-2 case with $M_{-1}^{\\star}=\\lambda_{2}^{\\star}{\\pmb u}_{2}^{\\star}{\\pmb u}_{2}^{\\star\\top}$ ${\\cal H}u_{2}^{\\star}$ behaves similarly to a random vector drawn uniformly from $\\mathbb{S}^{n-1}$ Therefore, one would expect $H u_{2}^{\\star}{u_{2}^{\\star}}^{\\top}H$ to behave similarly to a noise matrix $n^{-1}g g^{\\top}$ , where $\\pmb{g}\\sim N(0,I_{n})$ (see Chapter 3 of [54]). As in the discussion after Lemma 2, we can find a set of indices $I_{\\alpha_{0}}$ such that the signal in the corresponding entries of $\\pmb{H}\\pmb{u}_{1}^{\\star}$ dominates the noise. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 Coherent optimal eigenvector estimation algorithm ", "page_idx": 6}, {"type": "text", "text": "Input: Observed matrix $\\mathbf{Y}\\in\\mathbb{R}^{n\\times n}$ $k$ -th leading eigenvalue estimate $\\widehat{\\lambda}_{k}$ .If $\\widehat{\\lambda}_{k}{<}0$ set $Y{=}{-}Y$ Output: $\\widehat{\\pmb{u}}_{k}\\in\\mathbb{R}^{n}$   \n1: Obtain the top- ${\\bf\\nabla}r$ eigenvectors $\\widetilde{U}$ of $H Y H^{\\top}$ where $H\\in\\mathbb{O}_{n}$ is Haar-distributed.   \n2: Set $Q=\\mathrm{diag}\\left(\\mathrm{sgn}\\left(\\widetilde{U}_{\\cdot,k}\\right)\\right)$ and set $\\widetilde{Y}=Q H Y H^{\\top}Q$   \n3: Pi 4: Set $\\begin{array}{r l}&{\\mathrm{:k~an~}\\alpha_{0}\\in\\mathcal{A}\\mathrm{~such~that~for~}\\hat{I}:=\\left\\{\\,i:|\\hat{U}_{i,k}|\\geq\\alpha_{0}\\,\\right\\},|\\hat{I}|\\geq1/\\alpha_{0}^{2}\\log^{2}n.}\\\\ &{\\hat{S}=\\left(\\sum_{j,\\ell\\in\\hat{I}}\\tilde{Y}_{j\\ell}\\right)^{1/2}\\mathrm{~and~set~}\\hat{v}_{j}=\\sum_{\\ell\\in\\hat{I}}\\tilde{Y}_{j\\ell}\\bigg/\\left(\\hat{S}\\sqrt{\\hat{\\lambda}_{k}}\\right)\\mathrm{~for~}j\\in[n].}\\\\ &{\\,\\quad U=H^{\\top}\\tilde{U}.\\mathrm{~For~}j\\in[n],\\mathrm{set~}\\hat{u}_{k,j}=\\left\\{\\!\\!\\begin{array}{l l}{U_{k,j}}&{\\mathrm{~if~}|U_{k,j}|\\leq\\left(\\sigma/|\\hat{\\lambda}_{k}|\\right)\\log n}\\\\ {\\!\\left(H^{\\top}Q\\hat{v}\\right)_{j}}&{\\mathrm{~otherwise}.}\\end{array}\\!\\!\\right.}\\end{array}$ 5: Let ", "page_idx": 6}, {"type": "text", "text": "As mentioned above, our experiments in Section 5.2 indicate that Algorithm 2 performs well. A proof of its performance, however, is more complicated than Theorem 1. The main difficulty arises from the fact that in Algorithm 2, we conjugate by a random orthogonal transformation. This ensures that when considering the large entries of one signal eigenvector, the other signal eigenvectors are ignorable. Unfortunately, this random orthogonal transformation breaks Assumption 2 and introduces complicated dependency structure, requiring a more careful analysis that we leave for future work. ", "page_idx": 6}, {"type": "text", "text": "4  Estimation lower bounds under $\\ell_{2,\\infty}$ distance ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 1 demonstrates that for a rank-one signal, dependence of the estimation rate on $\\mu$ can be removed when $|\\lambda^{\\star}|\\,=\\,\\Omega(\\sigma\\sqrt{n\\log n})$ : In this regime, our result matches the lower bound in Equation (9) up to log-factors, making this the minimax lower bound under Assumption 2. As discussed in Remark 2, we expect Assumption 2 can be removed via a more careful analysis, rendering the lower bound in Equation (9) optimal under $|\\lambda^{\\star}|=\\Omega(\\sigma\\sqrt{n\\log n})$ . On the other hand, as discussed in Section 1.2, the lower bound in Equation (9) is sub-optimal in the regime where $|\\lambda^{\\star}|=O(\\sigma{\\sqrt{n}})$ . In what follows, we aim to improve upon Equation (9) by deriving metric entropy bounds [54] for rank- $^r$ singular subspaces under the $\\ell_{2,\\infty}$ distance when $|{\\dot{\\lambda}}^{\\star}|=O({\\bar{\\sigma}}{\\sqrt{n}})$ ", "page_idx": 6}, {"type": "text", "text": "Recall that for a semi-metric $\\rho$ defined on a set $\\mathbb{K}$ , we may define the $\\delta$ -packing number $\\mathcal{M}(\\mathbb{K},\\rho,\\delta)$ of $\\mathbb{K}$ under $\\rho$ (see Chapter 15 in [56]). The packing $\\delta\\!\\cdot$ -entropy, $\\log\\mathcal{M}(\\mathbb{K},\\rho,\\delta)$ , captures the complexity of the space $\\mathbb{K}$ , and a lower bound on the $\\delta\\!\\cdot$ -entropy can be translated into a lower bound on the minimax estimation error rate. For a given $r\\in[n]$ and $1\\leq\\mu\\leq n/r$ , we consider the parameter set ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{K}(n,r,\\sqrt{\\mu r/n})=\\left\\{U\\in\\mathbb{R}^{n\\times r}:U^{\\top}U=I_{r},\\ \\|U\\|_{2,\\infty}\\leq\\sqrt{\\frac{r\\mu}{n}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Below, we write $\\mathbb{K}_{r,\\mu}$ for $\\mathbb{K}(n,r,\\sqrt{\\mu r/n})$ . Lemma 4 lower bounds $\\log\\mathcal{M}(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta)$ . We focus on the range $r/n\\lesssim\\delta^{2}\\lesssim\\mu r/n$ as the lower bound in Equation (9) shows that $\\delta^{2}\\ll r/n$ is not achievable when $|\\lambda^{\\star}|=O(\\sigma\\sqrt{n})$ and $\\delta^{2}\\gg\\mu r/n$ is achieved by the trivial all-zeros estimate. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4. Suppose that $n/\\mu\\geq\\operatorname*{max}\\{4,r\\}$ and $\\mu\\geq12\\log(12n)$ ,and let $\\delta>0$ be such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{c_{0}^{2}r}{8e^{2}n}\\leq\\delta^{2}\\leq\\frac{c_{0}^{2}\\mu r}{96e^{2}n\\log\\left(12n/\\mu\\right)},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $c_{0}>0$ is a universal constant. Then when $n$ is sufficiently large, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\log\\mathcal{M}\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta\\right)\\gtrsim\\frac{r^{2}}{\\delta^{2}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 5. Lemma 4 applies when $\\log n\\lesssim\\mu\\lesssim n/r$ .A more careful analysis might relax the lower bound, but when $\\mu\\lesssim\\log n$ any $U\\in\\mathbb{K}_{r,\\mu}$ is nearly incoherent and Equation (9) is nearly optimal. An upper bound matching Lemma $^{4}$ up to log-factors can be found in the appendix. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4 implies an improved lower bound compared to Equation (9). Consider the parameter space ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\Omega(\\lambda^{\\star},\\mu,r)=\\{(\\Lambda^{\\star},U^{\\star}):\\Lambda^{\\star}=\\lambda^{\\star}I_{r},U^{\\star}\\in\\mathbb{K}_{r,\\mu}\\}\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using the Yang-Barron method [59], we obtain a lower bound for eigenspace estimation in the rank- $r$ signal-plus-noise model for $|\\lambda^{\\star}|=O(\\sigma{\\sqrt{n}})$ . A detailed proof can be found in the appendix. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Under Assumption 1 and the conditions of Lemma 4, for any $0<\\lambda^{\\star}\\leq(6\\sqrt{C_{0}})^{-1}\\sigma\\sqrt{n}_{\\star}$ where $C_{0}>0$ is a universal constant related to covering numbers of Grassmann manifolds, there is $a$ universal constant $c>0$ such that for all sufficiently large $n$ \uff0c ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\substack{\\widehat{U}\\in\\mathbb{R}^{n\\times r}\\,\\left(\\Lambda^{\\star},U^{\\star}\\right)\\in\\Omega(\\lambda^{\\star},\\mu,r)}}\\mathbb{E}_{\\Lambda^{\\star},U^{\\star}}d_{2,\\infty}\\left(\\widehat{U},U^{\\star}\\right)\\geq c\\left(\\frac{\\sigma\\sqrt{r}}{\\lambda^{\\star}}\\wedge\\sqrt{\\frac{\\mu r}{n\\log(n/\\mu)}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Remark 6.Theorem 2 removes the upper limit $\\sqrt{r/n}$ from Equation (9), suggesting that $\\mu$ only comes to bear when $\\lambda^{\\star}\\leq\\sigma\\sqrt{n}$ This regime is not well studied,as theBBP transition $I8J$ implies that spectral methods fail, but other algorithms might achieve our lower bound. For example, signal detection is possible if structure is present $\\it l^{6}J$ Unfortunately, any such algorithm is likely tobe computationally expensive, given the general belief that no polynomial-time algorithm can succeed when $\\lambda^{\\star}\\leq\\sigma\\sqrt{n}\\,[39,\\,9]$ .Weleavefurtherexplorationof thissmall- $\\lambda^{\\star}$ regime to future work. ", "page_idx": 7}, {"type": "text", "text": "Remark7.Theparameter space $\\Omega(\\lambda^{\\star},\\mu,r)$ considered inTheorem2contains only signalmatrices withconditionnumber $\\kappa\\,=\\,1$ .Inrecentwork,theauthorshaveestablishedlowerboundsakin to Theorem 2 that show the role of condition number. A full accounting of the interplay between condition number and coherence is a promising area for future work. ", "page_idx": 7}, {"type": "text", "text": "5  Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We turn to a brief experimental exploration of our theoretical results. All experiments were run in a distributed environment on commodity hardware without GPUs. In total, the experiments reported below used 3425 compute-hours. Mean memory usage was $3.5\\;\\mathrm{GB}$ , with a maximum of 11 GB. ", "page_idx": 7}, {"type": "text", "text": "5.1   Simulations for rank-one eigenspace estimation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We begin with the rank-one setting considered in Algorithm 1, in which we observe ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pmb{Y}=\\pmb{M}^{\\star}+\\pmb{W}=\\lambda^{\\star}\\pmb{u}^{\\star}\\pmb{u}^{\\star\\top}+\\pmb{W},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and wish to recover $\\pmb{u}^{\\star}\\in\\mathbb{S}^{n-1}$ .We take $\\lambda^{\\star}=\\sqrt{n\\log n}$ in all experiments, matching the rate in Remark 1. We consider three distributions for the entries of $W$ : Gaussian, Laplacian and Rademacher, all scaled to have variance $\\sigma^{2}=1$ . We consider two approaches to generating $\\pmb{u}^{\\star}$ . In either case, we set a random entry of $\\pmb{u}^{\\star}$ to be $a\\in\\{0.3,0.55,0.8\\}$ , then generate the remaining entries by either ", "page_idx": 7}, {"type": "text", "text": "1. drawing uniformly from ${\\sqrt{1-a^{2}}}\\mathbb{S}^{n-2}$ ,or   \n2. drawing uniformly from $\\{\\pm1\\}^{n-1}$ then normalizing these to have $\\ell_{2}$ norm $\\sqrt{1-a^{2}}$ ", "page_idx": 7}, {"type": "text", "text": "In both cases, $\\|\\pmb{u}^{\\star}\\|_{\\infty}{=}a$ and $\\mu\\!=\\!a^{2}n$ with high probability. We take $\\scriptstyle a=\\Theta(1)$ , since in finite samples, $C n^{-1/2}\\log n$ (which is nearly incoherent) is hard to discern from a constant (e.g., when $n\\!=\\!20000$ $4n^{-1/2}\\mathrm{log}\\,n\\approx0.28$ nearly matching $a\\!=\\!0.3$ ). Having generated $\\pmb{Y}=\\pmb{M}^{\\star}+\\pmb{W}$ , we estimate $\\pmb{u}^{\\star}$ using both the spectral estimate $\\textbf{\\em u}$ and Algorithm 1 and measure their estimation error under $d_{\\infty}$ .We report the mean of 20 independent trials for each combination of problem size $n$ , magnitude $a$ and methods for generating $\\pmb{u}^{\\star}$ and $W$ . We vary $n$ from 100 to 15100 in increments of 1000. ", "page_idx": 7}, {"type": "text", "text": "When running Algorithm 1, we use the debiased estimate $\\widehat{\\lambda}_{\\mathrm{c}}$ from Equation (14). This requires an estimate of $\\sigma$ , for which we use the plug-in estimator in Equation (17). We set $\\beta=0$ eliminating Equation (16). Similar to Algorithm 2, we conjugate $\\mathbf{\\deltaY}$ by a random orthogonal matrix $H\\in\\mathbb{O}_{n}$ We expect the top eigenvector $\\widetilde{\\pmb{u}}$ of $H Y H^{\\top}$ to be approximately uniformly distributed on $\\mathbb{S}^{n-1}$ , as it is close to. $H u^{\\star}$ (see Theorem 2.1 in [44]). Thus, the median of the absolute values of $\\widetilde{\\pmb{u}}$ should be $\\Theta(n^{-1/2})$ . Instead of selecting $\\alpha_{0}$ according to Equation (15), we set $\\alpha_{0}$ to be this median. The requirement in Equation (15) is then fulfilled, since there are roughly $n/2$ entries larger than the median. After obtaining $\\widehat{\\pmb{u}}$ from Algorithm 1, we return $H^{\\top}\\widehat{\\pmb{u}}$ as our estimate of $\\pmb{u}^{\\star}$ . We note that this random rotation further serves to illustrate that Assumption 2 is merely a technical requirement: after a random rotation, with high probability, $H u^{\\star}$ does not satisfy Assumption 2. ", "page_idx": 7}, {"type": "image", "img_path": "CiuH7zOBCQ/tmp/7ad21f402e2784251413aafb567d66a58bc91ff0ba97b30d9262a924727b99cb.jpg", "img_caption": ["Figure 1: Estimation error measured in $d_{\\infty}$ as a function of dimension $n$ , by the leading eigenvector (blue) and Algorithm 1 (orange), for $\\lVert\\pmb{u}^{\\star}\\rVert_{\\infty}$ equal to 0.8, 0.55 and 0.3 (dotted, dashed and solid lines, respectively). We consider $\\pmb{u}^{\\star}$ generated from the Bernoulli (top row) and Haar (bottom row) schemes, and we consider Gaussian (left), Rademacher (middle) and Laplacian (right) noise. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 1 compares the accuracy in estimating $\\pmb{u}^{\\star}$ using the leading eigenvector of $\\mathbf{Y}$ (blue) and Algorithm 1 (orange) under the three noise settings and two generating procedures for $\\pmb{u}^{\\star}$ . Shaded bands indicate $95\\%$ bootstrap confidence intervals (CIs). Across settings, Algorithm 1 recovers $\\pmb{u}^{\\star}$ with a much smaller estimation error under $d_{\\infty}$ compared to the naive spectral estimate, especially when $\\lVert\\pmb{u}^{\\star}\\rVert_{\\infty}$ (i.e., the coherence $\\mu_{.}^{-}$ is large. The spectral method degrades noticeably as coherence increases, while Algorithm 1 has far less dependence on $\\mu$ . Indeed, under Gaussian noise (the first column of Figure 1), it has no visible dependence on $\\mu$ . Under Rademacher noise (middle column of Figure 1), the dependence of Algorithm 1 on $\\mu$ appears slightly reversed from that of the spectral estimator. Under Laplacian noise (right column of Figure 1), there seems to be a slight dependence on $\\mu$ . Further examination in the appendix suggests that this is due to estimating the entries other than the largest element of $\\pmb{u}^{\\star}$ , and is likely asymptotically smaller than the rate in Theorem 1. ", "page_idx": 8}, {"type": "text", "text": "5.2  Simulations for rank- $^r$ eigenvector estimation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For the rank- $^r$ setting, we have a signal matrix with eigenvalues $|\\lambda_{1}^{\\star}|\\geq\\ldots\\geq|\\lambda_{r}^{\\star}|$ We observe ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\pmb{Y}=\\pmb{M}^{\\star}+\\pmb{W}=\\pmb{U}^{\\star}\\pmb{\\Lambda}^{\\star}\\pmb{U}^{\\star}+\\pmb{W},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\underline{{\\mathbf{A}}}^{\\star}=\\mathrm{diag}(\\lambda_{1}^{\\star},\\dots,\\lambda_{r}^{\\star})$ , and we wish to recover $U^{\\star}\\,\\in\\,\\mathbb{R}^{n\\times r}$ .We take $\\sigma\\,=\\,1$ and $\\lambda_{r}^{\\star}=$ $\\sqrt{n\\log n}$ . Recovering each column of $U^{\\star}$ separately requires an eigengap, defined for $k\\,\\in\\,[r]$ as $\\Delta_{k}:=|\\lambda_{k}^{\\star}-\\lambda_{k+1}^{\\star}|$ and $\\Delta_{0}\\,=\\,\\infty$ .Typically, the estimation error of the $k$ -th eigenvector has a $O(\\operatorname*{min}\\{\\Delta_{k},\\Delta_{k-1}\\}^{-1})$ dependence on the eigengaps [57]. Here, we set $\\Delta_{k}\\,=\\,0.5\\sqrt{n\\log n}$ $\\lambda_{k}^{\\star}=0.5(r-k+2)\\sqrt{n\\log n}$ for all $k\\,\\in\\,[r]$ . As in the rank-one case, we generate entries of $W$ from Gaussian, Laplacian and Rademacher distributions, all scaled to have unit variance. The true eigenvectors $U^{\\star}$ are generated by repeating the following procedure for $k\\in[r]$ ", "page_idx": 8}, {"type": "text", "text": "1. Randomly select an element of $\\pmb{v}\\in\\mathbb{R}^{n}$ and set it to be $a\\in\\{0.3,0.55,0.8\\}$ 2. Generate the rest of $\\pmb{v}$ by drawing uniformly from ${\\sqrt{1-a^{2}}}\\mathbb{S}^{n-2}$ 3. Set ut = (In - U\\*,1:(k-1)U\\*1\u00b1(k-1) , normalize $\\pmb{u}_{k}^{\\star}$ to have unit $\\ell_{2}$ norm, and set $U^{\\star}.,_{k}=u_{k}^{\\star}$ If $k=1$ then we take $U^{\\star}.,\\stackrel{\\cdot}{1:(k-1)}U^{\\star}{}_{\\cdot,1:(k-1)}^{\\top}$ \\*,1:(e-1) to be the zero matrix. ", "page_idx": 8}, {"type": "text", "text": "Under this procedure, each $\\pmb{u}_{k}^{\\star}$ has coherence approximately $a^{2}n$ . Since the large entries of the $\\pmb{u}_{k}^{\\star}$ are unlikely to appear in the same rows, $U^{\\star}$ also has coherence $a^{2}n$ . We take $r=2$ here. ", "page_idx": 8}, {"type": "image", "img_path": "CiuH7zOBCQ/tmp/e884feeaa15567644d512c93639fa5111ed5c9089940eca0ac3769932d864cf4.jpg", "img_caption": ["Figure 2: Estimation error under $d_{\\infty}$ as a function of size $n$ , by the $k$ -th eigenvector (blue/purple) and the estimator in Algorithm 2 (orange/red) for $\\lVert\\pmb{u}^{\\star}\\rVert_{\\infty}$ equal to 0.8 (dotted lines), 0.55 (dashed lines) or 0.3 (solid lines) with Gaussian (left), Rademacher (center) or Laplacian (right) noise. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Having generated $\\pmb{Y}\\!\\!=\\!\\!M^{\\star}\\!\\!\\!+W$ , we obtain estimates via the naive spectral method and Algorithm 2. and measure their estimation error under $d_{\\infty}$ . We vary $n$ from 100 to 15100 in increments of 1000 and report the mean of 20 independent trials for each combination of $n,\\,a$ and noise distribution. The results are summarized in Figure 2, showing the error for $\\pmb{u}_{k}^{\\star}$ $k\\in\\{1,2\\}$ using the spectral estimate (blue/purple) and Algorithm 2 (orange/red), under Gaussian (left), Rademacher (middle) and Laplacian (right) noise. Shaded bands indicate $95\\%$ bootstrap CIs. In all settings, Algorithm 2 improves markedly on the spectral estimator. Algorithm 2 shows no visible dependence on $\\mu$ under Gaussian noise. Under Rademacher noise, its $\\mu$ -dependence is the reverse of the spectral estimator. Under Laplacian noise, it shows slight dependence on $\\mu$ , which we again expect to be asymptotically smaller than the rate in Theorem 1. In the appendix, we consider $r\\!=\\!3$ and measure error under $d_{2,\\infty}$ \uff0c and we again find that Algorithm 2 outperforms spectral estimators and is far less sensitive to $\\mu$ ", "page_idx": 9}, {"type": "text", "text": "5.3   Comparison with other methods ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To the best of our knowledge, we are the first paper to consider the task of non-spectral entrywise eigenvector estimation. The nearest obvious competing method might be based on approximate message passing (AMP; see [34] for an overview). Such a comparison is included in the appendix, where our experiments indicate that while AMP performs well in recovering the signal eigenvectors as measuredby $\\ell_{2}$ error, our method as specified in Algorithms 1 and 2 perform better under entrywise and $\\ell_{2,\\infty}$ error. Experimental details and further discussion can be found in the appendix. ", "page_idx": 9}, {"type": "text", "text": "6   Discussion, limitations and conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented new methods for eigenvector estimation in signal-plus-noise matrix models and new lower bounds for estimation rates in these models. The entrywise estimation error of our method has no dependence on the coherence $\\mu$ for rank-one signal matrices, and achieves the optimal estimation rate up to log-factors. Simulations show that our method tolerates non-Gaussian noise and its extension to rank- ${\\bf\\nabla}r$ signal matrices has little dependence on $\\mu$ . One limitation of our method is that it assumes homoscedastic noise. Future work will aim to relax this assumption and the technical condition in Assumption 2. In the rank- ${\\bf\\nabla}r$ case, Algorithm 2 estimates each eigenvector separately, requiring an eigengap. Future work will avoid this by simultaneously or iteratively estimating multiple eigenvectors. We note in closing that inequitable social impacts from abuse or misuse of models and methods are common, but we see no particular such impacts in the present work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors gratefully acknowledge the support of the United States National Science Foundation via grants NSF DMS 2052918 and NSF DMS 2023239. KL was additionally supported by the University of Wisconsin-Madison Office of the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  E. Abbe. Community detection and stochastic block models: Recent developments. Journal of Machine Learning Research, 18(177):1-86, 2018.   \n[2] E. Abbe, J. Fan, K. Wang, and Y. Zhong. Entrywise eigenvector analysis of random matrices with low expected rank. Annals of Statistics, 48(3):1452, 2020.   \n[3] J. Agterberg and J. Sulam. Entrywise recovery guarantees for sparse PCA via sparsistent algorithms. In International Conference on Artifcial Intelligence and Statistics, pages 6591-6629. PMLR, 2022.   \n[4]  J. Agterberg and A. Zhang. Estimating higher-order mixed memberships via the $\\ell_{2,\\infty}$ tensor perturbation bound. arXiv preprint arXiv:2212.08642, 2022.   \n[5]  J. Agterberg, Z. Lubberts, and C. E. Priebe. Entrywise estimation of singular vectors of low-rank matrices with heteroskedasticity and dependence. IEEE Transactions on Information Theory, 68(7):4618-4650, 2022.   \n[6]  A. E. Alaoui, F. Krzakala, and M. Jordan. Fundamental limits of detection in the spiked Wigner model. Annals of Statistics, 48(2):863 - 885, 2020.   \n[7] S. Athey, M. Bayati, N. Doudchenko, G. Imbens, and K. Khosravi. Matrix completion methods for causal panel data models. Journal of the American Statistical Association, 116(536):1716-1730, 2021.   \n[8]JI Baik, G. B.Arous, and S. Peche. Phase transition of the largest eigenvalue for nonnull comlex saple covariance matrices. The Annals of Probability, 33(5):1643 - 1697, 2005.   \n[9]  J. Banks, C. Moore, R. Vershynin, N. Verzelen, and J. Xu. Information-theoretic bounds and phase transitions in clustering, sparse PCA, and submatrix localization. IEEE Transactions on Information Theory, 64(7):4872-4894, 2018.   \n[10]Z.Ba, X.Ding, and K.Wang.Singular vetr and singular ubspace distrutionfor the matrix enoisng model. Annals of Statistics, 49(1):370 - 392, 2021.   \n[11]  Z. Bao, X. Ding, J. Wang, and K. Wang. Statistical inference for principal components of spiked covariance matrices. Annals of Statistics, 50(2):1144 - 1169, 2022.   \n[12]  F. Benaych-Georges and R. R. Nadakuditi. The eigenvalues and eigenvectors of finite, low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494-521, 2011. ISSN 0001-8708.   \n[13]  T. Bendokat, R. Zimmermann, and P-A. Absil. A Grassmann manifold handbook: Basic geometry and computational aspects. Advances in Computational Mathematics, 50(1):1-51, 2024.   \n[14]  A.Bhardwaj and VVu. Matrix perturbation: Davis-Kahan in the infinity norm. In Proceedings of the 2024 Anual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 880-934. SIAM, 2024.   \n[15]  R.Bhatia. Matrix Analysis, volume 169 of Graduate Texts in Mathematics. Springer Science & Business Media, 2013.   \n[16]  N. Boumal. An Introduction to Optimization on Smooth Manifolds. Cambridge University Press, 2023.   \n[17]  W. Bryc and J. W. Silverstein. Singular values of large non-central random marices. Random Matrices: Theory and Applications, 9(04):2050012, 2020.   \n[18]  C. Cai, G. Li, Y. Chi, H. V. Poor, and Y. Chen. Subspace estimation from unbalanced and incomplete data matrices: $\\ell_{2,\\infty}$ statistical guarantes. Annals of Statistics, 49(2):944 - 967, 2021.   \n[19]  T. Cai, H. Li, and R. Ma. Optimal structured principal subspace estimation: Metric entropy and minimax rates. Journal of Machine Learning Research, 22(46):1-45, 2021.   \n[20] T. T. Cai and A. Zhang. Rate-optimal perturbation bounds for singular subspaces with applications to high-dimensional statistics. Annals of Statistics, 46(1):60 - 89, 2018.   \n[21]  T. T. Cai, Z. Ma, and Y. Wu. Sparse PCA: Optimal rates and adaptive estimation. Annals of Statistics, 41 (6):3074 - 3110, 2013.   \n[22] J. Cape, M. Tang, and C. E. Priebe. The two-to-infinity norm and singular subspace geometry with applications to high-dimensional statistics. Annals of Statistics, 47(5):2405 - 2439, 2019.   \n[23]  M. Capitaine, C. Donati-Martin, and D. Feral. The largest eigenvalues of finite rank deformation of large Wigner matrices: Convergence and nonuniversality of the fuctuations. The Annals of Probability, 37(1):1 - 47, 2009.   \n[24] S. Chatterjee. Matrx estimationby universal singular alu thresholding. Annals of Statistics, 43(1):177- 214, 2015.   \n[25]  P. Chen, C. Gao, and A. Y. Zhang. Partial recovery for top-k ranking: optimality of MLE and suboptimality of the spectral method. Annals of Statistics, 50(3):1618-1652, 2022.   \n[26]  Y. Chen, C. Cheng, and J. Fan. Asymmetry helps: Eigenvalue and eigenvector analyses of asymmetrically perturbed low-rank matrices. Annals of Statistics, 49(1):435, 2021.   \n[27]  Y. Chen, Y. Chi, J. Fan, and C. Ma. Spectral methods for data science: A statistical perspective. Foundations and Trends in Machine Learning, 14(5):566-806, 2021.   \n[28]  C. Cheng, Y. Wei, and Y. Chen. Tackling small eigen-gaps: Fine-grained eigenvector estimation and inference under heteroscedastic noise. IEEE Transactions on Information Theory, 67(11):7380-7419, 2021.   \n[29]  Y. Chikuse. Statistics on Special Manifolds, volume 174 of Lecture Notes in Statistics. Springer Science & Business Media, 2012.   \n[30]  D. Donoho and M. Gavish. Minimax risk of matrix denoising by singular value thresholding. Annals of Statistics, 42(6):2413-2440, 2014.   \n[31]  J. Fan, W. Wang, and Y. Zhong. An $\\ell_{\\infty}$ eigenvector perturbation bound and its application to robust covariance estimation. Journal of Machine Learning Research, 18(207):1-42, 2018.   \n[32]  J. Fan, K. Wang, Y. Zhong, and Z. Zhu. Robust high dimensional factor models with applications to statistical machine learning. Statistical Science: a review journal of the Institute of Mathematical Statistics, 36(2):303, 2021.   \n[33] JFan, YFan,X.Han, and J Lv. Asyptotic thery of igenvectors for randmmatrces with diverging spikes. Journal of the American Statistical Association, 117(538):996-1009, 2022.   \n[34]  O. Y. Feng, R. Venkataramanan, C. Rush, and R. J. Samworth. A unifying tutorial on approximate message passing. Foundations and Trends in Machine Learning, 15(4):335-536, 2022.   \n[35]  C. Gao and Z. Ma. Minimax rates in network analysis: Graphon estimation, community detection and hypothesis testing. Statistical Science, 36(1):16 - 33, 2021. doi: 10.1214/19-STS736.   \n[36]  M. Gavish and D. L. Donoho. The optimal hard threshold for singular values is $4/\\sqrt{3}$ IEEE Transactions on Information Theory, 60(8):5040-5053, 2014.   \n[37]  G. H. Golub and C. F. Van Loan. Matrix Computations. Johns Hopkins University Press, 4 edition, 2013.   \n[38] F. Haddadi and A. Amini. Eigenvectors of deformed Wigner random matrices. IEEE Transactions on Information Theory, 67(2):1069-1079, 2021.   \n[39]  F. Krzakala, J. Xu, and L. Zdeborova. Mutual information in rank-one matrix estimation. In 2016 IEEE Information Theory Workshop (ITW), pages 71-75, 2016.   \n[40]  J Lei and K.Z.Lin. Bias-ajusted spectral clustering i multi-layer stochastic block modls. Joual f the American Statistical Association, pages 1-13, 2022.   \n[41]  L. Lei., Unified $\\ell_{2\\to\\infty}$ eigenspace perturbation theory for symmetric random matrices. arXiv preprint arXiv: 1909.04798, 2019.   \n[42]  G. Li, C. Cai, H. V. Poor, and Y. Chen. Minimax estimation of linear functions of eigenvectors in the face of small eigen-gaps. arXiv preprint arXiv:2104.03298, 2021.   \n[43]  S. Negahban, S. Oh, and D. Shah. Iterative ranking from pair-wise comparisons. In F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.   \n[44]  S. O'Rourke, V. Vu, and K. Wang. Eigenvectors of random matrices: a survey. Journal of Combinatorial Theory, Series A, 144:361-442, 2016.   \n[45]  S. O'Rourke, V. Vu, and K. Wang. Matrices with gaussian noise: optimal estimates for singular subspace perturbation. IEEE Transactions on Information Theory, 2023.   \n[46]  A. Pajor. Metric entropy of the Grassmann manifold. In Convex Geometric Analysis, pages 181-188. Cambridge University Press, 1998.   \n[47]  M. Peng. Eigenvalues of deformed random matrices. arXiv preprint arXiv: 1205.0572, 2012.   \n[48]  B. Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12(12), 2011.   \n[49]  K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel. Annals of Statistics, 39(4):1878 - 1915, 2011.   \n[50]  P. Rubin-Delanchy, J. Cape, M. Tang, and C. E. Priebe. A statistical interpretation of spectral embedding: the generalised random dot product graph. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4):1446-1473, 2022.   \n[51]  J. W. Silverstein. The spectral radi and norms of large dimensional non-central random matrices. Stochastic Models, 10(3):525-532, 1994.   \n[52]  D. L. Sussman, M. Tang, and C. E. Priebe. Consistent latent position estimation and vertex classification for random dot product graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(1): 48-57, 2013.   \n[53]  J. A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 8(1-2):1-230, 2015.   \n[54]  R. Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.   \n[55]  V. Vu and J Lei. Minimax sparse principal subspace estimation in high dimensions. Annals of Statistics, 41(6):2905 - 2947, 2013.   \n[56]  M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.   \n[57] K. Wang. Analysis of singular subspaces under random perturbations. arXiv preprint arXiv:2403.09170, 2024.   \n[58]  D. Xia. Normal approximation and confidence region of singular subspaces. Electronic Journal of Statistics, 15(2):3798-3851, 2021.   \n[59]  Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. Annals of Statistics, pages 1564-1599, 1999.   \n[60] Y. Yu, T. Wang, and R. J. Samworth. A useful variant of the Davis-Kahan theorem for statisticians. Biometrika, 102(2):315-323, 2014.   \n[61]  A. R. Zhang, T. T. Cai, and Y. Wu. Heteroskedastic PCA: Algorithm, optimality, and applications. Annals of Statistics, 50(1):53-80, 2022.   \n[62]  Y. Zhou and Y. Chen. Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA. arXiv preprint arXiv:2303.06198, 2023.   \n[63] Z. Zhou, F. Zhou, P. Li, and C.-H. Zhang. Rate-optimal subspace estimation on random graphs. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 20283-20294. Curran Associates, Inc., 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof of Lemma 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{\\theta=\\operatorname*{lim}_{n\\rightarrow\\infty}(\\sigma\\sqrt{n})^{-1}\\lambda^{\\star}}\\end{array}$ .For any $\\pmb{u}^{\\star}\\in\\mathbb{S}^{n-1}$ , by the Baik-Ben Arous-P\u00e9ch\u00e9 (BBP) phase transition ([8]; see also Theorem 2 in [38]), the top eigenvector $\\textbf{\\em u}$ of $\\mathbf{Y}$ obeys ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\langle\\pmb{u},\\pmb{u}^{\\star}\\rangle^{2}\\xrightarrow{\\mathrm{~a.s.~}}\\left\\{1-\\theta^{-2}\\quad\\mathrm{~if~}\\theta>1\\atop0\\right.\\cdot\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $s=\\lceil n/\\mu\\rceil$ , and consider ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{u}^{\\star}=\\left(\\frac{1}{\\sqrt{s}}\\mathbf{1}_{s}^{\\top},\\mathbf{0}_{n-s}^{\\top}\\right)^{\\top}\\in\\mathbb{R}^{n}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Without loss of generality, we assume that $d_{\\infty}(\\pmb{u},\\pmb{u}^{\\star})=\\|\\pmb{u}-\\pmb{u}^{\\star}\\|_{\\infty}$ , since otherwise we can repeat the following argument with $-\\pmb{u}$ instead of $\\textbf{\\em u}$ .We have ", "page_idx": 13}, {"type": "equation", "text": "$$\nd_{\\infty}(\\pmb{u},\\pmb{u}^{\\star})\\geq\\operatorname*{max}_{i\\in[s]}|u_{i}-u_{i}^{\\star}|\\geq\\frac{1}{\\sqrt{s}}\\left(\\sum_{i=1}^{s}|u_{i}-u_{i}^{\\star}|^{2}\\right)^{1/2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Expanding the term inside the square root, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{s}\\left|u_{i}-u_{i}^{\\star}\\right|^{2}=\\sum_{i=1}^{s}u_{i}^{2}+\\sum_{i=1}^{s}u_{i}^{\\star2}-2\\sum_{i=1}^{s}u_{i}^{\\star}u_{i}=\\sum_{i=1}^{s}u_{i}^{2}+1-2\\langle\\pmb{u}^{\\star},\\pmb{u}\\rangle,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last equality follows from the construction of $\\pmb{u}^{\\star}$ in Equation (24). By the Cauchy-Schwarz inequality, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{s}u_{i}^{2}\\geq\\frac{1}{s}\\left(\\sum_{i=1}^{s}u_{i}\\right)^{2}=\\left(\\sum_{i=1}^{s}\\frac{1}{\\sqrt{s}}u_{i}\\right)^{2}=\\langle\\pmb{u}^{\\star},\\pmb{u}\\rangle^{2},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last equality follows from the construction of $\\pmb{u}^{\\star}$ in Equation (24). Plugging Equation (27) into Equation (26), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{s}\\left|u_{i}-u_{i}^{\\star}\\right|^{2}\\geq\\langle\\pmb{u}^{\\star},\\pmb{u}\\rangle^{2}+1-2\\langle\\pmb{u}^{\\star},\\pmb{u}\\rangle=\\left(1-\\langle\\pmb{u}^{\\star},\\pmb{u}\\rangle\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "When $\\theta>1$ , applying Equation (23) yields that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\sum_{i=1}^{s}\\left|u_{i}-u_{i}^{\\star}\\right|^{2}\\geq\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{inf}\\left(1-\\langle u^{\\star},\\pmb{u}\\rangle\\right)^{2}=\\left(1-\\sqrt{1-\\frac{1}{\\theta^{2}}}\\right)^{2}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "holds almost surely. Again using the fact that $\\theta>1$ ,wehave ", "page_idx": 13}, {"type": "equation", "text": "$$\n1-{\\sqrt{1-{\\frac{1}{\\theta^{2}}}}}=\\left(1+{\\sqrt{1-{\\frac{1}{\\theta^{2}}}}}\\right)^{-1}\\left[1-\\left(1-{\\frac{1}{\\theta^{2}}}\\right)\\right]\\geq{\\frac{1}{2\\theta^{2}}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining Equations (25), (28) and (29), it holds almost surely that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{inf}_{\\mathbf{\\omega}}d_{\\infty}(\\pmb{u},\\pmb{u}^{\\star})\\geq\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{inf}_{\\sqrt{s}}\\Big(\\vert u_{i}-u_{i}^{\\star}\\vert^{2}\\Big)^{1/2}\\geq\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{2\\theta^{2}}\\cdot\\frac{1}{\\sqrt{1+n/\\mu}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inequality follows from $s=\\lceil n/\\mu\\rceil$ . Since $\\mu\\le n$ , it follows that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{inf}_{\\mathbf{\\pi}}d_{\\infty}(\\pmb{u},\\pmb{u}^{\\star})\\geq\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{2\\theta^{2}}\\cdot\\sqrt{\\frac{\\mu}{\\mu+n}}\\geq\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{2\\sqrt{2}\\:\\theta^{2}}\\cdot\\sqrt{\\frac{\\mu}{n}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "almost surely. By the definition of $\\theta$ ,we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\operatorname*{inf}_{\\mathbf{\\Phi}}d_{\\infty}(\\pmb{u},\\pmb{u}^{\\star})\\geq\\operatorname*{lim}_{n\\to\\infty}\\frac{\\sigma^{2}n}{2\\sqrt{2}|\\lambda^{\\star}|^{2}}\\sqrt{\\frac{\\mu}{n}}=\\operatorname*{lim}_{n\\to\\infty}\\frac{\\sigma^{2}\\sqrt{n\\mu}}{2\\sqrt{2}|\\lambda^{\\star}|^{2}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "completing the proof. ", "page_idx": 13}, {"type": "text", "text": "B Proof of Lemma 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. The upper bound ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\{i:|v_{i}|\\geq\\alpha_{0}\\}|\\leq\\frac{1}{\\alpha_{0}^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "follows trivially from the fact that $\\|\\pmb{v}\\|_{2}=1$ ", "page_idx": 14}, {"type": "text", "text": "To prove the lower bound, we consider the disjoint intervals ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{I}_{\\ell}=\\left\\{\\begin{array}{l l}{\\left[\\log^{-1/2}n,1\\right]}&{\\mathrm{~if~}\\ell=0}\\\\ {\\left[\\log^{-(\\ell+1)/2}n,\\log^{-\\ell/2}n\\right)}&{\\mathrm{~if~}\\ell\\in\\left\\{1,2,\\ldots,\\lceil L\\rceil-2\\right\\}}\\\\ {\\left[\\log^{-L/2}n,\\log^{-(\\lceil L\\rceil-1)/2}n\\right)}&{\\mathrm{~if~}\\ell=\\lceil L\\rceil-1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall the definition of $L$ from Equation (11), ", "page_idx": 14}, {"type": "equation", "text": "$$\nL=\\frac{\\log(2n)}{\\log\\log n}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Rearranging terms, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n-\\frac L2\\log\\log n=-\\frac12\\log(2n).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Exponentiating both sides of the above display, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log^{-L/2}n={\\frac{1}{\\sqrt{2n}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Noting that by Equation (30) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\bigcup_{\\ell=0}^{[L]-1}\\mathcal{I}_{\\ell}=\\left[\\log^{-L/2}n,1\\right]=\\left[\\frac{1}{\\sqrt{2n}},1\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any $i\\in[n]$ such that $|v_{i}|\\notin\\cup_{\\ell=0}^{\\lceil L\\rceil-1}\\mathcal{S}_{\\ell}$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\vert v_{i}\\vert<\\log^{-L/2}n=\\frac{1}{\\sqrt{2n}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and therefore, summing over all $i\\in[n]$ such that $\\textstyle|v_{i}|\\not\\in\\bigcup_{\\ell=0}^{\\lceil L\\rceil-1}\\mathcal{S}_{\\ell}$ yields that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{\\substack{i:|v_{i}|\\notin\\bigcup_{\\ell=0}^{\\lceil L\\rceil-1}\\mathcal{S}_{\\ell}}}v_{i}^{2}<\\sum_{\\substack{i:|v_{i}|\\notin\\bigcup_{\\ell=0}^{\\lceil L\\rceil-1}\\mathcal{S}_{\\ell}}}\\frac{1}{2n}\\leq n\\cdot\\frac{1}{2n}=\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let $x_{\\ell}=|\\{i:|v_{i}|\\in\\mathcal{S}_{\\ell}\\}|$ for all $\\ell\\in\\{0,1,\\ldots,\\lceil L\\rceil-1\\}$ . We have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{i:|v_{i}|\\in\\mathcal{S}_{\\ell}}v_{i}^{2}\\leq x_{\\ell}\\operatorname*{max}_{i:|v_{i}|\\in\\mathcal{S}_{\\ell}}v_{i}^{2}\\leq\\frac{x_{\\ell}}{\\log^{\\ell}n},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality follows from the definition of $\\mathcal{I}_{\\ell}$ in Equation (30). Summing over all $\\ell\\in\\{0,1,\\ldots,\\lceil L\\rceil-1\\}$ onboth sides,we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{\\ell=0}^{[L]-1}\\frac{x_{\\ell}}{\\log^{\\ell}n}\\geq\\sum_{\\substack{i:|v_{i}|\\in\\bigcup_{\\ell=0}^{[L]-1}\\mathcal{S}_{\\ell}}}v_{i}^{2}=1-\\sum_{\\substack{i:|v_{i}|\\notin\\bigcup_{\\ell=0}^{[L]-1}\\mathcal{S}_{\\ell}}}v_{i}^{2}>\\frac{1}{2},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last inequality follows from Equation (31). The above further implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n[L]\\cdot\\operatorname*{max}_{\\ell\\in\\{0,1,\\cdots,\\lceil L\\rceil-1\\}}\\left\\{\\frac{x_{\\ell}}{\\log^{\\ell}n}\\right\\}\\geq\\sum_{\\ell=0}^{\\lceil L\\rceil-1}\\frac{x_{\\ell}}{\\log^{\\ell}n}>\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, rearranging, there exists an $\\ell_{0}\\in\\{0,1,\\cdots\\,,|L|-1\\}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{\\ell_{0}}>{\\frac{\\log^{\\ell_{0}}n}{2[L]}}>\\log^{\\ell_{0}-1}n={\\frac{\\log^{\\ell_{0}+1}n}{\\log^{2}n}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the second inequality holds for suitably large $n$ by the definition of $L$ given in Equation (11). ", "page_idx": 15}, {"type": "text", "text": "Finall, note that for all $i\\in[n]$ such that $|v_{i}|\\in\\mathcal{S}_{\\ell_{0}}$ , we have $|v_{i}|\\geq\\log^{-(\\ell_{0}+1)/2}n$ by the definition f $\\mathcal{I}_{\\ell_{0}}$ in Equation (30). Taking ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha_{0}=\\left\\{\\begin{array}{l l}{\\log^{-(\\ell_{0}+1)/2}n}&{\\mathrm{~if~}\\ell_{0}\\in\\{0,1,\\cdot\\cdot\\cdot,\\lceil L\\rceil-2\\}}\\\\ {\\log^{-L/2}n}&{\\mathrm{~if~}\\ell_{0}=\\lceil L\\rceil-1}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "yields that ", "page_idx": 15}, {"type": "equation", "text": "$$\n|\\{i:|v_{i}|\\geq\\alpha_{0}\\}|\\geq x_{\\ell_{0}}>\\frac{1}{\\alpha_{0}^{2}\\log^{2}n}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We complete the proof by noting that $\\alpha_{0}$ is in $\\boldsymbol{\\mathcal{A}}$ ,where $\\boldsymbol{\\mathcal{A}}$ is defined in Equation (10). ", "page_idx": 15}, {"type": "text", "text": "CProof of Lemma 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Expanding the right hand side of Equation (17), we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\sigma}^{2}=\\displaystyle\\frac{2}{n(n+1)}\\sum_{1\\leq i\\leq j\\leq n}\\left(M_{i j}^{\\star}-\\widehat{M}_{i j}+W_{i j}\\right)^{2}}\\\\ &{\\quad=\\displaystyle\\frac{2}{n(n+1)}\\sum_{1\\leq i\\leq j\\leq n}\\left(M_{i j}^{\\star}-\\widehat{M}_{i j}\\right)^{2}+\\displaystyle\\frac{2}{n(n+1)}\\sum_{1\\leq i\\leq j\\leq n}W_{i j}^{2}}\\\\ &{\\qquad\\quad\\displaystyle+\\,\\frac{4}{n(n+1)}\\sum_{1\\leq i<j\\leq n}W_{i j}\\left(M_{i j}^{\\star}-\\widehat{M}_{i j}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Subtracting $\\sigma^{2}$ on both sides and applying the triangle inequality, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\sigma}^{2}-\\sigma^{2}\\big|\\leq\\displaystyle\\frac{2}{n(n+1)}\\sum_{1\\leq i\\leq j\\leq n}\\left(M_{i j}^{\\star}-\\widehat{M}_{i j}\\right)^{2}+\\left|\\frac{2}{n(n+1)}\\sum_{1\\leq i\\leq j\\leq n}W_{i j}^{2}-\\sigma^{2}\\right|}\\\\ &{\\qquad\\qquad+\\left|\\frac{4}{n(n+1)}\\sum_{1\\leq i\\leq j\\leq n}W_{i j}\\left(M_{i j}^{\\star}-\\widehat{M}_{i j}\\right)\\right|}\\\\ &{\\qquad\\leq\\displaystyle\\frac{2}{n(n+1)}\\left\\|M^{\\star}-\\widehat{M}\\right\\|_{\\mathrm{F}}^{2}+\\left|\\frac{2}{n(n+1)}\\sum_{1\\leq i\\leq j\\leq n}W_{i j}^{2}-\\sigma^{2}\\right|}\\\\ &{\\qquad\\qquad+\\left|\\frac{2}{n(n+1)}\\operatorname{tr}W\\left(M^{\\star}-\\widehat{M}\\right)\\right|+\\displaystyle\\frac{2}{n(n+1)}\\left|\\frac{\\displaystyle N}{\\displaystyle\\sum_{i=1}^{n}}W_{i i}\\left(M_{i i}^{\\star}-\\widehat{M}_{i i}\\right)\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We proceed to bound each term in the last inequality of Equation (32) separately. ", "page_idx": 15}, {"type": "text", "text": "For the first term in Equation (32), by an argument analogous to that of Equation (3.15) in [27], ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{2}{n(n+1)}\\left\\|\\widehat{M}-M^{\\star}\\right\\|_{\\mathrm{F}}^{2}\\leq\\frac{8r}{n(n+1)}\\left\\|W\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By standard matrix concentration inequalities [53, 54], with probability at least $1-O(n^{-8})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\pmb{W}\\|\\leq5\\sqrt{\\nu_{W}n\\log n}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining the above two displays, we obtain an analogue of Equation (3.16) in [27], namely that with probability at least $1-\\bar{O}(n^{-8})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{2}{n(n+1)}\\left\\|\\widehat{M}-M^{\\star}\\right\\|_{\\mathrm{F}}^{2}\\leq\\frac{200\\nu_{W}r}{n+1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the second term in Equation (32), applying standard concentation inequalities for subexponential random variables (see, e.g., Chapter 2 in [54]), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{1\\leq i\\leq j\\leq n}\\left(W_{i j}^{2}-\\sigma^{2}\\right)\\right|\\geq t\\right]\\leq2\\exp\\left\\{-c^{2}\\operatorname*{min}\\left\\{\\frac{2t^{2}}{n(n+1)\\nu_{W}^{2}},\\frac{t}{\\nu_{W}}\\right\\}\\right\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $c>0$ is an absolute constant. Taking $t=2\\nu_{W}\\sqrt{n(n+1)\\log n}/c$ and dividing through by $n(n+1)/2$ , it holds with probability at least $1-O(n^{-8})$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\frac{2}{n(n+1)}\\sum_{1\\le i\\le j\\le n}W_{i j}^{2}-\\sigma^{2}\\right|\\le\\frac{4\\nu_{W}\\sqrt{\\log n}}{c n}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the third term in Equation (32), using the matrix Holder inequality (see Corollary IV.2.6 in [15]), wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\operatorname{tr}W\\left(M^{\\star}-\\widehat{M}\\right)\\right|\\leq\\|W\\|\\|M^{\\star}-\\widehat{M}\\|_{*},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\|\\cdot\\|_{*}$ denotes the nuclear norm, defined to be the sum of the singular values. Using the fact that $M^{\\star}-\\widehat{M}$ has at most rank $2r$ ,wehave ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\operatorname{tr}W\\left(M^{\\star}-{\\widehat{M}}\\right)\\right|\\leq2r\\|W\\|\\|M^{\\star}-{\\widehat{M}}\\|.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying Equation (33) and following an argument analogous to that in Equations (3.12) and (3.15) in [27], we have that with probability at least $1-O(n^{-8})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\pmb{W}\\|\\leq5\\sqrt{\\nu_{W}n}\\enspace\\mathrm{and}\\enspace\\|\\pmb{M}^{\\star}-\\widehat{\\pmb{M}}\\|\\leq10\\sqrt{\\nu_{W}n}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It follows that with probability at least $1-O(n^{-8})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{2}{n(n+1)}\\left|\\mathrm{tr}\\,W\\left(M^{\\star}-\\widehat{M}\\right)\\right|\\leq\\frac{200\\nu_{W}r}{n+1}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For the fourth term in Equation (32), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n}W_{i i}\\left(M_{i i}^{\\star}-\\widehat{M}_{i i}\\right)\\Bigg|\\le\\operatorname*{max}_{i\\in[n]}\\left\\{|W_{i i}|\\right\\}\\|\\widehat{M}-M^{\\star}\\|_{*}.}\\\\ &{\\displaystyle\\frac2{n(n+1)}\\left|\\sum_{i=1}^{n}W_{i i}\\left(M_{i i}^{\\star}-\\widehat{M}_{i i}\\right)\\right|\\le\\frac{200\\nu_{W}r\\sqrt{\\log n}}{n^{3/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Applying Equations (34), (35), (36) and (37) to Equation (32), with probability at least $1-O(n^{-8})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\widehat{\\sigma}^{2}-\\sigma^{2}|\\leq\\frac{400\\nu_{W}r}{n}+\\frac{4\\nu_{W}\\sqrt{\\log n}}{c n}+\\frac{200\\nu_{W}r\\sqrt{\\log n}}{n^{3/2}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as we set out to show. ", "page_idx": 16}, {"type": "text", "text": "D Proof of Theorem 1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Without loss of generality, we derive our results under the assumption that ", "page_idx": 16}, {"type": "equation", "text": "$$\nd_{\\infty}\\left(\\pmb{u},\\pmb{u}^{\\star}\\right)=\\|\\pmb{u}-\\pmb{u}^{\\star}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Otherwise, we can replace $\\textbf{\\em u}$ With $-\\pmb{u}$ and obtain the same results. We remind the reader that we use $c$ and $C$ to denote constants with respect to $n$ , whose precise value might change from line to line. To prove Theorem 1, we first state and prove a few technical lemmas. ", "page_idx": 16}, {"type": "text", "text": "D.1 Technical lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 5. If $|\\lambda^{\\star}|\\geq80\\sqrt{\\nu_{W}n}$ then under Assumption $^{\\,l}$ it holds with probability at least $1\\!-\\!O(n^{-8})$ that for all $\\ell\\in[n]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\left\\{\\left|u_{\\ell}-u_{\\ell}^{\\star}\\right|,|u_{\\ell}+u_{\\ell}^{\\star}|\\right\\}\\leq\\frac{80\\sqrt{\\nu_{W}\\log n}+120\\sqrt{\\nu_{W}n}|u_{\\ell}^{\\star}|}{\\left|\\lambda^{\\star}\\right|}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We largely follow the proof of Theorem 4.1 in [27], albeit with a slightly more careful analysis. In particular, we note that the proof of Theorem 4.1 in [27] is written for the case where $W$ has Gaussian entries. It is straightforward to extend this argument to subgaussian entries by applying standard subgaussian concentration inequalities and truncation arguments [54, 56], in essence replacing all appearances of $\\sigma^{2}$ in their results with the subgaussian parameter (i.e., \u201c\"variance proxy'\") $\\nu_{W}$ . Here, we only sketch out a few key points and refer the reader to Section 4.1.4 of [27] for a full argument. For each $\\ell\\in[n]$ , we construct a leave-one-out copy ${\\pmb Y}^{(\\ell)}$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{\\chi}^{(\\ell)}=\\lambda^{\\star}\\mathbf{u}^{\\star}\\mathbf{u}^{\\star\\top}+\\mathbf{W}^{(\\ell)},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{i j}^{(\\ell)}=\\left\\{{W_{i j},\\quad\\mathrm{if}\\,\\ell\\notin\\{i,j\\}}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For each $\\ell\\in[n]$ ,let $\\lambda^{(\\ell)}$ and $\\pmb{u}^{(\\ell)}$ denote, respectively, the largest-magnitude eigenvalue of $Y^{(\\ell)}$ and its corresponding eigenvector. Without loss of generality, flipping signs if necessary, we assume that ", "page_idx": 17}, {"type": "equation", "text": "$$\nd_{\\infty}\\left(\\pmb{u},\\pmb{u}^{\\star}\\right)=\\|\\pmb{u}-\\pmb{u}^{\\star}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Equation (4.20) in [27], with probability at least $1-O\\left(n^{-8}\\right)$ wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|u_{\\ell}^{(\\ell)}-u_{\\ell}^{\\star}\\right|\\le\\frac{20\\sqrt{\\nu_{W}n}}{|\\lambda^{\\star}|}|u_{\\ell}^{\\star}|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Equation (4.15) in [27], we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{(\\ell)}\\right\\|_{2}\\leq\\frac{4\\left\\|\\left(\\boldsymbol{Y}-\\boldsymbol{Y}^{(\\ell)}\\right)\\boldsymbol{u}^{(\\ell)}\\right\\|_{2}}{\\left|\\lambda^{\\star}\\right|}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Following the argument just after Equation (4.18) in [27], replacing the Gaussian concentration inequalities with subgaussian concentration inequalities, one can show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|\\left(\\boldsymbol{Y}-\\boldsymbol{Y}^{(\\ell)}\\right)\\boldsymbol{u}^{(\\ell)}\\right\\|_{2}\\leq5\\sqrt{\\nu_{W}\\log n}+5\\sqrt{\\nu_{W}n}\\left|u_{\\ell}\\right|+5\\sqrt{\\nu_{W}n}\\left\\|\\boldsymbol{u}-\\boldsymbol{u}^{(\\ell)}\\right\\|_{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $1-O\\left(n^{-8}\\right)$ . Combining the above two displays and rearranging slightly. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(1-\\frac{5\\sqrt{\\nu_{W}n}}{|\\lambda^{\\star}|}\\right)\\|\\pmb{u}-\\pmb{u}^{(\\ell)}\\|_{2}\\leq\\frac{5\\sqrt{\\nu_{W}\\log n}+5\\sqrt{\\nu_{W}n}\\,|u_{\\ell}|}{|\\lambda^{\\star}|}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\lambda^{\\star}\\geq40\\sqrt{\\nu_{W}n}$ by assumption, it follows that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\pmb{u}-\\pmb{u}^{(\\ell)}\\|_{2}\\leq\\frac{40\\sqrt{\\nu_{W}\\log n}+40\\sqrt{\\nu_{W}n}\\,|u_{\\ell}|}{|\\lambda^{\\star}|}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining the triangle inequality with the trivial upper bound $\\|\\boldsymbol{u}-\\boldsymbol{u}^{(\\ell)}\\|_{\\infty}\\leq\\|\\boldsymbol{u}-\\boldsymbol{u}^{(\\ell)}\\|_{2}$ we have for any $\\ell\\in[n]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|{u_{\\ell}-u_{\\ell}^{\\star}}\\right|\\le\\left|{u_{\\ell}^{\\star}-u_{\\ell}^{(\\ell)}}\\right|+\\left\\|{\\pmb u}-{\\pmb u}^{(\\ell)}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying Equations (39) and (40), it holds with probability at least $1-O(n^{-8})$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|u_{\\ell}-u_{\\ell}^{\\star}|\\leq\\frac{20\\sqrt{\\nu_{W}n}}{|\\lambda^{\\star}|}\\,|u_{\\ell}^{\\star}|+\\frac{40\\sqrt{\\nu_{W}\\log n}+40\\sqrt{\\nu_{W}n}\\,|u_{\\ell}|}{|\\lambda^{\\star}|}}\\\\ &{\\qquad\\qquad\\leq\\frac{20\\sqrt{\\nu_{W}n}}{|\\lambda^{\\star}|}|u_{\\ell}^{\\star}|+\\frac{40\\sqrt{\\nu_{W}\\log n}+40\\sqrt{\\nu_{W}n}|u_{\\ell}^{\\star}|+40\\sqrt{\\nu_{W}n}\\,|u_{\\ell}-u_{\\ell}^{\\star}|}{|\\lambda^{\\star}|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the second inequality follows from the triangle inequality. After rearranging terms and using the fact that $|\\lambda^{\\star}|\\geq80\\sqrt{\\nu_{W}n}$ , we obtain the desired bound ", "page_idx": 18}, {"type": "equation", "text": "$$\n|u_{\\ell}-u_{\\ell}^{\\star}|\\leq\\frac{80\\sqrt{\\nu_{W}\\log n}+120\\sqrt{\\nu_{W}n}|u_{\\ell}^{\\star}|}{|\\lambda^{\\star}|},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $\\ell\\in[n]$ , completing the proof. ", "page_idx": 18}, {"type": "text", "text": "Lemma 6, which controls the behavior of the index set $\\hat{I}$ in Algorithm 1, follows from an application of Lemma 5. ", "page_idx": 18}, {"type": "text", "text": "Lemma 6. Under the model in Equation (1) with $M^{\\star}=\\lambda^{\\star}u^{\\star}u^{\\star\\top}$ ,suppose that Assumption $^{\\,l}$ holds and Assumption 3 holds with constant $C_{1}>2400/\\epsilon_{0}$ ,where $\\epsilon_{0}\\in(0,1)$ is fixed. For $\\alpha\\in{\\mathcal{A}}$ define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{I}_{\\alpha}=\\{i:|u_{i}|\\geq\\alpha\\}\\,,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where u is the leading eigenvector of $\\mathbf{\\deltaY}$ , as in Step 1 of Algorithm $^{\\,l}$ . For all suffciently large $n$ the following all hold with probability at least $1-{\\bar{O(n^{-8}\\log{n})}}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\{i:|u_{i}^{\\star}|\\geq(1+\\epsilon_{0}/2)\\alpha\\}\\subseteq\\hat{I}_{\\alpha}\\subseteq\\{i:|u_{i}^{\\star}|\\geq(1-\\epsilon_{0}/2)\\alpha\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{i:(1-\\epsilon_{0}/2)\\alpha<|u_{i}|<(1+\\epsilon_{0}/2)\\alpha\\right\\}\\subseteq\\left\\{i:(1-\\epsilon_{0})\\alpha<|u_{i}^{\\star}|<(1+\\epsilon_{0})\\alpha\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\{i:|u_{i}^{\\star}|\\geq\\alpha\\}\\subseteq\\{i:|u_{i}|>(1-\\epsilon_{0}/2)\\alpha\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\{i:|u_{i}|\\geq(1+\\epsilon_{0}/2)\\alpha\\}\\subseteq\\left\\{i:|u_{i}^{\\star}|\\geq\\alpha\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Fix $\\alpha\\in{\\mathcal{A}}$ . For any $i\\in\\hat{I}_{\\alpha}$ , we have $|u_{i}|\\geq\\alpha$ by definition. We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\bigcup_{i\\in\\hat{I}_{\\alpha}}\\left\\{|u_{i}^{\\star}|<\\alpha-\\frac{80\\sqrt{\\nu_{W}\\log n}+120\\sqrt{\\nu_{W}n}|u_{i}^{\\star}|}{|\\lambda^{\\star}|}\\right\\}\\right)}\\\\ &{\\qquad\\le\\mathbb{P}\\left(\\bigcup_{i\\in\\hat{I}_{\\alpha}}\\left\\{|u_{i}^{\\star}|<|u_{i}|-\\frac{80\\sqrt{\\nu_{W}\\log n}+120\\sqrt{\\nu_{W}n}|u_{i}^{\\star}|}{|\\lambda^{\\star}|}\\right\\}\\right)}\\\\ &{\\qquad\\le\\mathbb{P}\\left(\\bigcup_{i\\in[n]}\\left\\{|u_{i}^{\\star}|<|u_{i}|-\\frac{80\\sqrt{\\nu_{W}\\log n}+120\\sqrt{\\nu_{W}n}|u_{i}^{\\star}|}{|\\lambda^{\\star}|}\\right\\}\\right)\\le O(n^{-8}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality follows from the fact that $|u_{i}|\\,\\geq\\,\\alpha$ for $i\\,\\in\\,\\hat{I}_{\\alpha}$ , the second inequality follows from set inclusion, and the last inequality foliows from combining the triangle inequality with Lemma 5. If ", "page_idx": 18}, {"type": "equation", "text": "$$\n|u_{i}^{\\star}|\\geq\\alpha-\\frac{80\\sqrt{\\nu_{W}\\log n}+120\\sqrt{\\nu_{W}n}|u_{i}^{\\star}|}{|\\lambda^{\\star}|}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and we take $C_{1}>2400/\\epsilon_{0}$ in Assumption 3, then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|u_{i}^{\\star}|\\geq\\alpha-\\frac{80\\epsilon_{0}\\sqrt{\\nu_{W}\\log n}}{2400\\sqrt{\\nu_{W}n\\log n}}-\\frac{120\\epsilon_{0}\\sqrt{\\nu_{W}n}}{2400\\sqrt{\\nu_{W}n\\log n}}|u_{i}^{\\star}|\\geq\\alpha-\\frac{\\epsilon_{0}}{30}\\sqrt{\\frac{1}{n}}-\\frac{\\epsilon_{0}}{20\\sqrt{\\log n}}|u_{i}^{\\star}|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\alpha\\geq\\sqrt{1/2n}$ , after rearranging terms, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|u_{i}^{\\star}|\\geq\\left(1+\\frac{\\epsilon_{0}}{20\\sqrt{\\log n}}\\right)^{-1}\\left(\\alpha-\\frac{\\epsilon_{0}}{30}\\sqrt{\\frac{1}{n}}\\right)\\geq\\left(1+\\frac{\\epsilon_{0}}{20\\sqrt{\\log n}}\\right)^{-1}\\left(1-\\frac{\\epsilon_{0}}{15}\\right)\\alpha}\\\\ {\\displaystyle\\qquad>(1-\\epsilon_{0}/2)\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality holds for all $i\\in[n]$ satisfying Equation (46) (i.e., all $i\\in\\hat{I}_{\\alpha},$ when $n$ is sufficiently large. Thus, we conclude that, applying a union bound followed by Equation (45), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\hat{I}_{\\alpha}\\subseteq\\{i:|u_{i}^{\\star}|\\geq(1\\!-\\!\\epsilon_{0}/2)\\alpha\\}\\right)=\\mathbb{P}\\left(\\bigcap_{i\\in\\hat{I}_{\\alpha}}\\{|u_{i}^{\\star}|\\geq(1-\\epsilon_{0}/2)\\alpha\\}\\right)}\\\\ &{\\qquad\\qquad\\geq1\\!-\\!\\mathbb{P}\\left(\\bigcup_{i\\in\\hat{I}_{\\alpha}}\\Bigl\\{|u_{i}^{\\star}|\\!<\\!\\alpha\\!-\\!\\frac{80\\sqrt{\\nu_{W}\\log n}+120\\sqrt{\\nu_{W}n}|u_{i}^{\\star}|}{|\\lambda^{\\star}|}\\Bigr\\}\\right)}\\\\ &{\\qquad\\qquad\\geq1-O(n^{-8}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following a similar argument, we can show that Equation (44) holds with probability at least $1-O(n^{-8})$ , and that, also with probability at least $1-O(n^{-8})$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{i:(1-\\epsilon_{0}/2)\\,\\alpha<|u_{i}|\\right\\}\\subseteq\\left\\{i:(1-\\epsilon_{0})\\alpha<|u_{i}^{\\star}|\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "On the other hand, combining the triangle inequality with the bound in Equation (38) in Lemma 5, it holds with probability at least $1-O(\\bar{n}^{-8})$ that for all $i\\in[n]$ satisfying $|u_{i}^{\\star}|\\geq(1+\\epsilon_{0}/2)\\alpha$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n|u_{i}|\\geq|u_{i}^{\\star}|-|u_{i}-u_{i}^{\\star}|\\geq\\left(1+\\frac{\\epsilon_{0}}{2}\\right)\\alpha-\\frac{\\epsilon_{0}}{30\\sqrt{n}}-\\frac{\\epsilon_{0}(1+\\epsilon_{0}/2)\\alpha}{20\\sqrt{\\log n}}>\\alpha,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality holds when $n$ is sufficiently large. It follows that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\hat{I}_{\\alpha}\\supseteq\\{i:|u_{i}^{\\star}|\\geq(1+\\epsilon_{0}/2)\\alpha\\}\\right)=\\mathbb{P}\\left(\\bigcap_{\\substack{i\\in\\{i:|u_{i}^{\\star}|\\geq(1+\\epsilon_{0}/2)\\alpha\\}}}\\{|u_{i}|\\geq\\alpha\\}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq1-O(n^{-8}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following the same argument, we have that Equation (43) holds with probability at least $1-O(n^{-8})$ and that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\{i:|u_{i}^{\\star}|\\geq(1+\\epsilon_{0})\\alpha\\}\\subseteq\\{i:|u_{i}|\\geq(1+\\epsilon_{0}/2)\\alpha\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "also with probability at least $1-O(n^{-8})$ ", "page_idx": 19}, {"type": "text", "text": "Combining Equation (48) and Equation (50) yields that Equation (41) holds with probability at least $1-O(n^{-{\\frac{\\rceil}{8}}})$ . Similarly, combining Equations (49) and (51) implies that Equation (42) holds with probability at least $1-{\\cal O}(n^{-8})$ . A union bound over all $\\alpha\\in A$ yields that Equations (41), (42) (43) and (44) hold simultaneously for all $\\alpha\\in{\\mathcal{A}}$ with probability at least $1-O(|\\bar{A}|n^{-8})$ . Recalling from Equation (10) that $|A|=O{\\dot{(}}\\log n)$ completes the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "The results in Lemma 6 lead to Lemma 7. ", "page_idx": 19}, {"type": "text", "text": "Lemma 7. Consider the model in Equation (1) with $M^{\\star}=\\lambda^{\\star}\\boldsymbol{u}^{\\star}\\boldsymbol{u}^{\\star\\top}$ and suppose that Assumptions 1,2 and 3 hold.For each $\\alpha\\in{\\mathcal{A}}$ let ", "page_idx": 19}, {"type": "equation", "text": "$$\nI_{\\alpha}:=\\{i:|u_{i}^{\\star}|\\geq\\alpha\\}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and define the set ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{A}_{0}=\\{\\alpha\\in\\mathcal{A}:|I_{\\alpha}|\\ge(\\alpha^{2}\\log^{2}n)^{-1}\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\boldsymbol{\\mathcal{A}}$ is as defined in Equation (10). Let $\\hat{I}$ be the set constructed in Step 2 of Algorithm $^{\\,l}$ with $\\beta=\\epsilon_{0}/2$ .With probability at least $1-O(n^{-8}\\log n)$ there exists an $\\alpha\\in\\mathcal{A}_{0}$ such that $\\hat{I}=I_{\\alpha}$ ", "page_idx": 19}, {"type": "text", "text": "Proof. We first show that with probability at least $1-O(n^{-8}\\log n)$ , for any $\\alpha\\,\\in\\,A$ satisfying Equations (15) and (16), we have $\\alpha\\in\\mathcal{A}_{0}$ and $\\hat{I}_{\\alpha}=I_{\\alpha}$ , where $\\hat{I}_{\\alpha}$ is as defined in Lemma 6. We then prove that there exists at least one $\\alpha\\in{\\mathcal{A}}_{0}\\subseteq{\\mathcal{A}}$ such that both Equations (15) and (16) hold. Having established this, the value $\\widehat{\\alpha}\\in A$ chosen in Step 2 of Algorithm 1, which satisfies Equations (15) and (16) by definition, must be an element of ${\\mathcal{A}}_{0}\\subseteq A$ Since $\\hat{I}=\\hat{I}_{\\widehat{\\alpha}}$ by definition, we must have $\\hat{I}=I_{\\widehat{\\alpha}}$ with probability at least $1-O(n^{-8}\\log n)$ which will complete the proof. ", "page_idx": 19}, {"type": "text", "text": "Suppose that $\\alpha\\in{\\mathcal{A}}$ satisfies Equations (15) and (16). Trivially, by the definition of $\\mathcal{A}_{\\mathrm{0}}$ in Equation (53), Equation (15) implies that $\\alpha\\in\\mathcal{A}_{0}$ . By Lemma 6, with probability at least $1\\!-\\!O(n^{-8}\\log n)$ Equations (43) and (44) hold for all elements of $\\boldsymbol{\\mathcal{A}}$ . Thus, in particular, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\{i:\\left|u_{i}\\right|\\geq(1+\\epsilon_{0}/2)\\widehat{\\alpha}\\right\\}\\subseteq\\left\\{i:\\left|u_{i}^{\\star}\\right|\\geq\\widehat{\\alpha}\\right\\}\\subseteq\\left\\{i:\\left|u_{i}\\right|>(1-\\epsilon_{0}/2)\\widehat{\\alpha}\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Equation (16) and the choice $\\beta=\\epsilon_{0}/2$ in Algorithm 1, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{I}_{\\alpha}=\\{i:|u_{i}|>(1-\\epsilon_{0}/2)\\alpha\\}=\\{i:|u_{i}|\\geq(1+\\epsilon_{0}/2)\\alpha\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "from which it follows that $\\hat{I}_{\\alpha}=I_{\\alpha}$ with probability at least $1-O(n^{-8}\\log n)$ ", "page_idx": 20}, {"type": "text", "text": "We now establish the existence of at least one $\\alpha\\in{\\mathcal{A}}_{0}\\subseteq{\\mathcal{A}}$ satisfying Equations (15) and (16). By Assumption 2, for sufficiently large $n$ , there exists $\\alpha\\in A$ such that Equation (12) holds, from which $\\alpha\\in\\mathcal{A}_{0}$ immediately. Also by Assumption 2, there are no $i\\in[n]$ for which ", "page_idx": 20}, {"type": "equation", "text": "$$\n(1-\\epsilon_{0})\\alpha\\leq|u_{i}^{\\star}|\\leq(1+\\epsilon_{0})\\alpha.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Lemma 6, Equation (41) holds with probability at least $1-O(n^{-8}\\log n)$ for all elements of ${\\mathcal{A}}_{0}\\subseteq A$ , and thus $\\hat{I}_{\\alpha}\\,=\\,I_{\\alpha}$ . Therefore, we have $|\\hat{I}_{\\alpha}|\\,=\\,|I_{\\alpha}|\\,\\geq\\,(\\alpha^{2}\\log^{2}n)^{-1}$ , where the lowerbound follows from Equation (12). As a result, any such $\\alpha$ guaranteed by Assumption 2 satisfies Equation (15). ", "page_idx": 20}, {"type": "text", "text": "By Equation (42) in Lemma 6 and the fact that $\\alpha$ satisfies Assumption 2, with probability at least $1-{\\dot{O(n^{-8}\\log n)}}$ wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\{i:(1-\\epsilon_{0}/2)\\alpha<|u_{i}|<(1+\\epsilon_{0}/2)\\alpha\\}=\\emptyset.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, with $\\beta\\;=\\;\\epsilon_{0}/2$ , this particular $\\alpha$ also satisfies Equation (16) and we conclude that with probability at least $1^{\\'}-O(n^{-\\bar{8}}\\log n)$ , there exists at least one $\\alpha\\in\\mathcal{A}_{0}$ such that both Equations (15) and (16) hold. ", "page_idx": 20}, {"type": "text", "text": "Our argument above ensures that with probability at least $1-O(n^{-8}\\log n)$ , there exists $\\alpha\\in A_{0}$ satisfying Equations (15) and (16). Thus, $\\widehat{\\alpha}\\in A_{0}$ as constructed in Step 2 exists, and our argument above ensures that $\\hat{I}$ as constructed in Step 2 satisfies $\\hat{I}=\\hat{I}_{\\widehat{\\alpha}}=I_{\\widehat{\\alpha}}$ , completing the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Lemma 8. Under Assumptions $^{\\,I}$ and 3, with probability at least $1-O(n^{-8})$ it holds for all $i\\in[n]$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n|u_{i}^{\\star}|\\geq\\frac{1}{24\\sqrt{n}}\\ \\ \\,i m p l i e s\\ \\ \\ \\mathrm{sgn}\\left(u_{i}\\right)=\\mathrm{sgn}\\left(u_{i}^{\\star}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma 8.Let $i\\in[n]$ and suppose that $|u_{i}^{\\star}|\\geq1/24\\sqrt{n}$ with $u_{i}^{\\star}\\geq0$ . The case for $u_{i}^{\\star}<0$ follows by an analogous argument and details are omitted. By Equation (38) in Lemma 5, it holds with probability at least $1-\\bar{O}(n^{-8})$ that for all $i\\in[n]$ such that $u_{i}^{\\star}\\geq0$ \uff0c ", "page_idx": 20}, {"type": "equation", "text": "$$\nu_{i}\\geq u_{i}^{\\star}-|u_{i}-u_{i}^{\\star}|\\geq\\left(1-\\frac{120\\sqrt{\\nu_{W}n}}{|\\lambda^{\\star}|}\\right)u_{i}^{\\star}-\\frac{80\\sqrt{\\nu_{W}\\log n}}{|\\lambda^{\\star}|}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Assumption 3, it follows that when $|\\lambda^{\\star}|\\geq2400\\sqrt{\\nu_{W}n}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nu_{i}\\geq\\frac{19}{20}u_{i}^{\\star}-\\frac{80\\sqrt{\\nu_{W}\\log n}}{\\left|\\lambda^{\\star}\\right|}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $u_{i}^{\\star}\\geq1/24{\\sqrt{n}}$ by assumption, it follows that when $|\\lambda^{\\star}|\\geq2400\\sqrt{\\nu_{W}n}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nu_{i}^{\\star}\\geq\\frac{100\\sqrt{\\nu_{W}\\log n}}{|\\lambda^{\\star}|},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "from which we have $u_{i}>0$ and therefore $\\mathrm{sgn}\\left(u_{i}\\right)=\\mathrm{sgn}\\left(u_{i}^{\\star}\\right)$ ", "page_idx": 20}, {"type": "text", "text": "Lemma 9. Fix $\\alpha\\;\\in\\;{\\mathcal{A}}$ and $\\pmb{s}~\\in~\\{\\pmb{\\pm}1\\}^{n}$ .For a random noise matrix $W\\,\\in\\,\\mathbb{R}^{n\\times n}$ satisfying Assumption $^{\\,l}$ ,it holds with probability at least $1-O(n^{-8})$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\sum_{j,k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}\\right|\\leq C|I_{\\alpha}|\\sqrt{\\nu_{W}\\log n}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[n]}\\left\\vert\\sum_{k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}\\right\\vert\\leq C\\sqrt{|I_{\\alpha}|\\nu_{W}\\log n}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Since $s\\in\\{\\pm1\\}^{n}$ is fixed and the entries of $W$ are symmetric about zero, we assume without loss of generality that $\\pmb{s}$ is a vector of all 1's. By standard concentration inequalities [54], we have that for any $t>0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lvert\\sum_{j,k\\in I_{\\alpha}}W_{j k}\\right\\rvert\\geq t\\right)\\leq2\\exp\\left(\\frac{-t^{2}}{2\\lvert I_{\\alpha}\\rvert^{2}\\nu_{W}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Setting $t=C|I_{\\alpha}|\\sqrt{\\nu_{W}\\log n}$ for suitably-chosen $C>0$ , Equation (54) holds with probability at least $1-O(n^{{\\dot{-}}8})$ : Similarly, by standard subgaussian concentration inequalities [54] and a union bound over $j\\in[n]$ , it holds for all $t>0$ that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{max}_{j\\in[n]}\\left\\lvert\\sum_{k\\in I_{\\alpha}}W_{j k}\\right\\rvert>t\\right)\\leq2n\\exp\\left(\\frac{-t^{2}}{2|I_{\\alpha}|\\nu_{W}}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking $t^{2}=C|I_{\\alpha}|\\nu_{W}\\log n$ for $C>0$ chosen suitably large, Equation (55) holds with probability at least $1-O(n^{{\\frac{1}{-8}}})$ , completing the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D.2  Proof of Theorem 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. In this proof, we control the estimation error of $\\widehat{\\pmb{u}}$ under $d_{\\infty}$ . There are two types of entries of $\\widehat{\\pmb{u}}$ , as stated in Step 5 of Algorithm 1. One type is derived from $\\widehat{\\pmb{v}}$ constructed in Step 4 of Algorithm 1, and corresponds to large entries of $\\pmb{u}^{\\star}$ . The other type is given by the top eigenvector $\\textbf{\\em u}$ of $\\mathbf{\\deltaY}$ , corresponding to small entries of $\\pmb{u}^{\\star}$ . We handle these two types of entries separately. In Part I of the proof, we control the estimation error corresponding to the first type of entries, derived from $\\widehat{\\pmb{v}}$ . In Part $\\mathbf{II}$ of the proof, we control the estimation error corresponding to the second type of entries, derived from $\\textbf{\\em u}$ . Combining these two parts, we obtain a high probability bound for the estimation error $d_{\\infty}(\\widehat{\\boldsymbol{u}},\\boldsymbol{u}^{\\star})$ ", "page_idx": 21}, {"type": "text", "text": "Part I. Estimation error related to $\\widehat{\\pmb{v}}$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We define the event ${\\mathcal{E}}_{1}$ according to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{E}_{1}=\\{|\\widehat{\\lambda}-\\lambda^{\\star}|\\leq C_{2}\\sqrt{\\nu_{W}}\\log^{5/2}n\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On event ${\\mathcal{E}}_{1}$ ,if $\\lambda^{\\star}>0$ , then by the triangle inequality, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\lambda}\\geq\\lambda^{\\star}-\\left|\\lambda^{\\star}-\\widehat{\\lambda}\\right|\\geq\\lambda^{\\star}-C_{2}\\sqrt{\\nu_{W}}\\log^{5/2}n>0,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second inequality holds for all sufficiently large $n$ by Assumption 3. Similarly, for all sufficiently large $n$ wehave $\\widehat\\lambda<0$ $\\lambda^{\\star}<0$ In other words, for all sufficiently large $n$ ,we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{sgn}\\left({\\widehat{\\lambda}}\\right)=\\operatorname{sgn}\\left(\\lambda^{\\star}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "on event ${\\mathcal{E}}_{1}$ . Hence, Step 1 in Algorithm 1 guarantees that we work with the original $\\mathbf{Y}$ when $\\lambda^{\\star}>0$ or with $-\\mathbf{Y}$ when $\\lambda^{\\star}<0$ . In either case, the algorithm proceeds with a signal matrix that has a positive leading eigenvalue on ${\\mathcal{E}}_{1}$ . Without loss of generality, we assume that $\\lambda^{\\star}>0$ ", "page_idx": 21}, {"type": "text", "text": "On event ${\\mathcal{E}}_{1}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}-1\\right|\\leq\\frac{C_{2}\\sqrt{\\nu_{W}}(\\log n)^{5/2}}{\\lambda^{\\star}-|\\lambda^{\\star}-\\widehat{\\lambda}|}\\leq\\frac{C\\sqrt{\\nu_{W}}(\\log n)^{5/2}}{\\lambda^{\\star}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second inequality holds for all sufficiently large $n$ by Assumption 3. Similarly, for $n$ sufficiently large, following Equation (58), event ${\\mathcal{E}}_{1}$ also implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n1/2\\leq|\\lambda^{\\star}/\\widehat\\lambda|\\leq2.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recalling the set $\\mathcal{A}_{\\mathrm{0}}$ from Equation (53) and $I_{\\alpha}$ from Equation (52) above, for each $\\alpha\\in\\mathcal{A}_{0}$ , define the events $\\mathcal{E}_{2,\\alpha}$ and $\\mathcal{E}_{3,\\alpha}$ according to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{E}_{2,\\alpha}:=\\{\\hat{I}=I_{\\alpha}\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\hat{I}$ is as constructed in Step 2 of Algorithm 1, and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{E}_{3,\\alpha}:=\\{Q_{k k}=\\mathrm{sgn}\\left(u_{k}^{\\star}\\right)\\mathrm{~for~all~}k\\in I_{\\alpha},\\mathrm{~and~}Q_{k k}=1\\mathrm{~for~all~}k\\in I_{\\alpha}^{c}\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Define $s\\in\\{\\pm1\\}^{n}$ according to ", "page_idx": 22}, {"type": "equation", "text": "$$\ns_{k}:={\\binom{\\mathrm{sgn}\\left(u_{k}^{\\star}\\right)}{1}}\\quad\\mathrm{if~}k\\in I_{\\alpha},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and for each $\\alpha\\in\\mathcal{A}_{0}$ , define the event $\\mathcal{E}_{4,\\alpha}$ as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{E}_{4,\\alpha}=\\left\\{\\left|\\sum_{j,k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}\\right|\\leq C|I_{\\alpha}|\\sqrt{\\nu_{W}\\log n}\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For $\\widehat S$ given in Step 4 of Algorithm 1, plugging in $\\widetilde{Y}=Q Y Q$ and expanding, it fllows that on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}$ \uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{S}^{2}=\\sum_{j,k\\in\\hat{I}}Q_{j j}Q_{k k}Y_{j k}=\\lambda^{\\star}\\left(\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right)^{2}+\\sum_{j,k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "On the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ we can lower-bound this right-hand side, obtaining ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{S}^{2}\\geq\\lambda^{\\star}\\left(\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right)^{2}-C|I_{\\alpha}|\\sqrt{\\nu_{W}\\log n}\\geq\\lambda^{\\star}|I_{\\alpha}|^{2}\\alpha^{2}-C|I_{\\alpha}|\\sqrt{\\nu_{W}\\log n},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality follows from the fact that event $\\mathcal{E}_{4,\\alpha}$ holds and the second inequality follows from the definition of $I_{\\alpha}$ in Equation (52). Using Equation (53) and the fact that $\\alpha\\in A_{0}$ ,wemay further bound this by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{S}^{2}\\geq\\frac{\\lvert I_{\\alpha}\\rvert}{\\log^{2}n}\\left(\\lambda^{\\star}-C\\sqrt{\\nu_{W}}\\log^{5/2}n\\right)>0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality holds under Assumption 3 when $n$ is sufficiently large. Thus, on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ we have $\\sum_{j,k\\in\\hat{I}}\\widetilde{Y}_{j k}>0$ and thus $\\widehat S$ is well-defined (i.e., it is the square root of a positive number). ", "page_idx": 22}, {"type": "text", "text": "We now show that $\\widehat S$ is close to its population target. Rearanging the terms in Equation (64), we have that on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\widehat{S}^{2}-\\lambda^{\\star}\\left(\\sum_{k\\in I_{\\alpha}}\\vert u_{k}^{\\star}\\vert\\right)^{2}\\right|\\leq C|I_{\\alpha}|\\sqrt{\\nu_{W}\\log n}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Dividing both sides by $\\begin{array}{r}{\\widehat{S}+\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|}\\end{array}$ , when the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ holds, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\widehat{S}-\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right|\\leq\\frac{C|I_{\\alpha}|\\sqrt{\\nu_{W}\\log n}}{\\left(\\widehat{S}+\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right)}\\leq\\frac{C|I_{\\alpha}|\\sqrt{\\nu_{W}\\log n}}{\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|}\\leq\\frac{C\\sqrt{\\nu_{W}\\log n}}{\\sqrt{\\lambda^{\\star}}\\alpha},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality follows from the fact that $\\widehat S>0$ and the last inequality follows from the defnition of $I_{\\alpha}$ in Equation (52). Dividing by $\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\mathfrak{g}}}\\left|u_{k}^{\\star}\\right|$ on both ides note that this quanty is positive by definition of $I_{\\alpha}$ and the fact that $\\alpha\\in\\mathcal{A}_{0}]$ , it follows that on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\frac{\\widehat{S}}{\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|}-1\\right|\\leq\\frac{C\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}\\alpha\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|}\\leq\\frac{C\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}|I_{\\alpha}|\\alpha^{2}}\\leq\\frac{C\\sqrt{\\nu_{W}}(\\log n)^{5/2}}{\\lambda^{\\star}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality follows from the definition of $I_{\\alpha}$ in Equation (52) and the last inequality follows from Equation (53). ", "page_idx": 22}, {"type": "text", "text": "We move on to Step 4 of Algorithm 1, from which we recall, for ease of reference, that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{v}_{j}=\\frac{\\sum_{k\\in\\hat{I}}\\widetilde{Y}_{j k}}{\\sqrt{\\widehat{\\lambda}}\\widehat{S}}\\quad\\mathrm{~for~}j\\in[n].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Note that by Equation (57),the square root of $\\widehat{\\lambda}$ is well-defined on ${\\mathcal{E}}_{1}$ . Rearranging the definition of ${\\widehat{v}}_{j}$ in Equation (69), plugging in $\\widetilde{Y}=Q Y Q$ and expanding, we have for $j\\in[n]$ \uff0c ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\widehat{v}_{j}\\widehat{S}}{\\sqrt{\\lambda^{\\star}}}=\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}Q_{j j}u_{j}^{\\star}\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}+\\frac{\\sum_{k\\in\\widehat{I}}Q_{j j}Q_{k k}W_{j k}}{\\sqrt{\\widehat{\\lambda\\lambda^{\\star}}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Rearranging terms and adding $\\begin{array}{r}{\\widehat{v}_{j}\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}}\\end{array}$ to both sides, we have for any $j\\in[n]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left(\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right)\\sum_{k\\in\\hat{I}}Q_{k k}u_{k}^{\\star}=\\widehat{v}_{j}\\left(\\sum_{k\\in\\hat{I}}Q_{k k}u_{k}^{\\star}-\\frac{\\widehat{S}}{\\sqrt{\\lambda^{\\star}}}\\right)}}\\\\ &{}&{+\\left(\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}-1\\right)Q_{j j}u_{j}^{\\star}\\sum_{k\\in\\hat{I}}Q_{k k}u_{k}^{\\star}+\\frac{\\sum_{k\\in\\hat{I}}Q_{j j}Q_{k k}W_{j k}}{\\sqrt{\\widehat{\\lambda\\lambda^{\\star}}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Dividing by $\\sum_{k\\in\\hat{I}}Q_{k k}u_{k}^{\\star}$ on both sides, again noting that this quantity is positive on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}=\\widehat{v}_{j}\\left(1-\\frac{\\widehat{S}}{\\sqrt{\\lambda^{\\star}}\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}}\\right)+\\Bigg(\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}-1\\Bigg)Q_{j j}u_{j}^{\\star}+\\frac{\\sum_{k\\in\\widehat{I}}Q_{j j}Q_{k k}W_{j k}}{\\sqrt{\\widehat{\\lambda}\\lambda^{\\star}}\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking absolute values in Equation (70) and applying the triangle inequality, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right|\\leq\\left|\\widehat{v}_{j}\\right|\\left|1-\\frac{\\widehat{S}}{\\sqrt{\\lambda^{\\star}}\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}}\\right|+\\left|\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}-1\\right|\\left|Q_{j j}u_{j}^{\\star}\\right|+\\left|\\frac{\\sum_{k\\in\\widehat{I}}Q_{j j}Q_{k k}W_{j k}}{\\sqrt{\\widehat{\\lambda}\\lambda^{\\star}}\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}}\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Trivially, $|Q_{j j}u_{j}^{\\star}|\\leq1$ . Further, on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}$ , we have $\\hat{I}=I_{\\alpha}$ and $Q_{j j}=\\mathrm{sgn}\\left(u_{j}^{\\star}\\right)=s_{j}$ for all $j\\in I_{\\alpha}$ . Thus, we may write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right|\\leq\\left|\\widehat{v}_{j}\\right|\\left|1-\\frac{\\widehat{S}}{\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}\\left|u_{k}^{\\star}\\right|}\\right|+\\left|\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}-1\\right|+\\left|\\frac{\\sum_{k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}}{\\lambda^{\\star}\\sum_{k\\in I_{\\alpha}}\\left|u_{k}^{\\star}\\right|}\\right|\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the event ${\\mathcal{E}}_{1}$ , applying Equations (58) and (59) yields that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}-1\\right|=\\frac{\\left|\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}-1\\right|}{\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}+1}\\le\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Applying this bound in Equation (71), on the event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}$ wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\vert\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right\\vert\\leq\\left\\vert\\widehat{v}_{j}\\right\\vert\\left\\vert1-\\frac{\\widehat{S}}{\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}\\left\\vert u_{k}^{\\star}\\right\\vert}\\right\\vert+\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+\\left\\vert\\frac{\\sum_{k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}}{\\lambda^{\\star}\\sum_{k\\in I_{\\alpha}}\\left\\vert u_{k}^{\\star}\\right\\vert}\\right\\vert\\sqrt{\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We remind the reader that in the proof, constant $C$ may change its precise value from line to line. Another application of Equation (72) and using Assumption 3 yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right|\\leq\\left|\\widehat{v}_{j}\\right|\\left|1-\\frac{\\widehat{S}}{\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}\\left|u_{k}^{\\star}\\right|}\\right|+\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+C\\left|\\frac{\\sum_{k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}}{\\lambda^{\\star}\\sum_{k\\in I_{\\alpha}}\\left|u_{k}^{\\star}\\right|}\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "On the event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ , Equation (68) holds and we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right|\\leq|\\widehat{v}_{j}|\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+C\\left|\\frac{\\sum_{k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}}{\\lambda^{\\star}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|}\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Define the event ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}_{5,\\alpha}=\\left\\{\\operatorname*{max}_{j\\in[n]}\\left\\vert\\sum_{k\\in I_{\\alpha}}s_{j}s_{k}W_{j k}\\right\\vert\\leq C\\sqrt{|I_{\\alpha}|\\nu_{W}\\log n}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Under the event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}$ we can further bound Equation (73) by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right|\\leq|\\widehat{v}_{j}|\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+\\frac{C\\sqrt{|I_{\\alpha}|\\nu_{W}\\log n}}{\\lambda^{\\star}|I_{\\alpha}|\\alpha}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Applying the definition of $\\mathcal{A}_{\\mathrm{0}}$ in Equation (53), on the event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}$ , it holds for all $j\\in[n]$ that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right|\\leq|\\widehat{v}_{j}|\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Applying the triangle inequality and using the fact that $|Q_{j j}u_{j}^{\\star}|\\leq1$ by definition, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle(|\\widehat{v}_{j}|+1)\\,\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}\\leq|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}|\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+\\left(|Q_{j j}u_{j}^{\\star}|+1\\right)\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}|\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}+\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Applying this bound to Equation (75), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Big|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\Big|\\le|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}|\\frac{C\\sqrt{\\nu_{W}}(\\log n)^{5/2}}{\\lambda^{\\star}}+\\frac{C\\sqrt{\\nu_{W}}(\\log n)^{5/2}}{\\lambda^{\\star}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Rearranging the terms, under Assumption 3 for $n$ sufficiently large and when the event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap$ $\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}$ holds, we have for all $j\\in[n]$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}\\right|\\leq\\frac{C\\sqrt{\\nu_{W}}(\\log n)^{5/2}}{\\lambda^{\\star}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recall from Step 5 of Algorithm 1 that we set $\\widehat{u}_{j}=Q_{j j}\\widehat{v}_{j}$ for all $j\\in[n]$ such that $|u_{j}|>(\\sigma/\\widehat{\\lambda})\\log n$ For any such $j$ , on the event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}$ Equation (76) holds, and we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\widehat{u}_{j}-u_{j}^{\\star}|=|Q_{j j}\\widehat{v}_{j}-Q_{j j}^{2}u_{j}^{\\star}|=|Q_{j j}||\\widehat{v}_{j}-Q_{j j}u_{j}^{\\star}|\\leq\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Consider the event ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}=\\left\\{\\|Q\\widehat{\\boldsymbol{v}}-\\boldsymbol{u}^{\\star}\\|_{\\infty}\\leq\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Recalling the events $\\mathcal{E}_{1},\\mathcal{E}_{2,\\alpha},\\mathcal{E}_{3,\\alpha},\\mathcal{E}_{4,\\alpha}$ and $\\mathcal{E}_{5,\\alpha}$ defined in Equations (56) (60) (61) (63) and (74), respectively, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\mathcal{E}\\right)\\geq\\mathbb{P}\\left(\\mathcal{E}\\cap\\mathcal{E}_{1}\\cap\\left(\\bigcup_{\\alpha\\in A_{0}}\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}\\right)\\right)}\\\\ &{\\qquad=\\mathbb{P}\\left(\\bigcup_{\\alpha\\in A_{0}}\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last equality holds because for any $\\alpha\\in\\mathcal{A}_{0}$ , the event $\\mathcal{E}$ is implied by the event $\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap$ $\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}$ which is a fact established in the proof of Equations (76) and (77). ", "page_idx": 24}, {"type": "text", "text": "Since by the definition in Equation (60), $\\{\\mathcal{E}_{2,\\alpha}\\}_{\\alpha\\in A_{0}}$ are disjoint,it fllows from Equation (78) that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\mathcal{E}\\right)\\geq\\displaystyle\\sum_{\\alpha\\in A_{0}}\\mathbb{P}\\left(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}\\right)}\\\\ &{\\qquad=\\displaystyle\\sum_{\\alpha\\in A_{0}}\\left[\\mathbb{P}\\left(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\right)-\\mathbb{P}\\left(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\left(\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}\\right)^{c}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By the union bound and basic set inclusions, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\left(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\left(\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}\\right)^{c}\\right)=\\mathbb{P}\\left[\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\left(\\mathcal{E}_{3,\\alpha}^{c}\\cup\\mathcal{E}_{4,\\alpha}^{c}\\cup\\mathcal{E}_{5,\\alpha}\\right)^{c}\\right)\\right]}&{}\\\\ {\\qquad\\qquad\\leq\\mathbb{P}\\left(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}^{c}\\right)+\\mathbb{P}\\left(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}^{c}\\right)}\\\\ &{\\qquad\\qquad\\qquad+\\mathbb{P}\\left(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{5,\\alpha}^{c}\\right)}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\left(\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}^{c}\\right)+\\mathbb{P}\\left(\\mathcal{E}_{4,\\alpha}^{c}\\right)+\\mathbb{P}\\left(\\mathcal{E}_{5,\\alpha}^{c}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying this bound in Equation (79), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathcal{E}\\right)\\geq\\sum_{\\alpha\\in A_{0}}\\left[\\mathbb{P}\\left(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha}\\right)-\\left(\\mathbb{P}(\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}^{c})+\\mathbb{P}(\\mathcal{E}_{4,\\alpha}^{c})+\\mathbb{P}(\\mathcal{E}_{5,\\alpha}^{c})\\right)\\right]\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The terms $\\mathbb{P}(\\mathcal{E}_{4,\\alpha}^{c})$ and $\\mathbb{P}(\\mathcal{E}_{5,\\alpha}^{c})$ are bounded in Lemma 9 as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{4,\\alpha}^{c})+\\mathbb{P}(\\mathcal{E}_{5,\\alpha}^{c})\\leq O(n^{-8}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the term $\\mathbb{P}(\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}^{c})$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}^{c})=\\mathbb{P}\\left(\\bigcup_{k\\in I_{\\alpha}}\\left\\{Q_{k k}\\neq\\mathrm{sgn}\\left(u_{k}^{\\star}\\right),\\hat{I}=I_{\\alpha}\\right\\}\\cup\\bigcup_{k\\in I_{\\alpha}^{c}}\\left\\{Q_{k k}\\neq1,\\hat{I}=I_{\\alpha}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Step 3 in Algorithm 1, we must have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\bigcup_{k\\in I_{\\alpha}^{c}}\\left\\{Q_{k k}\\neq1,\\hat{I}=I_{\\alpha}\\right\\}=\\bigcup_{k\\in\\hat{I}^{c}}\\left\\{Q_{k k}\\neq1,\\hat{I}=I_{\\alpha}\\right\\}=\\varnothing.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}^{c})=\\mathbb{P}\\left(\\bigcup_{k\\in I_{\\alpha}}\\left\\{\\mathrm{sgn}\\left(u_{k}\\right)\\neq\\mathrm{sgn}\\left(u_{k}^{\\star}\\right),\\hat{I}=I_{\\alpha}\\right\\}\\right)}\\\\ &{\\qquad\\qquad\\leq\\mathbb{P}\\left(\\bigcup_{k\\in I_{\\alpha}}\\left\\{\\mathrm{sgn}\\left(u_{k}\\right)\\neq\\mathrm{sgn}\\left(u_{k}^{\\star}\\right)\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For any $k\\in I_{\\alpha}$ , by the construction of $\\boldsymbol{\\mathcal{A}}$ in Equation (10), we must have $|u_{k}^{\\star}|\\geq\\alpha\\geq1/\\sqrt{2n}$ .Thus, by Lemma 8, we can further bound Equation (82) as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}^{c})\\leq O(n^{-8}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying Equations (81) and (83) to Equation (80), we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\mathcal{E})\\geq\\displaystyle\\sum_{\\alpha\\in A_{0}}\\mathbb{P}(\\mathcal{E}_{1}\\cap\\mathcal{E}_{2,\\alpha})-O(|A_{0}|n^{-8})}\\\\ &{\\qquad=1-\\mathbb{P}\\left(\\mathcal{E}_{1}^{c}\\cup\\left(\\bigcap_{\\alpha\\in A_{0}}\\mathcal{E}_{2,\\alpha}^{c}\\right)\\right)-O(|A_{0}|n^{-8})}\\\\ &{\\qquad\\geq1-\\mathbb{P}(\\mathcal{E}_{1}^{c})-\\mathbb{P}\\left(\\bigcap_{\\alpha\\in A_{0}}\\left\\{\\hat{I}\\neq I_{\\alpha}\\right\\}\\right)-O(|A_{0}|n^{-8}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Under Assumption 4, we have $\\mathbb{P}(\\mathcal{E}_{1}^{c})\\leq O(n^{-8}\\log n)$ , and it follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E})\\geq1-O(n^{-8}\\log n)-\\mathbb{P}\\left(\\bigcap_{\\alpha\\in\\mathcal{A}_{0}}\\left\\{\\hat{I}\\neq I_{\\alpha}\\right\\}\\right)-O(|\\mathcal{A}_{0}|n^{-8}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Applying Lemma 7 to the right hand side yields that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E})\\geq1-O(n^{-8}\\log n)-O(n^{-8})-O(|A_{0}|n^{-8})\\geq1-O(n^{-8}\\log n),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from the fact that $|\\mathcal{A}_{0}|\\leq|\\mathcal{A}|$ and $|A|=O(\\log n)$ by Equation (10). ", "page_idx": 25}, {"type": "text", "text": "At this stage, we have shown that with probability at least $\\displaystyle1-O(n^{-8}\\log n),$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lVert Q\\widehat{\\boldsymbol{v}}-\\boldsymbol{u}^{\\star}\\rVert_{\\infty}\\leq\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Part II. Estimation error related to $\\textbf{\\em u}$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Note that Step 5 in Algorithm 1 further refines $\\widehat{\\pmb{v}}$ to adjust the estimates of small entries of $\\pmb{u}^{\\star}$ . In the remainder of the proof, we show that this refinement does not affect our bound in Equation (84). By Lemma 5, it holds with probability at least $1-O(n^{-8})$ that for all $j\\in[n]$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{|u_{j}-u_{j}^{\\star}|\\leq\\frac{80\\sqrt{\\nu_{W}\\log{n}}+120\\sqrt{\\nu_{W}n}\\,\\big|u_{j}^{\\star}\\big|}{\\lambda^{\\star}}}}\\\\ &{}&{\\leq\\frac{80\\sqrt{\\nu_{W}\\log{n}}}{\\lambda^{\\star}}+\\frac{120\\sqrt{\\nu_{W}n}}{\\lambda^{\\star}}\\left(|u_{j}|+\\big|u_{j}-u_{j}^{\\star}\\big|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second line follows from the triangle inequality. Rearranging terms and noting that $\\lambda^{\\star}>240\\sqrt{\\nu_{W}n\\log n}$ under Assumption 3, we have that with probability at least $1-O(n^{-8})$ ,it holds for all $j\\in[n]$ that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|u_{j}-u_{j}^{\\star}|\\leq\\displaystyle\\frac{1}{1-120\\sqrt{\\nu_{W}n}/\\lambda^{\\star}}\\left(\\frac{80\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}}+\\frac{120\\sqrt{\\nu_{W}n}}{\\lambda^{\\star}}\\left|u_{j}\\right|\\right)}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{160\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}}+\\frac{|u_{j}|}{\\sqrt{\\log n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "That is, defining the event ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{E}_{6}=\\bigcap_{j=1}^{n}\\left\\{|u_{j}-u_{j}^{\\star}|\\leq\\frac{160\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}}+\\frac{|u_{j}|}{\\sqrt{\\log n}}\\right\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{6}^{c})\\leq O(n^{-8}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that by Equation (58) and Assumption 3, when ${\\mathcal{E}}_{1}$ holds, we have $|\\lambda^{\\star}/\\widehat\\lambda|\\leq2$ for sufficiently large $n$ . It follows that when ${\\mathcal{E}}_{1}$ holds, for all $j\\in[n]$ such that $|u_{j}|\\leq(\\sqrt{\\nu_{W}}/\\widehat{\\lambda})\\log n$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n|u_{j}|=\\frac{\\sqrt{\\nu_{W}}\\log n}{\\lambda^{\\star}}\\cdot\\frac{\\lambda^{\\star}}{\\widehat{\\lambda}}\\leq\\frac{2\\sqrt{\\nu_{W}}\\log n}{\\lambda^{\\star}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the inequality follows from Equation (58). Define the set $\\hat{J}=\\{j:|u_{j}|\\leq(\\sqrt{\\nu_{W}}/\\hat{\\lambda})\\log n\\}$ Wehave ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\bigcup_{j\\in\\mathcal{J}}\\left\\{|u_{j}-u_{j}^{\\star}|>\\frac{162\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}}\\right\\}\\right)}\\\\ &{\\quad\\le\\mathbb{P}\\left(\\bigcup_{j\\in\\mathcal{J}}\\left\\{|u_{j}-u_{j}^{\\star}|>\\frac{162\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}}\\right\\}\\cap\\mathcal{E}_{1}\\right)+\\mathbb{P}(\\mathcal{E}_{1}^{c})}\\\\ &{\\quad\\le\\mathbb{P}\\left(\\bigcup_{j\\in\\mathcal{J}}\\left\\{|u_{j}-u_{j}^{\\star}|>\\frac{160\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}}+\\frac{|u_{j}|}{\\sqrt{\\log n}}\\right\\}\\cap\\mathcal{E}_{1}\\right)+O(n^{-8}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality follows from Equation (87) and Assumption 4. Recalling the definition Oof ${\\mathcal{E}}_{6}$ in Equation (85) and using the fact that $\\hat{J}\\subseteq[n]$ , we have that the set in the last inequality of the above display is a subset of $\\mathcal{E}_{6}^{c}$ . Following the above bound and basic set inclusions, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\bigcup_{j\\in\\mathcal{J}}\\left\\{|u_{j}-u_{j}^{\\star}|>\\frac{162\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}}\\right\\}\\right)\\leq\\mathbb{P}(\\mathcal{E}_{6}^{c})+O(n^{-8})\\leq O(n^{-8}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining this with Equation (84) and recalling the definition of $\\widehat{\\pmb{u}}$ in Step 5, we have that with probability at least $1-{\\bar{O}}(n^{-8})$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nd_{\\infty}(\\widehat{\\boldsymbol{u}},\\boldsymbol{u}^{\\star})\\leq\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}\\boldsymbol{n}}{\\lambda^{\\star}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "as we set out to show. ", "page_idx": 26}, {"type": "text", "text": "E A new estimator for $\\lambda^{\\star}$ ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Consider $\\widetilde{Y}$ and $\\widehat S$ as constructed in Steps 3 and 4, respectively, of Algorithm 1. We can estimate $\\lambda^{\\star}$ by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\widehat{\\lambda}=\\frac{\\sum_{j=1}^{n}\\left(\\sum_{k\\in\\widehat{I}}\\widetilde{Y}_{j k}\\right)^{2}-|\\widehat{I}|n\\sigma^{2}}{\\widehat{S}^{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proposition 1 controls the estimation error of $\\widehat{\\lambda}$ given in Equation (88). ", "page_idx": 27}, {"type": "text", "text": "Proposition 1. Under the model in Equation (1), suppose that Assumptions 1, 2 and 3 hold and consider the eigenvalue estimate X as defined in Equation (88). For n suffciently large, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\lesssim\\sqrt{\\nu_{W}}\\log^{5/2}n\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with probability at least $1-O(n^{-8}\\log n)$ ", "page_idx": 27}, {"type": "text", "text": "E.1 Technical lemmas for Proposition 1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our proof of Proposition 1, which appears in Section E.2 below, makes use of two technical lemmas, which we establish first. ", "page_idx": 27}, {"type": "text", "text": "Lemma 10. Let $W\\in\\mathbb{R}^{n\\times n}$ be arandommatrix satisfying Assumption 1.Forafxed $s\\in\\{\\pm1\\}^{n}$ \uff0c forany $\\alpha\\in\\mathcal{A}_{0}$ with probability at least $1-O(n^{-8})$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\sum_{j=1}^{n}\\left(\\sum_{k\\in I_{\\alpha}}s_{k}W_{j k}\\right)^{2}-n|I_{\\alpha}|\\sigma^{2}\\right|\\leq C\\nu_{W}|I_{\\alpha}|\\sqrt{n}\\log n.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Since $\\pmb{s}\\in\\{\\pmb{\\pm}1\\}^{n}$ is fixed and the entries of $W$ are symmetric about zero, without loss of generality, we assume that $\\pmb{s}$ is a vector of all $1$ 's. Reindexing if necessary, we assume without loss of generality that $I_{\\alpha}=[K]$ , where $K=|I_{\\alpha}|>0$ .We have $K>0$ since $\\alpha\\in\\mathcal{A}_{0}$ and by the definition of $\\mathcal{A}_{\\mathrm{0}}$ in Equation (53), $|I_{\\alpha}|\\geq\\left(\\alpha^{2}\\log^{2}n\\right)^{-1}>0$ . It follows that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1}^{n}\\left(\\displaystyle\\sum_{k\\in I_{\\alpha}}W_{j k}\\right)^{2}-n|I_{\\alpha}|\\sigma^{2}=\\displaystyle\\sum_{j=1}^{n}\\sum_{k=1}^{K}W_{j k}^{2}-n K\\sigma^{2}+2\\displaystyle\\sum_{j=1}^{n}\\sum_{1\\leq k<\\ell\\leq K}W_{j k}W_{j\\ell}}&{{}}\\\\ {=\\gamma_{1}+\\gamma_{2}+\\gamma_{3},}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\gamma_{1}:=\\displaystyle\\sum_{j=K+1}^{n}\\displaystyle\\sum_{k=1}^{K}W_{j k}^{2}+\\displaystyle\\sum_{k=1}^{K}W_{k k}^{2}+2\\displaystyle\\sum_{1\\le j<k\\le K}W_{j k}^{2}-n K\\sigma^{2},}\\\\ {\\gamma_{2}:=2\\displaystyle\\sum_{j=K+1}^{n}\\displaystyle\\sum_{1\\le k<\\ell\\le K}W_{j k}W_{j\\ell}\\;\\;\\mathrm{and}}\\\\ {\\gamma_{3}:=\\displaystyle\\sum_{j=1}^{K}\\displaystyle\\sum_{1\\le k\\neq\\ell\\le K}W_{j k}W_{j\\ell}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We will bound these three quantities separately. ", "page_idx": 27}, {"type": "text", "text": "We begin by bounding $\\gamma_{1}$ in Equation (90). For ease of notation, define $N=K(n-K/2+1/2)$ $\\gamma_{1}$ is a sum of $N$ independent sub-exponential random variables. By Equation (2.18) in [56], for any $t\\in(0,2)$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{|\\gamma_{1}|}{N}\\geq\\nu_{W}t\\right)\\leq2e^{-N t^{2}/32}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Taking $t=16\\sqrt{N^{-1}\\log n}$ yields that for all suitably large $n$ , with probability at least $1-O(n^{-8})$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\gamma_{1}\\right|\\leq16\\nu_{W}\\sqrt{N\\log n}\\leq16\\nu_{W}\\sqrt{K n\\log n},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality follows from the trivial bound $N\\le K n$ ", "page_idx": 28}, {"type": "text", "text": "Turning to $\\gamma_{2}$ in Equation (90), define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{A}=\\mathbf{1}_{K}\\mathbf{1}_{K}^{\\top}-\\pmb{I}_{K}\\in\\mathbb{R}^{K\\times K},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathbf{1}_{K}\\in\\mathbb{R}^{K}$ is a vector of all 1's and $\\pmb{I}_{K}$ is the $K\\times K$ identity matrix. It follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\gamma_{2}=\\sum_{j=K+1}^{n}\\sum_{1\\leq k\\neq\\ell\\leq K}W_{j k}W_{j\\ell}=\\sum_{j=K+1}^{n}\\sum_{1\\leq k,\\ell\\leq K}W_{j k}A_{k\\ell}W_{j\\ell}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the Hanson-Wright inequality (see Theorem 6.2.1 in [54]), for $t\\geq0$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\left|\\displaystyle\\sum_{j=K+1}^{n}\\sum_{1\\le k,\\ell\\le K}W_{j k}A_{k\\ell}W_{j\\ell}\\right|\\ge t\\right)}\\\\ &{\\qquad\\le2\\exp\\left\\{-c\\operatorname*{min}\\left(\\displaystyle\\frac{t^{2}}{(n-K)\\nu_{W}^{2}\\|A\\|_{\\mathrm{F}}^{2}},\\displaystyle\\frac{t}{\\sqrt{n-K}\\nu_{W}\\|A\\|_{\\mathrm{F}}}\\right)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $c\\,>\\,0$ is a universal constant. We note that $\\|A\\|_{\\mathrm{F}}\\,\\leq\\,K^{2}$ by construction. Taking $t\\,=$ $(8\\nu_{W}/c)K\\sqrt{n-K}\\log n$ , with probability at least $1-O(n^{-8})$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\gamma_{2}|=\\left|\\sum_{j=K+1}^{n}\\sum_{1\\leq k,\\ell\\leq K}W_{j k}A_{k\\ell}W_{j\\ell}\\right|\\leq\\frac{8\\nu_{W}}{c}K\\sqrt{n-K}\\log n.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, to bound $\\gamma_{3}$ in Equation (90), we define ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{w}=\\mathrm{vech}([W_{i j}]_{1\\leq i,j\\leq K})\\in\\mathbb{R}^{K(K+1)/2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\operatorname{vech}(\\cdot)$ is the half-vectorization operator that vectorizes the upper triangular part (including the diagonal) of a given matrix. ", "page_idx": 28}, {"type": "text", "text": "We identify the elements of $\\pmb{w}$ with the elements of ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{K}=\\left\\{(i_{1},i_{2}):1\\leq i_{1}\\leq i_{2}\\leq K\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "that is, pairs $(i_{1},i_{2})$ satisfying $1\\leq i_{1}\\leq i_{2}\\leq K$ .We note that $\\gamma_{3}$ is a sum of products of the form $w_{(i_{1},i_{2})}w_{(j_{1},j_{2})}$ such that $(i_{1},i_{2}),(j_{1},j_{2})\\in\\mathcal{I}_{K}$ and one element of $(i_{1},i_{2})$ agrees with one element of $(j_{1},j_{2})$ , while the others disagree. For $(i_{1},i_{2})\\in\\mathcal{I}_{K}$ with $i_{1}<i_{2}$ , there are $2(K-1)$ other pairs $(j_{1},j_{2})\\in\\mathcal{I}_{K}$ satisfying this requirement, while for $(i_{1},i_{2})\\in\\mathcal{I}_{K}$ with $i_{1}=i_{2}$ , there are $(K-1)$ other elements of $\\mathcal{I}_{K}$ satisfying the requirement. In total, there are $K^{2}(K-1)$ such pairs, which agreewiththenuberoftemi $\\gamma_{3}$ We defne matrix $B\\in\\mathbb{R}^{K(K+1)/2\\times K(K+1)/2}$ by identfying its rows and columns with elements of $\\mathcal{I}_{K}$ and setting ", "page_idx": 28}, {"type": "equation", "text": "$$\nB_{(i_{1},i_{2}),(j_{1},j_{2})}=\\operatorname*{max}\\left\\{\\delta_{i_{1}j_{1}}(1-\\delta_{i_{2}j_{2}}),\\delta_{i_{1}j_{2}}(1-\\delta_{i_{2}j_{1}}),\\delta_{i_{2}j_{1}}(1-\\delta_{i_{1}j_{2}}),\\delta_{i_{2}j_{2}}(1-\\delta_{i_{1}j_{2}})\\right\\}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for $(i_{1},i_{2}),(j_{1},j_{2})\\in\\mathcal{I}_{K}$ where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\delta_{i j}={\\binom{1}{0}}_{\\mathrm{\\otherwise.}}^{\\mathrm{\\if}\\ i}=j,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "There are $K^{2}(K-1)$ entries in $_B$ that are equal to 1, while all others are equal to zero. Thus, we have $\\|B\\|_{\\mathrm{F}}=K\\sqrt{K-1}\\leq K^{3/2}$ . By construction, one can verify that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\pmb{w}^{\\top}\\pmb{B}\\pmb{w}=\\sum_{(i_{1},i_{2})\\in\\mathcal{I}}\\sum_{(j_{1},j_{2})\\in\\mathcal{I}}w_{(i_{1},i_{2})}B_{(i_{1},i_{2}),(j_{1},j_{2})}w_{(j_{1},j_{2})}=\\sum_{j=1}^{K}\\sum_{1\\leq k\\neq\\ell\\leq K}W_{j k}W_{j\\ell}=\\gamma_{3}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Again, by the Hanson-Wright inequality, for every $t\\geq0$ wehave ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\pmb{w}^{\\top}\\pmb{B}\\pmb{w}\\right|\\geq t\\right)\\leq2\\exp\\left\\{-c\\operatorname*{min}\\left(\\frac{t^{2}}{\\nu_{W}^{2}\\|\\pmb{B}\\|_{\\mathrm{F}}^{2}},\\frac{t}{\\nu_{W}\\|\\pmb{B}\\|_{\\mathrm{F}}}\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $c>0$ is a universal constant. Taking $t=(8\\nu_{W}/c)\\lVert B\\rVert_{\\mathrm{F}}\\log n$ yields that with probability at least $1-O(n^{-8})$ \uff0c ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\gamma_{3}\\right|=\\left|\\pmb{w}^{\\top}\\pmb{B}\\pmb{w}\\right|\\leq\\frac{8\\nu_{W}}{c}\\|\\pmb{B}\\|_{\\mathrm{F}}\\log n\\leq\\frac{8\\nu_{W}}{c}K^{3/2}\\log n,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the second inequality follows from our bound on $\\|\\boldsymbol{B}\\|_{\\mathrm{F}}$ ", "page_idx": 29}, {"type": "text", "text": "Finally, plugging the bounds in Equations (91) (92) and (93) into Equation (90), we have that with probability at least $1-O(n^{-8})$ , Equation (89) holds, as we set out to show. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma 11. Under Assumption $^{\\,l}$ for any fixed $s\\in\\{\\pm1\\}^{n}$ and any fixed $\\pmb{u}^{\\star}\\in\\mathbb{S}^{n-1}$ for any $\\alpha\\in{\\mathcal{A}}$ it holds with probability at least $1^{\\bar{-}}-O(\\dot{n}^{-8})$ that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\sum_{k\\in I_{\\alpha}}\\sum_{j=1}^{n}u_{j}^{\\star}s_{k}W_{j k}\\right|\\leq4\\sqrt{2\\nu_{W}|I_{\\alpha}|\\log n}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Since $\\pmb{s}\\in\\{\\pmb{\\pm}1\\}^{n}$ is fixed and the entries of $W$ are symmetric about zero, without loss of generality, we assume that $\\pmb{s}$ is a vector of all 1's. Reindexing if necessary, we again assume that $\\bar{I}_{\\alpha}=[K]$ without loss of generality, where $K=|I_{\\alpha}|$ . Rearranging sums, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\sum_{j=1}^{n}u_{j}^{\\star}s_{k}W_{j k}=\\sum_{k=1}^{K}\\sum_{j=1}^{n}u_{j}^{\\star}W_{j k}}\\\\ &{\\displaystyle=\\sum_{j=1}^{K}\\sum_{k=j+1}^{K}(u_{j}^{\\star}+u_{k}^{\\star})W_{j k}+\\sum_{j=1}^{K}u_{j}^{\\star}W_{j j}+\\sum_{k=1}^{K}\\sum_{j=K+1}^{n}u_{j}^{\\star}W_{j k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By standard subgaussian concentration inequalities [54, 56], using the fact that $\\pmb{u}^{\\star}$ is unit-norm, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{k=1}^{K}\\sum_{j=1}^{n}u_{j}^{\\star}s_{k}W_{j k}\\right|\\geq4\\sqrt{\\nu_{W}2K\\log n}\\right)\\leq O(n^{-8}),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "completing the proof. ", "page_idx": 29}, {"type": "text", "text": "E.2  Proof of Proposition 1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof. Similar to the proof of Theorem 1, we assume that $\\lambda^{\\star}>0$ without loss of generality. Recalling the quantity $\\widehat S$ as defined in Step 4 of Algorithm 1 and the fact that $\\widehat{S}>0$ from Equation (65), we define ", "page_idx": 29}, {"type": "equation", "text": "$$\nR_{\\hat{I}}=\\frac{\\sqrt{\\lambda^{\\star}}\\left(\\sum_{k\\in\\hat{I}}Q_{k k}u_{k}^{\\star}\\right)}{\\widehat{S}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we recal that $\\hat{I}$ isthe set given in Step 2 of Algorithm 1 and $Q\\in\\mathbb{R}^{n\\times n}$ is a diagonal matrix given in Step 3 of Algorithm 1. ", "page_idx": 29}, {"type": "text", "text": "Plugging in $\\widetilde{Y}=Q Y Q$ and expanding the right hand side of Equation (88), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\lambda}=\\displaystyle\\frac{1}{\\widehat{S}^{2}}\\left(\\sum_{j=1}^{n}\\left(\\lambda^{\\star}Q_{j j}u_{j}^{\\star}\\displaystyle\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}+\\sum_{k\\in\\widehat{I}}Q_{j j}Q_{k k}W_{j k}\\right)^{2}\\right)-\\frac{n|\\widehat{I}|\\sigma^{2}}{\\widehat{S}^{2}}}\\\\ &{\\quad=\\displaystyle\\frac{1}{\\widehat{S}^{2}}\\sum_{j=1}^{n}(\\lambda^{\\star}u_{j}^{\\star})^{2}\\left(\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}\\right)^{2}+\\displaystyle\\frac{1}{\\widehat{S}^{2}}\\sum_{j=1}^{n}\\left(\\sum_{k\\in\\widehat{I}}Q_{k k}W_{j k}\\right)^{2}}\\\\ &{\\quad\\qquad\\qquad+\\displaystyle\\frac{2\\lambda^{\\star}}{\\widehat{S}^{2}}\\sum_{j=1}^{n}\\left(u_{j}^{\\star}\\displaystyle\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}\\displaystyle\\sum_{\\ell\\in\\widehat{I}}Q_{\\ell\\ell}W_{j\\ell}\\right)-\\frac{n|\\widehat{I}|\\sigma^{2}}{\\widehat{S}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Applying the definition of $R_{\\hat{I}}$ in Equation (94) and rearranging terms, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\widehat{\\lambda}=\\lambda^{\\star}R_{\\widehat{I}}^{2}+R_{\\widehat{I}}^{2}\\frac{\\sum_{j=1}^{n}\\left(\\sum_{k\\in\\widehat{I}}Q_{k k}W_{j k}\\right)^{2}-n|\\widehat{I}|\\sigma^{2}}{\\left(\\sqrt{\\lambda^{\\star}}\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}\\right)^{2}}+2R_{\\widehat{I}}^{2}\\sum_{j=1}^{n}u_{j}^{\\star}\\frac{\\sum_{k\\in\\widehat{I}}Q_{k k}W_{j k}}{\\sum_{k\\in\\widehat{I}}Q_{k k}u_{k}^{\\star}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "To control the term $R_{\\hat{I}}$ in Equation (95), we proceed to bound a related term $R_{\\alpha}$ for all $\\alpha\\in\\mathcal{A}_{0}$ defined as ", "page_idx": 30}, {"type": "equation", "text": "$$\nR_{\\alpha}=\\frac{\\sqrt{\\lambda^{\\star}}\\left(\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right)}{\\widehat{S}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We recall the events $\\mathcal{E}_{2,\\alpha},\\mathcal{E}_{3,\\alpha}$ and $\\mathcal{E}_{4,\\alpha}$ for all $\\alpha\\in\\mathcal{A}_{0}$ as defined in Equations (60), (61) and (63) in the proof of Theorem 1. For any $\\alpha\\in\\mathcal{A}_{0}$ , to control the term $R_{\\alpha}$ , we divide $\\widehat S$ on both sides of Equation (67) stated in the proof of Theorem 1. It follows from Equation (67) that ", "page_idx": 30}, {"type": "equation", "text": "$$\n|R_{\\alpha}-1|=\\left|\\frac{\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|}{\\widehat{S}}-1\\right|\\leq\\frac{C\\sqrt{\\nu_{W}\\log n}}{\\alpha\\sqrt{\\lambda^{\\star}}\\widehat{S}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "holds on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ . Following the previous bound, on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ we have that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|R_{\\alpha}-1|\\leq\\frac{C\\sqrt{\\nu_{W}\\log n}}{\\alpha\\sqrt{\\lambda^{\\star}}\\widehat{S}}\\leq\\frac{C\\sqrt{\\nu_{W}\\log n}}{\\alpha\\lambda^{\\star}\\left(\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right)-\\alpha\\sqrt{\\lambda^{\\star}}\\left|\\widehat{S}-\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right|}}\\\\ &{\\qquad\\qquad\\leq\\frac{C\\sqrt{\\nu_{W}\\log n}}{\\lambda^{\\star}|I_{\\alpha}|\\alpha^{2}-C\\sqrt{\\nu_{W}\\log n}}\\leq\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}-C\\sqrt{\\nu_{W}}\\log^{5/2}n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the second inequality holds by the triangle inequality, the third inequality follows from Equations (66) and (52), the last inequality holds by Equation (53). We remind the reader that we allowconstant $C$ to change its precise value from line to line in the proof. Under Assumption 3, it follows from the previous bound that for $n$ sufficiently large, we have on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ that ", "page_idx": 30}, {"type": "equation", "text": "$$\n|R_{\\alpha}-1|\\leq\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By Equation (96) and Assumption 3, for $n$ sufficiently large, we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n1/2\\le R_{\\alpha}\\le2\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ ", "page_idx": 30}, {"type": "text", "text": "Recall $s\\in\\{\\pm1\\}^{n}$ as defined in Equation (62). On the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}$ we find that, using Equation (95) to substitute for X and applying the triangle inequality, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\leq\\lambda^{\\star}\\left|R_{\\alpha}^{2}-1\\right|+R_{\\alpha}^{2}\\frac{\\left|\\sum_{j=1}^{n}\\big(\\sum_{k\\in I_{\\alpha}}s_{k}W_{j k}\\big)^{2}-n\\left|I_{\\alpha}\\right|\\sigma^{2}\\right|}{\\left(\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}\\left|u_{k}^{\\star}\\right|\\right)^{2}}}\\\\ &{\\qquad\\qquad\\qquad+\\,2R_{\\alpha}^{2}\\left|\\displaystyle\\sum_{j=1}^{n}u_{j}^{\\star}\\frac{\\sum_{k\\in I_{\\alpha}}s_{k}W_{j k}}{\\sum_{k\\in I_{\\alpha}}\\left|u_{k}^{\\star}\\right|}\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we substitute $R_{\\hat{I}}$ by $R_{\\alpha}$ using the fact that on $\\mathcal{E}_{2,\\alpha}$ wehave $\\hat{I}=I_{\\alpha}$ . Continuing from the previous bound, it follows from Equation (96) that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\leq\\lambda^{\\star}\\cdot\\frac{C\\sqrt{\\nu_{W}}\\log^{5/2}n}{\\lambda^{\\star}}\\cdot\\left|R_{\\alpha}+1\\right|+R_{\\alpha}^{2}\\frac{\\displaystyle\\left|\\sum_{j=1}^{n}\\left(\\sum_{k\\in I_{\\alpha}}s_{k}W_{j k}\\right)^{2}-n\\right|I_{\\alpha}|\\sigma^{2}\\right|}{\\left(\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right)^{2}}}\\\\ {\\displaystyle\\qquad+\\,2R_{\\alpha}^{2}\\left|\\sum_{j=1}^{n}u_{j}^{\\star}\\frac{\\displaystyle\\sum_{k\\in I_{\\alpha}}s_{k}W_{j k}}{\\displaystyle\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Applying Equation (97) to $R_{\\alpha}$ in the above display, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\leq C\\sqrt{\\nu_{W}}\\log^{5/2}n+\\frac{C\\left|\\sum_{j=1}^{n}\\left(\\sum_{k\\in I_{\\alpha}}s_{k}W_{j k}\\right)^{2}-n\\left|I_{\\alpha}\\right|\\sigma^{2}\\right|}{\\left(\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}\\left|u_{k}^{\\star}\\right|\\right)^{2}}}\\\\ &{\\qquad\\qquad+\\,C\\left|\\frac{\\sum_{k\\in I_{\\alpha}}\\sum_{j=1}^{n}u_{j}^{\\star}s_{k}W_{j k}}{\\sum_{k\\in I_{\\alpha}}\\left|u_{k}^{\\star}\\right|}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For all $\\alpha\\in\\mathcal{A}_{0}$ , define the events ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{1,\\alpha}=\\left\\{\\left|\\displaystyle\\sum_{j=1}^{n}\\left(\\sum_{k\\in I_{\\alpha}}s_{k}W_{j k}\\right)^{2}-n|I_{\\alpha}|\\sigma^{2}\\right|\\leq C\\nu_{W}\\left|I_{\\alpha}\\right|\\sqrt{n}\\log n\\right\\}}\\\\ &{\\mathcal{G}_{2,\\alpha}=\\left\\{\\left|\\displaystyle\\sum_{k\\in I_{\\alpha}}\\sum_{j=1}^{n}u_{j}^{\\star}s_{k}W_{j k}\\right|\\leq4\\sqrt{2\\nu_{W}\\left|I_{\\alpha}\\right|\\log n}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "On the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{G}_{1,\\alpha}\\cap\\mathcal{G}_{2,\\alpha}$ , we can further bound Equation (98) according to ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\leq C\\sqrt{\\nu_{W}}\\log^{5/2}n+\\frac{C\\nu_{W}|I_{\\alpha}|\\sqrt{n}\\log n}{\\left(\\sqrt{\\lambda^{\\star}}\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|\\right)^{2}}+\\frac{C\\sqrt{\\nu_{W}}|I_{\\alpha}|\\log n}{\\sum_{k\\in I_{\\alpha}}|u_{k}^{\\star}|},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "following the definition of $\\mathcal{G}_{1,\\alpha}$ and $\\mathcal{G}_{2,\\alpha}$ in Equation (99). By the definition of $I_{\\alpha}$ in Equation (52), it follows that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\leq C\\sqrt{\\nu_{W}}\\log^{5/2}n+\\frac{C\\nu_{W}|I_{\\alpha}|\\sqrt{n}\\log n}{\\lambda^{\\star}|I_{\\alpha}|^{2}\\alpha^{2}}+\\frac{C\\sqrt{\\nu_{W}|I_{\\alpha}|\\log n}}{|I_{\\alpha}|\\alpha}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "holds on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{G}_{1,\\alpha}\\cap\\mathcal{G}_{2,\\alpha}$ . Applying Equation (53) to lower bound $\\lvert I_{\\alpha}\\rvert$ wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\leq C\\sqrt{\\nu_{W}}\\log^{5/2}n+\\frac{C\\nu_{W}\\sqrt{n}\\log^{3}n}{\\lambda^{\\star}}+C\\sqrt{\\nu_{W}}\\log^{3/2}n\\leq C\\sqrt{\\nu_{W}}\\log^{5/2}n,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "holds on the event $\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{G}_{1,\\alpha}\\cap\\mathcal{G}_{2,\\alpha}$ where the last inequality holds under Assumption 3 when $n$ is sufficiently large. ", "page_idx": 31}, {"type": "text", "text": "Consider the event ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{G}=\\left\\{\\left|\\widehat{\\lambda}-\\lambda^{\\star}\\right|\\leq C\\sqrt{\\nu_{W}}\\log^{5/2}n\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By the proof leading to Equation (100), we have for all $\\alpha\\in\\mathcal{A}_{0}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{G}_{1,\\alpha}\\cap\\mathcal{G}_{2,\\alpha}\\subseteq\\mathcal{G}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "It follows that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{P}(\\mathcal{G})\\ge\\mathbb{P}\\left(\\bigcup_{\\alpha\\in\\mathcal{A}_{0}}\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{G}_{1,\\alpha}\\cap\\mathcal{G}_{2,\\alpha}\\right)}}\\\\ &{}&{=\\sum_{\\alpha\\in\\mathcal{A}_{0}}\\mathbb{P}\\left(\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}\\cap\\mathcal{E}_{4,\\alpha}\\cap\\mathcal{G}_{1,\\alpha}\\cap\\mathcal{G}_{2,\\alpha}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the first inequality follows from set inclusion and the last equality follows from the fact that $\\{\\mathcal{E}_{2,\\alpha}\\}_{\\alpha\\in A_{0}}$ are disjoint events according to Equation (60). By basic set inclusions, it follows that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{G})\\geq\\sum_{\\alpha\\in A_{0}}\\left(\\mathbb{P}(\\mathcal{E}_{2,\\alpha})-\\mathbb{P}(\\mathcal{E}_{2,\\alpha}\\cap\\mathcal{E}_{3,\\alpha}^{c})-\\mathbb{P}(\\mathcal{E}_{4,\\alpha}^{c})-\\sum_{j=1}^{2}\\mathbb{P}(\\mathcal{G}_{j,\\alpha}^{c})\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Applying Lemma 9, Equations (83), Lemma 10 and Lemma 11 to the above display yields that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{G})\\geq\\sum_{\\alpha\\in A_{0}}\\mathbb{P}(\\mathcal{E}_{2,\\alpha})-O(|A_{0}|n^{-8})=1-\\mathbb{P}\\left(\\bigcap_{\\alpha\\in A_{0}}\\left\\{\\hat{I}\\neq I_{\\alpha}\\right\\}\\right)-O(|A_{0}|n^{-8}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Applying Lemma 7 and using the fact that $|\\mathcal{A}_{0}|\\,\\le\\,|\\mathcal{A}|$ and $|A|\\,=\\,O(\\log n)$ by its definition in Equation (10), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{G})\\geq1-O(n^{-8})-O(|\\mathcal{A}_{0}|n^{-8})\\geq1-O(n^{-8}\\log n),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "completing the proof. ", "page_idx": 31}, {"type": "text", "text": "F Proofs for lower bounds of metric entropy under $d_{2,\\infty}$ ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "To prove Lemma 4, we first state a few technical lemmas. ", "page_idx": 32}, {"type": "text", "text": "F.1 Technical Lemmas ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "For a semi-metric $\\rho$ defined over a set $\\mathbb{K}$ , we let $\\mathcal{N}(\\mathbb{K},\\rho,\\delta)$ denote the $\\delta$ -covering number of $\\mathbb{K}$ under $\\rho$ (see Chapter 15 in [56]). The collection of all linear subspaces of fixed dimension $r$ of the Euclidean space $\\mathbb{R}^{n}$ forms the Grassmann manifold $\\mathbb{G}_{n,r}$ , also termed the Grassmannian. For points on the Grassmannian, we adopt the projector perspective [13]: a subspace $\\mathcal{U}\\in\\mathbb{G}_{n,r}$ is identified with the (unique) orthogonal projector $\\b{P}\\in\\mathbb{R}^{n\\times n}$ onto $\\boldsymbol{\\mathcal{U}}$ , which in turn is uniquely represented by ${\\boldsymbol{P}}={\\boldsymbol{U}}{\\boldsymbol{U}}^{\\bar{T}}$ , where $U\\in\\mathbb{R}^{n\\times r}$ whose columns form an orthonormal basis of $\\boldsymbol{\\mathcal{U}}$ . For any matrix $A\\in\\mathbb{R}^{m\\times n}$ with singular values denoted by $\\sigma_{i}(A)$ for $1\\leq i\\leq\\operatorname*{min}\\{m,n\\}$ , the Schatten- $q$ norm $\\|\\cdot\\|_{S_{q}}$ is defined for any $1\\leq q\\leq\\infty$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|A\\|_{S_{q}}:=\\left(\\sum_{i=1}^{\\operatorname*{min}\\{m,n\\}}\\sigma_{i}^{q}(A)\\right)^{1/q}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "For a pair of subspaces $\\mathcal{U}_{1},\\mathcal{U}_{2}\\,\\in\\,\\mathbb{G}_{n,r}$ identified with projectors $P_{1}=U_{1}U_{1}^{\\top},P_{2}=U_{2}U_{2}^{\\top}\\in$ $\\mathbb{R}^{n\\times n}$ , respectively, we consider the distance $d_{S_{q}}(\\cdot,\\cdot)$ induced by the Schatten- $q$ norm ", "page_idx": 32}, {"type": "equation", "text": "$$\nd_{S_{q}}(\\mathcal{U}_{1},\\mathcal{U}_{2}):=\\|P_{1}-P_{2}\\|_{S_{q}}=\\|U_{1}U_{1}^{\\top}-U_{2}U_{2}^{\\top}\\|_{S_{q}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma 12 controls the covering number of $\\mathbb{G}_{n,r}$ under $d_{S_{q}}$ ", "page_idx": 32}, {"type": "text", "text": "Lemma 12 ([46] Proposition 8). For any integer $1\\leq r\\leq n$ suchthat $2r\\,\\leq\\,n$ any $q$ suchthat $1\\leq q\\leq\\infty$ andany $\\delta>0$ wehave ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left(\\frac{c_{0}}{\\delta}\\right)^{r(n-r)}\\leq\\mathcal{N}(\\mathbb{G}_{n,r},\\;d_{S_{q}},\\;\\delta r^{1/q})\\leq\\left(\\frac{C_{0}}{\\delta}\\right)^{r(n-r)}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $c_{0},C_{0}>0$ are universal constants, $d_{S_{q}}$ is the distance defined in Equation (101) induced by the Schatten-q norm. ", "page_idx": 32}, {"type": "text", "text": "Let $\\mathbb{V}_{n,r}:=\\{{\\pmb U}\\in\\mathbb{R}^{n\\times r}:{\\pmb U}^{\\top}{\\pmb U}={\\pmb I}_{r}\\}$ be the $n\\times r$ Stiefel manifold [16]. The distance $d_{S_{q}}(\\cdot,\\cdot)$ can also be viewed as a distance defined on $\\mathbb{V}_{n,r}$ . For a pair of orthogonal matrices $U_{1},U_{2}\\in\\dot{\\mathbb{V}}_{n,r}$ welet ", "page_idx": 32}, {"type": "equation", "text": "$$\nd_{S_{q}}(\\boldsymbol{U}_{1},\\boldsymbol{U}_{2}):=\\|\\boldsymbol{U}_{1}\\boldsymbol{U}_{1}^{\\top}-\\boldsymbol{U}_{2}\\boldsymbol{U}_{2}^{\\top}\\|_{S_{q}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "When $q=2$ , the Schatten- $q$ norm coincides with the Frobenius norm, and we have ", "page_idx": 32}, {"type": "equation", "text": "$$\nd_{S_{2}}(\\pmb{U}_{1},\\pmb{U}_{2})=\\left\\lVert\\pmb{U}_{1}\\pmb{U}_{1}^{\\top}-\\pmb{U}_{2}\\pmb{U}_{2}^{\\top}\\right\\rVert_{\\mathrm{F}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Define a distance $d_{\\mathrm{{F}}}$ over $\\mathbb{V}_{n,r}$ as ", "page_idx": 32}, {"type": "equation", "text": "$$\nd_{\\mathrm{F}}(\\boldsymbol{U}_{1},\\boldsymbol{U}_{2}):=\\operatorname*{min}_{\\boldsymbol{\\Gamma}\\in\\mathbb{O}_{r}}\\|\\boldsymbol{U}_{1}-\\boldsymbol{U}_{2}\\boldsymbol{\\Gamma}\\|_{\\mathrm{F}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma 13 controls the covering number of $\\mathbb{V}_{n,r}$ under $d_{\\mathrm{{F}}}$ , which follows immediately from Lemma 12. ", "page_idx": 32}, {"type": "text", "text": "Lemma 13. For any integer $1\\leq r\\leq n/2$ and for every $\\delta>0$ we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\left(\\frac{c_{0}\\sqrt{r}}{\\delta}\\right)^{r(n-r)}\\leq\\mathcal{N}(\\mathbb{V}_{n,r},d_{\\mathrm{F}},\\delta)\\leq\\left(\\frac{C_{0}\\sqrt{2r}}{\\delta}\\right)^{r(n-r)},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $c_{0},C_{0}>0$ are universal constants and $d_{\\mathrm{{F}}}$ is the ", "page_idx": 32}, {"type": "text", "text": "Proof of Lemma 13. We identify each element $U$ of $\\mathbb{V}_{n,r}$ with the element $U U^{\\top}$ in $\\mathbb{G}_{n,r}$ and apply Lemma 12 to $\\mathbb{V}_{n,r}$ under $d_{\\mathrm{{F}}}$ . By the fact that (see Lemma 2.6 in [27]) ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2}}\\left\\|\\boldsymbol{U}_{1}\\boldsymbol{U}_{1}^{\\top}-\\boldsymbol{U}_{2}\\boldsymbol{U}_{2}^{\\top}\\right\\|_{\\mathrm{F}}\\leq d_{\\mathrm{F}}(\\boldsymbol{U}_{1},\\boldsymbol{U}_{2})\\leq\\left\\|\\boldsymbol{U}_{1}\\boldsymbol{U}_{1}^{\\top}-\\boldsymbol{U}_{2}\\boldsymbol{U}_{2}^{\\top}\\right\\|_{\\mathrm{F}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{N}\\left(\\mathbb{V}_{n,r},d_{S_{2}},\\delta\\right)\\leq\\mathcal{N}(\\mathbb{V}_{n,r},d_{\\mathrm{F}},\\delta)\\leq\\mathcal{N}\\left(\\mathbb{V}_{n,r},d_{S_{2}},\\frac{\\delta}{\\sqrt{2}}\\right)\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and it follows from Lemma 12 that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left(\\frac{c_{0}\\sqrt{r}}{\\delta}\\right)^{r(n-r)}\\leq\\mathcal{N}(\\mathbb{V}_{s,r},d_{\\mathrm{F}},\\delta)\\leq\\left(\\frac{C_{0}\\sqrt{2r}}{\\delta}\\right)^{r(n-r)},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "completing the proof. ", "page_idx": 33}, {"type": "text", "text": "For any $s\\geq r$ , Lemma 14 relates the packing number of any subset $\\mathbb{K}\\subseteq\\mathbb{V}_{s,r}$ to its Haar measure and the covering number of $\\mathbb{V}_{s,r}$ . Recall that the Haar measure on $\\mathbb{V}_{s,r}$ is the invariant measure under both left- and right-orthogonal transformation [29]. In other words, for any subset $\\mathbb{K}\\subseteq\\mathbb{V}_{s,r}$ and orthogonal matrices $\\Gamma_{1}\\in\\mathbb{O}_{s}$ and $\\Gamma_{2}\\in\\mathbb{O}_{r}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\xi_{H}\\left(\\boldsymbol{\\mathbf{T}}_{1}\\cdot\\mathbb{K}\\right)=\\xi_{H}\\left(\\mathbb{K}\\right)\\;\\;\\mathrm{and}\\;\\;\\xi_{H}\\left(\\mathbb{K}\\cdot\\boldsymbol{\\mathbf{T}}_{2}\\right)=\\xi_{H}\\left(\\mathbb{K}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Gamma_{1}\\cdot\\mathbb{K}:=\\left\\{\\Gamma_{1}U:U\\in\\mathbb{K}\\right\\}\\ {\\mathrm{and~}}\\mathbb{K}\\cdot\\Gamma_{2}:=\\left\\{U\\Gamma_{2}:U\\in\\mathbb{K}\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For any norm $\\lVert\\cdot\\rVert$ defined over a set $\\mathbb{K}$ ,we alsouse the notation $\\mathcal{M}(\\mathbb{K},\\|\\cdot\\|,\\delta)$ and $\\mathcal{N}(\\mathbb{K},\\|\\cdot\\|,\\delta)$ to denote the $\\delta$ packing number and $\\delta$ -covering number of $\\mathbb{K}$ under the distance induced by $\\lVert\\cdot\\rVert$ respectively. With the above setup, we state Lemma 14 below. ", "page_idx": 33}, {"type": "text", "text": "Lemma 14.For $1\\,\\leq\\,r\\,\\leq\\,s,$ let $\\xi_{H}$ denote theHaar measure on $\\mathbb{V}_{s,r}$ For $1\\,\\le\\,r\\,\\le\\,s$ and any $\\mathbb{K}\\subseteq\\mathbb{V}_{s,r}$ such that $\\xi_{H}(\\mathbb{K})\\ge\\gamma>0,$ for any unitarily invariant norm $\\lVert\\cdot\\rVert$ on $\\mathbb{R}^{s\\times r}$ , we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{K},\\|\\!\\|\\cdot\\|\\right|,\\frac{\\delta}{2}\\right)\\geq\\gamma\\mathcal{N}\\left(\\mathbb{V}_{s,r},\\|\\!\\cdot\\!\\|,\\delta\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{K},\\|\\|\\cdot\\|\\right),\\frac{\\delta}{2}\\right)\\leq\\mathcal{N}\\left(\\mathbb{V}_{s,r},\\|\\vert\\cdot\\|\\vert,\\frac{\\delta}{8}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof of Lemma $I4.$ The bound in Equation (104) follows from the fact that $\\mathbb{K}\\subseteq\\mathbb{V}_{s,r}$ ,and (see Exercise 4.2.10 in [54]), ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{K},\\|\\|,\\frac{\\delta}{2}\\right)\\leq\\mathcal{N}\\left(\\mathbb{K},\\|\\|,\\frac{\\delta}{4}\\right)\\leq\\mathcal{N}\\left(\\mathbb{V}_{s,r},\\|\\|,\\frac{\\delta}{8}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To establish the bound in Equation (103), suppose that $\\mathbb{V}_{s,r}$ has a $\\delta$ packing set $\\mathcal{P}\\,=\\,\\{\\pmb{U}^{(i)}\\}_{i=1}^{M}$ under $\\lVert\\cdot\\rVert$ ,then ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\bigcup_{i=1}^{M}\\mathbb{B}\\left(U^{(i)},\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\subseteq\\mathbb{V}_{s,r}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbb{B}\\left(\\pmb{U}^{(i)},\\lVert\\rVert,\\lVert,\\frac{\\delta}{2}\\right):=\\left\\{\\pmb{U}\\in\\mathbb{V}_{s,r}:\\left\\lVert\\pmb{U}-\\pmb{U}^{(i)}\\rVert\\right\\rvert\\leq\\delta/2\\right\\}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $\\mathcal{P}$ is a $\\delta$ -packing set, the sets $\\mathbb{B}\\left(\\boldsymbol{U}^{(i)},\\|\\cdot\\|,\\delta/2\\right)$ for $i=1,2,\\dots,M$ are disjoint and we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{M}\\xi_{H}\\left(\\mathbb{B}\\left(U^{(i)},\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\right)=\\xi_{H}\\left(\\bigcup_{i=1}^{M}\\mathbb{B}\\left(U^{(i)},\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\right)\\leq\\xi_{H}(\\mathbb{V}_{s,r})=1.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "For any $\\boldsymbol{U},\\tilde{\\boldsymbol{U}}\\in\\mathbb{V}_{s,r}$ , there exists $\\mathbf{r}\\in\\mathbb{O}_{s}$ such that $\\mathbf{\\Delta}\\Gamma U=\\widetilde{U}$ . Since $\\lVert\\cdot\\rVert$ is invariant under $\\mathbb{O}_{s}$ ,we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{T}\\cdot\\mathbb{B}\\left(\\pmb{U},\\lVert\\lVert,\\rVert,\\frac{\\delta}{2}\\right)=\\mathbb{B}\\left(\\mathbf{I}\\pmb{U},\\lVert\\lVert\\cdot\\rVert\\lVert,\\frac{\\delta}{2}\\right)=\\mathbb{B}\\left(\\pmb{\\tilde{U}},\\lVert\\lVert\\cdot\\rVert\\lVert,\\frac{\\delta}{2}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Since $\\xi_{H}$ is invariant under left-orthogonal transformation, it follows that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\xi_{H}\\left(\\mathbb{B}\\left(U,\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\right)=\\xi_{H}\\left(\\mathbf{r}\\cdot\\mathbb{B}\\left(U,\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\right)=\\xi_{H}\\left(\\mathbb{B}\\left(\\Tilde{U},\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Therefore, all $\\lVert\\cdot\\rVert$ -balls with the same radius have the same Haar measure, which does not depend on the particular choice of $U$ due to the invariance properties of $\\lVert\\cdot\\rVert$ and $\\xi_{H}$ . We denote the Haar measure of a $\\lVert\\cdot\\rVert$ -balls with radius $\\delta/2$ as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\psi(\\delta/2):=\\xi_{H}\\left(\\mathbb{B}\\left(U,\\lVert\\cdot\\rVert,\\frac\\delta2\\right)\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It then follows from Equation (105) that ", "page_idx": 34}, {"type": "equation", "text": "$$\nM\\psi(\\delta/2)=\\sum_{i=1}^{M}\\xi_{H}\\left(\\mathbb{B}\\left(U^{(i)},\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\right)\\leq1,\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "from which we conclude that $M\\leq1/\\psi(\\delta/2)$ . Taking the maximal such $\\delta$ -packing set yields that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{V}_{s,r},\\Vert\\cdot\\Vert,\\delta\\right)\\leq\\frac{1}{\\psi(\\delta/2)}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Recall that $\\mathbb{K}$ is a subset of $\\mathbb{V}_{s,r}$ and $\\xi_{H}(\\mathbb{K})\\ge\\gamma$ . Suppose that $\\mathcal{C}_{\\mathbb{K}}=\\{V^{(i)}\\}_{i=1}^{N}$ is a $\\delta/2$ -covering set of $\\mathbb{K}$ under $\\lVert\\cdot\\rVert$ , then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{K}\\subseteq\\bigcup_{i=1}^{N}\\mathbb{B}\\left(V^{(i)},\\|\\cdot\\|,\\frac{\\delta}{2}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and by the subadditivity of measures, ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{N}\\xi_{H}\\left(\\mathbb{B}\\left(V^{(i)},\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\right)\\geq\\xi_{H}\\left(\\bigcup_{i=1}^{N}\\mathbb{B}\\left(V^{(i)},\\lVert\\cdot\\rVert,\\lVert,\\frac{\\delta}{2}\\right)\\right)\\geq\\xi_{H}(\\mathbb{K})\\geq\\gamma.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, it follows that ", "page_idx": 34}, {"type": "equation", "text": "$$\nN\\psi(\\delta/2)\\geq\\gamma\\implies N\\geq\\gamma\\mathcal{M}\\left(\\mathbb{V}_{s,r},\\lVert\\cdot\\rVert,\\delta\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Taking the maximal $\\delta/2$ -covering set yields that ", "page_idx": 34}, {"type": "equation", "text": "$$\nN\\left(\\mathbb{K},\\|\\cdot\\|,\\frac{\\delta}{2}\\right)\\geq\\gamma\\mathcal{M}\\left(\\mathbb{V}_{s,r},\\|\\cdot\\|\\right|,\\delta\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Finally, by Lemma 4.2.8 in [54], we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nM\\left(\\mathbb{K},\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right)\\geq N\\left(\\mathbb{K},\\lVert\\cdot\\rVert,\\frac{\\delta}{2}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{V}_{s,r},\\lVert\\cdot\\rVert,\\delta\\right)\\ge\\mathcal{N}\\left(\\mathbb{V}_{s,r},\\lVert\\cdot\\rVert,\\delta\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Combining the three displays above yields Equation (103), completing the proof. ", "page_idx": 34}, {"type": "text", "text": "Lemma 15 controls the Haar measure of $\\mathbb{K}(s,r,\\sqrt{\\mu r/n})$ . We remind the reader that $\\mathbb{K}(s,r,\\sqrt{\\mu r/n})$ is defined in Equation (20). ", "page_idx": 34}, {"type": "text", "text": "Lemma 15. For $n/\\mu\\geq\\operatorname*{max}\\{4,r\\}$ and $s\\geq(12n/\\mu)\\log(12n/\\mu)$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\xi_{H}(\\mathbb{K}(s,r,\\sqrt{\\mu r/n}))\\geq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof of Lemma 15. Define the set ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{L}\\left(s,r,\\sqrt{\\frac{\\mu}{n}}\\right)=\\left\\{U\\in\\mathbb{V}_{s,r}:\\|U\\|_{\\infty}\\geq\\sqrt{\\frac{\\mu}{n}}\\right\\},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\|U\\|_{\\infty}$ denotes the entrywise $\\ell_{\\infty}$ norm (i.e., $\\|\\pmb{U}\\|_{\\infty}:=\\operatorname*{max}_{i,j}|U_{i,j}|)$ . In what follows, we omit the parameters and abbreviate $\\mathbb{L}(s,r,\\sqrt{\\mu/n})$ to $\\mathbb{L}$ . Noting that for any $U\\,\\in\\,\\mathbb{L}^{c}$ ,we have $\\lVert U\\rVert_{\\infty}\\leq\\sqrt{\\mu/n}$ , which implies that $\\|U\\|_{2,\\infty}\\leq\\sqrt{\\mu r/n}$ . Therefore, $\\mathbb{L}^{c}\\subseteq\\mathbb{K}(s,r,\\sqrt{\\mu r/n})$ and ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\xi_{H}\\left(\\mathbb{K}(s,r,\\sqrt{\\mu r/n})\\right)\\geq1-\\xi_{H}\\left(\\mathbb{L}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{L}=\\bigcup_{i=1}^{r}\\left\\{U\\in\\mathbb{V}_{s,r}:\\|U_{\\cdot,i}\\|_{\\infty}\\geq\\sqrt{\\frac{\\mu}{n}}\\right\\},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "it follows that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\xi_{H}\\left(\\mathbb{L}\\right)\\leq r\\,\\xi_{H}\\left(\\left\\{U\\in\\mathbb{V}_{s,r}:\\|U_{\\cdot,1}\\|_{\\infty}\\geq\\sqrt{\\frac{\\mu}{n}}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Since $U$ is distributed according to the Haar measure on $\\mathbb{V}_{s,r}$ , the column $U_{\\cdot,1}\\in\\mathbb{R}^{s}$ is uniformly distribution on $\\mathbb{S}^{s-1}$ . As a result, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\xi_{H}\\left(\\left\\{U\\in\\mathbb{V}_{s,r}:\\|U_{\\cdot,1}\\|_{\\infty}\\geq\\sqrt{\\frac{\\mu}{n}}\\right\\}\\right)=\\mathbb{P}\\left(\\frac{\\|g\\|_{\\infty}}{\\|g\\|_{2}}\\geq\\sqrt{\\frac{\\mu}{n}}\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\pmb{g}\\sim N(0,\\pmb{I}_{s})$ (see Lemma 10.1 in [44]). Combining the above two displays, ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\xi_{H}\\left(\\mathbb{L}\\right)\\leq r\\,\\mathbb{P}\\left({\\frac{\\|{\\pmb g}\\|_{\\infty}}{\\|{\\pmb g}\\|_{2}}}\\geq{\\sqrt{\\frac{\\mu}{n}}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We introduce two events ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0,1}:=\\left\\{\\|g\\|_{2}^{2}\\geq\\frac{3n}{\\mu}\\log s\\right\\}\\quad\\mathrm{and}\\quad\\mathcal{E}_{0,2}:=\\left\\{\\|g\\|_{\\infty}\\leq\\sqrt{3\\log s}\\right\\}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By Lemma 10.2 in [44], we have for all $t>0$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\pmb{\\mathscr{g}}\\|_{2}^{2}\\leq s-2\\sqrt{s t}\\right)\\leq\\exp\\left(-t\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Taking $t=2\\log s$ yields that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\|\\pmb{g}\\|_{2}^{2}\\leq s-2\\sqrt{2s\\log s}\\right)\\leq\\frac{1}{s^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "When $n\\geq4\\mu$ and $s\\geq(12n/\\mu)\\log{(12n/\\mu)}$ , one can verify that ", "page_idx": 35}, {"type": "equation", "text": "$$\ns-2\\sqrt{2s\\log s}\\geq s/2\\geq\\frac{3n}{\\mu}\\log s.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, it follows that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\pmb{\\mathscr{E}}_{0,1}^{c})\\leq\\mathbb{P}\\left(\\|\\pmb{g}\\|_{2}^{2}\\leq s-2\\sqrt{2s\\log s}\\right)\\leq\\frac{1}{s^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By standard Gaussian concentration inequalities and the union bound, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathscr{E}_{0,2}^{c}\\right)=\\mathbb{P}\\left(\\|g\\|_{\\infty}\\geq\\sqrt{3\\log s}\\right)\\leq2s\\exp\\left(-3\\log s\\right)=\\frac{2}{s^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "On the event $\\mathcal{E}_{0,1}\\cap\\mathcal{E}_{0,2}$ , one has ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\frac{\\|\\pmb{g}\\|_{\\infty}}{\\|\\pmb{g}\\|_{2}}\\le\\frac{\\sqrt{3\\log s}}{\\sqrt{3n\\log s/\\mu}}=\\sqrt{\\frac{\\mu}{n}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining Equations (108) and Equations (109), it follows that ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{\\|\\pmb{g}\\|_{\\infty}}{\\|\\pmb{g}\\|_{2}}\\ge\\sqrt{\\frac{\\mu}{n}}\\right)\\le\\mathbb{P}\\left(\\mathcal{E}_{0,1}^{c}\\cup\\mathcal{E}_{0,2}^{c}\\right)\\le\\frac{3}{s^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Applying this bound to Equation (107), we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\xi_{H}(\\mathbb{L})\\leq r\\mathbb{P}\\left(\\frac{\\|\\pmb{g}\\|_{\\infty}}{\\|\\pmb{g}\\|_{2}}\\geq\\sqrt{\\frac{\\mu}{n}}\\right)\\leq\\frac{3r}{s^{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "By the assumption that $n/\\mu\\geq\\operatorname*{max}\\{4,r\\}$ for any $r\\geq1$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\ns\\geq(12n/\\mu)\\log(12n/\\mu)>12r>\\sqrt{6r}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Combining the above bound with Equation (110), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\xi_{H}(\\mathbb{L})\\leq{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Finally, applying this upper bound to Equation (106), we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\xi_{H}(\\mathbb{K}(s,r,\\sqrt{\\mu r/n}))\\geq1-\\xi_{H}(\\mathbb{L})\\geq{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "as desired. ", "page_idx": 35}, {"type": "text", "text": "F.2Proof of Lemma 4 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof. Set $s~=~\\lceil c_{0}^{2}r/8e^{2}\\delta^{2}\\rceil$ . Under our upper bound assumption on $\\delta$ in Equation (21) and $n/\\mu\\geq\\operatorname*{max}\\{4,r\\}$ ,we have ", "page_idx": 36}, {"type": "equation", "text": "$$\ns\\geq\\frac{c_{0}^{2}r}{8e^{2}}\\cdot\\frac{96e^{2}n\\log(12n/\\mu)}{c_{0}^{2}\\mu r}=(12n/\\mu)\\log\\left(12n/\\mu\\right)>12r,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "so that we always have ", "page_idx": 36}, {"type": "equation", "text": "$$\ns\\geq\\operatorname*{max}\\left\\{\\frac{12n}{\\mu}\\log\\left(\\frac{12n}{\\mu}\\right),\\frac{c_{0}^{2}r}{8e^{2}\\delta^{2}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Note that owing to our assumptions that $\\mu\\,\\geq\\,12\\log(12n)$ and $\\delta^{2}\\,\\geq\\,c_{0}^{2}r/8e^{2}n$ ,we have $s\\,\\leq\\,n$ Consider the subset ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n}):=\\{U\\in\\mathbb{K}_{r,\\mu}:U_{i,\\cdot}=0,\\;\\mathrm{for}\\;s+1\\leq i\\leq n\\}\\,,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which is a subset related (but different) to $\\mathbb{K}(s,r,\\sqrt{\\mu r/n})$ previously considered in Lemma 15. Our lower bound relies on the key observation that ", "page_idx": 36}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{M}(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta)\\geq\\mathcal{M}(\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n}),d_{2,\\infty},2\\delta)\\geq\\mathcal{M}(\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n}),d_{\\mathrm{F}},2\\sqrt{s}\\delta),}\\end{array}$ (113) where the first inequality follows from Exercise 4.2.10 in [54], and the second inequality follows from the fact that for any $U_{1},U_{2}\\in\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n})$ wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\nd_{2,\\infty}(\\pmb{U}_{1},\\pmb{U}_{2})\\geq\\frac{1}{\\sqrt{s}}d_{\\mathrm{F}}(\\pmb{U}_{1},\\pmb{U}_{2}).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Starting from Equation (113), we proceed to obtain a $\\sqrt{s}\\delta$ -packing for $\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n})$ under $d_{\\mathrm{{F}}}$ Since any $U\\in\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n})$ can be uniquely identified with an element in $\\mathbb{K}(s,r,\\sqrt{\\mu r/n})$ by restricting it to its first $s$ rows. By the assumption that $n/\\mu\\geq\\operatorname*{max}\\{4,r\\}$ and the lower bound on $s$ in Equation (111), we verify that the conditions of Lemma 15 are all satisfied. Thus, following directly from the lower bound of $\\xi_{H}(\\mathbb{K}(s,r,\\sqrt{\\mu r/n}))$ in Lemma 15 and Lemma 14, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n}),d_{\\mathrm{F}},2\\sqrt{s}\\delta\\right)\\geq\\frac{1}{2}\\mathcal{N}\\left(\\mathbb{V}_{s,r},d_{\\mathrm{F}},4\\sqrt{s}\\delta\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Applying the lower bound in Lemma 13 to the right hand side of the above bound, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n}),d_{\\mathrm{F}},2\\sqrt{s}\\delta\\right)\\geq\\frac{1}{2}\\left(\\frac{c_{0}\\sqrt{r}}{4\\sqrt{s}\\delta}\\right)^{r(s-r)}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Under our upper bound assumption on $\\delta$ in Equation (21) and $n/\\mu\\geq\\operatorname*{max}\\{4,r\\}$ ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\frac{c_{0}^{2}r}{384e^{2}\\delta^{2}}}\\geq{\\frac{c_{0}^{2}r}{384e^{2}}}\\cdot{\\frac{96e^{2}n\\log(12n/\\mu)}{c_{0}^{2}\\mu r}}={\\frac{n}{4\\mu}}\\log(12n/\\mu)\\geq1,\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "from our choice of $s$ , it follows that ", "page_idx": 36}, {"type": "equation", "text": "$$\ns\\leq\\frac{c_{0}^{2}r}{8e^{2}\\delta^{2}}+1\\leq\\frac{c_{0}^{2}r}{8e^{2}\\delta^{2}}+\\frac{c_{0}^{2}r}{384e^{2}\\delta^{2}}\\leq\\frac{c_{0}^{2}r}{7e^{2}\\delta^{2}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "which implies $\\sqrt{s}\\delta\\le(\\sqrt{7}e)^{-1}c_{0}\\sqrt{r}$ . Thus, it follows from Equation (114) that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n}),d_{\\mathrm{F}},2\\sqrt{s}\\delta\\right)\\geq\\frac{1}{2}\\left(\\frac{\\sqrt{7}e}{4}\\right)^{r(s-r)}\\geq\\frac{1}{2}\\left(\\frac{\\sqrt{7}e}{4}\\right)^{\\frac{r s}{2}},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality follows from Equation (111). Noting that $\\sqrt{7}e/4\\ge\\sqrt{e}$ ,wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n}),d_{\\mathrm{F}},2\\sqrt{s}\\delta\\right)\\geq\\frac{1}{2}\\exp\\left(\\frac{r s}{4}\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Applying Equation (113) followed by Equation (112), we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta\\right)\\geq\\mathcal{M}\\left(\\mathbb{K}_{s}(n,r,\\sqrt{\\mu r/n}),d_{\\mathrm{F}},2\\sqrt{s}\\delta\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\exp\\left(\\operatorname*{max}\\left\\{\\frac{3n r}{\\mu}\\log\\left(\\frac{12n}{\\mu}\\right),\\frac{c_{0}^{2}r^{2}}{32e^{2}\\delta^{2}}\\right\\}\\right)\\geq\\frac{1}{2}\\exp\\left(\\frac{c_{0}^{2}r^{2}}{32e^{2}\\delta^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Taking the logarithm on both sides of the above display yields Equation (22)) ", "page_idx": 36}, {"type": "text", "text": "GUpper bounds of metric entropy under $d_{2,\\infty}$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "To provide a complete picture, Lemma 16 establishes an upper bound on the packing $\\delta$ -entropy Of $\\mathbb{K}_{r,\\mu}$ under $d_{2,\\infty}$ that matches the lower bound in Lemma 4 up to log-factors when $\\delta$ satisfies Equation (21). ", "page_idx": 37}, {"type": "text", "text": "Lemma 16. Assume that $n\\,\\geq\\,\\mu r$ for $r\\,\\in\\,[n]$ and $\\mu\\,>\\,0$ .For all n suffciently large and any $\\sqrt{4/(n-1)}<\\delta<\\sqrt{\\mu r/n}$ we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\log\\mathcal{M}\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta\\right)\\lesssim\\frac{r^{2}}{\\delta^{2}}\\log n.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Proof of Lemma $I6$ We obtain an upper bound of $\\mathcal{M}(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta)$ by finding an upper bound of ${\\cal N}(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta)$ . Since for any $U_{1}$ and $U_{2}\\in\\mathbb{R}^{n\\times r}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\nd_{2,\\infty}\\left(\\pmb{U}_{1},\\pmb{U}_{2}\\right)\\leq\\|\\pmb{U}_{1}-\\pmb{U}_{2}\\|_{2,\\infty},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{N\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta\\right)\\leq N\\left(\\mathbb{K}_{r,\\mu},\\|\\cdot\\|_{2,\\infty},\\delta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, it suffices to obtain an upper bound for $\\mathcal{N}(\\mathbb{K}_{r,\\mu},\\Vert\\cdot\\Vert_{2,\\infty},\\delta)$ instead. Let ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{T}:=\\left\\{\\pmb{u}\\geq0:\\pmb{u}\\in\\sqrt{r}\\mathbb{S}^{n-1},\\|\\pmb{u}\\|_{\\infty}\\leq1\\right\\}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "For any $U\\in\\mathbb{K}_{r,\\mu}$ , noting that $\\sqrt{\\mu r/n}\\leq1$ , we define a mapping $h:\\mathbb{K}_{r,\\mu}\\to\\mathbb{T}$ given by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(\\pmb{U}):=(\\|\\pmb{U}_{1,\\cdot}\\|_{2},\\|\\pmb{U}_{2,\\cdot}\\|_{2},\\cdot\\cdot\\cdot,\\|\\pmb{U}_{n,\\cdot}\\|_{2})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Indeed, $h(\\pmb{U})\\in\\mathbb{T}$ since by definition, we have $h(\\pmb{U})\\geq0$ $\\|h(\\boldsymbol{U})\\|_{2}=\\sqrt{r}$ and $\\|h(\\boldsymbol{U})\\|_{\\infty}\\leq1$ ", "page_idx": 37}, {"type": "text", "text": "We pause to give a roadmap of our proof. Recall that for a set $\\mathbb{K}$ , its exterior $\\delta$ -covering is a $\\delta$ -covering, except that the exterior $\\delta$ -covering allows elements not in $\\mathbb{K}$ to form the covering (see Exercise 4.2.9 in [54]). To obtain $\\mathcal{N}(\\mathbb{K}_{r,\\mu},\\Vert\\cdot\\Vert_{2,\\infty},\\delta)$ , we will explicitly construct an exterior $\\delta$ -covering of $\\mathbb{K}_{r,\\mu}$ under $\\|\\cdot\\|_{2,\\infty}$ . To do so, we proceed in three steps. In the first step, we construct an exterior $\\delta$ -covering $\\mathcal{C}$ of $\\mathbb{T}$ under $\\|\\cdot\\|_{\\infty}$ . In the second step, we divide the set $\\mathbb{K}_{r,\\mu}$ into separate subsets $\\{\\mathbb{U}_{v}\\}_{v\\in\\mathbb{T}}$ via the mapping $h:\\mathbb{K}_{r,\\mu}\\to\\mathbb{T}$ . In this way, each element $\\pmb{v}$ in $\\mathbb{T}$ is associated with a subset $\\mathbb{U}_{\\boldsymbol{v}}\\subseteq\\mathbb{K}_{r,\\mu}$ . For every $\\pmb{v}\\in\\mathcal{C}$ (the exterior $(\\delta/2)$ -covering set of $\\mathbb{T}$ ), we construct an exterior $(\\delta/2)$ -covering $\\mathcal{C}_{v}$ of $\\mathbb{U}_{v}$ under $\\|\\cdot\\|_{2,\\infty}.$ In the last step, we take the union of $\\mathcal{C}_{v}$ over $\\pmb{v}\\in\\mathcal{C}$ to form a set $\\mathcal{C}_{\\mathbb{K}}$ , and show this set is an exterior $\\delta$ -covering set for $\\mathbb{K}_{r,\\mu}$ under $\\|\\cdot\\|_{2,\\infty}$ . Finally, we control the cardinality of $\\mathcal{C}_{\\mathbb{K}}$ to obtain an upper bound of $\\mathcal{N}(\\mathbb{K}_{r,\\mu},\\Vert\\cdot\\Vert_{2,\\infty},\\delta)$ , which in turn will yield our desired upper bound on $\\mathcal{M}(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta)$ ", "page_idx": 37}, {"type": "text", "text": "Step 1. Construct an exterior $(\\delta/2)$ -covering set of $\\mathbb{T}$ under $\\|\\cdot\\|_{\\infty}$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Recall that $\\mathbb{T}$ is given in Equation (118). Let $s~=~\\lceil4/\\delta^{2}\\rceil$ . Under the assumption that $\\delta\\ >$ $\\sqrt{4/(n-1)}$ \uff0cwehave $s~\\leq~n$ .We consider the nonnegative integer solutions to the indeterminate equation ", "page_idx": 37}, {"type": "equation", "text": "$$\nz_{1}+z_{2}+\\cdot\\cdot\\cdot+z_{n}=r s,\\quad\\|z\\|_{\\infty}\\leq s+1\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and let ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}:=\\left\\{\\sqrt{\\frac{1}{s}}\\left(\\sqrt{z_{1}},\\cdot\\cdot\\cdot,\\sqrt{z_{n}}\\right):\\mathbf{z}\\mathrm{~is~a~solution~of~Equation~}(119)\\right.\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By Lemma 17, we have $\\mathcal{C}$ forms an exterior $\\delta/2$ -covering set of $\\mathbb{T}$ under $\\|\\cdot\\|_{\\infty}$ . Since without the constraint $\\|z\\|_{\\infty}\\leq s+1$ , Equation (119) has $\\binom{n+r s-1}{r s}$ solutions, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n|\\mathcal{C}|\\le\\binom{n+r s-1}{r s}\\le\\left(\\frac{e(n+r s)}{r s}\\right)^{r s}=\\left(\\frac{e(n+r\\left[4/\\delta^{2}\\right])}{r\\left[4/\\delta^{2}\\right]}\\right)^{r\\left[4/\\delta^{2}\\right]}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Step 2. Construct an exterior $(\\delta/2)$ -covering set of $\\mathbb{U}_{v}$ under $\\|\\cdot\\|_{2,\\infty}$ ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "For a given $\\pmb{v}\\in\\mathcal{C}$ , consider the subset ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{U}_{\\pmb{v}}:=\\{{\\pmb{U}}\\in\\mathbb{K}_{r,\\mu}:h({\\pmb{U}})={\\pmb{v}}\\}\\,,\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "our next step is to construct an exterior $(\\delta/2)$ -covering set under $\\|\\cdot\\|_{2,\\infty}$ for $\\mathbb{U}_{v}$ . For every $i\\in[n]$ consider a $(\\delta/2)$ -covering set $\\mathcal{C}_{v_{i}}$ of $v_{i}\\mathbb{S}^{r-1}$ under the $\\ell_{2}$ norm. By Corollary 4.2.13 in [54], ", "page_idx": 38}, {"type": "equation", "text": "$$\n|\\mathcal{C}_{v_{i}}|\\leq\\left(\\frac{6v_{i}}{\\delta}\\right)^{r}\\quad\\mathrm{~for~all~}i\\in[n].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Consider the set ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{C}_{v}:=\\left\\{\\Theta\\in\\mathbb{R}^{n\\times r}:\\Theta=(\\pmb{\\theta}_{1},\\pmb{\\theta}_{2},\\dots,\\pmb{\\theta}_{n})^{\\top},\\pmb{\\theta}_{i}\\in\\mathcal{C}_{v_{i}}\\mathrm{~for~}i\\in[n]\\right\\}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We claim that $\\mathcal{C}_{v}$ is an exterior $(\\delta/2)$ -covering set for $\\mathbb{U}_{v}$ under $\\|\\cdot\\|_{2,\\infty}$ . To see this, note that for any $U\\in\\mathbb{U}_{v}$ and any $i\\in[n]$ , since $U_{i,\\cdot}^{\\top}\\in v_{i}\\mathbb{S}^{r-1}$ and $\\mathcal{C}_{v_{i}}$ is a $(\\delta/2)$ -covering of $v_{i}\\mathbb{S}^{r-1}$ under the $\\ell_{2}$ norm, there exists a $\\pmb{\\theta}_{i}\\in\\mathcal{C}_{v_{i}}$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\|\\pmb{\\theta}_{i}-\\pmb{U}_{i,\\cdot}^{\\top}\\right\\|_{2}\\leq\\frac\\delta2,\\quad\\mathrm{~for~all~}i\\in[n].\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Let $\\Theta=(\\pmb{\\theta}_{1},\\pmb{\\theta}_{2},\\dots,\\pmb{\\theta}_{n})^{\\top}\\in\\mathcal{C}_{\\pmb{v}}$ . It follows that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lVert\\Theta-U\\rVert_{2,\\infty}=\\operatorname*{max}_{i\\in[n]}\\leq\\left\\lVert\\pmb\\theta_{i}-\\pmb U_{i,\\cdot}^{\\top}\\right\\rVert_{2}\\leq\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since $\\pmb{v}\\in\\mathcal{C}$ , from Equation (119), we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\lVert\\pmb{v}\\rVert_{0}\\leq r s\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "as any solution $_{\\textit{z}}$ to Equation (119) has at most $r s$ nonzero entries. By Equations (119) and (120), we also have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\|v\\|_{\\infty}\\leq\\frac{\\sqrt{s+1}}{\\sqrt{s}}\\leq2.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By Equations (123) and (124), ", "page_idx": 38}, {"type": "equation", "text": "$$\n|\\mathcal{C}_{v}|\\le\\prod_{i=1}^{n}\\left(\\frac{6v_{i}}{\\delta}\\right)^{r}=\\prod_{i:v_{i}>0}\\left(\\frac{6v_{i}}{\\delta}\\right)^{r}\\le\\left(\\frac{12}{\\delta}\\right)^{s r^{2}}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality follows from Equations (125) and (126). By the choice of $s=\\lceil4/\\delta^{2}\\rceil$ ,we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\vert\\mathcal{C}_{v}\\right\\vert\\leq\\left(\\frac{12}{\\delta}\\right)^{r^{2}\\left\\lceil4/\\delta^{2}\\right\\rceil}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Step 3. Construct an exterior $\\delta$ -covering set for $\\mathbb{K}_{r,\\mu}$ under $\\|\\cdot\\|_{2,\\infty}$ ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Consider the set ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{C}_{\\mathbb{K}}:=\\bigcup_{\\boldsymbol{v}\\in\\mathcal{C}}\\mathcal{C}_{\\boldsymbol{v}},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "we claim that this set forms an exterior $\\delta$ -covering set for $\\mathbb{K}_{r,\\mu}$ under $\\|\\cdot\\|_{2,\\infty}$ ", "page_idx": 38}, {"type": "text", "text": "For any $U\\in\\mathbb{K}_{r,\\mu}$ , since $\\mathcal{C}$ is an exterior $(\\delta/2)$ -covering of $\\mathbb{T}$ under $\\|\\cdot\\|_{\\infty}$ , we can find $\\pmb{v}\\in\\mathcal{C}$ such that $\\|h(\\boldsymbol{U})-\\boldsymbol{v}\\|_{\\infty}\\leq\\delta/2$ Consider a matrix $\\b{\\widetilde U}\\in\\mathbb{R}^{n\\times r}$ with rows given by ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{U}}_{i,\\cdot}=\\left\\{\\!\\!\\begin{array}{l l}{v_{i}\\pmb{U}_{i,\\cdot}/h_{i}(\\pmb{U})}&{\\mathrm{~if~}h_{i}(\\pmb{U})>0,}\\\\ {\\pmb{\\theta}_{i}^{\\top}}&{\\mathrm{~if~}h_{i}(\\pmb{U})=0,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for all $i\\in[n]$ , where $\\theta_{i}$ is an arbitrary element of $\\mathcal{C}_{v_{i}}$ . Recall the definition of $\\mathbb{U}_{v}$ from Equation (122). By construction, $\\tilde{U}\\in\\mathbb{U}_{v}$ , which implies that there exists $\\Theta\\in\\mathcal{C}_{v}\\subseteq\\mathcal{C}_{\\mathbb{K}}$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\lVert\\Theta-\\widetilde{U}\\right\\rVert_{2,\\infty}\\leq\\frac{\\delta}{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we use the fact that $\\mathcal{C}_{v}$ defined in Equation (124) is an exterior $(\\delta/2)$ -covering of $\\mathbb{U}_{v}$ . For $i\\in[n]$ such that $h_{i}(\\pmb{U})>0$ ,we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\boldsymbol{U}}_{i,\\cdot}-\\boldsymbol{U}_{i,\\cdot}\\right\\|_{2}=\\left|\\frac{v_{i}}{h_{i}(\\boldsymbol{U})}-1\\right|\\|\\boldsymbol{U}_{i,\\cdot}\\|_{2}=|v_{i}-h_{i}(\\boldsymbol{U})|\\leq\\frac{\\delta}{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and for $i\\in[n]$ such that $h_{i}(\\pmb{U})=0$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\|\\widetilde{\\boldsymbol{U}}_{i,\\cdot}-\\boldsymbol{U}_{i,\\cdot}\\right\\|_{2}=\\|\\pmb{\\theta}_{i}\\|_{2}=v_{i}=|v_{i}-h_{i}(\\boldsymbol{U})|\\leq\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Combining the above two displays, it follows that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left\\lVert\\widetilde{\\boldsymbol{U}}-\\boldsymbol{U}\\right\\rVert_{2,\\infty}=\\operatorname*{max}_{i\\in[n]}\\left\\lVert\\widetilde{\\boldsymbol{U}}_{i,\\cdot}-\\boldsymbol{U}_{i,\\cdot}\\right\\rVert_{2}\\le\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Thus, we have found $\\Theta\\in\\mathcal{C}_{\\mathbb{K}}$ such that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\Theta-{\\cal U}\\|_{2,\\infty}\\leq\\|\\Theta-\\widetilde{{\\cal U}}\\|_{2,\\infty}+\\|\\widetilde{{\\cal U}}-{\\cal U}\\|_{2,\\infty}\\leq\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and it follows that $\\mathcal{C}_{\\mathbb{K}}$ is an exterior $\\delta$ -covering set for $\\mathbb{K}_{r,\\mu}$ under $\\|\\cdot\\|_{2,\\infty}$ ", "page_idx": 39}, {"type": "text", "text": "Step 4. An upper bound on $\\mathcal{M}(\\mathbb{K}_{r,\\mu},\\Vert\\cdot\\Vert_{2,\\infty},\\delta)$ ", "page_idx": 39}, {"type": "text", "text": "Recalling the definition of $\\mathcal{C}_{\\mathbb{K}}$ from Equation (128), Equations (121) and (127) imply that ", "page_idx": 39}, {"type": "equation", "text": "$$\n|\\mathcal{C}_{\\mathbb{K}}|\\le\\sum_{v\\in\\mathcal{C}}|\\mathcal{C}_{v}|\\le\\left(\\frac{e\\left(n+r\\left\\lceil4/\\delta^{2}\\right\\rceil\\right)}{r\\left\\lceil4/\\delta^{2}\\right\\rceil}\\right)^{r\\left\\lceil4/\\delta^{2}\\right\\rceil}\\cdot\\left(\\frac{12}{\\delta}\\right)^{r^{2}\\left\\lceil4/\\delta^{2}\\right\\rceil}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Using this bound and the fact that $\\mathcal{C}_{\\mathbb{K}}$ is an exterior $\\delta$ -covering set for $\\mathbb{K}_{r,\\mu}$ under $\\|\\cdot\\|_{2,\\infty}$ , Exercise 4.2.9 in [54] implies that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{N}(\\mathbb{K}_{r,\\mu},\\|\\cdot\\|_{2,\\infty},2\\delta)\\leq|\\mathcal{C}_{\\mathbb{K}}|\\leq\\left(\\displaystyle\\frac{e\\left(n+r\\left[4/\\delta^{2}\\right]\\right)}{r\\left[4/\\delta^{2}\\right]}\\right)^{r\\left[4/\\delta^{2}\\right]}\\left(\\frac{12}{\\delta}\\right)^{r^{2}\\left\\lceil4/\\delta^{2}\\right\\rceil}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left(\\displaystyle\\frac{e n\\delta^{2}}{4r}+e\\right)^{\\frac{4r}{\\delta^{2}}+r}\\left(\\frac{12}{\\delta}\\right)^{\\frac{4r^{2}}{\\delta^{2}}+r^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Combining this with with Equation (117) and Lemma 4.2.8 in [54], we conclude that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathcal{M}\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta\\right)\\leq\\mathcal{N}\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta/2\\right)\\leq\\left(\\frac{e n\\delta^{2}}{64r}+e\\right)^{\\frac{64r}{\\delta^{2}}+r}\\left(\\frac{48}{\\delta}\\right)^{\\frac{64r^{2}}{\\delta^{2}}+r^{2}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Taking the logarithm on both sides of Equation (129), we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\mathcal{M}\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta\\right)\\leq r\\left(\\frac{64+\\delta^{2}}{\\delta^{2}}\\right)\\log\\left(\\frac{e n\\delta^{2}}{64r}+e\\right)+r^{2}\\left(\\frac{64+\\delta^{2}}{\\delta^{2}}\\right)\\log\\left(\\frac{48}{\\delta}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{65r}{\\delta^{2}}\\log\\left(\\frac{e\\mu}{64}+e\\right)+\\frac{65r^{2}}{\\delta^{2}}\\log\\left(\\frac{48}{\\delta}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality follows from $\\delta<\\sqrt{\\mu r/n}\\leq1$ Under the assumption that $\\delta>\\sqrt{4/n}$ and $\\mu r\\leq n$ , the right hand side of the above display can be further written as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\log\\mathcal{M}\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta\\right)\\leq\\frac{65r}{\\delta^{2}}\\log\\left(\\frac{e n}{64}+e\\right)+\\frac{65r^{2}}{\\delta^{2}}\\log\\left(24\\sqrt{n}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "yields Equation (116) for all $n$ sufficiently large, completing the proof. ", "page_idx": 39}, {"type": "text", "text": "Recall the definition of $\\mathbb{T}$ in Equation (118) and $\\mathcal{C}$ defined in Equation (120), we have Lemma 17. ", "page_idx": 39}, {"type": "text", "text": "Lemma 17. Let the set $\\mathbb{T}\\subseteq{\\sqrt{r}}\\mathbb{S}^{n-1}$ be as defined in Equation (118), and let $^h$ be an arbitrary element in T. Then there exists $\\pmb{v}\\in\\mathcal{C}$ such that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\|h-v\\|_{\\infty}\\leq\\frac{1}{\\sqrt{s}}\\leq\\frac{\\delta}{2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where 8 is given in Lemma $I6$ and $s=\\left\\lceil4/\\delta^{2}\\right\\rceil$ ", "page_idx": 39}, {"type": "text", "text": "Proof. The inequality $1/\\sqrt{s}\\le\\delta/2$ in Equation (130) follows directly from $s=\\left\\lceil4/\\delta^{2}\\right\\rceil$ . To construct a $\\pmb{v}\\in\\mathcal{C}$ satisfying Equation (130), we first construct a $\\zeta\\in\\mathbb{R}^{n}$ and then make a slight modification to $\\zeta$ to obtain $\\pmb{v}$ \uff1a ", "page_idx": 40}, {"type": "text", "text": "To construct $\\zeta$ , we set $\\zeta_{1}^{2}$ to be either $\\left\\lfloor h_{1}^{2}s\\right\\rfloor/s$ or $\\left\\lceil h_{1}^{2}s\\right\\rceil/s$ , and follow the recursive procedure to obtain an entrywise positive $\\zeta\\geq0$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\zeta_{i}^{2}=\\left\\{\\left\\lfloor h_{i}^{2}s\\right\\rfloor/s\\quad\\mathrm{if}\\,\\,\\sum_{j=1}^{i-1}h_{j}^{2}-\\sum_{j=1}^{i-1}\\zeta_{j}^{2}<0,\\right.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for $2\\leq i\\leq n$ . The construction given in Equation (131) guarantees that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left\\|h-\\zeta\\right\\|_{\\infty}\\leq\\operatorname*{max}_{i\\in[n]}\\left\\{h_{i}-\\sqrt{\\left\\lfloor h_{i}^{2}s\\right\\rfloor/s},\\sqrt{\\left\\lceil h_{i}^{2}s\\right\\rceil/s}-h_{i}\\right\\}\\leq\\frac{1}{\\sqrt{s}},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the last inequality follows from the fact that ${\\sqrt{a}}-{\\sqrt{b}}\\leq{\\sqrt{a-b}}$ for any $a\\ge b>0$ Nowwe show that following the procedure in Equation (131), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{k}\\zeta_{i}^{2}-\\sum_{i=1}^{k}h_{i}^{2}\\right|\\leq\\frac{1}{s},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for all $k\\in[n]$ . We prove the above bound by induction on $k\\in[n]$ .When $k=1$ , we trivially have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\zeta_{1}^{2}-h_{1}^{2}\\right|\\leq\\frac{1}{s}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For the inductive step, suppose that for $k>1$ ,wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{k-1}h_{i}^{2}-\\sum_{i=1}^{k-1}\\zeta_{i}^{2}\\right|\\leq\\frac{1}{s}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "By definition in Equation (131), if ", "page_idx": 40}, {"type": "equation", "text": "$$\n0\\leq\\sum_{i=1}^{k-1}h_{i}^{2}-\\sum_{i=1}^{k-1}\\zeta_{i}^{2}\\leq\\frac{1}{s},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then ", "page_idx": 40}, {"type": "equation", "text": "$$\n-\\frac{1}{s}\\leq\\sum_{i=1}^{k-1}h_{i}^{2}-\\sum_{i=1}^{k-1}\\zeta_{i}^{2}+h_{k}^{2}-\\frac{\\left\\lceil h_{k}^{2}s\\right\\rceil}{s}\\leq\\frac{1}{s}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Thus, $\\zeta_{k}^{2}=\\left\\lceil h_{k}^{2}s\\right\\rceil/s$ satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{k}h_{i}^{2}-\\sum_{i=1}^{k}\\zeta_{i}^{2}\\right|\\leq\\frac{1}{s}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "If, contrary to Equation (134), we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n-\\frac{1}{s}\\leq\\sum_{i=1}^{k-1}h_{i}^{2}-\\sum_{i=1}^{k-1}\\zeta_{i}^{2}<0,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then ", "page_idx": 40}, {"type": "equation", "text": "$$\n-\\frac{1}{s}\\leq\\sum_{i=1}^{k-1}h_{i}^{2}-\\sum_{i=1}^{k-1}\\zeta_{i}^{2}+h_{k}^{2}-\\frac{\\left\\lfloor h_{k}^{2}s\\right\\rfloor}{s}\\leq\\frac{1}{s},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and thus $\\zeta_{k}^{2}=\\left\\lfloor h_{k}^{2}s\\right\\rfloor/s$ again satisfies ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{k}h_{i}^{2}-\\sum_{i=1}^{k}\\zeta_{i}^{2}\\right|\\leq\\frac{1}{s}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Therefore, Equation (133) holds for all $k\\in[n]$ . Taking $k=n$ in Equation (133) and by the fact that $\\|h\\|_{2}=r$ , we obtain that ", "page_idx": 40}, {"type": "equation", "text": "$$\n|s\\|\\zeta\\|_{2}^{2}-s r\\|\\leq1.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Following Equation (131), we also have that $s\\zeta_{i}^{2}$ is an integer for every $i\\,\\in\\,[n]$ . Combined with Equation (135), we have a stronger statement that ", "page_idx": 41}, {"type": "equation", "text": "$$\n(s\\|\\zeta\\|_{2}^{2}-s r)\\in\\{-1,0,1\\}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "If $(s\\|\\zeta\\|_{2}^{2}-s r)=0$ , setting $\\pmb{v}=\\zeta$ already guarantees that $\\|\\pmb{v}\\|_{2}^{2}\\,=\\,r$ and $\\|\\pmb{v}-\\pmb{h}\\|_{\\infty}\\leq1/\\sqrt{s}$ Otherwise, if $(s\\|\\zeta\\|_{2}^{2}-s r)\\stackrel{\\cdot}{=}-1$ , then by Equation (131), there must be an $i_{0}\\in[n]$ such that ", "page_idx": 41}, {"type": "equation", "text": "$$\ns\\zeta_{i_{0}}^{2}=\\lfloor h_{i_{0}}^{2}s\\rfloor<h_{i_{0}}^{2}s<\\lceil h_{i_{0}}^{2}s\\rceil=\\lfloor h_{i_{0}}^{2}s\\rfloor+1.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Setting ", "page_idx": 41}, {"type": "equation", "text": "$$\nv_{i}=\\left\\{\\sqrt{\\zeta_{i_{0}}^{2}+1/s}=\\sqrt{\\lceil h_{i_{0}}^{2}s\\rceil/s},\\quad\\mathrm{if}\\ i=i_{0}\\right.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "guarantees that $\\|\\pmb{v}\\|_{2}^{2}=r$ and ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\|v-h\\right\\|_{\\infty}=\\operatorname*{max}\\left\\{\\operatorname*{max}_{i\\neq i_{0}}|\\zeta_{i}-h_{i}|,\\left|\\sqrt{\\lceil h_{i_{0}}^{2}s\\rceil/s}-h_{i_{0}}\\right|\\right\\}\\leq\\frac{1}{\\sqrt{s}},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the last inequality follows from Equation (132). Similarly, if $(s\\|\\zeta\\|_{2}^{2}-s r)=1$ , then we can alter one element of $\\zeta$ to obtain $\\pmb{v}$ , such that $\\|\\pmb{v}\\|_{2}^{2}=r$ and $\\|\\pmb{v}-\\pmb{h}\\|_{\\infty}\\leq1/\\sqrt{s}$ ", "page_idx": 41}, {"type": "text", "text": "Since $s{v_{i}}^{2}$ are integers for all $i\\in[n],s\\|\\pmb{v}\\|_{2}^{2}=s r$ , and the fact that ", "page_idx": 41}, {"type": "equation", "text": "$$\ns\\|\\pmb{v}\\|_{\\infty}^{2}\\leq\\operatorname*{max}_{i\\in[n]}\\lceil h_{i}^{2}s\\rceil\\leq s+1,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the last inequality follows from $\\|h\\|_{\\infty}\\leq1$ for all $\\pmb{h}\\in\\mathbb{T}$ , we see that $\\pmb{v}\\in\\mathcal{C}$ ", "page_idx": 41}, {"type": "text", "text": "H Proof of Theorem 2 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this section, we use the Yang-Barron method [59] along with a Fano lower bound (see Proposition 15.12 and Lemma 15.21 in [56]) to prove the minimax lower bound for eigenspace estimation stated in Theorem 2. We follow the notation used in [56]. Given a class of distributions $\\mathcal{P}$ , we let $\\theta$ denote a functional on the space $\\mathcal{P}$ , that is, a mapping from a distribution $\\mathbb{P}$ to a parameter $\\theta(\\mathbb{P})$ taking values in some space $\\Omega$ .We let $\\rho:\\Omega\\times\\Omega\\rightarrow[0,\\infty)$ be a semi-metric. Proposition 2 states the Fano lower bound (Proposition 15.12 in [56]). ", "page_idx": 41}, {"type": "text", "text": "Proposition 2 (Fano lower bound). Let $\\{\\theta_{1},\\theta_{2},\\dots,\\theta_{M}\\}\\subseteq\\Omega$ be a $2\\delta$ separated set under the $\\rho$ semimetric, and suppose that $J$ is uniformly distributed over the index set $[M]$ and $(Z\\mid J=j)\\sim\\mathbb{P}_{\\theta_{j}}$ Then for any increasing function $\\Phi:[0,\\infty)\\rightarrow[0,\\infty)$ , the minimax risk is lower bounded as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\widehat{\\theta}}\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}}\\mathbb{E}_{\\mathbb{P}}\\,\\Phi\\Big(\\rho\\,\\Big(\\widehat{\\theta},\\theta(\\mathbb{P})\\Big)\\Big)\\geq\\Phi(\\delta)\\,\\left(1-\\frac{I(Z;J)+\\log2}{\\log M}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the infimum is over all estimators $\\widehat{\\theta}$ and $I(Z;J)$ is the mutual information between $Z$ and $J$ ", "page_idx": 41}, {"type": "text", "text": "The Yang-Barron method gives an upper bound for the mutual information $I(Z;J)$ ", "page_idx": 41}, {"type": "text", "text": "Lemma 18 (Yang-Barron method [59]). Let $\\mathcal{N}_{\\mathrm{KL}}(\\varepsilon;\\mathcal{P})$ denotethe $\\varepsilon$ -coveringnumberof $\\mathcal{P}$ in the square-root $K L$ -divergence. Then the mutual information is upper bounded as ", "page_idx": 41}, {"type": "equation", "text": "$$\nI(Z;J)\\leq\\operatorname*{inf}_{\\varepsilon>0}\\log\\mathcal{N}_{\\mathrm{KL}}(\\varepsilon;\\mathcal{P})+\\varepsilon^{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof of Theorem 2. To each $(\\mathbf{\\boldsymbol{\\Lambda}}^{\\star},\\mathbf{\\boldsymbol{U}}^{\\star})\\in\\Omega(\\lambda^{\\star},\\mu,r)$ we associate a probability distribution $\\mathbb{P}_{\\mathbf{A}^{\\star},U^{\\star}}$ on $\\mathbb{R}^{n\\times n}$ with density given by ", "page_idx": 41}, {"type": "equation", "text": "$$\nf_{\\Lambda^{\\star},U^{\\star}}(W)=\\prod_{1\\leq i\\leq j\\leq n}g\\left(\\frac{(W-U^{\\star}\\Lambda^{\\star}U^{\\star^{\\top}})_{i j}}{\\sigma}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $g(\\cdot)$ denotes the density of a standard normal. We define the class of distributions ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}=\\left\\{\\mathbb{P}_{\\Lambda^{\\star},U^{\\star}}:(\\Lambda^{\\star},U^{\\star})\\in\\Omega(\\lambda^{\\star},\\mu,r)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For $\\mu_{1},\\mu_{2}\\,\\in\\,\\mathbb{R}^{n}$ and a pair of normal distributions $N(\\mu_{1},\\sigma^{2}I_{n})$ and $N(\\mu_{2},\\sigma^{2}I_{n})$ , their KLdivergence is given by (see Example 15.13 in [56]) ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\Big(N(\\pmb{\\mu}_{1},\\sigma^{2}I_{n})\\Big|\\Big|N(\\pmb{\\mu}_{2},\\sigma^{2}I_{n})\\Big)=\\frac{1}{2\\sigma^{2}}\\|\\pmb{\\mu}_{2}-\\pmb{\\mu}_{1}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "It follows that for any $\\mathbb{P}_{\\mathbf{1}^{\\star},U_{1}^{\\star}}\\mathbb{P}_{\\mathbf{1}^{\\star},U_{2}^{\\star}}\\in\\mathcal{P}$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\mathbb{P}_{\\Lambda^{\\star},U_{1}^{\\star}}\\Big|\\Big|\\mathbb{P}_{\\Lambda^{\\star},U_{2}^{\\star}}\\right)\\le\\frac{\\lambda^{\\star^{2}}}{2\\sigma^{2}}\\left\\|U_{1}^{\\star}U_{1}^{\\star^{\\top}}-U_{2}^{\\star}U_{2}^{\\star^{\\top}}\\right\\|_{F}^{2}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and thus, taking square roots, ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sqrt{\\mathrm{KL}(\\mathbb{P}_{\\Lambda^{\\star},U_{1}^{\\star}}\\|\\mathbb{P}_{\\Lambda^{\\star},U_{1}^{\\star}})}\\le\\frac{\\sqrt{2}\\lambda^{\\star}}{2\\sigma}\\left\\|U_{1}^{\\star}U_{1}^{\\star^{\\top}}-U_{2}^{\\star}U_{2}^{\\star^{\\top}}\\right\\|_{F}\\le\\frac{\\lambda^{\\star}}{\\sigma}\\;d_{\\mathrm{F}}\\left(U_{1}^{\\star},U_{2}^{\\star}\\right),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the second inequality holds from Lemma 2.6 in [27]. ", "page_idx": 42}, {"type": "text", "text": "To apply the Yang-Barron method, we follow a two-step procedure (see Chapter 15 of [56]): ", "page_idx": 42}, {"type": "text", "text": "1. Pick the smallest $\\varepsilon$ such that $\\varepsilon^{2}\\geq\\log\\ensuremath{\\mathcal{N}}_{\\mathrm{KL}}(\\ensuremath{\\mathcal{P}},\\varepsilon)$ ", "page_idx": 42}, {"type": "text", "text": "2. Choose the largest $\\delta$ that we can find a $\\delta$ -packing that satisfies the lower bound ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\log\\mathcal{M}(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta)\\geq4\\varepsilon^{2}+2\\log2.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then it follows from Lemma 18 and Proposition 2 that the minimax risk is lower bounded by $\\delta/2$ For the first step, noting that by Equation (136), a $(\\sigma\\varepsilon/\\lambda^{\\star})$ -covering set for $\\mathbb{V}_{n,r}$ under $d_{\\mathrm{{F}}}$ yields an $\\varepsilon$ -covering set for the $\\sqrt{\\mathrm{KL}}-$ divergence, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathcal{N}_{\\mathrm{KL}}(\\mathcal{P},\\varepsilon)\\le\\mathcal{N}(\\mathbb{V}_{n,r},d_{\\mathrm{F}},\\varepsilon/\\lambda^{\\star})\\le\\left(\\frac{C_{0}\\lambda^{\\star}\\sqrt{2r}}{\\sigma\\varepsilon}\\right)^{r(n-r)},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the last inequality follows from Lemma 13. Thus, in order to have $\\varepsilon^{2}\\geq\\log\\ensuremath{\\mathcal{N}}_{\\mathrm{KL}}(\\ensuremath{\\mathcal{P}},\\varepsilon)$ it suffices to have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\varepsilon^{2}\\geq r(n-r)\\log\\left(\\frac{C_{0}\\lambda^{\\star}\\sqrt{2r}}{\\sigma\\varepsilon}\\right)\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "which holds if we set $\\varepsilon=C_{0}\\lambda^{\\star}\\sqrt{2r}/\\sigma$ . With this choice of $\\varepsilon$ , we then follow the second step of the Yang-Barron method and pick a $\\delta$ to satisfy ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\log\\mathcal{M}\\left(\\mathbb{K}_{\\mu,r},d_{2,\\infty},\\delta\\right)\\geq\\frac{8C_{0}r\\lambda^{\\star^{2}}}{\\sigma^{2}}+2\\log2.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By Equation (115) in the proof of Lemma 4, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\log\\mathcal{M}\\left(\\mathbb{K}_{r,\\mu},d_{2,\\infty},\\delta\\right)\\geq\\frac{c_{0}^{2}r^{2}}{32e^{2}\\delta^{2}}-\\log2\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "for $\\delta$ satisfying Eqaution (21). Combining Equations (137) and (138), our goal is to find a $\\delta$ such that ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\frac{c_{0}^{2}r^{2}}{32e^{2}\\delta^{2}}\\geq\\frac{8C_{0}r\\lambda^{\\star2}}{\\sigma^{2}}+3\\log{2}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We pick $\\delta$ to be ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\delta=\\frac{c_{0}\\sigma\\sqrt{r}}{12e\\lambda^{\\star}\\sqrt{2C_{0}}}\\wedge\\frac{c_{0}}{4e}\\sqrt{\\frac{\\mu r}{6n\\log(12n/\\mu)}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "One can verify that $\\delta$ satisfies Equation (21) under the assumption $\\lambda^{\\star}\\leq(6\\sqrt{C_{0}})^{-1}\\sigma\\sqrt{n}$ . To see that the $\\delta$ given by Equation (140) satisfies Equation (139), we separate our discussion into two cases, one for large $\\lambda^{\\star}$ , and the other for small $\\lambda^{\\star}$ . When ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\lambda^{\\star}\\geq\\sigma\\sqrt{\\frac{n\\log(12n/\\mu)}{3C_{0}\\mu}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "by Equation (140) we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\delta=\\frac{c_{0}\\sigma\\sqrt{r}}{12e\\lambda^{\\star}\\sqrt{2C_{0}}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and therefore, Equation (139) follows from ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\frac{c_{0}^{2}r^{2}}{32e^{2}\\delta^{2}}=\\frac{9C_{0}r\\lambda^{\\star2}}{\\sigma^{2}}\\geq\\frac{8C_{0}r\\lambda^{\\star2}}{\\sigma^{2}}+3\\log2,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the first equality holds from Equation (142) and the last inequality follows from Equation (141) for all $n$ suficiently large. ", "page_idx": 43}, {"type": "text", "text": "On the other hand, when ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\lambda^{\\star}<\\sigma\\sqrt{\\frac{n\\log(12n/\\mu)}{3C_{0}\\mu}},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "by Equation (140) we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\delta=\\frac{c_{0}}{4e}\\sqrt{\\frac{\\mu r}{6n\\log(12n/\\mu)}},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and therefore, Equation (139) follows from ", "page_idx": 43}, {"type": "equation", "text": "$$\n{\\frac{c_{0}^{2}r^{2}}{32e^{2}\\delta^{2}}}={\\frac{3r n\\log(12n/\\mu)}{\\mu}}\\geq{\\frac{8r n\\log(12n/\\mu)}{\\mu}}+3\\log2\\geq{\\frac{8C_{0}r\\lambda^{\\star^{2}}}{\\sigma^{2}}}+3\\log2,\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the first equality follows from Equation (144), the next inequality holds for all $n$ sufficiently large, and the last inequality holds from Equation (143). Thus, we conclude that the choice of $\\delta$ given in Equation (140) satisfies Equation (139), completing the second step of the Yang-Barron method. ", "page_idx": 43}, {"type": "text", "text": "Finally, combining Proposition 2 and Lemma 18, we conclude that for a universal constant $c>0$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{U}}\\operatorname*{sup}_{(\\Lambda^{\\star},U^{\\star})\\in\\Omega(\\lambda^{\\star},\\mu,r)}\\mathbb{E}_{\\Lambda^{\\star},U^{\\star}}\\;d_{2,\\infty}\\left(\\widehat{U},U^{\\star}\\right)\\geq\\frac{\\delta}{2}\\geq c\\left(\\frac{\\sigma\\sqrt{r}}{\\lambda^{\\star}}\\wedge\\sqrt{\\frac{\\mu r}{n\\log(n/\\mu)}}\\right),\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "completing the proof. ", "page_idx": 43}, {"type": "text", "text": "1 Additional experiments ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Here we collect additional experiments to complement our simulations in Section 5. ", "page_idx": 43}, {"type": "text", "text": "1.1  Additional numerical results on rank-one eigenvector estimation ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "We provide additional details for the experiments previously discussed in Section 5.1. Recall that we observe ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\pmb{Y}=\\pmb{M}^{\\star}+\\pmb{W}=\\lambda^{\\star}\\pmb{u}^{\\star}\\pmb{u}^{\\star\\top}+\\pmb{W},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and wish to recover $\\pmb{u}^{\\star}\\in\\mathbb{S}^{n-1}$ . We refer the reader to Section 5.1 for the details of how $\\lambda^{\\star},\\,u^{\\star}$ and $W$ are generated and how we run Algorithm 1. ", "page_idx": 43}, {"type": "text", "text": "In Section 5.1 we mentioned that under Laplacian noise, as shown in the third column of Figure 1, the estimator $\\widehat{\\pmb{u}}$ given by Algorithm 1 seems to have a slight dependence on the coherence parameter $\\mu$ We give a close examination of the estimation error of the largest entry of $\\pmb{u}^{\\star}$ in Figure 3, with shaded bands indicating $95\\%$ bootstrap confidence intervals. Figure 3 shows that the estimation error of the largest entry of $\\pmb{u}^{\\star}$ for $\\widehat{\\pmb{u}}$ is seen to be much smaller than $10^{-2}$ , while in the third column of Figures 1, the estimation error $d_{\\infty}(\\widehat{\\boldsymbol{u}},\\boldsymbol{u}^{\\star})$ is well above $10^{-2}$ . This indicates that the dependence on $\\mu$ observed in the right-hand subplot of Figure 1 comes not from estimating the largest entry of $\\pmb{u}^{\\star}$ , but rather from estimating the other entries. In our experiment, the other entries are all nearly incoherent (that is, they have a magnitude at most $O({\\sqrt{\\log n/n}}))$ , whence their estimation errors are expected to have no dependence on $\\mu$ asymptotically. Thus, the slight dependence on $\\mu$ exhibited Figure 1 is most likely to be a small order dependence compared to the error rate stated in Theorem 1, and should not affect the asymptotic error rate. ", "page_idx": 43}, {"type": "image", "img_path": "CiuH7zOBCQ/tmp/650b3b76d93d05c83de3b58ccdca9822f75842bdd2c1609fa13fc9b188bd039f.jpg", "img_caption": ["Figure 3: Numerical error in recovering the largest entry of $\\pmb{u}^{\\star}$ as a function of matrix dimension $n$ by the leading eigenvector (blue line) or the estimator given in Algorithm 1 (orange line) for three different choices of $\\lVert\\boldsymbol{u}^{\\star}\\rVert_{\\infty}\\colon0.8$ (dotted lines), 0.55 (dashed lines) and 0.3 (solid lines). The plot on the left corresponds to $\\pmb{u}^{\\star}$ generated via the Bernoulli scheme, while the plot on the right corresponds to the Haar scheme. "], "img_footnote": [], "page_idx": 44}, {"type": "text", "text": "I.2  Additional numerical results on rank- $r$ eigenvector estimation ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We provide additional details for the experiments previously discussed in Section 5.2. Recall that we observe ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\pmb{Y}=\\pmb{M}^{\\star}+\\pmb{W}=\\pmb{U}^{\\star}\\pmb{\\Lambda}^{\\star}\\pmb{U}^{\\star\\top}+\\pmb{W},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and wish to recover $U^{\\star}\\,\\in\\,\\mathbb{R}^{n\\times r}$ . In addition to the rank-2 setting discussed in Section 5.2, we include experimental results for the case when $M^{\\star}$ has rank 3. We also provide estimation error under $d_{2,\\infty}$ for our estimate $\\widehat{U}$ in Algorithm 2 and the spectral estimate $U$ , to complement our results under $d_{\\infty}$ reported in Section 5.2. The experiments follow the same setup outlined Section 5.2, and we refer the readers there for details regarding running Algorithm 2 the generation of $\\Lambda^{\\star}$ \uff0c $U^{\\star}$ and $W$ ", "page_idx": 44}, {"type": "text", "text": "We first provide more details as to how we obtain the estimation error under $d_{2,\\infty}$ . Normally, since ", "page_idx": 44}, {"type": "equation", "text": "$$\nd_{2,\\infty}(\\widehat{\\pmb{U}},\\pmb{U}^{\\star})=\\operatorname*{min}_{\\mathbf{\\epsilon}_{\\mathbf{\\epsilon}_{\\mathbf{\\epsilon}}}}\\left\\|\\widehat{\\pmb{U}}\\mathbf{\\Gamma}-\\pmb{U}^{\\star}\\right\\|_{2,\\infty},\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "obtaining the $d_{2,\\infty}$ estimation error requires finding an orthogonal matrix that minimizes a nonsmooth function, which is hard to achieve. In our simulation, however, since we have sufficiently large eigengaps between the eigenvalues, we know how each column of U corresponds to the columns of $U^{\\star}$ . Thus, we merely need to resolve the ambiguity in the sign of each column of $\\widehat{\\pmb U}$ , rather than searching over all $\\mathbf{T}\\in\\mathbb{O}_{r}$ . By assigning each column of U the sign of the corresponding column in $U^{\\star}$ , we resolve this ambiguity and can obtain the $d_{2,\\infty}$ estimation error. ", "page_idx": 44}, {"type": "text", "text": "Figure 4 compares the empirical accuracy, measured by $d_{2,\\infty}$ , of estimating $U^{\\star}$ via the $r$ leading eigenvectors $U$ of $\\mathbf{Y}$ (blue lines) and via U produced by Algorithm 2 (orange lines). Shaded bands are generated from point-wise $95\\%$ confidence intervals using bootstrap approximation. Similar to the rank-one setting, Algorithm 2 recovers $U^{\\star}$ with a much smaller estimation error under $d_{2,\\infty}$ compared to the naive spectral estimate, especially when the coherence $\\mu$ is large. The figure also shows that Algorithm 2 performs well under different noise distributions. ", "page_idx": 44}, {"type": "text", "text": "Figure 5 corresponds to $M^{\\star}$ having rank-3. It compares the empirical accuracy, measured under $d_{\\infty}$ , of estimating each $\\pmb{u}_{k}^{\\star}$ , for $k=1,2,3$ via the leading eigenvectors $\\pmb{u}_{k}$ of $\\mathbf{\\deltaY}$ (blue/purple lines) and via $\\widehat{\\pmb{u}}_{k}$ given by Algorithm 2 (orange/red lines). Shaded bands are generated from point-wise $95\\%$ confidence intervals using bootstrap approximation. As in the first plot of Figure 2, under Gaussian noise, the estimation error of $\\widehat{\\pmb{u}}_{k}$ shows little to no visible dependence on $\\mu$ and is much smaller compared to the leading eigenvectors of $\\mathbf{\\deltaY}$ . Under Rademacher noise, as in the second plot of Figure 2, the dependence on $\\mu$ again appears slightly reversed from that of the spectral estimator. For Laplacian noise, as in the third plot of Figure 2, there again seems to be a slight dependence on $\\mu$ For the same reason discussed in Sections 5.1 and 5.2, we expect such dependence to be of smaller order than the rate in Theorem 1 and should not affect the asymptotic error rate. ", "page_idx": 44}, {"type": "image", "img_path": "CiuH7zOBCQ/tmp/cc2f4122e6c93ec417ee0f118e7e5fdb39876fb86b21c394b8ee3e0f0b6e4e83.jpg", "img_caption": ["Figure 4: Error as measured in $d_{2,\\infty}$ as a function of matrix dimension $n$ , by spectral estimate (blue) and the estimator in Algorithm 2 (orange) for three different choices of $\\lVert U^{\\star}\\rVert_{\\infty}\\colon0.8$ (dotted lines), 0.55 (dashed lines) and 0.3 (solid lines). Columns correspond to $W$ being Gaussian (left), Rademacher (center) and Laplacian (right). The rows correspond to the signal matrix having rank-2 (top) and rank-3 (bottom). "], "img_footnote": [], "page_idx": 45}, {"type": "image", "img_path": "CiuH7zOBCQ/tmp/2ea4db3803a60e7d681a482e0f69c21060fb585777c82083e77d0aef2328f149.jpg", "img_caption": ["Figure 5: Error measured in $d_{\\infty}$ as a function of matrix dimension $n$ , for the three leading signal eigenvectors $\\pmb{u}_{k}^{\\star},k=1,2,3$ (line width) by the spectral estimate (blue/purple) or the estimator given in Algorithm 2 (orange/red) for three different choices of $\\|\\boldsymbol{u}^{\\star}\\|_{\\infty}\\colon0.8$ (dotted lines), 0.55 (dashed lines) and 0.3 (solid lines). The plots correspond to $W$ being Gaussian (left), Rademacher (center) and Laplacian (right). "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "", "page_idx": 45}, {"type": "text", "text": "I.3 Comparison with AMP-based eigenvector estimation ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "As mentioned in the main text, to the best of our knowledge, we are the first paper to consider the problem of non-spectral entrywise eigenvector estimation. The nearest obvious method to serve as a comparison point, if we were to insist upon one, would likely be one based on approximate message passing (AMP; see [34]for an overview). AMP methods make no explicit coherence assumptions, but the underlying mechanism essentially requires incoherence: inherent to AMP-based eigenvector recovery methods is that the eigenvector is modeled as having its entries drawn i.i.d. according to a common distribution. Specifically, AMP methods typically make a mean field assumption whereby the empirical distribution of the entries of $\\pmb{u}^{\\star}$ convergesin $\\ell_{2}$ to some distribution $\\pi$ .Since ", "page_idx": 45}, {"type": "image", "img_path": "CiuH7zOBCQ/tmp/61e716bbb6d41dcb208ac96131eb68f552f3fcae1972bd64cf92c7fea5fb31d2.jpg", "img_caption": ["Figure 6: Estimation error under $d_{\\infty}$ as a function of size $n$ , by approximate message passing (AMP; green), the leading eigenvector (blue) and Algorithm 1 (orange) for $\\sqrt{n}\\|\\boldsymbol{u}^{\\star}\\|_{\\infty}$ equal to $3n^{1/3}$ (solid lines), $3n^{1/4}$ (dashed lines) or $3n^{1/5}$ (dotted lines) under Gaussian noise. Each data point is the mean of 30 independent trials. "], "img_footnote": [], "page_idx": 46}, {"type": "text", "text": "AMP-based methods are tailored to $\\ell_{2}$ -recovery, they are suboptimal for entrywise recovery problems: small $\\ell_{2}$ error does not necessary imply small entrywise error. ", "page_idx": 46}, {"type": "text", "text": "The unsuitability of typical AMP methods notwithstanding, we include here a comparison against our Algorithm 1 for the sake of completeness. The experimental setup mirrors that of Figure 1, but now includes estimation error for an AMP-based method, as well. We generate $\\pmb{u}^{\\star}$ according to the following procedure: set a random entry of $\\pmb{u}^{\\star}$ to be $a\\in\\{3n^{1/3}/\\sqrt{n},\\bar{3}n^{1/4}/\\sqrt{n},3n^{1/5}/\\sqrt{n}\\}$ , then generate the remaining entries by drawing uniformly from $\\{\\pm1\\}^{n-1}$ and normalizing these to have $\\ell_{2}$ norm $\\sqrt{1-a^{2}}$ . This way, $\\|\\boldsymbol{u}^{\\star}\\|_{\\infty}=a$ and the coherence is $\\mu=a^{2}n$ We choose this setting to ensure that the limiting prior distribution satisfies the assumptions required by AMP, and thus we can apply the update procedures using Equations (22) (34) and the example after Equation (35) in [34]. ", "page_idx": 46}, {"type": "text", "text": "Having generated $\\pmb{Y}=\\pmb{M}^{\\star}+\\pmb{W}$ , we estimate $\\pmb{u}^{\\star}$ using the spectral estimate $\\textbf{\\em u}$ , the AMP method and our method as described in Section 5 and measure the estimation error under $d_{\\infty}$ . We report the mean of 30 independent trials for each combination of conditions (i.e., each combination of problem size $n$ and magnitude $a$ ).Wevarythematrixsize $n$ from 3000 to 22000 in increments of 1000. ", "page_idx": 46}, {"type": "text", "text": "Figure 6 compares the entrywise estimation error of the AMP method (green), the leading eigenvector Oof $\\mathbf{Y}$ (blue) and our Algorithm 1 (orange). Across settings, Algorithm 1 recovers $\\pmb{u}^{\\star}$ with amuch smaller error under $d_{\\infty}$ compared to the other two estimates, and shows an error rate with no visible dependence on the coherence. Both the spectral method and the AMP-based method exhibit different estimation error rates as the coherence increases. In particular, the AMP method performs much worse due to the fact that it is a mean field approximation method and not well-suited for our setting. Adapting AMP-based methods to target entrywise recovery rather than $\\ell_{2}$ recovery, in hopes of rectifying the poor performance exhibited in Figure 6, is a promising direction for future work. ", "page_idx": 46}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 47}, {"type": "text", "text": "Justification: the abstract and introduction give overviews of our two main results: 1) a new method for estimating the eigenvectors in signal-plus-noise matrix models and 2) a new lower bound for eigenvector estimation in the small-eigenvalue regime. These results are expanded upon (including a thorough discussion of assumptions and limitations) in Sections 2, 3 4 and 6. Our experiments in Section 5 illustrate which of our assumptions can likely be relaxed or removed entirely. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 47}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: Our main assumptions, the reasons for needing them, and their implications are discussed in Sections 1 through 4. In particular, see Remarks 1, 2 and 6. Our experiments in Section 5 explore which of our assumptions might be relaxed or removed. A broad, albeit short, overview of limitations and areas for future work is also given in Section 6, titled \"Discussion, limitations and conclusion\". Computational costs and efficiency are addressed just before Algorithm 1. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 47}, {"type": "text", "text": "\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 48}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 48}, {"type": "text", "text": "Justification: Our main technical assumptions, Assumptions 1, 2, 3 and 4 are prominently laid out and explained in Sections 1 and 2. The necessity of these assumptions and the potential to relax or remove them are discussed in those sections, as well as in Section 6. All theorems and lemmas are stated with their necessary additional assumptions (e.g., growth rates and relations between parameters), and all are numbered, cross-referenced and have working hyperlinks. Full proofs are given in the supplementary materials. We have provided thorough background and intuition surrounding these proofs, though proof sketches have largely been removed due to space constraints. We are committed to including proof sketches in a camera-ready version using the allotted extra page, should the reviewers request them. ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 48}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Justification: Algorithms are described in the text, including details regarding how hyperparameters were selected and minor changes made to Algorithms 1 and 2 \u201cas written\" as opposed to \u201cas run on the cluster'\". ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \nWhile NeurIPS does not require releasing code, the conference does require all submis  \nsions to provide some reasonable avenue for reproducibility, which may depend on the   \nnature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b)If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 48}, {"type": "text", "text": "", "page_idx": 49}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: in addition to the algorithmic descriptions in Sections 2 and 3 and the details in Section 5, we have included code for running all reported experiments in our supplemental materials. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 49}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 49}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 49}, {"type": "text", "text": "Justification: the main such concern in this paper surrounds the selection of the parameters $\\alpha_{0}$ and $\\beta$ in Algorithm 1. Selection of these parameters is discussed at length in Remark 2 and in our description of the experiments in Section 5. We have also provided code that reflects the discussion in the paper. ", "page_idx": 49}, {"type": "text", "text": "Guidelines: ", "page_idx": 49}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 50}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: all experiment plots include error bars denoting $95\\%$ bootstrapconfidence intervals. This is discussed in Section 5. It is clearly stated in the text that the randomness in the error bars comes from independent repetitions of the same experimental setup. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 50}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: This is addressed at the beginning of Section 5. ", "page_idx": 50}, {"type": "text", "text": "Guidelines: ", "page_idx": 50}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 50}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 50}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 50}, {"type": "text", "text": "Justification: This paper concerns a theoretical question surrounding minimax estimation rates. It involves neither human subjects nor sensitive data. Societal impacts, either positive or negative, are of course possible (e.g., statistical methods and models are frequently abused or used toward malicious ends), but our work creates no risks that warrant mitigation, to the best of our knowledge. All experimental code uses only open source software tools. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 51}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 51}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 51}, {"type": "text", "text": "Justification: This paper concerns a theoretical/methodological question regarding estimation of eigenvectors from noisy matrices. Given the theoretical nature of this work, it is hard to foresee direct social impacts, either positive or negative, from this work. Positive impacts of eigenvector estimation methods (e.g., PCA, matrix denoising, etc) are ubiquitous (see, for example, medical image processing, to name just one application area). These positive impacts are alluded to in the introduction. Of course, the abuse or misuse of statistical methods and models has frequently resulted in negative societal impacts and inequitable outcomes for underrepresented groups. We acknowledge these possible negative outcomes, albeit briefly, in Section 6. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 51}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 51}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 51}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 51}, {"type": "text", "text": "Justification: This paper concerns a theoretical question surrounding minimax estimation rates and includes no models or data with risk of misuse, to the best of our knowledge. ", "page_idx": 51}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 52}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: the experimental code, included in the supplementary materials, uses only open sourcesoftware tools. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 52}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 52}, {"type": "text", "text": "Justification: the experimental code, included in the supplementary materials, uses only open sourcesoftware tools. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 52}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 52}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This paper involves neither crowdsourcing nor human subject research. Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 53}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 53}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 53}, {"type": "text", "text": "Justification: This paper involves neither crowdsourcing nor human subject research. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 53}]