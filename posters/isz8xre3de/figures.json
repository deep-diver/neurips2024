[{"figure_path": "isZ8XRe3De/figures/figures_1_1.jpg", "caption": "Figure 1: Gradient similarity of LoRA modules across training steps. The sequence dataset is partitioned into 8 clusters using Euclidean distance, with hierarchical clustering applied to reorder clusters, so that clusters closer in the collaborative space are also closer together in the heatmap. Gradient similarity is used to assess the geometric characteristics of the loss, with darker cells indicating higher similarity. In the case study on the right, dashed lines connect similar items, while solid lines link identical items. Users with a gradient similarity of 0.86 share a strong interest in thriller movies, while those with -0.75 cosine similarity show no clear preference alignment.", "description": "This figure visualizes the gradient similarity of LoRA modules during training.  The sequence dataset is clustered based on Euclidean distance, and hierarchical clustering reorders these clusters for better visualization in a heatmap. Darker colors represent higher gradient similarity between clusters. A case study highlights two user pairs with contrasting similarity scores (0.86 and -0.75). The high similarity pair shares a strong preference for thriller movies, while the low similarity pair shows no clear preference alignment, demonstrating the variability captured by the model.", "section": "1 Introduction"}, {"figure_path": "isZ8XRe3De/figures/figures_3_1.jpg", "caption": "Figure 2: The iLoRA framework, which integrates the idea of MoE with LoRA, to implement sequence-customized activation patterns for various sequences.", "description": "The iLoRA framework is shown, integrating Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA).  A user's history (item sequence) is fed into a sequential recommender (e.g., SASRec) to generate a sequence embedding. This, along with textual descriptions, forms a hybrid prompt for the LLM.  The sequence embedding is then used by a gating network to assign weights to multiple LoRA expert modules (B and A matrices).  These weighted experts are combined into an instance-wise LoRA, adapting the LLM to the specific user behavior. This process addresses the negative transfer problem by personalizing the LLM to each user's sequence.", "section": "3 Methodology"}, {"figure_path": "isZ8XRe3De/figures/figures_6_1.jpg", "caption": "Figure 3: 3a and 3b separately show gradient similarities of LLaRA and iLoRA, with sequences partitioned into 8 clusters; 3c exhibits the attention scores over four experts, for ten sequences.", "description": "This figure compares the gradient similarities of LLaRA and iLoRA, visualizing how well their loss functions align across different sequences.  The dataset's sequences are divided into 8 clusters based on Euclidean distance.  Subfigure 3a shows the gradient similarity heatmap for LLaRA, revealing strong clustering along the diagonal, indicating negative transfer between dissimilar sequences.  Subfigure 3b presents the iLoRA heatmap, demonstrating reduced clustering and better alignment between diverse sequences.  Finally, subfigure 3c displays the attention scores of the four experts within iLoRA across ten sequences, showcasing the dynamic allocation of expert contributions based on sequence characteristics.", "section": "4.1 Investing Rationale of Instance-wise LoRA (RQ1)"}, {"figure_path": "isZ8XRe3De/figures/figures_7_1.jpg", "caption": "Figure 4: 4a illustrates the performance of iLoRA w.r.t. HitRatio@1 across different datasets with varying numbers of experts. 4b further demonstrates the HitRatio@1 performance of the model across different epochs during training on the Steam dataset with varying numbers of experts.", "description": "This figure shows the impact of the number of experts used in the iLoRA model on its performance. Subfigure 4a presents the HitRatio@1 metric across three benchmark datasets (LastFM, MovieLens, and Steam) for different numbers of experts (1, 2, 4, and 8).  Subfigure 4b focuses on the Steam dataset and illustrates the HitRatio@1 performance over multiple training epochs for models trained with various numbers of experts. This visualization helps understand how the number of experts influences both the overall performance and the training dynamics of the iLoRA model.", "section": "4.1 Investing Rationale of Instance-wise LORA (RQ1)"}, {"figure_path": "isZ8XRe3De/figures/figures_7_2.jpg", "caption": "Figure 4: 4a illustrates the performance of iLoRA w.r.t. HitRatio@1 across different datasets with varying numbers of experts. 4b further demonstrates the HitRatio@1 performance of the model across different epochs during training on the Steam dataset with varying numbers of experts.", "description": "This figure shows two subfigures. Subfigure (a) presents a bar chart illustrating the impact of the number of experts in iLoRA on the HitRatio@1 metric across three datasets (LastFM, MovieLens, and Steam).  It shows that using 4 experts achieves optimal performance. Subfigure (b) displays a line chart focusing on the Steam dataset, tracking the HitRatio@1 over multiple training epochs for different numbers of experts (2, 4, and 8).  This subfigure demonstrates how the performance evolves with training time for various expert configurations.", "section": "4.1 Investing Rationale of Instance-wise LoRA (RQ1)"}, {"figure_path": "isZ8XRe3De/figures/figures_7_3.jpg", "caption": "Figure 5: Effects of iLoRA's components", "description": "This figure shows the ablation study of iLoRA's components. Specifically, it compares the performance of iLoRA using different types of embedding as guidance for the gating network: random embedding, singular LoRA, token embedding, and sequence embedding. The results demonstrate the superiority of using sequence embedding as guidance, highlighting the effectiveness of the proposed gating network in leveraging sequence representations for customized attention scores over experts.  The chart displays the HitRatio@1 for each embedding type across three datasets: LastFM, MovieLens, and Steam.", "section": "4.3 Ablation Study (RQ3)"}]