{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a model that is foundational to many of the visual perception methods discussed in the paper, making it a highly important reference."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "publication_date": "2022-07-18", "reason": "This paper introduces BLIP, a key multimodal large language model (MLLM) that is frequently compared to and built upon in the VLORA model, making it highly significant."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-12-01", "reason": "This paper introduces LLaVA, a highly influential MLLM that serves as a major point of comparison for VLORA, demonstrating its importance in the field."}, {"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-07-18", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning method that VLORA builds upon and is conceptually similar to, making it a crucial technical foundation."}, {"fullname_first_author": "Christoph Schuhmann", "paper_title": "Laion-5b: An open large-scale dataset for training next generation image-text models", "publication_date": "2022-12-01", "reason": "This paper introduces a large-scale image-text dataset used for pre-training in the VLORA model, making it an essential resource for the model's development."}]}