[{"figure_path": "WEs4WMzndY/figures/figures_3_1.jpg", "caption": "Figure 1: Overcoming limitations of Winner-Takes-All training with annealing. Illustrations of the test-time predictions on a Mixture of three Gaussians (green points) with 49 hypotheses. Shaded blue circles represent the hypothesis predictions, with intensity corresponding to the predicted scores. (Left) Predictions of MCL as proposed in [41, 42]. (Middle) Predictions of Relaxed WTA [63] with \u03b5 = 0.1. (Right) Annealed MCL with initial temperature T0 = 0.6. Each model was trained with the same backbone (a three-layer MLP). We see that WTA leaves out some hypotheses, achieving a higher quantization error than aMCL. Moreover, we see that Relaxed-WTA is biased toward the barycenter of the distribution, in contrast with aMCL.", "description": "This figure compares the performance of three different methods for handling ambiguous prediction tasks: MCL, Relaxed WTA, and Annealed MCL.  Each method is visualized by its predictions (shaded circles) on a mixture of three Gaussian distributions (green points). The intensity of the circles represents the predicted scores.  The figure highlights that MCL suffers from hypothesis collapse (some hypotheses are unused), while Relaxed WTA shows a bias toward the distribution's barycenter.  In contrast, Annealed MCL effectively captures the ambiguity and provides a more robust and accurate prediction.", "section": "3 Annealed Multiple Choice Learning"}, {"figure_path": "WEs4WMzndY/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of the training trajectory in the Rate-Distortion curve. Training trajectories of MCL (blue) and aMCL (red) in the case of a single Gaussian. The optimal reachable distortion ('+') is the distortion D* satisfying R(D*) = log2(n) (See Section 4.2).", "description": "This figure shows how the training process of MCL and aMCL affects the rate-distortion curve.  The rate-distortion curve represents the trade-off between the rate (number of bits used to represent the data) and the distortion (difference between the original and compressed data). MCL tends to converge towards a local minimum on this curve while aMCL, using simulated annealing, explores more of the curve, leading to a potentially better overall performance. The optimal distortion corresponds to the point where the rate is equal to log2(n), where n is the number of hypotheses.", "section": "4.2 Rate-distortion curve"}, {"figure_path": "WEs4WMzndY/figures/figures_5_2.jpg", "caption": "Figure 3: Regimes in the distortion (1) vs. temperature training curve on the setup of Figure 4. At first, the hypotheses converge to the conditional mean. It is followed by a plateau phase where performance stagnates. Transition begins at T\u2080: the hypotheses migrate toward the barycenter of each Gaussian. Then, they split and we observe a last phase transition. For reference, 2\u03c3\u00b2 is the critical temperature for a Gaussian with variance \u03c3\u00b2.", "description": "This figure shows the relationship between distortion and temperature during the training process. The training starts with high temperature and the model predicts the conditional mean. Then there is a plateau phase.  As the temperature decreases below T\u2080, the model transitions to a different phase and the predictions move towards the barycenter of the Gaussian, then they split, causing another transition.", "section": "4.3 Phase transitions"}, {"figure_path": "WEs4WMzndY/figures/figures_6_1.jpg", "caption": "Figure 1: Overcoming limitations of Winner-Takes-All training with annealing. Illustrations of the test-time predictions on a Mixture of three Gaussians (green points) with 49 hypotheses. Shaded blue circles represent the hypothesis predictions, with intensity corresponding to the predicted scores. (Left) Predictions of MCL as proposed in [41, 42]. (Middle) Predictions of Relaxed WTA [63] with \u03b5 = 0.1. (Right) Annealed MCL with initial temperature To = 0.6. Each model was trained with the same backbone (a three-layer MLP). We see that WTA leaves out some hypotheses, achieving a higher quantization error than aMCL. Moreover, we see that Relaxed-WTA is biased toward the barycenter of the distribution, in contrast with aMCL.", "description": "This figure compares the performance of three different methods for multiple choice learning on a mixture of three Gaussians dataset.  It shows that the Winner-Takes-All (WTA) approach in standard MCL leads to some hypotheses being unused and higher quantization error, while the Relaxed-WTA approach biases predictions towards the distribution's center.  Annealed MCL addresses these issues by using simulated annealing, resulting in better coverage of the hypothesis space and improved robustness.", "section": "3 Annealed Multiple Choice Learning"}, {"figure_path": "WEs4WMzndY/figures/figures_25_1.jpg", "caption": "Figure 1: Overcoming limitations of Winner-Takes-All training with annealing. Illustrations of the test-time predictions on a Mixture of three Gaussians (green points) with 49 hypotheses. Shaded blue circles represent the hypothesis predictions, with intensity corresponding to the predicted scores. (Left) Predictions of MCL as proposed in [41, 42]. (Middle) Predictions of Relaxed WTA [63] with \u03b5 = 0.1. (Right) Annealed MCL with initial temperature To = 0.6. Each model was trained with the same backbone (a three-layer MLP). We see that WTA leaves out some hypotheses, achieving a higher quantization error than aMCL. Moreover, we see that Relaxed-WTA is biased toward the barycenter of the distribution, in contrast with aMCL.", "description": "This figure compares the performance of three different methods for multiple choice learning: MCL, Relaxed WTA, and Annealed MCL.  It shows that Annealed MCL overcomes the limitations of the other two methods by achieving a lower quantization error and avoiding bias toward the distribution's barycenter. The visualization uses a mixture of three Gaussian distributions to illustrate the differences in prediction accuracy and hypothesis diversity.", "section": "3 Annealed Multiple Choice Learning"}, {"figure_path": "WEs4WMzndY/figures/figures_29_1.jpg", "caption": "Figure 1: Overcoming limitations of Winner-Takes-All training with annealing. Illustrations of the test-time predictions on a Mixture of three Gaussians (green points) with 49 hypotheses. Shaded blue circles represent the hypothesis predictions, with intensity corresponding to the predicted scores. (Left) Predictions of MCL as proposed in [41, 42]. (Middle) Predictions of Relaxed WTA [63] with \u03b5 = 0.1. (Right) Annealed MCL with initial temperature T0 = 0.6. Each model was trained with the same backbone (a three-layer MLP). We see that WTA leaves out some hypotheses, achieving a higher quantization error than aMCL. Moreover, we see that Relaxed-WTA is biased toward the barycenter of the distribution, in contrast with aMCL.", "description": "This figure compares the performance of three different methods for ambiguous prediction tasks: MCL, Relaxed WTA, and Annealed MCL.  It uses a Mixture of three Gaussians as a test case.  The visualizations show that standard MCL suffers from hypothesis collapse (unused hypotheses). Relaxed WTA shows a bias towards the center of the distribution. Annealed MCL offers improved performance and handles the ambiguity of the data more effectively by utilizing annealing.", "section": "3 Annealed Multiple Choice Learning"}, {"figure_path": "WEs4WMzndY/figures/figures_30_1.jpg", "caption": "Figure 8: Phase transition in speech separation training. Impact of the initial temperature T\u2080 of the temperature scheduler on the source separation performance during training. The y-axis corresponds to the negative MCL SI-SDR metric. The x-axis corresponds to the temperature T(t) at each training step t, and is displayed in logarithmic scale. Comparison of several initial temperatures T\u2080 \u2248 0.1 (green solid line), T\u2080 \u2248 5 (red dashed and dotted line), and T\u2080 \u2248 23 (blue dashed line). A lower score indicates better separation performance.", "description": "This figure shows the impact of the initial temperature (T\u2080) on the training trajectory of Annealed Multiple Choice Learning (aMCL) for speech separation.  The plot displays the negative MCL SI-SDR (a measure of separation quality) against the temperature on a logarithmic scale.  Different initial temperatures (T\u2080 \u2248 0.1, T\u2080 \u2248 5, and T\u2080 \u2248 23) result in distinct curves. A lower score indicates better performance. The figure demonstrates the 'phase transition' phenomenon of aMCL where the loss initially plateaus before decreasing sharply at a critical temperature.", "section": "E.3.3 Phase transition in aMCL training"}]