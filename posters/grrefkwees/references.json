{"references": [{"fullname_first_author": "Sherwin Bahmani", "paper_title": "4D-FY: Text-to-4D generation using hybrid score distillation sampling", "publication_date": "2023-11-17", "reason": "This paper proposes a text-to-4D generation method using hybrid score distillation sampling, which is directly related to the core task of the current paper and is frequently referenced."}, {"fullname_first_author": "Yuyang Yin", "paper_title": "4DGen: Grounded 4D content generation with spatial-temporal consistency", "publication_date": "2023-12-17", "reason": "This paper focuses on generating 4D content with spatial-temporal consistency, a key aspect addressed in the current paper, and provides a strong baseline for comparison."}, {"fullname_first_author": "Andreas Blattmann", "paper_title": "Stable video diffusion: Scaling latent video diffusion models to large datasets", "publication_date": "2023-11-15", "reason": "This paper is cited as the foundation for the 3D-aware video diffusion model used in the current paper, demonstrating the importance of its methodology in 4D generation."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Imagen Video: High definition video generation with diffusion models", "publication_date": "2022-10-02", "reason": "Imagen Video is a highly influential work in video generation with diffusion models, providing a foundational technique used in many related works, including the current paper's video diffusion model."}, {"fullname_first_author": "Ben Poole", "paper_title": "DreamFusion: Text-to-3D using 2D diffusion", "publication_date": "2022-09-14", "reason": "This paper introduces a novel approach of text-to-3D generation using 2D diffusion, a significant advancement in 3D generation that directly inspires the current paper's methodology and is frequently cited."}]}