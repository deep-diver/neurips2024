[{"figure_path": "E1nBLrEaJo/figures/figures_4_1.jpg", "caption": "Figure 1: Private training on three datasets. (a) PCam is a binary classification task on which private training from scratch achieves relatively high accuracy, but linear probing on the pretrained model still improves accuracy up to 4%. (b) The fMoW model trained from scratch is unusable at low privacy levels while linear probing achieves close to nonprivate accuracy. (c) On RESISC45, linear probing outperforms full finetuning by over 50% at all \u03b5 levels.", "description": "This figure shows the results of private training on three different datasets (PCam, fMoW, and RESISC45) using three different training methods (full training from scratch, full finetuning, and linear probing).  For each dataset and training method, the accuracy is plotted against different levels of privacy (epsilon). The figure highlights that linear probing consistently outperforms the other two methods, especially when the privacy level is low, illustrating the benefits of using public pre-trained features even under high distribution shifts between public and private data.", "section": "4.3 Results"}, {"figure_path": "E1nBLrEaJo/figures/figures_4_2.jpg", "caption": "Figure 1: Private training on three datasets. (a) PCam is a binary classification task on which private training from scratch achieves relatively high accuracy, but linear probing on the pretrained model still improves accuracy up to 4%. (b) The fMoW model trained from scratch is unusable at low privacy levels while linear probing achieves close to nonprivate accuracy. (c) On RESISC45, linear probing outperforms full finetuning by over 50% at all \u03b5 levels.", "description": "This figure shows the results of private training on three datasets: PCam, fMoW, and RESISC45.  Each dataset shows test accuracy (y-axis) as a function of privacy level (\u03b5 on the x-axis) for three approaches:  training from scratch, finetuning a pretrained model, and linear probing with pretrained features. The figure highlights that linear probing consistently outperforms both training from scratch and full finetuning, particularly at lower privacy levels (higher \u03b5).  The results demonstrate the benefit of using pretrained public features for private training even when there's a significant distribution shift between the public and private datasets.", "section": "4.3 Results"}, {"figure_path": "E1nBLrEaJo/figures/figures_5_1.jpg", "caption": "Figure 5: Eigenspectra of feature covariance matrices for features extracted from pretrained CLIP ViT-B/32 model.", "description": "This figure displays the eigenspectra of the feature covariance matrices for three datasets (PCam, fMoW, and RESISC45).  The eigenspectra are derived from features extracted using a pretrained CLIP ViT-B/32 model.  The plots visually demonstrate the distribution of eigenvalues, showing how quickly the eigenvalues decay for each dataset. This decay rate suggests that the data lies in a low-dimensional subspace, supporting the theoretical model of the paper that assumes a shared low-dimensional subspace between public and private tasks, even in cases of extreme distribution shift.", "section": "A Empirical evidence for shared subspace assumption"}, {"figure_path": "E1nBLrEaJo/figures/figures_9_1.jpg", "caption": "Figure 4: Empirical verification of setup described in Section 5.1.", "description": "This figure empirically validates the theoretical model proposed in Section 5.1.  It shows the L2 parameter error for different private training methods across varying numbers of private examples (n2). The methods include non-private linear regression (Nonprivate LR), differentially private stochastic gradient descent (DP-SGD) with and without prior subspace estimation using different numbers of public examples (n1=500 and n1=2000).  The results demonstrate that incorporating public data (using Method of Moments (MoM) for subspace estimation) effectively reduces the error in private linear regression, approaching the performance of DP-SGD with the true subspace known. The shaded areas represent confidence intervals.", "section": "5 Theoretical Model"}, {"figure_path": "E1nBLrEaJo/figures/figures_14_1.jpg", "caption": "Figure 5: Eigenspectra of feature covariance matrices for features extracted from pretrained CLIP ViT-B/32 model.", "description": "This figure displays the eigenspectra of feature covariance matrices for three datasets (PCam, fMoW, and RESISC45). Each eigenspectrum shows how much variance in the feature space is explained by each eigenvector.  The rapid decay of eigenvalues in all three datasets indicates that the data lies in a low-dimensional subspace, even though these datasets are highly out-of-distribution compared to the CLIP model's pretraining data. This finding supports the paper's hypothesis that even with significant distribution shift between public and private data, a shared low-dimensional representation exists, allowing public features to improve private model training.", "section": "A Empirical evidence for shared subspace assumption"}]