[{"type": "text", "text": "Revisiting the Message Passing in Heterophilous Graph Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Graph Neural Networks (GNNs) have demonstrated strong performance in graph   \n2 mining tasks due to their message-passing mechanism, which is aligned with the   \n3 homophily assumption that adjacent nodes exhibit similar behaviors. However,   \n4 in many real-world graphs, connected nodes may display contrasting behaviors,   \n5 termed as heterophilous patterns, which has attracted increased interest in het  \n6 erophilous GNNs (HTGNNs). Although the message-passing mechanism seems   \n7 unsuitable for heterophilous graphs due to the propagation of class-irrelevant infor  \n8 mation, it is still widely used in many existing HTGNNs and consistently achieves   \n9 notable success. This raises the question: why does message passing remain effec  \n10 tive on heterophilous graphs? To answer this question, in this paper, we revisit the   \n11 message-passing mechanisms in heterophilous graph neural networks and refor  \n12 mulate them into a unified heterophilious message-passing (HTMP) mechanism.   \n13 Based on HTMP and empirical analysis, we reveal that the success of message   \n14 passing in existing HTGNNs is attributed to implicitly enhancing the compatibility   \n15 matrix among classes. Moreover, we argue that the full potential of the compat  \n16 ibility matrix is not completely achieved due to the existence of incomplete and   \n17 noisy semantic neighborhoods in real-world heterophilous graphs. To bridge this   \n18 gap, we introduce a new approach named CMGNN, which operates within the   \n19 HTMP mechanism to explicitly leverage and improve the compatibility matrix. A   \n20 thorough evaluation involving 10 benchmark datasets and comparative analysis   \n21 against 13 well-established baselines highlights the superior performance of the   \n22 HTMP mechanism and CMGNN method. ", "page_idx": 0}, {"type": "text", "text": "23 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "24 Graph Neural Networks (GNNs) have shown remarkable performance in graph mining tasks, such   \n25 as social network analysis [1, 2] and recommender systems [3, 4]. The design principle of GNNs is   \n26 typically based on the homophily assumption [5], which assumes that nodes are inclined to exhibit   \n27 behaviors similar to their neighboring nodes [6]. However, this assumption does not always hold   \n28 in real-world graphs, where the connected nodes demonstrate a contrasting tendency known as the   \n29 heterophily [7]. In response to the challenges of heterophily in graphs, heterophilous GNNs (HTGNNs)   \n30 have attracted considerable research interest [6, 8\u201310], with numerous innovative approaches being   \n31 introduced recently [11\u201324]. However, the majority of these methods continue to employ a message  \n32 passing mechanism, which was not originally designed for heterophilous graphs, as they tend to   \n33 incorporate excessive information from disparate classes. This naturally raises a question: Why does   \n34 message passing remain effective on heterophilous graphs?   \n35 Recently, a few efforts [6] have begun to investigate this question and reveal that vanilla message   \n36 passing can work on heterophilous graphs under certain conditions. However, the absence of a unified   \n37 and comprehensive understanding of message passing within existing HTGNNs has hindered the   \n38 creation of innovative approaches. In this paper, we first revisit the message-passing mechanisms   \n39 in existing HTGNNs and reformulate them into a unified heterophilous message-passing (HTMP)   \n40 mechanism, which extends the definition of neighborhood in various ways and simultaneously utilizes   \n41 the messages of multiple neighborhoods. Specifically, HTMP consists of three major steps namely   \n42 aggregating messages with explicit guidance, combining messages from multiple neighborhoods, and   \n43 fusing intermediate representations.   \n44 Equipped with HTMP, we further conduct empirical analysis on real-world graphs. The results reveal   \n45 that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the   \n46 compatibility matrix, which exhibits the probabilities of observing edges among nodes from different   \n47 classes. In particular, by increasing the distinctiveness between the rows of the compatibility matrix   \n48 via different strategies, the node representations of different classes become more discriminative in   \n49 heterophilous graphs.   \n50 Drawing from previous observations, we contend that nodes within real-world graphs might exhibit a   \n51 semantic neighborhood that only reveals a fraction of the compatibility matrix, accompanied by noise.   \n52 This could limit the effectiveness of enhancing the compatibility matrix and result in suboptimal   \n53 representations. To flil this gap, we further propose a novel Compatibility Matrix-aware Graph Neural   \n54 Network (CMGNN) under HTMP mechanism, which utilizes the compatibility matrix to construct   \n55 desired neighborhood messages as supplementary for nodes and explicitly enhances the compatibility   \n56 matrix by a targeted constraint. We build a benchmark to fairly evaluate CMGNN and existing   \n57 methods, which encompasses 13 diverse baseline methods and 10 datasets that exhibit varying   \n58 levels of heterophily. Extensive experimental results demonstrate the superiority of CMGNN and   \n59 HTMP mechanism. The contributions of this paper are summarized as:   \n60 \u2022 We revisit the message-passing mechanisms in existing HTGNNs and reformulate them into a   \n61 unified heterophilous message-passing mechanism (HTMP), which not only provides a macroscopic   \nview of message passing in HTGNNs but also enables people to develop new methods flexibly.   \n63 \u2022 We reveal that the effectiveness of message passing on heterophilous graphs is attributed to   \n64 implicitly enhancing the compatibility matrix among classes, which gives us a new perspective to   \n65 understand the message passing in HTGNNs.   \n66 \u2022 Based on HTMP mechanism and empirical analysis, we propose CMGNN to unlock the potential   \n67 of the compatibility matrix in HTGNNs. We further build a unified benchmark that overcomes the   \n68 issues of current datasets for fair evaluation1. Experiments show the superiority of CMGNN. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "69 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 Given a graph $\\mathcal{G}=(\\mathcal{V},\\mathcal{E},\\mathbf{X},\\mathbf{A},\\mathbf{Y})$ , $\\mathcal{V}$ is the node set and $\\mathcal{E}$ is the edge set. Nodes are characterized   \n71 by the feature matrix $\\mathbf{X}\\in\\mathbb{R}^{N\\times d_{f}}$ , where $N=|\\mathcal{V}|$ denotes the number of nodes, $d_{f}$ is the features   \n72 dimension. $\\textbf{Y}\\in\\,\\mathbb{R}^{N\\times1}$ is the node labels with the one-hot version $\\textbf{C}\\in\\,\\mathbb{R}^{N\\times K}$ , where $K$ is   \n73 the number of node classes. The neighborhood of node $v_{i}$ is denoted as ${\\mathcal N}_{i}$ . $\\mathbf{A}\\,\\in\\,\\mathbb{R}^{N\\times N}$ is   \n74 the adjacency matrix , and $\\mathbf{D}\\,=\\,\\mathrm{diag}(\\mathbf{d}_{1},...,\\mathbf{d}_{n})$ represents the diagonal degree matrix, where   \n75 $\\begin{array}{r}{\\mathbf{d}_{i}=\\sum_{j}\\mathbf{A}_{i j}}\\end{array}$ . $\\tilde{\\mathbf{A}}=\\mathbf{A}+\\mathbf{I}$ represents the adjacency matrix with self-loops. Let $\\mathbf{Z}\\in\\mathbb{R}^{N\\times d_{r}}$ be the   \n76 node representations with dimension $d_{r}$ learned by the models. We use 1 to represent a matrix with   \n77 all elements equal to 1, and 0 for a matrix with all elements equal to 0.   \n78 Homophily and Heterophily. High homophily is observed in graphs where a substantial portion of   \n79 connected nodes shares identical labels, while high heterophily corresponds to the opposite situation.   \n80 hFoorm ompehaisluy [ t1h5e] , hdoemfionepdh ialsy icasn da $h^{e}$ .   \n81 $h^{n}$ $\\begin{array}{r}{h^{e}=\\frac{|\\{e_{u,v}|e_{u,v}\\in\\boldsymbol{\\xi},\\ \\mathbf{Y}_{u}=\\mathbf{Y}_{v}\\}|}{|\\boldsymbol{\\mathcal{E}}|}}\\end{array}$ $\\begin{array}{r}{h^{n}=\\frac{1}{|\\mathcal{V}|}\\sum_{v\\in\\mathcal{V}}\\frac{|\\{u|u\\in\\mathcal{N}_{v},\\ \\mathbf{Y}_{u}=\\mathbf{Y}_{v}\\}|}{\\mathbf{d}_{v}}}\\end{array}$   \n82 Both metrics have a range of $[0,1]$ , where higher values indicate stronger homophily and lower values   \n83 indicate stronger heterophily.   \n84 Vanilla Message Passing (VMP). The vanilla message-passing mechanism plays a pivotal role in   \n85 transforming and updating node representations based on the neighborhood [25]. Typically, the ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/a88a8cec2b8d91a47086c91eb24bb98c7318a4f1927ee27fa9ae3e81f7003199.jpg", "table_caption": ["Table 1: Revisiting the message passing in representative heterophilous GNNs under the perspective of HTMP mechanism. "], "table_footnote": ["\\* The correspondence between the full form and the abbreviation: Raw Neighborhood (Raw), Neighborhood Redefine (ReDef), Neighborhood Discrimination (Dis), Degree-based Averaging (DegAvg), Adaptive Weights (AdaWeight), Relation Estimation (RelaEst), Addition (Add), Weighted Addition (WeightAdd), Adaptive Weighted Addition (AdaAdd), Concatenation (Cat), Adaptive Dimension Concatenation (AdaCat). \\* More details about the notations are available in Appendix A.1. "], "page_idx": 2}, {"type": "text", "text": "86 mechanism operates iteratively and comprises two stages: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf Z}^{l}=\\mathrm{AGGREGATE}(\\mathbf A,\\mathbf Z^{l-1}),\\quad\\mathbf Z^{l}=\\mathrm{COMBINE}\\left(\\mathbf Z^{l-1},\\widetilde{\\mathbf Z}^{l}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "87 where the AGGREGATE function first aggregates the input messages ${\\bf Z}^{l-1}$ from neighborhood A   \n88 into the aggregated one $\\widetilde{\\mathbf Z}^{l}$ , and subsequently, the COMBINE function combines the messages of   \n89 node ego and neighborhood aggregation, resulting in updated representations ${\\bf Z}^{l}$ . ", "page_idx": 2}, {"type": "text", "text": "90 3 Revisiting Message Passing in Heterophilous GNNs. ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "91 To gain a thorough and unified insight into the effectiveness of message passing in HTGNNs, we   \n92 revisit message passing in various notable HTGNNs [11\u201324] and propose a unified heterophilous   \n93 message passing (HTMP) mechanism, structured as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\mathbf{Z}}_{r}^{l}=\\mathrm{AGGREGATE}(\\mathbf{A}_{r},\\mathbf{B}_{r},\\mathbf{Z}^{l-1}),\\ \\mathbf{Z}^{l}=\\mathrm{COMBINE}(\\{\\tilde{\\mathbf{Z}}_{r}^{l}\\}_{r=1}^{R}),\\ \\mathbf{Z}=\\mathrm{FUSE}(\\{\\mathbf{Z}^{l}\\}_{l=0}^{L}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "94 Generally, HTMP extends the definition of neighborhood in various ways and simultaneously utilize   \n95 the messages of multiple neighborhoods, which is the key for better adapting to heterophily. We   \n96 use $R$ to denote the number of neighborhoods used by the model. In each message passing layer $l$ ,   \n97 HTMP separately aggregates messages within $R$ neighborhoods and combines them. The method  \n98 ological analysis of some representative HTGNNs and more details can be seen in Appendix A.   \n99 Compared to the VMP mechanism, HTMP mechanism has advances in the following functions:   \n100 (i) To characterize different neigborhoods, the AGGREGATE function in HTMP includes the neigh  \n101 borhood indicator ${\\bf A}_{r}$ to indicate the neighbors within a specific neighborhood $r$ . The adjacency   \n102 matrix A in VMP is a special neighborhood indicator that marks the neighbors in the raw neigh  \n103 borhood. To further characterize the aggregation of different neighborhoods, HTMP introduces the   \n104 aggregation guidence $\\mathbf{B}_{r}$ for each neighborhood $r$ . In VMP, the aggregation guidance is an implicit   \n110056 pusaread mfeotremr  ooff  tthhee  AAGGGGRREEGGAATTE Ef ufuncntcitoino ns iins ceA iGt GonRlyE GwAorTkE $\\mathbf{\\Xi}_{3}(\\mathbf{A}_{r},\\mathbf{B}_{r},\\mathbf{Z}^{l-1}\\widecheck{)}=(\\mathbf{A}_{r}\\odot\\mathbf{B}_{r})\\mathbf{Z}^{l-1}\\mathbf{W}_{r}^{l}$ ,   \n107 where $\\odot$ is the Hadamard product and $\\mathbf{W}_{r}^{l}$ is a weight matrix for message transformation. We take   \n108 this as the general form of the AGGREGATE function and only analyze the neighborhood indicators   \n109 and the aggregation guidance in the following.   \n110 The neighborhood indicator $\\mathbf{A}_{r}\\,\\in\\,\\{0,1\\}^{N\\times N}$ indicates neighbors associated with central nodes   \n111 within neighborhood $r$ . To describe the multiple neighborhoods in HTGNNs, neighborhood indicators   \n112 can be formed as a list $\\mathcal{A}=[\\mathbf{A}_{1},...,\\mathbf{A}_{r},...,\\mathbf{A}_{R}]$ . For the sake of simplicity, we consider the identity   \n113 matrix $\\mathbf{I}\\in\\mathbb{R}^{N\\times N}$ as a special neighborhood indicator for acquiring the nodes\u2019 ego messages. The   \n114 aggregation guidance $\\dot{\\mathbf{B}_{r}}\\in\\mathbb{R}^{N\\times\\mathbf{\\bar{N}}}$ can be viewed as pairwise aggregation weights in most cases,   \n115 which has the multiple form $\\boldsymbol{B}=[\\mathbf{B}_{1},...,\\mathbf{B}_{r},...,\\mathbf{B}_{R}]$ . Table 1 illustrates the connection between   \n116 message passing in various HTGNNs and HTMP mechanism.   \n117 (ii) Considering the existence of multiple neighborhoods, the COMBINE function in HTMP need to   \n118 integrate multiple messages instead of only the ego node and the raw neighborhood. Thus, the input   \n119 of the COMBINE function is a set of messages $\\widetilde{\\mathbf{Z}}_{r}^{l}$ aggregated from the corresponding neighborhoods.   \n120 In HTGNNs, addition and concatenation are t wo common approaches, each of which has variants.   \n121 An effective COMBINE function is capable of simultaneously processing messages from various   \n122 neighborhoods while preserving their distinct features, thereby reducing the effects of heterophily.   \n123 (iii) In VMP, the final output representations are usually the one of the final layer: ${\\bf Z}={\\bf Z}^{L}$ . Some   \nHTGNNs utilize the combination of intermediate representations to leverage messages from different   \n125 localities, adapting to the heterophilous structural properties in different graphs. Thus, we introduce   \n126 an additional FUSE function in HTMP which integrates multiple representations ${\\bf Z}^{l}$ of different   \n127 layers $l$ into the final Z. Similarly, the FUSE function is based on addition and concatenation. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "128 4 Why Does Message Passing Still Remain Effective in Heterophilous 129 Graphs? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "130 Based on HTMP mechanism, we further dive into the motivation behind the message passing of   \n131 existing HTGNNs. Our discussion begins by examining the difference between homophilous and   \n132 heterophilous graphs. Initially, we consider the homophily ratios $h^{e}$ and $h^{n}$ , as outlined in Section 2.   \n133 However, a single number is not able to indicate enough conditions of a graph. Ma et al. [6] propose   \n134 the existence of a special case of heterophily, named \"good\" heterophily, where the VMP mechanism   \n135 can achieve strong performance and the homophily ratio shows no difference. Thus, to better study   \n136 the heterophily property, here we introduce the Compatibility Matrix [7] to describe graphs:   \n137 Definition 1 Compatibility Matrix (CM): The potential connection preference among classes within   \n138 a graph. It\u2019s formatted as a matrix $\\mathbf{M}\\in\\mathbb{R}^{K\\times K}$ , where the $i$ -th row $\\mathbf{M}_{i}$ denotes the connection   \n139 probabilities between class $i$ and all classes. It can be estimated empirically by the statistics among   \n140 nodes as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{M}=N o r m(\\mathbf{C}^{T}\\mathbf{C}^{n b}),\\quad\\mathbf{C}^{n b}=\\hat{\\mathbf{A}}\\mathbf{C},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "141 where Norm $(\\cdot)$ denotes the L1 normalization and $T$ is the matrix transpose operation. ${\\bf C}^{n b}\\in{\\bf R}^{N\\times K}$   \n142 is the semantic neighborhoods of nodes, which indicates the proportion of neighbors from each class   \n143 in nodes\u2019 neighborhoods.   \n144 We visualize the CM of a homophilous graph Photo [29] and a heterophilous graph Amazon  \n145 Ratings [30] in Figure 1(a) and 1(b). The CM in Photo displays an identity-like matrix, where the   \n146 diagonal elements can be viewed as the homophily level of each class. With this type of CM, the VMP   \n147 mechanism learns representations comprised mostly of messages from same the class, while messages   \n148 of other classes are diluted. Then how does HTMP mechanism work on heterophilous graphs without   \n149 an identity-like CM? The \"good\" heterophily inspires us, which we believe corresponds to a CM with   \n150 enough discriminability among classes. We conduct experiments on synthetic graphs to confirm this   \n151 idea, with details available in Appendix C. Also, we find \"good\" heterophily in real-world graphs   \n152 though it\u2019s not as significant as imagined. Thus, we have the following observation:   \n153 Observation 1 (Connection between CM and VMP). When enough (depends on data) discriminabil  \n154 ity exists among classes in CM, vanilla message passing can work well in heterophilous graphs.   \n155 With this observation, we have a conjecture: Is HTMP mechanism trying to enhance the discriminabil  \n156 ity of CM? Some special designs in HTMP intuitively meet this. For example, feature-similarity-based   \n157 neighborhood indicators and neighborhood discrimination are designed to construct neighborhoods   \n158 with high homophily, that is, an identity-like CM with high discriminability. We plot the CM of   \n159 feature-similarity-based neighborhood on Amazon-Ratings in Figure 1(c) to confirm it. Moreover,   \n160 we investgate two representative methods ACM-GCN [18] and GPRGNN [20], showing that they   \n161 also meet this conjecture with the posterior proof in Appendix D. ACM-GCN combines the messages   \n162 of node ego, low-frequency and high-frequency with adaptive weights, which actually motifs the   \n163 edge weights and node weights to build a new CM. GPRGNN has a FUSE function with adaptive   \n164 weights while other settings are the same as GCN. It actually integrates the CMs of multiple-order   \n165 neighborhoods with adaptive weights to form a more discriminative CM. These lead to the answer to   \n166 the aforementioned question:   \n167 Observation 2 (Connection between CM and HTMP). The unified goal of various message passing   \n168 in existing HTGNNs is to utilize and enhance the discriminability of CM on heterophilous graphs.   \n169 In other words, the success of message passing in existing HTGNNs benefits from utilizing and   \n170 enhancing the discriminability of CM.   \n171 Furthermore, we notice that the power of CM is not fully released due to the incomplete and noisy   \n172 semantic neighborhoods in real-world heterophilous graphs. We use the perspective of distribution   \n173 to describe the issue more intuitively: The semantic neighborhoods of nodes from the same class   \n174 collectively form a distribution, whose mean value indicates the connection preference of that class,   \n175 i.e. $\\mathbf{M}_{i}$ for class $i$ . Influenced by factors such as degree and randomness, the semantic neighborhood   \n176 of nodes in real-world graphs may display only a fraction of CM accompanied by noise. It can   \n177 lead to the overlap between different distributions as shown in Figure 1(d), where the existence of   \n178 overlapping parts means nodes from different classes may have the same semantic neighborhood.   \n179 This brings a great challenge since the overlapping semantic neighborhood may become redundant   \n180 information during message passing. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "image", "img_path": "7g8WSOHJtP/tmp/f4e7b3e4ffeb912f421b51745c5bd3794ca13ec28385aa93f4a11e916edd78e9.jpg", "img_caption": ["Figure 1: Visualizations of the compatibility matrix and the example of distribution overlap. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "181 5 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "182 To fill this gap, we further propose a method named Compatibility Matrix-Aware GNN (CMGNN),   \n183 which leverages the CM to construct desired neighborhood messages as supplementary, providing   \n184 valuable neighborhood information for nodes to mitigate the impact of incomplete and noisy se  \n185 mantic neighborhoods. The desired neighborhood message denotes the averaging message within   \n186 a neighborhood when a node\u2019s semantic neighborhoods meet the CM of the corresponding class,   \n187 which converts the discriminability from CM into messages. CMGNN follows the HTMP mechanism   \n188 and constructs a supplementary neighborhood indicator along with the corresponding aggregation   \n189 guidance to introduce supplementary messages. Further, CMGNN introduces a simple constraint to   \n190 explicitly enhance the discriminability of CM.   \n191 Message Passing in CMGNN. CMGNN aggregates messages from three neighborhoods for   \n192 each node, including the ego neighborhood, raw neighborhood, and supplementary neighborhood.   \n193 Following the HTMP mechanism, the message passing of CMGNN cen be described as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathbf Z}_{r}^{l}=\\mathrm{AGGREGATE}(\\mathbf A_{r},\\mathbf B_{r},\\mathbf Z^{l-1})=(\\mathbf A_{r}\\odot\\mathbf B_{r})\\mathbf Z^{l-1}\\mathbf W_{r}^{l},}\\\\ &{\\mathbf Z^{l}=\\mathrm{COMBINE}(\\{\\widetilde{\\mathbf Z}_{r}^{l}\\}_{r=1}^{3})=\\mathrm{AdaWeight}(\\{\\widetilde{\\mathbf Z}_{r}^{l}\\}_{r=1}^{3}),}\\\\ &{\\mathbf Z=\\mathrm{FUSE}(\\{\\mathbf Z^{l}\\}_{l=0}^{L})=\\underset{l=0}{\\overset{L}{\\prod}}\\mathbf Z^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "194 where AdaWeight is the adaptive weighted addition implemented by an MLP with Softmax, $\\parallel$ denotes   \n195 the concatenation. The neighborhood indicators and aggregation guidance of the three neighborhoods   \n196 are formatted as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{A}_{1}^{l}=\\mathbf{I},\\ \\mathbf{B}_{1}^{l}=\\mathbf{I},\\ \\ \\ \\mathbf{A}_{2}^{l}=\\mathbf{A},\\ \\mathbf{B}_{2}^{l}=\\mathbf{D}^{-1}\\mathbf{1},\\ \\ \\ \\mathbf{A}_{3}^{l}=\\mathbf{A}^{s u p},\\ \\mathbf{B}_{3}^{l}=\\mathbf{B}^{s u p},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "197 where $\\mathbf{A}^{s u p}$ and $\\mathbf{B}^{s u p}$ are described below. ", "page_idx": 5}, {"type": "text", "text": "198 The supplementary neighborhood indicator $\\mathbf{A}^{s u p}$ assigns $K$ additional virtual neighbors for each   \n199 node: $\\mathbf{\\dot{A}}^{s u p}=\\mathbf{1}\\in\\mathbb{R}^{N\\times K}$ . Specifically, these additional neighbors are $K$ virtual nodes, constructed   \n200 as the prototypes of classes based on the labels of the training set. The attributes $\\mathbf{X}^{p t t}\\in\\mathbb{R}^{K\\times d_{f}}$ ,   \n201 neighborhoods $\\mathbf{A}^{p t t}\\in\\mathbb{R}^{K\\times N}$ and labels $\\mathbf{Y}^{p t t}\\in\\mathbb{R}^{K\\times K}$ of prototypes are defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbf{X}}^{p t t}=\\operatorname{Norm}({\\mathbf{C}_{t r a i n}}^{T}{\\mathbf{X}_{t r a i n}}),\\;{\\mathbf{A}^{p t t}}=\\mathbf{0},\\;{\\mathbf{Y}^{p t t}}=\\mathbf{I},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "202 where $\\mathbf{C}_{t r a i n}$ and $\\mathbf{X}_{t r a i n}$ are the one-hot labels and attributes of nodes in the training set. Utilizing   \n203 class prototypes as supplementary neighborhoods can provide each node with representative messages   \n204 of classes, which builds the basis for desired neighborhood messages.   \n205 The supplementary aggregation guidance $\\mathbf{B}^{s u p}=\\hat{\\mathbf{C}}\\hat{\\mathbf{M}}$ indicates the desired semantic neighborhood   \n206 of nodes, i.e. the desired proportion of neighbors from each class in nodes\u2019 neighborhoods according   \n207 to the probability that nodes belong to each class. $\\hat{{\\bf M}}$ is the estimated compatibility matrix described   \n208 in below. Using soft logits instead of one-hot pseudo labels preserves the real characteristics of nodes   \n209 and reduces the impact of wrong predictions. During the message aggregation in the supplementary   \n210 neighborhoods, the input representations ${\\bf Z}^{l-1}$ are replaced by the representations of virtual prototype   \n211 nodes $\\mathbf{Z}_{p t t}^{l-1}$ , which are obtained by the same message-passing mechanism as real nodes.   \n212 Similar to existing methods [18, 19], we also regard topology structure as a kind of additional   \n213 available node features. Thus, the input representation of the first layer can be obtained in two ways: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}^{0}=[\\mathbf{X}\\mathbf{W}^{X}\\lVert\\hat{\\mathbf{A}}\\mathbf{W}^{A}\\rVert\\mathbf{W}^{0},\\mathrm{~or~}\\mathbf{Z}^{0}=\\mathbf{X}\\mathbf{W}^{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "214 Note that in practice, we use ReLU as the activation function between layers. From the perspective of   \n215 HTMP mechanism, our special design is to introduce an additional neighborhood indicator $\\mathbf{A}^{s u p}$ by   \n216 neighborhood redefining and aggregation guidance $\\mathbf{B}^{s u p}$ , which can be seen as a form of relation   \n217 estimation along with good interpretability. Meanwhile, these designs greatly reduce the time and   \n218 space cost via the $N\\times K$ form.   \n219 Compatibility Matrix Estimation. The CM can be directly calculated via Eq 3 with full-available   \n220 labels. However, the label information is not entirely available in semi-supervised settings. Thus, we   \n221 try to estimate the CM with the help of semi-supervised and pseudo labels. Since the pseudo labels   \n222 predicted by the model might be wrong, which can lead to low-quality estimation, we introduce the   \n223 confidence $\\mathbf{g}\\in\\mathbb{R}^{N\\times1}$ based on the information entropy to reduce the impact of wrong predictions,   \n224 where a high entropy means low confidence: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{g}_{i}=\\log K-\\mathsf{H}(\\hat{\\mathbf{C}}_{i})\\in[0,\\log K],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "225 where $\\hat{\\mathbf{C}}\\in\\mathbb{R}^{N\\times K}$ is the soft pseudo labels composed of labels from the training set and model   \n226 predictions. Then the nodes\u2019 semantic neighborhoods $\\mathbf{C}^{n b}\\,=\\,\\mathrm{Norm}(\\mathbf{A}(\\mathbf{g}\\cdot\\hat{\\mathbf{C}}))\\,\\in\\,\\mathbb{R}^{N\\times K}$ are   \n227 calculated considering the confidence.   \n228 Further, the degrees of nodes also influence the estimation. As we mentioned in Section 4, the   \n229 semantic neighborhood of low-degree nodes may display incomplete CM, leading to a significant gap   \n230 between semantic neighborhoods and corresponding CM. Thus, they deserve low weights during the   \n231 estimation. We manually set up two fixed thresholds and a weighting function range in $[0,1]$ : ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{w}_{i}^{d}=\\left\\{\\begin{array}{c c}{\\mathbf{d}_{i}/2K,}&{\\mathbf{d}_{i}\\le K,}\\\\ {0.25+\\mathbf{d}_{i}/4K,}&{K<\\mathbf{d}_{i}\\le3K,}\\\\ {1,}&{o t h e r w i s e.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "232 When a node\u2019s degree ${\\bf d}_{i}$ is smaller than the number of classes $K$ , its semantic neighborhood is   \n233 unlikely to display complete CM, corresponding to a low weight. And when the node degree is   \n234 greater than $3K$ , we believe it can display near-complete CM, corresponding to the maximum weight.   \n235 Finally, we can estimate the compatibility matrix $\\bar{\\mathbf{M}}\\in\\mathbb{R}^{K\\times K}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{M}}=\\mathbf{Norm}((\\mathbf{w}^{d}\\cdot\\mathbf{g}\\cdot\\hat{\\mathbf{C}})^{T})\\mathbf{C}^{n b}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/f0d914bd9c4a991a42b3bd4e36089f536336e80db64066e8e21ccb4fee82528a.jpg", "table_caption": ["Table 2: Node classification accuracy comparison $(\\%)$ . The error bar $(\\pm)$ denotes the standard deviation of results over 10 trial runs. The best and second-best results in each column are highlighted in bold font and underlined. OOM denotes out-of-memory error during the model training. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "236 Objective Function. As mentioned in Sec 4, the CMs in real-world graphs don\u2019t always have   \n237 significant discriminability, which may lead to low effectiveness of supplementary messages. Thus, we   \n238 introduce an additional discrimination loss $\\mathcal{L}_{d i s}$ to reduce the similarity of the desired neighborhood   \n239 message among different classes, which enhances the discriminability among classes in CM. The   \n240 overall loss consists of a CrossEntropy loss $\\mathcal{L}_{c e}$ and the discrimination loss $\\mathcal{L}_{d i s}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{c e}+\\lambda\\mathcal{L}_{d i s},\\quad\\mathcal{L}_{d i s}=\\sum_{i\\neq j}\\mathrm{Sim}(\\hat{\\bf M}_{i}{\\bf Z}_{p t t},\\hat{\\bf M}_{j}{\\bf Z}_{p t t}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "241 where $\\mathbf{Z}_{p t t}\\,\\in\\,\\mathbb{R}^{K\\times d_{r}}$ is the representation of virtual prototypes nodes. More details about the   \n242 implementation of CMGNN is available in Appendix $\\boldsymbol{\\mathrm E}$ . ", "page_idx": 6}, {"type": "text", "text": "243 6 Benchmarks and Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "244 In this section, we conduct comprehensive experiments to demonstrate the effectiveness of the   \n245 proposed CMGNN with a newly organized benchmark for fair comparisons. ", "page_idx": 6}, {"type": "text", "text": "246 6.1 New Benchmark ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "247 As reported in [30], some widely adopted datasets in existing works have critical drawbacks, which   \n248 lead to unreliable results. Therefore, with a comprehensive review of existing benchmark evaluation,   \n249 we construct a new benchmark to fairly perform experimental validation. Specifically, we integrate 13   \n250 representative homophilous and heterophilous GNNs, construct a unified codebase, and evaluate their   \n251 node classification performances on 10 unified organized datasets with various heterophily levels.   \n252 Drawbacks of Existing Datasets. Existing works mostly follow the settings and datasets used   \n253 in [15], including 6 heterophilous datasets (Cornell, Texas, Wisconsin, Actor, Chameleon, and   \n254 Squirrel) and 3 homophilous datasets (Cora, Citeseer, and Pubmed). Platonov et al. [30] pointed out   \n255 that there are serious data leakages in Chameleon and Squirrel, while Cornell, Texas, and Wisconsin   \n256 are too small with very imbalanced classes. Further, we revisit other datasets and discover new   \n257 drawbacks: (i) In the ten splits of Citeseer, there are two inconsistent ones, which have smaller   \n258 training, validation, and test sets that could cause issues with statistical results; (ii) The data split   \n259 ratios for Cora are not consistent with the expected ones. These drawbacks may lead to certain issues   \n260 with the conclusions of previous works. The detailed descriptions of dataset drawbacks are listed in   \n261 Appendix F.1.   \n262 Newly Organized Datasets. The datasets used in the benchmark include Roman-Empire, Amazon  \n263 Ratings, Chameleon-F, Squirrel-F, Actor, Flickr, BlogCatalog, Wikics, Pubmed, and Photo. Their   \n264 statistics are summarized in Table 2, with details in Appendix F.2. For consistency with existing meth  \n265 ods, we randomly construct 10 splits with predefined proportions $(48\\%/32\\%/20\\%$ for train/valid/test)   \n266 for each dataset and report the mean performance and standard deviation of 10 splits.   \n267 Baseline Methods. As baseline methods, we choose 13 representative homophilous and het  \n268 erophilous GNNs, including (i) shallow base model: MLP; (ii) homophilous GNNs: GCN [1],   \n269 GAT [28], GCNII [27]; (iii) heterophilous GNNs: H2GCN [12], MixHop [16], GBK-GNN [24],   \n270 GGCN [23], GloGNN [19], HOGGCN [17], GPR-GNN [20], ACM-GCN [18] and OrderedGNN [21].   \n271 For each method, we integrate its official/reproduced code into a unified codebase and search for   \n272 parameters in the space suggested by the original papers. More experimental settings can be found in   \n273 Appendix F.4 and G.1. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/233e7ffb5392cb71083f4824c10f0bbfcd865af378dcf51035e0d0cc594082b1.jpg", "table_caption": ["Table 3: Ablation study results $(\\%)$ between CMGNN and three ablation variants, where SM denotes supplementary messages of the desired neighborhoods and DL denotes the discrimination loss. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "274 6.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "275 Following the constructed benchmark, we evaluate methods and report the performance in Table 2. ", "page_idx": 7}, {"type": "text", "text": "276 Performance of Baseline Methods. With the new benchmarks, some interesting observations and   \n277 conclusions can be found when analyzing the performance of baseline methods. First, comparing the   \n278 performance of MLP and GCN, we can find \"good\" heterophily in Amazon-Ratings, Chameleon-F,   \n279 and Squirrel-F. Meanwhile, when the homophily level is not high enough, \"bad\" homophily may also   \n280 exist as shown in BlogCatalog and Wikics. These results once again support the observations about   \n281 CMs. Therefore, homophilous GNNs can also work well in heterophilous graphs as GCNII has   \n282 an average rank of 4.1, which is better than most HTGNNs. This is attributed to the initial residual   \n283 connection in GCNII actually playing the role of ego/neighbor separation, which is suitable in   \n284 heterophilous graphs. As for heterophilous GNNs, they are usually designed for both homophilous   \n285 and heterophilous graphs. Surprisingly, MixHop, as an early method, demonstrated quite good   \n286 performance. In fact, from the perspective of HTMP, it can be considered a degenerate version   \n287 of OrderedGNN with no learnable dimensions. As previous SOTA methods, OrderedGNN and   \n288 ACM-GCN prove their strong capabilities again.   \n289 Performance of CMGNN. CMGNN achieves the best performance in 6 datasets and an average   \n290 rank of 2.1, which outperforms baseline methods. This demonstrates the superiority of utilizing   \n291 and enhancing the CM to handle incomplete and noisy semantic neighborhoods, especially in   \n292 heterophilous graphs. Regarding the suboptimal performance in Actor, we believe that this is due   \n293 to the CM in this dataset are not discriminative enough to provide valuable information via the   \n294 supplementary messages and hard to enhance. In homophilous graphs, due to the identity-like CMs,   \n295 the overlap between distributions is relatively less, leading to a minor contribution from supplement   \n296 messages. Yet CMGNN still achieves top-level performances. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "297 6.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "298 We conduct an ablation study on two key designs of CMGNN , including the supplementary messages   \n299 of the desired neighborhood (SM) and the discrimination loss (DL). The results are shown in Table 3.   \n300 First of all, both SM and DL have indispensable contributions except for Flickr, BlogCatalog, and   \n301 Pubmed, in which the discrimination loss has no effect. This may be due to the discriminability of   \n302 desired neighborhood messages reaching the bottlenecks and can not be further improved by DL   \n303 Meanwhile, the extent of their contributions varies across datasets. SM plays a more important role in   \n304 most datasets except Roman-Empire, Wikics, and Photo, in which the number of nodes that need   \n305 supplementary messages is relatively small and DL has great effects. Further, we notice that with   \n306 SM and DL, CMGNN can reach a smaller standard deviation most of the time. This illustrates   \n307 that CMGNN achieves more stable results by handling nodes with incomplete and noisy semantic   \n308 neighborhoods. As for the opposite result on Chameleon-F, this may attributed to the small size of   \n309 this dataset (890 nodes), which can lead to naturally unstable results. ", "page_idx": 7}, {"type": "image", "img_path": "7g8WSOHJtP/tmp/0e2182ea75f512fc965500d260d559f69726a67df3abd2b2b16f38f56c49818d.jpg", "img_caption": ["Figure 2: The visualization of observed (Obs) and estimated (Est) compatibility matrixes. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/4dbdf85e821712002ec13b5e5ccd0f57d6e4a48f0ed4f3cb84f9f24159970e4b.jpg", "table_caption": ["Table 4: Node classification accuracy $(\\%)$ comparison among nodes with different degrees. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "310 6.4 Visualization of Compatibility Matrix Estimation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "311 We visualize the observed and estimated CMs by CMGNN in Figure 2 with heat maps. Obviously,   \n312 CMGNN estimates CMs that are very close to those existing in graphs. This shows that even   \n313 with incomplete node labels, CMGNN can estimate high-quality CMs which provides valuable   \n314 neighborhood information to nodes. Meanwhile, it can adapt to graphs with various levels of   \n315 heterophily. More results can be seen in Appendix G.2.1. ", "page_idx": 8}, {"type": "text", "text": "316 6.5 Performance on Nodes with Various Levels of Degrees ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "317 To verify the effect of CMGNN on nodes with incomplete and noisy semantic neighborhoods, we   \n318 divide the test set nodes into 5 parts according to their degrees and report the classification accuracy   \n319 respectively. We compare CMGNN with 3 top-performance methods and show the results in Table 4.   \n320 In general, nodes with low degrees tend to have incomplete and noisy semantic neighborhoods.   \n321 Thus, our outstanding performances on the top $20\\%$ nodes with the least degree demonstrate the   \n322 effectiveness of CMGNN for providing desired neighborhood messages. Further, we can find that   \n323 OrderedGNN and GCNII are good at dealing with nodes with high degrees, while ACM-GCN is   \n324 relatively good at nodes with low degrees. And CMGNN , to a certain extent, can be adapted to both   \n325 situations at the same time. ", "page_idx": 8}, {"type": "text", "text": "326 7 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "327 In this paper, we revisit the message passing mechanism in existing heterophilous GNNs and   \n328 reformulate them into a unified heterophilous message passing (HTMP) mechanism. Based on the   \n329 HTMP mechanism and empirical analysis, we reveal that the reason for message passing remaining   \n330 effective is attributed to implicitly enhancing the compatibility matrix among classes. Further, we   \n331 propose a novel method CMGNN to unlock the potential of the compatibility matrix by handling the   \n332 incomplete and noisy semantic neighborhoods. The experimental results show the effectiveness of   \n333 CMGNN and the feasibility of designing a new method following HTMP mechanism. We hope the   \n334 HTMP mechanism and benchmark can further provide convenience to the community.   \n335 This work mainly focuses on the message passing mechanism in existing HTGNNs under the   \n336 semi-supervised setting. Thus, the other designs in HTGNNs such as objective functions are not   \n337 analyzed in this paper. The proposed HTMP mechanism is suitable for only a large part of existing   \n338 HTGNNs which still follow the message passing mechanism. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "339 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "340 [1] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional   \n341 networks. In International Conference on Learning Representations, 2017.   \n342 [2] Yanfu Zhang, Hongchang Gao, Jian Pei, and Heng Huang. Robust self-supervised struc  \n343 tural graph neural network for social network prediction. In Proceedings of the ACM Web   \n344 Conference 2022, pages 1352\u20131361, 2022.   \n345 [3] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collabo  \n346 rative filtering. In Proceedings of the 42nd international ACM SIGIR conference on Research   \n347 and development in Information Retrieval, pages 165\u2013174, 2019.   \n348 [4] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. Lightgcn:   \n349 Simplifying and powering graph convolution network for recommendation. In Proceedings   \n350 of the 43rd International ACM SIGIR conference on research and development in Information   \n351 Retrieval, pages 639\u2013648, 2020.   \n352 [5] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in   \n353 social networks. Annual review of sociology, 27(1):415\u2013444, 2001.   \n354 [6] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. Is homophily a necessity for graph neural   \n355 networks? In International Conference on Learning Representations, 2022.   \n356 [7] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai   \n357 Koutra. Graph neural networks with heterophily. In Proceedings of the AAAI conference on   \n358 artificial intelligence, volume 35, pages 11168\u201311176, 2021.   \n359 [8] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks   \n360 for graphs with heterophily: A survey. arXiv preprint arXiv:2202.07082, 2022.   \n361 [9] Jiong Zhu, Yujun Yan, Mark Heimann, Lingxiao Zhao, Leman Akoglu, and Danai Koutra.   \n362 Heterophily and graph neural networks: Past, present and future. IEEE Data Engineering   \n363 Bulletin, 2023.   \n364 [10] Chenghua Gong, Yao Cheng, Xiang Li, Caihua Shan, Siqiang Luo, and Chuan Shi. Towards   \n365 learning from graphs with heterophily: Progress and future. arXiv preprint arXiv:2401.09769,   \n366 2024.   \n367 [11] Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph   \n368 convolutional networks. In Proceedings of the AAAI Conference on Artificial Intelligence,   \n369 volume 35, pages 3950\u20133957, 2021.   \n370 [12] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Be  \n371 yond homophily in graph neural networks: Current limitations and effective designs. Advances   \n372 in neural information processing systems, 33:7793\u20137804, 2020.   \n373 [13] Di Jin, Zhizhi Yu, Cuiying Huo, Rui Wang, Xiao Wang, Dongxiao He, and Jiawei Han.   \n374 Universal graph convolutional networks. Advances in Neural Information Processing Systems,   \n375 34:10654\u201310664, 2021.   \n376 [14] Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. Node similarity preserving   \n377 graph convolutional networks. In Proceedings of the 14th ACM international conference on   \n378 web search and data mining, pages 148\u2013156, 2021.   \n379 [15] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geo  \n380 metric graph convolutional networks. In International Conference on Learning Representations,   \n381 2020.   \n382 [16] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr   \n383 Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional   \n384 architectures via sparsified neighborhood mixing. In international conference on machine   \n385 learning, pages 21\u201329. PMLR, 2019.   \n386 [17] Tao Wang, Di Jin, Rui Wang, Dongxiao He, and Yuxiao Huang. Powerful graph convolutional   \n387 networks with adaptive propagation mechanism for homophily and heterophily. In Proceedings   \n388 of the AAAI conference on artificial intelligence, volume 36, pages 4210\u20134218, 2022.   \n389 [18] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen   \n390 Chang, and Doina Precup. Revisiting heterophily for graph neural networks. Advances in   \n391 neural information processing systems, 35:1362\u20131375, 2022.   \n392 [19] Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and Weining Qian.   \n393 Finding global homophily in graph neural networks when meeting heterophily. In International   \n394 Conference on Machine Learning, pages 13242\u201313256. PMLR, 2022.   \n395 [20] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized   \n396 pagerank graph neural network. In International Conference on Learning Representations,   \n397 2021.   \n398 [21] Yunchong Song, Chenghu Zhou, Xinbing Wang, and Zhouhan Lin. Ordered GNN: Ordering   \n399 message passing to deal with heterophily and over-smoothing. In The Eleventh International   \n400 Conference on Learning Representations, 2023.   \n401 [22] Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. Breaking the limit of   \n402 graph neural networks by improving the assortativity of graphs with local mixing patterns. In   \n403 Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,   \n404 pages 1541\u20131551, 2021.   \n405 [23] Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. Two sides of the   \n406 same coin: Heterophily and oversmoothing in graph convolutional neural networks. In 2022   \n407 IEEE International Conference on Data Mining (ICDM), pages 1287\u20131292. IEEE, 2022.   \n408 [24] Lun Du, Xiaozhou Shi, Qiang Fu, Xiaojun Ma, Hengyu Liu, Shi Han, and Dongmei Zhang.   \n409 Gbk-gnn: Gated bi-kernel graph neural networks for modeling both homophily and heterophily.   \n410 In Proceedings of the ACM Web Conference 2022, pages 1550\u20131558, 2022.   \n411 [25] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural   \n412 message passing for quantum chemistry. In International conference on machine learning,   \n413 pages 1263\u20131272. PMLR, 2017.   \n414 [26] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate:   \n415 Graph neural networks meet personalized pagerank. In International Conference on Learning   \n416 Representations, 2019.   \n417 [27] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph   \n418 convolutional networks. In International conference on machine learning, pages 1725\u20131735.   \n419 PMLR, 2020.   \n420 [28] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and   \n421 Yoshua Bengio. Graph attention networks. In The International Conference on Learning   \n422 Representations, 2018.   \n423 [29] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann.   \n424 Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.   \n425 [30] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila   \n426 Prokhorenkova. A critical look at the evaluation of GNNs under heterophily: Are we re  \n427 ally making progress? In The Eleventh International Conference on Learning Representations,   \n428 2023.   \n429 [31] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and   \n430 Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In   \n431 International conference on machine learning, pages 5453\u20135462. PMLR, 2018.   \n432 [32] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks   \n433 on graphs with fast localized spectral filtering. Advances in neural information processing   \n434 systems, 29, 2016.   \n435 [33] Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural   \n436 network. In International Conference on Learning Representations, 2018.   \n437 [34] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large   \n438 graphs. Advances in neural information processing systems, 30, 2017.   \n439 [35] Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on   \n440 graph neural networks as graph signal denoising. In Proceedings of the 30th ACM International   \n441 Conference on Information & Knowledge Management, pages 1202\u20131211, 2021.   \n442 [36] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph   \n443 neural networks with an optimization framework. In Proceedings of the Web Conference 2021,   \n444 pages 1215\u20131226, 2021.   \n445 [37] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly   \n446 detection on attributed networks via contrastive self-supervised learning. IEEE Transactions on   \n447 Neural Networks and Learning Systems, 2021.   \n448 [38] P\u00e9ter Mernyei and Ca\u02d8ta\u02d8lina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural   \n449 networks. arXiv preprint arXiv:2007.02901, 2020.   \n450 [39] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi  \n451 Rad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \n452 [40] \u00c9douard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tom\u00e1\u0161 Mikolov. Learning   \n453 word vectors for 157 languages. In Proceedings of the Eleventh International Conference on   \n454 Language Resources and Evaluation (LREC 2018), 2018.   \n455 [41] Leskovec Jure. Snap datasets: Stanford large network dataset collection. Retrieved December   \n456 2021 from http://snap. stanford. edu/data, 2014.   \n457 [42] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding.   \n458 Journal of Complex Networks, 9(2):cnab014, 2021.   \n459 [43] Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks.   \n460 In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery   \n461 and data mining, pages 807\u2013816, 2009.   \n462 [44] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for   \n463 word representation. In Proceedings of the 2014 conference on empirical methods in natural   \n464 language processing (EMNLP), pages 1532\u20131543, 2014. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "465 A More Details of HTMP Mechanism ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "466 In this part, we list more details about the HTMP mechanism, including additional analysis about   \n467 HTMP, method-wise analysis and overall analysis. ", "page_idx": 12}, {"type": "text", "text": "468 A.1 Additional Analysis of HTMP Mechanism ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "469 A.1.1 Neighborhood Indicators ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "470 The neighborhood indicator explicitly marks the neighbors of all nodes within a specific neighbor  \n471 hood. In existing heterophilous GNNs, neighborhood indicators typically take one of the following   \n472 forms: (i) Raw Neighborhood (Raw); (ii) Neighborhood Redefining (ReDef); and (3) Neighborhood   \n473 Discrimination (Dis).   \n474 Raw Neighborhood. Raw neighborhood, including A and A\u02dc, provides the basic neighborhood   \n475 information. The only difference between them lies in whether there is differential treatment of the   \n476 node\u2019s ego messages. For example, APPNP [26] applies additional weighting to the nodes\u2019 ego   \n477 messages compared with GCN [1]. For the sake of simplicity, we consider the identity matrix $\\mathbf{I}\\in$   \n478 $\\mathbb{R}^{N\\times N}$ as a special neighborhood indicator for acquiring the nodes\u2019 ego messages. In heterophilous   \n479 GNNs, ego/neighbor separation is a common strategy that can mitigate the confusion of ego messages   \n480 with neighbor messages.   \n481 Neighborhood Redefining. Neighborhood redefining is the most commonly used technique in   \n482 heterophilous GNNs, aiming to capture additional information from new neighborhoods. As a repre  \n483 sentative example, high-order neighborhood $\\mathbf{A}_{h}$ can provide long-distance connection information   \n484 but also result in additional computational costs. Feature-similarity-based neighborhood $\\mathbf{A}_{f}$ is often   \n485 defined by the k-NN relationships within the feature space. Fundamentally, it only utilizes node   \n486 features and thus needs to be used in conjunction with other neighborhood indicators. Otherwise,   \n487 the model will be limited by the amount of information in node features. GloGNN [19] introduces   \n488 fully-connected neighborhood $\\mathbf{1}\\in\\mathbb{R}^{N\\times N}$ , which can capture global neighbor information from all   \n489 nodes. However, it can also cause significant time and space consumption. Additionally, there are   \n490 some custom-defined neighborhood $\\mathbf{A}_{c}$ . For example, Geom-GCN [15] redefines neighborhoods   \n491 based on the geometric relationships between node pairs. These neighborhood indicators may have   \n492 limited generality, and the effectiveness is reliant on the specific method.   \n493 Neighborhood Discrimination. Neighborhood discrimination aims to mark whether neighbors   \n494 share the same label with central nodes. The neighborhoods are partitioned into positive $\\mathbf{A}_{p}$ and   \n495 negative ones $\\mathbf{A}_{n}$ , which include homophilous and heterophilous neighbors respectively. GGCN [23]   \n496 divides the raw neighborhood based on the similarity of node representations with a threshold   \n497 of 0. Explicitly distinguishing neighbors allows for targeted processing, making the model more   \n498 interpretable. However, its performance is influenced by the accuracy of the discrimination, which   \n499 may lead to the accumulation of errors. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "500 A.1.2 Aggregation Guidance ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "501 After identifying the neighborhood, the aggregation guidance controls what type of messages to   \n502 gather from the corresponding neighbors. The existing aggregation guidance mainly includes three   \n503 kinds of approaches: (1) Degree Averaging (DegAvg), (2) Adaptive Weights (AdaWeight), and (3)   \n504 Relationship Estimation (RelaEst).   \n505 Degree Averaging. Degree averaging, formatted as $\\mathbf{B}^{d}=\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{1}\\mathbf{D}^{-\\frac{1}{2}}$ or $\\mathbf{B}^{d}=\\mathbf{D}^{-1}\\mathbf{1}$ , is the most   \n506 common aggregation guidance, which plays the role of a low-pass fliter to capture the smooth signals   \n507 and is fixed during model training. Further, combining negative degree averaging with an identity   \n508 aggregation guidance $\\mathbf{I}\\in\\mathbb{R}^{N\\times N}$ can capture the difference between central nodes and neighbors, as   \n509 used in ACM-GCN [18]. Degree averaging is simple and efficient but depends on the discriminability   \n510 of corresponding neighborhoods.   \n511 Adaptive Weights. Another common strategy is allowing the model to learn the appropriate aggrega  \n512 tion guidances ${\\bf B}^{a w}$ . GAT [28] proposes an attention mechanism to learn aggregate weights, which   \n513 guides many subsequent heterophilous methods. To better handle heterophilous graphs, FAGCN [11]   \n514 introduces negative-available attention weights $\\mathbf{B}^{n a w}$ to capture the difference between central nodes   \n515 and heterophilous neighbors. Adaptive weights can personalize message aggregation for different   \n516 neighbors, yet it\u2019s difficult for models to attain the desired effect.   \n517 Relationship Estimation. Recently, some methods have tried to estimate the pair-wise relationships   \n518 ${\\bf B}^{r e}$ between nodes and use them to guide message aggregation. HOG-GCN [17] estimates the   \n519 pair-wise homophily levels between nodes as aggregation guidances based on both attribute and   \n520 topology space. GloGNN [19] treats all nodes as neighbors and estimates a coefficient matrix   \n521 as aggregation guidance based on the idea of linear subspace expression. GGCN [23] estimates   \n522 appropriate weights for message aggregation with the degrees of nodes and the similarities between   \n523 node representations. Relationship estimation usually has theoretical guidance, which brings strong   \n524 interpretability. However, it may also result in significant temporal and spatial complexity when   \n525 estimating pair-wise relations. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "526 A.1.3 COMBINE Function ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "527 After message aggregation, the COMBINE functions integrate messages from multiple neighborhoods   \n528 into layer representations. COMBINE functions in heterophilous GNNs are commonly based on   \n529 two operations: addition and concatenation, each of which has variants. To merge several messages   \n530 together, addition (Add) is a naive idea. Further, to control the weight of messages from different   \n531 neighborhoods, weighted addition (WeightedAdd) is applied. However, it is a global setting and   \n532 cannot adapt to the differences between nodes. Thus, adaptive weighted addition (AdaAdd) is   \n533 proposed, which can learn personalized message combination weights for each node, but it will result   \n534 in additional time consumption. Although the addition is simple and efficient, some methods [12, 16]   \n535 believe that it may blur messages from different neighborhoods, which can be harmful in heterophilous   \n536 GNNs, so they employ a concatenation operation (Cat) to separate the messages. Nevertheless, such   \n537 an approach not only increases the space cost but may also retain additional redundant messages. To   \n538 address these issues, OrderedGNN [21] proposes an adaptive concatenation mechanism (AdaCat)   \n539 that can combine multiple messages with learnable dimensions. This is an innovative and worthy   \n540 further exploration practice, but the difficulty of model learning should also be considered. ", "page_idx": 13}, {"type": "text", "text": "541 A.1.4 FUSE Function ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "542 Further, the FUSE functions integrate messages from multiple layers into the final representation. For   \n543 the FUSE function, utilizing the representation of the last layer as the final representation is widely   \n544 accepted: ${\\bf Z}={\\bf Z}^{L}$ . JKNet [31] proposes that the combination of representations from intermediate   \n545 layers can capture both local and global information. H2GCN [12] applies it in heterophilous graphs,   \n546 preserving messages from different localities with concatenation. Similarly, GPRGNN [20] combines   \n547 the representations of multiple layers into the final representation through adaptive weighted addition. ", "page_idx": 13}, {"type": "text", "text": "548 A.1.5 AGGREGATE function ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "549 The most commonly used AGGREGATE function is $\\begin{array}{r}{\\mathbf{AGGREGATE}(\\mathbf{A}_{r},\\mathbf{B}_{r},\\mathbf{Z}_{r}^{l-1})\\,=\\,(\\mathbf{A}_{r}\\odot}\\end{array}$   \n550 $\\mathbf{B}_{r})\\mathbf{Z}_{r}^{l-1}\\mathbf{W}_{r}^{l}$ . We take this as the fixed form of the AGGREGATE function following. Actually,   \n551 the input representations $\\mathbf{Z}_{r}^{-1}$ and weight matrixes $\\mathbf{W}_{r}^{l}$ also can be specially designed. Taking   \n552 the initial node representations ${\\bf Z}^{0}$ as input is a relatively common approach as in APPNP [26],   \n553 GCNII [27], FAGCN [11] and GloGNN [19]. Further, GCNII [27] adds an identity matrix ${\\bf I}_{w}$ to the   \n554 weight matrixes to keep more original messages. However, the methods that specially design these   \n555 components are few and with a similar form. Thus, we don\u2019t discuss them too much, but leave it for   \n556 future extensions. ", "page_idx": 13}, {"type": "text", "text": "557 A.2 Revisiting Representative GNNs with HTMP Mechanism ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "558 In this part, we utilize HTMP mechanism to revisit the representative GNNs. We start from ho  \n559 mophilous GNNs as simple examples and further extend to heterophilous GNNs. ", "page_idx": 13}, {"type": "text", "text": "560 A.2.1 GCN ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "561 Graph Convolutional Networks (GCN) [1] utilizes a low-pass fliter to gather messages from neighbors   \n562 as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{Z}^{l}=\\hat{\\tilde{\\mathbf{A}}}\\mathbf{Z}^{l-1}\\mathbf{W}^{l}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "563 It can be revisited by HTMP with the following components: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf{A}}_{0}=\\tilde{{\\mathbf{A}}},\\quad{\\mathbf{B}}_{0}={\\mathbf{B}}^{d}=\\tilde{{\\mathbf{D}}}^{-\\frac{1}{2}}{\\mathbf{1}}\\tilde{{\\mathbf{D}}}^{-\\frac{1}{2}},}\\\\ &{{\\mathbf{Z}}^{l}={\\mathbf{Z}}_{0}^{l}=({\\mathbf{A}}_{0}\\odot{\\mathbf{B}}_{0}){\\mathbf{Z}}^{l-1}{\\mathbf{W}}^{l}=\\hat{\\tilde{{\\mathbf{A}}}}{\\mathbf{Z}}^{l-1}{\\mathbf{W}}^{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "564 Specifically, GCN has a raw neighborhood indicator $\\tilde{\\mathbf A}$ and a degree averaging aggregation guidance   \n565 $\\bar{\\mathbf{B}^{d}}$ . Since there is only one neighborhood, the COMBINE function is meaningless in GCN. GCN   \n566 utilizes a naive way to fuse messages about the original neighborhood and central nodes. However, it   \n567 may confuse the representations in heterophilous graphs. ", "page_idx": 14}, {"type": "text", "text": "568 A.2.2 APPNP ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "569 PPNP [26] is also a general method whose message passing is based on Personalized PageRank   \n570 (PPR). To avoid massive consumption, APPNP is introduced as the approximate version of PPNP   \n571 with an iterative message-passing mechanism: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Z}^{l}=\\mu\\mathbf{Z}^{0}+(1-\\mu)\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "572 It can be revisited by with the following components: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{A}=[\\mathbf{A}_{0},\\ \\mathbf{A}_{1}],\\quad\\boldsymbol{B}=[\\mathbf{B}_{0},\\ \\mathbf{B}_{1}],}\\\\ &{\\mathbf{A}_{0}=\\mathbf{I},\\quad\\mathbf{B}_{0}=\\mathbf{I},\\quad\\mathbf{W}_{0}^{l}=\\mathbf{I},}\\\\ &{\\tilde{\\mathbf{Z}}_{0}^{l}=(\\mathbf{A}_{0}\\odot\\mathbf{B}_{0})\\mathbf{Z}^{0}\\mathbf{W}_{0}^{l}=\\mathbf{Z}^{0},}\\\\ &{\\mathbf{A}_{1}=\\mathbf{A},\\quad\\mathbf{B}_{1}=\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{1}\\mathbf{D}^{-\\frac{1}{2}},\\quad\\mathbf{W}_{1}^{l}=\\mathbf{I},}\\\\ &{\\tilde{\\mathbf{Z}}_{1}^{l}=(\\mathbf{A}_{1}\\odot\\mathbf{B}_{1})\\mathbf{Z}^{l-1}\\mathbf{W}_{1}^{l}=\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "573 Specifically, APPNP aggregates messages from node ego and neighborhoods separately and combines   \n574 them with a weighted addition. Compared with GCN, APPNP assigns adjustable weights to nodes,   \n575 for controlling the proportion of ego and neighbor messages during message-passing, which becomes   \n576 a worthy design in heterophilous graphs. ", "page_idx": 14}, {"type": "text", "text": "577 A.2.3 GAT ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "578 Going a step further, Graph Attention Networks (GAT) [28] allows learnable weights for each   \n579 neighbor: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{Z}_{i}^{l}=\\sum_{j\\in\\tilde{\\mathcal{N}}(i)}\\alpha_{i j}\\mathbf{Z}_{j}^{l-1}\\mathbf{W}^{l},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "580 where $\\alpha_{i j}$ is the weight for aggregating neighbor node $j$ to center node $i$ , whose construction process   \n581 is as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha_{i j}=\\frac{\\exp\\left(e_{i j}\\right)}{\\sum_{k\\in\\tilde{\\mathcal{N}}(i)}\\exp\\left(e_{i k}\\right)},}\\\\ &{e_{i j}=\\mathbf{LeakyReLU}\\left(\\left[\\mathbf{Z}_{i}^{l-1}|\\mathbf{Z}_{j}^{l-1}\\right]\\mathbf{a}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "582 Let $\\mathbf{P}^{G A T}$ be the matrix of aggregation weights in GAT: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{P}_{i j}^{G A T}=\\left\\{\\begin{array}{c c}{\\alpha_{i j},}&{\\tilde{\\mathbf{A}}_{i j}=1,}\\\\ {0,}&{\\tilde{\\mathbf{A}}_{i j}=0.}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "583 HTMP can revisit GAT with the following components: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf{A}}_{0}=\\tilde{\\bf{A}},\\quad{\\bf{B}}_{0}={\\bf{B}}^{a w}={\\bf{P}}^{G A T},}\\\\ &{{\\bf{Z}}^{l}={\\bf{Z}}_{0}^{l}=({\\bf{A}}_{0}\\odot{\\bf{B}}_{0}){\\bf{Z}}^{l-1}{\\bf{W}}^{l}={\\bf{P}}^{G A T}{\\bf{Z}}^{l-1}{\\bf{W}}^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "584 which is the matrix version of Eq 16. Specifically, GAT aggregate messages from raw neighborhood   \n585 $\\tilde{\\mathbf A}$ with adaptive weights ${\\bf B}^{a w}$ . Aggregation guidance with adaptive weights is a nice idea, but simple   \n586 constraints are not enough for the model to learn ideal results. ", "page_idx": 14}, {"type": "text", "text": "587 A.2.4 GCNII ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "588 GCNII [27] is a novel homophilous GNN with two key designs: initial residual connection and   \n589 identity mapping, which can be formatted as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{Z}^{l}=\\left(\\alpha\\mathbf{Z}^{0}+(1-\\alpha)\\tilde{\\mathbf{D}}^{-\\frac{1}{2}}\\tilde{\\mathbf{A}}\\tilde{\\mathbf{D}}^{-\\frac{1}{2}}\\mathbf{Z}^{l-1}\\right)\\left(\\beta\\mathbf{W}^{l}+(1-\\beta)\\mathbf{I}_{w}\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "590 where $\\alpha$ and $\\beta$ are two predefined parameters and $\\mathbf{I}_{w}\\in\\mathbb{R}^{d_{r}\\times d_{r}}$ is an identity matrix. ", "page_idx": 15}, {"type": "text", "text": "591 From the perspective of HTMP, it can be viewed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=[\\mathbf{I},\\tilde{\\mathbf{A}}],\\quad B=[\\mathbf{I},\\tilde{\\mathbf{B}}^{d}],\\quad\\mathbf{W}_{0}^{l}=\\mathbf{W}_{1}^{l}=\\left(\\beta\\mathbf{W}^{l}+(1-\\beta)\\mathbf{I}_{w}\\right),}\\\\ &{\\tilde{\\mathbf{Z}}_{0}^{l}=(\\mathbf{I}\\odot\\mathbf{I})\\mathbf{Z}^{0}\\left(\\beta\\mathbf{W}^{l}+(1-\\beta)\\mathbf{I}_{w}\\right)=\\mathbf{Z}^{0}\\left(\\beta\\mathbf{W}^{l}+(1-\\beta)\\mathbf{I}_{w}\\right),}\\\\ &{\\tilde{\\mathbf{Z}}_{1}^{l}=(\\tilde{\\mathbf{A}}\\odot\\tilde{\\mathbf{B}}^{d})\\mathbf{Z}^{l-1}\\left(\\beta\\mathbf{W}^{l}+(1-\\beta)\\mathbf{I}_{w}\\right)=\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}\\left(\\beta\\mathbf{W}^{l}+(1-\\beta)\\mathbf{I}_{w}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "592 where the COMBINE function is weighted addition. Specifically, the first design of GCNII is a form   \n593 of ego/neighbor separation, and the second design is a novel transformation weights matrix. This can   \n594 also be specially designed, but only GCNII does this, so we won\u2019t analyze it too much and leave it as   \n595 a future extension. ", "page_idx": 15}, {"type": "text", "text": "596 A.2.5 Geom-GCN", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "597 Geom-GCN [15] is one of the most influential heterophilous GNNs, which employs the geometric   \n598 relationships of nodes within two kinds of neighborhoods to aggregate the messages through bi-level   \n599 aggregation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Z}^{l}=\\left(\\underset{i\\in\\{g,s\\}}{\\|}~\\underset{r\\in R}{\\|}~\\mathbf{Z}_{i,r}^{l}\\right)\\mathbf{W}^{l},}\\\\ &{\\mathbf{Z}_{i,r}^{l}=\\mathbf{D}_{i,r}^{-\\frac{1}{2}}\\mathbf{A}_{i,r}\\mathbf{D}_{i,r}^{-\\frac{1}{2}}\\mathbf{Z}^{l-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "600 where $\\parallel$ denotes the concatenate operator, $\\{g,s\\}$ is the set of neighborhoods including the original   \n601 graph and the latent space. $R$ is the set of geometric relationships. $\\mathbf{A}_{i,r}$ is the corresponding adjacency   \n602 matrix in neighborhood $i$ and relationship $r$ . ", "page_idx": 15}, {"type": "text", "text": "603 It can be revisited by HTMP with the following components: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{A}=[\\mathbf{A}_{i,r}|i\\in\\{g,s\\},r\\in R],\\quad\\boldsymbol{B}=[\\mathbf{B}_{i,r}^{d}||i\\in\\{g,s\\},r\\in R],}\\\\ &{\\widetilde{\\mathbf{Z}}_{i,r}^{l}=(\\mathbf{A}_{i,r}\\odot\\mathbf{B}_{i,r}^{d})\\mathbf{Z}_{l-1}\\mathbf{W}_{i,r}^{l}=\\mathbf{D}_{i,r}^{-\\frac{1}{2}}\\mathbf{A}_{i,r}\\mathbf{D}_{i,r}^{-\\frac{1}{2}}\\mathbf{Z}^{l-1}\\mathbf{W}_{i,r}^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "604 where the COMBINE function is concatenation and the weight matrix $\\mathbf{W}^{l}$ in $\\operatorname{Eq}22$ can be viewed as   \n605 the combination of multiple $\\mathbf{W}_{i,r}^{l}$ . Specifically, Geom-GCN redefines multiple neighborhoods based   \n606 on the customized geometric relations in both raw and latent space. The messages are aggregated   \n607 from each neighborhood and combined by a concatenation. This approach may be applicable to some   \n608 datasets, yet it has weak universality. ", "page_idx": 15}, {"type": "text", "text": "609 A.2.6 H2GCN ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "610 H2GCN [12] is also an influential method with three key designs: ego- and neighbor-message   \n611 separation, higher-order neighborhoods, and the combination of intermediate representations. Its   \n612 single-layer representations are constructed as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\bf Z}^{l}=\\left[\\hat{\\bf A}{\\bf Z}^{l-1}\\parallel\\hat{\\bf A}_{h2}{\\bf Z}^{l-1}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "613 where $\\hat{\\mathbf{A}}_{h2}$ denotes the 2-order adjacency matrix with normalization. ", "page_idx": 15}, {"type": "text", "text": "614 It can be revisited by HTMP with the following components: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{A}=[\\mathbf{A},\\mathbf{A}_{h2}],\\quad\\boldsymbol{B}=[\\mathbf{B}^{d},\\mathbf{B}_{h2}^{d}],\\quad\\mathbf{W}_{0}^{l}=\\mathbf{W}_{1}^{l}=\\mathbf{I},}\\\\ &{\\tilde{\\mathbf{Z}}_{0}^{l}=(\\mathbf{A}\\odot\\mathbf{B}^{d})\\mathbf{Z}^{l-1}\\mathbf{I}=\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1},}\\\\ &{\\tilde{\\mathbf{Z}}_{1}^{l}=(\\mathbf{A}_{h2}\\odot\\mathbf{B}_{h2}^{d})\\mathbf{Z}^{l-1}\\mathbf{I}=\\hat{\\mathbf{A}}_{h2}\\mathbf{Z}^{l-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "615 where the COMBINE function is concatenation. Meanwhile, H2GCN also uses the concatenation   \n616 as the FUSE function. Specifically, H2GCN aggregates messages from the raw and 2-order neigh  \n617 borhoods in a layer of message passing and keeps them apart in the representations. The design   \n618 of ego/neighbor separation is first introduced by H2GCN and gradually becomes a necessity for   \n619 subsequent methods. ", "page_idx": 16}, {"type": "text", "text": "620 A.2.7 SimP-GCN ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "621 SimP-GCN [14] constructs an additional graph based on the feature similarity. It has two key concepts:   \n622 (1) the information from the original graph and feature kNN graph should be balanced, and (2) each   \n623 node can adjust the contribution of its node features. Specifically, the message passing in SimP-GCN   \n624 is as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{Z}^{l}=\\left(\\mathrm{diag}(\\mathbf{s}^{l})\\hat{\\mathbf{A}}+\\mathrm{diag}(1-\\mathbf{s}^{l})\\hat{\\mathbf{A}}_{f}+\\gamma\\mathbf{D}_{K}^{l}\\right)\\mathbf{Z}^{l-1}\\mathbf{W}^{l},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "625 where $\\mathbf{s}^{l}\\in\\mathbb{R}^{n}$ is a learnable score vector that balances the effect of the original and feature graphs,   \n626 $\\mathbf{D}_{K}^{l}=\\mathrm{diag}(K_{1}^{l},K_{2}^{l},...,K_{n}^{l})$ is a learnable diagonal matrix. ", "page_idx": 16}, {"type": "text", "text": "627 It can be revisited by HTMP with the following components: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=[\\mathbf{I},\\tilde{\\mathbf{A}},\\mathbf{A}_{f}],\\quad\\mathcal{B}=[\\mathbf{I},\\tilde{\\mathbf{B}}^{d},\\mathbf{B}_{f}^{d}],}\\\\ &{\\widetilde{\\mathbf{Z}}_{0}^{l}=(\\mathbf{I}\\odot\\mathbf{I})\\mathbf{Z}^{l-1}\\mathbf{W}^{l}=\\mathbf{Z}^{l-1}\\mathbf{W}^{l},}\\\\ &{\\widetilde{\\mathbf{Z}}_{1}^{l}=(\\tilde{\\mathbf{A}}\\odot\\tilde{\\mathbf{B}}^{d})\\mathbf{Z}^{l-1}\\mathbf{W}^{l}=\\hat{\\tilde{\\mathbf{A}}}\\mathbf{Z}^{l-1}\\mathbf{W}^{l},}\\\\ &{\\widetilde{\\mathbf{Z}}_{2}^{l}=(\\mathbf{A}_{f}\\odot\\mathbf{B}_{f}^{d})\\mathbf{Z}^{l-1}\\mathbf{W}^{l}=\\hat{\\mathbf{A}}_{f}\\mathbf{Z}^{l-1}\\mathbf{W}^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "628 where the COMBINE function is adaptive weighted addition. Specifically, SimP-GCN aggregates   \n629 messages from ego, raw and feature-similarity-based neighborhoods, and combines them with   \n630 node-specific learnable weights. The feature-similarity-based neighborhoods can provide more   \n631 homophilous messages to enhance the discriminability of the compatibility matrix. However, it\u2019s still   \n632 limited by the amount of information on node features. ", "page_idx": 16}, {"type": "text", "text": "633 A.2.8 FAGCN ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "634 FAGCN [11] proposes considering both low-frequency and high-frequency information simultane  \n635 ously, and transferring them into the negative-allowable weights during message passing: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\bf Z}_{i}^{l}=\\mu{\\bf Z}_{i}^{0}+\\sum_{j\\in\\mathcal{N}_{i}}\\frac{\\alpha_{i j}^{G}}{\\sqrt{d_{i}d_{j}}}{\\bf Z}_{j}^{l-1},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "636 where $\\alpha_{i j}^{G}$ can be negative as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha_{i j}^{G}=\\operatorname{tanh}(\\mathbf{g}^{T}[\\mathbf{X}_{i}\\|\\mathbf{X}_{j}]),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "637 which can form a weight matrix: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{P}_{i j}^{F A G}=\\left\\{\\begin{array}{c c}{\\alpha_{i j}^{G},}&{\\mathbf{A}_{i j}=1,}\\\\ {0,}&{\\mathbf{A}_{i j}=0.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "638 It can be revisited by HTMP with the following components: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A=[{\\bf I},{\\bf A}],\\quad B=[{\\bf I},{\\bf D}^{-\\frac{1}{2}}{\\bf P}^{F A G}{\\bf D}^{-\\frac{1}{2}}],\\quad{\\bf W}_{0}^{l}={\\bf W}_{1}^{l}={\\bf I},}\\\\ &{\\widetilde{{\\bf Z}}_{0}^{l}=({\\bf I}\\odot{\\bf I}){\\bf Z}^{0}{\\bf I}={\\bf Z}^{0},}\\\\ &{\\widetilde{{\\bf Z}}_{1}^{l}=({\\bf A}\\odot{\\bf D}^{-\\frac{1}{2}}{\\bf P}^{F A G}{\\bf D}^{-\\frac{1}{2}}){\\bf Z}^{l-1}{\\bf I}={\\bf D}^{-\\frac{1}{2}}{\\bf P}^{F A G}{\\bf D}^{-\\frac{1}{2}}{\\bf Z}^{l-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "639 where the COMBINE function is weighted addition, same as the matrix form of Eq 28. Specifically,   \n640 FAGCN aggregates messages from node ego and raw neighborhood with negative-allowable weights.   \n641 It has a similar form to GAT but allows for ego/neighbor separation and negative weights, which   \n642 means the model can capture the difference between center nodes and neighbors. ", "page_idx": 16}, {"type": "text", "text": "643 A.2.9 GGCN ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "644 GGCN [23] explicitly distinguishes between homophilous and heterophilous neighbors based on   \n645 node similarities, and assigns corresponding positive and negative weights: ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\bf Z}^{l}=\\alpha^{l}\\left(\\beta_{0}^{l}\\hat{\\bf Z}^{l}+\\beta_{1}^{l}({\\bf S}_{p o s}^{l}\\odot\\tilde{\\bf A}_{T}^{l})\\hat{\\bf Z}^{l}+\\beta_{2}^{l}({\\bf S}_{n e g}^{l}\\odot\\tilde{\\bf A}_{T}^{l})\\hat{\\bf Z}^{l}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "646 where $\\begin{array}{r}{\\hat{\\mathbf{Z}}^{l}=\\mathbf{Z}^{l-1}\\mathbf{W}^{l}+b^{l}}\\end{array}$ , $\\tilde{\\mathbf{A}}_{T}^{l}=\\tilde{\\mathbf{A}}\\odot T^{l}$ is an adjacency matrix weighted by the structure property,   \n647 $\\beta_{0}^{l},\\,\\beta_{1}^{l}$ and $\\beta_{2}^{l}$ are learnable scalars. The neighbors are distinguished by the cosine similarity of node   \n648 representations with a threshold of 0: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{S}_{i j}^{l}=\\left\\{\\begin{array}{c c}{\\mathrm{Cosine}(\\mathbf{Z}_{i},\\mathbf{Z}_{j}),}&{i\\neq j\\ \\&\\mathbf{A}_{i j}=1,}\\\\ {0,}&{\\mathrm{otherwise}.}\\end{array}\\right.,}\\\\ &{\\mathbf{S}_{p o s,\\ i j}^{l}=\\left\\{\\begin{array}{c c}{\\mathbf{S}_{i j}^{l},}&{\\mathbf{S}_{i j}^{l}>0,}\\\\ {0,}&{\\mathrm{otherwise}.}\\end{array}\\right.,}\\\\ &{\\mathbf{S}_{n e g,\\ i j}^{l}=\\left\\{\\begin{array}{c c}{\\mathbf{S}_{i j}^{l},}&{\\mathbf{S}_{i j}^{l}<0,}\\\\ {0,}&{\\mathrm{otherwise}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "649 It can be revisited by HTMP with the following components: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{A}=[\\mathbf{I},\\mathbf{A}_{p},\\mathbf{A}_{n}],\\quad\\boldsymbol{B}=[\\mathbf{I},\\mathbf{S}_{p o s}^{l}\\odot\\mathcal{T}^{l},\\mathbf{S}_{n e g}^{l}\\odot(T)^{l}],}\\\\ &{\\widetilde{\\mathbf{Z}}_{0}^{l}=(\\mathbf{I}\\odot\\mathbf{I})\\mathbf{Z}^{l-1}\\mathbf{W}^{l}=\\mathbf{Z}^{l-1}\\mathbf{W}^{l},}\\\\ &{\\widetilde{\\mathbf{Z}}_{1}^{l}=(\\mathbf{A}_{p}\\odot\\mathbf{S}_{p o s}^{l}\\odot\\mathcal{T}^{l})\\mathbf{Z}^{l-1}\\mathbf{W}^{l}=(\\mathbf{S}_{p o s}^{l}\\odot\\mathcal{T}^{l})\\mathbf{Z}^{l-1}\\mathbf{W}^{l},}\\\\ &{\\widetilde{\\mathbf{Z}}_{2}^{l}=(\\mathbf{A}_{n}\\odot\\mathbf{S}_{n e g}^{l}\\odot\\mathcal{T}^{l})\\mathbf{Z}^{l-1}\\mathbf{W}^{l}=(\\mathbf{S}_{n e g}^{l}\\odot\\mathcal{T}^{l})\\mathbf{Z}^{l-1}\\mathbf{W}^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "650 where $\\mathbf{A}_{p}$ and $\\mathbf{A}_{n}$ are discriminated by the representation similarities: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{A}_{p,i j}=\\left\\{\\begin{array}{l l}{1,}&{\\mathbf{S}_{p o s,i j}^{l}>0\\&\\mathbf{A}_{i j}=1,}\\\\ {0,}&{\\mathrm{otherwise}.}\\end{array}\\right.,}\\\\ {\\mathbf{A}_{n,i j}=\\left\\{\\begin{array}{l l}{1,}&{\\mathbf{S}_{n e g,i j}^{l}<0\\&\\mathbf{A}_{i j}=1,}\\\\ {0,}&{\\mathrm{otherwise}.}\\end{array}\\right..}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "651 The COMBINE function is an adaptive weighted addition. Specifically, GGCN divides the raw   \n652 neighborhood into positive and negative ones based on the similarities among node presentations.   \n653 On this basis, it aggregates messages from node ego, positive and negative neighborhoods, and   \n654 combines them with node-specific learnable weights. This approach allows for targeted processing   \n655 for homophilous and heterophilous neighbors, yet can suffer from the accuracy of discrimination,   \n656 which may lead to the accumulation of errors. ", "page_idx": 17}, {"type": "text", "text": "657 A.2.10 ACM-GCN ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "658 ACM-GCN [18] introduces 3 channels (identity, low pass and high pass) to capture different informa  \n659 tion and mixes them with node-wise adaptive weights: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{Z}^{l}=\\mathrm{diag}(\\boldsymbol{\\alpha}_{I}^{l})\\mathbf{Z}^{l-1}\\mathbf{W}_{I}^{l}+\\mathrm{diag}(\\boldsymbol{\\alpha}_{L}^{l})\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}\\mathbf{W}_{L}^{l}+\\mathrm{diag}(\\boldsymbol{\\alpha}_{H}^{l})(\\mathbf{I}-\\hat{\\mathbf{A}})\\mathbf{Z}^{l-1}\\mathbf{W}_{H}^{l},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "660 where $\\mathrm{diag}(\\alpha_{I}^{l}),\\mathrm{diag}(\\alpha_{L}^{l}),\\mathrm{diag}(\\alpha_{H}^{l})\\in\\mathbb{R}^{N\\times1}$ are learnable weight vectors. ", "page_idx": 17}, {"type": "text", "text": "661 It can be revisited by HTMP with the following components: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{A}=[\\mathbf{I},\\mathbf{A},\\mathbf{A}],\\quad\\boldsymbol{B}=[\\mathbf{I},\\mathbf{B}^{d},\\mathbf{I}-\\mathbf{B}^{d}],}\\\\ &{\\widetilde{\\mathbf{Z}}_{0}^{l}=(\\mathbf{I}\\odot\\mathbf{I})\\mathbf{Z}^{l-1}\\mathbf{W}_{I}^{l}=\\mathbf{Z}^{l-1}\\mathbf{W}_{I}^{l},}\\\\ &{\\widetilde{\\mathbf{Z}}_{1}^{l}=(\\mathbf{A}\\odot\\mathbf{B}^{d})\\mathbf{Z}^{l-1}\\mathbf{W}_{L}^{l}=\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}\\mathbf{W}_{L}^{l},}\\\\ &{\\widetilde{\\mathbf{Z}}_{2}^{l}=(\\mathbf{A}\\odot(\\mathbf{I}-\\mathbf{B}^{d}))\\mathbf{Z}^{l-1}\\mathbf{W}_{H}^{l}=(\\mathbf{I}-\\hat{\\mathbf{A}})\\mathbf{Z}^{l-1}\\mathbf{W}_{H}^{l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "662 where the COMBINE function is adaptive weighted addition. Specifically, ACM-GCN aggregates   \n663 node ego, low-frequency, and high-frequency messages from ego and raw neighborhoods, and   \n664 combines them with node-wise adaptive weights. With simple but effective designs, ACM-GCN   \n665 achieves outstanding performance, which shows that complicated designs are not necessary. ", "page_idx": 17}, {"type": "text", "text": "666 A.2.11 OrderedGNN ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "667 OrderedGNN [21] is a SOTA method that introduces a node-wise adaptive dimension concatenation   \n668 function to combine messages from neighbors of different hops: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{Z}^{l}=\\mathbf{P}_{d}^{l}\\odot\\mathbf{Z}^{l-1}+(1-\\mathbf{P}_{d}^{l})\\odot(\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "669 where $\\mathbf{P}_{d}\\in\\mathbb{R}^{N\\times d_{r}}$ is designed to be matrix with each line $\\mathbf{P}_{d,i}^{l}$ being a dimension indicate vector,   \n670 which starts with continuous 1s while the others be 0s. In practice, to keep the differentiability, it\u2019s   \n671 \"soften\" as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\hat{\\mathbf{P}}_{d}^{l}}=\\mathrm{cumsum}_{\\leftarrow}\\left(\\mathrm{softmax}\\left(f_{\\xi}^{l}\\left(\\mathbf{Z}^{l-1},\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}\\right)\\right)\\right),}\\\\ &{\\mathbf{P}_{d}^{l}=\\mathrm{SOFTOR}(\\mathbf{P}_{d}^{l-1},\\hat{\\mathbf{P}}_{d}^{l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "672 where $f_{\\xi}^{l}$ is a learnable layer that fuses two messages. ", "page_idx": 18}, {"type": "text", "text": "673 It can be revisited by HTMP with the following components: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{A}=[\\mathbf{I},\\mathbf{A}],\\quad\\boldsymbol{B}=[\\mathbf{I},\\boldsymbol{B}^{d}],\\quad\\mathbf{W}_{0}^{l}=\\mathbf{W}_{1}^{l}=\\mathbf{I},}\\\\ &{\\widetilde{\\mathbf{Z}}_{0}^{l}=(\\mathbf{I}\\odot\\mathbf{I})\\mathbf{Z}^{l-1}=\\mathbf{Z}^{l-1},}\\\\ &{\\widetilde{\\mathbf{Z}}_{1}^{l}=(\\mathbf{A}\\odot\\mathbf{B}^{d})\\mathbf{Z}^{l-1}=\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "674 where the COMBINE function is concatenation with node-wise adaptive dimensions. Specifically, in   \n675 each layer, OrderedGNN aggregates messages from node ego and raw neighborhood and concatenates   \n676 them with learnable dimensions. Combined with the multi-layer architecture, this approach can   \n677 aggregate messages from neighbors of different hops and combine them not only with adaptive   \n678 contributions but also as separately as possible. ", "page_idx": 18}, {"type": "text", "text": "679 A.3 Analysis and Advice for Designing Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "680 The HTMP mechanism splits the message-passing mechanism of HTGNNs into multiple modules,   \n681 establishing connections among methods. For instance, most message passing in HTGNNs have   \n682 personalized processing for nodes. Some methods [24, 11, 13, 22] utilize the learnable aggregation   \n683 guidance and some others [14, 18, 21, 23] count on learnable COMBINE functions. Though   \n684 neighborhood redefining is commonly used in HTGNNs, there are also many methods [24, 11, 18,   \n685 20, 21] using only raw neighborhoods to handle heterophily and achieve good performance. Degree   \n686 averaging, which plays the role of a low-pass fliter to capture the smooth signals, can still work well   \n687 in many HTGNNs [12, 14\u201316, 20]. High-order neighbor information may be helpful in heterophilous   \n688 graphs. Existing HTGNNs utilize it in two ways: directly defining high-order [12, 13, 16, 17] or   \n689 even full-connected [19] neighborhood indicators and by the multi-layer architecture of message   \n690 passing [20, 21].   \n691 With the aid of HTMP, we can revisit existing methods from a unified and comprehensible perspective.   \n692 An obvious observation is that the coordination among designs is important while good combinations   \n693 with easy designs can also achieve wonderful results. For instance, in ACM-GCN [18], the separation   \n694 and adaptive addition of ego, low-frequency, and high-frequency messages can accommodate the   \n695 personalized conditions of each node. OrderedGNN\u2019s design [21], which includes an adaptive   \n696 connection mechanism, ego/neighbor separation, and multi-layer architecture, allows discrete and   \n697 adaptive combinations of messages from multi-hop neighborhoods. This advises us to take into   \n698 account all components simultaneously when designing models. As an illustration, please be cautious   \n699 about using multiple learnable components. Also, here are some additional model design tips and   \n700 considerations. Please separate the messages from node ego and neighbors. When combining them   \n701 afterward, whether by weighted addition or concatenation, this approach is at least harmless if not   \n702 beneficial, especially when dealing with heterophilous graphs. Last but not least, try to design a   \n703 model capable of personalized handling different nodes. Available components include but are not   \n704 limited to, custom-defined neighborhood indicators, aggregation guidance with adaptive weights or   \n705 estimated relationships, and learnable COMBINE functions. This is to accommodate the diversity   \n706 and sparsity of neighborhoods that nodes in real-world graphs may have. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "707 B Related Works ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "708 Homophilous Graph Neural Networks. Graph Neural Networks (GNNs) have showcased impres  \n709 sive capabilities in handling graph-structured data. Traditional GNNs are predominantly founded   \n710 on the assumption of homophily, broadly categorized into two classes: spectral-based GNNs and   \n711 spatial-based GNNs. Firstly, spectral-based GNNs acquire node representations through graph   \n712 convolution operations employing diverse graph filters [1, 32, 33]. Secondly, spatial-based meth  \n713 ods gather information from neighbors and update the representation of central nodes through the   \n714 message-passing mechanism [26, 28, 34]. Moreover, for a more comprehensive understanding of   \n715 existing homophilous GNNs, several unified frameworks [35, 36] have been proposed. Ma et al. [35]   \n716 propose that the aggregation process in some representative homophilous GNNs can be regarded   \n717 as solving a graph denoising problem with a smoothness assumption. Zhu et al. [36] establishes   \n718 a connection between various message-passing mechanisms and a unified optimization problem.   \n719 However, these methods have limitations, as the aggregated representations may lose discriminability   \n720 when heterophilous neighbors dominate [11, 12].   \n721 Heterophilous Graph Neural Networks. Recently, some heterophilous GNNs have emerged to   \n722 tackle the heterophily problem [11\u201323]. Firstly, a commonly adopted strategy involves expanding the   \n723 neighborhood with higher homophily or richer messages, such as high order neighborhooods [12, 13],   \n724 feature-similarity-based neighborhoods [13, 14], and custom-defined neighborhoods [15, 22]. Sec  \n725 ondly, some approaches [11, 17\u201319, 23] aim to leverage information from heterophilous neighbors,   \n726 considering that not all heterophily is detrimental et al.[6]. Thirdly, some methods [12, 16, 20, 21]   \n727 adapt to heterophily by extending the combine function in message passing, creating variations for   \n728 addition and concatenation. On this basis, several works have reviewed existing heterophilous   \n729 methods. Zheng et al. [8] and Zhu et al. [9] identifies effective designs in heterophilous GNNs and   \n730 analyzes the relationship between heterophily and graph-related issues. Gong et al. [10] provide   \n731 a higher-level perspective on learning heterophilous graphs, summarizing and classifying existing   \n732 methods based on learning strategies, architectures, and applications. However, these reviews merely   \n733 classify and list methods hierarchically, lacking unified understandings and not exploring the reason   \n734 behind the effectiveness of message passing in heterophilous graphs. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "735 C The Detail of Experiments on Synthetic Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "736 To explore the performance impact of homophily level, node degrees and compatibility matrix (CMs)   \n737 on simple GNNs, we conduct some experiments on synthetic datasets. ", "page_idx": 19}, {"type": "text", "text": "738 C.1 Synthetic Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "739 We construct synthetic graphs considering the factors of homophily, CMs and degrees. For homophily,   \n740 we set 3 levels including Lowh (0.2), Midh (0.5), and Highh (0.8). For CMs, we set two levels of   \n741 discriminability, including Easy and Hard. For degrees, we set two levels including Lowdeg (4)   \n742 and Highdeg (18). Note that with a certain homophily level, we can only control the non-diagonal   \n743 elements of CMs. Thus, there are a total of 12 synthetic graphs following the above settings. These   \n744 synthetic graphs are based on the Cora dataset, which provides node features and labels, which means,   \n745 only the edges are constructed. We visualize the CMs of these graphs in Figure 3. Since there is no   \n746 significant difference in CMs between low-degree and high-degree, we only plot the high-degree   \n747 ones. Further, the edges are randomly constructed under the guidance of these CMs and degrees to   \n748 form the synthetic graphs. ", "page_idx": 19}, {"type": "text", "text": "749 C.2 Experiments on Synthetic Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "750 We use GCN to analyze the performance impact of the above factors. The semi-supervised node   \n751 classification performance of GCN is shown in Table 5 while the baseline performance of MLP (72.54   \n752 $\\pm\\,2.18)$ is the same among these datasets since their difference is only on edges. From these results,   \n753 we have some observations: (1) high homophily is not necessary, GCN can also work well on low   \n754 homophily but discriminative CM; (2) low degrees have a negative impact on performance, especially   \n755 when the CMs are relatively weak discriminative, this also indicates that nodes with lower degrees   \n756 are more likely to have confused neighborhoods; and (3) when dealing with nodes with confused   \n757 neighborhoods, GCN may contaminate central nodes with their neighborhoods\u2019 messages, which   \n758 leads to performance worse than MLP. This once again remind us the importance of ego/neighbor   \n759 separation. ", "page_idx": 19}, {"type": "image", "img_path": "7g8WSOHJtP/tmp/a93d73915a2139c7e1ffaba9ec034abcdb2cf6942fbc7890d0223bc053f0ff5e.jpg", "img_caption": ["Figure 3: The visualization of compatibility matrix on synthetic graphs. "], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/2ca37e10d293b6de88cd0fb4cf474882de44afebb52df6a796ee3a6a5cebcecb.jpg", "table_caption": ["Table 5: Node classification accuracy of GCN on Synthetic Datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "760 D Empirical Evidence for the Conjecture about CM ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "761 In this part, we show the empirical evidence for the conjecture about CM as mentioned in Sec 4.   \n762 Specifically, we plot the observed and desired CM of ACM-GCN and GPRGNN in Figure 4. The   \n763 results show that ACM-GCN and GPRGNN have enhanced the discriminability of CM, which can be   \n764 empirical evidence for the conjecture.   \n765 The desired CMs are obtained as follows: For ACM-GCN, we leverage the learned weights in the   \n766 COMBINE function to rebuild a weighted adjacency matrix ${\\bf A}^{a c m}$ based on the low-pass filter $\\hat{\\bf A}$   \n767 and high-pass filter $\\mathbf{I}-\\hat{\\mathbf{A}}$ , then regard ${\\bf A}^{a c m}$ as the neighborhood and calculate the desired CM.   \n768 For GPRGNN, we utilize the leaned weights in the FUSE function to rebuild a weighted adjacency   \n769 matrix $\\mathbf{A}^{g p r}$ based on the multi-hop adjacency matrixes $[\\mathbf{I},\\mathbf{A},\\mathbf{A}^{2},...,\\mathbf{A}^{k}]$ then regard $\\mathbf{A}^{g p r}$ as the   \n770 neighborhood and calculate the desired CM. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "image", "img_path": "7g8WSOHJtP/tmp/9591a222ad48fccc0e66737f092a2f6f2e02c844cf2a435a34f0b0d320ff6c0d.jpg", "img_caption": ["Figure 4: The visualization of compatibility matrix on Amazon-Ratings. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "771 E Additional Detailed Implementation of CMGNN ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "772 Overall Message Passing Mechanism. The overall message passing mechanism in CMGNN is   \n773 formatted as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}^{l}=\\mathrm{diag}(\\alpha_{0}^{l})\\mathbf{Z}^{l-1}\\mathbf{W}_{0}^{l}+\\mathrm{diag}(\\alpha_{1}^{l})\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}\\mathbf{W}_{1}^{l}+\\mathrm{diag}(\\alpha_{2}^{l})(\\mathbf{A}^{s u p}\\odot\\mathbf{B}^{s u p})\\mathbf{Z}^{l-1}\\mathbf{W}_{2}^{l},}\\\\ {\\mathbf{Z}=\\underset{l=0}{\\overset{L}{\\prod}}\\ \\mathbf{Z}^{l},\\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "774 where $\\mathrm{diag}(\\alpha_{0}^{l}),\\mathrm{diag}(\\alpha_{1}^{l}),\\mathrm{diag}(\\alpha_{2}^{l})\\mathbb{R}^{N\\times1}$ are the learned combination weights introduced below. ", "page_idx": 21}, {"type": "text", "text": "775 COMBNIE Function with Adaptive Weights. Firstly, we list the aggregated messages $\\widetilde{\\mathbf{Z}}_{r}^{l}$ from 3   \n776 neighborhoods: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathbf{Z}}_{0}^{l}=\\mathbf{Z}^{l-1}\\mathbf{W}_{0}^{l},\\;\\widetilde{\\mathbf{Z}}_{1}^{l}=\\hat{\\mathbf{A}}\\mathbf{Z}^{l-1}\\mathbf{W}_{1}^{l},}\\\\ &{\\widetilde{\\mathbf{Z}}_{2}^{l}=(\\mathbf{A}^{s u p}\\odot\\mathbf{B}^{s u p})\\mathbf{Z}^{l-1}\\mathbf{W}_{2}^{l}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "777 The combination weights are learned by an MLP with Softmax: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\alpha_{0}^{l},\\alpha_{1}^{l},\\alpha_{2}^{l}]=\\mathrm{Softmax}(\\mathrm{Sigmoid}([\\mathbf{Z}_{0}^{l}||\\mathbf{Z}_{1}^{l}||\\mathbf{Z}_{2}^{l}||\\mathbf{d}]\\mathbf{W}_{a t t}^{l})\\mathbf{W}_{m i x}^{l}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "778 where $\\mathbf{W}_{a t t}^{l}\\in\\mathbb{R}^{(3d_{r}+1)\\times3}$ and $\\mathbf{W}_{m i x}^{l}\\,\\in\\,\\mathbb{R}^{3\\times3}$ are two learnable weight matrixes, $\\mathbf{d}$ is the node   \n779 degrees which may be helpful to weights learning.   \n780 The Message Passing of Supplementary Prototypes. In practice, the virtual prototype nodes are   \n781 viewed as additional nodes, which have the same message passing mechanism as real nodes: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathbf Z}^{p t t,l}=\\mathrm{diag}(\\alpha_{0}^{p t t,l}){\\mathbf Z}^{p t t,l-1}{\\mathbf W}_{0}^{l}+\\mathrm{diag}(\\alpha_{1}^{p t t,l})\\hat{{\\mathbf A}}^{p t t}{\\mathbf Z}^{p t t,l-1}{\\mathbf W}_{1}^{l}}\\\\ &{\\quad\\quad+\\operatorname{diag}(\\alpha_{2}^{p t t,l})({\\mathbf A}^{p t t,s u p}\\odot{\\mathbf B}^{p t t,s u p}){\\mathbf Z}^{p t t,l-1}{\\mathbf W}_{2}^{l},}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad{\\mathbf Z}^{p t t}=\\underset{l=0}{\\overset{L}{\\prod}}{\\mathbf Z}^{p t t,l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "782 where $\\mathbf{A}^{s u p,p t t}=\\mathbf{1}\\in\\mathbb{R}^{K\\times K}$ and $\\mathbf{B}^{s u p,p t t}=\\hat{\\mathbf{C}}^{p t t}\\hat{\\mathbf{M}}$ are similar with those of real nodes. ", "page_idx": 21}, {"type": "text", "text": "783 Update Strategy for the Estimation of the Compatibility Matrix. For the sake of efficiency, we do   \n784 not estimate the compatibility matrix in each epoch. Instead, we save it as fixed parameters and only   \n785 update it when the evaluation performance is improved during the training.   \n786 Predition of CMGNN. CMGNN leverages the prediction of the model during message passing. For   \n787 initialization, nodes have the same probabilities belonging to each class. During the message passing,   \n788 the prediction soft label $\\hat{\\mathbf{C}}$ is replaced by the output of CMGNN, formatted as follow: ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{C}}=\\mathbf{CLA}((Z)),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "789 where CLA is a classifier implemented by an MLP and $\\mathbf{Z}$ is the final node representations. ", "page_idx": 21}, {"type": "text", "text": "790 F More Detail about the Benchmark ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "791 In this section, we describe the details of the new benchmarks, including (i) the reason why we need   \n792 a new benchmark: drawbacks of existing datasets; (ii) detailed descriptions of new datasets; (iii)   \n793 baseline methods and the codebase; and (iv) details of obtaining benchmark performance. ", "page_idx": 21}, {"type": "text", "text": "794 F.1 Drawbacks in Existing Datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "795 As mentioned in [30], the widely used datasets Cornell, Texas, and Wisconsin2 have a too small scale   \n796 for evaluation. Further, the original datasets Chameleon and Squirrel have an issue of data leakage,   \n797 where some nodes may occur simultaneously in both training and testing sets. Then, the splitting   \n798 ratio of training, validation, and testing sets are different across various datasets, which is ignored in   \n799 previous works.   \n800 Therefore, to build a comprehensive and fair benchmark for model effectiveness evaluation, we   \n801 will newly organize 10 datasets with unified splitting across various homophily values in the next   \n802 Subsection F.2. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "803 F.2 New Datasets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "804 In our benchmark, we adopt ten different types of publicly available datasets with a unified splitting   \n805 setting $(48\\%/32\\%/20\\%$ for training/validation/testing) for fair model comparison, including Roman  \n806 Empire [30], Amazon-Ratings [30], Chameleon-F [30], Squirrel-F [30], Actor [15], Flickr [37],   \n807 BlogCatalog [37], Wikics [38], Pubmed [39], and Photo [29]. The datasets have a variety of   \n808 homophily values from low to high. The statistics and splitting of these datasets are shown in Table 6.   \n809 The detailed description of the datasets is as follows:   \n810 \u2022 Roman-Empire3 [30] is derived from the extensive article on the Roman Empire found on the   \n811 English Wikipedia, chosen for its status as one of the most comprehensive entries on the platform.   \n812 It contains 22,662 nodes and 65,854 edges between nodes. Each node represents an individual word   \n813 from the text, with the total number of nodes mirroring the length of the article. An edge between   \n814 two nodes is established under one of two conditions: the words are sequential in the text or they   \n815 are linked in the sentence\u2019s dependency tree, indicating a grammatical relationship where one word   \n816 is syntactically dependent on the other. Consequently, the graph is structured as a chain graph,   \n817 enriched with additional edges that represent these syntactic dependencies. The graph encompasses   \n818 a total of 18 distinct node classes, with each node being equipped with 300-dimensional attributes   \n819 obtained by fastText word embeddings [40].   \n820 \u2022 Amazon-Ratings3 [30] is sourced from the Amazon product co-purchasing network metadata   \n821 dataset [41]. It contains 24,492 nodes and 186,100 edges between nodes. The nodes within   \n822 this graph represent products, encompassing a variety of categories such as books, music CDs,   \n823 DVDs, and VHS video tapes. An edge between nodes signifies that the respective products are   \n824 often purchased together. The objection is to forecast the average rating assigned to a product by   \n825 reviewers, with the ratings being categorized into five distinct classes. For the purpose of node   \n826 feature representation, we have utilized the 300-dimensional mean values derived from fastText   \n827 word embeddings [40], extracted from the textual descriptions of the products.   \n828 \u2022 Chameleon-F and Squirrel- $\\mathbf{\\nabla}\\cdot\\mathbf{F}^{3}$ [30] are specialized collections of Wikipedia page-to-page net  \n829 works [42], of which the data leakage nodes are filtered out by [30]. Within these datasets, each   \n830 node symbolizes a web page, and edges denote the mutual hyperlinks that connect them. The   \n831 node features are derived from a selection of informative nouns extracted directly from Wikipedia   \n832 articles. For the purpose of classification, nodes are categorized into five distinct groups based   \n833 on the average monthly web traffic they receive. Specifically, Chameleon-F contains 890 nodes   \n834 and 13,584 edges between nodes, with each node being equipped with 2,325-dimensional features.   \n835 Squirrel-F contains 2,223 nodes and 65,718 edges between nodes, with each node being equipped   \n836 with a 2,089-dimensional feature vector.   \n837 \u2022 Actor4 [15] is an actor-centric induced subgraph derived from the broader flim-director-actor-writer   \n838 network, as originally presented by [43]. In this refined network, each node corresponds to an   \n839 individual actor, and the edges signify the co-occurrence of these actors on the same Wikipedia   \n840 page. The node features are identified through the presence of certain keywords found within   \n841 the actors\u2019 Wikipedia entries. For the purpose of classification, the actors are organized into five   \n842 distinct categories based on the words of the actor\u2019s Wikipedia. Statistically, it contains 7,600   \n843 nodes and 30,019 edges between nodes, with each node being equipped with a 932-dimensional   \n844 feature vector.   \n845 \u2022 Flickr and Blogcatalog5 [37] are two datasets of social networks, originating from the blog-sharing   \n846 platform BlogCatalog and the photo-sharing platform Flickr, respectively. Within these datasets,   \n847 nodes symbolize the individual users of the platforms, while links signify the followship relation  \n848 ships that exist between them. In the context of social networks, users frequently create personalized   \n849 content, such as publishing blog posts or uploading and sharing photos with accompanying tag   \n850 descriptions. These textual contents are consequently treated as attributes associated with each   \n851 node. The classification objection is to predict the interest group of each user. Specifically, Flickr   \n852 contains 7,575 nodes and 479,476 edges between nodes. The graph encompasses a total of 9   \n853 distinct node classes, with each node being equipped with a 12047-dimensional attribute vector.   \n854 BlogCatalog contains 5,196 nodes and 343,486 edges between nodes. The graph encompasses a   \n855 total of 6 distinct node classes, with each node being equipped with 8189-dimensional attributes.   \n856 \u2022 Wikics6 [38] is a dataset curated from Wikipedia, specifically designed for benchmarking the   \n857 performance of GNNs. It is meticulously constructed around 10 distinct categories that represent   \n858 various branches of computer science, showcasing a high degree of connectivity. The node features   \n859 are extracted from the text of the associated Wikipedia articles, leveraging the power of pretrained   \n860 GloVe word embeddings [44]. These features are computed as the average of the word embeddings,   \n861 yielding a comprehensive 300-dimensional representation for each node. The dataset encompasses   \n862 a substantial network of 11,701 nodes interconnected by 431,206 edges.   \n863 \u2022 Pubmed7 [39] is a classical citation network consisting of 19,717 scientific publications with   \n864 44,338 links between them. The text contents of each publication are treated as their node attributes,   \n865 and thus each node is assigned a 500-dimensional attribute vector. The target is to predict which of   \n866 the paper categories each node belongs to, with a total of 3 candidate classes.   \n867 \u2022 Photo8 [29] is one of the Amazon subset network from [29]. Nodes in the graph represent goods   \n868 and edges represent that two goods are frequently bought together. Given product reviews as   \n869 bag-of-words node features, each node is assigned a 745-dimensional feature vector. The task is to   \n870 map goods to their respective product category. It contains 7,650 nodes and 238,162 edges between   \n871 nodes. The graph encompasses a total of 8 distinct product categories. ", "page_idx": 22}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/6b0fffa0e69f3c060ad97de5e38a4788f726103c7efb1a32a19bbcba210998ad.jpg", "table_caption": ["Table 6: Statistics and splitting of the experimental benchmark datasets. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "872 F.3 Baseline Methods and the Codebase ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "873 For comprehensive comparisons, we choose 13 representative homophilous and heterophilous GNNs   \n874 as baseline methods in the benchmark, including (i) Shallow base model: MLP; (ii) Homopihlous   \n875 GNNs: GCN, GAT, GCNII; and (iii) Heterophilous GNNs: H2GCN, MixHop, GBK-GNN, GGCN,   \n876 GloGNN, HOGGCN, GPR-GNN. Detailed descriptions of some of these methods can be seen in   \n877 Appendix A.2.   \n878 To explore the performance of baseline methods on new datasets and facilitate future expansions,   \n879 we collect the official/reproduced codes from GitHub and integrate them into a unified codebase.   \n880 Specifically, all methods share the same data loaders and evaluation metrics. One can easily run   \n881 different methods with only parameters changing within the codebase. The codebase is based on the   \n882 PyTorch9 framework, supporting DGL10 and $\\mathbf{\\bar{P}}\\mathbf{y}\\bar{\\mathbf{G}}^{11}$ . Detailed usages of the codebase are available in   \n883 the Readme file of the codebase. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "884 F.4 Details of Obtaining Benchmark Performance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "885 Following the settings in existing methods, we construct 10 random splits $(48\\%/32\\%/20\\%$ for   \n886 train/valid/test) for each dataset and report the average performance among 10 runs on them along   \n887 with the standard deviation.   \n888 For all baseline methods except MLP, GCN, and GAT, we conduct parameter searches within the   \n889 search space recommended by the original papers. The searches are based on the NNI framework   \n890 with an anneal strategy. We use Adam as the optimizer for all methods. Each method has dozens   \n891 of search trails according to their time costs and the best performances are reported. The currently   \n892 known optimal parameters of each method are listed in the codebase. We run these experiments   \n893 on NVIDIA GeForce RTX 3090 GPU with 24G memory. The out-of-memory error during model   \n894 training is reported as OOM in Table 2. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "895 G More Details about Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "896 In this section, we describe the additional details of the experiments, including experimental settings   \n897 and results. ", "page_idx": 24}, {"type": "text", "text": "898 G.1 Additional Experimental Settings ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "899 Our method has the same experimental settings within the benchmark, including datasets, splits,   \n900 evaluations, hardware, optimizer and so on as in Appendix F.4.   \n901 Parameters Search Space. We list the search space of parameters in Table 7, where patience is for   \n902 early stopping, nhidden is the embedding dimension of hidden layers as well as the representation   \n903 dimension $d_{r}$ , relu_varient decides ReLU applying before message aggregation or not as in ACM  \n904 GCN, structure_info determines whether to use structure information as supplement node features or   \n905 not.   \n906 Ablation Study. In the ablation study, there are three variants of our methods: without SM, without   \n907 DL, without SM and DL. For \"without $\\mathrm{SM\"}$ , we delete the supplementary messages during message   \n908 passing, using only messages from node ego and raw neighborhood for combination. For \"without   \n909 DL\", we simply set $\\lambda=0$ to delete the discrimination loss. For \"without SM and DL\", we just   \n910 combine the above two settings. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/abe02cd37a1909da75018207d74f0ec5b8e48279df2fa15682569d638bccf9a7.jpg", "table_caption": ["Table 7: Parameters search space of our method. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "911 G.2 Additional Experimental Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "912 In this subsection, we show some additional experimental results and analysis. ", "page_idx": 24}, {"type": "text", "text": "913 G.2.1 Additional Results of CM Estimations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "914 The additional visualizations of CM estimations are shown in Figure 5. As we can see, our method   \n915 can estimate quite accurate CMs among various homophily and class numbers, which provides a   \n916 good foundation for the construction of supplementary messages. ", "page_idx": 24}, {"type": "image", "img_path": "7g8WSOHJtP/tmp/4d3695f0f8c40f8a01d0c95c1540dc94223b02f5b271f3a7f91eb5517b2c5c1f.jpg", "img_caption": ["Figure 5: The visualization of real and estimated CMs on other datasets. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "917 G.2.2 Additional Performance on Nodes with Various Levels of Degrees. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "918 We show the additional performance on nodes with various degrees in Table 8. The results show that   \n919 CMGNN can achieve relatively good performance on low-degree nodes, especially on heterophilous   \n920 graphs. For the opposite results on homophilous graphs, we guess it may be due to the low-degree   \n921 nodes in homophilous graphs having a more discriminative semantic neighborhood, such as a one-hot   \n922 form. On the contrary, there are relatively more high-degree nodes with confused neighborhoods due   \n923 to the randomness, which leads to the shown results on homophilous graphs.   \n925 Complexity Analysis. The number of learnable parameters in layer $l$ of CMGNN is $3d_{r}(d_{r}+1)+9$ ,   \n926 compared to $d_{r}d_{r}$ in GCN and $3d_{r}(d_{r}+1)+9$ in ACM-GCN. The time complexity of layer $l$ is   \n927 composed of 3 parts (i) AGGREGATE function: $O(N d_{r}^{\\ 2}),O(N d_{r}^{\\ 2}\\!+\\!M d_{r})$ and $O(N d_{r}^{\\ 2}\\!+\\!N K d_{r})$   \n928 for identity neighborhood, raw neighborhood and the supplementary neighborhood respectively, where   \n929 $M=|\\mathcal{E}|$ denotes the number of edges; (ii) COMBINE function: $\\dot{O}(3N(3d_{r}\\!+\\!1)\\!+\\!12\\bar{N})$ for adaptive   \n930 weights calculating and $O(3N)$ for combination; (iii) FUSE function: $O(1)$ for concatenations.   \n931 To this end, the time complexity of CMGNN is $O(N d_{r}(3d_{r}+K+9)+\\Dot{M}d_{r}+18N+1)$ , or   \n932 $O(N d_{r}{}^{2}+M d_{r})$ for brevity.   \n933 Experimental Running Time. we report the actual average running time (ms per epoch) of baseline   \n934 methods and CMGNN in Table 9 for comparison. The results demonstrate that CMGNN can balance   \n935 both performance effectiveness and running efficiency. ", "page_idx": 25}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/a5555a6cb5205ca189c6386cc0d9e10e2cc61f93ef73fae466ecb135da0c5817.jpg", "table_caption": ["Table 8: Node classification accuracy comparison $(\\%)$ among nodes with different degrees. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Table 9: Effiency study results of average model running time (ms/epoch). OOM denotes out-ofmemory error during the model training. ", "page_idx": 26}, {"type": "table", "img_path": "7g8WSOHJtP/tmp/7b7c949f576a60fa6bc781db12eb2647eb62f7eb09500c4510c6e9738478129d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "936 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "939 paper\u2019s contributions and scope?   \n940 Answer: [Yes]   \n941 Justification: The contributions and scope of this paper are included in the abstract and   \n942 introduction.   \n943 Guidelines:   \n944 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n945 made in the paper.   \n946 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n947 contributions made in the paper and important assumptions and limitations. A No or   \n948 NA answer to this question will not be perceived well by the reviewers.   \n949 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n950 much the results can be expected to generalize to other settings.   \n951 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n952 are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "53 2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Guidelines:   \n58 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n9 the paper has limitations, but those are not discussed in the paper.   \n0 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n1 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n2 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n63 model well-specification, asymptotic approximations only holding locally). The authors   \n4 should reflect on how these assumptions might be violated in practice and what the   \n5 implications would be.   \n6 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n7 only tested on a few datasets or with a few runs. In general, empirical results often   \n8 depend on implicit assumptions, which should be articulated.   \n9 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n0 For example, a facial recognition algorithm may perform poorly when image resolution   \nis low or images are taken in low lighting. Or a speech-to-text system might not be   \n2 used reliably to provide closed captions for online lectures because it fails to handle   \n3 technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n5 and how they scale with dataset size.   \n6 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n7 address problems of privacy and fairness.   \n8 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n9 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n0 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n81 judgment and recognize that individual actions in favor of transparency play an impor  \n2 tant role in developing norms that preserve the integrity of the community. Reviewers   \n3 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 27}, {"type": "text", "text": "984 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "985 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n986 a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide the formalization analysis in Appendix. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The details of the method and experimental settings are provided in Appendix. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "988   \n989   \n990   \n991   \n992   \n993   \n994   \n995   \n996   \n997   \n998   \n999   \n1000   \n1001   \n1002   \n1003   \n1004   \n1005   \n1006   \n1007   \n1008   \n1009   \n1010   \n1011   \n1012   \n1013   \n1014   \n1015   \n1016   \n1017   \n1018   \n1019   \n1020   \n1021   \n1022   \n1023   \n1024   \n1025   \n1026   \n1027   \n1028   \n1029   \n1030   \n1031   \n1032   \n1033   \n1034   \n1035   \n1036   \n1037   \n1038   \n1039   \n1040   \n1041 ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "1042 Answer: [Yes]   \n1043 Justification: The data and code are available in the supplementary material.   \n1044 Guidelines:   \n1045 \u2022 The answer NA means that paper does not include experiments requiring code.   \n1046 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n1047 public/guides/CodeSubmissionPolicy) for more details.   \n1048 \u2022 While we encourage the release of code and data, we understand that this might not be   \n1049 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n1050 including code, unless this is central to the contribution (e.g., for a new open-source   \n1051 benchmark).   \n1052 \u2022 The instructions should contain the exact command and environment needed to run to   \n1053 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n1054 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n1055 \u2022 The authors should provide instructions on data access and preparation, including how   \n1056 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n1057 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n1058 proposed method and baselines. If only a subset of experiments are reproducible, they   \n1059 should state which ones are omitted from the script and why.   \n1060 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n1061 versions (if applicable).   \n1062 \u2022 Providing as much information as possible in supplemental material (appended to the   \n1063 paper) is recommended, but including URLs to data and code is permitted.   \n1064 6. Experimental Setting/Details   \n1065 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n1066 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n1067 results?   \n1068 Answer: [Yes]   \n1069 Justification: The detailed experimental settings are provided in Appendix.   \n1070 Guidelines:   \n1071 \u2022 The answer NA means that the paper does not include experiments.   \n1072 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n1073 that is necessary to appreciate the results and make sense of them.   \n1074 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n1075 material.   \n1076 7. Experiment Statistical Significance   \n1077 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n1078 information about the statistical significance of the experiments?   \n1079 Answer: [Yes]   \n1080 Justification: We report the average accuracy and the standard deviation as the performance   \n1081 in experiments.   \n1082 Guidelines:   \n1083 \u2022 The answer NA means that the paper does not include experiments.   \n1084 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n1085 dence intervals, or statistical significance tests, at least for the experiments that support   \n1086 the main claims of the paper.   \n1087 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n1088 example, train/test split, initialization, random drawing of some parameter, or overall   \n1089 run with given experimental conditions).   \n1090 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n1091 call to a library function, bootstrap, etc.)   \n1092 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1093 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1094 of the mean.   \n1095 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1096 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1097 of Normality of errors is not verified.   \n1098 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1099 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1100 error rates).   \n1101 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1102 they were calculated and reference the corresponding figures or tables in the text.   \n1103 8. Experiments Compute Resources   \n1104 Question: For each experiment, does the paper provide sufficient information on the com  \n1105 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1106 the experiments?   \n1107 Answer: [Yes]   \n1108 Justification: We list the hardware and software resources along with the space and space   \n1109 complexity in Appendix.   \n1110 Guidelines:   \n1111 \u2022 The answer NA means that the paper does not include experiments.   \n1112 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1113 or cloud provider, including relevant memory and storage.   \n1114 \u2022 The paper should provide the amount of compute required for each of the individual   \n1115 experimental runs as well as estimate the total compute.   \n1116 \u2022 The paper should disclose whether the full research project required more compute   \n1117 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1118 didn\u2019t make it into the paper).   \n1119 9. Code Of Ethics   \n1120 Question: Does the research conducted in the paper conform, in every respect, with the   \n1121 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1122 Answer: [Yes]   \n1123 Justification: The research conducted in this paper conforms, in every respect, with the   \n1124 NeurIPS Code of Ethics.   \n1125 Guidelines:   \n1126 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1127 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1128 deviation from the Code of Ethics.   \n1129 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1130 eration due to laws or regulations in their jurisdiction).   \n1131 10. Broader Impacts   \n1132 Question: Does the paper discuss both potential positive societal impacts and negative   \n1133 societal impacts of the work performed?   \n1134 Answer: [Yes]   \n1135 Justification: The potential positive societal impacts are provided in the Introduction while   \n1136 the potential negative societal impacts are meaningless since this work is a foundational   \n1137 research.   \n1138 Guidelines:   \n1139 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1140 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1141 impact or why the paper does not address societal impact.   \n1142 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1143 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1144 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1145 groups), privacy considerations, and security considerations.   \n1146 \u2022 The conference expects that many papers will be foundational research and not tied   \n1147 to particular applications, let alone deployments. However, if there is a direct path to   \n1148 any negative applications, the authors should point it out. For example, it is legitimate   \n1149 to point out that an improvement in the quality of generative models could be used to   \n1150 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1151 that a generic algorithm for optimizing neural networks could enable people to train   \n1152 models that generate Deepfakes faster.   \n1153 \u2022 The authors should consider possible harms that could arise when the technology is   \n1154 being used as intended and functioning correctly, harms that could arise when the   \n1155 technology is being used as intended but gives incorrect results, and harms following   \n1156 from (intentional or unintentional) misuse of the technology.   \n1157 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1158 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1159 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1160 feedback over time, improving the efficiency and accessibility of ML).   \n1161 11. Safeguards   \n1162 Question: Does the paper describe safeguards that have been put in place for responsible   \n1163 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1164 image generators, or scraped datasets)?   \n1165 Answer: [NA]   \n1166 Justification: This paper poses no such risk. The datasets we used are all publicly available   \n1167 online.   \n1168 Guidelines:   \n1169 \u2022 The answer NA means that the paper poses no such risks.   \n1170 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1171 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1172 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1173 safety filters.   \n1174 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1175 should describe how they avoided releasing unsafe images.   \n1176 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1177 not require this, but we encourage authors to take this into account and make a best   \n1178 faith effort.   \n1179 12. Licenses for existing assets   \n1180 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1181 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1182 properly respected?   \n1183 Answer: [Yes]   \n1184 Justification: The datasets and code of baseline methods are publicly available online. We   \n1185 cite the original paper and mark the URL in both papers and codebase.   \n1186 Guidelines:   \n1187 \u2022 The answer NA means that the paper does not use existing assets.   \n1188 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1189 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1190 URL.   \n1191 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1192 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1193 service of that source should be provided.   \n194 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n195 package should be provided. For popular datasets, paperswithcode.com/datasets   \n196 has curated licenses for some datasets. Their licensing guide can help determine the   \n197 license of a dataset.   \n198 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n199 the derived asset (if it has changed) should be provided.   \n200 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n201 the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide a public codebase along with an illustrative README file. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "1 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n2 Subjects   \n233 Question: Does the paper describe potential risks incurred by study participants, whether   \n234 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n235 approvals (or an equivalent approval/review based on the requirements of your country or   \n236 institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}]