[{"type": "text", "text": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Beitao Chen1 Xinyu Lyu2,5\u2217 Lianli Gao1\u2217 chenbeitao@gmail.com xinyulyu68@gmail.com lianli.gao@uestc.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Jingkuan Song1 jingkuan.song@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Heng Tao Shen3,4 shenhengtao@hotmail.com ", "page_idx": 0}, {"type": "text", "text": "1 Shenzhen Institute for Advanced Study, University of Electronic Science and Technology of China 2Southwestern University of Finance and Economics, Chengdu, China 3Center for Future Media, University of Electronic Science and Technology of China 4Tongji University 5Engineering Research Center of Intelligent Finance, Ministry of Education https://github.com/BT-C/HIO ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnection between the generated text and the corresponding images. Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones. However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations. To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs. Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks. Code is released at https://github.com/BT-C/HIO. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The recent success of Large Vision-Language Models (LVLMs) marks a major milestone in artificial intelligence research [OpenAI, 2023, Alayrac et al., 2022, Li et al., 2023a, Liu et al., 2023c, Zhu et al., 2023, Bai et al., 2023, Dai et al., 2023, Wang et al., 2023b, Driess et al., 2023]. By seamlessly integrating visual cues with Large Language Models (LLMs), LVLMs have demonstrated unparalleled expertise in multimodal comprehension, logical reasoning, and interactive engagement. This integration has ushered in a new era in AI, breaking through traditional limitations and enabling a more holistic understanding of complex information OpenAI [2023], Yang et al. [2023], Lu et al. [2023], Zhang et al. [2022], Sun et al. [2024]. Despite these advancements, certain challenges remain, particularly the issue of hallucination Li et al. [2023b], Gunjal et al. [2023], Liu et al. [2023b], Lovenia et al. [2023]. Hallucination occurs when the language model generates content that deviates from the image\u2019s actual content, including imagined objects, fabricated scenes, incorrect spatial relationships, and misidentified categories. ", "page_idx": 0}, {"type": "image", "img_path": "SF2GlFhVsS/tmp/23173210411740febb3a432ed17a37a654438354f85d4ad1002a87a20e011d11.jpg", "img_caption": ["Figure 1: (Left) Challenges and Solutions of Contrast Decoding Strategy. Visual Contrastive Decoding, despite introducing perturbations to induce hallucinations, fails to effectively enlarge the logits gap between hallucinatory and targeted tokens, resulting in unsatisfactory outputs. On the contrary, our method addresses the issue by significantly amplifying the logits gap between hallucinatory and targeted tokens. (Right) The performance of various methods on CHAIR metrics. Our HIO generates descriptions with fewer hallucination tokens compared to other visual contrastive decoding methods, achieving lower scores on the CHAIRs and CHAIRi metrics. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Substantial research efforts have been directed towards mitigating hallucinations in Large VisionLanguage Models (LVLMs). These efforts include post-hoc correction methods that refine LVLM outputs after the fact Zhou et al. [2023] and self-correcting frameworks specifically designed to reduce object hallucinations Yin et al. [2023]. Additionally, numerous decoding strategies have been developed to minimize hallucinations through the enhanced use of textual and visual priors Leng et al. [2023], Zhang et al. [2024], Favero et al. [2024], Zhu et al. [2024], Wang et al. [2024], Chen et al. [2024]. These methods aim to alleviate hallucinatory tendencies by integrating visual uncertainty, thereby increasing the contrastive disparity between hallucinatory and target logits. For example, Leng et al. [2023] augment the hallucinatory effect by introducing Gaussian noise into the images. Similar approaches by Zhang et al. [2024] and Favero et al. [2024] introduce substantial image noise, effectively reducing the original image to pure noise or unrecognizable content. Zhu et al. [2024] use instructional bias to enable the model to amplify its own hallucinations, while Wang et al. [2024] focus on deliberately amplifying the inherent image bias in LVLMs. ", "page_idx": 1}, {"type": "text", "text": "However, the inherent uncontrollable nature of global visual uncertainty challenges the precise induction of hallucinatory tokens. This limitation significantly undermines the effectiveness of these methods in reducing hallucinations and may inadvertently lead to undesired hallucinatory outputs. As shown in the left portion of the Fig. 1 Spoon, Table, and Fork are identified as hallucinated words, while People being the accurate term. For Greedy Decoding method shown in Fig. 1 (a), Table is selected as the final output based on the logits distribution. Moreover, although Visual Contrastive Decoding introduces perturbations to images to enhance hallucinations in Fig. 1 (b), it fails to widen the logits gaps between hallucinatory (Spoon, Table, and Fork) and targeted tokens (People), yielding a new hallucination as Fork. ", "page_idx": 1}, {"type": "text", "text": "To tackle this issue, we conducted the theoretical analysis to explore mechanisms for more effective contrast decoding (refer to Section 5 for detailed information on the process). Theoretically, a clear distinction between hallucinatory and target tokens can significantly enhance the effectiveness of contrast decoding methods in mitigating hallucinations. Based on this crucial insight, we introduce a novel optimization strategy called Hallucination-Induced Optimization (HIO). This strategy enhances the distinction between hallucinatory and targeted tokens by utilizing a refined theoretical preference model(as shown in the Fig. 1 on the left, section (c)), accurately outputting the correct result, People. ", "page_idx": 1}, {"type": "text", "text": "Consequently, this improves the efficiency of contrast decoding, thereby mitigating hallucinations in Large Vision-Language Models (LVLMs). Furthermore, our proposed method significantly reduces hallucinations in LVLMs compared to existing contrast decoding methods(as shown in the Fig. 1 on the right). To sum up, our main contributions are as follows: ", "page_idx": 2}, {"type": "text", "text": "1. We conducted a comprehensive theoretical analysis to explore mechanisms that enhance the effectiveness of the contrast decoding strategy.   \n2. We introduce Hallucination-Induced Optimization (HIO), an innovative strategy that utilizes a finely-tuned theoretical preference model to intensify the contrast between hallucinatory and target tokens. This enhancement strengthens the effectiveness of contrast decoding and effectively reduces hallucinations in Large Visual Language Models (LVLMs).   \n3. Extensive experimental research demonstrates that our Hallucination-Induced Optimization (HIO) strategy effectively reduces hallucinations in Large Visual Language Models (LVLMs), surpassing state-of-the-art methods across various benchmarks. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Hallucination in LVLMs. Before the advent of Large Language Models (LLMs), \"hallucination\" in natural language processing (NLP) primarily referred to generating nonsensical or source-deviating content Lee et al. [2018], Zhou et al. [2020], Lin et al. [2021], Ji et al. [2023], Zhang et al. [2023], Shi et al. [2023]. Recent studies have tackled the complexities of object hallucination in Large VisionLanguage Models (LVLMs), focusing on evaluation and detection methods Wang et al. [2023a], Liu et al. [2023a], Li et al. [2023b], Lovenia et al. [2023]. The CHAIR metric Rohrbach et al. [2018] evaluates the exact match between generated and ground-truth image captions, while POPE Li et al. [2023b] assesses the model\u2019s awareness of object existence through binary classification. ", "page_idx": 2}, {"type": "text", "text": "Decoding Method. The decoding method determines the generation of text tokens at each time step within language models. Traditional decoding strategies such as beam search BoulangerLewandowski et al. [2013], top-k decoding Fan et al. [2018], and sampling methods Holtzman et al. [2019], despite their widespread use, are prone to producing hallucinatory content. Recent research Li et al. [2022], Chuang et al. [2023], Leng et al. [2023], Huang et al. [2023] has made attempts to address this issue by proposing better decoding methods. For instance, Leng et al. [2023] uses contrastive decoding in LVLMs; However, global visual uncertainty poses challenges to the precise induction of hallucinatory tokens, limiting the effectiveness of mitigation strategies and risking unwanted hallucinations. To address this, we developed Hallucination-Induced Optimization (HIO), a novel strategy that enhances the contrast between hallucinatory and targeted tokens. Fig.1 presents the comparison results, where our approach demonstrates superior performance than other decoding methods. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first review the Contrast Decoding pipeline in Leng et al. [2023] (and later Zhang et al. [2024], Favero et al. [2024]). Then take a close look at the Bradley-Terry model Bradley and Terry [1952] and its application such as Direct Preference Optimization Rafailov et al. [2024]. Inspired by these studies, we propose our Hallucination-Induced Optimization. ", "page_idx": 2}, {"type": "text", "text": "Visual Contrastive Decoding. We consider an LVLM parameterized by $\\theta$ . The model takes a textual query input $x$ and a visual input $v$ , where $v$ provides contextual visual information to assist the model in generating a relevant response $y$ to the textual query. The response $y$ is sampled auto-regressively from the probability distribution conditioned on the query $x$ and the visual context $v$ . Mathematically, this can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{t}\\sim p_{\\theta}\\left(y_{t}\\mid v,x,y_{<t}\\right)\\propto\\exp\\log\\mathrm{it}_{\\theta}\\left(y_{t}\\mid v,x,y_{<t}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $y_{t}$ denotes the token at time step $t$ , and $y{<}t$ represents the sequence of generated tokens up to the time step $t-1$ . Specifically, given a textual query $x$ and a visual input $v$ , the model generates two distinct output distributions: one conditioned on the original $v$ and the other on the distorted visual input $v^{\\prime}$ , which is derived by applying pre-defined distortions (i.e., Gaussian noise mask) to the original $v$ . Then, a new contrastive probability distribution is computed by exploiting the differences between the two initially obtained distributions. The new contrastive distribution $p_{v c d}$ is formulated as: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\np_{v c d}\\left(y\\mid v,v^{\\prime},x\\right)=\\operatorname{softmax}[(1+\\alpha)\\log\\operatorname{it}_{\\theta}\\left(y\\mid v,x\\right)-\\alpha\\log\\operatorname{it}_{\\theta}\\left(y\\mid v^{\\prime},x\\right)]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where larger value of $\\alpha$ indicate a stronger amplification of differences between the two distributions $\\alpha=0$ reduces to regular decoding). ", "page_idx": 3}, {"type": "text", "text": "Direct Preference Optimization. Reinforcement learning (RL) effectively fine-tunes Large Language Models (LLMs) to align with human behavior. Given an input $x$ and a response $y$ , a language model policy $\\pi_{\\theta}$ generates a conditional distribution $\\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{y}\\mid\\boldsymbol{x})$ . RL aims to maximize the average reward of outputs, with the reward function $r(x,y)$ . To prevent overoptimization Gao et al. [2023], the objective loss includes a KL-divergence term, controlling the divergence between the language model policy and its reference policy $\\bar{\\pi}_{\\mathrm{ref}}(y\\mid x)$ , typically derived from supervised fine-tuning. Thus, the overall objective is formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi_{\\theta}}\\mathbb{E}_{x\\sim\\mathcal{D},y\\sim\\pi_{\\theta}(y\\mid x)}\\big[r(x,y)-\\alpha\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\pi_{\\mathrm{ref}}(y\\mid x)}\\big]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{D}$ is a dataset of prompts and $\\alpha$ is a coefficient to control KL-divergence term. However, optimizing the above loss term with common strategies like proximal policy optimization (PPO) Schulman et al. [2017] is complex to tune. Recently, direct preference optimization (DPO) Rafailov et al. [2024] simplifies the above process by leveraging preference data for optimization. Here, the preference data is defined as $\\bar{D}\\overset{\\cdot}{=}\\{x^{(i)},y_{w}^{\\bar{(i)}},y_{l}^{(i)}\\}_{i=1}^{\\bar{N}}$ , where $y_{w}^{(i)}$ and $y_{l}^{(i)}$ represent preferred and dispreferred responses given an input prompt $x$ . These are then presented to human labelers who express preferences for one answer, denoted as $y_{w}\\succ y_{l}\\mid x$ where $y_{w}$ and $y_{l}$ denote the preferred and dispreferred respectively. Following a Bradley-Terry model [Bradley and Terry, 1952], the probability of obtaining each preference pair is: ", "page_idx": 3}, {"type": "equation", "text": "$$\np(y_{w}\\succ y_{l}\\mid x)=\\frac{\\exp\\left(r(x,y_{w})\\right)}{\\exp\\left(r(x,y_{w})\\right)+\\exp\\left(r(x,y_{l})\\right)}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the superscript $i$ is omitted for simplicity. In DPO, the optimization of Eqn. (3) can be formulated as classification loss over the preference data as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{D P O}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\mathbb{E}_{(x,y_{w},y_{l})\\sim\\mathcal{D}}\\left[\\log\\sigma\\left(\\alpha\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}-\\alpha\\log\\frac{\\pi_{\\theta}(y_{l}|x)}{\\pi_{\\mathrm{ref}}(y_{l}|x)}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "DPO enables learning $\\pi_{\\theta}$ from a fixed dataset of preferences, which is lightweight. However, the challenge arises because the direct application of DPO does not reliably induce hallucinations in a manner that meets the criteria specified in Eqn. (17). ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "An overview of the proposed HIO method is shown in Fig. 2. It constructs a more-hallucinated LVLM by inducing hallucinations from the original LVLM to amplify the contrast between hallucinatory and targeted tokens, thereby enhancing the efficiency of contrast decoding and mitigating hallucinations in LVLMs. In Section 4.1, we harness a fine-tuned theoretical preference model to amplify the contrast between hallucinatory and targeted tokens. Furthermore, to induce more potential hallucinations for effective contrast decoding, we propose to amplify multiple hallucination tokens based on a theoretical foundation presented in Eqn. 17 of Section 5. This theory demonstrates that effective contrastive decoding requires a consistent difference between the logits of potential hallucinated tokens and the correct token. And Section 4.3 introduces additional constraints to overcome the limitations of existing classification loss in amplifying the contrast between hallucinatory and targeted tokens. ", "page_idx": 3}, {"type": "text", "text": "4.1 Contrary Bradley-Terry Model (CBTM) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We harness a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model [Bradley and Terry, 1952]) to amplify the contrast between hallucinatory and targeted tokens. The studies on hallucination mitigation Zhao et al. [2023], Yu et al. [2023], Zhou et al. [2024] utilize BT model by defining the non-hallucinatory output as $y_{w}$ and the hallucinatory output as $y_{l}$ . Subsequently, they employ BT model training to incentivize the model to prioritize outputs without hallucinations over ", "page_idx": 3}, {"type": "image", "img_path": "SF2GlFhVsS/tmp/5aff066f27564bf5b0785e7315e658ccb9096f0daa72f9cd3014952ed62af59a.jpg", "img_caption": ["Figure 2: An overview of Hallucination-Induced Optimization (HIO). Our approach comprises two phases: the training stage and inference decoding. During the training stage, given an input image, a query, and a manually annotated correction, the Large Visual Language Model (LVLM) produces multiple instances of hallucinated content. We then apply our Hallucination-Induced Optimization (HIO) method to train an \u2018Evil\u2019 LVLM by inducing hallucinations from the original LVLM. In the inference phase, the logits from the trained \u2018Evil\u2019 LVLM are used to contrast with those generated by the original LVLM, effectively reducing the presence of hallucinations. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "those containing them. ", "page_idx": 4}, {"type": "text", "text": "However, within the context of contrast decoding, inducing hallucinations is crucial, and the resulting model output must satisfy the criteria outlined in Eqn. (17). (The detailed derivation of this formula is provided in Section5). To meet the requirements specified in Eqn. (17), the logits associated with hallucinated tokens $\\hat{l_{i}}^{\\{v,x,y<t\\}}$ need amplification, while at least one of the logits for the correct token l\u02c6j $\\hat{l_{j}}^{\\{v,x,y<t\\}}$ must be reduced. In contrast to the prevailing research efforts focused on alleviating hallucinations, our approach enables the model to learn to fti the distribution containing hallucinations while avoiding convergence with the distribution of correct outputs. The details are outlined as follows. To regulate $\\hat{l_{i}}^{\\{v,x,y<t\\}}$ and l\u02c6j $\\hat{l_{j}}^{\\{v,x,y<t\\}}$ , we utilize the dataset introduced by Yu et al. [2023]. This dataset is notable for providing a pair of outputs per input, with the output paragraphs being mostly identical except for differences in certain words or short phrases. By leveraging this dataset, we approximate the conditions outlined in Eqn. (17) within a unified statement. Different from Eqn. (5), we apply the Bradley-Terry (BT) [Bradley and Terry, 1952] model in a reversed way, the objective is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p(y_{l}\\sim y_{w}\\mid x)=\\frac{\\exp{(r(x,y_{l}))}}{\\exp{(r(x,y_{l}))}+\\exp{(r(x,y_{w}))}}}\\\\ &{\\qquad\\qquad\\qquad=\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(y_{l}\\mid v,x)}{\\pi_{\\mathrm{ref}}(y_{l}\\mid v,x)}-\\beta\\log\\frac{\\pi_{\\theta}(y_{w}\\mid v,x)}{\\pi_{\\mathrm{ref}}(y_{w}\\mid v,x)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma(\\cdot)$ is defined as a sigmoid function and the reference model $\\pi_{\\mathrm{ref}}(y|x)$ is usually implemented by an instruction-tuned base model we want to improve, and is kept fixed during DPO training. Only the policy model $\\pi_{\\theta}(y|x)$ is updated. ", "page_idx": 4}, {"type": "text", "text": "4.2 Amplification of Multiple Targeted Hallucination (AMTH) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The methodology delineated in Eqn. (6), along with the conventional application of Direct Preference Optimization (DPO) for mitigating hallucinations, is limited to highlight the difference between a single hallucination token and the target token. Consequently, these approaches fall short in enhancing the distinctions among other hallucinations relative to the target tokens, which is critical as shown in Eqn. (17). In this section, we will explain how to amplify the differences between multiple hallucination tokens and target tokens through modifications at both the loss function and data levels. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Multiple Hallucination-Induced Optimization. Achieving the desired distribution through single positive and negative sample fitting preference training is not feasible, leading conventional Direct Preference Optimization (DPO) applications Zhao et al. [2023], Yu et al. [2023], Zhou et al. [2024] to overlook a significant number of hallucinations. Thus, drawing inspiration from the implications of Eqn. (17), our approach strategically induces multiple hallucinations to increase the probability of producing a correct word in the output. As demonstrated in Eqn. (17), effective contrast decoding necessitates not only the amplification of one hallucination but also the consideration of a diverse set of potential hallucinations. We propose the simultaneous fitting of multiple pairs of preference data when modeling distributions for the same input preference, treating all pairs of preference data with equal importance. Based on Eqn. (6), we apply the Bradley-Terry (BT) [Bradley and Terry, 1952] model in a multi-pair way, the objective is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\prod_{i=1}^{k}p(y_{l}\\succ y_{w}\\mid x)=\\displaystyle\\prod_{i=1}^{k}\\frac{\\exp\\left(r(x,y_{l i})\\right)}{\\exp\\left(r(x,y_{l i})\\right)+\\exp\\left(r(x,y_{w})\\right)}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\prod_{i=1}^{k}\\sigma\\left(\\beta\\log\\frac{\\pi_{\\theta}(y_{l i}|x)}{\\pi_{\\mathrm{ref}}(y_{l i}|x)}-\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|x)}{\\pi_{\\mathrm{ref}}(y_{w}|x)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\{y_{l i}\\},i\\in\\{1,2,...\\,,k\\}$ represent the multiple potential hallucination tokens. Assuming access   \ntroe wa asrtda timc oddaetla oanmdp aersitsiomnast $\\mathcal{D}=\\left\\{x^{(i)},y_{w}^{(i)},\\{y_{l i}^{(i)}\\}\\right\\}_{i=1}^{N}$ u sma mlipkleelidh foroodm. $p$ ,r awmei ncga nt hpea rparombelterimz ea as $r(x,y)$   \na binary classification we have the negative log-likelihood loss: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{\\mathrm{AMTH}}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\mathbb{E}_{(x,y_{l},y_{w})\\sim{\\cal D}}\\bigg[\\log\\bigg(\\displaystyle\\prod_{i=1}^{k}p(y_{l}\\times y_{w}\\mid x)\\bigg)\\bigg]}&{}\\\\ {=-\\mathbb{E}_{(x,y_{l},y_{w})\\sim{\\cal D}}\\displaystyle\\sum_{i=1}^{k}\\bigg[\\log\\sigma\\bigg(\\beta\\log\\displaystyle\\frac{\\pi_{\\theta}(y_{l i}\\mid v,x)}{\\pi_{\\mathrm{ref}}(y_{l i}\\mid v,x)}-\\beta\\log\\displaystyle\\frac{\\pi_{\\theta}(y_{w}\\mid v,x)}{\\pi_{\\mathrm{ref}}(y_{w}\\mid v,x)}\\bigg)\\bigg]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Acquisition of Multiple Candidate Hallucinations. While numerous hallucination datasets exist Yu et al. [2023], Zhao et al. [2023], Zhou et al. [2024], they are either generated by GPT or manually rewritten, and thus do not accurately represent the model\u2019s potential for multiple hallucinations. Therefore, we propose a novel approach: allowing the model to directly output tokens with high confidence as negative samples. While this approach may incorrectly classify some correct tokens as hallucinations, it compensates by providing true value-labeled data for correction and supplementation. Consequently, this method effectively amplifies multiple hallucinations while reducing the target token. The detailed training process of our method is outlined in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "4.3 Advanced Constraints for Inducing (ACI) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To overcome the limitations of existing classification loss in amplifying the contrast between hallucinatory and targeted tokens, we introduces additional constraints. The preference optimization strategy outlined in Eqn. (8) allows the model to accommodate a specific range of preference distributions through the cross-entropy in the classification loss function. The precise formulation is as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{\\theta}(y_{l}|v,x)=\\sum_{t=1}^{m}\\frac{\\exp{l_{k_{t}}^{\\wedge}\\{v,x,y_{<t}\\}}}{\\sum_{j}^{N}\\exp{\\hat{l_{j}}^{\\{v,x,y_{<t}\\}}}},\\{k_{t}\\}\\in y_{l},t=\\{1,2,\\dots,m\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $m$ represents the length of the sentence $y_{l}$ and $\\{k_{T}\\}$ is token of each word, and the definition of $\\hat{l_{i}}^{\\{v,x,y<t\\}}$ is shown in Section 5. While the use of cross-entropy to minimize encoding length helps the model align with the desired output sentence, it does not consistently ensure that the logits of induced hallucinations meet the conditions specified in Eqn. (17). ", "page_idx": 5}, {"type": "text", "text": "For example, the goal of Eqn. (8) is to increase $\\pi_{\\boldsymbol{\\theta}}(y_{l}|\\boldsymbol{v},\\boldsymbol{x})$ , but both increasing $\\exp{l_{k_{t}}^{\\mathrm{~\\tiny~\\frown~}\\{v,x,y<t\\}}}$ or decreasing jN exp l\u02c6j $\\sum_{j}^{N}\\exp{\\hat{l_{j}}}^{\\{v,x,y<t\\}}$ can achieve this goal. Meanwhile, decreasing the value of $\\sum_{j}^{N}\\exp{\\hat{l_{j}}}^{\\{v,x,y<\\dot{t}\\}}$ can also allow $\\pi_{\\boldsymbol{\\theta}}(y_{w}|\\boldsymbol{v},\\boldsymbol{x})$ to meet the optimization criteria. As shown in Fig. 3, the blue curve, representing the disparity between the logits of the hallucinatory and targeted tokens, typically exhibits a positive trend. Nevertheless, it\u2019s important to note occasional segments where this value dips below zero. To tackle this issue, we further add restrictions based on Eqn. (8): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{HIO}}(\\pi_{\\theta};\\pi_{\\mathrm{ref}})=-\\mathbb{E}_{(x,y_{l},y_{w})\\sim{D}}\\displaystyle\\sum_{i=1}^{k}\\left[\\log\\sigma\\!\\left(\\beta\\log\\frac{\\pi_{\\theta}(y_{l i}|v,x)}{\\pi_{\\mathrm{ref}}(y_{l i}|v,x)}-\\beta\\log\\frac{\\pi_{\\theta}(y_{w}|v,x)}{\\pi_{\\mathrm{ref}}(y_{w}|v,x)}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+\\left.\\gamma\\!\\left(\\frac{1}{m}\\displaystyle\\sum_{t=1}^{m}{\\hat{l}}_{k_{t}}^{\\star}\\{v,x,y<t\\}-\\hat{l}_{i}^{\\{v,x,y<t\\}}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By implementing this constraint, the model can be ftited to the distribution of preference statements, thereby further expanding the difference between hallucination tokens and target tokens. ", "page_idx": 6}, {"type": "text", "text": "5 Fundamental Conditions for Contrast Decoding ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Contrast decoding is capable of mitigating hallucinations when specific conditions are met. This section delves into a comprehensive discussion and analysis of these conditions. ", "page_idx": 6}, {"type": "text", "text": "Definition. Let $l_{i}^{\\{v,x,y<t\\}}$ represent the probability of the $i$ -th token in the model\u2019s vocabulary given the query $x$ , the visual context $v$ and the sequence of generated tokens up to the time step $(t-1)$ . The logits can be formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{logit}_{\\theta}\\left(y_{t}\\mid v,x,y_{<t}\\right)=L^{\\{v,x,y_{<t}\\}}=(l_{1}^{\\{v,x,y_{<t}\\}},l_{2}^{\\{v,x,y_{<t}\\}},\\ldots,l_{N}^{\\{v,x,y_{<t}\\}})\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $N$ denotes the vocabulary length. ", "page_idx": 6}, {"type": "text", "text": "Definition. Let $\\hat{L}^{\\{v,x,y<t\\}}$ represents the ideal logits for contrast decoding, $L^{'\\{v,x,y<t\\}}$ represents the logits with hallucination and $L^{*}\\{v,x,y_{<t}\\}$ represents the logits of correct token, where $\\{L^{'\\{v,x,y_{<t}\\}},L^{*\\{v,x,y_{<t}\\}}\\}\\in L^{\\{v,x,y_{<t}\\}}$ . The results of contrast decoding of logits can be formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\delta^{\\{v,x,y_{<t}\\}}}&{{}=}&{(1+\\alpha)L^{\\{v,x,y_{<t}\\}}-\\alpha\\hat{L}^{\\{v,x,y_{<t}\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where larger $\\alpha$ values indicate a stronger amplification of differences between the two distributions $\\alpha=0$ reduces to regular decoding). The condition for the absence of hallucination in the logits subsequent to subtraction is that the values of the logits corresponding to all hallucinatory tokens are less than the magnitudes of the logits corresponding to the correct lexical tokens. The aforementioned condition is articulated mathematically as follows: ", "page_idx": 6}, {"type": "text", "text": "Proposit. ", "text_level": 1, "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\delta^{'\\{v,x,y<t\\}}~~~<~~\\operatorname*{min}\\delta^{*\\{v,x,y<t\\}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\delta^{'}\\{v,x,y<t\\}$ denotes the result of the subtraction between the logits of all hallucinated vocabulary tokens and the logits after their ideal amplification. $\\delta^{*\\{v,x,y<t\\}}$ represents the outcome of the subtraction between the logits corresponding to all correct vocabulary tokens and the logits under the ideal scenario. Eqn. 14 represents a theoretical upper bound, which guides us in enhancing the effectiveness of Contrast Decoding method for hallucination elimination by ensuring that the logits of all hallucinated words are lower than those of the correct words. Upon expansion of the left side of the equation, the following result is obtained: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{max}\\delta^{'\\{v,x,y<t\\}}=\\operatorname*{max}\\{(1+\\alpha)L^{'\\{v,x,y<t\\}}-\\alpha\\hat{L}^{'\\{v,x,y<t\\}}\\}}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname*{max}\\{(1+\\alpha)l_{i}^{\\{v,x,y<t\\}}-\\alpha\\hat{l}_{i}^{\\{v,x,y<t\\}}\\},i\\in\\{k_{1}^{'},k_{2}^{'},\\ldots,k_{m}^{'}\\}}\\\\ &{\\qquad\\qquad\\geq\\displaystyle\\frac{1}{m}\\sum_{i=k_{1}}^{k_{m}}((1+\\alpha)l_{i}^{\\{v,x,y<t\\}}-\\alpha\\hat{l}_{i}^{\\{v,x,y<t\\}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $m$ denotes the total number of hallucinated vocabulary items, and $k_{j}$ represents the subscript position of the $i{-}t h$ hallucinated vocabulary within the set $L^{\\{v,x,y<t\\}}$ . For the right side of the equation, ", "page_idx": 6}, {"type": "text", "text": "one of the correct lexical items is selected as the subject for amplification. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}\\delta^{*\\{v,x,y<t\\}}=\\operatorname*{min}\\{(1+\\alpha)L^{*\\{v,x,y<t\\}}-\\alpha\\hat{L}^{*\\{v,x,y<t\\}}\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq(1+\\alpha)l_{j}^{\\{v,x,y<t\\}}-\\alpha\\hat{l}_{j}^{\\{v,x,y<t\\}},j\\in\\{k_{1}^{*},k_{2}^{*},\\ldots,k_{n}^{*}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $n$ denotes the total number of correct lexical items. Based on Eqn. (15) and Eqn. (16), Eqn. (14) can be simplified to the form presented as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m\\times((1+\\alpha)l_{j}^{\\{v,x,y<t\\}}-\\alpha\\hat{l}_{j}^{\\{v,x,y<t\\}})-\\displaystyle\\sum_{i=k_{1}}^{k_{m}}((1+\\alpha)l_{i}^{\\{v,x,y<t\\}}-\\alpha\\hat{l}_{i}^{\\{v,x,y<t\\}})>0}\\\\ {\\displaystyle\\sum_{i=k_{1}}^{k_{m}}(\\hat{l}_{i}^{\\{v,x,y<t\\}}-\\hat{l}_{j}^{\\{v,x,y<t\\}})>J}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $J$ represents $\\begin{array}{r}{\\frac{(1+\\alpha)}{\\alpha}\\sum_{i=k_{1}}^{k_{m}}(l_{i}^{\\{v,x,y_{<t}\\}}-l_{j}^{\\{v,x,y_{<t}\\}})}\\end{array}$ . In the context of the contrast decoding method, given that the parameters of the original model remain invariant, the output can be characterized as a constant. Eqn. 17 delineates the logits for all hallucinated tokens $\\hat{l_{i}}^{\\{v,\\bar{x},y<t\\}}$ and contrasts these with the logits of a single correct token $\\hat{l_{j}}^{\\{v,x,y<t\\}}$ . It postulates that, for an optimal logits output, a pronounced divergence must be maintained between the logits of hallucinated tokens and the logit of the correct token. ", "page_idx": 7}, {"type": "text", "text": "Eqn. 17 illustrates that hallucinations can be effectively eliminated through contrastive decoding if the difference between the logits of the hallucinatory token and the correct token in the \u2018Evil\u2019 LVLM\u2019s output (Left part of Eqn.17) exceeds that in the original LVLM output ( $J$ in Eqn.17). For example, as depicted in the lower part of Fig. 2, where \"Dogs\" is a hallucination and \"Benches\" is the correct label, the hallucination of \"Dogs\" is removed when the difference between the logits for \"Dogs\" and \"Benches\" in the \u2018Evil\u2019 LVLM output surpasses the difference in the original LVLM output. When this condition is met for all potential hallucinations, all hallucinations are effectively eliminated. ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "6.1 Experimental Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Benchmarks. We evaluate HIO on three benchmarks including: (1) Quantitative metrics POPE Li et al. [2023b] on MSCOCO Lin et al. [2014] dataset. The Polling-based Object Probing Evaluation Li et al. [2023b] offers a streamlined approach to assessing object hallucination. In this benchmark, LVLMs are queried about the existence of specific objects in a given image. (2) CHAIR Rohrbach et al. [2018], Caption Hallucination Assessment with Image Relevance, is a specialized tool designed to evaluate the occurrence of object hallucination in image captioning tasks. (3) General-purposed Multimodal Large Language Model Evaluation (MME) Fu et al. [2023] benchmark, which provides an extensive benchmark designed to evaluate LVLMs across multiple dimensions, including ten perception-related subtasks and four cognition-focused ones. ", "page_idx": 7}, {"type": "text", "text": "Implementation Description We evaluate our model across three Large Vision-Language Models (LVLMs): LLaVA 1.5, InstructBLIP, and MiniGPT-4. For decoding, we use Llama-7B and Vicuna7B as the linguistic decoder for LLaVA and InstructBLIP/MiniGPT-4, respectively. Our model\u2019s performance is compared against three leading models in the field: OPERA Huang et al. [2023], VCD Leng et al. [2023], and VDD Zhang et al. [2024]. To ensure a fair and rigorous comparison, we adhere to the configurations and guidelines from the original works and codebases of the compared models. The training is conducted on a robust computational setup: 4x RTX 3090 GPUs for LLaVA 1.5, 8x V100 GPUs for MiniGPT-4, and 4x A6000 GPUs for InstructBLIP. Each training session lasts approximately 2-4 hours. Hyperparameters including alpha and beta are set to 1.0 and 0.1, respectively, in accordance with the VCD model\u2019s specifications. ", "page_idx": 7}, {"type": "text", "text": "6.2 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "POPE. To evaluate HIO\u2019s capability on object hallucination, we compare it with several state-of-theart Decoding methods on POPE. The results are shown in Tab. 1, which presents the experimental results on the POPE dataset across random, popular, and adversarial settings. Our method consistently outperforms the standard decoding strategy, with average improvements of $6.2\\%$ in accuracy and $7.3\\bar{\\%}$ in F1 score across all LVLMs. Additionally, our approach clearly surpasses state-of-the-art decoding methods, demonstrating its effectiveness in mitigating object hallucinations. The improved performance across random, popular, and adversarial settings further confirms that our HIO method effectively reduces hallucinations in diverse scenario. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "SF2GlFhVsS/tmp/d258e26004371ba257b36db6c9171ce39937c897fb56b61a4256936935fee9fd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "CHAIR. Beyond the \"Yes-or-No\" discriminative evaluations conducted on the POPE and MME datasets, we also assess our model\u2019s performance in open-ended caption generation using the CHAIR benchmark. Tab.2 and Tab.5 display results for 500 randomly selected images from the COCO val2017 and val2014 datasets, respectively. These results show consistent improvements in our model compared to other methods. Specifically, our approach significantly reduces object hallucinations in generated captions, as evidenced by lower CHAIRS and CHAIRI scores $8.1\\%$ reduction in CHAIRS and $4.9\\%$ in CHAIRI). Furthermore, it enhances caption detail, as indicated by higher Recall scores. Overall, our method achieves an effective balance between accuracy and detail in open-ended caption generation by widening the gap between hallucinated and correct tokens. ", "page_idx": 8}, {"type": "table", "img_path": "SF2GlFhVsS/tmp/097ef910ba0f6e5d3058e91203bda99632852de577fc962a942034e5d60ca799.jpg", "table_caption": ["Table 2: Hallucination performance of different methods. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "MME. To evaluate HIO\u2019s capability on object-level and attribute-level hallucination, we compare it with several state-of-the-art Decoding methods on MME. The results are shown in Tab. 3. Consistent with the performance on POPE and CHAIR, HIO also achieves competitive results on MME compared to other decoding methods. Concretely, HIO outperforms the VCD $6.4\\%$ , $21.7\\%$ , $4.7\\%$ and $17.\\bar{0}\\%$ at Existence, Count, Position on MME, respectively. The results demonstrate the effectiveness of our method. ", "page_idx": 8}, {"type": "text", "text": "6.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To verify the effectiveness of each component of the proposed HIO, we conduct ablation studies on Contrary Bradley-Terry Model(CBTM), Amplification of Multiple Targeted Hallucination(AMTH) and Advanced Constraints for Inducing(ACI) under the MSCOCO Lin et al. [2014]. The results are shown in Tab. 4. when constrained by CBTM in Exp 2, the model outperforms the baseline(i.e., Exp 1). This helps LVLM amplify hallucinations. Furthermore, after being integrate with AMTH in Exp 3, LVLM obtain sigificant gains on ${\\mathrm{CHAIR}}_{S}$ and ${\\mathrm{CHAIR}}_{I}$ . When integrate with ACI, the LVLM achieve superior performance on $\\mathrm{CHAIR}_{S}$ , $\\mathrm{CHAIR}_{I}$ and Recall. These results demonstrate the effective of each component. Moreover, we have enriched the ablation study to analyze the generalization capability of our proposed components to unseen categories, as detailed in Table 4. For the Unseen-P dataset, we collected data from MSCOCO, A-OKVQA, and GQA, ensuring no overlap with the training set, resulting in 495 samples across 10 distinct classes. These experiments show that our components generalize effectively to unseen data. Finally, we have integrated the ablation study into the experimental results section, rather than presenting it separately. ", "page_idx": 8}, {"type": "table", "img_path": "SF2GlFhVsS/tmp/bf13e6d8a07979e3d0e005c2b62644ee1d6ad50473c2724e32adf59aedee4170.jpg", "table_caption": [], "table_footnote": ["Table 3: Results on the hallucination subset of MME. Regular decoding denotes direct sampling, VCD denotes Visual Contrastive Decoding method, whereas $V D D$ refers to Visual Debias Decoding. The best performances within each setting are bolded. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "SF2GlFhVsS/tmp/4d56dc363e4abd5a285f1edc2028386d43a7bfde3035cec7863aa9ef8163d930.jpg", "table_caption": [], "table_footnote": ["Table 4: Ablation study with different components of our model on CHAIR-COCO. "], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "SF2GlFhVsS/tmp/3f820fd93255148b1bcafdb46a7218ecaedd955d9455d75a8ddb52fca4b1d2de.jpg", "table_caption": [], "table_footnote": ["Table 5: Ablation study on the generalization of each component on unseen datasets. "], "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we conduct an in-depth examination of the principles governing contrast decoding and the prerequisites for its efficacy. Based on our findings, we introduce HIO, an innovative model optimization approach designed to induce hallucinations. This method significantly amplifies hallucinatory elements within the model, thereby effectively mitigating them through contrast decoding. Extensive experimentation across various datasets has demonstrated that HIO effectively reduces hallucinations and achieves state-of-the-art performance. ", "page_idx": 9}, {"type": "text", "text": "Limitations & Future Work. ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our findings establish a necessary, but not sufficient, condition for the successful operation of contrast decoding. Further exploration of more effective conditions could significantly enhance the efficiency of contrast decoding in mitigating hallucinations. Additionally, exploring training-free methods to induce hallucinations could reduce the computational costs associated with decoding. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study is supported by grants from the National Natural Science Foundation of China (Grant No.   \n62122018, No. 62020106008, No. U22A2097, No. U23A20315), and Kuaishou. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.   \nJ. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.   \nN. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Audio chord recognition with recurrent neural networks. In ISMIR, pages 335\u2013340. Curitiba, 2013.   \nR. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.   \nZ. Chen, Z. Zhao, H. Luo, H. Yao, B. Li, and J. Zhou. Halc: Object hallucination reduction via adaptive focal-contrast decoding. arXiv preprint arXiv:2403.00425, 2024.   \nY.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883, 2023.   \nW. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2306.04387, 2023.   \nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.   \nA. Fan, M. Lewis, and Y. Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2018.   \nA. Favero, L. Zancato, M. Trager, S. Choudhary, P. Perera, A. Achille, A. Swaminathan, and S. Soatto. Multi-modal hallucination control by visual information grounding. arXiv preprint arXiv:2403.14003, 2024.   \nC. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.   \nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. 2023.   \nA. Gunjal, J. Yin, and E. Bas. Detecting and preventing hallucinations in large vision language models. arXiv preprint arXiv:2308.06394, 2023.   \nA. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.   \nQ. Huang, X. Dong, P. Zhang, B. Wang, C. He, J. Wang, D. Lin, W. Zhang, and N. Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospectionallocation. arXiv preprint arXiv:2311.17911, 2023.   \nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338, 2023.   \nK. Lee, O. Firat, A. Agarwal, C. Fannjiang, and D. Sussillo. Hallucinations in neural machine translation. OpenReview, 2018.   \nS. Leng, H. Zhang, G. Chen, X. Li, S. Lu, C. Miao, and L. Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint arXiv:2311.16922, 2023.   \nJ. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023a.   \nX. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022.   \nY. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023b.   \nS. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.   \nT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \nF. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023a.   \nF. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023b.   \nH. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023c.   \nH. Lovenia, W. Dai, S. Cahyawijaya, Z. Ji, and P. Fung. Negative object presence evaluation (nope) to measure object hallucination in vision-language models. arXiv preprint arXiv:2310.05338, 2023.   \nP. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.   \nOpenAI. GPT-4V(ision) system card. 2023.   \nR. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \nA. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018.   \nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.   \nW. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.   \nY. Sun, S. Yuan, X. Wang, L. Gao, and J. Song. Any target can be offense: Adversarial example generation via generalized latent infection. In ECCV, 2024.   \nJ. Wang, Y. Zhou, G. Xu, P. Shi, C. Zhao, H. Xu, Q. Ye, M. Yan, J. Zhang, J. Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023a.   \nW. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al. CogVLM: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023b.   \nX. Wang, J. Pan, L. Ding, and C. Biemann. Mitigating hallucinations in large vision-language models with instruction contrastive decoding. arXiv preprint arXiv:2403.18715, 2024.   \nZ. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang. The dawn of LMMs: Preliminary explorations with GPT-4V(ision). arXiv preprint arXiv:2309.17421, 9, 2023.   \nS. Yin, C. Fu, S. Zhao, T. Xu, H. Wang, D. Sui, Y. Shen, K. Li, X. Sun, and E. Chen. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045, 2023.   \nT. Yu, Y. Yao, H. Zhang, T. He, Y. Han, G. Cui, J. Hu, Z. Liu, H.-T. Zheng, M. Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. arXiv preprint arXiv:2312.00849, 2023.   \nS. Y. Q. Zhang, L. Gao, Y. Chen, and J. Song. Natural color fool: Towards boosting black-box unrestricted attacks. In NeurIPS, 2022.   \nY. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, et al. Siren\u2019s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.   \nY.-F. Zhang, W. Yu, Q. Wen, X. Wang, Z. Zhang, L. Wang, R. Jin, and T. Tan. Debiasing large visual language models. arXiv preprint arXiv:2403.05262, 2024.   \nZ. Zhao, B. Wang, L. Ouyang, X. Dong, J. Wang, and C. He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023.   \nC. Zhou, G. Neubig, J. Gu, M. Diab, P. Guzman, L. Zettlemoyer, and M. Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593, 2020.   \nY. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023.   \nY. Zhou, C. Cui, R. Rafailov, C. Finn, and H. Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024.   \nD. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \nL. Zhu, D. Ji, T. Chen, P. Xu, J. Ye, and J. Liu. Ibd: Alleviating hallucinations in large vision-language models via image-biased decoding. arXiv preprint arXiv:2402.18476, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Algorithm ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The algorithm outlines the process by which the model generates its own series of potential hallucinations. Using the sample pairs produced by the model, we apply our proposed Hallucination-Induced Optimization (HIO) to enhance the distinction between hallucinated and target labels. Ultimately, hallucinations are mitigated through contrastive decoding. ", "page_idx": 13}, {"type": "text", "text": "Using paired hallucination and non-hallucination annotations from the RLHF-V dataset, we apply beam search to generate multiple outputs where hallucination token annotations occur. These outputs include both correct and hallucinated results, which we use as hallucination samples to reinforce the model\u2019s confidence in its outputs. The correct annotations from RLHF-V serve as ground truth, helping the model avoid hallucinations by differentiating between hallucinated and target tokens. This approach expands the contrast between hallucinated and target tokens, effectively reducing hallucinations. ", "page_idx": 13}, {"type": "text", "text": "Algorithm 1 Training LVLM to Amplify Multiple Targeted Hallucination ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Require: training image set $\\nu$ ; user prompt set $\\mathcal{X}$ ; pair-wise groundtruth descriptions, $y^{\\prime}$ for hallucination   \ndescription and $y^{*}$ for correct description; LVLM $\\mathcal{M}(\\cdot)$ with parameters $\\theta$   \n1: According to each pair\u2019s hallucination description $y^{\\prime}$ and correct description $y^{*}$ , get starting subscripts of   \n$y^{\\prime}$ compared with $y^{*}$ . Different subscripts denoted as $\\mathcal{T}=\\{i_{1}^{'},i_{2}^{'},\\ldots,i_{n}^{'}\\}$   \n2: Initialize the LVLM\u2019s parameter $\\theta$ and an empty set $S_{n e w}\\leftarrow\\{\\}$   \n3: for each image $v\\,\\in\\,\\mathcal{V}$ , each prmopt $x\\in\\mathcal{X}$ , the correpsonding hallucinatory description $y^{'}\\in y^{'}$ and   \ncorrepsonding hallucinatory description $y^{\\ast}\\in\\mathcal{V}^{\\ast}$ do   \n4: Get starting subscripts of $y^{\\prime}$ compared with $y^{*}$ . Different subscripts denoted as $\\mathbb{Z}^{'}=\\{i_{1}^{'},i_{2}^{'},\\dots,i_{m}^{'}\\}$   \n5: for $i_{t}^{'}\\in\\mathcal{T}^{'}$ do   \n6: y\u2032<i\u2032 represents the sequence of generated tokens up to the time step (i\u2032t \u22121)   \n7: $\\begin{array}{r l}{\\mathrm{~Generate~next~logits~}L^{\\{v,x,y_{<i_{t}^{\\prime}}\\}}=\\mathcal{M}(v,x,y_{<i_{t}^{\\prime}}^{\\prime})=(l_{1}^{\\{v,x,y_{<i_{t}^{\\prime}}\\}},l_{2}^{\\{v,x,y_{<i_{t}^{\\prime}}\\}},\\qquad\\{v,x,y_{<i_{t}^{\\prime}}\\}}\\\\ {\\mathrm{~Find~\\Top\\!\\!-\\!K~\\subscripts~\\!\\!~}J^{\\{v,x,y_{<i_{t}^{\\prime}}\\}}\\!\\!\\!\\!}&{{}=\\qquad\\operatorname*{argmin}_{T\\subset\\{1,2,\\ldots,n\\},\\:\\vert T\\vert=K}\\sum_{j\\in T}l_{j}^{\\{v,x,y_{<i_{t}^{\\prime}}\\}}}\\\\ {\\{\\!\\{v,x,y_{<i}\\}\\!\\!\\!\\!}&{{}=\\!\\!\\!\\{\\nu,x,y_{<i_{t}^{\\prime}}\\}\\!\\!\\!\\}\\ge\\!\\!\\!\\!}&{{}=\\!\\!\\!\\{\\nu,x,y_{<i_{t}^{\\prime}}\\}}\\end{array}\\quad=\\!\\!\\!\\begin{array}{r l}{\\{v,x,y_{<i_{t}^{\\prime}}\\}}&{{}=\\!\\!\\!\\{\\nu,x,y_{<i_{t}^{\\prime}}\\}\\!\\!\\!}\\\\ {\\!\\!\\{\\nu,x,y_{<i_{t}^{\\prime}}\\}\\!\\!\\!}&{{}=\\!\\!\\!\\{\\nu,\\stackrel{\\eta_{\\mathrm{v}}}{\\le}\\!\\!\\!\\!}}\\\\ {\\!\\!\\{\\nu,x,y_{<i_{t}^{\\prime}}\\}\\!\\!\\!\\}&{{}=\\!\\!\\!\\!}&{{}\\!\\!\\!\\{\\nu,\\stackrel{\\eta_{\\mathrm{v}}}{\\le}\\!\\!\\!\\!}}\\end{array}$   \n8:   \n9: for $j_{t}\\in J^{\\{v,x,y_{<i_{t}^{\\prime}}\\}}\\,\\mathbf{dc}$ )   \n10: y<(i\u2032t+1) y \u222ajt   \n11: \u03b4 = 1   \n12: while y(i\u2032+\u03b4) is not period do   \n13: $\\begin{array}{r l}&{L^{\\stackrel{\\{v,\\dot{x},\\dot{y}}^{\\prime}\\leq i_{t}^{\\prime}+\\delta}{<i_{t}^{\\prime}+\\delta}\\}=\\mathcal M(v,x,y_{<i_{t}^{\\prime}+\\delta}^{\\prime})}\\\\ &{y_{<(i_{t}^{\\prime}+\\delta+1)}^{\\prime}=y_{<i_{t}^{\\prime}+\\delta}^{\\prime}\\cup\\arg\\operatorname*{min}_{j}L^{\\{v,x,y_{<i_{t}^{\\prime}+\\delta+1}\\}}}\\\\ &{\\delta=\\delta+1}\\end{array}$   \n14:   \n15:   \n16: end while   \n17: end for   \n18: end for   \n19: end for ", "page_idx": 13}, {"type": "text", "text": "B Mathematical Derivations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we present a comprehensive verification of Eqn. (17), which is elucidated through the following detailed procedure: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{m\\times((1+\\alpha)l_{i}^{\\{v,x,y_{<t}\\}}-\\alpha\\hat{l}_{j}^{\\{v,x,y_{<t}\\}})-\\displaystyle\\sum_{i=k_{1}}^{k_{m}}((1+\\alpha)l_{i}^{\\{v,x,y_{<t}\\}}-\\alpha\\hat{l}_{i}^{\\{v,x,y_{<t}\\}})>0}}\\\\ {{\\displaystyle\\alpha\\displaystyle\\sum_{i=k_{1}}^{k_{m}}(\\hat{l}_{i}^{\\{v,x,y_{<t}\\}}-\\hat{l}_{j}^{\\{v,x,y_{<t}\\}})-(1+\\alpha)\\displaystyle\\sum_{i=k_{1}}^{k_{m}}(l_{i}^{\\{v,x,y_{<t}\\}}-l_{j}^{\\{v,x,y_{<t}\\}})>0}}\\\\ {{\\displaystyle\\displaystyle\\frac{\\alpha}{(1+\\alpha)}\\displaystyle\\sum_{i=k_{1}}^{k_{m}}(\\hat{l}_{i}^{\\{v,x,y_{<t}\\}}-\\hat{l}_{j}^{\\{v,x,y_{<t}\\}})>\\displaystyle\\sum_{i=k_{1}}^{k_{m}}(l_{i}^{\\{v,x,y_{<t}\\}}-l_{j}^{\\{v,x,y_{<t}\\}})}}\\\\ {{\\displaystyle\\displaystyle\\sum_{i=k_{1}}^{k_{m}}(\\hat{l}_{i}^{\\{v,x,y_{<t}\\}}-\\hat{l}_{j}^{\\{v,y_{<t}\\}}-\\hat{l}_{j}^{\\{v,y_{<t}\\}})>\\displaystyle\\sum_{i=k_{1}}^{k_{m}}(\\hat{l}_{i}^{\\{v,x,y_{<t}\\}})>J}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C Visualization ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This figure demonstrates the effectiveness of our ACI method (described in Section 4.3). The y-axis shows the difference between the hallucination token and the target token. The blue curve represents this difference without ACI, while the orange curve represents it with our proposed ACI. Clearly, our method accurately induces hallucinations, further amplifies the difference between the hallucination token and the target token, and thus effectively reduces hallucinations. ", "page_idx": 14}, {"type": "image", "img_path": "SF2GlFhVsS/tmp/05ee889e00b876fd29cbdcfc372c3c00bcf83c8fba436b974509acdfe3cc113c.jpg", "img_caption": ["Figure 3: The Difference between hallucination token and target token. The horizontal axis represents the progression of training steps, while the vertical axis quantifies the disparity in logits, calculated as the hallucination token\u2019s logits minus those of the target token. It is evident that ACI effectively augments the distinction between the hallucination and target tokens. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "D Additional experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "MME. To evaluate HIO\u2019s capability on object-level and attribute-level hallucination, we compare it with several state-of-the-art Decoding methods on MME. The results are shown in Tab. 3. Consistent with the performance on POPE and CHAIR, HIO also achieves competitive results on MME compared to other decoding methods. Concretely, HIO outperforms the VCD at Existence, Count, Position, Color, Posters, on MME, respectively. The complete POPE evaluation is shown in the Tab 7. ", "page_idx": 14}, {"type": "table", "img_path": "SF2GlFhVsS/tmp/0a4c1b572e0361c3c96ae8700e72281fcfd8ab8615f98aa6ccfbe94a1e7c75d1.jpg", "table_caption": ["Table 6: Results on all MME perception-related tasks. The best performance of each setting is bolded. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "SF2GlFhVsS/tmp/5aa2fd1a0415d050a992970c8da8c0013bbc2dc3c2e67cfbdf51ba984a58f5e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 7: Results on POPE. Regular decoding denotes direct sampling. Higher accuracy and F1 score indicate better performance and fewer hallucinations. The best performances within each setting are bolded. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We have listed our contributions in both abstract and introduction. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: We have listed our contributions in dicussion ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: As shown in our proof. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: All code and data are provided. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: It is faithfully reproduce the main experimental results. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We specify all the training and test details. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: sure ", "page_idx": 18}, {"type": "text", "text": "Guidelines: As demonstrated in Fig.3. ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: As depicted in Implementation Description. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 19}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: As demonstrated in Limitations and Future Works section. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Yes, the paper describe safeguards. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Yes, the creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited and respected. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 20}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, the new assets introduced in the paper is well documented and the documentation is provided alongside the assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: yes ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we describe potential risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]