[{"type": "text", "text": "Revisiting Differentially Private ReLU Regression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Meng Ding Mingxi Lei Liyang Zhu   \nUniversity at Buffalo University at Buffalo KAUST Shaowei Wang Di Wang\u2217 Jinhui $\\mathbf{Xu}^{*}$   \nGuangzhou University KAUST University at Buffalo ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As one of the most fundamental non-convex learning problems, ReLU regression under differential privacy (DP) constraints, especially in high-dimensional settings, remains a challenging area in privacy-preserving machine learning. Existing results are limited to the assumptions of bounded norm $\\|\\mathbf{x}\\|_{2}\\leq1$ , which becomes stringent and strong with increasing data dimensionality. In this work, we revisit the problem of DP ReLU regression in overparameterized regimes. We propose two innovative algorithms, DP-GLMtron and DP-TAGLMtron, that outperform the conventional DPSGD. DP-GLMtron is based on a generalized linear model perceptron approach, integrating adaptive clipping and Gaussian mechanism for enhanced privacy. To overcome the constraints of small privacy budgets in DPGLMtron, represented by $\\widetilde{O}(\\sqrt{1/N})$ where $N$ is the sample size, we introduce DP-TAGLMtron, which uti lizes a tree aggregation protocol to balance privacy and utility effectively, showing that DP-TAGLMtron achieves comparable performance with only an additional factor of ${\\cal O}(\\log N)$ in the utility upper bound. Moreover, our theoretical analysis extends beyond Gaussian-like data distributions to settings with eigenvalue decay, showing how data distribution impacts learning in high dimensions. Notably, our findings suggest that the utility bound could be independent of the dimension $d$ , even when $d\\gg N$ . Experiments on synthetic and real-world datasets also validate our results. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Protecting individual privacy in data analysis has emerged as a critical concern. In light of the vast quantities of personal and sensitive information involved, traditional methods of ensuring privacy are encountering significant challenges. Differential Privacy (DP), introduced by [13], has gained widespread recognition as a method for preserving privacy by adding a controlled amount of random noise to the data or query responses, thereby effectively concealing the details of any individual. ", "page_idx": 0}, {"type": "text", "text": "Differentially Private Stochastic Optimization (DP-SO) and its empirical form, Differentially Private Empirical Risk Minimization (DP-ERM), are foundational models extensively studied in the DP community. Although there is a substantial body of research on DP-SO and DP-ERM with convex loss functions [10, 46, 7, 16, 35, 37, 6, 9, 26, 36, 38, 39], the understanding of nonconvex optimization in a DP context remains limited. Recent efforts have developed private algorithms with established proven utility bounds for differentially private nonconvex optimization [51, 46, 43, 52, 8, 47]. However, most of the current work employs the gradient norm of the population risk function as a measurement instead of excess population risk, commonly used in convex scenarios. Recently, [33] comprehensively studied different instances of non-convex learning, such as generalized linear models, ReLU regression, and multi-layer neural networks. However, for each problem, they imposed different strong assumptions, which left a big room to further understanding DP non-convex learning. In this paper, we consider the most fundamental one, the ReLU (Rectified Linear Unit) regression model. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As one of the most fundamental non-convex models, ReLU regression stands out due to its widespread use and effectiveness in deep learning. Despite its prevalence in industry, theoretical exploration of ReLU regression has been relatively limited. [33] studies the DP ReLU regression problem in both well-specified and misspecified settings. However, the problem is still far from well-understood. By and large, several challenges need to be addressed. Their analysis, for instance, heavily relies on strong assumptions about bounded norms of feature vectors and labels, i.e. $\\|\\mathbf{x}\\|_{2}\\leq O(1)$ and $|y|\\,\\le\\,\\bar{O(1)}$ , which does not hold even for normal Gaussian distributions. Secondly, while they provide a utility upper bound of $\\begin{array}{r}{\\widetilde O(\\frac{1}{\\sqrt{N}}\\!+\\!\\operatorname*{min}\\{\\frac{d^{1/2}}{(N\\varepsilon)^{1/2}},\\frac{1}{(N\\varepsilon)^{2/3}}\\})}\\end{array}$ {(Nd\u03b51)/12/2 ,(N\u03b51)2/3 }) with sample size N and dimension $d$ , their dimension-independent results heavily rely on their data assumption, leading to a dimensi\u221aondependent upper bound when considering Bernoulli or $O(1)$ -subGaussian data where $\\|\\mathbf{x}\\|_{2}\\leq O({\\sqrt{d}})$ with high probability. That means when $d\\gg N$ , i.e., when in the overparameterization regime, previous results become trivial. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we revisit the DP ReLU regression problem in the overparameterized regime. Specifically, we provide a general analysis under the well-specified setting and propose two novel algorithms (DP-GLMtron and DP-TAGLMtron). Our contributions are three-fold, see Table 1 for details. ", "page_idx": 1}, {"type": "text", "text": "1) We propose an innovative algorithm, DP-GLMtron, built upon the Generalized Linear Model Perceptron (GLM-tron) algorithm of [23]. Specifically, DP-GLMtron operates by making a single pass over the dataset and processes one data point at a time by integrating appropriate clipping and adding Gaussian noise to maintain privacy. Additionally, we provide an excess population risk upper bound of $\\begin{array}{r}{\\widetilde{O}(\\frac{D_{\\mathrm{eff}}}{N}+\\frac{D_{\\mathrm{eff}}^{\\mathrm{pri}}}{N^{2}\\varepsilon^{2}})}\\end{array}$ where $D_{\\mathrm{eff}}~(D_{\\mathrm{eff}}^{\\mathrm{pri}})$ is the (private) effective dimension. It is noticed that Deff and Dpri are defined in a way that does not directly rely on the dimension $d$ . Furthermore, our results are applicable not only to Gaussian-like data, typically considered in the traditional differential privacy community, but also to data distributions that exhibit either polynomial or exponential decay. In these cases, we demonstrate that the utility bound is independent of the dimension $d$ , which marks a significant departure from the traditional one (see more discussions in Remark 4). Finally, we also show that our analysis on DP-GLMtron is almost tight by providing a lower bound of $\\begin{array}{r}{\\widetilde\\Omega(\\frac{D_{\\mathrm{eff}}}{N}+\\frac{D_{\\mathrm{eff}}^{\\mathrm{pri}}}{N^{2}\\varepsilon^{2}})}\\end{array}$ 2) A significant issue with DP-GLMtron is its upper bound only holds with a small privacy budget $\\begin{array}{r}{\\varepsilon=\\widetilde O(\\frac{1}{\\sqrt{N}})}\\end{array}$ (for further details, see Remark 3), which is due to privacy amplification via shuffling. To tackle this limitation, we propose DP-TAGLMtron, which utilizes the tree aggregation protocol to introduce noise into the sum of mini-batch gradients instead of using privacy amplification. Our analysis reveals that the utility upper bound of DP-TAGLMtron is closely comparable to that of DP-GLMtron only with an additional factor of ${\\cal O}(\\log N)$ . This finding indicates that DP-TAGLMtron can offer similar performance benefits while potentially being applicable in settings with larger privacy budgets. ", "page_idx": 1}, {"type": "text", "text": "3) We conducted experiments with both synthetic data and real data on the ReLU regression model across various algorithms and privacy budgets. Specifically, in the first part, synthetic data was generated under two eigenvalue decay scenarios, $\\lambda_{i}\\propto i^{-\\check{2}}$ and $\\lambda_{i}\\propto i^{-3}$ . The results revealed that faster eigenvalue decay consistently resulted in lower excess risk compared to slower decay, indicating that it may suffer less from dimensionality with respect to utility. Additionally, in both synthetic and real data, DP-GLMtron and DP-TAGLMtron outperformed DP-SGD and DP-FTRL [22], especially as the sample size increased, highlighting the effectiveness of our proposed methods. ", "page_idx": 1}, {"type": "text", "text": "Due to space limitations, we have included the algorithms and proofs in the appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Private Nonconvex Optimization Previous research focusing on the utility of DP-SO/DP-ERM with convex loss functions predominantly employed excess population risk as the metric of utility. In contrast, for non-convex scenarios, utility assessment can be broadly categorized into three types: first-order stationary-based, second-order stationary-based, and excess population risk. The first-order stationary-based method [43, 53, 34, 8, 52, 50] involves evaluating the $\\ell_{2}$ -norm of the gradient of the population risk function. This method, however, encounters several challenges. For example, [2] shows that the gradient norm tends to approach zero as the sample size increases indefinitely, while there exists no assurance that a differentially private estimator will converge to or even be near any significant local minimum. The Second-order stationary-based method [42, 45] employs the norm of the gradient and the Hessian matrix minimal eigenvalue of the population risk function. However, this approach is particularly suited to specific scenarios where any second-order stationary point is a local minimum of the problem, and all the local minima are the global minimum, such as matrix completion and dictionary learning. The third approach involves directly employing the excess population risk as the measure of utility [33, 43] and our work aligns with this direction. However, as we mentioned these bounds either need strong assumptions or become trivial when $d\\gg N$ . ", "page_idx": 1}, {"type": "table", "img_path": "3uUIwMxYbR/tmp/be1ac74f35b0a7c29ffd280e49d71ed2e18b0089e0030404a1279ef58607a614.jpg", "table_caption": ["Table 1: Comparison of our work with related studies on $(\\varepsilon,\\delta)$ -DP ReLU regression in the statistical estimation setting. Here, $N$ represents the sample size, $d$ refers to the dimension and $D_{\\mathrm{eff},}$ , $D_{\\mathrm{eff}}^{\\mathrm{pri}}$ are the (private) effective dimensions. t is noticed that $D_{\\mathrm{eff}}$ and $D_{\\mathrm{eff}}^{\\mathrm{pri}}$ are defined in a way that does not directly rely on the dimension $d$ . Instead, they segment the feature space into an effective subspace.For further discussion, please see Remark 4. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Private Generalized Linear Models DP-SO and DP-ERM with generalized linear loss (DP-GLL) and generalized linear models (DP-GLM) have gained significant attention in recent years. The key developments in this area can be regarded as three aspects: 1) For convex loss functions, an early study on DP-GLL [21] revealed that, assuming features have bounded $l_{2}$ norm, the error bound can reach $\\begin{array}{r}{\\widetilde O(\\frac{1}{\\sqrt{N}\\varepsilon})}\\end{array}$ , contrasting with the general convex DP-ERM bound of $O\\big(\\frac{\\sqrt{d}}{N\\varepsilon}\\big)$ . A subsequent study [25] found that in constrained cases, the error bound depends on the Gaussian width of the constraint set. [34] improved the bound to O(N\u03b8\u03b5) under the assumption that the data space is a bounded set, where $\\theta$ is the rank of the expectation of the data matrix. 2) For constrained DP-GLM, [8] explored various scenarios, including smooth/non-smooth losses and losses in $\\ell_{p}$ space for $1\\le p\\le2$ . [5] provided the optimal rates of DP-GLM in unconstrained settings, identifying different rates depending on the nature of the loss function (smooth and non-negative but not necessarily Lipschitz, or Lipschitz). 3) For non-convex losses, [34, 7] provided dimension-independent bounds for the gradient $\\ell_{2}$ norm of the population risk function. As a specific instance of GLM, this work focuses on the overparameterized regime with sub-Gaussian data rather than bounded data. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notations: In this paper, we adhere to a consistent notation style for clarity. We use boldface lower letters such as $\\mathbf{x}$ , w for vectors, and boldface capital letters (e.g. $\\mathbf{A},\\mathbf{H})$ for matrices. Let $\\lVert\\mathbf{A}\\rVert_{2}$ denote the spectral norm of A. For two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of appropriate dimension, their inner product is defined as $\\langle\\mathbf{A},\\mathbf{B}\\rangle:=\\operatorname{tr}(\\mathbf{A}^{\\top}\\mathbf{B})$ . For a positive semi-definite (PSD) matrix A and a vector $\\mathbf{v}$ of appropriate dimension, we write $\\|\\mathbf{v}\\|_{\\mathbf{A}}^{2}:=\\mathbf{v}^{\\top}\\mathbf{A}\\mathbf{v}$ . The outer product is denoted by $\\otimes$ . ", "page_idx": 2}, {"type": "text", "text": "Given a dataset $D~=~\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{N}$ with each data point has a feature vector $\\mathbf{x}_{i}~\\in~\\mathcal{X}~\\subseteq~\\mathbb{R}^{d}$ and a response variable $y_{i}\\in\\mathcal{V}$ . In this paper, we assume that $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{N}$ are i.i.d. sampled from a ReLU regression model, i.e., each $\\left({{\\bf{x}}_{i}},{y_{i}}\\right)$ is a realization of the ReLU regression model $y=\\mathrm{ReLU}(\\mathbf{x}^{\\top}\\mathbf{w}_{*})+z$ , where $\\mathrm{ReLU(\\cdot)}:=\\operatorname*{max}\\{\\cdot,0\\}$ is the Rectified Linear Unit (ReLU); $z$ is a zero mean randomized noise; $\\mathbf{w}_{\\ast}\\in\\mathbb{R}^{d}$ is the optimal model parameter. Our goal is to output a model $\\mathbf{w}_{\\mathrm{priv}}$ that maintains privacy while minimizing the excess population risk, i.e., $\\mathcal{L}(\\mathbf{w}_{\\mathrm{priv}}\\mathbf{\\bar{\\alpha}})-\\mathcal{L}(\\mathbf{w}_{*})$ , where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\mathbf{w})=\\frac{1}{2}\\mathbb{E}_{(\\mathbf{x},y)\\sim\\mathcal{D}}[(\\mathrm{ReLU}(\\mathbf{x}^{\\top}\\mathbf{w})-y)^{2}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In this paper, we will focus on classic Differential Privacy to ensure privacy. ", "page_idx": 2}, {"type": "text", "text": "Definition 3.1 (Differential Privacy [13]). Given a data universe $\\mathcal{X}$ , we say that two datasets $D,D^{\\prime}\\subseteq\\mathcal{X}$ are neighbors if they differ by only one element, which is denoted as $D\\,\\sim\\,D^{\\prime}$ . A randomized algorithm $\\boldsymbol{\\mathcal{A}}$ is $(\\varepsilon,\\delta)$ -differentially private (DP) if for all adjacent datasets $D,D^{\\prime}$ and for all events $S$ in the output space of $\\boldsymbol{\\mathcal{A}}$ , we have $\\mathbb{P}(\\boldsymbol{A}(\\boldsymbol{D})\\in\\mathcal{S})\\le e^{\\varepsilon}\\cdot\\mathbb{P}(\\boldsymbol{\\mathcal{A}}(\\boldsymbol{D}^{\\prime})\\in\\mathcal{S})+\\delta$ . ", "page_idx": 3}, {"type": "text", "text": "In the following, we will introduce some definitions and assumptions related to the model. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (Well-specified Condition). Assume that there exists a parameter $\\mathbf{w}_{\\ast}\\,\\in\\,\\mathbb{R}^{d}$ such that $\\mathbb{E}[y\\ |\\ \\mathbf{x}]\\,=\\,\\mathrm{ReL}\\dot{\\mathrm{U}}(\\mathbf{x}^{\\top}\\mathbf{w}_{*})$ , and the variance of the model noise can be denoted by $\\sigma^{2}~:=$ $\\mathbb{E}\\left[(y-\\mathrm{ReLU}(\\mathbf{x}^{\\top}\\mathbf{w}_{*}))^{2}\\right]$ . ", "page_idx": 3}, {"type": "text", "text": "In the literature, the well-specified setting is also extensively referred to as the \u201dnoisy teacher\u201d setting [18] or the well-structured noise mode [19]. It has been widely studied previously [56, 41, 33, 12]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.3 (Data Covariance). Define $\\mathbf{H}:=\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{\\top}]$ as the expected data covariance matrix. ", "page_idx": 3}, {"type": "text", "text": "Denote the eigen decomposition of the data covariance by $\\begin{array}{r}{\\mathbf{H}=\\sum_{i}\\lambda_{i}\\mathbf{v}_{i}\\mathbf{v}_{i}^{\\top}}\\end{array}$ , where $(\\lambda_{i})_{i\\geq1}$ are eigenvalues in a nonincreasing order and $(\\mathbf{v}_{i})_{i\\geq1}$ are the correspond ing eigenvectors. Define $\\bar{\\mathbf{H}}_{k_{1}:k_{2}}$ as $\\begin{array}{r}{\\mathbf{H}_{k_{1}:k_{2}}:=\\sum_{k_{1}<i\\leq k_{2}}\\lambda_{i}\\mathbf{v}_{i}\\mathbf{v}_{i}^{\\top}}\\end{array}$ , and allow $k_{2}=\\infty$ to imply that $\\begin{array}{r}{\\mathbf{H}_{k:\\infty}=\\sum_{i>k}\\lambda_{i}\\mathbf{v}_{i}\\mathbf{v}_{i}^{\\top}}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "In the case of a Gaussian distribution, the data covariance matrix reduces to an identity matrix scaled by variance. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.4 (Fourth moment conditions). Define that the fourth moment of $\\mathbf{x}$ as $\\mathcal{M}$ : ", "page_idx": 3}, {"type": "text", "text": "(A) There exists a constant $\\alpha>0$ such that for any Positive Semi-Definite (PSD) matrix A, the following holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{xx}^{\\top}\\mathbf{Axx}^{\\top}]\\preceq\\alpha\\cdot\\mathrm{tr}(\\mathbf{HA})\\cdot\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "(B) There exists a constant $\\beta>0$ , such that for every PSD matrix $\\mathbf{A}$ , the following holds: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{xx}^{\\top}\\mathbf{Axx}^{\\top}]-\\mathbf{HAH}\\succeq\\beta\\cdot\\mathrm{tr}(\\mathbf{HA})\\cdot\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark 1. For Gaussian distribution, it can be verified that Assumption 3.4 holds with $\\alpha=3$ and $\\beta=1$ [56]. Furthermore, the assumption of sub-Gaussianity in data, specifically that $\\mathbf{H}^{-\\frac{1}{2}}\\mathbf{x}$ is a sub-Gaussian random vector, as posited in previous work [44, 41, 54, 28, 55, 20, 32, 31] for private linear regression model, can be shown to imply Assumption 3.4 (A) [56]. It is important to note that Assumption 3.4 (B) is specifically utilized in our analysis for establishing the lower bound only. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.5 $(\\mathbf{H},C_{2},a,b)$ -Tail). Let both $\\mathbf{H}$ and $\\mathcal{M}$ exist and are finite. A random vector $\\mathbf{x}$ satisfies $(\\mathbf{H},C_{2},a,b)$ -Tail if the the following holds: ", "page_idx": 3}, {"type": "text", "text": "\u2022 $\\exists a>0$ s.t. with probability $\\geq1-b$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}\\|_{2}^{2}\\leq\\mathbb{E}[\\|\\mathbf{x}\\|_{2}^{2}]\\cdot\\log^{2a}(1/b),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "\u2022 We have, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\mathbf{v},\\|\\mathbf{v}\\|=1}\\mathbb{E}[\\exp((\\frac{|\\langle\\mathbf{x},\\mathbf{v}\\rangle|^{2}}{C_{2}^{2}\\|\\mathbf{H}\\|_{2}})^{1/2a})]\\leq1,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "That is, for any fixed $\\mathbf{v}$ , with probability $\\geq1-b$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\langle\\mathbf{x},\\mathbf{v}\\rangle)^{2}\\leq C_{2}^{2}\\|\\mathbf{H}\\|_{2}\\|\\mathbf{v}\\|^{2}\\log^{2a}(1/b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Both conditions above provide Gaussian-like tail bounds determining the resilience of the dataset. Note that Definition 3.5 is widely used in the recent literature on DP analysis for the sub-Gaussian type of data such as [41, 29, 27]. In this work, we will assume each sample $\\mathbf{x}$ satisfying $(\\mathbf{H},C_{2},a,b)$ - Tail and the distribution of the inherent noise $z$ satisfies $(\\sigma^{2},C_{2},a,b)$ -Tail, which could capture more statistical properties beyond expectations. Additionally, according to Assumption 3.4 (A), it holds that $\\|\\mathbf{x}\\|_{2}^{2}\\leq\\bar{\\alpha}\\operatorname{tr}(\\mathbf{H})\\cdot\\mathrm{log}^{2a}(1/\\bar{b})$ in Equation (2). ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.6 (Symmetricity conditions). Assume that for every $\\mathbf{u},\\mathbf{v}\\in\\mathbb{R}^{d}$ , it holds that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{xx}^{\\top}\\cdot\\mathbb{1}[\\mathbf{x}^{\\top}\\mathbf{u}>0,\\mathbf{x}^{\\top}\\mathbf{v}>0]]=\\mathbb{E}[\\mathbf{xx}^{\\top}\\cdot\\mathbb{1}[\\mathbf{x}^{\\top}\\mathbf{u}<0,\\mathbf{x}^{\\top}\\mathbf{v}<0]]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}[({\\mathbf{x}}^{\\top}{\\mathbf{v}})^{2}{\\mathbf{x}}{\\mathbf{x}}^{\\top}\\cdot\\mathbb{1}[{\\mathbf{x}}^{\\top}{\\mathbf{u}}>0,{\\mathbf{x}}^{\\top}{\\mathbf{v}}>0]]=\\mathbb{E}[({\\mathbf{x}}^{\\top}{\\mathbf{v}})^{2}{\\mathbf{x}}{\\mathbf{x}}^{\\top}\\cdot\\mathbb{1}[{\\mathbf{x}}^{\\top}{\\mathbf{u}}<0,{\\mathbf{x}}^{\\top}{\\mathbf{v}}<0]].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Remark 2. Here we require that both the second and fourth moments of $\\mathbf{x}$ remain symmetric. It can be shown that Assumption 3.6 is satisfied when $\\mathbf{x}$ and $-\\mathbf{x}$ follow the same distribution, which can apply to symmetric sub-Gaussian such as symmetric Bernoulli or Gaussian distributions. ", "page_idx": 3}, {"type": "image", "img_path": "3uUIwMxYbR/tmp/b0dedc8042be1cc5d053ca4ad2e68ecef4e0187bd82e65e92890a77902a95a51.jpg", "img_caption": ["Figure 1: Training trajectories of DP-SGD and DP-GLMtron on a 2D noiseless ReLU regression with symmetric Bernoulli data. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Proposed DP-GLMtron Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Before presenting the analysis of DP ReLU regression problem, we first introduce our novel algorithm: DP-GLMtron (Algorithm 1), which is inspired by and built upon the Generalized Linear Model Perceptron (GLM-tron) algorithm of [23]. ", "page_idx": 4}, {"type": "text", "text": "The fundamental distinction between SGD and GLMtron lies in their respective update rules. Specifically, it takes the following rules: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{SGD};\\quad\\mathbf{w}_{t}=\\mathbf{w}_{t-1}-\\eta\\cdot\\left(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1})-y_{t}\\right)\\mathbf{x}_{t}\\cdot\\mathbb{I}\\left[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0\\right]}\\\\ {\\mathrm{GLMtron};\\quad\\mathbf{w}_{t}=\\mathbf{w}_{t-1}-\\eta\\cdot\\left(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1})-y_{t}\\right)\\mathbf{x}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the algorithm commence from an initial point ${\\bf w}_{0}$ and iterate from $t\\,=\\,0$ to $t\\,=\\,N$ with stepsize $\\eta$ . In contrast to the update rule of SGD, GLMtron diverges by changing the derivative of the ReLU function in its update mechanism. Typically, in neural network training, the gradient of the activation function, such as ReLU, is a critical component of the backpropagation process. The exclusion of this derivative in GLMtron\u2019s framework not only simplifies the computational process but also enhances efficiency. Our motivation for developing the DP-GLMtron algorithm stems from observations regarding the limitations of DP-SGD, as it sometimes fails to reach the optimal solution and can struggle with convergence under conditions of high noise, often settling at a saddle point instead (as illustrated in Figure 4). This issue is particularly pronounced when considering a low privacy budget. Furthermore, [23] demonstrates that this specific omission plays a significant role in GLMtron\u2019s ability to efficiently identify a predictor that closely approximates the optimal solution. ", "page_idx": 4}, {"type": "text", "text": "Building upon these foundations, we now present the detailed implementation of our proposed DP-GLMtron. For Algorithm 1, we begin with a random permutation of the dataset, ensuring that each data point is treated independently and uniformly. Also, such shuffilng can amplify privacy[17]. Subsequently, the algorithm performs a single pass over the dataset. Throughout this pass, we need to determine the clipping threshold before undergoing iterative updates for w. Establishing an appropriate clipping threshold is crucial for balancing the trade-off between utility and privacy. If the clipping threshold is set excessively low, it results in loss of gradient information, consequently leading to high bias [30, 3]. Consequently, we propose an advanced Algorithm 2 based on the residual estimator in [28] to achieve an adaptive clipping [4] by utilizing a small batch of data points. ", "page_idx": 4}, {"type": "text", "text": "Recall that the clipping term is a product of the residual, $\\left(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-\\mathbf{y}_{t}\\right)$ , and the covariate, $\\mathbf{x}_{t}^{\\top}$ . Based on Definition 3.5 and Assumption 3.4, we understand that $\\|\\mathbf{x}_{t}\\|_{2}^{2}\\leq\\alpha\\operatorname{tr}(\\mathbf{H})\\cdot\\log^{2a}(1/b)$ with a probability of at least $1-b$ . Thus, it is sufficient to get an upper bound of $\\lvert\\mathrm{\\;ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-\\mathbf{y}_{t}\\rvert$ . To bound this, we consider three cases, 1) $\\mathbf{x_{t}^{\\top}w_{t}}>\\bar{0},\\mathbf{x_{t}^{\\top}w_{*}}>0;\\mathbf{2}$ ) $\\mathbf{x_{t}^{\\top}w_{t}}<0,\\mathbf{x_{t}^{\\top}w_{*}}>0;\\mathbf{3})$ $\\mathbf{x_{t}^{\\top}w_{t}}>0,\\mathbf{x_{t}^{\\top}w_{*}}<0$ , corresponding to $d_{t}^{1}$ , $d_{t}^{2}$ , and $d_{t}^{3}$ in step 2 of Algorithm 2. It is noticed that the inherent model noise is not considered when initially calculating the residual for the third case at step 2, while this noise is taken into account at step 15. Moreover, when considering a special case that $\\mathbf{x_{t}^{\\top}w_{t}}<0,\\mathbf{x_{t}^{\\top}w_{*}}<0$ , the clipping threshold is integrated into the third scenario. For each residual, we first partition the dataset into $K$ batches, compute an estimate for each batch, and form a histogram with over those $K$ estimates. We then apply a private histogram mechanism with geometrically increasing bin sizes. The bin with the most estimates is used, ensuring a constant factor approximation of the distance to the optimum. Finally, we take the maximum among the three private residual estimates. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Upon determining the clipping threshold $\\gamma_{t}$ , it is subsequently incorporated into the existing clipping list $\\gamma$ . Following this, Gaussian noise with a standard deviation of $2f\\gamma_{t}$ is introduced to the clipped $\\boldsymbol{l}_{t}$ and then the weight vector $\\mathbf{w}_{t+1}$ will be updated. Finally, the algorithm culminates by computing the average of the iterates. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 (Privacy Guarantee). Suppose $D P$ -GLMtron is applied to $N$ input samples with a noise multiplier set to $f=\\dot{O}(\\frac{\\log(N/\\delta)}{\\varepsilon\\sqrt{N}})$ . Under these conditions, the algorithm satisfies $(\\varepsilon,\\delta)$ -differential privacy for $\\begin{array}{r}{\\varepsilon=O(\\sqrt{\\frac{\\log(N/\\delta)}{N}})}\\end{array}$ ), with \u03b5\u2032= \u221a8N l\u03b5og(2/\u03b4), $\\begin{array}{r}{\\delta^{\\prime}=\\frac{\\delta}{2N}}\\end{array}$ in Algorithm 2. ", "page_idx": 5}, {"type": "text", "text": "Remark 3. For general data distributions, DP-GLMtron necessitates a more complex approach to clipping decisions. To this end, we incorporate Algorithm 2, which uses the magnitude of the residual along with privacy parameters $\\varepsilon^{\\prime}$ and $\\delta^{\\prime}$ , as a means to estimate the private threshold. Additionally, we take only one pass over random shuffling data for Algorithm 1 [15, 17, 41], which is a wellknown method of privacy amplification. Henc\u221ae, we need to add noise $\\begin{array}{r}{O(\\frac{\\gamma_{t}}{\\varepsilon\\sqrt{N}})}\\end{array}$ , where $\\gamma_{t}$ is the clipping threshold and $\\varepsilon$ must be restricted to $1/\\sqrt{N}$ , to satisfy the requirement of [17] and to ensure differential privacy. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2 (Utility Guarantee Upper). Consider a dataset $D=\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{N}$ , where each $\\mathbf{x}_{i}$ is drawn from a distribution $\\mathcal{D}$ satisfying $(\\mathbf{H},C_{2},a,b)$ -Tail and the distribution of the inherent noise $z$ satisfies (\u03c32, C2, a, b)-Tail. Given Assumptions 3.4 and 3.6, if we set a step size \u03b7 \u2264 2\u03b1 tr(H) 1log2a(N), a noise multiplier $f\\,=\\,O(\\frac{\\log(N/\\delta)}{\\varepsilon\\sqrt{N}})$ , estimating data size $\\begin{array}{r}{m\\,=\\,\\Omega(\\frac{\\log(N)\\log(N/\\delta^{\\prime})}{\\varepsilon^{\\prime}})}\\end{array}$ , then with $a$ probability of $1-1/N$ , the output of Algorithm $^{\\,I}$ will satisfy: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}[L(\\overline{{\\mathbf{w}}}_{N})]-L(\\mathbf{w}_{*})\\lesssim b i a s+\\nu a r i a n c e+p r i\\nu a c y,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the bias, variance, and private terms are upper bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b i a s\\lesssim\\frac{1}{\\eta^{2}N^{2}}\\cdot\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{0,k^{*}}^{-1}}^{2}+\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{k^{*};\\infty}}^{2}+(\\frac{\\alpha}{N}\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\frac{\\mathbf{I}_{0,k^{*}}}{N\\eta}+\\mathbf{H}_{k^{*};\\infty}}^{2})\\cdot D_{e f f}}\\\\ {\\nu a r i a n c e\\lesssim\\frac{\\sigma^{2}}{N}\\cdot D_{e f f},\\quad p r i v a t e\\lesssim\\frac{\\Gamma^{2}f^{2}}{N}\\cdot D_{e f f}^{p r i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the effective dimensions are given by ", "page_idx": 5}, {"type": "equation", "text": "$$\nD_{e f f}=k^{*}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}\\lambda_{i}^{2},\\quad D_{e f f}^{p r i}=\\sum_{i<k^{*}}\\lambda_{i}^{-1}+\\frac{N^{2}\\eta^{2}}{4}\\cdot\\sum_{i>k^{*}}\\lambda_{i}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\begin{array}{r}{k^{*}=\\operatorname*{max}\\{k:\\lambda_{k}\\ge\\frac{2}{\\eta N}\\}}\\end{array}$ and $\\Gamma=\\operatorname*{max}\\{\\gamma_{t}\\}_{t=0}^{T-1}$ ", "page_idx": 5}, {"type": "text", "text": "Remark 4. The utility bound is typically decomposed into three terms: the bias term, which is associated with the initial value of the model; the variance term, stemming from the inherent noise intrinsic to the model itself; and the privacy term, introduced by the privacy-preserving mechanism. Notably, the results contain two crucial factors: the effective dimension $D_{\\mathrm{eff}}$ and the private effective dimension Depfrfi, which can be regarded as the function of data covariance matrix\u2019s eigenvalues. ", "page_idx": 5}, {"type": "text", "text": "Technical Understanding Under Simplified Cases To provide a more intuitive explanation, we explore two simplified scenarios. 1) $\\|\\mathbf{x}\\|_{2}\\leq1$ : In this scenario, where the norm of the data is constrained, the trace of the covariance matrix $\\mathrm{tr}(\\mathbf{H})\\leq1$ . This condition indicates that the sum of the eigenvalues, $\\lambda_{i}$ , is less than 1. Given that the eigenvalues are ordered non-increasingly (refer to Assumption 3.3), the majority of these eigenvalues must be very small\u221a. Assuming the step size $\\eta$ is sufficiently small and scaled with the square root of the da\u221ata size, $\\sqrt{N}$ , the optimal number of iterations $k^{*}$ must be confined within a scope proportional to $\\sqrt{N}$ . If not, the inequality $\\begin{array}{r}{\\mathrm{tr}(\\mathbf{H})\\geq k^{*}\\cdot\\frac{2}{N\\eta}\\geq1}\\end{array}$ would contradict the data assumptions. Furthermore, the clipping threshold, which depends on $\\operatorname{tr}(\\mathbf{H})$ , remains a constant term, and the private effective dimension $D_{\\mathrm{eff}}^{\\mathrm{pri}}$ is bounded above by $N$ . Therefore, the total utility is $\\begin{array}{r}{\\widetilde{O}(\\frac{1}{\\sqrt{N}}+\\frac{\\mathrm{tr}(\\mathbf{H}^{-1})}{N^{2}\\varepsilon^{2}})}\\end{array}$ + tr(NH2\u03b5\u221221 )). 2) sub-Gaussian tail: In this case, the eigenvalue of the covariance matrix corresponds to the variance of sub-Gaussian data. Both the effective dimension, $D_{\\mathrm{eff}}$ , and the private effective dimension, $D_{\\mathrm{eff}}^{\\mathrm{pri}}$ , effectively reduce to the feature dimension $d$ . Moreover, the clipping threshold contributes a factor of $d$ , leading to a total utility bound ofO (\u221adN +N d2\u03b52 ). ", "page_idx": 5}, {"type": "text", "text": "The Role of Eigenvalue in High-dimensional Setting Existing work in the DP community primarily relies on data assumptions satisfying Gaussian-like distributions, as discussed in Remark 1, which reduces the data covariance matrix to an identity matrix scaled by variance. Such an assumption may cause the case to be overlooked when the spectrum exhibits decay. Theorem 4.2 offers a novel perspective by elucidating the interplay between the feature space and the utility bound. Notice the crucial cut-off index $\\begin{array}{r}{\\bar{k}^{*}=\\operatorname*{max}\\lbrace\\bar{k}\\,:\\,\\lambda_{k}\\,\\geq\\,\\frac{2}{\\eta N}\\rbrace}\\end{array}$ \u03b72N }, which separates the entire space into a k\u2217 dimensional subspace. It enables us to obtain a bound that does not directly depend on the feature dimension $d$ in the overparameterized regime. To achieve a diminishing bound, it is necessary for $k^{*}$ to be $o(N)$ and for the tail summation of eigenvalues to be $o(1/N)$ , which indicates that it is possible to achieve improved results without suffering the dimension $d$ in the overparameterized regime. ", "page_idx": 6}, {"type": "text", "text": "To better understand the effective dimension, we offer examples of data spectra where the excess risk diminishes under certain conditions. ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.3. Given the same assumptions in Theorem 4.2 and suppose $\\lVert\\mathbf{w}_{0}-\\mathbf{w}^{*}\\rVert_{2}$ is bounded, it holds that: ", "page_idx": 6}, {"type": "text", "text": "1. If the spectrum of $\\mathbf{H}$ satisfies $\\lambda_{k}\\,=\\,k^{-r}$ for some $r\\,>\\,1$ , then with probability at least $1-1/N$ , the utility is upper bounded by $\\tilde{\\cal O}(N^{-\\frac{r}{1+r}}(1+(\\varepsilon^{-2}N^{\\frac{r}{1+r}})^{\\frac{\\bar{r}}{1+2r}})$ ). 2. If the spectrum of $\\mathbf{H}$ satisfies $\\lambda_{k}=e^{-k}$ , then with probability at least $1-1/N$ , the utility is upper bounded by $\\tilde{O}(N^{-1}(1+(\\varepsilon^{-2}N)^{\\frac{1}{2}}))$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 5. Corollary 4.3 indicates that utility bound in the overparameterized setting can still vanish if the spectrum exhibits either polynomial or exponential decay. Specifically, a constant privacy budget $\\varepsilon=O(1)$ can yield utility bounds of $\\tilde{\\mathcal{O}}(\\bar{N}^{-r/(1+2r)})$ for polynomial decay and $\\tilde{\\mathcal O}(\\bar{N}^{-1/\\bar{2}})$ for exponential decay. Notably, faster eigenva lue decay (e.g. $r=2,3,$ ) consistently result ed in lower excess risk compared to slower decay, indicating that it may suffer less from dimensionality with respect to utility. This insight cannot be derived when considering only traditional Gaussian-like data. Our experiment in Section 6 also validates the conclusion. Moreover, increasing the privacy budget to $\\varepsilon=\\bar{O}(N^{r/(2+2r)})$ for polynomial decay and $\\varepsilon=O(N^{1/2})$ for exponential decay improves utility bounds to $\\widetilde{\\mathcal{O}}(N^{-r/(1+r)})$ and $\\widetilde{\\mathcal{O}}(N^{-1})$ , respectively, matching non-private and linear regression scenario results [49, 48]. ", "page_idx": 6}, {"type": "text", "text": "In the following, we show that our previous analysis for Algorithm 1 is almost tight. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.4 (Utility Guarantee Lower). Consider a dataset $D=\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{N}$ , where each $\\mathbf{x}_{i}$ is drawn from a distribution $\\mathcal{D}$ satisfying $(\\mathbf{H},C_{2},a,b)$ -Tail and the distribution of the inherent noise $z$ satisfies $(\\sigma^{2},C_{2},a,b)$ -Tail. Given Assumptions 3.4 and 3.6, if we set the same parameters as in Theorem 4.2, then with a probability of $1-1/N$ , the output of Algorithm 1 will satisfy: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}[L(\\overline{{\\mathbf{w}}}_{N})]-L(\\mathbf{w}_{*})\\gtrsim b i a s+\\nu a r i a n c e+p r i\\nu a c y,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b i a s\\gtrsim\\frac{1}{\\eta^{2}N^{2}}\\cdot\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{0,k^{*}}^{-1}}^{2}+\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{k^{*};\\infty}}^{2}+(\\frac{\\beta}{N}\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\frac{\\mathbf{I}_{0,k^{*}}}{N\\eta}+\\mathbf{H}_{k^{*};\\infty}}^{2})\\cdot D_{e f}}\\\\ {\\nu a r i a n c e\\gtrsim\\frac{\\sigma^{2}}{N}\\cdot D_{e f f},\\quad p r i v a t e\\gtrsim\\frac{\\widetilde{\\Gamma}^{2}f^{2}}{N}\\cdot D_{e f f}^{p r i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the effective dimensions, $k_{m}^{*}$ are defined same as in Theorem 4.2 and denotes $\\widetilde{\\Gamma}=\\operatorname*{min}\\{\\gamma_{t}\\}_{t=0}^{T-1}$ ", "page_idx": 6}, {"type": "text", "text": "Remark 6. Similar to the Theorem 4.2, the lower bound stated here consists of three terms: the bias term, the variance term, and the privacy term. It is noticed that Theorem 4.4 provides the algorithmic lower bound Algorithm 1, applicable in both under-parameterized and over-parameterized settings. Notably, our lower bound aligns with the upper bound, differing only by absolute constants. ", "page_idx": 6}, {"type": "text", "text": "5 Advanced DP-TAGLMtron ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A pivotal concern with Algorithm 1 DP-GLMtron lies in its primary applicability in the setting where the privacy budget $\\varepsilon$ is small, which could potentially limit its practical utility (see Remark 3 for more details). To address this challenge, we introduce DP-TAGLMtron (Algorithm 3) in this section, which is designed to attain an improved trade-off between privacy and utility. ", "page_idx": 6}, {"type": "text", "text": "In Algorithm 3, the process starts with setting the initial weights. For each iteration $t$ , a private residual is determined by Algorithm 2 on the estimating samples, current weights, and privacy parameters. Subsequently, the clipping bound is computed as the product of the private residual and the covariate, analogous to the previous case. Once this clipping bound $\\gamma_{t}$ for the current iteration is computed, it is added to the existing clipping list $\\gamma$ . With $\\gamma_{t}$ in place, the clipped \"gradient\" $\\left(l_{t}\\right)$ is calculated. The maximum value from this list is selected as the clipping threshold $\\Gamma$ for our next step. The \"gradient\" $l_{t}$ is then forwarded to the differentially private tree-aggregation mechanism with noise multiplier $f$ and the maximal clipping threshold $\\Gamma$ . ", "page_idx": 7}, {"type": "text", "text": "It is noticed that our proposed algorithm, DP-TAGLMtron, does not depend on privacy amplification. Instead, it primarily involves employing tree aggregation [14, 11, 22, 40] to introduce noise into the sum of mini-batch gradients. At the beginning of training, we construct a binary tree consisting of $(2^{\\lceil\\log_{2}N\\rceil+1}-1)$ nodes, corresponding to $N$ leaves. Each leaf node is assigned a label $\\imath_{i}$ , and the value of each internal node represents the sum of its child nodes. In each iteration, the tree receives a vector $\\boldsymbol{l}_{t}$ , derived from the current data pair $\\left(\\mathbf{x}_{t},y_{t}\\right)$ , which is used to update the tree, incorporating the new node. The final output, denoted as $l_{\\le t}^{\\prime}$ , is obtained by aggregating the gradients accumulated in the binary tree and adding Gaussian noise to this aggregate. As a result, we can understand that $l_{\\le t}^{\\prime}$ is the private estimation of $\\textstyle\\sum_{i=0}^{t}l_{i}$ generated by tree aggregation protocol as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\nl_{\\le t}^{\\prime}=\\sum_{i=0}^{t}l_{i}+\\mathbf{v}_{t},\\mathrm{where}\\quad l_{i}=\\mathrm{clip}_{\\gamma_{t}}\\big(\\mathbf{x}_{i}\\big(\\mathrm{ReLU}(\\mathbf{x}_{i}^{\\top}\\mathbf{w}_{i})-y_{i}\\big)\\big),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and $\\mathbf{v}_{t}$ is a combination of at most $O(\\lceil\\log_{2}t\\rceil)$ Gaussian. Therefore, each $\\imath_{i}$ influences only its ancestor nodes, affecting at most $\\lceil\\log t\\rceil+1$ nodes. By introducing Gaussian noise with a standard deviation of $O(\\log t/\\varepsilon)$ to each node, the entire tree achieves $(\\varepsilon,\\delta)$ -differential privacy. ", "page_idx": 7}, {"type": "text", "text": "When receiving the output of Algorithm 4, the optimization objective at step 9 is to determine the weight parameter w. This objective function is composed of two key components: the first term, $(l_{\\leq t}^{\\prime},\\mathbf{w})$ , represents the inner product of $l_{\\le t}^{\\prime}$ and the weight parameter w. The second term, $\\begin{array}{r}{\\frac{1}{2\\eta}\\lVert\\mathbf{w}\\rVert_{2}^{2}}\\end{array}$ , introduces a regularization mechanism, where $\\eta$ controls the balance between ftiting the training data and constraining the magnitude of the parameters to prevent overftiting. One common method to find the optimal solution for this optimization problem is to take the derivative of the objective function and set it to zero. Therefore, for the given problem at step 9, we have the following update: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}\\leftarrow-\\eta\\cdot l_{\\leq t}^{\\prime}=-\\eta\\cdot(\\sum_{i=0}^{t}l_{i}+\\mathbf{v}_{t}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "It should be noted that in Algorithm 1, if the initialization is set to $\\mathbf{w}_{0}=\\mathbf{0}$ , then the update rule of w can be considered equivalent to the one presented here, provided that the variance of the noise and the clipping threshold are disregarded. Upon completing all iterations, the final model weights are determined by averaging all the updated weights from each iteration. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.1 (Privacy Guarantee). Suppose $D P$ -GLMtron is applied to $N$ input samples with a noise multiplier set to f = 2\u2308lg(N+1)\u2309ln(1/\u03b4). Under these conditions, the Algorithm 3 satisfies $(\\varepsilon,\\delta)$ -differential privacy, with $\\begin{array}{r}{\\varepsilon^{\\prime}=\\frac{\\varepsilon}{\\sqrt{8N\\log(2/\\delta)}}}\\end{array}$ \u03b5 and $\\begin{array}{r}{\\delta^{\\prime}=\\frac{\\delta}{2N}}\\end{array}$ in Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "Remark 7. Differing from Algorithm 1, the update rule of DP-TAGLMtron mainly utilizes the DP-Tree-Aggregation mechanism, which involves aggregating the \"gradients\" collected in a binary tree and then adding Gaussian noise to this aggregate. Base\u221ad on [22], Algorithm 4 achieves (\u03b5, \u03b4)-differential privacy with appropriate noise multiplier f = 2\u2308lg(N+\u03b51)\u2309ln(1/\u03b4), ensuring that Algorithm 3 also upholds $(\\varepsilon,\\delta)$ -DP under similar settings with $\\varepsilon^{\\prime},\\delta^{\\prime}$ as Algorithm 2. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2 (Utility Guarantee). Consider a dataset $D=\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{N}$ , where each $\\mathbf{x}_{i}$ is drawn from a distribution $\\mathcal{D}$ satisfying $(\\mathbf{H},C_{2},a,b)$ -Tail and the distribution of the inherent noise $z$ satisfies $\\left(\\sigma^{2},C_{2},a,b\\right)$ -Tai\u221al. Given Assumptions 3.4 and 3.6 hold, $i f$ we set a stepsize $\\eta<1/(\\alpha\\,\\mathrm{tr}(\\mathbf{H}))$ , a noise wmiulll tsipaltiisefry $\\begin{array}{r}{f=\\frac{\\sqrt{2\\lceil\\log(N+1)\\rceil\\ln(1/\\delta)}}{\\varepsilon}}\\end{array}$ , then with a probability of $1-1/N$ , the output of Algorithm 3 ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[L(\\overline{{\\mathbf{w}}}_{N})]-L(\\mathbf{w}_{*})\\lesssim b i a s+\\nu a r i a n c e+p r i\\nu a c y,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b i a s\\lesssim\\frac{1}{\\eta^{2}N^{2}}\\cdot\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{0;k^{*}}^{-1}}^{2}+\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{k^{*};\\infty}}^{2}+(\\frac{\\alpha}{N}\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\frac{\\mathbf{I}_{0;k^{*}}}{N\\eta}+\\mathbf{H}_{k^{*};\\infty}}^{2})\\cdot D_{e f f}}\\\\ {v a r i a n c e\\lesssim\\frac{\\sigma^{2}}{N}\\cdot D_{e f f},\\quad p r i v a t e\\lesssim\\frac{f^{2}\\log_{2}N}{N}\\cdot(\\alpha\\eta\\log_{2}N\\,\\mathrm{tr}(\\mathbf{H})+\\Gamma^{2})\\cdot(D_{e f f}+D_{e f f}^{p r i v})}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the effective dimensions and $k^{*}$ are defined same as in Theorem 4.2. ", "page_idx": 8}, {"type": "text", "text": "Remark 8. It is noteworthy that in Theorem 5.2, both the bias term and the variance term are consistent with those in Theorem 4.2. This alignment extends to the non-private results, with the only differences being in the absolute constants. Additionally, the private term in Theorem 5.2 closely aligns with that in Theorem 4.2, with the primary difference being a factor of $\\log_{2}N$ . It is intuitive that the tree aggregation mechanism utilized in DP-TAGLMtron indicates that the noise added for privacy is derived from a combination of at most $\\lceil\\log_{2}N+1\\rceil$ and at least one random Gaussian vector. This aspect suggests that DP-TAGLMtron diverges from the upper bound of DP-GLMtron at most by a factor of $\\log_{2}N$ and adheres to the same lower bound as DP-GLMtron with a different n\u221aoise variance. Moreover, in contrast to Algorithm 1, DP-TAGLMtron does not necessitate $\\varepsilon=O(1/\\sqrt{N})$ to maintain differential privacy, allowing a broader range of privacy budget settings in terms of flexibility and applicability. ", "page_idx": 8}, {"type": "text", "text": "6 Empirical Simulation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we first conduct experiments with synthetic data to validate the theoretical insights related to the role of eigenvalues in a high-dimensional setting. Additionally, due to space constraints, we present a real-data experiment on the MNIST dataset in Appendix B to demonstrate the performance of our proposed method. ", "page_idx": 8}, {"type": "image", "img_path": "3uUIwMxYbR/tmp/c5f29e909b7ea16bf4a5346ab74cffd6ef786e2e56b2f00a4bc77746bd98b9bf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Comparative Performance of Various Differential Privacy Algorithms Across Different Data Spectrum and Privacy Budgets. This figure illustrates the performance of four differential privacy algorithms\u2014DP-SGD, DP-GLMtron, DP-TAGLMtron, and DP-FTRL\u2014on generated Bernoulli distributed data, comparing excess risk across different sample sizes and privacy budgets. ", "page_idx": 8}, {"type": "text", "text": "Experiment Setup. The experiments were designed with varying privacy budgets $(\\varepsilon)$ set at 0.05, 0.2, and 0.5 with $\\begin{array}{r}{\\bar{\\delta}=\\frac{1}{n^{1.1}}}\\end{array}$ to observe the impact of differential privacy constraints on the learning process. We generated the data with two eigenvalue decay scenarios, $\\lambda_{i}\\,\\propto\\,i^{-2},i^{-3}$ , simulating different distributions of feature importance. The dimensionality of the data was fixed at 1,024 to maintain consistency across trials. The learning rate was initially set to $10^{-2}$ , with $N$ representing the sample size, which varied from 50 to 550 and increased in steps to 100. We utilized a ReLU regression model with noise to be trained using the previously mentioned algorithms, integrating privacy-preserving noise adjustments. The algorithms underwent a single iteration over generated data. The primary metric for evaluation was the average excess risk across varying dataset sizes and $\\varepsilon$ values. For each experiment, we also consider DP-SGD [1] and DP-FTRL [22] as baselines. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Empirical Findings. Compared with Figures (2a-2c), which illustrate data with eigenvalue decay $\\lambda_{i}\\propto i^{-2}$ , Figures (2f-2e), which illustrate data with eigenvalue decay $\\lambda_{i}\\propto i^{-3}$ exhibit the lower excess risk under the same training algorithms and conditions regardless of privacy budget. This indicates that fast eigenvalue decay suffers less from the dimensionality with respect to utility, thereby validating our results in Section 4. Additionally, results in both data with eigenvalue decay $\\bar{\\lambda}_{i}\\propto i^{-\\bar{2}}$ and data with eigenvalue decay $\\lambda_{i}\\propto i^{-3}$ show that DP-GLMtron and DP-TAGLMtron consistently outperform DP-SGD and DP-FTRL regarding excess risk, particularly as the sample size increased. Moreover, it is noticed that the DP-SGD and DP-FTRL exhibit poor convergence performance even with comparatively large data sizes. Finally, in both figures, increasing the privacy budget from $\\varepsilon=0.05$ to $\\varepsilon=0.5$ resulted in lower excess risks for both algorithms. This observation confirms the anticipated trade-off between privacy and learning performance, with a looser privacy constraint leading to more accurate models. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, our study significantly advances the field of differential privacy in ReLU regression problems. Our first proposed algorithm, DP-GLMtron, provides an excess population risk upper bound of the impac t of eigenvalues in overparameterized settings. Our results apply not only to Gaussian-like $\\begin{array}{r}{\\widetilde{O}(\\frac{D_{\\mathrm{eff}}}{N}+\\frac{D_{\\mathrm{eff}}^{\\mathrm{pri}}}{N^{2}\\varepsilon^{2}})}\\end{array}$ , offering a novel perspective on utility decomposition and highlighting data, typically considered in traditional differential privacy studies, but also to data distributions with polynomial or exponential decay. To address limitations with small privacy budgets, we developed DP-TAGLMtron, which offers performance comparable to DP-GLMtron but supports broader privacy budgets. Empirical results from both synthetic and real data experiments demonstrate that DPGLMtron and DP-TAGLMtron consistently outperform DP-SGD and DP-FTRL. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research of Meng Ding and Jinhui Xu was supported in part by KAUST through grant CRG10- 4663.2. Di Wang was supported in part by the baseline funding BAS/1/1689-01-01, funding from the CRG grand URF/1/4663-01-01, REI/1/5232-01-01, REI/1/5332-01-01, FCC/1/1976-49-01 from CBRC of King Abdullah University of Science and Technology (KAUST). Di Wang was also supported by the funding RGC/3/4816-09-01 of the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI). ", "page_idx": 9}, {"type": "text", "text": "Impact Statements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 9}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While this study advances previous work by relaxing certain assumptions and providing both synthetic and real-data validations of the theoretical findings, the analysis is specifically confined to the ReLU regression problem. Consequently, it remains uncertain whether these conclusions can be generalized to other nonconvex optimization scenarios. Future research could explore whether similar patterns hold across a broader range of nonconvex problems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \n[2] Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1195\u20131199, 2017.   \n[3] Kareem Amin, Alex Kulesza, Andres Munoz, and Sergei Vassilvtiskii. Bounding user contributions: A bias-variance trade-off in differential privacy. In International Conference on Machine Learning, pages 263\u2013271. PMLR, 2019.   \n[4] Galen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially private learning with adaptive clipping. Advances in Neural Information Processing Systems, 34:17455\u201317466, 2021.   \n[5] Raman Arora, Raef Bassily, Crist\u00f3bal Guzm\u00e1n, Michael Menart, and Enayat Ullah. Differentially private generalized linear models revisited. Advances in Neural Information Processing Systems, 35:22505\u201322517, 2022.   \n[6] Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient methods for convex optimization. In International Conference on Machine Learning, pages 383\u2013392. PMLR, 2021.   \n[7] Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex optimization with optimal rates. Advances in neural information processing systems, 32, 2019.   \n[8] Raef Bassily, Crist\u00f3bal Guzm\u00e1n, and Michael Menart. Differentially private stochastic optimization: New results in convex and non-convex settings. Advances in Neural Information Processing Systems, 34:9317\u20139329, 2021.   \n[9] Raef Bassily, Crist\u00f3bal Guzm\u00e1n, and Anupama Nandi. Non-euclidean differentially private stochastic convex optimization. In Conference on Learning Theory, pages 474\u2013499. PMLR, 2021.   \n[10] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations of computer science, pages 464\u2013473. IEEE, 2014.   \n[11] T-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Transactions on Information and System Security (TISSEC), 14(3):1\u201324, 2011.   \n[12] Meng Ding, Kaiyi Ji, Di Wang, and Jinhui Xu. Understanding forgetting in continual learning with linear regression. arXiv preprint arXiv:2405.17583, 2024.   \n[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265\u2013284. Springer, 2006.   \n[14] Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N Rothblum. Differential privacy under continual observation. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 715\u2013724, 2010.   \n[15] \u00dalfar Erlingsson, Ilya Mironov, Ananth Raghunathan, and Shuang Song. That which we call private. arXiv preprint arXiv:1908.03566, 2019.   \n[16] Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 439\u2013449, 2020.   \n[17] Vitaly Feldman, Audra McMillan, and Kunal Talwar. Hiding among the clones: A simple and nearly optimal analysis of privacy amplification by shuffling. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 954\u2013964. IEEE, 2022.   \n[18] Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient descent. Advances in Neural Information Processing Systems, 33:5417\u20135428, 2020.   \n[19] Surbhi Goel and Adam R Klivans. Learning neural networks with two nonlinear layers in polynomial time. In Conference on Learning Theory, pages 1470\u20131499. PMLR, 2019.   \n[20] Lijie Hu, Shuo Ni, Hanshen Xiao, and Di Wang. High dimensional differentially private stochastic optimization with heavy-tailed data. In Proceedings of the 41st ACM SIGMODSIGACT-SIGAI Symposium on Principles of Database Systems, pages 227\u2013236, 2022.   \n[21] Prateek Jain and Abhradeep Guha Thakurta. (near) dimension independent risk bounds for differentially private learning. In International Conference on Machine Learning, pages 476\u2013 484. PMLR, 2014.   \n[22] Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng Xu. Practical and private (deep) learning without sampling or shuffilng. In International Conference on Machine Learning, pages 5213\u20135225. PMLR, 2021.   \n[23] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. Advances in Neural Information Processing Systems, 24, 2011.   \n[24] Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals. arXiv preprint arXiv:1711.03908, 2017.   \n[25] Shiva Prasad Kasiviswanathan and Hongxia Jin. Efficient private empirical risk minimization for high-dimensional learning. In International Conference on Machine Learning, pages 488\u2013497. PMLR, 2016.   \n[26] Janardhan Kulkarni, Yin Tat Lee, and Daogao Liu. Private non-smooth empirical risk minimization and stochastic convex optimization in subquadratic steps. arXiv preprint arXiv:2103.15352, 2021.   \n[27] Xiyang Liu, Prateek Jain, Weihao Kong, Sewoong Oh, and Arun Suggala. Label robust and differentially private linear regression: Computational and statistical efficiency. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[28] Xiyang Liu, Prateek Jain, Weihao Kong, Sewoong Oh, and Arun Sai Suggala. Near optimal private and robust linear regression. arXiv preprint arXiv:2301.13273, 2023.   \n[29] Xiyang Liu, Weihao Kong, Prateek Jain, and Sewoong Oh. Dp-pca: Statistically optimal and differentially private pca. Advances in Neural Information Processing Systems, 35:29929\u201329943, 2022.   \n[30] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. arXiv preprint arXiv:1710.06963, 2017.   \n[31] Yuan Qiu, Jinyan Liu, and Di Wang. Truthful generalized linear models. In Kristoffer Arnsfelt Hansen, Tracy Xiao Liu, and Azarakhsh Malekian, editors, Web and Internet Economics - 18th International Conference, WINE 2022, Troy, NY, USA, December 12-15, 2022, Proceedings, volume 13778 of Lecture Notes in Computer Science, pages 369\u2013370. Springer, 2022.   \n[32] Yuan Qiu, Jinyan Liu, and Di Wang. Truthful and privacy-preserving generalized linear models. Information and Computation, 301:105225, 2024.   \n[33] Hanpu Shen, Cheng-Long Wang, Zihang Xiang, Yiming Ying, and Di Wang. Differentially private non-convex learning for multi-layer neural networks. arXiv preprint arXiv:2310.08425, 2023.   \n[34] Shuang Song, Thomas Steinke, Om Thakkar, and Abhradeep Thakurta. Evading the curse of dimensionality in unconstrained private glms. In International Conference on Artificial Intelligence and Statistics, pages 2638\u20132646. PMLR, 2021.   \n[35] Shuang Song, Om Thakkar, and Abhradeep Thakurta. Characterizing private clipped gradient descent on convex generalized linear problems. arXiv preprint arXiv:2006.06783, 2020.   \n[36] Jinyan Su, Lijie Hu, and Di Wang. Faster rates of differentially private stochastic convex optimization. Journal of Machine Learning Research, 25(114):1\u201341, 2024.   \n[37] Jinyan Su and Di Wang. Faster rates of differentially private stochastic convex optimization. arXiv preprint arXiv, 2108, 2021.   \n[38] Jinyan Su, Changhong Zhao, and Di Wang. Differentially private stochastic convex optimization in (non)-euclidean space revisited. In Uncertainty in Artificial Intelligence, pages 2026\u20132035. PMLR, 2023.   \n[39] Youming Tao, Yulian Wu, Xiuzhen Cheng, and Di Wang. Private stochastic convex optimization and sparse learning with heavy-tailed data revisited. In IJCAI, pages 3947\u20133953. ijcai.org, 2022.   \n[40] Hoang Tran and Ashok Cutkosky. Momentum aggregation for private non-convex erm. Advances in Neural Information Processing Systems, 35:10996\u201311008, 2022.   \n[41] Prateek Varshney, Abhradeep Thakurta, and Prateek Jain. (nearly) optimal private linear regression via adaptive clipping. arXiv preprint arXiv:2207.04686, 2022.   \n[42] Di Wang, Changyou Chen, and Jinhui Xu. Differentially private empirical risk minimization with non-convex loss functions. In International Conference on Machine Learning, pages 6526\u20136535. PMLR, 2019.   \n[43] Di Wang and Jinhui Xu. Differentially private empirical risk minimization with smooth nonconvex loss functions: A non-stationary view. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 1182\u20131189, 2019.   \n[44] Di Wang and Jinhui Xu. On sparse linear regression in the local differential privacy model. In International Conference on Machine Learning, pages 6628\u20136637. PMLR, 2019.   \n[45] Di Wang and Jinhui Xu. Escaping saddle points of empirical risk privately and scalably via dptrust region method. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14\u201318, 2020, Proceedings, Part III, pages 90\u2013106. Springer, 2021.   \n[46] Di Wang, Minwei Ye, and Jinhui Xu. Differentially private empirical risk minimization revisited: Faster and more general. Advances in Neural Information Processing Systems, 30, 2017.   \n[47] Lingxiao Wang, Bargav Jayaraman, David Evans, and Quanquan Gu. Efficient privacypreserving stochastic nonconvex optimization. In Uncertainty in Artificial Intelligence, pages 2203\u20132213. PMLR, 2023.   \n[48] Lingxiao Wang, Difan Zou, Kumar Kshitij Patel, Jingfeng Wu, and Nathan Srebro. Private overparameterized linear regression without suffering in high dimensions, 2024.   \n[49] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Finite-sample analysis of learning high-dimensional single relu neuron. 2023.   \n[50] Hanshen Xiao, Zihang Xiang, Di Wang, and Srinivas Devadas. A theory to instruct differentiallyprivate learning via clipping bias reduction. In 2023 IEEE Symposium on Security and Privacy $(S P)$ , pages 2170\u20132189. IEEE Computer Society, 2023.   \n[51] Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang. Efficient private erm for smooth objectives. arXiv preprint arXiv:1703.09947, 2017.   \n[52] Qiuchen Zhang, Jing Ma, Jian Lou, and Li Xiong. Private stochastic non-convex optimization with improved utility rates. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 2021.   \n[53] Yingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private sgd with gradient subspace identification. arXiv preprint arXiv:2007.03813, 2020.   \n[54] Liyang Zhu, Meng Ding, Vaneet Aggarwal, Jinhui Xu, and Di Wang. Improved analysis of sparse linear regression in local differential privacy model. arXiv preprint arXiv:2310.07367, 2023.   \n[55] Liyang Zhu, Amina Manseur, Meng Ding, Jinyan Liu, Jinhui Xu, and Di Wang. Truthful high dimensional sparse linear regression. arXiv preprint arXiv:2410.13046, 2024.   \n[56] Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, and Sham Kakade. Benign overfitting of constant-stepsize sgd for linear regression. In Conference on Learning Theory, pages 4633\u20134635. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Algorithms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Algorithm 1 DP-GLMtron ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1: Input: Training Samples: $\\{(x_{i},y_{i})\\}_{i=0}^{N-1}$ , Estimating Samples: $\\{(x_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{m}$ , Learning Rate: $\\eta$ ,   \nDP Noise Multiplier: $f$ , Expected $\\mathbf{x}$ Norm: $\\sqrt{\\alpha\\operatorname{tr}(\\mathbf{H})}$ , Privacy Parameters: $\\varepsilon^{\\prime},\\delta^{\\prime}$ , Clipping list   \n$\\gamma=\\{\\}$   \n2: Randomly permute $\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=0}^{N-1}$   \n3: Initialize $\\mathbf{w}_{0}\\to0$   \n4: for $t=0,\\ldots,N-1$ do   \n5: $s_{t}\\gets\\mathbf{DP-Threshold}(\\{(\\mathbf{x}_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{m},\\mathbf{w}_{t},\\varepsilon^{\\prime},\\delta^{\\prime})$   \n6: $\\gamma_{t}=\\sqrt{2\\alpha\\operatorname{tr}(\\mathbf{H})}C_{2}\\log^{2a}N\\cdot s_{t}$   \n7: Sample $\\mathbf{g}_{t}\\sim\\mathcal{N}(0,\\mathbf{I}_{d\\times d})$   \n8: $\\mathbf{\\boldsymbol{\\mathsf{w}}}_{t+1}\\gets\\mathbf{\\boldsymbol{\\mathsf{w}}}_{t}-\\eta(\\boldsymbol{l}_{t}+2f\\gamma_{t}\\mathbf{g}_{t})$   \n9: where $l_{t}=\\mathrm{clip}_{\\gamma_{t}}(\\mathbf{x}_{t}^{\\top}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-\\mathbf{y}_{t}))$   \n10: end for   \n11: return $\\begin{array}{r}{\\mathbf{w}\\leftarrow\\frac{1}{N}\\sum_{t=0}^{N-1}\\mathbf{w}_{t}}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 DP-Threshold ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1: Input: Estimating Samples: $\\{(x_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{m}$ , Privacy Parameters: $\\varepsilon^{\\prime},\\delta^{\\prime}$ , Model w, Distribution   \nParameters: $a,b_{z}$ in Definition 3.5   \n2: $\\forall i\\in[m]$ , $d_{i}^{1}\\gets(\\mathbf{x}_{i}^{\\top}\\mathbf{w}-y_{i})^{2}$ , $d_{i}^{2}\\gets y_{i}^{2}$ , $d_{i}^{3}\\gets(\\mathbf{x}_{i}^{\\top}\\mathbf{w})^{2}$ , $\\forall i\\in[m]$   \n3: Partition $\\{d_{i}^{1}\\}_{i=1}^{m}$ , $\\{d_{i}^{2}\\}_{i=1}^{m}$ and $\\{d_{i}^{3}\\}_{i=1}^{m}$ into three $K=\\lceil C_{1}\\log(N/(\\delta^{\\prime}))/\\varepsilon^{\\prime}\\rceil$ subsets of equal   \nsize separately, namely $\\{D_{j}^{1}\\}_{j=1}^{K}$ , $\\{D_{j}^{2}\\}_{j=1}^{K}$ and $\\{D_{j}^{3}\\}_{j=1}^{K}$   \n4: for $\\mathsf{p}{=}1\\mathrm{,}2\\mathrm{,}3$ do   \n5: $\\begin{array}{r}{(\\ensuremath{\\dot{d}}_{j}^{p})^{\\prime}\\leftarrow\\sum_{i\\in D_{j}^{p}}d_{i}^{p}/|D_{j}^{p}|,\\forall j\\in[K]}\\end{array}$   \n6: Partition $[0,\\infty)$ into geometrically increasing intervals $\\Omega^{p}$ :=   \n$\\{\\ldots,[2^{-1},1),[1,2),[2,2^{2}),\\ldots\\}\\ {\\cup}\\left\\{[0,0]\\right\\}$   \n7: Compute $\\begin{array}{r}{\\overline{{d}}_{k}^{p}\\leftarrow\\sum_{j\\in D_{k}^{p}}d_{j}^{p}\\mathbb{1}(d_{j}^{p}\\leq(d_{k}^{p})^{\\prime})/|D_{k}^{p}|}\\end{array}$ , where $D_{k}^{p}$ is the interval in $\\Omega^{p}$   \n8: if $\\overline{{d}}_{k}^{p}\\in(0,2\\log(1/\\delta^{\\prime})/(\\varepsilon^{\\prime}K)+(1/K))$ then   \n9: $\\widetilde{d}_{k}^{p}\\gets\\overline{{d}}_{k}^{p}+p_{k}^{p}$ , where $q_{k}^{p}\\sim\\mathrm{Lap}(0,2/(\\varepsilon^{\\prime}K))$   \n10: else   \n11: $\\bar{\\tilde{d}_{k}^{p}}\\gets0$   \n12: end if   \n13: Let $[s_{1}^{p},s_{2}^{p}]$ be the nonempty interval containing the largest $\\widetilde{d_{k}^{p}}$   \n14: if $\\mathsf{p}{=}3$ then   \n15: se $:s^{p}=\\sqrt{2(s_{2}^{p}+\\sigma^{2}\\cdot\\log^{2a}(1/b_{z}))}$   \n16: else   \n17: set $s^{p}=\\sqrt{2s_{2}^{p}}$   \n18: end if   \n19: end for   \n20: return $s=\\operatorname*{max}\\{s^{p}\\}_{p=1}^{3}$ ", "page_idx": 14}, {"type": "text", "text": "Algorithm 3 DP-TAGLMtron ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Input: Training Samples: $\\{(x_{i},y_{i})\\}_{i=0}^{N-1}$ , Estimating Samples: $\\{(x_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{m}$ , Learning Rate: $\\eta$ ,   \nDP Noise Multiplier: $f$ , Expected x Norm: $\\sqrt{\\alpha\\operatorname{tr}(\\mathbf{H})}$ , Privacy Parameters: $\\varepsilon^{\\prime},\\delta^{\\prime}$ , Clipping list   \n$\\gamma=\\{\\}$   \n2: Initialize $w_{0}\\gets\\mathbf{0}$   \n3: for $t=0\\ldots N-1$ do   \n4: $s_{t}\\gets\\mathbf{DP-Threshold}(\\{(\\mathbf{x}_{i}^{\\prime},y_{i}^{\\prime})\\}_{i=1}^{m},\\mathbf{w}_{t},\\varepsilon^{\\prime},\\delta^{\\prime})$   \n5: $\\gamma_{t}\\leftarrow\\sqrt{2\\alpha\\operatorname{tr}(\\mathbf{H})}C_{2}\\log^{2a}N\\cdot s_{t}$   \n6: add $\\gamma_{t}$ to the list $\\gamma$ and $\\Gamma=\\operatorname*{max}\\gamma$   \n7: $l_{t}\\gets\\mathrm{clip}_{\\gamma_{t}}(\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t}))$   \n8: l\u2032\u2264t \u2190DP-Tree-Aggregation(l\u2264t, f, \u0393)   \n9: Update $\\begin{array}{r}{\\mathbf{w}_{t+1}\\leftarrow\\operatorname*{argmin}_{\\mathbf{w}}\\langle\\mathbf{w},\\boldsymbol{l}_{t}^{\\prime}\\rangle+\\frac{1}{2\\eta}\\|\\mathbf{w}\\|_{2}^{2}}\\end{array}$   \n1101::  erentdu fron $\\begin{array}{r}{\\overline{{\\mathbf{w}}}_{N}\\leftarrow\\frac{1}{N}\\sum_{t=0}^{N-1}\\mathbf{w}_{t}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Algorithm 4 DP-Tree-Aggregation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Input: Vectors: $\\{l_{i}\\}_{i=0}^{t-1}$ , Noise Paramters: $f,\\Gamma$   \n2: Initialization Create a binary tree $\\tau$ of size $2^{\\lceil\\log_{2}N\\rceil+1}-1$ with $N$ leaf nodes, and each node is   \nsampled i.i.d. from $\\mathcal{N}(0,f^{2}\\bar{\\Gamma}^{2}\\mathbf{I}_{d\\times d})$ .   \n3: At $t$ -th iteration, do:   \n4: Accept $\\boldsymbol{l}_{t}$ and add it to all the nodes along the path to the root of $\\tau$ .   \n5: Let $[e_{1},\\ldots,e_{h}]$ be the list of nodes from the root of $\\tau$ to the $t$ -th leaf node, with node $e_{1}$ being   \nthe root node and node $e_{h}$ being the leaf node, where $h$ is the height of tree.   \n6: Let $l_{\\le t}^{\\prime}=\\mathbf{0}^{d}$ and $\\{b_{1},\\ldots,b_{h}\\}$ be the $h$ bit binary representation of $t$ .   \n7: for $j\\in[h]$ do   \n8: if $b_{j}=1$ then   \n9: $\\check{l_{\\le t}^{\\prime}}\\gets l_{\\le t}^{\\prime}+e_{j}$ , where $e_{j}$ is the value in left sibling of $e_{j}$ node.   \n10: end if   \n11: end for   \n12: return $l_{\\le t}^{\\prime}$ ", "page_idx": 15}, {"type": "image", "img_path": "3uUIwMxYbR/tmp/a540085b9fd42aa6a78c6c36a14d12a2c6201f71eb81f99750c45c395faea25b.jpg", "img_caption": ["Figure 3: Comparative Performance of Various Differential Privacy Algorithms on MNIST Across Different Privacy Budgets. This figure illustrates the performance of four differential privacy algorithms\u2014DP-SGD, DP-GLMtron, DP-TAGLMtron, and DP-FTRL\u2014on generated Bernoulli distributed data, comparing excess risk across different sample sizes and privacy budgets $\\varepsilon=0.05$ , 0.2, 0.5). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "In our second experiment, we evaluate the performance of four different differential privacy (DP) algorithms\u2014DP-SGD, DP-GLMtron, DP-TAGLMtron, and DP-FTRL\u2014using the MNIST dataset across varying sample sizes and privacy budgets denoted by $\\varepsilon$ , with values set at 0.05, 0.2, and 0.5. The learning rate was initially set to 0.05, with $N$ representing the sample size, which varied from 0 to 1000 and increased in steps to 100. We utilized a ReLU regression model with noise to be trained using the previously mentioned algorithms, integrating privacy-preserving noise adjustments. The primary metric for evaluation was the average excess risk across varying dataset sizes and $\\varepsilon$ values. ", "page_idx": 16}, {"type": "text", "text": "The key insights from our findings are: Under Tight Privacy Constraints $\\varepsilon=0.05)$ ): DP-TAGLMtron demonstrated superior performance across all sample sizes, achieving the lowest excess risk, while DP-GLMtron\u2019s excess risk increased with larger sample sizes. Under Moderate Privacy Constraints $\\varepsilon=0.2,$ ): The performance differential between our DP-GLMtron and the baseline narrowed, and DP-TAGLMtron continued to exhibit lower excess risks. DP-SGD\u2019s performance suffered with larger sample sizes, highlighting its sensitivity to sample size under moderate privacy levels. Under Relaxed Privacy Constraints $\\varepsilon=0.5)$ ): Our proposed methods markedly outperformed both DP-SGD and DP-FTRL, showcasing their enhanced risk mitigation capabilities when privacy constraints are loosened. These insights demonstrate the effectiveness of our proposed algorithms, DP-TAGLMtron and DP-GLMtron, in optimizing the balance between privacy protection and model accuracy. ", "page_idx": 16}, {"type": "image", "img_path": "3uUIwMxYbR/tmp/7bbf91d6ed94a03d0cc48668c9a14e8c889e169899f3619edb3f7e4339d41e53.jpg", "img_caption": ["Figure 4: Performance of Different Algorithms under Various Privacy Budgets. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Sensitivity of Privacy Budgets. Here, we also included additional experimental results related to the performance of different algorithms under various privacy budgets. we demonstrate the sensitivity of the privacy budget for each algorithm, and it is clear that DP-SGD faces the same issue. When $\\varepsilon$ is small, the tree mechanism in DP-FTRL allows it to add less noise compared to the naive Gradient Descent or GLMtron algorithm, resulting in similar performance between DP-FTRL and DP-GLMtron. Additionally, it is important to note that DP-FTRL should be compared with DP-TAGLMtron rather than DP-GLMtron, as both DP-FTRL and DP-TAGLMtron utilize the tree mechanism, whereas DP-GLMtron does not. Moreover, even though DP-FTRL converges at the end of the training, it can be observed that the excess risk for DP-FTRL is significantly larger than that of DP-TAGLMtron, indicating that it may converge to a suboptimal solution. ", "page_idx": 16}, {"type": "text", "text": "C Supported Lemmas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here, we provide some relaxation for Assumption 3.6. Assumption C.1 (Moment symmetricity conditions [49]). Assume that: ", "page_idx": 17}, {"type": "text", "text": "For every $\\mathbf{u}\\in\\mathbb{H}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\bf x x}^{\\top}\\cdot\\mathbb{1}[{\\bf x}^{\\top}{\\bf u}>0]]=\\mathbb{E}[{\\bf x x}^{\\top}\\cdot\\mathbb{1}[{\\bf x}^{\\top}{\\bf u}<0]].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "2. For every $\\mathbf{u}\\in\\mathbb{H}$ and $\\mathbf{v}\\in\\mathbb{H}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{xx}^{\\top}\\cdot\\mathbb{1}[\\mathbf{x}^{\\top}\\mathbf{u}>0,\\mathbf{x}^{\\top}\\mathbf{v}>0]]=\\mathbb{E}[\\mathbf{xx}^{\\top}\\cdot\\mathbb{1}[\\mathbf{x}^{\\top}\\mathbf{u}<0,\\mathbf{x}^{\\top}\\mathbf{v}<0]].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "3. For every $\\mathbf{u}\\in\\mathbb{H}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\mathbf{x}^{\\otimes4}}\\cdot\\mathbb{1}[{\\mathbf{x}^{\\top}}{\\mathbf{u}}>0]]=\\mathbb{E}[{\\mathbf{x}^{\\otimes4}}\\cdot\\mathbb{1}[{\\mathbf{x}^{\\top}}{\\mathbf{u}}<0]].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "4. For every $\\mathbf{u}\\in\\mathbb{H}$ and $\\mathbf{v}\\in\\mathbb{H}$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[({\\mathbf{x}}^{\\top}{\\mathbf{v}})^{2}{\\mathbf{x}}{\\mathbf{x}}^{\\top}\\cdot\\mathbb{1}[{\\mathbf{x}}^{\\top}{\\mathbf{u}}>0,{\\mathbf{x}}^{\\top}{\\mathbf{v}}>0]]=\\mathbb{E}[({\\mathbf{x}}^{\\top}{\\mathbf{v}})^{2}{\\mathbf{x}}{\\mathbf{x}}^{\\top}\\cdot\\mathbb{1}[{\\mathbf{x}}^{\\top}{\\mathbf{u}}<0,{\\mathbf{x}}^{\\top}{\\mathbf{v}}<0]].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma C.2 ([49]). The following results are direct consequences of Assumption C.1. ", "page_idx": 17}, {"type": "text", "text": "1. Under Assumption C.1, it holds that: for every vector $\\mathbf{u}\\in\\mathbb{H}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\bf x x}^{\\top}\\cdot\\mathbb{1}[{\\bf x}^{\\top}{\\bf u}>0]]=\\frac{1}{2}\\cdot\\mathbb{E}[{\\bf x x}^{\\top}]=:\\frac{1}{2}\\cdot{\\bf H}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "2. Under Assumption C.1, it holds that: for every vector $\\mathbf{u}\\in\\mathbb{H}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}[{\\mathbf{x}}^{\\otimes4}\\cdot\\mathbb{1}[{\\mathbf{x}}^{\\top}{\\mathbf{u}}>0]]=\\frac{1}{2}\\cdot\\mathbb{E}[{\\mathbf{x}}^{\\otimes4}]=:\\frac{1}{2}\\cdot\\mathcal{M}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D DP-GLMtron ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Privacy Guarantee ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To guarantee the privacy of Algorithm 1, we should first ensure that each step of DP-GLMtron is private. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.1. Each update step of $D P$ -GLMtron (Algorithm 1)ensures $(\\varepsilon_{0},\\delta_{0})$ -differential privacy, provided that $\\begin{array}{r}{f=\\frac{c}{\\varepsilon_{0}}}\\end{array}$ , where $c\\geq\\sqrt{2\\log(1.25/\\delta_{0})}$ , and $\\gamma_{t}$ denotes the clipping norm. ", "page_idx": 17}, {"type": "text", "text": "Proof. We first consider $\\{\\mathbf{w}_{t}\\}_{t=0}^{N-1}$ ", "page_idx": 17}, {"type": "text", "text": "Each update step (excluding the DP-noise addition) is of the form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}\\leftarrow\\mathbf{w}_{t}-\\eta\\operatorname{clip}_{\\gamma_{t}}\\bigl(\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t})\\bigr),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathrm{clip}_{\\gamma_{t}}(\\mathbf{v})=\\mathbf{v}\\cdot\\operatorname*{max}\\{1,\\frac{\\gamma_{t}}{\\|\\mathbf{v}\\|_{2}}\\}}\\end{array}$ . Consequently, the local $L_{2}$ sensitivity of $\\mathbf{w}_{t+1}$ is determined by analyzing the variation in the $t^{\\mathrm{th}}$ iteration data sample, as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{2}=\\|\\mathbf{w}_{t+1}^{\\prime}-\\mathbf{w}_{t+1}\\|}\\\\ &{\\quad=\\|\\eta\\mathrm{clip}_{\\gamma_{t}}(\\mathbf{x}_{t}^{\\prime}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\prime\\,\\top}\\mathbf{w}_{t})-y_{t}^{\\prime}))-\\eta\\mathrm{clip}_{\\gamma_{t}}(\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t}))\\|}\\\\ &{\\quad\\leq2\\eta\\|\\mathrm{clip}_{\\gamma_{t}}(\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t}))\\|}\\\\ &{\\quad=2\\eta\\gamma_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, denoting $\\varepsilon^{\\prime}=\\varepsilon_{0}/\\sqrt{8N\\log(2/\\delta_{0})}$ and $\\delta^{\\prime}=\\delta_{0}/(2N)$ , according to Lemma 2.3 in [24], we could know that $\\{s_{t}\\}_{t=0}^{N}$ is $(\\varepsilon^{\\prime},\\delta^{\\prime})$ -DP. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.2 ([17]). For a domain $\\mathcal{D}$ , let $\\mathcal{R}^{(i)}:f\\times D\\to S^{(i)}$ for $i\\in[n]$ (where $S^{(i)}$ is the range space of $\\mathcal{R}^{(i)}$ ) be a sequence of algorithms such that $\\mathcal{R}^{(i)}(z_{1:i-1},\\cdot)$ is $a\\left(\\varepsilon_{0},\\delta_{0}\\right)$ -DP local randomizer for all values of auxiliary inputs $z_{1:i-1}\\in\\mathcal{S}^{(1)}\\times\\cdot\\cdot\\cdot\\times\\mathcal{S}^{(i-1)}$ . Let $A_{s}:{\\mathcal{D}}^{n}\\to S^{(1)}\\times\\cdot\\cdot\\cdot\\times S^{(n)}$ be the algorithm that given a dataset $x_{1:n}\\in{\\mathcal{D}}^{n}$ , samples a uniformly random permutation $\\pi$ , then sequentially computes $z_{i}=\\mathcal{R}^{(i)}(z_{1:i-1},x_{\\pi(i)}).$ for $i\\in[n]$ , and outputs $z_{1:n}$ . Then for any $\\delta\\in[0,1]$ such that $\\begin{array}{r}{\\varepsilon_{0}\\leq\\log(\\frac{n}{16\\log(2/\\delta)}),\\mathcal{A}_{s}}\\end{array}$ is $(\\varepsilon,\\delta+O(e^{\\varepsilon}\\delta_{0}n))\\cdot$ - $.D P$ where $\\varepsilon$ is: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varepsilon=O((1-e^{-\\varepsilon_{0}})(\\frac{\\sqrt{e^{\\varepsilon_{0}}\\log(1/\\delta)}}{\\sqrt{n}}+\\frac{e^{\\varepsilon_{0}}}{n})).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We are now prepared to prove Theorem 4.1, utilizing the lemmas discussed above. ", "page_idx": 18}, {"type": "text", "text": "Firstly, we reformulate the update rule into a sequence of one-step algorithms as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathfrak{I}^{(t+1)}(u_{0:t},(\\mathbf{x},y)):=\\mathbf{w}_{t+1}\\leftarrow\\mathbf{w}_{t}(u_{0:t})-\\eta\\operatorname{clip}_{\\gamma_{t}}(\\mathbf{x}_{\\pi(t)}(\\mathbf{ReLU}(\\mathbf{x}_{\\pi(t)}^{\\top}\\mathbf{w}_{t}(u_{0:t}))-y_{\\pi(t)}))-2\\eta\\gamma_{t}f g_{t},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $u$ denotes auxiliary inputs, and $\\pi(t)$ represents the sample at the $t$ -th iteration after randomly permuting the input data. ", "page_idx": 18}, {"type": "text", "text": "From Lemma D.1, each $\\mathcal{R}^{(t+1)}(u_{0:t},\\cdot)$ is a $(\\varepsilon_{0},\\delta_{0})$ -DP local randomizer algorithm, where $\\varepsilon_{0}\\leq$ $\\log\\bigl(\\frac{N}{16\\log(2/\\widehat{\\delta})}\\bigr)$ . The output of DP-GLMtron is derived through post-processing of the shuffled outputs $u_{t+1}=\\mathcal{R}^{(t+1)}\\big(u_{0:t},(\\mathbf{x},y)\\big)$ for $t\\in0,\\ldots,N-1$ . Therefore, by Lemma D.2, Algorithm DP-GLMtron adheres to $(\\widehat{\\varepsilon},\\widehat{\\delta}+O(e^{\\widehat{\\varepsilon}}\\delta_{0}N))$ -DP, where: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\varepsilon}=O((1-e^{-\\varepsilon_{0}})(\\frac{\\sqrt{e^{\\varepsilon_{0}}\\log(1/\\widehat{\\delta})}}{\\sqrt{N}}+\\frac{e^{\\varepsilon_{0}}}{N})).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Assuming $\\varepsilon_{0}\\leq\\frac{1}{2}$ , we can infer the existence of some constant $c_{1}>0$ such that: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\varepsilon}\\le c_{1}\\cdot(1-e^{-\\varepsilon_{0}})(\\frac{\\sqrt{e^{\\varepsilon_{0}}\\log(1/\\widehat{\\delta})}}{\\sqrt{N}}+\\frac{e^{\\varepsilon_{0}}}{N})}\\\\ &{\\quad\\le c_{1}\\cdot((e^{\\varepsilon_{0}/2}-e^{-\\varepsilon_{0}/2})\\sqrt{\\frac{\\log(1/\\widehat{\\delta})}{N}}+(e^{\\varepsilon_{0}}-1)\\frac{1}{N})}\\\\ &{\\quad\\le c_{1}\\cdot(((1+\\varepsilon_{0})-(1-\\varepsilon_{0}/2))\\sqrt{\\frac{\\log(1/\\widehat{\\delta})}{N}}+((1+2\\varepsilon_{0})-1)\\frac{1}{N})}\\\\ &{\\quad=c_{1}\\cdot\\varepsilon_{0}(\\frac{1}{2}\\sqrt{\\frac{\\log(1/\\widehat{\\delta})}{N}}+\\frac{2}{N}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "iBnyd espeettnindeg $\\begin{array}{r}{f\\,=\\,\\frac{\\sqrt{2\\log(1.25/\\delta_{0})}}{\\varepsilon_{0}}}\\end{array}$ $(\\varepsilon_{0},\\delta_{0})$ -inD PL,e bmasmead  Do.n1 , stwaen deanrsdu rGe atuhsast ieaanc hm eucphdaanteis smte.p  Rofe pDlaPc-iGngL $\\varepsilon_{0}~=$ $\\frac{\\sqrt{2\\log(1.25/\\delta_{0})}}{f}$ , we obtain: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\varepsilon}\\leq c_{1}\\cdot\\frac{\\sqrt{2\\log(1.25/\\delta_{0})\\log(1/\\widehat{\\delta})}}{f\\sqrt{N}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To satisfy overall $(\\varepsilon,\\delta)$ -DP, set $\\begin{array}{r}{\\widehat{\\delta}=\\frac{\\delta}{2}}\\end{array}$ , and $\\delta_{0}=c_{2}\\cdot\\frac{\\delta}{e^{\\widehat{\\varepsilon}N}}$ for some constant $c_{2}>0$ . From this, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\varepsilon}\\le c_{1}\\cdot\\frac{\\sqrt{2\\log(c_{2}\\cdot1.25\\cdot e^{\\widehat{\\varepsilon}}N/\\delta)\\cdot\\log(2/\\delta)}}{f\\sqrt{N}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For any $\\varepsilon\\leq1$ , setting $\\begin{array}{r}{f=c_{3}\\cdot\\frac{\\log(N/\\delta)}{\\varepsilon\\sqrt{N}}\\geq c_{3}\\frac{\\sqrt{\\log(N/\\delta)\\log(1/\\delta)}}{\\varepsilon\\sqrt{N}}}\\end{array}$ for a sufficiently large $c_{3}>0$ ensures that $\\widehat{\\varepsilon}\\le\\varepsilon$ . Additionally, to fulfill Lemma D.2\u2019s assumption, $\\varepsilon_{0}\\,<\\,\\frac{1}{2}$ must be satisfied, which is attainable by setting $\\begin{array}{r}{\\varepsilon=O(\\sqrt{\\frac{\\log(N/\\delta)}{N}})}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "This implies that for $\\begin{array}{r c l}{f}&{=}&{\\Omega(\\frac{\\log(N/\\delta)}{\\varepsilon\\sqrt{N}})}\\end{array}$ , DP-GLMtron achieves $(\\varepsilon,\\delta)$ -DP as long as $\\varepsilon=$ $O(\\sqrt{\\frac{\\log(N/\\delta)}{N}})$ , thereby completing the proof. ", "page_idx": 18}, {"type": "text", "text": "D.2 Utility Guarantee ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here we first provide several results that will be used in our DP-GLMtron utility bound. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.3 ([28]). Let $\\begin{array}{r l r}{m}&{{}}&{=}&{\\Omega(b\\log(b/\\delta^{\\prime})\\log N/\\varepsilon^{\\prime})}\\end{array}$ and denote $\\begin{array}{r l r l}{V}&{{}}&{=}&{}\\end{array}$ $\\mathrm{max}\\{\\|\\mathbf{w}^{*}-\\mathbf{w}_{t}\\|_{\\mathbf{H}}^{2}+\\sigma^{2},\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}+\\sigma^{2},\\|\\mathbf{w}_{t}\\|_{\\mathbf{H}}^{2}+\\sigma^{2}\\}$ , with probability at least $1\\,-\\,1/b$ , Algorithm 2 returns $s_{t}$ such that $\\bar{V}\\leq\\gamma_{t}^{2}\\leq2V$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Consider each sample $\\mathbf{x}_{i}\\sim\\mathcal{D}$ satisfying $(\\mathbf{H},C_{2},a,b)$ -Tail and the inherent noise $z_{i}$ satisfying $(\\sigma^{2},C_{2},a,b)$ -Tail. It implies the following facts with Assumption 3.4 ", "page_idx": 19}, {"type": "text", "text": "\u2022 $\\exists a>0$ s.t. with probability $\\geq1-b_{\\mathbf{x}}$ , such that $\\|\\mathbf{x}\\|_{2}^{2}\\leq\\alpha\\operatorname{tr}(\\mathbf{H})\\cdot\\log^{2a}(1/b_{\\mathbf{x}}).$ .   \n\u2022 $\\exists a>0$ s.t. with probability $\\ge1-b_{v}$ , such that $\\langle\\mathbf{x},\\mathbf{v}\\rangle^{2}\\leq C_{2}^{2}\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}\\log(1/b_{v}).$ .   \n\u2022 $\\exists a>0$ s.t. with probability $\\ge1-b_{z}$ , such that $\\|z\\|_{2}^{2}\\leq\\sigma^{2}\\cdot\\log^{2a}(1/b_{z})$ . ", "page_idx": 19}, {"type": "text", "text": "Additionally, we know that ", "page_idx": 19}, {"type": "text", "text": "$\\|\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t})\\|=\\|\\mathbf{x}_{t}(\\operatorname*{max}(0,\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t})\\|\\leq\\|\\mathbf{x}_{t}\\|\\|(\\operatorname*{max}(0,\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t})\\|$ According to the aforementioned facts, it holds that with probability $\\geq1-b_{\\mathbf{x}}$ , such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}\\|_{2}^{2}\\leq\\alpha\\operatorname{tr}(\\mathbf{H})\\cdot\\log^{2a}(1/b_{\\mathbf{x}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{s_{t}}\\|\\leq\\operatorname*{max}\\{\\sqrt{2((\\mathbf{x}_{t}^{\\top}(\\mathbf{w}_{t}-\\mathbf{w}^{*}))^{2}+z_{t}^{2})},\\sqrt{2((\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{*})^{2}+z_{t}^{2})},\\sqrt{2((\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})^{2}+z_{t}^{2})}\\}}\\\\ &{\\qquad\\leq\\sqrt{2C_{2}^{2}\\log^{2a}(1/b)}\\cdot\\operatorname*{max}\\{\\sqrt{\\|\\mathbf{w}^{*}-\\mathbf{w}_{t}\\|_{\\mathbf{H}}^{2}+\\sigma^{2}},\\sqrt{\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}+\\sigma^{2}},\\sqrt{\\|\\mathbf{w}_{t}\\|_{\\mathbf{H}}^{2}+\\sigma^{2}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second part of the proof can be directly derived from Lemma 2.3 in [24] and Theorem 5 in [28], because each part of $d_{i}^{\\bar{p}}$ , with high probability, holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{s_{t}}\\|\\geq\\operatorname*{max}\\{\\sqrt{\\|\\mathbf{w}^{*}-\\mathbf{w}_{t}\\|_{\\mathbf{H}}^{2}+\\sigma^{2}},\\sqrt{\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}+\\sigma^{2}},\\sqrt{\\|\\mathbf{w}_{t}\\|_{\\mathbf{H}}^{2}+\\sigma^{2}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, we have the following clipping results: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t})\\|_{2}^{2}}\\\\ &{\\leq2\\alpha\\operatorname{tr}(\\mathbf{H})\\cdot\\log^{2a}(1/b_{\\mathbf{x}})\\cdot C_{2}^{2}\\log^{2a}(1/b)\\cdot\\operatorname*{max}\\{\\|\\mathbf{w}^{*}-\\mathbf{w}_{t}\\|_{\\mathbf{H}}^{2}+\\sigma^{2},\\|\\mathbf{w}^{*}\\|_{\\mathbf{H}}^{2}+\\sigma^{2},\\|\\mathbf{w}_{t}\\|_{\\mathbf{H}}^{2}+\\sigma^{2}\\}}\\\\ &{\\leq2\\alpha\\operatorname{tr}(\\mathbf{H})\\cdot\\log^{4a}(1/b)\\cdot C_{2}^{2}\\cdot s_{t}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma D.4 (Generic bounds on the DP-GLMtron iterates [49]). Suppose that Assumption 3.6 holds. Considering the $D P$ -GLMtron algorithm, we have the following recursion: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;\\mathbf{A}_{t}\\preceq\\mathbf{A}_{t-1}-\\frac{\\eta}{2}(\\mathbf{H}\\mathbf{A}_{t-1}+\\mathbf{A}_{t-1}\\mathbf{H})+\\eta^{2}\\mathcal{M}\\circ\\mathbf{A}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+4\\eta^{2}\\Gamma^{2}f^{2}\\mathbf{I}}\\\\ &{\\bullet\\;\\mathbf{A}_{t}\\succeq\\mathbf{A}_{t-1}-\\frac{\\eta}{2}(\\mathbf{H}\\mathbf{A}_{t-1}+\\mathbf{A}_{t-1}\\mathbf{H})+\\frac{\\eta^{2}}{4}\\mathcal{M}\\circ\\mathbf{A}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+4\\eta^{2}\\widetilde{\\Gamma}^{2}f^{2}\\mathbf{I}}\\\\ &{\\mathbf{A}_{t}:=\\mathbb{E}(\\mathbf{w}_{t}-\\mathbf{w}_{\\ast})(\\mathbf{w}_{t}-\\mathbf{w}_{\\ast})^{\\top},t\\geq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now consider the recursion of ${\\bf A}_{t}$ given in Lemma D.4. Note that ${\\bf A}_{t}$ is related to $\\mathbf{A}_{t-1}$ through a linear operator, therefore ${\\bf A}_{t}$ can be understood as the sum of two iterates, i.e., $\\mathbf{A}_{t}:=\\mathbf{B}_{t}+\\mathbf{C}_{t}$ , where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbf{B}_{t}\\preceq({\\boldsymbol{\\mathcal{Z}}}-\\frac{\\eta}{2}\\cdot{\\mathcal{T}}(2\\eta))\\circ\\mathbf{B}_{t-1};}\\\\ {\\mathbf{B}_{0}=(\\mathbf{w}_{0}-\\mathbf{w}_{*})^{\\otimes2}}&{\\left\\{\\begin{array}{l l}{\\mathbf{C}_{t}\\preceq({\\boldsymbol{\\mathcal{Z}}}-\\frac{\\eta}{2}\\cdot{\\mathcal{T}}(2\\eta))\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+4\\eta^{2}\\Gamma^{2}f^{2}\\mathbf{I};}\\\\ {\\mathbf{C}_{0}=0}\\end{array}\\right.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbf{B}_{t}\\succeq(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(\\frac{\\eta}{2}))\\circ\\mathbf{B}_{t-1};}\\\\ {\\mathbf{B}_{0}=(\\mathbf{w}_{0}-\\mathbf{w}_{*})^{\\otimes2}}&{\\;\\;\\left\\{\\begin{array}{l l}{\\mathbf{C}_{t}\\succeq(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(\\frac{\\eta}{2}))\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+4\\eta^{2}\\widetilde{\\Gamma}^{2}f^{2}\\mathbf{I};}\\\\ {\\mathbf{C}_{0}=0}\\end{array}\\right.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{({\\mathbb{Z}}-\\frac{\\eta}{2}\\cdot{\\mathcal{T}}(2\\eta))\\circ\\mathbf{A}_{t-1}:=\\mathbf{A}_{t-1}-\\frac{\\eta}{2}(\\mathbf{HA}_{t-1}+\\mathbf{A}_{t-1}\\mathbf{H})+\\eta^{2}{\\mathcal{M}}\\circ\\mathbf{A}_{t-1};}\\\\ {({\\mathbb{Z}}-\\frac{\\eta}{2}\\cdot{\\mathcal{T}}(\\frac{\\eta}{2}))\\circ\\mathbf{A}_{t-1}:=\\mathbf{A}_{t-1}-\\frac{\\eta}{2}(\\mathbf{HA}_{t-1}+\\mathbf{A}_{t-1}\\mathbf{H})+\\frac{\\eta^{2}}{4}{\\mathcal{M}}\\circ\\mathbf{A}_{t-1}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Besides, since our DP-GLM-tron is run with constant stepsize $\\eta$ and outputs the average of the iterates: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{w}}}_{N}:=\\frac{1}{N}\\sum_{t=0}^{N-1}\\mathbf{w}_{t}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, the following lemma holds: ", "page_idx": 20}, {"type": "text", "text": "Lemma D.5. Suppose that Assumption 3.6 hold. For $\\overline{{\\mathbf{w}}}_{N}$ defined in Equation (9), we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\langle\\mathbf{H},(\\overline{{\\mathbf{w}}}_{N}-\\mathbf{w}_{*})^{\\otimes2}\\rangle\\leq\\displaystyle\\sum_{t=0}^{N-1}\\sum_{k=t}^{N-1}\\frac{1}{\\eta N^{2}}\\langle(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{k-t}\\mathbf{H},\\mathbf{A}_{t}\\rangle}\\\\ &{\\mathbb{E}\\langle\\mathbf{H},(\\overline{{\\mathbf{w}}}_{N}-\\mathbf{w}_{*})^{\\otimes2}\\rangle\\geq\\displaystyle\\sum_{t=0}^{N-1}\\sum_{k=t}^{N-1}\\frac{1}{2\\eta N^{2}}\\langle(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{k-t}\\mathbf{H},\\mathbf{A}_{t}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\ E}[\\mathbf{w}_{t}-\\mathbf{w}_{*}\\mid\\mathbf{w}_{t-1}]=\\mathbb{E}[(\\mathbf{I}-\\eta\\mathbb{I}\\{\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0\\}\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})(\\mathbf{w}_{t-1}-\\mathbf{w}_{*})\\mid\\mathbf{w}_{t-1}]+2\\eta\\Gamma\\mathcal{I}\\mathbb{E}[\\mathbf{g}_{t-1}\\mid\\mathbf{w}_{t-1}]}\\\\ &{\\phantom{=}\\quad+\\eta\\cdot\\mathbb{E}[(\\mathbb{I}\\{\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{*}>0\\}-\\mathbb{I}\\{\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0\\})\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{*}\\mid\\mathbf{w}_{t-1}]+\\eta\\mathbb{E}[z_{t}\\mathbf{x}_{t}\\mid\\mathbf{w}_{t-1}}\\\\ &{\\phantom{=}=\\mathbb{E}[(\\mathbf{I}-\\eta\\mathbb{I}\\{\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0\\}\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})(\\mathbf{w}_{t-1}-\\mathbf{w}_{*})\\mid\\mathbf{w}_{t-1}]}\\\\ &{\\phantom{=}=\\!(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})(\\mathbf{w}_{t-1}-\\mathbf{w}_{*})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The remaining proof simply follows [56]. ", "page_idx": 20}, {"type": "text", "text": "From the decomposition presented in Equation (7) and Equation (8), we know that $\\sum_{t=0}^{N}\\mathbf{A}_{t}=$ $\\begin{array}{r}{\\sum_{t=0}^{N}\\mathbf{B}_{t}+\\sum_{t=0}^{N}\\mathbf{C}_{t}}\\end{array}$ . With this foundation, we can now bound the bias and variance terms separately. ", "page_idx": 20}, {"type": "text", "text": "Upper bound for variance error ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We then assume that $\\begin{array}{r}{\\mathbf{C}_{t-1}\\preceq\\frac{\\eta\\sigma^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{I}+\\frac{4\\eta\\Gamma^{2}f^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{H}^{-1}}\\end{array}$ , and exam $\\mathbf{C}_{t}$ based on Equation (7): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{C}_{t}\\preceq({Z}-\\eta\\cdot{\\mathcal{T}}(\\eta))\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+4\\eta^{2}\\Gamma^{2}f^{2}\\mathbf{I}}\\\\ &{\\quad\\quad=\\mathbf{C}_{t-1}-\\frac{\\eta}{2}(\\mathbf{H}\\mathbf{C}_{t-1}+\\mathbf{C}_{t-1}\\mathbf{H})+\\eta^{2}{\\mathcal{A}}\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+4\\eta^{2}\\Gamma^{2}f^{2}\\mathbf{I}}\\\\ &{\\quad\\quad\\preceq\\frac{\\eta\\sigma^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{I}+\\frac{4\\eta\\Gamma^{2}f^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{H}^{-1}-\\eta(\\frac{\\eta\\sigma^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{H}+\\frac{4\\eta\\Gamma^{2}f^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{I})}\\\\ &{\\quad\\quad+\\eta^{2}(\\alpha\\operatorname{tr}(\\mathbf{H}))(\\frac{\\eta\\sigma^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{H}+\\frac{4\\eta\\Gamma^{2}f^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{I})+\\eta^{2}\\sigma^{2}\\mathbf{H}+4\\eta^{2}\\Gamma^{2}f^{2}\\mathbf{I}}\\\\ &{\\quad\\quad\\prec\\frac{\\eta\\sigma^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{I}+\\frac{4\\eta\\Gamma^{2}f^{2}}{1-\\eta\\left(\\alpha\\operatorname{tr}(\\mathbf{H})\\right)}\\mathbf{H}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the simplicity, we define $\\Sigma:=\\sigma^{2}\\mathbf{H}+4\\Gamma^{2}f^{2}\\mathbf{I}$ . By the definitions of $\\tau$ and $\\tilde{\\tau}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{C}_{t}=(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\Sigma}\\\\ &{\\quad=(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))\\circ\\mathbf{C}_{t-1}+\\eta^{2}(\\mathcal{M}-\\frac{1}{4}\\widetilde{\\mathcal{M}})\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\Sigma}\\\\ &{\\quad\\preceq(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\mathcal{M}\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\Sigma,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality is due to the fact that $\\widetilde{\\mathcal{M}}$ is a PSD mapping. Then by the iteration of variance, we have for all $t\\geq0$ , ", "page_idx": 21}, {"type": "text", "text": "$\\mathsf{M o C}_{t}\\preceq\\mathcal{M}\\circ(\\frac{\\eta\\sigma^{2}}{1-\\eta(\\alpha\\operatorname{tr}(\\mathbf{H}))}\\mathbf{I}+\\frac{4\\eta\\Gamma^{2}f^{2}}{1-\\eta(\\alpha\\operatorname{tr}(\\mathbf{H}))}\\mathbf{H}^{-1})\\preceq\\frac{\\eta\\sigma^{2}(\\alpha\\operatorname{tr}(\\mathbf{H}))}{1-\\eta(\\alpha\\operatorname{tr}(\\mathbf{H}))}\\mathbf{H}+\\frac{4\\eta\\Gamma^{2}f^{2}(\\alpha\\operatorname{tr}(\\mathbf{H}))}{1-\\eta(\\alpha\\operatorname{tr}(\\mathbf{H}))}\\mathbf{I}$ Substituting above into Appendix D.2, we obtain: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{C}_{t}\\equiv(T-\\frac{\\eta}{2},\\widetilde{T}(2\\eta))\\circ\\mathbf{C}_{t-1}+\\frac{\\eta^{3}(\\alpha\\mathrm{H}(\\mathbf{H}))}{1-\\eta(\\alpha\\mathrm{H}(\\mathbf{H}))}\\circ(\\sigma^{2}\\mathbf{H}+4\\Gamma^{2}\\rho^{2}\\mathbb{I})+\\eta^{2}\\mathbf{C}}\\\\ &{\\quad=(T-\\frac{\\eta}{2},\\widetilde{T}(2\\eta))\\circ\\mathbf{C}_{t-1}+\\frac{\\eta^{3}(\\alpha\\mathrm{H}(\\mathbf{H}))}{1-\\eta(\\alpha\\mathrm{H}(\\mathbf{H}))}\\cdot(\\sigma^{2}\\mathbf{H}+4\\Gamma^{2}\\rho^{2}\\mathbb{I})+\\eta^{2}(\\sigma^{2}\\mathbf{H}+4\\Gamma^{2}\\rho^{2}\\mathbb{I})}\\\\ &{\\quad=(T-\\frac{\\eta}{2},\\widetilde{T}(2\\eta))\\circ\\mathbf{C}_{t-1}+\\frac{\\eta^{2}(\\alpha)}{1-\\eta(\\alpha\\mathrm{H}(\\mathbf{H}))}\\cdot(\\sigma^{2}\\mathbf{H}+4\\Gamma^{2}\\rho^{2}\\mathbb{I})}\\\\ &{\\quad\\le\\frac{\\eta^{2}}{1-\\eta(\\alpha\\mathrm{H}(\\mathbf{H}))}\\cdot\\frac{\\Gamma^{2}}{\\log{0}}(T-\\frac{\\eta}{2},\\widetilde{T}(2\\eta))^{k}\\circ(\\sigma^{2}\\mathbf{H}+4\\Gamma^{2}\\rho^{2}\\mathbb{I})}\\\\ &{\\quad=\\frac{\\eta^{2}}{1-\\eta(\\alpha\\mathrm{H}(\\mathbf{H}))}\\cdot\\frac{\\Gamma^{2}}{k-0}(\\mathbf{I}-\\frac{\\eta}{2}\\mathbb{I})^{k}(\\sigma^{2}\\mathbf{H}+4\\Gamma^{2}\\rho^{2}\\mathbb{I})(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{k}}\\\\ &{\\quad\\le\\frac{\\eta^{2}}{1-\\eta(\\alpha\\mathrm{H}(\\mathbf{H}))}\\cdot\\frac{\\Gamma^{2}}{k-0}(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{k}(\\sigma^{2}\\mathbf{H}+4\\Gamma^{2}\\rho^{2}\\mathbb{I})(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{k}}\\\\ &{\\quad\\le\\frac{\\eta^{2}}{1-\\eta(\\alpha\\mathrm{H}(\\mathbf{H}))} \n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consequently, the variance error can be represented as follows, in accordance with Lemma D.5: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Vatance~error}\\leq\\displaystyle\\frac{1}{\\eta N^{2}}\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\sum_{t=0}^{N}\\mathbf{C}_{t}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{\\eta N^{2}}\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\sum_{t=0}^{N}\\frac{\\eta\\sigma^{2}}{1-\\eta(\\alpha\\mathrm{tr}(\\mathbf{H}))}\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{t})\\rangle}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\frac{1}{\\eta N^{2}}\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\sum_{t=0}^{N}\\frac{4\\eta\\Gamma^{2}f^{2}}{1-\\eta(\\alpha\\mathrm{tr}(\\mathbf{H}))}\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{t})\\cdot\\mathbf{H}^{-1}\\rangle}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\frac{\\sigma^{2}}{(1-\\eta(\\alpha\\mathrm{tr}(\\mathbf{H})))N}\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})\\rangle}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\frac{4\\Gamma^{2}f^{2}}{(1-\\eta(\\alpha\\mathrm{tr}(\\mathbf{H})))N}\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})\\cdot\\mathbf{H}^{-1}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Noting that for any $x\\in(0,1/\\eta)$ , we have $1-(1-\\eta x)^{N}\\leq\\operatorname*{min}\\{1,N\\eta x\\}$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N}\\preceq\\mathbf{I}_{0:k}+N\\frac{\\eta}{2}\\mathbf{H}_{k:\\infty}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Final results can be obtained: variance error: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\!\\frac{\\sigma^{2}}{(1-\\eta(\\alpha\\operatorname{tr}(\\mathbf{H})))N}(k^{*}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}^{\\sum_{i}2}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sum_{i}\\!\\!\\!\\!\\lambda_{i}^{2})+\\frac{4\\Gamma^{2}f^{2}}{(1-\\eta(\\alpha\\operatorname{tr}(\\mathbf{H})))N}\\operatorname{Tr}(\\mathbf{H}_{0,k^{*}}^{-1}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}^{\\sum_{i}}.}\\\\ &{\\le\\!\\frac{\\sigma^{2}}{(1-\\eta(\\alpha\\operatorname{tr}(\\mathbf{H})))N}(k^{*}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}^{\\sum_{i}\\lambda_{i}^{2}})+\\frac{4\\Gamma^{2}f^{2}}{N(1-\\eta(\\alpha\\operatorname{tr}(\\mathbf{H})))}(k^{*}N\\eta+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}^{\\sum_{i}\\lambda_{i}})}\\\\ &{\\lesssim\\!\\frac{\\sigma^{2}}{N}(k^{*}+N^{2}\\eta^{2}\\sum_{i>k^{*}}^{\\sum_{i}\\lambda_{i}^{2}})+\\frac{\\Gamma^{2}f^{2}}{N}(\\sum_{i<k^{*}}^{\\sum_{i}\\lambda_{i}^{-1}}+N^{2}\\eta^{2}\\cdot\\sum_{i>k^{*}}^{\\sum_{i}\\lambda_{i}})}\\\\ &{\\lesssim\\!\\frac{\\sigma^{2}}{N}(k^{*}+N^{2}\\eta^{2}\\sum_{i>k^{*}}^{\\sum_{i}\\lambda_{i}^{2}})+\\Gamma^{2}f^{2}\\eta(k^{*}+N\\eta\\sum_{i>k^{*}}^{\\sum_{i}\\lambda_{i}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality holds since $\\begin{array}{r}{\\mathrm{tr}(\\mathbf{H}_{0:k^{*}}^{-1})=\\sum_{i=1}^{k^{*}}\\lambda_{i}^{-1}\\leq N\\eta\\cdot k^{*}}\\end{array}$ and $\\begin{array}{r}{\\lambda_{i}\\geq\\frac{1}{\\eta N}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Lower bound for variance error ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Similarly, we derive a recursion for the variance term to establish the lower bound: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{C}_{t}=(\\mathcal{Z}-\\eta\\mathcal{T}(\\eta))\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\Sigma}\\\\ &{\\quad=(\\mathcal{T}-\\eta\\widetilde{\\mathcal{T}}(\\eta))\\circ\\mathbf{C}_{t-1}+\\eta^{2}(\\mathcal{M}-\\frac{1}{4}\\widetilde{\\mathcal{M}})\\circ\\mathbf{C}_{t-1}+\\eta^{2}\\Sigma}\\\\ &{\\quad\\succ(\\mathcal{T}-\\eta\\widetilde{\\mathcal{T}}(\\eta))\\circ\\mathbf{C}_{t-1}+\\eta^{2}(\\sigma^{2}\\mathbf{H}+4\\widetilde{\\Gamma}^{2}f^{2}\\mathbf{I}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Upsilon_{t}=\\eta^{2}\\cdot\\sum_{k=0}^{t-1}({\\bf I}-\\frac{\\eta}{2}{\\bf H})^{k}\\big(\\sigma^{2}{\\bf H}+4\\widetilde{\\Gamma}^{2}f^{2}{\\bf I}\\big)({\\bf I}-\\frac{\\eta}{2}{\\bf H})^{2k}}\\\\ {\\displaystyle\\quad\\succ\\eta^{2}\\cdot\\sum_{k=0}^{t-1}({\\bf I}-\\frac{\\eta}{2}{\\bf H})^{k}\\big(\\sigma^{2}{\\bf H}+4\\widetilde{\\Gamma}^{2}f^{2}{\\bf I}\\big)}\\\\ {\\displaystyle}\\\\ {\\displaystyle=\\eta^{2}\\sigma^{2}\\cdot({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf H})^{2t})\\cdot\\big(\\eta{\\bf I}-\\frac{\\eta^{2}}{4}{\\bf H}\\big)^{-1}+4\\eta^{2}\\widetilde{\\Gamma}^{2}f^{2}\\cdot({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf H})^{2t})\\cdot\\big(\\eta{\\bf H}-\\frac{\\eta^{2}}{4}{\\bf H}{\\bf H}\\big)^{-1}}\\\\ {\\displaystyle\\quad\\succ\\eta\\sigma^{2}\\cdot({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf H})^{2t})+4\\eta\\widetilde{\\Gamma}^{2}f^{2}\\cdot({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf H})^{2t})\\cdot{\\bf H}^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality comes from $\\begin{array}{r}{\\eta\\mathbf{I}-\\frac{\\eta^{2}}{4}\\mathbf{H}\\preceq\\eta\\mathbf{I}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Applying Lemma D.5, we could derive the lower bound for variance error: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{curationce~corros:}\\quad\\frac{1}{2\\eta\\eta}\\frac{N^{-1}-1}{\\gamma\\omega_{\\infty}^{2}}(({\\bf I}-\\frac{\\eta}{2}{\\bf I})^{n-1}{\\bf I},{\\bf G},{\\bf C}_{1})}\\\\ &{\\geq\\frac{\\alpha^{2}}{N^{2}}\\sum_{t=0}^{\\infty}({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf I})^{n-t},({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf I})^{n}))}\\\\ &{\\quad+\\frac{4\\overline{{{\\bf J}}}\\eta^{2}}{2}\\sum_{t=0}^{\\infty}({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf I})^{n-t},({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf I})^{n}),{\\bf H}^{-1})}\\\\ &{\\geq\\frac{\\alpha^{2}}{N^{2}}\\sum_{t=0}^{\\infty}({\\bf I}-({\\bf I}-\\frac{\\eta}{2}{\\bf I})^{n-t})(1-({\\bf I}-\\frac{\\eta}{2}{\\bf I})^{n})}\\\\ &{\\quad+\\frac{4\\overline{{{\\bf J}}}\\eta^{2}}{N^{2}}\\sum_{t=0}^{\\infty}({\\bf I}-(1-\\frac{\\eta}{2}{\\bf I})^{n-t})(1-(1-\\frac{\\eta}{2}{\\bf I})^{n})\\lambda^{n-t}}\\\\ &{\\geq\\frac{\\alpha^{2}}{N^{2}}\\sum_{t=0}^{\\infty}(1-\\frac{\\eta}{2}\\overline{{{\\bf J}}})^{n-t})(1-(1-\\frac{\\eta}{2}{\\bf I})^{n})(1-\\frac{\\eta}{2}\\overline{{\\lambda}})^{n-t}}\\\\ &{\\geq\\frac{\\alpha^{2}}{N^{2}}\\sum_{t=0}^{\\infty}(1-(1-\\frac{\\eta}{2}{\\bf I})^{n-t-1})(1-(1-\\frac{\\eta}{2}{\\bf I})^{n})}\\\\ &{\\quad+\\frac{4\\overline{{{\\bf J}}}\\eta^{2}}{N^{2}}\\sum_{s=0}^{\\infty}(1-(1-\\frac{\\eta}{2}{\\bf I})^{n-s-1})(1-(1 \n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By defining $\\begin{array}{r}{f(\\boldsymbol{x}):=\\sum_{t=0}^{N-1}(1-(1-\\boldsymbol{x})^{N-t-1})(1-(1-\\boldsymbol{x})^{t})}\\end{array}$ and according to Lemma C.3 in [56], we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{variance\\;error}\\ge\\displaystyle\\frac{\\sigma^{2}}{N^{2}}\\sum_{i}f(\\eta\\lambda_{i})+\\frac{4\\widetilde{\\Gamma}^{2}f^{2}}{N^{2}}\\sum_{i}f(\\eta\\lambda_{i})\\lambda_{i}^{-1}}\\\\ &{\\qquad\\qquad\\ge\\displaystyle\\frac{\\sigma^{2}}{N^{2}}(\\frac{N k^{*}}{10}+\\frac{2N^{3}}{25}\\eta^{2}\\cdot\\sum_{i>k^{*}}\\lambda_{i}^{2})+\\frac{4\\widetilde{\\Gamma}^{2}f^{2}}{N^{2}}(\\frac{N}{10}\\sum_{i<k^{*}}\\lambda_{i}^{-1}+\\frac{2N^{3}}{25}\\eta^{2}\\cdot\\sum_{i>k^{*}}\\lambda_{i})}\\\\ &{\\qquad\\qquad\\ge\\displaystyle\\frac{\\sigma^{2}}{15N}(k^{*}+N^{2}\\eta^{2}\\cdot\\sum_{i>k^{*}}\\lambda_{i}^{2})+\\frac{4\\widetilde{\\Gamma}^{2}f^{2}}{15N}(\\sum_{i<k^{*}}\\lambda_{i}^{-1}+N^{2}\\eta^{2}\\cdot\\sum_{i>k^{*}}\\lambda_{i})}\\\\ &{\\qquad\\qquad\\gtrsim\\displaystyle\\frac{\\sigma^{2}}{N}(k^{*}+N^{2}\\eta^{2}\\cdot\\sum_{i>k^{*}}\\lambda_{i}^{2})+\\frac{\\widetilde{\\Gamma}^{2}f^{2}}{N}(\\sum_{i<k^{*}}\\lambda_{i}^{-1}+N^{2}\\eta^{2}\\cdot\\sum_{i>k^{*}}\\lambda_{i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Upper and Lower bound for bias error ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "According to Theorem B.5 in [49], the upper and lower bounds on the bias error can be directly derived: ", "page_idx": 23}, {"type": "text", "text": "Lemma D.6. Suppose Assumption 3.6 holds and the step size satisfies $\\eta\\leq1/\\lambda_{1}$ , then it holds that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{b i a s\\lesssim\\displaystyle\\frac{1}{\\eta^{2}N^{2}}\\cdot\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{0;k^{*}}^{-1}}^{2}+\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{k^{*};\\infty}^{2}}^{2}}\\\\ &{\\qquad\\qquad+\\,\\frac{\\alpha(\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{I}_{0;k^{*}}^{2}}^{2}+N\\eta\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{k^{*};\\infty}^{2}}^{2})}{N\\eta}\\cdot(\\displaystyle\\frac{k^{*}}{N}+N\\eta^{2}\\sum_{i>k^{*}}\\lambda_{i}^{2})}\\\\ &{b i a s\\lesssim\\!\\frac{1}{\\eta^{2}N^{2}}\\cdot\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{0;k^{*}}^{-1}}^{2}+\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{k^{*};\\infty}^{2}}^{2}}\\\\ &{\\qquad\\qquad+\\,\\frac{\\beta(\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{I}_{0;k^{*}}^{2}}^{2}+N\\eta\\|\\mathbf{w}_{0}-\\mathbf{w}_{*}\\|_{\\mathbf{H}_{k^{*};\\infty}^{2}}^{2})}{N\\eta}\\cdot(\\displaystyle\\frac{k^{*}}{N}+N\\eta^{2}\\sum_{i>k^{*}}\\lambda_{i}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By integrating the aforementioned results, we thus complete the proof. ", "page_idx": 23}, {"type": "text", "text": "E DP-TAGLMtron ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Privacy Guarantee ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Lemma E.1 ([22]). Algorithm $^{4}$ guaran\u221atees $(\\tau,\\frac{\\tau\\lceil\\mathrm{g}(N{+}1)\\rceil}{2\\sigma^{2}})$ -R\u00e9nyi differential privacy, where $N$ is the number of samples. Setting \u03c3 = 2\u2308lg(N+\u03b51)\u2309ln(1/\u03b4), one can guarantee $(\\varepsilon,\\delta)$ -differential privacy, for $\\varepsilon\\le2\\ln(1/\\delta)$ . ", "page_idx": 24}, {"type": "text", "text": "To guarantee the privacy of Algorithm 3, we should first ensure that each step of DP-TAGLMtron is private. ", "page_idx": 24}, {"type": "text", "text": "We first consider $\\{l_{t}\\}_{t=0}^{N-1}$ . According to Lemma E.1, it ensures that DP-tree-aggregation step is   \n(\u03b5, \u03b4)-DP with f = 2\u2308lg(N+1)\u2309ln(1/\u03b4). \u03b5   \nMoreover, denoting $\\varepsilon^{\\prime}=\\varepsilon/\\sqrt{8N\\log(2/\\delta)}$ and $\\delta^{\\prime}=\\delta/(2N)$ , according to Lemma 2.3 in [24], we   \ncould know that $\\{s_{t}\\}_{t=0}^{N}$ is $(\\varepsilon^{\\prime},\\delta^{\\prime})$ -DP. ", "page_idx": 24}, {"type": "text", "text": "Combining with the advanced composition theorem, it holds that Algorithm 3 is $(\\varepsilon,\\delta)$ -DP. ", "page_idx": 24}, {"type": "text", "text": "E.2 Utility Guarantee ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Here we introduce several lemmas for future proof. ", "page_idx": 24}, {"type": "text", "text": "Lemma E.2. Given a sequence of Positive Semi-Definite $(P S D)$ matrices $\\mathbf{P}_{t},t=0\\,.\\,.\\,.\\,N,$ , where $\\begin{array}{r}{\\mathbf{P}_{t}=(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))\\circ\\bar{\\mathbf{P}}_{t-1}}\\end{array}$ and $\\mathbf{P}_{0}=\\mathbf{I}$ , i $\\begin{array}{r}{f\\eta\\leq1/(4\\beta\\,\\mathrm{tr}(\\mathbf{H})\\log N)}\\end{array}$ , then for any $t\\in[0,N]$ , the following inequality holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{tr}(\\mathbf{H}\\mathbf{P}_{t})\\leq2\\cdot\\langle\\frac{1}{\\eta N}\\mathbf{I}_{0:k^{*}}+\\mathbf{H}_{k^{*}:\\infty},\\mathbf{I}\\rangle,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $k^{*}\\in[N]$ can be arbitrarily chosen. ", "page_idx": 24}, {"type": "text", "text": "Lemma E.3. Under Assumption $3.4(A)_{\\cdot}$ , for every $\\mathbf A\\succeq\\mathbf0$ , it holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n(M-\\widetilde{\\mathcal{M}})\\circ\\mathcal{T}^{-1}\\circ\\mathbf{A}\\preceq\\mathcal{M}\\circ\\mathcal{T}^{-1}\\circ\\mathbf{A}\\preceq\\frac{2\\alpha\\operatorname{tr}(\\mathbf{A})}{1-\\alpha\\eta\\operatorname{tr}(\\mathbf{H})}\\cdot\\mathbf{H}\\preceq2\\alpha\\operatorname{tr}(\\mathbf{A})\\mathbf{H}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma E.4 (Generic bounds on the GLM-tron iterates [49]). Suppose that Assumption 3.6 holds. Consider the GLM-tron algorithm, we have the following recursion: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\mathbf{A}_{t}\\underset{\\eta^{2}\\sigma^{2}\\mathbf{H}}{\\mathbf{A}_{t-1}}-\\frac{\\eta}{2}(\\mathbf{H}\\mathbf{A}_{t-1}+\\mathbf{A}_{t-1}\\mathbf{H})+\\eta^{2}\\mathcal{M}\\circ\\mathbf{A}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}:=(\\mathcal{Z}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))\\circ\\mathbf{A}_{t-1}+\\mathbf{A}_{t-1}}\\\\ &{\\quad\\eta^{2}\\sigma^{2}\\mathbf{H}}\\\\ &{\\bullet\\ \\mathbf{A}_{t}\\succeq\\mathbf{A}_{t-1}-\\frac{\\eta}{2}(\\mathbf{H}\\mathbf{A}_{t-1}+\\mathbf{A}_{t-1}\\mathbf{H})+\\frac{\\eta^{2}}{4}\\mathcal{M}\\circ\\mathbf{A}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}:=(\\mathcal{Z}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(\\frac{\\eta}{2}))\\circ\\mathbf{A}_{t-1}+\\mathbf{A}_{t-1}}\\\\ &{\\quad\\eta^{2}\\sigma^{2}\\mathbf{H}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathbf{A}_{t}:=\\mathbb{E}(\\mathbf{w}_{t}-\\mathbf{w}_{*})(\\mathbf{w}_{t}-\\mathbf{w}_{*})^{\\top},\\,t\\geq0.}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Lemma E.5 (Generic bounds on the DP-TAGLMtron iterates). Suppose that Assumptions 3.6 (A) hold. Consider the $D P$ -TAGLMtron algorithm, we have the following recursion: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}_{t}\\preceq({\\mathcal{Z}}-\\frac{\\eta}{2}\\cdot{\\mathcal{T}}(2\\eta))\\circ\\mathbf{A}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+\\eta^{2}f^{2}(\\lceil\\log_{2}N\\rceil+1)\\mathbb{E}[\\gamma^{4}(t)+\\gamma^{2}(t)]\\mathbf{I}.}\\\\ &{\\ \\ \\ \\ \\ +\\eta^{2}f^{2}(\\lceil\\log_{2}N\\rceil+1)(\\sum_{j\\in{\\mathcal{Q}}(t)}({\\mathcal{T}}-\\frac{\\eta}{2}\\cdot\\tilde{\\mathcal{T}}(2\\eta))^{p}\\circ\\mathbf{I}+4\\eta\\alpha|{\\mathcal{Q}}(t)|\\operatorname{tr}(\\mathbf{H})(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N}))}\\\\ &{\\mathbf{A}_{t}\\succeq\\mathbf{A}_{t-1}-\\frac{\\eta}{2}(\\mathbf{H}\\mathbf{A}_{t-1}+\\mathbf{A}_{t-1}\\mathbf{H})+\\frac{\\eta^{2}}{4}{\\mathcal{M}}\\circ\\mathbf{A}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+4\\eta^{2}\\widetilde{\\Gamma}^{2}f^{2}\\mathbf{I}}\\\\ &{\\,\\,h e r e\\,\\mathbf{A}_{t}:=\\mathbb{E}(\\mathbf{w}_{t}-\\mathbf{w}_{\\ast})(\\mathbf{w}_{t}-\\mathbf{w}_{\\ast})^{\\top},t\\succeq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Lemma E.5. According to the update rule of DP-TAGLMtron, we know: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}=\\mathbf{w}_{0}-\\eta l_{\\le t}^{\\prime},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $l_{\\le t}^{\\prime}$ is the private estimation of $\\textstyle\\sum_{i=0}^{t}l_{i}$ generated by tree aggregation protocol. From [22], we notice $\\begin{array}{r}{l_{\\le t}^{\\prime}=\\sum_{i=0}^{t}l_{i}+\\mathbf{v}_{t}}\\end{array}$ , where $l_{i}=(\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t}))$ ) and $\\mathbf{v}_{t}$ is a combination of at most $O(\\lceil\\log_{2}t\\rceil)$ ) Gaussian noise. If we set $\\mathbf{w}_{0}=0$ , then it holds that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{w}_{t+1}=\\mathbf{w}_{0}-\\eta\\cdot(\\displaystyle\\sum_{i=0}^{t}l_{i}+\\mathbf{v}_{t})}\\\\ &{\\qquad=\\mathbf{w}_{0}-\\eta\\cdot\\displaystyle\\sum_{i=0}^{t}(\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t}))-\\eta\\cdot\\mathbf{v}_{t}}\\\\ &{\\qquad=\\mathbf{w}_{t}-\\eta(\\mathbf{x}_{t}(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t})-y_{t})+\\mathbf{v}_{t}-\\mathbf{v}_{t-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By denoting $\\mathbf{T}_{t}=\\mathbf{v}_{t}-\\mathbf{v}_{t-1}$ and noticing that $\\mathbf{v}_{t}$ is a combination of at most $O(\\lceil\\log_{2}t\\rceil)$ ) Gaussian noise, it implies that $\\mathbf{\\Gamma}_{t}$ is a combination of at most $\\lceil\\log_{2}t+1\\rceil$ random Gaussian vectors, i.e. $\\begin{array}{r}{\\bar{\\mathbf{T}}_{t}=\\sum_{q\\in\\dot{\\mathcal{Q}}(t)}\\dot{\\gamma}(q)\\Gamma f^{2}\\mathbf{g}_{q}}\\end{array}$ , where $\\mathcal{Q}(t)$ is an index set and $\\breve{\\mathscr Q}^{\\breve{(}t)}=\\{{2^{Q-1},2^{Q-1}\\!+\\!2^{Q-2},...\\,,2^{Q-1}}+}$ $2^{Q-2}+\\cdots+2^{0}\\}$ when $t=2^{Q}$ with $Q\\in\\mathbb{N}$ and $\\gamma(q)=\\operatorname*{max}\\{\\gamma_{1},\\ldots,\\gamma_{q}\\}$ . Moreover, we know $\\mathbf{g}_{q}\\sim\\mathcal{N}(0,\\mathbf{I}_{d\\times d})$ , indicating: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{w}_{t}-\\mathbf{w}_{*}=(\\mathbf{I}-\\eta\\mathbb{I}[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0]\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})\\big(\\mathbf{w}_{t-1}-\\mathbf{w}_{*}\\big)}\\\\ &{\\qquad\\qquad+\\eta\\big(\\mathbb{I}[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{*}>0]-\\mathbb{I}[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0]\\big)\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{*}+\\eta z_{t}\\mathbf{x}_{t}-\\eta\\mathbf{I}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Let us consider the expected outer product: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{A}_{t}=\\mathbb{E}[\\|\\mathbf{v}_{t}-\\mathbf{v}_{*}\\|_{2}^{2}]=\\mathbb{E}[(\\mathbf{w}_{t}-\\mathbf{w}_{*})\\big(\\mathbf{w}_{t}-\\mathbf{w}_{*}\\big)^{\\top}]}\\\\ {=}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "It can be observed that quadratic terms 1,2,3 and crossing term 1 are original terms for non-private problems, where the remaining are introduced by the private noise. ", "page_idx": 25}, {"type": "text", "text": "According to Assumption 3.6, Lemma C.2 and the independence of $\\mathbf{w}_{*}$ , we have: ", "page_idx": 25}, {"type": "text", "text": "(crossing term 3) $=0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{crossing\\;term\\;2})\\;=\\mathbb{E}[(\\mathbf{I}-\\eta\\mathbb{I}[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0]\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})\\mathbf{w}_{t}\\mathbf{I}_{t}^{\\top}]+\\mathbb{E}[\\mathbf{T}_{t}\\mathbf{w}_{t}^{\\top}(\\mathbf{I}-\\mathbb{I}[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0]\\eta\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now we examine the crossing term 2, denoting $\\mathbf{R}_{t}:=(\\mathbf{I}-\\eta\\mathbb{1}[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0]\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})$ . According to the update rule, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{w}_{t}=\\mathbf{w}_{t-1}-\\eta\\cdot(\\mathrm{ReLU}(\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1})-y_{t})\\mathbf{x}_{t}+\\mathbf{T}_{t}}\\\\ &{\\mathrm{\\quad}=(\\mathbf{I}-\\eta\\mathbb{I}\\left[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0\\right]\\cdot\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})\\mathbf{w}_{t-1}+\\eta\\mathbf{x}_{t}y_{t}+\\mathbf{T}_{t}}\\\\ &{\\mathrm{\\quad}=\\displaystyle\\sum_{j=1}^{t-1}(\\mathbf{R}_{t-1}\\mathbf{R}_{t-2}\\cdot\\cdot\\cdot\\mathbf{R}_{j}(\\eta\\mathbf{x}_{j-1}y_{j-1}+\\eta\\mathbf{T}_{j-1}))+\\eta\\mathbf{x}_{t-1}y_{t-1}+\\eta\\mathbf{P}_{t-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t}\\mathbf{P}_{t}^{\\top}=\\sum_{j=1}^{t-1}(\\mathbf{R}_{t-1}\\mathbf{R}_{t-2}\\cdot\\cdot\\cdot\\mathbf{R}_{j}(\\eta\\mathbf{x}_{j-1}y_{j-1}+\\eta\\mathbf{P}_{j-1}))\\mathbf{P}_{t}^{\\top}+\\eta\\mathbf{x}_{t-1}\\mathbf{P}_{t}^{\\top}y_{t-1}+\\eta\\mathbf{P}_{t-1}\\mathbf{P}_{t}^{\\top}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By multiplying both sides by A and taking the expected value, we can obtain: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathfrak{L}[(\\mathbf{I}-\\eta\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})\\mathbf{w}_{t}\\mathbf{I}_{t}^{\\top}]=\\mathbb{E}[\\eta\\mathbf{R}_{t}\\sum_{j=1}^{t-1}(\\mathbf{R}_{t-1}\\mathbf{R}_{t-2}\\cdots\\mathbf{R}_{j}\\mathbf{r}_{j-1})\\mathbf{P}_{t}^{\\top}]+\\mathbb{E}[\\eta\\mathbf{R}_{t}\\mathbf{x}_{t-1}y_{t-1}\\mathbf{P}_{t}^{\\top}]+\\mathbb{E}[\\eta\\mathbf{R}_{t}\\mathbf{P}_{t}\\mathbf{x}_{t-1}y_{t}]\\mathbf{P}_{t}^{\\top}]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We first consider the second term, notice that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\eta\\mathbf{R}_{t}\\mathbf{x}_{t-1}y_{t-1}\\mathbf{P}_{t}^{\\top}]=\\mathbb{E}[\\eta\\mathbf{R}_{t}(\\mathbb{1}[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{*}>0]\\cdot\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{*}+z_{t}\\mathbf{x}_{t})\\mathbf{I}_{t}^{\\top}]=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore, considering $\\mathbf{\\boldsymbol{\\Gamma}}_{t}$ and $\\Gamma_{j}$ , when $j~<t$ and $j\\notin\\;\\mathcal{Q}(t)$ , it follows that $\\mathbb{E}[\\mathbf{r}_{j}\\mathbf{r}_{t}^{\\top}]\\,=\\,0$ , otherwise, we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\Gamma_{j}\\mathbf{{P}}_{t}^{\\top}]=\\big(\\sum_{q\\in\\mathcal{Q}(j-1)}\\gamma\\big(q\\big)f^{2}\\mathbf{g}_{q}\\big)\\big(\\sum_{q^{\\prime}\\in\\mathcal{Q}(t)}\\gamma\\big(q^{\\prime}\\big)f^{2}\\mathbf{g}_{q^{\\prime}}\\big)^{\\top}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "These results arise because only $\\Gamma_{q}$ is dependent on $\\mathbf{g}_{q}$ for $q\\,\\in\\,\\mathcal{Q}(t)$ . Hence, denoting $\\mathbf{R}_{[t:j]}=$ $\\mathbf{R}_{t}\\cdot\\cdot\\mathbf{\\nabla}\\cdot\\mathbf{R}_{j}$ , we have: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\eta\\mathbf{R}_{t}\\sum_{j=1}^{t-1}(\\mathbf{R}_{t-1}\\mathbf{R}_{t-2}\\cdots\\mathbf{R}_{j}\\mathbf{r}_{j-1})\\mathbf{F}_{t}^{\\top}]}\\\\ &{=\\eta\\mathbb{E}[\\mathbf{R}_{t}\\sum_{j\\in\\mathcal{Q}(t)}(\\mathbf{R}_{t-1}\\mathbf{R}_{t-2}\\cdots\\mathbf{R}_{j}\\mathbf{r}_{j-1})\\mathbf{F}_{t}^{\\top}]}\\\\ &{=\\eta\\mathbb{E}[\\sum_{j\\in\\mathcal{Q}(t)}\\mathbf{R}_{[t;j]}(\\sum_{\\ell\\in\\mathcal{Q}(j-1)}\\gamma(q)f^{2}\\mathbf{g}_{\\ell})(\\sum_{\\ell^{\\prime}\\in\\mathcal{Q}(t)}\\gamma(q^{\\prime})f^{2}\\mathbf{g}_{\\ell^{\\prime}})^{\\top}]}\\\\ &{=\\eta\\mathbb{E}[\\sum_{j\\in\\mathcal{Q}(t)}\\mathbf{R}_{[t;j]}(\\sum_{\\ell\\in\\mathcal{Q}(t)\\cap\\mathcal{Q}(t)}\\gamma^{2}(\\hat{q})f^{2}\\mathbf{g}_{\\ell}\\mathbf{g}_{\\ell}^{\\top}+\\sum_{q\\in\\mathcal{I}}\\gamma(q^{\\prime})\\gamma(q)f^{2}\\mathbf{g}_{\\ell}\\mathbf{g}_{\\ell^{\\prime}}^{\\top})]}\\\\ &{=\\eta f^{2}\\!\\!\\!\\!\\!\\sum_{j\\in\\mathcal{Q}(t)}\\!\\!\\!\\!\\!\\!\\sum_{\\ell\\in\\mathcal{Q}(t)\\cap\\mathcal{Q}(t)}\\!\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell}\\!\\!\\!\\!\\!\\!\\sum_{\\ell}\\!\\!\\!\\!\\!\\!\\sum_{j}\\!\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell^{\\prime}}\\!\\!\\!\\!\\!\\!\\sum_{\\ell^\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the last term of crossing term 3, we could also derive: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\eta\\mathbf{R}_{t}\\mathbf{r}_{t-1}\\mathbf{P}_{t}^{\\top}]=\\sum_{\\widetilde{q}\\in\\mathcal{Q}(t-1)\\cap\\mathcal{Q}(t)}\\mathbb{E}[\\gamma^{2}(\\widetilde{q})f^{2}\\mathbf{R}_{t}].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Combining the above results with the symmetric of ${\\bf R}_{t}$ , it implies that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle-(\\mathrm{crossing\\;term\\;2})\\;=-\\eta f^{2}\\sum_{j\\in{\\mathcal{Q}}(t)}\\sum_{{\\widetilde{q}}\\in Q(j-1)\\cap Q(t)}\\mathbb{E}[\\gamma^{2}({\\widetilde{q}})\\mathbf{R}_{[t:j]}+\\gamma^{2}({\\widetilde{q}})\\mathbf{R}_{[j:t]}]}&{}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\preceq\\eta f^{2}(\\lceil\\log_{2}N\\rceil+1)\\sum_{j\\in{\\mathcal{Q}}(t)}\\mathbb{E}[\\mathbf{R}_{t}\\ldots\\mathbf{R}_{j}\\mathbf{R}_{j}\\ldots\\mathbf{R}_{t}]+\\eta^{2}f^{2}(\\lceil\\log_{2}N\\rceil+1)\\mathbb{E}[\\gamma^{4}(\\log_{2}\\Delta_{t})\\biggr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Recall $\\mathbf{R}_{t}:=(\\mathbf{I}-\\eta\\mathbb{1}[\\mathbf{x}_{t}^{\\top}\\mathbf{w}_{t-1}>0]\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})$ and the lemma in [49]. ", "page_idx": 26}, {"type": "text", "text": "It indicates that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\mathbf{R}_{t}\\cdot\\cdot\\cdot\\mathbf{R}_{j}\\mathbf{R}_{j}\\cdot\\cdot\\cdot\\mathbf{R}_{t}]\\preceq({\\mathbb{Z}}-{\\frac{\\eta}{2}}\\cdot{\\mathcal{T}}(2\\eta))^{t-j+1}\\circ\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We first consider $\\begin{array}{r}{\\mathbf P_{p}=(\\mathcal T-\\frac{\\eta}{2}\\cdot\\mathcal T(2\\eta))\\circ\\mathbf P_{p-1}}\\end{array}$ and $\\mathbf{P}_{0}=\\mathbf{I}$ . Then, it follows that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbf{P}_{p}\\preceq({\\mathcal{T}}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))\\circ\\mathbf{P}_{p-1}+\\eta^{2}({\\mathcal{M}}-\\widetilde{\\mathcal{M}})\\circ\\mathbf{P}_{p-1}}\\\\ {\\preceq({\\mathcal{T}}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))\\circ\\mathbf{P}_{p-1}+\\eta^{2}\\alpha\\operatorname{tr}(\\mathbf{H}\\mathbf{P}_{p-1})\\mathbf{H}}\\\\ {=({\\mathcal{T}}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))^{p}\\circ\\mathbf{I}+2\\eta^{2}\\alpha\\operatorname{tr}(\\mathbf{H})\\sum_{i=0}^{p}({\\mathcal{T}}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))^{i}\\mathbf{H}}\\\\ {\\preceq({\\mathcal{T}}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))^{p}\\circ\\mathbf{I}+4\\eta\\alpha\\operatorname{tr}(\\mathbf{H})(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the term $\\mathbf{H}\\mathbf{P}_{p-1}$ can be bounded by Lemma E.2. Now, we could derive: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{\\in\\mathcal{Q}(t)}\\mathbb{E}[\\mathbf{R}_{t}\\cdot\\cdot\\cdot\\mathbf{R}_{j}\\mathbf{R}_{j}\\cdot\\cdot\\cdot\\mathbf{R}_{t}]\\preceq\\sum_{j\\in\\mathcal{Q}(t)}(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))^{p}\\circ\\mathbf{I}+4\\eta\\alpha|\\mathcal{Q}(t)|\\operatorname{tr}(\\mathbf{H})(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As a results, the generic bounds on the DP-TAGLMtron iterate could have the following recursion: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}_{t}\\preceq({\\mathbb{Z}}-\\frac{\\eta}{2}\\cdot{\\mathcal{T}}(2\\eta))\\circ\\mathbf{A}_{t-1}+\\eta^{2}\\sigma^{2}\\mathbf{H}+\\eta^{2}f^{2}(\\lceil\\log_{2}N\\rceil+1){\\mathbb{E}}[\\gamma^{4}(t)+\\gamma^{2}(t)]\\mathbf{I}.}\\\\ &{\\quad\\,+\\,\\eta^{2}f^{2}(\\lceil\\log_{2}N\\rceil+1)\\big(\\sum_{j\\in{\\mathcal{Q}}(t)}({\\mathbb{Z}}-\\frac{\\eta}{2}\\cdot\\widetilde{{\\mathcal{T}}}(2\\eta))^{p}\\circ\\mathbf{I}+4\\eta\\alpha|{\\mathcal{Q}}(t)|\\operatorname{tr}(\\mathbf{H})(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now move on to the second part. In light of the $\\mathbf{\\Gamma}_{t}$ definition, we know it is at least a random Gaussian vector such that $\\mathbf{T}_{t}=\\widetilde{\\Gamma}\\mathbf{g}_{t}$ , where $\\widetilde\\Gamma=\\operatorname*{min}\\gamma$ . Moreover, it holds that ", "page_idx": 27}, {"type": "text", "text": "$\\mathfrak{L}[(\\mathbf{I}-\\eta\\mathbf{x}_{t}\\mathbf{x}_{t}^{\\top})\\mathbf{w}_{t}\\mathbf{I}_{t}^{\\top}]=\\mathbb{E}[\\eta\\mathbf{R}_{t}\\sum_{j=1}^{t-1}(\\mathbf{R}_{t-1}\\mathbf{R}_{t-2}\\cdots\\mathbf{R}_{j}\\mathbf{r}_{j-1})\\mathbf{P}_{t}^{\\top}]+\\mathbb{E}[\\eta\\mathbf{R}_{t}\\mathbf{x}_{t-1}y_{t-1}\\mathbf{P}_{t}^{\\top}]+\\mathbb{E}[\\eta\\mathbf{R}_{t}\\mathbf{P}_{t}\\mathbf{r}_{t-1}y_{t}].$ \u22121\u0393t\u22a4 ] = 0. Since $\\mathbb{E}[\\mathbf{r}_{j}\\mathbf{r}_{t}^{\\top}]\\,=\\,0$ for any $j\\neq t$ . Therefore, the iteration of ${\\bf A}_{t}$ will be the same case in DPGLMtron. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "Consequently, analogous to the previous case, we can decompose ${\\bf A}_{t}$ into bias term $\\mathbf{B}_{t}$ and variance term $\\mathbf{C}_{t}$ . ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf B}_{t}=(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))\\circ{\\bf B}_{t-1},\\quad{\\bf B}_{0}=({\\bf w}_{0}-{\\bf w}^{*})({\\bf w}_{0}-{\\bf w}^{*})^{\\top};}\\\\ &{{\\bf C}_{t}=(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))\\circ{\\bf C}_{t-1}+\\eta^{2}{\\bf M}_{t},\\quad{\\bf C}_{0}={\\bf0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{M}_{t}\\preceq\\sigma^{2}\\mathbf{H}+f^{2}(\\lceil\\log_{2}N\\rceil+1)\\mathbb{E}[\\gamma^{4}(t)+\\gamma^{2}(t)]\\mathbf{I}}\\\\ &{\\quad\\quad+\\ f^{2}(\\lceil\\log_{2}N\\rceil+1)(\\sum_{j\\in\\mathcal{Q}(t)}(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\tilde{\\mathcal{T}}(2\\eta))^{p}\\circ\\mathbf{I}+4\\eta\\alpha\\lvert\\mathcal{Q}(t)\\rvert\\operatorname{tr}(\\mathbf{H})(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma E.6. Let M represent a union upper bound for $\\mathbf{M}_{0},\\dots,\\mathbf{M}_{N-1}$ . Under Assumptions (A) and 3.6, and provided that the step size $\\eta$ satisfies $\\eta\\lesssim1/(\\mathrm{tr}(\\mathbf{H})\\log N)$ , the following holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathsf{\\bar{\\Delta}}_{0}\\preceq\\cdots\\preceq\\mathbf{C}_{N}\\preceq\\frac{8\\alpha\\eta^{2}}{1-\\alpha\\eta\\operatorname{tr}(\\mathbf{H})}\\cdot\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{M}\\rangle\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})+\\eta^{2}\\cdot\\sum_{t=0}^{N-1}(\\mathbb{Z}-\\frac{\\eta}{2}\\mathbf{\\tilde{\\mathcal{T}}}(2\\eta))^{t}\\circ\\mathbf{M}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Lemma E.6. Our proofs follow the main idea of [48], we include them here for completeness. According to the update rule of $\\mathbf{C}_{t}$ , it follows that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{C}_{t}=\\eta^{2}\\sum_{t=0}^{N-1}(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))\\circ\\mathbf{M}_{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "It can be observed that $\\mathbf{C}_{0}\\preceq\\mathbf{C}_{1}\\preceq\\cdots\\preceq\\mathbf{C}_{N}$ since $\\begin{array}{r}{(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))}\\end{array}$ is a PSD mapping. ", "page_idx": 27}, {"type": "text", "text": "Let $\\mathbf{M}$ be an union upper bound of $\\{\\mathbf{M}_{0},\\dots,\\mathbf{M}_{N-1}\\}$ , we will have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\Sigma_{N}\\preceq\\eta^{2}\\sum_{t=0}^{N-1}(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))\\circ\\mathbf{M}=2\\eta\\cdot\\mathcal{T}^{-1}\\circ(\\mathbb{Z}-(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))^{N})\\circ\\mathbf{M}\\preceq2\\eta\\cdot\\mathcal{T}^{-1}\\circ(\\mathbb{Z}-(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))^{N})\\circ\\mathbf{M}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The last inequality comes from $(\\widetilde{T}-\\widetilde{T}),(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\mathcal{T}(2\\eta))$ and $\\begin{array}{r}{(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\widetilde{\\mathcal{T}}(2\\eta))}\\end{array}$ are PSD mappings. Hence, it implies that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Sigma}_{t+1}=({\\cal Z}-\\frac{\\eta}{2}\\cdot{\\cal T}(2\\eta))\\circ{\\bf C}_{t}+\\eta^{2}\\cdot({\\cal M}-\\widetilde{{\\cal M}})\\circ{\\bf C}_{t}+\\eta^{2}\\mathbf{M}_{t}}\\\\ &{\\qquad\\preceq\\left({\\cal Z}-\\frac{\\eta}{2}\\cdot{\\cal T}(2\\eta)\\right)\\circ{\\bf C}_{t}+\\eta^{2}\\cdot({\\cal M}-\\widetilde{{\\cal M}})\\circ{\\bf C}_{N}+\\eta^{2}\\mathbf{M}}\\\\ &{\\qquad\\preceq\\left({\\cal Z}-\\frac{\\eta}{2}\\cdot{\\cal T}(2\\eta)\\right)\\circ{\\bf C}_{t}+\\eta^{2}\\cdot({\\cal M}-\\widetilde{{\\cal M}})\\circ2\\eta\\cdot{\\mathcal T}^{-1}\\circ\\left({\\cal Z}-({\\cal Z}-\\frac{\\eta}{2}\\cdot\\widetilde{{\\cal T}}(2\\eta))^{N}\\right)\\circ\\mathbf{M}+\\eta^{2}\\mathbf{M}}\\\\ &{\\qquad\\preceq\\left({\\cal Z}-\\frac{\\eta}{2}\\cdot{\\cal T}(2\\eta)\\right)\\circ{\\bf C}_{t}+\\frac{4\\alpha\\eta^{3}}{1-\\alpha\\eta\\,\\mathrm{tr}(\\mathbf{H})}\\cdot\\left\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{M}\\right\\rangle\\cdot\\mathbf{H}+\\eta^{2}\\mathbf{M},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality comes from Lemma E.3. Therefore, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\preceq\\displaystyle\\frac{4\\alpha\\eta^{3}}{1-\\alpha\\eta\\operatorname{tr}(\\mathbf{H})}\\cdot\\left\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{M}\\right\\rangle\\cdot\\sum_{t=0}^{N-1}(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\widetilde{T}(2\\eta))^{N-1-t}\\circ\\mathbf{H}+\\eta^{2}\\cdot\\sum_{t=0}^{N-1}(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\widetilde{T}(2\\eta))^{T}}\\\\ &{=\\displaystyle\\frac{8\\alpha\\eta^{2}}{1-\\alpha\\eta\\operatorname{tr}(\\mathbf{H})}\\cdot\\left\\langle\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{M}\\right\\rangle\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})+\\eta^{2}\\cdot\\sum_{t=0}^{N-1}(\\mathbb{Z}-\\frac{\\eta}{2}\\cdot\\widetilde{T}(2\\eta))^{t}\\circ\\mathbf{M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Lemma E.7. Variance error] If the step size satisfies $\\eta\\lesssim1/(\\mathrm{tr}(\\mathbf{H})\\log N).$ , then the following holds for any $k\\geq1$ , ", "page_idx": 28}, {"type": "text", "text": "variance error $\\lesssim\\frac{\\alpha\\eta}{N(1-\\alpha\\eta\\,\\mathrm{tr}(\\mathbf{H}))}\\cdot\\langle\\mathbf{I}_{0:k^{*}}+\\frac{N\\eta}{2}\\mathbf{H}_{k^{*}:\\infty},\\widetilde{\\mathbf{M}}\\rangle\\cdot(k^{*}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}\\lambda_{i}^{2})+\\frac{1}{N}\\cdot\\langle\\mathbf{H}_{0:k^{*}}^{-1}+\\frac{N^{2}\\eta^{2}}{4}\\mathbf{H}_{k^{*}:\\infty},\\widetilde{\\mathbf{M}}\\rangle.$ \u27e9, ", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\widetilde{\\mathbf{d}}=\\sigma^{2}\\mathbf{\\cdot}\\mathbf{H}+\\eta\\alpha f^{2}(\\log{2}\\,N)^{2}\\operatorname{tr}(\\mathbf{H})\\cdot(\\mathbf{I}_{0:k^{*}}+\\frac{N\\eta}{2}\\mathbf{H}_{k^{*}:\\infty})+f^{2}\\log_{2}{N}\\cdot(\\mathbb{E}[\\gamma(N)^{2}+\\gamma(N)^{4}]+1)\\cdot\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. According to Lemma D.5, we have: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{cotance~enor}\\:\\leq\\frac{16\\mathrm{or}}{N(1-\\alpha\\eta\\tau(\\mathbf{H}))}(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{C}_{N})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\frac{16\\mathrm{or}}{N(1-\\alpha\\eta\\tau(\\mathbf{H}))}\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{M})\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{\\eta}{N}\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\sum_{t=0}^{N-1}(\\mathcal{Z}-\\frac{\\eta}{2},\\tilde{\\mathcal{T}}(2\\eta))^{t}\\cdot(\\mathbf{M})}\\\\ &{\\quad\\quad\\quad=\\frac{16\\mathrm{or}}{N(1-\\alpha\\eta\\tau(\\mathbf{H}))}\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{M})\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})}\\\\ &{\\quad\\quad\\quad\\quad+\\frac{\\eta}{N}\\cdot(\\mathcal{L}-\\frac{\\eta}{2}\\cdot\\tilde{\\mathcal{T}}(2\\eta))^{t}\\circ(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N}),\\mathbf{M})}\\\\ &{\\quad\\quad\\quad=\\frac{16\\mathrm{or}}{N(1-\\alpha\\eta\\tau(\\mathbf{H}))}\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{M})\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\frac{2}{N}\\cdot((\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})^{T}\\mathbf{H}\\cdot(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N},\\mathbf{M}).}\\\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Furthermore, Lemma E.5 implies that, for any $t\\leq N$ : ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{M}_{t}\\preceq\\sigma^{2}\\mathbf{H}+f^{2}(\\lceil\\log_{2}N\\rceil+1)\\mathbb{E}[\\gamma^{4}(t)+\\gamma^{2}(t)]\\mathbf{I}}\\\\ &{\\quad\\quad+\\ f^{2}(\\lceil\\log_{2}N\\rceil+1)(\\sum_{j\\in\\mathcal{Q}(t)}(\\mathcal{T}-\\frac{\\eta}{2}\\cdot\\tilde{\\mathcal{T}}(2\\eta))^{p}\\circ\\mathbf{I}+4\\eta\\alpha\\lvert\\mathcal{Q}(t)\\rvert\\operatorname{tr}(\\mathbf{H})(\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now we consider an arbitrary PSD mapping A commuting with $\\mathbf{H}$ , it holds that: ", "page_idx": 29}, {"type": "text", "text": "\u27e8A, M $\\mathbf{\\langle}\\lesssim\\sigma^{2}\\langle\\mathbf{H},\\mathbf{A}\\rangle+f^{2}\\log_{2}N\\cdot\\mathbb{E}[\\gamma^{4}(t)+\\gamma^{2}(t)]\\operatorname{tr}(\\mathbf{A})+\\eta\\alpha f^{2}(\\log_{2}N)^{2}\\operatorname{tr}(\\mathbf{H})\\cdot\\langle\\mathbf{A},\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{\\Phi}\\mathbf{H})^{N}\\rangle$ . Moreover, noting that for any $x\\in(0,2/\\eta)$ , we have $\\begin{array}{r}{1-(1-\\frac{\\eta x}{2})^{N}\\le\\operatorname*{min}\\{1,N\\eta x/2\\}}\\end{array}$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbf{I}-(\\mathbf{I}-\\frac{\\eta}{2}\\mathbf{H})^{N}\\preceq\\mathbf{I}_{0:k^{*}}+\\frac{N\\eta}{2}\\mathbf{H}_{k^{*}:\\infty},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for any $k^{*}\\geq0$ . Therefore, we can define a new matrix as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n=\\sigma^{2}\\cdot\\mathbf{H}+\\eta\\alpha f^{2}(\\log_{2}N)^{2}\\operatorname{tr}(\\mathbf{H})\\cdot(\\mathbf{I}_{0:k^{*}}+{\\frac{N\\eta}{2}}\\mathbf{H}_{k^{*}:\\infty})+f^{2}\\log_{2}N\\cdot(\\mathbb{E}[\\gamma(N)^{2}+\\gamma(N)^{4}]+1)\\cdot\\mathbf{I}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combining the previous results, we obtain ", "page_idx": 29}, {"type": "text", "text": "variance ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{\\alpha\\eta}{\\nabla(1-\\alpha\\eta\\mathrm{tr}({\\bf H}))}\\cdot\\langle\\mathbf{I}_{0:k^{*}}+\\frac{N\\eta}{2}\\mathbf{H}_{k^{*}:\\infty},\\widetilde{\\mathbf{M}}\\rangle\\cdot(k^{*}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}\\lambda_{i}^{2})+\\frac{1}{N}\\cdot\\langle\\mathbf{H}_{0:k^{*}}^{-1}+\\frac{N^{2}\\eta^{2}}{4}\\mathbf{H}_{k^{*}:\\infty},\\widetilde{\\mathbf{M}}\\rangle},}}\\\\ {{\\displaystyle\\frac{\\gamma^{2}}{N}(1+\\alpha\\eta(\\sum_{i<k^{*}}\\lambda_{i}+\\frac{N\\eta}{2}\\sum_{i>k^{*}}\\lambda_{i}^{2}))\\cdot(k^{*}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}\\lambda_{i}^{2})}}\\\\ {{\\displaystyle\\frac{\\kappa^{2}\\log_{2}N}{N}\\cdot(\\alpha\\eta\\log_{2}N\\mathrm{tr}({\\bf H})+\\mathbb{E}[\\gamma(N)^{2}+\\gamma(N)^{4}])\\cdot(\\alpha\\eta(k^{*}+\\frac{N\\eta}{2}\\sum_{i>k^{*}}\\lambda_{i})+(\\sum_{i<k^{*}}\\lambda_{i}^{-1}+\\frac{N^{2}\\eta^{2}}{4}))}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Now, we introduce the lemma in [49] to complete the proof. ", "page_idx": 29}, {"type": "text", "text": "Lemma E.8 ([49]). If the step size satisfies $\\eta\\le1/\\lambda_{1}$ , then it holds that for any $k^{*}\\geq1$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{bias}\\lesssim\\displaystyle\\frac{1}{\\eta^{2}N^{2}}\\cdot\\|\\mathbf{w}_{0}-\\mathbf{w}^{*}\\|_{\\mathbf{H}_{0;k^{*}}^{-1}}^{2}+\\|\\mathbf{w}_{0}-\\mathbf{w}^{*}\\|_{\\mathbf{H}_{k^{*};\\infty}}^{2}}\\\\ {\\displaystyle\\quad\\qquad+\\,\\frac{\\alpha(\\|\\mathbf{w}_{0}-\\mathbf{w}^{*}\\|_{\\mathbf{I}_{0;k^{*}}^{2}}^{2}+N\\eta\\|\\mathbf{w}_{0}-\\mathbf{w}^{*}\\|_{\\mathbf{H}_{k^{*};\\infty}}^{2})}{N^{2}\\eta(1-\\alpha\\eta\\operatorname{tr}(\\mathbf{H}))}\\cdot(k^{*}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{i>k^{*}}\\lambda_{i}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "E.3 Specific Disrtibutions ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof of Corollary. 1. if $\\lambda_{k}=k^{-r}$ , by the definition of $k^{*}$ , we have $\\begin{array}{r}{k^{*}=(\\frac{N\\eta}{2})^{1/r}}\\end{array}$ , then: ", "page_idx": 29}, {"type": "equation", "text": "$$\nk^{*}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{k>k^{*}}\\lambda_{k}^{2}\\simeq k^{*}+\\frac{N\\eta}{2}\\sum_{k>k^{*}}\\lambda_{k}\\simeq(N\\eta)^{\\frac{1}{r}}+N\\eta\\cdot(N\\eta)^{\\frac{1-r}{r}}\\simeq(N\\eta)^{\\frac{1}{r}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the sake of simplicity, we assume $\\lVert\\mathbf{w}_{0}-\\mathbf{w}_{*}\\rVert^{2}\\leq W^{2}$ , then it hold that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Utility}\\lesssim\\frac{W^{2}}{N\\eta}+(\\frac{W^{2}}{N\\eta}+\\frac{W^{2}}{N\\eta}+\\frac{\\sigma^{2}}{N}+\\frac{\\Gamma^{2}f^{2}}{N^{2}})\\cdot(N\\eta)^{\\frac{1}{r}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "If we choose the step size as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\eta\\simeq\\operatorname*{min}\\{N^{-\\frac{1}{1+r}},N^{-\\frac{1+r}{1+2r}}f^{-\\frac{2r}{1+2r}}\\},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which indicates: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{Utility}\\lesssim(\\sigma+W^{2})\\cdot N^{-\\frac{r}{1+r}}+(W^{2}+\\Gamma^{2})\\cdot(f^{-2}N)^{-\\frac{r}{1+2r}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, it holds that Utilit $\\mathbf{y}=\\widetilde O(N^{-\\frac{r}{1+r}}\\cdot(1+(f^{2}N^{\\frac{r}{1+r}})^{\\frac{r}{1+2r}})).$ ", "page_idx": 29}, {"type": "text", "text": "2. If $\\lambda_{k}=e^{-k}$ , we have $k^{*}=\\log(N\\eta)$ , then: ", "page_idx": 30}, {"type": "equation", "text": "$$\nk^{\\ast}+\\frac{N^{2}\\eta^{2}}{4}\\sum_{k>k^{\\ast}}\\lambda_{k}^{2}\\simeq k^{\\ast}+N\\eta\\sum_{k>k^{\\ast}}\\lambda_{k}\\simeq\\log(N\\eta)+\\frac{N\\eta}{2}\\cdot(N\\eta)^{-1}\\simeq\\log(N\\eta).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{Utility}\\lesssim\\frac{W^{2}}{N\\eta}+(\\frac{W^{2}}{N}+\\frac{W^{2}}{N\\eta}+\\frac{\\sigma^{2}}{N}+\\frac{\\Gamma^{2}f^{2}}{N^{2}})\\cdot\\log(N\\eta)\\lesssim\\frac{W^{2}}{N\\eta}+\\frac{W^{2}}{N}+\\frac{\\sigma^{2}}{N}+\\frac{\\Gamma^{2}f^{2}}{N^{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "If we choose the following step size $\\begin{array}{r}{\\eta\\simeq\\operatorname*{min}\\{1,\\frac{1}{\\sqrt{N}f}\\}}\\end{array}$ , then we complete our results: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{Utility}\\simeq\\widetilde{O}(N^{-1}(1+(\\varepsilon^{-2}N)^{1/2})).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper\u2019s main contributions and scope are included in the abstract and introduction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The paper provides a discussion on the limitation and future direction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All assumptions and proofs are included in the submission. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: All experiments can be reproduced. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: All data and code will be released after acceptance. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the detailed experiment setup Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The factors that will influence the experiment are discussed. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The Experiments Compute Resources are discussed in the Appendix. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All related works are cited correctly and explicitly. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]