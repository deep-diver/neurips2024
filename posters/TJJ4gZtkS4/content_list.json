[{"type": "text", "text": "Tropical Expressivity of Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose an algebraic geometric framework to study the expressivity of linear   \n2 activation neural networks. A particular quantity that has been actively studied in   \n3 the field of deep learning is the number of linear regions, which gives an estimate   \n4 of the information capacity of the architecture. To study and evaluate information   \n5 capacity and expressivity, we work in the setting of tropical geometry\u2014a com  \n6 binatorial and polyhedral variant of algebraic geometry\u2014where there are known   \n7 connections between tropical rational maps and feedforward neural networks. Our   \n8 work builds on and expands this connection to capitalize on the rich theory of   \n9 tropical geometry to characterize and study various architectural aspects of neural   \n10 networks. Our contributions are threefold: we provide a novel tropical geometric   \n11 approach to selecting sampling domains among linear regions; an algebraic result   \n12 allowing for a guided restriction of the sampling domain for network architectures   \n13 with symmetries; and an open source library to analyze neural networks as tropical   \n14 Puiseux rational maps. We provide a comprehensive set of proof-of-concept nu  \n15 merical experiments demonstrating the breadth of neural network architectures to   \n16 which tropical geometric theory can be applied to reveal insights on expressivity   \n17 characteristics of a network. Our work provides the foundations for the adaptation   \n18 of both theory and existing software from computational tropical geometry and   \n19 symbolic computation to deep learning. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Deep learning has become the undisputed state-of-the-art for data analysis and has wide-reaching   \n22 prominence in many fields of computer science, despite still being based on a limited theoretical   \n23 foundation. Developing theoretical foundations to better understand the unparalleled success of deep   \n24 neural networks is one of the most active areas of research in modern statistical learning theory.   \n2 Expressivity is one of the most important approaches to quantifiably measuring the performance of a   \n26 deep neural network\u2014such as how they are able to represent highly complex information implicitly   \n27 in their weights and to generalize from data\u2014and therefore key to understanding the success of deep   \n28 learning.   \n29 Tropical geometry is a reinterpretation of algebraic geometry that features piecewise linear and   \n30 polyhedral constructions, where combinatorics naturally comes into play [e.g., 1, 2, 3]. These   \n31 characteristics of tropical geometry make it a natural framework for studying the linear regions in a   \n32 neural network\u2014an important quantity in deep learning representing the network information capacity   \n33 [4, 5, 6, 7, 8, 9, 10]. The intersection of deep learning theory and tropical geometry is a relatively   \n34 new area of research with great potential towards the ultimate goal of understanding how and why   \n35 deep neural networks perform so well. In this paper, we propose a new perspective for measuring   \n36 and estimating the expressivity and information capacity of a neural networks by developing and   \n37 expanding known connections between neural networks and tropical rational functions in both theory   \n38 and practice.   \n39 Related Work. Tropical geometry has been used to characterize deep neural networks with piece  \n40 wise linear activation functions, including two of the most popular and widely-used activation   \n41 functions, namely, rectified linear units (ReLUs) and maxout units. The first explicit connection   \n42 between tropical geometry and neural networks establishes that the decision boundary of a deep   \n43 neural network with ReLU activation functions is a tropical rational function [11]. Concurrently,   \n44 it was established that the maxout activation function fits input data by a tropical polynomial [12].   \n45 These works considered neural networks whose input domain is Euclidean, which was recently   \n46 developed to incorporate tropically-motivated input domains, in particular, the tropical projective   \n47 torus [13]. Most recently, tropical geometry has been used to construct convolutional neural networks   \n48 that are robust to adversarial attacks via tropical decision boundaries [14].   \n49 Contributions. In this paper, we establish novel algebraic and geometric tools to quantify the   \n50 expressivity of a neural network. Networks with a piecewise linear activation compute piecewise   \n51 linear functions where the input space is divided into areas; the network computing a single linear   \n52 function on each area. These areas are referred to as the linear regions of the network; the number   \n53 of distinct linear regions is a quantifiable measure of expressivity of the network [e.g., 5]. In our   \n54 work, we not only study the number of linear regions, we aim to understand their geometry. The main   \n55 contributions of our work are the following.   \n56 \u2022 We provide a geometric characterization of the linear regions in a neural network via the   \n57 input space: estimating the linear regions is typically carried out by random sampling from   \n58 the input space, where randomness may cause some linear regions of a neural network to be   \n59 missed and result in an inaccurate information capacity measure. We propose an effective   \n60 sampling domain as a ball of radius $R$ , which is a subset of the entire sampling space that   \n61 hits all of the linear regions of a given neural network. We compute bounds for the radius $R$   \n62 based on a combinatorial invariant known as the Hoffman constant, which effectively gives   \n63 a geometric characterization and guarantee for the linear regions of a neural network.   \n64 \u2022 We exploit geometric insight into the linear regions of a neural network to gain dramatic   \n65 computational efficiency: when networks exhibit invariance under symmetry, we can restrict   \n66 the sampling domain to a fundamental domain of the group action and thus reduce the   \n67 number of samples required. We experimentally demonstrate that sampling from the   \n68 fundamental domain provides an accurate estimate of the number of linear regions with a   \n69 fraction of the compute requirements.   \n70 \u2022 We provide an open source library integrated into the Open Source Computer Algebra   \n71 Research (OSCAR) system [15] which converts both trained and untrained arbitrary neural   \n72 networks into algebraic symbolic objects. This contribution then opens the door for the   \n73 extensive theory and existing software on symbolic computation and computational tropical   \n74 geometry to be used to study neural networks.   \n75 The remainder of this paper is organized as follows. We provide an overview of the technical   \n76 background on tropical geometry and its connection to neural networks in Section 2. We then devote   \n77 a section to each of the contributions listed above\u2014Sections 3, 4, and 5, respectively\u2014in which we   \n78 present our theoretical contributions and numerical experiments. We close the paper with a discussion   \n79 on limitations of our work and directions for future research in Section 6. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "80 2 Technical Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "81 In this section, we give basic definitions from tropical geometry required to write tropical expressions   \n82 for neural networks. ", "page_idx": 1}, {"type": "text", "text": "83 2.1 Tropical Polynomials ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "84 Algebraic geometry studies geometric properties of solution sets of polynomial systems that can   \n85 be expressed algebraically, such as their degree, dimension, and irreducible components. Tropical   \n86 geometry is a variant of algebraic geometry where the polynomials are defined in the tropical semiring,   \n87 $\\bar{\\mathbb{R}}=(\\mathbb{R}\\cup\\{\\infty\\},\\oplus,\\odot)$ where the addition and multiplication operators are given by $\\boldsymbol{a}\\oplus\\boldsymbol{b}=\\operatorname*{max}(\\boldsymbol{a},\\boldsymbol{b})$   \n88 and $a\\odot b=a+b$ , respectively. Define $a\\oslash b:=a-b$ .   \n89 Using these operations, we can write polynomials as $\\bigoplus_{m}a_{m}T^{m}$ , where $a_{i}$ are coefficients, $T\\in{\\bar{\\mathbb{R}}}$ ,   \n90 and where the sum is indexed by a finite subset of $\\mathbb{N}^{n}$ . In our work, we consider the following   \n91 generalizations of tropical polynomials.   \n92 Definition 2.1. A tropical Puiseux polynomial in the indeterminates $T_{1},\\ldots,T_{n}$ is a formal expression   \n93 of the form $\\bigoplus_{m}a_{m}T^{m}$ where the index $n$ runs through a finite subset of $\\mathbb{Q}_{\\geq0}^{m}$ and $T^{m}=T_{1}^{m_{1}}\\odot$   \n94 $\\cdots\\odot T_{n}^{m_{n}}$ ,  and taking powers in the tropical sense.   \n95 Definition 2.2. A tropical Puiseux rational map in $T_{1},\\mathrm{...}\\,,T_{n}$ is a tropical quotient of the form $p\\oslash q$   \n96 where $p,q$ are tropical Puiseux polynomials.   \n97 Tropical (Puiseux) polynomials and rational maps induce functions from $\\mathbb{R}^{n}\\to\\mathbb{R}$ , which take a point   \n98 $x\\in\\mathbb{R}^{n}$ to the number obtained by substituting $T=x$ in the algebraic expression and performing the   \n99 (tropical) operations. It is important to note that tropically, the formal algebraic expression contains   \n100 strictly more information than the corresponding function, since different tropical expressions can   \n101 induce the same function. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "102 2.2 Tropical Expressions for Neural Networks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "103 We now overview and recast the framework of [11], which establishes the first explicit connection   \n104 between tropical geometry and neural networks, in a slightly different language for our results.   \n105 As in [11], the neural networks we will focus on are fully connected multilayer perceptrons with ReLU   \n106 activation, i.e., functions $\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$ of the form $\\sigma\\circ L_{d}\\circ\\sigma\\circ L_{i-1}\\circ\\cdot\\cdot\\cdot\\circ L_{1}$ where $L_{i}:\\mathbb{R}^{n_{i-1}}\\rightarrow\\mathbb{R}^{n_{i}}$   \n107 is an affine map and $\\sigma(t)=\\operatorname*{max}\\{t,0\\}$ . For the remainder of this paper, we use the term \u201cneural   \n108 network\u201d to refer solely to these. We will always assume that the weights and biases of our neural   \nnetworks are rational numbers. From a computational perspective, this is not a serious restriction   \n110 since this is sufficient to describe any neural network with weights and biases given by floating point   \n111 numbers. We refer to the tuple $[n,n_{1},...,n_{d-1},m]$ as the architecture of the neural network.   \n112 One of the key observations intersecting tropical geometry and deep learning is that, up to rescaling   \n113 of rational weights to obtain integers, neural networks can be written as tropical rational functions   \n114 [11, Theorem 5.2]. From a more computational perspective, it is usually preferable to avoid such   \n115 rescaling and simply work with the original weights. The proof of Theorem 5.2 in [11] can directly   \n116 be adapted to show that any neural network can be written as the function associated to a tropical   \n117 Puiseux rational map. In their language, this corresponds to saying that any neural network is a   \n118 tropical rational signomial with nonnegative rational exponents. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "119 3 Sampling Domain Selection Using a Hoffman Constant ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 Estimating the number of linear regions of a neural network typically proceeds by sampling points   \n121 from the input domain and counting the memberships of these points. To guarantee that membership   \n122 is exhaustive, we seek a sampling domain as a sufficiently large ball so that all linear regions are   \n123 intersected. At the same time, we would like for the ball to be as small as possible to guarantee   \n124 efficient sampling. We are thus searching for the smallest ball from which we can sample in such a   \n125 way that all linear regions are intersected. Given the polyhedral geometry of tropical Puiseux rational   \n126 maps, it turns out that the radius of this smallest ball that we seek is closely related to the Hoffman   \n127 constant, which is a combinatorial invariant.   \n128 Our contribution in this section is a definition of a Hoffman constant of a neural network; we   \n129 demonstrate its relationship to the smallest sampling ball and propose algorithms to compute its true   \n130 value and lower and upper bounds. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "131 3.1 Defining a Neural Network Hoffman Constant ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "132 In simpler terms, the Hoffman constant can be expressed for a matrix as follows. Let $A$ be an $m\\times n$   \n133 matrix. For any $b\\in\\mathbb{R}^{m}$ , let $P(A,b)=\\{x\\in\\mathbb{R}^{\\bar{n}}:A x\\leq b\\}$ denote the polyhedron determined by   \n134 $A$ and $b$ . For a nonempty polyhedron $P(A,b)$ , let $d(u,P(\\mathcal{A},b))=\\operatorname*{min}\\{\\|\\bar{u}-x\\|:x\\in P(A,b)\\}$   \n135 denote the distance from a point $u\\in\\mathbb{R}^{n}$ to the polyhedron, measured under an arbitrary norm $\\|\\cdot\\|$ ", "page_idx": 2}, {"type": "text", "text": "136 on $\\mathbb{R}^{n}$ . Then there exists a constant $H(A)$ only depending on $A$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nd(u,P_{A,b})\\leq H(A)\\|(A u-b)_{+}\\|\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 where $x_{+}=\\operatorname*{max}\\{x,0\\}$ is applied coordinate-wise [16]. The constant $H(A)$ is called the Hoffman   \n138 constant of $A$ .   \n139 The Hoffman Constant for Tropical Polynomials and Rational Functions. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$   \n140 be a tropical Puiseux polynomial and let $\\mathcal{U}=\\{U_{1},\\d...,U_{m}\\}$ be the set of linear regions of $f$ . Let   \n141 $f(x)=a_{i1}x_{1}+\\ldots+a_{i n}x_{n}+b_{i}$ occur on the region $U_{i}$ . Further, let $A=[a_{i j}]_{m\\times n}$ be the matrix of   \n142 coefficients in the expression of $f$ over $\\boldsymbol{\\mathcal{U}}$ . The linear region $U_{i}$ is defined by the following inequalities ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\na_{i1}x_{1}+\\cdot\\cdot+a_{i n}x_{n}+b_{i}\\geq a_{j1}x_{1}+\\cdot\\cdot+a_{j n}x_{n}+b_{j},\\quad\\forall\\,j=1,2,\\cdot\\cdot\\cdot\\,,m.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "143 In matrix form, (2) is equivalent to ", "page_idx": 3}, {"type": "equation", "text": "$$\n(A-{\\bf1}a_{i})x\\leq b_{i}{\\bf1}-b\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "144 where 1 is a column vector of all 1\u2019s; $a_{i}$ is the ith row vector of $A$ ; and $b$ is a column vector of all   \n145 $b_{i}$ . Denote $\\widetilde{A}_{U_{i}}:=A-\\mathbf{1}a_{i}$ and $\\widetilde{b}_{U_{i}}:=b_{i}{\\bf1}-b$ . Then the linear region $U_{i}$ is captured by the linear   \n146 system of inequalities $\\widetilde{A}_{U_{i}}x\\le\\widetilde{b}_{U_{i}}$ .   \n147 Definition 3.1. Let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a tropical Puiseux polynomial. The Hoffman constant of $f$ is   \n148 defined as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nH(f)=\\operatorname*{max}_{U_{i}\\in{\\mathcal{U}}}H(\\widetilde{A}_{U_{i}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "149 Care needs to be taken in defining a Hoffman constant for a tropical Puiseux rational map: We want   \n150 to avoid having all linear regions defined by systems of linear inequalities, since there exist linear   \n151 regions which are not convex. To do so, we consider convex refinements of linear regions induced by   \n152 intersections of linear regions of tropical polynomials.   \n153 Definition 3.2. Let $p\\oslash q$ be a difference of two tropical Puiseux polynomials. Let $A_{p}$ (respectively   \n154 $A_{q.}$ ) be the $m_{p}\\times n$ (respectively $m_{q}\\times n)$ ) matrix of coefficients for $p$ (respectively $q$ ). The Hoffman   \n155 constant of $p\\oslash q$ is ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nH(p\\oslash q):=\\operatorname*{max}\\Bigg\\{H\\bigg(\\left[\\!\\!\\begin{array}{l}{A_{p}}\\\\ {A_{q}}\\end{array}\\!\\!\\right]-1\\left[\\!\\!\\begin{array}{l}{a_{i_{p}}}\\\\ {a_{i_{q}}}\\end{array}\\!\\!\\right]\\bigg):i_{p}=1,\\cdots,m_{p};\\ i_{q}=1,\\cdots,m_{q}\\Bigg\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "156 Let $f$ be a tropical Puiseux rational map. Then the Hoffman constant of $f$ is defined as the minimal   \n157 Hoffman constant of $H(p\\oslash q)$ over all possible expressions of $f=p\\oslash q$ .   \n158 Given the correspondence between neural networks and tropical Puiseux rational maps, the Hoffman   \n159 constant is well-defined for any neural network and may be computed from the geometry and   \n160 combinatorics of its linear regions. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "161 3.2 The Minimal Effective Radius ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "162 For a neural network whose tropical Puiseux rational map is $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ , let $\\mathcal{U}=\\{U_{1},\\dotsc,U_{m}\\}$ be   \n163 the collection of all linear regions. For any $x\\in\\mathbb{R}^{n}$ , define the minimal effective radius of $f$ at $x$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{f}(x):=\\operatorname*{min}\\{r:B(x,r)\\cap U_{i}\\neq\\emptyset,U_{i}\\in\\mathcal{U}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "164 where $B(x,r)$ is the ball of radius $r$ centered at $x$ . That is, $R_{f}(x)$ is the minimal radius such that the   \n165 ball $B(x,r)$ intersects all linear regions. It is the smallest required radius of sampling around $x$ in   \n166 order to express the full classifying capacity of the neural network $f$ .   \n167 We start with the following lemma which relates the minimal effective radius to the Hoffman constant   \n168 when $f$ is a tropical Puiseux polynomial. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "169 Lemma 3.3. Let $f$ be a tropical Puiseux polynomial and $x\\in\\mathbb{R}^{n}$ be any point, then ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{f}(x)\\leq H(f)\\operatorname*{max}_{U_{i}\\in\\mathcal{U}}\\|(\\widetilde{A}_{U_{i}}x-\\widetilde{b}_{U_{i}})_{+}\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "170 In particular, we are interested in studying when $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$ are equipped with the $\\infty$ -norm. In   \n171 this case, the minimal effective radius can be related to the Hoffman constant and function value   \n172 of $f\\,=\\,p\\odot q$ . For a tropical Puiseux polynomial $p(x)\\,=\\,\\mathrm{max}_{1\\le i\\le m_{p}}\\bigl\\{a_{i}x\\,+\\,b_{i}\\bigr\\}$ , let $\\check{p}(x)\\,=$   \n173 $\\mathrm{min}_{1\\le j\\le m_{q}}\\{a_{j}x+b_{j}\\}$ be its min-conjugate. ", "page_idx": 4}, {"type": "text", "text": "174 Proposition 3.4. Let $f=p\\oslash q$ be a tropical Puiseux rational map. For any $x\\in\\mathbb{R}^{n}$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nR_{f}(x)\\leq H(p\\oslash q)\\operatorname*{max}\\{p(x)-\\check{p}(x),\\,q(x)-\\check{q}(x)\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "175 3.3 Computing and Estimating Hoffman Constants ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "176 The PVZ Algorithm. In [17], the authors proposed a combinatorial algorithm to compute the   \n177 precise value of the Hoffman constant for a matrix $A\\in\\mathbb{R}^{m\\times n}$ , which we refer to as the Pe\u00f1a\u2013Vera\u2013   \n178 Zuluaga $(P V Z)$ algorithm and sketch its main steps here.   \n179 Definition 3.5. A set-valued map $\\Phi:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ assigns a set $\\Phi(x)\\subseteq\\mathbb{R}^{m}$ . The map is surjective   \n180 if $\\Phi(\\mathbb{R}^{n})=\\cup_{x}\\Phi(x)=\\mathbb{R}^{m}$ . Let $A\\in\\mathbb{R}^{m\\times n}$ . For any $J\\subseteq\\{1,2,\\ldots,m\\}$ , let $A_{J}$ be the submatrix   \n181 of $A$ consisting of rows with indices in $J$ . The set $J$ is called $A$ -surjective if the set-valued map   \n182 $\\Phi(x)=A_{J}x+\\left\\{y\\in\\mathbb{R}^{J}:y\\geq0\\right\\}$ is surjective.   \n183 Notice that $A$ -surjectivity is a generalization of linear independence of row vectors. We illustrate this   \n184 observation in the following two examples.   \n185 Example 3.6. If $J$ is such that $A_{J}$ is full-rank, then $J$ is $A$ -surjective, since for any $y\\in\\mathbb{R}^{J}$ , there   \n186 exists $x\\in\\mathbb{R}^{n}$ such that $y=A_{J}x$ .   \n187 Example 3.7. Let $A\\,=\\,{\\mathbf{1}}_{m\\times n}$ be the $m\\times n$ matrix whose entries are 1\u2019s. For any subset $J$ of   \n188 $\\{1,\\ldots,m\\}$ and for any $y\\in\\mathbb{R}^{J}$ , let $x\\in\\mathbb{R}^{n}$ such that $\\textstyle\\sum_{i}x_{i}\\leq\\operatorname*{min}\\{y_{j},j\\in J\\}$ . Then $y-A_{J}x\\geq0$ .   \n189 Thus any $J$ is $A$ -surjective. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "190 The PVZ algorithm is based on the following characterization of Hoffman constant. ", "page_idx": 4}, {"type": "text", "text": "191 Proposition 3.8. $I I7,$ , Proposition $2J$ Let $A\\in\\mathbb{R}^{m\\times n}$ . Equip $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$ with norm $\\Vert\\cdot\\Vert$ and denote   \n192 its dual norm by $\\|\\cdot\\|^{*}$ . Let ${\\cal S}(A)$ be the set of all $A$ -surjective sets. Then ", "page_idx": 4}, {"type": "equation", "text": "$$\nH(A)=\\operatorname*{max}_{J\\in S(A)}H_{J}(A)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 where ", "page_idx": 4}, {"type": "equation", "text": "$$\nH_{J}(A)=\\operatorname*{max}_{\\substack{y\\in\\mathbb{R}^{m}\\,\\|y\\|\\leq1}}\\operatorname*{min}_{\\substack{x\\in\\mathbb{R}^{n}}}\\|x\\|=\\frac{1}{\\substack{v\\in\\mathbb{R}_{+}^{J},\\|v\\|^{*}=1}}\\|A_{J}^{\\top}v\\|^{*}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "194 This characterization is particularly useful when $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$ are equipped with the $\\infty$ -norm, since   \n195 the computation of (8) reduces to a linear programming (LP) problem. The key problem is how to   \n196 maximize over all $A$ -surjective sets. To do this, the PVZ algorithm maintains three collections of   \n197 sets $\\mathcal{F},\\mathcal{T}$ , and $\\mathcal{I}$ where during every iteration: (i) $\\mathcal{F}$ contains $J$ such that $J$ is $A$ -surjective; (ii) $\\mathcal{T}$   \n198 contains $J$ such that $J$ is not $A$ -surjective; and (iii) $\\mathcal{I}$ contains candidates $J$ whose $A$ -surjectivity   \n199 will be tested. ", "page_idx": 4}, {"type": "text", "text": "200 To detect whether a candidate $J\\in\\mathcal{T}$ is surjective, the PVZ algorithm requires solving ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\|A_{J}^{T}v\\|_{1},\\ s.t.\\ v\\in\\mathbb{R}_{+}^{J},\\|v\\|_{1}=1.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "201 If the optimal value is positive, then $J$ is $A$ -surjective, and $J$ is assigned to $\\mathcal{F}$ and all subsets of $J$ are   \n202 removed from $\\mathcal{I}$ . Otherwise, the optimal value is 0 and there is $\\bar{v}\\in\\mathbb{R}_{+}^{J}$ such that $A_{J}^{\\top}v=0$ . Let   \n203 $I(v)=\\{i\\in J:v_{i}>0\\}$ and assign $I(v)$ to $\\mathcal{T}$ . Let $\\hat{J}\\in\\mathcal{I}$ be any set containing $I(v)$ . Replace all   \n204 such $\\hat{J}$ by sets $\\hat{J}\\backslash\\{i\\},i\\in I(v)$ which are not contained in any sets in $\\mathcal{F}$ . The implementation used in   \n205 our paper directly uses the MATLAB code provided by [17].   \n206 Lower and Upper Bounds. A limitation of the PVZ algorithm is that during each loop, every set in   \n207 $\\mathcal{I}$ needs to be tested, and each test requires solving a LP problem. Although solving one LP problem   \n208 in practice is fast, a complete while loop calls the LP solver many times.   \n209 Here, we propose an algorithm to estimate lower and upper bounds for Hoffman constants. An   \n210 intuitive way to estimate the lower bound is to sample a number of random subsets from $\\{1,\\ldots,m\\}$   \n211 and test for $A$ -surjectivity. This method bypasses optimizing combinatorially over ${\\cal S}(A)$ of $A$ -   \n212 surjective sets and gives a lower bound of Hoffman constant by Proposition 3.8. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "213 To get an upper of Hoffman constant, we use the result from [18]. ", "page_idx": 5}, {"type": "text", "text": "214 Theorem 3.9. $I I{\\boldsymbol{\\vartheta}}_{:}$ , Theorem 4.2] Let $A\\in\\mathbb{R}^{m\\times n}$ . Let $\\mathcal{D}(A)$ be a set of subsets of $J\\subseteq\\{1,\\ldots,m\\}$   \n215 such that $A_{J}$ is full rank. Let $\\mathcal{D}^{*}(A)$ be the set of maximal elements in $\\mathcal{D}(A)$ . Then the Hoffman   \n216 constant measured under 2-norm is bounded by ", "page_idx": 5}, {"type": "equation", "text": "$$\nH(A)\\leq\\operatorname*{max}_{J\\in{\\mathcal{D}}^{*}(A)}{\\frac{1}{\\hat{\\rho}(A_{J})}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "217 where $\\hat{\\rho}(A)$ is the smallest singular value of $A$ . ", "page_idx": 5}, {"type": "text", "text": "218 Using the fact that $\\|\\cdot\\|_{1}\\geq\\|\\cdot\\|_{2}$ , and the characterization from (8), we see that the upper bound also   \n219 holds when $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$ are equipped with the $\\infty$ -norm. However, enumerating all maximal elements   \n220 in $\\mathcal{D}(A)$ is not an improvement over enumerating $A$ -surjective sets from a computational perspective.   \n221 Instead, we will retain the strategy as in lower bound estimation to sample a number of sets from   \n222 $\\{1,2,\\ldots,m\\}$ and approximate the upper bound by (10). We verify this approach via synthetic data.   \n223 The experiments are relegated to the Appendix. ", "page_idx": 5}, {"type": "text", "text": "224 4 Symmetry and the Fundamental Domain ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "225 In this section, we study a geometric characterization of the sampling domain for networks exhibiting   \n226 symmetry. This corresponds to invariant neural networks. ", "page_idx": 5}, {"type": "text", "text": "227 4.1 Linear Regions of Invariant Neural Networks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "228 The notion of invariance for a neural network describes when a manipulation of the input domain   \n229 does not affect the output of the network. The manipulations we consider here are group actions.   \n230 Definition 4.1. Let $\\sigma:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a piecewise linear function, and let $G$ be a group acting on the   \n231 domain $\\mathbb{R}^{n}$ . $\\sigma$ is invariant under the group action of $G$ if for any element $g\\in G$ , $\\sigma\\circ g=\\sigma$ .   \n232 Given an invariant neural network, we can then define a sampling domain that takes into account the   \n233 effect of the group action.   \n234 Definition 4.2. Let $G$ be a group acting on $\\mathbb{R}^{n}$ . A subset $\\Delta\\subseteq\\mathbb{R}^{n}$ is a fundamental domain if it   \n235 satisfies two following conditions: (i) $\\mathbb{R}^{n}=\\bigcup_{g\\in G}g\\cdot\\Delta$ ; and (ii) $g\\cdot\\mathrm{int}(\\Delta)\\cap h\\cdot\\mathrm{int}(\\Delta)=\\emptyset$ for all   \n236 $g,h\\in G,g\\neq h$ .   \n237 The fundamental domain of a group $G$ therefore provides a periodic tiling of $\\mathbb{R}^{n}$ by acting on $\\Delta$ .   \n238 This is very useful in the context of numerical sampling for neural networks which are invariant   \n239 under some symmetry, since it means we can sample from a smaller subset of the input domain with   \n240 a guarantee to find all the linear regions in the limit. This allows us, in principle, to be able to use far   \n241 fewer samples while maintaining the same density of points.   \n242 Theorem 4.3. Let $f:\\mathbb{R}^{N}\\to\\mathbb{R}$ be a tropical rational map invariant under group $G.$ . Let $\\Delta\\subseteq\\mathbb{R}^{N}$ be   \n243 a fundamental domain of $G$ . Suppose $\\mathcal{L}$ is the set of linear regions. Define the following two sets ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{U}_{c}:=\\{A\\in\\mathcal{U}:A\\subseteq\\Delta\\}}\\\\ &{\\mathcal{U}_{n}:=\\{A\\in\\mathcal{U}:A\\cap\\Delta\\neq\\emptyset\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "244 Then ", "page_idx": 5}, {"type": "equation", "text": "$$\n|G||\\mathcal{U}_{c}|\\leq|\\mathcal{U}|\\leq|G||\\mathcal{U}_{c}|+\\sum_{A\\in\\mathcal{U}_{n}\\backslash\\mathcal{U}_{c}}\\frac{|G|}{|G_{A}|}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "245 where $|G_{A}|$ is the size of the stabilizer of $A$ . ", "page_idx": 5}, {"type": "text", "text": "246 This gives us a method for estimating the total number of linear regions from sampling in the   \n247 fundamental domain using multiplicity, which we discuss next. ", "page_idx": 5}, {"type": "text", "text": "248 4.2 Sampling from the Fundamental Domain ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "249 To demonstrate the potential performance improvements in numerical sampling exploiting symmetry   \n250 in the network architecture, we consider permutation invariant neural networks inspired by deep sets   \n251 [19]. Our numerical sampling approach is inspired by very recent work in this area [20].   \n252 Lemma 4.4 ([19]). An $m\\times m$ matrix $W$ acting as a linear operator of the form $W=\\lambda I_{m\\times m}+$   \n253 $\\gamma(\\mathbf{1}^{T}\\mathbf{1})$ , where $\\lambda,\\gamma\\in\\mathbb{R}$ is permutation equivariant, meaning $W P x=P W x$ for any $x\\in\\mathbb{R}^{m}$ , so it   \n254 commutes with any permutation matrix.   \n255 Using a weight matrix of this form, we can construct permutation invariant neural networks by setting   \n256 the bias to 0, applying a ReLU activation after multiplication by $W$ , and then summing. In this case,   \n257 the network is invariant under the group action $S_{n}$ , so the fundamental domain is the set of points   \n258 with increasing coordinates, i.e., $\\Delta=\\{(x_{1},\\ldots,x_{n}):x_{1}\\geq x_{2}\\geq\\ldots\\geq x_{n}\\}$ . This splits $\\mathbb{R}^{n}$ into $n!$   \n259 tiles, so we have a clear and significant advantage in restricting sampling to the fundamental domain.   \n260 Note, however, that it is important to address the multiplicities of symmetric linear regions correctly:   \n261 If a given Jacobian of shape $n\\times1$ has no repeated elements, this means it is contained in the interior   \n262 of some group action applied to the fundamental domain. This means there are $n!$ total linear regions   \n263 with this Jacobian. If, on the other hand, there are repeated coefficients in a given Jacobian $J$ , we   \n264 consider the set $C(J)$ of counts of repeated elements. For example, for $J=[1,1,0],C(J)=(2,1)$ .   \n265 Then the multiplicity of a given Jacobian is given by ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{mult}(J)=\\frac{n!}{\\prod_{c\\in C(J)}c!}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "266 Using this multiplicity calculation we can efficiently estimate the number of linear regions while   \n267 reducing the number of point samples by a factor of $n!$ . This provides a dramatic gain in sampling   \n268 efficiency.   \n269 In Figure 1, we present the results when Algorithm 2 is run with $R=10,N=10,M=50$ . These   \n270 results show that the fundamental domain estimate performs well for low dimensional inputs but   \n271 appears to overcount linear regions as $n$ increases. Despite divergence, there is still utility in this   \n272 metric because we are often more concerned with obtaining an upper bound on the expressivity of a   \n273 neural network than an exact figure and the fundamental domain estimate does not undercount the   \n274 number of linear regions. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "275 5 Symbolic Neural Networks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "276 Here, we present the details on our practical contribution of a symbolic representation of neural   \n277 networks as a new library integrated into OSCAR [15]. ", "page_idx": 6}, {"type": "text", "text": "278 5.1 Computing Linear Regions of Tropical Puiseux Rational Maps ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "279 We present an algorithm that can compute the linear regions of any tropical Puiseux rational function.   \n280 Intuitively, we do this by computing the linear regions of the numerator and denominator, and then   \n281 considering intersections of such regions and how they fti together. Thus, a first step is to understand   \n282 how the computation of linear regions works for tropical Puiseux polynomials. The key to our   \n283 approach will be to exploit the polyhedral connection of tropical geometry and recast the problem   \n284 in the language of polyhedral geometry. This, among other things, will allow us to make use the   \n285 extensive polyhedral geometry library in OSCAR [15] for implementation.   \n286 One important upshot from this study is that there is a strong connection between the number of   \n287 linear regions of a tropical Puiseux rational function and the number of monomials that appear in   \n288 its algebraic expression. Note, however, that the two are independent, in the sense that two Puiseux   \n289 rational functions could have the same number of linear regions but different numbers of (nonzero)   \n290 monomials, and conversely, the same number of monomials and a different number of linear regions.   \n291 For instance, computing the number of linear regions requires some combinatorial data about the   \n292 intersections of the polyhedra defined by monomials.   \n293 First, we need to know how to compute the linear regions of tropical polynomials. Let $P\\,=$   \n294 $\\bigoplus_{n}a_{n}\\odot x^{n}$ where by $x^{n}$ we mean $x_{1}^{n_{1}}\\odot\\cdots\\odot x_{k}^{n_{k}}$ and powers are taken in the tropical sense. Then   \n295 as function $\\mathbb{R}^{k}\\rightarrow\\mathbb{R}$ , $P$ is given by $\\operatorname*{max}_{n}\\left\\{a_{n}+n_{1}x_{1}\\cdot\\cdot\\cdot+n_{k}x_{k}\\right\\}$ . It follows that the linear regions   \n296 of $P$ are precisely the sets of the form ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nS_{n}=\\left\\{x\\in\\mathbb{R}^{n}\\mid a_{m}+m_{1}x_{1}\\cdot\\cdot\\cdot+m_{k}x_{k}\\leq a_{n}+n_{1}x_{1}\\cdot\\cdot\\cdot+n_{k}x_{k}\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For any set $U$ on which $P$ is linear, we write $L(P,U)$ for the corresponding linear map. This gives us ", "page_idx": 7}, {"type": "equation", "text": "$$\nL(P,S_{n})(x)=a_{n}+n_{1}x_{1}\\cdot\\cdot\\cdot+n_{k}x_{k}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "298 We now rewrite (11) using polyhedral geometry. Recall that a polyhedron in $\\mathbb{R}^{k}$ is a set of the form   \n299 $P(A,b)=\\{x\\in\\mathbb{R}^{k}\\mid A x^{\\stackrel{\\cdot}{\\leq}}b\\}$ . We claim that each linear region is a polyhedron: For a fixed index   \n300 $n$ , define the matrix $A_{n}$ to be the $(N-1)\\times k$ matrix whose rows are the vectors $m-n$ , where $m$   \n301 ranges over the support of the coefficients of $P$ (ordered lexicographically) and $b_{n}$ to be the vector   \n302 with entries $a_{n}-a_{m}$ . Then $S_{n}=P(A_{n},b_{n})$ . This gives us a way to encode the computation of the   \n303 linear regions of tropical Puiseux polynomials using polyhedral geometry. As a direct consequence,   \n304 intersections of linear regions of tropical Puiseux polynomials are also polyhedra. In particular, there   \n305 are algorithms from polyhedral geometry for determining whether such polyhedra are realizable. One   \n306 of the key observations given by our algorithm is that the linear regions of tropical Puiseux rational   \n307 maps are almost given by $k$ -dimensional intersections of the linear regions of the numerator and   \n308 the denominator. Indeed, note that if $U$ is a linear region of $p$ and $V$ a linear region of $q$ , then we   \n309 have $L(U\\cap V,p\\oslash q)=L(U,p)-L(V,q)$ . The only issue that arises is that there might be some   \n310 repetition in the $L(U\\cap V,p\\odot q)$ as $U$ ranges over the linear regions of $p$ and $V$ over the linear regions   \n311 of $q$ . In particular, linear regions of $p\\oslash q$ might end up corresponding to unions of such $U\\cap V$ . ", "page_idx": 7}, {"type": "text", "text": "312 5.2 Computing Linear Regions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "313 Determining the linear regions of a neural network may be approached numerically or symbolically.   \n314 The numerical approach exploits the fact that linear regions of a neural network correspond to regions   \n315 where the gradient is constant. Thus, to estimate the number of linear regions, we can evaluate the   \n316 gradient on a sample of points (e.g., a mesh) in some large box $[-R,R]^{\\bar{n}}$ . For sufficiently large $R$   \nand a sufficiently dense sample of points, we get an accurate estimate. The symbolic approach, on   \n318 the other hand, exploits the connection between neural networks and tropical Puiseux rational maps.   \n319 Indeed, we can symbolically compute a Puiseux rational map that represents the neural network and   \nthen compute the number of linear regions using the approach outlined in section 5.1.   \n321 To compare each method, we ran the computations on smaller networks with varying sizes to compare   \n322 run times and precision. For the symbolic approach, we generate 20 neural networks with random   \n323 weights for each architecture and then compute the tropical Puiseux rational function associated to   \n324 each neural network and compute the linear regions using Algorithm 3.   \n325 For the numerical approach, we also work with synthetic data and generate 1000 neural networks   \n326 with random weights for each architecture. We then estimate the number of linear regions in a box of   \n327 size $[-10,10]^{n}$ and sample 1000 points from this domain.   \nIn both cases, we use He initialization for the weights, i.e., we generate weights with distribution   \n329 $\\textstyle N(0,{\\frac{2}{\\sqrt{d}}})$ where $d$ is the input dimension. The data we obtain in this manner is summarized in Tables   \n330 10 and 11. For the symbolic approach, we also track the number of nonzero monomials to compare   \n331 this quantity with the number of linear regions. For networks with 3 layers, we find the numerical   \n332 estimate to be quite close, but for 4 it seems to diverge. This could be because in the numerical   \n333 approach, we are only counting the number of unique Jacobians that can be found in the domain. A   \n334 situation could arise where the same linear function is disconnected and hence counted twice by the   \n335 symbolic approach but only once for the numerical approach.   \n336 The main observations from our experimental study are as follows. The numerical approach is faster,   \n337 but offers no guarantee of precision: When running the computation for a given $R$ and mesh grid,   \n338 there seems to be no easy way of determining whether we have indeed hit all the linear regions or   \n339 whether we have obtained an accurate estimate of the arrangements of these regions. It is possible   \n340 to either overestimate or underestimate the number of linear regions. In particular, there is a priori   \n341 no obvious way to select the parameters. We found the symbolic approach to be more precise, but   \n342 slower. In general, the number of monomials seems to be far larger than the number of linear regions,   \n343 which contradicts the intuition of Figure 2.   \n344 Both algorithms suffer from the curse of dimensionality: in the case of the numerical approach, the   \n345 number of samples in a meshgrid grows exponenially with respect to the dimension. In the case of   \n346 the symbolic approach, calculations with polytopes seem to scale poorly with dimension and with the   \n347 complexity of the neural network. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "348 6 Discussion: Limitations & Directions for Future Research ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "349 In this paper, we set up a framework to interpret and analyzed the expressivity of neural networks   \n350 using techniques from polyhedral and tropical geometry. We demonstrated several ways in which a   \n351 symbolic interpretation can often enable computational optimizations for otherwise intractable tasks   \n352 and provided new insights into the inner workings of these networks. To the best of our knowledge,   \n353 ours is the first work to provide practical tropical geometric theory and algorithms to numerically   \n354 compute and analyze the expressivity of a neural network both in terms of inherent neural network   \n355 quantities as well as tropical geometric quantities.   \n356 Despite the theoretical and practical advancement of tropical deep learning that our work offers, it   \n357 is nevertheless subject to limitations, which we now discuss and which inspire directions for future   \n358 research.   \n359 Experimental Limitations. The curse of dimensionality is a common theme in deep learning, and   \n360 our work is unfortunately no exception. The methods introduced in this paper are quite fast for small   \n361 enough networks, but scale poorly with dimension and more complex architectures.   \n362 We note that the main computational bottlenecks of the Puiseux rational function associated with a   \n363 neural network are the implementation of fast multivariate Puiseux series operations. Our current   \n364 computations rely on a custom implementation of this type of operation, and one potential avenue for   \nimprovement would be using such methods once they have been implemented in OSCAR [15].   \n366 For the computation of linear regions, both the numerical and symbolic approaches suffer from the   \ncurse of dimensionality. For instance, the numerical approach requires sampling on a mesh grid in a   \n368 box of the form $[-R,R]^{n}$ where $n$ is the input dimension. In particular, the number of points needed   \n369 is proportional to the volume, which scales exponenially in $n$ . Similarly, the symbolic approach relies   \n370 on the computation of the Puiseux rational function associated with a neural network and polytope   \n371 computations, both of which are challenging computational problems in higher dimensions.   \n372 Most of our computations rely on carrying out some elementary computations many times. Thus, ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "373 another avenue of improvement for this would be to parallelize. ", "page_idx": 8}, {"type": "text", "text": "374 Structural Limitations. Much of what we are studying are basically framed as a combinatorial   \n375 optimization problem, which are known to be difficult. In particular, computing the Hoffman constant   \n376 is equivalent to the Stewart\u2013Todd condition measure of a matrix and both quantities are NP-hard to   \n377 compute in general cases [17, 21].   \n378 Further studying and understanding where and how symbolic computation algorithms can be made   \n379 more efficient, e.g., by parallelization, would make our proposed approaches more applicable to   \n380 larger neural networks. Our work effectively proposes a new intersection of symbolic computation   \n381 and deep learning, so there remains infrastructure to set up to make methods from these two fields   \n382 compatible. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "383 References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "384 [1] Grigory Mikhalkin and Johannes Rau. Tropical geometry, volume 8. MPI for Mathematics,   \n385 2009.   \n386 [2] David Speyer and Bernd Sturmfels. Tropical Mathematics. Mathematics Magazine, 82(3):163\u2013   \n387 173, 2009.   \n388 [3] Diane Maclagan and Bernd Sturmfels. Introduction to tropical geometry, volume 161. American   \n389 Mathematical Society, 2021.   \n390 [4] Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of   \n391 deep feed forward networks with piece-wise linear activations. arXiv preprint arXiv:1312.6098,   \n392 2013.   \n393 [5] Guido F Mont\u00fafar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of   \n394 linear regions of deep neural networks. Advances in neural information processing systems, 27,   \n395 2014.   \n396 [6] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep   \n397 neural networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.   \n398 [7] Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the   \n399 expressive power of deep neural networks. In international conference on machine learning,   \n400 pages 2847\u20132854. PMLR, 2017.   \n401 [8] Boris Hanin and David Rolnick. Deep ReLU Networks Have Surprisingly Few Activation   \n402 Patterns. Advances in neural information processing systems, 32, 2019.   \n403 [9] Huan Xiong, Lei Huang, Mengyang Yu, Li Liu, Fan Zhu, and Ling Shao. On the number   \n404 of linear regions of convolutional neural networks. In International Conference on Machine   \n405 Learning, pages 10514\u201310523. PMLR, 2020.   \n406 [10] Alexis Goujon, Arian Etemadi, and Michael Unser. On the number of regions of piecewise   \n407 linear neural networks. Journal of Computational and Applied Mathematics, 441:115667, 2024.   \n408 [11] Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks.   \n409 In International Conference on Machine Learning, pages 5824\u20135832. PMLR, 2018.   \n410 [12] Vasileios Charisopoulos and Petros Maragos. A tropical approach to neural networks with   \n411 piecewise linear activations. arXiv preprint arXiv:1805.08749, 2018.   \n412 [13] Ruriko Yoshida, Georgios Aliatimis, and Keiji Miura. Tropical neural networks and its applica  \n413 tions to classifying phylogenetic trees. arXiv preprint arXiv:2309.13410, 2023.   \n414 [14] Kurt Pasque, Christopher Teska, Ruriko Yoshida, Keiji Miura, and Jefferson Huang. Tropical   \n415 decision boundaries for neural networks are robust against adversarial attacks. arXiv preprint   \n416 arXiv:2402.00576, 2024.   \n417 [15] Oscar \u2013 open source computer algebra research system, version 1.0.0, 2024.   \n418 [16] Alan J Hoffman. On approximate solutions of systems of linear inequalities. In Selected Papers   \n419 Of Alan J Hoffman: With Commentary, pages 174\u2013176. World Scientific, 2003.   \n420 [17] Javier Pena, Juan Vera, and Luis Zuluaga. An algorithm to compute the hoffman constant of a   \n421 system of linear constraints. arXiv preprint arXiv:1804.08418, 2018.   \n422 [18] Osman G\u00fcler, Alan J Hoffman, and Uriel G Rothblum. Approximations to solutions to systems   \n423 of linear inequalities. SIAM Journal on Matrix Analysis and Applications, 16(2):688\u2013696, 1995.   \n424 [19] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,   \n425 and Alexander J Smola. Deep sets. Advances in neural information processing systems, 30,   \n426 2017.   \n427 [20] Alexis Goujon, Arian Etemadi, and Michael Unser. On the number of regions of piecewise   \n428 linear neural networks. Journal of Computational and Applied Mathematics, 441:115667, 2024.   \n429 [21] Javier F Pena, Juan C Vera, and Luis F Zuluaga. Equivalence and invariance of the chi and   \n430 hoffman constants of a matrix. arXiv preprint arXiv:1905.06366, 2019. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "431 A Further Experimental Details ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "432 We ran the final computations on NVIDIA GeForce RTX 3090 GPUs. Table 7 lists the time taken by   \n433 each experiment. Given that our experiments do not include training on large datasets, the experiments   \n434 are not particularly expensive from the perspective memory usage, and all the code can be run on a   \n435 laptop. The detail provided in the paper correspond roughly to the amount of computational resources   \n436 that were used for this work, omitting trial and testing runs. ", "page_idx": 10}, {"type": "text", "text": "437 B Algorithms ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Algorithm 1 Lower and approximate upper bound of Hoffman constant   \nRequire: $A$ : an $m\\times n$ matrix; $B$ max number of iterations; $\\epsilon$ threshold of testing surjectivity. 1: Initialize $H_{L}=H^{U}=0$ .   \n2: for $i\\in{1,\\dots,B}$ do   \n3: Sample a random integer $K$ .   \n4: Sample a random subset $J$ from $\\{1,\\ldots,m\\}$ of size $K$ .   \n5: Solve (9). Let $t$ be the optimal value;   \n6: if $t>\\epsilon$ then   \n7: $J$ is surjective. Update $H_{L}=\\operatorname*{max}\\{H_{L},\\frac{1}{t}\\}$ ;   \n8: Compute the minimal singular value of $\\hat{\\rho}\\!\\left(A_{J}\\right)$ ;   \n10: 9: if $\\hat{\\rho}(\\bar{A}_{J})>0$ Update HU = max{HU,\u03c1\u02c6(A1J)}; then   \nreturn Lower bound $H_{L}$ and approximate upper bound $H^{U}$ . ", "page_idx": 10}, {"type": "text", "text": "Algorithm 2 Estimation of the ratio of fundamental domain sampling to regular sampling ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Require: The input dimension $n$ , $R\\in\\mathbb{R}$ side length for cube centered at the origin from which the samples are taken, $M$ number of models to use, $N$ base number of points to sample.   \n1: for $m\\in1..M$ do   \n2: Create a permutation invariant model $\\sigma$ with input dimension $n$ .   \n3: Sample $N^{n}$ points in the cube with side length $R$ centered at the origin. Note that the number of points in the sample grows exponentially with the input dimension $n$ .   \n4: Compute the Jacobian matrices of the network at each point, round to 10 decimal place to avoid numerical errors, remove duplicates, and count the number of unique Jacobians.   \n5: Sample Nn! points from the fundamental domain of $\\mathbb{R}^{n}$ intersected with the sampling cube.   \n6: Compute the unique Jacobians similarly as for the regular sampling.   \n7: Sum the multiplicities of each Jacobian to get an estimate of the total number of linear regions.   \n8: Record the ratio of the fundamental domain estimate to the regular estimate. return The average ratio across $M$ models. ", "page_idx": 10}, {"type": "text", "text": "438 C Proofs ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "439 C.1 Proof of Proposition 3.4 ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "440 Proof. The polyhedra defined by ", "page_idx": 10}, {"type": "equation", "text": "$$\n{\\left(\\begin{array}{l}{\\left[A_{p}\\right]}-\\mathbf{1}\\left[a_{i_{p}}\\right]}\\\\ {A_{q}}\\end{array}\\right)}x\\leq{\\left[b_{i_{p}}\\mathbf{1}-b_{p}\\right]}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "441 form a convex refinement of linear regions of $f$ . Let ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\mathrm{res}_{i_{p},j_{q}}(x):={\\binom{\\left[A_{p}\\right]}{A_{q}}}-\\mathbf{1}{\\binom{\\!a_{i_{p}}}{\\!a_{j_{q}}}}\\right)x-{\\binom{\\!b_{i_{p}}\\mathbf{1}-b_{p}}{\\!b_{j_{q}}\\mathbf{1}-b_{q}}}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "442 denote the residual of $x$ to the polyhedron. We have ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{f}(x)\\leq H(p\\oslash q)\\operatorname*{max}\\{\\|\\mathrm{res}_{i_{p},j_{q}}(x)_{+}\\|_{\\infty}:1\\leq i_{p}\\leq m_{p}\\,;1\\leq j_{q}\\leq m_{q}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "Algorithm 3 Linear regions of tropical Puiseux rational functions ", "page_idx": 11}, {"type": "text", "text": "Require: Tropical Puiseux polynomials $p,q$ in $n$ variables.   \n1: Compute the linear regions $U_{1},\\ldots,U_{l}$ of $p$ , and set $L_{i}=L(p,U_{i})$ .   \n2: Compute the linear regions $V_{1},\\ldots,V_{m}$ of $q$ , and set $S_{j}=L(q,V_{j})$   \n3: Compute the pairs $(i,j)$ such that $U_{i}\\cap V_{j}$ has dimension $n$   \n4: for $(i,j)$ such that $U_{i}\\cap V_{j}$ has dimension $n$ do   \n5: Compute the linear map $T_{i j}=L_{i}-S_{j}$   \n6: Set $S$ to be the set of all $T_{i j}$   \n7: for $T\\in S$ do   \n8: Compute the set $I(T)$ indices $(i,j)$ such that $T=T_{i j}$ .   \n9: Compute the set $C(T)$ of connected components of ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\bigcup_{(i,j)\\in I(T)}U_{i}\\cap V_{j}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "return $\\textstyle\\bigcup_{T\\in S}C(T)$ . ", "page_idx": 11}, {"type": "text", "text": "Algorithm 4 Numerical estimation of neural network linear regions   \nRequire: The architecture of a linear activation neural network $\\sigma$ with scalar output, $R\\in\\mathbb{R}$ side length for cube centered at the origin from which the samples are taken, $M$ number of models to use, $N$ number of points to sample.   \n1: for $m\\in1..M$ do   \n2: Create a model with architecture $\\sigma$ and initialise weights and biases using He inialisation.   \n3: Sample $N$ points in the cube with side length $R$ centered at the origin.   \n4: Compute the Jacobian matrices of the network at each point.   \n5: Round the Jacobians matrices to 5 decimal places to avoid floating point errors.   \n6: Remove duplicates and count the number of unique Jacobians. return The average number of linear regions. ", "page_idx": 11}, {"type": "text", "text": "443 Note that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathrm{res}_{i_{p},j_{q}}(x)_{+}\\|_{\\infty}=\\bigg\\|\\bigg(\\bigg[\\boldsymbol{A}_{p}x+b_{p}-\\mathbf{1}(a_{i_{p}}x+b_{i_{p}})\\bigg]\\bigg)_{+}\\bigg\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\underset{k,\\ell}{\\operatorname*{max}}\\,\\big\\{(\\boldsymbol{A}_{p}x+b_{p})_{k}-(a_{i_{p}}x+b_{i_{p}}),\\,(\\boldsymbol{A}_{q}x+b_{q})_{\\ell}-(a_{j_{q}}x+b_{j_{q}}),\\,0\\big\\}}\\\\ &{\\qquad\\qquad\\qquad=\\operatorname*{max}\\big\\{p(x)-(a_{i_{p}}x+b_{i_{p}}),\\,q(x)-(a_{j_{q}}x+b_{j_{q}}),\\,0\\big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "444 Therefore, ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{i_{p},j_{q}}\\|\\mathrm{res}_{i_{p},j_{q}}(x)\\|_{\\infty}=\\displaystyle\\operatorname*{max}_{i_{p},j_{q}}\\big\\{p(x)-(a_{i_{p}}x+b_{i_{p}}),\\,q(x)-(a_{j_{q}}x+b_{j_{q}}),\\,0\\big\\}}\\\\ {\\displaystyle=\\operatorname*{max}_{i_{p}}\\big\\{p(x)-\\operatorname*{min}_{i_{p}}\\{a_{i_{p}}x+b_{i_{p}}\\},\\,q(x)-\\operatorname*{min}_{j_{q}}\\{a_{j_{q}}x+b_{j_{q}}\\},\\,0\\big\\}}\\\\ {\\displaystyle=\\operatorname*{max}_{i_{p}}\\big\\{p(x)-\\check{p}(x),\\,q(x)-\\check{q}(x)\\big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "445 which proves (6). ", "page_idx": 11}, {"type": "text", "text": "446 C.2 Proof of Lemma 3.3 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "447 Proof. From the definition of minimal effective radius we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R}_{f}(x)=\\operatorname*{min}\\{r:B(x,r)\\cap{U}_{i}\\neq\\varnothing,{U}_{i}\\in\\mathcal{U}\\}=\\operatorname*{min}\\{r:d(x,{U}_{i})\\leq r,{U}_{i}\\in\\mathcal{U}\\}}\\\\ &{\\qquad=\\operatorname*{max}\\{d(x,{U}_{i}):{U}_{i}\\in\\mathcal{U}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "448 For each linear region $U_{i}$ characterized by $\\widetilde{A}_{U_{i}}x\\le\\widetilde{b}_{U_{i}}$ , by (1), $d(x,U_{i})\\leq H(\\widetilde{A}_{U_{i}})\\lVert(\\widetilde{A}_{U_{i}}x\\!-\\!\\widetilde{b}_{U_{i}})_{+}\\rVert.$   \n449 Passing to maximum we have ", "page_idx": 11}, {"type": "equation", "text": "$$\nR_{f}(x)=\\operatorname*{max}_{U_{i}\\in\\mathcal{U}}d(x,U_{i})\\le\\operatorname*{max}_{U_{i}\\in\\mathcal{U}}H(\\Tilde{A}_{U_{i}})\\operatorname*{max}_{U_{i}\\in\\mathcal{U}}||(\\Tilde{A}_{U_{i}}x-\\Tilde{b}_{U_{i}})_{+}||=H(f)\\operatorname*{max}_{U_{i}\\in\\mathcal{U}}||(\\Tilde{A}_{U_{i}}x-\\Tilde{b}_{U_{i}})_{+}||.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/84d8787cf7f2b4879b5cb7270f63a20961276fa409e022960846fdb303025868.jpg", "table_caption": [], "table_footnote": ["Table 1: Hoffman constants, lower bounds, approximate upper bounds, and their corresponding computational time for $m_{p}=10$ , $m_{q}=5$ and $n=3$ "], "page_idx": 12}, {"type": "text", "text": "451 C.3 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "452 Proof. The action of $G$ partitions $\\boldsymbol{\\mathcal{U}}$ into a set of orbits $[\\mathcal{U}]$ and we have $\\begin{array}{r}{\\left|\\mathcal{U}\\right|=\\sum_{[A]\\in[\\mathcal{U}]}\\left|\\left[A\\right]\\right|}\\end{array}$ . From property (i) defining a fundamental domain $\\cup_{A\\in\\mathcal{U}}A=\\cup_{\\sigma\\in G}\\sigma\\cdot\\Delta.$ , we have the trivial lower bound on the number of linear regions $\\begin{array}{r}{|\\mathcal{U}|\\geq\\sum_{A\\in\\mathcal{U}_{c}}|[A]|=|\\bar{G^{}}||\\mathcal{U}_{c}|}\\end{array}$ . Let $A\\in\\mathcal{U}$ . By Lagrange\u2019s theorem, the orbit of $A$ is such that $|[A]||G_{A}|=|G|$ . Thus we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n|\\mathcal{U}|\\leq\\sum_{A\\in\\mathcal{U}_{n}}|[A]|\\leq|G||\\mathcal{U}_{c}|+\\sum_{A\\in\\mathcal{U}_{n}\\backslash\\mathcal{U}_{c}}\\frac{|G|}{|G_{A}|}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "453 ", "page_idx": 12}, {"type": "text", "text": "454 D Numerical Calculations of the Hoffman Constant ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "455 We illustrate the computation of Hoffman constant of tropical Puiseux rational map on synthetic data.   \n456 We generate tropical Puiseux rational maps by randomly generating two tropical Puiseux polynomials   \n457 $p$ and $q$ . Specifically, suppose $p$ has $m_{p}$ monomials and $q$ has $m_{q}$ monomials. We construct an   \n458 $m_{p}\\times n$ matrix $A_{p}$ and an $m_{q}\\times n$ matrix $A_{q}$ by uniformly sampling entries from $[0,1]$ . We then form   \n459 the matrix defined by (4). We then compute the exact Hoffman constant using the PVZ algorithm and   \n460 estimate its lower bound and approximate its upper bound by our proposed algorithm. We record the   \n461 computation time and the number of calls to solve the LP problem in the whole loop.   \n462 In the experiment we take different values of $m_{p}$ , $m_{q}$ , $n$ and $B$ . For each of the parameters we   \n463 repeat all computation for 8 times. The true Hoffman constants, lower bounds, upper bounds, and   \n464 the computation time per linear region can be found in Table 1,2,3, and the number of iterations of   \n465 the PVZ algorithm and average time to solve LP during each iteration can be found in Table 4,5.6.   \n466 From the tables we can see that computing the true Hoffman constants requires testing surjectivity   \n467 and solving over thousands LP problems, which costs a lot of time. Although the lower bounds and   \n468 approximate upper bounds can be loose, the computational time is much faster for lower bounds and   \n469 upper bounds, which implies its potential to apply for real data applications. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "470 E Tables ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "471 Tables 8 and 9 summarise the outcomes of the experiments on the computation of linear regions for   \n472 tropical Puiseux rational functions. For a fixed number of variables $n_{\\mathrm{var}}$ and number of monomials   \n473 $n_{\\mathrm{monomials}}$ , we generate $n_{\\mathrm{samples}}$ random Puiseux rational functions by picking random coefficients   \n474 and exponents using Julia\u2019s inbuilt random number generation functions, where both the numerator   \n475 and the denominator have nmonomials monomials. We then compute the number of linear regions for   \n476 each of these rational functions and take the average over our all the samples that were generated. ", "page_idx": 12}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/a36e420625ba345f84fba924ee46e6f175a1bd68afb9a1762abe10f3c3121939.jpg", "table_caption": [], "table_footnote": ["Table 2: Hoffman constants, lower bounds, approximate upper bounds, and their corresponding computational time for $m_{p}=15$ , $m_{q}=9$ and $n=6$ "], "page_idx": 13}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/bb5bc4b7374ab9d7a22fb4faa7bb6763301f1c478fbd80cc428a1f9fcdb49f13.jpg", "table_caption": [], "table_footnote": ["Table 3: Hoffman constants, lower bounds, approximate upper bounds, and their corresponding computational time for $m_{p}=15$ , $m_{q}=5$ and $n=7$ "], "page_idx": 13}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/6bbb13837149da7124cd00ef5004eb836db07001b9cca0a5439111ed3d50d084.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 4: Number of iterations in the PVZ algorithm and average time to solve LP during each iteration for $m_{p}=10$ , $m_{q}=5$ and $n=3$ ", "page_idx": 13}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/5f49e161f8b239be3ae04798ee2ffb0f96771416478c72b6e2f7bffa1e05a9fc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 5: Number of iterations in the PVZ algorithm and average time to solve LP during each iteration for $m_{p}=15$ , $m_{q}=9$ and $n=6$ ", "page_idx": 13}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/fd23f797a01593623ec35d4736883de420bebeea8865b58fe8a9385c90d3eedc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 6: Number of iterations in the PVZ algorithm and average time to solve LP during each iteration for $m_{p}=15$ , $m_{q}=5$ and $n=7$ ", "page_idx": 13}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/1f097ffe88309fb12aee8db73e78a47a5496dcd57d45cf40519ffe4fe0eddaa7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/13ed06d244d789d5a36f25fbc05c0033ade465befb3d3ce8d2774d8f275d89f4.jpg", "table_caption": ["Table 7: Compute details "], "table_footnote": ["Table 8: Computation for $n_{\\mathrm{var}}=3$ , $\\overline{{n_{\\mathrm{samples}}=12}}$ "], "page_idx": 13}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/80cdcf47da75cc6482f6f6d46d111139d3d132f46f002557bf20e1312dd1674d.jpg", "table_caption": [], "table_footnote": ["Table 9: Computation for $\\overline{{n_{\\mathrm{var}}=4}}$ , $\\overline{{n_{\\mathrm{samples}}=4}}$ "], "page_idx": 14}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/6c49c1efe9b646944c77b746a260e90a38868df464b40de9c4de81386bf58274.jpg", "table_caption": ["Table 10: Symbolic computation "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "TJJ4gZtkS4/tmp/671e81fc95af02ea9e4f876dfad13c82927b1c22a42037186b3084715fc07804.jpg", "table_caption": [], "table_footnote": ["Table 11: Numerical computation "], "page_idx": 14}, {"type": "image", "img_path": "TJJ4gZtkS4/tmp/53a44032475b4e5079dac037b3fd68192267d044b88a8dbf1916069ba1db293f.jpg", "img_caption": ["Figure 1: Ratio estimates for different input sizes with standard deviation error bars "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "TJJ4gZtkS4/tmp/7d26272c6c4e00d50845f6f60c3c2520b5e2d093daf29394f1b564abee4a262d.jpg", "img_caption": ["Figure 2: Linear regions of a Puiseux rational function in 3 variables "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "TJJ4gZtkS4/tmp/f8615f520501cf3720aced7134926dde9bcae23f990148f19576aba9359a896a.jpg", "img_caption": ["Figure 3: Linear regions of a Puiseux rational function in 4 variables "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "478 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "81 paper\u2019s contributions and scope?   \n82 Answer: [Yes]   \n83 Justification: Each of the three contributions mentioned in the abstract has a whole section   \n84 devoted to it, including theoretical results and experiments. We also provided an in-depth   \n85 discussion of the limitations of our work in Section 6.   \n86 Guidelines:   \n87 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n88 made in the paper.   \n89 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n90 contributions made in the paper and important assumptions and limitations. A No or   \n91 NA answer to this question will not be perceived well by the reviewers.   \n92 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n93 much the results can be expected to generalize to other settings.   \n94 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n95 are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provided a detailed discussion of the limitations or our work, both computational and theoretical in Section 6. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 16}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "9 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n0 a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Justification: We clearly define all mathematical terms and the proofs are explained in detail and are correct to the best of our knowledge. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "545 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: Our paper describes the algorithms that are used to run the experiments, and our submission includes all the code necessary to run these together with instructions detailing how to use it. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5 5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes]   \nJustification: We provided all the code that is necessary to reproduce the experimental   \nresults, together with instructions on how to run this.   \nGuidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper gives some details about how the synthetic data used for experiments was generated. Moreover, we also provide the code that is necessary to run the experiments. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 18}, {"type": "text", "text": "625 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: Our figures clearly demonstrate error bars when appropriate and we disclose the experimental setup relevant to the statistical significance of our experiments. ", "page_idx": 18}, {"type": "text", "text": "636 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n637 example, train/test split, initialization, random drawing of some parameter, or overall   \n638 run with given experimental conditions).   \n639 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n640 call to a library function, bootstrap, etc.)   \n641 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n642 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n643 of the mean.   \n644 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n645 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n646 of Normality of errors is not verified.   \n647 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n648 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n649 error rates).   \n650 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n651 they were calculated and reference the corresponding figures or tables in the text.   \n652 8. Experiments Compute Resources   \n653 Question: For each experiment, does the paper provide sufficient information on the com  \n654 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n655 the experiments?   \n656 Answer: [Yes]   \n657 Justification: The appendix provides some detail on the type of compute that was used (type   \n658 of GPU, memory), as well was runtimes for each experiment.   \n659 Guidelines:   \n660 \u2022 The answer NA means that the paper does not include experiments.   \n661 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n662 or cloud provider, including relevant memory and storage.   \n663 \u2022 The paper should provide the amount of compute required for each of the individual   \n664 experimental runs as well as estimate the total compute.   \n665 \u2022 The paper should disclose whether the full research project required more compute   \n666 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n667 didn\u2019t make it into the paper).   \n668 9. Code Of Ethics   \n669 Question: Does the research conducted in the paper conform, in every respect, with the   \n670 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n671 Answer: [Yes]   \n672 Justification: We are carefully read through the code of ethics and to the best of our   \n673 knowledge the contributions in this paper do not violate it in any way.   \n674 Guidelines:   \n675 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n676 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n677 deviation from the Code of Ethics.   \n678 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n679 eration due to laws or regulations in their jurisdiction).   \n680 10. Broader Impacts   \n681 Question: Does the paper discuss both potential positive societal impacts and negative   \n682 societal impacts of the work performed?   \n683 Answer: [NA]   \n684 Justification: The work does not have any obvious harmful applications or any potential   \n685 negative societal impact.   \n686 Guidelines:   \n687 \u2022 The answer NA means that there is no societal impact of the work performed.   \n688 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n689 impact or why the paper does not address societal impact.   \n690 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n691 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n692 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n693 groups), privacy considerations, and security considerations.   \n694 \u2022 The conference expects that many papers will be foundational research and not tied   \n695 to particular applications, let alone deployments. However, if there is a direct path to   \n696 any negative applications, the authors should point it out. For example, it is legitimate   \n697 to point out that an improvement in the quality of generative models could be used to   \n698 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n699 that a generic algorithm for optimizing neural networks could enable people to train   \n700 models that generate Deepfakes faster.   \n701 \u2022 The authors should consider possible harms that could arise when the technology is   \n702 being used as intended and functioning correctly, harms that could arise when the   \n703 technology is being used as intended but gives incorrect results, and harms following   \n704 from (intentional or unintentional) misuse of the technology.   \n705 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n706 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n707 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n708 feedback over time, improving the efficiency and accessibility of ML).   \n709 11. Safeguards   \n710 Question: Does the paper describe safeguards that have been put in place for responsible   \n711 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n712 image generators, or scraped datasets)?   \n713 Answer: [NA]   \n714 Justification: The work does not pose any such risks.   \n715 Guidelines:   \n716 \u2022 The answer NA means that the paper poses no such risks.   \n717 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n718 necessary safeguards to allow for controlled use of the model, for example by requiring   \n719 that users adhere to usage guidelines or restrictions to access the model or implementing   \n720 safety filters.   \n721 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n722 should describe how they avoided releasing unsafe images.   \n723 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n724 not require this, but we encourage authors to take this into account and make a best   \n725 faith effort.   \n726 12. Licenses for existing assets   \n727 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n728 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n729 properly respected?   \n730 Answer: [Yes]   \n731 Justification: Where we have used or been inspired by previous work we have made sure   \n732 that we have legal permission to use it and have clearly cited it in each case.   \n733 Guidelines:   \n734 \u2022 The answer NA means that the paper does not use existing assets.   \n735 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n736 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n737 URL.   \n738 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n739 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n740 service of that source should be provided.   \n741 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n742 package should be provided. For popular datasets, paperswithcode.com/datasets   \n743 has curated licenses for some datasets. Their licensing guide can help determine the   \n744 license of a dataset.   \n745 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n746 the derived asset (if it has changed) should be provided.   \n747 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n748 the asset\u2019s creators.   \n749 13. New Assets   \n750 Question: Are new assets introduced in the paper well documented and is the documentation   \n751 provided alongside the assets?   \n752 Answer: [Yes]   \n753 Justification: The new Julia library contains well documented code which has a clear and   \n754 accessible API. All our code for all experiments and applications is released under the CC   \n755 BY 4.0 licence.   \n756 Guidelines:   \n757 \u2022 The answer NA means that the paper does not release new assets.   \n758 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n759 submissions via structured templates. This includes details about training, license,   \n760 limitations, etc.   \n761 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n762 asset is used.   \n763 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n764 create an anonymized URL or include an anonymized zip file.   \n765 14. Crowdsourcing and Research with Human Subjects   \n766 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n767 include the full text of instructions given to participants and screenshots, if applicable, as   \n768 well as details about compensation (if any)?   \n769 Answer: [NA]   \n770 Justification: This work did not involve crowdsourcing nor research with human subjects.   \n771 Guidelines:   \n772 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n773 human subjects.   \n774 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n775 tion of the paper involves human subjects, then as much detail as possible should be   \n776 included in the main paper.   \n777 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n778 or other labor should be paid at least the minimum wage in the country of the data   \n779 collector.   \n780 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n781 Subjects   \n782 Question: Does the paper describe potential risks incurred by study participants, whether   \n783 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n784 approvals (or an equivalent approval/review based on the requirements of your country or   \n785 institution) were obtained?   \n786 Answer: [NA]   \n787 Justification: This work did not involve crowdsourcing nor research with human subjects.   \n788 Guidelines:   \n789 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n790 human subjects. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]