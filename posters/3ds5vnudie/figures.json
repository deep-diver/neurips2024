[{"figure_path": "3Ds5vNudIE/figures/figures_3_1.jpg", "caption": "Figure 1: Task behavior across models and time (higher indicates a better match with expected behavior). Across tasks and scales, model abilities tend to develop at the same number of tokens.", "description": "This figure shows the performance of various sized language models (Pythia-70m, Pythia-160m, Pythia-410m, Pythia-1.4b, Pythia-2.8b, Pythia-6.9b, Pythia-12b) on four different tasks (Indirect Object Identification, Gendered Pronoun, Greater-Than, Subject-Verb Agreement) over the course of their training (measured in tokens seen).  The x-axis represents the number of tokens the model has processed, and the y-axis represents the model's performance on the task.  The plot shows that despite the significant differences in model size, the models tend to achieve peak performance at roughly the same number of tokens processed across various tasks, demonstrating a similarity in task acquisition rate across different scales.", "section": "3 Circuit Formation"}, {"figure_path": "3Ds5vNudIE/figures/figures_5_1.jpg", "caption": "Figure 2: The development of components relevant to IOI and Greater-Than, across models and time. Each line indicates the degree to which attention heads in the circuit at each timestep exhibit the relevant component behavior. The timesteps at which component behavior emerges parallel those at which task performance emerges in Figure 1.", "description": "This figure shows the emergence of four attention head types (Successor, Induction, Copy Suppression, and Name Mover Heads) over the course of training in five different sized models.  The y-axis represents the component score (a measure of how strongly each head exhibits the given behavior) and the x-axis represents the number of tokens seen during training.  The figure demonstrates that these components tend to emerge at similar points in training across all model sizes, mirroring the timing of task acquisition shown in Figure 1.", "section": "3.2 Component Emergence"}, {"figure_path": "3Ds5vNudIE/figures/figures_6_1.jpg", "caption": "Figure 2: The development of components relevant to IOI and Greater-Than, across models and time. Each line indicates the degree to which attention heads in the circuit at each timestep exhibit the relevant component behavior. The timesteps at which component behavior emerges parallel those at which task performance emerges in Figure 1.", "description": "This figure shows the development of four key attention head components (Successor Heads, Induction Heads, Copy Suppression Heads, and Name Mover Heads) over time and across different model sizes.  Each line represents a specific attention head within a model's circuit, showing the degree to which it exhibits the behavior characteristic of that component type.  The x-axis represents the number of tokens seen during training, while the y-axis shows the component score (a metric quantifying the degree of component behavior). The figure demonstrates that the emergence of these components closely parallels the emergence of task performance (as shown in Figure 1), suggesting a strong relationship between the presence of these components and the model's ability to perform the IOI and Greater-Than tasks.  It highlights the consistency of component emergence across model scales.", "section": "Component Emergence"}, {"figure_path": "3Ds5vNudIE/figures/figures_7_1.jpg", "caption": "Figure 4: A: Pythia-160m's IOI circuit at the end of training (300B tokens). The remaining plots show the percent of model IOI performance that is explained by the Copy Suppression and Name-Mover Heads (B), the S-Inhibition Heads' edges to those heads (C), and the Induction / Duplicate Token Heads' connections to the S-Inhibition heads (D); higher percentages indicate that the corresponding edge is indeed important. Each of plots B-D verifies the importance of an edge from diagram A.", "description": "The figure shows the IOI circuit for the Pythia-160m model at the end of its training, along with three additional plots. Plot A is a visual representation of the circuit, highlighting the various attention heads involved and their connections. Plots B, C, and D quantitatively analyze the contribution of different circuit components to the overall model performance. Specifically, plot B assesses the importance of the Copy Suppression and Name-Mover Heads, plot C examines the impact of S-Inhibition Heads, and plot D analyzes the role of Tertiary (Induction/Duplicate Token) Heads. These plots provide a detailed breakdown of how various parts of the IOI circuit contribute to the task.", "section": "Circuit Algorithm Stability Over Training"}, {"figure_path": "3Ds5vNudIE/figures/figures_8_1.jpg", "caption": "Figure 5: Exponentially-weighted moving average Jaccard similarity for circuit node sets over training token count. In general, larger models tend to have both higher average EWMA-JS and fewer abrupt fluctuations, indicating higher stability in the circuit constituents.", "description": "This figure displays the exponentially weighted moving average Jaccard similarity for the circuit node sets over training token counts for different sized models.  The Jaccard similarity measures the overlap between the circuit nodes at a given checkpoint and those at all previous checkpoints. The exponential weighting smooths out short-term fluctuations to reveal longer-term trends.  Larger models exhibit higher average Jaccard similarity and fewer sharp fluctuations, indicating greater stability and consistency in their circuit composition over the course of training.", "section": "Graph-Level Circuit Analysis"}, {"figure_path": "3Ds5vNudIE/figures/figures_22_1.jpg", "caption": "Figure 1: Task behavior across models and time (higher indicates a better match with expected behavior). Across tasks and scales, model abilities tend to develop at the same number of tokens.", "description": "This figure displays the performance of different sized language models (70M to 12B parameters) on four different tasks over the course of 300 billion tokens of training.  Each line represents a different model size. The x-axis represents the number of tokens processed during training, and the y-axis shows the model's performance on each task.  The key observation is that despite differences in model size, the models reach peak performance at roughly the same number of training tokens.  This suggests that the acquisition of task abilities happens at similar stages of training regardless of model scale.", "section": "3 Circuit Formation"}, {"figure_path": "3Ds5vNudIE/figures/figures_22_2.jpg", "caption": "Figure 1: Task behavior across models and time (higher indicates a better match with expected behavior). Across tasks and scales, model abilities tend to develop at the same number of tokens.", "description": "This figure shows the performance of different sized language models (Pythia-70m, Pythia-160m, Pythia-410m, Pythia-1.4b, Pythia-2.8b, Pythia-6.9b, Pythia-12b) on four different tasks (Indirect Object Identification, Gendered Pronoun, Greater-Than, Subject-Verb Agreement) over the course of their training (300 billion tokens).  The x-axis represents the number of tokens seen during training, and the y-axis represents the model's performance on each task, with higher values indicating better performance. The figure demonstrates that across different model sizes and tasks, the models achieve peak performance at similar token counts, suggesting a consistency in learning behavior.", "section": "3 Circuit Formation"}, {"figure_path": "3Ds5vNudIE/figures/figures_23_1.jpg", "caption": "Figure 1: Task behavior across models and time (higher indicates a better match with expected behavior). Across tasks and scales, model abilities tend to develop at the same number of tokens.", "description": "This figure displays the performance of various sized language models on four different tasks over the course of their training.  The x-axis represents the number of tokens processed during training, while the y-axis shows the model's performance on each task.  The figure demonstrates that regardless of model size (parameters), models tend to reach peak performance at a similar number of training tokens.  This suggests a consistency in how these models acquire task abilities, indicating that model scaling does not necessarily lead to drastically faster learning.", "section": "3 Circuit Formation"}, {"figure_path": "3Ds5vNudIE/figures/figures_25_1.jpg", "caption": "Figure 1: Task behavior across models and time (higher indicates a better match with expected behavior). Across tasks and scales, model abilities tend to develop at the same number of tokens.", "description": "This figure displays the performance of different sized language models on four different tasks over the course of their training.  Each line represents a different model size, from 70 million to 12 billion parameters. The x-axis represents the number of tokens processed during training, and the y-axis represents the model's performance on each task.  The key observation is that despite the differences in model size, the models achieve similar levels of performance on each task at roughly the same number of training tokens. This suggests a degree of consistency in how models learn these tasks regardless of scale.", "section": "3 Circuit Formation"}]