[{"figure_path": "rcch4UsMBi/tables/tables_2_1.jpg", "caption": "Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam benchmarks. Best results are in boldface, while the second best results are underscored.", "description": "This table presents the performance of various large language models (LLMs) across four categories of benchmarks: Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exams.  The models are evaluated using several datasets within each category, and the results show the accuracy or success rate (percentage) achieved by each model on each dataset.  The table highlights the best-performing models in bold and the second-best models with underlines, providing a clear comparison of different LLMs' capabilities.", "section": "3 Experiments"}, {"figure_path": "rcch4UsMBi/tables/tables_5_1.jpg", "caption": "Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam benchmarks. Best results are in boldface, while the second best results are underscored.", "description": "This table presents the performance of various large language models (LLMs) across a range of benchmark tasks, categorized into four areas: mathematical reasoning, coding, logical reasoning, and academic exams.  The results are shown as percentages.  The table includes both general-purpose LLMs (e.g., GPT-3.5-turbo, LLaMA2) and models specifically tuned for particular tasks (e.g., MetaMath Mistral, WizardMath).  The model GLAN, introduced in the paper, is also included for comparison.", "section": "3 Experiments"}, {"figure_path": "rcch4UsMBi/tables/tables_6_1.jpg", "caption": "Table 2: Detailed Results on Academic Exam benchmarks.", "description": "This table presents the detailed results of the academic exam benchmarks, specifically focusing on the performance of Mistral and GLAN across various sub-categories within the MMLU benchmark (STEM, Humanities, Social Sciences, and Other).  It offers a more granular view of the results presented in Table 1, allowing for a more nuanced comparison of the two models' capabilities in different academic areas.  Pass@1 metrics are provided for each benchmark. ", "section": "3.3 Benchmark Evaluation"}, {"figure_path": "rcch4UsMBi/tables/tables_7_1.jpg", "caption": "Table 3: The evaluation of loss values between the test data and training data. Large positive \u0394 (or \u25b2(%)) indicates task-specific in-domain training data might be exposed to the model during training.", "description": "This table shows the difference in loss values between test and training data for several models, including LLAMA2-7B, Orca2-7B, Mistral-7B-Instruct, WizardLM-13B-V1.2, and GLAN-7B across different benchmarks (ARC-C, ARC-E, GSM8K, and MATH). A large positive difference suggests that the model may have been exposed to task-specific training data.  The table helps assess if the models learned task-specific information during training instead of generalizing from the data generated by GLAN.", "section": "3.5 Task-specific Training Data"}, {"figure_path": "rcch4UsMBi/tables/tables_7_2.jpg", "caption": "Table 4: Instruction following capability evaluation on IFEval.", "description": "This table presents the instruction following capabilities of various language models evaluated on the IFEval benchmark.  It shows the performance of each model in terms of prompt-level and instruction-level strict accuracy, as well as instruction-level loose accuracy.  Higher scores indicate better performance in instruction following. The models compared include GPT-3.5-turbo, GPT-4, LLaMA2-7B, Orca2-7B, Mistral-7B-Instruct-v0.1, WizardLM-13B-V1.2, and GLAN-7B.", "section": "3.6 Instruction Following Evaluation"}, {"figure_path": "rcch4UsMBi/tables/tables_8_1.jpg", "caption": "Table 5: Pairwise comparison on various difficulty levels between GLAN and other models on Evol-Instruct testset. The scores are the average gap of scores assigned by GPT-4, calculated as avg_score(GLAN) - avg_score(x).", "description": "This table presents a pairwise comparison of GLAN's performance against other models on the Evol-Instruct test set, categorized by difficulty levels (easy and hard).  The scores represent the difference in average scores assigned by GPT-4 for GLAN and each comparative model.  A positive score indicates superior performance of GLAN.", "section": "3.6 Instruction Following Evaluation"}, {"figure_path": "rcch4UsMBi/tables/tables_13_1.jpg", "caption": "Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam benchmarks. Best results are in boldface, while the second best results are underscored.", "description": "This table presents the performance of various large language models (LLMs) across multiple benchmark tasks.  The benchmarks are categorized into four areas: Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exams.  For each model, the table shows the results obtained on each benchmark, with the best and second-best results highlighted.  The models include several state-of-the-art LLMs and GLAN, the model proposed in the paper.  The goal is to compare GLAN's performance against existing models on diverse tasks, demonstrating its ability to generalize to unseen data.", "section": "3 Experiments"}, {"figure_path": "rcch4UsMBi/tables/tables_14_1.jpg", "caption": "Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam benchmarks. Best results are in boldface, while the second best results are underscored.", "description": "This table presents the performance of various language models across four categories of benchmarks: Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exams.  The models are evaluated on several datasets within each category and the best and second-best results are highlighted.  The table provides a comparison of different model architectures and their abilities across different tasks, demonstrating the relative strengths and weaknesses of each approach.", "section": "3 Experiments"}, {"figure_path": "rcch4UsMBi/tables/tables_14_2.jpg", "caption": "Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam benchmarks. Best results are in boldface, while the second best results are underscored.", "description": "This table presents the performance of various large language models (LLMs) across four categories of benchmark tasks: Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exams.  The results show the percentage accuracy of each model on each benchmark, allowing for a comparison of their relative strengths and weaknesses in different areas.  The best and second-best results for each benchmark are highlighted.", "section": "3.3 Benchmark Evaluation"}, {"figure_path": "rcch4UsMBi/tables/tables_15_1.jpg", "caption": "Table 11: Pairwise comparison between GLAN and other models on GLAN-Test (the 126 disciplines are categorized into 8 fields for clarity of the illustration). The scores are the average gap of scores assigned by GPT-4, calculated as avg_score(GLAN) - avg_score(x).", "description": "This table presents a pairwise comparison of GLAN's performance against other models across 126 disciplines categorized into eight fields.  The scores represent the average difference in scores assigned by GPT-4, indicating the relative advantage of GLAN. A positive score suggests GLAN outperforms the other model in that field.", "section": "3.4 Scaling Property of GLAN"}, {"figure_path": "rcch4UsMBi/tables/tables_16_1.jpg", "caption": "Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam benchmarks. Best results are in boldface, while the second best results are underscored.", "description": "This table presents the performance of various large language models (LLMs) on several benchmarks assessing different capabilities: mathematical reasoning (GSM8K and MATH), coding (HumanEval and MBPP), logical reasoning (BBH), and academic exam performance (ARC-E, ARC-C, and MMLU).  The models are compared using their respective scores on each benchmark, with the best-performing model highlighted in bold and the second-best underscored.  This allows for a comparison of overall performance across diverse tasks.", "section": "3.3 Benchmark Evaluation"}, {"figure_path": "rcch4UsMBi/tables/tables_17_1.jpg", "caption": "Table 1: Main results on Mathematical Reasoning, Coding, Logical Reasoning, and Academic Exam benchmarks. Best results are in boldface, while the second best results are underscored.", "description": "This table presents the performance of various large language models (LLMs) on several benchmark tests, including mathematical reasoning, coding, logical reasoning, and academic exams.  The results highlight the effectiveness of the proposed Generalized Instruction Tuning (GLAN) method in improving the performance of LLMs across multiple tasks.", "section": "3 Experiments"}]