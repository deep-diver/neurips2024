[{"type": "text", "text": "Quantum Algorithms for Non-smooth Non-convex Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chengchang Liu\\* 1 Chaowen Guan\\* 2 Jianhao He# 1 John C.S. Lui 1 ", "page_idx": 0}, {"type": "text", "text": "The Chinese University of Hong Kong 2 University of Cincinnati 7liuchengchang@gmail.com guance@ucmail.uc.edu jianhaohe9@cuhk.edu.hk cslui@cse.cuhk.edu.hk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper considers the problem for finding the $(\\delta,\\epsilon)$ -Goldstein stationary point of Lipschitz continuous objective, which is a rich function class to cover a great number of important applications. We construct a novel zeroth-order quantum estimator for the gradient of the smoothed surrogate. Based on such estimator, we propose a novel quantum algorithm that achieves a query complexity of $\\tilde{\\mathcal{O}}(d^{3/2}\\dot{\\delta}^{-1}\\dot{\\epsilon}^{-3})$ on the stochastic function value oracle, where $d$ is the dimension of the problem. We also enhance the query complexity to $\\tilde{\\mathcal{O}}(d^{3/2}\\delta^{-1}\\epsilon^{-7/3})$ by introducing a variance reduction variant. Our findings demonstrate the clear advantages of utilizing quantum techniques for non-convex non-smooth optimization, as they outperform the optimal classical methods on the dependency of $\\epsilon$ by a factor of \u01eb\u22122/3. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study the following problem ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathbb{R}^{d}}\\big\\{f(\\mathbf{x})\\triangleq\\mathbb{E}_{\\xi}\\left[F(\\mathbf{x};\\xi)\\right]\\big\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where the stochastic component $F(\\mathbf{x};\\boldsymbol{\\xi})$ is $L$ -Lipschitz continuous but possibly non-convex and nonsmooth. Such problem receives growing attention recently since it is general enough to cover many important applications including deep neural networks [21, 40], reinforcement learning [9, 49], and statistical learning [17, 39, 62]. ", "page_idx": 0}, {"type": "text", "text": "Due to the absence of both smoothness and convexity in the objective function, neither the gradient nor the sub-differentials are valid anymore to measure the convergence behaviour. Clarke sub-differential is a natural extension for describing the first-order information of the Lipschitz continuous function [10], however, it is intractable for finding the near-approximate stationary point in terms of the Clarke sub-differential as suggested by the hard instances [31, 50, 63]. Zhang et al. [63] introduce the notion of $(\\delta,\\epsilon)$ -Goldstein stationary point (cf. Section 2.2), which weakens the traditional stationary point by considering the convex hull of the Clarke sub-differentials. Following this, we focus on the problem of finding the $(\\delta,\\epsilon)$ -Goldstein stationary points of the objective. ", "page_idx": 0}, {"type": "text", "text": "There are great many optimization methods for finding the $(\\delta,\\epsilon)$ -Goldstein stationary points via stochasic classical oracles [6, 14, 28, 31, 35, 47, 52, 63]. Zhang et al. [63] proposed stochastic interpolated normalized gradient descent method (SINGD) with the first non-asymptotic result, which has the stochastic first-order complexity of $\\mathcal{O}(\\delta^{-1}\\epsilon^{-4})$ . Later on, Tian et al. [52] developed the perturbed SINGD method which queries the gradient at the differentiable point and established the ", "page_idx": 0}, {"type": "table", "img_path": "", "table_caption": ["Table 1: We summarize the complexities of classical and quantum zeroth-order methods for finding the $(\\epsilon,\\delta)$ -Goldstein point of a non-smooth non-convex objective, where $d$ is the dimension of the problem. "], "table_footnote": [], "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbf{\\overrightarrow{Methods}}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathbf{Qracle}\\,\\,\\,\\,\\,\\,\\,\\,\\mathbf{Query}\\,\\,\\mathbf{Complexity}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathbf{Reference}}{\\mathbf{\\overrightarrow{GFM}}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathrm{classical}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathcal{O}\\left(d^{3/2}\\delta^{-1}\\epsilon^{-4}\\right)}}\\\\ &{\\frac{\\mathbf{\\overrightarrow{GFM}}+\\mathbf{\\overrightarrow{\\mathrm{\\Pi}\\,\\,\\,\\,\\,\\,c l a s s i c a l}}\\,\\,\\,\\,\\,\\,\\,\\,\\mathcal{O}\\left(d^{3/2}\\delta^{-1}\\epsilon^{-3}\\right)}{\\mathbf{\\overrightarrow{Qptimal}}\\,\\,\\,\\,\\,\\,\\,\\mathrm{classical}}}\\\\ &{\\frac{\\mathbf{\\overrightarrow{Qptimal}}\\,\\,\\,\\,\\,\\mathrm{classical}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathcal{O}\\left(d\\delta^{-1}\\epsilon^{-3}\\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathrm{Kornowski~and~Shamir~[32]}}{\\mathbf{\\overrightarrow{QGFM}}\\,\\,\\,\\,\\,\\,\\,\\mathrm{quantum}\\,\\,\\,\\,\\,\\,\\,\\tilde{\\mathcal{O}}\\left(d^{3/2}\\delta^{-1}\\epsilon^{-3}\\right)}}\\\\ &{\\mathrm{\\small\\texttt{\\small\\texttt{QGFM}}\\,\\,\\,\\,\\,\\,\\,\\,\\,q u a n t u m}\\,\\,\\,\\,\\,\\,\\,\\,\\tilde{\\mathcal{O}}\\left(d^{3/2}\\delta^{-1}\\epsilon^{-7/3}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "table", "img_path": "wsGzvhnoaX/tmp/d15a747a9fabde2fe8fa311643dee89e6eca28073bbc848c350fbdb66968af3b.jpg", "table_caption": ["Table 2: We summarize the complexities of classical and quantum first-order methods for finding the $\\epsilon$ -stationary point of a smooth non-convex objective, where $d$ is the dimension of the problem. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "same complexity. Cutkosky et al. [13] improved the stochastic first-order oracle complexities to $\\mathcal{O}(\\delta^{-1}\\epsilon^{-3})$ by using the \u201conline to non-convex conversation\u201d, assuming $f(\\cdot)$ is differentiable. This improvement aligns with the theoretical lower bound [13]. ", "page_idx": 1}, {"type": "text", "text": "Zeroth-order methods, which only query the function value oracle, are more practical for the Lipschitz continuous objective. This is because computing the first-order oracles can be extremely challenging [29, 52] or even inaccessible for numerous real-world applications [16, 27, 43]. Lin et al. [35] proposed a gradient-free method for finding the $(\\delta,\\epsilon)$ -Goldstein stationary point within $\\mathcal{O}(d^{3/2}\\delta^{-1}\\epsilon^{-4})$ query complexity to the stochastic function value via a connection between the randomized smoothing [41] and the Goldstein stationary point. This complexity was further improved to $\\mathcal{O}(d^{3/2}\\delta^{-1}\\epsilon^{-3})$ and $\\mathcal{O}(d\\delta^{-1}\\epsilon^{-3})$ by Chen et al. [6], Kornowski and Shamir [32] respectively. However, all these methods using the classical oracles for finding the Goldstein stationary point face a bottleneck of $\\delta^{-1}\\epsilon^{-3}$ due to the lower bound reported by [13]. ", "page_idx": 1}, {"type": "text", "text": "Recently, we have witnessed the power of the quantum optimization methods by accessing the quantum counterparts of the classical oracles for non-convex optimization [7, 23, 37, 48, 61, 64], convex optimization [4, 5, 48, 55, 64], and semi-definite programming [1, 2, 53, 54]. However, most of these results focus on the deterministic methods and the case that the objective function is smooth. Garg et al. [19] and Zhang and Li [60] showed the negative results for non-smooth convex and smooth non-convex optimization that quantum algorithms have no improved rates over the classical ones when the dimension is large. Sidford and Zhang [48] proposed stochastic quantum methods which show the advantage of using quantum stochastic first-order oracles for smooth objectives when the dimension is relatively small. To the best of our knowledge, there have no work that shows the quantum speedups for minimizing non-smooth non-convex objectives, which is the most general and fundamental function class. Built upon this, it is a natural question to ask: ", "page_idx": 1}, {"type": "text", "text": "Can we go beyond the complexity of $\\mathcal{O}(\\delta^{-1}\\epsilon^{-3})$ for finding the $(\\delta,\\epsilon)$ -Goldstein stationary point for stochastic non-smooth non-convex optimization by involving quantum oracles? ", "page_idx": 1}, {"type": "text", "text": "We give an affirmative answer to the above question by proposing novel quantum zeroth-order methods and showing their explicit query complexities. We summarize our contributions as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We construct efficient quantum gradient estimators for the smoothed surrogate of the objectives with $\\mathcal{O}(1)$ -queries of the function value oracles, which allows us to construct efficient quantum zeroth-order methods. Moreover, we provide explicit constructions of quantum superposition over required distributions. We present these results in Section 3 and Appendix A. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose the quantum gradient-free method (QGFM) and fast quantum gradient-free method $\\mathrm{(QGFM+)}$ for non-smooth non-convex optimization. We achieve the query complexities of $\\mathcal{O}(d^{3/2}\\delta^{-1}\\epsilon^{-3})$ for QGFM and $\\mathcal{O}(d^{3/2}\\delta^{-1}\\epsilon^{-7/3})$ for $\\mathrm{QGFM+}$ in finding the $(\\delta,\\epsilon)$ -Goldstein stationary point using quantum stochastic function value oracle. The query complexity of $\\mathrm{QGFM+}$ surpasses the optimal result achieved by classical methods by a factor of $\\epsilon^{-2/3}$ . We compare our methods with the classical zeroth-order methods in Table 1 and present the results in Section 4. \u2022 We generalize the algorithm framework of $\\mathrm{QGFM+}$ for smooth non-convex optimization (i.e. the gradient of the objective function is Lipschitz continuous). We propose the fast quantum gradient method $\\mathrm{(QGM+)}$ , which takes the advantage of $\\mathrm{QGFM+}$ to choose the variance level adaptively. $\\mathrm{QGM+}$ enjoys an improved complexity of $\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-7/3})$ queries of the quantum stochastic gradient oracle, which outperforms the existing state-of-the-art method (Q-SPIDER [48]) by a factor of $\\epsilon^{-1/6}$ . We compare our method with the classical and quantum first-order methods in Table 2. A discussion on this is presented in Remark 4.5, and the formal results are stated in Appendix G. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce preliminaries for quantum computing model and non-smooth non-convex optimization in this section. ", "page_idx": 2}, {"type": "text", "text": "2.1 Preliminaries for Quantum Computing Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Here we formally review the basics and some concepts from quantum computing that we work with.   \nFor more details, please refer to Nielsen and Chuang [42]. ", "page_idx": 2}, {"type": "text", "text": "Quantum Basics. A quantum state can be seen as a vector $\\mathbf{x}=(x_{1},x_{2},\\ldots,x_{m})^{\\intercal}$ in Hilbert space $\\mathcal{H}^{m}$ such that $\\begin{array}{r}{\\sum_{i}\\left|x_{i}\\right|^{2}=1}\\end{array}$ . We follow the Dirac bra/ket notation on quantum states, i.e., we denote the quantum state for $\\mathbf{x}$ by $|\\mathbf{x}\\rangle$ and denote $\\mathbf{x}^{\\dagger}$ by $\\langle\\mathbf{x}|$ , where $\\dagger$ means the Hermitian conjugation. ", "page_idx": 2}, {"type": "text", "text": "Given a state $\\begin{array}{r}{|\\psi\\rangle=\\sum_{i=1}^{m}c_{i}|i\\rangle}\\end{array}$ , we call $c_{i}\\in\\mathbb{C}$ the amplitude of the state $|i\\rangle$ . Given two quantum states $|\\mathbf{x}\\rangle\\in\\mathcal{H}^{m}$ and $|\\mathbf{y}\\rangle\\in\\mathcal{H}^{m}$ , we denote their inner product by $\\begin{array}{r}{\\langle\\mathbf{x}|\\mathbf{y}\\rangle\\,\\triangleq\\,\\sum_{i}x_{i}^{\\dagger}y_{i}}\\end{array}$ . Given $|\\mathbf{x}\\rangle\\in\\mathcal{H}^{m}$ and $\\vert\\mathbf{y}\\rangle\\in\\mathcal{H}^{n}$ , we denote their tensor product by $|\\mathbf{x}\\rangle\\otimes|\\mathbf{y}\\rangle\\triangleq(x_{1}y_{1},\\cdots,x_{m}y_{n})^{\\intercal}\\in\\mathcal{H}_{\\quad\\circ}^{m\\times n}$ . If we measure state $\\begin{array}{r}{|\\psi\\rangle=\\sum_{i=1}^{m}c_{i}|i\\rangle}\\end{array}$ in computational basis, we will obtain $i$ with probability $|c_{i}|^{2}$ and the state will collapse into $|i\\rangle$ after measurement, for all $i$ . A quantum algorithm works by applying a sequence of unitary operators to a initial quantum state. ", "page_idx": 2}, {"type": "text", "text": "Quantum Query Complexity. Corresponding to the classical query model, quantum query complexity considers the number of querying a black box of a particular function which needs to be invoked in order to solve a problem. In many cases, the black box corresponds to the process that has the highest overhead, and therefore reducing the number of queries to it will effectively reduce the computational complexity of the entire algorithm. For example, if a classical oracle $\\mathbf{C}_{f}$ for a function $f$ is a black box that, when queried with a point $\\mathbf{x}$ , outputs the function value $\\mathbf{C}_{f(\\mathbf{x})}=f(\\mathbf{x})$ , then the corresponding quantum oracle $\\mathbf{U}_{f}$ is a unitary transformation that maps a quantum state $\\left|\\mathbf{x}\\right\\rangle\\left|q\\right\\rangle$ to the state $\\left|\\mathbf{x}\\right\\rangle\\left|q+f(\\mathbf{x})\\right\\rangle$ . Moreover, given the superposition input $\\begin{array}{r}{\\sum_{\\mathbf{x},q}\\alpha_{\\mathbf{x},q}\\left|\\mathbf{x}\\right\\rangle\\left|q\\right\\rangle}\\end{array}$ , applying the quantum oracle once will, by linearity, output the quantum state $\\begin{array}{r}{\\sum_{\\mathbf{x},q}\\alpha_{\\mathbf{x},q}\\left|\\mathbf{x}\\right\\rangle\\left|q+f(\\mathbf{x})\\right\\rangle}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Preliminaries for Non-convex Non-smooth Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce the necessary background for non-convex non-smooth optimization, with the following mild assumption that the objective function is Lipschitz continuous. ", "page_idx": 2}, {"type": "text", "text": "Assumption 1. We assume the stochastic component $F(\\cdot;\\xi)$ of the objective $f(\\cdot)$ satisfies that $|F(\\mathbf{x};\\boldsymbol{\\xi})-F(\\mathbf{y};\\boldsymbol{\\xi})|\\,\\le\\,L\\|\\mathbf{x}-\\mathbf{y}\\|$ for every $\\mathbf{x},\\mathbf{y}\\,\\in\\,\\mathbb{R}^{d}$ . Besides, we assume $f:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ is lower bounded and denote $f^{*}\\triangleq\\operatorname*{inf}_{\\mathbf{x}\\in\\mathbb{R}^{d}}f(\\mathbf{x})$ . ", "page_idx": 2}, {"type": "text", "text": "The Rademencher\u2019s theorem indicates that $f(\\cdot)$ is differentiable almost everywhere under Assumption 1, which allows us to define its Clarke sub-differential as follows [10]. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Clarke sub-differential). The Clarke sub-differential of a Lipschitz function at point x is defined by $\\begin{array}{r}{\\partial f(\\mathbf{x})\\triangleq\\operatorname{conv}\\left\\lbrace\\mathbf{g}:\\mathbf{g}=\\operatorname*{lim}_{\\mathbf{x}_{k}\\to\\mathbf{x}}\\nabla f(\\mathbf{x}_{k})\\right\\rbrace}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "We then introduce the Goldstein sub-differential [22] and the $(\\delta,\\epsilon)$ -Goldstein stationary point [63]. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Goldstein sub-differential). The Goldstein sub-differential of a Lipschitz function at point x is defined by $\\partial_{\\delta}f(\\mathbf{x})\\triangleq\\operatorname{conv}\\left\\{\\cup_{\\mathbf{y}\\in\\mathbf{B}_{\\delta}(\\mathbf{x})}\\partial f(\\mathbf{y})\\right\\}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3 $(\\delta,\\epsilon)$ -Goldstein stationary point). We call $\\mathbf{x}$ the $(\\delta,\\epsilon)$ -Goldstein stationary point of a given Lipschitz function if it satisfies that dist $(0,\\partial_{\\delta}f(\\mathbf{x}))\\,\\le\\,\\epsilon$ , where $\\partial_{\\delta}f(\\mathbf{x})$ is the Goldstein sub-differential. ", "page_idx": 3}, {"type": "text", "text": "Next we define the smoothed surrogate of $f(\\cdot)$ as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4 $\\boldsymbol{\\delta}$ -smoothed surrogate). The $\\delta$ -smoothed surrogate of $f$ is defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\delta}(\\mathbf{x})\\triangleq\\mathbb{E}_{\\mathbf{w}\\sim\\mathcal{P}}\\left[f(\\mathbf{x}+\\delta\\mathbf{w})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{P}$ is the uniform distribution on a unit ball. ", "page_idx": 3}, {"type": "text", "text": "Although $f(\\cdot)$ is non-smooth, its smoothed surrogate $f_{\\delta}(\\cdot)$ enjoys some good properties as presented in the following proposition [6, 15, 35, 59]. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1. If $f(\\cdot)$ satisfies Assumption $^{\\,I}$ , its smoothed surrogate $f_{\\delta}(\\cdot)$ satisfies that: ", "page_idx": 3}, {"type": "text", "text": "\u2022 $|f_{\\delta}(\\cdot)-f(\\cdot)|\\leq\\delta L\\ \\ a n d\\ \\ |f_{\\delta}(\\mathbf{x})-f_{\\delta}(\\mathbf{y})|\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|.$   \n\u2022 $\\nabla f_{\\delta}(\\cdot)\\;i s\\;c\\sqrt{d}L\\delta^{-1}$ -Lipschitz for some constant $c>0$ , i.e. $\\left\\|\\nabla f_{\\delta}(\\mathbf{x})-\\nabla f_{\\delta}(\\mathbf{y})\\right\\|\\leq c{\\sqrt{d}}L\\|\\mathbf{x}-\\mathbf{y}\\|$ . \u2022 $\\nabla f_{\\delta}(\\cdot)\\in\\partial_{\\delta}f(\\cdot)$ , where $\\partial_{\\delta}f(\\cdot)$ is the Goldstein sub-differential. ", "page_idx": 3}, {"type": "text", "text": "Remark 2.2. Proposition 2.1 implies that the task of finding the $(\\delta,\\epsilon)$ -Goldstein stationary point of $f(\\cdot)$ is equivalent to finding the $\\epsilon$ -stationary point of a smoothed function $f_{\\delta}(\\cdot)$ , i.e. finding some point $\\mathbf{x}$ such that $\\|\\nabla f_{\\delta}(\\mathbf{x})\\|\\leq\\epsilon$ . ", "page_idx": 3}, {"type": "text", "text": "3 Zeroth-order Based Stochastic Quantum Estimator ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present a novel quantum estimator for the gradient of the smoothed surrogate $f_{\\delta}(\\cdot)$ by using the quantum stochastic function value oracle, which is essential for designing our quantum algorithms for non-convex non-smooth optimization. ", "page_idx": 3}, {"type": "text", "text": "3.1 Quantum Estimators via Quantum Stochastic Function Value Oracle ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we construct quantum estimators for the gradient of the smoothed surrogate by $\\mathcal{O}(1)$ - queries of quantum stochastic function value oracle. ", "page_idx": 3}, {"type": "text", "text": "We start with the definition of the stochastic function value oracle. Classically, a stochastic function value evaluation is defined as $F(\\mathbf{x},\\boldsymbol{\\xi})$ for a function $f\\,:\\,\\mathbb{R}^{d}\\;\\rightarrow\\;\\mathbb{R}$ with $\\xi$ being such that $\\mathbb{E}_{\\xi}[F(\\mathbf{x},\\xi)]\\,=\\,f(\\mathbf{x})$ . In this work, we assume the access of a quantum stochastic function value oracle $\\mathbf{U}_{F}$ for $f(\\cdot)$ , which is defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Quantum stochastic function value oracle). For $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R},$ , the quantum stochastic function value oracle, denoted by $\\mathbf{U}_{F}$ , works as: $\\mathbf{U}_{F}:\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\xi\\right\\rangle\\otimes\\left|b\\right\\rangle\\longmapsto\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\xi\\right\\rangle\\otimes\\left|b+F(\\mathbf{x},\\xi)\\right\\rangle$ , where $F(\\mathbf{x},\\boldsymbol{\\xi})$ is sampled from a distribution $p_{\\xi}(\\cdot)$ such that $\\mathbb{E}_{\\xi}[F(\\mathbf{x};\\xi)]=F(\\mathbf{x})$ . ", "page_idx": 3}, {"type": "text", "text": "It is common to construct the following stochastic gradient estimator for $\\nabla f_{\\delta}(\\cdot)$ [6, 32, 35, 36, 41]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})\\triangleq\\frac{d}{2\\delta}\\left(F(\\mathbf{x}+\\delta\\mathbf{w};\\boldsymbol{\\xi})-F(\\mathbf{x}-\\delta\\mathbf{w};\\boldsymbol{\\xi})\\right)\\cdot\\mathbf{w},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\textbf{w}\\in\\mathbb{R}^{d}$ is uniformly distributed on a unit sphere. The following proposition shows that $\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})$ is a good estimator of $\\nabla f_{\\delta}(\\cdot)$ . ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1 ([6, Proposition 3 and 4]). Under Assumption $^{\\,l}$ , i.e. the random variable $\\xi$ satisfies that ", "page_idx": 3}, {"type": "equation", "text": "$$\n|F(\\mathbf{x};\\boldsymbol{\\xi})-F(\\mathbf{y};\\boldsymbol{\\xi})|\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|\\ \\ a n d\\ \\ \\mathbb{E}_{\\boldsymbol{\\xi}}[F(\\mathbf{x};\\boldsymbol{\\xi})]=f(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "hold for all x, $\\mathbf{y}\\in\\mathbb{R}^{d}$ , then $\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})$ defined in eq. (3) satisfies that $\\mathbb{E}_{\\mathbf{w},\\boldsymbol{\\xi}}\\big[\\mathbf{g}_{\\boldsymbol{\\delta}}\\big(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi}\\big)\\big]=\\nabla f_{\\boldsymbol{\\delta}}\\big(\\mathbf{x}\\big),$ , $\\mathbb{E}_{\\mathbf{w},\\xi}\\big[\\|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\xi)-\\nabla f_{\\delta}(\\mathbf{x})\\|^{2}\\big]\\leq c\\pi d L^{2}$ , and $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{w},\\xi}[\\|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\xi)-\\mathbf{g}_{\\delta}(\\mathbf{y};\\mathbf{w},\\xi)\\|^{2}\\ \\leq\\frac{d^{2}L^{2}}{\\delta^{2}}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}}\\end{array}$ , where $c=16\\sqrt{2}\\pi$ . ", "page_idx": 4}, {"type": "text", "text": "Next, to exploit the power of quantum algorithms, we generalize eq. (3) to its quantum counterpart. Building on eq. (3) and Proposition 3.1, $\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})$ can be interpreted as a random variable. In the quantum setting, accessing a random variable typically involves querying a quantum sampling oracle, which returns a quantum superposition over the associated distribution. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.2 (Quantum sampling oracle). For a random variable $X$ with sample space $\\Omega$ , its quantum sampling oracle $\\mathbf{O}_{X}$ is defined as $\\begin{array}{r}{\\mathbf{O}_{X}:|0\\rangle\\longmapsto\\sum_{\\mathbf{x}}\\sqrt{\\mathrm{Pr}[X=\\mathbf{x}]}|\\mathbf{x}\\rangle\\otimes|\\psi_{\\mathbf{x}}\\rangle}\\end{array}$ , where $|\\psi_{\\mathbf{x}}\\rangle$ is an arbitrary quantum state for every x. ", "page_idx": 4}, {"type": "text", "text": "The content in second quantum register can also be viewed as possible quantum garbage appeared during the implementation of the oracle. Observe that if we directly measure the output of $\\mathbf{O}_{X}$ , it will collapse to a classical sampling access to $X$ that returns a random sample $\\mathbf{x}$ with respect to probability $\\mathrm{Pr}[X=\\mathbf{x}]$ . Note that the output of $\\mathbf{O}_{X}$ can be represented as integral over continuous random variables as well, as used in [8, 48]. ", "page_idx": 4}, {"type": "text", "text": "Hence, based on our observation that $\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})$ can be viewed as a random variable, our target oracle $\\mathbf{O}_{\\mathbf{g}_{\\delta}}$ \u2013quantum stochastic gradient oracle\u2013is essentially a quantum sampling oracle. Given upon this, we formally define the quantum $\\delta$ -estimated stochastic gradient oracle as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.3 (Quantum $\\delta$ -estimated stochastic gradient oracle). For $f_{\\delta}(\\cdot):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , its quantum $\\delta$ -estimated stochastic gradient oracle is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{O}_{\\mathbf{g}_{\\delta}}:|\\mathbf{x}\\rangle\\otimes|\\mathbf{0}\\rangle\\otimes|\\mathbf{0}\\rangle\\longmapsto|\\mathbf{x}\\rangle\\otimes\\sum_{\\xi,\\mathbf{w}}\\sqrt{\\mathrm{Pr}[\\mathbf{w},\\xi]}|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\xi)\\rangle\\otimes|\\psi_{\\mathbf{w},\\xi}\\rangle,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the random variable w is uniformly distributed on a unit sphere and $\\xi$ satisfies eq. (4). ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1 implies $\\mathbf{g}_{\\delta}(\\cdot)$ can serve as an estimator of $\\nabla f_{\\delta}$ , and it can be calculated with access to a quantum $\\delta$ -estimated stochastic gradient oracle as defined above. The following theorem shows that such oracle can be built with only $\\mathcal{O}(1)$ access to the quantum stochastic function value oracle. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.2. Given access to a quantum sampling oracle $\\mathbf{O}_{\\xi,\\mathbf{w}}$ to the joint distribution on $(\\xi,\\mathbf{w})$ , one can construct a quantum $\\delta$ -estimated stochastic gradient oracle (as defined in Definition 3.3) with two queries to the quantum stochastic function value oracle $\\mathbf{U}_{F}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 3.3. In Lemma 3.2, we assume a black box access to quantum sampling oracle $\\mathbf{O}_{\\xi,\\mathbf{w}}$ following Sidford and Zhang [48]. We present the explicit construction of such oracle in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "Similarly, we can also constructed the estimator of $\\nabla f_{\\delta}(\\mathbf{x})-\\nabla f_{\\delta}(\\mathbf{y})$ by the following oracle: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\bf O}_{\\Delta\\mathbf{g}_{\\delta}}:|\\mathbf{x}\\rangle\\otimes|\\mathbf{y}\\rangle\\otimes|\\mathbf{0}\\rangle\\otimes|\\mathbf{0}\\rangle\\longmapsto|\\mathbf{x}\\rangle\\otimes|\\mathbf{y}\\rangle\\otimes\\sum_{\\xi,\\mathbf{w}}{\\sqrt{\\operatorname*{Pr}[\\mathbf{w},\\xi]}}|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\xi)-\\mathbf{g}_{\\delta}(\\mathbf{y};\\mathbf{w},\\xi)\\rangle\\otimes|\\psi_{\\mathbf{w},\\xi}\\rangle,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with only $\\mathcal{O}(1)$ -queries of stochastic quantum function value oracle. ", "page_idx": 4}, {"type": "text", "text": "Corollary 3.4. Under the same condition as in Lemma 3.2, one can construct $\\mathbf{O}_{\\Delta\\mathbf{g}_{\\delta}}$ with four queries to the quantum stochastic function value oracle $\\mathbf{U}_{F}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Mini-batch Quantum Estimators via Quantum Mean Estimation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We constructed the quantum oracles $\\mathbf{O}_{\\mathbf{g}_{\\delta}}$ and $\\mathbf{O}_{\\Delta\\mathbf{g}_{\\delta}}$ with $\\mathcal{O}(1)$ -queries of quantum function value oracles in Section 3.1. These oracles produce outputs in the form of random variables. Specifically, $\\mathbf{O}_{\\mathbf{g}_{\\delta}}$ provides an output with expectation $\\nabla f_{\\delta}(\\mathbf{x})$ with the input $\\mathbf{x}$ , and $\\mathbf{O}_{\\Delta\\mathbf{g}_{\\delta}}$ provides an output with the expectation $\\nabla f_{\\delta}(\\mathbf{x})-\\nabla f_{\\delta}(\\mathbf{y})$ for $\\mathbf{O}_{\\Delta\\mathbf{g}_{\\delta}}$ with the inputs $\\mathbf{x}$ and $\\mathbf{y}$ . ", "page_idx": 4}, {"type": "text", "text": "The variance of the outputs can be reduced by constructing the mini-batch estimator. Inspired by the recent advance on quantum mean estimation [11, 12, 48] which improve the classical mini-batch estimator for multi-dimensional random variables, we construct improved estimators for $\\nabla f_{\\delta}(\\mathbf{x})$ and $\\nabla f_{\\delta}(\\mathbf{x})-\\nabla f_{\\delta}(\\mathbf{y})$ . We formally present the results in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.5. Under Assumption $^{\\,l}$ , and given access to a quantum sampling oracle $\\mathbf{O}_{\\xi,\\mathbf{w}}$ to the joint distribution on $(\\xi,\\up w)$ , it holds that: ", "page_idx": 4}, {"type": "table", "img_path": "wsGzvhnoaX/tmp/125e1c8e38476abbc318a9b70ab4a04c63604eb00803440dece4925b2b863809.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "1. there exists an algorithm that can construct an unbiased quantum estimator $\\hat{\\bf g}$ of $\\nabla f_{\\delta}(\\mathbf{x})$ such that $\\mathbb{E}\\left[\\|\\hat{\\mathbf{g}}-\\nabla f_{\\delta}(\\mathbf{x})\\|^{2}\\right]\\le\\hat{\\sigma}_{1}^{2}$ within $\\tilde{\\mathcal{O}}(d L\\hat{\\sigma}_{1}^{-1})$ queries of $\\mathbf{U}_{F}$ in expectation. ", "page_idx": 5}, {"type": "text", "text": "2. there exists an algorithm that can construct an unbiased quantum estimator $\\Delta\\mathbf{g}$ of $\\nabla f_{\\delta}(\\mathbf{x})~-$ $\\nabla f_{\\delta}(\\mathbf{y})$ such that $\\begin{array}{r l}{\\mathbb{E}\\left[\\|\\Delta\\mathbf{g}-\\left(\\nabla f_{\\delta}(\\mathbf{x})-\\nabla f_{\\delta}(\\mathbf{y})\\right)\\|^{2}\\right]\\ \\le\\ \\hat{\\sigma}_{2}^{2}}\\end{array}$ within $\\tilde{\\mathcal{O}}(d^{3/2}L\\|\\mathbf{y}\\,-\\,\\mathbf{x}\\|\\hat{\\sigma}_{2}^{-1}\\delta^{-1})$ queries of $\\mathbf{U}_{F}$ in expectation. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.6. Compared to the classical mini-batch estimator for $\\nabla f_{\\delta}(\\mathbf{x})$ , which require $\\mathcal{O}(d L^{2}\\hat{\\sigma}_{1}^{-2})$ queries of $\\mathbf{C}_{F}$ to achieve $\\hat{\\sigma}_{1}^{2}$ variance level ([6, Corollary 2.1]), our mini-batch quantum estimator for $\\nabla f_{\\delta}(\\mathbf{x})$ in Theorem 3.5 reduces a factor of $L\\hat{\\sigma}_{1}^{-1}$ without increasing the dimension dependency. ", "page_idx": 5}, {"type": "text", "text": "4 Quantum Algorithms for Finding the Goldstein Stationary Point ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we develop novel quantum algorithms for finding the $(\\delta,\\epsilon)$ -Goldstein stationary point of a non-smooth non-convex objective ${\\bar{f}}(\\cdot)$ . Instead of finding the stationary point directly, we consider finding the $\\epsilon_{}$ -stationary point of its smoothed surrogate $f_{\\delta}(\\cdot)$ , which is equivalent to the original problem according to Remark 2.2. The classical zeroth-order methods based on such equivalence require to access the gradient estimator to $\\nabla f_{\\delta}(\\cdot)$ by stochastic function values [6, 32, 35, 36]. Different from the classical methods, we can take the advantage of the quantum estimators, which can be constructed by accessing quantum stochastic function value oracles due to our novel results in Section 3. ", "page_idx": 5}, {"type": "text", "text": "We first propose an algorithm which uses the quantum gradient estimator to replace $\\nabla f_{\\delta}(\\mathbf{x})$ to do the gradient descent step at each iteration. We present the quantum gradient-free method (QGFM) in Algorithm 1. Given a desired variance level $\\hat{\\sigma}_{t}^{2}$ , line 2 of Algorithm 1 can be constructed explicitly and efficiently by the quantum stochastic function value oracles $\\mathbf{U}_{F}$ according to Theorem 3.5. The following theorem gives the upper bound on the total $\\mathbf{U}_{F}$ that Algorithm 1 require to access for finding the $(\\delta,\\epsilon)$ -Goldstein stationary point. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Under Assumption 1, by setting the parameter in Algorithm $^{\\,I}$ as $\\begin{array}{r l}{\\eta}&{{}=}\\end{array}$ $\\delta/(2d^{1/2}L)$ and $\\hat{\\sigma}_{t}^{2}\\equiv\\epsilon^{2}/2$ , then the total queries of stochastic quantum function value oracle $\\mathbf{U}_{F}$ for finding the $(\\delta,\\epsilon)$ -Goldstein stationary point of $f(\\cdot)$ can be bounded by $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(d^{3/2}\\left(\\frac{L^{3}}{\\epsilon^{3}}+\\frac{L^{2}\\Delta}{\\delta\\epsilon^{3}}\\right)\\right)}\\end{array}$ , where $\\Delta=f(\\mathbf{x}_{0})-f^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 4.2. QGFM(Algorithm 1) speedups the gradient-free method (GFM) [35] for finding $(\\delta,\\epsilon)$ - stationary point by a factor of $L\\epsilon^{-1}$ . ", "page_idx": 5}, {"type": "text", "text": "Notably, Algorithm 1 utilized a simple gradient descent step can achieve $\\Omega(\\delta^{-1}\\epsilon^{-3})$ , which is optimal for classical zeroth-order and first-order methods in terms of $\\epsilon$ and $\\delta$ . It is worth mentioning that the classical methods that achieve this lower bound typically involve multiple loops [6] or rely on additional online optimization algorithms [13, 32]. ", "page_idx": 5}, {"type": "text", "text": "To further enhance the query complexity in Theorem 4.1, we propose the fast quantum gradientfree method $\\mathrm{(QGFM+)}$ ) by incorporating variance reduction techniques, as outlined in Algorithm 2. $\\mathrm{QGFM+}$ can be seen as a quantum-accelerated version of $\\mathrm{GFM+}$ [6]. Unlike $\\mathrm{GFM+}$ , which required double loops, $\\mathrm{QGFM+}$ simplifies the implementation by utilizing a single loop based on the PAGE framework [34]. Moreover, we replace all classical estimators with quantum estimators in line 6 and line 8 of Algorithm 2. These quantum estimators can be efficiently constructed using stochastic quantum function value oracles with a desired variance level, as demonstrated in Theorem 3.5. We present the total number of queries of $\\mathbf{U}_{F}$ for $\\mathrm{QGFM+}$ in the following theorem. We present the total queries of $\\mathbf{U}_{F}$ for $\\mathrm{QGFM+}$ in the following theorem. ", "page_idx": 5}, {"type": "text", "text": "1: Construct ${\\bf g}_{0}$ as an unbiased estimator of $\\nabla f_{\\delta}(\\mathbf{x}_{0})$ with variance at most $\\hat{\\sigma}_{1,0}^{2}$ .   \n2: for $t=0,1\\ldots T$   \n3: $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathbf{g}_{t}$   \n4: Flip a coin $\\theta_{t}\\in\\{0,1\\}$ where $P(\\theta_{t}=1)=p_{t}$   \n5: If $\\theta_{t}=1$ then   \n6: Construct $\\mathbf{g}_{t+1}$ as an unbiased quantum estimator of $\\nabla f_{\\delta}(\\mathbf{x}_{t+1})$ with variance at most   \n$\\hat{\\sigma}_{1,t+1}^{2}$ using $\\mathbf{U}_{F}$ according to Theorem 3.5.   \n7: else   \n8: Construct $\\Delta\\mathbf{g}_{t+1}$ as an unbiased quantum estimator of $\\nabla f_{\\delta}\\big(\\mathbf{x}_{t+1}\\big)-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)$ with variance   \nat most $\\hat{\\sigma}_{2,t+1}^{2}$ using $\\mathbf{U}_{F}$ according to Theorem 3.5.   \n9: $\\mathbf{g}_{t+1}=\\mathbf{g}_{t}+\\Delta\\mathbf{g}_{t+1}.$   \n10: end for ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3. Under Assumption $^{\\,I}$ , by setting the parameters in Algorithm 2 as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\eta=\\delta/\\bigl(2d^{1/2}L\\bigr),\\ \\ \\ p_{t}\\equiv\\epsilon^{2/3}/L^{2/3},\\ \\ \\ \\hat{\\sigma}_{1,t}^{2}\\equiv\\epsilon^{2}/2,\\ \\ \\ a n d\\ \\ \\hat{\\sigma}_{2,t}^{2}=\\epsilon^{2/3}L^{4/3}d\\|{\\bf x}_{t}-{\\bf x}_{t-1}\\|^{2}/\\delta^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "then the total queries of stochastic quantum function value oracle $\\mathbf{U}_{F}$ for finding the $(\\delta,\\epsilon)$ -Goldstein stationary point of $f(\\cdot)$ can be bounded by $\\begin{array}{r}{\\tilde{\\mathcal{O}}\\left(d^{3/2}\\left(\\frac{L^{7/3}}{\\epsilon^{7/3}}+\\frac{L^{4/3}\\Delta}{\\delta\\epsilon^{7/3}}\\right)\\right)}\\end{array}$ + L\u03b44\u01eb/73/3\u2206 )), where \u2206= f(x0) \u2212f \u2217. ", "page_idx": 6}, {"type": "text", "text": "Remark 4.4. QGFM $^+$ (Algorithm 2) speedups the $\\mathrm{GFM+}$ [6] for finding $(\\delta,\\epsilon)$ -stationary point by a factor of L\u01eb\u22122/3. ", "page_idx": 6}, {"type": "text", "text": "We can see that $\\mathrm{QGFM+}$ achieves the query complexity of $\\tilde{\\mathcal{O}}(d^{3/2}\\epsilon^{-7/3}\\delta^{-1})$ , which cannot be achieved by any of the classical methods. Furthermore, we observe the applicability of our framework to smooth non-convex optimization. ", "page_idx": 6}, {"type": "text", "text": "Remark 4.5. $\\mathrm{QGFM+}$ is different from the quantum speedups algorithm (Q-SPIDER) for nonconvex smooth stochastic optimization [48]: $\\mathrm{QGFM+}$ adjusts the variance level of $\\Delta{\\bf g}_{t}$ according to the difference between the current iteration point and the previous one, while Q-SPIDER fixes the variance levels. By using the adaptive variance level and $\\mathrm{QGFM+}$ framework, we can further accelerate the Q-SPIDER for smooth non-convex optimization. In Appendix G, we propose fast quantum gradient method $\\mathrm{(QGM+)}$ with the query complexity of $\\tilde{\\mathcal{O}}(\\sqrt{\\bar{d}}\\epsilon^{-7/3})$ , which improves the one of $\\tilde{\\mathcal{O}}(\\sqrt{d}\\epsilon^{-5/2})$ obtained in Sidford and Zhang [48]. ", "page_idx": 6}, {"type": "text", "text": "5 Conclusion and Future Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this paper, we have presented quantum algorithms for finding the $(\\delta,\\epsilon)$ -Goldstein stationary point for a non-smooth non-convex objective. Our query complexities demonstrate a clearly quantum speedup over the classical methods. In future work, it would be intriguing to explore the framework without ideal distributions which is caused by the limitation of classical or quantum resources. It is also interesting to figure out the quantum speedups for the deterministic methods [14, 28, 51] or the NS-NC objective with constraints [38]. We are also interested to see if similar strategies can be applied to the quantum online optimization with zeroth-order feedback [25, 26, 33, 56, 58]. The query complexity of the proposed methods still have heavy dependency on the dimension, it is also possible to reduce the dimension dependency based on other quantum techniques and design efficient first-order quantum methods. ", "page_idx": 6}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Chengchang Liu would like to thank Luo Luo and Zongqi Wan for valuable discussion. The work of John C.S. Lui was supported in part by the RGC GRF:14207721. ", "page_idx": 6}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "[1] Fernando GSL Brandao and Krysta M Svore. Quantum speed-ups for solving semidefinite programs. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 415\u2013426. IEEE, 2017.   \n[2] Fernando GSL Brand\u02dcao, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M Svore, and Xiaodi Wu. Quantum sdp solvers: Large speed-ups, optimality, and applications to quantum learning. In 46th International Colloquium on Automata, Languages, and Programming (ICALP 2019). Schloss-Dagstuhl-Leibniz Zentrum f\u00a8ur Informatik, 2019.   \n[3] Sergey Bravyi, David Gosset, and Robert K\u00a8onig. Quantum advantage with shallow circuits. Science, 362(6412):308\u2013311, 2018.   \n[4] Shouvanik Chakrabarti, Andrew M Childs, Tongyang Li, and Xiaodi Wu. Quantum algorithms and lower bounds for convex optimization. Quantum, 4:221, 2020.   \n[5] Shouvanik Chakrabarti, Andrew M Childs, Shih-Han Hung, Tongyang Li, Chunhao Wang, and Xiaodi Wu. Quantum algorithm for estimating volumes of convex bodies. ACM Transactions on Quantum Computing, 4(3):1\u201360, 2023.   \n[6] Lesi Chen, Jing Xu, and Luo Luo. Faster gradient-free algorithms for nonsmooth nonconvex stochastic optimization. ICML, 2023.   \n[7] Andrew M Childs, Jiaqi Leng, Tongyang Li, Jin-Peng Liu, and Chenyi Zhang. Quantum simulation of real-space dynamics. Quantum, 6:860, 2022.   \n[8] Andrew M Childs, Tongyang Li, Jin-Peng Liu, Chunhao Wang, and Ruizhe Zhang. Quantum algorithms for sampling log-concave distributions and estimating normalizing constants. Advances in Neural Information Processing Systems, 35:23205\u201323217, 2022.   \n[9] Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller. Structured evolution with compact architectures for scalable policy optimization. In International Conference on Machine Learning, pages 970\u2013978. PMLR, 2018.   \n[10] Frank H Clarke. Optimization and nonsmooth analysis. SIAM, 1990.   \n[11] Arjan Cornelissen and Yassine Hamoudi. A sublinear-time quantum algorithm for approximating partition functions. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1245\u20131264. SIAM, 2023.   \n[12] Arjan Cornelissen, Yassine Hamoudi, and Sofiene Jerbi. Near-optimal quantum algorithms for multivariate mean estimation. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 33\u201343, 2022.   \n[13] Ashok Cutkosky, Harsh Mehta, and Francesco Orabona. Optimal stochastic non-smooth nonconvex optimization through online-to-non-convex conversion. In International Conference on Machine Learning, pages 6643\u20136670. PMLR, 2023.   \n[14] Damek Davis, Dmitriy Drusvyatskiy, Yin Tat Lee, Swati Padmanabhan, and Guanghao Ye. A gradient sampling method with complexity guarantees for lipschitz functions in high and low dimensions. Advances in neural information processing systems, 35:6692\u20136703, 2022.   \n[15] John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674\u2013701, 2012.   \n[16] Darrell Duffie. Dynamic asset pricing theory. Princeton University Press, 2010.   \n[17] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American statistical Association, 96(456):1348\u20131360, 2001.   \n[18] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal nonconvex optimization via stochastic path-integrated differential estimator. Advances in neural information processing systems, 31, 2018.   \n[19] Ankit Garg, Robin Kothari, Praneeth Netrapalli, and Suhail Sherif. No quantum speedup over gradient descent for non-smooth convex optimization. arXiv preprint arXiv:2010.01801, 2020.   \n[20] Craig Gidney. Asymptotically efficient quantum karatsuba multiplication. arXiv preprint arXiv:1904.07356, 2019.   \n[21] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 315\u2013323. JMLR Workshop and Conference Proceedings, 2011.   \n[22] AA Goldstein. Optimization of lipschitz continuous functions. Mathematical Programming, 13:14\u201322, 1977.   \n[23] Weiyuan Gong, Chenyi Zhang, and Tongyang Li. Robustness of quantum algorithms for nonconvex optimization. arXiv preprint arXiv:2212.02548, 2022.   \n[24] Lov Grover and Terry Rudolph. Creating superpositions that correspond to efficiently integrable probability distributions. arXiv preprint quant-ph/0208112, 2002.   \n[25] Jianhao He, Feidiao Yang, Jialin Zhang, and Lvzhou Li. Quantum algorithm for online convex optimization. Quantum Science and Technology, 7(2):025022, 2022.   \n[26] Jianhao He, Chengchang Liu, Xutong Liu, Lvzhou Li, and John CS Lui. Quantum algorithm for online exp-concave optimization. In Forty-first International Conference on Machine Learning, 2024.   \n[27] L Jeff Hong, Barry L Nelson, and Jie Xu. Discrete optimization via simulation. Handbook of simulation optimization, pages 9\u201344, 2015.   \n[28] Michael Jordan, Guy Kornowski, Tianyi Lin, Ohad Shamir, and Manolis Zampetakis. Deterministic nonsmooth nonconvex optimization. In The Thirty Sixth Annual Conference on Learning Theory, pages 4570\u20134597. PMLR, 2023.   \n[29] Sham M Kakade and Jason D Lee. Provably correct automatic sub-differentiation for qualified programs. Advances in neural information processing systems, 31, 2018.   \n[30] Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. In 8th Innovations in Theoretical Computer Science Conference (ITCS 2017). Schloss Dagstuhl-LeibnizZentrum fuer Informatik, 2017.   \n[31] Guy Kornowski and Ohad Shamir. Oracle complexity in nonsmooth nonconvex optimization. Advances in Neural Information Processing Systems, 34:324\u2013334, 2021.   \n[32] Guy Kornowski and Ohad Shamir. An algorithm with optimal dimension-dependence for zeroorder nonsmooth nonconvex stochastic optimization. arXiv preprint arXiv:2307.04504, 2023.   \n[33] Tongyang Li and Ruizhe Zhang. Quantum speedups of optimizing approximately convex functions with applications to logarithmic regret stochastic convex bandits. Advances in Neural Information Processing Systems, 35:3152\u20133164, 2022.   \n[34] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt\u00b4arik. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In International conference on machine learning, pages 6286\u20136295. PMLR, 2021.   \n[35] Tianyi Lin, Zeyu Zheng, and Michael Jordan. Gradient-free methods for deterministic and stochastic nonsmooth nonconvex optimization. Advances in Neural Information Processing Systems, 2022.   \n[36] Zhenwei Lin, Jingfan Xia, Qi Deng, and Luo Luo. Decentralized gradient-free methods for stochastic non-smooth non-convex optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024.   \n[37] Yizhou Liu, Weijie J Su, and Tongyang Li. On quantum speedups for nonconvex optimization via quantum tunneling walks. Quantum, 7:1030, 2023.   \n[38] Zhuanghua Liu, Cheng Chen, Luo Luo, and Bryan Kian Hsiang Low. Zeroth-order methods for constrained nonconvex nonsmooth stochastic optimization. In Forty-first International Conference on Machine Learning, 2024.   \n[39] Rahul Mazumder, Jerome H Friedman, and Trevor Hastie. Sparsenet: Coordinate descent with nonconvex penalties. Journal of the American Statistical Association, 106(495):1125\u20131138, 2011.   \n[40] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814, 2010.   \n[41] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17(2):527\u2013566, 2017.   \n[42] Michael A Nielsen and Isaac L Chuang. Quantum computation and quantum information. Cambridge university press, 2010.   \n[43] Damien Power. Supply chain management integration and implementation: a literature review. Supply chain management: an International journal, 10(4):252\u2013263, 2005.   \n[44] John Preskill. Quantum computing in the nisq era and beyond. Quantum, 2:79, 2018.   \n[45] Mehdi Ramezani, Morteza Nikaeen, Farnaz Farman, Seyed Mahmoud Ashraf,i and Alireza Bahrampour. Quantum multiplication algorithm based on the convolution theorem. Physical Review A, 108(5):052405, 2023.   \n[46] Lidia Ruiz-Perez and Juan Carlos Garcia-Escartin. Quantum arithmetic with the quantum fourier transform. Quantum Information Processing, 16:1\u201314, 2017.   \n[47] Emre Sahinoglu and Shahin Shahrampour. An online optimization perspective on first-order and zero-order decentralized nonsmooth nonconvex stochastic optimization. In Forty-first International Conference on Machine Learning, 2024.   \n[48] Aaron Sidford and Chenyi Zhang. Quantum speedups for stochastic optimization. In Thirtyseventh Conference on Neural Information Processing Systems, 2023.   \n[49] Hyung Ju Suh, Max Simchowitz, Kaiqing Zhang, and Russ Tedrake. Do differentiable simulators give better policy gradients? In International Conference on Machine Learning, pages 20668\u201320696. PMLR, 2022.   \n[50] Lai Tian and Anthony Man-Cho So. On the hardness of computing near-approximate stationary points of clarke regular nonsmooth nonconvex problems and certain dc programs. In ICML Workshop on Beyond First-Order Methods in ML Systems, 2021.   \n[51] Lai Tian and Anthony Man-Cho So. No dimension-free deterministic algorithm computes approximate stationarities of lipschitzians. Mathematical Programming, pages 1\u201324, 2024.   \n[52] Lai Tian, Kaiwen Zhou, and Anthony Man-Cho So. On the finite-time complexity and practical computation of approximate stationarity concepts of lipschitz functions. In International Conference on Machine Learning, pages 21360\u201321379. PMLR, 2022.   \n[53] Joran Van Apeldoorn and Andr\u00b4as Gily\u00b4en. Improvements in quantum sdp-solving with applications. arXiv preprint arXiv:1804.05058, 2018.   \n[54] Joran Van Apeldoorn, Andr\u00b4as Gily\u00b4en, Sander Gribling, and Ronald de Wolf. Quantum sdpsolvers: Better upper and lower bounds. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 403\u2013414. IEEE, 2017.   \n[55] Joran van Apeldoorn, Andr\u00b4as Gily\u00b4en, Sander Gribling, and Ronald de Wolf. Convex optimization using quantum oracles. Quantum, 4:220, 2020.   \n[56] Zongqi Wan, Zhijie Zhang, Tongyang Li, Jialin Zhang, and Xiaoming Sun. Quantum multiarmed bandits and stochastic linear bandits enjoy logarithmic regrets. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023.   \n[57] Daochen Wang, Aarthi Sundaram, Robin Kothari, Ashish Kapoor, and Martin Roetteler. Quantum algorithms for reinforcement learning with a generative model. In International Conference on Machine Learning, pages 10916\u201310926. PMLR, 2021.   \n[58] Yulian Wu, Chaowen Guan, Vaneet Aggarwal, and Di Wang. Quantum heavy-tailed bandits. arXiv preprint arXiv:2301.09680, 2023.   \n[59] Farzad Yousefian, Angelia Nedi\u00b4c, and Uday V Shanbhag. On stochastic gradient and subgradient methods with adaptive steplength sequences. Automatica, 48(1):56\u201367, 2012.   \n[60] Chenyi Zhang and Tongyang Li. Quantum lower bounds for finding stationary points of nonconvex functions. In International Conference on Machine Learning, pages 41268\u201341299. PMLR, 2023.   \n[61] Chenyi Zhang, Jiaqi Leng, and Tongyang Li. Quantum algorithms for escaping from saddle points. Quantum, 5:529, 2021.   \n[62] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(1):894\u2013942, 2010.   \n[63] Jingzhao Zhang, Hongzhou Lin, Stefanie Jegelka, Suvrit Sra, and Ali Jadbabaie. Complexity of finding stationary points of nonconvex nonsmooth functions. In International Conference on Machine Learning, pages 11173\u201311182. PMLR, 2020.   \n[64] Yexin Zhang, Chenyi Zhang, Cong Fang, Liwei Wang, and Tongyang Li. Quantum algorithms and lower bounds for finite-sum optimization. arXiv preprint arXiv:2406.03006, 2024. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Explicit Construction of Quantum Sampling Oracles ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In this section, we propose a novel quantum process to realize quantum sampling oracle $\\mathbf{O}_{\\mathbf{w},\\xi}$ \u2236 $\\begin{array}{r}{|\\mathbf{0}\\rangle\\longmapsto\\sum_{\\mathbf{w},\\xi}\\sqrt{\\mathrm{Pr}[\\mathbf{w},\\xi]}|\\mathbf{w},\\xi\\rangle|\\psi_{\\mathbf{w},\\xi}\\rangle}\\end{array}$ with uniform distribution, where $\\xi$ is uniformly distributed on $\\{0,\\cdots,N-1\\}$ and w is sampled uniformly on a discrete unit sphere. ", "page_idx": 11}, {"type": "text", "text": "The uniform distribution of $\\xi$ in quantum state can be constructed by using Hadamard gates. The construction of the uniform distribution on a discrete unit sphere is more tricky. Classically, such a distribution can be constructed by sampling each coordinate from a standard Gaussian distribution, and then normalize the vector to have unit length by dividing by its norm. However, preparing a superposition state with Gaussian amplitudes is not trivial because Gaussian distribution is defined in a infinite interval. Constructing such state with Grover\u2019s method [24] will lead to some issues in dealing with the domain and normalization of the measurement probability. Instead, here, starting with the simple uniform superposition state, we use central limit theorem to construct the standard Gaussian distribution. ", "page_idx": 11}, {"type": "text", "text": "The overall quantum algorithm proceeds as follows: ", "page_idx": 11}, {"type": "text", "text": "Step 1. Prepare initial quantum state $\\left|0\\right\\rangle^{\\otimes m_{1}}\\otimes\\left|0\\right\\rangle^{\\otimes\\left(d m_{2}\\right)}\\otimes\\left|0\\right\\rangle^{\\otimes\\left(d\\log m_{2}\\right)}$ . Set $k=0$ . Apply $H^{\\otimes m_{1}}\\otimes$ $H^{\\otimes d m_{2}}\\otimes I$ , that is, apply Hadamard gates to the first and the second registers. Here, $m_{1},m_{2}\\in\\mathbb{N}_{+}$ . ", "page_idx": 11}, {"type": "text", "text": "Step 2. Define $\\begin{array}{r}{h\\,:\\,\\{0,1\\}^{m_{2}}\\,\\to\\,\\mathbb{R},\\ h(\\mathbf{j})\\,=\\,2\\sqrt{m_{2}}\\left(\\frac{j_{1}+j_{2}+\\cdots+j_{m_{2}}}{\\sqrt{m_{2}}}-0.5\\right)}\\end{array}$ (j1+j2\u221a+\u22c5m\u22c5\u22c52+jm2\u22120.5). Apply I \u2297U h\u2297 d, where $U_{h}$ , the unitary transform corresponding to $h$ , maps quantum state $\\left|\\mathbf{j}\\right\\rangle\\left|0\\right\\rangle$ to quantum state $\\left|\\mathbf{j}\\right\\rangle\\left|0+h(\\mathbf{j})\\right\\rangle$ . The $k$ -th $U_{h}$ takes the $k$ -th $m_{2}$ qubits in the second register as input, and the output is stored in the $k$ -th $\\log{m_{2}}$ qubits in the third register, for all $\\bar{k}\\in\\{0,\\ldots,{d-1}\\}$ . ", "page_idx": 11}, {"type": "text", "text": "Step 3. Consider the third register as a $d$ -dimension vector $\\mathbf{w}^{\\prime}$ , with $\\log{m_{2}}$ qubits to store each coordinate $\\mathbf{w}_{k}^{\\prime}$ . Apply $U_{\\mathrm{norm}}:\\left|\\mathbf{w}\\right\\rangle\\left|0+\\left\\|\\mathbf{w}\\right\\|\\right\\rangle$ , the result is stored in an additional ancillary register. Then normalize $\\mathbf{w}^{\\prime}$ to have unit length by dividing by $\\lVert\\mathbf{w}\\rVert$ in each component. ", "page_idx": 11}, {"type": "text", "text": "Analysis and Correctness. In Step 1, it starts with the quantum state $\\left|0\\right\\rangle^{\\otimes m_{1}}\\otimes\\left|0\\right\\rangle^{\\otimes\\left(d m_{2}\\right)}\\otimes$ $\\left|0\\right\\rangle^{\\otimes(d\\log m_{2})}$ , where all of the registers are initialized to 0. The first register is prepared for creating the superposition of $\\xi$ , and the second and the third registers are prepared for creating the superposition of $\\mathbf{w}$ . We apply Hadamard gates to the first and the second registers, to obtain a uniform superposition of computation basis, which gives ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2^{m_{1}d m_{2}}}}\\sum_{i=0}^{2^{m_{1}}-1}|i\\rangle\\otimes\\sum_{j_{1}^{(0)},\\ldots,j_{m_{2}^{(0)}}^{(0)}=0}^{1}\\left|j_{1}^{(0)}\\cdot\\cdot\\cdot j_{m_{2}}^{(0)}\\right\\rangle\\otimes\\cdots\\otimes\\sum_{j_{1}^{(d-1)},\\ldots,j_{m_{2}^{(d-1)}}^{(d-1)}=0}^{1}\\left|j_{1}^{(d-1)}\\cdot\\cdot\\cdot j_{m_{2}}^{(d-1)}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Let $m_{1}=\\lceil\\log N\\rceil$ , and we relabel the first register to obtain ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2^{m_{1}d m_{2}}}}\\sum_{\\xi}|\\xi\\rangle\\otimes\\sum_{\\stackrel{j_{1}^{(0)},\\ldots,j_{m_{2}^{(0)}}=0}{j_{1}^{(0)},\\ldots,j_{m_{2}^{(0)}}=0}}^{\\mathrm{~1~}}\\Bigr|j_{1}^{(0)}\\cdot\\cdot\\cdot j_{m_{2}}^{(0)}\\Bigr\\rangle\\otimes\\cdots\\otimes\\sum_{j_{1}^{(d-1)},\\ldots,j_{m_{2}^{(d-1)}}=0}^{\\mathrm{~1~}}\\Bigr|j_{1}^{(d-1)}\\cdot\\cdot\\cdot j_{m_{2}}^{(d-1)}\\Bigr\\rangle\\otimes|\\mathbf{0}\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "After Step 2, as each $U_{h}$ operates in the same manner, we take one as an example, ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2^{m_{1}d m_{2}}}}\\sum_{\\xi}\\left|\\xi\\right\\rangle\\otimes\\cdots\\sum_{j_{1},\\ldots,j_{m_{2}}=0}^{1}\\left|j_{1}j_{2}\\ldots j_{m_{2}}\\right\\rangle\\cdot\\ldots\\left|2\\sqrt{m_{2}}\\left(\\frac{j_{1}+j_{2}+\\cdots+j_{m_{2}}}{\\sqrt{m_{2}}}-0.5\\right)\\right\\rangle\\ldots.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Once measure, $j_{1},\\dots,j_{m_{2}}$ are independent and identically distributed random variables with mean of 0.5 and variance of 0.25. By the central limit theorem, the average of $\\{j_{i}\\}_{i=1}^{m_{2}}$ approximates the Gaussian distribution when $m_{2}$ is large. We obtain the standard Gaussian distribution after shifting and scaling the average of them. We denote $\\begin{array}{r}{\\mathbf{w}_{k}^{\\prime}\\triangleq2\\sqrt{m_{2}}\\,\\bigg(\\frac{j_{1}^{(k)}+j_{2}^{(k)}+\\cdots+j_{m_{2}}^{(k)}}{\\sqrt{m_{2}}}-0.5\\bigg),}\\end{array}$ , then the measurement results of $\\sum_{\\mathbf{j}^{(k)}}|\\mathbf{w}_{k}^{\\prime}\\rangle$ follow the distribution of ${\\mathcal{N}}(0,1)$ . ", "page_idx": 11}, {"type": "text", "text": "After Step 3, the vector in the third register is mapped into the unit sphere, and the measurement result follows the uniform distribution on a discrete unit sphere. Rearrange the order of the registers, denote all the garbage qubits as $|\\psi_{{\\bf w},\\xi}\\rangle$ , we obtain ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{w},\\xi}\\sqrt{\\mathrm{Pr}[\\mathbf{w},\\xi]}\\,|\\mathbf{w},\\xi\\rangle\\,|\\psi_{\\mathbf{w},\\xi}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\mathbf{w}=\\mathbf{w}^{\\prime}/\\Vert\\mathbf{w}^{\\prime}\\Vert$ is uniformly distributed on a discrete unit sphere and $\\xi$ is uniformly distributed on $\\{0,\\cdots,N-1\\}$ . ", "page_idx": 12}, {"type": "text", "text": "This realizes the discrete version of the quantum sample oracle $\\mathbf{O}_{\\mathbf{w},\\xi}$ with uniform distribution. Remark A.1. In the ideal scenario where we do not need to limit the number of qubits, allowing $m_{1}$ and $m_{2}$ to be sufficiently large, we can achieve $\\begin{array}{r}{\\int_{\\mathbf{w}\\in S^{d-1}}\\sqrt{\\mu(\\mathbf{w})d\\mathbf{w}}|\\mathbf{w}\\rangle}\\end{array}$ as needed in Proposition 3.1. Specifically, our process requires $m_{1}+m_{2}\\times d$ Hadamard gates, $\\mathcal{O}(d m_{2})$ fundamentally arithmetic operations and 1 calls to the norm circuit. Here, $m_{1}\\,=\\,\\lceil\\log N\\rceil$ and $m_{2}$ is the number of random variables which are used to approximate the Gaussian distribution. Note that many gates here can be performed in parallel, for example, all the $H$ gates can be performed simultaneously, and the sum of $m_{2}$ qubits can be implemented in a circuit of ${\\mathcal{O}}(\\log m_{2})$ depth. The total depth complexity is $\\mathcal{O}(\\log m_{2}^{-}+\\log(d\\log m_{2}^{-}))$ , which indicates that the circuit depth will remain small when $m_{2}$ increases. This ensures that our construction is feasible even under the context of NISQ quantum computer [3, 44], which only supports low depth circuits. Notably, this procedure does not require querying $\\mathbf{U}_{F}$ , thereby not increasing the query complexity of $\\mathbf{U}_{F}$ . Nevertheless, it is still important to give such a explicit and efficient construction to ensure that the quantum state preparation will not ruin the quantum advantage for the overall time complexity. ", "page_idx": 12}, {"type": "text", "text": "Remark A.2. If $\\xi$ is sampled from distribution other than uniform distribution, there still exist quantum techniques which can construct such quantum sample oracle. When the detailed classical sampling circuits are known, we can make it reversible by replacing gates in the classical circuits with reversible quantum gates such as Toffoli gate [42], so as to obtain a quantum circuit [57]. When there is only a black box access to the classical circuit, we discuss the construction by cases. For the continues case where the distribution is described by a probability density function, we can use the Grover\u2019s method [24], which requires an efficient integrating circuit. For the discrete case, we can extend the Grover\u2019s method by the QRAM data structure. The complexity of constructing QRAM data structure depends linearly on the size of sample space. Once it is constructed, the complexity of generating the quantum sample oracle depends only logarithmly on the size of sample space [30]. ", "page_idx": 12}, {"type": "text", "text": "B The Proof of Lemma 3.2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof. First, we claim that a unitary operator $\\mathbf{U}_{\\mathbf{g},\\delta}$ for computing the stochastic gradient estimator $\\mathbf{g}_{\\delta}(\\cdot;\\mathbf{w},\\boldsymbol{\\xi})$ can be efficiently constructed. More precisely, we can construct ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{U}_{\\mathbf{g},\\delta}:|\\mathbf{x}\\rangle\\otimes|\\boldsymbol{\\xi}\\rangle\\otimes|\\mathbf{w}\\rangle\\otimes|b\\rangle\\longmapsto|\\mathbf{x}\\rangle\\otimes|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})\\rangle\\otimes|\\psi_{\\mathbf{w},\\boldsymbol{\\xi}}\\rangle\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with 2 queries to $\\mathbf{U}_{F}$ . Now we assume the access to $\\mathbf{U}_{\\mathbf{g},\\delta}$ and the description of its construction will be deferred to the end of this proof. Next we show how this can lead to a quantum $\\delta$ -estimated stochastic gradient oracle $\\mathbf{O}_{\\mathbf{g},\\delta}$ as defined in Definition 3.3. Given initial state $\\mathbf{\\left|x\\right\\rangle}\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle$ , we can prepare the desired quantum state by first applying the quantum sampling oracle $\\mathbf{O}_{\\mathbf{w},\\xi}$ and then $\\mathbf{U}_{\\mathbf{g},\\delta}$ as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbf{U}_{\\mathbf{g},\\delta}\\cdot\\left(\\mathbf{I}\\otimes\\mathbf{O}_{\\xi,\\mathbf{w}}\\otimes\\mathbf{I}\\right)\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle=\\mathbf{U}_{\\mathbf{g},\\delta}(\\left|\\mathbf{x}\\right\\rangle\\otimes\\displaystyle\\sum_{\\xi,\\mathbf{w}}\\sqrt{p(\\xi,\\mathbf{w})}|\\xi,\\mathbf{w}\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad=\\sum_{\\xi,\\mathbf{w}}\\sqrt{p(\\xi,\\mathbf{w})}\\mathbf{U}_{\\mathbf{g},\\delta}(\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\xi,\\mathbf{w}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle)}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\left|\\mathbf{x}\\right\\rangle\\otimes\\displaystyle\\sum_{\\xi,\\mathbf{w}}\\sqrt{p(\\xi,\\mathbf{w})}|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\xi)\\rangle\\otimes\\left|\\psi_{\\mathbf{w},\\xi}\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Next we finish the proof by presenting how to implement $\\mathbf{U}_{\\mathbf{g},\\delta}$ with two queries to $\\mathbf{U}_{F}$ . Since $\\delta$ and $d$ are fixed and known beforehand, we can easily construct the following three operators via the quantum unitary implementations of the corresponding classical arithmetic operations: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{A}_{+}:|\\mathbf{x}\\rangle\\otimes|\\mathbf{w}\\rangle\\otimes|\\mathbf{0}\\rangle\\longmapsto|\\mathbf{x}\\rangle\\otimes|\\mathbf{w}\\rangle\\otimes|\\mathbf{x}+\\delta\\mathbf{w}\\rangle,\\quad\\mathbf{A}_{-}:|\\mathbf{x}\\rangle\\otimes|\\mathbf{w}\\rangle\\otimes|\\mathbf{0}\\rangle\\longmapsto|\\mathbf{x}\\rangle\\otimes|\\mathbf{w}\\rangle\\otimes|\\mathbf{x}-\\delta\\mathbf{w}\\rangle,}\\\\ &{\\mathsf{s u b}:|a\\rangle\\otimes|b\\rangle\\longmapsto|a\\rangle\\otimes|a-b\\rangle,\\quad\\quad\\mathrm{and}\\qquad\\mathsf{F r m u l}:|c\\rangle\\longmapsto\\displaystyle\\left|\\frac{\\delta}{2d}c\\right\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Let $\\begin{array}{r}{F^{\\prime}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})\\triangleq\\frac{\\delta}{2d}\\left(F(\\mathbf{x}+\\delta\\mathbf{w};\\boldsymbol{\\xi})-F(\\mathbf{x}-\\delta\\mathbf{w};\\boldsymbol{\\xi})\\right)}\\end{array}$ . Then we construct a unitary $\\mathbf{D}$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}:\\quad\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\mathbf{\\xi}\\right\\rangle\\otimes\\left|\\mathbf{w}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle}\\\\ &{\\quad\\mapsto_{(a)}\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\xi\\right\\rangle\\otimes\\left|\\mathbf{w}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{x}-\\delta\\mathbf{w}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle}\\\\ &{\\quad\\mapsto_{(b)}\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\xi\\right\\rangle\\otimes\\left|\\mathbf{w}\\right\\rangle\\otimes\\left|F(\\mathbf{x}-\\delta\\mathbf{w};\\xi)\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{x}-\\delta\\mathbf{w}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle}\\\\ &{\\quad\\mapsto_{(c)}\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\xi\\right\\rangle\\otimes\\left|\\mathbf{w}\\right\\rangle\\otimes\\left|F(\\mathbf{x}-\\delta\\mathbf{w};\\xi)\\right\\rangle\\otimes\\left|F(\\mathbf{x}+\\delta\\mathbf{w};\\xi)\\right\\rangle\\otimes\\left|\\mathbf{x}-\\delta\\mathbf{w}\\right\\rangle\\otimes\\left|\\mathbf{x}+\\delta\\mathbf{w}\\right\\rangle}\\\\ &{\\quad\\mapsto_{(d)}\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\xi\\right\\rangle\\otimes\\left|\\mathbf{w}\\right\\rangle\\otimes\\left|F^{\\prime}(\\mathbf{x};\\mathbf{w},\\xi)\\right\\rangle\\otimes\\left|F(\\mathbf{x}+\\delta\\mathbf{w};\\xi)\\right\\rangle\\otimes\\left|\\mathbf{x}-\\delta\\mathbf{w}\\right\\rangle\\otimes\\left|\\mathbf{x}+\\delta\\mathbf{w}\\right\\rangle}\\\\ &{\\quad=\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\xi\\right\\rangle\\otimes\\left|\\mathbf{w}\\right\\rangle\\otimes\\left|F^{\\prime}(\\mathbf{x};\\mathbf{w},\\xi)\\right\\rangle\\otimes\\left|\\psi_{\\mathbf{w},\\xi}^{\\prime}\\right\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(a)$ follows by applying $\\mathbf{A}_{-}$ on the first, third and sixth registers; $(b)$ uses the quantum stochastic function value oracle $\\mathbf{U}_{F}$ on the second, fourth and sixth registers; $(c)$ uses $\\mathbf{A}_{+}$ and $\\mathbf{U}_{F}$ in a way similar to steps $(a)$ and $(b);\\,(d)$ applies sub on the fourth and fifth registers, and then applies Fmul on the fourth register. It is easy to see that this unitary $\\mathbf{D}$ uses only 2 queries to $\\mathbf{U}_{F}$ . ", "page_idx": 13}, {"type": "text", "text": "For any input state $\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\mathbf{w},\\boldsymbol{\\xi}\\right\\rangle\\otimes\\left|0\\right\\rangle\\otimes\\left|0\\right\\rangle^{\\otimes d}$ , apply $\\mathbf{D}$ to obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathbf{x}\\rangle\\otimes|\\boldsymbol{\\xi}\\rangle\\otimes|\\mathbf{w}\\rangle\\otimes|F^{\\prime}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})\\rangle\\otimes|\\psi_{\\mathbf{w},\\boldsymbol{\\xi}}^{\\prime}\\rangle\\otimes|0\\rangle^{\\otimes d}}\\\\ &{=|\\mathbf{x}\\rangle\\otimes|\\boldsymbol{\\xi}\\rangle\\otimes|w_{1},\\cdots,w_{d}\\rangle\\otimes|F^{\\prime}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})\\rangle\\otimes|\\psi_{\\mathbf{w},\\boldsymbol{\\xi}}^{\\prime}\\rangle\\otimes|0\\rangle^{\\otimes d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next we will utilize quantum multiplication operator $\\mathbf{U}_{\\sf m u l}:|a\\rangle\\otimes|b\\rangle\\otimes|c\\rangle\\longrightarrow|a\\rangle\\otimes|b\\rangle\\otimes|c\\oplus a b\\rangle$ . This can be implemented by the quantization of classical multiplication algorithms, whose details can be found in [20, 45, 46]. ", "page_idx": 13}, {"type": "text", "text": "Applying $\\mathrm{{\\mathbf{U}}_{m u l}}$ to each $\\left|w_{i},F^{\\prime}\\right\\rangle\\otimes\\left|0\\right\\rangle$ for all $i\\in[d]$ yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\ \\ |\\mathbf{x}\\rangle\\otimes|\\xi\\rangle\\otimes|w_{1},\\cdots,w_{d}\\rangle\\otimes|F^{\\prime}(\\mathbf{x}+\\delta\\mathbf{w};\\xi)\\rangle\\otimes|\\psi_{\\mathbf{w},\\xi}^{\\prime}\\rangle\\otimes|F^{\\prime}(\\mathbf{x}+\\delta\\mathbf{w};\\xi)w_{1},\\cdots,F^{\\prime}(\\mathbf{x}+\\delta\\mathbf{w};\\xi)w_{d}\\rangle}\\\\ &{=|\\mathbf{x}\\rangle\\otimes|\\xi\\rangle\\otimes|w_{1},\\cdots,w_{d}\\rangle\\otimes|F^{\\prime}(\\mathbf{x}+\\delta\\mathbf{w};\\xi)\\rangle\\otimes|\\psi_{\\mathbf{w},\\xi}^{\\prime}\\rangle\\otimes|F^{\\prime}(\\mathbf{x}+\\delta\\mathbf{w};\\xi)\\mathbf{w}\\rangle}\\\\ &{=|\\mathbf{x}\\rangle\\otimes|\\xi\\rangle\\otimes|w_{1},\\cdots,w_{d}\\rangle\\otimes|F^{\\prime}(\\mathbf{x}+\\delta\\mathbf{w};\\xi)\\rangle\\otimes|\\psi_{\\mathbf{w},\\xi}^{\\prime}\\rangle\\otimes|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\xi)\\rangle}\\\\ &{=|\\mathbf{x}\\rangle\\otimes|\\psi_{\\mathbf{w},\\xi}\\rangle\\otimes|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\xi)\\rangle.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By swapping the last two quantum registers, we obtain $|\\mathbf{x}\\rangle\\otimes|\\mathbf{g}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})\\rangle\\otimes|\\psi_{\\mathbf{w},\\boldsymbol{\\xi}}\\rangle$ . Hence, $\\mathbf{U}_{\\mathbf{g},\\delta}$ can be implemented with two queries to $\\mathbf{U}_{F}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "C The Proof of Corollary 3.4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Analogous to eq. (5), we claim that the following unitary $\\mathbf{V_{g,\\delta}}\\delta$ can be implemented with 4 queries to $\\mathbf{U}_{F}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{V_{g,\\delta}};|\\mathbf{x}\\rangle\\otimes|\\mathbf{y}\\rangle\\otimes|\\boldsymbol{\\xi}\\rangle\\otimes|\\mathbf{w}\\rangle\\otimes|\\boldsymbol{b}\\rangle\\longmapsto|\\mathbf{x}\\rangle\\otimes|\\mathbf{y}\\rangle\\otimes|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\boldsymbol{\\xi})-\\mathbf{g}_{\\delta}(\\mathbf{y};\\mathbf{w},\\boldsymbol{\\xi})\\rangle\\otimes|\\psi_{\\mathbf{w},\\boldsymbol{\\xi}}\\rangle.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With access to $\\mathbf{V_{g,\\delta}}\\delta$ and $\\mathbf{O}_{\\mathbf{g},\\delta}$ , we can construct $\\mathbf{O}_{\\Delta\\mathbf{g}_{\\delta}}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{O}_{\\Delta\\mathbf{g}_{\\delta}}=\\mathbf{V}_{\\mathbf{g},\\delta}\\cdot(\\mathbf{I}\\otimes\\mathbf{I}\\otimes\\mathbf{O}_{\\xi,\\mathbf{w}}\\otimes\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, to implement $\\mathbf{V_{g,\\delta}}\\delta$ with 4 queries to $\\mathbf{U}_{F}$ , we can first follow the steps in eq. (6), eq. (7) and eq. (8) to get a unitary that performs the mapping below ", "page_idx": 13}, {"type": "text", "text": "\u2223 $\\mathbf{x}\\big)\\otimes\\left|\\mathbf{y}\\right\\rangle\\otimes\\left|\\mathbf{\\xi}\\right\\rangle\\otimes\\left|\\mathbf{w}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\otimes\\left|\\mathbf{0}\\right\\rangle\\longmapsto\\left|\\mathbf{x}\\right\\rangle\\otimes\\left|\\mathbf{y}\\right\\rangle\\otimes\\left|\\psi_{\\mathbf{w},\\xi}\\right\\rangle\\otimes\\left|\\mathbf{g}_{\\delta}\\!\\left(\\mathbf{x};\\mathbf{w},\\xi\\right)\\right\\rangle\\otimes\\left|\\mathbf{g}_{\\delta}\\!\\left(\\mathbf{y};\\mathbf{w},\\xi\\right)\\right\\rangle.$ Then applying sub and a SWAP gate to the output above yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\mathbf{x}\\rangle\\otimes|\\mathbf{y}\\rangle\\otimes\\sum_{\\xi,\\mathbf{w}}\\sqrt{\\operatorname*{Pr}[\\mathbf{w},\\xi]}|\\mathbf{g}_{\\delta}(\\mathbf{x};\\mathbf{w},\\xi)-\\mathbf{g}_{\\delta}(\\mathbf{y};\\mathbf{w},\\xi)\\rangle\\otimes|\\psi_{\\mathbf{w},\\xi}\\rangle.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "D The Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Before we present the proof, we first introduce the results for the quantum mean estimation by Sidford and Zhang [48]. ", "page_idx": 13}, {"type": "text", "text": "Theorem D.1 ([48, Theorem 4]). For a random variable $X$ with bounded variance such that V $\\arg\\,[X]\\,\\leq\\,{\\hat{L}}^{2}$ , there exists an algorithm that can output an unbiased estimator $\\hat{\\mu}$ of $\\mu\\\\,=\\,\\mathbb{E}\\left[X\\right]$ satisfying $\\mathbb{E}\\left[\\|\\hat{\\boldsymbol{\\mu}}-\\boldsymbol{\\mu}\\|^{2}\\right]\\leq\\hat{\\sigma}^{2}$ using an expected $\\tilde{\\mathcal{O}}(\\hat{L}\\sqrt{\\hat{d}}\\hat{\\sigma}^{-1})$ queries of quantum sampling oracle $\\mathbf{O}_{X}$ as defined in Definition 3.2. ", "page_idx": 14}, {"type": "text", "text": "Proof. According to Proposition 3.1, the quantum $\\delta\\!\\cdot$ -estimated stochastic gradient oracle given the input $\\mathbf{x}$ satisfies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathbf{g}_{\\delta}\\right]=\\nabla f_{\\delta}(\\mathbf{x})\\quad\\mathrm{and}\\quad\\mathrm{Var}\\left[\\mathbf{g}_{\\delta}\\right]\\leq16\\sqrt{2}\\pi d L^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using Theorem D.1 with $\\hat{L}\\;=\\;\\sqrt{d}L$ , it requires only $\\tilde{\\mathcal{O}}(d L\\hat{\\sigma}_{1}^{-1})$ queries of $\\mathbf{O}_{\\mathbf{g}_{\\delta}}$ to construct the quantum estimator $\\hat{\\bf g}$ such that $\\mathbb{E}\\left[\\|\\hat{\\mathbf{g}}-\\nabla f_{\\delta}(\\mathbf{x})\\|^{2}\\right]\\le\\hat{\\sigma}_{1}^{2}$ . According to Lemma 3.2, we can construct each $\\mathbf{O}_{\\mathbf{g}_{\\delta}}$ by $\\mathcal{O}(1)$ -queries of $\\mathbf{U}_{F}$ . Thus, it only requires $\\tilde{\\mathcal{O}}(d L\\hat{\\sigma}_{1}^{-1})$ queries of $\\mathbf{U}_{F}$ to construct the mini-batch quantum estimator $\\hat{\\bf g}$ . ", "page_idx": 14}, {"type": "text", "text": "Similarly, since we can construct the quantum estimator $\\Delta\\mathbf{g}_{\\delta}$ by $\\mathcal{O}(1)$ -queries of $\\mathbf{U}_{F}$ according to Corollary 3.4, with the following properties ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\Delta\\mathbf{g}_{\\delta}\\right]=\\nabla f_{\\delta}(\\mathbf{x})-\\nabla f_{\\delta}(\\mathbf{y})\\quad\\mathrm{and}\\quad\\mathrm{Var}\\left[\\Delta\\mathbf{g}_{\\delta}\\right]\\le\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{g}_{\\delta}\\right\\|^{2}\\right]\\le d^{2}L^{2}\\delta^{-2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then, using Theorem D.1 with $\\hat{L}=d L\\delta^{-1}\\lVert\\mathbf{x}-\\mathbf{y}\\rVert$ directly leads to second statement. ", "page_idx": 14}, {"type": "text", "text": "E The Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. According to the variance level we set, ${\\bf g}_{t}$ satisfies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\mathbf{g}_{t}-\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}\\right]\\le\\frac{\\epsilon^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "According to Proposition 2.1, $f_{\\delta}(\\cdot)$ is a nonconvex function, with $(\\sqrt{d}L\\delta^{-1})$ -Lipschitz gradient, which implies that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{\\delta}}({\\bf x}_{t+1})\\le f_{\\delta}({\\bf x}_{t})+\\langle\\nabla f_{\\delta}({\\bf x}),{\\bf x}_{t+1}-{\\bf x}_{t}\\rangle+\\frac{\\sqrt{d}L\\delta^{-1}}{2}\\|{\\bf x}_{t+1}-{\\bf x}_{t}\\|^{2}}\\\\ {\\qquad={f_{\\delta}}({\\bf x}_{t})-\\eta\\langle\\nabla f_{\\delta}({\\bf x}),{\\bf g}_{t}\\rangle+\\frac{\\sqrt{d}L\\delta^{-1}}{2}\\|{\\bf x}_{t+1}-{\\bf x}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking expectation on both sides of the above inequality, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{\\delta}(\\mathbf{x}_{t+1})\\leq f_{\\delta}(\\mathbf{x}_{t})-\\eta\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}+\\frac{\\eta^{2}\\sqrt{d}L\\delta^{-1}}{2}\\mathbb{E}\\big[\\|\\mathbf{g}_{t}\\|^{2}\\big]}\\\\ &{\\qquad\\qquad\\leq f_{\\delta}(\\mathbf{x}_{t})-\\eta\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}+\\eta^{2}\\sqrt{d}L\\delta^{-1}\\left(\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}+\\mathbb{E}\\left[\\|\\mathbf{g}_{t}-\\nabla_{\\delta}(\\mathbf{x}_{t})\\|^{2}\\right]\\right)}\\\\ &{\\qquad\\qquad\\leq f_{\\delta}(\\mathbf{x}_{t})-\\left(\\eta-\\sqrt{d}L\\delta^{-1}\\eta^{2}\\right)\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}+\\sqrt{d}L\\delta^{-1}\\eta^{2}\\cdot\\frac{\\epsilon^{2}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We let \u03b7 = 2\u221a\u03b4dL, then it holds that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}\\right]\\leq2\\sqrt{d}L\\delta^{-1}\\left(f_{\\delta}\\big(\\mathbf{x}_{t}\\big)-f_{\\delta}\\big(\\mathbf{x}_{t+1}\\big)\\right)+\\frac{\\epsilon^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing up the above inequality, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\frac{\\sum_{t=0}^{T}\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}}{T}\\right]\\leq\\frac{2\\sqrt{d}L\\delta^{-1}(f_{\\delta}(\\mathbf{x}_{0})-f_{\\delta}^{*})}{T}+\\frac{\\epsilon^{2}}{4}\\leq\\frac{2\\sqrt{d}L\\delta^{-1}(f(x_{0})-f^{*}+2\\delta L)}{T}+\\frac{\\epsilon^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By setting ", "page_idx": 14}, {"type": "equation", "text": "$$\nT=\\left\\lceil2\\epsilon^{-2}(4\\sqrt{d}L^{2}+2\\sqrt{d}L\\delta^{-1}\\Delta)\\right\\rceil,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and choosing $\\mathbf{x}_{\\mathrm{out}}$ randomly from $\\{\\mathbf{x}_{0},\\cdots,\\mathbf{x}_{T-1}\\}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla f_{\\delta}(\\mathbf{x}_{\\mathrm{out}})\\Vert^{2}\\right]\\leq\\frac{1}{T}\\mathbb{E}\\left[\\sum_{i=1}^{T}\\Vert\\nabla f_{\\delta}(\\mathbf{x}_{t})\\Vert^{2}\\right]\\leq\\frac{\\epsilon^{2}}{4}+\\frac{\\epsilon^{2}}{2}\\leq\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using Theorem 3.5, we require ", "page_idx": 15}, {"type": "equation", "text": "$$\nb=\\tilde{\\mathcal{O}}(d L\\epsilon^{-1}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "to achieve the desired variance level. Thus the total quantum query of $\\mathbf{U}_{F}$ can be bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\nb\\cdot T=\\tilde{\\mathcal{O}}\\left(d^{3/2}\\left(\\frac{L\\Delta}{\\epsilon^{3}\\delta}+\\frac{L^{2}}{\\epsilon^{3}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "F The Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We denote $L_{\\delta}~\\triangleq~\\frac{\\sqrt{d}L}{\\delta}$ . We also denote $\\hat{\\bf g}_{t+1}$ as the unbiased estimator of $\\nabla f_{\\delta}(\\mathbf{x}_{t+1})$ we have constructed in line 6 and $\\Delta\\mathbf{g}_{t+1}$ as the unbiased estimator of $\\nabla f_{\\delta}\\big(\\mathbf{x}_{t+1}\\big)-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)$ we have constructed in line 8. We can see that $\\mathbf{g}_{t+1}$ is equivalent to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{g}_{t+1}=\\left\\{\\begin{array}{l l}&{\\hat{\\mathbf{g}}_{t+1}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\mathrm{with~probability~}p_{t}}\\\\ &{\\mathbf{g}_{t}+\\Delta\\mathbf{g}_{t+1}\\;\\;\\;\\;\\;\\mathrm{with~probability~}1-p_{t}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "According to the variance level we set in Theorem 4.3, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\hat{\\mathbf{g}}_{t+1}-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t+1}\\big)\\Vert^{2}\\right]\\leq\\hat{\\sigma}_{1,t+1}^{2}=\\frac{\\epsilon^{2}}{2},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\Delta\\mathbf{g}_{t+1}-\\left(\\nabla f_{\\delta}\\big(\\mathbf{x}_{t+1}\\right)-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)\\right)\\Vert^{2}\\right]\\leq\\hat{\\sigma}_{2,t+1}^{2}=\\epsilon^{2/3}\\Vert\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\Vert^{2}\\frac{L^{4/3}d}{\\delta^{2}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "According to Proposition 2.1, $\\nabla f_{\\delta}(\\cdot)$ is $L_{\\delta}$ -Lipschitz continuous, which means ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{\\delta}\\big(\\mathbf{x}_{t+1}\\big)\\leq f_{\\delta}\\big(\\mathbf{x}_{t}\\big)+\\big\\langle\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\big\\rangle+\\displaystyle\\frac{L_{\\delta}}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}}\\\\ {\\qquad=\\ f_{\\delta}\\big(\\mathbf{x}_{t}\\big)+\\big\\langle\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)-\\mathbf{g}_{t},\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\big\\rangle+\\big\\langle\\mathbf{g}_{t},\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\big\\rangle+\\displaystyle\\frac{L_{\\delta}}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}}\\\\ {\\qquad\\leq\\ f_{\\delta}\\big(\\mathbf{x}_{t}\\big)-\\displaystyle\\frac{\\eta}{2}\\|\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)\\|^{2}-\\displaystyle\\frac{\\eta}{2}\\|\\mathbf{g}_{t}-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)\\|^{2}-\\left(\\displaystyle\\frac{1}{2\\eta}-\\frac{L_{\\delta}}{2}\\right)\\big\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, we track the variance of $\\mathbf{g}_{t+1}$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|\\mathbf{g}_{t+1}-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t+1}\\big)\\right\\|^{2}\\right]}\\\\ &{\\quad=p_{t}\\mathbb{E}\\left[\\left\\|\\hat{\\mathbf{g}}_{t+1}-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t+1}\\big)\\right\\|^{2}\\right]}\\\\ &{\\qquad\\qquad+\\left(1-p_{t}\\right)\\mathbb{E}\\left[\\left\\|\\mathbf{g}_{t}-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)+\\big(\\Delta\\mathbf{g}_{t+1}-\\big(\\nabla f_{\\delta}\\big(\\mathbf{x}_{t+1}\\big)-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)\\big)\\big)\\right\\|^{2}\\right]}\\\\ &{\\quad=p_{t}\\epsilon^{2}+\\left(1-p_{t}\\right)\\left\\|\\mathbf{g}_{t}-\\nabla f_{\\delta}\\big(\\mathbf{x}_{t}\\big)\\right\\|^{2}+\\left(1-p_{t}\\right)\\cdot\\frac{L^{4/3}\\epsilon^{2/3}d}{\\delta^{2}}\\big\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have $p_{t}\\,\\,\\equiv\\,p$ and can denote $\\begin{array}{r}{\\Phi_{t}\\triangleq f_{\\delta}(\\mathbf{x}_{t})-f^{*}+\\frac{\\eta}{2p}\\|\\mathbf{g}_{t}-\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}}\\end{array}$ . Combining eq. (9) and eq. (10), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Phi_{t+1}\\right]=\\mathbb{E}\\left[f_{\\delta}(\\mathbf{x}_{t+1})+\\frac{\\eta}{2p}\\|\\mathbf{g}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[f_{\\delta}(\\mathbf{x}_{t})-\\frac{\\eta}{2}\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}-\\frac{\\eta}{2}\\|\\mathbf{g}_{t}-\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}-\\left(\\frac{1}{2\\eta}-\\frac{L_{\\delta}}{2}\\right)\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\frac{\\eta}{2p}\\mathbb{E}\\left[p\\epsilon^{2}+(1-p)\\|\\mathbf{g}_{t}-\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}+(1-p)\\frac{L^{4/3}d\\epsilon^{2/3}}{\\delta^{2}}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\Phi_{t}\\right]-\\frac{\\eta}{2}\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}-\\left(\\frac{1}{2\\eta}-\\frac{L_{\\delta}}{2}-\\frac{\\eta(1-p)}{p}\\cdot\\left(\\frac{L^{4/3}d\\epsilon^{2/3}}{\\delta^{2}}\\right)\\right)\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}+\\frac{\\eta\\epsilon^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We have chosen ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2L_{\\delta}}\\ \\ \\mathrm{and}\\ \\,p=\\frac{\\epsilon^{2/3}}{L^{2/3}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "such that ", "page_idx": 16}, {"type": "equation", "text": "$$\nA\\geq{\\frac{\\sqrt{d}L}{2\\delta}}-{\\frac{\\delta}{2{\\sqrt{d}}L}}\\cdot{\\frac{L^{2/3}}{\\epsilon^{2/3}}}\\cdot{\\frac{L^{4/3}d\\epsilon^{2/3}}{\\delta^{2}}}=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, eq. (11) implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\|\\nabla f_{\\delta}(\\mathbf{x}_{t})\\|^{2}\\right]\\leq\\frac{2}{\\eta}\\mathbb{E}\\left[\\Phi_{t}-\\Phi_{t+1}\\right]+\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{2}{\\eta}\\mathbb{E}\\left[\\Phi_{0}-\\Phi_{T}\\right]\\leq\\displaystyle\\frac{2}{\\eta}\\mathbb{E}\\left[f_{\\delta}\\big(\\mathbf{x}_{0}\\big)-f_{\\delta}^{*}+\\frac{\\eta}{2p}\\big\\|\\hat{\\mathbf{g}}_{0}-\\nabla f\\big(\\mathbf{x}_{0}\\big)\\big\\|^{2}\\right]}\\\\ {\\displaystyle\\leq\\frac{2}{\\eta}\\mathbb{E}\\left[f\\big(\\mathbf{x}_{0}\\big)-f^{*}+2\\delta L+\\frac{\\eta}{2p}\\big\\|\\hat{\\mathbf{g}}_{0}-\\nabla f\\big(\\mathbf{x}_{0}\\big)\\big\\|^{2}\\right]}\\\\ {\\displaystyle\\leq\\frac{2}{\\eta}\\left(\\Delta+2\\delta L\\right)+\\frac{1}{p}\\epsilon^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "summing up eq. (13) from $t=0,\\cdots,T-1$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{i=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla f_{\\delta}(\\mathbf{x}_{i})\\right\\|^{2}\\right]\\leq\\frac{2}{\\eta T}\\mathbb{E}\\left[\\Phi_{0}-\\Phi_{T}\\right]+\\frac{\\epsilon^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By choosing ", "page_idx": 16}, {"type": "equation", "text": "$$\nT=\\left\\lceil8L_{\\delta}\\epsilon^{-2}\\left(\\Delta+2\\delta L\\right)+\\frac{4}{p}\\right\\rceil,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\nabla f_{\\delta}(\\mathbf{x}_{\\mathrm{out}})\\Vert^{2}\\right]=\\frac{1}{T}\\sum_{i=0}^{T-1}\\mathbb{E}\\left[\\Vert\\nabla f_{\\delta}(\\mathbf{x}_{i})\\Vert^{2}\\right]\\leq\\frac{2}{\\eta T}\\mathbb{E}\\left[\\Phi_{0}-\\Phi_{T}\\right]+\\frac{\\epsilon^{2}}{2}\\leq\\frac{\\epsilon^{2}}{4}+\\frac{\\epsilon^{2}}{4}+\\frac{\\epsilon^{2}}{2}=\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using Theorem 3.5, the expectation queries of $\\mathbf{U}_{F}$ to construct $\\hat{\\bf g}_{t}$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\nb_{0}=\\tilde{\\mathcal{O}}\\left(d L\\hat{\\sigma}_{1,t}^{-1}\\right)=\\tilde{\\mathcal{O}}\\left(d L\\epsilon^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the expectation queries of $\\mathbf{U}_{F}$ to construct $\\Delta{\\bf g}_{t}$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\nb_{1}=\\tilde{\\mathcal{O}}\\left(d^{3/2}L\\lVert\\mathbf{x}_{t-1}-\\mathbf{x}_{t}\\rVert\\hat{\\sigma}_{2,t}^{-1}\\delta^{-1}\\right)=\\tilde{\\mathcal{O}}\\left(d L^{1/3}\\epsilon^{-1/3}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, the total quantum queries of $\\mathbf{U}_{F}$ for finding the $(\\delta,\\epsilon)$ -stationary point of $f(\\cdot)$ can be bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathcal{O}}(T(b_{0}p+b_{1}(1-p)))=\\tilde{\\mathcal{O}}\\left(\\sqrt{d}L\\epsilon^{2}(\\Delta+2\\delta L)\\cdot\\left(d L\\epsilon^{-1}L^{-2/3}\\epsilon^{2/3}+d L^{1/3}\\epsilon^{-1/3}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\tilde{\\mathcal{O}}\\left(d^{3/2}\\left(\\frac{L^{4/3}\\Delta}{\\epsilon^{7/3}\\delta}+\\frac{L^{7/3}}{\\epsilon^{7/3}}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which finishes the proof. ", "page_idx": 16}, {"type": "text", "text": "1: Construct ${\\bf g}_{0}$ as an unbiased estimator of $\\nabla f(\\mathbf{x}_{0})$ with variance at most $\\hat{\\sigma}_{1,0}^{2}$ .   \n2: for $t=0,1\\ldots T$   \n3: $\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta\\mathbf{g}_{t}$   \n4: Flip a coin $\\theta_{t}\\in\\{0,1\\}$ where $P(\\theta_{t}=1)=p_{t}$   \n5: If $\\theta_{t}=1$ then   \n6: Construct $\\mathbf{g}_{t+1}$ as an unbiased quantum estimator of $\\nabla f(\\mathbf{x}_{t+1})$ with variance at most   \n\u03c3\u02c612,t+1.   \n7: else   \n8: Construct $\\Delta\\mathbf{g}_{t+1}$ as an unbiased quantum estimator of $\\nabla f(\\mathbf{x}_{t+1})-\\nabla f(\\mathbf{x}_{t})$ with variance   \nat most \u03c3\u02c622,t+1.   \n9: $\\mathbf{g}_{t+1}=\\mathbf{g}_{t}+\\Delta\\mathbf{g}_{t+1}.$ ", "page_idx": 17}, {"type": "text", "text": "G Improved Results for Quantum Stochastic Smooth Non-convex Optimization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Sidford and Zhang [48] introduced Q-SPIDER for smooth non-convex optimization, with the query complexity of $\\bar{\\mathcal{O}}(\\bar{d}^{1/2}\\epsilon^{-5/2})$ on the quantum stochastic gradient oracle. Using the same framework of $\\mathrm{QGFM+}$ , we propose the fast quantum gradient method $\\mathrm{(QGM+)}$ , which further improves the query complexity of Q-SPIDER. ", "page_idx": 17}, {"type": "text", "text": "We present the $\\mathrm{QGM+}$ in Algorithm 3. The main difference between $\\mathrm{QGFM+}$ and $\\mathrm{QGM+}$ is that QGM constructs the estimators for $\\nabla f(\\mathbf{x})$ and $\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})$ instead of their smoothed surrogates in line 6 and line 8 by using the quantum stochastic gradient oracle directly [48, Definition 4]. We present the setting for Q-SPIDER as follows for being self-contained. ", "page_idx": 17}, {"type": "text", "text": "Assumption 2 ([48, Setting of Theorem 7]). We assume that we are able to access the quantum stochastic oracle that outputs $\\nabla F(\\cdot;\\xi)$ which is a stochastic gradient of $f(\\cdot)$ that satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}[\\nabla F(\\mathbf{x};\\xi)]=\\nabla f(\\mathbf{x}),~~\\mathbb{E}_{\\xi}\\left[\\|\\nabla F(\\mathbf{x};\\xi)-\\nabla f(\\mathbf{x})\\|\\right]\\le\\sigma^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi}\\left[\\|\\nabla F(\\mathbf{x};\\xi)-\\nabla F(\\mathbf{y};\\xi)\\|^{2}\\right]\\le l^{2}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We also present the definition of $\\epsilon$ -stationary point of a smooth function. ", "page_idx": 17}, {"type": "text", "text": "Definition G.1. We say $\\mathbf{x}$ is an $\\epsilon$ -stationary point of a smooth function $f(\\cdot)$ , if it satisfies that $\\|\\nabla f(\\mathbf{x})\\|\\leq\\epsilon.$ . ", "page_idx": 17}, {"type": "text", "text": "We present the query complexity of $\\mathrm{QGM+}$ in the following theorem. ", "page_idx": 17}, {"type": "text", "text": "Theorem G.1. Under the same setting of $I^{48}$ , Theorem 7] for $Q$ -SPIDER, $Q G M+$ (Algorithm 3) finds the $\\epsilon$ -stationary point of $f(\\cdot)$ using an expected $\\tilde{\\mathcal{O}}(\\sqrt{d}\\epsilon^{-7/3})$ queries of quantum stochasic gradient oracle by setting ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\eta=\\frac{1}{2l},\\ \\ \\ p_{t}\\equiv\\epsilon^{2/3}\\sigma^{-2/3},\\ \\ \\ {\\hat{\\sigma}_{1,t}}^{2}\\equiv\\frac{\\epsilon^{2}}{2},\\ \\ \\ a n d\\ \\ {\\hat{\\sigma}_{2,t}}^{2}=\\frac{l^{2}\\epsilon^{2/3}\\lVert{\\bf x}_{t}-{\\bf x}_{t-1}\\rVert}{\\sigma^{2/3}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. According to the variance level we set in Theorem G.1 We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\Vert\\hat{\\mathbf{g}}_{t+1}-\\nabla f\\big(\\mathbf{x}_{t+1}\\big)\\right\\Vert^{2}\\right]\\leq\\hat{\\sigma}_{1,t+1}^{2}=\\frac{\\epsilon^{2}}{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\Vert\\Delta\\mathbf{g}_{t+1}-\\left(\\nabla f\\left(\\mathbf{x}_{t+1}\\right)-\\nabla f(\\mathbf{x}_{t})\\right)\\Vert^{2}\\right]\\leq\\hat{\\sigma}_{2,t+1}^{2}=\\frac{l^{2}\\epsilon^{2/3}}{\\sigma^{2/3}}\\Vert\\mathbf{x}-\\mathbf{y}\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f\\big(\\mathbf{x}_{t+1}\\big)\\leq f\\big(\\mathbf{x}_{t}\\big)+\\big\\langle\\nabla f\\big(\\mathbf{x}_{t}\\big),\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\big\\rangle+\\displaystyle\\frac{l}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}}\\\\ {\\qquad=f\\big(\\mathbf{x}_{t}\\big)+\\big\\langle\\nabla f\\big(\\mathbf{x}_{t}\\big)-\\mathbf{g}_{t},\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\big\\rangle+\\big\\langle\\mathbf{g}_{t},\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\big\\rangle+\\displaystyle\\frac{l}{2}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}}\\\\ {\\qquad\\leq f\\big(\\mathbf{x}_{t}\\big)-\\displaystyle\\frac{\\eta}{2}\\|\\nabla f\\big(\\mathbf{x}_{t}\\big)\\|^{2}-\\displaystyle\\frac{\\eta}{2}\\|\\mathbf{g}_{t}-\\nabla f\\big(\\mathbf{x}_{t}\\big)\\|^{2}-\\left(\\displaystyle\\frac{1}{2\\eta}-\\displaystyle\\frac{l}{2}\\right)\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The variance of $\\mathbf{g}_{t+1}$ can be traced by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t+1}-\\nabla f\\big(\\mathbf{x}_{t+1}\\big)\\right\\Vert^{2}\\right]}\\\\ &{=p_{t}\\mathbb{E}\\left[\\left\\Vert\\hat{\\mathbf{g}}_{t+1}-\\nabla f\\big(\\mathbf{x}_{t+1}\\big)\\right\\Vert^{2}\\right]}\\\\ &{\\qquad\\qquad+\\left(1-p_{t}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}-\\nabla f\\big(\\mathbf{x}_{t}\\big)+\\left(\\Delta\\mathbf{g}_{t+1}-\\left(\\nabla f\\big(\\mathbf{x}_{t+1}\\big)-\\nabla f\\big(\\mathbf{x}_{t}\\big)\\right)\\right)\\right\\Vert^{2}\\right]}\\\\ &{=p_{t}\\epsilon^{2}+\\left(1-p_{t}\\right)\\left\\Vert\\mathbf{g}_{t}-\\nabla f\\big(\\mathbf{x}_{t}\\big)\\right\\Vert^{2}+\\left(1-p_{t}\\right)\\frac{l^{2}\\epsilon^{2/3}}{\\sigma^{2/3}}\\cdot\\left\\Vert\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\right\\Vert^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We let $p_{t}\\,\\equiv\\,p$ and denote $\\begin{array}{r}{\\Phi_{t}\\triangleq f(\\mathbf{x}_{t})-f^{*}+\\frac{\\eta}{2p}\\|\\mathbf{g}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}}\\end{array}$ . Combining eq. (15) and eq. (16), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Phi_{t+1}\\right]=\\mathbb{E}\\left[f(\\mathbf{x}_{t+1})+\\frac{\\eta}{2p}\\|\\mathbf{g}_{t+1}-\\nabla f(\\mathbf{x}_{t+1})\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[f(\\mathbf{x}_{t})-\\frac{\\eta}{2}\\|\\nabla f(\\mathbf{x}_{t})\\|^{2}-\\frac{\\eta}{2}\\|\\mathbf{g}_{t}-\\nabla f(\\mathbf{x}_{t})\\|^{2}-\\left(\\frac{1}{2\\eta}-\\frac{l}{2}\\right)\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad+\\underbrace{\\frac{\\eta}{2p}\\mathbb{E}\\left[p\\epsilon^{2}+(1-p)\\left\\|\\mathbf{g}_{t}-\\nabla f(\\mathbf{x}_{t})\\right\\|^{2}+\\left(1-p\\right)\\frac{l^{2}\\epsilon^{2/3}}{\\sigma^{2/3}}\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}\\right]}_{\\leq\\mathbb{E}\\left[\\Phi_{t}\\right]-\\frac{\\eta}{2}\\|\\nabla f(\\mathbf{x}_{t})\\|^{2}-\\underbrace{\\left(\\frac{1}{2\\eta}-\\frac{l}{2}-\\frac{\\eta\\left(1-p\\right)}{p}\\cdot\\left(\\frac{l^{2}\\epsilon^{2/3}}{\\sigma^{2/3}}\\right)\\right)\\|\\mathbf{x}_{t+1}-\\mathbf{x}_{t}\\|^{2}+\\frac{\\eta\\epsilon^{2}}{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since we have chosen $\\begin{array}{r}{\\eta=\\frac{1}{2l}}\\end{array}$ and $p=\\epsilon^{2/3}\\sigma^{-2/3}$ , it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\nB\\geq\\frac{l}{2}\\left(1-\\frac{\\epsilon^{2/3}}{p\\sigma^{2/3}}\\right)\\geq0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{i=0}^{T-1}\\mathbb{E}\\left[\\left\\|\\nabla f(\\mathbf{x}_{i})\\right\\|^{2}\\right]\\leq\\displaystyle\\frac{2}{\\eta T}\\mathbb{E}\\left[\\Phi_{0}-\\Phi_{T}\\right]+\\frac{\\epsilon^{2}}{2}}&{}\\\\ {\\displaystyle\\leq\\frac{2}{\\eta T}\\mathbb{E}\\left[f(\\mathbf{x}_{0})-f(\\mathbf{x}_{T})+\\frac{\\eta}{p}\\left\\|\\mathbf{g}_{0}-\\nabla f(\\mathbf{x}_{0})\\right\\|^{2}\\right]+\\frac{\\epsilon^{2}}{2}}&{}\\\\ {\\displaystyle}&{\\leq\\epsilon^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality is by setting ", "page_idx": 18}, {"type": "equation", "text": "$$\nT=\\left\\lceil8l\\Delta\\epsilon^{-2}+4\\sigma^{2/3}\\epsilon^{-4/3}\\right\\rceil.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Below, we bound the total queries of quantum stochastic gradient oracles. The expectation oracles to construct $\\hat{\\bf g}_{t}$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\nb_{0}=\\tilde{\\mathcal{O}}\\left(\\sqrt{d}\\sigma\\hat{\\sigma}_{1,t}^{-1}\\right)=\\tilde{\\mathcal{O}}\\left(\\sigma\\sqrt{d}\\epsilon^{-1}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and the expectation queries to construct $\\Delta{\\bf g}_{t}$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\nb_{1}=\\tilde{\\mathcal{O}}\\left(\\sqrt{d}l\\|\\mathbf{x}_{t}-\\mathbf{x}_{t-1}\\|\\hat{\\sigma}_{2,t}^{-1}\\right)=\\tilde{\\mathcal{O}}\\left(\\sqrt{d}\\sigma^{1/3}\\epsilon^{-1/3}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, the total stochastic quantum gradient oracles for finding the $\\epsilon$ -stationary point of $f(\\cdot)$ can be bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\nT(b_{0}p+(1-p)b_{1})=\\tilde{\\mathcal{O}}\\left(\\sqrt{d}(l\\Delta\\sigma^{1/3}\\epsilon^{-7/3}+\\sigma\\epsilon^{-5/3})\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remark G.2. $\\mathrm{QGM+}$ (Algorithm 3) improves the quantum stochastic gradient oracle of Q-SPIDER ([48, Algorithm 7]) by a factor of $\\epsilon^{-1/6}$ . ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We have discussed our limitations in Section 5. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ", "page_idx": 21}, {"type": "text", "text": "\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] This paper focus on the theory of solving nonlinear equations. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake proflies, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety fliters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] We use open access datasets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip flie. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]