[{"figure_path": "xqc8yyhScL/figures/figures_1_1.jpg", "caption": "Figure 1: Domains, including standard ones that resemble programs found in pretraining data, as well as a less common graphics domain, which is likely less represented in LLM pretraining data.", "description": "This figure shows three different domains used to evaluate the performance of LLMs in programming-by-example (PBE) tasks.  These domains represent varying levels of complexity and typicality in LLM training data. \n\n* **Lists:** This domain involves manipulating lists of numbers, which is relatively common in programming datasets used for pre-training LLMs.\n* **Graphics:** This domain is less common in LLM training data and involves generating graphics programs. This helps evaluate the model's ability to generalize beyond typical programming tasks.\n* **Text Editing Macros:** This domain is somewhere in between in terms of frequency and focuses on string manipulations that is prevalent in spreadsheet software. This task evaluates inductive reasoning in transforming input strings into output strings. Each domain is visually illustrated with example inputs, outputs, and the automatically generated program (in Python) by the fine-tuned language model.", "section": "1 Introduction"}, {"figure_path": "xqc8yyhScL/figures/figures_2_1.jpg", "caption": "Figure 2: Left: Data generation pipeline. Right: The fine-tuned network qe learns to do inference in a graphical model where the prior over programs, G, is defined by prompting an LLM with example code in Dseed, while the likelihood p(Y|p, X) is defined by program execution.", "description": "The figure illustrates the data generation pipeline and the fine-tuned model's inference process.  The left side shows how a seed dataset (Dseed) defines a prior (G) over programs, which is then used to generate a larger dataset (Dtune). The right side depicts the fine-tuned LLM (qe) performing inference in a graphical model, using the generated dataset to predict programs given input-output pairs (X,Y). The likelihood in this model is based on program execution, connecting the generated program's output to the observed outputs.", "section": "Methods"}, {"figure_path": "xqc8yyhScL/figures/figures_2_2.jpg", "caption": "Figure 2: Left: Data generation pipeline. Right: The fine-tuned network qe learns to do inference in a graphical model where the prior over programs, G, is defined by prompting an LLM with example code in Dseed, while the likelihood p(Y|p, X) is defined by program execution.", "description": "The figure illustrates the data generation pipeline and the fine-tuned model for program synthesis.  The left side shows how a seed dataset (Dseed) is used to generate a larger dataset (Dtune) by prompting a large language model (LLM) and running the generated programs to obtain outputs (Y). The right side displays the graphical model used for fine-tuning the LLM (qe). In this model, the prior over programs (G) is based on the seed dataset, and the likelihood p(Y|p, X) is determined by program execution.", "section": "3 Methods"}, {"figure_path": "xqc8yyhScL/figures/figures_4_1.jpg", "caption": "Figure 3: Test set performance. A problem is solved if the predicted program generates correct outputs on the holdout inputs.", "description": "This figure presents the performance of the fine-tuned model on three different domains: lists, strings, and graphics.  The x-axis represents the search budget (number of samples generated and evaluated), and the y-axis shows the percentage of problems solved. Each line represents a different model, including various sizes of the fine-tuned model, GPT-4, and GPT-4 with chain-of-thought prompting.  The figure demonstrates that the fine-tuned models outperform other baselines, especially in the list and string domains.  The relatively flat performance curve for larger search budgets suggests that the fine-tuned models effectively solve most problems within their respective domains.", "section": "4 Experiments"}, {"figure_path": "xqc8yyhScL/figures/figures_5_1.jpg", "caption": "Figure 1: Domains, including standard ones that resemble programs found in pretraining data, as well as a less common graphics domain, which is likely less represented in LLM pretraining data.", "description": "This figure shows three different domains used to evaluate the effectiveness of LLMs in solving programming-by-example (PBE) tasks.  The domains are: (a) List manipulation, which involves simple list processing algorithms; (b) Text editing macros, demonstrating tasks such as date formatting; and (c) Graphics programming using the LOGO/Turtle programming language. The figure highlights the variety of tasks and the varying levels of representation in typical LLM pretraining datasets.", "section": "1 Introduction"}, {"figure_path": "xqc8yyhScL/figures/figures_5_2.jpg", "caption": "Figure 6: Example out-of-distribution LOGO test: inferring a graphics program from a hand drawing. See also Appendix Fig. 12", "description": "This figure demonstrates the model's ability to generalize to out-of-distribution data.  The model was initially trained on clean, computer-generated LOGO graphics.  However, this figure shows the model's performance when given hand-drawn images as input.  While the model's accuracy decreases, it still generates reasonable programs, indicating a degree of robustness and generalization capability.", "section": "4.3 Out-of-distribution generalization"}, {"figure_path": "xqc8yyhScL/figures/figures_7_1.jpg", "caption": "Figure 8: Out-of-distribution generalization and adaptation to new test distribution.", "description": "This figure shows the results of out-of-distribution generalization experiments on three different domains: List, String, and Logo.  Each subplot displays the percentage of problems solved as a function of the search budget (number of samples). The blue line represents the performance before adaptation, the orange line shows the performance after adaptation, the dotted line shows the finetuned model's performance on in-distribution problems, and the orange line represents the performance after adaptation. The results demonstrate that while there is some degradation in performance when the model is tested out-of-distribution, adaptation techniques can significantly improve the model's ability to solve out-of-distribution problems.", "section": "4.3 Out-of-distribution generalization"}, {"figure_path": "xqc8yyhScL/figures/figures_7_2.jpg", "caption": "Figure 9: Out-of-distribution LOGO problems (requiring long programs with > 12 lines of code). We show example problems that are solved by the original model fine-tuned on short programs, which then become training data for the next round of adaptation. Adaptation allows consistent solving of problems similar to those that the original fine-tuned model could sometimes solve, but is not a panacea: Problems dissimilar to those solved by the initial model are not ever correctly generated, despite the fact that they are solvable by a model fine-tuned in-distribution.", "description": "This figure visualizes the specific problems solved before and after adaptation on LOGO graphics. Before adaptation, only a few out-of-distribution problems are solvable, often requiring a significant search budget. After adaptation, the system can solve similar out-of-distribution problems more quickly. However, it cannot generalize to problems drastically different from those initially solvable by the fine-tuned model.  The results suggest that while adaptation helps, it's not a complete solution for handling all out-of-distribution problems.", "section": "Out-of-distribution generalization"}, {"figure_path": "xqc8yyhScL/figures/figures_16_1.jpg", "caption": "Figure 3: Test set performance. A problem is solved if the predicted program generates correct outputs on the holdout inputs. Metagol [47], RobustFill [20], and Fleet [48] results taken from [17].", "description": "This figure shows the test set performance of the models on three different domains: lists, strings, and graphics.  The x-axis represents the search budget (number of samples), and the y-axis represents the percentage of problems solved.  The plot shows that the fine-tuned models significantly outperform existing symbolic and neuro-symbolic methods across all domains.", "section": "4 Experiments"}, {"figure_path": "xqc8yyhScL/figures/figures_21_1.jpg", "caption": "Figure 1: Domains, including standard ones that resemble programs found in pretraining data, as well as a less common graphics domain, which is likely less represented in LLM pretraining data.", "description": "This figure showcases three different domains used to evaluate the effectiveness of LLMs in Programming-by-Example (PBE):  lists (algorithms operating on numerical lists), text editing macros (string manipulation), and LOGO/Turtle graphics (geometric drawing). The inclusion of the graphics domain is particularly relevant because it represents a less common programming paradigm, likely under-represented in the datasets used to train large language models.  Each domain includes example input-output pairs and the corresponding generated Python program.  The intention is to illustrate the broad applicability of LLMs to various programming tasks, some more familiar to LLM training and others less so.", "section": "Experiments"}, {"figure_path": "xqc8yyhScL/figures/figures_22_1.jpg", "caption": "Figure 1: Domains, including standard ones that resemble programs found in pretraining data, as well as a less common graphics domain, which is likely less represented in LLM pretraining data.", "description": "This figure showcases three different domains used to evaluate the effectiveness of LLMs in solving programming-by-example (PBE) tasks.  These domains represent varying levels of complexity and representation in typical LLM training data:\n\n1. **Lists:** Involves manipulating lists of numbers, a common task in programming and well-represented in LLM training data.\n2. **Graphics:** Uses a LOGO/Turtle-based graphics programming language, representing a less common domain likely underrepresented in LLM training data. This demonstrates the model's ability to generalize to less familiar tasks.\n3. **Text Editing Macros:** Focuses on tasks involving string manipulation common in text-editing applications. This also tests the model's ability to handle string-based tasks.\n\nEach domain includes provided examples (input and output), and the generated program produced by an LLM attempting to solve the PBE task. The selection of these diverse domains helps to thoroughly evaluate the LLM's generalization capabilities across various programming paradigms and levels of difficulty.", "section": "Experiments"}, {"figure_path": "xqc8yyhScL/figures/figures_23_1.jpg", "caption": "Figure 12: Hand-drawn LOGO test showing every generated sample. We built a graphical interface to allow users to draw images as input. The sample budget for this demo is 64.", "description": "This figure shows examples of hand-drawn LOGO images used as input to test the model's ability to generalize to out-of-distribution data.  A graphical interface was created to allow users to input their own hand drawings. The figure showcases various generated outputs from the model for a sample budget of 64, demonstrating the diversity of generated LOGO programs based on the input hand drawing.", "section": "4.3 Out-of-distribution generalization"}]