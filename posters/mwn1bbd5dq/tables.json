[{"figure_path": "mwN1bbD5DQ/tables/tables_7_1.jpg", "caption": "Table 1: Ablation study of debLoRA. We apply our debLoRA based on LoRA and cLORA. Results are reported for the adaptation from SD \u2192 DOTA recognizer. Params (M) refers to the number of updated parameters during the adaptation. Our results are marked in gray.", "description": "This table presents an ablation study evaluating the effectiveness of the proposed debLoRA method. It compares different model adaptation techniques: Zero-Shot, Fine-Tune, cLoRA, cLoRA with debLoRA, LoRA, and LoRA with debLoRA, in terms of macro F1 score (considering head, middle, and tail classes) and the number of parameters updated during adaptation on the DOTA dataset. The results demonstrate that debLoRA consistently improves the performance on tail classes without significantly affecting the performance on head classes.", "section": "5 Experiments and Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_7_2.jpg", "caption": "Table 2: Compare LoRA ranks. The table compares different ranks of the LoRA module. Our results are marked in gray.", "description": "This table presents the results of an ablation study comparing different ranks (8, 16, 32, 64) of the LoRA module on a long-tailed classification task.  The macro F1 score (a metric considering all classes equally important) is shown for head, middle, and tail classes, along with an overall score. The number of parameters (in millions) for each LoRA rank is also provided.  Rows with gray shading highlight the results when the debLoRA method is used.", "section": "Experiments and Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_8_1.jpg", "caption": "Table 3: State-of-the-art comparison under different adaptation settings. The experiments are conducted on two RS adaptation settings: 1) Natural\u2192ORS, where we adopt Stable Diffusion (SD) and OpenCLIP as foundation models and DOTA as the target dataset. 2) ORS\u2192SAR, where we adopt SatMAE as the foundation model and FUSRS (SAR imagery dataset) as the target dataset. Results are evaluated by linear probing and reported in macro F1-Score (%). The highest result in each position is highlighted by bold. Our results are marked in gray.", "description": "This table compares the proposed debLoRA method with state-of-the-art methods for two remote sensing (RS) adaptation tasks: adapting natural image foundation models to RS (Natural \u2192 ORS) and adapting ORS foundation models to SAR (ORS \u2192 SAR).  The results are presented as macro F1-scores for three class groups (head, middle, and tail classes) and an overall mean, reflecting performance on different data distribution scenarios. The table highlights debLoRA's superior performance, especially for tail classes (under-represented categories), demonstrating its effectiveness in mitigating class imbalance issues.", "section": "5 Experiments and Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_8_2.jpg", "caption": "Table 4: Evaluation on the oriented object detection task. We implement debLoRA for long-tailed detection tasks. Our results are marked in gray", "description": "This table presents the results of an oriented object detection task on a long-tailed dataset.  The methods compared include several baselines (Zero-Shot, Fine-Tune, LoRA) and other state-of-the-art long-tailed object detection methods (Reweight, ECM).  The proposed debLoRA method is shown to improve performance, particularly for tail classes, outperforming other methods across all categories (head, middle, and tail). mAP (mean Average Precision) is used as the evaluation metric.  The results highlight the effectiveness of debLoRA in addressing the long-tailed distribution issue in object detection tasks.", "section": "5 Experiments and Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_20_1.jpg", "caption": "Table 3: State-of-the-art comparison under different adaptation settings. The experiments are conducted on two RS adaptation settings: 1) Natural\u2192ORS, where we adopt Stable Diffusion (SD) and OpenCLIP as foundation models and DOTA as the target dataset. 2) ORS\u2192SAR, where we adopt SatMAE as the foundation model and FUSRS (SAR imagery dataset) as the target dataset. Results are evaluated by linear probing and reported in macro F1-Score (%). The highest result in each position is highlighted by bold. Our results are marked in gray.", "description": "This table compares the performance of debLoRA against state-of-the-art methods on two remote sensing image adaptation tasks: adapting natural image foundation models to optical remote sensing (ORS) images (Natural \u2192 ORS) and adapting ORS models to synthetic aperture radar (SAR) images (ORS \u2192 SAR).  The results, evaluated using macro F1-score, show debLoRA's superiority in handling long-tailed distributions, especially for the challenging tail classes.  The table highlights the consistent improvement debLoRA provides across different adaptation settings and foundation models.", "section": "5 Experiments and Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_21_1.jpg", "caption": "Table 3: State-of-the-art comparison under different adaptation settings. The experiments are conducted on two RS adaptation settings: 1) Natural\u2192ORS, where we adopt Stable Diffusion (SD) and OpenCLIP as foundation models and DOTA as the target dataset. 2) ORS\u2192SAR, where we adopt SatMAE as the foundation model and FUSRS (SAR imagery dataset) as the target dataset. Results are evaluated by linear probing and reported in macro F1-Score (%). The highest result in each position is highlighted by bold. Our results are marked in gray.", "description": "This table compares the performance of the proposed debLoRA method against state-of-the-art methods on two remote sensing (RS) adaptation tasks: adapting natural image models to optical RS images (Natural\u2192ORS), and adapting optical RS models to synthetic aperture radar (SAR) images (ORS\u2192SAR).  The results, evaluated using macro F1-score, show debLoRA's superior performance across different adaptation scenarios, especially for the challenging tail classes (classes with fewer samples).", "section": "Experiments and Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_21_2.jpg", "caption": "Table A3: Sensitivity to cluster number K. Default value is marked in gray.", "description": "This table presents the ablation study results on the sensitivity of the debLoRA method to the number of clusters (K) used in the de-biasing process. The experiments were conducted on the SD \u2192 DOTA adaptation task, and the results are reported in terms of macro F1 score for head, middle, and tail classes. The default value of K=32 is highlighted in gray.", "section": "A.3 Ablation Studies and Additional Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_21_3.jpg", "caption": "Table A4: Quantitative feature analysis. Class distance is evaluated by cosine distance and reported on the DOTA dataset.", "description": "This table presents a quantitative analysis of the learned features by different methods (Fine-tuning, LoRA, and debLoRA) on the DOTA dataset.  It shows the inter-class and intra-class distances measured by cosine similarity. Inter-class distances are calculated between head and tail classes, and between tail classes only. Intra-class distance is measured within the tail classes. The results demonstrate debLoRA's effectiveness in increasing inter-class distances while reducing intra-class distances, especially for tail classes, leading to improved feature separation and classification performance.", "section": "A.3 Ablation Studies and Additional Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_21_4.jpg", "caption": "Table 1: Ablation study of debLoRA. We apply our debLoRA based on LoRA and cLORA. Results are reported for the adaptation from SD \u2192 DOTA recognizer. Params (M) refers to the number of updated parameters during the adaptation. Our results are marked in gray.", "description": "This table presents an ablation study comparing different methods for adapting a Stable Diffusion model to the DOTA dataset for object recognition.  It shows the macro F1 scores (a measure of classification accuracy) for head, middle, and tail classes (representing classes with varying numbers of samples) using different methods: zero-shot, fine-tuning, cLoRA, LoRA, LoRA with ResLT, and LoRA with debLoRA.  The table highlights the performance improvement achieved by the proposed debLoRA method, especially for tail classes, while maintaining comparable performance on head and middle classes, and also shows the number of parameters updated for each method.", "section": "5 Experiments and Analyses"}, {"figure_path": "mwN1bbD5DQ/tables/tables_22_1.jpg", "caption": "Table 6: Comparison with SSL methods.", "description": "This table compares the proposed debLoRA method with self-supervised learning (SSL) methods such as SatMAE and MoCo-v3 in terms of computational cost (GPU hours), model parameters (#Params), and data size used for training.  It highlights the significant reduction in computational cost and data requirements achieved by debLoRA compared to SSL methods.", "section": "A.3 Ablation Studies and Additional Analyses"}]