[{"type": "text", "text": "RAVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Maya Varma Jean-Benoit Delbrouck Stanford University Stanford University; Hugging Face mayavarma@cs.stanford.edu jbdel@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Zhihong Chen Akshay Chaudhari\u2020 Curtis Langlotz\u2020 Stanford University Stanford University Stanford University zhihongc@stanford.edu akshaysc@stanford.edu langlotz@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RAVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RAVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RAVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RAVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RAVL accurately discovers ( $191\\%$ improvement over the closest baseline) and mitigates $8.2\\%$ improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on generaldomain and medical-domain VLMs confirm our findings.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Contrastive vision-language models (VLMs) (e.g., CLIP [36] and ALIGN [24]) are a powerful class of models that jointly learn relationships between images and text. VLMs are generally pretrained on web-scale datasets with millions of image-text pairs and have been shown to exhibit impressive capabilities on a wide range of downstream tasks. In particular, VLMs have the ability to perform tasks in a zero-shot manner without utilizing explicit task-specific training data; this is accomplished by modeling downstream tasks (e.g., image classification, text-to-image retrieval) as image-text matching tasks [36]. ", "page_idx": 0}, {"type": "text", "text": "However, pretrained VLMs can exhibit poor zero-shot performance when compared to state-of-the-art task-specific models, particularly on challenging or out-of-domain downstream tasks [36, 7, 17, 19]. As a result, pretrained VLMs are often fine-tuned on domain-specific vision-language datasets in order to improve zero-shot performance on tasks of interest. For instance, recent works have fine-tuned the CLIP VLM [36] on vision-language datasets consisting of (i) chest X-rays and paired physician reports [45], (ii) pathology data and paired text [17, 19], and (iii) product images and paired captions from online fashion retailers [7]. ", "page_idx": 0}, {"type": "image", "img_path": "UFRZHFYW8e/tmp/56710670da8b0c4e3b8877863b4606679eb6a2b56a999cbd82f9fc436ca3236b.jpg", "img_caption": ["Figure 1: Region-aware Vision-Language learning (RAVL). RAVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Domain-specific vision-language datasets used to fine-tune VLMs may be small in size, preventing VLMs from gaining the robustness beneftis that come with training on diverse, web-scale data [6, 14]. As a result, fine-tuned VLMs may capture spurious correlations between image features and textual attributes [56]. For instance, consider a VLM fine-tuned on an animal image-text dataset where the presence of butterflies is closely correlated with the presence of flowers (Figure 1). Consequently, the VLM may learn to incorrectly associate the image features corresponding to flower with the textual attribute butterfly. At test time, the VLM is likely to exhibit degraded zero-shot classification performance on (i) images of butterfiles without flowers and (ii) images of other animals with flowers. ", "page_idx": 1}, {"type": "text", "text": "Improving robustness of fine-tuned VLMs to spurious correlations is challenging for the following two reasons. First, existing automated approaches primarily discover and mitigate spurious correlations at the global image level rather than intervening directly on fine-grained image features. Such approaches discover spurious correlations by identifying coherent groups of misclassified images in an automated fashion [13, 43, 22, 42]; then, the identified spurious correlation can be mitigated during training using data augmentation or robust optimization [43, 39, 22, 56]. However, recent works have suggested that such global image-level strategies (i) discover spurious correlations that align poorly with human-interpretable attributes [25] and (ii) may not effectively enable models to ignore spurious correlations during training [15, 18]. Second, existing approaches for discovering and mitigating spurious correlations are predominantly designed to improve robustness of unimodal image classification models [39, 43] or pretrained VLMs [60, 49]. These settings differ substantially from the fine-tuned VLM setting, which presents several unique challenges such as the absence of class and subgroup labels in the training set and the inclusion of free-form text. ", "page_idx": 1}, {"type": "text", "text": "In this work, we address these challenges by introducing Region-aware Vision-Language learning (RAVL), an approach for improving the robustness of fine-tuned VLMs to spurious correlations. RAVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features, rather than operating at the global image level. Our contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 First, given a fine-tuned VLM, RAVL discovers learned spurious correlations between image features and textual attributes. Using a labeled classification dataset, we decompose images into candidate regions, utilize the VLM embedding space to group visually-similar regions into feature clusters, and quantitatively evaluate the effects of each feature on zero-shot classification errors. \u2022 Second, given a ranked list of image features that the VLM has learned to spuriously correlate with one or more textual attributes, RAVL mitigates the identified spurious correlations. Our key insight is that region-level information can be leveraged during VLM fine-tuning in order to improve model robustness. To this end, we introduce a novel region-aware loss function that encourages the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In order to evaluate RAVL, we introduce a large-scale evaluation framework for controlled, finegrained evaluations of VLM robustness on synthetic and real-world data. Our framework consists of 654 fine-tuned VLMs paired with annotations for the ground-truth spurious correlations learned by each VLM. Across these evaluation settings, (i) RAVL accurately discovers spurious correlations, achieving a $191\\%$ improvement over the closest baseline, and (ii) RAVL effectively mitigates spurious correlations, achieving up to an $8.2\\%$ improvement on worst-group image classification accuracy. Qualitative evaluations on general-domain and medical-domain VLMs confirm the utility of RAVL. ", "page_idx": 2}, {"type": "text", "text": "This paper is organized as follows. In Section 2, we introduce our problem setting. Then, in Section 3, we present Stage 1 of RAVL, including our proposed methodology for discovering spurious correlations, our large-scale evaluation framework, and experimental results. In Section 4, we introduce Stage 2 of RAVL, including our proposed methodology for mitigating spurious correlations as well as experimental results. Finally, we conclude in Section 5. ", "page_idx": 2}, {"type": "text", "text": "Related Work: Our work builds on several recent research directions for discovering and mitigating spurious correlations. We provide an analysis of related works in Appendix Section A. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formally describe our problem setting. Datasets used for fine-tuning VLMs can be expressed as $\\smash{\\mathcal{D}_{F}\\,=\\,\\{(\\bar{I}_{i},T_{i})\\}_{i=1}^{m}}$ , where $I_{i}$ represents image inputs and $T_{i}$ represents paired free-form text. We do not assume access to any class or subgroup labels. ", "page_idx": 2}, {"type": "text", "text": "The performance of fine-tuned VLMs can be characterized with zero-shot classification tasks. In line with prior work [13, 22, 56], we assume that the zero-shot classification dataset includes a validation split $D_{V}\\,=\\,\\{(I_{i},y_{i})\\}_{i=1}^{n}$ with images $I_{i}$ and known ground-truth class labels $y_{i}\\in\\mathcal{V}$ , where $\\boldsymbol{\\wp}$ denotes the set of all possible class labels. At evaluation time, classification performance is computed by encoding class labels in $\\boldsymbol{\\wp}$ as text and matching images to the closest class label using embedding similarity. We do not assume access to any subgroup labels. ", "page_idx": 2}, {"type": "text", "text": "Fine-tuned VLMs may learn spurious correlations between image features and textual attributes. Let ${\\bf e}_{a}$ represent the image features corresponding to a visual concept $a$ (e.g., flowers in Figure 1) and $y\\ \\in\\ \\mathcal{V}$ represent a class label (e.g., \u201cbutterfly\" in Figure 1) such that ${\\bf e}_{a}$ and $y$ share no causal relationship. Then, a fine-tuned VLM that has learned a spurious correlation will be unable to disentangle ${\\bf e}_{a}$ and $y$ at evaluation time. This will manifest in low zero-shot classification performance on the following two subgroups of data: (i) images from class label $y$ without the feature ${\\bf e}_{a}$ and (ii) images from other class labels $y\\,\\backslash\\,\\{y\\}$ with the feature ${\\bf e}_{a}$ . ", "page_idx": 2}, {"type": "text", "text": "However, since neither the fine-tuning dataset $\\mathcal{D}_{F}$ nor the evaluation dataset $\\mathcal{D}_{V}$ include subgroup labels corresponding to visual concepts $a$ , discovering and mitigating such spurious correlations poses a challenge. For instance, in Figure 1, there are no annotations for flowers in datasets $\\mathcal{D}_{F}$ and $\\mathcal \u1e0a \\mathcal \u1e0a V \u1e0c \u1e0c$ , making it challenging to identify and address the learned spurious correlation between image features corresponding to flowers and the textual attribute corresponding to \u201cbutterfly\". ", "page_idx": 2}, {"type": "text", "text": "In the following sections, we will discuss our automated approach RAVL, which aims to address this challenge by employing fine-grained region-level information to discover (Section 3) and mitigate (Section 4) spurious correlations in fine-tuned vision-language models. ", "page_idx": 2}, {"type": "text", "text": "3 Discovering Spurious Correlations in Fine-Tuned Vision-Language Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present the first stage of RAVL, which aims to discover learned spurious correlations in VLMs. In Section 3.1, we discuss our region-aware approach for discovering fine-grained spurious correlations. Then, in order to quantitatively evaluate the efficacy of spurious feature discovery methods, we introduce a large-scale evaluation framework in Section 3.2. Finally, in Section 3.3, we use our evaluation framework to demonstrate that RAVL outperforms prior approaches in discovering fine-grained spurious correlations between image features and textual attributes. ", "page_idx": 2}, {"type": "text", "text": "3.1 Our Approach: Discovering Spurious Correlations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first stage of RAVL aims to identify spurious correlations between image features and textual attributes learned by a fine-tuned VLM $\\mathcal{M}$ . In contrast to prior works that have incorporated humans in the loop in order to identify spurious correlations [56, 30], RAVL is a fully automated approach. Additionally, whereas previous automated methods for discovering spurious correlations focus predominantly on identifying groups of images with high error rates [22, 13], our approach identifies specific image features that model $\\mathcal{M}$ has learned to spuriously correlate with a textual attribute. Our goal is to discover precise spurious correlations that can be easily interpreted by humans. ", "page_idx": 3}, {"type": "text", "text": "As discussed in Section 2, a model $\\mathcal{M}$ that has learned a spurious correlation between an image feature ${\\bf e}_{a}$ and a textual attribute $y$ will demonstrate low zero-shot performance on (i) images in $\\mathcal{D}_{V}$ with label $y$ without the feature ${\\bf e}_{a}$ and (ii) images in $\\mathcal{D}_{V}$ with other labels $y\\setminus\\{y\\}$ with the feature ${\\bf e}_{a}$ . The key challenge lies in identifying such relationships when no annotations are provided for visual concepts $a$ . RAVL addresses this challenge by (1) obtaining candidate image features in $\\mathcal{D}_{V}$ , (2) identifying the candidate image features that, when present in an image, directly contribute to classification errors, and (3) ranking the identified image features by degree of learned spurious correlations. ", "page_idx": 3}, {"type": "text", "text": "Obtaining candidate image features. RAVL first utilizes the zero-shot classification dataset $\\mathcal{D}_{V}$ to identify candidate image features. To this end, we use the fine-tuned VLM $\\mathcal{M}$ to extract an image embedding for each image $I_{i}$ in $\\mathcal{D}_{V}$ and a text embedding for each class $y\\in\\mathcal{V}$ . Zero-shot classification is performed using the computed embeddings; this results in a softmax-normalized image score distribution vector $\\mathbf{s}_{I_{i}}\\,\\in\\,\\mathbb{R}^{|\\bar{\\boldsymbol{y}}|}$ , where $|\\mathcal{V}|$ represents the number of classes. Then, we decompose each image $I_{i}$ in $\\mathcal{D}_{V}$ into a set of candidate regions $\\mathcal{R}_{i}$ . There are a variety of ways in which an image can be decomposed into regions, such as dividing images into equal-sized segments (e.g., quadrants) or using region proposal networks (RPNs) [38]. Ideally, regions should capture key features in the image; however, we emphasize that RAVL does not require groundtruth region-level annotations. We then apply RoIAlign [16, 63] to the image encoder of $\\mathcal{M}$ to extract embeddings for each region. Zero-shot classification is performed using the computed region embeddings, resulting in a softmax-normalized region score distribution matrix $\\mathbf{S}_{R_{i}}\\in\\dot{\\mathbb{R}}^{|\\mathcal{R}_{i}|\\times|\\bar{\\mathcal{V}}|}$ . ", "page_idx": 3}, {"type": "text", "text": "Given region-level embeddings for all candidate regions in $\\mathcal{D}_{V}$ , we next aim to identify coherent groups of image features that occur consistently throughout the dataset (e.g., features corresponding to \u201cflower\" or \u201cbutterfly\" in Figure 1). To this end, we cluster the computed region-level embeddings using the K-Medoids algorithm with cosine distance. The optimal number of clusters is selected in an automated fashion using Silhouette distance. The resulting clusters (denoted as $\\mathcal{C}$ ) capture key image features in $\\mathcal{D}_{V}$ . For feature cluster $c\\in{\\mathcal{C}}$ , let $\\mathbf{e}_{c}$ denotes the set of features in cluster $c$ . ", "page_idx": 3}, {"type": "text", "text": "Identifying candidate image features that directly contribute to classification errors. We now seek to identify features that, when present in an image, are directly responsible for prediction errors. ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{R}_{c}$ represent the set of regions assigned to cluster $c$ and let $\\mathcal{Z}_{c}$ represent the set of images associated with the regions in cluster $c$ . We identify labels for images in $\\mathcal{Z}_{c}$ ; we designate this label set as $\\mathcal{V}_{c}$ . For each class label $y\\in\\mathcal{V}_{c}$ , we identify all images in $\\mathcal{Z}_{c}$ with label $y$ , and we designate zero-shot classification accuracy on this subset of $n_{i n}^{y}$ images as $p_{i n}^{y}$ . Then, we identify all images in $\\mathcal{D}_{v}$ with label $y$ that do not have a region included in cluster $c$ , and we designate zero-shot classification accuracy on this subset of noyut images as poyut. ", "page_idx": 3}, {"type": "text", "text": "We now introduce the cluster influence score, which evaluates the extent to which features $\\mathbf{e}_{c}$ contribute to mispredicted image classification labels. We restrict our evaluation to only include mispredicted images in $\\mathcal{Z}_{c}$ with ground-truth labels $y$ such that $p_{i n}^{y}<p_{o u t}^{y}$ ; we will refer to this subset as $\\mathcal{T}_{c}^{e r r}\\subset\\mathcal{T}_{c}$ . For each image $I_{i}\\in\\mathcal{T}_{c}^{e r r}$ , we extract (i) the image score distribution vector $\\mathbf{s}_{I_{i}}$ and (ii) the region score distribution matrix $\\mathbf{S}_{R_{i}}$ . We use $\\mathbf{s}_{I_{i}}$ to identify the predicted image class $\\hat{y}$ , and we then identify the region $r_{i}^{m a x}$ in $\\mathcal{R}_{i}$ with the highest score for class $\\hat{y}$ . ", "page_idx": 3}, {"type": "text", "text": "Definition $^{\\,l}$ (Cluster Influence Score). For cluster $c$ and label $y$ , the cluster influence score is the proportion of images $I_{i}\\in\\mathcal{Z}_{c}^{e r r}$ with label $y$ where the identified highest-scoring region $r_{i}^{m a x}$ is part of cluster $c$ (i.e., $r_{i}^{m a x}\\in\\mathcal{R}_{c})$ ): ", "page_idx": 3}, {"type": "equation", "text": "$$\nH_{c}^{y}=\\frac{1}{|\\{I_{i}\\in\\mathcal{L}_{c}^{e r r}|y_{i}=y\\}|}\\sum_{I_{i}\\in\\mathcal{Z}_{c}^{e r r};y_{i}=y}\\mathbb{1}[r_{i}^{m a x}\\in\\mathcal{R}_{c}]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The final cluster influence score for cluster $c$ is computed as the maximum over all labels $y$ as $H_{c}=m a x_{y\\in\\mathcal{Y}_{c}}H_{c}^{y}$ . High values of $H_{c}$ show that features $\\mathbf{e}_{c}$ are similar to the incorrect label in the vision-language embedding space; this suggests that for a given image with an incorrect prediction, feature $\\mathbf{e}_{c}$ is more likely to contribute to the misprediction than other features in the image. On the other hand, low values of $H_{c}$ are likely to indicate that feature $\\mathbf{e}_{c}$ represents a core feature associated with the class label or a neutral feature that does not affect predictions. ", "page_idx": 4}, {"type": "text", "text": "Given $H_{c}$ for each feature cluster, we prune all clusters with influence scores below a threshold of $\\tau_{l}$ , which we set to 0.25 in all experiments. ", "page_idx": 4}, {"type": "text", "text": "Ranking image features by degree of learned spurious correlation. For each remaining feature cluster, we next aim to determine the extent to which the presence or absence of features $\\mathbf{e}_{c}$ affects classification performance; we introduce the cluster performance gap metric to this end. ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Cluster Performance Gap). For cluster $c$ and label $y$ , the cluster performance gap is the weighted difference between zero-shot classification accuracy on images with features $\\mathbf{e}_{c}$ and images without features $\\mathbf{e}_{c}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nG_{c}^{y}=w_{y}\\times(p_{i n}^{y}-p_{o u t}^{y}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $w_{y}$ is a simple weighting factor computed as $w_{y}=2\\times[\\mathrm{min}(n_{i n}^{y},n_{o u t}^{y})/(n_{i n}^{y}+n_{o u t}^{y})].$ . ", "page_idx": 4}, {"type": "text", "text": "Since spurious correlations result in consistent errors as opposed to isolated misclassifications, the weighting factor is designed to prioritize stronger spurious correlations that result in a larger number of errors. $G_{c}^{y}$ ranges between 0 and 1. The final performance gap metric for cluster $c$ is computed across all labels as $\\begin{array}{r}{G_{c}=\\sum_{y\\in\\mathcal{y}_{c}}|G_{c}^{y}|}\\end{array}$ . A high value of $G_{c}$ suggests that the presence or absence of features $\\mathbf{e}_{c}$ contribute to large class-level variations in image classification performance. ", "page_idx": 4}, {"type": "text", "text": "Given $G_{c}$ for each feature cluster, we rank clusters in order from highest to lowest values. The output of this stage is a ranked list of image features that model $\\mathcal{M}$ has learned to spuriously correlate with one or more class labels in $\\boldsymbol{\\wp}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Experimental Setup: Designing a Large-Scale Evaluation Framework ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now discuss our approach for evaluating RAVL. Evaluating the accuracy of predicted spurious correlations is challenging because the ground-truth spurious correlations learned by a model $\\mathcal{M}$ are typically unknown. Previous works on VLM robustness evaluate discovered spurious correlations with qualitative experiments, human-in-the-loop evaluations, or small-scale datasets [56]. Our aim in this section is to introduce a large-scale experimental setup where the ground-truth spurious correlations learned by VLMs are known and annotated in advance; this can then enable us to determine whether the features discovered by RAVL in Section 3.1 accurately align with the groundtruth. Our evaluation framework is motivated by prior work [26, 13]; however, in contrast to existing approaches, we introduce evaluation settings that are designed (i) for evaluating robustness approaches at the fine-grained region level rather than the global image-level, and (ii) for evaluating VLMs rather than unimodal models. ", "page_idx": 4}, {"type": "text", "text": "Designing Controlled Evaluations: Our evaluation framework artificially induces spurious correlations in the VLM fine-tuning data; then, given the known pre-defined spurious correlation and a VLM that learned the desired spurious correlation, we can quantitatively evaluate the extent to which RaVL discovers the correlation. ", "page_idx": 4}, {"type": "text", "text": "We create a set of evaluation settings using data from two domains: (1) synthetic data (MNIST [11] and FashionMNIST [51]) and (2) real-world data (COCO [27]). Each evaluation setting consists of the following components: ", "page_idx": 4}, {"type": "text", "text": "1. Predefined spurious correlation: We define a spurious image feature and textual attribute pair $({\\bf e}^{e v a l},a^{e v a l})$ . For MNIST and FashionMNIST, ${\\dot{\\mathbf{e}}}^{e v a l}$ represents a red rectangle; $a^{e v a l}$ is generated from the set of class labels $\\{{\\bf z e r o}$ , one, two, three, four five, six, seven, eight, nine $\\}$ for MNIST and {t-shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boot} for FashionMNIST. For COCO, we sample ${\\bf e}^{e v a l}$ and $\\boldsymbol{a}^{e v a l}$ from the list of annotated attributes.   \n2. Fine-tuning dataset: We construct a vision-language fine-tuning dataset $\\mathcal{D}_{F}^{e v a l}=\\{(I_{i},T_{i})\\}_{i=1}^{m}$ with images $I_{i}$ and text $T_{i}$ . Dataset $\\mathcal{D}_{F}^{e v a l}$ is sampled from the training sets of MNIST, FashionMNIST, or COCO such that the presence of image feature ${\\bf e}^{e v a l}$ is closely correlated with the presence of text attribute $\\boldsymbol{a}^{e v a l}$ as measured by Cramer\u2019s $\\mathrm{v}$ [57]. ", "page_idx": 4}, {"type": "image", "img_path": "UFRZHFYW8e/tmp/008726d2402fda839acc09e42da63a3a91089108cbedc829b9cc303b86d9d0d0.jpg", "img_caption": ["Figure 2: RAVL accurately identifies spurious correlations. Using our evaluation settings, we show that RAVL consistently outperforms prior methods in discovering learned spurious correlations between image features and textual attributes. Here, we provide Precision $@10$ metrics for a CLIPRN50 model fine-tuned on synthetic data (129 settings) and real-world data (171 settings). "], "img_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "UFRZHFYW8e/tmp/d24766d2b13d49cc1360e40e689ebb752a53c155733bb008a538dbb1cf731f4c.jpg", "table_caption": ["Table 1: Mean Precision@10 metrics demonstrate the efficacy of RAVL in discovering spurious correlations. On average across 654 evaluation settings, RAVL consistently outperforms baselines. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "UFRZHFYW8e/tmp/456a791f100bbff7c4606236a0b56a639325149f321c7e1327d4c2e14e28b63c.jpg", "table_caption": ["Table 2: Ablations show the utility of the cluster performance gap and influence metrics. We report Precision $@10$ metrics for a CLIP-RN50 model fine-tuned on real-world data (171 settings). "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "3. Fine-tuned VLM: A VLM $\\mathcal{M}$ is fine-tuned on $\\mathcal{D}_{F}^{e v a l}$ . ", "page_idx": 5}, {"type": "text", "text": "4. Evaluation dataset: Model $\\mathcal{M}$ is evaluated using a zero-shot classification dataset $\\mathcal{D}_{V}^{e v a l}\\;=\\;$ $\\{(I_{i},y_{i},\\mathcal{R}_{i},\\mathcal{L}_{i})\\}_{i=1}^{n}$ with images $I_{i}$ , class labels $y_{i}$ , region bounding boxes $\\mathcal{R}_{i}$ , and region-level labels ${\\mathcal{L}}_{i}$ . In particular, $a^{e v a l}$ must be included in the class label set, and ${\\bf e}^{e v a l}$ must be annotated in the region-level label set. Since $\\mathcal{D}_{V}^{e v a l}$ is designed to reflect a real-world setting, we assume that a correlation between $a^{e v a l}$ and ${\\bf e}^{e v a l}$ does not exist. Dataset $\\mathcal{D}_{V}^{e v a l}$ is constructed from the test sets of MNIST, FashionMNIST, or COCO. ", "page_idx": 5}, {"type": "text", "text": "Given the four components listed above, we classify an evaluation setting as valid if model $\\mathcal{M}$ learned the intended spurious correlation. In order to measure this, we first identify images with label $\\boldsymbol{a}^{e v a l}$ in $\\mathcal{D}_{V}^{e v a l}$ and compute the performance difference between images with feature ${\\bf e}^{e v a l}$ and images without feature ${\\bf e}^{e v a l}$ ; we designate this value as $\\epsilon_{1}$ . Then, for labels $y\\ne a^{e v a l}$ , we compute the maximum performance difference between images without feature ${\\bf e}^{e v a l}$ and images with feature ${\\bf e}^{e v a l}$ ; we designate this value as $\\epsilon_{2}$ . Large values of $\\epsilon_{1}$ and $\\epsilon_{2}$ suggest that model $\\mathcal{M}$ has learned the desired spurious correlation between image feature ${\\bf e}^{e v a l}$ and textual attribute $a^{e v a l}$ , as defined in Section 2. We remove settings where $\\epsilon_{1}$ or $\\epsilon_{2}$ are below some predefined performance threshold $\\tau_{e v a l}$ The performance threshold $\\tau_{e v a l}$ serves as a quantitative indicator of learned correlation strength. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details: In total, we generate 620 fine-tuning datasets $D_{F}^{e v a l}$ (100 synthetic; 520 real-world). We then fine-tune model $\\mathcal{M}$ on each dataset with three random seeds, resulting in 1860 candidate evaluation settings. Finally, we filter out settings where model $\\mathcal{M}$ does not consistently learn the spurious correlation; to this end, we only retain settings where both $\\epsilon_{1}$ and $\\epsilon_{2}$ exceed $\\tau_{e v a l}=10$ across all three random seeds. We repeat this procedure across various pretrained VLMs $\\mathcal{M}$ , resulting in 654 valid experimental settings. Additional implementation details are provided in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "3.3 Results: RaVL Effectively Discovers Spurious Correlations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Comparisons to Prior Approaches: Given an evaluation setting with a predefined spurious correlation $\\left(\\mathbf{e}^{e v a l},a^{e v a l}\\right)$ , a fine-tuned VLM $\\mathcal{M}$ , and an evaluation dataset $\\mathcal{D}_{V}^{e v a l}$ , our goal is to determine the extent to which RAVL can discover the correlation between ${\\bf e}^{e v a l}$ and $a^{e v a l}$ . ", "page_idx": 6}, {"type": "text", "text": "To this end, we use the labeled zero-shot classification dataset $\\mathcal{D}_{V}^{e v a l}$ , which includes ground-truth region bounding boxes and associated region labels. We provide the ground-truth bounding boxes as input to RAVL, which returns a single top-ranked cluster of regions likely to include spurious features. We rank regions within the cluster based on similarity to the cluster medoid, and we utilize the provided region-level labels in $\\mathcal{D}_{V}^{e v a l}$ to evaluate the proportion of top- $K$ regions that contain the desired spurious feature ${\\bf e}^{e v a l}$ . In line with prior work [13], we report performance with Precision $@\\mathrm{K}$ metrics. We note that given an identified spurious feature ${\\bf e}^{e v a l}$ , the correlated textual attribute $\\boldsymbol{a}^{e v a l}$ can be detected by identifying the class label in $\\mathcal{D}_{V}^{e v a l}$ where the absence of feature ${\\bf e}^{e v a l}$ leads to degraded performance. ", "page_idx": 6}, {"type": "text", "text": "There are few existing approaches for performing automated detection of fine-grained spurious features learned by VLMs. Here, we compare RAVL with four previously-developed methods: Distilling Failures [22], George [43], Domino [13], and Spurious-Aware Detection [56]. Distilling Failures, George, and Domino are state-of-the-art approaches for automatic identification of model failures resulting from spurious correlations; although these methods operate at the global image level and are designed for unimodal settings, we adapt these approaches for our setting by utilizing regions and zero-shot classification scores as input. Spurious-Aware Detection operates at the fine-grained region level by computing class-based performance gaps resulting from the presence or absence of particular features. To enable a fair comparison with RAVL, we provide the same set of regions and associated embeddings as input to all baselines. We also compare RAVL with a random baseline, where the ranked list of regions is shuffled randomly. ", "page_idx": 6}, {"type": "text", "text": "Table 1 summarizes mean Precision $@10$ metrics across all 654 evaluation settings. Results demonstrate that RAVL consistently outperforms prior approaches in discovering spurious correlations between image features and textual attributes, contributing to a $191\\%$ improvement over the closest baseline. In Table 1, we evaluate the effects of learned spurious correlation strength by varying the error threshold $\\tau_{e v a l}$ from 10 to 40 and reporting performance for the subset of valid evaluation settings. Results show that RAVL is particularly effective when VLM $\\mathcal{M}$ learns a strong spurious correlation; as learned correlation strength increases, performance of RAVL increases by $47\\%$ whereas most baselines degrade in performance. We also observe that Domino, George, and Distilling Failures often achieve performance near or below the random baseline across our evaluation settings; this suggests that methods designed for detecting errors resulting from spurious correlations at the global image-level cannot be easily adapted for fine-grained region-level discovery. Figure 2 demonstrates that our findings hold for both synthetic and real-world data. ", "page_idx": 6}, {"type": "text", "text": "Ablations: Our ablation study evaluates the role of the cluster influence score $H_{c}$ and the cluster performance gap metric $G_{c}$ (Section 3.1) in enabling accurate discovery of spurious correlations between image features and textual attributes. We compare the following three metrics for ranking clusters: (1) an unweighted cluster performance gap metric where $w_{y}$ is set to 1, (2) the cluster performance gap with $w_{y}$ computed as in Section 3.1, and (3) a combination of the cluster performance gap and cluster influence metric as used in RAVL. As shown in Table 2, the metrics utilized by RAVL consistently demonstrate the best performance across various learned correlation strengths $(\\tau_{e v a l})$ . Our results suggest the utility of both the performance gap metric and the influence score in identifying fine-grained spurious correlations. ", "page_idx": 6}, {"type": "text", "text": "Evaluations in the Wild: In addition to our controlled evaluations, we evaluate the ability of RAVL to surface spurious correlations learned by 12 off-the-shelf VLMs [12, 36, 20]; this presents a realistic and uncontrolled evaluation setting. We consider two zero-shot classification tasks $\\mathcal{D}_{V}$ : (1) a 397- class scene classification task on SUN397 [52] and (2) binary classification of cardiomegaly in chest X-rays from ObjectCXR [23]. We use the cluster performance gap metric $G_{c}$ , introduced in Section 3.1, to quantify the degree of the learned spurious correlation. ", "page_idx": 6}, {"type": "text", "text": "Our results demonstrate that all evaluated models, which span a range of architecture, training data, and parameter counts, show evidence of having learned spurious correlations; this is demonstrated by nonzero values of the cluster performance gap metric $G_{c}$ . On average across the evaluated models, the top-ranked spurious feature cluster discovered by RAVL on SUN397 achieves a cluster performance gaps $(G_{c})$ of $9.9{\\scriptstyle\\pm3.2}$ $(\\mathrm{minimum}=5.1\\$ , maximum $=14.0)$ . On ObjectCXR, the mean value of $G_{c}$ is $0.08_{\\pm0.04}$ $\\mathrm{\\minimum=0.04}$ , maximum $=0.12\\$ ).2 Our results support findings from previous work suggesting that all models may learn spurious correlations [30]. ", "page_idx": 6}, {"type": "image", "img_path": "UFRZHFYW8e/tmp/a55d02140dd016772df1692a06f4492c786fd317808f0a582ffa84121ad4f31b.jpg", "img_caption": ["Figure 3: RAVL surfaces spurious correlations in off-the-shelf VLMs. RAVL identifies a spurious correlation learned by CLIP ViT-B/16 between the presence of text-based retail signage and the class label fast food restaurant in a scene classification task. RAVL also surfaces a spurious correlation learned by PubMedCLIP ResNet-50 between metal clips (found in clothing) and the class label cardiomegaly (a heart condition) on a chest X-ray classification task. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "UFRZHFYW8e/tmp/b30897c8d474a93bec7fae7c81abee64673d9436ef1a3b8b1ff0ef5726b95c5f.jpg", "table_caption": ["Table 3: RAVL effectively mitigates spurious correlations. Here, we report mean Image Overall, Image Worst-Group (Img. WG), Region Overall, and Region Worst-Group (Reg. WG) metrics across our real-world evaluation settings. Since performance of mitigation methods is dependent on the results of Stage 1, we report metrics across settings where Stage 1 Precision $\\left(\\omega\\,10\\right)>0.6$ and Stage 1 Precision $\\left(\\!\\frac{\\omega}{{}}10\\!>0.8\\right)$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In Figure 3, we provide qualitative examples of discovered spurious features for the CLIP ViT-B/16 model evaluated on SUN397 and the PubmedCLIP ResNet-50 model evaluated on ObjectCXR. For the CLIP ViT-B/16 model, RAVL surfaces a feature cluster consisting of text-based retail signage. We observe significant performance gaps between images containing the RAVL-identified feature and images that do not contain the feature. For instance, we note a 48.2 point difference in zero-shot classification accuracy for the class label fast food restaurant, suggesting that a CLIP ViT-B/16 model can better classify a scene of a fast food restaurant when a text-based retail sign is present. For the PubmedCLIP ResNet-50 model, RAVL discovers that the presence of metal clips (found in the patient\u2019s clothing) is spuriously correlated with cardiomegaly. We observe that the presence of clips improves zero-shot classification accuracy for the class label cardiomegaly by 15.3 points. ", "page_idx": 7}, {"type": "text", "text": "Our evaluations show that RAVL can surface fine-grained spurious correlations in realistic settings.   \nAdditional implementation details and qualitative examples are provided in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "4 Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we present the second stage of RAVL, which aims to mitigate learned spurious correlations in VLMs. In Section 4.1, we discuss our methodology for mitigating fine-grained spurious correlations with a novel region-aware loss function. In Section 4.2, we use the evaluation framework previously introduced in Section 3.2 to demonstrate that RAVL substantially outperforms prior approaches in mitigating spurious correlations between image features and textual attributes. ", "page_idx": 7}, {"type": "text", "text": "4.1 Our Approach: Mitigating Spurious Correlations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As described in Section 3, Stage 1 of RAVL discovers image features that VLM $\\mathcal{M}$ has learned to spuriously correlate with textual attributes. We next aim to mitigate the spurious correlation. Motivated by prior work on fine-grained VLMs [58, 46], our key insight is that utilizing region-level information during VLM training can enable models to focus on relevant image-text relationships and ignore spurious correlations. ", "page_idx": 8}, {"type": "text", "text": "Since dataset $\\mathcal{D}_{F}$ exclusively consists of images and text, ground-truth subgroup and class labels are not available. As a result, we first assign plausible (i) region-level subgroup labels and (ii) image-level class labels to the vision-language fine-tuning dataset $\\mathcal{D}_{F}$ . To assign subgroup labels, we decompose each image $I_{i}$ in dataset $\\mathcal{D}_{F}$ into a set of candidate regions $\\mathcal{R}_{i}$ . We then fit the trained K-Medoids clustering model from Section 3.1 on $\\mathcal{R}_{i}$ and identify all spurious regions associated with the top ranked cluster. We represent the identified spurious regions as $\\mathcal{R}_{i}^{s}$ and remaining non-spurious regions as $\\mathcal{R}_{i}^{r}$ such that $\\mathcal{R}_{i}^{s}\\cup\\mathcal{R}_{i}^{r}=\\mathcal{R}_{i}$ . In order to assign plausible class labels, we parse the paired text $T_{i}$ associated with each image to identify samples that reference the class labels included in the zero-shot classification label set $\\boldsymbol{\\wp}$ ; we refer to the assigned class label for image $I_{i}$ as $\\hat{y_{i}}$ . ", "page_idx": 8}, {"type": "text", "text": "We now introduce a novel region-aware contrastive loss function for training VLM $\\mathcal{M}_{n e w}$ . For batch $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ , we define $\\mathcal{R}_{B}^{s}$ as the set of all spurious regions in the batch: $\\begin{array}{r}{\\mathcal{R}_{B}^{s}=\\bigcup_{I_{i}\\in B}\\mathcal{R}_{i}^{s}}\\end{array}$ . For image $I_{i}\\in B$ the first loss component $L_{R}^{i}$ encourages high embedding similarity between non-spurious regions $\\mathcal{R}_{i}^{r}$ and assigned class label $\\hat{y_{i}}$ when compared to other class labels. ", "page_idx": 8}, {"type": "equation", "text": "$$\nL_{R}^{i}=-\\log\\frac{\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y}_{i})}{\\sum_{\\hat{y_{j}}\\in\\mathcal{B}}\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y_{j}})+P(\\mathcal{R}_{B}^{s})}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here, for region embedding function $f$ and text embedding function $g$ , $\\begin{array}{r l}{\\sigma_{m}(A,b)}&{{}=}\\end{array}$ $\\exp(\\operatorname*{max}_{a\\in A}(\\left\\langle f(a),g(b)\\right\\rangle/\\tau))$ with temperature $\\tau$ . The term $P(\\mathcal{R}_{B}^{s})$ is a penalty that enforces embedding-level dissimilarity between spurious regions and correlated class labels. ", "page_idx": 8}, {"type": "text", "text": "The second loss component $L_{A}^{i}$ encourages high embedding similarity between non-spurious regions $\\mathcal{R}_{i}^{r}$ and assigned class label $\\hat{y_{i}}$ when compared to other regions. We define $\\sigma(a,b)\\;=\\;$ $\\bar{\\exp}(\\left<f(\\bar{a}),g(b)\\right>/\\bar{\\tau})$ with temperature $\\tau$ . ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{A}^{i}=-\\log\\frac{\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y}_{i})}{\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y}_{i})+\\sum_{j=1,\\hat{y}_{j}\\ne\\hat{y}_{i}}^{|B|}\\sigma_{m}(\\mathcal{R}_{j}^{r},\\hat{y}_{i})+\\sum_{r_{j}\\in\\mathcal{R}_{B}^{s}}\\sigma(r_{j},\\hat{y}_{i})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The final loss is expressed as $\\begin{array}{r}{L=\\lambda L_{C L}+(1-\\lambda)\\sum_{i=1}^{|\\mathcal{B}|}(L_{R}^{i}+L_{A}^{i})}\\end{array}$ . Here, $\\lambda$ is a hyperparameter and $L_{C L}$ takes the form of the original loss function  used for training $\\mathcal{M}$ ; in our experiments, $L_{C L}$ is the CLIP objective [36]. Extended formulations of our loss function are provided in Appendix C. ", "page_idx": 8}, {"type": "text", "text": "4.2 Results: RaVL Effectively Mitigates Spurious Correlations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Comparisons to Prior Approaches: We use the evaluation framework previously introduced in Section 3.2 to compare RAVL with prior approaches. There are few existing approaches for mitigating spurious correlations in the setting of fine-tuned VLMs. Here, we compare RAVL with standard VLM fine-tuning, upsampled VLM fine-tuning, ERM, GDRO [39], and Spurious-Aware Mitigation [56]. Since ERM and GDRO are traditionally used in unimodal classification settings, we adapt these approaches for our setting by adding a contrastive vision-language objective and using zero-shot classification scores during fine-tuning; we refer to these approaches as VL-ERM and VL-GDRO respectively. ", "page_idx": 8}, {"type": "text", "text": "Table 3 summarizes mean zero-shot classification results across our real-world evaluation settings. Since performance of mitigation methods is dependent on the accuracy of the discovered spurious correlations in Stage 1, Table 3 displays results for two evaluation categories: (i) the 192 settings where RAVL Stage 1 Precision $@10$ is greater than 0.6, and (ii) the 106 settings where RAVL Stage 1 Precision $@10$ is greater than 0.8. In line with prior works on robustness [39, 56], we report image overall performance and image worst-group performance. Additionally, in order to evaluate the extent to which the VLM understands fine-grained features, we introduce two new metrics: region overall performance and region worst-group performance. Region-level accuracies are computed by performing zero-shot classification with region embeddings and comparing predicted labels to the ground-truth region-level labels provided in the zero-shot classification dataset. ", "page_idx": 8}, {"type": "text", "text": "Results show that RAVL consistently outperforms prior approaches in mitigating spurious correlations. Across the two evaluation categories in Table 3, RAVL contributes to an improvement of up to $8.2\\%$ on image worst-group performance and $10.8\\%$ on region worst-group performance over the nearest baseline. Improvements in region worst-group performance are particularly notable, suggesting that RAVL can better interpret fine-grained features when compared to prior approaches. Additionally, as the accuracy of the discovered spurious correlations in Stage 1 increases, the performance of the RAVL mitigation approach increases proportionally. Our results demonstrate the efficacy of our fine-tuning procedure in mitigating spurious correlations when compared to prior approaches. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced RAVL, a fine-grained region-aware approach for addressing spurious correlations in VLMs. We demonstrate through large-scale, controlled experiments as well as in-thewild evaluations that RAVL can discover ( $191\\%$ improvement in identified correlations) and mitigate $8.2\\%$ improvement on worst-group performance) spurious correlations in VLMs. We hope that our work can help (i) diagnose and correct critical failure modes in VLMs prior to deployment and (ii) drive progress towards the development of fine-grained approaches for model robustness. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We are thankful to Sophie Ostmeier, Eduardo Reis, and Ashwin Kumar for helpful discussions and feedback. MV is supported by graduate fellowship awards from the Department of Defense (NDSEG), the Knight-Hennessy Scholars program at Stanford University, and the Quad program. AC is supported by NIH grants R01 HL167974, R01HL169345, R01 AR077604, R01 EB002524, R01 AR079431, P41 EB027060, AY2AX000045, and 1AYSAX0000024-01; and NIH contracts 75N92020C00008 and 75N92020C00021. CL is supported by NIH grants R01 HL155410, R01 HL157235, by AHRQ grant R18HS026886, and by the Gordon and Betty Moore Foundation. JBD and CL are supported by the Medical Imaging and Data Resource Center (MIDRC), which is funded by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) under contract 75N92020C00021 and through The Advanced Research Projects Agency for Health (ARPA-H). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Dyah Adila, Changho Shin, Linrong Cai, and Frederic Sala. Zero-shot robustification of zero-shot models with foundation models, 2023.   \n[2] Amit Alfassy, Assaf Arbelle, Oshri Halimi, Sivan Harary, Roei Herzig, Eli Schwartz, Rameswar Panda, Michele Dolf,i Christoph Auer, Kate Saenko, PeterW. J. Staar, Rogerio Feris, and Leonid Karlinsky. Feta: Towards specializing foundation models for expert task applications, 2022.   \n[3] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.   \n[4] Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Magdalini Paschali, Zhihong Chen, Jean-Benoit Delbrouck, Eduardo Reis, Cesar Truyts, Christian Bluethgen, Malte Engmann Kjeldskov Jensen, Sophie Ostmeier, Maya Varma, Jeya Maria Jose Valanarasu, Zhongnan Fang, Zepeng Huo, Zaid Nabulsi, Diego Ardila, Wei-Hung Weng, Edson Amaro Junior, Neera Ahuja, Jason Fries, Nigam H. Shah, Andrew Johnston, Robert D. Boutin, Andrew Wentland, Curtis P. Langlotz, Jason Hom, Sergios Gatidis, and Akshay S. Chaudhari. Merlin: A vision language foundation model for 3d computed tomography, 2024.   \n[5] Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston, Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari, and Curtis Langlotz. Chexagent: Towards a foundation model for chest x-ray interpretation, 2024.   \n[6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022.   \n[7] Patrick John Chia, Giuseppe Attanasio, Federico Bianchi, Silvia Terragni, Ana Rita Magalhaes, Diogo Goncalves, Ciro Greco, and Jacopo Tagliabue. Contrastive language and vision learning of general fashion concepts. Scientific Reports, 12(1), November 2022.   \n[8] Joseph Paul Cohen, Joseph D Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera, Matthew P Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, et al. Torchxrayvision: A library of chest $\\mathbf{X}$ -ray datasets and models. In International Conference on Medical Imaging with Deep Learning, pages 231\u2013249. PMLR, 2022.   \n[9] Elliot Creager, J\u00f6rn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In International Conference on Machine Learning, pages 2189\u20132200. PMLR, 2021.   \n[10] Alex J DeGrave, Joseph D Janizek, and Su-In Lee. AI for radiographic COVID-19 detection selects shortcuts over signal. Nat. Mach. Intell., 3(7):610\u2013619, May 2021.   \n[11] Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.   \n[12] Sedigheh Eslami, Christoph Meinel, and Gerard De Melo. Pubmedclip: How much does clip benefti visual question answering in the medical domain? In Findings of the Association for Computational Linguistics: EACL 2023, pages 1151\u20131163, 2023.   \n[13] Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Re. Domino: Discovering systematic errors with cross-modal embeddings. In International Conference on Learning Representations, 2022.   \n[14] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip), 2022.   \n[15] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021.   \n[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn, 2017.   \n[17] Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas J Montine, and James Zou. A visual-language foundation model for pathology image analysis using medical twitter. Nat. Med., 29(9):2307\u20132316, September 2023.   \n[18] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In Bernhard Sch\u00f6lkopf, Caroline Uhler, and Kun Zhang, editors, Proceedings of the First Conference on Causal Learning and Reasoning, volume 177 of Proceedings of Machine Learning Research, pages 336\u2013351. PMLR, 11\u201313 Apr 2022.   \n[19] Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt-1m: One million image-text pairs for histopathology. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.   \n[20] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. If you use this software, please cite it as below.   \n[21] Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew G Wilson. On feature learning in the presence of spurious correlations. Advances in Neural Information Processing Systems, 35:38516\u201338532, 2022.   \n[22] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Aleksander Madry. Distilling model failures as directions in latent space. In The Eleventh International Conference on Learning Representations, 2023.   \n[23] JFHealthcare. Object-cxr - automatic detection of foreign objects on chest x-rays.   \n[24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 4904\u20134916. PMLR, 18\u201324 Jul 2021.   \n[25] Nari Johnson, \u00c1ngel Alexander Cabrera, Gregory Plumb, and Ameet Talwalkar. Where does my model underperform? a human evaluation of slice discovery algorithms, 2023.   \n[26] Weixin Liang and James Zou. Metashift: A dataset of datasets for evaluating contextual distribution shifts and training conflicts. In International Conference on Learning Representations, 2021.   \n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision \u2013 ECCV 2014, pages 740\u2013755, Cham, 2014. Springer International Publishing.   \n[28] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In International Conference on Machine Learning, pages 6781\u20136792. PMLR, 2021.   \n[29] Mazda Moayeri, Phillip Pope, Yogesh Balaji, and Soheil Feizi. A comprehensive study of image classification model sensitivity to foregrounds, backgrounds, and visual attributes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19087\u201319097, 2022.   \n[30] Mazda Moayeri, Wenxiao Wang, Sahil Singla, and Soheil Feizi. Spuriosity rankings: Sorting data to measure and mitigate biases, 2023.   \n[31] Nicolas M. M\u00fcller, Simon Roschmann, Shahbaz Khan, Philip Sperl, and Konstantin B\u00f6ttinger. Shortcut detection with variational autoencoders, 2023.   \n[32] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: De-biasing classifier from biased classifier. Advances in Neural Information Processing Systems, 33:20673\u201320684, 2020.   \n[33] Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin. Spread spurious attribute: Improving worst-group accuracy with spurious attribute estimation. In International Conference on Learning Representations, 2021.   \n[34] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher R\u00e9. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. September 2019.   \n[35] Obioma Pelka, Sven Koitka, Johannes R\u00fcckert, Felix Nensa, and Christoph M. Friedrich. Radiology objects in context (roco): A multimodal image dataset. In Danail Stoyanov, Zeike Taylor, Simone Balocco, Raphael Sznitman, Anne Martel, Lena Maier-Hein, Luc Duong, Guillaume Zahnd, Stefanie Demirci, Shadi Albarqouni, Su-Lin Lee, Stefano Moriconi, Veronika Cheplygina, Diana Mateus, Emanuele Trucco, Eric Granger, and Pierre Jannin, editors, Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis, pages 180\u2013189, Cham, 2018. Springer International Publishing.   \n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 18\u201324 Jul 2021.   \n[37] Pranav Rajpurkar, Jeremy Irvin, Aarti Bagul, Daisy Ding, Tony Duan, Hershel Mehta, Brandon Yang, Kaylie Zhu, Dillon Laird, Robyn L. Ball, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, and Andrew Y. Ng. Mura: Large dataset for abnormality detection in musculoskeletal radiographs, 2018.   \n[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28, pages 91\u201399. Curran Associates, Inc., 2015.   \n[39] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2020.   \n[40] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022.   \n[41] Sahil Singla and Soheil Feizi. Salient imagenet: How to discover spurious features in deep learning? In International Conference on Learning Representations, 2022.   \n[42] Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, and Eric Horvitz. Understanding failures of deep networks via robust feature extraction. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021. Computer Vision Foundation / IEEE, 2021.   \n[43] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R\u00e9. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19339\u201319352. Curran Associates, Inc., 2020.   \n[44] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: the new data in multimedia research. Communications of the ACM, 59(2):64\u201373, January 2016.   \n[45] Ekin Tiu, Ellie Talius, Pujan Patel, Curtis P Langlotz, Andrew $\\mathrm{~Y~Ng~}$ , and Pranav Rajpurkar. Expert-level detection of pathologies from unannotated chest $\\mathbf{X}$ -ray images via self-supervised learning. Nat. Biomed. Eng., 6(12):1399\u20131406, December 2022.   \n[46] Maya Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, and Curtis Langlotz. Villa: Finegrained vision-language representation learning from real-world data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.   \n[47] Maya Varma, Mandy Lu, Rachel Gardner, Jared Dunnmon, Nishith Khandwala, Pranav Rajpurkar, Jin Long, Christopher Beaulieu, Katie Shpanskaya, Li Fei-Fei, Matthew P. Lungren, and Bhavik N. Patel. Automated abnormality detection in lower extremity radiographs using deep learning. Nature Machine Intelligence, 1(12):578\u2013583, December 2019.   \n[48] Julia K Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein, Rainer HofmannWellenhof, Luc Thomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz, et al. Association between surgical skin markings in dermoscopic images and diagnostic performance of a deep learning convolutional neural network for melanoma recognition. JAMA dermatology, 155(10):1135\u20131141, 2019.   \n[49] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021. https://arxiv.org/ abs/2109.01903.   \n[50] Shirley Wu, Mert Yuksekgonul, Linjun Zhang, and James Zou. Discover and cure: Concept-aware mitigation of spurious correlation. In ICML, 2023.   \n[51] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.   \n[52] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Largescale scene recognition from abbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 3485\u20133492, 2010.   \n[53] Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. In International Conference on Learning Representations, 2020.   \n[54] Hanwen Xu, Naoto Usuyama, Jaspreet Bagga, Sheng Zhang, Rajesh Rao, Tristan Naumann, Cliff Wong, Zelalem Gero, Javier Gonz\u00e1lez, Yu Gu, Yanbo Xu, Mu Wei, Wenhui Wang, Shuming Ma, Furu Wei, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Jaylen Rosemon, Tucker Bower, Soohee Lee, Roshanthi Weerasinghe, Bill J Wright, Ari Robicsek, Brian Piening, Carlo Bifulco, Sheng Wang, and Hoifung Poon. A whole-slide foundation model for digital pathology from real-world data. Nature, 630(8015):181\u2013188, June 2024.   \n[55] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with domain mixup. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 6502\u20136509, 2020.   \n[56] Yu Yang, Besmira Nushi, Hamid Palangi, and Baharan Mirzasoleiman. Mitigating spurious correlations in multi-modal models during fine-tuning. In International Conference on Machine Learning, 2023.   \n[57] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In Proceeding of the Thirty-ninth International Conference on Machine Learning, 2022.   \n[58] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training, 2022.   \n[59] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? In International Conference on Learning Representations, 2020.   \n[60] Michael Zhang and Christopher Re. Contrastive adapters for foundation model group robustness. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[61] Michael Zhang, Nimit S. Sohoni, Hongyang R. Zhang, Chelsea Finn, and Christopher R\u00e9. Correct-ncontrast: A contrastive approach for improving robustness to spurious correlations, 2022.   \n[62] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, Andrea Tupini, Yu Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng Gao, Matthew P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon. Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs, 2024.   \n[63] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16793\u201316803, June 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B Extended Details on Evaluation Settings ", "page_idx": 14}, {"type": "text", "text": "C Extended Details on RAVL Mitigation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Extended Results for RAVL Stage 1 (Discovery) 18   \nD.1.1 Extended Comparisons to Prior Approaches . . . 18   \nD.1.2 Additional details for in-the-wild evaluations . . 18   \nD.2 Extended Results for RAVL Stage 2 (Mitigation) . \u00b7\u00b7\u00b7 21   \nD.3 Computational Complexity Analysis . . . . . . . 22 ", "page_idx": 14}, {"type": "text", "text": "E Extended Discussion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Machine learning models often learn spurious correlations (also known as shortcuts) between image features and class labels. For instance, models have been shown to rely on the presence of chest tubes rather than disease features when identifying collapsed lungs in chest $\\boldsymbol{\\mathrm{X}}$ -rays [34]; surgical skin markings when detecting melanoma from skin lesions [48]; and environmental features when performing object recognition tasks [3]. Models that learn spurious correlations will generalize poorly to real-world settings. Our work builds on several recent research directions for (i) discovering and (ii) mitigating spurious correlations. ", "page_idx": 14}, {"type": "text", "text": "Discovering Spurious Correlations. In the unimodal setting, prior works have developed automated methods for identifying systematic errors resulting from learned spurious correlations in vision models. Using a labeled validation set, these approaches utilize clustering algorithms [13, 43] or lightweight models [42, 22, 31] to identify subgroups of images with high error rates; for instance, in the example in Figure 1, images containing butterfiles without flowers may be identified as one such subgroup. Given a set of images in the identified subgroups, a user can then identify the common features and rectify the data or model. However, recent work has suggested that it is often challenging for humans to interpret identified subgroups and accurately determine the shared features resulting in model failure [25]. Additionally, such methods often focus solely on identifying images with high error rates (e.g. butterfiles without flowers) rather than identifying the specific class of features contributing to the error (e.g. flowers). A related line of work has aimed to identify spurious features using human supervision [41] or external concept banks [50]. ", "page_idx": 14}, {"type": "text", "text": "In the vision-language setting, Yang et al. use an external off-the-shelf object detector to annotate features [56]. Then, for each feature, the difference in zero-shot classification accuracy between images containing the feature and those without the feature is measured; high performance gaps are used to signal spurious features. However, the efficacy of this approach is reliant on the quality of the object detector and a human-in-the-loop is used to verify results; also, as we show in this work, performance gaps alone are not always sufficient for discovering spurious features. ", "page_idx": 14}, {"type": "text", "text": "Mitigating Spurious Correlations. There is a line of work aiming to mitigate spurious correlations in the context of deep learning [61, 39, 32, 9, 28, 33, 21]. These works explore strategies like data augmentation [55, 59, 57, 22, 50] and instance upsampling [39, 43]. While these approaches have been explored widely in unimodal tasks [29, 53], mitigating spurious correlations in vision-language settings has not been extensively studied. Some previous works have studied this problem within the context of pretrained VLMs [60, 49, 1]; however, their setting differs markedly from the fine-tuned ", "page_idx": 14}, {"type": "text", "text": "VLM setting, where datasets are composed of image-text pairs with no class or subgroup labels. In the fine-tuned VLM setting, existing works predominantly operate at the global image-level [56], which is unlikely to be sufficient for mitigating fine-grained spurious correlations. ", "page_idx": 15}, {"type": "text", "text": "B Extended Details on Evaluation Settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We create 654 evaluation settings using data from two domains: (1) synthetic data (MNIST [11] and FashionMNIST [51]) and (2) real-world data (COCO [27]). Below, we provide implementation details for the four components included in each evaluation setting: ", "page_idx": 15}, {"type": "text", "text": "1. Predefined spurious correlation: We define a spurious image feature and textual attribute pair $(\\mathbf{\\bar{e}}^{e v a l},\\bar{a}^{e v a l})$ . For MNIST and FashionMNIST, ${\\bf e}^{e v a l}$ represents a red rectangle; $\\boldsymbol{a}^{e v a l}$ is generated from the set $\\{\\mathrm{zero}$ , one, two, three, four five, six, seven, eight, nine $\\}$ for MNIST and $\\{{\\mathfrak{t}}$ -shirt, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag, ankle boo $\\}$ for FashionMNIST. For COCO, we sample ${\\bf e}^{e v a l}$ and $\\boldsymbol{a}^{e v a l}$ from the list of annotated attributes. ", "page_idx": 15}, {"type": "text", "text": "2. Fine-tuning dataset: Vision-language fine-tuning datasets $\\mathcal{D}_{F}^{e v a l}$ are sampled from the training sets of MNIST, FashionMNIST, and COCO such that the presence of feature ${\\bf e}^{e v a l}$ is correlated with the presence of text attribute $\\boldsymbol{a}^{e v a l}$ as measured by Cramer\u2019s V. For MNIST and FashionMNIST, we synthetically generate text captions by randomly sampling from the following pre-defined prompt templates: THE IMAGE SHOWS A [CLASS LABEL], THE DIGIT APPEARS TO BE [CLASS LABEL], THERE IS AN IMAGE SHOWING A [CLASS LABEL], and THE NUMBER IS A [CLASS LABEL]. In order to reflect real-world settings where spurious features (e.g. skin markings in dermoscopic images [48]) may not be annotated in text, text captions in our synthetic settings solely refer to class labels and do not describe the spurious feature. For COCO, we use the provided text captions. ", "page_idx": 15}, {"type": "text", "text": "3. Fine-tuned VLM: We fine-tune each model $\\mathcal{M}$ on dataset $\\mathcal{D}_{F}^{e v a l}$ using a single NVIDIA A100 GPU with an initial learning rate of 5e-5. We use a batch size of 128 and train for 100 epochs with early stopping. We set the loss temperature as $\\tau\\,=\\,0.07$ . In line with prior works that explore the beneftis of locked image-text training [2, 46], we freeze the text encoder and only learn weights for the image encoder. ", "page_idx": 15}, {"type": "text", "text": "4. Evaluation dataset: We construct zero-shot classification datasets $\\mathcal{D}_{V}^{e v a l}$ from the test sets of MNIST, FashionMNIST, and COCO. For MNIST and FashionMNIST, we generate region bounding boxes using equally-sized quadrants. For COCO, we use the ground-truth bounding boxes and associated labels. Evaluation datasets are sampled to ensure that a correlation between $a^{e v a l}$ and ${\\bf e}^{e v a l}$ does not exist. For MNIST, we perform prompt ensembling for zero-shot classification using the following prompts: A PHOTO OF THE NUMBER [CLASS LABEL]; THE DIGIT [CLASS LABEL]; AN IMAGE OF A [CLASS LABEL]; [CLASS LABEL]. For FashionMNIST, we use the following prompts: A PHOTO OF A [CLASS LABEL]; THE [CLASS LABEL]; AN IMAGE OF A [CLASS LABEL]; [CLASS LABEL]. For COCO, we use the following prompts: THERE IS A [CLASS LABEL]; A PHOTO OF THE [CLASS LABEL]; A PHOTO OF A [CLASS LABEL]; [CLASS LABEL]. ", "page_idx": 15}, {"type": "text", "text": "Our 654 evaluation settings are summarized in Table 4. Datasets are licensed under CC BY, CC BY-SA, CC BY-NC, or MIT licenses. In Figure 4, we provide examples of both synthetic and real-world evaluation settings. ", "page_idx": 15}, {"type": "text", "text": "Table 4: Evaluation settings. We evaluate our approach on 654 settings, divided across 2 data domains and 2 model initializations. ", "page_idx": 15}, {"type": "table", "img_path": "UFRZHFYW8e/tmp/91716292f53d7b993306d3d55194d8bcc8944e6f99ea47ec5a7ac7b4bdabd566.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "UFRZHFYW8e/tmp/0f981fc6a07d07c8b3c88725d20a3f2aed53769a7aab188396af9a661c64718a.jpg", "img_caption": ["Figure 4: Example evaluation settings. Here, we provide examples of predefined spurious correlations, fine-tuning datasets, and evaluation datasets associated with a synthetic evaluation setting (top row) and a real-world evaluation setting (bottom row). The example synthetic evaluation setting consists of a predefined spurious correlation between a red rectangle (spurious image feature ${\\bf e}^{e v a l}$ ) and nine (textual attribute $\\bar{a}^{e v a l}$ ). This spurious correlation is visible in the vision-language fine-tuning dataset, where the presence of red rectangles and nines are strongly correlated, but not in the evaluation dataset. Similarly, the example real-world evaluation setting consists of a predefined spurious correlation between a person (spurious image feature ${\\bf e}^{e v a l}$ ) and couch (textual attribute $\\dot{a}^{e v a l}$ ). Again, this spurious correlation is visible in the vision-language fine-tuning dataset, where the presence of people and couches are strongly correlated, but not in the evaluation dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Extended Details on RAVL Mitigation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we extend Section 4.1 by providing additional descriptions of our region-aware loss function. ", "page_idx": 16}, {"type": "text", "text": "For batch $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , we define $\\mathcal{R}_{\\mathcal{B}}^{s}$ as the set of all spurious regions in the batch: $\\begin{array}{r}{\\mathcal{R}_{B}^{s}=\\bigcup_{I_{i}\\in B}\\mathcal{R}_{i}^{s}}\\end{array}$ . For image $I_{i}$ in batch $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , the first component of our region-aware loss function $L_{R}^{i}$ is designed to maximize embedding similarity between non-spurious regions $\\mathcal{R}_{i}^{r}$ and assigned class label $\\hat{y}_{i}$ ; simultaneously, $L_{R}^{i}$ will minimize embedding similarity between non-spurious regions $\\mathcal{R}_{i}^{r}$ and other class labels in the batch. We formulate $L_{R}^{i}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{R}^{i}=-\\log\\frac{\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y}_{i})}{\\sum_{\\hat{y_{j}}\\in\\mathcal{B}}\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y_{j}})+P(\\mathcal{R}_{B}^{s})},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $P(\\mathcal{R}_{B}^{s})$ is a penalty term that encourages dissimilarity between spurious features and correlated class labels as expressed below. Including this term in the denominator of $L_{R}^{i}$ is meant to pull embeddings of spurious regions away from correlated class labels. ", "page_idx": 16}, {"type": "equation", "text": "$$\nP(\\mathcal{R}_{B}^{s})=\\sum_{r_{j}\\in\\mathcal{R}_{B}^{s}}\\operatorname*{max}_{\\hat{y_{k}}\\in\\mathcal{B}}\\sigma(r_{j},\\hat{y_{k}})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The formula for $L_{R}^{i}$ includes two similarity functions: $\\sigma$ and $\\sigma_{m}$ . We define $\\sigma$ and $\\sigma_{m}$ as follows. Let $f$ represent a region embedding function (associated with the image encoder of VLM $\\mathcal{M}$ ) and let $g$ represent a text embedding function (associated with the text encoder of $\\operatorname{VLM}\\mathcal{M})$ . Then, for an arbitrary region $a$ , the function $f(a)$ will generate region embedding $f(a)\\in\\mathbb{R}^{d}$ with embedding dimension $d$ . For an arbitrary class label $b$ , $g(b)$ will generate text embedding $\\boldsymbol{g}(\\boldsymbol{b})\\in\\mathbb{R}^{d}$ . Given this notation, we establish the following definitions for $\\sigma$ and $\\sigma_{m}$ : ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\quad\\sigma(a,b)=\\exp(\\left<f(a),g(b)\\right>/\\tau)\\,\\,\\,}\\\\ {\\sigma_{m}(A,b)=\\exp(\\underset{a\\in\\mathcal{A}}{\\operatorname*{max}}(\\left<f(a),g(b)\\right>/\\tau))\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the loss term $L_{R}^{i}$ , the function $\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y_{i}})$ will compute the maximum similarity between regions in $\\mathcal{R}_{i}^{r}$ and class label $\\hat{y_{i}}$ . We specifically use the maximum operation in this computation since there are likely to be regions included in $\\mathcal{R}_{i}^{r}$ that do not reflect the class label; for instance, in the example provided in Figure 1, there may be non-spurious regions such as trees or leaves included in $\\mathcal{R}_{i}^{r}$ , which do not align with the animal class labels. The maximum operation ensures that the similarity between at least one region in $\\mathcal{R}_{i}^{r}$ and the class label should be high. ", "page_idx": 17}, {"type": "text", "text": "The second component of our region-aware loss function $L_{A}^{i}$ is designed to maximize embedding similarity between non-spurious regions $\\mathcal{R}_{i}^{r}$ and assigned class label $\\hat{y_{i}}$ ; simultaneously, $L_{A}^{i}$ will minimize embedding similarity between other regions in the batch and class label $\\hat{y_{i}}$ . We formulate $L_{A}^{i}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{A}^{i}=-\\log\\frac{\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y}_{i})}{\\sigma_{m}(\\mathcal{R}_{i}^{r},\\hat{y}_{i})+\\sum_{j=1,\\hat{y}_{j}\\ne\\hat{y}_{i}}^{|B|}\\sigma_{m}(\\mathcal{R}_{j}^{r},\\hat{y}_{i})+\\sum_{r_{j}\\in\\mathcal{R}_{B}^{s}}\\sigma(r_{j},\\hat{y}_{i})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "D Extended Evaluations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Extended Results for RAVL Stage 1 (Discovery) ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we extend the results provided in Section 3.3 with additional evaluations of Stage 1 of RAVL. Our goal is to evaluate the ability of RAVL to discover fine-grained spurious correlations between image features and textual attributes. ", "page_idx": 17}, {"type": "text", "text": "D.1.1 Extended Comparisons to Prior Approaches ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We implement RAVL according to the details provided in Section 3.1. RAVL includes a clustering step that identifies groups of visually-similar regions. For all evaluation settings, we identify the optimal number of clusters by sweeping all cluster sizes ranging between $|\\mathcal{V}|*2$ and $|\\mathcal{V}|*5$ ; we then select the optimal number of clusters using Silhouette scores. We select these bounds to be larger than the class label set size by several multiples in order to ensure that clusters adequately separate distinct features. Prior works [13, 43] have also utilized overclustering approaches for this objective. We note that users can adjust the bounds based on the composition of their dataset; for instance, complex datasets with diverse features may require a larger range. For MNIST and FashionMNIST, the size of the label set $|\\mathcal{V}|$ is 10; For COCO, the size of the label set ranges between 2 and 5. ", "page_idx": 17}, {"type": "text", "text": "For all baselines, we utilize the official implementations provided by the authors. We adapt Domino, George, and Distilling Failures for our setting by providing region embeddings as input rather than image embeddings. ", "page_idx": 17}, {"type": "text", "text": "In Figure 5, we provided an extended version of Figure 2. We demonstrate that RAVL consistently outperforms baselines across two domains (synthetic images and real images), two model initializations (CLIP-RN50 and CLIP-RN101), and four learned correlation strengths (measured by varying $\\tau_{e v a l}\\in\\{10,20,30,40\\})$ . ", "page_idx": 17}, {"type": "text", "text": "D.1.2 Additional details for in-the-wild evaluations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Evaluations on scene classification: Here, we provided extended details on our in-the-wild evaluations performed on scene images (Section 3.3). ", "page_idx": 17}, {"type": "text", "text": "We leverage ten off-the-shelf VLMs as our model $\\mathcal{M}$ : CLIP-RN50, OpenCLIP-RN50, CLIP-RN101, OpenCLIP-RN101, CLIP-ViTB/32, OpenCLIP-ViTB/32, CLIP-ViTB/16, OpenCLIP-ViTB/16, CLIPViTL/14, and OpenCLIP-ViTL/14 [36, 20]. The four RN models utilize ResNet vision encoders and the six ViT models utilize Vision Transformer backbones. The CLIP models were trained on a proprietary dataset with $400\\mathrm{M}$ image-text pairs. OpenCLIP ResNet models were trained on YFCC15M [44], and OpenCLIP ViT models were trained on LAION2B [40]. ", "page_idx": 17}, {"type": "image", "img_path": "UFRZHFYW8e/tmp/8991059c9cf94bf42e87a4c747b61bc15d07cc7d99007172aa6f345c25334367.jpg", "img_caption": ["Figure 5: RAVL accurately identifies spurious correlations. Here, we provide an extended version of Figure 2, which demonstrates that RAVL consistently outperforms prior methods in discovering learned spurious correlations between image features and textual attributes. Here, we provide Precision $@10$ metrics for a CLIP-RN50 model fine-tuned on synthetic data (129 settings) and realworld data (171 settings); a CLIP-RN101 model fine-tuned on synthetic data (162 settings) and real-world data (192 settings); and an average across both model architectures. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "We select SUN397 as our zero-shot classification dataset $\\mathcal{D}_{V}$ [52]. SUN397 consists of scene images from 397 classes. We use the test data from official partition number 1, which consists 19,850 images. We then use an off-the-shelf region proposal network [63] to identify candidate regions. ", "page_idx": 18}, {"type": "text", "text": "For each VLM $\\mathcal{M}$ , we perform 397-class zero-shot scene classification on SUN397. We use a prompt ensemble consisting of two prompt templates as provided by CLIP [36]. Due to the large size of the zero-shot classification dataset $\\mathcal{D}_{V}$ , we perform clustering using the CLARA (Clustering for Large Applications) algorithm, which is an efficient implementation of K-Medoids, and fix the number of clusters as $|\\mathcal{V}|*2$ , which is 794 in this case. ", "page_idx": 18}, {"type": "text", "text": "Evaluations on chest $\\mathbf{X}$ -ray classification: Here, we provided extended details on our in-the-wild evaluations performed on medical images (Section 3.3). In recent years, a range of vision [47, 54, 37] and vision-language [45, 5, 4, 62] models have been proposed for learning diagnostic patterns in medical images, and there is a critical need for methods capable of identifying spurious correlations in this domain. Our goal is to determine if RAVL can effectively surface spurious correlations learned by real-world fine-tuned VLMs developed for medical image interpretation. ", "page_idx": 18}, {"type": "text", "text": "We leverage two off-the-shelf variants of the PubMedCLIP model [12] as our VLM $\\mathcal{M}$ : PubMedCLIPRN50 and PubMedCLIP-ViTB/32. The PubMedCLIP-RN50 model utilizes a ResNet-50 vision encoder and was fine-tuned from the CLIP-RN50 model. The PubMedCLIP-ViTB/32 model utilizes a Vision Transformer backbone for the vision encoder and was fine-tuned from the CLIP-ViTB/32 model. Both variants of PubMedCLIP are fine-tuned using ROCO, a large radiology dataset consisting of images and captions collected from PubMed [35]. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We select Object-CXR as our zero-shot classification dataset $\\mathcal{D}_{V}$ . Object-CXR is a dataset of 10,000 frontal chest X-rays compiled from around 300 township hospitals in China [23]. Twelve radiologists with 1-3 years of experience annotated the images, identifying foreign objects within the lung field using bounding boxes, ellipses, or masks, excluding support devices. We retain only bounding boxes and exclude chest X-rays without annotations, resulting in 8,726 object annotations across 4,372 images. For our evaluations, we use the Object-CXR dev split, which includes 974 object annotations across 489 images. We assign image-level labels to the dataset using torchxrayvision [8], a library that includes a variety of pretrained chest X-ray models. Specifically, we use the XRVDENSENET121-DENSENET121-RES224-ALL pretrained model to produce multi-class labels for a variety of diseases, including Enlarged Cardiomediastinum, Cardiomegaly, Lung Opacity, Lung Lesion, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, and Fracture. A disease is identified as present if it meets a confidence threshold of 0.60. ", "page_idx": 19}, {"type": "text", "text": "For each PubMedCLIP VLM $\\mathcal{M}$ , we perform binary zero-shot classification of cardiomegaly in Object-CXR. Cardiomegaly is a medical condition characterized by the presence of an enlarged heart. After performing a manual search over the text prompt space, we identify CARDIOMEGALY and NORMAL as the prompts that lead to the highest zero-shot classification accuracy for both model variants. The PubMedCLIP-RN50 model achieves an overall zero-shot classification accuracy of 74.2, with an accuracy of 14.0 on the group of images with cardiomegaly and an accuracy of 91.9 on the group of images without cardiomegaly. The PubMedCLIP-ViTB/32 achieves an overall zero-shot classification accuracy of 39.4, with an accuracy of 80.4 on the group of images with cardiomegaly and an accuracy of 28.0 on the group of images without cardiomegaly. Interestingly, given the selected prompts, we note that the PubMedCLIP-RN50 and the PubMedCLIP-ViTB/32 models exhibit inverse trends, with PubMedCLIP-RN50 achieving higher performance on the class of images without cardiomegaly and PubMedCLIP-ViTB/32 achieving higher performance on the class of images with cardiomegaly. ", "page_idx": 19}, {"type": "text", "text": "Given VLM $\\mathcal{M}$ and zero-shot classification dataset $\\mathcal{D}_{V}^{e v a l}$ , we apply RAVL in order to surface learned spurious correlations. Similar to our controlled evaluations on synthetic datasets, we perform K-Medoids clustering with the number of clusters ranging from 20 to 50. The optimal number of clusters is selected using Silhouette distance; we use 24 clusters for PubMedCLIP-RN50 and 20 clusters for PubMedCLIP-ViTB/32. The final cluster performance gap metric $G_{c}$ associated with the top-ranked spurious feature cluster is 0.041 and 0.119 for the PubMedCLIP-RN50 and PubMedCLIP-ViTB/32 models respectively. ", "page_idx": 19}, {"type": "image", "img_path": "UFRZHFYW8e/tmp/6b2d1a86069cd93ceba7102d8f2ab87b1ca64b41c4a7b60c079035dad6d8beb8.jpg", "img_caption": ["Figure 6: RAVL surfaces spurious correlations in off-the-shelf VLMs. Here, we extend Figure 3 with additional examples of spurious correlations discovered by RAVL in off-the-shelf-VLMs. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Extended Results: In Figure 6, we extend the qualitative results provided in Figure 3 with additional examples of spurious correlations surfaced by RAVL in off-the-shelf VLMs. We make the following observations: ", "page_idx": 19}, {"type": "text", "text": "\u2022 For the OpenCLIP ViT-L/14 model, RaVL surfaces a feature cluster consisting of green plants and fences. We observe a performance gap of 18.3 points between images with class label outdoor chicken coop that contain the RaVL-identified feature and those that do not contain the feature. This suggests that the OpenCLIP ViT-L/14 model can better classify an outdoor chicken coop scene when green plants and fences are present. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 For the CLIP ViT-B/32 model, RaVL surfaces a feature cluster consisting of people. We observe a performance gap of 24.3 points between images with class label pub (indoor) that contain the RaVL-identified feature and those that do not contain the feature. This suggests that the CLIP ViT-B/32 model can better classify a pub (indoor) scene when people are present.   \n\u2022 For the OpenCLIP ResNet-101 model, RaVL surfaces a feature cluster consisting of chairs. We observe a performance gap of 23.3 points between images with class label restaurant patio that contain the RaVL-identified feature and those that do not contain the feature. This suggests that the OpenCLIP ResNet-101 model can better classify restaurant patio scenes when chairs are present. ", "page_idx": 20}, {"type": "text", "text": "D.2 Extended Results for RAVL Stage 2 (Mitigation) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We train model $\\mathcal{M}_{n e w}$ using a single NVIDIA A100 GPU with an initial learning rate of 5e-5. We use a batch size of 128 and train for 100 epochs with early stopping. We set the loss temperature as $\\tau=0.07$ and use $\\lambda=0.8$ in loss function $\\mathcal{L}$ . In line with prior works that utilize locked image-text fine-tuning [2, 46], we freeze the text encoder and solely learn weights for the image encoder. We generate candidate regions for the fine-tuning dataset $\\bar{D_{F}^{e v a l}}$ using a region proposal network with identical settings to prior work [63]. ", "page_idx": 20}, {"type": "text", "text": "Below, we provide additional implementation details for the five mitigation baselines we explore in this study. Since there are a limited number of existing approaches designed for mitigating spurious correlations in fine-tuned VLMs, we adapt several existing methods for our setting in order to train model Mnew: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Standard VLM Fine-Tuning: We perform standard VLM fine-tuning with the original loss function $L_{C L}$ used to train model $\\mathcal{M}$ . In our experiments, $L_{C L}$ is the CLIP objective [36].   \n\u2022 Upsampled VLM Fine-Tuning: We perform VLM fine-tuning with the original loss function $L_{C L}$ used to train model $\\mathcal{M}$ . In our experiments, $L_{C L}$ is the CLIP objective [36]. We utilize a weighted sampler to upsample minority groups during training; class and subgroup labels are derived from Stage 1 of RAVL as detailed in Section 4.1.   \n\u2022 VL-ERM: Since empirical risk minimization (ERM) is traditionally used in unimodal classification settings, we adapt ERM for our multimodal setting by incorporating an extra contrastive vision-language objective function; this loss function is intended to ensure that VLM $\\mathcal{M}_{n e w}$ learns image-text relationships during training. Specifically, the final loss function during training is $\\mathcal{L}_{V L E R M}=\\lambda L_{C L}+(1-\\lambda)L_{E R M}$ . Here, $L_{C L}$ takes the form of the original loss function used to train model $\\mathcal{M}$ ; in our experiments $L_{C L}$ is the CLIP objective [36]. We set $\\lambda=0.8$ . During training, we apply ERM to zero-shot classification logits computed using image embeddings and text embeddings of class labels. We utilize a weighted sampler to upsample minority subgroups; class and subgroup labels are derived from Stage 1 of RAVL as detailed in Section 4.1.   \n\u2022 VL-GDRO [39]: Since GDRO is traditionally used in unimodal classification settings, we adapt GDRO for our multimodal setting by incorporating an extra contrastive visionlanguage objective function; this loss function is intended to ensure that VLM $\\mathcal{M}_{n e w}$ learns image-text relationships during training. Specifically, the final loss function during training is $\\mathcal{L}_{V L G D R O}=\\lambda L_{C L}+(1-\\lambda)L_{G D R O}$ . Here, $L_{C L}$ takes the form of the original loss function used to train model $\\mathcal{M}$ ; in our experiments $L_{C L}$ is the CLIP objective [36]. We set $\\lambda=0.8$ . During training, we apply GDRO to zero-shot classification logits computed using image embeddings and text embeddings of class labels. In line with standard practice, we utilize a weighted sampler to upsample minority subgroups; class and subgroup labels are derived from Stage 1 of RAVL as detailed in Section 4.1.   \n\u2022 Spurious-Aware Mitigation [56]: Spurious-aware mitigation aims to address spurious correlations in VLMs using a combination of five loss functions: one CLIP objective function, two contrastive image objective functions meant to address spurious correlations in the image space, and two contrastive language objective functions meant to address spurious ", "page_idx": 20}, {"type": "table", "img_path": "UFRZHFYW8e/tmp/3707c9ce23335d47f612024517cf681aadd0c6f68e2ae9b94107b7a266ab0344.jpg", "table_caption": ["Table 5: RAVL effectively mitigates spurious correlations across various model initializations. Here, we provide an extended version of Table 3 with a breakdown of results by model initialization (CLIP-RN50 vs. CLIP-RN101). Our results demonstrate that RAVL consistently outperforms prior methods in mitigating spurious correlations. We report mean Image Overall (Img. Overall), Image Worst Group (Img. WG), Region Overall (Reg. Overall), and Region Worst Group (Reg. WG) metrics across our real-world evaluation settings. "], "table_footnote": ["correlations in the text space. We note that Spurious-Aware Mitigation was explicitly designed for the fine-tuned VLM setting. We follow the implementation of Spurious-Aware Mitigation provided by [56]. In our work, since we solely fine-tuned the vision encoders of VLMs $\\mathcal{M}$ , we use a version of Spurious-Aware Mitigation with the CLIP objective function and two contrastive image objective functions. "], "page_idx": 21}, {"type": "text", "text": "Prior works on model robustness predominantly evaluate model performance using image worstgroup scores [39]. In addition to image worst-group accuracy, we also report region overall and region worst-group accuracies, which evaluate the extent to which the VLM understands fine-grained features. Region-level accuracies are computed by performing zero-shot classification with region embeddings and comparing predicted labels to the ground-truth region-level labels provided in the zero-shot classification dataset. ", "page_idx": 21}, {"type": "text", "text": "In Table 5, we provide an extended version of Table 3 with a breakdown of results by model initialization (CLIP-RN50 and CLIP-RN101). We demonstrate that RAVL consistently outperforms prior methods across both model initializations. In Table 6, we provide an extended version of Table 3 with a breakdown of results by the learned correlation strength of the original VLM $\\mathcal{M}$ . RAVL consistently outperforms prior methods across four correlation strengths $\\tau_{e v a l}\\in\\{10,20,30,40\\}$ . ", "page_idx": 21}, {"type": "text", "text": "D.3 Computational Complexity Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we provide an analysis of the computational complexity of RAVL. RAVL is computationally inexpensive; in particular, the RAVL discovery stage can be run efficiently on CPU and the mitigation stage adds only a small computational overhead. Below, we provide an analysis of computational complexity for each stage of RAVL. ", "page_idx": 21}, {"type": "text", "text": "Computational complexity analysis of RAVL Stage 1: The discovery stage of RAVL is specifically designed to be run on a labeled validation dataset $\\mathcal{D}_{V}$ ; in real-world settings, validation datasets are often relatively small in size due to the human effort needed for securing labels, rendering this stage as computationally inexpensive for diverse applications. Even if the validation dataset is large in size, RAVL operates efficiently as follows: ", "page_idx": 21}, {"type": "text", "text": "\u2022 First, RAVL preprocesses images by decomposing each image into candidate regions; there are a variety of ways in which a user can decompose an image into regions, such as by using equal-sized segments (e.g. quadrants) or running inference with region proposal networks (RPNs). Both methods are inexpensive and only need to be run once in an offilne manner. Similar approaches have been applied to large-scale datasets in prior work [63]. \u2022 Then, embeddings need to be generated for each region, which can be done by utilizing VLM $\\mathcal{M}$ for inference (forward passes only). Across a set of 10 FashionMNIST and COCO ", "page_idx": 21}, {"type": "text", "text": "Table 6: RAVL effectively mitigates spurious correlations across learned correlation strengths. Here, we provide an extended version of Table 3 with a breakdown of results by the learned correlation strength $(\\tau_{e v a l}\\in\\{10,20,30,40\\}$ of the original model $\\mathcal{M}$ . We use the subset of 106 evaluation settings where RAVL Stage 1 Precision $@10$ is greater than 0.8. Our results demonstrate that RAVL consistently outperforms prior methods in mitigating spurious correlations across various correlation strengths and model initializations. We report mean Image Worst Group (Img. WG) and Region Worst Group (Reg. WG) metrics across our real-world evaluation settings. We note that there are no valid evaluation settings for CLIP-RN101 when the learned correlation strength $\\tau_{e v a l}$ of the original model $\\mathcal{M}$ is set to 40. ", "page_idx": 22}, {"type": "table", "img_path": "UFRZHFYW8e/tmp/8d8200c6cabdd3718328a30eeeeaa4c72bde7c2a8a71e3c1aa6091778b0355b9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "evaluation settings, we observe embedding generation to take a mean of 24.5 seconds on a single A100 GPU. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Finally, given candidate regions and corresponding embeddings, the remainder of the RaVL discovery procedure (clustering and computation of metrics) can be run completely on CPU. Across a set of 10 evaluation settings on COCO and FashionMNIST, we observe that clustering and computation of metrics require a mean of 3.4 seconds to run. ", "page_idx": 22}, {"type": "text", "text": "Computational complexity analysis of RAVL Stage 2: The mitigation stage of RAVL requires finetuning a VLM $\\mathcal{M}_{n e w}$ . Across a set of 10 evaluation settings on COCO and FashionMNIST, we observe that the inclusion of our fine-grained region-aware loss function at this stage adds an average of 0.15 seconds per training step (on a single A100 GPU) in comparison to the original fine-tuning procedure for $\\mathcal{M}$ . ", "page_idx": 22}, {"type": "text", "text": "E Extended Discussion ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Societal Impact: The goal of our work is to improve robustness of fine-tuned VLMs to spurious correlations. As VLMs become more commonplace in society, we hope that our approach can enable users to better detect and mitigate model failures prior to deployment. We also note that our work includes a series of evaluations on medical images; rigorous clinical testing is necessary before robustness approaches are deployed in healthcare settings. ", "page_idx": 22}, {"type": "text", "text": "Limitations: In line with prior works in vision-only and vision-language settings, our method is specifically designed to surface and mitigate local, fine-grained spurious features. There may be some sources of spurious signal that do not manifest in this way; for instance, features like image brightness or gender can be considered global features, where the spurious signal is not localized to a particular image region. Our approach is not designed for these global spurious features. Rather, our problem setting is inspired by the many real-world, practical examples of region-level spurious features that have been demonstrated in literature to affect model performance, such as image-level markings in dermoscopic images [48], medical devices in radiographs [34], and text markers in medical images [10]. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The abstract and introduction describe the claims made in the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss limitations in Appendix Section E. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not include theoretical proofs. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided details for all experiments in Sections 3 and Sections 4. Hyperparameters and implementation details are discussed in detail in Appendix Sections B, C, and D. Code is available. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Code has been made available. All hyperparameters and details necessary for replicating our results are detailed in the main text and the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have provided implementation details and hyperparameters for all models.   \nDetails are included in Appendix Sections B, C, and D. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Figures 2 and 5 include $95\\%$ confidence intervals. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Details on compute resources and computational complexity are provided in Appendix Sections B and D. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Our work conforms to the NeurIPS code of ethics. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We discuss societal impacts of our work in Appendix Section E. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All existing data (MNIST, FashionMNIST, COCO, SUN397, ObjectCXR), models (CLIP VLMs, OpenCLIP VLMs, PubMedCLIP VLMs), and code used in this paper have been cited and are available under CC BY, CC BY-SA, CC BY-NC, or MIT licences. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our evaluation framework and algorithm are documented in Sections 3 and 4 as well as Appendix Sections B, C, and D. Training and implementation details are also documented in these sections. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]