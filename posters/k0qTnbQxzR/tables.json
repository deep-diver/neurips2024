[{"figure_path": "k0qTnbQxzR/tables/tables_6_1.jpg", "caption": "Table 1: Performance on Visual Question Answering benchmarks, where the results labeled with \u2020 refer to the few-shot setting. CogCoM achieves SOTA across the board, and demonstrates the effectiveness on the visual reasoning and scene texts recognition benchmarks.", "description": "This table presents the results of several Vision-Language Models (VLMs) on four detailed visual question answering (VQA) benchmark datasets: GQA, TextVQA, ST-VQA, and TallyVQA.  The performance of each model is measured by its accuracy on different question types (simple, complex, test-balanced).  CogCoM, the model introduced in the paper, is shown to achieve state-of-the-art (SOTA) performance across all benchmarks, outperforming other generalist VLMs and achieving comparable results to specialist SOTAs.  The table highlights CogCoM's superior ability in detailed visual reasoning and scene text recognition tasks.", "section": "5.1 Experiments on Detailed VQA"}, {"figure_path": "k0qTnbQxzR/tables/tables_8_1.jpg", "caption": "Table 2: Results on VG benchmarks, where the specialist SOTAs are quoted from (Bai et al., 2023).", "description": "This table presents the performance of different models on visual grounding benchmarks (RefCOCO, RefCOCO+, RefCOCOg).  It compares the performance of CogCoM against other generalist models and specialist state-of-the-art models (SOTAs) on various subsets of the benchmarks. The results show the effectiveness of CogCoM in visual grounding tasks, demonstrating superior performance in many subsets compared to other generalist models and competitive results against the specialized state-of-the-art.", "section": "5.2 Experiments on Visual Grounding"}, {"figure_path": "k0qTnbQxzR/tables/tables_8_2.jpg", "caption": "Table 3: Evaluation results on the general and hallucination assessment benchmarks.", "description": "This table presents the performance of various vision-language models on two benchmarks: MM-Vet, which assesses general multimodal capabilities, and POPEadv, which evaluates the models' resistance to hallucination.  The results show the accuracy of each model on each benchmark. The LLM used as the backbone for each model is also specified.", "section": "5.3 Experiments on General Multimodal Evaluation and Hallucination Examination"}, {"figure_path": "k0qTnbQxzR/tables/tables_15_1.jpg", "caption": "Table 4: Detailed statistics the the training data and evaluation data synthesised with CoM production", "description": "This table presents a statistical summary of the training and evaluation data generated using the Chain of Manipulations (CoM) method.  It shows the number of questions (#QAs), chains (#Chains), average steps per chain (#Steps/Chain), and average manipulation types per chain (#Manipulations Types/Chain) for five different datasets: TextVQA, ST-VQA, TDIUC-count, TDIUC-absurd, and CoM-test.  The data highlights the varying complexity of the visual reasoning tasks across different datasets.", "section": "C.3 Data Statistics"}, {"figure_path": "k0qTnbQxzR/tables/tables_17_1.jpg", "caption": "Table 1: Performance on Visual Question Answering benchmarks, where the results labeled with \u2020 refer to the few-shot setting. CogCoM achieves SOTA across the board, and demonstrates the effectiveness on the visual reasoning and scene texts recognition benchmarks.", "description": "This table presents the performance comparison of different models on four visual question answering benchmarks: GQA, TextVQA, ST-VQA, and TallyVQA.  The results are categorized by model type (generalist vs. specialist) and task complexity (simple vs. complex). The table highlights CogCoM's state-of-the-art performance and substantial improvements over baseline models, particularly on complex tasks demanding detailed visual reasoning and text recognition.", "section": "5.1 Experiments on Detailed VQA"}]