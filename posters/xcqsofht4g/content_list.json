[{"type": "text", "text": "Simplified and Generalized Masked Diffusion for Discrete Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jiaxin Shi\u2217, Kehang Han\u2217, Zhe Wang, Arnaud Doucet, Michalis K. Titsias Google DeepMind ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet $64\\!\\times\\!64)$ bits per dimension that are better than autoregressive models of similar sizes. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Since their inception [1\u20133], diffusion models have emerged as the workhorse for generative media, achieving state-of-the-art in tasks such as image synthesis [4\u20136], audio [7, 8] and video generation [9\u2013 13]. The majority of existing successes are for continuous state space diffusions. While diffusion models have been extended to discrete state spaces [1, 14, 15] and have been successfully applied to applications ranging from graph generation [16], text-to-sound generation [17] or protein design [18], they remain not as widely used as their continuous counterparts as they are not competitive with autoregressive models in important domains such as text modeling. This has motivated the development of continuous space diffusion models where the discrete data are embedded in the Euclidean space [19\u201323] or the simplex [24\u201328]. We believe that one of the reasons for the limited success of discrete diffusions is that they have been hindered by fairly complex formulations and training objectives. This paper is a step towards closing this gap. ", "page_idx": 0}, {"type": "text", "text": "In this work, we focus on \u201cmasked\u201d (or \u201cabsorbing\u201d) diffusions, a discrete diffusion formulation first presented by Austin et al. [14], and later explored by the literature from various perspectives [29\u201332]. We follow here a continuous-time framework which has proven very useful to improve the training and understanding of continuous state space diffusions [see e.g., 3, 33, 34]. We make several technical contributions which simplify the training of these models and improve significantly their performance. Our contributions are as follows: ", "page_idx": 0}, {"type": "text", "text": "\u2022 We provide a remarkably simple expression of the Evidence Lower Bound (ELBO) for masked diffusion models, showing that it corresponds to a weighted integral over time of cross-entropy losses. Similarly to continuous space diffusions [33], this objective can be rewritten in terms of signal-to-noise ratio and exhibits invariance properties. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a unifying understanding of previously proposed continuous-time discrete diffusion models [29, 32, 35], revealing the changes they made to our ELBO objective and/or model parameterization. We show that these changes either lead to expensive model evaluations, or large variance in training, or breaking the consistency between forward and reverse processes. \u2022 On GPT-2 scale text modeling and pixel-level image modeling tasks, masked diffusions trained using our simple ELBO objective outperform previous proposals, leading to the best likelihood and zero-shot transfer performance among discrete diffusion models. \u2022 Finally, based on our simplified masked diffusion formulation, we propose a generalized masked diffusion model that allows state-dependent masking schedules. This generalized masked diffusion model further improves predictive performance measured by test likelihoods. ", "page_idx": 1}, {"type": "text", "text": "Concurrent work by Ou et al. [36] and Sahoo et al. [37] derives a similar simplified expression of the ELBO. Ou et al. [36]\u2019s derivation relies on an observation similar to the one we made in Proposition 1. ", "page_idx": 1}, {"type": "text", "text": "2 Masked Diffusion ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Assume our data lives in a finite discrete state space of size $m$ . We use the integers $0,1,\\ldots,m\\!-\\!1$ and their corresponding one-hot vectors $e_{0},e_{1},\\ldots,e_{m-1}$ to represent the states.2 In masked diffusion, we augment the space with an additional mask state, which is assigned the index $m$ . A forward \u201cmasking\u201d process is defined such that a data point is evolved into the mask state at some random time. Reversing this process then gives a generative model of the discrete data. ", "page_idx": 1}, {"type": "text", "text": "Discrete-time forward process. The forward process is defined as a Markovian sequence of discrete random variables $x_{t}$ indexed by time $t$ , where $t$ runs from 0 to 1. Throughout the work, we abuse the notation such that $x_{t}$ can be either an integer or its corresponding one-hot vector, whenever it is clear from the context. We divide $[0,1]$ into $T$ intervals, and let $s(i)\\bar{=(i-1)}/T,t(i)=i/T$ . Following Austin et al. [14], the state transition between $[s(i),t(i)]$ is determined by a transition matrix of size $(m+1)\\times(m+1)$ : $Q_{i}=(1-\\beta_{i})I+\\beta_{i}\\mathbf{1}e_{m}^{\\top}$ , where $\\mathbf{1}$ is an all-one vector of size $m+1$ , $e_{m}$ represents a one-hot vector where element at index $m$ is 1. Each entry $[Q_{i}]_{j k}$ denotes the probability of transition from the state $j$ to the state $k$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\n[Q_{i}]_{j k}=q(x_{t(i)}=k|x_{s(i)}=j)=(1-\\beta_{i})\\delta_{j k}+\\beta_{i}\\delta_{k m}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "This means that, with probability $1-\\beta_{i}$ , $x_{t(i)}=x_{s(i)}$ , otherwise it jumps to the mask state. Given the above transition matrix, the marginal distribution at time $t(i)$ given $x_{0}$ is ", "page_idx": 1}, {"type": "equation", "text": "$$\nq(x_{t(i)}|x_{0})=\\mathrm{Cat}(x_{t(i)};\\bar{Q}_{i}^{\\top}x_{0})=x_{0}^{\\top}\\bar{Q}_{i}x_{t(i)}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Here, we use $\\operatorname{Cat}(x;p)$ to denote a Categorical distribution where $p$ is the vector of probabilities of being in each category, and $\\begin{array}{r}{\\bar{Q}_{i}\\triangleq\\prod_{j=1}^{i}Q_{j}=\\alpha_{i}I+\\left(1-\\alpha_{i}\\right)\\mathbf{1}e_{m}^{\\top}}\\end{array}$ for $\\begin{array}{r}{\\alpha_{i}=\\prod_{j=1}^{i}(1-\\beta_{j})}\\end{array}$ . We expect $\\alpha_{T}$ to become very small or zero for a sufficiently large $T$ such that $q(x_{1}|x_{0})$ for any $x_{0}$ will become a delta mass at the mask state. ", "page_idx": 1}, {"type": "text", "text": "Continuous-time limit. We can define a continuous-time forward process by taking a limit of the above discrete-time process. We first specify a continuous function $\\beta(t)$ such that $\\bar{\\beta_{i}}=\\beta(t(i))/T$ We then let $T\\rightarrow\\infty$ in the discrete-time process and compute the limit of $\\bar{Q}_{i}$ (proved in Austin et al. 14, Appendix A.6, see also App. A) as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\bar{Q}(t)\\triangleq\\operatorname*{lim}_{T\\to\\infty}\\bar{Q}_{i}=\\alpha_{t}I+(1-\\alpha_{t})\\mathbf{1}e_{m}^{\\top},\\mathrm{~where~}\\alpha_{t}\\triangleq\\exp\\Big(-\\int_{0}^{t}\\beta(s)\\mathrm{d}s\\Big),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "image", "img_path": "xcqSOfHt4g/tmp/bbd0a3f5767263f511afbddd60d40f7fe60fffdda5d300405565a6d493633c8d.jpg", "img_caption": ["Figure 1: Masking schedules in the literature: (Left) $\\alpha_{t}$ ; (Right) weight of the cross-entropy loss w.r.t. $t$ ; Equations for these schedules are given in Tab. 4 in Appendix. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "so that $q(x_{t}|x_{0})\\,=\\,\\mathrm{Cat}(x_{t};\\bar{Q}(t)^{\\top}x_{0})$ . For two arbitrary times, $0\\,\\leq\\,s\\,<\\,t\\,\\leq\\,1$ , the transition distribution that is compatible with the above marginal (i.e., $\\begin{array}{r}{q(x_{t}|x_{0})=\\sum_{x_{s}}q(x_{t}|x_{s})q(x_{s}|x_{0}))}\\end{array}$ is $q(x_{t}|x_{s})=\\mathrm{Cat}(x_{t};\\bar{Q}(s,t)^{\\top}x_{s}),\\mathrm{~where~}\\bar{Q}(s,t)\\triangleq\\bar{Q}(s)^{-1}\\bar{Q}(t)=\\frac{\\alpha_{t}}{\\alpha_{s}}I+\\big(1-\\frac{\\alpha_{t}}{\\alpha_{s}}\\big)\\mathbf{1}e_{m}^{\\top}.$ ", "page_idx": 2}, {"type": "text", "text": "Note that Austin et al. [14] did not derive this explicit form of transition matrix between two arbitrary time $s$ and $t$ , which appeared later in Zhao et al. [38] concurrently with our work. ", "page_idx": 2}, {"type": "text", "text": "Masking schedules. From the definition of $\\alpha_{t}$ , we have that $\\alpha_{0}=1$ . And similar to the discretetime formulation, we would like $\\alpha_{1}$ be zero or very close to zero. We provide a summary of masking schedules from literature that satisfy these properties in Fig. 1. The linear schedule was proposed in Sohl-Dickstein et al. [1] for binary variables and then re-derived by Austin et al. [14] from mutual information for discrete-time models. The geometric schedule $\\alpha_{t}$ is plotted for $\\bar{\\beta}_{\\mathrm{min}}=10^{-5}$ and $\\bar{\\beta}_{\\mathrm{max}}=20$ . It was first used for continuous diffusions [3] and then for discrete by Lou et al. [32]. The cosine schedule was originally proposed in MaskGIT [39], an iterative unmasking generative model inspired by diffusion. This schedule has the property of slowing down the unmasking process at the beginning of the reverse generation. Aligning with their observation, we find that this results in a lower chance of confilcting tokens being unmasked simultaneously at the start of generation, thereby enhancing the overall generation quality. ", "page_idx": 2}, {"type": "text", "text": "Time reversal of the forward process given $x_{0}$ . The analytic property of our forward process allows to compute many quantities of interest in closed form. One such quantity frequently used in diffusion models is the time reversal of the forward process given $x_{0}{\\mathrm{:~}}\\dot{q}(x_{s}|x_{t},x_{0})$ for $s\\leq t$ . We derive it in App. C as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(x_{s}|x_{t},x_{0})=\\mathrm{Cat}(x_{s};\\bar{R}^{x_{0}}(t,s)^{\\top}x_{t}),\\mathrm{~where~}\\bar{R}^{x_{0}}(t,s)=I+\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}e_{m}(x_{0}-e_{m})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "From the transition matrix $\\bar{R}^{x_{0}}(t,s)\\in\\mathbb{R}^{(m+1)\\times(m+1)}$ we can see the reverse process conditioned on $x_{0}$ has a very simple logic\u2014if $x_{t}$ is a mask, with probability $\\frac{\\alpha_{s}\\!-\\!\\alpha_{t}}{1\\!-\\!\\alpha_{t}}$ , it will jump to the state $x_{0}$ at time $s$ , otherwise it will stay masked. Once $x_{t}$ is unmasked, it remains in the same state until the end. ", "page_idx": 2}, {"type": "text", "text": "3 Model and Objective ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For a discrete-time masked diffusion process, we define our generative model by approximately reversing the forward transitions using a reverse model $p_{\\theta}(x_{s}|x_{t})$ . One way to define this model is ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\theta}(x_{s}|x_{t})\\triangleq q(x_{s}|x_{t},\\mu_{\\theta}(x_{t},t)),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mu_{\\theta}(x_{t},t)\\in\\Delta^{m+1}$ is a probability vector parametrized by a neural network $f_{\\theta}$ with a softmax applied to the output logits (note the $m$ -th output is forced to 0 since the clean data cannot be masks): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu_{\\boldsymbol{\\theta}}(x_{t},t)=\\left\\{\\!\\!\\begin{array}{l l}{\\mathrm{softmax}(f_{\\boldsymbol{\\theta}}(x_{t},t))}&{x_{t}=m,\\smallskip}\\\\ {x_{t}}&{x_{t}\\neq m.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This is known as mean-parameterization since it leverages a prediction model for the mean of $x_{0}$ . A matrix-form depiction of $p_{\\theta}(x_{s}|x_{t})$ is shown in Fig. 6 (right). In fact, we can select a time-invariant parametrization $\\mu_{\\theta}(x_{t},t)=\\mu_{\\theta}(x_{t})$ as [36] showed that $p(x_{0}|x_{t})$ given $x_{t}=x$ is identical for any $t$ . ", "page_idx": 2}, {"type": "text", "text": "Besides $p_{\\theta}(x_{s}|x_{t})$ , we also need to specify $p(x_{0}|x_{t(1)})$ and the prior distribution $p\\big(x_{t(T)}\\big)=p\\big(x_{1}\\big)$ .   \nFollowing the practice in continuous diffusion models [33], we choose $p(x_{0}|x_{t(1)})\\propto q(x_{t(1)}|x_{0})$ .   \nAnd since $q(x_{1}|x_{0})\\approx\\delta_{x_{1},m}$ for any $x_{0}$ as $\\alpha_{1}\\approx0$ , we set $p(x_{1})\\approx\\delta_{x_{1},m}$ , see App. E. ", "page_idx": 3}, {"type": "text", "text": "We then write out the discrete-time diffusion model objective [1, 2], which is a lower bound of the log marginal likelihood of data $x_{0}$ under the model $p$ (known as the Evidence Lower Bound, or ELBO): ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log p(x_{0})\\geq\\mathbb{E}_{q(x_{t(1)}|x_{0})}[\\log p(x_{0}|x_{t(1)})]-\\mathrm{KL}(q(x_{1}|x_{0})\\|p(x_{1}))-\\mathcal{L}_{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{L}_{T}=\\sum_{i=2}^{T}\\mathbb{E}_{q(x_{t(i)}\\mid x_{0})}[\\mathrm{KL}(q(x_{s(i)}\\vert x_{t(i)},x_{0})\\Vert p_{\\theta}(x_{s(i)}\\vert x_{t(i)}))]}\\end{array}$ . For the above choices of the prior distribution, the term $\\mathrm{KL}(q(x_{1}|x_{0})||p(x_{1}))$ becomes zero. Under the reverse model (2), the KL divergence terms in $\\mathcal{L}_{T}$ becomes (proof in App. D) ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{KL}(q(x_{s}|x_{t},x_{0})\\|p_{\\theta}(x_{s}|x_{t}))=-\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\delta_{x_{t},m}\\cdot x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is a simple cross-entropy loss between the predicted logits and the clean data. In App. D, we show that $\\mathcal{L}_{T}$ is a Riemann sum and is lower bounded by the corresponding continuous integral: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}\\triangleq\\operatorname*{lim}_{T\\rightarrow\\infty}\\mathcal{L}_{T}=\\int_{t(1)}^{1}\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\mathbb{E}_{q(x_{t}|x_{0})}\\left[\\delta_{x_{t},m}\\cdot x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t)\\right]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha_{t}^{\\prime}$ denotes the derivative of $\\alpha_{t}$ with respect to $t$ . Therefore, we can obtain an ELBO that is tighter than that of any finite $T$ by pushing $T\\to\\infty$ . This ELBO can be further simplified by letting $t(1)\\to0$ . As a result, $\\mathbb{E}_{q(x_{t(1)}|x_{0})}[\\log p(x_{0}|x_{t(1)})]$ goes to 0 and the ELBO becomes $-\\mathcal{L}_{\\infty}$ . ", "page_idx": 3}, {"type": "text", "text": "For continuous state-space diffusions, the ELBO depends on the signal-to-noise ratio (SNR) at its endpoints but is otherwise invariant to the noise schedule [33]. We establish here a similar result f.f Iuns itohniss .c oCnotenxsti,d tehre  clhoog-oSsiNngR $\\alpha_{t}=\\sigma(\\lambda_{t})$ y, $\\sigma$ .o iBd yf umnacktiionng $\\begin{array}{r}{\\sigma(x)=\\frac{1}{1+e^{-x}}}\\end{array}$ $\\begin{array}{r}{\\lambda_{t}=\\log\\frac{\\alpha_{t}}{1-\\alpha_{t}}=\\log\\!-\\!\\operatorname{SNR}(t)}\\end{array}$ a change of variables in (4) to make everything a function of the log-SNR, we obtain ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=\\int_{\\lambda_{t(1)}}^{\\lambda_{1}}\\sigma(\\lambda)\\mathbb{E}_{\\tilde{q}(x_{\\lambda}\\mid x_{0})}\\left[\\delta_{x_{\\lambda},m}\\cdot x_{0}^{\\top}\\log\\tilde{\\mu}_{\\theta}(x_{\\lambda},\\lambda)\\right]\\mathrm{d}\\lambda.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\tilde{\\mu}_{\\theta}(x,\\lambda):=\\mu_{\\theta}(x,t)$ and $\\tilde{q}(x_{\\lambda}|x_{0}):=q(x_{t}|x_{0})$ for $t=\\log\\!{-}\\mathrm{SNR}^{-1}(\\lambda)$ . This shows that the only effect $\\alpha_{t}$ has on the loss is through the values of the SNR at the endpoints. Still, because we draw uniform samples of $t$ to estimate the integral, the choice of masking schedule affects the variance. ", "page_idx": 3}, {"type": "text", "text": "Multidimensional data. In the previous sections, $x_{t}$ was assumed to be a single discrete token. To extend the method to multidimensional data, let $x_{t}$ be now a sequence $(x_{t}^{(1)},x_{t}^{(2)},\\ldots,x_{t}^{(N)})$ , where each element $\\boldsymbol{x}_{t}^{(n)}$ represents a discrete token. We select a forward process which factorizes across all $N$ tokens: $\\begin{array}{r}{\\dot{q}(x_{t}|x_{s})=\\prod_{n=1}^{N}q(x_{t}^{(n)}|x_{s}^{(n)})}\\end{array}$ xt(n)|x(sn )). As a result, the forward marginals q(xt|x0) and reversal $q(x_{s}|x_{t},x_{0})$ also factorize. In this case, we define the reverse model as $p_{\\theta}(x_{s}|x_{t})\\ \\triangleq$ $\\begin{array}{r}{\\prod_{n=1}^{N}q(x_{s}^{(n)}|x_{t}^{(n)},\\mu_{\\theta}^{(n)}(x_{t},t))}\\end{array}$ , where $\\mu_{\\theta}(x_{t},t)$ is a neural network that takes the full $N$ tokens as input and outputs $N$ probability vectors.3 The $n$ -th output $\\mu_{\\theta}^{(n)}(x_{t},t)$ is a prediction model for $\\mathbb{E}[x_{0}^{(n)}|x_{t}]$ , the mean value of the $n$ -th token. Repeating above derivations gives ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}^{(N)}\\triangleq\\int_{0}^{1}\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\mathbb{E}_{q(x_{t}|x_{0})}\\Big[\\sum_{n:x_{t}^{(n)}=m}(x_{0}^{(n)})^{\\top}\\log\\mu_{\\theta}^{(n)}(x_{t},t)\\Big]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A single step of our training algorithm is described in Alg. 1 in Appendix. ", "page_idx": 3}, {"type": "text", "text": "4 Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We use ancestral sampling from our discrete-time reverse process for generation. We have found this yields slightly higher sample quality compared to other methods such as Euler discretization [29, 32]. For conditional generation tasks such as infliling, we find that the simple approach works best \u2014 we keep the conditioning tokens unmasked throughout the generation process. A complete description of the sampling algorithm can be found in Alg. 2 in Appendix. ", "page_idx": 3}, {"type": "image", "img_path": "xcqSOfHt4g/tmp/4c8ed917114cfc92971ee7cc73dbcda8fc67509ca0dae3d29fdc94bbc7fae230.jpg", "img_caption": ["Figure 2: Left: FID evaluation for 50K samples randomly generated from our model on pixel-level modeling of ImageNet $64\\!\\times\\!64$ . Right: Number of tokens revealed in each step of generation. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Impact of schedules and discretization. For comparing different sampling configurations, we primarily use the FID score [40] on image datasets as our evaluation metric. We favor it over text generative perplexity4 used in prior work [32], as the latter can be misleadingly reduced by lowering sample diversity [41]. We initially trained our model using the linear schedule, which achieves the best final ELBO overall; however, we found that sampling did not perform well with a standard uniform discretization grid $\\begin{array}{r}{t(i)=\\frac{i}{T}}\\end{array}$ . We hypothesize that time discretization can lead to confilcts by generating multiple tokens in a single step. We then switched to the cosine schedule (Tab. 4) that slows down unmasking at the beginning of reverse process. This drastically improves the FID on ImageNet $64\\!\\times\\!64$ from 70 to 17 for $T=256$ steps (Fig. 2, left). Building on this observation, we suggest using a \u201ccosine\u201d discretization grid $\\begin{array}{r}{t(i)=\\cos\\!\\big(\\frac{\\pi}{2}(1-\\frac{i}{T})\\big)}\\end{array}$ for sampling in models trained with a linear schedule. This induces the same discretization in $\\alpha_{t}$ as the cosine schedule with a uniform grid, leading to comparable sample quality, as shown in Fig. 2 (left). In Fig. 2 (right), we plot the number of tokens unmasked per step for linear and cosine schedules with a uniform grid. We believe the cosine schedule performs better because it leverages information redundancy: with more tokens revealed, the remaining tokens become more predictable, reducing confilcts when unmasking them in a single step. ", "page_idx": 4}, {"type": "text", "text": "Although these findings were originally developed on images, we find them translate well to text (see Fig. 9). we expect other techniques such as top- $\\boldsymbol{p}$ sampling [41], classifier-free guidance [42, 43], and predictor-correctors [29, 44] to further improve sample quality of our models. While we reserve these for future work, we note that the JAX [45] implementation of categorical sampling implicitly truncates small probabilities, creating a similar effect to top- $\\boldsymbol{p}$ sampling. See App. G for details. ", "page_idx": 4}, {"type": "text", "text": "5 Relation to Existing Work ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We discuss how to unify several existing masked diffusion models using our framework. ", "page_idx": 4}, {"type": "text", "text": "Continuous-Time Markov Chains (CTMC). To show the connection with the CTMC view presented in Austin et al. [14], Campbell et al. [29], we can write out the forward and reverse masked diffusion using CTMC machinery. To see this, for a short time $\\Delta t$ , given $x_{0}$ , the Taylor expansions of our forward and reverse transition matrices at $t$ are ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(t,t+\\Delta t)=I+Q(t)\\Delta t+o(\\Delta t)\\quad\\mathrm{for}\\quad Q(t)\\triangleq\\beta(t)(\\mathbf{1}e_{m}^{\\top}-I),}\\\\ &{\\bar{R}^{x_{0}}(t,t-\\Delta t)=I+R^{x_{0}}(t)\\Delta t+o(\\Delta t)\\quad\\mathrm{for}\\quad R^{x_{0}}(t)\\triangleq-\\displaystyle\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}e_{m}(x_{0}-e_{m})^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Q(t)$ and $R^{x_{0}}(t)$ are known as the transition rate matrices. Austin et al. [14] derived the same $Q(t)$ in App. A.6 of their paper. However, they did not explore the reverse process or a continuoustime objective. Campbell et al. [29] established an alternative expression for the ELBO using the rate matrices. In App. H.1, we show how to recover their expression by separating out a constant from our ELBO expression (4) and applying a discrete \u201cintegration-by-part\u201d. A key limitation of their expression is that it needs $N$ evaluations of the prediction model $\\mu_{\\theta}(\\cdot,t)$ to compute an inner summation. To circumvent this computational burden, they used a doubly stochastic estimate. However, this leads to significantly higher variance compared to the analytic cross-entropy (4) which only requires one pass of $\\mu_{\\theta}(\\cdot,t)$ . Please refer to App. H.2 for more details. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Score parameterization. While so far we used a prediction model $\\mu_{\\theta}(x_{t},t)$ for the mean of clean data given $x_{t}$ (i.e., mean parameterization), one can choose other ways of parameterizing the reverse model. Lou et al. [32], Benton et al. [35] proposed to parameterize the discrete \u201cscore\u201d $\\begin{array}{r}{s(x_{t},t)_{j}\\triangleq\\frac{q_{t}(j)}{q_{t}(x_{t})}}\\end{array}$ and introduced a score-based loss for discrete diffusions. In App. H.3, we provide an alternative derivation of their loss which is simpler. We show the link between score and mean parameterizations through the following proposition. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1 (Score Parameterization vs. Mean Parameterization). Let $q_{t}$ be the marginal distribution of the masked diffusion defined in Sec. 2 at time t. The discrete score $\\begin{array}{r}{s(x_{t},t)_{j}=\\frac{q_{t}(j)}{q_{t}(x_{t})}}\\end{array}$ for $a$ mask state $x_{t}=m$ and $j\\neq m$ can be expressed as ", "page_idx": 5}, {"type": "equation", "text": "$$\ns(m,t)_{j}=\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\mathbb{E}[x_{0}|x_{t}=m]^{\\top}e_{j},\\,w h i c h\\,s a t i s f i e s\\,\\sum_{j\\neq m}s(m,t)_{j}=\\frac{\\alpha_{t}}{1-\\alpha_{t}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proposition 1 (proved in App. H.3) implies that a reasonable score model for a mask state is ", "page_idx": 5}, {"type": "equation", "text": "$$\ns_{\\theta}(m,t)_{j}=\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\mu_{\\theta}(m,t)_{j}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Indeed, substituting (9) into the score-based loss of Lou et al. [32], Benton et al. [35] recovers our objective (4). In Lou et al. [32], the score is parameterized as a neural network without enforcing the constraint in (8). This means the learned reverse model can be incompatible with the forward process. We find that our parameterization, which enforces the constraint, leads to more stable training and better results. ", "page_idx": 5}, {"type": "text", "text": "Any-order autoregressive models. The continuous-time reverse process of our masked diffusion model can be viewed as an any-order autoregressive model (AO-ARM) [46]. To see this, we reorder the tokens according to the timing of their unmasking events in the reverse process. For all tokens, the cumulative distribution functions (CDFs) of unmasking times $\\{\\tau_{n}\\}_{n=1}^{N}$ are identical and satisfy $P(\\tau_{n}\\leq t)=P(x_{t}^{(n)}=m)=1-\\alpha_{t}$ . As a result, the ordering is uniformly random across all possible arrangements, and the token prediction during each unmasking event represents a prediction step in AO-ARMs. This connection was initially pointed out in Hoogeboom et al. [47, App. C]. The relation between our simplified ELBO (5) and the AO-ARM objective is independently clarified by Ou et al. [36]. Despite this equivalence, our work demonstrates that the masking schedule $\\alpha_{t}$ introduces a new degree of freedom in the design of such models. Variations in $\\alpha_{t}$ can lead to different distributions of unmasking times, significantly impacting performance in diffusion-style parallel sampling under time discretization, as shown in Fig. 2. ", "page_idx": 5}, {"type": "text", "text": "Other related work. Due to space constraint, we defer the discussion on other related work, including MaskGIT [39], discrete flow matching [48], SDDM [30], and SUNDAE [49], to App. H.4. ", "page_idx": 5}, {"type": "text", "text": "6 Generalization to State-dependent Masking Schedules ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We consider here state-dependent masking schedules by making the time-dependent probability of masking a token dependent also on the token value. Thus, some tokens will have higher probability to be masked earlier in time compared to others. ", "page_idx": 5}, {"type": "text", "text": "We first define the forward process for a single token $x_{t}$ . Let $\\alpha_{t}$ be a $m+1$ dimensional vector function, i.e., there is a different function $\\alpha_{t,i}$ for each possible value $i$ of the token $x_{t}$ . Also, by vector $\\frac{\\alpha_{t}}{\\alpha_{s}}$ we denote the element-wise division of the two vectors. We define the forward transition as $q(x_{t}|\\bar{x}_{s})=\\mathrm{Cat}(x_{t};\\bar{Q}(s,t)^{\\top}x_{s})$ where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bar{Q}(s,t)=\\mathrm{diag}\\Big(\\frac{\\alpha_{t}}{\\alpha_{s}}\\Big)+\\Big(I-\\mathrm{diag}\\Big(\\frac{\\alpha_{t}}{\\alpha_{s}}\\Big)\\Big)\\mathbf{1}e_{m}^{\\top}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "xcqSOfHt4g/tmp/a731458273e4c89f2c694d0198d559d0df5bd88c77159bcec21ad637adb3099f.jpg", "img_caption": ["Figure 3: Iterative unmasking process for an unconditionally generated sample by MD4. This visualization only includes a subsequence from a generated sequence of 1024 tokens. \"?\" represents masks. Masked tokens are revealed sequentially: green (steps 500-700), yellow (700-850), and red (850-1000). Additional unconditional generation from MD4 can be found in App. K.4. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "and $\\textstyle\\operatorname{diag}\\left({\\frac{\\alpha_{t}}{\\alpha_{s}}}\\right)$ is a diagonal matrix with the vector $\\frac{\\alpha_{t}}{\\alpha_{s}}$ in its diagonal. The probability of moving from current state $x_{s}$ to a future state $x_{t}$ (either the same as $x_{s}$ or mask) is determined by a state-dependent rate $\\left(\\frac{\\alpha_{t}}{\\alpha_{s}}\\right)^{\\top}x_{s}$ , while the marginal at time $s$ given $x_{0}$ is ", "page_idx": 6}, {"type": "equation", "text": "$$\nq(x_{s}|x_{0})=\\mathrm{Cat}(x_{s};\\bar{Q}(s)^{\\top}x_{0})\\quad\\mathrm{for}\\quad\\bar{Q}(s)=\\mathrm{diag}(\\alpha_{s})+(I-\\mathrm{diag}(\\alpha_{s}))\\mathbf{1}e_{m}^{\\top}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Further, for any time $0\\leq s<t\\leq1$ it holds that $\\begin{array}{r}{q(x_{t}|\\boldsymbol x_{0})=\\sum_{\\boldsymbol x_{s}}q(x_{t}|\\boldsymbol x_{s})q(x_{s}|\\boldsymbol x_{0})}\\end{array}$ so the above is a valid continuous-time Markov chain. ", "page_idx": 6}, {"type": "text", "text": "Given the forward conditionals and marginals, we can now compute the time reversal conditioned on $x_{0}$ . The full form of $q(x_{s}|x_{t},x_{0})$ is derived in App. I.1. For $x_{t}=m$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(x_{s}|x_{t}=m,x_{0})=q(x_{s}|x_{t}=m,x_{0},x_{0}x_{0}^{\\top})=\\left(\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\right)^{\\top}\\!x_{0}e_{m}^{\\top}x_{s}+\\left(\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\right)^{\\top}\\!x_{0}x_{0}^{\\top}x_{s}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This suggests that the reverse model given $x_{t}=m$ can be chosen as $p_{\\theta}(x_{s}|x_{t}=m)\\triangleq q(x_{s}|x_{t}=$ $m,\\mu_{\\theta}(x_{t},t),\\mathrm{diag}(\\mu_{\\theta}(x_{t},t)))$ where $\\mu_{\\theta}(x_{t},t)$ is a probability vector defined as in Sec. 3 that approximates $\\mathbb{E}[x_{0}|x_{t}]$ while $\\mathrm{diag}(\\mu_{\\theta}(x_{t},t))$ approximates $\\mathbb{E}[x_{0}x_{0}^{\\top}|x_{t}]=\\mathrm{diag}(\\mathbb{E}[x_{0}|x_{t}])$ . We show in App. I.1 that the negative continuous-time ELBO for the state-dependent rate case is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=\\int_{0}^{1}\\Big(\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\Big)^{\\top}\\mathbb{E}_{q(x_{t}|x_{0})}\\left[\\delta_{x_{t},m}\\cdot\\big(x_{0}-\\mu_{\\theta}(x_{t},t)+x_{0}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t)\\big)\\right]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\alpha_{t}^{\\prime}$ is the elementwise derivative of $\\alpha_{t}$ w.r.t. $t$ . This generalizes the loss from (4), which can be recovered by choosing $\\alpha_{t}$ as a scalar schedule times an all-one vector. For $N$ tokens the reverse model and the loss further generalize similarly to Sec. 3; see App. I.2. ", "page_idx": 6}, {"type": "text", "text": "To learn the token dependent masking schedule using ELBO optimization, we parametrize the $m+1$ dimensional function $\\alpha_{t}$ using the polynomial schedule (see Fig. 1) as $\\alpha_{t,i}=1-t^{w_{i}}$ and optimize each parameter $w_{i}\\,>\\,0$ .5 The value of $w_{i}$ , through the masking probability $1-\\alpha_{t,i}$ , determines how fast the token with value $i$ jumps to the mask state. Since in the loss (11) the distribution $q(x_{t}|x_{0})$ depends on $\\alpha_{t}$ and thus the vector $w$ , optimizing $w$ poses a discrete gradient estimation problem [see, e.g., 50]. Naive autodiff leads to biased gradients and pushes $w$ towards zero because the gradients cannot propagate through the (discrete) samples drawn from $q(x_{t}|x_{0})$ . To fix this, we used the REINFORCE leave-one-out estimator [51, 52] to compute low-variance unbiased gradients for optimizing $w$ . Details are given in App. I.2. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Zero-shot unconditional perplexity on five benchmark datasets from Radford et al. [55]. The numbers for other methods are from Lou et al. [32] except our reimplementation of SEDD Absorb. Our MD4 model achieves the best result on all benchmarks except LAMBADA where it is the second best. \u2217The GPT-2 numbers are reported for the GPT-2 checkpoint pretrained on WebText instead of OWT thus is not a direct comparison. ", "page_idx": 7}, {"type": "table", "img_path": "xcqSOfHt4g/tmp/09e6ec09569a4e68abcbaac80012f927a9d5fdb7423083bfb985092b41a6edab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "7.1 Text ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Text is natural discrete data with rich structures. We experiment with two datasets: text8 [53], a character-level text modeling benchmark extracted from English Wikipedia, and OpenWebText [54], an open clone of the unreleased WebText dataset used to train GPT-2 [55]. We choose the linear masking schedule described in Sec. 2 for all text experiments since we find it works best. We refer to our simple masked diffusion model as MD4 (Masked Discrete Diffusion for Discrete Data) and our generalized state-dependent model as GenMD4. ", "page_idx": 7}, {"type": "text", "text": "OpenWebText. We train a GPT2-small and GPT2-medium size of MD4 on OpenWebText $98\\%$ training, $2\\%$ validation) and evaluate zeroshot perplexity on the test splits of five benchmark datasets used in Radford et al. [55]: LAMBADA [56], WikiText2, Penn Treebank [57], WikiText103 [58], and One Billion Words [59]. We keep our evaluation setup the same as SEDD [32]. To ensure the comparison is fair, we reproduced their result in our implementation. ", "page_idx": 7}, {"type": "text", "text": "The results are shown in Tab. 1. Our small model outperforms previous best discrete diffusion models on all five tasks. We also confirmed that our re-implementation of SEDD leads to similar and slightly better results than those reported in their paper. We believe this is because ", "page_idx": 7}, {"type": "image", "img_path": "xcqSOfHt4g/tmp/74f097571f9d13ac776c74653ae293befdad60d000f65893556d7c02fe37c4e7.jpg", "img_caption": ["Figure 4: Perplexity on OpenWebText (OWT) validation set during training. The final numbers are reported in Tab. 5 in Appendix. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "they were using mixed-precision training (thus larger variances) while we are using full precision (float32) training. We are also better than GPT-2 on all tasks except LAMBADA where we are the second best among all methods. When we scale up the model size to medium, we observe that MD4 similarly beats SEDD and GPT-2 on all tasks except LAMBADA. ", "page_idx": 7}, {"type": "text", "text": "To validate that the strong zero-shot performance is a result of a better-trained model, we plot perplexity on the OpenWebText validation set in Fig. 4. It demonstrates that our model converges faster and has better final likelihoods than prior methods. We also observed that SEDD [32] has training instabilities. We believe this is due to the score parameterization breaking consistency between forward and reverse processes, as explained in Sec. 5. Although GenMD4 achieves lower perplexity than MD4, we observed that the learned ws can easily overfti to dataset statistics, making it less effective on zero-shot transfer tasks. Beyond likelihood evaluation, we also examine generation quality of our models. Fig. 3 showcases a randomly selected, notably coherent sample produced ", "page_idx": 7}, {"type": "text", "text": "Table 2: Bits Per Character (BPC) on Text8 test set. All models use standard 12-layer transformers similar to GPT2-small [55] except Discrete Flow which uses $8\\times3$ layers. ", "page_idx": 8}, {"type": "table", "img_path": "xcqSOfHt4g/tmp/e191a664405764e4df74ffaccf3f3148d06315f0ddf7d298a0fd8aa844435bd3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 3: Bits Per Dimension (BPD) on CIFAR-10 test set and Downsampled ImageNet $64\\!\\times\\!64$ [63] validation set. All models in the table are trained without data augmentation. ", "page_idx": 8}, {"type": "table", "img_path": "xcqSOfHt4g/tmp/b98481ab99e5020e522208265bd0fe12097b13fb96500aa5c91430ec5f686cc5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "by MD4-medium, alongside its iterative denoising process. Fig. 9 demonstrates MD4\u2019s ability to perform text infliling and highlights a substantial improvement in sample quality when transitioning from the linear to the cosine schedule. ", "page_idx": 8}, {"type": "text", "text": "We follow prior work to employ the GPT-2 Large model to evaluate perplexity of the generated samples. As noted in Sec. 4, the generative perplexity metric can be misleadingly decreased by reducing sample diversity; we include these results in Appendix (Fig. 7) mainly for completeness. Under this metric, MD4 significantly outperforms the GPT-2 checkpoint and SEDD. Notably, after 64 decoding steps, our small-sized MD4 matches the performance of SEDD-medium. ", "page_idx": 8}, {"type": "text", "text": "text8. Following the convention in prior work [14, 32], we experimented with training masked diffusion models on text chunks of length 256. We used the same dataset splits and transformers of the same size to parameterize the prediction model. Details can be found in App. J.1. Results are summarized in Tab. 2. We used the standard bits-per-character metric (a normalized negative log likelihood) for this dataset. We can see that our models outperform previous diffusion models, whether discrete or continuous in nature. We also outperform the best any-order autoregressive models, which do not assume a fixed generation order and has a strong connection to discrete diffusion [47]. Our model is only beaten by an autoregressive transformer and the Discrete Flow (which also used an autoregressive backbone). We believe this is because autoregressive models only require learning a fixed order of generation thus better utilize model capacity. ", "page_idx": 8}, {"type": "text", "text": "The text8 dataset has a small vocabulary that consists of 26 letters a-z and a space token. Therefore, we did not expect much flexibility introduced by our state-dependent formulation. Still, with the generalized objective in (11), GenMD4 is able to achieve significantly better BPC than MD4, showing the promise of a state-dependent diffusion on discrete data tasks. ", "page_idx": 8}, {"type": "text", "text": "7.2 Pixel-level image modeling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To demonstrate MD4 beyond text domains, we follow Austin et al. [14], Campbell et al. [29] and train MD4 on order-agnostic image data. Specifically, we take the CIFAR-10 and the Downsampled ImageNet $64\\!\\times\\!64$ [63] datasets and treat each image as a set of discrete tokens from a vocabulary of size 256 such that the model is unaware of the relative proximity between different pixel values. We compare to other discrete diffusion and autoregressive models that have reported likelihood results on the same dataset, although to our knowledge there are no published result on discrete diffusion methods for ImageNet $64\\times64$ that directly model raw pixel space. ", "page_idx": 8}, {"type": "image", "img_path": "xcqSOfHt4g/tmp/7a5db67e161e590f18d680df7f6b2c63563bc7e164c2151e6608927cbac5d4f1.jpg", "img_caption": ["Figure 5: Non cherry-picked unconditional samples from MD4 trained on ImageNet 64x64, treating pixels as discrete tokens. More samples can be found in Fig. 8 in Appendix. The model is optimized for likelihood instead of visual quality\u2014see e.g., Kingma et al. [33] for samples from a continuous diffusion model optimized similarly for likelihood. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Results are summarized in Tab. 3. We establish a new state-of-the-art for discrete diffusion models, outperforming previous discrete-time [14] and continuous-time [29] models by a significant margin. Notably, despite lacking knowledge of the ordinal structure of pixel values, MD4 still outperforms models trained with this inductive bias, including D3PM Gauss and $\\tau\\mathrm{LDR}$ [29] where the noising distribution is a discrete Gaussian distribution that gives larger probabilities to near pixel values. To isolate the differences caused by training objectives, we also implemented the objective from Campbell et al. [29] in our code. The result, labeled as $\\tau$ LDR Mask, demonstrates that the high variance of this objective negatively impacts model learning, even when using the same architecture as ours. Our CIFAR-10 model without data augmentation also achieves better likelihood than the previous best autoregressive models on this task, while on ImageNet $64\\times64$ our result is competitive with Transformer AR models which is approximately $4\\times$ larger in model size and the continuous diffusion model VDM [33]. ", "page_idx": 9}, {"type": "text", "text": "We provide a random sample from our ImageNet $64\\!\\times\\!64$ model in Fig. 5. More results can be found in in App. K. In Fig. 2, we plot the FID values of samples generated under different choices of schedules and discretization grids. We can see that the model with the linear schedule and a cosine grid achieves an FID close to the model with cosine schedule, both significantly outperform the linear schedule with a uniform grid. We further trained a class-conditional model on ImageNet $64\\!\\times\\!64$ that boosts the FID to around 7. Although these are not state-of-the-art FIDs on ImageNet $64\\!\\times\\!64$ , we emphasize our models are optimized for likelihood instead of sample quality. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we revisit masked diffusion models, focusing on a flexible continuous-time formulation. Existing works in this area are not easily accessible to non-specialists and present ELBOs that are difficult to optimize, often resulting in performance that is not competitive with continuous diffusions and autoregressive models. The framework we propose provides a very simple expression of the ELBO as a weighted integral of cross-entropy losses. Additionally, we propose a generalized masked diffusion formulation (GenMD4), where the masking schedule depends on the current state of the process, and derive its corresponding ELBO. On text data, our MD4 models outperform existing discrete and continuous diffusion models. For pixel-level image modeling, we significantly improve discrete diffusion results, outperforming similar-sized AR models and achieving comparable likelihoods to continuous diffusion models such as VDM. GenMD4 provides further improvements in terms of likelihoods over the state-independent case. ", "page_idx": 9}, {"type": "text", "text": "Although we have improved masked diffusion models, they still suffer from limitations. First, in some tasks such as text8, masked diffusions are not yet competitive with autoregressive models. We conjecture that this is because autoregressive models can better leverage model capacity since they only require learning one order. It would be interesting to develop better architectures for discrete diffusions. Moreover, GenMD4 is promising, but it can easily overfti to the dataset, making it less effective for zero-shot transfer compared to simpler versions. Additionally, inference with a state-dependent schedule is more challenging. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2015.   \n[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020.   \n[3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.   \n[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[6] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022.   \n[7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Conference on Learning Representations, 2021.   \n[8] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021.   \n[9] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In Advances in Neural Information Processing Systems, 2022.   \n[10] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2023.   \n[11] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024.   \n[12] OpenAI. Sora. https://openai.com/index/sora/, 2024.   \n[13] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: a highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024.   \n[14] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems, 2021.   \n[15] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. In Advances in Neural Information Processing Systems, 2021.   \n[16] Cl\u00e9ment Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. DiGress: Discrete denoising diffusion for graph generation. In International Conference on Learning Representations, 2023.   \n[17] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.   \n[18] Nate Gruver, Samuel Stanton, Nathan Frey, Tim GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew G Wilson. Protein design with guided discrete diffusion. In Advances in Neural Information Processing Systems, 2023.   \n[19] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022.   \n[20] Ting Chen, Ruixiang ZHANG, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. In International Conference on Learning Representations, 2022.   \n[21] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-LM improves controllable text generation. In Advances in Neural Information Processing Systems, 2022.   \n[22] Ishaan Gulrajani and Tatsunori B Hashimoto. Likelihood-based diffusion language models. In Advances in Neural Information Processing Systems, 2023.   \n[23] Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, and Kilian Q Weinberger. Latent diffusion for language generation. In Advances in Neural Information Processing Systems, 2024.   \n[24] Pierre H Richemond, Sander Dieleman, and Arnaud Doucet. Categorical SDEs with simplex diffusion. arXiv preprint arXiv:2210.14784, 2022.   \n[25] Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In International Conference on Machine Learning, 2023.   \n[26] Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian flow networks. arXiv preprint arXiv:2308.07037, 2023.   \n[27] Kaiwen Xue, Yuhao Zhou, Shen Nie, Xu Min, Xiaolu Zhang, Jun Zhou, and Chongxuan Li. Unifying Bayesian flow networks and diffusion models through stochastic differential equations. arXiv preprint arXiv:2404.15766, 2024.   \n[28] Guan-Horng Liu, Tianrong Chen, Evangelos Theodorou, and Molei Tao. Mirror diffusion models for constrained and watermarked generation. In Advances in Neural Information Processing Systems, 2024.   \n[29] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. In Advances in Neural Information Processing Systems, 2022.   \n[30] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. In International Conference on Learning Representations, 2022.   \n[31] Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. A reparameterized discrete diffusion model for text generation. arXiv preprint arXiv:2302.05737, 2023.   \n[32] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. In International Conference on Machine Learning, 2024.   \n[33] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In Advances in Neural Information Processing Systems, 2021.   \n[34] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems, 2022.   \n[35] Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. From denoising diffusions to denoising Markov models. arXiv preprint arXiv:2211.03595, 2022.   \n[36] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024.   \n[37] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024.   \n[38] Lingxiao Zhao, Xueying Ding, Lijun Yu, and Leman Akoglu. Improving and unifying discrete and continuous-time discrete denoising diffusion. arXiv preprint arXiv:2402.03701, 2024.   \n[39] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.   \n[40] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017.   \n[41] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2019.   \n[42] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[43] Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance for discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572, 2024.   \n[44] Yixiu Zhao, Jiaxin Shi, Lester Mackey, and Scott Linderman. Informed correctors for discrete diffusion models. arXiv preprint arXiv:2407.21243, 2024.   \n[45] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax.   \n[46] Benigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In International Conference on Machine Learning, pages 467\u2013475. PMLR, 2014.   \n[47] Emiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. In International Conference on Learning Representations, 2021.   \n[48] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. In International Conference on Machine Learning, 2024.   \n[49] Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. Stepunrolled denoising autoencoders for text generation. In International Conference on Learning Representations, 2022.   \n[50] Jiaxin Shi, Yuhao Zhou, Jessica Hwang, Michalis Titsias, and Lester Mackey. Gradient estimation with discrete Stein operators. In Advances in Neural Information Processing Systems, 2022.   \n[51] Tim Salimans and David A Knowles. On using control variates with stochastic approximation for variational bayes and its connection to stochastic linear regression. arXiv preprint arXiv:1401.1022, 2014.   \n[52] W. Kool, H. V. Hoof, and M. Welling. Buy 4 REINFORCE samples, get a baseline for free! In DeepRLStructPred@ICLR, 2019.   \n[53] Matt Mahoney. Text8. https://mattmahoney.net/dc/textdata.html. Accessed: 2024-05-14.   \n[54] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.   \n[55] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[56] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525\u20131534, 2016.   \n[57] Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313\u2013330, 1993.   \n[58] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2016.   \n[59] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. In Interspeech, 2014.   \n[60] Andy Shih, Dorsa Sadigh, and Stefano Ermon. Training and inference on any-order autoregressive models the right way. In Advances in Neural Information Processing Systems, 2022.   \n[61] Zachary Ziegler and Alexander Rush. Latent normalizing flows for discrete sequences. In International Conference on Machine Learning, 2019.   \n[62] Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, and Ben Poole. Discrete flows: Invertible generative models of discrete data. In Advances in Neural Information Processing Systems, 2019.   \n[63] A\u00e4ron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International Conference on Machine Learning, 2016.   \n[64] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing systems, 2016.   \n[65] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn $^{++}$ : Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations, 2016.   \n[66] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In International Conference on Machine Learning, 2018.   \n[67] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, 2018.   \n[68] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.   \n[69] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9: 53\u201368, 2021.   \n[70] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in Neural Information Processing Systems, 30, 2017.   \n[71] Kehang Han, Kathleen Kenealy, Aditya Barua, Noah Fiedel, and Noah Constant. Transfer learning for text diffusion models. arXiv preprint arXiv:2401.17181, 2024.   \n[72] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, 2015.   \n[73] Peter W. Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75\u201384, 1990.   \n[74] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229\u2013256, 1992.   \n[75] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "xcqSOfHt4g/tmp/b7c9d4a7e795b6d3e8e3257a6cf98eab19d0e34fded0da6899821962bfae1bfe.jpg", "table_caption": ["Table 4: Masking schedule formulas. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Discrete-time derivation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We divide time from 0 to 1 into $T$ intervals, and let $s(i)\\,=\\,(i\\,-\\,1)/T,\\,t(i)\\,=\\,i/T$ . The forward transition matrix $Q_{i}\\in\\mathbb{R}^{(m+1)\\times(m+1)}$ ( $m$ is vocabulary size) at time $t(i)$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n[Q_{i}]_{j k}={\\left\\{\\begin{array}{l l}{1}&{j=k=m}\\\\ {1-\\beta_{i}}&{j=k\\neq m}\\\\ {\\beta_{i}}&{k=m,j\\neq m}\\\\ {0}&{{\\mathrm{otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "or more compactly written as ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{i}=(1-\\beta_{i})I+\\beta_{i}\\mathbf{1}e_{m}^{\\top},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where 1 denotes an all-one vector of size $m+1$ , and $e_{m}$ is an one-hot vector of size $m+1$ with the $m$ -th element (recall that counting starts from 0) being one. We use an one-hot vector $x_{t}$ of length $m+1$ to denote the discrete state. The forward conditionals are defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(x_{t(i)}|x_{s(i)})=\\mathrm{Cat}(x_{t(i)};Q_{i}^{\\top}x_{s(i)})=x_{s(i)}^{\\top}Q_{i}x_{t(i)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $Q_{i}^{\\top}x_{s(i)}$ is the probabilities for each of the $m+1$ categories that $x_{t(i)}$ can take. The marginal forward distribution at time $t(i)$ given $x_{0}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\nq(x_{t(i)}|x_{0})=\\mathrm{Cat}(x_{t(i)};\\bar{Q}_{i}^{\\top}x_{0})=x_{0}^{\\top}\\bar{Q}_{i}x_{t(i)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{Q}_{i}=\\prod_{j=1}^{i}Q_{j}=\\prod_{j=1}^{i}(1-\\beta_{j})I+\\big(1-\\prod_{j=1}^{i}(1-\\beta_{j})\\big)\\mathbf{1}e_{m}^{\\top}.}\\end{array}$ To see what this leads to in continuous time, we let \u03b2i = \u03b2(tT(i)) and $T\\rightarrow\\infty$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\prod_{j=1}^{i}(1-\\beta_{j})=\\exp\\Big(\\displaystyle\\sum_{j=1}^{i}\\log(1-\\beta_{j})\\Big)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad=\\exp\\Big(\\displaystyle\\sum_{j=1}^{i}-\\frac{\\beta(t(j))}{T}+o(1/T)\\Big)}}\\\\ {{\\displaystyle\\qquad\\qquad T\\Rightarrow\\exp\\Big(-\\displaystyle\\int_{0}^{t(i)}\\beta(s)\\mathrm{d}s\\Big).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We let $\\bar{Q}(t)$ denote the limit of $\\bar{Q}_{i}$ in this case: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\bar{Q}(t)=\\exp\\big(-\\displaystyle\\int_{0}^{t}\\beta(s)\\mathrm{d}s\\big)I+\\Big(1-\\exp\\big(-\\displaystyle\\int_{0}^{t}\\beta(s)\\mathrm{d}s\\big)\\Big)\\mathbf{1}e_{m}^{\\top}}\\\\ {\\displaystyle\\triangleq\\alpha_{t}I+(1-\\alpha_{t})\\mathbf{1}e_{m}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here we define $\\begin{array}{r}{\\alpha_{t}\\triangleq\\exp(-\\int_{0}^{t}\\beta(s)\\mathrm{d}s)}\\end{array}$ . And the marginal forward transition is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(x_{t}|x_{0})=\\mathrm{Cat}(x_{t};\\bar{Q}(t)^{\\top}x_{0})=x_{0}^{\\top}\\bar{Q}(t)x_{t}=\\alpha_{t}x_{0}^{\\top}x_{t}+(1-\\alpha_{t})e_{m}^{\\top}x_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Continuous-time derivation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We consider a continuous-time Markov chain with transition rates ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ(t)=(Q_{i}-I)/(1/T)=\\beta(t)(\\mathbf{1}e_{m}^{\\top}-I).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For simplicity, we let $Q\\,=\\,{\\bf1}e_{m}^{\\top}\\mathrm{~-~}I$ . The marginal forward distribution at time $t$ given $x_{0}$ is $q(x_{t}|x_{0})=\\mathrm{Cat}(x_{t};\\bar{Q}(t)^{\\top}x_{0})$ , where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{Q}(t)=\\exp\\Big(\\int_{0}^{t}Q(s)\\mathrm{d}s\\Big)=\\exp\\Big(Q\\int_{0}^{t}\\beta(s)\\mathrm{d}s\\Big)=\\exp(\\bar{\\beta}(t)Q).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here we define $\\begin{array}{r}{\\bar{\\beta}(t)\\triangleq\\int_{0}^{t}\\beta(s)\\mathrm{d}s}\\end{array}$ . The matrix exponential can be computed via eigendecomposition: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\beta}(t)Q=U\\Lambda U^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{U=I-e_{m}e_{m}^{\\top}+\\frac{1}{\\sqrt{n+1}}\\mathbf{1}e_{m}^{\\top},}\\\\ {U^{-1}=I+\\sqrt{n+1}e_{m}e_{m}^{\\top}-\\mathbf{1}e_{m}^{\\top},}\\\\ {\\Lambda=\\bar{\\beta}(t)(e_{m}e_{m}^{\\top}-I),}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and thus $\\exp(\\Lambda)=\\alpha_{t}I+(1-\\alpha_{t})e_{m}e_{m}^{\\top}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{Q}(t)=U\\exp(\\Lambda)U^{-1}=\\alpha_{t}I+(1-\\alpha_{t})\\mathbf{1}e_{m}^{\\top}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A simpler derivation uses the following property: ", "page_idx": 15}, {"type": "equation", "text": "$$\nQ^{2}=-Q.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{Q}(t)=\\exp(\\bar{\\beta}(t)Q)}\\\\ &{\\phantom{\\bar{Q}(t)}=I+\\bar{\\beta}(t)Q+\\frac{1}{2}\\bar{\\beta}(t)^{2}Q^{2}+\\frac{1}{3}\\bar{\\beta}(t)^{3}Q^{3}+\\dots}\\\\ &{\\phantom{\\bar{Q}(t)}=I+Q-(1-\\bar{\\beta}(t)+\\frac{1}{2}\\bar{\\beta}(t)^{2}-\\frac{1}{3}\\bar{\\beta}(t)^{3}+\\dots)Q}\\\\ &{\\phantom{\\bar{Q}(t)}=I+Q-\\exp(-\\bar{\\beta}(t))Q}\\\\ &{\\phantom{\\bar{Q}(t)}=\\alpha_{t}I+(1-\\alpha_{t})\\mathbf{1}e_{m}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This marginal forward transition matrix at time $t$ coincides with the result (1) we get by taking the limit of discrete-time derivation. ", "page_idx": 15}, {"type": "text", "text": "Arbitrary discretization of the continuous-time forward process. For the discrete-time process we have defined the per-step transition in (12). For the continuous-time process, we can derive the transition matrix $\\bar{Q}(s,t)_{i j}\\triangleq q(x_{t}=j|x_{s}=i)$ between two arbitrary time $s$ and $t$ as the solution to the following differential equation (known as Kolmogorov forward equation) ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}}{\\mathrm{d}t}}{\\bar{Q}}(s,t)={\\bar{Q}}(s,t)Q(t){\\mathrm{~where~}}Q(t)=\\beta(t)Q\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with initial condition $\\bar{Q}(s,s)=I$ . The solution is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{Q}(s,t)=\\exp\\left((\\bar{\\beta}(t)-\\bar{\\beta}(s))Q\\right)=\\bar{Q}(s)^{-1}\\bar{Q}(t).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Routine work (using the Woodbury matrix inversion lemma) shows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bar{Q}(t)^{-1}=\\alpha_{t}^{-1}I+(1-\\alpha_{t}^{-1})\\mathbf{1}e_{m}^{\\top}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plugging the result back, we get the forward transition distribution from $s$ to $t$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(x_{t}|x_{s})=\\mathrm{Cat}(x_{t};\\bar{Q}(s,t)^{\\top}x_{s})=x_{s}^{\\top}\\bar{Q}(s,t)x_{t},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$\\bar{Q}(s,t)\\triangleq\\bar{Q}(s)^{-1}\\bar{Q}(t)=\\frac{\\alpha_{t}}{\\alpha_{s}}I+\\bigl(1-\\frac{\\alpha_{t}}{\\alpha_{s}}\\bigr)\\mathbf{1}e_{m}^{\\top}.$ ", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nq(x_{s}=\\cdot|x_{t}=\\cdot,x_{0})\\qquad\\qquad\\qquad q(x_{s}=\\cdot|x_{t}=\\cdot,\\mu_{\\theta}(x_{t},t))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Figure 6: The reverse transition probability and our generative model. Left: $q(x_{s}=\\cdot\\vert x_{t}=\\cdot,x_{0})$ in matrix form where first index is $x_{t}$ and second index is $x_{s}$ . Right: $p_{\\theta}(x_{s}=\\cdot|x_{t}=\\cdot)\\triangleq q(x_{s}=$ $|x_{t}=\\cdot,\\mu_{\\theta}(x_{t},t))$ also in matrix form. ", "page_idx": 16}, {"type": "text", "text": "C Time reversal of the forward process given $x_{0}$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The analytic property of our forward process allows to compute many quantities of interest in closed form. One such quantity frequently used in diffusion models is the time reversal of the forward process given $x_{0}{\\mathrm{:~}}q(x_{s}|x_{t},x_{0})$ . We can compute it using (13) and (15) as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(x_{s}|x_{t},x_{0})=\\frac{q(x_{t}|x_{s})q(x_{s}|x_{0})}{q(x_{t}|x_{0})}}\\\\ &{\\qquad\\qquad=\\left\\{\\frac{\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}x_{s}^{\\top}x_{0}}{1-\\alpha_{t}}{x_{s}^{\\top}}\\right.\\ \\ \\left.x_{s}\\neq m,x_{t}=m\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\right.\\qquad\\qquad\\left.x_{s}=m,x_{t}=m\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.x_{s}^{\\top}x_{t}\\quad\\qquad\\quad x_{t}\\neq m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Visually, eqn (16) is a $\\mathbb{R}^{(m+1)\\times(m+1)}$ matrix (Fig. 6, left) whose first index is $x_{t}$ and the second is $x_{s}$ . The matrix is almost an identity matrix except the last row corresponding to $x_{t}$ is the mask token. The last row m1ea\u2212n\u03b1s with probability of $\\frac{\\alpha_{s}\\!-\\!\\alpha_{t}}{1\\!-\\!\\alpha_{t}}$ the mask token gets unmasked to become $x_{0}$ , and with probability of $\\textstyle{\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}}$ it remains masked. ", "page_idx": 16}, {"type": "text", "text": "Alternatively, we can rewrite the above using reverse transition matrix $\\bar{R}^{x_{0}}(t,s)\\in\\mathbb{R}^{(m+1)\\times(m+1)}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(x_{s}|x_{t},x_{0})=\\mathrm{Cat}(x_{s};\\bar{R}^{x_{0}}(t,s)^{\\top}x_{t}),\\mathrm{~where~}\\bar{R}^{x_{0}}(t,s)=I+\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}e_{m}(x_{0}-e_{m})^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We are also interested in what would happen in the infinitesimal time limit, i.e., when $s=t-\\Delta t$ and $\\Delta t\\rightarrow0$ . Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\alpha_{t-\\Delta t}-\\alpha_{t}=-\\alpha_{t}^{\\prime}\\Delta t+o(\\Delta t).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Plugging it into the original formula, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{R}^{x_{0}}(t,t-\\Delta t)=I-\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}e_{m}(x_{0}-e_{m})^{\\top}\\Delta t+o(\\Delta t).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Comparing the above with the transition rate matrix $R^{x_{0}}(t)$ definition ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{R}^{x_{0}}(t,t-\\Delta t)=I+R^{x_{0}}(t)\\Delta t+o(\\Delta t),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "we have determined the transition rate matrix for the reverse process conditioned on $x_{0}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nR^{x_{0}}(t)=-\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}e_{m}(x_{0}-e_{m})^{\\top}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "D Details of the ELBO ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Using (16) and (3), we compute the KL divergences between forward and reverse transitions ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{KL}(q(x_{s}|x_{t},x_{0})\\|p_{\\theta}(x_{s}|x_{t}))=\\mathrm{KL}(q(x_{s}|x_{t},x_{0})\\|q(x_{s}|x_{t},\\mu_{\\theta}(x_{t},t)))}&{}\\\\ {=\\left\\{\\displaystyle\\sum_{0}^{m}q(x_{s}|x_{t},x_{0})\\log\\frac{q(x_{s}|x_{t},x_{0})}{q(x_{s}|x_{t},\\mu_{\\theta}(x_{t},t))}}&{x_{t}=m\\right.}\\\\ &{\\quad\\displaystyle=\\delta_{x_{t}=m}\\sum_{k\\neq m}\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}x_{0}^{\\top}e_{k}\\log\\frac{x_{0}^{\\top}e_{k}}{\\mu_{\\theta}(x_{t},t)^{\\top}e_{k}}}\\\\ &{=-\\delta_{x_{t}=m}\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $0\\log0=0$ . Alternatively, this result can be easily obtained from the visual depictions of $q(x_{s}|x_{t},x_{0})$ and $p_{\\theta}(x_{s}|x_{t})$ shown in Fig. 6. In this case, the reconstruction term becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{q(x_{t(1)}\\mid x_{0})}[\\log p(x_{0}|x_{t(1)})]=\\displaystyle\\sum_{k=0}^{m}q_{t(1)\\mid0}(k|x_{0})\\log\\frac{q_{t(1)\\mid0}(k|x_{0})}{\\sum_{j\\neq m}q_{t(1)\\mid0}(k|j)}}&{}\\\\ {=\\alpha_{t(1)}\\cdot\\log\\frac{\\alpha_{t(1)}}{\\alpha_{t(1)}}+(1-\\alpha_{t(1)})\\log\\frac{1}{m}}&{}\\\\ &{=-(1-\\alpha_{t(1)})\\log m.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The prior $\\mathrm{KL}$ term can be computed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{KL}(q(x_{1}|x_{0})\\|p(x_{1}))=\\mathrm{KL}(\\delta_{x_{1},m}\\|\\delta_{x_{1},m})=0.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As usual, we take the continuous-time limit by letting $T\\to\\infty$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\infty}\\triangleq\\underset{T\\rightarrow\\infty}{\\mathrm{lim}}\\mathcal{L}_{T}}\\\\ &{\\quad=\\underset{T\\rightarrow\\infty}{\\mathrm{lim}}\\sum_{i=2}^{T}-\\frac{\\alpha_{s(i)}-\\alpha_{t(i)}}{s(i)-t(i)}\\frac{s(i)-t(i)}{1-\\alpha_{t(i)}}x_{0}^{\\top}\\mathbb{E}_{q(x_{t(i)}\\mid x_{0})}\\left[\\delta_{x_{t(i)},m}\\log\\mu_{\\theta}(x_{t(i)},t(i))\\right]}\\\\ &{\\quad=\\int_{t(1)}^{1}\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}x_{0}^{\\top}\\mathbb{E}_{q(x_{t}\\mid x_{0})}\\left[\\delta_{x_{t},m}\\log\\mu_{\\theta}(x_{t},t)\\right]\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "E Avoiding undefined KL divergence ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "When defining the forward process, we often do not want $\\alpha_{1}$ to be exactly 0, or equivalently, $\\lambda_{1}$ to be $\\infty$ for numerical stability reasons. Instead, we set $\\lambda_{1}$ to be a finite value, and thereby $\\alpha_{1}$ has a small positive value. This has a problem that the support of $q(x_{1}|x_{0})$ is no longer $\\{m\\}$ and instead becomes $\\{m,x_{0}\\}$ . As a result, the KL divergence between $q(x_{1}|x_{0})$ and $p(x_{1})$ is undefined because $q(x_{1}|x_{0})$ is not absolutely continuous with respect to $p(x_{1})=\\delta_{x_{1},m}$ . To resolve the issue, we modify the prior distribution $p(x_{1})$ such that it has support over all $m+1$ values. One such choice is letting ", "page_idx": 17}, {"type": "equation", "text": "$$\np(x_{1})=\\frac{\\alpha_{1}}{m}\\sum_{j\\neq m}\\delta_{x_{1},j}+(1-\\alpha_{1})\\delta_{x_{1},m}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, the prior $\\mathrm{KL}$ divergence term becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{KL}(q(x_{1}|x_{0})\\|p(x_{1}))=\\displaystyle\\sum_{x_{1}=0}^{m}q(x_{1}|x_{0})\\log{\\frac{q(x_{1}|x_{0})}{p(x_{1})}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{x_{1}=0}^{m}(\\alpha_{1}\\delta_{x_{1},x_{0}}+(1-\\alpha_{1})\\delta_{x_{1},m})\\log{\\frac{\\alpha_{1}\\delta_{x_{1},x_{0}}+(1-\\alpha_{1})\\delta_{x_{1}=m}}{p(x_{1})}}}\\\\ &{\\qquad\\qquad\\qquad=\\alpha_{1}\\log{\\frac{\\alpha_{1}}{\\alpha_{1}/m}}+(1-\\alpha_{1})\\log{\\frac{1-\\alpha_{1}}{1-\\alpha_{1}}}}\\\\ &{\\qquad\\qquad\\qquad=\\alpha_{1}\\log m.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "F Details of Training and Sampling with MD4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "F.1 Training ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Algorithm 1 A single step of training with MD4. Input: data minibatch $\\{x_{t}^{i}\\}_{i=1}^{B}$ , network $\\mu_{\\theta}(\\cdot,t)$ , masking schedule $\\alpha_{t}$ for $i=1,\\ldots,B$ do (in parallel): $t_{i}\\leftarrow\\mathrm{mod}(u+i/B,1)$ , $u\\sim U[0,1]$ for $n\\in[N]$ , mask out each token xi0,(n)independently with probability 1 \u2212\u03b1ti to obtain xiti for $n\\in[N]$ , if $x_{t_{i}}^{(n)}\\!=\\!m$ , compute weighted cross entropy loss $\\begin{array}{r}{\\frac{\\alpha_{t_{i}}^{\\prime}}{1-\\alpha_{t_{i}}}(x_{0}^{i,(n)})^{\\top}\\log\\mu_{\\theta}^{(n)}(x_{t_{i}}^{i},t_{i})}\\end{array}$ Sum over all weighted cross entropy losses for mask positions and optimize via autodiff ", "page_idx": 18}, {"type": "text", "text": "F.2 Sampling ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Algorithm 2 Unconditional and conditional generation (e.g., infilling) with MD4. Input: Token sequence $x^{c}$ of length $N$ , with masks indicating the target areas for generation Init: $\\{t(i)\\}_{i=0}^{T}\\in\\dot{\\leftarrow}$ discretize $([0,\\bar{1}])$ , $x_{t(T)}\\leftarrow x^{c}$ for $i=T,T-1,\\ldots,1$ do $t\\gets t(i)$ , $s\\gets t(i-1)$ for n \u2208[N], if xt(n) $\\boldsymbol{x}_{t}^{(n)}=m$ , draw $\\begin{array}{r}{x_{s}^{(n)}\\sim\\mathrm{Cat}(\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\mu_{\\theta}^{(n)}(x_{t},t)+\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}e_{m})}\\end{array}$ else $x_{s}^{(n)}\\gets x_{t}^{(n)}$ return $x_{0}$ . ", "page_idx": 18}, {"type": "text", "text": "G JAX Categorical Sampling and Implicit Top- $\\boldsymbol{p}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We noticed that the following equivalent implementation of Alg. 2 leads to significantly worse sample quality in JAX: ", "page_idx": 18}, {"type": "text", "text": "Algorithm 3 Variant of Alg. 2 that yields lower sample quality when implemented in JAX. Input: Token sequence $x^{c}$ of length $N$ , with masks indicating the target areas for generation Init: $\\{t(i)\\}_{i=0}^{T}\\in\\dot{\\overline{{\\mathbf{\\Gamma}}}}$ discretize $([0,1])$ , $x_{t(T)}\\leftarrow x^{c}$ for $i=T,T-1,\\ldots,1$ do $t\\gets t(i)$ , $s\\gets t(i-1)$ for $n\\in[N]$ do (in parallel) draw $\\bar{u}\\sim U[0,1]$ if xt( $\\boldsymbol{x}_{t}^{(n)}=m$ and $\\begin{array}{r}{u<\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}}\\end{array}$ \u03b1s\u2212\u03b1tthen draw x(sn)\u223cCat(\u00b5(\u03b8n )(xt, t)) else $\\overline{{x_{s}^{(n)}\\leftarrow x_{t}^{(n)}}}$ return $x_{0}$ . ", "page_idx": 18}, {"type": "text", "text": "However, mathetically it is equivalent to Alg. 2 and should produce identical results. Our investigation revealed that the issue arises because Alg. 2 scales the output probabilities of $\\mu_{\\theta}$ by a small factor $\\frac{\\alpha_{s}\\!-\\!\\alpha_{t}}{1\\!-\\!\\alpha_{t}}$ as $s$ is close to $t$ , causing some categories to have very low probabilities. JAX, however, implements categorical sampling using Gumbel argmax, which is less numerically stable than methods like binary search. As a result, categories with low probabilities are rarely sampled, even when their cumulative probability is significant. In our experiment, we found that categories with probabilities below 1e-8 are rarely sampled out of a total of 50K categories. Thus, Alg. 2 implicitly performs top- $\\boldsymbol{p}$ sampling (with a dynamic p) under JAX\u2019s categorical sampling, yielding better sample quality than Alg. 3 where $\\mu_{\\theta}$ is not scaled by a small factor and has fewer categories truncated. ", "page_idx": 18}, {"type": "text", "text": "H Unifying Existing Masked Diffusion Models ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "H.1 The CTMC point of view ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first prove a lemma that connects the forward and reverse transition rate matrices. This follows from the results in [29] but we give a proof for completeness. ", "page_idx": 19}, {"type": "text", "text": "Lemma 2. The forward transition rate matrix $Q(t)$ and the reverse transition rate matrix (given $x_{0}$ ) $R^{x_{0}}(t)$ satisfy: ", "page_idx": 19}, {"type": "equation", "text": "$$\nR^{x_{0}}(t)_{k j}=Q(t)_{j k}\\frac{q_{t|0}(j|x_{0})}{q_{t|0}(k|x_{0})}\\,f\\!o r\\,j\\neq k.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof Consider the reverse transition from time $t+\\tau$ to $t$ . For $j\\neq k$ , Bayes\u2019 rule yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(x_{t}=j|x_{t+\\tau}=k,x_{0})=\\frac{q(x_{t}=j|x_{0})q(x_{t+\\tau}=k|x_{t}=j)}{q(x_{t+\\tau}=k|x_{0})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{q(x_{t}=j|x_{0})(\\delta_{j k}+Q(t)_{j k}\\tau+o(\\tau))}{q(x_{t+\\tau}=k|x_{0})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\tau\\_{\\Xi}^{\\rightarrow0}\\,\\delta_{k j}+\\frac{q(x_{t}=j|x_{0})}{q(x_{t}=k|x_{0})}Q(t)_{j k}\\tau+o(\\tau).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, it follows from the definition of the transition rate matrix that Rx0(t)kj = Q(t)jkqqtt||00((jk||xx00)). \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Proposition 3. We use the shorthand $R_{\\theta}(t)_{k j}$ to denote the approximate reverse transition rate from the state $k$ to $j$ obtained by substituting our prediction model $\\mu_{\\theta}(k)$ for $x_{0}$ in $R^{x_{0}}(t)_{k j}$ . Then, the continuous-time objective (4) can be equivalently expressed as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=-\\int_{t(1)}^{1}\\mathbb{E}_{q_{t|0}(k|x_{0})}\\Big[R_{\\theta}(t)_{k k}+\\sum_{j\\neq k}Q(t)_{k j}\\log R_{\\theta}(t)_{j k}\\Big]\\mathrm{d}t+C,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C$ is a constant independent of $\\theta$ . ", "page_idx": 19}, {"type": "text", "text": "Proof To rewrite our objective $\\mathcal{L}_{\\infty}$ with the transition rate matrices, we first go back to (18). There, instead of plugging in the explicit form of $\\bar{R}^{x_{0}}(t,s)$ , we substitute it with (7) which leverages the transition rate $R^{x_{0}}(t)$ . To simplify the notation, we assume $x_{t}\\,=\\,k$ and use the shorthand $R_{\\theta}(t)_{k j}\\triangleq R^{\\mu_{\\theta}(k)}(t)_{k j}$ . We then have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(q(x_{t-\\Delta t}|x_{t},x_{0}|)|p_{\\theta}(x_{t-\\Delta t}|x_{t}))}\\\\ &{=\\mathrm{KL}(\\mathrm{Cat}(x_{s};\\bar{R}^{x_{0}}(t,t-\\Delta t)^{\\top}e_{k})||\\mathrm{Cat}(x_{s};\\bar{R}^{\\mu_{\\theta}(k)}(t,t-\\Delta t)^{\\top}e_{k}))}\\\\ &{=\\displaystyle\\sum_{j=0}^{m}e_{k}^{\\top}(I+R^{x_{0}}(t)\\Delta t+o(\\Delta t))e_{j}\\log\\frac{e_{k}^{\\top}(I+R^{x_{0}}(t)\\Delta t+o(\\Delta t))e_{j}}{e_{k}^{\\top}(I+R_{0}(t)\\Delta t+o(\\Delta t))e_{j}}}\\\\ &{=(1+R^{x_{0}}(t)_{k k}\\Delta t)\\log\\frac{1+R^{x_{0}}(t)_{k k}\\Delta t+o(\\Delta t)}{1+R_{0}(t)_{k k}\\Delta t+o(\\Delta t)}}\\\\ &{\\quad+\\displaystyle\\sum_{j\\neq k}(R^{x_{0}}(t)_{k j}\\Delta t)\\log\\frac{R^{x_{0}}(t)_{k j}\\Delta t+o(\\Delta t)}{R_{0}(t)_{k j}\\Delta t+o(\\Delta t)}+o(\\Delta t)}\\\\ &{=(R^{x_{0}}(t)_{k k}-R_{0}(t)_{k k})\\Delta t+\\displaystyle\\sum_{j\\neq k}(R^{x_{0}}(t)_{k j}\\Delta t)\\log\\frac{R^{x_{0}}(t)_{k j}\\Delta t+o(\\Delta t)}{R_{0}(t)_{k j}\\Delta t+o(\\Delta t)}+o(\\Delta t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the last identity, we have used the fact that $\\log(1+x)=x+o(x)$ . To obtain $\\mathcal{L}_{\\infty}$ , we take the limit of $\\mathcal{L}_{T}$ as $T\\to\\infty$ , which is equivalent to letting $\\Delta t=1/T\\to0$ . We obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathcal{L}_{\\infty}=\\displaystyle\\operatorname*{lim}_{T\\to\\infty}\\sum_{i=2}^{T}\\mathbb{E}_{q(x_{t(i)}|x_{0})}[\\mathrm{KL}(q(x_{s(i)}|x_{t(i)},x_{0})||p_{\\theta}(x_{s(i)}|x_{t(i)}))]}\\\\ {\\quad=\\displaystyle\\operatorname*{lim}_{T\\to\\infty}\\sum_{i=2}^{T}\\mathbb{E}_{q(x_{t(i)}|x_{0})}\\Big[\\Big(R^{x_{0}}(t(i))_{k k}-R_{\\theta}(t(i))_{k k}}\\\\ {\\quad\\quad+\\displaystyle\\sum_{j\\neq k}R^{x_{0}}(t(i))_{k j}\\log\\frac{R^{x_{0}}(t(i))_{k j}\\Delta t+o(\\Delta t)}{R_{\\theta}(t(i))_{k j}\\Delta t+o(\\Delta t)}\\Big)\\Delta t+o(\\Delta t)\\Big]}\\\\ {\\quad=\\displaystyle\\int_{t(i)}^{1}\\mathbb{E}_{q_{t(i)}(k|x_{0})}\\Big[R^{x_{0}}(t)_{k k}-R_{\\theta}(t)_{k k}+\\displaystyle\\sum_{j\\neq k}R^{x_{0}}(t)_{k j}\\log\\frac{R^{x_{0}}(t)_{k j}}{R_{\\theta}(t)_{k j}}\\Big]\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $R^{x_{0}}(t)$ is a constant matrix independent of $\\theta$ . Absorbing all constant terms into $C$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=-\\int_{t(1)}^{1}\\mathbb{E}_{q_{t|0}(k|x_{0})}\\Big[R_{\\theta}(t)_{k k}+\\sum_{j\\neq k}R^{x_{0}}(t)_{k j}\\log R_{\\theta}(t)_{k j}\\Big]\\mathrm{d}t+C.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we subtitute $R^{x_{0}}(t)$ with the forward transition rate using Lemma 2: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\mathcal{L}_{\\infty}=-\\displaystyle\\int_{t(1)}^{1}\\mathbb{E}_{q_{t|0}(k|x_{0})}\\Big[R_{\\theta}(t)_{k k}+\\displaystyle\\sum_{j\\neq k}Q(t)_{j k}\\displaystyle\\frac{q_{t|0}(j|x_{0})}{q_{t|0}(k|x_{0})}\\log R_{\\theta}(t)_{k j}\\Big]\\mathrm{d}t+C}\\\\ {=-\\displaystyle\\int_{t(1)}^{1}\\Big[\\displaystyle\\sum_{k=0}^{m}q_{t|0}(k|x_{0})R_{\\theta}(t)_{k k}+\\displaystyle\\sum_{k=0}^{m}\\sum_{j\\neq k}Q(t)_{j k}q_{t|0}(j|x_{0})\\log R_{\\theta}(t)_{k j}\\Big]\\mathrm{d}t+C}\\\\ {=-\\displaystyle\\int_{t(1)}^{1}\\Big[\\displaystyle\\sum_{k=0}^{m}q_{t|0}(k|x_{0})R_{\\theta}(t)_{k k}+\\displaystyle\\sum_{k=0}^{m}\\sum_{j\\neq k}Q(t)_{k j}q_{t|0}(k|x_{0})\\log R_{\\theta}(t)_{j k}\\Big]\\mathrm{d}t+C,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last identity used the discrete analog to integration-by-part (or summation-by-part): $\\begin{array}{r}{\\sum_{k=0}\\sum_{j\\neq k}f(j,k)\\stackrel{!}{=}\\sum_{k=0}\\sum_{j\\neq k}f(k,j)}\\end{array}$ . Rearranging the terms then gives (20). ", "page_idx": 20}, {"type": "text", "text": "H.2 Differences from Campbell et al. [29] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Campbell et al. [29] used the first term of (20) as the training loss. A key limitation of this loss function is from the inner summation term ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{j\\neq k}Q(t)_{k j}\\log R_{\\theta}(t)_{j k}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For single dimension case, the sum is analytically computable due to the sparse structure of $R_{\\theta}(t)$ \u2014if $x_{t}\\,=\\,k$ is mask, the second term disappears; otherwise the only possible neighbor $j$ is a mask. However, for multidimensional data, $j$ will represent all $N-1$ neighbors in the forward process, i.e., the states we get from mask out a single unmasked dimension of $x_{t}=k$ . Recall that $R_{\\theta}(t)_{j k}$ is computed as substituting our neural network prediction model $\\mu_{\\theta}(j)$ for $x_{0}$ in $R^{x_{0}}(t)_{j k}$ . Therefore, the summation together with $R_{\\theta}(t)_{k k}$ requires $N$ evaluations of $\\mu_{\\theta}(\\cdot)$ . This is prohibitive since the neural network model is usually expensive. To resolve this issue, Campbell et al. [29] proposed to rewrite the sum as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{j\\sim\\tilde{q}(\\cdot\\vert k)}\\left[Z_{k}\\log R_{\\theta}(t)_{j k}\\right]\\quad\\mathrm{where}\\quad\\tilde{q}(j\\vert k)=\\frac{Q(t)_{k j}}{Z_{k}},Z_{k}\\triangleq\\sum_{j^{\\prime}\\neq k}Q(t)_{k j^{\\prime}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and estimate it through Monte Carlo. Taking into account the outer expectation under $q_{t|0}(k|x_{0})$ , the computation of the loss then becomes a doubly stochastic estimate (using $k\\sim q_{t|0}(k|x_{0})$ and $j\\sim\\tilde{q}(j|k))$ which suffers from large variance. In contrast, the form of our loss (4) only requires evaluating $\\mu_{\\theta}$ once for a single stochastic estimation of the expectation w.r.t. $q(x_{t}|x_{0})$ . ", "page_idx": 20}, {"type": "text", "text": "H.3 Score parameterization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We provide a simpler derivation of the score-based loss [32, 35] below. We start from the form of the ELBO in (20) and rewrite it as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=\\int_{t(1)}^{1}\\mathbb{E}_{q_{t\\mid0}(k\\mid x_{0})}\\Big[\\sum_{j\\neq k}\\Big(R^{\\mu_{\\theta}}(t)_{k j}-R^{x_{0}}(t)_{k j}+R^{x_{0}}(t)_{k j}\\log\\frac{R^{x_{0}}(t)_{k j}}{R^{\\mu_{\\theta}}(t)_{k j}}\\Big)\\Big]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the last identity we used the zero-row-sum property of transition rate matrix: ", "page_idx": 21}, {"type": "equation", "text": "$$\nR^{x_{0}}(t)_{k k}=-\\sum_{j\\neq k}R^{x_{0}}(t)_{k j}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "If we plug (19) into (21) and reparameterize with a score model ", "page_idx": 21}, {"type": "equation", "text": "$$\ns_{\\theta}(x_{t})_{j}\\triangleq\\frac{q_{t|0}(j|\\mu_{\\theta}(x_{t}))}{q(x_{t}|\\mu_{\\theta}(x_{t}))},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we recover the score entropy loss function from Lou et al. [32], Benton et al. [35]: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=\\int_{t(1)}^{1}\\mathbb{E}_{q_{t|0}(k|x_{0})}\\Big[\\sum_{j\\neq k}Q(t)_{j k}\\Big(s_{\\theta}(k)_{j}-\\frac{q_{t|0}(j|x_{0})}{q_{t|0}(k|x_{0})}\\log s_{\\theta}(k)_{j}+\\psi\\Big(\\frac{q_{t|0}(j|x_{0})}{q_{t|0}(k|x_{0})}\\Big)\\Big)\\Big]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\psi(y)\\,\\triangleq\\,y\\log y\\,-\\,y$ . Note that our derivation above is different and simpler than that of Campbell et al. [29] (which Lou et al. [32] is based on) since we leverage the conditional reverse transition rate given $x_{0}$ instead of the transition rate matrix of the reverse process. We can further simplify the loss with the following relationship between the conditional score and $x_{0}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{q_{t|0}(j|x_{0})}{q_{t|0}(k|x_{0})}=\\frac{x_{0}^{\\top}\\bar{Q}(t)e_{j}}{x_{0}^{\\top}\\bar{Q}(t)e_{k}}=\\frac{\\alpha_{t}}{1-\\alpha_{t}}x_{0}^{\\top}e_{j}\\;\\mathrm{for}\\;k=m,j\\neq k.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that only the result under the case $k\\ =\\ m$ is needed. This is because when $x_{t}$ is unmasked, at any time between 0 and $t$ , the state must stay unchanged and remain $x_{0}$ . As a result, $\\mathrm{KL}(q(\\dot{x}_{t-\\Delta t}|x_{t},x_{0})\\|p_{\\theta}(x_{t-\\Delta t}|x_{t}))\\;=\\;0$ for $x_{t}~\\neq~m$ . From (14), we know $Q(t)_{j k}\\;=\\;$ $\\beta(t)(\\delta_{m k}-\\delta_{j k})$ . Combining (23) and (H.3), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=\\int_{t(1)}^{1}\\beta(t)\\Big(\\mathbb{E}_{q_{t|0}(k|x_{0})}\\big[\\delta_{m k}\\big(\\sum_{j\\neq k}s_{\\theta}(k)_{j}-\\frac{\\alpha_{t}}{1-\\alpha_{t}}x_{0}^{\\top}\\log s_{\\theta}(k)\\big)\\big]+\\psi\\big(\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\big)\\Big)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Further, we can show the connection between (24) and (4) by reverting the score parameterization to a mean parameterization using (22), or equivalently $\\begin{array}{r}{s_{\\theta}(x_{t})\\dot{_{j}}=\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\overbar{\\mu}_{\\theta}(x_{t})^{\\top}e_{j}}\\end{array}$ . By doing so, we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=\\int_{t(1)}^{1}\\beta(t)\\Big(\\mathbb{E}_{q_{t|0}(k|x_{0})}\\big[\\delta_{m k}\\big(\\sum_{j\\neq k}s_{\\theta}(k)_{j}-\\frac{\\alpha_{t}}{1-\\alpha_{t}}x_{0}^{\\top}\\log\\mu_{\\theta}(k)\\big]+\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\big)\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Observing that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{j\\neq m}s_{\\theta}(m)_{j}=\\frac{\\alpha_{t}}{1-\\alpha_{t}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we conclude that this recovers the objective in (4). Interestingly, in Lou et al. [32] the score parameterization is not constrained to satisfy (25). That means the learned reverse model might be incompatible with the forward process. ", "page_idx": 21}, {"type": "text", "text": "Below, we prove Proposition 1 using the result from Eq. (23). ", "page_idx": 21}, {"type": "text", "text": "Proof of Proposition 1 ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{q_{t}(j)}{q_{t}(m)}=\\frac{\\sum_{x_{0}}q_{t|0}(j|x_{0})q(x_{0})}{q_{t}(m)}=\\frac{\\sum_{x_{0}}q_{t|0}(j|x_{0})q_{0|t}(x_{0}|m)}{q_{t|0}(m|x_{0})}=\\mathbb{E}_{x_{0}|x_{t}=m}\\left[\\frac{q_{t|0}(j|x_{0})}{q_{t|0}(m|x_{0})}\\right]}\\\\ &{\\qquad=\\mathbb{E}_{x_{0}|x_{t}=m}\\left[\\frac{\\alpha_{t}}{1-\\alpha_{t}}x_{0}^{\\top}e_{j}\\right]=\\frac{\\alpha_{t}}{1-\\alpha_{t}}\\mathbb{E}[x_{0}|x_{t}=m]^{\\top}e_{j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "H.4 Other related work. ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "MaskGIT [39]. MaskGIT is a diffusion-inspired iterative denoising model for discrete image tokens obtained through models such as VQ-VAE [70]. Training of MaskGIT follows the steps: (a) Sample $t\\in[0,1]$ . (b) Given a mask scheduling function $\\gamma(t)$ , sample $\\gamma(t)N$ tokens to place masks. (c) For data $x_{0}$ of size $(m+1)\\times N$ and the partially masked state $x_{t}$ , minimize the negative log-likelihood ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{MaskGIT}}=-\\int_{0}^{1}\\mathbb{E}_{x_{t}}\\Big[\\sum_{n:x_{t}^{(n)}=m}(x_{0}^{(n)})^{\\top}\\log\\mu_{\\theta}^{(n)}(x_{t},t)\\Big]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Our forward process satisfies $q_{t|0}(m|x_{0})=1-\\alpha_{t}$ . Therefore, when we set the mask scheduling function as $\\gamma(t)=1-\\alpha_{t}$ we obtain a loss similar to (5) without the $\\frac{\\alpha_{t}^{\\prime}}{1\\!-\\!\\alpha_{t}}$ weighting. Note that there remains a difference in the sampling distribution of $x_{t}$ : in the masked diffusion forward process, tokens are sampled independently and do not necessarily yield exactly $(1-\\alpha_{t})N$ mask tokens at time $t$ , though the expected number is $(1-\\alpha_{t})N$ . One might be interested in whether the uniform weighting can be recovered by selecting an appropriate schedule $\\alpha_{t}$ . However, solving $\\alpha_{t}$ such that $\\alpha_{t}^{\\prime}=\\alpha_{t}-1$ yields $\\alpha_{t}=c e^{t}+1$ and there is no $c$ that satisfies both $\\alpha_{0}=1$ and $\\alpha_{1}=0$ . This shows that training with the MaskGIT loss (26) may not be faithfully optimizing the model likelihood. ", "page_idx": 22}, {"type": "text", "text": "Discrete flow matching [48]. For the linear schedule $\\alpha_{t}\\,=\\,1\\,-\\,t$ , our reverse transition rate matrix (7) conditioned on $x_{0}$ is: ", "page_idx": 22}, {"type": "equation", "text": "$$\nR^{x_{0}}(t)=-\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}e_{m}(x_{0}-e_{m})^{\\top}=\\frac{1}{t}e_{m}(x_{0}-e_{m})^{\\top}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is the same as the conditional reverse transition rate used in Campbell et al. [48, Eq. (22)]\u2014note that their time $t$ is reversed, and the rate matrix was therefore in the form $\\begin{array}{r}{R^{x_{0}}(t)=\\frac{1}{1-t}e_{m}(x_{0}\\!-\\!e_{m})^{\\top}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "SDDM [30]. Sun et al. [30] proposed a pseudo-likelihood-like objective for training discrete diffusion models that can also be applied to masked diffusion. However, their objective encounters the same challenge as Campbell et al. [29] \u2014 requiring $N$ passes of the mask prediction model. To mitigate this, they introduced a new transformer architecture, which unfortunately leads to some performance degradation. ", "page_idx": 22}, {"type": "text", "text": "SUNDAE [49, 71]. Unlike masked diffusion, SUNDAE uniformly corrupts data with random tokens in the vocab (known as uniform discrete diffusion [14]). Additionally, it uses a second loss term from cross entropy between clean data and 1-step unrolled model prediction. Similar ideas have been proposed in [72]. ", "page_idx": 22}, {"type": "text", "text": "I Details for state-dependent rates ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "I.1 Derivations and time continuous limit ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "All derivations in this section assume that $x_{t}$ is a single token, while for $N$ tokens the masked diffusion with state-dependent rates factorises across the $N$ tokens. Learning from data of $N$ tokens using variational inference is discussed in App. I.2. ", "page_idx": 22}, {"type": "text", "text": "Given the forward transition $q(x_{t}|x_{s})$ and marginal $q(x_{s}|x_{0})$ derived in main text (Sec. 6) The reversal given $x_{0}$ is $q(x_{s}|x_{t},x_{0})=\\mathrm{Cat}(x_{s};\\bar{R}^{x_{0}}(t,s)^{\\top}x_{t})$ for ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\bar{R}^{x_{0}}(t,s)_{j k}=\\left\\{\\begin{array}{l l}{\\left(\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\right)^{\\top}x_{0}x_{0}^{\\top}e_{k}}&{j=m,k\\neq m}\\\\ {\\left(\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\right)^{\\top}x_{0}}&{j=m,k=m}\\\\ {\\delta_{j k}}&{j\\neq m.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "or alternatively can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q(x_{s}|x_{t},x_{0})=\\frac{q(x_{t}|x_{s})q(x_{s}|x_{0})}{q(x_{t}|x_{0})}}\\\\ &{\\qquad\\qquad=\\frac{\\left[\\frac{\\alpha_{t}^{\\top}x_{s}}{\\alpha_{s}^{\\top}x_{s}}x_{s}^{\\top}x_{t}+(1-\\frac{\\alpha_{t}^{\\top}x_{s}}{\\alpha_{s}^{\\top}x_{s}})e_{m}^{\\top}x_{t}\\right]\\;\\left[\\alpha_{s}^{\\top}x_{0}x_{0}^{\\top}x_{s}+(1-\\alpha_{s}^{\\top}x_{0})e_{m}^{\\top}x_{s}\\right]}{\\left[\\alpha_{t}^{\\top}x_{0}x_{0}^{\\top}x_{t}+(1-\\alpha_{t}^{\\top}x_{0})e_{m}^{\\top}x_{t}\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To simplify this expression we consider the two cases: either $x_{t}=m$ (i.e. $x_{t}$ is mask) or $x_{t}\\neq m$ where in the second case $x_{t}=x_{0}$ . For the case $x_{t}=m$ , the denominator in (27) simplifies as ", "page_idx": 23}, {"type": "equation", "text": "$$\nq(x_{t}=m|x_{0})=1-\\alpha_{t}^{\\top}x_{0}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "due to $x_{0}^{\\top}x_{t}=0$ since $x_{0}\\neq m$ , i.e. the observed token $x_{0}$ cannot be a mask. Then given that $x_{t}=m$ the probability that $x_{s}=x_{t}=m$ is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1-\\alpha_{s}^{\\top}x_{0}}{1-\\alpha_{t}^{\\top}x_{0}}=\\frac{(1-\\alpha_{s})^{\\top}x_{0}}{(1-\\alpha_{t})^{\\top}x_{0}}=\\left(\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\right)^{\\top}x_{0}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "while the remaining probability for $x_{s}=x_{0}\\neq m$ is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{(\\alpha_{s}-\\alpha_{t})^{\\top}x_{0}}{1-\\alpha_{t}^{\\top}x_{0}}=\\frac{(\\alpha_{s}-\\alpha_{t})^{\\top}x_{0}}{({\\bf1}-\\alpha_{t})^{\\top}x_{0}}=\\left(\\frac{\\alpha_{s}-\\alpha_{t}}{{\\bf1}-\\alpha_{t}}\\right)^{\\top}x_{0}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, combining (28) and (29) to write $q(x_{s}|x_{t}=m,x_{0})$ in an unified way yields the expression (10) in the main Sec. 6. In the second case, when $x_{t}=x_{0}\\neq m$ , $q(x_{s}|x_{t}\\neq\\dot{m},x_{0})$ from (27) simplifies dramatically and it becomes $q(x_{s}|x_{t}\\neq m,x_{0})=x_{t}^{\\top}x_{s}$ which is a point mass that sets $x_{s}=x_{t}$ . ", "page_idx": 23}, {"type": "text", "text": "Derivation of the continuous-time limit of the loss in (11). To simplify the notation, we let $\\begin{array}{r}{\\xi_{s,t}\\triangleq\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}}\\end{array}$ . We first compute the KL divergence terms in the discrete-time ELBO as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(q(x_{s}|x_{t},x_{0})||p_{\\theta}(x_{s}|x_{t}))}\\\\ &{=\\left\\{\\sum_{x_{s}=0}^{m}q(x_{s}|x_{t},x_{0})\\log\\frac{q(x_{s}|x_{t},x_{0})}{p_{\\theta}(x_{s}|x_{t})}\\quad x_{t}=m\\right.}\\\\ &{\\quad\\left.=\\delta_{x_{t},m}\\bigg[\\sum_{k\\neq m}^{\\infty}\\!\\!\\!\\!\\!\\!\\sum_{s,t}^{\\top}\\!\\!\\!x_{0}x_{0}^{\\top}e_{k}\\log\\frac{\\xi_{s,t}^{\\top}\\!\\cdot\\!x_{0}x_{0}^{\\top}e_{k}}{\\xi_{s,t}^{\\top}\\mathrm{diag}(\\mu_{\\theta}(x_{t},t))e_{k}}+(1-\\xi_{s,t})^{\\top}{x_{0}\\log\\frac{(1-\\xi_{s,t})^{\\top}{x_{0}}}{(1-\\xi_{s,t})^{\\top}\\mu_{\\theta}(x_{t},t)}}\\bigg]\\!\\!\\right\\}}\\\\ &{\\quad=\\delta_{x_{t},m}\\Big[-\\xi_{s,t}^{\\top}\\!\\!\\cdot\\!\\!x_{0}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t)+(1-\\xi_{s,t})^{\\top}{x_{0}\\log\\frac{(1-\\xi_{s,t})^{\\top}{x_{0}}}{(1-\\xi_{s,t})^{\\top}\\mu_{\\theta}(x_{t},t)}}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $\\begin{array}{r}{\\Delta_{t}\\triangleq\\frac{1}{T}=t(i)-s(i)}\\end{array}$ for all $i$ . Plugging $\\alpha_{t-\\Delta t}=\\alpha_{t}-\\alpha_{t}^{\\prime}\\Delta t+o(\\Delta t)$ into the above formula and letting $\\begin{array}{r}{\\gamma_{t}=\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}}\\end{array}$ , we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\dot{\\Omega}}(q(x_{s}|x_{t},x_{0})\\|p_{\\theta}(x_{s}|x_{t}))}\\\\ &{=\\delta_{x_{t},m}\\left[\\gamma_{t}^{\\top}x_{0}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t)\\Delta t+\\left(1+\\gamma_{t}^{\\top}x_{0}\\Delta t\\right)\\cdot\\log\\frac{1+\\gamma_{t}^{\\top}x_{0}\\Delta t+o(\\Delta t)}{1+\\gamma_{t}^{\\top}\\mu_{\\theta}(x_{t},t)\\Delta t+o(\\Delta t)}+o(\\Delta t)\\right]}\\\\ &{=\\delta_{x_{t},m}\\left[\\gamma_{t}^{\\top}x_{0}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t)\\Delta t+\\left(1+\\gamma_{t}^{\\top}x_{0}\\Delta t\\right)\\left(\\gamma_{t}^{\\top}x_{0}\\Delta t-\\gamma_{t}^{\\top}\\mu_{\\theta}(x_{t},t)\\Delta t+o(\\Delta t)\\right)+o(\\Delta t)\\right]}\\\\ &{=\\delta_{x_{t},m}\\left[\\gamma_{t}^{\\top}x_{0}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t)\\Delta t+\\gamma_{t}^{\\top}x_{0}\\Delta t-\\gamma_{t}^{\\top}\\mu_{\\theta}(x_{t},t)\\Delta t+o(\\Delta t)\\right]}\\\\ &{=\\delta_{x_{t},m}\\cdot\\gamma_{t}^{\\top}(x_{0}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t)+x_{0}-\\mu_{\\theta}(x_{t},t))\\Delta t+o(\\Delta t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{t\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\displaystyle\\sum_{i=2}^{T}\\mathbb{E}_{q(x_{t(i)}|x_{0})}[\\mathrm{KL}(q(x_{s(i)}|x_{t(i)},x_{0})\\|p_{\\theta}(x_{s(i)}|x_{t(i)}))]}\\\\ &{=\\underset{t\\rightarrow\\infty}{\\operatorname*{lim}}\\,\\displaystyle\\sum_{i=2}^{T}\\mathbb{E}_{q(x_{t(i)}|x_{0})}[\\delta_{x_{t(i)},m}\\cdot\\gamma_{t}^{\\top}(x_{0}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t(i)},t(i))+x_{0}-\\mu_{\\theta}(x_{t(i)},t(i)))\\Delta t+o(\\Delta t)]}\\\\ &{=\\displaystyle\\int_{t(1)}^{1}\\gamma_{t}^{\\top}\\mathbb{E}_{q(x_{t(i)}|x_{0})}[\\delta_{x_{t},m}\\cdot(x_{0}x_{0}^{\\top}\\log\\mu_{\\theta}(x_{t},t)+x_{0}-\\mu_{\\theta}(x_{t},t))]\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Letting $t(1)\\to0$ proves the result. ", "page_idx": 23}, {"type": "text", "text": "I.2 Training and gradient estimation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The model is applied to data consisted of $N$ tokens where $x_{0}\\,=\\,(x_{0}^{1},\\ldots,x_{0}^{(N)})$ and where each state in the masked diffusion is $x_{t}=(x_{t}^{1},\\ldots,x_{t}^{(N)})$ xt(N)). The reverse generated model has a factorizing transition conditional of the form $\\begin{array}{r}{\\prod_{n=1}^{N}p_{\\theta}(x_{s}^{(n)}|x_{t})}\\end{array}$ where $p_{\\theta}(x_{s}^{(\\bar{n})}|x_{t})=q(x_{s}^{(n)}|x_{t}^{(n)},\\mu_{\\theta}^{(n)}(x_{t},t))$ has a form that depends on whether xt( $\\boldsymbol{x}_{t}^{(n)}=m$ or $x_{t}^{(n)}\\neq m$ . For the first case: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\L_{\\theta}(x_{s}^{(n)}|x_{t}^{(n)}=m,\\{x_{t}^{(k)}\\}_{k\\neq n})=\\Big(\\frac{1-\\alpha_{s}}{1-\\alpha_{t}}\\Big)^{\\top}\\mu_{\\theta}^{(n)}(x_{t},t)e_{m}^{\\top}x_{s}^{(n)}+\\Big(\\frac{\\alpha_{s}-\\alpha_{t}}{1-\\alpha_{t}}\\Big)^{\\top}\\mathrm{diag}(\\mu_{\\theta}^{(n)}(x_{t},t))x_{s}^{(n)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\mu_{\\theta}^{(n)}(x_{t},t)=\\mathrm{softmax}(f_{\\theta}(x_{t}))$ is a $m+1$ dimensional probability vector modelled by a NN (where the final value is constrained to be zero since $\\mu_{\\theta}^{(n)}(x_{t},t)$ is a reconstruction of $x_{0}^{(n)}$ which cannot be mask, so in practice the NN classifier needs to have a softmax output only over the $m$ actual token classes). Crucially, note that the NN classifier receives as input the full state $x_{t}$ of all tokens, while additional time features to encode $t$ are also included. When $x_{t}^{(n)}\\neq m$ the reverse transition model is set to be $p_{\\theta}(x_{s}|x_{t}^{(n)}\\neq m,\\{x_{t}^{(k)}\\}_{k\\neq n})=(x_{t}^{(n)})^{\\top}x_{s}^{(n)}$ which matches precisely $q(x_{s}^{(n)}|x_{t}^{(n)}=m,x_{0}^{(n)})=(x_{t}^{(n)})^{\\top}x_{s}^{(n)}$ from the forward process. ", "page_idx": 24}, {"type": "text", "text": "The full negative lower bound for state-dependent rates and assuming $N$ tokens is given by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=\\int_{0}^{1}\\Big(\\frac{\\alpha_{t}^{\\prime}}{1-\\alpha_{t}}\\Big)^{\\top}\\mathbb{E}_{q(x_{t}|x_{0})}\\left[\\sum_{n:x_{t}^{(n)}=m}(x_{0}^{(n)}-\\mu_{\\theta}^{(n)}(x_{t},t)+x_{0}^{(n)}(x_{0}^{(n)})^{\\top}\\log\\mu_{\\theta}^{(n)}(x_{t},t))\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Given that each $\\alpha_{t,i}=1-t^{w_{i}}$ , the reverse model becomes ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\L_{\\theta}(x_{s}^{(n)}|x_{t}^{(n)}\\neq m,\\{x_{t}^{(k)}\\}_{k\\neq n})=\\left(e^{w\\log\\frac{s}{t}}\\right)^{\\top}\\mu_{\\theta}^{(n)}(x_{t},t)e_{m}^{\\top}x_{s}^{(n)}+\\left(1-e^{w\\log\\frac{s}{t}}\\right)^{\\top}\\mathrm{diag}(\\mu_{\\theta}^{(n)}(x_{t},t))x_{s}^{(n)}\\Sigma_{g}^{(n)}(x_{s},t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $w$ is the $m+1$ dimensional vector of all $w_{i}\\mathbf{s}$ . Note that the probability of $\\boldsymbol{x}_{s}^{(n)}$ staying in the mask state, i.e., $x_{s}^{(n)}=m$ depends on the full $x_{t}$ and it is given by $\\left(e^{w\\log{\\frac{s}{t}}}\\right)^{\\top}\\mu_{\\theta}^{(n)}(x_{t},t)=$ $\\begin{array}{r}{\\sum_{i=0}^{m-1}e^{w_{i}\\log\\frac{s}{t}}\\mu_{\\theta}^{(n)}(x_{t},t)_{i}}\\end{array}$ while the probability for $\\boldsymbol{x}_{s}^{(n)}$ to take a certain non-mask token value $i$ is $\\left(1-e^{w_{i}\\log{\\frac{s}{t}}}\\right)\\mu_{\\theta}^{(n)}(x_{t},t)_{i}$ . The gradient wrt $t$ is $\\alpha_{t,i}^{\\prime}=-w_{i}t^{w_{i}-1}$ and $\\begin{array}{r}{\\frac{\\alpha_{t,i}^{\\prime}}{1-\\alpha_{t,i}}=-\\frac{w_{i}}{t}}\\end{array}$ the above loss is written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\infty}=-\\int_{0}^{1}\\frac{1}{t}w^{\\top}\\mathbb{E}_{q(x_{t}|x_{0})}\\left[\\sum_{n:x_{t}^{(n)}=m}(x_{0}^{(n)}-\\mu_{\\theta}^{(n)}(x_{t},t)+x_{0}^{(n)}(x_{0}^{(n)})^{\\top}\\log\\mu_{\\theta}^{(n)}(x_{t},t))\\right]\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $w$ is the vector of all $w_{i}$ \u2019s. An unbiased gradient over the NN parameters $\\theta$ is straightforward to obtain since we just need to sample one time point $t$ and an $x_{t}\\sim q(x_{t}|x_{0})$ to approximate the integral and expectation and then use the gradient: ", "page_idx": 24}, {"type": "equation", "text": "$$\n-\\nabla_{\\theta}\\sum_{n:x_{t}^{(n)}=m}\\frac{1}{t}w^{\\top}\\left(x_{0}^{(n)}-\\mu_{\\theta}^{(n)}(x_{t},t)+x_{0}^{(n)}(x_{0}^{(n)})^{\\top}\\log\\mu_{\\theta}^{(n)}(x_{t},t)\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The gradient wrt the $w$ parameters is more complex since these parameters appear also in the discrete distribution $q(x_{t}|x_{0})$ which is not reparametrizable. To deal with this we need REINFORCE unbiased gradients [73, 74], and in our implementation we consider REINFORCE leave-one-out (RLOO) [51, 52] with two samples. Firstly, the exact gradient wrt $w$ of the exact loss is written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n-\\int_{0}^{1}\\frac{1}{t}\\mathbb{E}_{q(x_{t}|x_{0})}\\left[g(x_{t},x_{0})\\right]\\mathrm{d}t-\\int_{0}^{1}\\frac{1}{t}\\mathbb{E}_{q(x_{t}|x_{0})}\\left[f(x_{t},x_{0})\\nabla_{w}\\log q(x_{t}|x_{0})\\right]\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\nt(x_{t},x_{0})=\\sum_{n:x_{t}^{(n)}=m}(x_{0}^{(n)}-\\mu_{\\theta}^{(n)}(x_{t},t)+x_{0}^{(n)}(x_{0}^{(n)})^{\\top}\\log\\mu_{\\theta}^{(n)}(x_{t},t)),\\ \\ f(x_{t},x_{0})=w^{\\top}g(x_{t},x_{0}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $g(x_{t},x_{0})$ is a vector while $f(x_{t},x_{0})$ is a scalar. The left term in (30) is easy since it just requires sampling $t$ and $\\boldsymbol x_{t}\\sim q(\\boldsymbol x_{t}|\\boldsymbol x_{0})$ , while the right term is the REINFORCE term which could have high variance. For this second term we use RLOO with two samples $x_{t}^{1},x_{t}^{2}$ and construct the unbiased estimate ", "page_idx": 25}, {"type": "equation", "text": "$$\n-\\frac{1}{2t}\\left(\\nabla_{w}\\log q(x_{t}^{1}|x_{0})-\\nabla_{w}\\log q(x_{t}^{2}|x_{0})\\right)\\left[f(x_{t}^{1},x_{0})-f(x_{t}^{2},x_{0})\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, the overall unbiased gradient for $w$ we use is ", "page_idx": 25}, {"type": "equation", "text": "$$\n-\\frac{1}{2t}\\left\\{g(x_{t}^{1},x_{0})+g(x_{t}^{2},x_{0})+\\left(\\nabla_{w}\\log q(x_{t}^{1}|x_{0})-\\nabla_{w}\\log q(x_{t}^{2}|x_{0})\\right)\\left[f(x_{t}^{1},x_{0})-f(x_{t}^{2},x_{0})\\right]\\right\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "J Experimental Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In all experiments, the model is trained with a continuous-time loss while samples are drawn from the discrete-time reverse model of 1000 timesteps unless otherwise noted. We used an exponential moving average factor 0.9999 for all evaluation including sample generation. ", "page_idx": 25}, {"type": "text", "text": "J.1 text8 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We followed the standard dataset split as in Austin et al. [14], Lou et al. [32] and trained our models on text chunks of length 256 for 1 million steps with batch size 512. All models in the table used a standard 12-layer transformer architecture unless otherwise noted. Our transformer has also the same number of heads (12) and hidden dimension (784) as in Austin et al. [14], Lou et al. [32]. ", "page_idx": 25}, {"type": "text", "text": "We used the continuous-time ELBO and drew one sample of $t$ for each data to estimate the integral. To reduce the variance of training, we used the same antithetic sampling trick described in Kingma et al. [33] for continuous diffusion models. We used the linear masking schedule $\\alpha_{t}=1-t$ and added a small shift $\\epsilon=10^{-4}$ when $t$ is close to 0 and 1 to ensure numerical stability. The shifted schedule is $\\alpha_{t}=(1-2\\epsilon)(1-t)+\\epsilon$ . The shift leads to a support mismatch between $q(x_{1}|x_{0})$ and the prior $p(x_{1})$ , leading to an undefined KL divergence term. We explain in app. E how to modify the prior distribution to allow small uniform probabilities in non-mask states to mitigate this problem. The shift leads to a non-zero reconstruction term and KL divergence term for the prior distribution but both are of negligible scale so we can safely ignore them when reporting the ELBO. ", "page_idx": 25}, {"type": "text", "text": "We used a cosine learning rate schedule with a linear warm up of 2000 steps. We applied channel-wise dropout of rate 0.05 and used AdamW optimizer with learning rate 0.0003 and a weight decay factor of 0.03. Our model is trained on 16 TPU-v5 lite for less than a day. ", "page_idx": 25}, {"type": "text", "text": "J.2 OpenWebText ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We kept $2\\%$ of the original training set for validation. Our small and medium transformer model have the same number of layers, heads, and hidden dimensions as in Lou et al. [32] and our tokenizer was also kept the same with a vocabulary size of around 50K. The training objective, masking schedule and other architectural choices were kept the same with the text8 experiment. We kept the training hyperparameters the same as text8 experiment except that we reduced the dropout rate to 0.02. ", "page_idx": 25}, {"type": "text", "text": "J.3 Images ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We used the same linear masking schedule as in previous experiments in all likelihood results. We used the same U-Net plus self-attention architectures from the continuous diffusion model described in Kingma et al. [33] for CIFAR-10, except that we did not use Fourier feature inputs and added an additional input embedding layer with embedding size the same as the hidden dimension of the model. For ImageNet $64\\times64$ , we reduced the number of residual blocks (in one side of the U-Net structure) from 64 to 48 and added a 12-layer diffusion transformer [75] with 768 hidden dimension and 12 heads in the middle. ", "page_idx": 25}, {"type": "text", "text": "For both datasets we used AdamW optimizer and trained for 2M iterations. We used learning rate 0.0004, batch size 256, weight decay factor 0.01 for CIFAR-10 and learning rate 0.0002, batch size 512, weight decay factor 0.03 for ImageNet $64\\!\\times\\!64$ . The learning rate follows a cosine annealing after 100 warm up steps. Our CIFAR-10 model is trained on 32 TPU-v5 lite for 24 hours. Our ImageNet- $64\\times64$ model is trained on 256 TPU-v5 lite for 3.5 days. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "We have observed that the cosine schedule leads to better sample quality so we used it to train a cheaper model for sample visualization. This model differs from the one that achieves best likelihood in that we used 8 residual blocks (in one side of the UNet structure) and a 20-layer diffusion transformer in the middle. All other configurations are kept the same. ", "page_idx": 26}, {"type": "text", "text": "K Additional Results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "K.1 Sample quality evaluation by GPT-2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We use the GPT-2 large model to evaluate the perplexity of samples generated by our model, following Lou et al. [32]. Results are shown in Fig. 7. ", "page_idx": 26}, {"type": "image", "img_path": "xcqSOfHt4g/tmp/d3bedbf7064a1a398a90eeabe564e16f95c6ec710871030e62643a6427681604.jpg", "img_caption": ["Figure 7: Generative perplexity evaluated by GPT-2 Large following Lou et al. [32]. We compare MD4 against the GPT-2 checkpoint (autoregressive baseline) and SEDD (the previous best discrete diffusion model on this task) in generating 1024-token text sequences. We investigate the effects of two orthogonal factors on sample quality: model size and decoding steps. The numbers for GPT-2 and SEDD are from Lou et al. [32]. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "K.2 Perplexity on OpenWebText validation set ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Tab. 5 reports the final perplexity number achieved on OpenWebText validation set, corresponding to Fig. 4. ", "page_idx": 26}, {"type": "table", "img_path": "xcqSOfHt4g/tmp/2412eed1330585f565ecc9800f54e4f8e4a02ac51494c37849f7161a440b535b.jpg", "table_caption": ["Table 5: Perplexity on OpenWebText validation set. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "K.3 Additional unconditional generation from MD4 trained on ImageNet ${\\bf64\\!\\times\\!64}$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide more unconditional generation results from our pixel-level modeling experiments on ImageNet $64\\!\\times\\!64$ in Fig. 8. ", "page_idx": 26}, {"type": "image", "img_path": "xcqSOfHt4g/tmp/05e08ecd82c3b8d6959d3e21a2b97679c5ce6c287b277bedb4a6746c155a38cd.jpg", "img_caption": ["Figure 8: More unconditional samples from MD4 trained on ImageNet $64\\!\\times\\!64$ . "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "K.4 Additional unconditional generation from MD4 trained on OpenWebText ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Below we include two unconditioned text samples generated by our MD4 Medium model trained on OpenWebText. ", "page_idx": 27}, {"type": "text", "text": "K.4.1 MD4-M unconditional sample 1: 1024 tokens ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "like, I don\u2019t have to be alive? Sometimes there are things that are too real and you\u2019re really supposed to experience them. So that\u2019s a good feeling. ", "page_idx": 27}, {"type": "text", "text": "That is the scary thing. Not actually, being able to experience things, being able to do these things, when you\u2019re doing them, which, for most people having to wake in a dream is something that seems the most significant, and then you think about it the next day. It\u2019s like the hope of the future, and you wake up right now thinking about it. What happens is,, then you have to stop and think about it and then all of a sudden, somebody always says, \"You\u2019re dreaming.\" ", "page_idx": 28}, {"type": "text", "text": "And sometimes I wonder if this is a good time to teach your gut instincts to your actors when you\u2019re doing a show like this. Because even on this particular show, it feels like everyone\u2019s been through this all the time before, if even a few years ago. I mean, if you\u2019re doing a show together, at least not on continuous development, you you\u2019re a vet. I mean, you should really be along. If you\u2019re not sure, well -- ", "page_idx": 28}, {"type": "text", "text": "VS: I\u2019m working on that one. ", "page_idx": 28}, {"type": "text", "text": "Did any of you guys feel that an instinct could work? I thought, \"Well, because you didn\u2019t do \u2019Deadwood\u2019 you should stop doing this.\" But when I read the story for the first time, I thought, \"I think this is going to work.\" What I can\u2019t picture is a way to hold this apart. ", "page_idx": 28}, {"type": "text", "text": "VS: That\u2019s me. It\u2019s what we have to do. So do we. When we wrote the first episode, we wrote a script that we felt like me and myself would want to see. I knew that I wanted to be able to be in something -- and I wanted to be able to take refuge in something that was real, that you could see and just really step out of yourself. And then I saw it. Then, you get rehearsing it and doing it. And then I actually started shooting. I think I knew I didn\u2019t think it was going to be good. But, I know it was good. And now people are talked about because it\u2019s not good enough. ", "page_idx": 28}, {"type": "text", "text": "Growing up, you say that you just completely hated the show, \"Lost.\" Isn\u2019t that what you wish for at the end of the day? ", "page_idx": 28}, {"type": "text", "text": "VS: I don\u2019t like the concept. ", "page_idx": 28}, {"type": "text", "text": "And so there\u2019s a lot that you don\u2019t know about that, so I think for me to have had these ideas, if you didn\u2019t understand even that it was coming out of this world that doesn\u2019t exist, we might never get together. ", "page_idx": 28}, {"type": "text", "text": "It\u2019s so weird. This happened to happen at the same time? ", "page_idx": 28}, {"type": "text", "text": "VS: Yes. It happened to happen at basically the same time. ", "page_idx": 28}, {"type": "text", "text": "Nobody\u2019s even had a show or had a movie/come out of the movie, but ... ", "page_idx": 28}, {"type": "text", "text": "VS: If I\u2019m going to pretend $\\mathtt{I}\\,^{\\bullet}\\mathtt{m}$ definitely not you and have to live through that stuff, I don\u2019t think $\\mathtt{I}\\,^{\\bullet}\\mathtt{m}$ going to swallow that. I didn\u2019t expect it to do quite that long. ", "page_idx": 28}, {"type": "text", "text": "There are always things now that happen with \u2019Deadwood\u2019 where you don\u2019t know where it\u2019s going to end up next time, but I think there are occasions now where we have to keep the fight, even if \u2019Lost\u2019 was pretty consistent in the mindset and the form. ", "page_idx": 28}, {"type": "text", "text": "VS: I\u2019m glad that we did fight the odds, because we should have understood that there was a direct link. But there was almost a sense of not that we had showed up on the same day, we know we work in the same pieces, but a lot of stuff we don\u2019t know about. Some of it, we need to deal with. We also just have to accept the language, and there are a lot of things where we take from them and we do this what they did because we want to ", "page_idx": 28}, {"type": "text", "text": "K.4.2 MD4-M unconditional sample 2: 1024 tokens ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "the groups let recreational vehicles use the three roads that will stay open in the meantime of fighting off the permit. \"The purpose of the permit is to make sure that we work with the NPS and made roadways and rest areas. We\u2019re not just scaring guys kind of messing around.\" Community plans to build an urban bike facility marched forward at the ongoing staff meeting of the King County Commission. ", "page_idx": 29}, {"type": "text", "text": "Trail will be finished just south of the Greenview 5. ", "page_idx": 29}, {"type": "text", "text": "Instead of continuing with a pedestrian and bike trail to the MBTA\u2019s campus, these two trails could bridle the areas from Market to 14 and carry communities closer. ", "page_idx": 29}, {"type": "text", "text": "\"This project will provide a car-free path to King County,\" said Andrew Weed. It\u2019s been put the brakes on in the past several months, but there are those residents still skeptical. ", "page_idx": 29}, {"type": "text", "text": "\"I\u2019ve addressed some of the community concerns that\u2019ve been raised. They\u2019ve expressed some of their concerns. I don\u2019t think it\u2019s terribly reasonable from a transportation standpoint.\" ", "page_idx": 29}, {"type": "text", "text": "The trail had been set up to meet on for more than a year when the council approved funding for a different proposal. ", "page_idx": 29}, {"type": "text", "text": "Mayor Muriel Bowser said after meetings with Commissioner Bushell on Thursday that the new plan will be on board in December. ", "page_idx": 29}, {"type": "text", "text": "\"There\u2019s enough of a finish for this project to roll out on time, and we\u2019re going to get it done,\" Bowser said. ", "page_idx": 29}, {"type": "text", "text": "For the public, the campaign appears over. ", "page_idx": 29}, {"type": "text", "text": "\u201cThere was one meeting that I feel like I lost at last night\u2019s meeting,\" said Shelley Potts, a local resident. ", "page_idx": 29}, {"type": "text", "text": "Local resident Joel Grimy, who lives on Uman Road, met residents there as well. ", "page_idx": 29}, {"type": "text", "text": "And in other groups that rode through Mayor assistant Stacey Land and even her son held fliers saying to look for light sign, and also met with Bowser\u2019s son, Deion Bowser, about a future plan to also have a dog park on the transit corridor. ", "page_idx": 29}, {"type": "text", "text": "Advocates at Brickley\u2019s event, many one waited at least 11 minutes in during the start of the public meeting, said they expect at least another month from the Board of Commissioners, even after a public hearing on Nov. 13. ", "page_idx": 29}, {"type": "text", "text": "\"We\u2019ve been trying to be a talkative board where we are meeting in advance, being respectful of folks,\" Bowser said. ", "page_idx": 29}, {"type": "text", "text": "He considered that the proposal for the section of trail between the Greenview 5 and 3 \u201chas to move on a schedule. We have other historic preservation projects that would take over that.\u201d ", "page_idx": 29}, {"type": "text", "text": "But Chad Routledge, a local advocate of the project, spoke out against the mayor\u2019s plan. ", "page_idx": 29}, {"type": "text", "text": "\u201cThe mayor has sent a new meeting to the public using the same route that resulted from the loud criticism and onslaught of complaints from the community committee back during the public hearing,\u201d Routledge said. ", "page_idx": 29}, {"type": "text", "text": "The BDC doesn\u2019t have a particular plan-turns around for the end of the planned path, and says \u201cnothing practical can happen right now.\u201d But, she said the agency still \"looking to make investments in facilities along the route.\" ", "page_idx": 30}, {"type": "text", "text": "And still there is another part of the trail that might be just as much a wish for the dogs, as cars: the district wants to go west foot a couple blocks south, to make the trail safer for dogs. ", "page_idx": 30}, {"type": "text", "text": "\u201cI feel that the accessibility of the trail is pretty important. I think the education of the trail, and the uses along different routes are very important pieces of a balanced outcome,\u201d said Bushell. ", "page_idx": 30}, {"type": "text", "text": "Trams coming off Route 1 ", "page_idx": 30}, {"type": "text", "text": "K.5 Conditional generation from MD4 trained on OpenWebText ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We share conditionally generated text samples by MD4 Medium in Fig. 9 and observe that slow unmasking near $t\\,=\\,1$ , enabled by the cosine schedule, tends to help produce more consist and meaningful samples than uniform unmasking counterpart. ", "page_idx": 30}, {"type": "image", "img_path": "xcqSOfHt4g/tmp/c5aba3ca054f1a595c9fdf093061277bab1bb4a9350f62d15b26c258e20893fa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 9: Conditionally generated text samples from MD4-M. Top: MD4-M trained with linear masking schedule; Bottom: MD4-M trained with cosine masking schedule. Context text shown in blue, model-generated text in black. ", "page_idx": 30}, {"type": "text", "text": "K.6 Effect of discretization on zero-shot perplexity ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We carried out ablation study on the effect of discretization on zero-shot perplexity. Results are included in Tab. 6. Note that this is an inference ablation with the same trained model (MD4-S trained with the continuou-time objective). ", "page_idx": 30}, {"type": "table", "img_path": "xcqSOfHt4g/tmp/3974a70a87fc7515a7aaf35ab84db96b901c90181c6c2a5e1bd4da6884c20fc3.jpg", "table_caption": ["Table 6: Effect of discretization on zero-shot perplexity. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our main theoretical and experimental contributions are claimed in the abstract and demonstrated in the paper. They reflect the paper\u2019s contributions and scope. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The limitations of our work are detailed in the very last paragraph of the paper (see Section 7). ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 32}, {"type": "text", "text": "Justification: All the theoretical results are proven in the supplementary material. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We included all experimental details in App. J. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 32}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: The datasets we used are all public datasets. We have a plan to release code. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Included in App. J. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [No] ", "page_idx": 33}, {"type": "text", "text": "Justification: We follow the practice in prior work [33] to not include error bars, partly because the models are expensive to train. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Included in App. J. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 34}, {"type": "text", "text": "Justification: After careful review of the NeurIPS Code of Ethics, it is clear that the research presented in this paper conforms with the Code of Ethics in every respect. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper is mostly theoretical and methodological. We do not see immediate societal impact of this work. However, we acknowledge that large scale implementation of our algorithm might suffer from the same societal biases as any other generative models. ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We cited the dataset sources. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] . ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 36}]