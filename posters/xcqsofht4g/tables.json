[{"figure_path": "xcqSOfHt4g/tables/tables_7_1.jpg", "caption": "Table 1: Zero-shot unconditional perplexity on five benchmark datasets from Radford et al. [55]. The numbers for other methods are from Lou et al. [32] except our reimplementation of SEDD Absorb. Our MD4 model achieves the best result on all benchmarks except LAMBADA where it is the second best. *The GPT-2 numbers are reported for the GPT-2 checkpoint pretrained on WebText instead of OWT thus is not a direct comparison.", "description": "This table presents the zero-shot unconditional perplexity results on five benchmark datasets (LAMBADA, WikiText2, PTB, WikiText103, and IBW) for several methods, including GPT-2, D3PM, Plaid, SEDD Absorb, and the authors' proposed MD4 model.  The results are broken down by model size (small and medium). The table highlights that MD4 achieves superior performance compared to other methods on most datasets, showcasing its effectiveness in zero-shot text generation.", "section": "7 Experiments"}, {"figure_path": "xcqSOfHt4g/tables/tables_8_1.jpg", "caption": "Table 2: Bits Per Character (BPC) on Text8 test set. All models use standard 12-layer transformers similar to GPT2-small [55] except Discrete Flow which uses 8 \u00d7 3 layers.", "description": "This table presents the Bits Per Character (BPC) results for various models on the Text8 test dataset.  The models are categorized into Continuous Diffusion, Any-order Autoregressive, Autoregressive, and Discrete Diffusion.  The table compares different approaches to text modeling in terms of their performance in terms of bits per character.  Lower BPC indicates better performance.", "section": "7.1 Text"}, {"figure_path": "xcqSOfHt4g/tables/tables_8_2.jpg", "caption": "Table 3: Bits Per Dimension (BPD) on CIFAR-10 test set and Downsampled ImageNet 64\u00d764 [63] validation set. All models in the table are trained without data augmentation.", "description": "This table compares the performance of various autoregressive and diffusion models on image generation tasks.  The metrics used is bits per dimension (BPD), a measure of how well the model compresses the image data. Lower BPD values indicate better performance.  The table is divided into autoregressive and discrete diffusion models, and further subdivided into CIFAR-10 and ImageNet 64x64 datasets, showcasing the model performance at different scales and complexities.", "section": "7.2 Pixel-level image modeling"}, {"figure_path": "xcqSOfHt4g/tables/tables_14_1.jpg", "caption": "Table 1: Zero-shot unconditional perplexity on five benchmark datasets from Radford et al. [55]. The numbers for other methods are from Lou et al. [32] except our reimplementation of SEDD Absorb. Our MD4 model achieves the best result on all benchmarks except LAMBADA where it is the second best. *The GPT-2 numbers are reported for the GPT-2 checkpoint pretrained on WebText instead of OWT thus is not a direct comparison.", "description": "This table presents the zero-shot unconditional perplexity results on five benchmark datasets (LAMBADA, WikiText2, PTB, WikiText103, and IBW) for several language models.  The models compared include GPT-2, D3PM, Plaid, SEDD Absorb (both the original and a re-implementation), and the authors' MD4 model.  The table highlights the superior performance of the MD4 model, achieving the best perplexity scores on four out of five datasets and second-best on the remaining one.  The table also notes a difference in the GPT-2 results due to the use of different training datasets.", "section": "7 Experiments"}, {"figure_path": "xcqSOfHt4g/tables/tables_26_1.jpg", "caption": "Table 5: Perplexity on OpenWebText validation set.", "description": "This table presents the perplexity scores achieved on the OpenWebText validation set for different models. The perplexity metric measures how well a language model predicts a sequence of words. Lower perplexity indicates better performance. The table shows the results for different model sizes (small and medium) and methods, including Gaussian diffusion models, SEDD Absorb (reimplementation), MD4 (the proposed model), and GenMD4 (a generalized version of MD4).", "section": "7 Experiments"}, {"figure_path": "xcqSOfHt4g/tables/tables_31_1.jpg", "caption": "Table 6: Effect of discretization on zero-shot perplexity.", "description": "This table presents an ablation study on the impact of discretization on the zero-shot perplexity of the MD4 model. It compares the perplexity scores obtained using different numbers of timesteps (T) for the model's reverse diffusion process on various text datasets. The continuous-time limit (T = \u221e) is also included as a reference point. This helps to understand how the discretization level in the sampling process affects the model's performance.", "section": "K.6 Effect of discretization on zero-shot perplexity"}]