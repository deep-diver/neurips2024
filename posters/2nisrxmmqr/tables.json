[{"figure_path": "2nisrxMMQR/tables/tables_6_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 5-way 1-shot cross-domain FSL. Average classification accuracies (%) are provided. \u2020 stands for exploiting the full data of FSL task. * means that the feature embedding network needs to be fine-tuned (Ft) on each target domain tasks. The best results are in bold.", "description": "This table compares the proposed method's performance with several state-of-the-art methods on a 5-way 1-shot cross-domain few-shot learning (FSL) task.  It shows the average classification accuracy across multiple target domains (CUB, Cars, Places, Plantae, ChestX, ISIC, EuroSAT, CropDisease).  The table indicates whether methods fine-tune their feature embedding networks on each target domain and whether they use the full dataset for training.  The best performance for each domain is highlighted in bold.", "section": "3.2 Comparison with state-of-the-art methods"}, {"figure_path": "2nisrxMMQR/tables/tables_7_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 5-way 1-shot cross-domain FSL. Average classification accuracies (%) are provided. \u2020 stands for exploiting the full data of FSL task. * means that the feature embedding network needs to be fine-tuned (Ft) on each target domain tasks. The best results are in bold.", "description": "This table compares the proposed method's performance with several state-of-the-art methods on cross-domain few-shot learning (CD-FSL) tasks.  It shows the average classification accuracy across eight different target domains (CUB, Cars, Places, Plantae, ChestX, ISIC, EuroSAT, CropDisease) for a 5-way 1-shot setting (5 classes, 1 training example per class, multiple testing examples).  The table notes whether methods fine-tune their feature embedding network on each target domain and indicates which methods use the full dataset for training.", "section": "3.2 Comparison with state-of-the-art methods"}, {"figure_path": "2nisrxMMQR/tables/tables_8_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 5-way 1-shot cross-domain FSL. Average classification accuracies (%) are provided. \u2020 stands for exploiting the full data of FSL task. * means that the feature embedding network needs to be fine-tuned (Ft) on each target domain tasks. The best results are in bold.", "description": "This table compares the proposed method's performance with existing state-of-the-art methods on a 5-way 1-shot cross-domain few-shot learning (FSL) task.  It shows the average classification accuracy across multiple target datasets (CUB, Cars, Places, Plantae, ChestX, ISIC, EuroSAT, CropDisease). The table highlights the proposed method's superior performance, even without fine-tuning the feature embedding network on each target domain, compared to other methods that either use fine-tuning or access to full task data.  The best results for each target dataset are marked in bold.", "section": "3.2 Comparison with state-of-the-art methods"}, {"figure_path": "2nisrxMMQR/tables/tables_14_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 5-way 1-shot cross-domain FSL. Average classification accuracies (%) are provided. \u2020 stands for exploiting the full data of FSL task. * means that the feature embedding network needs to be fine-tuned (Ft) on each target domain tasks. The best results are in bold.", "description": "This table compares the proposed method with other state-of-the-art methods on 5-way 1-shot cross-domain few-shot learning (FSL) tasks.  It shows the average classification accuracy across various target domains (CUB, Cars, Places, Plantae, ChestX, ISIC, EuroSAT, CropDisease). The table indicates whether methods used fine-tuning (*),  exploited full FSL task data(\u2020), and highlights the best performing methods in bold. The results demonstrate the effectiveness of the proposed method compared to existing approaches.", "section": "3.2 Comparison with state-of-the-art methods"}, {"figure_path": "2nisrxMMQR/tables/tables_16_1.jpg", "caption": "Table 5: Comparison with other data augmentation methods and self-supervised learning methods. Average classification accuracies (%) are provided. The best results are in bold.", "description": "This table compares the proposed method's performance with other data augmentation techniques (Rotation augmentation) and self-supervised learning approaches (SimCLR, BYOL) on five target domains (CUB, Places, Plantae, CropDisease).  The results are presented as average classification accuracies for 1-shot and 5-shot settings, highlighting the superior performance of the proposed method.", "section": "Further analysis"}, {"figure_path": "2nisrxMMQR/tables/tables_16_2.jpg", "caption": "Table 2: Comparison with state-of-the-art methods on 5-way 5-shot cross-domain FSL. Average classification accuracies (%) are provided. \u2020 stands for exploiting the full data of FSL task. * means that the feature embedding network needs to be fine-tuned (Ft) on each target domain tasks. The best results are in bold.", "description": "This table compares the proposed method's performance with other state-of-the-art methods on 5-way 5-shot cross-domain few-shot learning (FSL) tasks.  It shows the average classification accuracy across eight different target domains (CUB, Cars, Places, Plantae, ChestX, ISIC, EuroSAT, CropDisease).  The table indicates whether methods used fine-tuning on each target domain and whether they used the full dataset. The best results for each target domain are highlighted in bold.", "section": "3.2 Comparison with state-of-the-art methods"}, {"figure_path": "2nisrxMMQR/tables/tables_17_1.jpg", "caption": "Table 7: Compared three times baseline with ours. Average classification accuracies (%) are provided. The best results are in bold.", "description": "This table compares the performance of the proposed method against a baseline model with three times the number of parameters. The average classification accuracy across eight target domains (CUB, Cars, Places, Plantae, Chest, ISIC, EuroSAT, and CropDisease) is presented for both 1-shot and 5-shot scenarios.  The results show that the proposed method achieves higher accuracy even when compared to a significantly larger baseline model.", "section": "3.2 Comparison with state-of-the-art methods"}, {"figure_path": "2nisrxMMQR/tables/tables_17_2.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 5-way 1-shot cross-domain FSL. Average classification accuracies (%) are provided. \u2020 stands for exploiting the full data of FSL task. * means that the feature embedding network needs to be fine-tuned (Ft) on each target domain tasks. The best results are in bold.", "description": "This table compares the proposed method's performance with other state-of-the-art methods on 5-way 1-shot cross-domain few-shot learning tasks.  It shows the average classification accuracy across eight different target domains (CUB, Cars, Places, Plantae, ChestX, ISIC, EuroSAT, and CropDisease).  The table indicates whether each method uses fine-tuning and whether it utilizes the full dataset, and highlights the best results for each domain.", "section": "3.2 Comparison with state-of-the-art methods"}, {"figure_path": "2nisrxMMQR/tables/tables_20_1.jpg", "caption": "Table 1: Comparison with state-of-the-art methods on 5-way 1-shot cross-domain FSL. Average classification accuracies (%) are provided. \u2020 stands for exploiting the full data of FSL task. * means that the feature embedding network needs to be fine-tuned (Ft) on each target domain tasks. The best results are in bold.", "description": "This table compares the proposed method's performance against several state-of-the-art methods on a 5-way 1-shot cross-domain few-shot learning (FSL) benchmark.  It shows the average classification accuracy across multiple target domains (CUB, Cars, Places, Plantae, ChestX, ISIC, EuroSAT, CropDisease) for each method.  The table highlights whether the methods used fine-tuning on the target domains and if they used the full dataset for training, providing context for the results.  The best-performing method for each dataset is shown in bold.", "section": "3.2 Comparison with state-of-the-art methods"}]