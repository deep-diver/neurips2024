[{"type": "text", "text": "Segmenting Watermarked Texts From Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xingchi Li\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guanxun Li\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics Texas A&M University College Station, TX 77843 anthony.li@stat.tamu.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics Beijing Normal University at Zhuhai Zhuhai, Guangdong 519087 guanxun@bnu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Xianyang Zhang\u2020   \nDepartment of Statistics   \nTexas A&M University   \nCollege Station, TX 77843   \nzhangxiany@stat.tamu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and nonwatermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google\u2019s C4 dataset and obtain encouraging numerical results.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the increasing use of large language models in recent years, it has become essential to differentiate between text generated by these models and text written by humans. Some of the most advanced LLMs, such as GPT-4, Llama 3, and Gemini, are very good at producing human-like texts, which could be challenging to distinguish from human-generated texts, even for humans. However, it is crucial to distinguish between human-produced texts and machine-produced texts to prevent the spread of misleading information, improper use of LLM-based tools in education, model extraction attacks through distillation, and the contamination of training datasets for future language models. ", "page_idx": 0}, {"type": "text", "text": "Watermarking is a principled method for embedding nearly unnoticeable statistical signals into text generated by LLMs, enabling provable detection of LLM-generated content from its human-written counterpart. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted large language model (LLM) provider, who then generates a text from their LLM with a watermark. This makes it possible for a detector to later identify the source of the text if the user publishes it. The user is allowed to modify the generated text by making substitutions, insertions, or deletions before publishing it. We aim to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. When the text is declared to be watermarked, we study a problem that has been relatively less investigated in the literature: to separate the published text into watermarked and non-watermarked sub-strings. In particular, we divide the texts into a sequence of moving sub-strings with a pre-determined length and sequentially test if each sub-string is watermarked based on the $p$ -values obtained from a randomization test. Given the $p$ -value sequence, we examine if there are structural breaks in the underlying distributions. For non-watermarked sub-strings, the $p$ -values are expected to be uniformly distributed over [0, 1], while for watermarked sub-strings, the corresponding $p$ -values are more likely to concentrate around zero. By identifying the change points in the distributions of the $p$ -values, we can segment the texts into watermarked and non-watermarked sub-strings. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In theory, we demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. Specifically, we obtain the convergence rate for the estimated change point locations. To validate our technique, we apply it to texts generated by different language models with prompts extracted from the real news-like data set from Google\u2019s C4 dataset, followed by different modifications/attacks that can introduce change points in the text. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related works and contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Digital watermarking is a field that focuses on embedding a signal in a medium, such as an image or text, and detecting it using an algorithm. Early watermarking schemes for natural language processing-generated text were presented by Atallah et al. [2001]. Hopper et al. [2007] formally defined watermarking and its desired properties, but their definitions were not specifically tailored to LLMs. More recently, Abdelnabi and Fritz [2021] and Munyer and Zhong [2023] proposed schemes that use machine learning models in the watermarking algorithm. These schemes take a passage of text and use a model to produce a semantically similar altered passage. However, there is no formal guarantee for generating watermarked text with desirable properties when using machine learning. ", "page_idx": 1}, {"type": "text", "text": "The current work is more related to Aaronson [2023], Kirchenbauer et al. [2023a], Kuditipudi et al. [2023]. In particular, Kirchenbauer et al. [2023a] introduced a \u201cred-green list\" watermarking technique that splits the vocabulary into two lists based on hash values of previous tokens. This technique slightly increases the probability of embedding the watermark into \u201cgreen\" tokens. Other papers that discuss this type of watermarking include Kirchenbauer et al. [2023b], Cai et al. [2024], Liu and Bu [2024], and Zhao et al. [2023]. However, this watermarking technique is biased, as the next token prediction (NTP) distributions have been modified, leading to a performance degradation of the LLM. Aaronson [2023] describes a technique for watermarking LLMs using exponential minimum sampling to sample tokens from an LLM, where the inputs to the sampling mechanism are also a hash of the previous tokens. This approach is closely related to the so-called Gumbel trick in machine learning. Kuditipudi et al. [2023] proposed an inverse transform watermarking method that can be made robust against potential random edits. Other unbiased watermarks in this fast-growing line of research include Zhao et al. [2024], Fernandez et al. [2023], Hu et al. [2023], Wu et al. [2023]. ", "page_idx": 1}, {"type": "text", "text": "However, less attention has been paid to understanding the statistical properties of watermark generation and detection schemes. The paper by Huang et al. [2023] considers watermark detection as a problem of composite dependence testing. The authors aim to understand the minimax Type II error and the most powerful test that achieves it. However, Huang et al. [2023] assumes that the NTP distributions remain unchanged from predicting the first token to the last token, which can be unrealistic. On the other hand, Li et al. [2024] introduced a flexible framework for determining the statistical efficiency of watermarks and designing powerful detection rules. This framework reduces the problem of determining the optimal detection rule to solving a minimax optimization program. ", "page_idx": 1}, {"type": "text", "text": "Compared to the existing literature, we make two contributions: ", "page_idx": 1}, {"type": "text", "text": "(a) We rigorously study the Type I and Type II errors of the randomization test to test the existence of a watermark; see Theorems 1-2. We apply these results to the recently proposed inverse transform watermark and Gumbel watermark schemes; see Corollary 1. ", "page_idx": 1}, {"type": "text", "text": "(b) We develop a systematic statistical method to segment texts into watermarked and nonwatermarked sub-strings. We have also investigated the theoretical and finite sample performance of this methodology. As far as we know, this problem has not been studied in recent literature on generating and detecting watermarked texts from LLMs. ", "page_idx": 2}, {"type": "text", "text": "2 Problem setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Watermarked text generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Denote by $\\nu$ the vocabulary (a discrete set), and let $P$ be an autoregressive LLM which maps a string $y_{-n_{0}:t-1}=y_{-n_{0}}y_{-n_{0}+1}\\cdot\\cdot\\cdot y_{t-1}\\in\\mathcal{V}^{t+n_{0}}$ to a distribution over the vocabulary, with $p(\\cdot|y_{-n_{0}:t-1})$ being the distribution of the next token $y_{t}$ . Here $y_{-n_{0}:0}$ denotes the prompt provided by the user. Set $V=|\\mathcal{V}|$ be the vocabulary size, and let $\\xi_{1:t}=\\xi_{1}\\xi_{2}\\cdot\\cdot\\cdot\\xi_{t}$ be a watermark key sequence with $\\xi_{i}\\in\\Xi$ for each $i$ , where $\\Xi$ is a general space. Given a prompt sent from a third-party user, the LLM provider calls a generator to autoregressitvely generate text from an LLM using a decoder function $\\Gamma$ , which maps $\\xi_{t}$ and a distribution $p_{t}$ over the next token to a value in $\\nu$ . The watermarking scheme should preserve the original text distribution, i.e., $P(\\Gamma(\\xi_{t},p_{t})=y)=p_{t}(y)$ . A watermark text generation algorithm recursively generates a string $y_{1:n}$ by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{i}=\\Gamma(\\xi_{i},p(\\cdot|y_{-n_{0}:i-1})),\\quad1\\leq i\\leq n,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $n$ is the number of tokens in the text $y_{1:n}$ generated by the LLM, and $\\xi_{i}$ \u2019s are assumed to be independently generated from some distribution $\\nu$ over $\\Xi$ . In other words, given $p(\\cdot|y_{-n_{0}:i-1})$ , $y_{i}$ is completely determined by $\\xi_{i}$ and $y_{-n_{0}:i-1}$ . ", "page_idx": 2}, {"type": "text", "text": "For ease of presentation, we associate each token in the vocabulary with a unique index from $[V]:=\\{1,2,\\overline{{\\ldots}},V\\}$ , and we remark that the generator should be invariant to this assignment. ", "page_idx": 2}, {"type": "text", "text": "Example 1 (Inverse transform sampling). In this example, we describe a watermarked text generation method developed in Kuditipudi et al. [2023] and discuss an alternative formulation of their approach. Write $\\bar{\\mu_{i}(k)}=\\bar{p(k|y_{-n_{0}:i-1})}$ for $1\\leq k\\leq V$ and $1\\leq i\\leq n$ . To generate the ith token, we consider a permutation $\\pi_{i}$ on $\\left[V\\right]$ and $u_{i}\\sim\\mathrm{Unif}[0,1]$ (which denotes the uniform distribution over $[0,1],$ ), which jointly act as the key $\\xi_{i}$ . Let ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Gamma((\\pi_{i},u_{i}),\\mu_{i})=\\pi_{i}^{-1}(\\operatorname*{min}\\{\\pi_{i}(l):\\mu_{i}(j:\\pi_{i}(j)\\leq\\pi_{i}(l))\\geq u_{i}\\}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We note that $\\Gamma((\\pi_{i},u_{i}),\\mu_{i})=k$ if $\\operatorname*{min}\\{\\pi_{i}(l):\\mu_{i}(j:\\pi_{i}(j)\\leq\\pi_{i}(l))\\geq u_{i}\\}=\\pi_{i}(k)$ , which implies that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{i}(j:\\pi_{i}(j)\\le\\pi_{i}(k))\\ge u_{i}>\\mu_{i}(j:\\pi_{i}(j)<\\pi_{i}(k)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "As the length of this interval is $\\mu_{i}(k)$ $,P(\\Gamma((\\pi_{i},u_{i}),\\mu_{i})=k)=\\mu_{i}(k)$ . An alternate way to describe the same generator is as follows: Given a partition of the interval $[0,1]$ into $V$ sub-intervals denoted by $\\mathcal{T}_{1},\\ldots,\\mathcal{T}_{V}$ , we can order them in such a way that each interval ${\\mathcal{T}}_{i}$ is adjacent to its immediate right neighbor $\\mathcal{T}_{i+1}$ . Now let $\\mathcal{T}(k;i)$ \u2019s be $V$ intervals with the length $|{\\mathcal{T}}(k;i)|=p(k|y_{-n_{0}:i-1})$ for $1\\leq k\\leq V$ . Given the permutation $\\pi_{i}$ , we can order the $V$ intervals through $\\pi_{i}$ , i.e., $\\{{\\mathcal{T}}_{\\pi_{i}(k)}(k;i):$ $k=1,2,\\ldots,V\\}$ . Define $\\xi_{i k}=\\mathbf{I}\\{u_{i}\\in\\mathcal{Z}_{\\pi_{i}(k)}(k;i)\\}$ and set $y_{i}=k$ if $\\xi_{i k}=1$ . Clearly $P(y_{i}=$ $k)=P(\\xi_{i k}=1)=P(u_{i}\\in\\mathcal{Z}_{\\pi_{i}(k)}(k;i))=p(k|y_{-n_{0}:i-1})$ . ", "page_idx": 2}, {"type": "text", "text": "Now, for a string $\\widetilde{y}_{1:n}$ (with the same length as the key) that is possibly watermarked, Kuditipudi et al. [2023] suggest the following metric to quantify the dependence between the watermark key and the string: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{M}(\\xi_{1:n},\\widetilde{y}_{1:n})=\\frac{1}{n}\\sum_{i=1}^{n}(u_{i}-1/2)\\left(\\frac{\\pi_{i}(\\widetilde{y}_{i})-1}{V-1}-\\frac{1}{2}\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Observe that if $\\widetilde{y_{i}}$ is generated using the above scheme with the key $\\xi_{i}=(\\pi_{i},u_{i})$ , then $u_{i}$ and $\\pi_{i}(\\widetilde{y_{i}})$ are positively correlated. Thus, a large value of $\\mathcal{M}$ indicates that $\\widetilde{y}_{1:n}$ is potentially watermarked. ", "page_idx": 2}, {"type": "text", "text": "Example 2 (Exponential minimum sampling). We describe another watermarking technique proposed in Aaronson [2023]. To generate each token of a text, we first sample $\\xi_{i k}\\sim\\mathrm{Unif}[0,1]$ independently for $1\\leq k\\leq V$ . Let ", "page_idx": 2}, {"type": "equation", "text": "$$\ny_{i}=\\mathop{\\arg\\operatorname*{max}}_{1\\leq k\\leq V}\\frac{\\log(\\xi_{i k})}{p(k|y_{-n_{0}:i-1})}=\\mathop{\\arg\\operatorname*{min}}_{1\\leq k\\leq V}\\frac{-\\log(\\xi_{i k})}{p(k|y_{-n_{0}:i-1})}=\\mathop{\\arg\\operatorname*{min}}_{1\\leq k\\leq V}E_{i k},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $E_{i k}\\;:=\\;-\\log(\\xi_{i k})/p(k|y_{-n_{0}:i-1})\\,\\sim\\,\\mathrm{Exp}(p(k|y_{-n_{0}:i-1}))$ with ${\\mathrm{Exp}}(a)$ denoting an exponential random variable with the rate $a$ . For two exponential random variables $X\\sim\\operatorname{Exp}(a)$ and $Y\\,\\sim\\,\\mathrm{Exp}(b)$ , we have two basic properties: (i) $\\operatorname*{min}(X,Y)\\,\\sim\\,\\operatorname{Exp}(a\\,+\\,b)$ ; (ii) $P(X<{\\dot{Y}})\\,=$ $\\mathbb{E}[1-\\exp(-a Y)]=a/(a+b)$ . Using (i) and (ii), it is straightforward to verify that ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(y_{i}=k)=P\\left(E_{i k}<\\operatorname*{min}_{j\\neq k}E_{i j}\\right)=p(k|y_{-n_{0}:i-1}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, this generation scheme preserves the original text distribution. ", "page_idx": 3}, {"type": "text", "text": "Aaronson [2023] proposed to measure the dependence between a string $\\widetilde{y}_{1:n}$ and the key sequence $\\xi_{1:n}$ using the metric ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{M}(\\xi_{1:n},\\widetilde{y}_{1:n})=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\{\\log(\\xi_{i,\\widetilde{y}_{i}})+1\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The idea behind the definition of this function is that if $\\widetilde{y}_{i}$ was generated using the key $\\xi_{i}$ , then $\\xi_{i,\\widetilde{y}_{i}}$ tends to have a higher value than the other components  o f $\\xi_{i}$ . Therefore, a larger value of the metri c indicates that the string $\\widetilde{y}_{1:n}$ is more likely to be watermarked. ", "page_idx": 3}, {"type": "text", "text": "2.2 Watermarked text detection ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now consider the detection problem, which involves determining whether a given text is watermarked or not. Consider the case where a string $\\widetilde{y}_{1:m}$ is published by the third-party user and a key sequence $\\xi_{1:n}$ is provided to a detector. The detector calls a detection method to test ", "page_idx": 3}, {"type": "text", "text": "by computing a $p$ -value with respect to a test statistic $\\phi(\\xi_{1:n},\\widetilde{y}_{1:m})$ . It is important to note that the text published by the user can be quite different from the text initially generated by the LLM using the key $\\xi_{1:n}$ , which we refer to as $y_{1:n}$ . To account for this difference, we can use a transformation function $\\mathcal{E}$ that takes $y_{1:n}$ as the input and produces the published text $\\widetilde{y}_{1:m}$ as the output, i.e., $\\widetilde{y}_{1:m}=\\mathcal{E}(y_{1:n})$ . This transformation can involve substitutions, insertions, del e tions, paraphrases, or  ot her edits to the input text. ", "page_idx": 3}, {"type": "text", "text": "The test statistic $\\phi$ measures the dependence between the text $\\widetilde{y}_{1:m}$ and the key sequence $\\xi_{1:n}$ . Throughout our discussions, we will assume that a large value of $\\phi$ p rovides evidence against the null hypothesis (e.g., stronger dependence between $\\widetilde{y}_{1:m}$ and $\\xi_{1:n\\,,}$ ). To obtain the $p$ -value, we consider a randomization test. In particular, we generate \u03bei(t) $\\xi_{i}^{(t)}\\sim\\nu$ independently over $1\\leq i\\leq n$ and $1\\leq t\\leq T$ , and $\\xi_{i}^{(t)}\\mathbf{s}$ are independent with $\\widetilde{y}_{1:m}$ . Then the randomization-based $p$ -value is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{T}=\\frac{1}{T+1}\\left(1+\\sum_{t=1}^{T}\\mathbf{1}\\{\\phi(\\xi_{1:n},\\widetilde{y}_{1:m})\\le\\phi(\\xi_{1:n}^{(t)},\\widetilde{y}_{1:m})\\}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 1. For the randomization test, we have the following results. ", "page_idx": 3}, {"type": "text", "text": "(i) Under the null, $P(p_{T}\\leq\\alpha)={\\lfloor(T+1)\\alpha\\rfloor}/(T+1)\\leq\\alpha$ , where $\\lfloor a\\rfloor$ denotes the greatest integer that is less than or equal to $a$ ; ", "page_idx": 3}, {"type": "text", "text": "(ii) Suppose the following three conditions hold: ", "page_idx": 3}, {"type": "text", "text": "(a) $\\begin{array}{r l}&{\\operatorname*{max}\\{\\mathrm{Var}(\\phi(\\xi_{1:n},\\widetilde{y}_{1:m})|\\mathcal{F}_{m}),\\mathrm{Var}(\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})|\\mathcal{F}_{m})\\}\\le C_{v}/n\\,\\mathrm{with}\\,C_{v}>0;}\\\\ &{\\mathbb E[\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})|\\mathcal{F}_{m}]=O(n^{-1/2});}\\\\ &{\\operatorname*{lim}_{n\\to\\infty}\\sqrt{n}\\mathbb E[\\phi(\\xi_{1:n},\\widetilde{y}_{1:m})|\\mathcal{F}_{m}]=\\infty.}\\end{array}$ (b)   \n(c)   \nHere $\\mathcal{F}_{m}=[y_{-n_{0}:0},\\widetilde{y}_{1:m}]$ and $\\xi_{1:n}^{\\prime}$ is a key sequence generated in the same way as $\\xi_{1:n}$ but is independent of $\\widetilde{y}_{1:m}$ . Given any $\\epsilon>0$ , when $T>\\bar{2}/\\epsilon-1$ ,   \n$P(p_{T}\\leq\\alpha|\\mathcal{F}_{m})\\geq1-C_{1}\\exp(-2T\\epsilon^{2})+o(1),$ (3) ", "page_idx": 3}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "as $n\\to+\\infty$ , where $C_{1}>0$ . ", "page_idx": 3}, {"type": "text", "text": "We now apply the results in Theorem 1 to Examples 1-2 with $m\\ =\\ n$ and $\\phi(\\xi_{1:n},\\widetilde{y}_{1:n})\\;=$ $\\mathcal{M}(\\xi_{1:n},\\widetilde{y}_{1:n}^{\\widetilde{\\mathbf{\\Gamma}}})$ for $\\mathcal{M}$ defined in (1) and (2). ", "page_idx": 3}, {"type": "text", "text": "Corollary 1. If $\\widetilde{y}_{1:n}=y_{1:n}$ and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}\\bigl(1-p\\bigl(y_{i}|y_{-n_{0}:i-1}\\bigr)\\bigr)\\to\\infty,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then Conditions (a)-(c) in Theorem $^{\\,l}$ are satisfied for Examples 1-2. Consequently, the power of the randomization test converges to 1 in these examples as $T\\rightarrow+\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "However, as the published text can be modified, it is not expected that every token in $\\widetilde{y}_{1:m}$ will be related to the key sequence. Instead, we expect certain sub-strings of $\\widetilde{y}_{1:m}$ to be correlated with the key sequence under the alternative hypothesis $H_{a}$ . To measure the dependence, we use a scanning method that looks at every segment/sub-string of $\\widetilde{y}_{1:m}$ and a segment of $\\xi_{1:n}$ with the same length $B$ . We use a measure $\\mathcal{M}(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1})$ to   quantify the dependence between $\\xi_{a:a+B-1}$ and $\\widetilde{y}_{b:b+B-1}$ , chosen based on the watermarked text generation method described above. Given $\\mathcal{M}$ and t he block size $B$ , we can define the maximum test statistic as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi(\\xi_{1:n},\\widetilde{y}_{1:m})=\\operatorname*{max}_{1\\leq a\\leq n-B+1}\\operatorname*{max}_{1\\leq b\\leq m-B+1}\\mathcal{M}(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Consider the maximum statistic defined in (5), where the dependence measure takes the form of $\\begin{array}{r}{\\mathcal{M}(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1})=B^{-1}\\sum_{i=0}^{B-1}h_{i}(\\xi_{a+i},\\widetilde{y}_{b+i})}\\end{array}$ and $h_{i}\\mathbf{s}$ are independent conditional on $\\left[\\widetilde{y}_{1:m},y_{-n_{0}:n}\\right]$ . Und e r the setting of Example 1, maxi $|h_{i}|\\leq1/4$ . In this case, suppose ", "page_idx": 4}, {"type": "equation", "text": "$$\nC_{N,B}^{-1}\\operatorname*{max}_{a,b}\\mathbb{E}[\\mathcal{M}(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1})|\\widetilde{y}_{1:m},y_{-n_{0}:n}]\\to+\\infty,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $N\\,=\\,\\operatorname*{max}\\{n,m\\}$ and $C_{N,B}\\,=\\,\\sqrt{\\log(N)/B}$ . Then, (3) holds true. Under the setting of Example 2, $h_{i}\\mathbf{s}$ are exponentially distributed conditional on $[\\widetilde{y}_{1:m},y_{-n_{0}:n}]$ . In this case, (3) is still true under (6) with $C_{N,B}=\\log(N)/B$ . ", "page_idx": 4}, {"type": "text", "text": "3 Sub-string identification ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we aim to address the following question, which seems less explored in the existing literature: given that the global null hypothesis $H_{0}$ is rejected, how can we identify the sub-strings from the modified text $\\widetilde{y}_{1:m}$ that are machine-generated? ", "page_idx": 4}, {"type": "text", "text": "To describe the setup, we suppose the text published by the third-party user has the following structure: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underbrace{\\widetilde{y}_{1}\\widetilde{y}_{2}\\cdot\\cdot\\cdot\\widetilde{y}_{\\tau_{1}}}_{\\mathrm{non-watermarked}}\\underbrace{\\widetilde{y}_{\\tau_{1}+1}\\cdot\\cdot\\cdot\\widetilde{y}_{\\tau_{2}}}_{\\mathrm{watermarked}}\\underbrace{\\widetilde{y}_{\\tau_{2}+1}\\cdot\\cdot\\cdot\\widetilde{y}_{\\tau_{3}}}_{\\mathrm{non-watermarked}}\\underbrace{\\widetilde{y}_{\\tau_{3}+1}\\cdot\\cdot\\cdot\\widetilde{y}_{\\tau_{4}}}_{\\mathrm{watermarked}}\\cdot\\cdot\\cdot\\cdot\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "in which case the sub-strings $\\widetilde{y}_{\\tau_{1}+1}\\cdot\\cdot\\cdot\\widetilde{y}_{\\tau_{2}}$ and $\\widetilde{y}_{\\tau_{3}+1}\\cdot\\cdot\\cdot\\widetilde{y}_{\\tau_{4}}$ are watermarked. We emphasize that the orders of the watermarked and non-watermarked sub-strings can be arbitrary and do not affect our method described below. The goal here is to separate the text into watermarked and nonwatermarked sub-strings accurately. Our key insight to tackling this problem is translating it into a change point detection problem. To describe our method, we define a sequence of moving windows $\\mathcal{T}_{i}=\\bar{[(i-B/2)\\vee1,(i+B/2)\\wedge m]}$ with $B$ being the window size which is assumed to be an even number (for the ease of presentation) and $1\\,\\leq\\,i\\,\\leq\\,m$ . For each sub-string, we define a randomization-based $p$ -value given by ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{i}=\\frac{1}{T+1}\\left(1+\\sum_{t=1}^{T}\\mathbf{1}\\{\\phi(\\xi_{1:n},\\widetilde{y}_{\\mathcal{I}_{i}})\\leq\\phi(\\xi_{1:n}^{(t)},\\widetilde{y}_{\\mathcal{I}_{i}})\\}\\right),\\quad1\\leq i\\leq m,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we let $\\begin{array}{r}{\\phi(\\xi_{1:n},\\widetilde{y}_{\\mathcal{Z}_{i}})=\\operatorname*{max}_{1\\leq k\\leq n}\\mathcal{M}(\\xi_{\\mathcal{T}_{k}},\\widetilde{y}_{\\mathcal{Z}_{i}})}\\end{array}$ with $\\mathcal{T}_{k}=[(k-B/2)\\vee1,(k+B/2)\\wedge n].$ . ", "page_idx": 4}, {"type": "text", "text": "We have now transformed the text into a sequence of $p$ -values: $p_{1},\\ldots,p_{m}$ . Under the null, the $p$ -values are roughly uniformly distributed, while under the alternatives, the $p$ -values will concentrate around zero. Consider a simple setup where the published text can be divided into two halves, with the first half watermarked and the second half non-watermarked (or vice versa). Then, we can identify the change point location through ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\tau}=\\underset{1\\leq\\tau<m}{\\arg\\operatorname*{max}}\\,S_{1:m}(\\tau),\\quad S_{1:m}(\\tau):=\\underset{t\\in[0,1]}{\\operatorname*{sup}}\\,\\frac{\\tau(m-\\tau)}{m^{3/2}}|F_{1:\\tau}(t)-F_{\\tau+1:m}(t)|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $F_{a:b}(t)$ denotes the empirical cumulative distribution function (cdf) of $\\{p_{i}\\}_{i=a}^{b}$ . ", "page_idx": 5}, {"type": "text", "text": "We shall use the block bootstrap-based approach to determine if $\\hat{\\tau}$ is statistically significant. Note that conditional on ${\\mathcal{F}}_{m}$ , the $p$ -value sequence is $B$ -dependent in the sense that $p_{i}$ and $p_{j}$ are independent only if $|i-j|>B$ . Hence, the usual bootstrap or permutation methods for independent data may not work as they fail to capture the dependence from neighboring $p$ -values. Instead, we can employ the so-called moving block bootstrap for time series data [Kunsch, 1989, Liu et al., 1992]. Given a block size, say $B^{\\prime}$ , we create $m-B^{\\prime}+1$ blocks given by $\\{p_{i},\\dots,p_{i+B^{\\prime}-1}\\}$ for $1\\leq i\\leq m-B^{\\prime}+1$ . We randomly sample $m/B^{\\prime}$ (assuming that $m/B^{\\prime}$ is an integer for simplicity) blocks with replacement and paste them together. Denote the resulting resampled $p$ -values by $p_{1}^{*},\\ldots,p_{m}^{*}$ . We then compute $F_{a,b}^{*}(t)$ based on the bootstrapped $p$ -values and define ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{1:m}^{\\ast}(\\tau)=\\operatorname*{sup}_{t\\in[0,1]}\\frac{\\tau(m-\\tau)}{m^{3/2}}\\vert F_{1:\\tau}^{\\ast}(t)-F_{\\tau+1:m}^{\\ast}(t)\\vert.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Repeat this procedure $T^{\\prime}$ times and denote the statistics by $S_{1:m}^{*,(t)}(\\tau)$ for $t=1,2,\\dots,T^{\\prime}$ . Define the corresponding bootstrap-based $p$ -value as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{p}_{T^{\\prime}}=\\frac{1}{T^{\\prime}+1}\\left(1+\\sum_{t=1}^{T^{\\prime}}{\\bf1}\\left\\{\\operatorname*{max}_{1\\leq\\tau<m}S_{1:m}(\\tau)\\leq\\operatorname*{max}_{1\\leq\\tau<m}S_{1:m}^{*,(t)}(\\tau)\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We claim that there is a statistically significant change point if $\\tilde{p}_{T^{\\prime}}\\leq\\alpha$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 1. Alternatively, one can consider the Cram\u00e9r-von Mises type statistic $\\operatorname*{max}_{\\tau}C_{1:m}(\\tau)$ with ", "page_idx": 5}, {"type": "equation", "text": "$$\nC_{1:m}(\\tau)=\\int_{0}^{1}\\frac{\\tau^{2}(m-\\tau)^{2}}{m^{3}}|F_{1:\\tau}(t)-F_{\\tau+1:m}(t)|^{2}w(t)d t,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "to examine the existence of a change point, where $w(\\cdot)$ is a non-negative weight function defined over $[0,1]$ . If a change point exists, we can estimate its location through $\\tilde{\\tau}=\\arg\\operatorname*{max}_{1\\leq\\tau<m}C_{1:m}(\\tau)$ . Other methods to quantify the distributional shift for change point detection include the distance and kernel-based two-sample metrics [Matteson and James, 2014, Chakraborty and Zhang, 2021] and graph-based test statistics [Chen and Zhang, 2015]. ", "page_idx": 5}, {"type": "text", "text": "Consider the case where there is a single change point located at $\\tau^{*}$ . Without loss of generality, let us assume that before the change, the $p$ -values are uniformly distributed, i.e., $p_{1},\\ldots,p_{\\tau^{*}}\\;\\sim$ $F_{0}$ with $F_{0}(t)\\,=\\,t$ (the cdf of $\\mathrm{Unif}[0,1])$ , while after the change, the $p$ -values follow different distributions that concentrate around zero. We assume that $\\mathrm{lim}\\operatorname*{inf}_{m\\rightarrow+\\infty}D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])>0$ , where $\\begin{array}{r}{D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])\\,:=\\,\\operatorname*{sup}_{t\\in[0,1]}|F_{0}(t)-\\mathbb{E}[F_{\\tau^{*}+1:m}(t)]|}\\end{array}$ is the Kolmogorov-Smirnov distance. In other words, the empirical cdf of the $p$ -values from the watermarked sub-strings converges to a distribution that is different from the uniform distribution. This mild assumption allows for the detection of the structure break. As discussed before, we consider the scan statistic $\\mathrm{max}_{1\\leq\\tau<m}\\,S_{1:m}(\\tau)$ with $S_{1:m}(\\tau)$ defined in (9) for examining the existence of a change point. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. Assuming that $B\\to+\\infty$ and $B/m\\rightarrow0$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{1\\leq\\tau<m}S_{1:m}(\\tau)\\geq\\sqrt{m}\\gamma^{*}(1-\\gamma^{*})D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])+o_{p}(1)\\to+\\infty,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma^{*}=\\operatorname*{lim}_{m\\rightarrow+\\infty}\\tau^{*}/m\\in(0,1)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Next, we shall establish the consistency of the change point estimator $\\hat{\\tau}=\\arg\\operatorname*{max}_{1\\leq\\tau\\leq m}S_{1:m}(\\tau)$ In particular, we obtain the following result, which establishes the convergence rate of the change point estimate. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Under the Assumptions in Proposition $^{\\,l}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\hat{\\tau}-\\tau^{*}|=O_{p}\\left(\\frac{\\sqrt{m B\\log(m/B)}}{D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 2. Our scenario differs from the traditional nonparametric change point detection problem in a few ways: (i) Rather than testing the homogeneity of the original data sequence (which is the setup typically considered in the change point literature), we convert the string into a sequence of p-values, based on which we conduct the change-point analysis; (ii) In the classical change point literature, the observations (in our case, the $\\mathbf{p}$ -values) within the same segment are assumed to follow the same distribution. In contrast, for the watermark detection problem, the p-values from the watermarked segment could follow different distributions, adding a layer of difficulty to the analysis; (iii) The p-value sequence is dependent (where the strength of dependence is controlled by $B$ ), making our setup very different from the one in Carlstein [1988], which assumed the underlying data sequence to be independent; (iv) The technical tool used in our analysis must account for the particular dependence structure within the p-value sequence. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.1 Binary segmentation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we describe an algorithm to separate watermarked and non-watermarked sub-strings by identifying multiple change point locations. There are two main types of algorithms for identifying multiple change points in the literature: (i) exact or approximate optimization by minimizing a penalized cost function [Harchaoui and L\u00e9vy-Leduc, 2010, Truong et al., 2020, R. Killick and Eckley, 2012, Li and Zhang, 2024, Zhang and Dawn, 2023] and (ii) approximate segmentation algorithms. Our proposed algorithm is based on the popular binary segmentation method, a top-down approximate segmentation approach for finding multiple change point locations. Initially proposed by Vostrikova [1981], binary segmentation identifies a single change point in a dataset using a CUSUM-like procedure and then employs a divide-and-conquer approach to find additional change points within sub-segments until a stopping condition is reached. However, as a greedy algorithm, binary segmentation can be less effective with multiple change points. Wild binary segmentation (WBS) [Fryzlewicz, 2014] and seeded binary segmentation (SeedBS) [Kov\u00e1cs et al., 2022] improve upon this by defining multiple segments to identify and aggregate potential change points. SeedBS additionally addresses the issue of overly long sub-segments in WBS that may contain several change points. SeedBS uses multiple layers of intervals, each with a fixed number of intervals of varying lengths and shifts, to enhance the search for change points. When comparing multiple candidates for the next change point, the narrowest-over-threshold (NOT) method [Baranowski et al., 2019] prioritizes narrower sub-segments. Built upon these ideas, we develop an effective algorithm to identify the change points that separate watermarked and non-watermarked sub-strings. The details are described in Algorithm 1 below. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 SeedBS-NOT for change point detection in potentially partially watermarked texts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Require: Sequence of $p$ -values $\\{p_{i}\\}_{i=1}^{m}$ , decay parameter $a\\in[1/2,1)$ for SeedBS, threshold $\\zeta$ for NOT.   \nEnsure: Locations of the change points. Define $I_{1}=(0,m]$ . \u25b7Start of SeedBS for k \u21902, . . . , $\\lceil\\bar{\\log}_{1/a}\\,m\\rceil$ do Number of intervals in the $k$ -th layer: $n_{k}=2\\lceil(1/a)^{k-1}\\rceil-1$ . Length of intervals in the $k$ -th layer: $l_{k}=m a^{k-1}$ . Shift of intervals in the $k$ -th layer: $s_{k}=(m-l_{k})/(n_{k}-1)$ . $k$ -th layer intervals: $\\begin{array}{r}{\\mathcal{T}_{k}=\\bigcup_{i=1}^{n_{k}}\\{(\\lfloor(i-1)s_{k}\\rfloor}\\end{array}$ , $[(i-1)s_{k}+l_{k}]]\\}$ .   \neDned ntervals $\\textstyle{\\mathcal{T}}=\\bigcup_{k=1}^{\\lceil\\log_{1/a}m\\rceil}{\\mathcal{T}}_{k}$ . $\\triangleright$ End of SeedBS $i\\gets1,\\ldots,|\\mathcal{T}|$ $\\triangleright$ Start of NOT DDeeffiinnee $i$ $I_{i}=(r_{i},s_{i}]$ $\\begin{array}{r}{S_{r_{i}+1:s_{i}}(\\tau):=\\operatorname*{sup}_{t\\in[0,1]}\\frac{(\\tau-r_{i})(s_{i}-\\tau)}{(s_{i}-r_{i})^{3/2}}|F_{r_{i}+1:\\tau}(t)-F_{\\tau+1:s_{i}}(t)|.}\\end{array}$ $\\hat{\\tau}_{i}=\\arg\\operatorname*{max}_{r_{i}<\\tau\\leq s_{i}}S_{r_{i}+1:s_{i}}(\\tau)$ . Obtain $\\tilde{p}_{i}$ through block bootstrap (10). end for Define the set of potential change point locations $\\mathcal{O}=\\{i:\\tilde{p}_{i}<\\zeta\\}$ and the final set of change point locations ${\\mathcal{S}}=\\emptyset$ . while $O\\neq\\emptyset$ do Select $\\begin{array}{r}{i=\\arg\\operatorname*{min}_{i=1,...,|\\mathcal{O}|}\\{|I_{i}|\\}=\\arg\\operatorname*{min}_{i=1,...,|\\mathcal{O}|}\\{s_{i}-r_{i}\\}.}\\end{array}$ S \u2190S \u222a{\u03c4\u02c6i}; $\\mathcal{O}\\gets\\{j\\leq|\\mathcal{O}|:\\hat{\\tau}_{i}\\notin I_{j}\\}$ . end while return $\\boldsymbol{S}$ . ", "page_idx": 6}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/3038b1c26b322424ede958e7b8ed4968ef07596aa1bd2d28900b0667f4d263da.jpg", "img_caption": ["Figure 1: Left panel: boxplots of the number of false detections with respect to different thresholds $\\zeta$ Right panel: sequences of $p$ -values from different methods in Setting 1 for Prompt 1 with threshold $\\zeta=0.005$ . The detected change point locations are marked with dashed lines at the index 157 for ITS and 158 for ITSL, respectively. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4 Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct extensive real-data-based experiments following a similar empirical setting in Kirchenbauer et al. [2023a], where we generate watermarked text based on the prompts sampled from the news-like subset of the colossal clean crawled corpus (C4) dataset [Raffel et al., 2020]. We utilized three LLMs, namely openai-community/gpt2 [Radford et al., 2019], facebook/opt-1.3b [Zhang et al., 2022] and Meta-Llama-3-8B [AI $@$ Meta, 2024], to evaluate the effectiveness of the proposed method. We consider the following four watermark generation and detection methods: ", "page_idx": 7}, {"type": "text", "text": "\u2022 ITS: The inverse transform sampling method with the dependence measure defined in (1). \u2022 ITSL: The inverse transform sampling method with the dependence measure defined in (B.2), which is based on the Levenshtein cost (B.1) with base alignment cost (B.3). \u2022 EMS: The exponential minimum sampling method with the dependence measure defined in (2). \u2022 EMSL: The exponential minimum sampling method with the dependence measure defined in (B.3), which is based on the Levenshtein cost (B.1) with base alignment cost (B.4). ", "page_idx": 7}, {"type": "text", "text": "The details of the Levenshtein cost are deferred to Appendix B.1. For each of the experiment settings, 100 prompts were used to generate the watermarked text. We fix the length of text $m=500$ , the size of sliding window $B=20$ , and the block size used in the block bootstrap-based test $B^{\\prime}=20$ . Resu\u221alts for other choices of $B$ are shown in Appendix C. In Algorithm 1, we set the decay parameter $a={\\sqrt{2}}$ and the minimum length of the intervals generated by SeedBS to be 50 such that the block bootstrapbased test is meaningful, and the threshold $\\zeta\\in\\{0.05,0.01,0.005,0.001\\}$ . We present the results for openai-community/gpt2 in the main text and defer the results for facebook/opt-1.3b and Meta-Llama-3-8B to Appendix B.3 and Appendix B.4, respectively. ", "page_idx": 7}, {"type": "text", "text": "4.1 False positive analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We begin by analyzing the false discoveries of the change point detection method. We will generate watermarked text with a length of $m=500$ , where no change points exist. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Setting 1 (no change point): Generate 500 tokens with a watermark. ", "page_idx": 7}, {"type": "text", "text": "The results for openai-community/gpt2 are showed in Figure 1. The left panel illustrates that the two exponential minimum sampling methods result in fewer false discoveries compared to the two inverse transform sampling methods. Indeed, the existence of false discoveries highly depends on the quality of the obtained sequence of $p$ -values. In the right panel, we fixed one prompt and plotted the sequence of $p$ -values for the four methods. Most of the $p$ -values are near 0. However, certain sub-strings have relatively high $p$ -values for the two inverse transform methods, indicating that these methods failed to detect the watermark in these segments, resulting in false discoveries for change point detection. ", "page_idx": 7}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/037bd0f81cf569b04c76b82f1c16779905048e41c5f35d4b559b218cee8c3383.jpg", "img_caption": ["Figure 2: The boxplots of the Rand index comparing the clusters identified through the detected change points with the true clusters separated by the true change points with respect to different thresholds $\\zeta$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Change point analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "When users modify the text generated by LLM, there may be some sub-strings with watermarks and others without. Our goal is to accurately separate the text into watermarked and non-watermarked sub-strings. In this section, we will focus on two types of attacks: insertion and substitution. To demonstrate, we will consider the following three settings: ", "page_idx": 8}, {"type": "text", "text": "\u2022 Setting 2 (insertion attack): Generate 250 tokens with watermarks, then append with 250 tokens without watermarks. In this setting, there is a single change point at the index 251. \u2022 Setting 3 (substitution attack): Generate 500 tokens with watermarks, then substitute the token with indices ranging from 201 to 300 with non-watermarked text of length 100. In this setting, there are two change points at the indices 201 and 301, respectively. \u2022 Setting 4 (insertion and substitution attacks): Generate 400 tokens with watermarks, substitute the token with indices ranging from 101 to 200 with non-watermarked text of length 100, and then insert 100 tokens without watermarks at the index 300. In this setting, there are four change points located at the indices 101, 201, 301, and 401, respectively. ", "page_idx": 8}, {"type": "text", "text": "For more complex simulation settings, please refer to Appendix B.5. ", "page_idx": 8}, {"type": "text", "text": "We compare the clusters identified through the detected change points with the true clusters separated by the true change points using the Rand index [Rand, 1971]. A higher Rand index indicates better performance of different methods. ", "page_idx": 8}, {"type": "text", "text": "Figure 2 shows the Rand index for four methods in Settings 2-4 corresponding to different thresholds $\\zeta$ . For each method, their performance in Settings 2-3 is better than that of Setting 4 when the threshold $\\zeta\\leq0.01$ . This is because Setting 4 includes two types of attacks, making the problem more difficult than in Settings 2 and 3. In all cases, the two exponential minimum sampling methods outperform the two inverse transform sampling methods, and EMS delivers the highest Rand index value. We want to emphasize again that the performance of the change point detection method highly depends on the quality of the obtained sequence of $p$ -values. Figure 3 shows the $p$ -value sequence for all methods given one fixed prompt in Setting 4. The change points detected by the EMS and EMSL methods are closer to the true change points compared to those detected by the ITS and ITSL methods. Additionally, the sequence of $p$ -values for all methods in Settings 1-4 with the first 10 prompts extracted Google C4 dataset are shown in Figure B.1 in the Appendix. ", "page_idx": 8}, {"type": "text", "text": "5 Discussions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this study, we have introduced a method for detecting whether a text is generated using LLM through randomization tests. We have demonstrated that our method effectively controls Type I and Type II errors under appropriate assumptions. Additionally, we have developed a technique to partition the published text into watermarked and non-watermarked sub-strings by treating it as a change point detection problem. Our proposed method accurately identifies watermarked sub-strings by determining the locations of change points. Simulation results using real data indicate that the EMS method outperforms the other methods. ", "page_idx": 8}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/255c872f6e300c7384840ac9569544f99f50da60d8e068da22657ebceec3b1c6.jpg", "img_caption": ["Figure 3: Sequences of $p$ -values for all methods given one fixed prompt in Setting 4 with the threshold $\\zeta=0.005$ . The true change points are located at 101, 201, 301, and 401. The change points detected by the EMS and EMSL methods are closer to the actual change points compared to those detected by the ITS and ITSL methods. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The performance of the segmentation algorithm depends crucially on the quality of the randomizationbased $p$ -values from each sub-string. Intuitively, a more significant discrepancy between the $p$ -value distributions under the null and alternative will lead to better segmentation results. Thus, a powerful watermark detection algorithm is crucial to the success of the segmentation procedure. Motivated by Condition (4), an interesting future direction is to develop an adaptive watermark generation and detection procedure where the LLM provider adaptively embeds the key according to NTP, and the detector uses a corresponding adaptive procedure to detect the watermark. ", "page_idx": 9}, {"type": "text", "text": "Another interesting direction to explore is extending the algorithm to handle scenarios where the published text combines watermarked texts from different LLMs with varying watermark generation schemes. In this case, the goal is to separate the texts into sub-strings from different LLMs. This scenario involves multiple sequences of keys from different LLMs, each producing a sequence of $p$ -values and change points. Figuring out how to aggregate these results to separate different LLMs and user-modified texts would be an intriguing problem. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "GL and XZ were supported by the National Institutes of Health under Grant R01GM144351. Part of the research was conducted using the Arseven Computing Cluster at the Department of Statistics, Texas A&M University. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Scott Aaronson. Watermarking of large language models, 2023. URL https://simons.berkeley. edu/talks/scott-aaronson-ut-austin-openai-2023-08-17. ", "page_idx": 9}, {"type": "text", "text": "Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In 2021 IEEE Symposium on Security and Privacy (SP), pages 121\u2013140. IEEE, 2021. URL https://doi.org/10.1109/SP40001.2021.00083. ", "page_idx": 9}, {"type": "text", "text": "AI $@$ Meta. Llama 3 model card. https://github.com/meta-llama/llama3/blob/main/ MODEL_CARD.md, 2024. URL https://github.com/meta-llama/llama3/blob/main/ MODEL_CARD.md. ", "page_idx": 9}, {"type": "text", "text": "Mikhail J Atallah, Victor Raskin, Michael Crogan, Christian Hempelmann, Florian Kerschbaum, Dina Mohamed, and Sanket Naik. Natural language watermarking: Design, analysis, and a proof-of-concept implementation. In Information Hiding: 4th International Workshop, IH 2001 Pittsburgh, PA, USA, April 25\u201327, 2001 Proceedings 4, pages 185\u2013200. Springer, 2001. URL https://doi.org/10.1007/3-540-45496-9_14. ", "page_idx": 9}, {"type": "text", "text": "Rafal Baranowski, Yining Chen, and Piotr Fryzlewicz. Narrowest-Over-Threshold Detection of Multiple Change Points and Change-Point-Like Features. Journal of the Royal Statistical Society Series B: Statistical Methodology, 81(3):649\u2013672, 05 2019. ISSN 1369-7412. doi: 10.1111/rssb. 12322. URL https://doi.org/10.1111/rssb.12322. ", "page_idx": 9}, {"type": "text", "text": "Zhongze Cai, Shang Liu, Hanzhao Wang, Huaiyang Zhong, and Xiaocheng Li. Towards better statistical understanding of watermarking llms. arXiv preprint arXiv:2403.13027, 2024. URL https://doi.org/10.48550/arXiv.2403.13027.   \nEdward Carlstein. Nonparametric change-point estimation. The Annals of Statistics, 16(1):188\u2013197, 1988. URL https://doi.org/10.1214/aos/1176350699.   \nShubhadeep Chakraborty and Xianyang Zhang. High-dimensional change-point detection using generalized homogeneity metrics. arXiv preprint arXiv:2105.08976, 2021. URL https://doi. org/10.48550/arXiv.2105.08976.   \nHao Chen and Nancy Zhang. Graph-based change-point detection. Annals of Statistics, 2015. URL https://doi.org/10.1214/14-AOS1269.   \nPierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate watermarks for large language models. In 2023 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1\u20136. IEEE, 2023. URL https://doi.org/ 10.48550/arXiv.2308.00113.   \nPiotr Fryzlewicz. Wild binary segmentation for multiple change-point detection. The Annals of Statistics, 42(6):2243 \u2013 2281, 2014. doi: 10.1214/14-AOS1245. URL https://doi.org/10. 1214/14-AOS1245.   \nZa\u0131d Harchaoui and C\u00e9line L\u00e9vy-Leduc. Multiple change-point estimation with a total variation penalty. Journal of the American Statistical Association, 105(492):1480\u20131493, 2010.   \nNicholas Hopper, David Molnar, and David Wagner. From weak to strong watermarking. In Theory of Cryptography: 4th Theory of Cryptography Conference, TCC 2007, Amsterdam, The Netherlands, February 21-24, 2007. Proceedings 4, pages 362\u2013382. Springer, 2007. URL https: //doi.org/10.1007/978-3-540-70936-7_20.   \nZhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiased watermark for large language models. arXiv preprint arXiv:2310.10669, 2023. URL https: //doi.org/10.48550/arXiv.2310.10669.   \nBaihe Huang, Banghua Zhu, Hanlin Zhu, Jason D Lee, Jiantao Jiao, and Michael I Jordan. Towards optimal statistical watermarking. arXiv preprint arXiv:2312.07930, 2023. URL https://doi. org/10.48550/arXiv.2312.07930.   \nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 17061\u201317084. PMLR, 23\u201329 Jul 2023a. URL https://proceedings.mlr. press/v202/kirchenbauer23a.html.   \nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models. arXiv preprint arXiv:2306.04634, 2023b. URL https://doi.org/ 10.48550/arXiv.2306.04634.   \nS Kov\u00e1cs, P B\u00fchlmann, H Li, and A Munk. Seeded binary segmentation: a general methodology for fast and optimal changepoint detection. Biometrika, 110(1):249\u2013256, 10 2022. ISSN 1464-3510. doi: 10.1093/biomet/asac052. URL https://doi.org/10.1093/biomet/asac052.   \nRohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortionfree watermarks for language models. arXiv preprint arXiv:2307.15593, 2023. URL https: //doi.org/10.48550/arXiv.2307.15593.   \nHans R Kunsch. The jackknife and the bootstrap for general stationary observations. The annals of Statistics, pages 1217\u20131241, 1989. URL https://doi.org/10.1214/aos/1176347265.   \nSoumendra N Lahiri. Theoretical comparisons of block bootstrap methods. Annals of Statistics, pages 386\u2013404, 1999.   \nXiang Li, Feng Ruan, Huiyuan Wang, Qi Long, and Weijie J Su. A statistical framework of watermarks for large language models: Pivot, detection efficiency and optimal rules. arXiv preprint arXiv:2404.01245, 2024. URL https://doi.org/10.48550/arXiv.2404.01245.   \nXingchi Li and Xianyang Zhang. fastcpd: Fast change point detection in r. arXiv preprint arXiv:2404.05933, 2024. URL https://doi.org/10.48550/arXiv.2404.05933.   \nRegina Y Liu, Kesar Singh, et al. Moving blocks jackknife and bootstrap capture weak dependence. Exploring the limits of bootstrap, 225:248, 1992. URL https://www.wiley.com/en-us/ Exploring+the $^+$ Limits+of+Bootstrap-p-9780471536314.   \nYepeng Liu and Yuheng Bu. Adaptive text watermark for large language models. arXiv preprint arXiv:2401.13927, 2024. URL https://doi.org/10.48550/arXiv.2401.13927.   \nDavid S Matteson and Nicholas A James. A nonparametric approach for multiple change point analysis of multivariate data. Journal of the American Statistical Association, 109(505):334\u2013345, 2014. URL https://doi.org/10.1080/01621459.2013.849605.   \nTravis Munyer and Xin Zhong. Deeptextmark: Deep learning based text watermarking for detection of large language model generated text. arXiv preprint arXiv:2305.05773, 2023. URL https: //doi.org/10.48550/arXiv.2305.05773.   \nP. Fearnhead R. Killick and I. A. Eckley. Optimal detection of changepoints with a linear computational cost. Journal of the American Statistical Association, 107(500):1590\u20131598, 2012. doi: 10.1080/01621459.2012.737745. URL https://doi.org/10.1080/01621459.2012.737745.   \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. URL https://openai. com/index/better-language-models/.   \nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.   \nWilliam M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846\u2013850, 1971. doi: 10.1080/01621459.1971.10482356. URL https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356.   \nSidney Resnick. A probability path. Springer, 2019. URL https://doi.org/10.1007/ 978-0-8176-8409-9.   \nCharles Truong, Laurent Oudre, and Nicolas Vayatis. Selective review of offline change point detection methods. Signal Processing, 167:107299, 2020. ISSN 0165-1684. doi: https://doi.org/ 10.1016/j.sigpro.2019.107299. URL https://www.sciencedirect.com/science/article/ pii/S0165168419303494.   \nLyudmila Yur\u2019evna Vostrikova. Detecting \u201cdisorder\u201d in multidimensional random processes. In Doklady akademii nauk, volume 259, pages 270\u2013274. Russian Academy of Sciences, 1981. URL https://www.mathnet.ru/eng/dan44582.   \nYihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. Dipmark: A stealthy, efficient and resilient watermark for large language models. arXiv preprint arXiv:2310.07710, 2023. URL https://doi.org/10.48550/arXiv.2310.07710.   \nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. URL https://doi.org/10.48550/ arXiv.2205.01068.   \nXianyang Zhang and Trisha Dawn. Sequential gradient descent and quasi-newton\u2019s method for change-point analysis. In International Conference on Artificial Intelligence and Statistics, pages 1129\u20131143. PMLR, 2023.   \nXuandong Zhao, Yu-Xiang Wang, and Lei Li. Protecting language generation models via invisible watermarking. In International Conference on Machine Learning, pages 42187\u201342199. PMLR, 2023. URL https://doi.org/10.48550/arXiv.2302.03162.   \nXuandong Zhao, Lei Li, and Yu-Xiang Wang. Permute-and-flip: An optimally robust and watermarkable decoder for llms. arXiv preprint arXiv:2402.05864, 2024. URL https://doi.org/10. 48550/arXiv.2402.05864. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Proofs of the main results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 1. ", "page_idx": 13}, {"type": "text", "text": "(i) For simplicity of notation, denote $\\varphi:=\\phi(\\xi_{1:n},\\widetilde{y}_{1:m})$ and $\\varphi^{(t)}:=\\phi(\\xi_{1:n}^{(t)},\\widetilde{y}_{1:m})$ for $t=1,\\cdot\\cdot\\cdot,T$ Under the null hypothesis, we know that $\\xi_{1:n}$ is i n dependent of $\\widetilde{y}_{1:m}$ , whic h  implies that the pairs $(\\xi_{1:n},\\widetilde{y}_{1:m}),(\\xi_{1:n}^{(1)},\\widetilde{y}_{1:m}),\\dots,(\\xi_{1:n}^{(T)},\\widetilde{y}_{1:m})$ follow the same distribution. Hence $\\varphi,\\varphi^{(1)},\\ldots,\\varphi^{(T)}$ are excha n geable. Th  e exchangeabilit y  ensures that the rank of $\\varphi$ relative to $\\{\\varphi,\\varphi^{(1)},\\ldots,\\varphi^{(t)}\\}$ is uniformly distributed. Denote the order statistics $\\varphi_{(1)}\\leq\\cdots\\leq\\varphi_{(T+1)}$ . Then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nP\\left(\\varphi=\\varphi_{(j)}\\right)={\\frac{1}{T+1}},\\qquad j=1,\\ldots,T+1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, for $j=1,\\ldots,T+1$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nP\\left(p_{T}\\leq{\\frac{j}{T+1}}\\right)=P\\left(\\varphi\\in\\left\\{\\varphi_{(T+2-j)},\\ldots,\\varphi_{(T+1)}\\right\\}\\right)={\\frac{j}{T+1}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, we have $P\\left(p_{T}\\leq\\alpha\\right)=\\lfloor(T+1)\\alpha\\rfloor/(T+1)\\leq\\alpha$ . ", "page_idx": 13}, {"type": "text", "text": "(ii) Denote $\\mathsf E_{\\xi^{\\prime}}:=\\mathbb E[\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m}|\\mathcal F_{m})]$ . By Chebyshev\u2019s inequality and Condition (a), we get ", "page_idx": 13}, {"type": "equation", "text": "$$\nP\\left(|\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})-\\mathsf{E}_{\\xi^{\\prime}}|\\ge\\epsilon/\\sqrt{n}|\\mathcal{F}_{m}\\right)\\le\\frac{\\mathrm{Var}(\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})|\\mathcal{F}_{m})}{\\epsilon^{2}/n}\\le\\frac{C_{v}}{\\epsilon^{2}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for all $\\epsilon\\geq0$ . Thus $\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})-\\mathsf{E}_{\\xi^{\\prime}}=O_{p}(n^{-1/2})$ , which together with Condition (b) implies that $\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})=O_{p}(n^{-1/2})$ given ${\\mathcal{F}}_{m}$ . ", "page_idx": 13}, {"type": "text", "text": "Denote the distribution of $\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})$ conditional on ${\\mathcal{F}}_{m}$ by $F$ , and the empirical distribution of $\\{\\varphi^{(t)}\\}_{t=0}^{T}$ by $F_{T}$ , where we set $\\varphi^{(0)}=\\varphi$ . Let $q_{1-\\alpha,T}=\\varphi_{(T+2-j_{\\alpha})}$ with $j_{\\alpha}=\\lfloor(T+1)\\alpha\\rfloor$ . Note that $F_{T}(q_{1-\\alpha,T})=1-(j_{\\alpha}-1)/(T+1)$ . Our test rejects the null whenever $p_{T}\\,\\leq\\,\\alpha$ , which is equivalent to rejecting the null if $\\varphi\\ge\\varphi_{(T+2-j_{\\alpha})}$ . By the Dvoretzky-Kiefer-Wolfowitz inequality, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\nP(|F_{T}(q_{1-\\alpha,T})-F(q_{1-\\alpha,T})|>\\epsilon|\\mathcal{F}_{m})\\leq P(\\underset{x}{\\operatorname*{sup}}\\,|F_{T}(x)-F(x)|>\\epsilon|\\mathcal{F}_{m})\\leq C_{1}\\exp(-2T\\epsilon^{2})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for some constant $C_{1}>0$ , which implies that with probability greater than $1-C_{1}\\exp(-2T\\epsilon^{2})$ , $F(q_{1-\\alpha,T})<1-(j_{\\alpha}-1)/(T+1)+2\\epsilon$ . Define $F^{-\\hat{1}}(t)=\\operatorname*{inf}\\{\\bar{s}:F(s)\\geq t\\}$ and the event $A_{T}=$ $\\{q_{1-\\alpha,T}<F^{-1}(1-(j_{\\alpha}-1)/(T+1)+2\\epsilon)\\}$ . Then we have $P(A_{T}|\\mathcal{F}_{m})\\geq1-C_{1}\\exp(-2T\\epsilon^{2})$ . In addition, as $\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})=O_{p}(n^{-1/2})$ , we have $F^{-1}(s)=O(n^{-1/2})$ for any $s<1$ . By Condition (a), we have $\\sqrt{n}\\big(\\phi(\\xi_{1:n},\\widetilde{y}_{1:m})-\\mathbb{E}[\\phi(\\xi_{1:n},\\widetilde{y}_{1:m})|\\mathcal{F}_{m}]\\big)=O_{p}(1)$ . Hence, for $T>2/\\epsilon-1$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{>}\\operatorname{v}_{<\\sqrt{\\operatorname{s}_{1}},\\ldots,\\sigma,\\tau,w>-1}-_{2^{k}-\\omega,\\tau,i\\setminus\\sigma}}\\\\ &{\\geq P\\left(\\sqrt{n}\\left(\\phi(\\xi_{1:n},\\tilde{y}_{1:m})-\\mathbb{E}[\\phi(\\xi_{1:n},\\tilde{y}_{1:m})|\\mathcal{F}_{m}]\\right)+\\sqrt{n}\\mathbb{E}[\\phi(\\xi_{1:n},\\tilde{y}_{1:m})|\\mathcal{F}_{m}]\\geq\\sqrt{n}q_{1-\\alpha,T},A_{T}|\\mathcal{F}_{m}\\right)}\\\\ &{\\geq P\\Big(\\sqrt{n}\\left(\\phi(\\xi_{1:n},\\tilde{y}_{1:m})-\\mathbb{E}[\\phi(\\xi_{1:n},\\tilde{y}_{1:m})|\\mathcal{F}_{m}]\\right)+\\sqrt{n}\\mathbb{E}[\\phi(\\xi_{1:n},\\tilde{y}_{1:m})|\\mathcal{F}_{m}]}\\\\ &{\\quad\\geq\\sqrt{n}F^{-1}(1-(j_{\\alpha}-1)/(T+1)+2\\epsilon),A_{T}|\\mathcal{F}_{m}\\Big)}\\\\ &{\\geq P\\Big(\\sqrt{n}\\big(\\phi(\\xi_{1:n},\\tilde{y}_{1:m})-\\mathbb{E}[\\phi(\\xi_{1:n},\\tilde{y}_{1:m})|\\mathcal{F}_{m}]\\big)+\\sqrt{n}\\mathbb{E}[\\phi(\\xi_{1:n},\\tilde{y}_{1:m})|\\mathcal{F}_{m}]}\\\\ &{\\quad\\geq\\sqrt{n}F^{-1}(1-\\alpha+3\\epsilon),A_{T}|\\mathcal{F}_{m}\\Big)}\\\\ &{\\geq1-C_{1}\\exp(-2T\\epsilon^{2})+o(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we have used Condition (c) and the fact that $\\sqrt{n}F^{-1}(1\\,-\\,\\alpha\\,+\\,3\\epsilon)\\;=\\;O(1)$ to get the convergence. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "We state the following Lemma, which is useful for the proof of Corollary 1. ", "page_idx": 13}, {"type": "text", "text": "Lemma 1. $\\{\\xi_{i}\\}_{i=1}^{n}$ are conditionally independent given $y_{-n_{0}:n}$ ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . Recall that $y_{i}=\\Gamma(\\xi_{i},p(\\cdot|y_{-n_{0}:i-1}))=\\Gamma_{i}(\\xi_{i})$ , where $\\Gamma_{i}$ depends on $y_{-n_{0}:i-1}$ . We note that ", "page_idx": 14}, {"type": "equation", "text": "$$\np(\\xi_{1:n},y_{1:n}|y_{-n_{0}:0})=p(\\xi_{1},y_{1}|y_{-n_{0}:0})\\prod_{i=2}^{n}p(\\xi_{i},y_{i}|\\xi_{1:i-1},y_{-n_{0}:i-1})=\\prod_{i=1}^{n}\\mathbf{1}\\{\\Gamma_{i}(\\xi_{i})=y_{i}\\}p(\\xi_{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence the conditional distribution of $\\xi_{1:n}$ given $y_{-n_{0}:n}$ is equal to ", "page_idx": 14}, {"type": "equation", "text": "$$\np(\\xi_{1:n}|y_{-n_{0}:n})=\\prod_{i=1}^{n}\\frac{\\mathbf{1}\\{\\Gamma_{i}(u)=y_{i}\\}d\\nu(u)}{\\int_{\\Gamma_{i}(u)=y_{i}}d\\nu(u)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which implies that $\\{\\xi_{i}\\}_{i=1}^{n}$ are conditionally independent given $y_{-n_{0}:n}$ ", "page_idx": 14}, {"type": "text", "text": "Proof of Corollary $^{\\,I}$ . (i) Recall in Example 1 that the test statistic is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi(\\xi_{1:n},y_{1:n})={\\frac{1}{n}}\\sum_{i=1}^{n}(u_{i}-1/2)\\left({\\frac{\\pi_{i}(y_{i})-1}{V-1}}-{\\frac{1}{2}}\\right):={\\frac{1}{n}}\\sum_{i=1}^{n}h_{i}(\\xi_{i},y_{i}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\xi_{i}=(u_{i},\\pi_{i})$ . We first note that $h_{i}(\\xi_{i},y_{i})$ is bounded and thus Condition (a) of Theorem 1 holds. Since $\\xi_{1:n}^{\\prime}$ is independent of $y_{-n_{0}:n}$ , we have $u_{i}^{\\prime}|y_{1:n}\\sim\\mathrm{Unif}[0,1]$ . Thus, $\\mathbb{E}[\\phi(\\xi_{1:n}^{\\prime},y_{1:n})|y_{1:n}]=0$ , which implies that Condition (b) of Theorem 1 is fulfilled. ", "page_idx": 14}, {"type": "text", "text": "Denote $\\mu_{i}(\\cdot)\\;=\\;p(\\cdot|y_{-n_{0}:i-1})$ . Conditional on $y_{-n_{0}:i}$ and $\\pi_{i}(y_{i})$ , we know that $u_{i}$ follows the uniform distribution over the interval $[\\mu_{i}(y:\\pi_{i}(y)<\\tau_{i}(y_{i})),\\mu_{i}(y:\\pi_{i}(y)\\leq\\pi_{i}(y_{i}))]$ . As a result, we can calculate the expected value of $u_{i}$ given $y_{-n_{0}:i}$ and $\\pi_{i}(y_{i})$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[u_{i}|y_{-n_{0}:i},\\pi_{i}(y_{i})]=\\frac{1}{2}\\left\\{\\mu_{i}(y:\\pi_{i}(y)<\\pi_{i}(y_{i}))+\\mu_{i}(y:\\pi_{i}(y)\\leq\\pi_{i}(y_{i}))\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\mu_{i}(y_{i})}{2}+\\mu_{i}(y:\\pi_{i}(y)<\\pi_{i}(y_{i}))}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\mu_{i}(y_{i})}{2}+\\frac{\\pi_{i}(y_{i})-1}{V-1}(1-\\mu_{i}(y_{i}))}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{2}+(1-\\mu_{i}(y_{i}))\\left(\\frac{\\pi_{i}(y_{i})-1}{V-1}-\\frac{1}{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the third equality is because given $\\pi_{i}(y_{i})=k$ , $\\pi_{i}$ follows the uniform distribution over the permutation space with the restriction $\\pi_{i}(y_{i})=k$ . Then, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[(u_{i}-1/2)\\left(\\frac{\\pi_{i}(y_{i})-1}{V-1}-\\frac{1}{2}\\right)|y_{-n_{0}:i}\\right]}\\\\ &{=\\!\\mathbb{E}\\left[\\mathbb{E}\\left[(u_{i}-1/2)\\left(\\frac{\\pi_{i}(y_{i})-1}{V-1}-\\frac{1}{2}\\right)|y_{-n_{0}:i},\\pi_{i}(y_{i})\\right]|y_{-n_{0}:i}\\right]}\\\\ &{=\\!\\mathbb{E}\\left[(1-\\mu_{i}(y_{i}))\\left(\\frac{\\pi_{i}(y_{i})-1}{V-1}-\\frac{1}{2}\\right)^{2}\\right]}\\\\ &{=\\!\\left(1-p(y_{i}|y_{-n_{0}:i-1})\\right)\\!\\mathbb{E}\\left[\\left(\\frac{\\pi_{i}(y_{i})-1}{V-1}-\\frac{1}{2}\\right)^{2}|y_{-n_{0}:i}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Given $y_{i}$ , $\\left(\\pi_{i}(y_{i})-1\\right)/(V-1)$ follows the uniform distribution over the discrete space $\\{0,1/(V-$ $1),\\ldots,1\\}$ . Thus we have $\\mathbb{E}[(\\bar{\\pi}_{i}(y_{i})-1)/(V-1)]=1/2$ and $\\mathrm{Var}((\\pi_{i}(y_{i})-1)/(V\\stackrel{.}{-}1))=C$ , which is a constant less than $1/12$ (i.e., the variance of $\\mathrm{Unif}[0,1])$ . Hence, $\\mathbb{E}[\\phi(\\xi_{1:n},y_{1:n})]\\;=$ $\\begin{array}{r}{{C n^{-1}}\\sum_{i=1}^{n}\\bigl(1-p\\bigl(y_{i}|y_{1:i-1}\\bigr)\\bigr)}\\end{array}$ and Condition (c) of Theorem 1 is satisfied due to (4). ", "page_idx": 14}, {"type": "text", "text": "(ii) In Example 2, the test statistic is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\phi(\\xi_{1:n},y_{1:n})=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\{\\log(\\xi_{i,y_{i}})+1\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Observe that $E_{i k}:=-\\log(\\xi_{i k})/p(k|y_{-n_{0}:i-1})\\sim\\mathrm{Exp}(p(k|y_{-n_{0}:i-1}))$ . Since $\\xi_{1:n}^{\\prime}$ is independent of $y_{1:n}$ , we have $-\\log(\\xi_{i,y_{i}}^{\\prime})|y_{-n_{0}:n}\\sim\\mathrm{Exp}(1)$ . Hence, conditional on $y_{-n_{0}:n}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\phi(\\xi_{1:n}^{\\prime},y_{1:n})|y_{-n_{0}:n}]=0,\\quad\\mathrm{Var}(\\phi(\\xi_{1:n}^{\\prime},y_{1:n})|y_{-n_{0}:n})=\\frac{1}{n}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Condition (b) of Theorem 1 is satisfied. ", "page_idx": 15}, {"type": "text", "text": "Given $y_{-n_{0}:n}$ , we know $\\begin{array}{r l r l r l}{E_{i,y_{i}}}&{{}}&{\\ =\\ }&{{}}&{\\operatorname*{min}_{1\\le k\\le V}E_{i k}.}\\end{array}$ , which implies $-\\log(\\xi_{i,y_{i}})/p(y_{i}|y_{-n_{0}:i-1})|y_{-n_{0}:n}\\sim\\mathrm{Exp}(1)$ . It is worth noting that ", "page_idx": 15}, {"type": "equation", "text": "$$\nP(-\\log(\\xi_{i,y_{i}})\\geq t)=p\\left(-\\frac{\\log(\\xi_{i,y_{i}})}{p(y_{i}|y_{-n_{0}:i-1})}\\geq\\frac{t}{p(y_{i}|y_{-n_{0}:i-1})}\\right)=\\exp\\left(-\\frac{t}{p(y_{i}|y_{-n_{0}:i-1})}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "That is $-\\log(\\xi_{i,y_{i}})|y_{-n_{0}:n}\\sim\\mathrm{Exp}(1/p(y_{i}|y_{-n_{0}:i-1}))$ . According to Lemma 1, $\\xi_{i}\\mathbf{s}$ are conditionally independent given $y_{-n_{0}:n}$ . Thus, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\phi(\\xi_{1:n},y_{1:n})|y_{-n_{0}:n}]=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left(1-p(y_{i}|y_{-n_{0}:i-1})\\right),}\\\\ &{\\mathrm{Var}(\\phi(\\xi_{1:n},y_{1:n})|y_{-n_{0}:n})=\\displaystyle\\frac{1}{n^{2}}\\sum_{i=1}^{n}p(y_{i}|y_{-n_{0}:i-1})^{2}\\leq\\displaystyle\\frac{1}{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, Conditions (a) and (c) of Theorem 1 are satisfied. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 2. We only prove the result under the setting of Example 1 as the proof for Example 2 is similar with the help of Bernstein\u2019s inequality. Because $|h_{i}|\\leq1/4$ , by Hoeffding\u2019s inequality, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad P\\left(\\left|\\mathcal{M}\\left(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\right)-\\mathbb{E}\\big[\\mathcal{M}\\big(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\big)\\big|\\widetilde{y}_{1:m},y_{-n_{0}:n}\\big]\\right|>t\\big|\\widetilde{y}_{1:m},y_{-n_{0}:n}\\right)}\\\\ &{\\leq2\\exp\\left(-8B t^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the union bound, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\Bigg(\\underset{1\\leq a\\leq n-B+1,1\\leq b\\leq m-B+1}{\\operatorname*{max}}|\\mathcal{M}(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1})}\\\\ &{\\quad-\\,\\mathbb{E}[\\mathcal{M}(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1})|\\widetilde{y}_{1:m},y_{-n_{0}:n}]|>t|\\widetilde{y}_{1:m},y_{-n_{0}:n}\\Bigg)}\\\\ &{\\leq2(n-B+1)(m-B+1)\\exp\\left(-8B t^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Integrating out the strings in $y_{1:n}$ that are not contained in $\\widetilde{y}_{1:m}$ , we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\Bigg(\\underset{1\\leq a\\leq n-B+1,1\\leq b\\leq m-B+1}{\\operatorname*{max}}|\\mathcal{M}\\big(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\big)}\\\\ &{\\quad-\\,\\mathbb{E}[\\mathcal{M}\\big(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\big)|\\widetilde{y}_{1:m},y_{-n_{0}:n}]|>t|\\mathcal{F}_{m}\\Bigg)}\\\\ &{\\quad\\leq2(n-B+1)(m-B+1)\\exp\\left(-8B t^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathcal{F}_{m}=[\\widetilde{y}_{1:m},y_{-n_{0}:0}]$ . Thus, conditional on ${\\mathcal{F}}_{m}$ $,\\operatorname*{max}_{a,b}\\{|\\mathbb{E}[\\mathcal{M}(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1})|\\mathcal{F}_{m}]-$ $\\mathcal{M}(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1})|\\}=O(C_{N,B})$ . Note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi\\big(\\xi_{1:n},\\widetilde{y}_{1:m}\\big)=\\underset{a,b}{\\operatorname*{max}}\\mathcal{M}\\big(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\big)}\\\\ &{\\qquad\\qquad\\geq\\underset{a,b}{\\operatorname*{max}}\\mathbb{E}\\big[\\mathcal{M}\\big(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\big)\\big|\\mathcal{F}_{m}\\big]}\\\\ &{\\qquad\\qquad-\\underset{a,b}{\\operatorname*{max}}\\big\\{\\mathbb{E}\\big[\\mathcal{M}\\big(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\big)\\big|\\mathcal{F}_{m}\\big]-\\mathcal{M}\\big(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\big)\\big\\}}\\\\ &{\\qquad=\\underset{a,b}{\\operatorname*{max}}\\mathbb{E}\\big[\\mathcal{M}\\big(\\xi_{a:a+B-1},\\widetilde{y}_{b:b+B-1}\\big)\\big|\\mathcal{F}_{m}\\big]+O\\big(C_{N,B}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "On the other hand, for a randomly generated key $\\xi_{1:n}^{\\prime},\\mathbb{E}[\\mathcal{M}(\\xi_{a:a+B-1}^{\\prime},\\widetilde{y}_{b:b+B-1})|\\mathcal{F}_{m}]=0$ for all $a,b$ . By the same argument, we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P\\left(\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})>t|\\mathcal{F}_{m}\\right)=\\!P\\left(\\!\\!\\begin{array}{c}{\\operatorname*{max}}\\\\ {1\\!\\!\\leq\\!a\\!\\leq\\!n\\!-\\!B\\!+\\!1\\!,\\!1\\!\\leq\\!b\\!\\leq\\!m\\!-\\!B\\!+\\!1}\\\\ {\\leq\\!2(n-B+1)(m-B+1)\\exp\\left(-8B t^{2}\\right),}\\end{array}\\!\\!\\!\\begin{array}{c}{M(\\xi_{a:a+B-1}^{\\prime},\\widetilde{y}_{b:b+B-1})>t\\Big|\\mathcal{F}_{m}\\right)}\\\\ {\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\leq\\!2(n-B+1)(m-B+1)\\exp\\left(-8B t^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which suggests that $F^{-1}(s)=O(C_{N,B})$ with $F$ being the distribution of $\\phi(\\xi_{1:n}^{\\prime},\\widetilde{y}_{1:m})$ conditional on ${\\mathcal{F}}_{m}$ and $F^{-1}(t)=\\operatorname*{inf}\\{s:F(s)\\geq t\\}$ . The rest of the arguments are similar to those in the proof of Theorem 1. We skip the details. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . Note that conditional on ${\\mathcal{F}}_{m}$ , the $p$ -value sequence is $B$ -dependent in the sense that $p_{i}$ and $p_{j}$ are independent only if $|i-j|>B$ . Under the assumption on $B$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{Var}(F_{a+1:b}(t)|\\mathcal{F}_{m})=O\\left({\\frac{B}{|b-a|}}\\right)=o(1)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $|b-a|\\asymp m$ , which implies that $F_{a+1:b}(t)-\\mathbb{E}[F_{a+1:b}(t)]\\to^{p}0$ for any given $t\\in[0,1]$ . Using similar arguments as in the proof of Theorem 7.5.2 of Resnick [2019], we can strengthen the result to allow uniform convergence over $t\\in[0,1]$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in[0,1]}|F_{a+1:b}(t)-\\mathbb{E}[F_{a+1:b}(t)]|\\to^{p}0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{1\\leq r\\leq m}{\\operatorname*{max}}S_{1:m}(\\tau)}\\\\ &{\\geq S_{1:m}(\\tau^{*})}\\\\ &{=\\!\\sqrt{m}\\frac{\\tau^{*}(m-\\tau^{*})}{m^{2}}\\underset{t\\in[0,1]}{\\operatorname*{sup}}\\left|F_{1:\\tau^{*}}(t)-t-\\left(F_{\\tau^{*}+1:m}(t)-\\mathbb{E}[F_{\\tau^{*}+1:m}(t)]\\right)+t-\\mathbb{E}[F_{\\tau^{*}+1:m}(t)]\\right|}\\\\ &{\\geq\\!\\sqrt{m}\\frac{\\tau^{*}(m-\\tau^{*})}{m^{2}}\\!\\left\\{\\underset{t\\in[0,1]}{\\operatorname*{sup}}|t-\\mathbb{E}[F_{\\tau^{*}+1:m}(t)]|-\\underset{t\\in[0,1]}{\\operatorname*{sup}}|F_{1:\\tau^{*}}(t)-t|\\right.}\\\\ &{\\ \\ \\ -\\underset{t\\in[0,1]}{\\operatorname*{sup}}|F_{\\tau^{*}+1:m}(t)-\\mathbb{E}[F_{\\tau^{*}+1:m}(t)]|\\Bigg\\}}\\\\ &{\\left.=\\!\\sqrt{m}\\gamma^{*}(1-\\gamma^{*})\\{D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])+o_{p}(1)\\}(1+o(1))\\rightarrow+\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3. We begin the proof by noting that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\tau(m-\\tau)}{m^{3/2}}(F_{1:\\tau}(t)-F_{\\tau+1:m}(t))=\\frac{1}{\\sqrt{m}}\\sum_{i=1}^{\\tau}(\\mathbf{1}\\{p_{i}\\leq t\\}-F_{1:m}(t)).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let us focus on the case where $\\hat{\\tau}\\leq\\tau^{*}$ . The other case where $\\hat{\\tau}>\\tau^{*}$ can be proved in a similar way. By the definition of $\\hat{\\tau}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n0\\leq\\frac{1}{m}\\operatorname*{sup}_{t}\\left|\\sum_{i=1}^{\\hat{r}}(\\mathbf{1}\\{p_{i}\\leq t\\}-F_{1:m}(t))\\right|-\\frac{1}{m}\\operatorname*{sup}_{t}\\left|\\sum_{i=1}^{\\tau^{*}}(\\mathbf{1}\\{p_{i}\\leq t\\}-F_{1:m}(t))\\right|:=I(\\hat{\\tau})-I(\\tau^{*}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For any $1\\leq a<b\\leq m$ and there is no change point between $[a,b]$ , define $\\begin{array}{r}{W_{a:b}(t)=\\sum_{i=a}^{b}\\mathbf{1}\\{p_{i}\\leq}\\end{array}$ $t\\}$ and $\\begin{array}{r}{\\bar{W}_{a:b}(t)=\\sum_{i=a}^{b}\\{\\mathbf1\\{p_{i}\\leq t\\}-\\mathbb{E}[F_{a:b}(t)]\\}}\\end{array}$ . For any $\\tau\\leq\\tau^{*}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{I(\\tau)=\\displaystyle\\frac{1}{m}\\operatorname*{sup}_{t}\\left|W_{1:\\tau}(t)-\\frac{\\tau}{m}(W_{1:\\tau^{*}}(t)+W_{\\tau^{*}+1:m}(t))\\right|}}\\\\ {{\\mathrm{}\\;\\;\\;\\;\\;\\;=\\displaystyle\\frac{1}{m}\\operatorname*{sup}_{t}\\left|\\tau F_{0}(t)-\\frac{\\tau}{m}\\{\\tau^{*}F_{0}(t)+(m-\\tau^{*})\\mathbb{E}[F_{\\tau^{*}+1:m}(t)]\\}\\right|+R(\\tau)}}\\\\ {{\\mathrm{}\\;\\;\\;\\;\\;\\;=\\displaystyle\\frac{(m-\\tau^{*})\\tau}{m^{2}}D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])+R(\\tau),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $R(\\tau)$ is a reminder term satisfying that ", "page_idx": 17}, {"type": "equation", "text": "$$\n|R(\\tau)|\\leq\\frac{1}{m}\\operatorname*{sup}_{t}\\left|\\bar{W}_{1:\\tau}(t)-\\frac{\\tau}{m}\\big(\\bar{W}_{1:\\tau^{*}}(t)+\\bar{W}_{\\tau^{*}+1:m}(t)\\big)\\right|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\le I(\\hat{\\tau})-I(\\tau^{*})}\\\\ &{\\quad=\\displaystyle\\frac{(m-\\tau^{*})(\\hat{\\tau}-\\tau^{*})}{m^{2}}D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])+R(\\hat{\\tau})-R(\\tau^{*})}\\\\ &{\\quad\\le\\displaystyle\\frac{(m-\\tau^{*})(\\hat{\\tau}-\\tau^{*})}{m^{2}}D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])+2\\operatorname*{sup}_{\\tau\\le\\tau^{*}}|R(\\tau)|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tau^{*}-\\hat{\\tau}\\leq\\frac{2m^{2}}{(m-\\tau^{*})D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])}\\operatorname*{sup}_{\\tau\\leq\\tau^{*}}|R(\\tau)|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Notice that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\tau}{\\operatorname*{sup}}\\,|R(\\tau)|\\leq\\!\\frac{1}{m}\\underset{\\tau\\leq\\tau^{*}}{\\operatorname*{sup}}\\underset{t\\in[0,1]}{\\operatorname*{sup}}\\left|\\bar{W}_{1:\\tau}(t)-\\frac{\\tau}{m}\\{\\bar{W}_{1:\\tau^{*}}(t)+\\bar{W}_{\\tau^{*}+1:m}(t)\\}\\right|}\\\\ &{\\qquad\\qquad\\leq\\!\\frac{1}{m}\\underset{\\tau\\leq\\tau^{*}}{\\operatorname*{sup}}\\,\\underset{t\\in[0,1]}{\\operatorname*{sup}}\\,\\left|\\bar{W}_{1:\\tau}(t)\\right|+\\frac{1}{m}\\underset{t\\in[0,1]}{\\operatorname*{sup}}\\,\\left|\\bar{W}_{1:\\tau^{*}}(t)+\\bar{W}_{\\tau^{*}+1:m}(t)\\right|}\\\\ &{\\qquad\\qquad\\leq\\!\\frac{1}{m}\\underset{\\tau\\leq m}{\\operatorname*{sup}}\\,\\underset{t\\in[0,1]}{\\operatorname*{sup}}\\,\\left|\\bar{W}_{1:\\tau}(t)\\right|+O_{p}(m^{-1/2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It remains to analyze $\\operatorname*{sup}_{\\tau\\leq m}\\operatorname*{sup}_{t\\in[0,1]}\\left|\\bar{W}_{1:\\tau}(t)\\right|$ . We first present a result that slightly modifies the Ottaviani\u2019s inequality. For the ease of notation, we write $\\Vert\\bar{W}_{1:\\tau}\\Vert=\\operatorname*{sup}_{t\\in[0,1]}\\left|\\bar{W}_{1:\\tau}(t)\\right|$ . For any $u>0$ and $v>B$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nP\\left(\\operatorname*{max}_{\\tau\\leq m}\\|\\bar{W}_{1:\\tau}\\|>u+v\\right)\\leq\\frac{P(\\|\\bar{W}_{1:m}\\|>v-B)}{1-\\operatorname*{max}_{\\tau}P(\\|\\bar{W}_{\\tau+1:m}\\|>u)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To see this, let $A_{k}$ be the event that $\\left\\Vert\\bar{W}_{1:k}\\right\\Vert$ is the first $\\left\\|\\bar{W}_{1:j}\\right\\|$ (for $j=1,2,\\dots,m)$ that is strictly greater than $u\\!+\\!v$ . The event on the LHS is the disjoint union of $A_{1},\\ldots,A_{m}$ . Note that $\\lVert\\bar{W}_{k+1+B:m}\\rVert$ is independent of $\\|\\bar{W}_{1}\\|,\\ldots,\\|\\bar{W}_{k}\\|$ (and hence is independent of $A_{k}$ ). We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{P(A_{k})\\operatorname*{min}_{0\\leq\\tau<m}P(\\|\\bar{W}_{\\tau+1:m}\\|\\leq u)\\leq P(A_{k},\\|\\bar{W}_{k+1+B:m}\\|\\leq u)}}\\\\ &{}&{\\leq\\!P(A_{k},\\|\\bar{W}_{k+1:m}\\|\\leq u+B)}\\\\ &{}&{\\leq\\!P(A_{k},\\|\\bar{W}_{1:m}\\|>v-B),\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have usede the fact $\\|\\bar{W}_{k+1:m}\\|\\leq\\|\\bar{W}_{k+1:k+B}\\|+\\|\\bar{W}_{k+B+1:m}\\|\\leq u+B.$ . Summing over $k$ leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\nP\\left(\\operatorname*{max}_{\\tau\\leq m}\\|\\bar{W}_{1:\\tau}\\|>u+v\\right)\\operatorname*{min}_{0\\leq\\tau<m}P(\\|\\bar{W}_{\\tau+1:m}\\|\\leq u)\\leq P(\\|\\bar{W}_{1:m}\\|>v-B),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which gives the desired result. ", "page_idx": 17}, {"type": "text", "text": "Next, we study $P(||\\bar{W}_{\\tau+1:m}||>u)$ for any $\\tau=0,\\dots,m-1$ . Note that $P(\\|\\bar{W}_{\\tau+1:m}\\|>u)\\leq$ $P(||\\bar{W}_{\\tau+1:\\tau^{*}}||>u/2)+P(||\\bar{W}_{\\underline{{\\tau}}^{*}+1:m}||>u/2)$ . Thus, without loss of generality, let us focus our analysis on the probability $P(\\|\\bar{W}_{a+1:b}\\|>u)$ , where the corresponding segment does not contain a change point. Assume $b-a=2K B$ . (Notice that for general $a<b$ , we have an additional interval whose length is smaller than $B$ . Hence, a similar analysis below can be used.) We divide the index set $\\{a+1,a+2,\\ldots,b\\}$ into $2K$ consecutive blocks, denoted by $J_{1},\\dots,J_{2K}$ , with equal block size $B$ . Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nP(\\|\\bar{W}_{a+1:b}\\|>u)\\leq P\\left(\\left\\|\\sum_{i=1}^{K}\\bar{F}_{J_{2i-1}}\\right\\|>u/(2B)\\right)+P\\left(\\left\\|\\sum_{i=1}^{K}\\bar{F}_{J_{2i}}\\right\\|>u/(2B)\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\bar{F}_{J_{i}}\\,=\\,\\bar{W}_{J_{i}}/B$ , where $\\textstyle\\sum_{i=1}^{K}{\\bar{F}}_{J_{2i-1}}$ and $\\textstyle\\sum_{i=1}^{K}{\\bar{F}}_{J_{2i}}$ are both sums of independent bounded random variables. Let us ana lyze the second te rm on the RHS. Define ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{G(t)=\\displaystyle\\sum_{i=1}^{K}\\sum_{j\\in J_{2i}}P(p_{j}\\le t)/(K B),}\\\\ &{}&{G_{m}(t)=\\displaystyle\\sum_{i=1}^{K}\\sum_{j\\in J_{2i}}\\mathbf{1}\\{p_{j}\\le t\\}/(K B).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Let $t_{v,L}=G^{-1}(v/L)$ for $v\\,=\\,1,2,\\ldots,L$ . Following the argument in Theorem 7.5.2 of Resnick [2019], we can show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\left\\|\\sum_{i=1}^{K}\\bar{F}_{J_{2i}}\\right\\|=\\left\\|G_{m}-G\\right\\|\\le\\operatorname*{max}_{1\\le v\\le L}|G_{m}(t_{v,L})-G(t_{v,L})|\\vee|G_{m}(t_{v,L}-)-G(t_{v,L}-)|+\\frac{1}{L},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{G_{m}(t-)=\\sum_{i=1}^{K}\\sum_{i\\in J_{2i}}\\mathbf{1}\\{p_{i}<t\\}/(K B)}\\end{array}$ and $G(t-)$ is defined similarly. Thus we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad P\\left(\\left\\|\\displaystyle\\sum_{i=1}^{K}\\bar{F}_{J_{2i}}\\right\\|>u/(2B)\\right)}\\\\ &{\\leq P\\left(\\displaystyle\\operatorname*{max}_{1\\leq v\\leq L}|G_{m}(t_{v,L})-G(t_{v,L})|\\vee|G_{m}(t_{v,L}-)-G(t_{v,L}-)|+1/L>u/(b-a)\\right)}\\\\ &{\\leq\\displaystyle\\sum_{v=1}^{L}P\\left(|G_{m}(t_{v,L})-G(t_{v,L})|\\vee|G_{m}(t_{v,L}-)-G(t_{v,L}-)|>u/(b-a)-1/L\\right)}\\\\ &{\\leq C_{1}L\\exp\\left(-C_{2}K(u/(b-a)-1/L)^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the third inequ\u221aality follows from Hoeffding\u2019s ineq\u221auality. For any $\\epsilon\\,>\\,0$ , we can set $u=$ $C_{3}(b-a)\\sqrt{\\log(K)}/\\sqrt{K}=2C_{3}B\\sqrt{K\\log(K)}$ and $L=\\sqrt{K}$ for some large enough $C_{3}$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nP\\left(\\left\\|\\sum_{i=1}^{K}\\bar{F}_{J_{2i}}\\right\\|>u/(2B)\\right)\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now back to (A.2), set $u\\,=\\,C_{4}\\sqrt{m B\\log(m/B)}$ and $v\\,=\\,C_{5}\\sqrt{m B\\log(m/B)}$ . We can make the RHS of (A.2) arbitrarily small with large enough $C_{4}$ and $C_{5}$ . It thus gives $\\mathrm{max}_{\\tau\\leq m}\\left\\|\\bar{S}_{1:\\tau}\\right\\|=$ $O({\\sqrt{m B\\log(m/B)}})$ and $\\begin{array}{r}{\\operatorname*{sup}_{\\tau}|R(\\tau)|=O\\left(\\sqrt{B\\log(m/B)/m}\\right)}\\end{array}$ . Hence, we deduce that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\tau^{*}-\\hat{\\tau}\\leq O_{p}\\left(\\frac{\\sqrt{m B\\log(m/B)}}{D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "A similar argument applies to the other direction, which gives ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\hat{\\tau}-\\tau^{*}|=O_{p}\\left(\\frac{\\sqrt{m B\\log(m/B)}}{D(F_{0},\\mathbb{E}[F_{\\tau^{*}+1:m}(t)])}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B Additional numerical results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "B.1 The Levenshtein cost ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall that we use $\\nu$ to denote the vocabulary and $\\Xi$ to represent the space of watermark keys. Let $\\nu^{*}$ be the space of strings, where $^*$ can be any positive integer; for example, $\\widetilde{y}_{1:m}\\in\\mathcal{V}^{m}$ . Similarly, we define $\\Xi^{*}$ as the space of watermark key sequences. Given a string $\\widetilde{y}$ , let $\\widetilde{y}_{a}$ :  be the string from the ath token to the end; for example, if $\\widetilde{y}=\\widetilde{y}_{1:m}$ , then $\\widetilde{y}_{2:}=\\widetilde{y}_{2:m}$ . De n ote t h e length of a string $\\widetilde{y}$ as $\\mathtt{l e n}(\\widetilde{y})$ . ", "page_idx": 19}, {"type": "text", "text": "Definition 1 (Simple Levenshtein cost). Let $\\gamma\\in\\mathbb{R}$ and base alignment cost $d_{0}:\\mathcal{V}\\times\\Xi\\to\\mathbb{R}$ . Given a string $\\widetilde{y}\\in\\mathcal{V}^{*}$ and a watermark key sequence $\\xi\\,\\in\\,\\Xi^{*}$ , the simple Levenshtein cost $d_{\\gamma}(\\widetilde{y},\\xi)$ is defined b  y ", "page_idx": 19}, {"type": "equation", "text": "$$\nd_{\\gamma}(\\widetilde{y},\\xi):=\\operatorname*{min}\\left(d_{\\gamma}(\\widetilde{y}_{2:},\\xi_{2:})+d_{0}(y_{1},\\xi_{1}),d_{\\gamma}(\\widetilde{y},\\xi_{2:})+\\gamma,d_{\\gamma}(\\widetilde{y}_{2:},\\xi)+\\gamma\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $d_{\\gamma}(\\widetilde{y},\\xi):=\\gamma\\cdot{\\sf1e n}(\\widetilde{y})\\;i f\\xi$ is empty and $d_{\\gamma}(\\widetilde{y},\\xi):=\\gamma\\cdot1\\mathsf{e n}(\\xi)\\;i f$ $\\widetilde{y}$ is empty. ", "page_idx": 19}, {"type": "text", "text": "We use the following metric to quantify the dependence between the watermark key and the string: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{M}(\\xi_{1:n},\\widetilde{y}_{1:m})=-d_{\\gamma}(\\widetilde{y}_{1:m},\\xi_{1:n}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\gamma=0.4$ , and ", "page_idx": 19}, {"type": "equation", "text": "$$\nd_{0}(\\widetilde{y}_{1},(u_{1},\\pi_{1}))=\\left|u_{1}-\\frac{\\pi_{1}(\\widetilde{y}_{1})-1}{|\\mathcal{V}|-1}\\right|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "in inverse transform sampling method, while ", "page_idx": 19}, {"type": "equation", "text": "$$\nd_{0}(\\widetilde{y}_{1},\\xi_{1})=\\log\\left(1-\\xi_{1,\\widetilde{y}_{1}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "in exponential minimum sampling method. ", "page_idx": 19}, {"type": "text", "text": "B.2 Sequences of $p$ -values from OpenAI-Community/GPT2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure B.1 presents the $p$ -values for 500 text tokens generated from 10 prompts. The language model used is openai-community/gpt2 obtained from https://huggingface.co/ openai-community/gpt2. The $p$ -value sequences are organized into four groups, namely four settings corresponding to the numerical experiments section in the main paper (Section 4): ", "page_idx": 19}, {"type": "text", "text": "\u2022 Setting 1 (no change point): Generate 500 tokens with a watermark. \u2022 Setting 2 (insertion attack): Generate 250 tokens with watermarks, then append with 250 tokens without watermarks. In this setting, there is a single change point at the index 251. \u2022 Setting 3 (substitution attack): Generate 500 tokens with watermarks, then substitute the token with indices ranging from 201 to 300 with non-watermarked text of length 100. In this setting, there are two change points at the indices 201 and 301. \u2022 Setting 4 (insertion and substitution attacks): Generate 400 tokens with watermarks, substitute the token with indices ranging from 101 to 200 with non-watermarked text of length 100, and then insert 100 tokens without watermarks at the index 300. In this setting, there are four change points located at the indices 101, 201, 301, and 401. ", "page_idx": 19}, {"type": "text", "text": "Within each group, the rows represent $p$ -values calculated using four different distance metrics: EMS, EMSL, ITS, and ITSL. It is easy to see that EMS performs the best, followed by EMSL. ", "page_idx": 19}, {"type": "text", "text": "B.3 Results for Facebook/OPT-1.3b ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Figure B.2 shows the results for false discoveries under Setting 1, where the text tokens are all watermarked and generated with facebook/opt-1.3b. A lower threshold leads to fewer false discoveries. ", "page_idx": 19}, {"type": "text", "text": "Figure B.4 shows the boxplots of the Rand index for the four methods under different settings. EMS demonstrates the best performance across all settings, achieving its best performance at a threshold of $\\zeta=0.005$ . ", "page_idx": 19}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/2cb8214d75ec6c400a4fac076e4b9d7b71d950cc16739b08b3d524549c556b67.jpg", "img_caption": ["Figure B.1: Sequences of $p$ -values for the first 10 prompts extracted from the Google C4 dataset for LLM openai-community/gpt2, organized into groups of four consecutive rows, each group corresponding to a distinct setting. Within each group, the rows represent $p$ -values calculated using four different distance metrics: EMS, EMSL, ITS, and ITSL. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/1f8ea2256bbad8afdae72fc5ca46a0d71509407f154a1af1776772aac03ee54c.jpg", "img_caption": ["Figure B.2: The boxplots of the number of false positives with respect to different thresholds $\\zeta$ under Setting 1. The texts are generated using facebook/opt-1.3b, and the four distance metrics are EMS, EMSL, ITS, and ITSL. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/5b9cbc5eb7e818a2dd5a43f44266cb5e1328ddbb3a565f67d2d18d6814ba2bff.jpg", "img_caption": ["Figure B.3: Sequence of $p$ -values for the first 10 prompts extracted from the Google C4 dataset for LLM facebook/opt-1.3b, organized into groups of four consecutive rows, each group corresponding to a distinct setting. Within each group, the rows represent $p$ -values calculated using four different distance metrics: EMS, EMSL, ITS, and ITSL. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/8bd0ffe201f22ea5f3fe2c291cf7b9a632f5b83aa27d60e00f4206cd2846c15c.jpg", "img_caption": [], "img_footnote": ["Figure B.4: The boxplots of the Rand index comparing the clusters identified through the detected change points for texts generated using facebook/opt-1.3b with the true clusters separated by the true change points with respect to different thresholds $\\zeta$ . "], "page_idx": 21}, {"type": "text", "text": "The sequence of $p$ -values from the first 10 prompts extracted from the Google C4 dataset in all settings is presented in Figure B.3. The $p$ -value sequences are organized into four groups, corresponding to the four settings in the numerical experiments section of the main text. Within each group, each row corresponds to one method. ", "page_idx": 22}, {"type": "text", "text": "By comparing with the $p$ -value sequences obtained using openai-community/gpt2, we claim that the segmentation algorithm\u2019s performance depends on the quality of the $p$ -values obtained for each sub-string and this relies on not only the watermark generation schemes but also the language models from which the texts are generated. ", "page_idx": 22}, {"type": "text", "text": "B.4 Results for meta-llama/Meta-Llama-3-8B ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Figure B.6 shows the results for false discoveries under Setting 1, where the text tokens are all watermarked and generated with meta-llama/Meta-Llama-3-8B. A lower threshold leads to fewer false discoveries. ", "page_idx": 22}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/58bea1c8863b3fd3543935505b1bfafda56c12de51a0160043c188aef2495c92.jpg", "img_caption": ["Figure B.5: Sequences of $p$ -values for the first 10 prompts extracted from the Google C4 dataset for LLM meta-llama/Meta-Llama-3-8B, organized into groups of four consecutive rows, each group corresponding to a distinct setting. Within each group, the rows represent $p$ -values calculated using four different distance metrics: EMS, EMSL, ITS, and ITSL. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure B.7 shows the boxplots of the Rand index for the four methods under different settings. EMS demonstrates the best performance across all settings, achieving its best performance at a threshold of $\\zeta=0.005$ . ", "page_idx": 23}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/04c5854d754aad016a619b95871d52d7873823540dfdd68e8aba0ee325771ac4.jpg", "img_caption": ["Figure B.6: The boxplots of the number of false positives with respect to different thresholds $\\zeta$ under Setting 1. The texts are generated using meta-llama/Meta-Llama-3-8B, and the four distance metrics are EMS, EMSL, ITS, and ITSL. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/2467b926bb0e5cbc7e18226ebfdb88b75adb37e33c8fa7ce454022d1ccecbe7a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure B.7: The boxplots of the Rand index comparing the clusters identified through the detected change points for texts generated using meta-llama/Meta-Llama-3-8B with the true clusters separated by the true change points with respect to different thresholds $\\zeta$ . ", "page_idx": 23}, {"type": "text", "text": "B.5 Other settings ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We focus on the Meta-Llama-3-8B model to conduct simulation studies under some more difficult scenarios. Specifically, we increase the number of change points to 4, 8, and 12 and vary the segment lengths accordingly. The results are presented in Figure B.8. For scenarios with 4 and 8 change points, the proposed method successfully identifies all change points. As the number of change points increases, the change point detection problem becomes much more challenging. In a scenario with 12 change points, our method was able to identify 9 of them, showing its robust performance in handling more difficult situations. Generally, the difficulty of a change point detection problem depends on the distance between the change points and the magnitudes of the changes, as indicated by the theoretical results in Proposition 1. ", "page_idx": 23}, {"type": "text", "text": "C Choices of the tuning parameters ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We investigate the impact of the window size $B$ and the number of permutations $T$ on the proposed method. In particular, we focus on the EMS method in simulation Setting 4 of the main text, using the Meta-Llama-3-8B model. ", "page_idx": 23}, {"type": "image", "img_path": "FAuFpGeLmx/tmp/11f5b70a75ba8a686856b0a825ea344938d74e9aee4434f550a3adafd56d0938.jpg", "img_caption": ["Figure B.8: Sequences of p-values obtained using the EMS method across various settings. Change points identified by the proposed method are marked with red dashed lines. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "First, to study the impact of the window size $B$ , we fix $T=999$ and vary the value of $B$ in the set $\\{10,20,30,\\dot{4}0,50\\}$ . Table C.1 shows the rand index value for each setting, with a higher rand index indicating better performance. ", "page_idx": 24}, {"type": "table", "img_path": "FAuFpGeLmx/tmp/c90122ea95d048cf7ed1ae36b09fb018e5c69cacc66d07d8bc846f745a9d19fe.jpg", "table_caption": ["Table C.1: Results for different choices of $B$ when $T=999$ . "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "It is crucial to select an appropriate value for $B$ . If $B$ is too small, the corresponding window may not contain enough data to reliably detect watermarks, as longer strings generally make the watermark more detectable. Conversely, if $B$ is excessively large, it might prematurely shift the detected change point locations, thus reducing the rand index. For instance, let us consider a scenario with 200 tokens where the first 100 tokens are non-watermarked, and the subsequent 100 are watermarked, with the true change point at index 101. Assuming our detection test is highly effective, then it will yield a p-value uniformly distributed over $[0,1]$ over a non-watermarked window and a $\\mathbf{p}$ -value around zero over a window containing watermarked tokens. When $B=50$ , the window beginning at the 76th token contains one watermarked token, which can lead to a small p-value and thus erroneously indicate a watermark from the 76th token onwards. In contrast, if $B=20$ , the window starting at the 91st token will contain the first watermarked token, leading to a smaller error in identifying the change point location. The above phenomenon is the so-called edge effect, which will diminish as $B$ gets smaller. ", "page_idx": 24}, {"type": "text", "text": "The trade-off in the choice of window size is recognized in the time series literature. For instance, a common recommendation for the window size in time series literature is to set $B=C n^{1/3}$ , where $n$ is the sample size, as seen in Corollary 1 of Lahiri [1999]. Based on our experience, setting $B=\\left\\lfloor3n^{1/3}\\right\\rfloor$ (for example, when $n=500$ , $B=23$ ) often results in good finite sample performance. A more thorough investigation of the choice of $B$ is deferred to future research. ", "page_idx": 24}, {"type": "text", "text": "We next examine how our method is affected by the choice of the number of permutations $T$ . To this end, we set $B$ to 20 and consider $T\\in\\{99,\\bar{2}49,499,749,999\\}$ . The results are summarized in Table C.2, including the rand index value and computation times for each setting. As expected, the computation time increases almost linearly with the number of permutations $T$ . We also note that the rand index remains consistent across different values of $T$ , indicating a level of stability in our method. ", "page_idx": 24}, {"type": "table", "img_path": "FAuFpGeLmx/tmp/1a9a99a01c515bd8c6ba0e10639e39ab495a95043ea53faf6be1454ebf78dafe.jpg", "table_caption": ["Table C.2: Results (computational time in hours and rand index values) for different choices of $T$ when $B=20$ . "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We claim our contributions in the abstract and introduction. (i) We rigorously study the Type I and Type II errors of the randomization test for detecting the presence of a watermark. We then apply these findings to the inverse transform and Gumbel watermark schemes. (ii) We develop a systematic statistical method for segmenting texts into watermarked and non-watermarked sub-strings, and we investigate the theoretical and finite sample performance of this methodology, which has yet to be thoroughly explored in recent literature. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have emphasized that the segmentation algorithm\u2019s performance depends crucially on the quality of the simulation-based $p$ -values from each sub-string. Therefore, a powerful watermark detection algorithm is essential for the segmentation procedure\u2019s success. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 25}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In the main text, we formally state essential assumptions for our theoretical results and provide rigorous proof in the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We include all experimental details, such as LLM and hyperparameter values, in the main text and provide code for reproducibility in the supplementary materials. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 26}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have provided the code for reproducibility in the supplementary materials. The code has already been uploaded to GitHub. Due to the double-blind policy, the GitHub repository can\u2019t be accessed now, but we will make it open access if the paper is accepted. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We include all experimental details, such as LLM and hyperparameter values, in the main text and provide code for reproducibility in the supplementary materials. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We do not need to display the error bar in our experimental results; in other words, the experiment results don\u2019t involve an error bar. We do include boxplots as a way to show the performance of the algorithm. The statistical significance is obtained through bootstrap-based test for each and every single change point, and comparisons have been made between performances using different $p$ -value thresholds. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide information on our computer resources in the supplementary materials, specifically, the accompanied README.md file in the reproducing materials. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our research conducted in the paper conforms with the NeurIPS Code of Ethics in every respect. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: In the introduction, we emphasize the importance of distinguishing between human and LLM-produced texts. This is essential to prevent the spread of misleading information, improper use of LLM-based tools in education, model extraction attacks through distillation, and the contamination of training datasets for future language models. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper poses no such risks as no pretrained models are created. The models used in the paper are publicly available and peer-reviewed and thus have a minimum risk for misuse. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Two pretrained language models were used and properly cited, as well as the GNU Parallel used in the experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The code and algorithm are documented properly and provided alongside the code. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]