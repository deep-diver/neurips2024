[{"figure_path": "PKcCHncbzg/figures/figures_1_1.jpg", "caption": "Figure 1: Visualization of relationship attention map m from the well-trained RPN. The degree of attention from low to high is marked by colors from dark blue to red. The more attention, the darker red; the less attention, the darker blue. As the layers deepen, the attention maps exhibit increasingly precise pixel-level semantics. The images correspond to the attention maps for an aeroplane, a sheep, and a train, respectively.", "description": "This figure visualizes the relationship attention maps generated by the Relationship Prompt Network (RPN) for three different classes: an airplane, a sheep, and a train.  The color intensity represents the degree of attention, with darker blue indicating low attention and darker red indicating high attention.  The figure demonstrates how the attention maps become increasingly precise at the pixel level as the network processes information through its layers, highlighting the network's ability to refine its focus on relevant image regions to determine pixel-level semantics.", "section": "1 Introduction"}, {"figure_path": "PKcCHncbzg/figures/figures_1_2.jpg", "caption": "Figure 8: Our method vs existing VLM-based methods.", "description": "This figure compares our proposed method with existing VLM-based methods for open-vocabulary semantic segmentation (OVSS).  Our method directly adapts the Vision-Language Model (VLM) using prompt learning to produce segmentation results without needing additional segmentation-specific networks like mask proposal networks or semantic decoders. Existing methods, conversely, rely on these additional networks, leading to higher training costs and more parameters.  The diagram visually highlights this key difference: our method is simpler, more efficient, and directly uses prompt learning for OVSS, while existing approaches rely on the VLM to guide extra segmentation networks through feature adaptation or knowledge distillation.", "section": "Approach"}, {"figure_path": "PKcCHncbzg/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of RPN. The end-to-end architecture is delineated into four principal components: 1) the frozen plain encoder, which adopts ViT architecture to encode visual knowledge with relationship prompt; 2) the frozen text encoder, which adopts CLIP text encoder architecture to encode class knowledge with text templates; 3) Relationship Prompt Module (RPM), which generates relationship prompt to guide the plain encoder to output pixel-level semantic embeddings; 4) Linear Projection Module (LPM), which consists of two individual linear layers to output OVSS results.", "description": "This figure presents the overall architecture of the Relationship Prompt Network (RPN).  It shows the flow of image and text data through four main components:  a frozen ViT-based encoder for image features enhanced with relationship prompts, a frozen CLIP text encoder for class embeddings, the Relationship Prompt Module (RPM) generating pixel-level prompts, and finally, the Linear Projection Module (LPM) that produces the final open-vocabulary semantic segmentation (OVSS) results. The figure highlights the integration of prompt learning directly within the VLM to achieve OVSS without additional segmentation-specific networks.", "section": "3 Approach"}, {"figure_path": "PKcCHncbzg/figures/figures_3_2.jpg", "caption": "Figure 4: M20E. and denote matrix product and addition.", "description": "This figure illustrates the architecture of the Multi-scale Mixture-of-Experts (M20E) block.  The M20E block is a component of the Relationship Prompt Module (RPM), which is designed to aggregate patch embeddings from different scales.  It consists of several expert networks (Expert 1, Expert 2, ..., Expert n), each processing the input at a different scale.  A gating network dynamically selects the outputs from these experts, based on the input features. The selected expert's output is then passed through additional layers (Up-sample, DW Conv, Interpolation) before being used for the next stage. The figure visually depicts how the gating network routes information to the selected experts according to the input, thereby achieving multi-scale feature aggregation.", "section": "Relationship Prompt Module"}, {"figure_path": "PKcCHncbzg/figures/figures_4_1.jpg", "caption": "Figure 5: ITP and APG. Expand, Einsum and Mul denote expanding class dimension, Hadamard product and Matrix product.", "description": "This figure shows the architecture of the Image-to-Pixel Semantic Attention (ITP) block and the Adaptive Prompt Generation (APG) block.  The ITP block takes the class embeddings and the image embeddings as input and generates a relationship attention map. This map is then used by the APG block to generate a relationship prompt. The relationship prompt is a vector that is used to guide the plain encoder of the Vision-Language Model (VLM) to generate pixel-level semantic embeddings. This is crucial to perform pixel-level semantic segmentation.", "section": "Relationship Prompt Module"}, {"figure_path": "PKcCHncbzg/figures/figures_5_1.jpg", "caption": "Figure 6: Three kinds of LPMs. and denote element-wise product and matrix product.", "description": "This figure presents three different designs for the Linear Projection Module (LPM) used in the Relationship Prompt Network (RPN).  Each design has an image branch and a text branch, each with a linear layer and a normalization layer. The image branch processing remains the same across all three designs. The differences lie in how the text branch processes the last class embedding(s), which are input to the linear layer of the text branch in LPMa, LPMb processes the Hadamard product between the last class embedding and the last [CLS] token before inputting to the linear layer and LPMc concatenates the Hadamard product and the last class embedding before inputting to the linear layer.  The figure highlights the different ways that the image and text branch embeddings are combined to produce the segmentation results.", "section": "3 Approach"}, {"figure_path": "PKcCHncbzg/figures/figures_8_1.jpg", "caption": "Figure 1: Visualization of relationship attention map m from the well-trained RPN. The degree of attention from low to high is marked by colors from dark blue to red. The more attention, the darker red; the less attention, the darker blue. As the layers deepen, the attention maps exhibit increasingly precise pixel-level semantics. The images correspond to the attention maps for an aeroplane, a sheep, and a train, respectively.", "description": "This figure visualizes the relationship attention maps generated by the Relationship Prompt Network (RPN) at different layers.  The color intensity represents the attention level, with darker red indicating higher attention and darker blue indicating lower attention.  The maps show increasing precision in pixel-level semantic information as the layer depth increases.  Three example images (airplane, sheep, and train) are shown alongside their corresponding attention maps.", "section": "1 Introduction"}, {"figure_path": "PKcCHncbzg/figures/figures_14_1.jpg", "caption": "Figure 8: Our method vs existing VLM-based methods", "description": "This figure compares the proposed Relationship Prompt Network (RPN) method with existing VLM-based methods for open-vocabulary semantic segmentation.  The RPN method directly uses a Vision-Language Model (VLM) with prompt learning to produce segmentation results, eliminating the need for additional segmentation-specific networks like mask proposal networks and semantic decoders used in existing methods. The existing methods leverage VLMs but require extra networks and thus are more computationally expensive.  The key difference is that RPN is more straightforward and parameter-efficient, achieving state-of-the-art performance with fewer parameters.", "section": "Approach"}, {"figure_path": "PKcCHncbzg/figures/figures_15_1.jpg", "caption": "Figure 10: Relationship prompt. (a) LoRA. (b) VLM LORA in a common-bypass mode. (c) VLM relationship prompt tuning. (d) Details of A and B. Note that A and B refer to ours IPT and APG.", "description": "This figure illustrates the motivation behind the design of the Relationship Prompt Module (RPM). It compares different prompt tuning methods: (a) the standard LoRA method, (b) a VLM-adapted version of LoRA, (c) the proposed VLM relationship prompt method, and (d) a detailed breakdown of the components (IPT and APG) used in (c). The figure highlights the differences in how these methods interact with the frozen VLM weights, explaining why the RPM approach is more effective for open-vocabulary semantic segmentation.", "section": "Motivation behind the design of RPM"}, {"figure_path": "PKcCHncbzg/figures/figures_19_1.jpg", "caption": "Figure 11: Qualitative analysis. The unseen classes include tree, frisbee, grass and road.", "description": "This figure presents a qualitative comparison of the proposed Relationship Prompt Network (RPN) against a baseline method for open-vocabulary semantic segmentation.  It shows several example images with their corresponding ground truth segmentations, the segmentation results produced by the RPN method, and the segmentation results of the baseline. The comparison highlights the RPN's ability to accurately segment unseen classes (tree, frisbee, grass, and road), demonstrating its effectiveness in open-vocabulary semantic segmentation. The differences in segmentation quality between RPN and baseline underscore the improvements achieved by integrating the Relationship Prompt Module.", "section": "4.3 Ablation Study"}, {"figure_path": "PKcCHncbzg/figures/figures_20_1.jpg", "caption": "Figure 11: Qualitative analysis. The unseen classes include tree, frisbee, grass and road.", "description": "This figure shows a qualitative comparison of the proposed Relationship Prompt Network (RPN) method with a baseline method on the task of open-vocabulary semantic segmentation.  The comparison is made for four images, each containing objects from unseen classes. For each image, there are four sub-images showing: (a) The original image; (b) The ground truth segmentation mask; (c) The segmentation mask produced by the RPN method; (d) The segmentation mask produced by the baseline method.  The color legend identifies the unseen classes.", "section": "4.3 Ablation Study"}]