{"references": [{"fullname_first_author": "J. Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper introduces GPT-4, a large language model whose capabilities are extensively analyzed in the target paper for hallucination detection."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This foundational paper introduces the concept of few-shot learning in language models, a technique central to the understanding and evaluation of LLMs in the target paper."}, {"fullname_first_author": "C. Chen", "paper_title": "INSIDE: LLMs' internal states retain the power of hallucination detection", "publication_date": "2024-00-00", "reason": "This paper proposes a method for hallucination detection using internal states of LLMs, a technique directly compared and contrasted with the proposed LLM-Check method in the target paper."}, {"fullname_first_author": "P. Manakul", "paper_title": "SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models", "publication_date": "2023-00-00", "reason": "This paper introduces SelfCheckGPT, a prominent method for hallucination detection that is used as a key baseline and compared to the proposed method in the target paper."}, {"fullname_first_author": "A. Mishra", "paper_title": "Fine-grained hallucination detection and editing for language models", "publication_date": "2024-00-00", "reason": "This paper introduces a novel dataset (FAVA-Bench) with fine-grained annotations of hallucinations, which is used as a primary dataset for evaluating the hallucination detection methods in the target paper."}]}