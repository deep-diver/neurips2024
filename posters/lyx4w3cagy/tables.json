[{"figure_path": "LYx4w3CAgy/tables/tables_3_1.jpg", "caption": "Table 1: Overview of Hallucination Detection Methods with FAVA, SelfCheckGPT, INSIDE, and RAGTruth. Key- Train Indep: does not require model fine-tuning/training, Single Response: does not require multiple LLM responses, Efficient: compute & memory efficient, Sample Specific: sample-wise instead of population-level detection, Retrieval Indep: does not require RAG or reference databases.", "description": "This table compares several hallucination detection methods (FAVA, SelfCheckGPT, INSIDE, RAGTruth, and LLM-Check) across various criteria.  It highlights whether each method requires model training, multiple responses, is computationally efficient, performs sample-specific or population-level detection, and relies on external retrieval databases.", "section": "1 Introduction"}, {"figure_path": "LYx4w3CAgy/tables/tables_7_1.jpg", "caption": "Table 2: Detection results on the FAVA-Annotation Dataset wherein no External References are available. For methods such as INSIDE and SelfCheckGPT-Prompt, we utilize the multiple model responses generated by GPT-3.", "description": "This table presents the results of hallucination detection experiments on the FAVA-Annotation dataset, focusing on scenarios without external references.  It compares the performance of several methods, including the proposed LLM-Check and baselines such as Self-Prompt, FAVA Model, SelfCheckGPT-Prompt, and INSIDE, across different metrics: AUROC, Accuracy, TPR@5% FPR, and F1 Score.  Note that for the baselines INSIDE and SelfCheckGPT-Prompt, multiple model responses generated by GPT-3 were used.", "section": "5.1 Detection Results on Datasets with no External References as Context"}, {"figure_path": "LYx4w3CAgy/tables/tables_8_1.jpg", "caption": "Table 3: Detection results on the SelfCheckGPT Dataset wherein no External References are available, but when multiple model responses are indeed available. We report the results using the same data-split as the original SelfCheck [Manakul et al., 2023] paper, and thus the number of positive and negative samples are imbalanced in this table: 1392/1908 samples have hallucinations present.", "description": "This table presents the results of hallucination detection experiments using the SelfCheckGPT dataset.  Unlike previous tables, this experiment includes multiple model responses for each prompt, making it a different setting than the zero-resource settings of previous tables.  The table shows that LLM-Check (the proposed method) performs very well compared to other baselines, despite the imbalanced dataset.", "section": "5.1 Detection Results on Datasets with no External References as Context"}, {"figure_path": "LYx4w3CAgy/tables/tables_9_1.jpg", "caption": "Table 4: Detection on the RAGTruth Dataset using Llama-2-7b model in white-box and black-box setting, with the \u201cOverall\u201d column presenting the weighted average results for the black-box models.", "description": "This table presents the results of hallucination detection experiments on the RAGTruth dataset using a Llama-2-7b model in both white-box (access to the internal model activations) and black-box (no access to internal model activations) settings.  Several other LLMs were used for the black-box experiments (Llama-2-13b, Llama-2-70b, GPT-4, Mistral-7b). The table shows the AUROC, accuracy, TPR@5%FPR, and F1 score for different hallucination detection methods (Hidden Score, Logit (Perplexity), Logit (Win Entropy), Logit (Log Entropy), and Attention Score). The \"Overall\" column provides the weighted average of the black-box model results.", "section": "5.2 Detection Results on Datasets with External References"}, {"figure_path": "LYx4w3CAgy/tables/tables_17_1.jpg", "caption": "Table 2: Detection results on the FAVA-Annotation Dataset wherein no External References are available. For methods such as INSIDE and SelfCheckGPT-Prompt, we utilize the multiple model responses generated by GPT-3.", "description": "This table presents the results of hallucination detection experiments conducted on the FAVA-Annotation dataset.  The key characteristic of this dataset is the absence of external references. Several methods were tested, including the proposed LLM-Check and baseline techniques like SelfCheckGPT and INSIDE. For the baseline methods that use multiple model responses, the authors used multiple responses from GPT-3. The table shows AUROC, accuracy, True Positive Rate at 5% False Positive Rate (TPR@5%FPR), and F1 score for different models and metrics.", "section": "5.1 Detection Results on Datasets with no External References as Context"}]