[{"figure_path": "hK7XTpCtBi/figures/figures_2_1.jpg", "caption": "Figure 1: Comparison of the dynamics produced by three variants of OFTRL with different regularizers (negative entropy, logarithmic regularizer, and squared Euclidean norm) and OGDA in the same game As defined in (2) for \u03b4 := 10\u22122. The bottom row shows the duality gap achieved by the last iterates. The OFTRL variants exhibit poor performance due to their lack of forgetfulness, while OGDA converges quickly to the Nash equilibrium. Since the regularizers in the first two plots are Legendre, the dynamics are equivalent to the ones produced by optimistic OMD with the respective Bregman divergences. In the plot for OMWU we observe that xt[1] can get extremely close to the boundary (e.g., in the range 1 \u2013 e\u221250 < x\u207a[1] < 1). To correctly simulate the dynamics, we used 1000 digits of precision. The red star, blue dot, and green square illustrate the key times T1, T2, T3 defined in our analysis in Section 3.", "description": "This figure compares the dynamics of three Optimistic Follow-the-Regularized-Leader (OFTRL) variants with different regularizers (negative entropy, logarithmic, squared Euclidean norm) and Optimistic Gradient Descent-Ascent (OGDA) in a 2x2 zero-sum game.  The top row shows the trajectories of the players' strategies (xt and yt), while the bottom row displays the duality gap over iterations.  The figure highlights that OFTRL algorithms, due to their lack of forgetfulness, exhibit poor last-iterate convergence compared to OGDA, which converges quickly to Nash equilibrium. Key time points (T1, T2, T3) in the convergence process are also marked.", "section": "3 Slow Convergence of OFTRL: A Hard Game Instance"}, {"figure_path": "hK7XTpCtBi/figures/figures_2_2.jpg", "caption": "Figure 2: Performance of OMWU on the game A\u03b4 defined in eq. (2) for three choices of \u03b4. In all plots, the learning rate was set to \u03b7 = 0.1. As predicted by our analysis, the length of the \u201cflat region\u201d between iteration T\u2081 (red star) and T\u2082 (blue dot) scales inversely proportionally with \u03b4.", "description": "This figure shows the performance of the Optimistic Multiplicative Weights Update (OMWU) algorithm on a specific 2x2 zero-sum game. The game is parameterized by delta (\u03b4), which controls the distance of the Nash equilibrium to the boundary of the game's probability simplex.  The plots show the dynamics of the algorithm for three different values of \u03b4 (0.05, 0.01, and 0.005).  The x-axis represents the iteration number, and the y-axis represents the equilibrium gap.  Key observations: a two-phase convergence, a flat region before convergence, and an inverse relationship between the length of the flat region and \u03b4.  This demonstrates how the algorithm's last-iterate convergence rate depends on game parameters, highlighting a potential slow convergence issue.", "section": "3 Slow Convergence of OFTRL: A Hard Game Instance"}, {"figure_path": "hK7XTpCtBi/figures/figures_7_1.jpg", "caption": "Figure 3: Pictorial depiction of the three stages incurred by the OFTRL dynamics in the game A\u03b4 defined in (2). The point z* denotes the unique Nash equilibrium. The times T1 and T2 are shown for concrete instantiations of OFTRL in Figure 1 by a red star and a blue dot, respectively. The times T3 and Th are defined in the proof of Theorem 1 in Appendix B.2.", "description": "This figure shows a pictorial depiction of the three stages incurred by the Optimistic Follow-the-Regularized-Leader (OFTRL) dynamics in the hard game instance (A\u03b4) defined in the paper.  The three stages are: Stage I, where xt[1] increases until it reaches a point close to 1; Stage II, where y\u00b9[1] increases until it reaches a point close to 2(1+\u03b4); and Stage III, where the trajectory spirals toward the Nash equilibrium.  The figure highlights key iterations (T1, T2, T3, Th) used in the proof of Theorem 1 to demonstrate that OFTRL has slow last-iterate convergence.", "section": "3 Slow Convergence of OFTRL: A Hard Game Instance"}, {"figure_path": "hK7XTpCtBi/figures/figures_22_1.jpg", "caption": "Figure 1: Comparison of the dynamics produced by three variants of OFTRL with different regularizers (negative entropy, logarithmic regularizer, and squared Euclidean norm) and OGDA in the same game As defined in (2) for \u03b4 := 10\u22122. The bottom row shows the duality gap achieved by the last iterates. The OFTRL variants exhibit poor performance due to their lack of forgetfulness, while OGDA converges quickly to the Nash equilibrium. Since the regularizers in the first two plots are Legendre, the dynamics are equivalent to the ones produced by optimistic OMD with the respective Bregman divergences. In the plot for OMWU we observe that xt[1] can get extremely close to the boundary (e.g., in the range 1 \u2013 e\u221250 < x\u207a[1] < 1). To correctly simulate the dynamics, we used 1000 digits of precision. The red star, blue dot, and green square illustrate the key times T1, T2, T3 defined in our analysis in Section 3.", "description": "This figure compares the dynamics of three Optimistic Follow-the-Regularized-Leader (OFTRL) variants with different regularizers (negative entropy, logarithmic, and squared Euclidean norm) and Optimistic Gradient Descent Ascent (OGDA) in a 2x2 zero-sum game.  The top row shows the trajectories of the players' strategies (xt[1], yt[1]), while the bottom row displays the duality gap over iterations.  The figure highlights the poor performance of OFTRL algorithms due to their lack of forgetfulness, contrasting with the rapid convergence of OGDA to the Nash equilibrium.", "section": "3 Slow Convergence of OFTRL: A Hard Game Instance"}]