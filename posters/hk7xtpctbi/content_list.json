[{"type": "text", "text": "Fast Last-Iterate Convergence of Learning in Games Requires Forgetful Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yang Cai Gabriele Farina Julien Grand-Cl\u00e9ment Yale MIT HEC Paris   \nyang.cai@yale.edu gfarina@mit.edu grand-clement@hec.fr Christian Kroer Chung-Wei Lee Haipeng Luo Columbia USC USC   \nck2945@columbia.edu leechung@usc.edu haipengl@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Weiqiang Zheng Yale weiqiang.zheng@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Self-play via online learning is one of the premier ways to solve large-scale twoplayer zero-sum games, both in theory and practice. Particularly popular algorithms include optimistic multiplicative weights update (OMWU) and optimistic gradient-descent-ascent (OGDA). While both algorithms enjoy $O(1/T)$ ergodic convergence to Nash equilibrium in two-player zero-sum games, OMWU offers several advantages including logarithmic dependence on the size of the payoff matrix and $\\bar{O}(1/T)$ convergence to coarse correlated equilibria even in generalsum games. However, in terms of last-iterate convergence in two-player zero-sum games, an increasingly popu\u221alar topic in this area, OGDA guarantees that the duality gap shrinks at a rate of $(1/\\sqrt{T})$ , while the best existing last-iterate convergence for OMWU depends on some game-dependent constant that could be arbitrarily large. This begs the question: is this potentially slow last-iterate convergence an inherent disadvantage of OMWU, or is the current analysis too loose? Somewhat surprisingly, we show that the former is true. More generally, we prove that a broad class of algorithms that do not forget the past quickly all suffer the same issue: for any arbitrarily small $\\delta>0$ , there exists a $2\\times2$ matrix game such that the algorithm admits a constant duality gap even after $1/\\delta$ rounds. This class of algorithms includes OMWU and other standard optimistic follow-the-regularized-leader algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Self-play via online learning is one of the premier ways to solve large-scale two-player zero-sum games. Major examples include super-human AIs for Go, Poker [Brown and Sandholm, 2018], and human-level AI for Stratego [Perolat et al., 2022] and alignment of large language models [Munos et al., 2023]. In particular, Optimistic Multiplicative Weights Update (OMWU) and Optimistic Gradient Descent-Ascent (OGDA) are two of the most well-known online learning algorithms. When applied to learning a two-player zero-sum game via self-play for $T$ rounds, the average iterates of both algorithms are known to be an $O(1/T)$ -approximate Nash equilibrium [Rakhlin and Sridharan, 2013, Syrgkanis et al., 2015], while other algorithms, such as vanilla Multiplicative Weights Update (MWU\u221a) and vanilla Gradient Descent-Ascent (GDA), have a slower ergodic convergence rate of $O(1/\\sqrt{T})$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "For multiple practical reasons, there is growing interest in studying the last-iterate convergence of these learning dynamics [Daskalakis and Panageas, 2019, Golowich et al., 2020b, Wei et al., 2021, Lee et al., 2021]. In this regard, existing results seemingly exhibit a gap between OGDA and\u221a OMWU \u2014 the duality gap of the last iterate of OGDA is known to decrease at a rate of $O(1/\\sqrt{T})$ [Cai et al., 2022, Gorbunov et al., 2022], with no dependence on constants beyond the dimension and the smoothness of the players\u2019 utility functions of the game.1 In contrast, the existing convergence rate for OMWU depends on some game-dependent constant that could be arbitrarily large, even after fixing the dimension and the smoothness constant of the game [Wei et al., 2021].2 Given the fundamental role of OMWU in online learning and its other advantages over OGDA (such as its logarithmic dependence on the number of actions), it is natural to ask the following question: ", "page_idx": 1}, {"type": "text", "text": "Main Results. In this work, we show that the answer to this question is yes, contrary to a common belief that better analysis and better last-iterate convergence results similar to those of OGDA are possible for OMWU. More specifically, we show the following. ", "page_idx": 1}, {"type": "text", "text": "Theorem (Informal). For OMWU with constant step size, there is no function $f$ such that the corresponding learning dynamics $\\{(x^{t},y^{t})\\}_{t\\geq1}$ in two-player zero-sum games $[0,1]^{d_{1}\\times d_{2}}$ has $a$ last-iterate convergence rate of $f(d_{1},d_{2},T)$ .3More specifically, no function $f$ can satisfy ", "page_idx": 1}, {"type": "equation", "text": "$\\begin{array}{r}{\\operatorname*{lim}_{T\\rightarrow\\infty}f(d_{1},d_{2},T)\\rightarrow0.}\\end{array}$ ", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Our findings show that, despite the significantly superior regret properties of OMWU compared to OGDA, its last-iterate convergence properties are remarkably worse. In turn, this counters the viewpoint that \u201cFollow-the-Regularized-Leader (FTRL) is better than Online Mirror Descent (OMD)\u201d [van Erven, 2021]: crucially, while OMWU is an instance of (optimistic) FTRL, OGDA is an instance of optimistic OMD that cannot be expressed in the FTRL formalism. ", "page_idx": 1}, {"type": "text", "text": "We further show that similar negative results extend to several other standard online learning algorithms, including a close variant of OGDA. More concretely, our main results are as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We identify a broad family of Optimistic FTRL (OFTRL) algorithms that do not forget about the past quickly. We prove that, for any sufficiently small $\\delta>0$ , there exists a $2\\!\\times\\!2$ two-player zero-sum game such that, even after $1/\\delta$ iterations, the duality gap of the iterate output by these algorithms is still a constant (Theorem 1). This excludes the possibility of showing a game-independent last-iterate convergence rate similar to that of OGDA. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove that many standard online learning algorithms, such as OFTRL with the entropy regularizer (equivalently, OMWU), the Tsallis entropy family of regularizers, the log regularizer, and the squared Euclidean norm regularizer, all fall into this family of non-forgetful algorithms and thus all suffer from the same slow convergence. Also note that Optimistic OMD (OOMD), another well-known family of algorithms, is equivalent to OFTRL when given a Legendre regularizer. Therefore, OOMD with the entropy, Tsallis entropy, and log regularizer also suffer the same issue.4 \u2022 Finally, we also generalize our negative results from $2\\times2$ games to $2n\\times2n$ games for any positive integer $n$ , strengthening our message that forgetfulness is generally needed in order to achieve fast last-iterate convergence. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "hK7XTpCtBi/tmp/d1aa411f139ede7f7f90f32575fb16566c9e71a0135e00ba93ead6c0117f2dc9.jpg", "img_caption": ["Figure 1: Comparison of the dynamics produced by three variants of OFTRL with different regularizers (negative entropy, logarithmic regularizer, and squared Euclidean norm) and OGDA in the same game $A_{\\delta}$ defined in (2) for $\\delta:=10^{=2}$ . The bottom row shows the duality gap achieved by the last iterates. The OFTRL variants exhibit poor performance due to their lack of forgetfulness, while OGDA converges quickly to the Nash equilibrium. Since the regularizers in the first two plots are Legendre, the dynamics are equivalent to the ones produced by optimistic OMD with the respective Bregman divergences. In the plot for OMWU we observe that $x^{\\bar{t}}[1]$ can get extremely close to the boundary (e.g., in the range $\\bar{1}-e^{-50}<x^{t}[1]<1)$ . To correctly simulate the dynamics, we used 1000 digits of precision. The red star, blue dot, and green square illustrate the key times $T_{1},T_{2},T_{3}$ defined in our analysis in Section 3. "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "hK7XTpCtBi/tmp/6d6988721145c6670fbbed0878f0d189a7b4988ce6e1d6bacd6c5acc61731b54.jpg", "img_caption": ["Figure 2: Performance of OMWU on the game $A_{\\delta}$ defined in eq. (2) for three choices of $\\delta$ . In all plots, the learning rate was set to $\\eta=0.1$ . As predicted by our analysis, the length of the \u201cflat region\u201d between iteration $T_{1}$ (red star) and $T_{2}$ (blue dot) scales inversely proportionally with $\\delta$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Main Ideas. Intuitively, we trace the poor last-iterate convergence properties of OFTRL to its lack of forgetfulness. The high-level idea of our hard $2\\times2$ game instance, parametrized by $\\delta>0$ , is as follows. First, it has a unique Nash equilibrium at which one player is $\\bar{O}(\\delta)$ close to the boundary of the simplex. We refer to the first row of plots in Figure 1, where the equilibrium is noted by a blue dot (note that we can plot only $x[1],y[1]$ for each player, since $x[2]=1-x[1]$ and $y[2]=1-y[2];$ . As can be seen, the iterates of OGDA and all three OFTRL variants initially have a two-phase structure. In the first phase, they converge to the lower-right area denoted by a red star in Figure 1. Then, from there all algorithms start moving towards the equilibrium. In particular, $y[1]$ increases. However, once they enter the vicinity of the equilibrium, the behavior depends on the algorithms. For OGDA, the dynamics start spiraling closer and closer to the equilibrium. On the other hand, for the OFTRL algorithm, the $x$ player has built up a lot of \u201cmemory\" of $x[1]$ being better than $x[2]$ , and for this reason, $x[1]$ will stay very close to 1 for a long time. During the time when $x[1]$ is close to 1, y[1] keeps increasing since the $y$ player receives gradients that indicates $y[1]$ is better than $y[2]$ . As a result, the dynamics cannot \u201cstop\u201d near the equilibrium but start to move away from the equilibrium. The dynamics reach a point (denoted by a green square) whose duality gap is a constant and enter a new cycle where they move out towards the starting point of the learning process. This cycle repeats in smaller and smaller semi-ellipses that slowly converge to equilibrium. Note that the semi-ellipses correspond to the seesaw pattern in the equilibrium gap (second row of plots). OFTRL overshoots the equilibrium as it has built up a lot of \"memory\" of $x[1]$ being better than $x[2]$ along the phase from the red star to the blue circle, and it requires many iterations to \"forget\" this fact. We show that as we make $\\delta$ , the parameter defining the nearness to the boundary, smaller and smaller, it takes longer and longer for these semi-ellipses to get close to the equilibrium along the entire path, as illustrated in Figure 2. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Our results are related to numerical observations made in the literature on solving large-scale extensiveform games. There, algorithms based on the regret-matching+ $(\\mathbf{R}\\mathbf{M}^{+})$ algorithm [Tammelin et al., 2015], combined with the counterfactual regret minimization [Zinkevich et al., 2007], perform by far the best in practice. In contrast, the classical regret matching algorithm [Hart and Mas-Colell, 2000] performs much worse, in spite of similar regret guarantees. It was later discovered that $\\mathrm{RM}^{+}$ corresponds to OGD, while RM corresponds to FTRL [Farina et al., 2021, Flaspohler et al., 2021]. It was hypothesized that RM builds up too much negative regret at times, and thus is slow to adapt to changes in the learning dynamics related to the strategy of the other player. These numerical results and the hypothesis are consistent with our theoretical findings: FTRL (and thus RM) is not able to \u201cforget,\u201d whereas OGD and OGDA can forget and thereby quickly adapt to changes in which actions should be played. ", "page_idx": 3}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The literature on last-iterate convergence of online learning methods in games is vast. In this section, we will cover key contributions focusing on the case of interest for this paper: discrete-time dynamics for two-player zero-sum normal-form games. ", "page_idx": 3}, {"type": "text", "text": "Convergence of OGDA. Average-iterate convergence of OGDA has been studied for minimax optimization problems in both the unconstrained [Mokhtari et al., 2020] and constrained settings [Hsieh et al., 2019]. Last-iterate convergence of OGDA in unconstrained saddle-point problems has been shown in [Daskalakis et al., 2018, Golowich et al., 2020a]. In the (constrained) game setting, Wei et al. [2021], Anagnostides et al. [2022] showed best-iterate convergence to the set of Nash equilibri\u221aa in any two-player zero-sum game with payoff matrix $A$ at a rate of $O(\\mathrm{poly}(d_{1},d_{2},\\operatorname*{max}_{i,j}|A_{i,j}|)/\\sqrt{T})$ using constant learning rate, where $d_{1}$ and $d_{2}$ are the number of actions of the players. A stronger result was shown by Cai et al. [2022], who showed that the same rate applies to the last iterate. ", "page_idx": 3}, {"type": "text", "text": "Convergence of OMWU. Optimistic multiplicative weights update (also known as optimistic hedge) is often regarded as the premier algorithm for learning in games. Unlike OGDA, it guarantees sublinear regret with a logarithmic dependence on the number of actions, and it is known to guarantee only polylogarithmic regret per player when used in self play even for general-sum games [Daskalakis et al., 2021]. It can be applied with similar strong properties beyond normal-form games in several important combinatorial settings [Takimoto and Warmuth, 2003, Koolen et al., 2010, Farina et al., 2022]. The work by Daskalakis and Panageas [2019] established asymptotic last-iterate convergence for OMWU in games using a small learning rate under the assumption of a unique Nash equilibrium. Similar asymptotic results without the unique equilibrium assumption were also given by Mertikopoulos et al. [2019], Hsieh et al. [2021]. Wei et al. [2021] were the first to provide nonasymptotic learning rates for OMWU. Specifically, they showed a linear rate of convergence in games with a unique equilibrium, albeit with a dependence on a condition number-like quantity that could be arbitrarily large given fixed $d_{1}$ , $d_{2}$ , and $\\operatorname*{max}_{i,j}|A_{i,j}|$ .This result was later extended by Lee et al. [2021] to extensive-form games. Unlike OGDA, no last-iterate convergence result for OMWU with a polynomial dependence on only the natural parameters of the game (i.e., $d_{1}$ , $d_{2}$ , and $\\operatorname*{max}_{i,j}|A_{i,j}|)$ is known. As we show in this paper, perhaps surprisingly, this is no coincidence: in general, OMWU does not exhibit a last-iterate convergence rate that solely depends on these parameters, whether polynomial or not. ", "page_idx": 3}, {"type": "text", "text": "FTRL vs. OMD. While the last-iterate convergence of instantiations of Optimistic Online Mirror Descent has been observed before, the properties of Follow-the-Regularized-Leader dynamics remain mostly elusive. The present paper partly explains this vacuum: all standard instantiations of optimistic FTRL cannot hope to converge in iterates with only a polynomial dependence on the natural parameters of the game, unlike optimistic OMD. Complications in obtaining last-iterate convergence results for continuous-time FTRL instantiations were already reported by Vlatakis-Gkaragkounis et al. [2020], who showed the necessity of strict Nash equilibria. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Exploiting a no-regret learner. The forgetfulness property that we identify is closely related to the concept of mean-based learning algorithms from Braverman et al. [2018]. Intuitively, mean-based algorithms are ones such that if the mean reward for action $a$ is significantly greater than the mean reward for action $b$ , then the algorithm selects $b$ with negligible probability. They show that MWU is mean-based, along with Follow-the-Perturbed-Leader and the Exp3 bandit algorithm. Braverman et al. [2018] shows that \"mean-based\" algorithms are exploitable when learning to bid in first-price auctions, whereas Kumar et al. [2024] shows that OGD does not suffer from this exploitability issue. ", "page_idx": 4}, {"type": "text", "text": "2 Preliminaries and Problem Setup ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We consider the standard setting of no-regret learning in a zero-sum game $A\\,\\in\\,[0,1]^{d_{1}\\times d_{2}}$ . In each iteration $t\\geq1$ , the $x$ -player chooses $x^{t}\\in\\mathcal{X}:=\\Delta^{d_{1}}$ while the $y$ -player chooses $y^{t}\\in\\mathcal{V}:=$ $\\Delta^{d_{2}}$ . Then the $x$ -player receives loss vector $\\ell_{x}^{t}\\;=\\;A y^{t}$ while the $y$ -player receives loss vector $\\ell_{y}^{t}\\,=\\,-A^{\\top}x^{t}$ . The goal is to find or approximate a Nash equilibrium $(x^{*},y^{*})$ to the game such that $x^{*}\\,\\in\\,\\mathrm{argmin}_{x\\in{\\mathcal X}}\\,\\mathrm{max}_{y\\in{\\mathcal Y}}\\,x^{\\top}A y$ and $y^{*}\\,\\in\\,\\mathrm{argmax}_{y\\in\\mathcal{Y}}\\,\\mathrm{min}_{x\\in\\mathcal{X}}\\,x^{\\top}A y$ . The approximation error of a strategy pair $(x,y)$ is measured by its duality gap, defined as Duality $\\operatorname{Gap}(x,y)\\,=$ $\\begin{array}{r}{\\operatorname*{max}_{y^{\\prime}\\in\\mathcal{Y}}x^{\\top}A y^{\\prime}-\\operatorname*{min}_{x^{\\prime}\\in\\mathcal{X}}x^{\\prime^{\\top}}A y}\\end{array}$ , which is always non-negative. ", "page_idx": 4}, {"type": "text", "text": "Popular no-regret algorithms for solving the game include the Optimistic Follow-the-RegularizedLeader (OFTRL) algorithm and the Optimistic Online Mirror Descent (OOMD) algorithm, both defined in terms of a certain regularizer $R\\,:\\,\\Delta^{d}\\,\\rightarrow\\,\\mathbb{R}$ (for some general dimension $d$ ). The corresponding Bregman divergence of $R$ is $D_{R}(x,x^{\\prime})=R(x)-R(x^{\\prime})\\stackrel{\\cdot}{-}\\langle\\nabla R(x^{\\prime}),x-x^{\\prime}\\rangle$ , and the regularizer is 1-strongly convex if $\\begin{array}{r}{D_{R}(x,x^{\\prime})\\geq\\frac{1}{2}\\|x-x^{\\prime}\\|_{2}^{2}}\\end{array}$ for all $x,x^{\\prime}\\in\\Delta^{d}$ . ", "page_idx": 4}, {"type": "text", "text": "Optimistic Online Mirror Descent (OOMD) Starting from an initial point $(x^{1},y^{1})=({\\widehat{x}}^{1},{\\widehat{y}}^{1})$ , the OOMD algorithm with regularizer $R$ and steps size $\\eta>0$ updates in each iteration $t\\geq2$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{x}^{t}=\\underset{x\\in\\mathcal{X}}{\\mathrm{argmin}}\\{\\eta\\langle x,\\ell_{x}^{t-1}\\rangle+D_{R}(x,\\widehat{x}^{t-1})\\},\\quad x^{t}=\\underset{x\\in\\mathcal{X}}{\\mathrm{argmin}}\\{\\eta\\langle x,\\ell_{x}^{t-1}\\rangle+D_{R}(x,\\widehat{x}^{t})\\},}\\\\ &{\\widehat{y}^{t}=\\underset{y\\in\\mathcal{Y}}{\\mathrm{argmin}}\\{\\eta\\langle y,\\ell_{y}^{t-1}\\rangle+D_{R}(y,\\widehat{y}^{t-1})\\},\\quad y^{t}=\\underset{y\\in\\mathcal{Y}}{\\mathrm{argmin}}\\{\\eta\\langle y,\\ell_{y}^{t-1}\\rangle+D_{R}(y,\\widehat{y}^{t})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In particular, we call OOMD with a squared Euclidean norm regularizer, that is, $\\begin{array}{r}{R(x)=\\frac{1}{2}\\sum_{i=1}^{d}x[i]^{2}}\\end{array}$ optimistic gradient-descent-ascent (OGDA). When $R$ is the negative entropy, that is, ${\\dot{R}}({\\bar{x}})\\,=$ $\\textstyle\\sum_{i=1}^{d}x[i]\\log x[i]$ , we call the resulting OOMD algorithm optimistic multiplicative weights update (OMWU). OGDA and OMWU have been extensively studied in the literature regarding their lastiterate convergence properties in zero-sum games. Specifically, both OMWU and OGDA guarantee that $(x^{t},y^{t})$ approaches to a Nash equilibrium as $t\\to\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "Optimistic Follow-the-Regularized-Leader (OFTRL) Define the cumulative loss vectors $L_{x}^{t}:=$ $\\textstyle\\sum_{k=1}^{t}\\ell_{x}^{k}$ and $\\begin{array}{r}{L_{y}^{t}:=\\sum_{k=1}^{t}\\ell_{y}^{k}}\\end{array}$ . The update rule of OFTRL with regularizer $R$ is for each $t\\geq1$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x^{t}=\\underset{x\\in\\mathcal{X}}{\\operatorname{argmin}}\\left\\{\\left\\langle x,L_{x}^{t-1}+\\ell_{x}^{t-1}\\right\\rangle+\\frac{1}{\\eta}R(x)\\right\\},}}\\\\ {{\\displaystyle y^{t}=\\underset{y\\in\\mathcal{Y}}{\\operatorname{argmin}}\\left\\{\\left\\langle y,L_{y}^{t-1}+\\ell_{y}^{t-1}\\right\\rangle+\\frac{1}{\\eta}R(y)\\right\\}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Throughout the paper, we consider the following regularizers: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Negative entropy $\\begin{array}{r}{(R(x)\\,=\\,\\sum_{i=1}^{d}x[i]\\log x[i])}\\end{array}$ : the resulting OFTRL algorithm coincides with OMWU defined by the OO MD framework previously. \u2022 Squared Euclidean norm $\\begin{array}{r}{(R(\\underline{{x}})=\\frac{1}{2}\\sum_{i=1}^{d}x[i]^{2})}\\end{array}$ : note that the resulting algorithm is different from OGDA since the squared Euclidean norm is not a Legendre regularizer. As we will show, the two algorithms behave very differently in terms of last-iterate convergence. \u2022 Log barrier $\\begin{array}{r}{(R(x)=\\sum_{i=1}^{d}-\\log(x[i]))}\\end{array}$ : we also call it the log regularizer. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Negative Tsallis entropy regularizers $\\begin{array}{r}{(R(x)=\\frac{1-\\sum_{i=1}^{d}(x[i])^{\\beta}}{1-\\beta}}\\end{array}$ parameterized by $\\beta\\in(0,1)$ ). ", "page_idx": 5}, {"type": "text", "text": "The 2-dimension case We denote $\\boldsymbol{x}\\in\\mathbb{R}^{2}$ as $x=[x[1],x[2]]^{\\top}$ . For $d_{1}=2$ , finding $x^{t}$ of OFTRL reduces to the following 1-dimensional optimization problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\nx^{t}[1]=\\operatorname*{argmin}_{x\\in[0,1]}\\left\\{x\\cdot(L_{x}^{t-1}[1]+\\ell_{x}^{t-1}[1]-L_{x}^{t-1}[2]-\\ell_{x}^{t-1}[2])+\\frac{1}{\\eta}R(x)\\right\\},\\quad x^{t}[2]=1-x^{t}[1],\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we slightly abuse the notation and denote $R(x)=R([x,1-x])$ for $x\\in[0,1]$ . We introduce two notations (the case for the $y$ -player is similar): let $e_{x}^{t}=\\ell_{x}^{t}[1]-\\ell_{x}^{t}[2]$ be the difference between tohf et lhoes tsewso  oaf ctthioe ntsw. oF aocrt iOonFsT, RanL,d $\\begin{array}{r}{E_{x}^{t}=\\sum_{k=1}^{t}e_{x}^{k}}\\end{array}$ hbe e utphde acteu mofu ivoen ldyi fdfeerpeenncdes  boent wtheee nd itfhfee rleonscseess $x^{t}$   \n$E_{x}^{t-1},e_{x}^{t-1}$ , the step size $\\eta$ , and the regularizer $R$ . For this reason, we define $F_{\\eta,R}:\\mathbb{R}\\rightarrow[0,1]$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{\\eta,R}(e):=\\operatorname*{argmin}_{x\\in[0,1]}\\left\\{x\\cdot e+\\frac{1}{\\eta}R(x)\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We assume the function $F_{\\eta,R}$ is well-defined, i.e., the above optimization problem admits a unique solution in [0, 1]. This is a condition easily satisfied, for example, when the regularizer $R$ is strongly convex. Then the OFTRL algorithm can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\nx^{t}[1]=F_{\\eta,R}(E_{x}^{t-1}+e_{x}^{t-1}),\\quad x^{t}[2]=1-x^{t}[1].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The following lemma shows that the function $F_{\\eta,R}$ is non-increasing (we defer missing proofs in the section to Appendix A). ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Monotonicity of $F_{\\eta,R}$ ). The function $F_{\\eta,R}(\\cdot):\\mathbb{R}\\rightarrow[0,1]$ defined in (1) is non-increasing. ", "page_idx": 5}, {"type": "text", "text": "We present some blanket assumptions on the regularizer, which are satisfied by all the regularizers introduced before. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. We assume that the regularizer $R$ satisfies the following properties: the function $F_{\\eta,R}:\\mathbb{R}\\rightarrow[0,1]$ defined in (1) is, ", "page_idx": 5}, {"type": "text", "text": "1. Unbiased: $\\begin{array}{r}{F_{\\eta,R}(0)=\\frac{1}{2}}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "2. Rational: $\\begin{array}{r}{\\operatorname*{lim}_{E\\to-\\infty}F_{\\eta,R}(E)=1\\,a n d\\operatorname*{lim}_{E\\to+\\infty}F_{\\eta,R}(E)=0.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "3. Lipschitz continuous: There exists $L\\geq0$ such that $F_{1,R}$ is $L$ -Lipschitz. ", "page_idx": 5}, {"type": "text", "text": "Item 1 in Assumption 1 shows that the initial strategy is the uniform distribution over the two actions, which is standard in practice. The rational assumption (item 2 in Assumption 1) is natural since otherwise, the algorithm could not even converge to a pure Nash equilibrium. The Lipschitzness (item 3 in Assumption 1) is implied when the regularizer is strongly convex over $[0,1]^{2}$ (see Lemma 4), and it further implies Lipschitzness of $F_{\\eta,R}$ for any $\\eta$ as shown in the following proposition. ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. The function $F_{\\eta,R}$ satisfies $F_{\\eta,R}(E/\\eta)=F_{1,R}(E)$ . If $F_{1,R}$ is $L$ -Lipschitz, then $F_{\\eta,R}$ is $\\eta L$ -Lipschitz for any $\\eta>0$ . ", "page_idx": 5}, {"type": "text", "text": "3 Slow Convergence of OFTRL: A Hard Game Instance ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We give negative results on the last-iterate convergence properties of OFTRL by studying its behavior on a surprisingly simple $2\\!\\times\\!2$ two-player zero-sum games. The game\u2019s loss matrix $A_{\\delta}$ is parameterized by $\\delta\\in\\dot{(0,\\frac{1}{2})}$ and is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{\\delta}:=\\left[\\!\\!\\begin{array}{c c}{{\\frac{1}{2}+\\delta}}&{{\\frac{1}{2}}}\\\\ {{0}}&{{1}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.1 Basic Properties ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We summarize some useful properties of $A_{\\delta}$ in the following proposition (missing proofs of this section can be found in Appendix B). ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. The matrix game $A_{\\delta}$ satisfies: ", "page_idx": 6}, {"type": "text", "text": "2. For a strategy pair $(x^{t},y^{t})$ , the loss vectors (i.e., gradients) for the two players are respectively: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell_{x}^{t}=A_{\\delta}y^{t}=\\left[\\!\\!\\begin{array}{c}{\\frac{1}{2}+\\delta y^{t}[1]}\\\\ {1-y^{t}[1]}\\end{array}\\!\\!\\right]\\quad\\ell_{y}^{t}=-A_{\\delta}^{\\top}x^{t}=-\\left[\\!\\!\\begin{array}{c}{(\\frac{1}{2}+\\delta)x^{t}[1]}\\\\ {1-\\frac{1}{2}x^{t}[1]}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Moreover, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle e_{x}^{t}=\\ell_{x}^{t}[1]-\\ell_{x}^{t}[2]=-\\frac{1}{2}+(1+\\delta)y^{t}[1]\\in[-\\frac{1}{2},\\frac{1}{2}+\\delta]}\\\\ {\\displaystyle e_{y}^{t}=\\ell_{y}^{t}[1]-\\ell_{y}^{t}[2]=1-(1+\\delta)x^{t}[1]\\in[-\\delta,1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In particular, we notice that $e_{y}^{t}\\geq-\\delta$ . It implies that if the cumulative differences between the losses of the two actions $E_{y}^{t}$ is large, then it takes $\\Omega({\\textstyle{\\frac{1}{\\delta}}})$ iterations to make $E_{y}^{t}$ small (close to 0). This has important implications for non-forgetful algorithms like OFTRL that look at the whole history of losses. Since OFTRL chooses the strategy $\\bar{y}^{t}$ based on $E_{y}^{t}$ , it could be trapped in a bad action for a long time even if the current gradients suggest that the other action is better. This is the key observation for our main negative results on the slow last-iterate convergence rates of OFTRL. ", "page_idx": 6}, {"type": "text", "text": "The following lemma shows that in a particular region of $(x,y)$ , the duality gap is a constant. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2. Let $\\delta,\\epsilon\\in(0,\\frac{1}{2})$ . For any $x,y\\in\\Delta^{2}$ such that $\\begin{array}{r}{x[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ and $y[1]\\ge\\frac{1}{2}+\\epsilon,$ the duality gap of $(x,y)$ for game $A_{\\delta}$ (defined in (2)) satisfies DualityGap $\\,(x,y)\\geq\\epsilon$ . ", "page_idx": 6}, {"type": "text", "text": "3.2 Slow Last-Iterate Convergence ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We further require the following assumption on the regularizer $R$ (and thus the function $F_{1,R})$ . ", "page_idx": 6}, {"type": "text", "text": "Assumption 2. Let $L$ be the Lipschitness constant of $F_{1,R}$ in Assumption 1. Denote constant $\\begin{array}{r}{c_{1}=\\frac{1}{2}-F_{1,R}(\\frac{1}{20L})}\\end{array}$ . There exist universal constants $\\delta^{\\prime},c_{2}>0$ and $c_{3}\\in(0,\\frac{1}{2}]$ such that for any $0<\\delta\\bar{\\le}\\delta^{\\prime}$ , ", "page_idx": 6}, {"type": "text", "text": "Although Assumption 2 is technical, the idea is simple. Item 1 in Assumption 2 states that if a loss difference $E<0$ already makes $\\begin{array}{r}{F_{1,R}(E)\\ge\\frac{1}{1+\\delta}}\\end{array}$ , then the loss difference $\\begin{array}{r}{\\dot{E^{\\prime}}=E-\\Omega(\\frac{1}{\\delta})}\\end{array}$ is able to make $F_{1,R}(E^{\\prime})$ greater than $F_{1,R}(E)$ by a margin of $\\Omega(\\delta)$ . Item 2 in Assumption 2 states that if a loss difference $E$ already makes $\\begin{array}{r}{F_{1,R}(E)\\ge\\frac{1}{2(1+\\delta)}\\approx\\frac{1}{2}}\\end{array}$ , then the loss difference $E^{\\prime}=E-\\Omega(1)$ is able to make $F_{1,R}(E^{\\prime})$ greater than $\\frac{1}{2}$ by a constant margin $c_{2}$ . In Appendix C, we verify that Assumption 2 holds for the negative entropy, squared Euclidean norm, the log barrier, and the negative Tsallis entropy regularizers. ", "page_idx": 6}, {"type": "text", "text": "Now we present the main result of the section showing that even after $\\Omega(1/\\delta)$ iterations, the duality gap of the iterate output by OFTRL is still a constant. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Assume the regularizer $R$ satisfies Assumption $^{l}$ and Assumption 2. For any $\\delta\\in(0,\\hat{\\delta})$ , where $\\hat{\\delta}$ is a constant depending only on the constants $c_{1}$ and $\\delta^{\\prime}$ defined in Assumption 2, the OFTRL dynamics on $A_{\\delta}$ (defined in (2)) with any step size $\\begin{array}{r}{\\eta\\leq\\frac{1}{4L}}\\end{array}$ satisfies the following: there exists an iteration $\\begin{array}{r}{t\\geq\\frac{c_{1}}{3\\eta L\\delta}}\\end{array}$ with a duality gap of at least $c_{2}$ , a strictly positive constant defined in Assumption 2. ", "page_idx": 6}, {"type": "text", "text": "Proof Sketch: We decompose the analysis into three stages as illustrated in Figure 3. We describe the three stages and the high-level ideas of our proof below and defer the full proof to Appendix B.2. ", "page_idx": 6}, {"type": "image", "img_path": "hK7XTpCtBi/tmp/e460634e645fcddc14284cab0bb843e6bc1f3191256917573fe892df6a761d78.jpg", "img_caption": ["Figure 3: Pictorial depiction of the three stages incurred by the OFTRL dynamics in the game $A_{\\delta}$ defined in (2). The point $z^{*}$ denotes the unique Nash equilibrium. The times $T_{1}$ and $T_{2}$ are shown for concrete instantiations of OFTRL in Figure 1 by a red star and a blue dot, respectively. The times $T_{s}$ and $T_{h}$ are defined in the proof of Theorem 1 in Appendix B.2. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "\u2022 Stage I $[1,T_{1}-1]$ : Recall that $x^{1}[1]=y^{1}[1]={\\textstyle{\\frac{1}{2}}}$ by Assumption 1. We show that $x^{t}[1]$ increases and denote $T_{1}$ the first iteration that $\\begin{array}{r}{x^{T_{1}}[1]\\,\\ge\\,\\frac{1}{1+\\delta}}\\end{array}$ . During the time $[1,T_{1}\\mathrm{~-~}1]$ , since $x^{t}$ [1] is always smaller than $\\frac{1}{1+\\delta}$ , we know from Proposition 2 action 1 has larger loss than action 2 for the $y$ -player, i.e., $e_{y}^{t}=\\ell_{y}^{t}[1]-\\ell_{y}^{t}[2]\\geq0$ . Thus $y^{t}$ [1] decreases during stage I and we show that $\\begin{array}{r}{y^{T_{1}}[1]\\leq\\frac{1}{2}-c_{1}}\\end{array}$ with $c_{1}$ defined in Assumption 2. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Stage II $[T_{1},T_{2}]$ : Recall that $\\begin{array}{r}{y^{T_{1}}[1]\\,\\le\\,\\frac{1}{2}\\,-\\,c_{1}}\\end{array}$ . We define $T_{2}>T_{1}$ as the first iteration where $\\begin{array}{r}{y^{T_{2}}[1]\\ge\\frac{1}{2(1+\\delta)}>\\frac{1}{2}-c_{1}}\\end{array}$ . We remark that for $y^{t}[1]$ to increase, the loss vector must satisfy $e_{y}^{t}<0$ However, the game matrix $A_{\\delta}$ guarantees that $e_{u}^{t}\\ge-\\delta$ no matter what the $x$ -player\u2019s strategy is (Proposition 2). Thus by the $\\eta L$ -Lipschitzness of ${{F}_{\\eta,R}}$ (Proposition 1), the per-iteration increase in $y^{t}[1\\bar{]}$ is at most $\\eta L\\delta$ . Therefore, we know $\\begin{array}{r}{T_{2}-T_{1}=\\Omega(\\frac{c_{1}}{\\eta L\\delta})}\\end{array}$ . As a result, $e_{x}^{t}<0$ during $[T_{1},T_{2}]$ and the cumulative loss for the $x$ -player decreases to $\\begin{array}{r}{E_{x}^{T_{2}}\\stackrel{\\cdot}{\\leq}E_{x}^{T_{1}}-\\Omega\\big(\\frac{1}{\\eta L\\delta}\\big)}\\end{array}$ . Recall $\\begin{array}{r}{x^{T_{1}}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ . Thus $x^{T_{2}}[1]>x^{T_{1}}[1]$ is much closer to 1. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Stage III $[T_{2},T_{3}]$ : Recall that $\\begin{array}{r}{y^{T_{2}}[1]\\geq\\frac{1}{2(1+\\delta)}}\\end{array}$ . Moreover, $y^{t}[1]$ could keep increasing if $x^{t}[1]\\geq$ $\\textstyle{\\frac{1}{1+\\delta}}$ since that implies $e_{y}^{t}\\leq0$ . Now the question is how long would the $x$ -player stay close to the boundary, i.e, $\\begin{array}{r}{x^{t}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ . Since OFTRL-type algorithms are not forgetful, this happens only when $E_{x}^{t}\\geq E_{x}^{T_{1}}$ (recall $\\begin{array}{r}{x^{T_{1}}[1]\\ge\\frac{1}{1+\\delta})}\\end{array}$ . But we have at the end of stage II, $\\begin{array}{r}{E_{x}^{T_{2}}\\leq E_{x}^{T_{1}}-\\Omega\\big(\\frac{1}{\\eta L\\delta}\\big)}\\end{array}$ Since the per-iteration loss is bounded by 1, it requires at least $\\Omega\\big(\\frac{1}{\\eta L\\delta}\\big)$ iterations to cancel the cumulative loss of $\\Omega\\big(\\frac{1}{\\eta L\\delta}\\big)$ . Define $\\begin{array}{r}{T_{3}\\,=\\,T_{2}+\\Omega(\\frac{1}{\\eta L\\delta})}\\end{array}$ . During $[T_{2},T_{3}]$ , the $y$ -player always receives loss such that $e_{y}^{t}\\leq0$ and we prove that in the end $y^{T_{3}}[1]\\ge\\frac{1}{2}+c_{2}$ for some constant $c_{2}$ . ", "page_idx": 7}, {"type": "text", "text": "\u2022 Conclusion: Finally, we get one iteration $\\begin{array}{r}{T_{3}\\geq\\Omega(\\frac{1}{\\eta L\\delta})}\\end{array}$ with $\\begin{array}{r}{x^{T_{3}}[1]\\geq\\frac{1}{1+\\delta}}\\end{array}$ and $y^{T_{3}}[1]\\ge\\frac{1}{2}+c_{2}$ . Using Lemma 2, the duality gap of $(x^{T_{3}},y^{T_{3}})$ is at least $c_{2}>0$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 immediately implies the following (proof deferred to Appendix B.3). ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. For optimistic FTRL with any regularizer satisfying Assumption 1 and Assumption 2 and constant steps size $\\begin{array}{r}{\\eta\\leq\\frac{1}{4L}}\\end{array}$ ( $L$ is defined in Assumption $^{\\,l}$ ), there is no function $f$ such that the corresponding learning dynamics $\\{(x^{t},y^{t})\\}_{t\\geq1}$ in two-player zero-sum games $[0,1]^{d_{1}\\times d_{2}}$ has $a$ last-iterate convergence rate of $f(d_{1},d_{2},T)$ . More specifically, no function $f$ can satisfy ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 and Theorem 2 provide impossibility results for getting a last-iterate convergence rate for OFTRL that solely depends on the bounded parameters, even in two-player zero-sum games. Moreover, they show the necessity of forgetfulness for fast last-iterate convergence in games since OGDA has a last-iterate convergence rate of $\\cal O\\big(\\frac{\\mathrm{poly}(d_{1},d_{2})}{\\sqrt{T}}\\big)$ [Cai et al., 2022, Gorbunov et al., 2022]. ", "page_idx": 8}, {"type": "text", "text": "4 Extension to Higher Dimensions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we extend our negative results from $2\\times2$ matrix games to games with higher dimensions. We start by showing an equivalence result for a single player (say, the first player). We assume that a decision maker is using OFTRL with a 1-strongly convex (w.r.t. the $\\ell_{2}$ norm) and separable regularizer $R(x)=R_{1}(x_{1})+R_{2}(x_{2})$ to choose decisions. At a given time time $t$ , they see a loss $\\ell^{t}\\in[0,1]^{2}$ . ", "page_idx": 8}, {"type": "text", "text": "Now consider the following $2n$ -dimensional decision problem: The player uses OFTRL using the regularizer $\\begin{array}{r}{\\hat{R}(\\hat{x})=\\sum_{i=1}^{n}R_{1}(\\hat{x}_{i})+\\sum_{i=n+1}^{2n}R_{2}(\\hat{x}_{i})}\\end{array}$ i2=nn+1 R2(x\u02c6i), i.e., they use R1 on the first half of actions, and $R_{2}$ on the secon d half. This is aga in a 1-strongly convex regularizer (w.r.t. the $\\ell_{2}$ norm). Suppose the decision maker sees the rescaled and duplicated version of the losses $\\ell^{1},\\ldots,\\ell^{T}$ from the 2- dbiasmeedn soino nthale  craesgeu: $\\begin{array}{r}{\\hat{\\ell}_{i}^{t}=\\frac{1}{n^{\\alpha}}\\ell_{1}^{t}}\\end{array}$ if $i\\leq n$ , and $\\begin{array}{r}{\\hat{\\ell}_{i}^{t}=\\frac{1}{n^{\\alpha}}\\ell_{2}^{t}}\\end{array}$ if $i>n$ . The parameter $\\alpha$ will be chosen later ", "page_idx": 8}, {"type": "text", "text": "Now we wish to show that by choosing $\\alpha$ in the right way, we get that the decisions for the 2-dimensional and $2n$ -dimensional OFTRL algorithms are equivalent. Let $x^{1},\\ldots,x^{T}$ be the 2- dimensional OFTRL decisions, and let $\\hat{x}^{1},\\ldots,\\bar{\\hat{x}}^{T}$ be the $2n$ -dimensional OFTRL decisions. Then, we want to show that $\\textstyle\\sum_{i=1}^{n}{\\hat{x}}_{i}^{t}=x^{t}[1]$ and $\\begin{array}{r}{\\sum_{i=n+1}^{2n}\\hat{x}_{i}^{t}=x^{t}[2]}\\end{array}$ for all $t$ . ", "page_idx": 8}, {"type": "text", "text": "Lemma 3. Let the losses $\\hat{\\ell}^{1},\\dots,\\hat{\\ell}^{T}$ satisfy the duplication procedure given in the preceding paragraph. Then for any time $t$ , we have $\\hat{x}_{1}^{t}=\\cdot\\cdot\\cdot=\\hat{x}_{n}^{t}$ and $\\hat{x}_{n+1}^{t}=\\cdots=\\hat{x}_{2n}^{t}$ . ", "page_idx": 8}, {"type": "text", "text": "Proof. Suppose not and let ${\\hat{x}}^{t}$ be the corresponding solution. Then the optimal solution is such that $\\hat{x}_{i}^{t}\\neq\\hat{x}_{k}^{t}$ for some $i,k$ both less than $n$ , or both greater than $n$ . But then, by symmetry, we have that there is more than one optimal solution to the OFTRL optimization problem at time $t$ : the objective is exactly the same if we create a new solution where we swap the values of $\\hat{x}_{i}^{t}$ and $\\hat{x}_{k}^{t}$ . This is a contradiction due to strong convexity. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "From lemma 3, we have that the OFTRL decision problem in $2n$ dimensions can equivalently be written as a 2-dimensional decision problem: Since the first $n$ entries must be the same, we can simply optimize over that one shared value, say $x^{t}[1]$ , which we use for all $n$ entries, and similarly we use $x^{\\bar{t}}[2]$ for the second half of the entries. Let $\\mathrm{\\dot{Dupl}:}\\Delta^{2}\\rightarrow\\Delta^{2n}$ be a function that maps the two-dimensional solution into the corresponding duplicated $2n$ -dimensional solution. The equivalent 2-dimensional problem is then: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{x}^{t}=\\mathrm{Dupl}\\left[\\underset{x\\in\\frac{1}{n}\\cdot\\Delta^{2}}{\\mathrm{argmin}}\\left\\lbrace\\frac{n}{n^{\\alpha}}\\left\\langle x,\\sum_{\\tau=1}^{t-1}\\ell^{\\tau}+\\ell^{t-1}\\right\\rangle+\\frac{n}{\\eta}R_{1}(x[1])+\\frac{n}{\\eta}R_{2}(x[2])\\right\\rbrace\\right]}\\\\ &{\\quad=\\mathrm{Dupl}\\left[\\frac{1}{n}\\cdot\\underset{x\\in\\Delta^{2}}{\\mathrm{argmin}}\\left\\lbrace\\frac{n}{n^{\\alpha}}\\left\\langle\\frac{1}{n}x,\\sum_{\\tau=1}^{t-1}\\ell^{\\tau}+\\ell^{t-1}\\right\\rangle+\\frac{n}{\\eta}R(x/n)\\right\\rbrace\\right]}\\\\ &{\\quad=\\mathrm{Dupl}\\left[\\frac{1}{n}\\cdot\\underset{x\\in\\Delta^{2}}{\\mathrm{argmin}}\\left\\lbrace\\left\\langle x,\\sum_{\\tau=1}^{t-1}\\ell^{\\tau}+\\ell^{t-1}\\right\\rangle+\\frac{n^{\\alpha+1}}{\\eta}R(x/n)\\right\\rbrace\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The next theorem shows that we can choose $\\alpha$ for different regularizers and construct $2n\\times2n$ loss matrices whose learning dynamics are equivalent to the learning dynamics in $2\\times2$ games given in the preceding sections. We defer the proof to Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3. For any loss matrix $A\\in[0,1]^{2\\times2}$ , there exists a loss matrix $\\hat{A}\\in[0,n^{-\\alpha}]^{2n\\times2n}$ such that for the Euclidean $(\\alpha\\,=1,$ ), entropy $\\mathrm{\\Delta}\\alpha=0$ ), Tsallis ${'}\\beta\\,\\in\\,(0,1)$ and $\\alpha=-1+\\beta)$ , and log ( $\\alpha=-1$ ) regularizers, the resulting OFTRL learning dynamics are equivalent in the two games. ", "page_idx": 8}, {"type": "text", "text": "Combining Theorem 1 and Theorem 3, we have the following: ", "page_idx": 9}, {"type": "text", "text": "Corollary 1. In the same setup as Theorem 3, under Assumption 1 and Assumption 2, there exists a game matrix $\\hat{A}_{\\delta}\\in[0,n^{-\\alpha}]^{2n\\times2n}$ such that the OFTRL learning dynamics with any step size $\\begin{array}{r}{\\eta\\leq\\frac{1}{4L}}\\end{array}$ satisfies the following: there exists an iteration $\\begin{array}{r}{t\\geq\\frac{c_{1}}{3\\eta L\\delta}}\\end{array}$ with a duality gap at least $c_{2}n^{-\\alpha}$ . ", "page_idx": 9}, {"type": "text", "text": "Since $\\alpha=0$ for the entropy regularizer, the same results hold more generally for games where one player has more actions than the other. In particular, we can create a $2n\\times2m$ game such that the resulting dynamics are equivalent to those in a $2\\times2$ game. This does not work for the Euclidean and log regularizers because the rescaling factors would be different for the row and column players. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study last-iterate convergence rates of OFTRL algorithms with various popular regularizers, including the popular OMWU algorithm. Our main results show that even in simple $2\\times2$ two-player zero-sum games parametrized by $\\delta>0$ , the lack of forgetfulness of OFTRL leads to the duality gap remaining constant even after $1/\\delta$ iterations (Theorem 1). As a corollary, we show that the last-iterate convergence rate of OFTRL must depend on a problem-dependent constant that can be arbitrarily bad (Theorem 2). This highlights a stark contrast with OOMD algorithms: while OGDA with constant step size achieves a $\\scriptstyle{\\bar{O}}({\\frac{1}{\\sqrt{T}}})$ last-iterate convergence rate, such a guarantee is impossible for OMWU or more generally OFTRL. We now discuss several interesting questions regarding the convergence guarantees of learning in games and leave them as future directions. ", "page_idx": 9}, {"type": "text", "text": "Best-Iterate Convergence Rates While we focus on the last-iterate (i.e., DualityG $\\operatorname{xp}(x^{T},y^{T}))$ , the weaker notion of best-iterate $(i.e.,\\,\\operatorname*{min}_{t\\in[T]}\\mathrm{\\,Duality}\\mathrm{\\mathrm{Gap}}(x^{t},y^{t}))$ is also of both practical and theoretical interest. By definition, we know the best-iterate convergence rate is at least as good as the last-iterate convergence rate and could be much faster. This raises the following question: ", "page_idx": 9}, {"type": "text", "text": "What is the best-iterate convergence rate of OMWU/OFTRL? ", "page_idx": 9}, {"type": "text", "text": "To our knowledge, there are no concrete results on the best-iterate convergence rates of OMWU or other OFTRL algorithms. It is thus interesting to extend our negative results to the best-iterate convergence rates or develop fast best-iterate convergence rates of OMWU/OFTRL. ", "page_idx": 9}, {"type": "text", "text": "Dynamic Step Sizes Our negative results hold for OFTRL with fixed step sizes. We conjecture that the slow last-iterate convergence of OFTRL persists even with dynamic step sizes. In particular, we believe our counterexamples still work for OFTRL with decreasing step sizes. This is because decreasing the step size makes the players move even slower, and they may be trapped in the wrong direction for a longer time due to the lack of forgetfulness. In Appendix E, we include numerical results for OMWU with adaptive stepsize akin to Adagrad [Duchi et al., 2011], which supports our intuition. We observe the same cycling behavior as for fixed step size. While the cycle is smaller than that of fixed step sizes, the dynamics take more steps to finish each cycle. Investigating the effect of dynamic step sizes on last-iterate convergence rates is an interesting future direction. ", "page_idx": 9}, {"type": "text", "text": "Slow Convergence due to Lack of Forgetfulness Our work shows that various OFTRL-type algorithms do not have fast last-iterate convergence rates for learning in games. Our proof and hard game instance build on the intuition that these algorithms lack forgetfulness: they do not forget the past quickly. This intuition is also utilized in [Panageas et al., 2023]. In particular, they give an $d\\times d$ potential game where the last-iterate convergence rate of the Fictitious Play algorithm, which is equivalent to the Follow-the-Leader (FTL) algorithm, suffers exponential dependence in the dimension $d$ . One natural future direction is to formalize the intuition of non-forgetfulness further and give a general condition for algorithms under which they suffer slow last-iterate convergence. It is also interesting to show other lower-bound results for learning in games. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the anonymous reviewers for their constructive comments on improving the paper. Yang Cai was supported by the NSF Awards CCF-1942583 (CAREER) and CCF-2342642. Christian Kroer was supported by the Office of Naval Research awards N00014-22-1-2530 and N00014-23-1-2374, and the National Science Foundation awards IIS-2147361 and IIS-2238960. Julien Grand-Cl\u00e9ment was supported by Hi! Paris and Agence Nationale de la Recherche (Grant 11-LABX-0047). Haipeng Luo was supported by NSF award IIS-1943607. Weiqiang Zheng was supported by the NSF Awards CCF-1942583 (CAREER), CCF-2342642, and a Research Fellowship from the Center for Algorithms, Data, and Market Design at Yale (CADMY). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On last-iterate convergence beyond zero-sum games. In International Conference on Machine Learning, pages 536\u2013581. PMLR, 2022. ", "page_idx": 10}, {"type": "text", "text": "James P Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 321\u2013338, 2018. ", "page_idx": 10}, {"type": "text", "text": "Mark Braverman, Jieming Mao, Jon Schneider, and Matt Weinberg. Selling to a no-regret buyer. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 523\u2013538, 2018. ", "page_idx": 10}, {"type": "text", "text": "Noam Brown and Tuomas Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418\u2013424, 2018. ", "page_idx": 10}, {"type": "text", "text": "Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Finite-time last-iterate convergence for learning in multi-player games. In Advances in Neural Information Processing Systems (NeurIPS), 2022. ", "page_idx": 10}, {"type": "text", "text": "Yun Kuen Cheung and Georgios Piliouras. Vortices instead of equilibria in minmax optimization: Chaos and butterfly effects of online learning in zero-sum games. In Conference on Learning Theory, pages 807\u2013834. PMLR, 2019. ", "page_idx": 10}, {"type": "text", "text": "Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in min-max optimization. Advances in neural information processing systems (NeurIPS), 2018. ", "page_idx": 10}, {"type": "text", "text": "Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In 10th Innovations in Theoretical Computer Science Conference (ITCS), 2019. ", "page_idx": 10}, {"type": "text", "text": "Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. In International Conference on Learning Representations (ICLR), 2018. ", "page_idx": 10}, {"type": "text", "text": "Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. Advances in Neural Information Processing Systems (NeurIPS), 2021. ", "page_idx": 10}, {"type": "text", "text": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011. ", "page_idx": 10}, {"type": "text", "text": "Gabriele Farina, Christian Kroer, and Tuomas Sandholm. Faster game solving via predictive blackwell approachability: Connecting regret matching and mirror descent. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 5363\u20135371, 2021. ", "page_idx": 10}, {"type": "text", "text": "Gabriele Farina, Chung-Wei Lee, Haipeng Luo, and Christian Kroer. Kernelized multiplicative weights for 0/1-polyhedral games: Bridging the gap between learning in extensive-form and normal-form games. In International Conference on Machine Learning (ICML), pages 6337\u20136357, 2022. ", "page_idx": 10}, {"type": "text", "text": "Genevieve E Flaspohler, Francesco Orabona, Judah Cohen, Soukayna Mouatadid, Miruna Oprescu, Paulo Orenstein, and Lester Mackey. Online learning with optimism and delay. In International Conference on Machine Learning, pages 3363\u20133373. PMLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. Advances in neural information processing systems (NeurIPS), 2020a.   \nNoah Golowich, Sarath Pattathil, Constantinos Daskalakis, and Asuman Ozdaglar. Last iterate is slower than averaged iterate in smooth convex-concave saddle point problems. In Conference on Learning Theory (COLT), 2020b.   \nEduard Gorbunov, Adrien Taylor, and Gauthier Gidel. Last-iterate convergence of optimistic gradient method for monotone variational inequalities. In Advances in Neural Information Processing Systems, 2022.   \nSergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):1127\u20131150, 2000.   \nYu-Guan Hsieh, Franck Iutzeler, J\u00e9r\u00f4me Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. Advances in Neural Information Processing Systems, 32, 2019.   \nYu-Guan Hsieh, Kimon Antonakopoulos, and Panayotis Mertikopoulos. Adaptive learning in continuous games: Optimal regret bounds and convergence to nash equilibrium. In Conference on Learning Theory, pages 2388\u20132422. PMLR, 2021.   \nWouter M Koolen, Manfred K Warmuth, Jyrki Kivinen, et al. Hedging structured concepts. In COLT, pages 93\u2013105. Citeseer, 2010.   \nRachitesh Kumar, Jon Schneider, and Balasubramanian Sivan. Strategically-robust learning algorithms for bidding in first-price auctions. In Proceedings of the 2024 ACM Conference on Economics and Computation, 2024.   \nChung-Wei Lee, Christian Kroer, and Haipeng Luo. Last-iterate convergence in extensive-form games. Advances in Neural Information Processing Systems, 34:14293\u201314305, 2021.   \nHaipeng Luo. Lecture note 2, Introduction to Online Learning. 2022. URL https://haipeng-luo. net/courses/CSCI659/2022_fall/lectures/lecture2.pdf.   \nPanayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. In Proceedings of the twenty-ninth annual ACM-SIAM symposium on discrete algorithms, pages 2703\u20132717. SIAM, 2018.   \nPanayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In International Conference on Learning Representations (ICLR), 2019.   \nAryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of $\\mathcal{O}(1/k)$ for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. SIAM Journal on Optimization, 30(4):3230\u20133251, 2020.   \nR\u00e9mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 2023.   \nIoannis Panageas, Nikolas Patris, Stratis Skoulakis, and Volkan Cevher. Exponential lower bounds for fictitious play in potential games. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=tkenkPYkxj.   \nJulien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego with model-free multiagent reinforcement learning. Science, 378(6623):990\u2013996, 2022.   \nSasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. Advances in Neural Information Processing Systems, 2013. ", "page_idx": 11}, {"type": "text", "text": "Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of regularized learning in games. Advances in Neural Information Processing Systems (NeurIPS), 2015. ", "page_idx": 12}, {"type": "text", "text": "Eiji Takimoto and Manfred K Warmuth. Path kernels and multiplicative updates. The Journal of Machine Learning Research, 4:773\u2013818, 2003. ", "page_idx": 12}, {"type": "text", "text": "Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit texas hold\u2019em. In Twenty-fourth international joint conference on artificial intelligence, 2015. ", "page_idx": 12}, {"type": "text", "text": "Tim van Erven. Why FTRL is better than online mirror descent. https://www.timvanerven.nl/ blog/ftrl-vs-omd/, 2021. Accessed: 2024-05-22. ", "page_idx": 12}, {"type": "text", "text": "Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, Thanasis Lianeas, Panayotis Mertikopoulos, and Georgios Piliouras. No-regret learning and mixed nash equilibria: They do not mix. Advances in Neural Information Processing Systems, 33:1380\u20131391, 2020. ", "page_idx": 12}, {"type": "text", "text": "Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In International Conference on Learning Representations (ICLR), 2021. ", "page_idx": 12}, {"type": "text", "text": "Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. Advances in neural information processing systems, 20, 2007. ", "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Introduction 1   \n1.1 Related Work 4   \nPreliminaries and Problem Setup 5   \n3 Slow Convergence of OFTRL: A Hard Game Instance 6   \n3.1 Basic Properties 6   \n3.2 Slow Last-Iterate Convergence 7   \nExtension to Higher Dimensions 9   \n5 Conclusion and Discussions 10   \nMissing Proofs in Section 2 14   \nA.1 Proof of Lemma 1 . 14   \nA.2 Proof of Proposition 1 14   \nB Missing Proofs in Section 3 14   \nB.1 Proof of Lemma 2 14   \nB.2 Proof of Theorem 1 14   \nB.3 Proof of Theorem 2 17   \nVerifying Assumption 2 for Different Regularizers 18   \nC.1 Negative Entropy 18   \nC.2 Squared Euclidean Norm Regularizer 19   \nC.3 Log Barrier 19   \nC.4 Negative Tsallis Entropy 20 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "D Proof of Theorem 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "22 ", "page_idx": 13}, {"type": "text", "text": "E Numerical Experiments with Adaptive Stepsizes ", "page_idx": 13}, {"type": "text", "text": "22 ", "page_idx": 13}, {"type": "text", "text": "A Missing Proofs in Section 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Let $e_{1}<e_{2}$ . Denote $x_{1}=F_{\\eta,R}(e_{1})$ and $x_{2}=F_{\\eta,R}(e_{2})$ . By definition, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\ne_{2}(x_{2}-x_{1})\\leq{\\frac{1}{\\eta}}(R(x_{1})-R(x_{2}))\\leq e_{1}(x_{2}-x_{1}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $e_{1}<e_{2}$ , we have $x_{2}\\leq x_{1}$ . ", "page_idx": 13}, {"type": "text", "text": "A.2 Proof of Proposition 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. By definition, ", "page_idx": 13}, {"type": "equation", "text": "$$\nF_{\\eta,R}\\bigg(\\frac{E}{\\eta}\\bigg)=\\underset{x\\in[0,1]}{\\mathrm{argmin}}\\,\\bigg\\{x\\cdot\\frac{E}{\\eta}+\\frac{1}{\\eta}R(x)\\bigg\\}=\\underset{x\\in[0,1]}{\\mathrm{argmin}}\\,\\big\\{x\\cdot E+R(x)\\big\\}=F_{1,R}(E).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The second claim on the Lipschitzness follows directly. ", "page_idx": 13}, {"type": "text", "text": "B Missing Proofs in Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. We have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\mathrm{DualityGap}(x,y)=\\underset{\\Tilde{y}\\in\\Delta^{2}}{\\operatorname*{max}}x^{\\top}A_{\\delta}\\Tilde{y}-\\underset{\\Tilde{x}\\in\\Delta^{2}}{\\operatorname*{min}}\\Tilde{x}^{\\top}A_{\\delta}y}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{=(\\frac{1}{2}+\\delta)x[1]-(1-y[1])}&&{~~~~~~~~(x[1]\\geq\\frac{1}{1+\\delta},\\epsilon}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~\\geq\\frac{1}{2}\\frac{1+2\\delta}{1+\\delta}-\\frac{1}{2}+\\epsilon}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\\\ &{>\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof. Recall that $\\begin{array}{r l r}{c_{1}}&{{}=}&{\\frac{1}{2}\\;-\\;F_{1,R}\\big(\\frac{1}{20L}\\big)}\\end{array}$ defined in Assumption 2. We fix any $\\delta\\:<$ min{ 115, c61 , 3cc010, . Since $\\delta\\:<\\:\\delta^{\\prime}$ , Assumption 2 holds. We will prove that there exists an iteration $\\begin{array}{r}{t\\geq\\frac{c_{1}}{3\\eta L\\delta}}\\end{array}$ with duality gap $c_{2}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof Plan: We decompose the analysis into three stages as shown in Figure 3. Below, we describe the three stages and the high-level ideas in our proof. ", "page_idx": 13}, {"type": "text", "text": "\u2022 Stage I: Recall that $x^{1}[1]=y^{1}[1]={\\textstyle{\\frac{1}{2}}}$ . In Stage I, we show that $x^{t}[1]$ will increase and denote $T_{1}\\geq1$ the first iteration where $\\begin{array}{r}{x^{t}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ . The existence of $T_{1}$ can be proved by contradiction (Claim 1). Since before the end of Stage I, $\\begin{array}{r}{x^{t}[1]<\\frac{1}{1+\\delta}}\\end{array}$ , the loss vector for the $y$ -player satisfies $e_{y}^{t}\\,=\\,\\ell_{y}^{t}[1]\\,-\\,\\ell_{y}^{t}[2]\\,\\geq\\,0$ meaning action 1 is worse than action 2. We will prove that finally $\\begin{array}{r}{y^{T_{1}}[1]\\leq\\frac{1}{2}-c_{1}}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "\u2022 Stage II: Now we have that $y^{T_{1}}[1]\\;\\leq\\;{\\frac{1}{2}}\\;-\\;c_{1}$ , we denote $T_{2}~>~T_{1}$ the first iteration where $\\begin{array}{r}{y^{T_{2}}[1]\\ge\\frac{1}{2(1+\\delta)}>\\frac{1}{2}-c_{1}}\\end{array}$ . We remark that in order to increase $y^{t}[1]$ , the loss vector must satisfy $e_{y}^{t}<0$ . However, the game matrix $A_{\\delta}$ guarantees that $e_{y}^{t}\\ge-\\delta$ no matter what the $x$ -player is playing. Thus by the $\\eta L$ -Lipschitzness of $F_{\\eta,R}$ (Lemma 4), the increase in $y^{t}[1]$ is at most $\\eta L\\delta$ . Therefore, we know $\\begin{array}{r}{\\dot{T_{2}}-\\bar{T_{1}}=\\Omega(\\frac{c_{1}}{\\eta L\\delta})}\\end{array}$ . But during $[T_{1},T_{2}]$ , for the $x$ -player, we have $e_{x}^{t}<0$ which implies its cumulative loss $\\begin{array}{r}{E_{x}^{\\dot{T}_{2}}\\leq E_{x}^{T_{1}}-\\Omega\\big(\\frac{1}{\\eta L\\delta}\\big)}\\end{array}$ . In other words, $x^{t}[1]$ is very close to 1 and the cumulative loss for action 1 is much smaller than that of action 2. ", "page_idx": 14}, {"type": "text", "text": "\u2022 Stage III: Now we have $\\begin{array}{r}{y^{T_{2}}[1]\\geq\\frac{1}{2\\left(1+\\delta\\right)}}\\end{array}$ and that $y^{t}[1]$ could keep increasing if $\\begin{array}{r}{x^{t}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ since then the loss satisfies $e_{y}^{t}\\leq0$ . Now the question is how long would the $x$ -player stay close to the boundary, i.e, $\\begin{array}{r}{x^{t}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ . Since OFTRL-type algorithms are not forgetful, this happens only when $E_{x}^{t}\\geq E_{x}^{T_{1}}$ (recall $\\begin{array}{r}{x^{T_{1}}[1]\\ge\\frac{1}{1+\\delta})}\\end{array}$ . But we have at the end of stage II, $\\begin{array}{r}{E_{x}^{T_{2}}\\leq E_{x}^{T_{1}}-\\Omega\\big(\\frac{1}{\\eta L\\delta}\\big)}\\end{array}$ . Since $e_{x}^{t}$ is bounded by a constant, we know $\\begin{array}{r}{x^{t}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ even after $\\Omega\\big(\\frac{1}{\\eta L\\delta}\\big)$ iterations. Define $\\begin{array}{r}{T_{3}=T_{2}+\\Omega(\\frac{1}{\\eta L\\delta})}\\end{array}$ . During $[T_{2},T_{3}]$ , the $y$ -player always receives loss such that $e_{y}^{t}\\leq0$ and we prove that $y^{T_{3}}[\\dot{1}]\\ge\\frac{1}{2}+c_{2}$ for some constant $c_{2}$ . ", "page_idx": 14}, {"type": "text", "text": "\u2022 Conclusion Finally we get one iteration $\\begin{array}{r}{T_{3}\\geq\\Omega(\\frac{1}{\\eta L\\delta})}\\end{array}$ with $\\begin{array}{r}{x^{T_{3}}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ and $y^{T_{3}}[1]\\ge\\frac{1}{2}+c_{2}$ , Using Lemma 2, the duality gap of $(x^{T_{3}},y^{T_{3}})$ is at least $c_{2}$ . ", "page_idx": 14}, {"type": "text", "text": "Stage I: We know $x^{1}[1]=y^{1}[1]={\\textstyle{\\frac{1}{2}}}$ . We define (i) $T_{s}>1$ to be the smallest iteration such that $x^{T_{s}}[1]\\ge\\frac{3}{4}$ and (ii) $T_{1}>T_{s}$ to be the smallest iteration such that $\\begin{array}{r}{x^{T_{1}}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ . Both $T_{s}$ and $T_{1}$ must exist, and the reason will become clear in the following analysis. We postpone the proof of this fact in Claim 1 at the end of this paragraph. ", "page_idx": 14}, {"type": "text", "text": "Notice from Proposition 2, the difference $e_{x}^{t}$ is lower bounded: $e_{x}^{t}\\geq-\\frac{1}{2}$ for any $t$ . Thus $E_{x}^{t-1}+$ $e_{x}^{t-1}\\geq-\\frac{t}{2}$ for any $t\\geq1$ . Since $x^{T_{s}}[1]\\ge\\frac{3}{4}>\\frac{1}{2}$ , we know that $E_{x}^{T_{s}-1}{\\bar{+}}\\,e_{x}^{T_{s}-1}<0$ exTs\u22121< 0. As F\u03b7,R is $\\eta L$ -Lipschitz, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{4}\\leq x^{T_{s}}[1]-x^{1}[1]\\leq\\eta L\\cdot\\big|E_{x}^{T_{s}-1}+e_{x}^{T_{s}-1}\\big|\\leq\\frac{L\\eta T_{s}}{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This implies ", "page_idx": 14}, {"type": "equation", "text": "$$\nT_{s}\\geq\\frac{1}{2\\eta L}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $\\begin{array}{r}{x^{t}[1]<\\frac{3}{4}}\\end{array}$ for all $1\\,\\le\\,t\\,\\le\\,T_{s}\\,-\\,1$ , we know that $e_{y}^{t}=\\ell_{y}^{t}[1]-\\ell_{y}^{t}[2]=1-(1+\\delta)x^{t}[1]>$   \n$\\begin{array}{r}{\\frac{1-3\\delta}{4}\\geq\\frac{1}{5}}\\end{array}$ $\\begin{array}{r}{\\delta\\leq\\frac{1}{15})}\\end{array}$ ftohre  adlli $1\\leq t\\leq T_{s}-1$ t.  leMaosrt r,f ofro ra lall $1\\leq t\\leq T_{1}-1$ r, ewme akinnso wn otnh-ant $e_{y}^{t}\\geq0$ $\\begin{array}{r}{x^{t}[1]\\leq\\frac{1}{1+\\delta}}\\end{array}$ $e_{y}^{t}$ $1/5$ $t\\leq T_{s}-1$   \nfor all $t\\in[T_{s},T_{1}-1]$ , we can conclude that for all $T_{s}\\leq t\\leq T_{1}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\ny^{t}[1]=F_{\\eta,R}(E_{y}^{t-1}+e_{y}^{t-1})\\leq F_{\\eta,R}(E_{y}^{t-1}),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and moreover ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{F_{\\eta,R}(E_{y}^{t-1})\\leq F_{\\eta,R}\\bigg(\\frac{T_{s}-1}{5}\\bigg)}&{{}\\qquad\\qquad\\qquad}\\\\ {\\leq F_{\\eta,R}\\bigg(\\frac{1}{20L\\eta}\\bigg)}&{{}\\qquad\\qquad\\qquad}&{{}(T_{s}-1\\geq\\frac{1}{2\\eta L}-1\\geq\\frac{1}{4L\\eta})}\\\\ {=\\frac{1}{2}-c_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This completes the proof of Stage I, where $\\begin{array}{r}{x^{T_{1}}[1]\\ge\\frac{1}{1+\\delta}}\\end{array}$ and $\\begin{array}{r}{y^{T_{1}}[1]\\leq\\frac{1}{2}-c_{1}}\\end{array}$ . Before we proceed to the next stage, we prove the existence of $T_{s}$ and $T_{1}$ . ", "page_idx": 14}, {"type": "text", "text": "Claim 1. $T_{s}$ and $T_{1}$ exist. ", "page_idx": 14}, {"type": "text", "text": "Proof. It suffices to prove that $T_{1}$ exists as it implies the existence of $T_{s}$ . Assume for the sake of contradiction that $T_{1}$ does not exist, i.e., $\\begin{array}{r}{x^{t}[1]<\\frac{\\bar{1}}{1+\\delta}}\\end{array}$ for all $t\\geq1$ . By the same analysis as for Stage I, we get $y^{t}[1]\\leq\\textstyle{\\frac{1}{2}}-c_{1}$ for all $\\begin{array}{r}{t\\geq\\frac{1}{2\\eta L}}\\end{array}$ . This implies $\\begin{array}{r}{e_{x}^{t}=-\\frac{1}{2}+(1+\\delta)y^{t}[1]\\leq\\frac{\\delta}{2}-c_{1}\\leq-\\frac{c_{1}}{2}}\\end{array}$ for ", "page_idx": 14}, {"type": "text", "text": "all $\\begin{array}{r}{t\\geq\\frac{1}{2\\eta L}}\\end{array}$ . Then $E_{x}^{t}\\!+\\!e_{x}^{t}\\to-\\infty$ as $t\\to+\\infty$ . As a consequence, $x^{t}[1]=F_{\\eta,R}(E_{x}^{t-1}\\!+\\!e_{x}^{t-1})\\rightarrow1$ as t \u2192+\u221eby item 2 in Assumption 1. But this contradicts with the assumption that xt[1] <11+\u03b4 for all $t\\geq1$ . This completes the proof. ", "page_idx": 15}, {"type": "text", "text": "Stage II We define ", "text_level": 1, "page_idx": 15}, {"type": "equation", "text": "$$\nT:=\\left\\lfloor\\frac{c_{1}}{2L\\eta\\delta}\\right\\rfloor\\in\\left[\\frac{c_{1}}{3L\\eta\\delta},\\frac{c_{1}}{2L\\eta\\delta}\\right]\\!,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the lower bound on $T$ holds since $\\begin{array}{r}{\\frac{c_{1}}{6L\\eta\\delta}\\geq\\frac{c_{1}}{6\\delta}\\geq1}\\end{array}$ . We note that $\\begin{array}{r}{T=\\Omega\\big(\\frac{1}{\\delta}\\big)}\\end{array}$ since $\\begin{array}{r}{\\eta L\\leq\\frac{1}{4}}\\end{array}$ . ", "page_idx": 15}, {"type": "text", "text": "In Stage I, we have proved that $y^{T_{1}}[1]\\;\\leq\\;{\\frac{1}{2}}\\,-\\,c_{1}$ . Define $T_{h}\\,=\\,T_{1}\\,+\\,T$ . We claim that for all $\\begin{array}{r}{t\\in[T_{1},T_{h}-1],y^{t}[1]\\leq\\frac{1}{2}-\\frac{c_{1}}{2}.}\\end{array}$ . To prove the claim, we first notice that $-\\delta\\leq e_{y}^{t}\\leq1$ for all $t\\geq1$ . Then by the monotonicity and the $\\eta L$ -Lipschitzness of $F_{\\eta,R}$ (Lemma 1 and Lemma 4), we get for all $t\\in[T_{1},T_{h}-1]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y^{t}[1]\\leq F_{\\eta,R}(E_{y}^{T_{1}-1})+\\eta L\\operatorname*{max}\\left\\{E_{y}^{T_{1}-1}-E_{y}^{t-1}-e_{y}^{t-1},0\\right\\}}\\\\ &{\\qquad\\leq\\frac{1}{2}-c_{1}+\\eta L\\cdot(t-T_{1}+1)\\delta}\\\\ &{\\qquad\\leq\\frac{1}{2}-c_{1}+\\eta L T\\delta}\\\\ &{\\qquad\\leq\\frac{1}{2}-\\frac{c_{1}}{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where, in the second-to-last inequality, we use $\\begin{array}{r}{t-T_{1}+1\\leq T\\leq\\frac{c_{1}}{2\\eta L\\delta}}\\end{array}$ by Equation (4). ", "page_idx": 15}, {"type": "text", "text": "Now we denote $T_{2}\\geq T_{h}$ the smallest iteration when $\\begin{array}{r}{y^{T_{2}}[1]\\,\\ge\\,\\frac{1}{2(1+\\delta)}}\\end{array}$ . The existence of $T_{2}$ will become clear in the following analysis, and we postpone the proof to Claim 2 at the end of the discussion. Then for all $t\\in[T_{s},T_{2}-1]$ , we have $\\begin{array}{r}{\\dot{y}^{t}[1]\\leq\\frac{1}{2\\left(1+\\delta\\right)}}\\end{array}$ , which implies $e_{x}^{t}\\leq0$ . Moreover, for all $t\\in[T_{s},T_{1}+T-1]$ , since $\\begin{array}{r}{y^{t}[1]\\le\\frac{1}{2}-\\frac{c_{1}}{2}}\\end{array}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{e_{x}^{t}=\\ell_{x}^{t}[1]-\\ell_{x}^{t}[2]}\\\\ &{\\quad=-\\frac{1}{2}+(1+\\delta)y^{t}[1]}\\\\ &{\\quad\\leq\\frac{-1+(1+\\delta)(1-c_{1})}{2}}\\\\ &{\\quad\\leq\\frac{\\delta-c_{1}}{2}}\\\\ &{\\quad\\leq-\\frac{c_{1}}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\delta\\leq\\frac{c_{1}}{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then for any $T_{1}+T\\leq t\\leq T_{2}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{x^{t}[1]=F_{\\eta,R}(E_{x}^{t-1}+e_{x}^{t-1})}}\\\\ &{\\ge F_{\\eta,R}(E_{x}^{T_{1}+T-1})}\\\\ &{\\ge F_{\\eta,R}\\biggl(-\\frac{c_{1}T}{4}+E_{x}^{T_{1}-1}\\biggr)}\\\\ &{\\ge F_{\\eta,R}\\biggl(-\\frac{c_{1}T}{5}+E_{x}^{T_{1}-1}+e_{x}^{T_{1}-1}\\biggr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where in the last inequality, we use the fact that $\\begin{array}{r}{\\frac{c_{1}T}{20}\\geq\\frac{c_{1}^{2}}{60\\eta L\\delta}\\geq1}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Claim 2. $T_{2}$ exists. ", "page_idx": 15}, {"type": "text", "text": "Proof. Assume for the sake of contradiction that $T_{2}$ does not exist, i.e., $\\begin{array}{r}{y^{t}[1]\\;<\\;\\frac{1}{2(1+\\delta)}}\\end{array}$ for all $t\\geq T_{1}$ (since we know $\\begin{array}{r}{y^{t}[1]\\,\\le\\,\\frac{1}{2}\\,-\\,\\frac{c_{1}}{2}}\\end{array}$ for all $t\\,\\in\\,[T_{1},T_{1}+T-1])$ . Then by the analysis of Stage II and Equation (5), we have $\\begin{array}{r}{x^{t}[1]\\,\\geq\\,\\frac{4}{4+\\delta}}\\end{array}$ for all $t\\geq T_{1}$ . This implies $e_{y}^{t}\\,\\leq\\,-\\frac{3\\delta}{5}$ for all $t\\geq T_{1}$ . As a result, we have $E_{y}^{t-1}+e_{y}^{t-1}\\,\\to\\,-\\infty$ as $t\\to\\infty$ . By item 2 in Assumption 1, we get $\\begin{array}{r}{y^{t}[1]\\,=\\,F_{\\eta,R}(E_{y}^{t-1}+e_{y}^{t-1})\\,\\ge\\,\\frac{1}{2}\\,}\\end{array}$ as $t\\,\\rightarrow\\,\\infty$ . But this contradicts with the assumption that $\\begin{array}{r}{y^{t}[1]<\\frac{1}{2(1+\\delta)}}\\end{array}$ for all $t\\geq T_{1}$ . This completes the proof. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Stage III Recall that we have argued in State I that $F_{\\eta,R}(E_{x}^{T_{1}-1}+e_{x}^{T_{1}-1})=F_{1,R}(\\eta(E_{x}^{T_{1}-1}+$ $\\begin{array}{r}{e_{x}^{T_{1}\\bar{-}1}))=x^{T_{1}}[1]\\geq\\frac{1}{1+\\delta}}\\end{array}$ . By item 1 in Assumption 2, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{\\eta,R}\\bigg(-\\displaystyle\\frac{c_{1}T}{10}+E_{x}^{T_{1}-1}+e_{x}^{T_{1}-1})\\bigg)\\ge F_{\\eta,R}\\bigg(-\\displaystyle\\frac{c_{1}^{2}}{30L\\eta\\delta}+E_{x}^{T_{1}-1}+e_{x}^{T_{1}-1}\\bigg)\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=F_{1,R}\\bigg(-\\displaystyle\\frac{c_{1}^{2}}{30L\\delta}+\\eta(E_{x}^{T_{1}-1}+e_{x}^{T_{1}-1}))\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\ge\\displaystyle\\frac{1+c_{3}}{1+c_{3}+\\delta},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality follows from the definition of $T$ and the monotonicity of $F_{\\eta,R}$ (Lemma 1). Now denote $\\begin{array}{r}{T_{3}=T_{2}+\\lfloor\\frac{c_{1}T}{10}\\rfloor-2}\\end{array}$ . For any $T_{2}\\leq t\\leq T_{3}$ , we know that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{t}[1]=F_{\\eta,R}(E_{x}^{t-1}+e_{x}^{t-1})}\\\\ &{\\quad\\quad=F_{\\eta,R}(E_{x}^{{T}_{2}-1}+e_{x}^{{T}_{2}-1}+\\displaystyle\\sum_{k=T_{2}}^{t-1}e_{x}^{k}+e_{x}^{t-1}-e_{x}^{{T}_{2}-1})}\\\\ &{\\quad\\quad\\geq F_{\\eta,R}(-\\frac{c_{1}T}{5}+E_{x}^{{T}_{1}-1}+e_{x}^{T_{1}-1}+\\displaystyle\\sum_{k=T_{2}}^{t-1}e_{x}^{k}+e_{x}^{t-1}-e_{x}^{T_{2}-1})}\\\\ &{\\quad\\quad\\geq F_{\\eta,R}(-\\frac{c_{1}T}{5}+E_{x}^{{T}_{1}-1}+e_{x}^{T_{1}-1}+\\frac{c_{1}T}{10}-2+2)}\\\\ &{\\quad\\quad\\geq F_{\\eta,R}(-\\frac{c_{1}T}{10}+E_{x}^{{T}_{1}-1}+e_{x}^{T_{1}-1}))}\\\\ &{\\quad\\quad\\geq\\frac{1+c_{3}}{1+c_{3}+\\delta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $1\\!+\\!c_{3}\\!+\\!\\delta\\leq2$ . This implies $\\begin{array}{r}{e_{y}^{t}=1\\!-\\!(1\\!+\\!\\delta)x^{t}[1]=-\\frac{c_{3}\\delta}{1+c_{3}+\\delta}\\leq-\\frac{c_{3}\\delta}{2}}\\end{array}$ 1+cc33\u03b4+\u03b4 \u2264\u2212c32\u03b4 for all T2 \u2264t \u2264T3. Moreover, we know that $e_{y}^{t}\\geq-\\delta$ for any $t$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y^{T_{S}}[1]=F_{\\eta,R}(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1}))}\\\\ &{\\geq F_{\\eta,R}(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1}+\\sum_{k=1}^{3-1}e_{y}^{k}+e_{y}^{T_{S}-1}-e_{y}^{T_{S}-1}))}\\\\ &{\\geq F_{\\eta,R}(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1}-\\frac{c_{S}\\delta(T_{S}-T_{Z})}{1-\\delta}+\\delta)}\\\\ &{\\geq F_{\\eta,R}(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1}-\\frac{c_{S}\\delta(T_{S}-1)}{40}+\\delta)}\\\\ &{\\geq F_{\\eta,R}(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1}-\\frac{c_{S}\\delta(T_{S}+\\delta)}{1200}+\\delta)}\\\\ &{\\geq F_{\\eta,R}(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1}-\\frac{c_{S}c_{I}^{2}}{1200}+\\delta)}\\\\ &{=F_{\\eta,R}(\\eta(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1})-\\frac{c_{S}c_{I}^{2}}{1200}+\\eta\\delta)}\\\\ &{=F_{\\eta,R}(\\eta(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1})-\\frac{c_{S}c_{I}^{2}}{1202}+\\eta\\delta)}\\\\ &{\\geq F_{1,R}(\\eta(E_{y}^{T_{S}-1}+e_{y}^{T_{S}-1})-\\frac{c_{S}c_{I}^{2}}{1202}+\\frac{\\delta}{4b}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall that $\\begin{array}{r}{F_{1,R}\\big(\\eta(E_{y}^{T_{2}-1}+e_{y}^{T_{2}-1})\\big)=F_{\\eta,R}\\big(E_{y}^{T_{2}-1}+e_{y}^{T_{2}-1}\\big)=y^{T_{2}}[1]\\ge\\frac{1}{2(1+\\delta)}}\\end{array}$ . By item 2 in Assumption 2, we have $\\begin{array}{r}{F_{1,R}\\big(\\eta(E_{y}^{T_{2}-1}+e_{y}^{T_{2}-1})-\\frac{c_{1}^{2}}{120L}+\\frac{\\delta}{4L}\\big)\\geq\\frac{1}{2}+c_{2}}\\end{array}$ for some absolute constant $c_{2}>0$ . Thus, we have $y^{T_{3}}[1]\\,\\geq\\,{\\frac{1}{2}}+c_{2}$ . Recall that $\\begin{array}{r}{x^{T_{3}}[1]\\ge\\frac{1+c_{3}}{1+c_{3}+\\delta}\\ge\\frac{1}{1+\\delta}}\\end{array}$ . Then by Lemma 2 we can conclude that the duality gap of $(x^{T_{3}},y^{T_{3}})$ is at least $c_{2}>0$ . This completes the proof as T3 \u2265T2 \u2265T \u22653\u03b7c1L\u03b4. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Assume for the sake of contradiction that there is a function that satisfies both conditions. Then for any $A\\in[0,1]^{2\\times2}$ , we have the OFTRL learning dynamics over $A$ satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{T\\to\\infty}f(2,2,T)\\to0\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\operatorname*{lim}_{T\\to\\infty}f(2,2,T)~\\to~0$ , we know there exists $T_{0}~>~0$ such that for any $t~\\ge~T_{0}$ , Duality $\\mathrm{Gap}(x^{t},y^{t})\\,\\leq\\,f(2,2,t)\\,<\\,c_{2}$ . Now let $\\begin{array}{r}{\\delta\\,\\le\\,\\operatorname*{min}\\{\\hat{\\delta},\\frac{c_{1}}{3\\eta L T_{0}}\\}}\\end{array}$ . Then by Theorem 1, we know there exists an iteration $\\begin{array}{r}{t\\geq\\frac{c_{1}}{3\\eta L\\delta}\\geq T_{0}}\\end{array}$ such that Duality $\\dot{\\mathrm{Aap}}(x^{t},y^{t})\\geq c_{2}$ . This completes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C Verifying Assumption 2 for Different Regularizers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 4. If the regularizer $R$ is 1-strongly convex, then $F_{1,R}$ is $\\frac{1}{2}$ -Lipschitz. ", "page_idx": 17}, {"type": "text", "text": "Proof. Notice that $R(x)+R(1-x)$ is 2-strongly convex. Thus by standard analysis (see e.g., Luo [2022, Lemma 4]) we know $F_{1,R}$ is $\\frac{1}{2}$ -Lipschitz. ", "page_idx": 17}, {"type": "text", "text": "By Lemma 4, we can choose $\\begin{array}{r}{L=\\frac{1}{2}}\\end{array}$ for any 1-strongly convex regularizer in Assumption 1. ", "page_idx": 17}, {"type": "text", "text": "C.1 Negative Entropy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma 5 (Assumption 2 holds for the entropy regularizer). Consider the negative entropy regularizer $R$ defined as $R(x)=x\\log x+(1-x)\\log(1-x)$ . Then $F_{1,R}$ is $\\begin{array}{r}{L=\\frac{1}{2}}\\end{array}$ -Lipschitz. We have $c_{1}$ and Assumption 2 holds with $\\begin{array}{r}{\\delta^{\\prime}=\\frac{c_{1}^{2}}{480L}}\\end{array}$ , $\\begin{array}{r}{c_{2}=F_{1,R}\\big(-\\frac{c_{1}^{2}}{480L}\\big)-\\frac{1}{2}}\\end{array}$ , and $c_{3}=\\textstyle{\\frac{1}{2}}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. It is easy to verify that $F_{1,R}(x)$ has a closed-form representation ", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{1,R}(E)=\\frac{1}{1+\\exp(E)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus $\\begin{array}{r}{L=\\frac{1}{2}}\\end{array}$ and $\\begin{array}{r}{c_{1}=\\frac{1}{2}-F_{1,R}(\\frac{1}{20L})}\\end{array}$ is a universal constant. We also choose $c_{3}=\\textstyle{\\frac{1}{2}}$ ", "page_idx": 17}, {"type": "text", "text": "If $\\begin{array}{r}{F_{1,R}(E)\\ge\\frac{1}{1+\\delta}\\ge\\frac{1}{1+\\delta}}\\end{array}$ , then we have $E\\leq-\\log(1/\\delta)$ . We note that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\exp\\left(-\\frac{c_{1}^{2}}{30L\\delta}\\right)\\leq\\frac{1}{1+c_{3}}\\Rightarrow\\frac{1}{1+\\exp\\left(-\\frac{c_{1}^{2}}{30L\\delta}-\\log(1/\\delta)\\right)}\\geq\\frac{1+c_{3}}{1+c_{3}+\\delta}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\delta\\leq\\delta_{1}=\\frac{c_{1}^{2}}{30L\\log(1+c_{3}))}=\\frac{c_{1}^{2}}{30\\log(\\frac{3}{2}))L}}\\end{array}$ suffices for item 1 in Assumption 2. ", "page_idx": 17}, {"type": "text", "text": "If $\\begin{array}{r}{F_{1,R}(E)\\ge\\frac{1}{2(1+\\delta)}=\\frac{1}{1+1+2\\delta}}\\end{array}$ , we have $E\\le\\log(1+2\\delta)$ . Note that since $\\log(1+2y)\\leq2y$ for $y>0$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta\\leq\\frac{c_{3}c_{1}^{2}}{480L}\\Rightarrow-\\frac{c_{3}c_{1}^{2}}{120L}+\\log(1+2\\delta)<-\\frac{c_{3}c_{1}^{2}}{240L}\\qquad\\qquad}\\\\ {\\Rightarrow F_{1,R}\\biggl(-\\frac{c_{3}c_{1}^{2}}{120L}+E\\biggr)>F_{1,R}\\biggl(-\\frac{c_{3}c_{1}^{2}}{240L}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus item 2 in Assumption 2 holds for any $\\begin{array}{r}{\\delta\\le\\delta_{2}=\\frac{c_{3}c_{1}^{2}}{480L}=\\frac{c_{1}^{2}}{960L}}\\end{array}$ and $\\begin{array}{r}{c_{2}=F_{1,R}\\bigl(-\\frac{c_{3}c_{1}^{2}}{240L}\\bigr)-\\frac{1}{2}=}\\end{array}$ $F_{1,R}(-\\frac{c_{1}^{2}}{480L})-\\frac{1}{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Combining the above, we know Assumption 2 holds for the negative entropy regularizer with $\\begin{array}{r}{\\delta^{\\prime}=\\frac{c_{1}^{2}}{960L}}\\end{array}$ and $\\begin{array}{r}{c_{2}=F_{1,R}\\big(-\\frac{c_{1}^{2}}{480L}\\big)-\\frac{1}{2}}\\end{array}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "C.2 Squared Euclidean Norm Regularizer ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 6 (Assumption 2 holds for the Euclidean regularizer). Consider the Euclidean regularizer $R$ defined as $R(x)^{\\stackrel{.}{}}={\\textstyle{\\frac{1}{2}}}(x^{2}+(1-x)^{2})$ . We have $\\bar{L dot{=}}\\,\\frac{1}{2}$ and $\\begin{array}{r}{c_{1}={\\frac{1}{20}}}\\end{array}$ . We also have Assumption 2 holds with $\\begin{array}{r}{\\delta^{\\prime}=\\frac{c_{1}^{2}}{480L}}\\end{array}$ =48c01L, c2 =96c01L, and c3 = 12. ", "page_idx": 18}, {"type": "text", "text": "Proof. It is easy to verify that $F_{1,R}(x)$ has a closed-form representation ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{1,R}(x)={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{~if~}}x\\leq-1}\\\\ {{\\frac{1-x}{2}}}&{{\\mathrm{~if~}}x\\in(-1,1)}\\\\ {0}&{{\\mathrm{~if~}}x\\geq1}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus $F_{1,R}$ is $L$ -Lipschitz with $\\begin{array}{r}{L=\\frac{1}{2}}\\end{array}$ . Moreover, $\\begin{array}{r}{c_{1}=\\frac{1}{2}-F_{1,R}(\\frac{1}{20L})=\\frac{1}{20}}\\end{array}$ . We choose $c_{3}=\\textstyle{\\frac{1}{2}}$ . Fix any $E$ such that $\\begin{array}{r}{F_{1,R}(E)\\ge\\frac{1}{1+\\delta}}\\end{array}$ . We have $\\begin{array}{r}{E\\le-\\frac{1-\\delta}{1+\\delta}<0}\\end{array}$ . We note that for any $\\begin{array}{r}{\\delta\\le\\frac{c_{1}^{2}}{30L}=\\frac{c_{1}^{2}}{15}}\\end{array}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{1,R}\\!\\left(-\\frac{c_{1}^{2}}{30L\\delta}+E\\right)\\geq F_{1,R}(-1)=1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus $\\begin{array}{r}{\\delta\\le\\delta_{1}=\\frac{c_{1}^{2}}{30L}}\\end{array}$ suffices for item 1 in Assumption 2. ", "page_idx": 18}, {"type": "text", "text": "Fix any $E$ such that $\\begin{array}{r}{F_{1,R}(E)\\ge\\frac{1}{2(1+\\delta)}=\\frac{1}{2(1+\\delta)}}\\end{array}$ . We have $\\begin{array}{r}{E\\le\\frac{\\delta}{1+\\delta}\\le\\delta}\\end{array}$ . The for any $\\begin{array}{r}{\\delta\\le\\frac{c_{3}c_{1}^{2}}{240L}}\\end{array}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{1,R}\\biggl(-\\frac{c_{3}c_{1}^{2}}{120L}+E\\biggr)\\geq F_{1,R}\\biggl(-\\frac{c_{3}c_{1}^{2}}{240L}\\biggr)=\\frac{1}{2}+\\frac{c_{3}c_{1}^{2}}{480L}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus item 2 in Assumption 2 holds for any $\\begin{array}{r}{\\delta\\le\\delta_{2}=\\frac{c_{1}^{2}}{480L}}\\end{array}$ and $\\begin{array}{r}{c_{2}=\\frac{c_{1}^{2}}{960L}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Combining the above, we know Assumption 2 holds for the negative entropy regularizer with $\\begin{array}{r}{\\delta^{\\prime}=\\operatorname*{min}\\Bar{\\{\\delta}_{1},\\delta_{2}\\}=\\frac{c_{1}^{2}}{480L}}\\end{array}$ and $\\begin{array}{r}{c_{2}=\\frac{c_{1}^{2}}{960L}}\\end{array}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.3 Log Barrier ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma 7 (Assumption 2 holds for the log barrier). Consider the log barrier regularizer R defined as $R(x)=-\\log(\\bar{x})-\\log(1-x)$ . Then Assumption 2 holds with the following choices of constants: ", "page_idx": 18}, {"type": "text", "text": "Proof. By setting the gradient of $x\\cdot E+R(x)$ to 0, we get a closed-form expression of $F_{1,R}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{1,R}(E)={\\left\\{\\begin{array}{l l}{{\\frac{1}{2}}+{\\frac{1}{E}}-{\\sqrt{{\\frac{1}{4}}+{\\frac{1}{E^{2}}}}}}&{{\\mathrm{if}}\\,E>0}\\\\ {{\\frac{1}{2}}}&{{\\mathrm{if}}\\,E=0}\\\\ {{\\frac{1}{2}}+{\\frac{1}{E}}+{\\sqrt{{\\frac{1}{4}}+{\\frac{1}{E^{2}}}}}}&{{\\mathrm{if}}\\,E<0.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $x\\in(0,1)$ , the $F_{1,R}$ function admits an inverse function defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\nF_{1,R}^{-1}(x)={\\frac{2x-1}{x^{2}-x}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus we know $\\begin{array}{r}{E_{0}:=F_{1,R}^{-1}(\\frac{1}{1+\\delta})=-\\frac{1-\\delta^{2}}{\\delta}}\\end{array}$ satisfies $\\begin{array}{r}{F_{1,R}(E_{0})=\\frac{1}{1+\\delta}}\\end{array}$ . Moreover, we can calculate ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{F_{1,R}^{-1}\\bigg(\\frac{1+c_{3}}{1+c_{3}+\\delta}\\bigg)=-\\frac{(1+c_{3})^{2}-\\delta^{2}}{(1+c_{3})\\delta}}}\\\\ &{}&{=-\\frac{1+c_{3}}{\\delta}+\\frac{\\delta}{1+c_{3}}}\\\\ &{}&{=E_{0}-\\frac{c_{3}}{\\delta}-\\frac{c_{3}\\delta}{1+c_{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus we can choose $\\begin{array}{r}{c_{3}=\\frac{c_{1}^{2}}{60L}}\\end{array}$ so that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle E_{0}-\\frac{c_{1}^{2}}{30L\\delta}}\\\\ {\\displaystyle=E_{0}-\\frac{c_{3}}{\\delta}-\\frac{c_{3}}{\\delta}}\\\\ {\\displaystyle\\leq E_{0}-\\frac{c_{3}}{\\delta}-\\frac{c_{3}\\delta}{1+c_{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus we have $\\begin{array}{r}{F_{1,R}(E_{0}-\\frac{c_{1}^{2}}{30L\\delta})\\geq F_{1,R}(E_{0}-\\frac{c_{3}}{\\delta}-\\frac{c_{3}\\delta}{1+c_{3}})\\geq\\frac{1+c_{3}}{1+c_{3}+\\delta}.}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "We calculate E1 := F 1\u2212,R1(2(11+\u03b4)) $\\begin{array}{r}{E_{1}:=F_{1,R}^{-1}\\bigl(\\frac{1}{2(1+\\delta)}\\bigr)=\\frac{4(\\delta+\\delta^{2})}{1+2\\delta}\\le8\\delta.}\\end{array}$ . Then we can choose $\\begin{array}{r}{\\delta\\le\\delta^{\\prime}:=\\frac{c_{3}c_{1}^{2}}{2160L}}\\end{array}$ Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F_{1,R}(-\\frac{c_{3}c_{1}^{2}}{120L}+\\frac{\\delta}{4L}+E_{1})}\\\\ &{\\geq F_{1,R}(-\\frac{c_{3}c_{1}^{2}}{120L}+9\\delta)}\\\\ &{\\geq F_{1,R}(-\\frac{c_{3}c_{1}^{2}}{240L})}\\\\ &{=\\frac{1}{2}+c_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{c_{2}=\\sqrt{\\frac{1}{4}+(\\frac{c_{3}c_{1}^{2}}{240L})^{2}}-\\frac{c_{3}c_{1}^{2}}{240L}>0}\\end{array}$ by the closed-form expression of $F_{1,R}$ . ", "page_idx": 19}, {"type": "text", "text": "C.4 Negative Tsallis Entropy ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For $x\\in[0,1]$ , the negative Tsallis entropy is a family of regularizers parameterized by $\\beta\\in(0,1)$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\nR(x)={\\frac{1-x^{\\beta}}{1-\\beta}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The corresponding $F_{1,R}$ is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\nF_{1,R}(E)=\\operatorname*{argmin}_{x\\in(0,1)}\\left\\{x\\cdot E+{\\frac{1-x^{\\beta}}{1-\\beta}}+{\\frac{1-(1-x)^{\\beta}}{1-\\beta}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For $x\\in(0,1)$ , we note that $F_{1,R}$ has an inverse function ", "page_idx": 19}, {"type": "equation", "text": "$$\nF_{1,R}^{-1}(x)={\\frac{\\beta}{1-\\beta}}{\\big(}x^{\\beta-1}-(1-x)^{\\beta-1}{\\big)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma 8 (Assumption 2 holds for Tsallis entropy). Consider Tsallis entropy parameterized by $\\beta\\in(0,1)$ . Then $\\begin{array}{r}{L=\\frac{1}{2\\beta}}\\end{array}$ and Assumption 2 holds with the following choices of constants: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I.\\ c_{1}=\\frac{1}{2}-F_{1,R}(\\frac{1}{20L})>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{:\\ \\delta^{\\prime}=\\operatorname*{min}\\{\\big(\\frac{c_{1}^{2}\\left(1-\\beta\\right)}{120L\\beta c_{3}^{1-\\beta}}\\big)^{\\frac{1}{\\beta}},\\frac{c_{3}c_{1}^{2}}{120},\\frac{1-\\beta}{8\\beta}\\cdot\\frac{c_{3}c_{1}^{2}}{480L}\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We choose $c_{3}=\\textstyle{\\frac{1}{2}}$ . We have $\\begin{array}{r}{c_{1}=\\frac{1}{2}-F_{1,R}(\\frac{1}{20L})}\\end{array}$ is a constant. We note that ", "page_idx": 20}, {"type": "equation", "text": "$$\nE_{0}:=F_{1,R}^{-1}(\\frac{1}{1+\\delta})=\\frac{\\beta}{1-\\beta}\\Bigg((1+\\delta)^{1-\\beta}-\\Bigg(\\frac{1+\\delta}{\\delta}\\Bigg)^{1-\\beta}\\Bigg)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "satisfies $\\begin{array}{r}{F_{1,R}(E_{0})=\\frac{1}{1+\\delta}}\\end{array}$ . Similarly, we calculate ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{1}:=F_{1,R}^{-1}(\\frac{1+c_{3}}{1+c_{3}+\\delta})}\\\\ &{\\quad=\\frac{\\beta}{1-\\beta}\\Bigg(\\bigg(\\frac{1+c_{3}+\\delta}{1+c_{3}}\\bigg)^{1-\\beta}-\\bigg(\\frac{1+c_{3}+\\delta}{\\delta}\\bigg)^{1-\\beta}\\Bigg)}\\\\ &{\\quad\\geq\\frac{\\beta}{1-\\beta}\\Bigg((1+\\delta)^{1-\\beta}-2-\\bigg(\\frac{1+c_{3}+\\delta}{\\delta}\\bigg)^{1-\\beta}\\Bigg)}\\\\ &{\\quad\\geq\\frac{\\beta}{1-\\beta}\\Bigg((1+\\delta)^{1-\\beta}-\\bigg(\\frac{1+\\delta}{\\delta}\\bigg)^{1-\\beta}-\\bigg(\\frac{c_{3}}{\\delta}\\bigg)^{1-\\beta}-2\\Bigg)}\\\\ &{\\quad=E_{0}-\\frac{\\beta}{1-\\beta}\\bigg(\\bigg(\\frac{c_{3}}{\\delta}\\bigg)^{1-\\beta}+2\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the first inequality we use the fact that $(1+\\delta)^{1-\\beta}\\leq2$ since $\\delta\\leq1$ ; the second inequality we use the inequality $(x+\\overrightharpoon{y})^{1-\\beta}\\leq x^{1-\\beta}+y^{1-\\stackrel{.}{\\beta}}$ . We note that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta\\leq\\delta_{1}:=\\left(\\frac{c_{1}^{2}(1-\\beta)}{120L\\beta c_{3}^{1-\\beta}}\\right)^{\\frac{1}{\\beta}}\\Rightarrow-\\frac{c_{1}^{2}}{30L\\delta}\\leq-\\frac{\\beta}{1-\\beta}\\bigg(\\Big(\\frac{c_{3}}{\\delta}\\Big)^{1-\\beta}+2\\bigg).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus for any $\\delta\\leq\\delta_{1}$ , we have for any $E$ such that $\\begin{array}{r}{F_{1,R}(E)\\ge\\frac{1}{1+\\delta}}\\end{array}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n-\\frac{c_{1}^{2}}{30L\\delta}+E\\le-\\frac{c_{1}^{2}}{30L\\delta}+E_{0}\\le E_{0}-\\frac{\\beta}{1-\\beta}\\Bigg(\\Big(\\frac{c_{3}}{\\delta}\\Big)^{1-\\beta}+2\\Bigg)\\le E_{1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The above implies $\\begin{array}{r}{F_{1,R}\\bigl(-\\frac{c_{1}^{2}}{30L\\delta}+E\\bigr)\\geq\\frac{1+c_{3}}{1+c_{3}+\\delta}}\\end{array}$ and the first item in Assumption 2 is satisfied. We define $E_{2}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{2}:=F_{1,R}^{-1}\\Bigg(\\frac{1}{2(1+\\delta)}\\Bigg)=\\frac{\\beta}{1-\\beta}\\Bigg((2+2\\delta)^{1-\\beta}-\\bigg(\\frac{2+2\\delta}{1+2\\delta}\\bigg)^{1-\\beta}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{\\beta}{1-\\beta}(2+2\\delta)^{1-\\beta}\\cdot\\Bigg(1-\\bigg(\\frac{1}{1+2\\delta}\\bigg)^{1-\\beta}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{4\\beta}{1-\\beta}\\cdot\\Bigg(1-\\bigg(1-\\frac{2\\delta}{1+2\\delta}\\bigg)^{1-\\beta}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{4\\beta}{1-\\beta}\\cdot\\frac{2\\delta}{1+\\delta}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{8\\beta\\delta}{(1-\\beta)(1+\\delta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where in the first inequality we use $(2+2\\delta)^{1-\\beta}\\leq4$ since $0\\leq\\delta\\leq1$ and $\\beta\\in(0,1)$ ; in the second inequality we use the basic inequality $(1-x)^{r}\\leq1-x$ for $r,x\\in(0,1)$ . We define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta_{2}:=\\operatorname*{min}\\{\\frac{c_{3}c_{1}^{2}}{120},\\frac{1-\\beta}{8\\beta}\\cdot\\frac{c_{3}c_{1}^{2}}{480L}\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then for any $\\delta\\leq\\delta_{2}$ and $E$ such that $\\begin{array}{r}{F_{1,R}[E]\\ge\\frac{1}{2(1+\\delta)}}\\end{array}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\frac{c_{3}c_{1}^{2}}{120L}+\\frac{\\delta}{4L}+E\\leq-\\frac{c_{3}c_{1}^{2}}{120L}+\\frac{\\delta}{4L}+E_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\frac{c_{3}c_{1}^{2}}{120L}+\\frac{c_{3}c_{1}^{2}}{480L}+\\frac{8\\beta\\delta}{(1-\\beta)(1+\\delta)}}\\\\ &{\\qquad\\qquad\\qquad\\leq-\\frac{c_{3}c_{1}^{2}}{120L}+\\frac{c_{3}c_{1}^{2}}{480L}++\\frac{c_{3}c_{1}^{2}}{480L}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=-\\frac{c_{3}c_{1}^{2}}{240L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Thus we know $\\begin{array}{r}{F_{1,R}(-\\frac{c_{3}c_{1}^{2}}{120L}+\\frac{\\delta}{4L}+E)\\geq F_{1,R}(-\\frac{c_{3}c_{1}^{2}}{240L})}\\end{array}$ and item 2 in Assumption 2 is satisfied by $\\begin{array}{r}{c_{2}=F_{1,R}\\bigl(-\\frac{c_{3}c_{1}^{2}}{240L}\\bigr)-\\frac{1}{2}>0}\\end{array}$ . ", "page_idx": 21}, {"type": "text", "text": "Combining the above, we can choose $\\hat{\\delta}=\\operatorname*{min}\\{\\delta_{1},\\delta_{2}\\}$ so that both items in Assumption 2 hold for $\\delta\\leq\\hat{\\delta}$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D Proof of Theorem 3 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Recall the equivalent 2-dimensional problem: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{x}^{t}=\\mathrm{Dupl}\\left[\\underset{x\\in\\frac{1}{n}\\cdot\\Delta^{2}}{\\mathrm{argmin}}\\left\\lbrace\\frac{n}{n^{\\alpha}}\\left\\langle x,\\sum_{\\tau=1}^{t-1}\\ell^{\\tau}+\\ell^{t-1}\\right\\rangle+\\frac{n}{\\eta}R_{1}(x[1])+\\frac{n}{\\eta}R_{2}(x[2])\\right\\rbrace\\right]}\\\\ &{\\quad=\\mathrm{Dupl}\\left[\\frac{1}{n}\\cdot\\underset{x\\in\\Delta^{2}}{\\mathrm{argmin}}\\left\\lbrace\\frac{n}{n^{\\alpha}}\\left\\langle\\frac{1}{n}x,\\sum_{\\tau=1}^{t-1}\\ell^{\\tau}+\\ell^{t-1}\\right\\rangle+\\frac{n}{\\eta}R(x/n)\\right\\rbrace\\right]}\\\\ &{\\quad=\\mathrm{Dupl}\\left[\\frac{1}{n}\\cdot\\underset{x\\in\\Delta^{2}}{\\mathrm{argmin}}\\left\\lbrace\\left\\langle x,\\sum_{\\tau=1}^{t-1}\\ell^{\\tau}+\\ell^{t-1}\\right\\rangle+\\frac{n^{\\alpha+1}}{\\eta}R(x/n)\\right\\rbrace\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Euclidean regularizer: this regularizer is homogeneous of degree two. Choosing $\\alpha=1$ , the inner minimization problem is exactly the same as the one solved by OFTRL in two dimensions. ", "page_idx": 21}, {"type": "text", "text": "Entropy regularizer: we set $\\alpha=0$ to get equivalence: ", "page_idx": 21}, {"type": "equation", "text": "$$\nn R(x/n)=\\sum_{i=1}^{2}x[i]\\log(x[i]/n)=\\sum_{i=1}^{2}x[i]\\log x[i]-\\sum_{i=1}^{2}x[i]\\log n=\\sum_{i=1}^{2}x[i]\\log x[i]-\\log n.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we have equivalence because the last term is a constant that does not affect the argmin. ", "page_idx": 21}, {"type": "text", "text": "Log regularizer: we set $\\alpha=-1$ to get equivalence, using similar logic as for entropy: ", "page_idx": 21}, {"type": "equation", "text": "$$\nR(x/n)=\\sum_{i=1}^{2}-\\log(x[i]/n)=2\\log n+\\sum_{i=1}^{2}-\\log x[i].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Tsallis entropy regularizer: we set $\\alpha=-1+\\beta$ to get equivalence, using similar logic as for entropy: ", "page_idx": 21}, {"type": "equation", "text": "$$\nn^{\\beta}R(x/n)=n^{\\beta}\\cdot{\\frac{1-\\sum_{i=1}^{2}({\\frac{x[i]}{n}})^{\\beta}}{1-\\beta}}={\\frac{n^{\\beta}-1}{1-\\beta}}+{\\frac{1-\\sum_{i=1}^{2}x[i]^{\\beta}}{1-\\beta}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "E Numerical Experiments with Adaptive Stepsizes ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we present our numerical results when OFTRL and OOMD are instantiated with adaptive stepsize [Duchi et al., 2011]: $\\begin{array}{r}{\\eta_{t}=1/\\sqrt{\\epsilon+\\sum_{k=1}^{t-1}\\|\\ell_{k}\\|_{k}^{2}}}\\end{array}$ with some constant $\\epsilon>0$ . We present our numerical experiments in Figure 4, where we choose $\\epsilon=0.1$ . ", "page_idx": 21}, {"type": "image", "img_path": "hK7XTpCtBi/tmp/e8aec1c53296719141f4251d9e7ebfece2e52292058dcd1b15a6fa1d025ed130.jpg", "img_caption": ["Figure 4: Comparison of the dynamics produced by three variants of OFTRL with different regularizers (negative entropy, logarithmic regularizer, and squared Euclidean norm) and OGDA in the same game $A_{\\delta}$ defined in (2) for $\\delta:=10^{-2}$ and adaptive step size with $\\epsilon=0.1$ . The bottom row shows the duality gap achieved by the iterates. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction match both the theoretical and experimental results of the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The extension to the general $m\\times n$ game does not work for the Euclidean and log regularizers because the rescaling factors would be different for the row and column players. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: See Section 2, Section 3 and the appendix ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: See Figure 1 and Python codes in the supplementary materials. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: See Python code in the supplementary materials. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See Python code in the supplementary materials. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: There is no randomness in our experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our simulations were done within a few hours on an average consumer laptop. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics. The research conducted in this paper conforms with it in every respect. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper is mostly theoretical. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]