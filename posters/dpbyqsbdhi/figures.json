[{"figure_path": "DpByqSbdhI/figures/figures_0_1.jpg", "caption": "Figure 2: UniMTS pre-training framework: The physics engine computes motion time series for each joint based on motion skeleton data and enhances time series through rotation-invariant augmentation. During pre-training, we adopt contrastive learning to align motion time series encoded by graph convolutional neural networks with corresponding text descriptions augmented by an LLM.", "description": "This figure illustrates the UniMTS pre-training framework.  It shows how motion time series are generated from motion skeleton data using a physics engine, augmented with rotation invariance, and then aligned with text descriptions (augmented by a large language model) via contrastive learning. The framework aims to create a pre-trained model that generalizes across different device locations, orientations, and activities.", "section": "3 Method"}, {"figure_path": "DpByqSbdhI/figures/figures_1_1.jpg", "caption": "Figure 1: Our framework addresses three key generalization challenges (variation in device location, orientation, and activity) where existing methods fall short.", "description": "This figure illustrates the three main generalization challenges that the UniMTS model addresses.  Existing methods often struggle to generalize across different device locations (e.g., a smartwatch on the wrist versus a smartphone on the thigh), device mounting orientations (e.g., a phone held vertically vs. horizontally), and different activity types (e.g., sitting versus running).  The figure highlights these challenges visually, showing how the variation in device location, orientation, and activity during deployment differs from the training data, impeding generalization.", "section": "1 Introduction"}, {"figure_path": "DpByqSbdhI/figures/figures_2_1.jpg", "caption": "Figure 2: UniMTS pre-training framework: The physics engine computes motion time series for each joint based on motion skeleton data and enhances time series through rotation-invariant augmentation. During pre-training, we adopt contrastive learning to align motion time series encoded by graph convolutional neural networks with corresponding text descriptions augmented by an LLM.", "description": "The figure illustrates the UniMTS pre-training framework.  It shows how motion skeleton data is processed using a physics engine to generate motion time series, which are then augmented with rotation invariance. These time series are encoded using graph convolutional neural networks and aligned with text descriptions enriched by a large language model (LLM) through contrastive learning.  The goal is to learn the semantic relationships between motion time series and their textual descriptions, enabling better generalization to unseen activities and contexts.", "section": "3 Method"}, {"figure_path": "DpByqSbdhI/figures/figures_4_1.jpg", "caption": "Figure 3: Inference (left) and fine-tuning (right) phases of UniMTS. We assign real signals to the nearest location in the skeleton graph. During inference, we compute the similarity score between the graph embedding and each label candidate, and predict the one with the highest score. During fine-tuning, we freeze the text encoder and update weights of the graph encoder and linear layer.", "description": "This figure illustrates the inference and fine-tuning phases of the UniMTS model.  The left side shows the inference process: real-world motion time series data from various body locations are input into the graph encoder. The graph encoder generates an embedding that is compared against the text encoder's embeddings for different activity labels. The activity with the highest similarity score is predicted. The right side shows the fine-tuning process: pre-trained model weights (graph encoder) are updated using real-world motion data to improve accuracy. The text encoder remains frozen during fine-tuning.", "section": "3.3.3 Training and Inference"}, {"figure_path": "DpByqSbdhI/figures/figures_7_1.jpg", "caption": "Figure 4: Few-shot fine-tuning results. UniMTS consistently outperforms both baselines and our model ablation. We repeat 3 runs and report both mean and standard deviation.", "description": "This figure shows the macro-F1 scores achieved by UniMTS and several baseline models (ImageBind, IMU2CLIP, IMUGPT, GPT4TS, BioBankSSL, Random) across 18 different datasets in a few-shot learning setting.  Each dataset is represented by a separate subplot. The x-axis represents the number of samples used for training per class (1, 2, 3, 5, 10 samples). The y-axis shows the macro-F1 score. The shaded area around each line represents the standard deviation across three runs. UniMTS consistently outperforms all baselines across all datasets and training sample sizes, demonstrating the effectiveness of its unified pre-training approach.", "section": "4.3 Few-Shot Fine-tuning Results"}, {"figure_path": "DpByqSbdhI/figures/figures_7_2.jpg", "caption": "Figure 3: Inference (left) and fine-tuning (right) phases of UniMTS. We assign real signals to the nearest location in the skeleton graph. During inference, we compute the similarity score between the graph embedding and each label candidate, and predict the one with the highest score. During fine-tuning, we freeze the text encoder and update weights of the graph encoder and linear layer.", "description": "This figure illustrates the inference and fine-tuning phases of the UniMTS model.  The left side shows the inference process where real-world motion sensor data is mapped to the nearest joint in a skeleton graph.  The model then computes similarity scores between the graph embedding (representing the motion data) and embeddings for various activity labels. The activity with the highest similarity score is predicted as the activity. The right side depicts the fine-tuning process, where the pre-trained text encoder is frozen, and only the graph encoder and a linear layer are trained further using real-world motion sensor data and corresponding activity labels to enhance performance.", "section": "3.3.1 Graph Encoder"}, {"figure_path": "DpByqSbdhI/figures/figures_8_1.jpg", "caption": "Figure 6: Simulated motion time series closely resemble patterns of the real PAMAP2 time series.", "description": "This figure compares simulated motion time series generated by the physics engine in UniMTS with real-world motion time series from the PAMAP2 dataset.  Three different activities are shown: sitting, walking, and rope jumping.  For each activity, the plot shows three axes of motion data (likely acceleration or angular velocity) for both the simulated and real data.  The visual similarity between the simulated and real data suggests the effectiveness of the physics engine in generating realistic motion data for pre-training the UniMTS model.", "section": "4. Experiments"}, {"figure_path": "DpByqSbdhI/figures/figures_8_2.jpg", "caption": "Figure 7: UniMTS shows significant performance improvement compared with the best baseline when evaluated on new activities not seen.", "description": "This figure shows the macro-F1 scores for several unseen activities for both UniMTS and the ImageBind baseline.  UniMTS shows consistently higher performance, indicating its ability to generalize to new, previously unseen activities. The x-axis lists the activities, and the y-axis shows the macro-F1 score.  The bars show a comparison between UniMTS and ImageBind for each activity.", "section": "4.2 Zero-Shot Results"}, {"figure_path": "DpByqSbdhI/figures/figures_15_1.jpg", "caption": "Figure 2: UniMTS pre-training framework: The physics engine computes motion time series for each joint based on motion skeleton data and enhances time series through rotation-invariant augmentation. During pre-training, we adopt contrastive learning to align motion time series encoded by graph convolutional neural networks with corresponding text descriptions augmented by an LLM.", "description": "This figure illustrates the UniMTS pre-training framework.  It starts with motion skeleton data, which is fed into a physics engine to simulate realistic motion time series for each joint. Rotation-invariant augmentation is then applied to make the model robust to changes in device orientation. These augmented time series are input to a graph encoder (spatio-temporal graph convolutional neural network) to capture the relationships between joints.  Simultaneously, text descriptions of the motion are augmented using a large language model (LLM). Finally, contrastive learning aligns the graph-encoded motion time series with the LLM-augmented text descriptions.", "section": "3 Method"}, {"figure_path": "DpByqSbdhI/figures/figures_17_1.jpg", "caption": "Figure 6: Simulated motion time series closely resemble patterns of the real PAMAP2 time series.", "description": "This figure compares simulated motion time series generated by the UniMTS model with real-world motion time series from the PAMAP2 dataset. Three activities are shown: sitting, walking, and rope jumping. For each activity, the plot shows the simulated and real data for three axes (x, y, z) of acceleration. The close similarity between the simulated and real data demonstrates the effectiveness of the physics engine used in UniMTS to generate realistic motion time series.", "section": "4.1 Datasets and Experimental Setting"}]