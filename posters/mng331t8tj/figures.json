[{"figure_path": "MNg331t8Tj/figures/figures_0_1.jpg", "caption": "Figure 1: Various generative augmentation methods applied on Aircraft [30]. Text-to-image often compromises class fidelity, visible by the unrealistic aircraft design (i.e., tail at both ends). Img2Img trades off fidelity and diversity: lower strength (e.g., 0.5) introduces minimal semantic changes, resulting in higher fidelity but limited diversity, whereas higher strength (e.g., 0.75) introduces diversity but also inaccuracies such as the incorrectly added engine. In contrast, SaSPA achieves high fidelity and diversity, critical for Fine-Grained Visual Classification tasks. D - Diversity. F - Fidelity", "description": "This figure shows a comparison of different image augmentation techniques applied to an airplane image.  It highlights the trade-off between image fidelity (how realistic the augmented image is) and diversity (how different the augmented image is from the original). Text-to-image methods often lack fidelity. Img2Img methods with low strength improve fidelity but lack diversity; while high strength increases diversity but loses fidelity. The authors' method, SaSPA, achieves high fidelity and diversity.", "section": "Abstract"}, {"figure_path": "MNg331t8Tj/figures/figures_2_1.jpg", "caption": "Figure 2: SaSPA Pipeline: For a given FGVC dataset, we generate prompts via GPT-4 based on the meta-class. Each real image undergoes edge detection to provide structural outlines. These edges are used M times, each time with a different prompt and a different subject reference image from the same sub-class, as inputs to a ControlNet with BLIP-Diffusion as the base model. The generated images are then filtered using a dataset-trained model and CLIP to ensure relevance and quality.", "description": "This figure illustrates the SaSPA pipeline. It starts with generating prompts from GPT-4 based on the meta-class of the dataset.  Then, real images undergo edge detection to capture structural information.  These edges, along with different prompts and subject reference images from the same sub-class, are fed into ControlNet with BLIP-Diffusion. The generated images are finally filtered for relevance and quality using a dataset-trained model and CLIP.", "section": "3 Method"}, {"figure_path": "MNg331t8Tj/figures/figures_4_1.jpg", "caption": "Figure 1: Various generative augmentation methods applied on Aircraft [30]. Text-to-image often compromises class fidelity, visible by the unrealistic aircraft design (i.e., tail at both ends). Img2Img trades off fidelity and diversity: lower strength (e.g., 0.5) introduces minimal semantic changes, resulting in higher fidelity but limited diversity, whereas higher strength (e.g., 0.75) introduces diversity but also inaccuracies such as the incorrectly added engine. In contrast, SaSPA achieves high fidelity and diversity, critical for Fine-Grained Visual Classification tasks. D - Diversity. F - Fidelity", "description": "This figure compares different image augmentation methods applied to aircraft images from the Aircraft dataset.  It shows how text-to-image methods can create unrealistic results (low fidelity), while image-to-image methods struggle to balance fidelity and diversity. The authors' method, SaSPA, is highlighted for its ability to achieve both high fidelity and high diversity in generated images, which is crucial for fine-grained visual classification.", "section": "Abstract"}, {"figure_path": "MNg331t8Tj/figures/figures_6_1.jpg", "caption": "Figure 4: Few-shot test accuracy across three FGVC datasets: Aircraft, Cars, and DTD, using different augmentation methods. The number of few-shots tested includes 4, 8, 12, and 16. We can see that for all datasets and shots, SaSPA outperforms all other augmentation methods.", "description": "This figure compares the performance of various data augmentation methods in a few-shot learning setting on three fine-grained visual classification (FGVC) datasets: Aircraft, Cars, and DTD.  The x-axis represents the number of training examples (shots) used, while the y-axis shows the test accuracy achieved.  Different lines represent different augmentation techniques: Best Trad Aug (best traditional augmentation method), Real Guidance, ALIA, and SaSPA (the proposed method).  The figure clearly demonstrates that SaSPA consistently outperforms all other methods across all datasets and different numbers of shots.", "section": "4.3 Few-shot Learning"}, {"figure_path": "MNg331t8Tj/figures/figures_16_1.jpg", "caption": "Figure 5: Line plots of Augmentation Ratio (\u03b1) vs. validation accuracy for Aircraft, Cars, DTD, and CUB datasets.", "description": "This figure shows the relationship between the augmentation ratio (\u03b1) and the validation accuracy for four fine-grained visual classification (FGVC) datasets: Aircraft, Cars, DTD, and CUB.  The augmentation ratio represents the probability that a real training sample will be replaced with a synthetic sample during each epoch.  The plot reveals how the optimal augmentation ratio varies across different datasets, indicating that there's no single optimal ratio for all FGVC tasks.  Some datasets show an optimal ratio between 0.2 and 0.5, while others, such as CUB, exhibit a different trend.", "section": "B.1 Effect of Augmentation Ratio on Performance"}, {"figure_path": "MNg331t8Tj/figures/figures_18_1.jpg", "caption": "Figure 4: Few-shot test accuracy across three FGVC datasets: Aircraft, Cars, and DTD, using different augmentation methods. The number of few-shots tested includes 4, 8, 12, and 16. We can see that for all datasets and shots, SaSPA outperforms all other augmentation methods.", "description": "This figure displays the results of a few-shot learning experiment on three fine-grained visual classification (FGVC) datasets (Aircraft, Cars, and DTD) using various data augmentation methods.  The x-axis represents the number of \"shots\" (training examples per class), and the y-axis represents the test accuracy. The figure shows that SaSPA (Structure and Subject Preserving Augmentation), consistently outperforms other augmentation methods across all three datasets and across different numbers of shots.", "section": "4.3 Few-shot Learning"}, {"figure_path": "MNg331t8Tj/figures/figures_26_1.jpg", "caption": "Figure 7: Qualitative results of different generative augmentation methods: Real-Guidance, ALIA, and SaSPA on five FGVC datasets. Real Guidance produces very subtle variations from the original image due to the low translation strength they used. ALIA generates visible variations, but they are considerably less diverse compared to the augmentations produced by SaSPA.", "description": "This figure shows a qualitative comparison of three different generative augmentation methods (Real Guidance, ALIA, and SaSPA) on their ability to augment images from five fine-grained visual classification (FGVC) datasets.  The results highlight that SaSPA generates more diverse and visually distinct augmentations compared to the other two methods. Real Guidance produces minimal changes, ALIA produces noticeable but less diverse changes, and SaSPA creates substantial diversity in augmentations while maintaining visual quality.", "section": "More Visualizations"}, {"figure_path": "MNg331t8Tj/figures/figures_26_2.jpg", "caption": "Figure 8: Randomly selected augmentations of SaSPA that were and were not filtered for Aircraft, CompCars, and CUB.", "description": "This figure shows examples of augmentations generated by SaSPA for three different datasets (Aircraft, CompCars, and CUB).  It visually demonstrates the effectiveness of the filtering process used in the SaSPA pipeline. The top row displays augmentations that passed the filtering, meaning they were deemed to be of sufficient quality and relevant to their respective classes. The bottom row shows augmentations that were filtered out, indicating that they either did not adequately represent their class or were of insufficient quality for use in training. The images help to illustrate the criteria used in the filtering process and showcase the types of augmentations that are deemed suitable for inclusion in the dataset versus those that are discarded.", "section": "More Visualizations"}]