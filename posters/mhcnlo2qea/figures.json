[{"figure_path": "MHCnLo2QeA/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of DSPoser. Our goal is to estimate ego-body pose without dependency on hand controllers in an HMD environment. (a) Given the egocentric video and head tracking signals as input, (b) our approach first predicts the hand pose in the frames where hands are visible (dark blue). It then estimates the hand poses in frames with invisible hands (light blue) using imputation, and (c) estimates uncertainty associated with the hand poses where the hands are invisible, (d) The predicted and imputed hand pose is then used with head pose to predict the 3D full body pose.", "description": "This figure shows the overall pipeline of the proposed method, DSPoser.  It illustrates the process of estimating ego-body pose using egocentric video and head tracking data, even with temporally sparse hand observations. The method involves (a) input egocentric video and head tracking, (b) hand pose prediction, (c) hand pose imputation (filling in missing hand poses) with uncertainty estimation, and (d) using the imputed hand poses and head pose to predict the full 3D body pose.", "section": "1 Introduction"}, {"figure_path": "MHCnLo2QeA/figures/figures_2_1.jpg", "caption": "Figure 2: Overall pipeline of our proposed work DSPoser, composed of Temporal Completion stage and Spatial Completion stage to tackle pose estimation problem from doubly sparse data.", "description": "This figure illustrates the overall architecture of the proposed method, DSPoser, which consists of two main stages: Temporal Completion and Spatial Completion.  The Temporal Completion stage uses a Masked Autoencoder (MAE) to impute missing hand pose data by leveraging correlations between head pose and intermittently visible hand poses.  This stage also estimates uncertainty for the imputed hand poses. The Spatial Completion stage employs a conditional diffusion model (VQ-Diffusion) which utilizes the temporally completed hand poses, head tracking data, and uncertainty estimates to generate full-body pose sequences.", "section": "3 Methods"}, {"figure_path": "MHCnLo2QeA/figures/figures_8_1.jpg", "caption": "Figure 3: Uncertainty visualization of the right hand pose captured by the MAE. Gray areas represent frames where the hand is invisible, and white areas denote visible frames. We depict aleatoric uncertainty within ranges of \u00b110 and \u00b120 from the estimated \u03bc.", "description": "This figure visualizes the uncertainty in right-hand pose estimation by the masked autoencoder (MAE).  Three scenarios are shown: partially visible hands (a-1, a-2) and fully invisible hands (b). Gray shading indicates frames where the hand is not visible. The plots show the estimated hand pose (orange) against the ground truth (red), with shaded areas representing uncertainty intervals of \u00b110 and \u00b120 around the estimated mean. The plots show that the uncertainty increases as the hand visibility decreases.", "section": "3.2 Temporal Completion: Hand Trajectory Imputation from Sparse Hand Pose"}, {"figure_path": "MHCnLo2QeA/figures/figures_8_2.jpg", "caption": "Figure 4: (a) Ego-Exo4D video frames, (b) the corresponding skeleton ground truth and our prediction results, and (c) qualitative results on AMASS data under different input conditions. green indicates the ground truth, blue indicates the predicted result, and red indicates the visible hands. Head only estimates body pose from head trajectories, whereas Ours estimates body pose from imputed hand and head trajectories.", "description": "This figure shows a comparison of the ground truth and predicted human poses using the proposed method on the Ego-Exo4D and AMASS datasets.  The Ego-Exo4D section (b) displays the video frames and a comparison of ground truth skeleton pose with the results obtained by the model.  The AMASS section (c) provides a similar comparison. The model's ability to accurately reconstruct the body pose is visually demonstrated and compared against a baseline ('Head Only') that relies solely on head trajectory estimation, highlighting the benefits of incorporating imputed hand pose information for more complete body pose prediction.", "section": "4 Experiments"}, {"figure_path": "MHCnLo2QeA/figures/figures_15_1.jpg", "caption": "Figure 3: Uncertainty visualization of the right hand pose captured by the MAE. Gray areas represent frames where the hand is invisible, and white areas denote visible frames. We depict aleatoric uncertainty within ranges of \u00b110 and \u00b120 from the estimated \u03bc.", "description": "This figure visualizes the uncertainty in right-hand pose estimation from a masked autoencoder (MAE).  The gray shaded areas represent frames where the hand is not visible in the video. The orange lines represent the predicted hand pose from MAE. The red lines show the ground truth pose. The shaded areas around the predictions represent uncertainty calculated by MAE, showing that the uncertainty is larger when the hand is not visible (gray areas) and smaller when it is visible. The visualization demonstrates the MAE\u2019s ability to estimate uncertainty, which is important because it guides the subsequent step of generating full body pose.", "section": "3.2 Temporal Completion: Hand Trajectory Imputation from Sparse Hand Pose"}, {"figure_path": "MHCnLo2QeA/figures/figures_16_1.jpg", "caption": "Figure 6: Qualitative results showing the groundtruth in green and predicted human pose in blue using our method on Ego-Exo4D dataset.", "description": "This figure presents a qualitative comparison of the proposed method's performance on the Ego-Exo4D dataset.  It shows several sequences of human poses, with each sequence consisting of multiple frames. For each frame, the ground truth pose (obtained through manual annotation or other high-fidelity methods) is shown in green, while the pose estimated by the proposed method (DSPoser) is presented in blue. This visual comparison allows for an assessment of the accuracy and fidelity of DSPoser's estimations across a variety of body movements and viewpoints, providing a clear representation of how well the model captures the nuances of human motion.", "section": "4 Experiments"}, {"figure_path": "MHCnLo2QeA/figures/figures_17_1.jpg", "caption": "Figure 7: Qualitative results on AMASS dataset comparing DSPoser (Ours) against the baselines. Color gradient indicates an absolute positional error, with a higher error corresponding to higher blue intensity. Results demonstrate that motions generated by DSPoser exhibit greater similarity to the ground truth. Furthermore, it highlights higher errors (indicated with red circles) for baselines when the hand is occluded in the ground truth pose (indicated with a black circle).", "description": "This figure compares the qualitative results of different methods (Bodiffusion, AvatarPoser, AvatarJLM, and DSPoser) on the AMASS dataset.  It shows the estimated 3D human poses as a sequence. The color gradient represents the error, with blue indicating low error and lighter shades of blue/white indicating higher errors. Red circles highlight areas of higher error for comparison methods where the hand is occluded in the ground truth, indicating that the proposed method (DSPoser) is robust even when the hand is invisible for a short period. The ground truth poses are also shown for reference.", "section": "4.2 Full Body Pose Estimation from Doubly Sparse data"}, {"figure_path": "MHCnLo2QeA/figures/figures_18_1.jpg", "caption": "Figure 4: (a) Ego-Exo4D video frames, (b) the corresponding skeleton ground truth and our prediction results, and (c) qualitative results on AMASS data under different input conditions. green indicates the ground truth, blue indicates the predicted result, and red indicates the visible hands. Head only estimates body pose from head trajectories, whereas Ours estimates body pose from imputed hand and head trajectories.", "description": "This figure shows qualitative results comparing the ground truth and predicted human poses using the proposed method (DSPoser) on two datasets: Ego-Exo4D and AMASS.  The Ego-Exo4D results (b) illustrate the model's ability to accurately predict full-body poses from temporally and spatially sparse hand and head data.  The AMASS results (c) demonstrate the method's performance on different input conditions, comparing the use of only head trajectories versus the use of head and imputed hand trajectories.", "section": "4 Experiments"}]