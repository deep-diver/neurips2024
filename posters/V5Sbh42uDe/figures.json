[{"figure_path": "V5Sbh42uDe/figures/figures_4_1.jpg", "caption": "Figure 1: Second-order Potts relaxations in Tables 1 and 2: interaction potentials P for pairs of predictions (\u03c3i,\u03c3j) in (4) or pseudo-labels (yi, yj) in (6) are illustrated for K = 2 when each prediction \u03c3i or label yi, i.e. distribution in \u0394\u00b2, can be represented by a single scalar as (x, 1 \u2013 x). The contour maps are iso-levels of P((xi, 1 \u2212 xi), (xj, 1 \u2212 xj)) over domain (xi, xj) \u2208 [0, 1]\u00b2. The 3D plots above illustrate the potentials P as functions over pairs of \u201clogits\u201d (li, lj) \u2208 R\u00b2 where each scalar logit li defines binary distribution (xi, 1 \u2013 xi) for xi = 1+e\u207b\u00b2li \u2208 [0, 1].", "description": "This figure shows six different visualizations of second-order Potts relaxations, three from Table 1 and three from Table 2.  Each visualization includes a 3D plot and a contour map of the interaction potential P for pairs of predictions or pseudo-labels represented by a single scalar. The x-axis and y-axis of both plots represent the input values, while the z-axis shows the value of the interaction potential. The contour maps provide a 2D representation of this interaction.", "section": "2.1 Second-order relaxations of the Potts model"}, {"figure_path": "V5Sbh42uDe/figures/figures_4_2.jpg", "caption": "Figure 2: Examples of \"moves\" for neighboring pixels {i, j} \u2208 N. Their (soft) pseudo-labels yi and yj are illustrated on the probability simplex \u2206K for K = 3. In (a) both pixels i and j are inside a region/object changing its label from A to B. In (b) pixels i and j are on the boundary between two regions/objects; one is fixed to class A and the other changes from class C to B.", "description": "This figure illustrates two scenarios of changes in soft pseudo-labels for neighboring pixels on a probability simplex (a 3D triangle representing probabilities of three classes).  The first example shows both pixels transitioning together from class A to B, highlighting smoothness within a region. The second shows a boundary case where one pixel remains in class A while the other moves from class C to B, illustrating a change across the boundary.", "section": "2.1 Second-order relaxations of the Potts model"}, {"figure_path": "V5Sbh42uDe/figures/figures_4_3.jpg", "caption": "Figure 2: Examples of \"moves\" for neighboring pixels {i, j} \u2208 N. Their (soft) pseudo-labels yi and yj are illustrated on the probability simplex \u2206K for K = 3. In (a) both pixels i and j are inside a region/object changing its label from A to B. In (b) pixels i and j are on the boundary between two regions/objects; one is fixed to class A and the other changes from class C to B.", "description": "This figure shows two examples illustrating how different Potts model relaxations behave.  In the first example (a), both neighboring pixels smoothly transition from one class to another. In the second example (b), only one pixel changes class, while the other remains fixed, representing a boundary scenario.  This helps demonstrate how different relaxations handle smoothness and boundary conditions.", "section": "2.1 Second-order relaxations of the Potts model"}, {"figure_path": "V5Sbh42uDe/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of cross-entropy functions: (a) standard (7), (b) reverse (8), and (c) collision (9). (d) shows the empirical comparison on the robustness to label uncertainty. The test uses ResNet-18 architecture on fully-supervised Natural Scene dataset [30] where we corrupted some labels. The horizontal axis shows the percentage \u03b7 of training images where the correct ground truth labels were replaced by a random label. All losses trained the model using soft target distributions \u0177 = \u03b7*u+(1-\u03b7)*y representing the mixture of one-hot distribution y for the observed corrupt label and the uniform distribution u, following [29]. The vertical axis shows the test accuracy. Training with the reverse and collision cross-entropy is robust to much higher levels of label uncertainty.", "description": "This figure compares three cross-entropy functions: standard, reverse, and collision.  It shows how these functions handle uncertainty in soft pseudo-labels (probabilistic labels) during training. The experiment corrupts labels in a fully supervised setting and measures the test accuracy with different corruption levels.  Reverse and collision cross-entropy show greater robustness to label noise compared to the standard cross-entropy.", "section": "2.2 Cross-entropy and soft pseudo-labels"}, {"figure_path": "V5Sbh42uDe/figures/figures_6_1.jpg", "caption": "Figure 4: Comparison of cross-entropy terms.", "description": "This figure compares the performance of three different cross-entropy terms (HCE, HRCE, HCCE) across various scribble lengths.  The mIoU (mean Intersection over Union) on the validation set is plotted against the ratio of scribble length. The results show that the collision cross-entropy (HCCE) consistently outperforms the other two methods across all scribble lengths.", "section": "3.2 Comparison of cross-entropy terms"}, {"figure_path": "V5Sbh42uDe/figures/figures_7_1.jpg", "caption": "Figure 5: Pseudo-labels generated from given network predictions using different neighborhoods.", "description": "This figure shows pseudo-labels generated from network predictions using different neighborhood systems. It visually compares the pseudo-labels obtained using nearest neighbor (NN) and dense neighborhoods with bandwidths of 25 and 100.  The results illustrate how the choice of neighborhood system influences the quality and smoothness of the generated pseudo-labels, impacting the overall semantic segmentation performance.", "section": "3.3 Comparison of neighborhood systems"}, {"figure_path": "V5Sbh42uDe/figures/figures_8_1.jpg", "caption": "Figure 6: Comparison of different methods using Potts relaxations. The architecture is DeeplabV3+ with the backbone MobileNetV2. Optimization", "description": "This figure compares the performance of different methods for semantic segmentation using various Potts relaxations.  The x-axis represents the ratio of scribble length, and the y-axis shows the mean Intersection over Union (mIoU) on the validation set. The methods compared include full supervision (using all pixels for training), gradient descent with dense and grid neighborhoods using the quadratic Potts model, and soft self-labeling with different cross-entropy terms and Potts relaxations (including HCCE + P0 and HCCE + PCD).  The results illustrate the effectiveness of the soft self-labeling approach, particularly with HCCE + PCD, achieving performance close to or exceeding that of the fully supervised method.", "section": "3.4 Soft self-labeling vs. hard self-labeling vs. gradient descent"}, {"figure_path": "V5Sbh42uDe/figures/figures_13_1.jpg", "caption": "Figure 7: Illustration of the difference among Potts relaxations. The visualization of soft pseudo-labels uses the convex combination of RGB colors for each class weighted by pseudo-label itself.", "description": "Figure 7 shows the results of applying different Potts relaxations to the same input image and network predictions. The image on the left shows the input image, ground truth mask, network predictions and entropy. The images on the right show the pseudo-labels generated using different methods and visualization of soft pseudo-labels using different Potts relaxations. The visualization uses a convex combination of RGB colors for each class weighted by pseudo-label itself.", "section": "3.1 Comparison of Potts relaxations"}]