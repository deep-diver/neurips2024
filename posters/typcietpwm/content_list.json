[{"type": "text", "text": "Conditional Outcome Equivalence: A Quantile Alternative to CATE ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Josh Givens University of Bristol josh.givens@bristol.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Henry W J Reeve University of Bristol henry.reeve@bristol.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Song Liu University of Bristol song.liu@bristol.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Katarzyna Reluga University of Bristol katarzyna.reluga@bristol.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The conditional quantile treatment effect (CQTE) can provide insight into the effect of a treatment beyond the conditional average treatment effect (CATE). This ability to provide information over multiple quantiles of the response makes the CQTE especially valuable in cases where the effect of a treatment is not wellmodelled by a location shift, even conditionally on the covariates. Nevertheless, the estimation of the CQTE is challenging and often depends upon the smoothness of the individual quantiles as a function of the covariates rather than smoothness of the CQTE itself. This is in stark contrast to the CATE where it is possible to obtain high-quality estimates which have less dependency upon the smoothness of the nuisance parameters when the CATE itself is smooth. Moreover, relative smoothness of the CQTE lacks the interpretability of smoothness of the CATE making it less clear whether it is a reasonable assumption to make. We combine the desirable properties of the CATE and CQTE by considering a new estimand, the conditional quantile comparator (CQC). The CQC not only retains information about the whole treatment distribution, similar to the CQTE, but also having more natural examples of smoothness and is able to leverage simplicity in an auxiliary estimand. We provide finite sample bounds on the error of our estimator, demonstrating its ability to exploit simplicity. We validate our theory in numerical simulations which show that our method produces more accurate estimates than baselines. Finally, we apply our methodology to a study on the effect of employment incentives on earnings across different age groups. We see that our method is able to reveal heterogeneity of the effect across different quantiles. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In many real world scenarios such as personalised treatment allocation and individual level policy decisions, understanding the effect of a treatment/intervention at an individual level is invaluable in providing bespoke care. The field which aims to understand a treatment\u2019s effect given certain covariates is referred to as heterogeneous treatment effect (HTE) estimation and has seen popularity across many applications [11, 10, 24, 20]. Within HTE, the conditional average treatment effect (CATE) has proved itself to be a popular target of study in this area due to its simplicity and interpretability [2, 13, 28]. A key limitation of the CATE however, is that it fails to paint a full picture of the differences between distributions of the two responses. In addition, it can be sensitive to outliers, with extreme values leading to a biased outcome. As such, the conditional quantile treatment effect (CQTE), an estimand which compares the conditional quantiles of the distributions in the treated and untreated populations, has established itself as a popular alternative [1, 5, 26]. ", "page_idx": 0}, {"type": "text", "text": "While the CQTE offers more information than the CATE and is more robust to outliers, it lacks some of the CATE\u2019s desirable estimation properties. Specifically, CQTE estimation involves estimating the quantile functions for the two marginal outcomes. This harms the estimation procedure in cases where estimation of marginal quantile functions is more challenging than estimation of the CQTE itself. An example of this is when the marginal quantile functions are less smooth as a function of the covariates than the CQTE. This aligns with a recurring idea within HTE estimation that the effect of a treatment may be simpler than the marginal outcomes. In contrast to the CQTE, there are many CATE estimators which aim to learn the CATE directly allowing them to exploit its relative simplicity. These include the X-learner [17], R-learner [23], and Doubly Robust (DR) learner [15]. Before estimating the CATE, these procedures require estimation of intermediary estimands (nuisance parameters) which condition on the covariates such as the average marginal outcomes and the propensity score (the probability of being assigned to treatment group). These nuisance parameters are then used to aid the estimation of the CATE. With the DR learner specifically, it has been shown that it can still achieve optimal convergence rates even when estimation of the nuisance parameters is worse than estimation of the CATE itself. This notion is referred to as double robustness, as our estimation is robust to sub-optimal estimation of both of the nuisance parameters. ", "page_idx": 1}, {"type": "text", "text": "Some attempts have been made to improve CQTE estimation [34, 33] with a key work being that of Kallus and Oprescu [14]. In this they provide an extension of the double robustness property to the CQTE, creating an estimation procedure that can achieve strong convergence even when nuisance parameters are more difficult to estimate. Unfortunately, one of the nuisance parameters which must be estimated is the reciprocal of the conditional densities over the response. These are highly difficult to estimate and risk the errors blowing up in low density regions which could potentially nullify the desirable estimation rates they achieve even with the dependence on the estimation accuracy of these nuisance parameters being less strong. Furthermore it is still unclear how one can interpret relative smoothness in the CQTE compared to the individual quantiles with their being relatively little discussion of this within the literature. In general there is a distinct lack of illustrative examples; which are present for the CATE. To our knowledge no other works specifically aim to tackle this double robustness phenomenon for the CQTE or other quantile based treatment effect estimands. ", "page_idx": 1}, {"type": "text", "text": "We introduce a novel estimand called the \u201cconditional quantile comparator\u201d (CQC). The CQC gives the outcome for a treated individual in the same quantile as a given outcome for an untreated individual, conditional on covariates. This relates to the conditional Quantile-Quantile (QQ) plot for the treated and untreated outcomes as demonstrated in Figure 1. Similarly to the CQTE, our new estimand, the CQC, allows us to compare equivalent quantiles while working exclusively in the response landscape, making it a more interpretable tool. This allows us to construct canonical examples of the CQC being smoother than various nuisance parameters, the CQTE, and the CATE; adding to this interpretability. In addition, using the pseudo-outcome framework presented in Kennedy [15], we can leverage CATE estimation procedures to estimate the CQC in a doubly robust way, as mentioned above. Crucially, the CQC can keep the valuable quantile-level information previously offered by the CQTE while building on much of the CATE literature to acquire its desirable robustness properties and interpretability. Our contributions are as follows: ", "page_idx": 1}, {"type": "image", "img_path": "tyPcIETPWM/tmp/e7be22fe612f26856bae7b5d70cc2b97eb6bf61e371976ae20e6c3cc958bed76.jpg", "img_caption": ["Figure 1: The left plot gives the CQC surface which takes in covariates $(x)$ and an untreated response $(y)$ and returns the treated response of the equivalent quantile $(g^{*}(y|x))$ . The right plot is a QQ-plot of the responses $(Y)$ in the untreated $\\acute{A}=0$ ) vs treated ( $\\lvert A=1$ ) population conditional on various covariates $(X=0,0.5,1)$ ). These conditional QQ-plots correspond to \u201cslices\u201d of the CQC surface, as shown by the coloured lines in the left plot. The plot is best viewed in colour. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 Introduce a new estimand for HTE analysis: the conditional quantile comparator (CQC).   \n\u2022 Propose an estimation procedure which we prove to be doubly robust.   \n\u2022 Demonstrate better estimation accuracy especially when the CQC is smooth but individual conditional cumulative distribution functions are not.   \n\u2022 Provide insights into real-world datasets on employment intervention and medical treatment. ", "page_idx": 2}, {"type": "text", "text": "2 Set-up ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We now introduce the standard HTE set-up in our notation. Let $Z$ denote the random triple $(Y,X,A)$ with $Y$ a random variable (RV) on $y\\subseteq\\mathbb{R}$ , $X$ a RV on $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ , and $A$ a RV on $\\{0,1\\}$ . We treat $Z$ as representing an individual and interpret the components as ", "page_idx": 2}, {"type": "text", "text": "$Y$ : Outcome/Response $X$ : Observed covariates $A$ : Treatment assignment ", "page_idx": 2}, {"type": "text", "text": "Remark 1. We could view our setting as coming from a potential outcome framework [27]. Under this framework we assume there exists RVs $Y_{0},Y_{1}$ on $\\boldsymbol{\\wp}$ representing the outcome with and without treatment and that $Y\\equiv Y_{A}$ . $Y_{1-A}$ would then be unobserved/unknown for each individual. ", "page_idx": 2}, {"type": "text", "text": "We define the propensity score $\\pi:\\mathcal{X}\\rightarrow(0,1)$ by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi(\\pmb{x}):=\\mathbb{P}(A=1|X=\\pmb{x}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "in other words, $\\pi$ denotes the conditional probability of being assigned to treatment given the covariates. We shall assume that $\\pi$ is continuous and bounded away from 0 and 1. From a potential outcomes perspective, this means that each individual could potentially be assigned to either treatment. We also define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F_{a}(y|\\pmb{x}):=\\mathbb{P}(Y\\leq y|X=\\pmb{x},A=a),}\\\\ {F_{a}^{-1}(\\alpha|\\pmb{x}):=\\operatorname*{inf}\\{y\\in\\mathbb{R}|F_{a}(y|\\pmb{x})\\geq\\alpha\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and refer to them as the conditional cumulative distribution function $'C C D F)$ and the quantile function respectively. We also refer to $F_{a}^{-1}$ in (2) as the generalised inverse of $F_{a}$ . ", "page_idx": 2}, {"type": "text", "text": "We can now define the CATE and CQTE to be given by $\\tau:\\mathcal{X}\\rightarrow\\mathbb{R}$ and $\\tau_{q}:[0,1]\\times\\mathcal{X}\\to\\mathbb{R}$ with ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\tau(\\pmb{x}):=\\mathbb{E}[Y|X=\\pmb{x},A=1]-\\mathbb{E}[Y|X=\\pmb{x},A=0],}\\\\ &{\\tau_{q}(\\alpha|\\pmb{x}):=F_{1}^{-1}(\\alpha|\\pmb{x})-F_{0}^{-1}(\\alpha|\\pmb{x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We let $D\\,:=\\,\\{Z_{i}\\}_{i=1}^{2n}\\,\\equiv\\,\\{(Y_{i},X_{i},A_{i})\\}_{i=1}^{2n}$ for $n\\,\\in\\,\\mathbb{N}$ be IID copies of $Z$ representing our data sample with $i$ indexing the individual. We assume an even number of samples for notational convenience. For $a\\in\\{0,1\\}$ , we take $I_{a}:=\\{i|A_{i}=1\\}$ , the indices of individuals on treatment $a$ . We can then define $D_{a}:=\\{Z_{i}\\}_{\\{i\\in I_{a}\\}}$ and $n_{a}:=|I_{a}|$ as the dataset and sample size of those on treatment $a$ . ", "page_idx": 2}, {"type": "text", "text": "For $n\\in\\mathbb N$ , let $[n]:=\\{1,\\ldots,n\\}$ . For a vector $\\pmb{w}\\in\\mathbb{R}^{p}$ let $w_{j}$ to represent the $j^{\\mathrm{th}}$ component of $\\pmb{w}$ and let $\\lVert\\pmb{w}\\rVert$ be the Euclidean norm unless otherwise specified. We also take $\\lVert\\pmb{w}\\rVert_{1}$ as the 1-norm and $\\begin{array}{r}{\\|\\pmb{w}\\|_{\\infty};=\\operatorname*{max}_{j\\in[p]}|w_{j}|}\\end{array}$ . We keep a summary table of all notation used in Appendix A.1. ", "page_idx": 2}, {"type": "text", "text": "2.1 Introducing the quantile comparator ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Our aim is to find \u201cequivalent quantiles\" between the treated and non-treated distributions conditional on the covariates. Specifically, for each $y_{0}\\in\\mathcal{V}$ , $x\\in\\mathcal{X}$ we aim to find $y_{1}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\nF_{1}(y_{1}|x)=F_{0}(y_{0}|\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This now allows us to define our primary estimand of interest, the conditional quantile comparator. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Conditional quantile comparator (CQC)). For our triple $(Y,X,A)$ , the conditional quantile comparator is the measurable function $g^{*}:\\mathcal{V}\\times\\mathcal{X}\\to\\mathcal{V}$ such that, for all $y\\in\\mathcal{Y},\\;x\\in\\mathcal{X}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nF_{1}(g^{*}(y|\\pmb x)|\\pmb x)=F_{0}(y|\\pmb x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We then simply define $y_{1}$ as $g^{*}(y_{0}|x)$ . The name conditional quantile comparator derives from the fact that it returns the value of $y_{1}$ in the equivalent quantile of $Y|X=\\mathbf{x},A=1$ as the quantile of $Y|X=\\mathbf{x},A=0$ that $y_{0}$ is in. ", "page_idx": 3}, {"type": "text", "text": "Remark 2. For simplicity and to ensure such a function is well defined, we will assume that $Y|X=$ ${\\pmb x},A=a$ is a continuous $R V$ for any given $\\pmb{x}\\in\\mathcal{X}$ , $a\\in\\{0,1\\}$ with strictly positive density on its support. We will however allow the support of $Y|X=x,A=a$ to vary in both $\\textbf{\\em x}$ and $a$ . ", "page_idx": 3}, {"type": "text", "text": "We now introduce another estimand which will serve as a useful stepping stone in our estimation. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (CCDF contrasting function). The CCDF contrasting function is defined to be $h^{*}:\\mathcal{V}\\times\\mathcal{V}\\times\\mathcal{X}\\to[-1,1]$ given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nh^{*}(y_{0},y_{1}|x):=F_{1}(y_{1}|x)-F_{0}(y_{0}|x).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This estimand allows following alternative definitions for $g^{*}$ which help its interpretation and estimation (detailed later). We take $h^{*-1}$ representing the inverse of $h^{*}$ with respect to the $2^{\\mathrm{nd}}$ argument. ", "page_idx": 3}, {"type": "equation", "text": "$$\ng^{*}(y_{0}|\\pmb{x})=h^{*-1}(y_{0},0|\\pmb{x})=F_{1}^{-1}(F_{0}(y_{0}|\\pmb{x})|\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The second equality still holds if we replace the inverses in the above with generalised inverses. The equality (4) falls straight from the definition of each object and shows how we can use $h^{*}$ to estimate $g^{*}$ . Moreover, the equality (4) allows us to generalise $g^{\\ast}$ to discontinuous $Y$ (or pdfs with non-trivial 0 density regions inside the support). ", "page_idx": 3}, {"type": "text", "text": "2.2 Exploring the CQC ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We specifically focus on the CQC, $g^{\\ast}$ , as we feel it gives insightful information allowing comparison of the two distributions $(Y|X,A=1)$ and $(Y|X,A=0)$ . ", "page_idx": 3}, {"type": "text", "text": "The CQC allows us to compare the two distributions beyond simply a single point estimate such as that given by the CATE. This is especially valuable in cases where the two distributions differ beyond just a shift. For example, the effect of some treatments varies greatly between individuals with the same or similar covariates. An example of this is antidepressants, where some patients respond positively while others may have adverse reactions leading to a worse outcome than no treatment whatsoever. Another example is the use of opioids as painkillers where some patients have an increased tolerance making them less effective [12, 22]. ", "page_idx": 3}, {"type": "text", "text": "As well as being of interest on its own, the quantile comparator relates closely to other estimands of interest. For example, if we take $\\Delta^{*}(y_{0}|x):=g^{*}(y_{0}|x)\\,\\bar{-}\\,y_{0}$ then $\\Delta^{*}$ tells us whether the equivalent quantile in the treated distribution is higher or lower. This estimand then serves as a heuristic for whether the treatment is beneficial at that untreated response value. ", "page_idx": 3}, {"type": "text", "text": "Furthermore, CQTE can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tau_{q}(\\alpha|\\pmb{x})=\\Delta^{*}\\left(F_{Y|X,A=0}^{-1}(\\alpha|\\pmb{x})\\Big|\\pmb{x}\\right)=g^{*}\\left(F_{Y|X,A=0}^{-1}(\\alpha|\\pmb{x})\\Big|\\pmb{x}\\right)-F_{Y|X,A=0}^{-1}(\\alpha|\\pmb{x}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "linking the quantile comparator back to the CQTE. This equivalence highlights the perspective that the CQC can be seen as rephrasing the input of the CQTE in terms of the outcome space. ", "page_idx": 3}, {"type": "text", "text": "A key idea within CATE literature is the notion that the CATE itself may be a simpler estimand to study than the marginal treatment outcomes $(\\mathbb{E}[Y|X=x,A=a])$ may be individually. One can exploit this feature to improve the CATE\u2019s estimation. A similar concept exists with the CQC as we will see in the example below. ", "page_idx": 3}, {"type": "text", "text": "Example 1 (Illustrative Example). Suppose that ", "page_idx": 3}, {"type": "equation", "text": "$$\nY|X=x,A=0\\sim\\operatorname{N}(\\sin(10x),\\,\\,1^{2}),\\quad\\quad\\quad Y|X=x,A=1\\sim\\operatorname{N}(2\\sin(10x),\\,\\,2^{2}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then we have $g^{*}(y|x)=2y$ which does not depend on $x$ and does not include the sine term present in the individual $C D F s$ . Interestingly, $\\mathbb{E}[Y|\\bar{X},A\\;=\\;1]\\,-\\,\\mathbb{E}[Y|X,A\\;=\\;0]\\;=\\;\\sin(x)$ hence the CATE is still non-constant in this case (the same also holds for the CQTE). Additionally, we have $\\Delta^{*}(y|x)=g^{*}(y|x)-y=y$ suggesting the intervention is beneficial for positive y and detrimental for negative $y$ . We now show $3D$ plots of a CCDF, the CQC, and the CQTE in Figure 2. ", "page_idx": 3}, {"type": "image", "img_path": "tyPcIETPWM/tmp/2d88e1ea1dc2b0d0c6553bc50f6902fde67dfc3519cb7418197aded2c1a03259.jpg", "img_caption": ["Figure 2: Surface plots for CCDF (panel (a)), CQC (panel (b)) and CQTE (panel (c)). We can see that CCDF, and CQTE have high-frequency change in $x$ while the CQC does not depend on $x$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Example 2 (General Smoothness Case). Suppose we are in the potential outcomes framework so that $Y_{0},Y_{1}$ exist with $Y\\,\\equiv\\,Y_{A}$ . Now also suppose that $Y_{1}\\,=\\,\\phi(Y_{0},X)$ for some transformation $\\phi$ increasing in $Y_{0}$ for each $X$ . Then $\\phi$ gives the CQC (i.e. $\\phi\\;=\\;g^{*}.$ ) meaning that smoothness of the CQC can be seen as smoothness of $\\phi$ . This gives a generalisation of the CATE case where smoothness is present when $Y_{1}=Y_{0}+\\psi(X)$ with $\\psi$ smooth ", "page_idx": 4}, {"type": "text", "text": "$A$ specific example could be a treatment which halves all individuals blood pressure. In this case $\\phi^{*}\\dot{(}y,\\dot{\\pmb x})=g^{*}\\dot{(}\\dot{y}|\\pmb x)=\\textstyle{\\frac{1}{2}}y$ and so the CQC is smooth but the CQTE and CATE would not be if the individual responses are non-smooth. ", "page_idx": 4}, {"type": "text", "text": "3 Estimation procedure ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now describe our estimation procedure for the CQC which is motivated by equation (4). At a high level our approach for estimating $g^{*}(y_{0}|x)$ will be the following: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Estimate $h^{*}(y_{0},.|x)$ using a pseudo-outcome \u2013 a specified proxy response computed from $(Y,X,A)$ which we will regress against. \u2022 Find the value $\\hat{y}_{1}$ which makes our estimate of $h^{*}(y_{0},.|x)$ closest to 0. ", "page_idx": 4}, {"type": "text", "text": "Section 3.1 and Algorithm 1 give our $h^{*}$ estimation procedure while Section 3.3 and Algorithm 2 give our $g^{\\ast}$ estimation procedure. ", "page_idx": 4}, {"type": "text", "text": "3.1 Estimating the CCDF contrasting function $h^{*}$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We focus on estimating $h^{*}$ primarily for its two nice properties: ", "page_idx": 4}, {"type": "text", "text": "1. Similar to $g^{*}$ , $h^{*}$ can exhibit smoothness even when the individual CCDFs are not smooth.   \n2. The estimation of $h^{*}$ can be re-framed as a CATE problem. ", "page_idx": 4}, {"type": "text", "text": "The first property is important as the smoothness of $h^{*}$ determines the best estimation rate that can be achieved when using non-parametric regression, setting a target for our approach. In particular, smoother functions have better estimation rates. The second property is important as it gives us a method for attaining this target rate. By re-framing the estimation as a CATE problem, we can leverage existing results to build a robust estimator which achieves the target estimation accuracy rate even when the rate of estimating nuisance parameters is sub-optimal. We demonstrate this robustness later using finite sample bounds on the estimation accuracy (Proposition 1 & Theorem 2). ", "page_idx": 4}, {"type": "text", "text": "First, we show how the estimation of $h^{*}$ can be solved using a CATE estimator. Note that ", "page_idx": 4}, {"type": "equation", "text": "$$\nh^{*}(y_{0},y_{1}|x)=\\mathbb{E}[\\mathbb{1}\\{Y\\leq y_{1}\\}|X=x,A=1]-\\mathbb{E}[\\mathbb{1}\\{Y\\leq y_{0}\\}|X=x,A=0].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Hence, for a given $y_{0},y_{1}$ , if we define the RV $W_{y_{0},y_{1}}:=\\mathbb{1}\\{Y\\le y_{A}\\}$ , then estimating $h^{*}(y_{0},y_{1}|.)$ is equivalent to estimating the CATE with $W_{y_{0},y_{1}}$ replacing $Y$ as the response. To perform this estimation, we turn to a recent method developed by Kennedy [15]. They propose to write the CATE as a conditional expectation of a function of $Z$ called a pseudo-outcome. A robust estimator is then obtained by regressing this pseudo-outcome against $X$ . In our setting, the pseudo-outcome with the new response $W_{y_{0},y_{1}}$ , for a sample $(y,x,a)$ is given by ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\varphi_{y_{0},y_{1}}(y,{\\pmb x},a):=\\frac{a-\\pi({\\pmb x})}{\\pi({\\pmb x})(1-\\pi({\\pmb x}))}\\left\\{1\\{y\\leq y_{a}\\}-F_{a}(y_{a}|{\\pmb x})\\right\\}+F_{1}(y_{1}|{\\pmb x})-F_{0}(y_{0}|{\\pmb x})}}\\\\ {{=\\displaystyle\\frac{a-\\pi({\\pmb x})}{\\pi({\\pmb x})(1-\\pi({\\pmb x}))}\\left\\{1\\{{\\pmb y}\\leq y_{a}\\}-F_{a}(y_{a}|{\\pmb x})\\right\\}+h^{*}(y_{0},y_{1}|{\\pmb x}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $h^{*}(y_{0},y_{1}|x)=\\mathbb{E}[\\varphi_{y_{0},y_{1}}(Z)|X=x]$ (Proposition 5, Appendix C.1), regressing $\\varphi_{y_{0},y_{1}}(Z)$ on $X$ provides an estimate for $h^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "As we do not know the CDFs nor the propensity score, we need to replace them in (6) with estimates. We define $\\hat{\\pi},\\hat{F}_{0},\\hat{F}_{1}$ to be estimates of $\\pi,F_{0},F_{1}$ respectively. We then construct $\\hat{\\varphi}_{y_{0},y_{1}}$ in the same way as $\\varphi_{y_{0},y_{1}}$ , but using estimated quantities $\\hat{\\pi}$ , $\\hat{F}_{a}$ instead. $\\hat{\\varphi}_{y_{0},y_{1}}$ can now serve as the pseudooutcome in our regression. We also use sample splitting to de-correlate the propensity score and CDF estimates from the $h^{*}$ estimate. This helps make our estimator doubly robust, as we will see in the following theory. ", "page_idx": 5}, {"type": "text", "text": "We are now ready to define our Doubly Robust (DR)-learner to estimate $h^{*}$ in Algorithm 1. ", "page_idx": 5}, {"type": "table", "img_path": "tyPcIETPWM/tmp/6630dab330c69adb3a1bd77a9842ed5138f87765e3b0d6777cc3d6267802f6dc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Remark 3. Cross-fitting can be implemented by repeating the procedure with the roles of $\\mathcal{T}$ and $\\mathcal{I}$ switched and then averaging the two estimates of $h^{*}$ . We could also perform this procedure multiple times with different random splits of the data to improve our estimator\u2019s potential stability. ", "page_idx": 5}, {"type": "text", "text": "Note that our algorithm is not specific on which form of regression to use allowing for any parametric or non-parametric procedure. Further to this, it can also be easily adapted to use other pseudooutcome procedures such as the R-learner of Nie and Wager [23] or a standard inverse propensity weighting approach which we describe in Appendix A.3. ", "page_idx": 5}, {"type": "text", "text": "3.2 Finite sample bound of $h^{*}$ estimator ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we prove the estimation accuracy of $\\hat{h}$ , which will play important roles in the following notation and assumptions we need for estimating $g^{*}$ . These accuracy statements will be made for an arbitrarily fixed $\\pmb{x}\\in\\mathcal{X}$ . For our theoretical and experimental results we use linear smoothers for the final regression. This means our estimate $\\hat{h}$ and oracle estimate $h^{\\prime}$ are of the form ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{h}(y_{0},y_{1}|x)=\\sum_{j\\in\\mathcal{I}}w_{j}\\hat{\\varphi}_{y_{0},y_{1}}(Z_{j})\\qquad\\qquad h^{\\prime}(y_{0},y_{1}|x)=\\sum_{j\\in\\mathcal{I}}w_{j}\\varphi_{y_{0},y_{1}}(Z_{j})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the weights $w_{j}\\;\\equiv\\;w_{j}({\\pmb x},X_{\\mathcal{I}})$ are constructed using $X_{\\mathcal{I}}\\;:=\\;\\{X_{j}\\}_{j\\in\\mathcal{J}}$ with $w_{j}~\\geq~0$ and $\\lVert\\pmb{w}\\rVert_{1}$ . Linear smoothers encompass a broad class of estimation techniques used in both low and high-dimensional settings. Examples include $\\boldsymbol{\\mathrm{k}}$ -NN regression [8, 9], kernel ridge regression [29], generalised forests [3], and Mondrian forests [18]. Additionally linear smoothers have been shown to adapt to intrinsic low dimensionality in regression problems in higher dimensions [16], making them an apt estimator for our purposes. ", "page_idx": 5}, {"type": "text", "text": "For $\\phi:\\mathcal{Z}\\to\\mathbb{R}$ treated as deterministic and $p>1$ , we also define the norms ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\phi\\|_{w^{s}};={\\sqrt{\\frac{1}{\\|w^{s}\\|_{1}}\\sum_{i\\in{\\mathcal{I}}}w_{j}^{s}\\mathbb{E}\\left[\\phi(Z)^{2}|X=X_{i}\\right]}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and take $\\|\\phi\\|_{\\pmb{w}}:=\\|\\phi\\|_{\\pmb{w}^{1}}$ . Note that this norm is random as the weights depend upon $X_{\\mathcal{I}}$ . ", "page_idx": 5}, {"type": "text", "text": "We now aim to show that we are able to exploit smoothness in $h^{*}$ even when the CCDFs and propensity score are less smooth. We introduce the notion of smoothness through H\u00f6lder functions. ", "page_idx": 6}, {"type": "text", "text": "Definition 3 (H\u00f6lder functions). We say that a function $f:\\,x\\,\\rightarrow\\,\\mathbb{R}$ is $(\\gamma,C)$ -H\u00f6lder for $\\gamma\\ \\in$ $(0,1],C\\geq1$ if for any $\\mathbf{\\boldsymbol{x}}^{\\prime},\\mathbf{\\boldsymbol{x}}^{\\prime\\prime}\\in\\mathcal{X}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n|f(\\pmb{x}^{\\prime})-f(\\pmb{x}^{\\prime\\prime})|\\leq C\\|\\pmb{x}^{\\prime}-\\pmb{x}^{\\prime\\prime}\\|^{\\gamma}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, larger $\\gamma$ represents a smoother function which can be estimated at faster rates. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. For any $y_{0},y_{1}\\in\\mathcal{Y},\\;\\delta>0,a\\in\\{0,1\\}.$ : ", "page_idx": 6}, {"type": "text", "text": "(a) There exists $\\xi\\in(0,1/2]$ such that $\\pi(\\pmb{x}),\\hat{\\pi}(\\pmb{x}^{\\prime})\\in[\\xi,1-\\xi]$ for all $\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathcal{X}$ . ", "page_idx": 6}, {"type": "text", "text": "$(b)$ With probability at least $1-\\delta_{i}$ , $\\begin{array}{r}{\\|\\hat{\\varphi}_{y_{0},y_{1}}-\\varphi_{y_{0},y_{1}}\\|_{w^{2}}\\!\\leq\\varepsilon_{\\hat{\\varphi}}(n,\\delta)\\,.}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "(c) With probability at least 1 \u2212\u03b4, $\\lVert\\hat{\\pi}-\\pi\\rVert_{w}\\leq\\varepsilon_{\\alpha}(n,\\delta)$ and $\\|F_{a}(y_{a}|.)-\\hat{F}_{a}(y_{a}|.)\\|_{w}\\!\\leq\\varepsilon_{\\beta}(n,\\delta).$ (d) For $\\gamma\\in(0,1]$ , $C\\geq1$ , $h^{*}(y_{0},y_{1}\\vert.$ ) is $(\\gamma,C)$ -H\u00f6lder. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1(a) exists to ensure for any covariate value, neither treatment assignment has too low a probability. Assumption 1(b) controls the convergence of the estimated pseudo-outcome to the true pseudo-outcome. Assumption 1(c) sets up the smoothness of the propensity score and CCDFs alongside the convergence rates of their estimators as $\\varepsilon_{\\alpha},\\varepsilon_{\\beta}$ respectively. Assumption 1(d) sets up the smoothness of $h^{*}$ which will control the convergence rate of the oracle estimation procedure. Assumptions 1 (c) and (d) control the accuracy of our estimator in the following result. ", "page_idx": 6}, {"type": "text", "text": "Proposition 1. Suppose that Assumption $^{\\,I}$ holds and let $\\hat{h}$ be a linear smoother estimated as in Algorithm 1. Then for any $y_{1},y_{0}\\in\\mathcal{Y}$ , $\\delta\\in(0,2/e]$ and our $\\pmb{x}\\in\\mathcal{X}$ , with probability at least $1-\\delta$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|\\hat{h}(y_{0},y_{1}|\\pmb{x})-h^{*}(y_{0},y_{1}|\\pmb{x})\\Big|\\leq\\varepsilon_{h}(n,\\delta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Here, for each $\\delta\\in(0,2/e]$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{h}(n,\\delta):=\\sqrt{2\\log(8/\\delta)/n}\\,\\varepsilon_{\\hat{\\varphi}}(n,\\delta/4)+\\varepsilon_{\\alpha}(n,\\delta/4)\\varepsilon_{\\beta}(n,\\delta/4)+\\varepsilon_{\\gamma}(n,\\delta/4),}\\\\ &{\\varepsilon_{\\gamma}(n,\\delta):=|\\mathbb{E}[h^{\\prime}(y_{0},y_{1}|\\pmb{x})-h^{*}(y_{0},y_{1}|\\pmb{x})|X_{\\mathcal{T}},D_{\\mathcal{T}}]|}\\\\ &{\\qquad\\qquad+\\sqrt{2\\log(2/\\delta)/n}\\,\\|\\pmb{w}\\|\\|\\varphi-h(y_{0},y_{1}|.)\\|_{w^{2}}+2\\|\\pmb{w}\\|_{\\infty}\\log(2/\\delta)/(3\\xi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We have that $\\varepsilon_{\\gamma}$ gives an upper bound on the accuracy of the oracle estimation and so acts as a target for our estimation procedure. If $\\varepsilon_{\\hat{\\varphi}}(n,\\delta)\\rightarrow0$ then the first term in $\\varepsilon_{h}$ is guaranteed to be $o(\\varepsilon_{\\gamma}(n,\\bar{\\delta}))$ for fixed $\\delta$ . Hence the first and last terms converge at oracle rates with respect to $n$ . As the $\\varepsilon_{\\alpha}$ and $\\varepsilon_{\\beta}$ terms are multiplied together we can obtain better rates than either of them individually have. This is because both $\\varepsilon_{\\alpha}$ and $\\varepsilon_{\\beta}$ can converge to 0 slower than $\\varepsilon_{\\gamma}$ while their product $\\varepsilon_{\\alpha}\\cdot\\varepsilon_{\\beta}$ converges quicker. This provides the desired double robustness as our estimation can converge at oracle rates even when convergence for the nuisance parameters is slower. Now that we have this we can convert our estimate of $h^{*}$ , into an estimate of $g^{*}$ . ", "page_idx": 6}, {"type": "text", "text": "3.3 Estimating the conditional quantile comparator $g^{*}$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In order to obtain an estimate of $g^{*}(y_{0}|x)$ at a fixed $y_{0},x$ , we need to obtain estimates of $h^{*}(y_{0},y_{1}|\\pmb{x})$ at various values of $y_{1}$ . As $h^{*}$ is monotonic, we would then like to search for a monotonic function which aligns with these estimates. This monotonicity is especially important because it allows us to bound the estimation accuracy of $\\hat{h}$ uniformly over all $y_{1}$ at a similar rate to our pointwise accuracy. With this, we can easily translate the estimation accuracy in $\\hat{h}$ (obtained in proposition 1) into the accuracy in $\\hat{g}$ . Additionally, it simplifies the process of inverting $\\hat{h}$ . In general our method in Algorithm 1 will not produce monotonic $\\hat{h}$ (this is in contrast to some other approaches such as an IPW pseudo-outcome, see Appendix A.3), or separately estimating the CCDFs). We can however obtain a monotonic estimate of $h^{*}$ using isotonic projection. ", "page_idx": 6}, {"type": "text", "text": "Definition 4 (Isotonic Projection). We define the isotonic projection of $\\pmb{\\alpha}^{\\prime}\\in\\mathbb{R}^{p}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nP(\\pmb{\\alpha}^{\\prime}):=\\underset{\\pmb{\\alpha}\\in\\mathrm{Iso}(p)}{\\mathrm{argmin}}\\|\\pmb{\\alpha}-\\pmb{\\alpha}^{\\prime}\\|\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathrm{Iso}(p):=\\{\\alpha\\in\\mathbb{R}^{p}|\\alpha_{j}\\leq\\alpha_{l+1}\\;\\forall l\\in[p-1]\\}$ , the set of all isotonic vectors in $\\mathbb{R}^{p}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4. We can use the Pool Adjacent Violators Algorithm (PAVA) [6] which performs isotonic projection and is implemented in the IsotonicRegression class of sci-kit learn in Python $I25J$ . ", "page_idx": 7}, {"type": "text", "text": "Hence, for a fixed value of $(y_{0},x)$ and a set of predictions $\\hat{\\alpha}_{l}=\\hat{h}(y_{0},y_{1}^{(l)}|x)$ with $y^{(l)}\\leq y^{(l+1)}$ , we can take $\\tilde{\\alpha}:=P_{\\mathrm{Iso}(p)}(\\hat{\\alpha})$ and use these to obtain a new monotonic estimate of $h^{*}$ . Furthermore, by a result in Yang and Barber [32], $\\tilde{\\alpha}$ will be at least as accurate as $\\hat{\\pmb{\\alpha}}$ in the worst case. We now describe our approach for estimating the CQC using this projection approach in Algorithm 2. ", "page_idx": 7}, {"type": "table", "img_path": "tyPcIETPWM/tmp/67a8e87cd4109061b07f2338b3673999548f1c33ca86835c4bd8d6223351b620.jpg", "table_caption": ["Algorithm 2 DR estimation procedure for the CQC $g^{*}$ "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Remark 5. For the case where $\\hat{h}$ is a step function, these steps can serve as our evaluation points while for continuous $\\hat{h}$ one could take these candidate $y_{1}$ points at small evenly spaced intervals. Empirically we also find that $\\hat{h}$ is already close to isotonic and so step 3. of the algorithm is mostly for the theoretical justification of our approach. ", "page_idx": 7}, {"type": "text", "text": "While it may seem inefficient to be estimating the CQC via the CCDF contrasting function and then inverting, due to the monotonicity of $h^{*}$ , we actually pay a very small cost in estimation accuracy for having to estimate the CCDF contrasting function over all $y_{1}$ . We make this notion more explicit in the following section. ", "page_idx": 7}, {"type": "text", "text": "3.4 Finite sample bound of the CQC estimator ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now provide the accuracy of our estimate $\\hat{g}$ obtained by Algorithm 2 when used in conjunction with linear smoothers. We assume that $\\hat{F}_{a}$ are also fit using linear smoothers of the form ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{F}_{a}(y|\\pmb{x}^{\\prime},a)=\\sum_{i\\in\\mathbb{Z}}w_{F_{a};i}(\\pmb{x}^{\\prime};X_{\\mathbb{Z}},A_{\\mathbb{Z}})\\mathbb{1}\\{Y_{i}\\leq y\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with $w_{j}(\\pmb{x}^{\\prime})\\equiv w_{j}(\\pmb{x}^{\\prime},X_{\\mathcal{Z}},A_{\\mathcal{Z}})>0,\\|\\pmb{w}(\\pmb{x}^{\\prime})\\|_{1}\\!=1.$ . We will also require the following assumptions. ", "page_idx": 7}, {"type": "text", "text": "Assumption 2. For our $R V X$ , any $y\\in\\mathcal{Y},\\;\\pmb{x}^{\\prime}\\in\\mathcal{X},\\,\\delta<e^{-1}.$ : ", "page_idx": 7}, {"type": "text", "text": "(a) There exists some $s,\\eta>0$ such that $F_{1}(y^{\\prime}|x)\\ge\\eta$ for all $y^{\\prime}\\in B_{s}\\big(g^{*}(y|\\pmb{x})\\big)$ . (b) $W_{\\cdot}p$ . at least $1-\\delta$ , $\\begin{array}{r}{\\operatorname*{max}_{j\\in\\mathcal{J}}w_{j}\\le\\varepsilon_{w}(n,\\delta)\\;a n d\\operatorname*{max}_{i\\in\\mathcal{Z}}w_{F_{a};i}(X)\\le\\varepsilon_{w}(n,\\delta).}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Assumption 2(a) is a mild assumption which allows us to convert the \u02c6h accuracy into $\\hat{g}$ accuracy while Assumption 2(b) bounds the rates of decay of the weights in our linear smoothers. ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Let $\\hat{g}$ be estimated as using Algorithm 2 with $\\{Y_{i}\\}_{i\\in I_{1}}$ , sorted and then used as our evaluation points and linear smoothers used for regressions in Algorithm $^{\\,l}$ . Then provided Assumptions $^{\\,l}$ & 2 hold we have that for $\\delta\\in(0,e^{-\\tilde{1}})$ and sufficiently large $n$ , w.p. at least $1-\\delta$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\hat{g}(y|\\pmb{x})-g^{*}(y|\\pmb{x})|\\!\\leq2\\left(\\eta^{-1}\\varepsilon_{h}(n,\\delta/(2n))+\\xi^{-1}\\varepsilon_{w}(n,\\delta/(2n))\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "From this result we see that if our weights decay at rate faster than $\\varepsilon_{h}(n,\\delta)$ then this error will be dominated by the $\\varepsilon_{h}$ term. We believe this to hold in most cases and show that it does comfortably when using Nadaraya-Watson (NW) estimation [21, 31] with a box kernel in Appendix C.4. Furthermore, if the dependence on $\\delta$ in both terms is of the form $\\log^{c}(1/\\delta)$ for some $c>0$ then we obtain the same rate of estimation as for $h^{*}$ up to polylog factors. This means we translate our desirable double robustness $\\hat{h}$ over to $\\hat{g}$ . We also obtain finite sample bounds on $\\mathbb{E}[|\\hat{g}(Y|x)-g^{*}(Y|x)|~~|~A=0,\\hat{g}]$ with high probability and present this in Appendix C.3. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We now apply our approach to a series of simulated and real data scenarios in order to demonstrate the utility of our estimand and the effectiveness of our estimation procedure. For these, we use NW estimation as our regression procedure throughout. See appendix A.2 for details on NW estimation. ", "page_idx": 7}, {"type": "text", "text": "4.1 Simulated experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we test our method\u2019s performance in terms of our estimator\u2019s mean absolute error under simulated scenarios.1 In each scenario we test against a separate estimator which estimates the two CCDFs separately and simply takes their difference, an IPW pseudo-outcome estimator detailed in Appendix A.3, the CQTE estimator of Kallus and Oprescu [14], and the oracle DR estimator where $\\hat{\\varphi}$ is replaced with $\\varphi$ (i.e. exact $\\pi,F_{a}$ are used). In this experiment, we return back to the set-up of example 1. We now change the frequency of the sine term by taking ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{Y|X=x,A=0\\sim\\operatorname{N}(\\sin(\\gamma\\pi x),\\,1^{2}),\\quad}&{\\quad Y|X=x,A=1\\sim\\operatorname{N}(2\\sin(\\gamma\\pi x),\\,2^{2}),}\\\\ {\\pi(x)=0.4\\sin(\\gamma\\pi x)+0.5.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for $\\gamma\\,\\in\\,[0,10]$ so that increasing $\\gamma$ imitates decreasing smoothness of our nuisance parameters. In our experiments half the samples are used to estimate the propensity score and CCDFs and the other half are used to regress against the pseudo-outcome. Our estimate $\\hat{g}$ is then compared against $g^{*}$ using a hold-out testing set. This process is repeated 500 times with new training data on each run. From this, a Monte-Carlo estimate of $\\mathbb{E}_{\\hat{g}}[\\mathbb{E}_{X}[\\mathbb{E}_{Y|X,A=0}[|\\hat{g}(Y|X)-g^{*}(Y|X)|]]$ is produced alongside $95\\%$ confidence intervals (CIs). In our first experiment, we let $2n=1000$ and vary $\\gamma$ in $[0,1\\bar{0}]$ . In our second experiment, we let $\\gamma=6$ and $2n$ vary in [200, 5000]. The results of this are shown in Figure 3. ", "page_idx": 8}, {"type": "image", "img_path": "tyPcIETPWM/tmp/72ffbfc9b885f622a97fc10e8099bfdac8ff76669d610684e083a449b28cd3b7.jpg", "img_caption": ["Figure 3: Mean absolute error with $95\\%$ CIs for various estimators. The left plot has fixed sample size ( $2n=1000)$ ) and increasing $\\gamma$ . The right plot has $\\gamma=6$ and increasing sample size. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We can see that the average error decreases as the sample size $2n$ grows and mildly increases as $\\gamma$ grows. This is expected as increasing $\\gamma$ makes estimating the nuisance parameters more challenging. The result shows that the proposed DR method achieves the best performance compared with the Separate and IPW estimators and is only marginally outperformed by the oracle estimator. Additionally we see that the method of Kallus and Oprescu [14] is much more affected by the increase in $\\gamma$ . This is because unlike the CQC, the CQTE has a complexity that depends on the frequency term $(\\gamma)$ . We also observe much better performance as the sample size increases. We hypothesise that the plateau in the CQTE approach is due to the difficulty of estimating the reciprocal of the PDF, which causes the estimation to be unstable irrespective of sample size. Further simulated experiments including 10-dimensional $\\textbf{\\em x}$ and linear CQC are given in Appendix B.1. ", "page_idx": 8}, {"type": "text", "text": "4.2 Real world employment example ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To show the performance in the real-world scenarios, we use a dataset on an employment programme which has been studied in various prior works [4, 5, 26]. Within the programme, some participants were given job placements or temporary help jobs while others received no intervention. Participants\u2019 earnings were then monitored over the next 8 quarters following their enrolments. We take their net earnings as our response $(Y)$ and the employment intervention as the treatment ( $\\left.A\\right.=1$ ). We use each participant\u2019s age at their entry to the study as our covariate $(X)$ . We fit our quantile comparator function on 2,000 participants. Figure 4 shows our estimate of $\\Delta^{*}(y|\\pmb{x})=g^{*}(y|\\pmb{x})-y$ for various values of $(y,x)$ . ", "page_idx": 8}, {"type": "text", "text": "We see that participants around age 23 and between ages 32-37 benefit most from this scheme, as indicated by the darker colour on the heat map. Additionally, the lower quantile of the income distribution (wage $\\leq\\mathbb{S}7500;$ ) shows the least change, indicating that wage improvements primarily occur for the higher income group. For participants aged 40, there appears to be little change in outcomes overall. To demonstrate our approach in a medical setting, we apply our method to evaluate the effectiveness of a colon cancer treatment, as detailed in Appendix B.3. For comparative purposes, we also provide an estimate of the CQTE for this data in Appendix B.2. ", "page_idx": 8}, {"type": "image", "img_path": "tyPcIETPWM/tmp/ad62c433b6222e32fce03c320af0f24f3f7950f0194ebc401c82849187a29908.jpg", "img_caption": ["Figure 4: Surface and heat plot of $\\Delta^{*}(y|x)$ for our employment data with $X=\\!\\mathrm{Age}$ , $Y$ =Income. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The theory provided here gives a strong foundation and motivation for framing our problem in this particular manner. There is however a great deal more to be explored in this area from a theoretical perspective. For example, one immediate improvement would be to give a more general case where our weight decay (in Assumption 2 (b) for Theorem 2) is sufficiently fast. In addition more work needs to be done exploring the relationship between the smoothness of $g^{*}$ and the smoothness $h^{*}$ . Smoothness in $h^{*}$ appears to be a stronger condition so ideally we would like to make theoretical statements directly on the smoothness of $g^{*}$ . Interestingly our experiments on synthetic data do seem to suggest that it is the complexity of $g^{*}$ which drives the estimation rate as in these experiments $h^{*}$ increases in complexity while $g^{\\ast}$ remains constant. Additionally, changing our experiment in Section 4.1 to have uniform response so that both $h^{*},g^{*}$ are constant rather than just $g^{\\ast}$ , seems to give no material improvement to the performance of our estimator (see Appendix B.1.3). ", "page_idx": 9}, {"type": "text", "text": "Another limitation of our current estimation procedure is that it requires learning an estimand and then inverting it to obtain our final estimator. While this process is relatively simple, it could be streamlined and made more computationally efficient if we could produce a more direct estimator similar to the DR-Learner for CATE [15] or the CQTE estimator in Kallus and Oprescu [14]. ", "page_idx": 9}, {"type": "text", "text": "Finally, while we have been able to provide more concrete examples of smoothness for the CQC these are still limited to the case of a deterministic treatment effect which we would like to expand upon. This is closely related to a more general limitation with quantile-based estimands, the CQC included, in that they lack meaningful interpretability for the individual. While the CATE can be viewed as the expected difference in an individual\u2019s outcome on and off the treatment, no such individual-level interpretation exists for the CQC or indeed any other estimand trying to learn higher level distributional information than the mean. As a result the CATE is still a more naturally interpretable estimand. To facilitate this interpretation however, one still needs to make assumptions about a lack of confounding between the treatment assignment and the potential outcomes, which are only verifiable in certain restrictive scenarios [30]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we have introduced a new treatment effect estimand, the conditional quantile comparator and demonstrated its efficacy both in terms of its doubly robust estimation, and its ability to provide valuable data insights. This is a promising direction as it allows quantile-based treatment effect exploration to \u201ckeep up\" with the CATE in terms of estimation quality offering more flexibility as to which estimand can be used to best describe the data. For these reasons, we see the CQC as an exciting and worthwhile new direction within the HTE framework. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Josh Givens was supported by a PhD studentship from the EPSRC Centre for Doctoral Training in Computational Statistics and Data Science (COMPASS). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abadie, A., Angrist, J., and Imbens, G. (2002). Instrumental variables estimates of the effect of subsidized training on the quantiles of trainee earnings. Econometrica, 70(1):91\u2013117.   \n[2] Abadie, A. and Imbens, G. W. (2002). Simple and bias-corrected matching estimators for average treatment effects. Working Paper 283, National Bureau of Economic Research. Series: Technical working paper series.   \n[3] Athey, S., Tibshirani, J., and Wager, S. (2019). Generalized random forests. The Annals of Statistics, 47(2):1148 \u2013 1178. Publisher: Institute of Mathematical Statistics.   \n[4] Autor, D. H. and Houseman, S. N. (2010). Do temporary-help jobs improve labor market outcomes for low-skilled workers? Evidence from \"Work First\". American Economic Journal: Applied Economics, 2(3):96\u2013128.   \n[5] Autor, D. H., Houseman, S. N., and Kerr, S. P. (2017). The effect of work first job placements on the distribution of earnings: An instrumental variable quantile regression approach. Journal of Labor Economics, 35(1):149\u2013190.   \n[6] Barlow, R. E., Bartholomew, D. J., Bremner, J. M., and Brunk, H. D. (1972). Statistical Inference under Order Restrictions (The Theory and Application of Isotonic Regression). Wiley Series in Probability and Statistics. John Wiley & Sons Ltd.   \n[7] Boucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: a nonasymptotic theory of independence. Oxford University Press.   \n[8] Chen, G. (2019). Nearest neighbor and kernel survival analysis: Nonasymptotic error bounds and strong consistency rates. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th international conference on machine learning, volume 97 of Proceedings of machine learning research, pages 1001\u20131010. PMLR.   \n[9] Chen, P., Dong, W., Lu, X., Kaymak, U., He, K., and Huang, Z. (2019). Deep representation learning for individualized treatment effect estimation using electronic health records. Journal of Biomedical Informatics, 100:103303.   \n[10] Collins, F. S. and Varmus, H. (2015). A new initiative on precision medicine. The New England journal of medicine, 372(9):793\u2013795. Place: United States.   \n[11] Hirano, K. and Porter, J. R. (2009). Asymptotics for statistical treatment rules. Econometrica, 77(5):1683\u20131701. Publisher: [Wiley, The Econometric Society].   \n[12] Huynh, P., Villaluz, J., Bhandal, H., Alem, N., and Dayal, R. (2021). Long-Term Opioid Therapy: The Burden of Adverse Effects. Pain Medicine, 22(9):2128\u20132130.   \n[13] Imbens, G. W. (2004). Nonparametric estimation of average treatment effects under exogeneity: a review. The Review of Economics and Statistics, 86(1):4\u201329.   \n[14] Kallus, N. and Oprescu, M. (2023). Robust and agnostic learning of conditional distributional treatment effects. In Ruiz, F., Dy, J., and van de Meent, J.-W., editors, Proceedings of the 26th international conference on artificial intelligence and statistics, volume 206 of Proceedings of machine learning research, pages 6037\u20136060. PMLR.   \n[15] Kennedy, E. H. (2023). Towards optimal doubly robust estimation of heterogeneous causal effects. Electronic Journal of Statistics, 17(2):3008 \u2013 3049. Institute of Mathematical Statistics and Bernoulli Society.   \n[16] Kpotufe, S. (2011). k-NN regression adapts to local intrinsic dimension. In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K., editors, Advances in neural information processing systems, volume 24. Curran Associates, Inc.   \n[17] K\u00fcnzel, S. R., Sekhon, J. S., Bickel, P. J., and Bin Yu (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the National Academy of Sciences, 116(10):4156\u20134165.   \n[18] Lakshminarayanan, B., Roy, D. M., and Teh, Y. W. (2014). Mondrian forests: Efficient online random forests. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger, K., editors, Advances in neural information processing systems, volume 27. Curran Associates, Inc.   \n[19] Laurie, J. A., Moertel, C. G., Fleming, T. R., Wieand, H. S., Leigh, J. E., Rubin, J., McCormack, G. W., Gerstner, J. B., Krook, J. E., and Malliard, J. (1989). Surgical adjuvant therapy of large-bowel carcinoma: an evaluation of levamisole and the combination of levamisole and fluorouracil. The North Central Cancer Treatment Group and the Mayo Clinic. Journal of clinical oncology, 7(10):1447\u20131456. Place: United States.   \n[20] Lei, L. and Cand\u00e8s, E. J. (2021). Conformal inference of counterfactuals and individual treatment effects. Journal of the Royal Statistical Society, Series B, 83(5):911\u2013938.   \n[21] Nadaraya, E. (1965). On non-parametric estimates of density functions and regression curves. Theory of Probability & Its Applications, 10(1):186\u2013190.   \n[22] Nadeau, S. E., Wu, J. K., and Lawhern, R. A. (2021). Opioids and chronic pain: An analytic review of the clinical evidence. Frontiers in Pain Research, 2.   \n[23] Nie, X. and Wager, S. (2020). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299\u2013319.   \n[24] Obermeyer, Z. and Emanuel, E. J. (2016). Predicting the Future - Big Data, Machine Learning, and Clinical Medicine. The New England journal of medicine, 375(13):1216\u20131219. Place: United States.   \n[25] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830.   \n[26] Powell, D. (2020). Quantile Treatment Effects in the Presence of Covariates. The Review of Economics and Statistics, 102(5):994\u20131005.   \n[27] Rubin, D. B. (2005). Causal inference using potential outcomes. Journal of the American Statistical Association, 100(469):322\u2013331.   \n[28] Semenova, V. and Chernozhukov, V. (2021). Debiased machine learning of conditional average treatment effects and other causal functions. The Econometrics Journal, 24(2):264\u2013289.   \n[29] Singh, R., Xu, L., and Gretton, A. (2023). Kernel methods for causal functions: dose, heterogeneous and incremental response curves. Biometrika, 111(2):497\u2013516. tex.eprint: https://academic.oup.com/biomet/article-pdf/111/2/497/57467664/asad042.pdf.   \n[30] VanderWeele, T. J. (2008). The sign of the bias of unmeasured confounding. Biometrics. Journal of the International Biometric Society, 64(3):702\u2013706. Publisher: [Wiley, International Biometric Society].   \n[31] Watson, G. S. (1964). Smooth regression analysis. Sankhy\u00afa: The Indian Journal of Statistics, Series A, pages 359\u2013372.   \n[32] Yang, F. and Barber, R. F. (2019). Contraction and uniform convergence of isotonic regression. Electronic Journal of Statistics, 13(1):646 \u2013 677. Publisher: Institute of Mathematical Statistics and Bernoulli Society.   \n[33] Ying Zhang, Lei Wang, M. Y. and Shao, J. (2020). Quantile treatment effect estimation with dimension reduction. Statistical Theory and Related Fields, 4(2):202\u2013213. Publisher: Taylor & Francis tex.eprint: https://doi.org/10.1080/24754269.2019.1696645.   \n[34] Zhou, T., Carson, W. E. t., and Carlson, D. (2022). Estimating Potential Outcome Distributions with Collaborating Causal Networks. Transactions on machine learning research, 2022. Place: United States. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Additional details ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "tyPcIETPWM/tmp/014470daf390f3a3e9902771951254ecd89ac2885ee3d81bb3453e7582d74285.jpg", "table_caption": ["A.1 Notation table ", "Table 1: Table of notation. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "tyPcIETPWM/tmp/23d5becfddc840bd5df75b260cd7dafe888ad8a4394cde7c9b8c8454c0258145.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.2 Nadaraya-Watson estimation ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Throughout, we use NW estimation as our standard non-parametric regression technique. For a kernel $k:\\mathcal{X}\\times\\mathcal{X}\\to[0,\\infty)$ , IID data sample $D:=\\{(Y_{i},X_{i})\\}_{i=1}^{n}$ , and $\\pmb{x}\\in\\mathcal{X}$ the NW estimate of $\\mathbb{E}[Y|X={\\pmb x}]$ is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}{\\frac{k({\\pmb x},X_{i})}{\\sum_{j=1}^{n}k({\\pmb x},X_{j})}}Y_{i}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Applying this to our pseudo-outcome regression in Algorithm 1, we get that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\hat{h}(y_{0},y_{1}|x):=\\sum_{i\\in\\mathcal{I}}\\frac{k(x,X_{i})}{\\sum_{j\\in\\mathcal{I}}k(x,X_{j})}\\hat{\\varphi}_{y_{0},y_{1}}(Y_{i},X_{i},A_{i})}\\quad}&{}\\\\ &{=\\displaystyle\\sum_{i\\in\\mathcal{I}}\\frac{k(x,X_{i})}{\\sum_{j\\in\\mathcal{I}}k(x,X_{j})}\\Biggl(\\frac{A_{i}-\\hat{\\pi}(X_{i})}{\\hat{\\pi}(X_{i})(1-\\hat{\\pi}(X_{i}))}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\cdot\\,\\left\\{\\mathbb{I}\\{Y_{i}\\leq y_{A_{i}}\\}-\\hat{F}_{Y|X,A_{i}}(y_{A_{i}}|X_{i})\\right\\}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\,\\,\\hat{F}_{a}\\left(y_{1}|X_{i}\\right)-\\hat{F}_{0}\\left(y_{0}|X=X_{i}\\right)\\Biggr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where for any $\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathcal{X},a\\in\\{0,1\\}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\hat{\\pi}(\\pmb{x}^{\\prime}):=\\sum_{i\\in\\mathcal{I}}\\frac{k(\\pmb{x}^{\\prime},X_{i})}{\\sum_{j\\in\\mathcal{I}}k(\\pmb{x}^{\\prime},X_{j})}\\Im\\{A_{i}=1\\}}\\\\ &{\\displaystyle\\hat{F}_{a}(y_{a}|\\pmb{x}^{\\prime}):=\\sum_{i\\in\\mathcal{I}}\\frac{k(\\pmb{x}^{\\prime},X_{i})}{\\sum_{j\\in\\mathcal{I}}k(\\pmb{x}^{\\prime},X_{j})\\Im\\{A_{j}=a\\}}\\Im\\{Y_{i}\\leq y_{a}\\}\\mathbb{1}\\{A_{i}=a\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that for our theoretical results we do not specify how our nuisance parameters are estimated and the results only depend on the accuracy of our estimation of the nuisance parameters. ", "page_idx": 13}, {"type": "text", "text": "Remark 6. We can re-write (8) as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\hat{h}(y_{0},y_{1}|x):=\\sum_{i\\in{\\cal Z}_{1}}\\frac{k(x,X_{i})}{\\sum_{i=1}^{n}k(x,X_{i})}\\bigg(\\frac{1}{\\hat{\\pi}(X_{i})}\\left\\{\\mathbb{I}\\{Y_{i}\\leq y_{1}\\}-\\hat{F}_{1}(y_{1}|X_{i})\\right\\}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\hat{F}_{1}(y_{1}|X_{i})-\\hat{F}_{0}(y_{0}|X_{i})\\bigg)}}\\\\ {{\\displaystyle-\\,\\sum_{i\\in{\\cal Z}_{0}}\\frac{k(x,X_{i})}{\\sum_{i=1}^{n}k(x,X_{i})}\\bigg(\\frac{1}{1-\\hat{\\pi}(X_{i})}\\left\\{\\mathbb{I}\\{Y_{i}\\leq y_{0}\\}-\\hat{F}_{0}(y_{0}|X_{i})\\right\\}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\hat{F}_{0}(y_{0}|X_{i})-\\hat{F}_{1}(y_{1}|X_{i})\\bigg)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Which can be helpful in terms of the practical implementation. ", "page_idx": 13}, {"type": "text", "text": "A.3 IPW pseudo-outcome estimator ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Another pseudo-outcome one can use for estimating the treatment effect is the based on inverse propensity weighting. Specifically we can take our pseudo-outcome to be ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\psi_{y_{0},y_{1}}(Y^{\\prime},X^{\\prime},A^{\\prime}):=\\frac{A^{\\prime}-\\pi(X^{\\prime})}{\\pi(X^{\\prime})(1-\\pi(X^{\\prime}))}\\mathbb{1}\\{Y^{\\prime}\\leq y_{A^{\\prime}}\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If we regress against it using NW estimation, and also use NW estimation for our nuisance parameter estimation as in (9) & (10), our estimate $\\hat{h}$ will be increasing in $y_{1}$ and decreasing in $y_{0}$ meaning we do not need to perform any isotonic projection. ", "page_idx": 14}, {"type": "text", "text": "A.4 Additional experimental details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As the method of Kallus and Oprescu [14] is one for estimating the CQTE, we transform it into an estimator of the CQC (which we denote by $\\hat{g}$ ) using the following formula where $\\hat{\\tau}_{q}(.|.)$ is our CQTE estimator ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{g}(y|\\pmb{x})=\\hat{\\tau}_{q}\\left(F_{Y|X,0}(y|\\pmb{x})|\\pmb{x}\\right)+y.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "using the true CCDF, $F_{Y\\mid X,0}$ . ", "page_idx": 14}, {"type": "text", "text": "This is the inverse of equation (5) which defines the CQTE in terms of the CQC. Conversely we also tested transforming all our CQC estimators into CQTE estimators (using exactly equation (5) with $\\hat{g}$ replacing $g^{*}$ ) and testing the accuracy on this space and found similar results. ", "page_idx": 14}, {"type": "text", "text": "Each experiment took no longer than 1 hour to run on a single 4 core CPU with 8GB of RAM. ", "page_idx": 14}, {"type": "text", "text": "The bandwidths of the kernels for the NW estimation of the nuisance parameters were chosen by a limited grid search on additional simulated data by validating against the true value of the nuisance parameters. While this is unrealistic in practice it was done to make estimation of the nuisance parameters as strong as possible. This was to err on the side of caution as strong nuisance parameter estimation would naturally favour the baseline approaches. ", "page_idx": 14}, {"type": "text", "text": "Code to implement our approach alongside Jupyter notebooks running our numerical experiments can be found in the supplementary materials. The code to implement the kernels is adapted from https://github.com/wittawatj/kernel-gof/ which is free to use under the MIT Licence. ", "page_idx": 14}, {"type": "text", "text": "B Additional results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Simulation results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We run additional simulated experiments in order to further test and explore our approach. Throughout, our overall experimental set-up is the same as in Section 4.1 ", "page_idx": 14}, {"type": "text", "text": "B.1.1 10-dimensional example ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To test our problem in higher dimensions we ran experiments where $X$ was 10-dimensional with $X$ uniform on $[-1,1]^{10}$ . That is each component $X_{j}$ was independent with $X_{j}\\sim U(-1,1)$ . We then took ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y|X={\\pmb x},A={a}\\sim N(\\sin(\\gamma\\pi({\\beta}^{\\top}{\\pmb x}),1)}\\\\ {\\pi({\\pmb x})=0.4\\sin(\\gamma\\pi({\\beta}^{\\top}{\\pmb x}))+0.5}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\beta$ was randomly sampled\u221a from $N(0,(0.2)^{2})$ and $\\gamma\\in[0,3]$ . This gave the maximum gradient multiplied by $0.5\\dim(\\mathcal{X})=\\sqrt{d}$ close to 1 making the rate of change of the CDFs and propensities similar to our 1-dimensional examples. ", "page_idx": 14}, {"type": "text", "text": "In our first experiment we take $2n=1000$ and $\\gamma\\in\\{0,0.5,1,1.5,2,2.5,3\\}$ . The results of this are shown in Figure 5a In our second experiment we take $\\gamma=1$ and $2n\\in\\{200,\\bar{5}00,1000,2000,5000\\}$ . The results of this are shown in Figure 5b. ", "page_idx": 14}, {"type": "text", "text": "As we can see the DR approach performs the best with it performing close to oracle for low sample size and low frequency. As the frequency increases the DR estimator deviates from the oracle. ", "page_idx": 14}, {"type": "image", "img_path": "tyPcIETPWM/tmp/f980b3dc01b9dc4e45ede7882b7a9059e8f48ec6041a21a7004d518b88bd8b30.jpg", "img_caption": ["(a) Average error as $\\gamma$ increases with $95\\%$ CIs for(b) Average error as sample size $n$ increases with various approaches. $95\\%$ CIs for various approaches. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 5: Estimation accuracy of 10-dimensional CQC estimation procedures for highly varying CCDFs and constant CQC with respect to $\\textbf{\\em x}$ . ", "page_idx": 15}, {"type": "text", "text": "B.1.2 Varying CQC ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this experiment our set-up is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Y|X,A=0\\sim N\\left(\\frac{\\sin(\\gamma\\pi x)}{0.5x+1.5},1\\right)}\\\\ &{Y|X,A=1\\sim N\\left(\\sin(\\gamma\\pi x)+0.25x+0.75,(0.5x+1.5)^{2}\\right)}\\\\ &{\\qquad\\quad\\pi(x)=0.4\\sin(\\gamma\\pi x)+0.5}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for $\\gamma\\,\\in\\,[0,10]$ . This gives $g^{*}\\,=\\,(y+0.5)(0.5x+1.5)$ which is still simpler than the individual CCDFs but now does depend upon $x$ . ", "page_idx": 15}, {"type": "text", "text": "In our first experiment we take $2n=1000$ and $\\gamma\\in\\{0,2,4,6,8,10\\}$ . The results of this are shown in figure 5a In our second experiment we take $\\gamma=6$ and $2n\\in\\{200,500,1000,2000,5000\\}$ . The results of this are shown in Figure 5b. ", "page_idx": 15}, {"type": "image", "img_path": "tyPcIETPWM/tmp/7dc988849c8556113f2a1a2376ea16064efbc9a29350ce619b263827c372ae4c.jpg", "img_caption": ["(a) Average error as $_n$ increases with $95\\%$ CIs for(b) Average error as $n$ increases with $95\\%$ CIs for various approaches. various approaches. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: Estimation accuracy of CQC estimation procedures for highly varying CCDFs and linear CQC with respect to $x$ . ", "page_idx": 15}, {"type": "text", "text": "B.1.3 Constant $h^{*}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In our previous examples, while $g^{*}$ has been simple and or constant, $h^{*}$ has actually included the high frequency sine term. In this experiment we adjust our original illustrative example to give a constant $h^{*}$ as well. Specifically we take ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Y|X=x,A=0\\sim\\mathrm{Unif}(\\sin(\\gamma\\pi x),\\sin(\\gamma\\pi x)+1),}\\\\ &{Y|X=x,A=1\\sim\\mathrm{Unif}(2\\sin(\\gamma\\pi x),2\\sin(\\gamma\\pi x)+2).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for $\\gamma~\\in~[0,10]$ . We also take the propensity to be $\\pi(x)~=~0.4\\sin(\\gamma\\pi x)+0.5$ . In this case $h^{*}(y_{0},y_{1}|\\dot{\\pmb x})=\\dot{\\textstyle{\\frac{1}{2}}}y_{1}-y_{0}$ which does not depend on $\\textbf{\\em x}$ for any $y_{0},y_{1}\\in\\mathcal{Y}$ . ", "page_idx": 15}, {"type": "text", "text": "In our first experiment we take $2n=1000$ and $\\gamma\\in\\{0,2,4,6,8,10\\}$ . The results of this are shown in figure 5a In our second experiment we take $\\gamma=6$ and $2n\\in\\{200,500,1000,2000,5000\\}$ . The results of this are shown in Figure 5b. As we can see we obtain very similar results to original ", "page_idx": 16}, {"type": "image", "img_path": "tyPcIETPWM/tmp/7ec29016a8fdc7e05081a7886a975b491e66248e644f369eb9ee0a0b9dc3efbb.jpg", "img_caption": ["(a) Average error as $_n$ increases with $95\\%$ CIs for(b) Average error as $n$ increases with $95\\%$ CIs for various approaches. various approaches. ", "Figure 7: Constant $h^{*}$ "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 8: Estimation accuracy of CQC estimation procedures for highly varying CCDFs and constant $h^{*}$ and CQC. ", "page_idx": 16}, {"type": "text", "text": "illustrative example suggesting that our method is effectively exploiting simplicity in $g^{*}$ rather than in $h^{*}$ . ", "page_idx": 16}, {"type": "text", "text": "B.2 Employment example: Comparing to the CQTE ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To further explore the interpretability of our estimator we provide an estimate of the CQTE for comparison. This can be seen in Figure 9. ", "page_idx": 16}, {"type": "image", "img_path": "tyPcIETPWM/tmp/95d9242a49dd6791af77d165ce87fede808a704c428931f86af2f32f3df21936.jpg", "img_caption": ["Figure 9: Surface and heat plot of the CQTE, $\\tau_{q}^{*}(\\alpha|\\pmb{x})$ , for our employment data with $X=\\!\\mathrm{Age}$ , $\\alpha$ =Income Quantile. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "In these plots we can see that the interpretation of the CQTE is less immediate than for the case of the CQC. This is because, for each covariate value, the quantile value corresponds to a different untreated response making comparisons between various values of the covariates less direct. From this plot we do still see that higher income quantiles are associated with a greater increase in wages. Interestingly the value of the CQTE plummets $90\\%$ at the highest quantile for individuals of age 40. This is likely an estimation error due to the limited data at higher quantiles as and higher ages. ", "page_idx": 16}, {"type": "text", "text": "B.3 Colon cancer treatment ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To demonstrate the effectiveness of our approach in medical settings, we apply it to a trial on the effect of colon cancer treatment on survival time/time to remission. This dataset was originally introduced in Laurie et al. [19] and can be found in the \u201csurvival\" package in R and loaded with the line data(colon, package $=$ \"survival\"). In this dataset 929 are randomised to either receive Placebo or Levamisole. They are then followed up for a period of up to 3329 days and the time till either death or recurrence of their cancer. For our analysis we take our response $(Y)$ as the time to first of death or recurrence. For simplicity, when an individual makes it to the end of a trial without an event and is censored we take that to be the time of their event. Looking at the data censoring times are mostly at around 2,000-3,000 days while over $90\\%$ of death or recurrence events that do occur, occur within 1,500 days. Therefore censored individuals will still have better responses than those who had events, as we would want. As our covariate, we took the patients\u2019 ages when joining the trial. We show a 3D plot and heat plot of the estimate of $\\Delta(y|x)=\\bar{g}^{*}(y|x)-\\bar{y}$ in Figure 10. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "tyPcIETPWM/tmp/5c99ed314dd7114c84f60bf9b1b7560d30abf5429275f157f9dba5ec5bdabe49.jpg", "img_caption": ["Figure 10: Surface plot and heat plot of $\\Delta(y|x)$ over $y,x$ for colon cancer trial data with $X=\\!\\mathrm{Age}$ , $Y{=}^{\\prime}$ Time to Event. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Interestingly from Figure 10 we see that the treatment appears to have little effect on patients aged 53-57. For the reaming patients, we can see that there is relatively little effect on the time to event for events of 400 days or less while at around 500 days the difference in time to event jumps to 1000 days. This jump takes the time to event (TTE) to the time when censoring begins. This gives evidence that rather than the treatment delaying the time to event, it increases the proportion of the patients who have no event during the trial essentially fully treating those patients for the duration of the trial. This shows the value of the CQC it was able to reveal meaningful information about the treatment beyond simply its positive effect. On top of this, we are able to identify the censoring within our results without explicitly controlling for it in any way. ", "page_idx": 17}, {"type": "text", "text": "C Additional theory ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Proof of $\\hat{h}$ accuracy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We first define some additional notation. For an output $f$ which depends upon $z=(y,x,a)$ define $m_{f}(\\pmb{x})=\\mathbb{E}[f(Z)|X=\\pmb{x}]$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{m}_{f}(\\pmb{x})=\\sum_{j\\in\\mathcal{I}}w_{j}f(Z_{j}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $w_{j}$ defined as before so that $\\hat{m}_{\\hat{\\varphi}}({\\pmb x})$ is our estimator and $\\hat{m}_{\\varphi}({\\pmb x})$ is the oracle estimator. ", "page_idx": 17}, {"type": "text", "text": "Additionally define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{b}({\\pmb x}):=m_{\\hat{\\varphi}-\\varphi}({\\pmb x}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "in other words the bias in our estimate of the pseudo-outcome for a given $\\textbf{\\em x}$ . We will also work with $\\hat{m}_{b}({\\pmb x})$ where we view $b$ as being a function of $z$ . ", "page_idx": 17}, {"type": "text", "text": "Finally for $n\\in\\mathbb N$ , $\\delta\\in(0,1)$ , define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\varepsilon_{\\delta}(n):=\\left\\{\\!\\!\\begin{array}{l l}{\\log(2/\\delta)/n}&{\\mathrm{if}\\;\\delta\\in(0,2/e]}\\\\ {1/n}&{\\mathrm{otherwise}.}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem 3 (Stability Result). For $\\delta\\in(0,1)$ we have that with probability at least $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\hat{m}_{\\varphi}({\\pmb x})-m_{\\varphi}({\\pmb x})|\\leq\\mathbb{E}[\\hat{m}_{\\varphi}({\\pmb x})-m_{\\varphi}({\\pmb x})|X_{\\mathcal{I}},D_{\\mathbb{Z}}]|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\,\\sqrt{2\\varepsilon_{\\delta}(n)}\\|{\\pmb w}\\|\\|\\varphi-h(y_{0},y_{1}|.)\\|_{{\\pmb w}^{2}}+\\frac{2\\|{\\pmb w}\\|_{\\infty}\\varepsilon_{\\delta}(n)}{3\\xi}=:\\mathbb{B}(\\varphi).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Moreover, with probability at least $1-\\delta$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\hat{m}_{\\hat{\\varphi}}(\\pmb{x})-m_{\\varphi}(\\pmb{x})|\\leq\\mathbb{B}(\\varphi)+|\\hat{m}_{\\hat{b}}(\\pmb{x})|+\\sqrt{2\\varepsilon_{\\delta}(n)}\\|\\pmb{w}\\|_{2}\\|\\hat{\\varphi}-\\varphi\\|_{\\pmb{w}^{2}}=:\\mathbb{B}^{+}(\\hat{\\varphi}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. To prove (12) we first observe that since $\\pmb{w}$ is $D_{\\mathbb{Z}},X_{\\mathcal{I}}$ -measurable we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\hat{m}_{\\varphi}({\\pmb x})^{2}|D_{\\mathcal{T}},X_{\\mathcal{I}}]-\\mathbb{E}[\\hat{m}_{\\varphi}({\\pmb x})|D_{\\mathcal{T}},X_{\\mathcal{I}}]^{2}=\\sum_{j\\in\\mathcal{I}}w_{j}^{2}\\mathbb{E}[(\\varphi(Z_{j})-\\mathbb{E}[\\varphi(Z)|X=X_{j}])^{2}|D_{\\mathcal{T}},X_{\\mathcal{I}}]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note also that we have $\\begin{array}{r l}{}&{\\operatorname*{max}_{j\\in\\mathcal{I}}\\lvert w_{j}\\{\\varphi(Z_{j})\\rvert-\\mathbb{E}[\\varphi(Z_{j})\\lvert D_{\\mathcal{T}},X_{\\mathcal{I}}]\\}\\rvert\\leq\\ \\lVert\\pmb{w}\\rVert_{\\infty}2\\xi^{-1}}\\end{array}$ almost surely. Note also that $\\{\\varphi(Z_{j})\\}_{j}\\in\\mathcal{I}$ are conditionally independent given $D_{\\mathcal{T}},X_{\\mathcal{I}}$ . Hence, the bound (12) follows from two applications Bernstein\u2019s inequality Boucheron et al. [7, Theorem 2.10], applied conditionally on $D_{\\mathcal{T}},X_{\\mathcal{I}}$ . ", "page_idx": 18}, {"type": "text", "text": "Next we note that by the triangle inequality we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\mathbb{E}\\{\\hat{m}_{\\hat{\\varphi}}(\\pmb{x})-m_{\\varphi}(\\pmb{x}),|\\,D_{Z},X_{\\mathcal{I}}\\}|}\\\\ &{\\le|\\mathbb{E}\\{\\hat{m}_{\\hat{\\varphi}}(\\pmb{x})-\\hat{m}_{\\varphi}(\\pmb{x})\\,|\\,D_{Z},X_{\\mathcal{I}}\\}|+|\\mathbb{E}\\{\\hat{m}_{\\varphi}(\\pmb{x})-m_{\\varphi}(\\pmb{x})\\,|\\,D_{Z},X_{\\mathcal{I}}\\}|}\\\\ &{\\le\\displaystyle\\left\\vert\\sum_{j\\in\\mathcal{I}}w_{j}\\mathbb{E}\\,[\\hat{\\varphi}(Z_{j})-\\varphi(Z_{j})|D_{Z},X_{\\mathcal{I}}]\\right\\vert+|\\mathbb{E}\\{\\hat{m}_{\\varphi}(\\pmb{x})-m_{\\varphi}(\\pmb{x})\\,|\\,D_{Z},X_{\\mathcal{I}}\\}|}\\\\ &{=\\|\\hat{m}_{\\hat{b}}\\|_{w,1}+|\\mathbb{E}\\{\\hat{m}_{\\varphi}(\\pmb{x})-m_{\\varphi}(\\pmb{x})\\,|\\,D_{Z},X_{\\mathcal{I}}\\}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, since $\\mathbb{E}[\\hat{\\varphi}(Z_{j})|D_{\\mathcal{T}},X_{\\mathcal{T}}]$ is the projection of $\\hat{\\varphi}(Z_{j})$ onto the subspace of $D_{\\mathbb{Z}},X_{\\mathcal{I}}$ - measureable functions we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|w\\|_{2}^{2}\\|\\hat{m}_{\\hat{\\mathcal{P}}}-m_{\\hat{\\mathcal{P}}}(x)\\|_{\\infty}^{2}}\\\\ &{=\\displaystyle\\sum_{j\\in\\mathcal{J}}w_{j}^{2}\\mathbb{E}\\{(\\hat{\\varphi}(Z)-\\mathbb{E}\\{\\hat{\\varphi}(Z)\\vert X=X_{j}\\})^{2}\\vert D_{Z},X_{\\mathcal{I}}\\}}\\\\ &{\\le\\displaystyle\\sum_{i\\in[n]}w_{j}^{2}\\mathbb{E}\\{(\\hat{\\varphi}(Z_{j})-\\mathbb{E}\\{\\varphi(Z_{j})\\vert D_{Z},X_{\\mathcal{I}}\\})^{2}\\vert D_{Z},X_{\\mathcal{I}}\\}}\\\\ &{=\\displaystyle\\sum_{i\\in[n]}w_{j}^{2}\\mathbb{E}\\{\\{(\\hat{\\varphi}(Z_{j})-\\varphi(Z_{j}))+(\\varphi(Z_{j})-\\mathbb{E}\\{\\varphi(Z_{j})\\vert D_{Z},X_{\\mathcal{I}}\\})\\}^{2}\\vert D_{Z},X_{\\mathcal{I}}\\}}\\\\ &{=\\displaystyle\\sum_{i\\in[n]}w_{j}^{2}\\left(\\mathbb{E}\\{(\\hat{\\varphi}(Z_{j})-\\varphi(Z_{j}))^{2}\\vert D_{Z},X_{\\mathcal{I}}\\}+\\mathbb{E}\\{(\\varphi(Z_{j})-\\mathbb{E}\\{\\varphi(Z_{j})\\vert D_{Z},X_{\\mathcal{I}}\\})^{2}\\}[D_{Z},X_{\\mathcal{I}}]\\right)}\\\\ &{\\quad+\\displaystyle\\sum_{i\\in[n]}\\Big(\\|\\hat{w}_{\\hat{\\mathcal{P}}}-\\hat{w}_{\\varphi}\\|_{w^{2}}^{2}+\\|\\hat{\\mathfrak{m}}_{\\varphi}-m_{\\varphi}\\|_{w^{2}}^{2}\\Big)}\\\\ &{\\le\\|w\\|_{2}^{2}\\left\\{(\\|\\hat{w}_{\\hat{\\mathcal{P}}}-\\hat{w}_{\\varphi}\\|_{w^{2}}^{2}+\\|\\hat{\\mathfrak{m}}_{\\varphi}-m_{\\varphi}\\|_{w^{2}}^{2})\\right\\}}\\\\ &{\\le\\|w\\|_{2}^{2}\\left\\{(\\|\\hat{w}_{\\hat{\\mathcal{P}}}-\\hat{w}_{\\varphi}\\|_{w^{2}}+\\|\\hat{\\mathfrak{m}}_{\\varphi}-m_{\\varphi}\\|_{w^{2}}^{2}) \n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hence, to deduce (13) we apply the first bound with $\\hat{\\varphi}$ in place of $\\varphi$ to obtain the following bound with probability at least $1-\\delta$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\hat{m}_{\\varphi}(\\mathbf{x})-m_{\\varphi}(\\mathbf{x})|\\leq\\lvert\\mathbb{E}\\{\\hat{m}_{\\hat{\\varphi}}(\\mathbf{x})-m_{\\varphi}(\\mathbf{x})\\,\\vert\\,D_{T},X_{\\mathcal{I}}\\}\\vert+\\sqrt{2\\varepsilon_{\\delta}(n)}\\,\\Vert\\boldsymbol{w}\\Vert_{2}\\,\\Vert\\hat{m}_{\\hat{\\varphi}}-m_{\\hat{\\varphi}}\\Vert_{w^{2}}}\\\\ &{\\phantom{\\leq}\\,+2\\xi^{-1}\\Vert\\boldsymbol{w}\\Vert_{\\infty}\\varepsilon_{\\delta}(n)/3}\\\\ &{\\phantom{\\leq}\\leq\\lVert\\hat{m}_{\\hat{b}}\\rVert_{w,1}+\\lvert\\mathbb{E}\\{\\hat{m}_{\\varphi}(\\mathbf{x})-m_{\\varphi}(\\mathbf{x})\\,\\vert\\,D_{Z},X_{\\mathcal{I}}\\}\\vert}\\\\ &{\\phantom{\\leq\\,}+\\sqrt{2\\varepsilon_{n}(\\delta)}\\lVert\\boldsymbol{w}\\rVert_{2}\\,\\{(\\lVert\\hat{m}_{\\hat{\\varphi}}-\\hat{m}_{\\varphi}\\rVert_{w^{2}}+\\lVert\\hat{m}_{\\varphi}-m_{\\varphi}\\rVert_{w^{2}})}\\\\ &{\\phantom{\\leq\\,}+2\\xi^{-1}\\lVert\\boldsymbol{w}\\rVert_{\\infty}\\varepsilon_{\\delta}(n)/3}\\\\ &{=\\mathbb{B}(\\varphi)+\\lVert\\hat{m}_{\\hat{b}}\\rVert_{w,1}+\\sqrt{2\\varepsilon_{\\delta}(n)}\\,\\Vert\\boldsymbol{w}\\rVert_{2}\\,\\lVert\\hat{m}_{\\hat{\\varphi}}-\\hat{m}_{\\varphi}\\rVert_{w^{2}}}\\\\ &{=\\mathbb{B}^{+}(\\hat{\\varphi}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as required. ", "page_idx": 18}, {"type": "text", "text": "We apply the following result from Kennedy [15]. ", "page_idx": 18}, {"type": "text", "text": "Proposition 4 (Proposition 2 from Kennedy [15]). Suppose that $\\hat{b}({\\pmb x})=\\hat{b}_{1}({\\pmb x})\\hat{b}_{2}({\\pmb x})$ then ", "page_idx": 19}, {"type": "equation", "text": "$$\n|\\hat{m}_{\\hat{b}}({\\pmb x})|=\\|{\\pmb w}\\|_{1}\\|\\hat{b}_{1}\\|{\\pmb w}\\|\\hat{b}_{1}\\|{\\pmb w}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Follows from the Cauchy-Schwartz inequality. ", "page_idx": 19}, {"type": "text", "text": "Proposition 5. Our pseudo-outcome is conditionally unbiased, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\varphi_{y_{0},y_{1}}(Z)|X={\\pmb x}]=h^{\\ast}(y_{0},y_{1}|{\\pmb x}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\Xi[\\varphi_{y_{0},y_{1}}(Z)|X=x,A=1]=\\frac{1}{\\pi(x)}\\Big(\\mathbb{E}[{\\mathbb{I}\\{Y\\leq y_{1}\\}|X=x,A=1}]-F_{1}(y_{1}|x)\\Big)+h^{*}(y_{0},y_{1}|x)}\\\\ {\\displaystyle=h^{*}(y_{0},y_{1}|x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and similarly $\\begin{array}{r}{\\mathbb{E}[\\varphi_{y_{0},y_{1}}(Z)|X={\\pmb x},A=1]=h^{\\ast}(y_{0},y_{1}|{\\pmb x})}\\end{array}$ . The result now follows by the law of total expectation. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "C.1.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . Fix $y_{0},y_{1},x$ Let $\\hat{m}_{\\hat{\\varphi}}({\\pmb x})$ be the regression of $X$ against the estimated pseudo-outcome $\\hat{\\varphi}_{y_{0},y_{1}}(Z)$ evaluated at $\\textbf{\\em x}$ (i.e. $\\hat{h}(y_{0},y_{1}|\\pmb{x}))$ , let $\\hat{m}_{\\varphi}$ be the same but with the estimated pseudo-outcome replaced with the exact pseudo-outcome $\\varphi_{y_{0},y_{1}}$ . Finally take ", "page_idx": 19}, {"type": "equation", "text": "$$\nm_{\\varphi}({\\pmb x}):=\\mathbb{E}[\\varphi_{y_{0},y_{1}}(Z)|X={\\pmb x}]=\\mathbb{P}(Y\\leq y_{1}|X={\\pmb x},A=1)-\\mathbb{P}(Y\\leq y_{1}|X={\\pmb x},A=0).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Theorem 3 gives us that with probability. at least $1-\\delta$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\hat{m}_{\\hat{\\varphi}}(\\mathbf x)-m_{\\varphi}(\\mathbf x)|\\leq\\varepsilon_{\\gamma}(n,\\delta)+|\\hat{m}_{\\hat{b}}(\\mathbf x)|+\\sqrt{2\\varepsilon_{\\delta}(n)}\\|\\mathbf w\\|_{2}\\|\\hat{\\varphi}-\\varphi\\|_{w^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\varepsilon_{\\gamma}(n,\\delta)+|\\hat{m}_{\\hat{b}}(\\mathbf x)|+\\sqrt{2\\varepsilon_{\\delta}(n)}\\|\\hat{\\varphi}-\\varphi\\|_{w^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $\\hat{b}((y,\\pmb{x},a)):=\\mathbb{E}[\\hat{\\varphi}(Z)-\\varphi(Z)|X=\\pmb{x}].$ ", "page_idx": 19}, {"type": "text", "text": "To bound $\\hat{b}$ we have that from Proposition 5, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[\\varphi_{y_{0},y_{1}}(Z)|X=x]=\\mathbb{P}(Y\\leq y_{1}|X=x,A=1)-\\mathbb{P}(Y\\leq y_{0}|X=x,A=0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Additionally, by splitting over the events $\\{A=1\\},\\{A=0\\}$ and noting that $\\mathbb{P}(A=1|X=x)=$ $\\pi(x)$ we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\hat{\\varphi}_{y_{0},y_{1}}(Z)|X=x]=\\left(\\frac{\\pi(x)}{\\hat{\\pi}(x)}\\right)\\Big(\\mathbb{P}(Y\\leq y_{1}|X=x,A=1)-\\hat{\\mathbb{P}}(Y\\leq y_{1}|X=x,A=1)\\Big)}\\\\ &{\\phantom{=}-\\frac{1-\\pi(x)}{1-\\hat{\\pi}(x)}\\Big(\\mathbb{P}(Y\\leq y_{0}|X=x,A=0)-\\hat{\\mathbb{P}}(Y\\leq y_{0}|X=x,A=0)\\Big)}\\\\ &{\\phantom{=}+\\hat{\\mathbb{P}}(Y\\leq y_{1}|X=x,A=1)-\\hat{\\mathbb{P}}(Y\\leq y_{0}|X=x,A=0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{b}(x)=\\Bigg(\\frac{\\pi(x)}{\\hat{\\pi}(x)}-1\\Bigg)\\left(\\mathbb{P}(Y\\leq y_{1}|X=x,A=1)-\\hat{\\mathbb{P}}(Y\\leq y_{1}|X=x,A=1)\\right)}\\\\ &{\\qquad\\quad-\\!\\begin{array}{r l}{\\left(\\frac{1-\\pi(x)}{1-\\hat{\\pi}(x)}-1\\right)\\Big(\\mathbb{P}(Y\\leq y_{0}|X=x,A=0)-\\hat{\\mathbb{P}}(Y\\leq y_{0}|X=x,A=0)\\Big)\\,.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We have that using Proposition 4 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|\\hat{m}_{\\hat{b}}({\\pmb x})|\\leq\\|{\\pmb w}\\|_{1}\\left\\|\\frac{\\pi}{\\hat{\\pi}}-1\\right\\|_{{\\pmb w}}\\left\\|\\mathbb{P}({\\pmb Y}\\leq y_{1}|{\\pmb X},{\\pmb A}=1)-\\hat{\\mathbb{P}}({\\pmb Y}\\leq y_{1}|{\\pmb X},{\\pmb A}=1)\\right\\|_{{\\pmb w}}}&{}\\\\ {+\\ \\left\\|\\frac{1-\\pi}{1-\\hat{\\pi}}-1\\right\\|_{{\\pmb w}}\\left\\|\\mathbb{P}({\\pmb Y}\\leq y_{0}|{\\pmb X},{\\pmb A}=0)-\\hat{\\mathbb{P}}({\\pmb Y}\\leq y_{0}|{\\pmb X},{\\pmb A}=0)\\right\\|_{{\\pmb w}}}&{}\\\\ {\\leq\\frac{1}{\\xi}\\displaystyle\\sum_{a=0}^{1}\\|{\\pmb\\pi}-\\hat{\\pi}\\|_{{\\pmb w}}\\left\\|\\mathbb{P}({\\pmb Y}\\leq y_{a}|{\\pmb X},{\\pmb A}=a)-\\hat{\\mathbb{P}}({\\pmb Y}\\leq y_{a}|{\\pmb X},{\\pmb A}=a)\\right\\|_{{\\pmb w}}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now it is simply a matter of bounding all the relevant terms which we do through the following events ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{\\mathrm{oracle}}:=\\Big\\{|\\hat{m}_{\\hat{\\varphi}}-m_{\\varphi}|\\!\\le|\\hat{m}_{\\varphi}(x)-m_{\\varphi}(x)|\\!+\\!|\\hat{m}_{\\hat{b}}(x)|\\!+\\!\\sqrt{2\\varepsilon_{\\delta}(n)}||w||_{2}||\\hat{\\varphi}-\\varphi||_{w^{2}}\\Big\\}}\\\\ &{\\quad E_{\\hat{\\varphi}}:=\\{||\\hat{\\varphi}-\\varphi||_{w^{2}}\\!\\le\\varepsilon_{\\hat{\\varphi}}(n,\\delta/4)\\}}\\\\ &{\\quad E_{\\gamma}:=\\{\\hat{m}_{\\varphi}-m_{\\varphi}\\le\\varepsilon_{\\gamma}(n,\\delta/4)\\}}\\\\ &{\\quad E_{\\alpha}:=\\Big\\{||\\pi-\\hat{\\pi}||_{w,2}\\le\\varepsilon_{\\alpha}(n,\\delta/4)\\Big\\}}\\\\ &{\\quad E_{\\beta,0}:=\\Big\\{||\\hat{\\Psi}(Y\\leq y_{0}|X,A=0)-\\mathbb{P}(Y\\leq y_{0}|X,A=0)||_{w,2}\\!\\le g_{\\beta}(n,\\delta/4)\\Big\\}}\\\\ &{\\quad E_{\\beta,1}:=\\Big\\{||\\hat{\\Psi}(Y\\leq y_{1}|X,A=1)-\\mathbb{P}(Y\\leq y_{1}|X,A=1)||_{w,2}\\!\\le\\varepsilon_{\\beta}(n,\\delta/4)\\Big\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to our assumptions and previous results, $E_{\\mathrm{oracle}},E_{\\hat{\\varphi}},E_{\\gamma},E_{\\alpha}\\cap E_{\\beta,0}\\cap E_{\\beta,1}$ separately occur w.p. at least $1-\\delta\\bar{/}4$ . Then by the union bound, the intersection of all these events holds w.p. at least $1-\\delta$ and under these events ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\hat{m}_{\\hat{\\varphi}}-\\hat{m}_{\\varphi}+\\hat{m}_{\\varphi}-m_{\\varphi}|\\leq\\varepsilon_{T_{n}}(n,\\delta/4)+\\frac{2}{\\xi}\\varepsilon_{\\alpha}(n,\\delta/4)\\varepsilon_{\\beta}(n,\\delta/4)+\\varepsilon_{\\gamma}(n,\\delta/4)\\varepsilon_{w}(n,\\delta/4)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Where the first term comes from Proposition 3, the second from Proposition 4 and the final term from smoothness of $g^{\\ast}$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "C.2 Proof of $\\hat{g}$ accuracy ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proposition 6 (Maximum step-size bound). Suppose that both the outer regression and estimation of CCDFs is fit using linear smoothers so that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{h}(y_{0},y_{1}|x)=\\sum_{j\\in\\mathcal{I}}w_{j}(x;X_{\\mathcal{I}})\\hat{\\varphi}(Z_{j})\\quad\\quad\\quad\\hat{F}_{a}(y|x^{\\prime})=\\sum_{i\\in\\mathcal{Z}}w_{F_{a};i}(x^{\\prime};X_{\\mathcal{I}})\\mathbb{1}\\{Y_{i}\\leq y\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $w_{j}(\\pmb{x}),w_{i}(\\pmb{x}^{\\prime})$ Additionally suppose with probability at least $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\operatorname*{max}_{j\\in\\mathcal{I}}w_{j}(\\pmb{x}),\\operatorname*{max}_{i\\in\\mathcal{Z}}w_{F_{a};i}(X)\\right\\}\\leq\\varepsilon_{\\pmb{w}}(n,\\delta).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then with probability at least $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[n_{1}+1]_{0}}|\\hat{h}(y_{0},Y_{1}^{(j)}|\\pmb{x})-\\hat{h}(y_{0},Y_{1}^{(j+1)}|\\pmb{x})|\\ge\\xi^{-1}\\varepsilon_{w}(n,\\delta/n)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $[n_{1}+1]_{0}:=[n_{1}+1]\\cup\\{0\\},\\,\\hat{h}(y_{0},Y_{1}^{(0)}|x)=-1,\\,\\hat{h}(y_{0},Y_{1}^{(n_{1}+1)}|z)$ , and $\\{Y^{(i)}\\}_{i\\in I_{1}}$ is the sorted version of {Yi}i\u2208I1. ", "page_idx": 20}, {"type": "text", "text": "Proof. We immediately have via union bounds that with probability at least $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\left\\{\\operatorname*{max}_{j\\in\\mathcal{I}}w_{j}(\\pmb{x}),\\operatorname*{max}_{j\\in\\mathcal{I}}w_{F_{a};i}(X_{j})\\right\\}\\leq\\varepsilon_{\\pmb{w}}(n,\\delta/n).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we can try to bound the jump size of our function. To do so for a step function $f:y\\rightarrow\\mathbb{R}$ and a step points $y\\in\\mathcal{V}$ we define ", "page_idx": 20}, {"type": "equation", "text": "$$\nf(\\Delta y):=\\operatorname*{lim}_{\\varepsilon\\downarrow0}f(y)-f(y-\\varepsilon),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "i.e. the size of the step at $y$ . From the definition of our pseudo-outcome, the jumps occur in different forms at $\\{Y_{j}\\}_{\\{j\\in\\mathcal{J},A_{j}=1\\}}$ , $\\{Y(i)\\}_{\\{i\\in\\mathcal{Z}|A^{(i)}=1\\}}$ . Note that we are assuming $Y$ continuous so these points are all distinct a.s. . ", "page_idx": 21}, {"type": "text", "text": "For $Y_{j}$ with $j\\in\\mathcal{I}$ and $A_{j}=1$ . We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{h}(y_{0},\\Delta Y_{j}|x)=w_{\\varphi,j}\\frac{1}{\\hat{\\pi}(X_{j})}\\mathbb{1}\\{Y_{j}\\leq\\Delta Y_{j}\\}}\\\\ &{\\qquad\\qquad\\quad\\leq\\frac{w_{\\varphi,j}}{\\xi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For $i\\in\\mathcal{Z}$ with $A^{(i)}=1$ , we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\hat{h}(y_{0},\\Delta Y_{i}|x)|=\\left|\\sum_{j\\in\\mathcal{I}}w_{j}\\left(-1\\{A_{j}=1\\}\\frac1{\\hat{\\pi}(X_{j})}\\hat{F}_{1}(\\Delta Y_{i}|X_{j})+\\hat{F}_{1}(\\Delta Y_{i}|X_{j})\\right)\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac1{\\xi}\\operatorname*{max}_{j\\in\\mathcal{I}}\\hat{F}_{1}(\\Delta Y_{i}|X_{j})}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac1{\\xi}\\operatorname*{max}_{j\\in\\mathcal{I}}w_{F_{a};i}(X_{j}){\\mathbb1}\\{Y_{i}\\leq\\Delta Y_{i}\\}}\\\\ &{\\quad=\\displaystyle\\frac1{\\xi}\\operatorname*{max}_{j\\in\\mathcal{I}}w_{F_{a};i}(X_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Hence the maximum jump size is less than ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{\\xi}\\operatorname*{max}\\left\\{\\operatorname*{max}_{j\\in\\mathcal{I}}w_{\\varphi,j}(\\pmb{x}),\\operatorname*{max}_{i\\in\\mathcal{I},j\\in\\mathcal{I}}w_{F_{a};i}(X_{j})\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore combining this with our previous bounds gives that with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i\\in[n_{1}+1]_{0}}|\\hat{h}(y_{0},Y^{(i)}|\\pmb{x})-\\hat{h}(y_{0},Y^{(i+1)}|\\pmb{x})|\\leq\\xi^{-1}\\varepsilon_{\\pmb{w}}(n,\\delta/n)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$F_{1}$ We also have a result ensuring the step size of the projected function is even smaller. ", "page_idx": 21}, {"type": "text", "text": "Proposition 7. Let $\\pmb{\\alpha}\\in\\mathbb{R}^{p}$ and $\\tilde{\\alpha}:=P_{\\mathrm{Iso}(p)}(\\alpha)$ . Then $|\\alpha_{l}-\\alpha_{l+1}|\\geq|\\tilde{\\alpha}_{l}-\\tilde{\\alpha}_{l+1}|.$ for all $l\\in[p]$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. We will prove this by first giving an algorithm to compute the projection. Define ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\psi_{l}(\\pmb\\alpha):=\\left\\{\\begin{array}{l l}{\\alpha}&{\\mathrm{if}\\,\\alpha_{l}\\leq\\alpha_{l+1}}\\\\ {\\left(\\alpha_{1},\\ldots,\\frac{\\alpha_{l}+\\alpha_{l+1}}{2},\\frac{\\alpha_{l}+\\alpha_{l+1}}{2},\\alpha_{m}\\right)}&{\\mathrm{if}\\,\\alpha_{l}>\\alpha_{l}+1.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now define $\\pmb{\\alpha}^{(0,0)}=\\pmb{\\alpha}$ , $\\alpha^{(n,i)}=\\psi_{i}(\\alpha^{(n,i-1)})$ for $i\\in[p-1]$ and $\\pmb{\\alpha}^{(n,0)}=\\pmb{\\alpha}^{(n-1,p-1)}$ for $n\\in\\mathbb N$ . Finally define $\\pmb{\\alpha}^{(t)}=\\pmb{\\alpha}^{(\\lfloor t/p\\rfloor,\\mathrm{mod}(t,p)}$ . Then by Yang and Barber [32] we know that $\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\alpha}^{(t)}=$ $P_{\\mathrm{Iso}(p)}(\\alpha)$ . ", "page_idx": 21}, {"type": "text", "text": "Now if $|\\alpha_{l+1}^{(n,l+1)}\\,-\\,\\alpha_{l}^{(n,l+1)}|>\\;|\\alpha_{l+1}^{(n,l)}\\,-\\,\\alpha_{l}^{(n,l)}|$ \u03b1l(n,l)|. Then \u03b1l(n+,1l+1)has be moved by \u03c8l+1. Hence $\\alpha_{l+1}^{(n,l+1)}<\\alpha_{l+1}^{(n,l)}$ . For this to increase the distance then we must have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha_{l+1}^{(n,l+1)}<\\alpha_{l}^{(n,l+1)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore as \u03b1l $\\alpha_{l}^{(n+1,l-1)}\\geq\\alpha_{l}^{n,l+1}$ we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\alpha_{l+1}^{(n+1,l)}=\\alpha_{l}^{(n+1,l)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By a similar (simpler) argument |\u03b1l(n,l\u22121)\u2212 $|\\alpha_{l}^{(n,l-1)}-\\alpha_{l+1}^{(n,l-1)}|>|\\alpha_{l}^{(n,l-2)}-\\alpha_{l+1}^{(n,l-2)}|$ \u03b1(n,l\u22122)| then \u03b1( $\\alpha_{l}^{(n,l)}=\\alpha_{l+1}^{(n,l)}$ Hence if any iteration moves adjacent points further apart, a later iteration will make them equal meaning that on convergence they will be equal. Hence we have proved our claim. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "We now have the result which justifies our choice to take the isotonic projection of the data which is take from Yang and Barber [32]. ", "page_idx": 22}, {"type": "text", "text": "Proposition 8 (Theorem 1 from Yang and Barber [32]). Let $\\boldsymbol{z}^{\\ast},\\boldsymbol{\\hat{z}}\\in\\mathbb{R}^{p}$ with $z^{*}\\in\\mathrm{Iso}(p)$ . Then for $\\tilde{z}:=P_{\\mathrm{Iso}(p)}\\!\\left(\\hat{z}\\right)$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{l}\\lvert z_{l}^{*}-\\tilde{z}_{l}\\rvert\\leq\\operatorname*{max}_{l}\\lvert z_{l}^{*}-\\hat{z}_{l}\\rvert.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now use this isotonic projection to obtain supremum bounds on the accuracy of our estimator. ", "page_idx": 22}, {"type": "text", "text": "Proposition 9 (Supremum bound on $\\hat{h}$ accuracy). Fix $\\pmb{x}\\,\\in\\,\\mathcal{X},y\\,\\in\\,y_{0}$ and let $\\hat{h}$ be our original estimate of $h^{*}$ . For $m\\in\\mathbb{N}$ , take $\\{Y^{(l)}\\}_{l=1}^{m}$ to be a potentially random set of points in $\\boldsymbol{\\wp}$ in increasing order. ", "page_idx": 22}, {"type": "text", "text": "Now define $\\pmb{\\alpha}\\,\\in\\,\\mathbb{R}^{m}$ by ${\\alpha_{l}}\\;:=\\;\\hat{h}\\big(y_{0},Y^{(l)}|x\\big)$ and $\\tilde{\\alpha}\\,:=\\,P_{\\mathrm{Iso}(m)}(\\pmb{\\alpha})$ . Finally, define $\\tilde{h}$ to be the piecewise constant right continuous function with $\\tilde{h}(y_{0},Y^{(l)}|x):=\\tilde{\\alpha}_{l}$ . ", "page_idx": 22}, {"type": "text", "text": "Suppose for any $y_{1}\\in\\mathcal{V}$ , $n\\in\\mathbb{N}$ , and $\\delta,\\delta^{\\prime}>0$ that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|\\tilde{h}(y_{0},y_{1}|x)-h^{*}(y_{0},y_{1}|x)\\right|\\geq\\varepsilon_{h}(n,\\delta)\\right)\\leq\\delta}\\\\ {\\mathbb{P}\\left(\\underset{l\\in[m]_{0}}{\\operatorname*{max}}\\left|\\hat{h}(y_{0},Y^{(l)}|x)-\\hat{h}(y_{0},Y^{(l+1)}|x)\\right|\\geq\\varepsilon_{s t e p}(n,m,\\delta)\\right)\\leq\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we take $[m]_{0}:=[m]\\cup\\{0\\},\\,\\hat{h}(y_{0},Y^{(0)}|x):=-1,$ , and $\\hat{h}(y_{0},Y^{(m+1)}|x):=1$ . Then for any $\\delta,\\delta^{\\prime}>0,n\\in\\mathbb{N}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{y_{1}\\in\\mathcal{Y}}\\Big|\\hat{h}(y_{0},y_{1}|x)-h^{*}(y_{0},y_{1}|x)\\Big|\\leq\\varepsilon_{h}(n,\\delta/m)+\\varepsilon_{s t e p}(n,m,\\delta^{\\prime})\\right)\\geq1-\\delta-\\delta^{\\prime}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. For each $l\\in[m]$ define the event ", "page_idx": 22}, {"type": "equation", "text": "$$\nE_{h,l}:=\\left\\{\\left|\\hat{h}(y_{0},Y_{1}^{(l)}|x)-h^{*}(y_{0},Y_{1}^{(l)}|x)\\right|\\leq\\varepsilon_{h}(n,\\delta/m)\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and take $\\begin{array}{r}{E_{h}:=\\bigcap_{l=1}^{n}\\ E_{h,l}}\\end{array}$ . Additionally define the event ", "page_idx": 22}, {"type": "equation", "text": "$$\nE_{s t e p}:=\\left\\{\\operatorname*{max}_{l\\in[m]_{0}}\\left\\vert\\hat{h}(y_{0},Y^{(l)}|\\pmb{x})-\\hat{h}(y_{0},Y^{(l+1)}|\\pmb{x})\\right\\vert<\\varepsilon_{s t e p}(n,m,\\delta)\\right\\}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then $E_{h}$ and $E_{s t e p}$ hold w.p.s at least $1-\\delta$ and $1\\,-\\,\\delta^{\\prime}$ respectively. Also under $E_{h},E_{s t e p}$ , by Propositions 7 & 8, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{max}_{l\\in[m]}\\Big|\\tilde{h}(y_{0},Y_{1}^{(l)}|\\pmb{x})-h^{*}(y_{0},Y_{1}^{(l)}|\\pmb{x})\\Big|\\leq\\varepsilon_{h}\\big(n,\\delta/m\\big)}\\\\ {\\displaystyle\\operatorname*{max}_{l\\in[m]_{0}}\\Big|\\tilde{h}(y_{0},Y^{(l)}|\\pmb{x})-\\tilde{h}(y_{0},Y^{(l+1)}|\\pmb{x})\\Big|<\\varepsilon_{s t e p}\\big(n,m,\\delta\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We then have that for any $y_{1}\\in\\mathcal{V}$ there exists $i\\in[m]$ such that $y_{1}^{(l-1)}\\leq y_{1}\\leq y_{1}^{(l)}$ . ", "page_idx": 22}, {"type": "text", "text": "Hence by monotonicity, we have the following 2 inequalities ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{h}(y_{0},y_{1}|x)-h^{*}(y_{0},y_{1}|x)\\geq\\underbrace{\\tilde{h}(y_{0},Y_{1}^{(l-1)}|x)-h^{*}(y_{0},Y_{1}^{(l)}|x)}_{\\Lambda_{1}}}\\\\ &{\\hat{h}(y_{0},y_{1}|x)-h^{*}(y_{0},y_{1}|x)\\leq\\underbrace{\\tilde{h}(y_{0},Y_{1}^{(l)}|x)-h^{*}(y_{0},Y_{1}^{(l-1)}|x)}_{\\Lambda_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Now for the first inequality we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Lambda_{1}=\\widetilde{h}(y_{0},Y_{1}^{(l-1)}|\\pmb{x})-\\widetilde{h}^{*}(y_{0},Y_{1}^{(l)}|\\pmb{x})+\\widetilde{h}(y_{0},Y_{1}^{(l)}|\\pmb{x})-h^{*}(y_{0},Y_{1}^{(l)}|\\pmb{x})}\\\\ &{\\quad\\ge-\\varepsilon_{h}(n,\\delta)-\\varepsilon_{s t e p}(n,m,\\delta^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "With the final inequality coming from $E_{h,l-1}$ and our definition of $h$ . By a similar argument we get from $E_{h,l}$ that $\\Lambda_{2}\\,\\leq\\,\\varepsilon_{h}(n,\\delta)+\\varepsilon_{s t e p}(n,m,\\delta^{\\prime})$ . Again we can use the same approach to get that under $E_{h,l-1},E_{h,l},E_{s t e p}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\varepsilon_{h}(n,\\delta)-\\varepsilon_{s t e p}(n,m,\\delta^{\\prime})\\leq h^{*}(y_{0},y_{1}|x)-\\tilde{h}(y_{0},y_{1}|x)\\leq\\varepsilon_{h}(n,\\delta)+\\varepsilon_{s t e p}(n,m,\\delta^{\\prime}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence for our specific $y_{1}$ under $E_{h,l},E_{h,l-1},E_{s t e p}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big|h^{*}(y_{0},y_{1}|x)-\\tilde{h}(y_{0},y_{1}|x)\\Big|\\leq\\varepsilon_{h}(n,\\delta)+\\varepsilon_{s t e p}(n,m,\\delta^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now as this inequality holds for arbitrary $y_{1}$ under $E_{h}\\cap E_{s t e p}$ (an event which not depend on our choice of $y_{1}$ ) and the intersection of these two events holds w.p. at least $1-\\delta-\\delta^{\\prime}$ by the union bound we have our result. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Our final key result before we can piece them all together will be to obtain accuracy in $g$ from our accuracy in $h^{*}$ ", "page_idx": 23}, {"type": "text", "text": "Theorem 10 (Single point accuracy bound). Fix $\\mathbf{\\boldsymbol{x}},y_{0}$ , assume that $h,\\tilde{h}$ are strictly monotonic in $y_{1}$ and suppose that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{y_{1}}\\lvert h^{*}(y_{0},y_{1}\\vert x)-\\tilde{h}(y_{0},y_{1}\\vert x)\\rvert<\\varepsilon.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Additionally let $\\alpha:=F_{0}(y_{0}|\\pmb{x})$ and assume that $f_{Y|X,A=1}(y_{1}|x)>\\eta$ for all $y_{1}\\in F_{1}^{-1}(B_{\\varepsilon}(\\alpha)|x)$ where here we are taking $F_{1}^{-1}(A|\\pmb{x})$ to be the pre-image in $\\boldsymbol{\\wp}$ of the set $A$ for fixed $\\textbf{\\em x}$ . ", "page_idx": 23}, {"type": "text", "text": "Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n|g^{*}(y_{0}|\\pmb{x})-\\hat{g}(y_{0}|\\pmb{x})|\\leq\\frac{2\\varepsilon}{\\eta},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $g^{*}(y_{0}|x)$ is the unique $y_{1}$ such that $h^{*}(y_{0},y_{1}|x)=0$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $y_{1}^{*}:=g^{*}(y_{0}|\\pmb{x})$ and $\\hat{y}_{1}:=\\hat{g}(y_{0}|\\pmb{x})$ so that $F_{1}(y_{1}^{*}|x)$ . From our accuracy assumption on $\\hat{h}$ in the set-up of the Theorem we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|F_{1}(y_{1}^{*}|x)-F_{1}(\\hat{y}_{1}|x)|=|h^{*}(y_{0},y_{1}^{*}|x)-h^{*}(y_{0},\\hat{y}_{1}|x)|}&{}\\\\ {=|h^{*}(y_{0},\\hat{y}_{1}|x)|}&{}\\\\ {\\leq|h^{*}(y_{0},\\hat{y}_{1}|x)-\\hat{h}(y_{0},\\hat{y}_{1}|x)|+|\\hat{h}(y_{0},\\hat{y}_{1}|x)|}&{}\\\\ {\\leq|h^{*}(y_{0},\\hat{y}_{1}|x)-\\hat{h}(y_{0},\\hat{y}_{1}|x)|+|\\hat{h}(y_{0},y_{1}^{*}|x)|}&{}\\\\ {\\leq|h^{*}(y_{0},\\hat{y}_{1}|x)-\\hat{h}(y_{0},\\hat{y}_{1}|x)|+|\\hat{h}(y_{0},y_{1}^{*}|x)-h^{*}(y_{0},y_{1}^{*}|x)|}&{}\\\\ {\\leq2\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the second and fifth line come from the definition of $y_{1}^{*}$ and the fourth from the definition of $\\hat{y}_{1}$ . ", "page_idx": 23}, {"type": "text", "text": "If we define $\\partial_{k}f$ to be the derivative with respect to the $k^{\\mathrm{th}}$ argument of $f$ then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\hat{g}(y_{0}|x)-g^{*}(y_{0}|x)|=|F_{1}^{-1}(F_{1}(y_{1}^{*}|x)|x)-F_{1}^{-1}(F_{1}(\\hat{y}_{1}|x)|x)|}&{}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\left|\\int_{F_{1}(y_{1}^{*}|x)}^{F_{1}(\\hat{y}_{1}|x)}\\partial_{1}F_{1}^{-1}(\\beta|x)\\mathrm{d}\\beta\\right|}\\\\ &{\\quad\\quad\\quad\\quad=\\left|\\int_{F_{1}(y_{1}^{*}|x)}^{F_{1}(\\hat{y}_{1}|x)}\\frac{1}{f\\left((F_{1}^{-1}(\\beta|x)|x)\\right)}\\mathrm{d}\\beta\\right|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq2\\varepsilon\\underset{y_{1}\\in(y_{1}^{*},\\hat{y}_{1})}{\\operatorname*{max}}\\frac{1}{f(y_{1}|x)}}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{2\\varepsilon}{\\eta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "C.2.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We can now finally combine all these results to prove Theorem 2. ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 2. From proposition 1 we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|\\hat{h}(y_{0},y_{1}|x)-h^{*}(y_{0},y_{1}|x)\\right|\\le\\varepsilon_{h}(n,\\delta)\\right)\\ge1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\varepsilon_{h}(n,\\delta):=\\varepsilon_{T_{n}}(n,\\delta/4)+\\varepsilon_{\\alpha}(n,\\delta/4)\\varepsilon_{\\beta}(n,\\delta/4)+\\varepsilon_{\\gamma}(n,\\delta/4).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now define $\\{Y^{(i)}\\}_{i=1}^{n}$ to be the sorted version of $\\{Y_{i}\\}_{i=1}^{n}$ . Then by Proposition 6 we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{max}_{j\\in[n]_{0}}|\\hat{h}(y_{0},Y_{1}^{(i)}|x)-h^{*}(y_{0},Y_{1}^{(i)}|x)|\\ge\\varepsilon_{h-s t e p}(n,\\delta)\\right)\\le\\delta.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Plugging this into Proposition 9 with $\\varepsilon_{s t e p}(n,m,\\delta)$ replaced by $\\xi^{-1}\\varepsilon_{\\pmb{w}}(n,\\delta/n))$ ) and $\\varepsilon_{h}(n,\\delta)$ replaced with $\\varepsilon_{h}(n,\\delta/2)$ gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\underset{y_{1}\\in\\mathcal{Y}}{\\operatorname*{sup}}\\bigg|\\tilde{h}(y_{0},y_{1}|x)-h^{*}(y_{0},y_{1}|x)\\bigg|\\leq\\varepsilon_{h}(n,\\delta/(2n))+\\xi^{-1}\\varepsilon_{w}(n,\\delta/n))\\bigg)\\geq1-\\delta.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now let $y_{1}^{*}\\;:=\\;g^{*}(y_{0}|{\\pmb x})$ . Then for any $y_{1}\\,\\in\\,\\mathcal{Y}$ if $|y_{1}^{\\prime}-y_{1}^{*}|>\\;s$ this implies that $|F_{1}(y_{1}^{\\prime}|x)\\;-$ $F_{1}(y_{1}^{*}|x)|\\bar{>}\\;\\;s\\eta$ by our lower bound on the density in $B_{s}(y_{1}^{*})$ . Hence by the contrapositive, if $|F_{1}(y_{1}^{\\prime}|x)-F_{1}(y_{1}^{*}|x)|\\leq s\\eta$ then $|y_{1}^{\\prime}-y_{1}^{*}|<s$ . ", "page_idx": 24}, {"type": "text", "text": "Now as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|F_{1}(y_{1}^{\\prime}|\\pmb{x})-F_{1}(y_{1}^{*}|\\pmb{x})|\\!\\leq s\\eta\\Leftrightarrow y_{1}^{\\prime}\\in F_{1}^{-1}(B_{s\\eta}(F_{1}(y_{1}^{*})))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\Leftrightarrow y_{1}^{\\prime}\\in B_{s\\eta}F_{1}^{-1}(B_{s\\eta}(F_{0}(y_{0})))}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence if $n$ is sufficiently large so that $\\varepsilon:=\\varepsilon_{h}(n,\\delta/(2n))+\\xi^{-1}\\varepsilon_{w}(n,\\delta/n))\\leq\\eta s$ then we satisfy the bounded density condition of Theorem 10. Therefore we can plug our bound into theorem 10 gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|\\hat{g}(y_{0}|x)-g^{*}(y_{0}|x)\\right|\\leq2\\left(\\eta^{-1}\\varepsilon_{h}(n,\\delta/(2n))+\\xi^{-1}\\varepsilon_{w}(n,\\delta/n))\\right)\\right)\\geq1-\\delta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "C.3 Extension to expectation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proposition 11. Let $Y_{0},D$ be $R V s$ on $\\mathcal{V},\\mathcal{Z}^{n}$ respectively (with $D$ representing data used to fit $a$ model and $Y_{0}$ representing the point where the model is fit). Now take $l(Y_{0},D)$ to be a non-negative bounded loss so that $l(Y_{0},D)<l_{\\mathrm{max}}$ a.s. . Suppose that for any $\\delta>0,$ , for all $y\\in\\mathcal{V}$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}(l(y,D)>\\varepsilon(n,\\delta))<\\delta\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then for any $t,\\delta_{0}\\in[0,1]$ and $p,q\\in[1,\\infty]$ such that $1/p+1/q=1$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbb{E}[l(Y_{0},D)|D]\\le t^{1/q}\\mathbb{E}[l(Y_{0},D)^{p}|D]^{1/p}+\\varepsilon(n,\\delta_{0})\\right)\\ge1-\\frac{\\delta_{0}}{t}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "In particular if $l(y,D)<l_{\\mathrm{max}}$ a.s. then taking $q=1,p=\\infty$ yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbb{E}[l(Y_{0},D)|D]\\leq t l_{\\operatorname*{max}}+\\varepsilon(n,\\delta_{0})\\right)\\geq1-\\frac{\\delta_{0}}{t}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. First fix $t,\\delta_{0}\\,\\in\\,[0,1]$ We will first bound the probability that the number of $y$ which don\u2019t satisfy our bound isn\u2019t too large. We do this by defining the event ", "page_idx": 24}, {"type": "equation", "text": "$$\nA:=\\left\\{l(Y_{0},D)>\\varepsilon(n,\\delta_{0})\\right\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and now aim to bound the probability that $\\mathbb{P}(A|D)>t$ (this probability is just w.r.t $Y_{0}$ treating $D$ as fixed.) ", "page_idx": 24}, {"type": "text", "text": "By Markov\u2019s inequality, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}(\\mathbb{P}(A|D)>t)\\le\\frac{1}{t}\\mathbb{E}[\\mathbb{P}(A|D)]}}\\\\ &{=\\displaystyle\\frac{1}{t}\\mathbb{E}[\\mathbb{P}(A|Y_{0})]\\quad\\mathrm{by~Fubini^{*}s~T h e o r e m}}\\\\ &{<\\displaystyle\\frac{1}{t}\\mathbb{E}[\\delta_{0}]=\\frac{\\delta_{0}}{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now define $B:=\\{\\mathbb{P}(A|D)\\leq t\\}$ so that $\\begin{array}{r}{\\mathbb{P}(B)=1-\\frac{\\delta_{0}}{t}}\\end{array}$ . Then under $B$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[l(Y_{0},D)|D]=\\mathbb{E}[\\mathbb{1}_{A}l(Y_{0},D)|D]+\\mathbb{E}[l(Y_{0},D)|A^{c},D]\\mathbb{P}(A^{c}|D)}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}[\\mathbb{1}_{A}l(Y_{0},D)|D]+\\varepsilon(n,\\delta_{0})\\quad\\mathbf{by~definitions~of~}A}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}[\\mathbb{1}_{A}|D]^{1/q}\\mathbb{E}[l(Y_{0},D)^{p}|D]^{1/p}+\\varepsilon(n,\\delta_{0})\\quad\\mathrm{~by~Holder's~inequality}}\\\\ &{\\qquad\\qquad\\leq t^{1/q}\\mathbb{E}[l(Y_{0},D)^{p}|D]^{1/p}t+\\varepsilon(n,\\delta_{0})\\quad\\mathrm{As~we~are~assuming~}B.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Corollary 12. Assume that $F_{1}(y|x)>\\eta$ for all $y_{1}\\in\\mathcal{V}$ . Then, for our fixed $\\pmb{x}\\in\\mathcal{X}$ and $\\delta\\in(0,e^{-1})$ , $w.p$ . at least, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\cfrac{\\delta\\operatorname{diam}(\\mathcal{Y})}{2\\,\\left(\\eta^{-1}\\varepsilon_{h}(n,\\delta/2n)+\\xi^{-1}\\varepsilon_{w}(n,\\delta/n)\\right))},}\\\\ &{\\mathbb{E}\\Big[|\\hat{g}(Y|x)-g^{*}(Y|x)|\\ \\Big|A=0,\\hat{g}\\Big]\\leq4\\,\\big(\\eta^{-1}\\varepsilon_{h}(n,\\delta/n)+2\\xi^{-1}\\varepsilon_{w}(n,\\delta/n)\\big)\\Big)}\\\\ &{\\varepsilon_{h}(\\delta,n):=\\varepsilon_{T_{n}}(n,\\delta/4)+\\varepsilon_{\\alpha}(n,\\delta/4)\\varepsilon_{\\beta}(n,\\delta/4)+\\varepsilon_{\\gamma}(n,\\delta/4).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In particular for $2\\left(\\eta^{-1}\\varepsilon_{h}(\\underline{{n}},\\delta/n)+2\\xi^{-1}\\varepsilon_{w}(n,\\delta/n))\\right)\\leq\\log(e_{1}(n)/\\delta)^{a}/e_{2}(n).$ For $\\delta<1/e$ we have that $w.p$ . at least $1-\\delta$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[\\left|\\hat{g}(Y|x)-g^{*}(Y|x)\\right|\\;\\Big|A=0,\\hat{g}\\Big]\\leq\\log(2\\dim(\\mathcal{Y})e_{2}(n)e_{1}(n)/\\delta)^{a}/e_{2}(n)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Plugging the result of theorem 2 into proposition 11, noting that $|\\hat{g}(y_{0}|\\pmb{x})\\rrangle\\mathrm{~-~}g^{*}(y|\\pmb{x})|\\le$ $\\dim(\\mathcal{V})$ and taking ", "page_idx": 25}, {"type": "equation", "text": "$$\nt=\\frac{2}{\\mathrm{diam}(\\mathcal{V})}\\left(\\eta^{-1}\\varepsilon_{h}(n,\\delta/(2n))+\\xi^{-1}\\varepsilon_{w}(n,\\delta/n))\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "yields that w.p. at least ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle1-\\frac{\\delta\\,\\mathrm{diam}(y)}{2\\,\\big(\\eta^{-1}\\varepsilon_{h}(n,\\delta/n)+\\xi^{-1}\\varepsilon_{w}(n,\\delta/n)\\big))},}\\\\ {\\displaystyle\\mathbb{E}\\Big[|\\hat{g}(Y|x)-g^{*}(Y|x)|\\,\\left|A=0,\\hat{g}\\right|\\le4\\,\\big(\\eta^{-1}\\varepsilon_{h}(n,\\delta/n)+2\\xi^{-1}\\varepsilon_{w}(n,\\delta/n)\\big)\\big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Following from this if $2\\left(\\eta^{-1}\\varepsilon_{h}(n,\\delta/n)+2\\xi^{-1}\\varepsilon_{w}(n,\\delta/n))\\right)\\,\\leq\\,\\log(e_{1}(n)/\\delta)^{\\alpha}/e_{2}(n)$ . Then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathbb{E}\\Big[\\left|\\hat{g}(Y|x)-g^{*}(Y|x)\\right|\\;\\Big|A=0,\\hat{g}\\Big]\\geq2\\log(e_{1}(n)/\\delta)^{a}/e_{2}(n)\\right)\\leq\\frac{\\delta\\dim(y)e_{2}(n)}{\\log(e_{1}(n)/\\delta)^{a}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for $\\delta<1/e$ we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta<1/e\\Rightarrow\\delta<\\exp{\\left\\{-(\\frac{1}{2})^{1/a}\\right\\}}}\\\\ &{\\qquad\\quad\\Leftrightarrow\\log(1/\\delta)^{a}>\\frac{1}{2}}\\\\ &{\\qquad\\quad\\Rightarrow\\log(e_{1}(n)/\\delta)^{a}>\\frac{1}{2}}\\\\ &{\\qquad\\quad\\Rightarrow2\\delta\\log(e_{1}(n)/\\delta)^{a}>\\delta}\\\\ &{\\qquad\\quad\\Leftrightarrow2\\delta>\\frac{\\delta}{\\log(e_{1}(n)/\\delta)^{a}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\mathbb{E}\\Big[|\\hat{g}(Y|x)-g^{*}(Y|x)|\\,\\,\\Big|A=0,\\hat{g}\\Big]\\geq2\\log(e_{1}(n)/\\delta)^{a}/e_{2}(n)\\right)\\leq2\\delta\\dim(\\mathcal{Y})e_{2}(n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally $\\delta^{\\prime}=2\\delta\\dim(\\mathcal{V})e_{2}(n)$ gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\mathbb{E}\\Big[|\\hat{g}(Y|x)-g^{*}(Y|x)|\\,\\left|A=0,\\hat{g}\\right.\\right]\\geq2\\log(2\\operatorname{diam}(\\mathcal{Y})e_{1}(n)e_{2}(n)/\\delta^{\\prime})^{a}/e_{2}(n)\\right)\\leq\\delta^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "C.4 Application and justification of NW estimation with box kernel ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We aim to show that the box kernel satisfies some of our conditions. Specifically conditions 2 & 3 from Proposition 1. We first start by bounding the step size. ", "page_idx": 26}, {"type": "text", "text": "Proposition 13 (Effective sample size). Suppose that for our $\\pmb{x}\\in\\mathcal{X}$ there exists $C_{0},r_{0}>0$ such that for any $r\\in(0,r_{0})$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}(X\\in B_{r}(\\pmb{x}))\\geq C_{0}r^{d}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now for $r\\in(0,r_{0})$ take our kernel to be $k_{r}(\\pmb{x},\\pmb{x}^{\\prime}):=\\mathbb{1}\\{\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|\\leq r\\}$ . Then w.p. at least $1-\\delta$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\mathcal{I}}\\mathbb{1}\\{X_{j}\\in B_{r}(\\pmb{x})\\}\\geq\\left(n C_{0}r^{d}-\\sqrt{2n C_{0}r^{d}\\log(1/\\delta)}-\\log(1/\\delta)/3\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. We have $\\mathbb{1}\\{X_{j}\\in B_{r}({\\pmb x})\\}\\le1$ and $\\mathbb{E}[\\mathbb{1}\\{X_{j}\\in B_{r}(\\pmb{x})\\}^{2}]=\\mathbb{E}[\\mathbb{1}\\{X_{j}\\in B_{r}(\\pmb{x})\\}^{2}]=\\mathbb{P}(X_{j}\\in$ $B_{r}({\\pmb x}))=C_{0}r^{d}$ . Therefore by one sided Bernstein\u2019s inequality with $\\varepsilon=\\log(1/\\delta)$ we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{j\\in\\mathcal{I}}\\mathbb{1}\\{X_{j}\\in B_{r}(x)\\}\\leq n C_{0}r^{d}-\\sqrt{2n C_{0}r^{d}\\log(1/\\delta)}-\\frac{1}{3}\\log(1/\\delta)\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This gives our desired result. ", "page_idx": 26}, {"type": "text", "text": "Note that $\\textstyle\\sum_{j\\in{\\mathcal{J}}}\\mathbb{1}\\{X_{j}\\;\\in\\;B_{r}({\\pmb x})\\}$ is also the effective sample size of our estimation as it is the number of samples used in the average. ", "page_idx": 26}, {"type": "text", "text": "Now that we have the effective sample size result in terms of our kernel radius $r$ , we need to obtain the optimal rate of decay of $r$ for our estimation. ", "page_idx": 26}, {"type": "text", "text": "Proposition 14 (NW estimation with box kernel). let $\\hat{m}_{f}({\\pmb x})$ be the NW estimation of $m_{f}(\\pmb{x}):=$ $\\mathbb{E}[f(Z)|X={\\pmb x}]$ using $I I D$ copies $(Z_{i})_{i=1}^{n}$ of $Z$ . and assume $|f(Z)|\\le M$ . For a fixed $r\\in(0,r)$ , use kernel $k_{r}$ as defined above and suppose the same assumptions hold. Suppose that $m_{f}({\\pmb x})$ is $\\alpha$ smooth for $\\alpha\\leq1$ (i.e. $\\alpha$ -Holder continuous.) Then for sufficiently large n depending on $C_{0},\\alpha$ and $\\delta\\leq2e^{\\bar{-1}}\\;\\;\\;w.p$ . at least $1-\\delta$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\hat{m}_{f}({\\pmb x})-m_{f}({\\pmb x})|\\leq\\frac{2M+1}{C_{0}}\\log(2/\\delta)n^{-\\frac{1}{2+d/\\alpha}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. With our kernel define $\\mathcal{T}_{r}:=\\{i\\in[n]|k_{r}(\\pmb{x},X_{i})=1\\}$ and $n_{r}=|I_{r}|$ . Now define the event ", "page_idx": 26}, {"type": "equation", "text": "$$\nE_{n}:=\\{n_{r}\\geq C_{0}n r^{d}-\\sqrt{2C_{0}r^{d}n\\log(2/\\delta)}-\\log(2/\\delta)/3\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then by Proposition 13 this event occur w.p. at least $1\\!-\\!\\delta/2$ . Now if we define $\\varepsilon_{i}:=f(Z_{i})\\!-\\!m_{f}(X_{i})$ then we have $\\mathbb{E}[\\varepsilon_{i}|X_{i}]=0$ . Also, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|\\hat{m}_{f}({\\pmb x})-m_{f}({\\pmb x})|=\\left|\\frac{1}{n_{r}}\\sum_{i\\in\\mathcal{X}_{r}}f(Z_{i})\\right.\\,\\left.-m_{f}({\\pmb x})\\right|}\\\\ {\\displaystyle=\\left|\\frac{1}{n_{r}}\\sum_{i\\in\\mathcal{X}_{r}}m_{f}(X_{i})+\\varepsilon_{i}\\,-\\,m_{f}({\\pmb x})\\right|}\\\\ {\\displaystyle\\leq\\frac{1}{n_{r}}\\sum_{i\\in\\mathcal{X}_{r}}|m_{f}(X_{i})-m_{f}({\\pmb x})|+\\left|\\frac{1}{n_{r}}\\sum_{i\\in\\mathcal{X}_{r}}\\varepsilon_{i}\\right|}\\\\ {\\displaystyle\\leq r^{\\alpha}+\\left|\\frac{1}{n_{r}}\\sum_{i\\in\\mathcal{X}_{r}}\\varepsilon_{i}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "With the final equality coming by our smoothness condition and definition of ${{\\mathcal{Z}}_{r}}$ . Now by Hoeffding bounds we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left\\lvert\\sum_{i\\in\\mathbb{Z}_{r}}\\varepsilon_{i}\\right\\rvert\\ge\\sqrt{\\frac{2\\log(4/\\delta)M^{2}}{n_{r}}}\\right)\\le\\frac{\\delta}{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence by combining this event and $E_{n}$ then we have that with probability at least $1-\\delta$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\hat{m}_{f}(x)-m_{f}(x)|\\leq r^{\\alpha}+\\sqrt{\\frac{2\\log(2/\\delta)M^{2}}{n_{r}}}}\\\\ &{\\qquad\\qquad\\qquad\\leq r^{\\alpha}+\\sqrt{\\frac{2\\log(2/\\delta)M^{2}}{C_{0}n r^{d}-\\sqrt{2C_{0}n r^{d}\\log(2/\\delta)}-\\log(2/\\delta)/3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Now for sufficiently large $n$ ", "page_idx": 27}, {"type": "equation", "text": "$$\nC_{0}n r^{d}-\\sqrt{2C_{0}n r^{d}\\log(2/\\delta)}-\\log(2/\\delta)/3\\geq\\frac{C_{0}n r^{d}}{2\\log(2/\\delta)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This in turn gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\hat{m}_{f}(\\pmb{x})-m_{f}(\\pmb{x})|\\leq r^{\\alpha}+\\frac{2\\log(2/\\delta)M}{\\sqrt{C_{0}n r^{d}}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then the optimal choice of $r$ is such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{r^{\\alpha}=\\displaystyle\\frac{1}{\\sqrt{n r^{d}}}}}\\\\ {{\\Leftrightarrow r^{\\alpha+\\frac{d}{2}}=n^{-\\frac{1}{2}}}}\\\\ {{\\Leftrightarrow r=n^{-\\frac{1}{2\\alpha+d}}}}\\\\ {{\\Leftrightarrow r^{\\alpha}=n^{-\\frac{1}{2+d/\\alpha}}=\\displaystyle\\frac{1}{\\sqrt{n r^{d}}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence plugging this in we get that for sufficiently large $n$ depending on $C_{0},\\alpha$ we have that for $\\delta\\leq2e^{\\frac{\\lambda}{-1}}$ w.p. at least $1-\\delta$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n|\\hat{m}_{f}({\\pmb x})-m_{f}({\\pmb x})|\\leq\\frac{2M+1}{C_{0}}\\log(2/\\delta)n^{-\\frac{1}{2+d/\\alpha}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proposition 15 (Final weight decay). Suppose that for our $\\pmb{x}\\in\\mathcal{X}$ there exists $C_{0},r_{0}>0$ such that for any $r\\in(0,r)$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(X\\in B_{r}(\\pmb{x}))\\geq C_{0}r^{d}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Suppose that we are performing NW estimation of an $\\alpha$ smooth function at points $\\textbf{\\em x}\\in\\mathcal{X}$ using kernel $k_{r_{n}}(\\pmb{x},\\pmb{x}^{\\prime}):=\\bar{\\mathbb{1}}\\{\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|\\leq r\\}$ with $r_{n}$ decaying optimally. Now define ", "page_idx": 28}, {"type": "equation", "text": "$$\nw_{j}:=\\frac{k_{r_{n}}(\\pmb{x},X_{j})}{\\sum_{j^{\\prime}\\in\\mathcal{J}}k_{r_{n}}(\\pmb{x},X_{j^{\\prime}})}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "the weight of the $j^{t h}$ component. Then with probability at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in\\mathcal{I}}w_{j}\\le=\\frac{\\log(2/\\delta)}{C_{0}}n^{\\frac{-2}{2+d/\\alpha}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. We have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j\\in\\mathcal{I}}{\\operatorname*{max}}\\,w_{j}\\le\\frac{1}{\\sum_{j\\in\\mathcal{I}}k(\\pmb{x},X_{j})}}\\\\ &{\\qquad\\qquad=\\frac{1}{\\sum_{j\\in\\mathcal{I}}1\\{X_{j}\\in B_{r}(\\pmb{x})\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Hence by proposition 13 we have w.p. at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in\\mathcal{I}}w_{j}\\le\\left(C_{0}n r^{d}-\\sqrt{2C_{0}n r^{d}\\log(1/\\delta)}-\\log(1/\\delta)/3)\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We know from Proposition 14 that the optimal radius decay gives $\\begin{array}{r}{\\frac{1}{n r^{d}}\\,=\\,n^{-\\frac{2}{2+d/\\gamma}}}\\end{array}$ . Plugging this into our result gives that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{j\\in\\mathcal{I}}{\\operatorname*{max}}\\,w_{j}\\le\\bigg(C_{0}n^{\\frac{2}{2+d/\\gamma}}-\\sqrt{2C_{0}\\log(1/\\delta)}n^{\\frac{1}{2+d/\\gamma}}-\\log(1/\\delta)/3\\bigg)^{-1}}\\\\ {\\le\\frac{\\log(1/\\delta)}{C_{0}n^{\\frac{2}{2+d/\\gamma}}}\\ \\ \\mathrm{For\\;sufficiently\\;large}\\;n\\mathrm{~depending~on~}\\gamma,C_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that for a $\\gamma$ smooth function the MSE is $C^{-1}n^{-\\frac{1}{2+d/\\gamma}}$ . Hence the box kernel comfortably satisfies our weight decay condition 2 in Assumptions 2. ", "page_idx": 28}, {"type": "text", "text": "Additionally now if we have an $\\alpha$ smooth function and assume that for any $\\mathbf{\\boldsymbol{x}}^{\\prime}\\,\\in\\,\\mathcal{X}$ there exists $C_{0}(x^{\\prime})$ such that for all $r\\in\\left(0,r_{0}\\right)\\mathbb{P}(X\\in B_{r}({\\pmb x}^{\\prime})\\geq C_{0}r^{d}$ . Then we have that w.p. at least $1-\\delta$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nw_{F_{a};i}(X,X^{\\mathbb{Z}})\\leq\\frac{\\log(2/\\delta)}{C_{0}(X)}n^{-\\frac{2}{2+d/\\alpha}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus if we assume that there exists $C^{\\prime\\prime}>0$ such that w.p. at least $1-\\delta$ , $\\begin{array}{r}{\\frac{1}{C_{0}(X)}\\leq C^{\\prime\\prime}\\sqrt{\\log(1/\\delta)}}\\end{array}$ then we get that w.p. at least $1-\\delta$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nw_{F_{a};i}(X,X^{\\mathbb{Z}})\\leq C^{\\prime\\prime}\\log^{3/2}(3/\\delta)n^{-\\frac{2}{2+d/\\alpha}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This then gives our condition 2 in Assumptions 2. ", "page_idx": 28}, {"type": "text", "text": "We now try to bound $\\frac{\\sum_{j\\in\\mathcal{J}}w_{j}^{2}\\sigma(X_{j})}{\\mathbb{E}\\Big[\\sum_{j\\in\\mathcal{J}}w_{j}^{2}\\sigma(X_{j})\\Big]}$ ", "page_idx": 28}, {"type": "text", "text": "Corollary 16 (Accuracy under NW estimation with box kernel). Suppose that our linear smoother is NW estimation with the box kernel and optimally decaying radius additionally assume that: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\varepsilon_{\\hat{\\varphi}}(n,\\delta)=e_{\\hat{\\varphi}}(n)\\sqrt{\\log(1/\\delta)}\\ w i t h\\ e_{1}=o(1).}\\\\ &{\\bullet\\ \\varepsilon_{\\alpha}(n,\\delta)=C_{\\alpha}\\sqrt{\\log(1/\\delta)}n^{-\\frac{1}{2+d/\\alpha}}.}\\\\ &{\\bullet\\ \\varepsilon_{\\beta}(n,\\delta)=C_{\\beta}\\sqrt{\\log(1/\\delta)}n^{-\\frac{1}{2+d/\\beta}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\beta>\\frac{d}{2(1+d/\\gamma)}}\\end{array}$ . Then for any $\\delta$ , for sufficiently large n the following events each separately hold $w.p$ . at least $1-\\delta$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Big|\\hat{h}(y_{0},y_{1}|\\pmb{x})-h^{*}(y_{0},y_{1}|\\pmb{x})\\Big|\\leq C_{h}\\frac{\\log^{3/2}(1/\\delta)}{e_{h}(n)}}\\\\ {\\displaystyle|\\hat{g}(y|\\pmb{x})-g^{*}(y|\\pmb{x})|\\leq\\frac{C_{g}}{\\eta\\xi}\\frac{\\log^{3/2}(n/\\delta)}{e_{h}(n)}}\\\\ {\\mathbb{E}\\Big[|\\hat{g}(Y|\\pmb{x})-g^{*}(Y|\\pmb{x})|\\ \\Big|A=0,\\hat{g}\\Big]\\leq\\frac{C_{g,2}\\,\\mathrm{diam}(\\mathcal{Y})}{\\xi\\eta}\\frac{\\log^{3/2}(e_{2}(n)n/\\delta)}{e_{2}(n)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with $C_{h},C_{g},C_{g,2}$ depending on $C_{\\alpha},C_{\\beta},C^{\\prime},C^{\\prime\\prime}$ (where $C^{\\prime},C^{\\prime\\prime}$ are the constants define in Proposition $I5$ . ", "page_idx": 29}, {"type": "text", "text": "Proof. For any NW estimator we have that $\\lVert\\pmb{w}\\rVert_{1},\\lVert\\pmb{w}\\rVert_{2}\\leq1$ a.s. meaning we can take $\\varepsilon_{\\pmb{w}}(n,\\delta)\\equiv1$ . We also have that $\\varepsilon_{\\gamma}(n,\\delta)=C_{\\gamma}\\log(1/\\delta)n^{-\\frac{1}{1+d/\\gamma}}$ . Hence $\\varepsilon_{T_{n}}=\\sqrt{2\\varepsilon_{\\delta}(n)}e_{\\hat{\\varphi}}(n)\\sqrt{\\log(1/\\delta)}$ . This then gives ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Big|\\hat{h}(y_{0},y_{1}|x)-h^{*}(y_{0},y_{1}|x)\\Big|\\leq C_{\\gamma}\\log^{1/2}(7/\\delta)n^{-\\frac{1}{2+d/\\gamma}}+C_{\\alpha}C_{\\beta}\\log(7/\\delta)n^{-\\big(\\frac{1}{2+d/\\alpha}+\\frac{1}{2+d/\\beta}\\big)}}&{}\\\\ {+\\,\\sqrt{2\\varepsilon_{\\delta/4}(n)}e_{\\hat{\\varphi}}(n)\\sqrt{\\log(7/\\delta)}}&{}\\\\ {\\leq C_{h}\\log(1/\\delta)n^{-\\frac{1}{2+d/\\gamma}}+\\log(1/\\delta)n^{-\\big(\\frac{1}{2+d/\\alpha}+\\frac{1}{2+d/\\beta}\\big)}}&{}\\\\ {\\leq C_{h}\\frac{\\log(1/\\delta)}{e_{h}(n)}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for $\\delta<e^{-1}$ and $C_{h}=7(C_{\\gamma}+C_{\\alpha}C_{\\beta}+\\sqrt{2})$ giving our first result. ", "page_idx": 29}, {"type": "text", "text": "Using our weight decay results for NW estimation and plugging into Proposition 6 we get that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\varepsilon_{h-s t e p}=C^{\\prime\\prime}\\xi^{-1}\\frac{\\log^{3/2}}{e_{h}(n)}(n/\\delta).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence plugging into Theorem 2 gives ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\hat{g}(y|x)-g^{*}(y|x)|\\leq C_{h}\\sqrt{2}\\eta^{-1}\\frac{\\log\\left(n/\\delta\\right)}{e_{h}(n)}+4C^{\\prime\\prime}\\xi^{-1}\\log^{3/2}(n/\\delta)n^{-\\frac{1}{2+d/\\gamma}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{C_{g}}{\\xi\\eta}\\frac{\\log^{3/2}(n/\\delta)}{e_{h}(n)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with $C_{g}=C_{h}\\sqrt{2}+4C^{\\prime\\prime}$ giving our second result. Finally by Corollary 12 for $\\delta<e^{-1}$ w.p. at least $1-\\delta$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\lvert\\hat{g}(Y\\vert x)-g^{*}(Y\\vert x)\\rvert\\;\\left\\vert A=0,\\hat{g}\\right.\\right]\\le\\frac{C_{g}\\,\\mathrm{diam}(y)}{\\xi\\eta}\\frac{\\log^{3/2}(n e_{h}(n)/\\delta)}{e_{h}(n)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that both $\\varepsilon_{T_{n}}$ and $\\varepsilon_{h-s t e p}$ are actually both $o(\\varepsilon_{\\gamma}(n,c))$ , thus if we were allowed to take $n$ sufficiently large depending upon $\\delta$ then we would have all $\\log^{3/2}$ terms replaced with log terms and the dependence on $C^{\\prime},C^{\\prime\\prime}$ removed. ", "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We introduce a a new estimand, the CQC. We claim this to maintain the distributional information of the CQTE which we show through the definition and also on real world scenarios in Section 4. We claim that our method is able to obtain double robustness which we show in proposition 1 and Theorem 2. We also demonstrate its strong performance empirically on simulated data scenarios in Section 4 and Appendix B.1 ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have a dedicated limitations section where we discuss limitations in our theoretical results regarding both assumptions of sufficiently quickly decaying weights in our linear smoother and requiring smoothness in $h^{*}$ rather than in $g^{*}$ . We also discuss the limitations in the interpretability of quantile based approaches in general when exploring treatment effect as well as limitations in the interpretability of our assumptions within our theory. Finally we briefly address the methodological limitations of our approach acknowledging this as a potential area for improvement. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The two main results of the paper, Proposition 1 and Theorem 2 are proved in sections C.1 and C.2. The assumptions for these results are given in Assumptions 1 & 2 with the implications of these assumptions discussed. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The paper gives detailed information about the experimental set-up and explicit algorithms for methods introduced. The specific use of algorithm 1 with NW estimation is also described Appendix A.2. In addition code is provided in the supplementary materials with \u2018SimulatedExperiments.ipynb\u2018 reproducing all of the experiments. We also provide a link to a public github repository containig this in a footnote on page 9. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with ", "page_idx": 31}, {"type": "text", "text": "the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 32}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Code is provided in the supplementary materials with each function documented as well as in a linked public github repository. Jupyter Notebooks are provided to reproduce all of the experiments and real world data settings used within the paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Most of the experimental details are provided alongside the experimental results in Section 4 with additional details provided in Appendix A.4. In addition code to replicate all of the experiments is provided in the supplmentary materials. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: For our experimental results we explain that 500 replications of the entire fitting procedure (including newly generated data making each run entirely independent) was run and that $95\\%$ confidence intervals are provided for the mean absolute error of each estimator. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Information on runtime and compute resources is provided in Appendix A.4. Overall computational resources were very low with all experiments run on a personal laptop and each one taking less than an hour even with a large number of replications. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: No outsourcing was done for this project/paper and all datasets used are publicly available and completely anonymised. Additionally we feel there is essentially no scope for our work to cause negative societal impact. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We discuss the potential positive impact of our work in terms of further empowering HTE analysis. We do not for see any potential negative impacts of our work as it is simply furthering the field of HTE and quantile based treatment effect analysis. To our knowledge, there is no suggestion that these fields could be used maliciously or unfairly beyond the ways that any regression technique or analysis could. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. \u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. \u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: There are no new datasets released in our work. Our models use standard nonparametric regression techniques and adapt these to best estimate our estimand, the CQC meaning there is no real risk of misuse beyond the misuse of any regression technique. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All datasets are publicly available and properly referenced. Proprietary code used is also open source and used in accordance with its licence. The licence for this code can also be found in code itself. The model we use is our own and so no licensing is required. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The only asset is the code which is implemented in our method which is provided in the supplementary materials alongside a licence. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects is used. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]