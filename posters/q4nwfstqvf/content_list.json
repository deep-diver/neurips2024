[{"type": "text", "text": "Nearly Minimax Optimal Regret for Multinomial Logistic Bandit ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Joongkyu Lee   \nSeoul National University   \nSeoul, South Korea   \njklee0717@snu.ac.kr   \nMin-hwan Oh   \nSeoul National University   \nSeoul, South Korea   \nminoh@snu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we study the contextual multinomial logit (MNL) bandit problem in which a learning agent sequentially selects an assortment based on contextual information, and user feedback follows an MNL choice model. There has been a significant discrepancy between lower and upper regret bounds, particularly regarding the maximum assortment size $K$ . Additionally, the variation in reward structures between these bounds complicates the quest for optimality. Under uniform rewards, where all items have the same expected reward, we establish a regret lower bound of $\\Omega(d{\\sqrt{T/K}})$ and propose a constant-time algorithm, ${0}{\\mathrm{FU-MNL+}}$ , that achieves a matching upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{T/K})$ . We also provide instancedependent minimax regret bounds under uniform rewards. Under non-uniform rewards, we prove a lower bound of $\\Omega(d{\\sqrt{T}})$ and an upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ , also achievable by OFU-MNL $^+$ . Our empirical studies support these theoretical findings. To the best of our knowledge, this is the first work in the contextual MNL bandit literature to prove minimax optimality \u2014 for either uniform or non-uniform reward setting \u2014 and to propose a computationally efficient algorithm that achieves this optimality up to logarithmic factors. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The multinomial logistic (MNL) bandit framework [48, 49, 7, 8, 41, 42, 45, 5, 54] describes sequential assortment selection problems in which an agent offer a sequence of assortments of at most $K$ item from a set of $N$ possible items and receives feedback only for the chosen decisions. The choice probability of each outcome is characterized by an MNL model [38]. This framework allows modeling of various real-world situations such as recommender systems and online retails, where selections of assortments are evaluated based on the user-choice feedback among offered multiple options. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we study the contextual MNL bandit problem [8, 7, 44, 16, 41, 42, 45, 5], where the features of items and possibly contextual information about a user at each round are available. Despite many recent advances, [16, 41, 42, 45, 5], however, no previous studies have proven the minimax optimality of contextual MNL bandits. Chen et al. [16] proposed a regret lower bound of $\\Omega(d{\\sqrt{T}}/K)$ , where $d$ is the number of features, $T$ is the total number of rounds, and $K$ is the maximum size of assortments, assuming the uniform rewards, i.e., rewards are all same for each of the total $N$ items. Furthermore, Chen and Wang [15] established a regret lower bound of $\\Omega({\\sqrt{N T}})$ in the non-contextual setting (hence, dependence on $N$ appears instead of $d_{.}$ ), which is tighter in terms of $K$ . It is important to note the difference in the assumptions for the attraction parameter for the outside option $v_{0}$ . Chen and Wang [15] assumed for the attraction parameter for the outside option to be $v_{0}\\,=\\,K$ , whereas Chen et al. [16] assumed $v_{0}=1$ . Therefore, it remains an open question whether and how the value of $v_{0}$ affects both lower and upper bounds of regret. ", "page_idx": 0}, {"type": "text", "text": "Table 1: Comparisons of lower and upper regret bounds in related works on MNL bandits with $T$ rounds, $N$ items, the maximum size of assortments $K$ , $d$ -dimensional feature vectors, and problemdependent constants $1/\\kappa=\\mathcal{O}(K^{2})$ and $\\kappa^{\\prime}=\\mathcal{O}(1/K)$ . $\\tilde{\\mathcal{O}}$ represents big- $\\cdot\\mathcal{O}$ notation up to logarithmic factors. For the computational cost (abbreviated as \u201cComput.\u201d), we consider only the dependence on the number of rounds $t$ . \u201cIntractable\u201d means a non-polynomial runtime. The notation \u201c\u00b4\u201d denotes not applicable. The starred $(^{*})$ papers only consider the non-contextual setting. ", "page_idx": 1}, {"type": "table", "img_path": "Q4NWfStqVf/tmp/1bfd36c073d6394f81e3e51053926df138cedfce7da92c9e6eee8f0a2d7c1d34.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Regarding regret upper b?ounds, Chen et al. [16] proposed an exponential runtime algorithm that achieves a regret of $\\bar{\\mathcal{O}}(d\\sqrt{T})$ in the setting with stochastic contexts and the non-uniform rewards. Under the same setting, Oh and Iyengar [42] and Oh and Iyengar [41] introduced polynomial-time algorithms that attain regrets of $\\bar{\\mathcal{O}}(d\\bar{\\mathcal{T}}/\\kappa)$ and $\\tilde{\\mathcal{O}}(d^{3/2}\\dot{\\sqrt{T}}/\\dot{\\kappa)}$ respectively, where $1/\\kappa=\\mathcal{O}(K^{2})$ is a problem-dependent constant. Recently, Perivier and Goyal [45]? improved the dependency on $\\kappa$ in the adversarial context setting, achieving a regret of $\\tilde{\\mathcal{O}}(d K\\sqrt{\\kappa^{\\prime}T})$ , where $\\kappa^{\\prime}\\;=\\;\\mathcal{O}(1/K)$ . However, their approach focuses solely on the setting with uniform rewards, which is a special case of non-uniform rewards, and currently, there is no tractable method to implement the algorithm. ", "page_idx": 1}, {"type": "text", "text": "As summarized in Table 1, there has been a gap between the upper and lower bounds in the existing works of contextual MNL bandits. No previous studies have confirmed whether lower or upper bounds are tight, obscuring what the optimal regret should be. This ambiguity is further exacerbated because many studies introduce their methods under varying conditions such as different reward structures and values of $v_{0}$ , without explicitly explaining how these factors impact regret. Additionally, there is currently no computationally efficient algorithm whose regret does not scale with $1/\\kappa=\\dot{\\mathcal{O}}(K^{2})$ or directly with $K$ . Intuitively, increasing $K$ provides more information at least in the uniform reward setting, potentially leading to a more statistically efficient learning process. However, no previous results have reflected such intuition. Hence, the following research questions arise: ", "page_idx": 1}, {"type": "text", "text": "\u2022 What is the optimal regret lower bound in contextual MNL bandits? ", "page_idx": 1}, {"type": "text", "text": "\u2022 Can we design a computationally efficient, nearly minimax optimal algorithm under the adversarial context setting? ", "page_idx": 1}, {"type": "text", "text": "In this paper, we affirmatively answer the questions by first tackling the contextual MNL bandit problem separately based on the structure of rewards\u2014uniform and non-uniform\u2014and the value of the outside option $v_{0}$ . In the setting of uniform rewards, we establish the tightest regret lower bound, explicitl?y demonstrating the dependence of regret on $v_{0}$ . Specifically, we prove a regret lower bound of $\\Omega(d{\\sqrt{T/K}})$ when $v_{0}=\\Theta(1)$ , a common assumption in contextual settings [6, 18, 44, 8, 41, 42, 9, 45, 5, 54, 34] (see Appendix C.1 for more details), and a lower bound of $\\Omega(d{\\sqrt{T}})$ when $v_{0}=\\Theta(K)$ . Furthermore, in the adversarial context setting, we introduce a computationally efficient and provably optimal (up to logarithmic factors) algorithm, OFU-MNL+. W?e prove that our proposed algorithm achieves a regret of $\\tilde{\\mathcal{O}}(d\\sqrt{T/K})$ when $v_{0}=\\Theta(1)$ and $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ when $v_{0}=\\Theta(K)$ , each of which matches the respective lowaer bounds that we establish up to logarithmic factors. Furthermore, in the non-uniform reward setting, we provide the optimal lower bound of $\\Omega(d{\\sqrt{T}})$ assuming $v_{0}=\\Theta(1)$ . In the same setting, our proposed algorithm also attains a matching upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ up to logarithmic factors. Our main contributions are summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Under uniform rewards, we establish a regret lower bound of $\\Omega(\\sqrt{v_{0}K}/(v_{0}+K)d\\sqrt{T})$ (Theorem 1), which is the tightest known lower bound in contextual MNL bandits. We propose, for the first time, a computationally efficient and provably optimal algorithm, OFU- $\\tt M N L+$ , achieving a matching upper bound of $\\mathcal{O}(\\sqrt{v_{0}K}/(v_{0}+K)d\\sqrt{T})$ (Theorem 2) up to logarithmic factors, while requiring only a constant computation cost per round. The results indicate that the regret improves as the assortment size $K$ increases, unless $v_{0}~=~\\Theta(K)$ . To the best of our knowledge, this is the first study to demonstrate the dependence of regret on the attraction parameter for the outside option $v_{0}$ and to highlight the advantages of a larger assortment size $K$ which aligns with intuition. That is, this is the first work to show that a regret upper bound (in either contextual or non-contextual setting) decreases as $K$ increases. Additionally, we provide instance-dependent minimax regret bounds (Proposition 1 and 2), up to logarithmic factors.   \n\u2022 Under non-uniform rewards, with setting $v_{0}=\\Theta(1)$ following the convention in contextual MNL bandits [6, 18, 44, 8, 41, 42, 9, 45, 5, 54, 34], we establish a regret lower bound of $\\Omega(d{\\sqrt{T}})$ (Theorem 3). To the best of our knowledge, this is the first and tightest lower bound established under non-uniform rewards. Moreover, ${0}{\\mathrm{FU-MNL+}}$ also achieves a matching upper bound (up to logarithmic factors) of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ (Theorem 4) in this setting.   \n\u2022 We also conduct numerical experiments and show that our algorithm consistently outperforms the existing MNL bandit algorithms while maintaining a constant computation cost per round. Furthermore, the empirical results corroborate our theoretical findings regarding the dependence of regret on the reward structure, $v_{0}$ and $K$ . ", "page_idx": 2}, {"type": "text", "text": "Overall, our paper addresses the long-standing open problem of closing the gap between upper and lower bounds for contextual MNL bandits. Our proposed algorithm is the first to achieve both provably optimality (up to logarithmic factors) and practicality with improved computation. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Lower bounds of MNL bandits. In contextual MNL bandits, to the best of our knowledge, only Chen et al. [16] proved a lower bound of $\\Omega(d{\\sqrt{T}}/K)$ with the attraction parameter for the outside option set at $v_{0}=1$ . However, in the non-contextual setting, there exist improved lower bounds in terms of $K$ . Agrawal et al. [8] demonstrated a lower bound of $\\mathrm{i}\\sqrt{N T/K})$ , and Chen and Wang [15] established a lower bound of $\\Omega({\\sqrt{N T}})$ . By setting $d\\,=\\,N$ , one can derive equivalent lower bounds for the contextual setting, specifically $\\Omega({\\sqrt{d T/K}})$ and $\\Omega({\\sqrt{d T}})$ , respectively. However, Agrawal et al. [8] and Chen and Wang [15] assumead $v_{0}\\;=\\;K$ when establishing their lower bounds, which differs from the setting used by Chen et al. [16], where $v_{0}=1$ . Moreover, to the best of our knowledge, all existing works Chen et al. [16], Agrawal et al. [8], Chen and Wang [15] have established the lower bounds under uniform rewards. Consequently, it remains unclear what the optimal regret is, depending on the value of $v_{0}$ and the reward structure. ", "page_idx": 2}, {"type": "text", "text": "Upper bounds of contextual MNL bandits. Ou et al. [44] formulated a linear utility model and achieved $\\tilde{\\mathcal{O}}(d K\\sqrt{T})$ regret; however, they assumed that utilities are fixed over time. Chen et al. [16] considered contextual MNL bandits with changing and stochastic contexts, establishing a regret of $\\tilde{\\mathcal{O}}(d\\sqrt{T}+d^{2}K^{2})$ . However, they encountered computational issues due to the need to enumerate all possible ( $N$ choose $K$ ) assortments. To address this, Oh and Iyengar [42] proposed a polynomial-time assortment optimization algorithm, which maintains the confidence bounds in the parameter space and then calculates the upper confidence bounds of attraction parameter for each item, achieving a regret of $\\tilde{\\mathcal{O}}(d\\sqrt{T}/\\kappa)$ , where $1/\\kappa=\\mathcal{O}(K^{2})$ is a problem-dependent constant. Perivier and Goyal [45] considered the adversarial context and uniform reward setting and improved the dependency on $\\kappa$ to $\\tilde{\\mathcal{O}}(d K\\sqrt{\\kappa^{\\prime}T}+d^{2}K^{4}/\\kappa)$ , where $\\kappa^{\\prime}=\\mathcal{O}(1/K)$ . However, their algorithm is intractable. Agrawal et al. [5] considered a uniform rewards setting (with $v_{0}\\,=\\,1\\,\\$ ) and achieved a regret of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ . However, due to significant technical errors in their paper (refer Appendix L), we do not include a comparison with their results in this work. ", "page_idx": 2}, {"type": "text", "text": "Recently, Zhang and Sugiyama [54] utilized an online parameter update to construct a constant time algorithm. However, they consider a multiple-parameter choice model in which the learner estimates $K$ parameters and shares the contextual information $x_{t}$ across the items in the assortment. This model differs from ours; we use a single-parameter choice model with varying the context for each item in the assortment. Additionally, they make a stronger assumption regarding the reward than we do (see Assumption 1). Moreover, while they fix the assortment size at $K$ , we allow it to be smaller than or equal to $K$ . On the other hand, Zhan?g and Luo [53] considered a general function approximation, achieving a regret bound of $\\tilde{\\mathcal{O}}(K^{2.5}\\bar{\\sqrt{d N T}})$ . However, this bound scales with $K$ and $N$ , and the proposed algorithm is not tractable. To the best of our knowledge, all existing methods fail to show that the regret upper bound can improve as the assortment size $K$ increases. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3 Problem Setting ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notations. For a positive integer, $n$ , we denote $[n]:=\\{1,2,\\dots,n\\}$ . For a real-valued matrix $A$ , we denote $\\|A\\|_{2}:=\\operatorname*{sup}_{x:\\|x\\|_{2}=1}\\|A x\\|_{2}$ as the maximum singular value of $A$ . For two symmetric matrices, $V$ and $W$ of the same dimensions, $V\\succeq W$ means that $V-W$ is positive semi-definite. Finally, we define $\\boldsymbol{S}$ to be the set of candidate assortment with size constraint at most $K$ , i.e., ${\\mathcal{S}}=\\{{\\bar{S}}\\subset[N]:|S|\\leqslant K\\}$ . While, for simplicity, we consider both $\\boldsymbol{S}$ and the set of items $[N]$ to be stationary in this paper, it is important to note that both $\\boldsymbol{S}$ and $[N]$ can vary over time. ", "page_idx": 3}, {"type": "text", "text": "Contextual MNL bandits. We consider a sequential assortment selection problem which is defined as follows. At each round $t$ , the agent observes feature vectors $\\boldsymbol{x}_{t i}\\in\\mathbb{R}^{\\dot{d}}$ for every item $i\\in[N]$ . Based on this contextual information, the agent presents an assortment $S_{t}=\\{i_{1},\\ldots,i_{l}\\}\\in{\\cal S}$ , where $l\\leqslant K$ , and then observes the user purchase decision $c_{t}\\in S_{t}\\cup\\{0\\}$ , where $\\{0\\}$ represents the \u201coutside option\u201d which indicates that the user did not select any of the items in $S_{t}$ . The distribution of these selections follows a multinomial logit (MNL) choice model [38], where the probability of choosing any item $i_{k}\\in S_{t}$ (or the outside option) is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{t}(i_{k}|S_{t},\\mathbf{w}^{\\star}):=\\frac{\\exp(x_{t i_{k}}^{\\top}\\mathbf{w}^{\\star})}{v_{0}+\\sum_{j\\in S_{t}}\\exp(x_{t j}^{\\top}\\mathbf{w}^{\\star})},\\quad p_{t}(0|S_{t},\\mathbf{w}^{\\star}):=\\frac{v_{0}}{v_{0}+\\sum_{j\\in S_{t}}\\exp(x_{t j}^{\\top}\\mathbf{w}^{\\star})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $v_{0}$ is a known attraction parameter for the outside option and $\\mathbf{w}^{\\star}\\in\\mathbb{R}^{d}$ is an unknown parameter. ", "page_idx": 3}, {"type": "text", "text": "Remark 1. In the existing literature on MNL bandits, it is commonly assumed that $v_{0}=1\\,[4I,42,$ , 45, 5, 54]. On the other hand, Chen and Wang [15], Agrawal et al. $[8J$ assume that $v_{0}\\,=\\,K^{\\mathrm{~l~}}t o$ induce a tighter lower bound in terms of $K$ . Later, we will explore how these differing assumptions create fundamentally different problems, leading to different regret lower bounds (Subsection 5.1). ", "page_idx": 3}, {"type": "text", "text": "The choice response for each item $i\\in S_{t}\\cup\\{0\\}$ is defined as $y_{t i}:=\\mathbb{1}(c_{t}=i)\\in\\{0,1\\}$ . Hence, the choice feedback variable $\\mathbf{y}_{t}:=\\left(y_{t0},y_{t i_{1}},\\ldots y_{t i_{l}}\\right)$ is sampled from the following multinomial (MNL) distribution: $\\mathbf{y}_{t}\\sim\\mathrm{MNL}\\{1,(p_{t}(0|S_{t},\\mathbf{w}^{\\star}),\\ldots,p_{t}(i_{l}|S_{t},\\mathbf{w}^{\\star}))\\}$ , where the parameter 1 indicates that $\\mathbf{y}_{t}$ is a single-trial sample, i.e., lk\u201c1 ytik \u201c 1. For each i P St Y t0u, we define the noise $\\epsilon_{t i}\\;:=\\;y_{t i}\\,-\\,p_{t}(i|S_{t},\\mathbf{w}^{\\star})$ . Since eac h\u0159 $\\epsilon_{t i}$ is a bounded random variable in $[0,1]$ , $\\epsilon_{t i}$ is $1/4$ -subGaussian. At every round $t$ , the reward $r_{t i}$ for each item $i$ is also given. Then, we define the expected revenue of the assortment $S$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{t}(S,\\mathbf{w}^{\\star}):=\\sum_{i\\in S}p_{t}(i|S,\\mathbf{w}^{\\star})r_{t i}=\\frac{\\sum_{i\\in S}\\exp(x_{t i}^{\\top}\\mathbf{w}^{\\star})r_{t i}}{v_{0}+\\sum_{j\\in S}\\exp(x_{t j}^{\\top}\\mathbf{w}^{\\star})}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and define $S_{t}^{\\star}$ as the offline optimal assortment at time $t$ when $\\mathbf{w}^{\\star}$ is known a prior, i.e., $S_{t}^{\\star}\\,=$ $\\mathrm{argmax}_{S\\in S}\\,R_{t}(S,\\mathbf{w}^{\\star})$ . Our objective is to minimize the cumulative regret over the $T$ periods: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star})=\\sum_{t=1}^{T}R_{t}(S_{t}^{\\star},\\mathbf{w}^{\\star})-R_{t}(S_{t},\\mathbf{w}^{\\star}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "When $K\\,=\\,1$ , $r_{t1}~=~1$ , and $v_{0}~=~1$ , the MNL bandit recovers the binary logistic bandit with $R_{t}(S=\\{x\\},\\mathbf w^{\\star})=\\sigma\\left(x^{\\top}\\mathbf w^{\\star}\\right)=1/(1+\\exp(-x^{\\top}\\mathbf w^{\\star}))$ , where $\\sigma(\\cdot)$ is the sigmoid function. ", "page_idx": 3}, {"type": "text", "text": "Consistent with previous works on MNL bandits [42, 45, 5, 54], we make the following assumptions: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Bounded assumption). We assume that $\\|\\mathbf{w}^{\\star}\\|_{2}\\leqslant1$ , and for all $t\\geqslant1$ , $i\\,\\in\\,[N].$ , $\\|{\\boldsymbol x}_{t i}\\|_{2}\\bar{\\leqslant}1$ and $r_{t i}\\in[0,1]$ . ", "page_idx": 3}, {"type": "text", "text": "Assumption 2 (Problem-dependent constant). There exist $\\kappa>0$ such that for every item $i\\in S$ and any $S\\in S$ , and all round $t$ , $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{w}\\in\\mathcal{W}}p_{t}(i|S,\\mathbf{w})p_{t}(0|S,\\mathbf{w})\\geqslant\\kappa,}\\end{array}$ , where $\\mathcal{W}=\\{\\mathbf{w}\\in\\mathrm{\\dot{\\mathbb{R}}}^{d}\\ |\\ |\\mathbf{w}\\|_{2}\\leqslant1\\}$ . ", "page_idx": 4}, {"type": "text", "text": "In Assumption 1, we assume that the reward for each item $i$ is bounded by a constant, allowing the norm of the reward vector to depend on $K$ , e.g., $\\|\\pmb{\\rho}_{t}\\|_{2}\\leqslant\\sqrt{K}$ . In contrast, Zhang and Sugiyama [54] assume that the norm of the reward vector $\\pmb{\\rho}_{t}=[r_{t1},\\dots...r_{t|S_{t}|}]^{\\intercal}\\in\\mathbb{R}^{|S_{t}|}$ is bounded by a constant, independent of $K$ , e.g., $\\|\\rho_{t}\\|_{2}\\leqslant1$ . Thus, our assumption regarding rewards is weaker than theirs. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 is common in contextual MNL bandits [16, 42, 45, 54]. Note that $1/\\kappa$ depends on the maximum size of the assortment $K$ , i.e., $1/\\kappa=\\mathcal{O}(K^{2})$ . One of the primary goals of this paper is to show that as the assortment size $K$ increases, we can achieve an improved (or at least not worsened) regret bound. To this end, we design a dynamic assortment policy that enjoys improved dependence on $\\kappa$ . Note that our algorithm does not need to know $\\kappa$ a priori, whereas Oh and Iyengar [41, 42] do. ", "page_idx": 4}, {"type": "text", "text": "4 Existing Gap between Upper and Lower Bounds in MNL Bandits ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The primary objective of this paper is to establish minimax regrets in contextual MNL bandits. To explore the optimality of regret, we analyze how it depends on the attraction parameter for the outside option $v_{0}$ , the maximum assortment size $K$ , and the structure of rewards. ", "page_idx": 4}, {"type": "text", "text": "Dependence on $v_{0}$ . Currently, the established lower bounds are $\\Omega(d{\\sqrt{T}}/K)$ by Chen et al. [16], $\\Omega({\\sqrt{d T/K}})$ by the contextual version of Agrawal et al. [8], and $\\Omega({\\sqrt{d T}})$ , which is the tightest in terms of $K$ , by the contextual version of Chen and Wang [15]. These results can be misleading, as many subsequent studies [42, 40, 17, 53] have claimed that a $K$ -independent regret is achievable, without clearly addressing the influence of the value of $v_{0}$ . In fact, the improved regret bounds (in terms of $K$ ) obtained by Agrawal et al. [8] and Chen and Wang [15] were possible when $v_{0}\\,=\\,K$ . However, in the contextual setting, it is more common to set $v_{0}=\\Theta(1)$ . This is because, given the context for the outside option $x_{t0}$ , it is straightforward to construct an equivalent choice model where $v_{0}=\\Theta(1)$ (refer Appendix C.1). In this paper, we rigorously show the regret dependency on the value of $v_{0}$ . In Theorem 1, we establish a regret lower bound of $\\Omega\\left(\\sqrt{v_{0}K}/(v_{0}+K)d\\sqrt{T}\\right)$ , which implies that the value of $v_{0}$ , indeed, affects the regret. Then, in Theor\\`em 2, we show that our p\u02d8roposed computationally efficient algorithm, ${0}{\\mathrm{FU-MNL+}}$ achieves a regret of $\\tilde{\\mathcal{O}}\\left(\\sqrt{v_{0}K}/(v_{0}+\\bar{K})\\bar{d}\\sqrt{T}\\right)$ , which is minimax optimal up to logarithmic factors in terms of all $d,T,K$ \\`and even $v_{0}$ . ", "page_idx": 4}, {"type": "text", "text": "Dependence on $K$ & Uniform/Non-uniform rewards. To the best of our knowledge, the regret bound in all existing works in contextual MNL bandits does not decrease as the assortment size $K$ increases [16, 41, 42, 45]. However, intuitively, as the assortment size increases, we can gain more information because we receive more feedback. Therefore, it makes sense that regret could be reduced as $K$ increases, at least in the uniform reward setting. Under uniform rewards, the expected revenue (to be specified later) increases as more items are added in the assortment. Consequently, both the optimistically chosen assortment and the optimal assortment always have a size of $K$ . Thus, the agent obtain information about exactly $K$ items in each round. This phenomenon is also demonstrated empirically in Figure 1. In the uniform reward setting, as $K$ increases, the cumulative regrets of not only our proposed algorithm but also other baseline algorithms decrease. This indicates that the existing regret bounds are not tight enough in terms of $K$ . Conversely, in the non-uniform reward setting, the sizes of both the optimistically chosen assortment and the optimal assortment can be less than $K$ , so performance improvement is not guaranteed. In this paper, we show that the regret dependence on $K$ varies by case: uniform and non-uniform rewards. When $v_{0}=\\Theta(1)$ , we obtain a regret lower bound of $\\Omega(d{\\sqrt{T/K}})$ (Theorem 1) and a regret upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{T/K})$ (Theorem 2) under uniform rewards. Additionally, we achieve a regret lower bound of (Theorem 3) and a regret upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ (Theorem 4) under non-uniform rewards. ", "page_idx": 4}, {"type": "text", "text": "5 Algorithms and Main Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we begin by proving the tightest regret lower bound under uniform rewards (Subsection 5.1), explicitly showing the dependence on the attraction parameter for the outside option $v_{0}$ . We then introduce OFU-MNL $^+$ , an algorithm that achieves minimax optimality, up to logarithmic factors, under uniform rewards (Subsection 5.2). Notably, OFU-MNL $^+$ is designed for efficiency, requiring only constant computation and storage costs. Finally, we establish the tightest regret lower bound and a matching minimax optimal regret upper bound under non-uniform rewards (Subsection 5.3). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "5.1 Regret Lower Bound under Uniform Rewards ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we present a lower bound for the worst-case expected regret in the uniform reward setting $(r_{t i}=1)$ ). This covers all applications where the objective is to maximize the appropriate \u201cclick-through rate\u201d by offering the assortment. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Regret lower bound, Uniform rewards). Let d be divisible by 4 and let Assumption $^{\\,l}$ hold true. Suppose $T\\geqslant C\\cdot d^{4}(v_{0}+K)/K$ for some constant $C>0$ . Then, in the uniform reward setting, for any policy $\\pi_{.}$ , there exists a worst-case problem instance with $N=\\Theta(K\\cdot2^{d})$ items such that the worst-case expected regret of $\\pi$ is lower bounded as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{w}}\\mathbb{E}_{\\mathbf{w}}^{\\pi}\\left[\\mathbf{Reg}_{T}(\\mathbf{w})\\right]=\\Omega\\left({\\frac{\\sqrt{v_{0}K}}{v_{0}+K}}\\cdot d{\\sqrt{T}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Discussion of Theorem 1. If $v_{0}~=~\\Theta(1)$ , Theorem 1 demonstrates a regret lower bound of $\\Omega(d{\\sqrt{T/K}})$ . This indicates that, under uniform rewards, increasing the assortment size $K$ leads to an improvement in regret. Compared to the lower bound $\\Omega(d{\\sqrt{T}}/K)$ proposed by Chen et al. [16], our lower bound is improved by a factor of $\\sqrt{K}$ . This improvement is mainly due to the establishment of a tighter upper bound for the KL divergence (Lemma D.2). Notably, Chen et al. [16] also considered uniform rewards with $v_{0}\\;=\\;\\Theta(1)$ . On the other hand, Chen and Wang [15] and Agrawal et al. [8] established regret lower bounds of $\\Omega({\\sqrt{N T}})$ and $\\Omega({\\sqrt{N T/K}})$ , respectively, in non-contextual MNL bandits with uniform rewards, by setting $v_{0}=K$ tao achieve these regrets. Theorem 1 shows that if $v_{0}\\,=\\,\\Theta(K)$ , we can obtain a regret lower bound of $\\Omega(d{\\sqrt{T}})$ , which is consistent with the $K$ -independent regret in Chen and Wang [15]. To the best of our knowledge, this result is the first to explicitly show the dependency of regret on the attraction parameter for the outside option $v_{0}$ . The proof is deferred to Appendix D. ", "page_idx": 5}, {"type": "text", "text": "5.2 Minimax Optimal Regret Upper Bound under Uniform Rewards ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we propose a new algorithm ${0}{\\mathrm{FU-MNL+}}$ , which enjoys minimax optimal regret up to logarithmic factors in the case of uniform rewards. Note that, since the revenue is an increasing function when rewards are uniform, maximizing the expected revenue $R_{t}(S,\\mathbf{w})$ over all $S\\,\\in\\,{\\bar{S}}$ always yields exactly $K$ items, i.e., $|S_{t}|=|S_{t}^{\\star}|=K$ . ", "page_idx": 5}, {"type": "text", "text": "Our first step involves constructing the confidence set for the online parameter. ", "page_idx": 5}, {"type": "text", "text": "Online parameter estimation. Instead of performing MLE as in previous works [16, 42, 45], inspired by Zhang and Sugiyama [54], we use the online mirror descent algorithm to estimate parameter. We first define the multinomial logistic loss function at round $t$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell_{t}(\\mathbf{w}):=-\\sum_{i\\in S_{t}}y_{t i}\\log p_{t}(i|S_{t},\\mathbf{w}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Proposition C.1, we will show that the loss function has the constant parameter self-concordant-like property. We estimate the true parameter $\\mathbf{w}^{\\star}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}=\\underset{\\mathbf{w}\\in\\mathcal{W}}{\\mathrm{argmin}}\\big\\langle\\nabla\\ell_{t}(\\mathbf{w}_{t}),\\mathbf{w}\\big\\rangle+\\frac{1}{2\\eta}\\|\\mathbf{w}-\\mathbf{w}_{t}\\|_{\\tilde{H}_{t}}^{2}\\,,\\quad\\forall t\\geqslant1,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\eta>0$ is the step-size parameter to be specified later, and $\\tilde{H}_{t}:=H_{t}+\\eta\\mathcal{G}_{t}(\\mathbf{w}_{t})$ , where ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{G}_{t}(\\mathbf{w}):=\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w})x_{t i}x_{t i}^{\\top}-\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w})p_{t}(j|S_{t},\\mathbf{w})x_{t i}x_{t j}^{\\top},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and $\\begin{array}{r}{H_{t}:=\\lambda\\mathbf{I}_{d}+\\sum_{s=1}^{t-1}\\mathcal{G}_{s}\\big(\\mathbf{w}_{s+1}\\big)}\\end{array}$ . Note that $\\mathcal{G}_{t}(\\mathbf{w})=\\nabla^{2}\\ell_{t}(\\mathbf{w})$ . This online estimator is efficient in terms of both c o\u0159mputation and storage. By a standard online mirror descent formulation [43], the optimization problem in (3) can be solved using a single projected gradient step through the following equivalent formula: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}^{\\prime}=\\mathbf{w}_{t}-\\eta\\tilde{H}_{t}^{-1}\\nabla\\ell_{t}(\\mathbf{w}_{t}),\\quad\\mathrm{and}\\quad\\mathbf{w}_{t+1}=\\underset{\\mathbf{w}\\in\\mathcal{W}}{\\mathrm{argmin}}\\left\\lVert\\mathbf{w}-\\mathbf{w}_{t+1}^{\\prime}\\right\\rVert_{\\tilde{H}_{t}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "1: Inputs: regularization parameter $\\lambda$ , probability $\\delta$ , confidence radius $\\beta_{t}(\\delta)$ , step size $\\eta$ .   \n2: Initialize: $H_{1}=\\lambda\\mathbf{I}_{d}$ and $\\mathbf{w}_{1}$ as any point in $\\mathcal{W}$ ,   \n3: for round $t=1,2,\\cdots,T$ do   \n4: Compute $\\alpha_{t i}=x_{t i}^{\\top}\\mathbf{w}_{t}+\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}$ for all $i\\in[N]$ .   \n5: Offer $S_{t}=\\operatorname{argmax}_{S\\in S}\\tilde{R}_{t}(S)$ and observe $\\mathbf{y}_{t}$ .   \n6: Update $\\tilde{H}_{t}=H_{t}+\\eta\\mathcal{G}_{t}(\\mathbf{w}_{t})$ , and update the estimator $\\mathbf{w}_{t+1}$ by (3).   \n7: Update $H_{t+1}=H_{t}+\\mathcal{G}_{t}(\\mathbf{w}_{t+1})$ .   \n8: end for ", "page_idx": 6}, {"type": "text", "text": "which enjoys a computational cost of only $O(K d^{3})$ , completely independent of $t$ [39, 54]. Regarding storage costs, the estimator does not need to store all historical data because both ${\\tilde{H}}_{t}$ and $H_{t}$ can be updated incrementally, requiring only $O(d^{2})$ storage. ", "page_idx": 6}, {"type": "text", "text": "Furthermore, the estimator allows for a $\\kappa$ -independent confidence set, leading to an improved regret. Lemma 1 (Online parameter ?confidence set). Let $\\delta~\\in~(0,1]$ . Under Assumption $^{\\,l}$ , with $\\eta\\:=\\:$ ${\\textstyle\\frac{1}{2}}\\log(K+1)+2$ and $\\lambda=84{\\sqrt{2}}d\\eta_{\\l}$ , we define the following confidence set ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{t}(\\delta):=\\{\\mathbf{w}\\in\\mathcal{W}\\;|\\;\\|\\mathbf{w}_{t}-\\mathbf{w}\\|_{H_{t}}\\leqslant\\beta_{t}(\\delta)\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\beta_{t}(\\delta)=\\mathcal{O}\\left(\\sqrt{d}\\log t\\log K\\right)$ . Then, we have $\\operatorname*{Pr}[\\forall t\\geqslant1,\\mathbf{w}^{\\star}\\in\\mathcal{C}_{t}(\\delta)]\\geqslant1-\\delta.$ . ", "page_idx": 6}, {"type": "text", "text": "Armed with the online estimator, we construct the computationally efficient optimistic revenue. ", "page_idx": 6}, {"type": "text", "text": "Computationally efficient optimistic expected revenue. To balance the exploration and exploitation trade-off, we use the upper confidence bounds (UCB) technique, which have been widely studied in many bandit problems, including $K$ -arm bandits [11, 42] and linear bandits [1, 20]. ", "page_idx": 6}, {"type": "text", "text": "At each time $t$ , given the confidence set in Lemma 1, we first calculate the optimistic utility $\\alpha_{t i}$ as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\alpha_{t i}:=x_{t i}^{\\top}\\mathbf{w}_{t}+\\beta_{t}(\\delta)\\lVert x_{t i}\\rVert_{H_{t}^{-1}},\\quad\\forall i\\in[N].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The optimistic utility $\\alpha_{t i}$ is composed of two parts: the mean utility estimate $x_{t i}^{\\top}\\mathbf{w}_{t}$ and the standard deviation $\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}$ . In the proof of the regret upper bound, we show that $\\alpha_{t i}$ serves as an upper bound for xtJi $x_{t i}^{\\top}\\mathbf{w}^{\\star}$ , assuming that the true parameter $\\mathbf{w}^{\\star}$ falls within the confidence set $\\mathcal{C}_{t}(\\delta)$ . Based on $\\alpha_{t i}$ , we construct the optimistic expected revenue for the assortment , defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{R}_{t}(S):=\\frac{\\sum_{i\\in{\\cal S}}\\exp(\\alpha_{t i})r_{t i}}{v_{0}+\\sum_{j\\in{\\cal S}}\\exp(\\alpha_{t j})},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $r_{t i}\\,=\\,1$ . Then, we offer the set $S_{t}$ that maximizes the optimistic expected revenue, $S_{t}\\,=$ $\\mathrm{argmax}_{S\\in S}\\,\\tilde{R}_{t}(S)$ . Given our assumption that all rewards are of unit value, the optimization problem is equivalent to selecting the $K$ items with the highest optimistic utility $\\alpha_{t i}$ . Consequently, solving the optimization problem incurs a constant computational cost of $\\mathcal{O}(N)$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 2 (Comparison to Zhang and Sugiyama [54]). In Zhang and Sugiyama [54], the MNL choice model is outlined with a shared context $x_{t}$ and distinct parameters $\\mathbf{w}_{1}^{\\star},\\dots,\\mathbf{w}_{K}^{\\star}$ for each choice. Conversely, our model employs $a$ single parameter $\\mathbf{w}^{\\star}$ across all choices and has varying contexts for each item in the assortment $S$ , $x_{t1},\\ldots.x_{t i_{|S|}}$ . Due to this discrepancy in the choice model, directly applying Proposition 1 from Zhang and Sugiyama $I54J,$ which constructs the optimistic revenue by adding bonus terms to the estimated revenue, incurs an exponential computational cost in our problem setting. This complexity arises because the optimistic revenue must be calculated for every possible assortment $S\\in S$ ; therefore, it is necessary to enumerate all potential assortments (N choose $K$ ) to identify the one that maximizes the optimistic revenue As a result, extending the approach in Zhang and Sugiyama $I54J$ to our setting is non-trivial, requiring a different analysis. ", "page_idx": 6}, {"type": "text", "text": "We now present the regret upper bound of OFU-MNL $^+$ in the uniform reward setting. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 (Regret upper bound of OFU-MNL $^+$ , Uniform rewards). Let $\\delta\\in(0,1]$ and Ass?umptions 1 and 2 hold. In the uniform reward setting, by setting $\\begin{array}{r}{\\eta=\\frac{1}{2}\\log(K+1)+2}\\end{array}$ and $\\lambda=84\\sqrt{2}d\\eta,$ , with probability at least $1-\\delta$ , the cumulative regret of OFU-MNL $^+$ is upper-bounded by ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star})=\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{v_{0}K}}{v_{0}+K}\\cdot d\\sqrt{T}+\\frac{1}{\\kappa}d^{2}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Discussion of Theorem 2. If $T\\,\\geqslant\\,{\\mathcal O}(d^{2}(v_{0}\\,+\\,K)^{2}/(\\kappa^{2}v_{0}K))$ , Theorem 2 shows that our algorithm OFU-MNL $^+$ achieves minimax optimal regret (up to logarithmic factor) in terms of all $d$ , $T,\\,K$ , and even $v_{0}$ . To the best of our knowledge, ignoring logarithmic factors, our proposed algorithm is the first computationally efficient, minimax optimal algorithm in (adversarial) contextual MNL bandits. When $v_{0}\\,=\\,\\Theta(1)$ , which is the convention in existing MNL bandit literature [41, 42, 45, 5, 54], OFU-MNL $^+$ obtains $\\tilde{\\mathcal{O}}(d\\sqrt{T/K})$ regret. This represents an improvement over the previous upper bound of Perivier and Goyal [45] 2, which is $\\tilde{\\mathcal{O}}(d K\\sqrt{\\kappa^{\\prime}T}+d^{2}K^{4}/\\kappa)$ , where $\\kappa^{\\prime}=\\mathcal{O}(1/K)$ , by a factor of $K$ . This improvement can be attributed to two key factors: an improved, constant, self-concordant-like property of the loss function (Proposition C.1) and a $K$ -free elliptical potential lemma (Lemma E.2). Furthermore, by employing an improved bound for the second derivative of the revenue (Lemma E.3), we achieve an enhancement in the regret for the second term, $d^{2}/\\kappa$ , by a factor of $K^{4}$ , in comparison to Perivier and Goyal [45]. Unless $v_{0}=\\Theta(K)$ , Theorem 2 indicates that the regret decreases as the assortment size $K$ increases. To the best of our knowledge, this is the first algorithm in MNL bandits to show that increasing $K$ results in a reduction in regret. Moreover, when reduced to t?he logistic bandit, i.e., $K=1$ , $r_{t1}=1$ , and $v_{0}=1$ , our algorithm can also achieve a regret of $\\tilde{\\mathcal{O}}(d\\sqrt{\\kappa T})$ by Corollary 1 in Zhang and Sugiyama [54], which is consistent with the results in Abeille et al. [4], Faury et al. [23]. The proof is deferred to Appendix E. ", "page_idx": 7}, {"type": "text", "text": "Remark 3 (Efficiency of ${0}\\mathrm{FU-MNL+}$ ). The proposed algorithm is computationally efficient in both parameter updates and assortment selections. Since we employ online parameter estimation, akin to Zhang and Sugiyama [54], our algorithm demonstrates computational efficiency in parameter estimation, incurring only incurring $\\bar{O}(K d^{3})$ computation cost and $O(d^{2})$ storage cost, which are completely independent of t. Furthermore, a naive approach to selecting the optimistic assortment requires enumerating all possible $^N$ choose $K$ ) assortments, resulting in exponential computational cost [16]. However, by constructing the optimistic expected revenue according to (6) (inspired by Oh and Iyengar [42]), our algorithm needs only $\\mathcal{O}(N)$ computational cost. ", "page_idx": 7}, {"type": "text", "text": "5.3 Regret Upper & Lower Bounds under Non-Uniform Rewards ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this subsection, we propose regret upper and lower bounds in the non-uniform reward setting. In this scenario, the sizes of both the chosen assortment $S_{t}$ , and the optimal assortment, $S_{t}^{\\star}$ are not fixed at $K$ . Therefore, we cannot guarantee an improvement in regret even as $K$ increases. ", "page_idx": 7}, {"type": "text", "text": "We first prove the regret lower bound in the non-uniform reward setting. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (Regret lower bound, Non-uniform rewards). Under the same conditions as Theorem 1, let the rewards be non-uniform and $v_{0}=\\Theta(1)$ . Then, for any policy $\\pi$ , there exists a worst-case problem instance such that the worst-case expected regret of $\\pi$ is lower bounded as follows: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{w}}\\mathbb{E}_{\\mathbf{w}}^{\\pi}\\left[\\mathbf{Reg}_{T}(\\mathbf{w})\\right]=\\Omega\\left(d\\sqrt{T}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Discussion of Theorem 3. In contrast to Theorem 1, which considers uniform rewards, the regret lower bound is independent of the assortment size $K$ . Note that Theorem 3 does not claim that non-uniform rewards inherently make the problem more difficult. Rather, it implies that there exists an instance with adversarial non-uniform rewards, where regret does not improve even with an increase in $K$ . Moreover, the assumption that $v_{0}\\,=\\,\\Theta(1)$ is common in the existing literature on contextual MNL bandits [41, 42, 45, 5, 54] (refer Appendix C.1). To the best of our knowledge, this is the first established lower bound for non-uniform rewards in MNL bandits, even in the non-contextual setting. The proof is deferred to Appendix G. ", "page_idx": 7}, {"type": "text", "text": "We also prove a matching upper bound up to logarithmic factors. The algorithm ${0}{\\mathrm{FU-MNL+}}$ is also applicable in the case of non-uniform rewards. However, because the optimistic expected revenue $\\bar{\\tilde{R}_{t}}(S)$ is no longer an increasing function of $\\alpha_{t i}$ , optimizing for $S_{t}=\\operatorname*{argmax}_{S\\in S}\\tilde{\\tilde{R}_{t}}(S)$ no longer equates to simply selecting the top $K$ items with the highest optimistic utility. Instead, we employ assortment optimization methods introduced in Rusmevichientong et al. [48], Davis et al. [21], which are efficient polynomial-time (independent of $t$ ) 3 algorithms available for solving this optimization problem. Therefore, our algorithm is also computationally efficient under non-uniform rewards. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Theorem 4 (Regret upper bound of OFU-MNL $^+$ , Non-uniform rewards). Under the same assumptions and parameter settings as Theorem 2, if the rewards are non-uniform and $v_{0}=\\Theta(1)$ , then with $a$ probability at least $1-\\delta$ , the cumulative regret of OFU- $\\mathtt{M N L+}$ is upper-bounded by ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\bf R e g}_{T}({\\bf w}^{\\star})=\\tilde{\\mathcal{O}}\\left(d\\sqrt{T}+\\frac{1}{\\kappa}d^{2}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Discussion of Theorem 4. If $T\\geqslant{\\mathcal{O}}(d^{2}/\\kappa^{2})$ , our algorithm achieves a regret of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ when the reward for each item is non-uniform, demonstrating that OFU-MNL $^+$ is minimax optimal up to a logarithmic factor. Recall that we relax the bounded assumption on the reward compared to Zhang and Sugiyama [54] (refer Assumption 1); thus, we allow the sum of the squared rewards in the assortment to scale with $K$ . Consequently, we need a novel approach to achieve the regret that does not scale with $K$ . To this end, we centralize the features, i.e., $\\tilde{x}_{t i}=x_{t i}-\\mathbb{E}_{j\\sim p_{t}(\\cdot|S_{t},\\mathbf{w}_{t+1})}[x_{t j}]$ Ej\u201eptp\u00a8|St,wt\\`1qrxtjs, and propose a novel elliptical potential lemma for them, as detailed in Lemma H.3. Note that our algorithm is capable of achieving $1/\\kappa$ -free regret (in the leading term) under both uniform and non-uniform rewards. In contrast, the algorithm in Perivier and Goyal [45] is limited to achieving this only in the uniform reward setting. Furthermore, compared to the regret bound in Chen et al. [16], which is $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ , our regret bounds has the same order of regret with theirs. However, their algorithm is computationally intractable as it requires enumerating all possible assortments, whereas our algorithm incurs only a constant computational cost per round. The proof is deferred to Appendix $\\mathrm{H}$ . ", "page_idx": 8}, {"type": "text", "text": "6 Instance-Dependent Bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we show that instance-dependent upper and lower bounds are also achievable under uniform rewards. We define the degree of non-linearity for the optimal assortment $S_{t}^{\\star}$ at round $t$ under the true parameter $\\mathbf{w}^{\\star}$ as $\\begin{array}{r}{\\kappa_{t}^{\\bar{\\star}}:=\\,\\sum_{i\\in S_{t}^{\\star}}p_{t}(i|S_{t}^{\\star},\\mathbf{w}^{\\star})p_{t}(0|\\bar{S}_{t}^{\\star},\\mathbf{w}^{\\star})}\\end{array}$ . We first establish the instance-dependent lower bound under un iform rewards. ", "page_idx": 8}, {"type": "text", "text": "Proposition 1 (Instance-dependent regret lower bound, Uniform rewards). Under the same conditions as Theorem 1, for any policy $\\pi$ and for $T\\geqslant d^{2}/\\kappa$ , there exists a worst-case problem instance such that the worst-case expected regret of $\\pi$ is lower bounded as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbf{w}}\\mathbb{E}_{\\mathbf{w}}^{\\pi}\\left[\\mathbf{Reg}_{T}(\\mathbf{w})\\right]=\\Omega\\left(d\\sqrt{\\sum_{t=1}^{T}\\kappa_{t}^{\\star}}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof is deferred to Appendix I. We also provide the matching upper bound. ", "page_idx": 8}, {"type": "text", "text": "Proposition 2 (Instance-dependent regret upper bound of OFU-MNL $^+$ , Uniform rewards). Under the same assumptions, parameter settings, and reward structure as Theorem 2, with a probability at least $1-\\delta$ , the cumulative regret of OFU- $\\tt M N L+$ is upper-bounded by ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star})=\\tilde{\\mathcal{O}}\\left(d\\sqrt{\\sum_{t=1}^{T}\\kappa_{t}^{\\star}}+\\frac{1}{\\kappa}d^{2}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof is deferred to Appendix J. For sufficiently large $T$ , the regret upper bound (Proposition 2) matches the regret lower bound (Proposition 1), up to logarithmic factor. To the best of our knowledge, these are the first minimax instance-dependent regret bounds under uniform rewards. Note that, in the worst case, $\\kappa_{t}^{\\star}\\,=\\,\\tilde{\\mathcal{O}}(\\sqrt{v_{0}K}/(v_{0}~\\dot{+}~K))$ , which indicates that these results provide a strict improvement over the worst-case bounds given in Theorems 1 and 2. ", "page_idx": 8}, {"type": "text", "text": "Some readers may expect instance-dependent regret bounds for non-uniform rewards as well. Unfortunately, we were unable to establish these. Recall that $\\boldsymbol{\\kappa}_{t}^{\\star}$ represents the degree of non-linearity for the optimal assortment $S_{t}^{\\star}$ . However, in the proofs for regret bounds, we encounter terms associated with the chosen assortment $S_{t}$ , such as $\\begin{array}{r}{\\sum_{i\\in S_{t}^{\\star}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})p_{t}(0|S_{t},\\mathbf{w}^{\\star})}\\end{array}$ . To address this, we use the mean value theorem-based analysis (Le mma I.5) to replace this quantity with $\\boldsymbol{\\kappa}_{t}^{\\star}$ . Under non-uniform rewards, however, the mean value theorem does not apply because the sizes and rewards of $S_{t}^{\\star}$ and $S_{t}$ may differ. Addressing this problem would be an interesting direction for future research. ", "page_idx": 8}, {"type": "image", "img_path": "Q4NWfStqVf/tmp/5a4bef96dab48d191e943ce922fd3d375f5a47cf75f2d3d707d9ea1eff3600d7.jpg", "img_caption": ["Figure 1: Cumulative regret (left three, $K\\;=\\;5,10,15)$ ) and runtime per round (rightmost one, $K=15$ ) under uniform rewards (first row) and non-uniform rewards (second row) with $v_{0}=1$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "7 Numerical Experiments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we empirically evaluate the performance of our algorithm ${0}\\mathrm{FU-MNL+}$ . We measure cumulative regret over $T=3000$ rounds. For each experimental setup, we run the algorithms across 20 independent instances and report the average performance. In each instance, the underlying parameter $\\mathbf{w}^{\\star}$ is randomly sampled from a $d_{\\cdot}$ -dimensional uniform distribution, where each element of $\\mathbf{w}^{\\star}$ lies within the range $[-1/\\Bar{\\sqrt{d}},1/\\sqrt{d}]$ and is not known to the algorithms. Additionally, the context features $x_{t i}$ are drawn from a $d$ -d?imensi?onal multivariate Gaussian distribution, with each element of $x_{t i}$ clipped to the range $[-1/\\sqrt{d},1/\\sqrt{d}]$ . This setup ensures compliance with Assumption 1. In the uniform reward setting (first row of Figure 1), the combinatorial optimization step to choose the assortment reduces to sorting items by their utility estimate. In the non-uniform reward setting (second row of Figure 1), the rewards are sampled from a uniform distribution in each round, i.e., $r_{t i}\\sim\\mathrm{Unif}(0,1)$ . Refer Appendix $\\mathbf{K}$ for more details. ", "page_idx": 9}, {"type": "text", "text": "We compare the performance of ${0}{\\mathrm{FU-MNL+}}$ with those of the practical and state-of-the-art algorithms: the Upper Confidence Bound-based algorithm, UCB-MNL [41], and the Thompson Sampling-based algorithm, TS-MNL [41]. Figure 1 demonstrates that our algorithm significantly outperforms other baseline algorithms. In the uniform reward setting, as $K$ increases, the cumulative regrets of all algorithms tend to decrease. In contrast, this trend is not observed in the non-uniform reward setting. Furthermore, the results also show that our algorithm maintains a constant computation cost per round, while the other algorithms exhibit a linear dependence on $t$ . In Appendix K, we present the additional runtime curves (Figure K.1) as well as the regret curves of the other configuration where $v_{0}=\\Theta(K)$ (Figure K.2). All of these empirical results align with our theoretical results. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose minimax optimal lower and upper bounds for both uniform and non-uniform reward settings. We propose a computationally efficient algorithm, ${0}{\\mathrm{FU-MNL+}}$ , that achieves a regret of $\\tilde{\\mathcal{O}}(d\\sqrt{T/\\bar{K}})$ under uniform rewards and $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ under non-uniform rewards. We also prove matching lower bounds of $\\Omega(d{\\sqrt{T/K}})$ and $\\Omega(d{\\sqrt{T}})$ for each setting, respectively. Moreover, our empirical results support our theoretical findings, demonstrating that OFU-MNL $^+$ is not only provably but also experimentally efficient. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2022R1C1C1006859, 2022R1A4A1030579, and RS-2023-00222663) and by AI-Bio Research Grant through Seoul National University. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24:2312\u20132320, 2011.   \n[2] Naoki Abe and Philip M Long. Associative reinforcement learning using linear probabilistic concepts. In ICML, pages 3\u201311. Citeseer, 1999.   \n[3] Marc Abeille and Alessandro Lazaric. Linear Thompson Sampling Revisited. In Aarti Singh and Jerry Zhu, editors, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 176\u2013184. PMLR, 20\u201322 Apr 2017.   \n[4] Marc Abeille, Louis Faury, and Cl\u00e9ment Calauz\u00e8nes. Instance-wise minimax-optimal algorithms for logistic bandits. In International Conference on Artificial Intelligence and Statistics, pages 3691\u20133699. PMLR, 2021.   \n[5] Priyank Agrawal, Theja Tulabandhula, and Vashist Avadhanula. A tractable online learning algorithm for the multinomial logit contextual bandit. European Journal of Operational Research, 310(2):737\u2013750, 2023.   \n[6] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In International Conference on Machine Learning, pages 127\u2013135. PMLR, 2013.   \n[7] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Thompson sampling for the mnl-bandit. In Conference on learning theory, pages 76\u201378. PMLR, 2017.   \n[8] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453\u20131485, 2019.   \n[9] Sanae Amani and Christos Thrampoulidis. Ucb-based algorithms for multinomial logistic regression bandits. Advances in Neural Information Processing Systems, 34:2913\u20132924, 2021.   \n[10] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002.   \n[11] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235\u2013256, 2002.   \n[12] Nicolo Campolongo and Francesco Orabona. Temporal variability in implicit online learning. Advances in neural information processing systems, 33:12377\u201312387, 2020.   \n[13] Junyu Cao and Wei Sun. Tiered assortment: Optimization and online learning. Management Science, 70(8):5481\u20135501, 2024.   \n[14] Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and applications. In International conference on machine learning, pages 151\u2013159. PMLR, 2013.   \n[15] Xi Chen and Yining Wang. A note on a tight lower bound for capacitated mnl-bandit assortment selection models. Operations Research Letters, 46(5):534\u2013537, 2018.   \n[16] Xi Chen, Yining Wang, and Yuan Zhou. Dynamic assortment optimization with changing contextual information. The Journal of Machine Learning Research, 21(1):8918\u20138961, 2020.   \n[17] Xi Chen, Yining Wang, and Yuan Zhou. Optimal policy for dynamic assortment planning under multinomial logit models. Mathematics of Operations Research, 46(4):1639\u20131657, 2021.   \n[18] Wang Chi Cheung and David Simchi-Levi. Thompson sampling for online personalized assortment optimization problems with multinomial logit choice models. Available at SSRN 3075658, 2017.   \n[19] Hyun-jun Choi, Rajan Udwani, and Min-hwan Oh. Cascading contextual assortment bandits. Advances in Neural Information Processing Systems, 36, 2024.   \n[20] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208\u2013214. JMLR Workshop and Conference Proceedings, 2011.   \n[21] James M Davis, Guillermo Gallego, and Huseyin Topaloglu. Assortment optimization under variants of the nested logit model. Operations Research, 62(2):250\u2013273, 2014.   \n[22] Louis Faury, Marc Abeille, Cl\u00e9ment Calauz\u00e8nes, and Olivier Fercoq. Improved optimistic algorithms for logistic bandits. In International Conference on Machine Learning, pages 3052\u20133060. PMLR, 2020.   \n[23] Louis Faury, Marc Abeille, Kwang-Sung Jun, and Cl\u00e9ment Calauz\u00e8nes. Jointly efficient and optimal algorithms for logistic bandits. In International Conference on Artificial Intelligence and Statistics, pages 546\u2013580. PMLR, 2022.   \n[24] Sarah Filippi, Olivier Capp\u00e9, Aur\u00e9lien Garivier, and Csaba Szepesv\u00e1ri. Parametric bandits: The generalized linear case. In Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201910, page 586\u2013594, Red Hook, NY, USA, 2010. Curran Associates Inc.   \n[25] Dylan J Foster, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Logistic regression: The importance of being improper. In Conference on learning theory, pages 167\u2013208. PMLR, 2018.   \n[26] Vineet Goyal and Noemie Perivier. Dynamic pricing and assortment under a contextual mnl demand. arXiv preprint arXiv:2110.10018, 2021.   \n[27] Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends\u00ae in Optimization, 2(3-4):157\u2013325, 2016.   \n[28] Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, and Rebecca Willett. Scalable generalized linear bandits: Online computation and hashing. Advances in Neural Information Processing Systems, 30, 2017.   \n[29] Abbas Kazerouni and Lawrence M Wein. Best arm identification in generalized linear bandits. Operations Research Letters, 49(3):365\u2013371, 2021.   \n[30] Wonyoung Kim, Kyungbok Lee, and Myunghee Cho Paik. Double doubly robust thompson sampling for generalized linear contextual bandits. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 8300\u20138307, 2023.   \n[31] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In Artificial Intelligence and Statistics, pages 535\u2013543. PMLR, 2015.   \n[32] Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and Craig Boutilier. Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics, pages 2066\u20132076. PMLR, 2020.   \n[33] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n[34] Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. Improved regret bounds of (multinomial) logistic bandits via regret-to-confidence-set conversion. In International Conference on Artificial Intelligence and Statistics, pages 4474\u20134482. PMLR, 2024.   \n[35] Junghyun Lee, Se-Young Yun, and Kwang-Sung Jun. A unified confidence sequence for generalized linear models, with applications to bandits. arXiv preprint arXiv:2407.13977, 2024.   \n[36] Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In International Conference on Machine Learning, pages 2071\u20132080. PMLR, 2017.   \n[37] Xutong Liu, Jinhang Zuo, Siwei Wang, John CS Lui, Mohammad Hajiesmaili, Adam Wierman, and Wei Chen. Contextual combinatorial bandits with probabilistically triggered arms. In International Conference on Machine Learning, pages 22559\u201322593. PMLR, 2023.   \n[38] Daniel McFadden. Modelling the choice of residential location. 1977.   \n[39] Zakaria Mhammedi, Wouter M Koolen, and Tim Van Erven. Lipschitz adaptivity with multiple learning rates in online learning. In Conference on Learning Theory, pages 2490\u20132511. PMLR, 2019.   \n[40] Sentao Miao and Xiuli Chao. Dynamic joint assortment and pricing optimization with demand learning. Manufacturing & Service Operations Management, 23(2):525\u2013545, 2021.   \n[41] Min-hwan Oh and Garud Iyengar. Thompson sampling for multinomial logit contextual bandits. Advances in Neural Information Processing Systems, 32, 2019.   \n[42] Min-hwan Oh and Garud Iyengar. Multinomial logit contextual bandits: Provable optimality and practicality. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9205\u20139213, 2021.   \n[43] Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213, 2019.   \n[44] Mingdong Ou, Nan Li, Shenghuo Zhu, and Rong Jin. Multinomial logit bandit with linear utility functions. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 2602\u20132608. International Joint Conferences on Artificial Intelligence Organization, 2018.   \n[45] Noemie Perivier and Vineet Goyal. Dynamic pricing and assortment under a contextual mnl demand. Advances in Neural Information Processing Systems, 35:3461\u20133474, 2022.   \n[46] Lijing Qin, Shouyuan Chen, and Xiaoyan Zhu. Contextual combinatorial bandit and its application on diversified online recommendation. In Proceedings of the 2014 SIAM International Conference on Data Mining, pages 461\u2013469. SIAM, 2014.   \n[47] Idan Rejwan and Yishay Mansour. Top- $k$ combinatorial bandits with full-bandit feedback. In Aryeh Kontorovich and Gergely Neu, editors, Proceedings of the 31st International Conference on Algorithmic Learning Theory, volume 117 of Proceedings of Machine Learning Research, pages 752\u2013776. PMLR, 08 Feb\u201311 Feb 2020.   \n[48] Paat Rusmevichientong, Zuo-Jun Max Shen, and David B Shmoys. Dynamic assortment optimization with a multinomial logit choice model and capacity constraint. Operations research, 58(6):1666\u20131680, 2010.   \n[49] Denis Saur\u00e9 and Assaf Zeevi. Optimal dynamic assortment planning with demand learning. Manufacturing & Service Operations Management, 15(3):387\u2013404, 2013.   \n[50] Quoc Tran-Dinh, Yen-Huan Li, and Volkan Cevher. Composite convex minimization involving self-concordant-like cost functions. In Modelling, Computation and Optimization in Information Systems and Management Sciences: Proceedings of the 3rd International Conference on Modelling, Computation and Optimization in Information Systems and Management SciencesMCO 2015-Part I, pages 155\u2013168. Springer, 2015.   \n[51] Alexandre B Tsybakov. Introduction to Nonparametric Estimation. Springer Science & Business Media, 2008. doi: https://doi.org/10.1007/b13794.   \n[52] Lijun Zhang, Tianbao Yang, Rong Jin, Yichi Xiao, and Zhi-Hua Zhou. Online stochastic linear optimization under one-bit feedback. In International Conference on Machine Learning, pages 392\u2013401. PMLR, 2016.   \n[53] Mengxiao Zhang and Haipeng Luo. Contextual multinomial logit bandits with general value functions. arXiv preprint arXiv:2402.08126, 2024.   \n[54] Yu-Jie Zhang and Masashi Sugiyama. Online (multinomial) logistic bandit: Improved regret and constant computation cost. Advances in Neural Information Processing Systems, 36, 2024.   \n[55] Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, and Branislav Kveton. Cascading bandits for large-scale recommendation problems. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, page 835\u2013844. AUAI Press, 2016. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Further Related Work 16 ", "page_idx": 14}, {"type": "text", "text": "B Notation 17 ", "page_idx": 14}, {"type": "text", "text": "C Properties of MNL function 17 ", "page_idx": 14}, {"type": "text", "text": "C.1 Attraction parameter for Outside Option: $v_{0}=\\Theta(1)$ is Common in Contextual MNL Bandits . 18 C.2 Self-concordant-like Function . 18 ", "page_idx": 14}, {"type": "text", "text": "D Proof of Theorem 1 19 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Adversarial Construction and Bayes Risk 19   \nD.2 Main Proof of Theorem 1 . 20   \nD.3 Proofs of Lemmas for Theorem 1 22 ", "page_idx": 14}, {"type": "text", "text": "E Proof of Theorem 2 24 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Main Proof of Theorem 2 24   \nE.2 Proofs of Lemmas for Theorem 2 30 ", "page_idx": 14}, {"type": "text", "text": "F Proof of Lemma 1 32 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Main Proof of Lemma 1 32   \nF.2 Proofs of Lemmas for Lemma 1 34   \nF.3 Technical Lemmas for Lemma 1 39 ", "page_idx": 14}, {"type": "text", "text": "G Proofs of Theorem 3 40 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "G.1 Adversarial Rewards Construction . 40   \nG.2 Main Proof of Theorem 3 . 40   \nG.3 Proofs of Lemmas for Theorem 3 41 ", "page_idx": 14}, {"type": "text", "text": "H Proofs of Theorem 4 42 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "H.1 Proof of Theorem 4 43   \nH.2 Proofs of Lemmas for Theorem 4 47 ", "page_idx": 14}, {"type": "text", "text": "I Proof of Proposition 1 49 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "I.1 Main Proof of Proposition 1 . 49   \nI.2 Proofs of Lemmas for Proposition 1 . . 51   \nI.3 Technical Lemmas for Proposition 1 52 ", "page_idx": 14}, {"type": "text", "text": "J Proof of Proposition 2 52 ", "page_idx": 14}, {"type": "text", "text": "K Experiment Details and Additional Results 55 ", "page_idx": 14}, {"type": "text", "text": "L Technical Errors in Agrawal et al. [5] 56 ", "page_idx": 14}, {"type": "text", "text": "M Limitations 56 ", "page_idx": 14}, {"type": "text", "text": "A Further Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we discuss additional related works that complement Section 2. For simplicity, we consider only the dependence on the number of rounds $t$ for a computation cost in big-O notation. ", "page_idx": 15}, {"type": "text", "text": "Logistic Bandits. The logistic bandit model [24, 22, 4, 23] focuses on environments with binary rewards and explores the impact of non-linearity on the exploration-exploitation trade-off for parametrized bandits. The main research interest has been the algorithms\u2019 dependence on the degree of non-linearity $\\kappa$ , which can grow exponentially in terms of the diameter of the decision domain $\\mathcal{W}$ . Zhang et al. [52] introduced the first efficient algorithm for binary logistic bandits with a $\\mathcal{O}(1)$ ? computation cost, achieving a regret of $\\tilde{\\mathcal{O}}(d\\sqrt{T}/\\kappa)$ . Faury et al. [22] enhanced the regret to $\\tilde{\\mathcal{O}}(d\\sqrt{T/\\kappa})$ with a $\\mathcal{O}(t)$ computation cost. However, their regret bounds still suffered from a harmful depen?dence on $1/\\kappa$ . Abeille et al. [4] addressed this by achieving the tightest regret upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{\\kappa T})$ with a $\\mathcal{O}(t)$ computation cost, while Faury et al. [23] achieved the same regret with an improved computation cost of ${\\mathcal{O}}(\\log t)$ . More recently, Zhang and Sugiyama [54] proposed a jointly efficient algorithm that achieves the optimal regret with a constant $\\mathcal{O}(1)$ computation cost. Note that the logistic bandit is a special case of the multinomial logistic (MNL) bandit. When the maximum assortment size is one $[K=1]$ ), rewards are uniform $(r_{t1}=1)$ ), and the attraction parameter for the outside option is one $v_{0}=1$ , the MNL bandit reduces to the logistic bandit. In this logistic bandit setting, our proposed algorithm, ${0}{\\mathrm{FU-MNL+}}$ , can achieve a regret upper bound of $\\bar{\\mathcal{O}}(d\\bar{\\sqrt{\\kappa T}})$ with a constant $\\mathcal{O}(1)$ computation cost, consistent with the result in Zhang and Sugiyama [54]. ", "page_idx": 15}, {"type": "text", "text": "Multinomial Logistic (MNL) Bandits. There are two main approaches to multinomial logistic (MNL) bandits: the multiple-parameter choice model and the single-parameter choice model. In the multiple-parameter choice model, the learner estimates parameters for each choice in the assortment $(\\mathbf{w}_{1}^{\\star},\\dots,\\mathbf{w}_{K}^{\\star})$ with a shared context $x_{t}$ . In this setting, Amani and Thrampoulidis [9] proposed a feasible algorithm that achieves a regret upper bound of $\\tilde{\\mathcal{O}}(d K\\sqrt{\\kappa T})$ with a $\\mathcal{O}(t)$ computation cost. They also proposed an intractable algorithm that achieves an improved regret of $\\tilde{\\mathcal{O}}(d K^{3/2}\\sqrt{T})$ . Zhang and Sugiyama [54]? introduced a computationally and statistically efficient algorithm that obtains a regret of $\\tilde{\\mathcal{O}}(d K\\sqrt{T})$ . Recently, Lee et al. [34] further improved the regret by a factor of $\\sqrt{K}$ , achieving $\\tilde{\\mathcal{O}}(d\\sqrt{K T})$ regret. In the multiple-parameter case, the regret\u2019s dependence on $K$ is unavoidable since the number of unknown parameters depends on $K$ . ", "page_idx": 15}, {"type": "text", "text": "On the other hand, the single-parameter choice model, closely related to ours, shares the parameter $\\mathbf{w}^{\\star}$ cross the choices, with varying contexts for each choice. The learner offers a set of items $S_{t}$ , with $|S_{t}|\\,\\leqslant\\,K$ at each round. This setting involves a combinatorial optimization to choose the assortment $S_{t}$ , making it more challenging to devise a tractable algorithm. As extensively discussed in Section 2, no previous studies have definitively confirmed whether the existing lower or upper bounds are tight. As shown in Table 1, many studies have presented their results in inconsistent settings with varying reward structures and values of $v_{0}$ , adding to the ambiguity about the bounds\u2019 optimality. In this paper, we address these issues by bridging the gap between the lower and upper bounds of regret through a careful categorization of the settings. We propose an algorithm that is both provably optimal, up to logarithmic factors, and computationally efficient, significantly enhancing the theoretical and practical understanding of MNL bandits. ", "page_idx": 15}, {"type": "text", "text": "Generalized Linear Bandits. In generalized linear bandits [24, 28, 36, 3, 32, 29, 30, 35], the expected rewards are modeled using a generalized linear model. These problems generalize logistic bandits by incorporating a general exponential family link function instead of the logistic link function. The algorithms proposed for generalized linear bandits also exhibit a dependence on the nonlinear term $\\kappa$ . However, our problem setting (single-parameter MNL bandits) considers a more complex state space where multiple arms are pulled simultaneously. ", "page_idx": 15}, {"type": "text", "text": "Combinatorial Bandits. Another related stream of literature is combinatorial bandits [14, 46, 31, 55, 47, 37], particularly top- $k$ combinatorial bandits [47]. In top- $k$ combinatorial bandits, the decision set includes all subsets of size $k$ out of $n$ arms, and the reward for each action is the sum of the rewards of the $k$ selected arms. In this framework, the rewards are assumed to be independent of the entire set of arms played in round $t$ . In contrast, in our setting, the reward for each individual arm depends on the whole set of arms played. ", "page_idx": 15}, {"type": "text", "text": "Recently, Choi et al. [19], Cao and Sun [13] have considered the cascading assortment bandit problem, which encompasses the MNL bandit problem as a special case where the cascading length is 1. However, these works do not strictly encompass our results. Choi et al. [19] only consider uniform rewards, and achieve a regret upper bound of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ , which avoids dependence on both the cascading length and $\\kappa$ . When the cascading length is 1, our result (Theorem 2) improves upon theirs by a factor of $\\sqrt{K}$ . Moreover, their computation cost per round is $\\mathcal{O}(t)$ since they employ MLE to estimate the param?eter. Cao and Sun [13] consider non-uniform rewards, and achieve a regret upper bound of $\\tilde{\\mathcal{O}}(h^{2}d\\sqrt{M T})$ , where $M$ is the cascading length and $\\begin{array}{r}{h\\,\\geqslant\\,\\frac{p(i|S,\\mathbf{w})}{p(i|S,\\mathbf{w}^{\\prime})}}\\end{array}$ for all $\\mathbf{w},\\mathbf{w}^{\\prime}\\in\\mathcal{W}$ , $S\\in S$ , and $i\\in S\\cup\\{0\\}$ . However, their regret bound still suffers from a harmful dependence on $h^{2}$ , which can be exponentially large. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "B Notation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We denote $T$ as the total number of rounds and $t\\in[T]$ as the current round. We denote $N$ as the total number of items, $K$ as the maximum size of assortments, and $d$ as the dimension of feature vectors. For notational simplicity, we define the loss function in two different forms throughout the proof: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\ell_{t}({\\bf w})=-\\sum_{i\\in S_{t}}y_{t i}\\log p_{t}(i|S_{t},{\\bf w})=-\\sum_{i\\in S_{t}}y_{t i}\\log\\left(\\frac{\\exp(x_{t i}^{\\top}{\\bf w})}{v_{0}+\\sum_{j\\in S_{t}}\\exp(x_{t j}^{\\top}{\\bf w})}\\right),}}\\\\ {{\\displaystyle\\ell({\\bf z},{\\bf y}_{t})=-\\sum_{i\\in S_{t}}y_{t i}\\log\\left(\\frac{\\exp(z_{t i})}{v_{0}+\\sum_{j\\in S_{t}}\\exp(z_{t j})}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $z_{t i}=x_{t i}^{\\top}\\mathbf{w}$ , $\\mathbf{z}_{t}=(z_{t i})_{i\\in S_{t}}\\in\\mathbb{R}^{|S_{t}|}$ , and $\\mathbf{y}_{t}=(y_{t i})_{i\\in S_{t}}\\in\\mathbb{R}^{|S_{t}|}$ . Thus, $\\ell_{t}(\\mathbf{w})=\\ell(\\mathbf{z}_{t},\\mathbf{y}_{t})$ .   \nWe offer a Table B.1 for convenient reference. ", "page_idx": 16}, {"type": "table", "img_path": "Q4NWfStqVf/tmp/5adc6fb9c604a73c7865c276ca9a96eb5b1037c9374df9b781c33385ccc35cc3.jpg", "table_caption": ["Table B.1: Symbols "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Properties of MNL function ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present key properties of the MNL function and its associated loss, which are used throughout the paper. ", "page_idx": 16}, {"type": "text", "text": "C.1 Attraction parameter for Outside Option: $v_{0}=\\Theta(1)$ is Common in Contextual MNL Bandits ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this subsection, we explain why the assumption that $v_{0}=\\Theta(1)$ is made without loss of generality. Let the original feature vectors be $x_{t i}^{\\prime}\\in\\mathbb{R}^{d}$ for every item $i\\in[N]$ . Suppose that a context for the outside option $x_{t0}^{\\prime}$ is given and the probability of choosing any item $i\\in S_{t}\\cup\\{0\\}$ is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\np_{t}(i|S_{t},\\mathbf{w}^{\\star})=\\frac{\\exp((x_{t i}^{\\prime})^{\\top}\\mathbf{w}^{\\star})}{\\sum_{j\\in S_{t}\\cup\\{0\\}}\\exp((x_{t j}^{\\prime})^{\\top}\\mathbf{w}^{\\star})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, by dividing the denominator and numerator by $\\exp\\left((x_{t0}^{\\prime})^{\\top}\\mathbf{w}^{\\star}\\right)$ , and defining $x_{t i}:=x_{t i}^{\\prime}-x_{t0}^{\\prime}$ , we obtain the MNL probability in the form presented in\\` (1) with $v_{0}=\\exp(0)=1$ . Note that this division does not change the probability. Therefore, $v_{0}=\\Theta(1)$ is natural and common in contextual MNL bandit literature. ", "page_idx": 17}, {"type": "text", "text": "C.2 Self-concordant-like Function ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Definition C.1 (Self-concordant-like function, Tran-Dinh et al. 50). A convex function $f\\in\\mathcal{C}^{3}(\\mathbb{R}^{m})$ is $M$ -self-concordant-like function with constant $M$ if: ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\phi^{\\prime\\prime\\prime}(s)|\\leqslant M\\|\\mathbf{b}\\|_{2}\\phi^{\\prime\\prime}(s).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $s\\in\\mathbb{R}$ and $M>0,$ , where $\\phi(s):=f(\\mathbf{a}+s\\mathbf{b})$ for any a, $\\mathbf{b}\\in\\mathbb{R}^{m}$ . ", "page_idx": 17}, {"type": "text", "text": "Then, the MNL loss defined in (2) is $3\\sqrt{2}$ -self-concordant-like function. ", "page_idx": 17}, {"type": "text", "text": "Proposition C.1. For any $t\\in[T]$ , the multinomial logistic loss $\\ell_{t}(\\mathbf{w})$ , defined in (2), is $3\\sqrt{2}$ -selfconcordant-like. ", "page_idx": 17}, {"type": "text", "text": "Proof. Consider the function $\\begin{array}{r}{\\phi(s):=\\log\\left(\\sum_{i=0}^{n}e^{a_{i}s+b_{i}}\\right)}\\end{array}$ , where $\\mathbf{a}=[a_{0},\\ldots,a_{n}]^{\\top}\\in\\mathbb{R}^{n+1}$ and $\\mathbf{b}=[b_{0},\\ldots,b_{n}]^{\\top}\\in\\mathbb{R}^{n+1}$ . Then, by simpl\\`e \u0159calculus, we \u02d8have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi^{\\prime\\prime}(s)=\\frac{\\sum_{i<j}(a_{i}-a_{j})^{2}e^{a_{i}s+b_{i}}e^{a_{j}s+b_{j}}}{\\left(\\sum_{i=0}^{n}e^{a_{i}s+b_{i}}\\right)^{2}}\\geqslant0,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi^{\\prime\\prime\\prime}(s)=\\frac{\\sum_{i<j}(a_{i}-a_{j})^{2}e^{a_{i}s+b_{i}}e^{a_{j}s+b_{j}}\\left[\\sum_{k=0}^{n}(a_{i}+a_{j}-2a_{k})e^{a_{k}s+b_{k}}\\right]}{\\left(\\sum_{i=0}^{n}e^{a_{i}s+b_{i}}\\right)^{3}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that for all $i,j,k=0,\\dots,n$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n|a_{i}+a_{j}-2a_{k}|\\leqslant{\\sqrt{6}}{\\sqrt{a_{i}^{2}+a_{j}^{2}+a_{k}^{2}}}\\leqslant3{\\sqrt{2}}\\operatorname*{max}_{i=0,\\ldots,n}|a_{i}|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\sum_{k=0}^{n}(a_{i}+a_{j}-2a_{k})e^{a_{k}s+b_{k}}\\right|\\leqslant\\sum_{k=0}^{n}|a_{i}+a_{j}-2a_{k}|\\,e^{a_{k}s+b_{k}}\\leqslant3{\\sqrt{2}}\\operatorname*{max}_{i=0,\\ldots,n}|a_{i}|\\sum_{k=0}^{n}e^{a_{k}s+b_{k}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging in (C.3) into (C.1), we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi^{\\prime\\prime\\prime}(s)\\leqslant3\\sqrt{2}\\operatorname*{max}_{i=0,\\ldots,n}|a_{i}|\\phi^{\\prime\\prime}(s).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, we are ready to prove the proposition. For any $t\\,\\in\\,[T]$ , let $n\\,=\\,|S_{t}|$ and $c_{1}\\,=\\,x_{t i_{1}},c_{2}\\,=$ $x_{t i_{2}},\\ldots,c_{n}\\,=\\,x_{t i_{n}}$ . Define a function $f\\in\\mathcal{C}^{3}:\\mathbb{R}^{d}\\rightarrow\\ \\mathbb{R}$ as $\\begin{array}{r}{f(\\pmb\\theta):=\\log\\left(v_{0}+\\sum_{i=1}^{n}e^{c_{i}^{\\top}\\pmb\\theta}\\right)}\\end{array}$ . Let $\\pmb{\\delta}\\in\\mathbb{R}^{d}$ and let $\\begin{array}{r}{f(\\pmb\\theta+s\\pmb\\delta)\\,=\\,\\log\\left(v_{0}+\\sum_{i=1}^{n}e^{c_{i}^{\\top}\\pmb\\theta+s c_{i}^{\\top}\\pmb\\delta}\\right)\\,=\\,\\log\\left(\\sum_{i=0}^{n}e^{a_{i}s+b_{i}}\\right)\\,=\\,\\phi(s),}\\end{array}$ where $a_{i}=c_{i}^{\\top}\\delta,b_{i}=c_{i}^{\\top}\\theta$ for $i=1,\\hdots,n$ , and $a_{i}=0$ and $b_{i}=\\log{v_{0}}$ for $i=0$ . Then, by (C.4), we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{|\\phi^{\\prime\\prime\\prime}(s)|\\leqslant3\\sqrt{2}\\displaystyle\\operatorname*{max}_{i=0,\\ldots,n}_{i=0,\\ldots,n}|a_{i}|\\phi^{\\prime\\prime}(s)=3\\sqrt{2}\\displaystyle\\operatorname*{max}_{i=1,\\ldots,n}|c_{i}^{\\top}\\delta|\\phi^{\\prime\\prime}(s)}\\\\ &{}&{\\leqslant3\\sqrt{2}\\displaystyle\\operatorname*{max}_{i=1,\\ldots,n}\\|c_{i}\\|_{2}\\|\\delta\\|_{2}\\phi^{\\prime\\prime}(s)\\leqslant3\\sqrt{2}\\|\\delta\\|_{2}\\phi^{\\prime\\prime}(s),\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last in?equality holds due to Assumption 1 that $\\|c_{i}\\|_{2}\\,=\\,\\|x_{t j_{i}}\\|_{2}\\,\\leqslant\\,1$ . Then, by Definition C.1, $f$ is $3\\sqrt{2}$ -self-concordant-like. Since $\\ell_{t}$ is the sum of $f$ and a linear operator, which has third derivatives equal to zero, it follows that $\\ell_{t}$ is also $3{\\sqrt{2}}$ -self-concordant-like function. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Remark C.1. Contrary to the findings of Perivier and Goyal [45], which suggest that the MNL loss function $\\sqrt{6K}$ -self-concordant-like, our loss function is $3\\sqrt{2}$ -self-concordant-like. This yields an improved regret bound on the order of ${\\mathcal{O}}({\\sqrt{K}})$ . The improvement arises due to a $K$ -independent self-concordant-like property of $\\ell_{t}$ , as shown in Proposition C.1. In Perivier and Goyal [45], Lemma 4 from Tran-Dinh et al. [50] is used, which describes $a\\ {\\sqrt{6}}\\|\\mathbf{a}\\|_{2}$ self-concordant-like property. However, in the analysis of C.2, we show that their analysis is not tight because they bound the term $\\sqrt{a_{i}^{2}+a_{j}^{2}+a_{k}^{2}}$ by $\\|\\mathbf{a}\\|_{2}\\;=\\;{\\sqrt{\\textstyle\\sum_{i=0}^{n}a_{i}^{2}}}$ , thus making its upper bound dependent on $K$ , i.e., $n=|S_{t}|\\leqslant K$ . In contrast, we bouand \u0159t?he same term by a constant, $\\operatorname*{max}_{i=1,...,n}\\|a_{i}\\|_{2}$ , which allows our loss ?function to exhibit a constant $3\\sqrt{2}$ -self-concordant-like property. This key difference accounts for the $\\sqrt{K}$ -improved regret. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.1 (Theorem 3 of Tran-Dinh et al. 50). A convex function $\\ell\\in\\mathcal{C}^{3}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is $M$ -selfconcordant-like if and only if for any $\\mathbf{v},\\mathbf{u}_{1},\\mathbf{u}_{2},\\mathbf{u}_{3}\\in\\mathbb{R}^{d}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\langle D^{3}\\ell({\\mathbf v})[{\\mathbf u}_{1}]{\\mathbf u}_{2},{\\mathbf u}_{3}\\rangle|\\leqslant M\\|{\\mathbf u}_{1}\\|_{2}\\|{\\mathbf u}_{2}\\|_{\\nabla^{2}\\ell({\\mathbf v})}\\|{\\mathbf u}_{3}\\|_{\\nabla^{2}\\ell({\\mathbf v})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D Proof of Theorem 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide the proof of Theorem 1. The proof structure is similar to the one presented in Chen et al. [16]. However, unlike their approach, we explicitly derive a bound that includes $v_{0}$ . Furthermore, by establishing a tighter upper bound for the KL divergence (Lemma D.2), we derive a bound that is tighter than the one provided by Chen et al. [16]. ", "page_idx": 18}, {"type": "text", "text": "D.1 Adversarial Construction and Bayes Risk ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let $\\epsilon\\in(0,1/d\\sqrt{d})$ be a small positive parameter to be specified later. For every subset $V\\subseteq[d]$ , we define the corresponding parameter $\\mathbf{w}_{V}\\in\\mathbb{R}^{d}$ as $[\\mathbf{w}_{V}]_{j}\\,=\\,\\epsilon\\,$ for all $j\\in V$ , and $[\\mathbf{w}_{V}]_{j}\\,=\\,0$ for all $j\\not\\in V$ . Then, we consider the following parameter set ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{w}\\in\\mathcal{W}:=\\{\\mathbf{w}_{V}:V\\in\\mathcal{V}_{d/4}\\}:=\\{\\mathbf{w}_{V}:V\\subseteq[d],|V|=d/4\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathcal{V}_{k}$ denotes the class of all subsets of $[d]$ whose size is $k$ . Moreover, note that $d/4$ is a positive integer, as $d$ is divisible by 4 by construction. ", "page_idx": 18}, {"type": "text", "text": "The context vectors $\\{x_{t i}\\}$ are constructed to be invariant across rounds $t$ . For each $t$ and $U\\in\\mathcal{V}_{d/4}$ , $K$ identical context vectors 4 $x_{U}$ are constructed as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n[x_{U}]_{j}=1/\\sqrt{d}\\quad\\mathrm{for}\\ j\\in U;\\quad[x_{U}]_{j}=0\\quad\\mathrm{for}\\ j\\notin U.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For all $V,U\\;\\in\\;\\mathcal{V}_{d/4}$ , it can be verified that $\\mathbf{w}_{V}$ and $x_{U}$ satisfy the requirements of a bounded assumption 1 as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}_{V}\\|_{2}\\leqslant{\\sqrt{d\\epsilon^{2}}}\\leqslant1,\\quad\\|x_{U}\\|_{2}\\leqslant{\\sqrt{d\\cdot1/d}}=1.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the worst-case expected regret of any policy $\\pi$ can be lower bounded by the worst-case expected regret of parameters belonging to $\\mathcal{W}$ , which can be further lower bounded by the \u201caverage\u201d ", "page_idx": 18}, {"type": "text", "text": "regret over a uniform prior over $\\mathcal{W}$ as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{w}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbf{w}}^{\\pi}\\left[\\mathbf{Reg}_{T}(\\mathbf{w})\\right]=\\underset{\\mathbf{w}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbf{w}}^{\\pi}\\sum_{t=1}^{T}R(S^{\\star},\\mathbf{w})-R(S_{t},\\mathbf{w})}\\\\ &{\\phantom{\\sum_{t=1}^{T}\\sum_{i=1}^{T}\\sum_{i=1}^{T}\\sum_{i=1}^{T}R(S^{\\star},\\mathbf{w}_{V})-R(S_{t},\\mathbf{w}_{V})}\\\\ &{\\phantom{\\sum_{t=1}^{T}\\sum_{i=1}^{T}\\sum_{i=1}^{T}\\sum_{i=1}^{T}\\sum_{i=1}^{T}R(S^{\\star},\\mathbf{w}_{V})-R(S_{t},\\mathbf{w}_{V})}\\\\ &{\\phantom{\\sum_{t=1}^{T}\\sum_{i=1}^{T}\\sum_{i=1}^{T}\\sum_{i=1}^{T}\\sum_{t=1}^{T}\\left[\\displaystyle\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\displaystyle\\sum_{i\\in S_{t}}p(i|S_{t},\\mathbf{w}_{V})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This reduces the task of lower bounding the worst-case regret of any policy to the task of lower bounding the Bayes risk of the constructed parameter set. ", "page_idx": 19}, {"type": "text", "text": "D.2 Main Proof of Theorem 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 1. For any sequence of assortments $\\{S_{t}\\}_{t=1}^{T}$ produced by policy $\\pi$ , we denote an alternative sequence $\\{\\tilde{S}_{t}\\}_{t=1}^{T}$ that provably enjoys less regret under parameterization $\\mathbf{w}_{V}$ . ", "page_idx": 19}, {"type": "text", "text": "Let $x_{U_{1}},\\ldots,x_{U_{L}}$ be the distinct feature vectors contained in assortments $S_{t}$ (if $S_{t}=\\mathcal{Q}$ , then one may choose an arbitrary feature $x_{U}$ ) with $U_{1},\\dots,U_{L}\\in\\mathcal{V}_{d/4}$ . Let $U^{\\star}$ be the subset among $U_{1},\\ldots,U_{L}$ that maximizes $x_{U}^{\\top}\\mathbf{w}_{V}$ , i.e., $U^{\\star}\\in\\operatorname*{argmax}_{U\\in\\{U_{1},\\ldots,U_{L}\\}}x_{U}^{\\top}\\mathbf{w}_{V}$ , where $\\mathbf{w}_{V}$ is the underlying parameter. Then, we define ${\\tilde{S}}_{t}$ as the assortment consisting of all $K$ items corresponding to feature $x_{U^{\\star}}$ , i.e., $\\tilde{S}_{t}=\\underbrace{\\{x_{U^{\\star}},\\ldots,x_{U^{\\star}}\\}}_{K}$ . ", "page_idx": 19}, {"type": "text", "text": "Since the expected revenue is an increasing function, we have the following observation: ", "page_idx": 19}, {"type": "text", "text": "Proposition D.1 (Proposition 1 in Chen et al. 16). ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i\\in S_{t}}p(i|S_{t},\\mathbf{w}_{V})\\leqslant\\sum_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proposition D.1 implies that $\\begin{array}{r}{\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})\\,-\\,\\sum_{i\\in S_{t}}p(i|S_{t},\\mathbf{w}_{V})\\;\\geqslant\\;\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})\\;-\\;}\\end{array}$ $\\begin{array}{r}{\\sum_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})}\\end{array}$ . Hence, it i s \u0159sufficient to bound $\\begin{array}{r}{\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})}\\end{array}$ instead o\u0159f $\\begin{array}{r}{\\sum_{i\\in S^{\\star}}p(i|S^{\\star},{\\bf w}_{V})-\\sum_{i\\in S_{t}}p(i|S_{t},{\\bf w}_{V}).}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "To simplify notation, we denote $\\tilde{U}_{t}$ as the unique $U^{\\star}\\in\\mathcal{V}_{d/4}$ in ${\\tilde{S}}_{t}$ . We also use $\\mathbb{E}_{V}$ and $\\mathbb{P}_{V}$ to denote the expected value and probability, respectively, as governed by the law parameterized by $\\mathbf{w}_{V}$ and under policy $\\pi$ . Then, we can establish a lower bound for $\\begin{array}{r}{\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})}\\end{array}$ as follows: ", "page_idx": 19}, {"type": "text", "text": "Lemma D.1. Suppose $\\epsilon\\in(0,1/d\\sqrt{d})$ and define $\\delta:=d/4-|\\tilde{U}_{t}\\cap V|$ . Then, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})\\geqslant\\frac{v_{0}K}{(v_{0}+K e)^{2}}\\cdot\\frac{\\delta\\epsilon}{2\\sqrt{d}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For any $j\\in V$ , define random variables $\\begin{array}{r}{\\tilde{M}_{j}\\;:=\\;\\sum_{t=1}^{T}\\mathbf{1}\\{j\\;\\in\\;\\tilde{U}_{t}\\}}\\end{array}$ . Then, by Lemma D.1, for all $V\\in\\mathcal{V}_{d/4}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{V}\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})\\geqslant\\frac{v_{0}K}{(v_{0}+K e)^{2}}\\cdot\\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{4}-\\sum_{j\\in V}\\mathbb{E}_{V}[\\tilde{M}_{j}]\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, we define $\\mathcal{V}_{d/4}^{(j)}:=\\{V\\in\\mathcal{V}_{d/4}:j\\in V\\}$ and $\\mathcal{V}_{d/4-1}:=\\{V\\subseteq[d]:|V|=d/4-1\\}$ . By taking the average of both sides of Equation (D.2) with respect to all $V\\in\\mathcal{V}_{d/4}$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{|\\nabla\\tilde{\\mu}_{1}|}\\displaystyle\\sum_{i\\in\\mathcal{N}_{\\delta+1}}\\xi_{i}\\nu_{i}\\sum_{s=1}^{p}p_{i}(|\\boldsymbol{\\delta}|^{s},\\mathbf{w}_{i}\\nu)-\\sum_{t}p_{t}(|\\boldsymbol{\\delta}|_{s},\\mathbf{w}_{\\nu}\\nu)}\\\\ &{\\geq\\frac{\\nu_{0}K}{(\\nu_{0}+K)^{2}},\\ \\frac{\\epsilon}{2\\sqrt{d}}\\displaystyle\\frac{1}{|\\nabla\\tilde{\\mu}_{1}|}\\displaystyle\\sum_{|\\nu\\tilde{\\nu}\\tilde{\\nu}\\tilde{\\nu}_{\\tilde{\\nu}\\tilde{\\nu}_{\\tilde{\\nu}\\tilde{\\nu}}}}\\left(\\frac{d T}{4}-\\sum_{j\\nu\\in\\mathcal{N}_{\\delta}}\\mathrm{E}_{\\nu\\tilde{\\nu}}[\\tilde{M}_{j}]\\right)}\\\\ &{=\\frac{\\nu_{0}K}{(\\nu_{0}+K)^{2}},\\ \\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{4}-\\displaystyle\\frac{1}{|\\nabla\\tilde{\\mu}_{4}|}\\displaystyle\\sum_{j\\nu\\in\\mathcal{N}_{\\delta}}\\xi_{i}\\nu_{j}\\right)}\\\\ &{=\\frac{\\nu_{0}K}{(\\nu_{0}+K)^{2}},\\ \\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{4}-\\displaystyle\\frac{1}{|\\nabla\\tilde{\\mu}_{4}|}\\displaystyle\\sum_{|\\nu\\tilde{\\nu}_{\\tilde{\\nu}\\tilde{\\nu}}}\\sum_{i}\\xi_{i}\\nu_{\\nu\\tilde{\\nu}_{\\tilde{\\nu}}j}[\\tilde{M}_{j}]\\right)}\\\\ &{>\\frac{\\nu_{0}K}{(\\nu_{0}+K)^{2}},\\ \\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{4}-\\displaystyle\\frac{|\\tilde{\\nu}_{0}|+1}{|\\nabla\\tilde{\\mu}_{4}|}\\displaystyle\\sum_{|\\nu\\tilde{\\nu}_{0}\\tilde{\\nu}_{\\tilde{\\nu}}}\\xi_{i}\\nu_{\\nu\\tilde{\\nu}_{\\tilde{\\nu}}j}\\right)\\displaystyle\\tilde{M}_{j}]}\\\\ &{=\\frac{\\nu_{0}K}{(\\nu_{0}+K)^{2}},\\ \\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{4}-\\displaystyle\\frac{|\\tilde{\\nu}_{0}|+1}{|\\nabla\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For any fixed $V$ , we get $\\begin{array}{r}{\\sum_{j\\notin V}\\mathbb{E}_{V}[\\tilde{M}_{j}]\\,\\leqslant\\,\\sum_{j=1}^{d}\\mathbb{E}_{V}[\\tilde{M}_{j}]\\,\\leqslant\\,d T/4}\\end{array}$ . Also, we have $\\left|\\frac{\\gamma_{d/4-1}}{\\left|\\mathcal{V}_{d/4}\\right|}\\right.=$ $\\begin{array}{r}{\\binom{d}{d/4-1}/\\binom{d}{d/4}=\\frac{d/4}{3d/4+1}\\leqslant\\frac{1}{3}}\\end{array}$ . Consequently,  we derive that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{1}{\\left|\\mathcal{Y}_{d/4}\\right|}\\sum_{V\\in\\mathcal{V}_{d/4}}\\mathbb{E}_{V}\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\overleftarrow{S}_{t}}p(i|\\Tilde{S}_{t},\\mathbf{w}_{V})}}\\\\ &{\\ge\\frac{v_{0}K}{(v_{0}+K e)^{2}}\\cdot\\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{6}-\\underset{V\\in\\mathcal{V}_{d/4-1}}{\\operatorname*{max}}\\sum_{j\\notin V}\\left|\\mathbb{E}_{V\\setminus\\{j\\}}[\\Tilde{M}_{j}]-\\mathbb{E}_{V}[\\Tilde{M}_{j}]\\right|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we bound the term $\\biggl|\\mathbb{E}_{V\\cup\\{j\\}}[\\tilde{M}_{j}]-\\mathbb{E}_{V}[\\tilde{M}_{j}]\\biggr|$ in (D.3) for any $V\\in\\mathcal{V}_{d/4-1}$ . For simplicity, let $P=\\mathbb{P}_{V}$ and $Q=\\mathbb{P}_{V\\cup\\{j\\}}$ denote the laws under $\\mathbf{w}_{V}$ and $\\mathbf{w}_{V\\cup j}$ , respectively. Then, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\Big|\\mathbb{E}_{P}[\\tilde{M}_{j}]-\\mathbb{E}_{Q}[\\tilde{M}_{j}]\\,|\\leqslant\\sum_{t=0}^{T}t\\cdot\\Big|P[\\tilde{M}_{j}=t]-Q[\\tilde{M}_{j}=t]\\Big|}}\\\\ &{\\leqslant T\\cdot\\sum_{t=0}^{T}\\Big|P[\\tilde{M}_{j}=t]-Q[\\tilde{M}_{j}=t]\\Big|}\\\\ &{\\leqslant T\\cdot\\|P-Q\\|_{\\mathrm{TV}}\\leqslant T\\cdot\\sqrt{\\frac{1}{2}\\operatorname{KL}(P\\|Q)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\|P-Q\\|_{\\mathrm{TV}}\\;=\\;\\operatorname*{sup}_{A}\\left|P(A)-Q(A)\\right|\\,|$ is the total variation distance between $P$ and $Q$ , $\\mathrm{KL}(P\\|Q)=\\int(\\log\\mathrm{d}P/\\mathrm{d}Q)\\mathrm{d}P$ is s the Kullback-Leibler (KL) divergence between $P$ and $Q$ , and the last inequa\u015flity holds by Pinsker\u2019s inequality. Now, we bound the KL divergence term using the following Lemma. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.2. For any $V\\in\\mathcal{V}_{d/4-1}$ and $j\\in[d]$ , there exists a positive constant $C_{\\mathrm{KL}}>0$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{KL}(P_{V}\\|Q_{V\\cup\\{j\\}})\\leqslant C_{\\mathrm{KL}}\\cdot\\frac{v_{0}K}{(v_{0}+K)^{2}}\\cdot\\frac{\\mathbb{E}_{V}[\\tilde{M}_{j}]\\epsilon^{2}}{d}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, combining (D.3), (D.4), and Lemma D.2, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\left|\\mathcal{V}_{d/4}\\right|}\\displaystyle\\sum_{V\\in V_{d/4}}\\mathbb{E}_{V}\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\bar{S}_{t}}p(i|\\Tilde{S}_{t},\\mathbf{w}_{V})}\\\\ &{\\qquad\\geq\\frac{v_{0}K}{(v_{0}+K e)^{2}}\\cdot\\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{6}-T\\sum_{j=1}^{d}\\sqrt{C_{\\mathrm{KL}}\\cdot\\frac{v_{0}K}{(v_{0}+K)^{2}}\\cdot\\frac{\\mathbb{E}_{V}[\\Tilde{M}_{j}]\\epsilon^{2}}{d}}\\right)}\\\\ &{\\qquad\\geq\\frac{v_{0}K}{(v_{0}+K e)^{2}}\\cdot\\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{6}-T\\sqrt{d}\\cdot\\sqrt{\\displaystyle\\sum_{j=1}^{d}C_{\\mathrm{KL}}\\cdot\\frac{v_{0}K}{(v_{0}+K)^{2}}\\cdot\\frac{\\mathbb{E}_{V}[\\Tilde{M}_{j}]\\epsilon^{2}}{d}}\\right)}\\\\ &{\\qquad\\geq\\frac{v_{0}K}{(v_{0}+K e)^{2}}\\cdot\\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{6}-T\\sqrt{d}\\cdot\\sqrt{\\frac{C_{\\mathrm{KL}}}{4}\\cdot\\frac{v_{0}K}{(v_{0}+K)^{2}}\\cdot T\\epsilon^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second inequality is due to the Cauchy-Schwartz inequality and the last inequality holds because $\\begin{array}{r}{\\sum_{j=1}^{d}\\mathbb{E}_{V}[\\tilde{M}_{j}]\\leqslant d T/4}\\end{array}$ . Let $C_{\\mathrm{KL}}^{\\prime}=C_{\\mathrm{KL}}/4$ . ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{w}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbf{w}}^{\\pi}\\left[\\mathbf{Reg}_{T}(\\mathbf{w})\\right]\\geqslant\\frac{v_{0}K}{\\left(v_{0}+K e\\right)^{2}}\\cdot\\frac{\\epsilon}{2\\sqrt{d}}\\left(\\frac{d T}{6}-\\sqrt{C_{\\mathrm{KL}}^{\\prime}\\cdot\\frac{v_{0}K}{\\left(v_{0}+K\\right)^{2}}d T\\epsilon^{2}}\\right)}\\\\ &{\\quad\\quad\\quad\\quad=\\frac{v_{0}K}{\\left(v_{0}+K e\\right)^{2}}\\cdot\\sqrt{\\frac{\\left(v_{0}+K\\right)^{2}}{v_{0}K}}\\cdot\\frac{1}{288\\sqrt{C_{\\mathrm{KL}}^{\\prime}}}d\\sqrt{T}}\\\\ &{\\quad\\quad\\quad=\\Omega\\left(\\frac{\\sqrt{v_{0}K}}{v_{0}+K}\\cdot d\\sqrt{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This concludes the proof of Theorem 1. ", "page_idx": 21}, {"type": "text", "text": "D.3 Proofs of Lemmas for Theorem 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.3.1 Proof of Lemma D.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Lemma $D.I$ . Let $x=x_{V}$ and $\\hat{x}=x_{\\tilde{U}_{t}}$ be the corresponding context vectors. Then, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\bar{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})=\\frac{K\\exp\\big(x^{\\top}\\mathbf{w}_{V}\\big)}{v_{0}+K\\exp\\big(x^{\\top}\\mathbf{w}_{V}\\big)}-\\frac{K\\exp\\big(\\hat{x}^{\\top}\\mathbf{w}_{V}\\big)}{v_{0}+K\\exp\\big(\\hat{x}^{\\top}\\mathbf{w}_{V}\\big)}}}\\\\ &{}&{=\\frac{v_{0}K\\left(\\exp\\big(x^{\\top}\\mathbf{w}_{V}\\big)-\\exp\\big(\\hat{x}^{\\top}\\mathbf{w}_{V}\\big)\\right)}{\\left(v_{0}+K\\exp\\big(x^{\\top}\\mathbf{w}_{V}\\big)\\right)\\left(v_{0}+K\\exp\\big(\\hat{x}^{\\top}\\mathbf{w}_{V}\\big)\\right)}}\\\\ &{}&{\\geqslant\\frac{v_{0}K\\left(\\exp\\big(x^{\\top}\\mathbf{w}_{V}\\big)-\\exp\\big(\\hat{x}^{\\top}\\mathbf{w}_{V}\\big)\\right)}{\\left(v_{0}+K e\\right)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the inequality holds since max $\\left\\{\\exp\\left(x^{\\top}{\\mathbf w}_{V}\\right),\\exp\\left(\\hat{x}^{\\top}{\\mathbf w}_{V}\\right)\\right\\}\\,\\leqslant\\,e$ . To further bound the right-hand side of (D.5), we use the fac\u2423t that\\` $1+a\\leqslant e^{a}\\leqslant1+a+a^{2}/2$ for all $a\\in[0,1]$ , which can be easily shown by Taylor expansion. Thus, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\displaystyle\\sum_{i\\in\\bar{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})\\geq\\frac{v_{0}K\\left((x-\\hat{x})^{\\top}\\mathbf{w}_{V}-(\\hat{x}^{\\top}\\mathbf{w}_{V})^{2}/2\\right)}{(v_{0}+K e)^{2}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\geqslant\\frac{v_{0}K\\left(\\delta\\epsilon/\\sqrt{d}-(\\sqrt{d}\\epsilon)^{2}/2\\right)}{(v_{0}+K e)^{2}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\geqslant\\frac{v_{0}K\\delta\\epsilon}{2\\sqrt{d}(v_{0}+K e)^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality holds because $(\\sqrt{d}\\epsilon)^{2}\\leqslant\\delta\\epsilon/\\sqrt{d}$ when $\\epsilon\\in(0,1/d\\sqrt{d})$ . This concludes the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D.3.2 Proof of Lemma D.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof of Lemma $D.2$ . Fix a round $t$ , an assortment ${\\tilde{S}}_{t}$ , and $\\tilde{U}_{t}$ . Let $U\\,=\\,\\tilde{U}_{t}$ . Define $m_{j}(\\tilde{S}_{t}):=$ $\\textstyle\\sum_{x_{U}\\in\\tilde{S}_{t}}{\\mathbf{1}\\{j\\in U\\}}/K$ . Let $\\{p_{i}\\}_{i\\in\\tilde{S}_{t}\\cup\\{0\\}}$ and $\\{q_{i}\\}_{i\\in\\tilde{S}_{t}\\cup\\{0\\}}$ be the probabilities of choosing item $i$ u\u0159nder parameterizations $\\mathbf{w}_{V}$ and $\\mathbf{w}_{V\\cup\\{j\\}}$ , respectively. Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\mathbb{P}_{V}(\\cdot|\\tilde{S}_{t})\\|\\mathbb{P}_{V\\setminus\\{j\\}}(\\cdot|\\tilde{S}_{t})\\right)=\\sum_{i\\in\\tilde{S}_{t}\\cup\\{0\\}}p_{i}\\log\\frac{p_{i}}{q_{i}}\\leqslant\\sum_{i\\in\\tilde{S}_{t}\\cup\\{0\\}}p_{i}\\frac{p_{i}-q_{i}}{q_{i}}\\leqslant\\sum_{i\\in\\tilde{S}_{t}\\cup\\{0\\}}\\frac{(p_{i}-q_{i})^{2}}{q_{i}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality holds because $\\log(1+x)\\leqslant x$ for all $x>-1$ . ", "page_idx": 22}, {"type": "text", "text": "Let ${\\hat{x}}=x_{U}$ . Now, we separately upper bound $(p_{i}-q_{i})^{2}/q_{i}$ , by analyzing the following three cases: Case 1. The outside option, $i=0$ . ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|p_{i}-q_{i}\\right|=\\left|\\frac{v_{0}}{v_{0}+\\sum_{i\\in\\tilde{S}_{t}}\\exp\\left(x_{i}^{\\top}\\mathbf{w}_{V}\\right)}-\\frac{v_{0}}{v_{0}+\\sum_{i\\in\\tilde{S}_{t}}\\exp\\left(x_{i}^{\\top}\\mathbf{w}_{V\\setminus\\{j\\}}\\right)}\\right|}\\\\ &{\\qquad=\\left|\\frac{v_{0}}{v_{0}+K\\exp\\left(\\hat{x}^{\\top}\\mathbf{w}_{V}\\right)}-\\frac{v_{0}}{v_{0}+K\\exp\\left(\\hat{x}^{\\top}\\mathbf{w}_{V\\setminus\\{j\\}}\\right)}\\right|}\\\\ &{\\qquad\\leqslant\\frac{v_{0}K}{(v_{0}+K/e)^{2}}\\left|\\exp\\left(\\hat{x}^{\\top}\\mathbf{w}_{V}\\right)-\\exp\\left(\\hat{x}^{\\top}\\mathbf{w}_{V\\setminus\\{j\\}}\\right)\\right|}\\\\ &{\\qquad=\\frac{v_{0}K}{(v_{0}+K/e)^{2}}\\left|e^{\\varepsilon_{1}}(\\hat{x}^{\\top}\\mathbf{w}_{V}-\\hat{x}^{\\top}\\mathbf{w}_{V\\setminus\\{j\\}})\\right|}\\\\ &{\\qquad\\leqslant\\frac{v_{0}K e}{(v_{0}+K/e)^{2}}\\left|\\hat{x}^{\\top}\\left(\\mathbf{w}_{V}-\\mathbf{w}_{V\\setminus\\{j\\}}\\right)\\right|\\leqslant\\frac{v_{0}K e}{(v_{0}+K/e)^{2}}\\cdot\\frac{m_{j}(\\hat{x}_{i})\\epsilon}{\\sqrt{d}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the third equality holds by applying the mean value theorem for the exponential function, with $\\bar{c}_{1}:=(1-u)(\\hat{x}^{\\top}\\mathbf{w}_{V})+u(\\hat{x}^{\\top}\\mathbf{\\bar{w}}_{V\\cup\\{j\\}}^{\\top})$ for some $u\\in(0,1)$ . Then, there exist an absolute constant $C_{0}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{(p_{0}-q_{0})^{2}}{q_{0}}\\leqslant\\frac{v_{0}^{2}K^{2}e^{2}}{(v_{0}+K/e)^{4}}\\cdot\\frac{\\Big(m_{j}(\\tilde{S}_{t})\\Big)^{2}\\,\\epsilon^{2}}{d}\\cdot\\frac{v_{0}+K e}{v_{0}}}\\\\ &{\\qquad\\qquad\\leqslant C_{0}\\cdot\\frac{v_{0}K^{2}}{\\big(v_{0}+K\\big)^{3}}\\cdot\\frac{m_{j}(\\tilde{S}_{t})\\epsilon^{2}}{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality holds since $m_{j}(\\tilde{S}_{t})\\leqslant1$ . ", "page_idx": 22}, {"type": "text", "text": "Then, for any $i\\in\\tilde{S}_{t}$ corresponding to $x_{i}={\\hat{x}}$ and $j\\notin U$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|p_{i}-q_{i}\\right|=\\left|\\frac{\\exp\\left(\\hat{x}^{\\top}\\mathbf{w}_{V}\\right)}{v_{0}+K\\exp\\left(\\hat{x}^{\\top}\\mathbf{w}_{V}\\right)}-\\frac{\\exp\\left(\\hat{x}^{\\top}\\mathbf{w}_{V\\cup\\{j\\}}\\right)}{v_{0}+K\\exp\\left(\\hat{x}^{\\top}\\mathbf{w}_{V\\cup\\{j\\}}\\right)}\\right|=0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last equality holds because $\\exp\\left(\\hat{x}^{\\top}{\\mathbf w}_{V}\\right)=\\exp\\left(\\hat{x}^{\\top}{\\mathbf w}_{V\\cup\\{j\\}}\\right)$ , given that $j\\not\\in U$ . Thus, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{i\\in\\tilde{S}_{t},j\\notin U}\\frac{(p_{i}-q_{i})^{2}}{q_{i}}=0,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Case 3. $i\\in\\tilde{S}_{t}$ and $j\\in U$ . ", "page_idx": 22}, {"type": "text", "text": "Recall that for any i P S\u02dct, qi \u011bv0e\\`\u00b41Ke. Then, for any $i\\in\\tilde{S}_{t}$ corresponding to $x_{i}={\\hat{x}}$ and $j\\in U$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{i}-q_{i}|=\\left|\\frac{\\exp\\left(\\hat{x}^{\\top}{\\mathbf w}_{V}\\right)}{v_{0}+K\\exp\\left(\\hat{x}^{\\top}{\\mathbf w}_{V}\\right)}-\\frac{\\exp\\left(\\hat{x}^{\\top}{\\mathbf w}_{V\\setminus\\{j\\}}\\right)}{v_{0}+K\\exp\\left(\\hat{x}^{\\top}{\\mathbf w}_{V\\setminus\\{j\\}}\\right)}\\right|}\\\\ &{\\qquad=\\left|\\frac{\\exp\\left(\\bar{c}_{2}\\right)}{v_{0}+K\\exp\\left(\\bar{c}_{2}\\right)}\\cdot\\hat{x}^{\\top}\\left({\\mathbf w}_{V}-{\\mathbf w}_{V\\setminus\\{j\\}}\\right)-\\frac{K\\exp\\left(2\\bar{c}_{2}\\right)}{\\left(v_{0}+K\\exp\\left(\\bar{c}_{2}\\right)\\right)^{2}}\\cdot\\hat{x}^{\\top}\\left({\\mathbf w}_{V}-{\\mathbf w}_{V\\setminus\\{j\\}}\\right)\\right|}\\\\ &{\\qquad=\\frac{\\exp\\left(\\bar{c}_{2}\\right)v_{0}}{\\left(v_{0}+K\\exp\\left(\\bar{c}_{2}\\right)\\right)^{2}}\\left|\\hat{x}^{\\top}\\left({\\mathbf w}_{V}-{\\mathbf w}_{V\\setminus\\{j\\}}\\right)\\right|}\\\\ &{\\qquad\\leqslant\\frac{v_{0}e}{\\left(v_{0}+K/e\\right)^{2}}\\left|\\hat{x}^{\\top}\\left({\\mathbf w}_{V}-{\\mathbf w}_{V\\setminus\\{j\\}}\\right)\\right|\\leqslant\\frac{v_{0}e}{\\left(v_{0}+K/e\\right)^{2}}\\cdot\\frac{m_{j}\\left(\\tilde{S}_{t}\\right)\\epsilon}{\\sqrt{d}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "the second equality holds by applying the mean value theorem, with $\\bar{c}_{2}\\;:=\\;(1-u)(\\hat{x}^{\\top}{\\mathbf w}_{V})\\;+$ $u(\\hat{x}^{\\top}{\\mathbf w}_{V\\cup\\{j\\}})$ for some $u\\in(0,1)$ . Then, there exist an absolute constant $C_{1}$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i\\in\\tilde{S}_{t},j\\in U}\\frac{(p_{i}-q_{i})^{2}}{q_{i}}\\leqslant K m_{j}(\\tilde{S}_{t})\\cdot\\frac{v_{0}^{2}e^{2}}{(v_{0}+K/e)^{4}}\\cdot\\frac{\\left(m_{j}(\\tilde{S}_{t})\\right)^{2}\\epsilon^{2}}{d}\\cdot\\frac{v_{0}+K e}{e^{-1}}}&{}\\\\ {\\leqslant C_{1}\\cdot\\frac{v_{0}^{2}K}{(v_{0}+K)^{3}}\\cdot\\frac{m_{j}(\\tilde{S}_{t})\\epsilon^{2}}{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality holds since $m_{j}(\\tilde{S}_{t})\\leqslant1$ . ", "page_idx": 23}, {"type": "text", "text": "Combining (D.6), (D.7), and (D.8), we derive that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{i\\in\\tilde{S}_{t}\\cup\\{0\\}}\\frac{(p_{i}-q_{i})^{2}}{q_{i}}\\leqslant\\bigg(C_{0}\\cdot\\frac{v_{0}K^{2}}{(v_{0}+K)^{3}}+C_{1}\\cdot\\frac{v_{0}^{2}K}{(v_{0}+K)^{3}}\\bigg)\\cdot\\frac{m_{j}(\\tilde{S}_{t})\\epsilon^{2}}{d}}&{}\\\\ {\\leqslant\\operatorname*{max}\\{C_{0},C_{1}\\}\\cdot\\frac{v_{0}K}{(v_{0}+K)^{2}}\\cdot\\frac{m_{j}(\\tilde{S}_{t})\\epsilon^{2}}{d}}&{}\\\\ {=C_{\\mathrm{KL}}\\cdot\\frac{v_{0}K}{(v_{0}+K)^{2}}\\cdot\\frac{m_{j}(\\tilde{S}_{t})\\epsilon^{2}}{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $C_{\\mathrm{KL}}=\\operatorname*{max}\\{C_{0},C_{1}\\}$ . Since $\\begin{array}{r}{\\tilde{M}_{j}=\\sum_{t=1}^{T}m_{j}(\\tilde{S}_{t})}\\end{array}$ by definition, and subsequently summing over all $t=1$ to $T$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(P_{V}\\|Q_{V\\setminus\\{j\\}}\\right)=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{V}\\left[\\mathrm{KL}\\left(\\mathbb{P}_{V}(\\cdot|\\tilde{S}_{t})\\|\\mathbb{P}_{V\\setminus\\{j\\}}(\\cdot|\\tilde{S}_{t})\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leqslant C_{\\mathrm{KL}}\\cdot\\frac{v_{0}K}{\\left(v_{0}+K\\right)^{2}}\\cdot\\frac{\\mathbb{E}_{V}\\left[\\tilde{M}_{j}\\right]\\epsilon^{2}}{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the equality holds by the chain rule of relative entropy (cf. Exercise 14.11 of Lattimore and Szepesv\u00e1ri [33]). This concludes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "E Proof of Theorem 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we present the proof of Theorem 2. Note that when the rewards are uniform, the revenue increases as a function of the assortment size. Therefore, maximizing the expected revenue $R_{t}(S,\\mathbf{w})$ across all possible assortments $S\\in S$ always contains exactly $K$ items. In other words, the size of the chosen assortment $S_{t}$ and the size of the optimal assortment $S_{t}^{\\star}$ both equal to $K$ . ", "page_idx": 23}, {"type": "text", "text": "E.1 Main Proof of Theorem 2 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Before presenting the proof, we introduce useful lemmas, whose proof can be found in Appendix E.2.   \nLemma E.1 shows the optimistic utility for the context vectors. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.1. Let $\\alpha_{t i}=x_{t i}^{\\top}\\mathbf{w}_{t}+\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}$ . I $f_{\\mathbf{W}^{\\star}}\\in\\mathcal{C}_{t}(\\delta)$ , then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n0\\leqslant\\alpha_{t i}-x_{t i}^{\\top}\\mathbf{w}^{\\star}\\leqslant2\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma E.3 is a $K$ -free elliptical potential lemma that improves upon the one presented in Lemma 10 of Perivier and Goyal [45] in terms of $K$ . Lemma 10 of Perivier and Goyal [45] states: $\\begin{array}{r}{\\sum_{s=1}^{t}\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}^{\\star})p_{s}(0|S_{s},\\mathbf{w}^{\\star})\\|x_{s i}\\|_{H_{s}(\\mathbf{w}^{\\star})^{-1}}^{2}\\ \\leqslant\\ 2d K\\log\\left(\\lambda_{t+1}+\\frac{2t K}{d}\\right)}\\end{array}$ and $\\begin{array}{r}{\\sum_{s=1}^{t}\\operatorname*{max}_{i\\in S_{s}}\\|x_{s i}\\|_{H_{s}(\\mathbf{w}^{\\star})^{-1}}^{2}\\,\\leqslant\\,2d\\,\\big(K+\\frac{1}{\\kappa}\\big)\\log\\left(\\lambda_{t+1}+\\frac{2t K}{d}\\right)}\\end{array}$ , where $\\begin{array}{r}{H_{t}(\\mathbf{w})\\,=\\,\\sum_{s=1}^{t-1}\\mathcal{G}_{s}(\\mathbf{w})\\,+}\\end{array}$ $\\lambda_{t}\\mathbf{I}_{d}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma E.2. Let $\\begin{array}{r}{{\\cal H}_{t}\\ =\\ \\lambda{\\bf I}_{d}\\,+\\,\\sum_{s=1}^{t-1}{\\mathcal G}_{s}\\big({\\bf w}_{s+1}\\big),}\\end{array}$ , where $\\begin{array}{r}{\\mathcal{G}_{s}(\\mathbf{w})~=~\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w})x_{s i}x_{s i}^{\\top}~-}\\end{array}$ $\\begin{array}{r}{\\sum_{i\\in S_{s}}\\sum_{j\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w})p_{s}(j|S_{s},\\mathbf{w})x_{s i}x_{s j}^{\\top}}\\end{array}$ . Suppose $\\lambda\\geqslant1$ . Then th e \u0159following statements hold t\u0159rue: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(I)\\ \\sum_{s=1}^{t}\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf w_{s+1})p_{s}(0|S_{s},\\mathbf w_{s+1})\\|x_{s i}\\|_{H_{s}^{-1}}^{2}\\leqslant2d\\log\\big(1+\\frac{t}{d\\lambda}\\big),}\\\\ &{(2)\\ \\sum_{s=1}^{t}\\operatorname*{max}_{i\\in S_{s}}\\|x_{s i}\\|_{H_{s}^{-1}}^{2}\\leqslant\\frac{2}{\\kappa}d\\log\\big(1+\\frac{t}{d\\lambda}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, we provide a tighter bound for the second derivative of the expected revenue than that presented in Lemma 12 of Perivier and Goyal [45]. Lemma 12 of Perivier and Goyal [45] states: $\\left|\\frac{\\partial^{2}Q}{\\partial i\\partial j}\\right|\\leqslant5$ . ", "page_idx": 24}, {"type": "text", "text": "\u02c7Lem\u02c7ma E.3. Define $Q\\ :\\ \\mathbb{R}^{K}\\ \\rightarrow\\ \\mathbb{R}_{}$ , such that for any $\\textbf{u}=~\\left(u_{1},\\ldots,u_{K}\\right)~\\in~\\mathbb{R}^{K}$ , $Q(\\mathbf{u})\\;=\\;$ $\\begin{array}{r}{\\sum_{i=1}^{K}\\frac{\\exp(u_{i})}{v_{0}+\\sum_{k=1}^{K}\\exp(u_{k})}}\\end{array}$ . Let \u201c v0\\`ekKxp1p ueixqppukq. Then, for all i P rKs, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial^{2}Q}{\\partial i\\partial j}\\right|\\leqslant\\left\\{2p_{i}(\\mathbf{u})\\begin{array}{l l}{\\phantom{-}i f\\mathrm{\\Delta}i=j,}\\\\ {\\phantom{-}i f\\mathrm{\\Delta}i\\neq j.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, we are ready to prove Theorem 2. ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 2. First, we bound the regret as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}R_{t}(\\boldsymbol S_{t}^{*},\\mathbf w^{*})-R_{t}(\\boldsymbol S_{t},\\mathbf w^{*})=\\displaystyle\\sum_{t=1}^{T}\\left[\\sum_{i\\in S_{t}^{*}}p_{t}(i|\\boldsymbol S_{t}^{*},\\mathbf w^{*})-\\sum_{i\\in S_{t}}p_{t}(i|\\boldsymbol S_{t},\\mathbf w^{*})\\right]}\\\\ &{\\quad=\\displaystyle\\sum_{t=1}^{T}\\left[\\frac{\\sum_{i\\in S_{t}^{*}}\\exp(x_{t i}^{\\intercal}\\mathbf w^{*})}{v_{0}+\\sum_{j\\in S_{t}^{*}}\\exp(x_{t j}^{\\intercal}\\mathbf w^{*})}-\\frac{\\sum_{i\\in S_{t}}\\exp(x_{t i}^{\\intercal}\\mathbf w^{*})}{v_{0}+\\sum_{j\\in S_{t}}\\exp(x_{t j}^{\\intercal}\\mathbf w^{*})}\\right]}\\\\ &{\\quad\\leqslant\\displaystyle\\sum_{t=1}^{T}\\left[\\frac{\\sum_{i\\in S_{t}^{*}}\\exp(\\alpha_{t i})}{v_{0}+\\sum_{j\\in S_{t}^{*}}\\exp(\\alpha_{t j})}-\\frac{\\sum_{i\\in S_{t}}\\exp(x_{t i}^{\\intercal}\\mathbf w^{*})}{v_{0}+\\sum_{j\\in S_{t}}\\exp(x_{t j}^{\\intercal}\\mathbf w^{*})}\\right]}\\\\ &{\\quad\\leqslant\\displaystyle\\sum_{t=1}^{T}\\left[\\frac{\\sum_{i\\in S_{t}}\\exp(\\alpha_{t i})}{v_{0}+\\sum_{j\\in S_{t}}\\exp(\\alpha_{t j})}-\\frac{\\sum_{i\\in S_{t}}\\exp(x_{t i}^{\\intercal}\\mathbf w^{*})}{v_{0}+\\sum_{j\\in S_{t}}\\exp(x_{t i}^{\\intercal}\\mathbf w^{*})}\\right]=\\displaystyle\\sum_{t=1}^{T}\\tilde{R}_{t}(\\boldsymbol S_{t})-R_{t}(\\boldsymbol S_{t},\\mathbf w^{*}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality holds by Lemma E.1, and the last inequality holds by the assortment selection of Algorithm 1. ", "page_idx": 24}, {"type": "text", "text": "Now, we define $Q~:~\\mathbb{R}^{K}~\\rightarrow~\\mathbb{R}$ , such that for all $\\textbf{u}=~\\left(u_{1},\\ldots,u_{K}\\right)^{\\top}~\\in~\\mathbb{R}^{K}$ , $Q(\\mathbf{u})\\;\\;=\\;\\;$ \u201c iK\u201c1v0\\`ejxK\u201cp1p ueixqppujq. Noting that St always contains K elements since the expected revenue is an increasing function in the uniform reward setting, we can write $S_{t}\\,=\\,\\{i_{1},\\ldots,i_{K}\\}$ . Moreover, for all $t\\geqslant1$ , let $\\mathbf{u}_{t}\\,=\\,\\bigl(u_{t i_{1}},\\dots u_{t i_{K}}\\bigr)^{\\intercal}\\,=\\,\\bigl(\\alpha_{t i_{1}},\\dots,\\alpha_{t i_{K}}\\bigr)^{\\intercal}\\,$ and $\\mathbf{u}_{t}^{\\star}\\,=\\,(u_{t i_{1}}^{\\star},\\cdot\\cdot\\cdot u_{t i_{K}}^{\\star})^{\\top}\\,=$ $(x_{t i_{1}}^{\\top}\\mathbf{w}^{\\star},\\ldots,x_{t i_{K}}^{\\top}\\mathbf{w}^{\\star})^{\\top}$ . Then, by a second order Taylor expansion, we have ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\tilde{R}_{t}(S_{t})-R_{t}(S_{t},\\mathbf{w}^{\\star})=\\displaystyle\\sum_{t=1}^{T}Q(\\mathbf{u}_{t})-Q(\\mathbf{u}_{t}^{\\star})}&{}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\nabla Q(\\mathbf{u}_{t}^{\\star})^{\\top}(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})+\\frac{1}{2}\\displaystyle\\sum_{t=1}^{T}(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})^{\\top}\\nabla^{2}Q(\\bar{\\mathbf{u}}_{t})(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star}),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\bar{\\mathbf{u}}_{t}=\\left(\\bar{u}_{t i_{1}},\\ldots,\\bar{u}_{t i_{K}}\\right)^{\\top}\\in\\mathbb{R}^{K}$ is the convex combination of $\\mathbf{u}_{t}$ and $\\mathbf{u}_{t}^{\\star}$ .   \nFirst, we bound the term (A). ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{N}\\nabla q(u_{i}^{\\mathrm{t}})^{T}(u_{i}-u_{i}^{\\mathrm{t}})}\\\\ &{=\\displaystyle\\sum_{i=1}^{N}\\sum_{i\\neq i,\\nu_{i}}\\exp(x_{i}^{\\mathrm{t}}u_{i}^{\\mathrm{t}})^{T}(u_{i}-u_{i}^{\\mathrm{t}})-\\sum_{i\\neq i}\\sum_{\\nu_{i},\\nu_{i}}\\frac{\\exp(x_{i}^{\\mathrm{t}}u_{i}^{\\mathrm{t}})\\exp(x_{i}^{\\mathrm{t}}u_{i}^{\\mathrm{t}})}{-\\omega_{i}\\nu_{i}\\omega_{i}}(u_{i}^{\\mathrm{t}})\\frac{1}{\\omega_{i}}\\left(u_{i}-u_{i}^{\\mathrm{t}}\\right)}\\\\ &{=\\displaystyle\\sum_{i=1}^{N}\\sum_{\\nu_{i}=1}^{N}p(u_{i}|^{\\mathrm{D}},\\mathbf{v}_{i}^{\\mathrm{t}})(u_{i}-u_{i}^{\\mathrm{t}})-\\sum_{i\\neq i,\\nu_{i}}\\sum_{\\mu_{i}=1}^{N}p(i|S_{i,\\nu},\\mathbf{v}_{i}^{\\mathrm{t}})(\\hat{v}_{i,\\nu},\\mathbf{v}_{i}^{\\mathrm{t}})(u_{i}-u_{i}^{\\mathrm{t}})}\\\\ &{=\\displaystyle\\sum_{i=1}^{N}\\sum_{\\nu_{i}=1}^{N}p(|U_{i,\\nu},\\mathbf{v}_{i}^{\\mathrm{t}}|^{\\mathrm{u}})(u_{i}-u_{i}^{\\mathrm{t}})-\\sum_{i\\neq i,\\nu_{i}}\\sum_{\\mu_{i}=N}p_{i}(|U_{i,\\nu},\\mathbf{v}_{i}^{\\mathrm{t}}|^{\\mathrm{p}})(\\hat{v}_{i,\\nu},\\mathbf{w}_{i}^{\\mathrm{t}})(u_{i}-u_{i}^{\\mathrm{t}})}\\\\ &{=\\displaystyle\\sum_{i=1}^{N}\\sum_{\\nu_{i}=1}^{N}p(|U_{i,\\nu},\\mathbf{v}_{i}^{\\mathrm{t}}|^{\\mathrm{p}})\\left(1-\\sum_{j=N}\\hat{v}_{j}(|U_{i,\\nu}|^{\\mathrm{p}},\\mathbf{v}_{i}^{\\mathrm{t}})\\right)(u_{\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the first inequality holds by Lemma E.1, and the last inequality holds because $\\beta_{t}(\\delta)$ is increasing for $t\\in[\\bar{T}]$ . ", "page_idx": 25}, {"type": "text", "text": "Now we bound the term (B). Let $\\begin{array}{r}{p_{i}(\\bar{\\mathbf{u}}_{t})=\\frac{\\exp\\left(\\bar{u}_{t i}\\right)}{v_{0}+\\sum_{k=1}^{K}\\exp\\left(\\bar{u}_{t k}\\right)}}\\end{array}$ Then, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2}\\sum_{t=1}^{T}({\\mathbf{u}}_{t}-{\\mathbf{u}}_{t}^{\\star})^{\\top}\\nabla^{2}Q(\\bar{\\mathbf{u}}_{t})({\\mathbf{u}}_{t}-{\\mathbf{u}}_{t}^{\\star})}\\\\ {\\displaystyle}&{=\\frac{1}{2}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t}}\\left(u_{t i}-u_{t i}^{\\star}\\right)\\frac{\\tilde{\\sigma}^{2}Q}{\\tilde{\\sigma}i\\tilde{\\sigma}j}(u_{t j}-u_{t j}^{\\star})}\\\\ {\\displaystyle}&{=\\frac{1}{2}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t},j\\neq i}\\left(u_{t i}-u_{t i}^{\\star}\\right)\\frac{\\tilde{\\sigma}^{2}Q}{\\tilde{\\sigma}i\\tilde{\\sigma}j}(u_{t j}-u_{t j}^{\\star})+\\displaystyle\\frac{1}{2}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\left(u_{t i}-u_{t i}^{\\star}\\right)\\frac{\\tilde{\\sigma}^{2}Q}{\\tilde{\\sigma}i\\tilde{\\sigma}i}(u_{t i}-u_{t i}^{\\star})}\\\\ &{\\displaystyle\\leqslant\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t},j\\neq i}\\left|u_{t i}-u_{t i}^{\\star}\\right|p_{i}(\\bar{\\mathbf{u}}_{t})p_{j}(\\bar{\\mathbf{u}}_{t})|u_{t j}-u_{t j}^{\\star}|+\\frac{3}{2}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\left(u_{t i}-u_{t i}^{\\star}\\right)^{2}p_{i}(\\bar{\\mathbf{u}}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the inequality is by Lemma E.3. To bound the first term in (E.3), by applying the AM-GM inequality, we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t},j\\in\\mathcal{S}_{t}}\\big\\vert u_{t i}-u_{t i}^{\\star}\\vert p_{i}(\\bar{\\mathbf{u}}_{t})p_{j}(\\bar{\\mathbf{u}}_{t})\\vert u_{t j}-u_{t j}^{\\star}\\big\\vert}\\\\ {\\displaystyle~~\\leqslant\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t}}\\big\\vert u_{t i}-u_{t i}^{\\star}\\vert p_{i}(\\bar{\\mathbf{u}}_{t})p_{j}(\\bar{\\mathbf{u}}_{t})\\vert u_{t j}-u_{t j}^{\\star}\\big\\vert}\\\\ {\\displaystyle~~\\leqslant\\frac{1}{2}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t}}\\big(u_{t i}-u_{t i}^{\\star}\\big)^{2}p_{i}(\\bar{\\mathbf{u}}_{t})p_{j}(\\bar{\\mathbf{u}}_{t})+\\frac{1}{2}\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t}}\\big(u_{t j}-u_{t j}^{\\star}\\big)^{2}p_{i}(\\bar{\\mathbf{u}}_{t})p_{j}(\\bar{\\mathbf{u}}_{t})}\\\\ {\\displaystyle~~\\leqslant\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}(u_{t i}-u_{t i}^{\\star})^{2}p_{i}(\\bar{\\mathbf{u}}_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By plugging (E.4) into (E.3), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2}\\sum_{t=1}^{T}({\\mathbf u}_{t}-{\\mathbf u}_{t}^{*})^{\\top}\\nabla^{2}Q(\\bar{\\mathbf u}_{t})(\\mathbf u_{t}-\\bar{\\mathbf u}_{t}^{*})\\leqslant\\displaystyle\\frac{5}{2}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}(u_{t i}-u_{t i}^{*})^{2}p_{i}(\\bar{\\mathbf u}_{t})}\\\\ {\\displaystyle\\leqslant10\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{i}(\\bar{\\mathbf u}_{t})\\beta_{t}(\\delta)^{2}\\lVert x_{t i}\\rVert_{H_{t}^{-1}}^{2}}\\\\ {\\displaystyle\\leqslant10\\sum_{t=1}^{T}\\operatorname*{max}_{t}\\beta_{t}(\\delta)^{2}\\lVert x_{t i}\\rVert_{H_{t}^{-1}}^{2}}\\\\ {\\displaystyle\\leqslant10\\beta_{T}(\\delta)^{2}\\sum_{t=1}^{T}\\operatorname*{max}_{t}\\lVert\\boldsymbol\\chi_{t}\\rVert_{H_{t}^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality holds by Lemma E.1. Combining the upper bound for the terms (A) and (B), with probability at least $1-\\delta$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\tilde{R}_{t}(S_{t})-R_{t}(S_{t},\\mathbf{w}^{\\star})\\leqslant2\\beta_{T}(\\delta)\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})p_{t}(0|S_{t},\\mathbf{w}^{\\star})\\|x_{t i}\\|_{H_{t}^{-1}}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\ 10\\beta_{T}(\\delta)^{2}\\sum_{t=1}^{T}\\operatorname*{max}_{i\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, we bound each term of (E.6) respectively. For the first term, we decompose it as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{*})p_{t}(0|S_{t},\\mathbf w^{*})\\|x_{t}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w_{t+1})p_{t}(0|S_{t},\\mathbf w_{t+1})\\|x_{t}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle+\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\left(p_{t}(i|S_{t},\\mathbf w^{*})-p_{t}(i|S_{t},\\mathbf w_{t+1})\\right)p_{t}(0|S_{t},\\mathbf w_{t+1})\\|x_{t}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle+\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{*})\\left(p_{t}(0|S_{t},\\mathbf w^{*})-p_{t}(0|S_{t},\\mathbf w_{t+1})\\right)\\|x_{t}\\|_{H_{t}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To bound the first term on the right-hand side of (E.7), we apply the Cauchy-Schwarz inequality. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w_{t+1})p_{t}(0|S_{t},\\mathbf w_{t+1})\\lVert x_{t}\\rVert_{H_{t}^{-1}}}\\\\ &{\\le\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w_{t+1})p_{t}(0|S_{t},\\mathbf w_{t+1})}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w_{t+1})p_{t}(0|S_{t},\\mathbf w_{t+1})\\lVert x_{t}\\rVert_{H_{t}^{-1}}^{2}}}\\\\ &{\\leqslant\\displaystyle\\frac{\\sqrt{v_{0}K}}{(v_{0}+K e^{-1})}\\sqrt{T\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w_{t+1})p_{t}(0|S_{t},\\mathbf w_{t+1})\\lVert x_{t}\\rVert_{H_{t}^{-1}}^{2}}}\\\\ &{\\leqslant\\displaystyle\\frac{\\sqrt{v_{0}K}}{(v_{0}+K e^{-1})}\\sqrt{T\\cdot2d\\log\\left(1+\\displaystyle\\frac{T}{d\\lambda}\\right)},\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{(F)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality holds by Lemma E.2. ", "page_idx": 27}, {"type": "text", "text": "Now, we bound the second term on the right-hand side of (E.7). Let the virtual context for the outside option be $x_{t0}=\\mathbf{0}$ . Then, by the mean value theorem, there exists $\\pmb{\\xi}_{t}=(1-c)\\mathbf{w}^{\\star}+c\\mathbf{w}_{t+1}$ for some $c\\in(0,1)$ such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in S_{t}}\\left(p_{i}(|S_{t},\\mathbf{w}^{*})-p_{i}(i|S_{t},\\mathbf{w}_{t+1})\\right)p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\left|\\boldsymbol{x}_{t}\\right|}\\\\ &{\\displaystyle=\\sum_{i\\in S_{t}}\\nabla p_{t}(i|S_{t},\\xi_{t})^{\\top}(\\mathbf{w}^{*}-\\mathbf{w}_{t+1})p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\left|\\boldsymbol{x}_{t}\\right|}\\\\ &{\\displaystyle=\\sum_{i\\in S_{t}}\\left(p_{t}(i|S_{t},\\xi_{t})\\boldsymbol{x}_{t}-p_{t}(i|S_{t},\\xi_{t})\\sum p_{t}(j|S_{t},\\xi_{t})\\boldsymbol{x}_{t}\\right)^{\\top}(\\mathbf{w}^{*}-\\mathbf{w}_{t+1})p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\left|\\boldsymbol{x}_{t}\\right|}\\\\ &{\\displaystyle\\leqslant\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\left|\\boldsymbol{x}_{t}^{\\top}(\\mathbf{w}^{*}-\\mathbf{w}_{t+1})\\left|p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\right|\\boldsymbol{x}_{t}\\right|}\\\\ &{\\displaystyle\\leqslant\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\left|\\boldsymbol{x}_{t}\\right|_{t}\\left|\\mathbf{w}_{t}^{*}-\\mathbf{w}_{t+1}\\right|p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\left|\\boldsymbol{x}_{t}\\right|\\left|\\boldsymbol{\\mu}_{t}\\right|^{-1}}\\\\ &{\\displaystyle+\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\left|\\boldsymbol{x}_{t}\\right|_{t}\\left|\\boldsymbol{\\mu}_{t}\\right|_{t}-\\sum_{j\\in S_{t}}p_{t}(j|S_{t},\\xi_{t})\\left|\\boldsymbol{x}_{t}\\right|\\left(\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\right)\\left|p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\right.}\\\\ &{\\left.\\leqslant\\sum_{j\\in S_{t}}p_{t}(i|S_{t},\\xi_{t\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, since $x_{t0}=\\mathbf{0}$ , we can further bound the right-hand side as: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\|x_{t i}\\|_{H_{t}^{-1}}^{2}\\|\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\|_{H_{t}}+\\left(\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\|x_{t i}\\|_{H_{t}^{-1}}\\right)^{2}\\|\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\|_{H_{t}}}\\\\ &{\\displaystyle=\\sum_{t\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\|x_{t i}\\|_{H_{t}^{-1}}^{2}\\|\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\|_{H_{t}}+\\left(\\displaystyle\\sum_{i\\in S_{t}\\setminus\\{0\\}}p_{t}(i|S_{t},\\xi_{t})\\|x_{t i}\\|_{H_{t}^{-1}}\\right)^{2}\\|\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\|_{H_{t}}}\\\\ &{\\displaystyle\\leqslant\\sum_{t\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\|x_{t i}\\|_{H_{t}^{-1}}^{2}\\|\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\|_{H_{t}}+\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\|x_{t i}\\|_{H_{t}^{-1}}^{2}\\|\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\|_{H_{t}}}\\\\ &{\\displaystyle\\leqslant2\\sum_{t\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\|x_{t i}\\|_{H_{t}^{-1}}^{2}\\|\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\|_{H_{t}}}\\\\ &{\\displaystyle\\leqslant2\\beta_{t}(i)\\sum_{t\\in S_{t}}p_{t}(i|S_{t},\\xi_{t})\\|x_{t i}\\|_{H_{t}^{-1}}^{2}\\leqslant2\\beta_{t}(\\delta)\\operatorname*{max}_{t\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the first inequality holds due to Jensen\u2019s inequality and the second-to-last inequality holds by Lemma 1. Hence, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\left(p_{t}(i|S_{t},\\mathbf{w}^{\\star})-p_{t}(i|S_{t},\\mathbf{w}_{t+1})\\right)p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\|x_{t i}\\|_{H_{t}^{-1}}\\leqslant2\\beta_{T}(\\delta)\\sum_{t=1}^{T}\\operatorname*{max}_{i\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2}}}\\\\ &{}&{\\leqslant\\frac{4d}\\kappa\\beta_{T}(\\delta)\\log\\left(1+\\frac{T}{d\\lambda}\\right)_{\\infty}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality holds by Lemma E.2. ", "page_idx": 28}, {"type": "text", "text": "Finally, we bound the third term on the right-hand side of (E.7). By the mean value theorem, there exists $\\pmb{\\xi}_{t}^{\\prime}=(1-c^{\\prime})\\mathbf{w}^{\\star}+c^{\\prime}\\mathbf{w}_{t+1}$ for some $c^{\\prime}\\in(0,1)$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{u\\in S_{t}}{\\sum}p_{t}(i|S_{t},\\mathbf{w}^{*})\\left(p_{t}(\\theta|S_{t},\\mathbf{w}^{*})-p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\right)\\left|x_{t}\\right|_{H_{t}^{-1}}}\\\\ &{=\\underset{t\\in S_{t}}{\\sum}p_{t}(i|S_{t},\\mathbf{w}^{*})\\nabla p_{t}(0|S_{t},\\xi_{t}^{*})^{\\top}(\\mathbf{w}^{*}-\\mathbf{w}_{t+1})\\left|x_{t}\\right|_{H_{t}^{-1}}}\\\\ &{=-\\underset{t\\in S_{t}}{\\sum}p_{t}(i|S_{t},\\mathbf{w}^{*})p_{t}(0|S_{t},\\xi_{t}^{*})\\sum_{\\bar{p}\\in S_{t}}p_{t}(j|S_{t},\\xi_{t}^{*})\\underline{{\\gamma}}_{\\bar{p}^{*}}^{\\top}(\\mathbf{w}^{*}-\\mathbf{w}_{t+1})\\left|x_{t}\\right|_{H_{t}^{-1}}}\\\\ &{\\leqslant\\underset{t\\in S_{t}}{\\sum}p_{t}(i|S_{t},\\mathbf{w}^{*})\\left|x_{t}\\right|_{H_{t}^{-1}}\\!\\!p_{t}(0|S_{t},\\xi_{t}^{*})\\sum_{\\bar{p}\\in S_{t}}p_{t}(j|S_{t},\\xi_{t}^{*})|x_{t}|_{H_{t}^{-1}}\\!\\left|\\mathbf{w}^{*}-\\mathbf{w}_{t+1}\\right|\\left|x_{t}\\right|_{H_{t}^{-1}}}\\\\ &{\\leqslant\\underset{t\\in S_{t}}{\\sum}p_{t}(i|S_{t},\\mathbf{w}^{*})\\left|x_{t}\\right|_{H_{t}^{-1}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the third inequality holds by Lemma 1, and the last inequality holds since $\\left(\\operatorname*{max}_{i}a_{i}\\right)^{2}\\;=$ $\\operatorname*{max}_{i}a_{i}^{2}$ for any $a_{i}\\geqslant0$ . Therefore, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})\\left(p_{t}(0|S_{t},\\mathbf{w}^{\\star})-p_{t}(0|S_{t},\\mathbf{w}_{t+1})\\right)\\left\\|x_{t i}\\right\\|_{H_{t}^{-1}}\\leqslant\\beta_{T}(\\delta)\\sum_{t=1}^{T}\\displaystyle\\operatorname*{max}_{i\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2}}}\\\\ &{}&{\\leqslant\\displaystyle\\frac{2d}{\\kappa}\\beta_{T}(\\delta)\\log\\left(1+\\frac{T}{d\\lambda}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the last inequality holds by Lemma E.2. By plugging (E.8), (E.9), and (E.10) into (E.7) and multiplying $2\\beta_{T}(\\delta)$ , we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2\\beta_{T}(\\delta)\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})p_{t}(0|S_{t},\\mathbf{w}^{\\star})\\lVert x_{t i}\\rVert_{H_{t}^{-1}}}\\\\ &{\\phantom{2p c}\\leqslant2\\sqrt{2}\\displaystyle\\frac{\\sqrt{v_{0}K}}{(v_{0}+K e^{-1})}\\beta_{T}(\\delta)\\sqrt{d T}\\displaystyle\\sqrt{\\log\\left(1+\\displaystyle\\frac{T}{d\\lambda}\\right)}+\\frac{12d}{\\kappa}\\beta_{T}(\\delta)^{2}\\log\\left(1+\\displaystyle\\frac{T}{d\\lambda}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Moreover, by applying Lemma E.2, we can directly bound the second term of (E.6). ", "page_idx": 28}, {"type": "equation", "text": "$$\n10\\beta_{T}(\\delta)^{2}\\sum_{t=1}^{T}\\operatorname*{max}_{i\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2}\\leqslant10\\beta_{T}(\\delta)^{2}\\cdot\\frac{2}{\\kappa}d\\log\\left(1+\\frac{T}{d\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, plugging (E.11) and (E.12) into (E.6), we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf R e g}_{T}({\\bf w}^{\\star})\\leqslant2\\sqrt{2}\\frac{\\sqrt{v_{0}K}}{\\left(v_{0}+K e^{-1}\\right)}\\beta_{T}(\\delta)\\sqrt{d T}\\sqrt{\\log\\left(1+\\frac{T}{d\\lambda}\\right)}+\\frac{32d}{\\kappa}\\beta_{T}(\\delta)^{2}\\log\\left(1+\\frac{T}{d\\lambda}\\right)}\\ }\\\\ {{\\displaystyle=\\tilde{\\mathcal{O}}\\left(\\frac{\\sqrt{v_{0}K}}{v_{0}+K}d\\sqrt{T}+\\frac{1}{\\kappa}d^{2}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\beta_{T}(\\delta)=\\mathcal{O}\\left(\\sqrt{d}\\log T\\log K\\right)$ . This concludes the proof of Theorem 2. ", "page_idx": 29}, {"type": "text", "text": "Remark E.1. If the boundedness assumption on the parameter is relaxed to $\\|\\mathbf{w}\\|_{2}\\,\\leqslant\\,B$ , since $\\beta_{t}(\\delta)=\\mathcal{O}\\left(B\\sqrt{d}\\log t\\log K\\right)$ , we have $\\begin{array}{r}{\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star})=\\tilde{\\mathcal{O}}\\left(B e^{B}\\frac{\\sqrt{v_{0}K}}{v_{0}+K}d\\sqrt{T}+\\frac{1}{\\kappa}d^{2}\\right)}\\end{array}$ . It\u2019s important to note that one of our main goals is to explicitly demonstrate the regret depends on $K$ and $v_{0}$ . In deriving such a result, the dependence on $e^{B}$ is unavoidable to our best knowledge. Note that for non-uniform rewards, the regret bound does not depend on $e^{B}$ (refer Remark $H.l$ ). ", "page_idx": 29}, {"type": "text", "text": "E.2 Proofs of Lemmas for Theorem 2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "E.2.1 Proof of Lemma E.1 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof of Lemma E.1. Under the condition $\\mathbf{w}^{\\star}\\in\\mathcal{C}_{t}(\\delta)$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|x_{t i}^{\\top}\\mathbf{w}_{t}-x_{t i}^{\\top}\\mathbf{w}^{\\star}\\right|\\leqslant\\left\\|x_{t i}\\right\\|_{H_{t}^{-1}}\\!\\left\\|\\mathbf{w}_{t}-\\mathbf{w}^{\\star}\\right\\|_{H_{t}}\\leqslant\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the first inequality is by the H\u00f6lder\u2019s inequality, and the last inequality holds by Lemma 1. Hence, it follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\alpha_{t i}-x_{t i}^{\\top}\\mathbf{w}^{\\star}=x_{t i}^{\\top}\\mathbf{w}_{t}-x_{t i}^{\\top}\\mathbf{w}^{\\star}+\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}\\leqslant2\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Moreover, from $x_{t i}^{\\top}\\mathbf{w}_{t}-x_{t i}^{\\top}\\mathbf{w}^{\\star}\\geqslant-\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}$ , we also have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\alpha_{t i}-x_{t i}^{\\top}\\mathbf{w}^{\\star}=x_{t i}^{\\top}\\mathbf{w}_{t}-x_{t i}^{\\top}\\mathbf{w}^{\\star}+\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}\\geqslant0.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 29}, {"type": "text", "text": "E.2.2 Proof of Lemma E.2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Proof of Lemma $E.2$ . Since $x x^{\\top}+y y^{\\top}\\geq x y^{\\top}+y x^{\\top}$ for any $x,y\\in\\mathbb{R}^{d}$ , it follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathcal{G}_{s}(\\mathbf{w}_{s+1})}\\\\ &{\\displaystyle=\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top}-\\displaystyle\\sum_{i\\in S_{s}}\\sum_{j\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(j|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s j}^{\\top}}\\\\ &{\\displaystyle=\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top}-\\displaystyle\\frac{1}{2}\\sum_{i\\in S_{s}}\\sum_{j\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(j|S_{s},\\mathbf{w}_{s+1})(x_{s i}x_{s j}^{\\top}+x_{s j}x_{s i}^{\\top})}\\\\ &{\\displaystyle\\geq\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top}-\\displaystyle\\frac{1}{2}\\sum_{i\\in S_{s}}\\sum_{j\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(j|S_{s},\\mathbf{w}_{s+1})(x_{s i}x_{s i}^{\\top}+x_{s j}x_{s j}^{\\top})}\\\\ &{\\displaystyle=\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top}-\\displaystyle\\sum_{i\\in S_{s}}\\sum_{j\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(j|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{s}(\\mathbf{w}_{s+1})\\geq\\displaystyle\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})\\left(1-\\displaystyle\\sum_{j\\in S_{s}}p_{s}(j|S_{s},\\mathbf{w}_{s+1})\\right)x_{s i}x_{s i}^{\\top}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(0|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which implies that ", "page_idx": 29}, {"type": "equation", "text": "$$\nH_{t+1}\\geq H_{t}+\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}_{t+1})p_{t}(0|S_{t},\\mathbf{w}_{t+1})x_{t i}x_{t i}^{\\top}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(H_{t+1}\\right)\\geqslant\\operatorname*{det}\\left(H_{t}\\right)\\left(1+\\sum_{i\\in S_{t}}p_{t}(i|S_{t},{\\mathbf w}_{t+1})p_{0}(i|S_{t},{\\mathbf w}_{t+1})\\|x_{t+1}\\|_{H_{t}^{-1}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $\\lambda\\geqslant1$ , for all $t\\geqslant1$ , we have $\\begin{array}{r}{\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}_{t+1})p_{0}(i|S_{t},\\mathbf{w}_{t+1})\\|x_{t i}\\|_{H_{t}^{-1}}^{2}\\leqslant1.}\\end{array}$ . Then, using the fact that $z\\leqslant2\\log(1+z)$ for an y $z\\in[0,1]$ , we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(0|S_{s},\\mathbf{w}_{s+1})\\lVert x_{s}\\rVert_{H_{s}^{-1}}^{2}}\\\\ &{\\quad\\quad\\quad\\leqslant2\\displaystyle\\sum_{s=1}^{t}\\log\\Big(1+p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(0|S_{s},\\mathbf{w}_{s+1})\\lVert x_{s}\\rVert_{H_{s}^{-1}}^{2}\\Big)}\\\\ &{\\quad\\quad\\leqslant2\\displaystyle\\sum_{s=1}^{t}\\log\\Big(\\displaystyle\\frac{\\mathrm{det}(H_{s+1})}{\\mathrm{det}(H_{s})}\\Big)}\\\\ &{\\quad\\quad\\leqslant2d\\log\\Big(\\displaystyle\\frac{\\mathrm{tr}(H_{t+1})}{d\\lambda}\\Big)\\leqslant2d\\log\\Big(1+\\displaystyle\\frac{t}{d\\lambda}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This proves the first inequality. ", "page_idx": 30}, {"type": "text", "text": "To establish the proof for the second inequality, we return to (E.13): ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{G}_{s}(\\mathbf{w}_{s+1})\\geq\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(0|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top}\\geq\\kappa\\sum_{i\\in S_{s}}x_{s i}x_{s i}^{\\top},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which implies that ", "page_idx": 30}, {"type": "equation", "text": "$$\nH_{t+1}=H_{t}+\\mathcal G_{t}(\\mathbf w_{t+1})\\geq H_{t}+\\kappa\\sum_{i\\in S_{t}}x_{t i}x_{t i}^{\\top}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Since $\\lambda\\geqslant1$ , for all $t\\geqslant1$ , we have $\\kappa\\operatorname*{max}_{i\\in S_{t}}\\left\\|x_{t i}\\right\\|_{H_{t}^{-1}}^{2}\\leqslant\\kappa$ . We then conclude on the same way: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\operatorname*{max}_{i\\in S_{s}}\\|x_{s i}\\|_{H_{s}^{-1}}^{2}\\leq\\displaystyle\\frac{2}{\\kappa}\\sum_{s=1}^{t}\\log\\left(1+\\kappa\\displaystyle\\operatorname*{max}_{i\\in S_{s}}\\|x_{s i}\\|_{H_{s}^{-1}}^{2}\\right)}\\\\ &{\\displaystyle\\leq\\displaystyle\\frac{2}{\\kappa}\\sum_{s=1}^{t}\\log\\left(\\displaystyle\\frac{\\operatorname*{det}(H_{s+1})}{\\operatorname*{det}(H_{s})}\\right)\\leqslant\\displaystyle\\frac{2}{\\kappa}d\\log\\left(1+\\displaystyle\\frac{t}{d\\lambda}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which proves the second inequality. ", "page_idx": 30}, {"type": "text", "text": "E.2.3 Proof of Lemma E.3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof of Lemma E.3. Let $i,j\\in[K]$ . We first have ", "page_idx": 30}, {"type": "equation", "text": "$$\n{\\frac{\\partial Q}{\\partial i}}={\\frac{e^{u_{i}}}{v_{0}+\\sum_{k=1}^{K}e^{u_{k}}}}-{\\frac{e^{u_{i}}\\left(\\sum_{k=1}^{K}e^{u_{k}}\\right)}{(v_{0}+\\sum_{k=1}^{K}e^{u_{k}})^{2}}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, we get ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\cfrac{\\mathbb{1}_{i=j}e^{u_{i}}}{v_{0}+\\sum_{k=1}^{K}e^{u_{k}}}-\\cfrac{e^{u_{i}}e^{u_{j}}}{(v_{0}+\\sum_{k=1}^{K}e^{u_{k}})^{2}}-\\cfrac{\\mathbb{1}_{i=j}e^{u_{i}}\\left(\\sum_{k=1}^{K}e^{u_{k}}\\right)+e^{u_{i}}e^{u_{j}}}{(v_{0}+\\sum_{k=1}^{K}e^{u_{k}})^{2}}}\\\\ &{+\\cfrac{e^{u_{i}}\\left(\\sum_{k=1}^{K}e^{u_{k}}\\right)2e^{u_{j}}\\left(v_{0}+\\sum_{k=1}^{K}e^{u_{k}}\\right)}{(v_{0}+\\sum_{k=1}^{K}e^{u_{k}})^{4}}}\\\\ &{=\\cfrac{\\mathbb{1}_{i=j}e^{u_{i}}}{v_{0}+\\sum_{k=1}^{K}e^{u_{k}}}-\\cfrac{e^{u_{i}}e^{u_{j}}}{(v_{0}+\\sum_{k=1}^{K}e^{u_{k}})^{2}}-\\cfrac{\\mathbb{1}_{i=j}e^{u_{i}}\\left(\\sum_{k=1}^{K}e^{u_{k}}\\right)+e^{u_{i}}e^{u_{j}}}{(v_{0}+\\sum_{k=1}^{K}e^{u_{k}})^{2}}+\\cfrac{e^{u_{i}}\\left(\\sum_{k=1}^{K}e^{u_{k}}\\right)2e^{u_{j}}}{(v_{0}+\\sum_{k=1}^{K}e^{u_{k}})^{3}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\lvert\\frac{\\hat{\\sigma}^{2}Q}{\\hat{\\sigma}\\hat{i}\\hat{\\sigma}\\hat{j}}\\right\\rvert=\\displaystyle\\bigg\\lvert p_{i}(\\mathbf{u})-p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})-p_{i}(\\mathbf{u})\\frac{\\sum_{k=1}^{K}e^{u_{k}}}{v_{0}+\\sum_{k=1}^{K}e^{u_{k}}}-p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})}\\\\ {\\displaystyle\\quad\\quad+\\,2p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})\\frac{\\sum_{k=1}^{K}e^{u_{k}}}{v_{0}+\\sum_{k=1}^{K}e^{u_{k}}}\\,\\bigg\\rvert}\\\\ {\\displaystyle\\quad\\quad=\\bigg\\lvert p_{i}(\\mathbf{u})p_{0}(\\mathbf{u})-2p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})+2p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})\\frac{\\sum_{k=1}^{K}e^{u_{k}}}{v_{0}+\\sum_{k=1}^{K}e^{u_{k}}}\\,\\bigg\\rvert}\\\\ {\\displaystyle\\quad\\quad=\\bigg\\lvert p_{i}(\\mathbf{u})p_{0}(\\mathbf{u})-2p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})p_{0}(\\mathbf{u})\\bigg\\rvert}\\\\ {\\displaystyle\\quad\\quad\\leqslant3p_{i}(\\mathbf{u})}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For $i\\neq j$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{\\partial^{2}Q}{\\partial i\\partial j}\\right|=\\bigg|-p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})-p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})+2p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})\\displaystyle\\frac{\\sum_{k=1}^{K}e^{u_{k}}}{v_{0}+\\sum_{k=1}^{K}e^{u_{k}}}\\bigg|}\\\\ &{\\qquad\\quad=\\bigg|-2p_{i}(\\mathbf{u})p_{j}(\\mathbf{u})p_{0}(\\mathbf{u})\\bigg|}\\\\ &{\\qquad\\quad\\leqslant2p_{i}(\\mathbf{u})p_{j}(\\mathbf{u}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 31}, {"type": "text", "text": "F Proof of Lemma 1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "In this section, we provide the proof of Lemma 1. First, we present the main proof of Lemma 1, followed by the proof of the technical lemma utilized within the main proof. ", "page_idx": 31}, {"type": "text", "text": "F.1 Main Proof of Lemma 1 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . The proof is similar to the analysis presented in Zhang and Sugiyama [54]. However, their MNL choice model is constructed using a shared context $x_{t}$ and varying parameters across the choices $\\mathbf{w}_{1}^{\\star},\\dots,\\mathbf{w}_{K}^{\\star}$ , whereas our approach considers an MNL choice model that shares the parameter $\\mathbf{w}^{\\star}$ across the choices and has varying contexts for each item in the assortment $S$ , $x_{t1},\\ldots,x_{t i_{|S|}}$ . Moreover, Zhang and Sugiyama [54] only consider a fixed assortment size, whereas we consider a more general setting where the assortment size can vary in each round $t$ . We denote $K_{t}=|S_{t}|$ in the proof of Lemma 1. Note that $K_{t}\\leqslant K$ for all $t\\geqslant1$ . ", "page_idx": 31}, {"type": "text", "text": "Lemma F.1. Let the update rule be ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathbf{w}_{t+1}=\\underset{\\mathbf{w}\\in\\mathcal{W}}{\\mathrm{argmin}}\\,\\widetilde{\\ell}_{t}(\\mathbf{w})+\\frac{1}{2\\eta}\\|\\mathbf{w}-\\mathbf{w}_{t}\\|_{H_{t}}^{2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\begin{array}{r}{\\widetilde{\\ell}_{t}(\\mathbf{w})=\\ell_{t}(\\mathbf{w}_{t})\\!+\\!\\langle\\mathbf{w}\\!-\\!\\mathbf{w}_{t},\\nabla\\ell_{t}(\\mathbf{w}_{t})\\rangle\\!+\\!\\frac{1}{2}\\|\\mathbf{w}\\!-\\!\\mathbf{w}_{t}\\|_{\\nabla^{2}\\ell_{t}(\\mathbf{w}_{t})}^{2}}\\end{array}$ and $\\begin{array}{r}{H_{t}=\\lambda\\mathbf{I}_{d}\\!+\\!\\sum_{s=1}^{t-1}\\mathcal{G}_{s}(\\mathbf{w}_{s+1})}\\end{array}$ . Let $\\begin{array}{r}{\\eta=\\frac{1}{2}\\log(K+1)+2}\\end{array}$ and $\\lambda>0,$ . Then, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|{\\mathbf{w}}_{t+1}-{\\mathbf{w}}^{\\star}\\|_{H_{t+1}}^{2}\\leqslant2\\eta\\left(\\sum_{s=1}^{t}\\ell_{s}({\\mathbf{w}}^{\\star})-\\sum_{s=1}^{t}\\ell_{s}({\\mathbf{w}}_{s+1})\\right)+4\\lambda+12\\sqrt{2}\\eta\\sum_{s=1}^{t}\\left\\|{\\mathbf{w}}_{s+1}-{\\mathbf{w}}_{s}\\right\\|_{2}^{2}}\\\\ {\\displaystyle-\\sum_{i=1}^{t}\\left\\|{\\mathbf{w}}_{s+1}-{\\mathbf{w}}_{s}\\right\\|_{H_{s}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "We first bound the first term in (F.1). For simplicity, we define the softmax function at round $t$ $\\pmb{\\sigma}_{t}(\\mathbf{z}):\\mathbb{R}^{K_{t}}\\rightarrow\\mathbb{R}^{K_{t}}$ as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n[\\pmb{\\sigma}_{t}(\\mathbf{z})]_{i}=\\frac{\\exp([\\mathbf{z}]_{i})}{v_{0}+\\sum_{k=1}^{K_{t}}\\exp([\\mathbf{z}]_{k})},\\quad\\forall i\\in[K_{t}],\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $[\\cdot]_{i}$ denotes $i$ \u2019th element of the the input vector. We denote the probability of choosing the outside option as $\\begin{array}{r}{[\\sigma_{t}(\\mathbf{z})]_{0}=\\frac{v_{0}}{v_{0}+\\sum_{k=1}^{K_{t}}\\exp([\\mathbf{z}]_{k})}}\\end{array}$ . Although $[{\\pmb\\sigma}_{t}({\\bf z})]_{0}$ is not the output of the softmax function ${\\boldsymbol{\\sigma}}_{t}(\\mathbf{z})$ , we represent it in \u0159a form similar to that in (F.2) for simplicity. Then, the user choice model in (1) can be equivalently expressed as $p_{t}(i|S_{t},\\mathbf{w})\\,=\\,\\bigl[\\pmb{\\sigma}_{t}\\left((x_{t j}^{\\mp}\\mathbf{w})_{j\\in S_{t}}\\right)\\bigr]_{i}$ for all $i\\in[K_{t}]$ and $p_{t}(0|S_{t},\\mathbf{w})\\,=\\,\\left[\\pmb{\\sigma}_{t}\\left((x_{t j}^{\\top}\\mathbf{w})_{j\\in S_{t}}\\right)\\right]_{0}$ . Furthermore, the loss function (2) can also be written as $\\begin{array}{r}{\\ell({\\mathbf z}_{t},{\\mathbf y}_{t})=\\sum_{k=0}^{K_{t}}\\mathbf{1}\\left\\{y_{t i}=1\\right\\}\\cdot\\log\\Big(\\frac{1}{[\\sigma_{t}({\\mathbf z}_{t})]_{k}}\\Big).}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Define a pseudo-inverse function of $\\pmb{\\sigma}_{t}(\\cdot)$ as $\\pmb{\\sigma}_{t}^{+}\\quad:\\quad\\mathbb{R}^{K_{t}}\\quad\\rightarrow\\quad\\mathbb{R}^{K_{t}}$ , where $\\begin{array}{r l}{[\\pmb{\\sigma}_{t}^{+}(\\mathbf{q})]_{i}}&{{}=}\\end{array}$ \u201c $\\log\\left(q_{i}/(1-\\|\\mathbf{q}\\|_{1})\\right)$ for any $\\mathbf{q}\\;\\in\\;\\{\\mathbf{p}\\;\\in\\;[0,\\dot{1}]^{K_{t}}\\;\\;|\\;\\;\\|\\mathbf{p}\\|_{1}\\;<\\;1\\}$ . Then, inspired by the previous studies on binary logistic bandit [23], we decompose the regret into two terms by introducing an intermediate term. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}^{\\star})-\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}_{s+1})=\\underbrace{\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}^{\\star})-\\sum_{s=1}^{t}\\ell(\\tilde{\\mathbf{z}}_{s},\\mathbf{y}_{s})}_{(a)}+\\underbrace{\\sum_{s=1}^{t}\\ell(\\tilde{\\mathbf{z}}_{s},\\mathbf{y}_{s})-\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}_{s+1})}_{(b)},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\tilde{\\mathbf{z}}_{s}\\,:=\\,\\pmb{\\sigma}_{s}^{+}\\left(\\mathbb{E}_{\\mathbf{w}\\sim P_{s}}\\left[\\pmb{\\sigma}_{s}\\left((x_{s j}^{\\top}\\mathbf{w})_{j\\in S_{s}}\\right)\\right]\\right)$ , and $P_{s}\\,:=\\,\\mathcal{N}(\\mathbf{w}_{s},(1+c H_{s}^{-1}))$ is the Gaussian distribution with m\\`ean ${\\bf w}_{s}$ \u201cand \\`covariance \u02d8m\u2030a\u02d8trix $c H_{s}^{-1}$ , where $c>0$ is a positive constant to be specified later. We first show that the term $(a)$ is bounded by ${\\mathcal{O}}\\left(\\log K\\log t\\right)$ with high probability. ", "page_idx": 32}, {"type": "text", "text": "Lemma F.2. Let $\\delta\\in(0,1]$ and $\\lambda\\geqslant1$ . Under Assumptions $^{\\,l}$ , for all $t\\in[T]$ , with probability at least $1-\\delta$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}^{\\star})-\\sum_{s=1}^{t}\\ell(\\tilde{\\mathbf{z}}_{s},\\mathbf{y}_{s})}\\\\ &{\\ll(3\\log(1+(K+1)t)+3)\\left(\\frac{17}{16}\\lambda+2\\sqrt{\\lambda}\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)+16\\left(\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)^{2}\\right)+2.1}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Furthermore, we can bound the term $(b)$ by the following lemma. ", "page_idx": 32}, {"type": "text", "text": "Lemma F.3. For any $c>0$ , let $\\lambda\\geqslant\\operatorname*{max}\\{2,72c d\\}$ . Then, under Assumption $^{\\,l}$ , for all $t\\geqslant1$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left(\\ell(\\tilde{\\mathbf{z}}_{s},\\mathbf{y}_{s})-\\ell_{s}(\\mathbf{w}_{s+1})\\right)\\leqslant\\frac{1}{2c}\\sum_{s=1}^{t}\\|\\mathbf{w}_{s}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}+\\sqrt{6}c d\\log\\left(1+\\frac{t+1}{2\\lambda}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, we are ready to prove the Lemma 1. By combining Lemma F.1, Lemma F.2, and Lemma F.3, we derive that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{w}_{t+1}-\\mathbf{w}^{*}\\|_{H_{1}+1}^{2}}\\\\ &{\\leqslant2\\eta\\Bigg[(3\\log(1+(K+1)t)+3)\\left(\\frac{17}{16}\\lambda+2\\sqrt{\\lambda}\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)+16\\left(\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)^{2}\\right)}\\\\ &{+\\;2+\\sqrt{6}c d\\log\\left(1+\\frac{t+1}{2\\lambda}\\right)\\Bigg]+4\\lambda+12\\sqrt{2}\\eta\\underbrace{t}_{s=1}^{t}\\|\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\|_{2}^{2}+\\bigg(\\frac{\\eta}{c}-1\\bigg)\\displaystyle\\sum_{i=1}^{t}\\|\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\|_{H,}^{2}}\\\\ &{\\leqslant2\\eta\\Bigg[(3\\log(1+(K+1)t)+3)\\left(\\frac{17}{16}\\lambda+2\\sqrt{\\lambda}\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)+16\\left(\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)^{2}\\right)}\\\\ &{+\\;2+\\sqrt{6}c d\\log\\left(1+\\frac{t+1}{2\\lambda}\\right)\\Bigg]+4\\lambda=:\\beta_{t+1}(\\delta)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the second inequality holds because by setting $c=7\\eta/6$ and $\\lambda\\geqslant\\operatorname*{max}\\{84\\sqrt{2}\\eta,84d\\eta\\}$ , we obtain: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle12\\sqrt{2}\\eta\\sum_{s=1}^{t}\\lVert\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\rVert_{2}^{2}+\\left(\\frac{\\eta}{c}-1\\right)\\sum_{i=1}^{t}\\left\\lVert\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\right\\rVert_{H_{s}}^{2}}\\\\ {\\displaystyle=12\\sqrt{2}\\eta\\sum_{s=1}^{t}\\lVert\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\rVert_{2}^{2}-\\frac{1}{7}\\displaystyle\\sum_{i=1}^{t}\\left\\lVert\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\right\\rVert_{H_{s}}^{2}}\\\\ {\\displaystyle\\leqslant\\left(12\\sqrt{2}\\eta-\\frac{\\lambda}{7}\\right)\\displaystyle\\sum_{s=1}^{t}\\left\\lVert\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\right\\rVert_{2}^{2}\\leqslant0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the first inequality holds since $H_{s}\\succeq\\lambda\\mathbf{I}_{d}$ . By setting $\\begin{array}{r}{\\eta=\\frac{1}{2}\\log(K+1)+2}\\end{array}$ and $\\lambda=84\\sqrt{2}d\\eta$ , we derive that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}_{t}-\\mathbf{w}^{\\star}\\|_{H_{t}}\\leqslant\\beta_{t}(\\delta)=\\mathcal{O}\\left(\\sqrt{d}\\log t\\log K\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which conclude the proof of Lemma 1. ", "page_idx": 33}, {"type": "text", "text": "Remark F.1. If the boundedness assumption on the parameter is relaxed to $\\|\\mathbf{w}\\|_{2}\\,\\leqslant\\,B$ , then $\\beta_{t}(\\delta)=\\mathcal{O}\\left(B\\sqrt{d}\\log t\\log K\\right)$ . ", "page_idx": 33}, {"type": "text", "text": "F.2 Proofs of Lemmas for Lemma 1 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "F.2.1 Proof of Lemma F.1 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Proof of Lemma $F.l$ . Let $\\begin{array}{r}{\\widetilde{\\ell}_{s}(\\mathbf{w})=\\ell_{s}(\\mathbf{w}_{s})+\\langle\\mathbf{w}-\\mathbf{w}_{s},\\nabla\\ell_{s}(\\mathbf{w}_{s})\\rangle+\\frac{1}{2}\\|\\mathbf{w}-\\mathbf{w}_{s}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s})}^{2}}\\end{array}$ be a secondorder approximation of the original function $\\ell_{s}(\\mathbf{w})$ at the point ${\\bf w}_{s}$ . The update rule (3) can also be expressed as ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{w}_{s+1}=\\underset{\\mathbf{w}\\in\\mathcal{W}}{\\mathrm{argmin}}\\,\\widetilde{\\ell}_{s}(\\mathbf{w})+\\frac{1}{2\\eta}\\|\\mathbf{w}-\\mathbf{w}_{s}\\|_{H_{s}}^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, by Lemma F.4, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\langle\\nabla\\widetilde{\\ell}_{s}(\\mathbf{w}_{s+1}),\\mathbf{w}_{s+1}-\\mathbf{w}^{\\star}\\rangle\\leqslant\\frac{1}{2\\eta}\\left(\\|\\mathbf{w}_{s}-\\mathbf{w}^{\\star}\\|_{H_{s}}^{2}-\\|\\mathbf{w}_{s+1}-\\mathbf{w}^{\\star}\\|_{H_{s}}^{2}-\\|\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\|_{H_{s}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "To utilize Lemma F.6, we can rewrite the loss function as $\\ell\\left((x_{s i}^{\\top}\\mathbf{w})_{i\\in S_{s}},\\mathbf{y}_{s}\\right)=\\ell_{s}(\\mathbf{w})$ . Consequently, according to Lemma F.6, it follows that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\ell_{s}(\\mathbf{w}_{s+1})-\\ell_{s}(\\mathbf{w}^{\\star})\\leqslant\\langle\\nabla\\ell_{s}(\\mathbf{w}_{s+1}),\\mathbf{w}_{s+1}-\\mathbf{w}^{\\star}\\rangle-\\frac{1}{\\zeta}\\|\\mathbf{w}_{s+1}-\\mathbf{w}^{\\star}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{2},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\zeta=\\log(K+1)+4$ . Then, by combining (F.4) and (F.5), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{s}(\\mathbf{w}_{s+1})-\\ell_{s}(\\mathbf{w}^{\\star})\\leqslant\\langle\\nabla\\ell_{s}(\\mathbf{w}_{s+1})-\\nabla\\tilde{\\ell}_{s}(\\mathbf{w}_{s+1}),\\mathbf{w}_{s+1}-\\mathbf{w}^{\\star}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{\\zeta}\\left(\\|\\mathbf{w}_{s}-\\mathbf{w}^{\\star}\\|_{H_{s}}^{2}-\\|\\mathbf{w}_{s+1}-\\mathbf{w}^{\\star}\\|_{H_{s+1}}^{2}-\\|\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\|_{H_{s}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In above, we can further bound the first term of the right-hand side as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla\\ell_{s}(\\mathbf w_{s+1})-\\nabla\\tilde{\\ell}_{s}(\\mathbf w_{s+1}),\\mathbf w_{s+1}-\\mathbf w^{\\star}\\rangle}\\\\ &{\\qquad\\qquad=\\langle\\nabla\\ell_{s}(\\mathbf w_{s+1})-\\nabla\\ell_{s}\\big(\\mathbf w_{s}\\big)-\\nabla^{2}\\ell_{s}\\big(\\mathbf w_{s}\\big)\\big(\\mathbf w_{s+1}-\\mathbf w_{s}\\big),\\mathbf w_{s+1}-\\mathbf w^{\\star}\\rangle}\\\\ &{\\qquad\\qquad=\\langle D^{3}\\ell_{s}(\\xi_{s+1})\\big[\\mathbf w_{s+1}-\\mathbf w_{s}\\big]\\big(\\mathbf w_{s+1}-\\mathbf w_{s}\\big),\\mathbf w_{s+1}-\\mathbf w^{\\star}\\rangle}\\\\ &{\\qquad\\qquad\\leqslant3\\sqrt{2}\\|\\mathbf w_{s+1}-\\mathbf w^{\\star}\\|_{2}\\|\\mathbf w_{s+1}-\\mathbf w_{s}\\|_{\\nabla^{2}\\ell_{s}(\\xi_{s+1})}^{2}}\\\\ &{\\qquad\\qquad\\leqslant6\\sqrt{2}\\|\\mathbf w_{s+1}-\\mathbf w_{s}\\|_{\\nabla^{2}\\ell_{s}(\\xi_{s+1})}^{2}}\\\\ &{\\qquad\\qquad\\leqslant6\\sqrt{2}\\|\\mathbf w_{s+1}-\\mathbf w_{s}\\|_{2}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where in the second equality, we apply the Taylor expansion by introducing $\\xi_{s+1}$ , a convex combination of ${\\bf w}_{s+1}$ and ${\\bf w}_{s}$ . The first inequality follows from Lemma C.1 and Proposition C.1, the second inequality holds by Assumption 1, and the last inequality holds because ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla^{2}\\ell_{s}(\\xi_{s+1})=\\mathcal{G}_{s}(\\xi_{s+1})}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\xi_{s+1})x_{s i}x_{s i}^{\\top}-\\displaystyle\\sum_{i\\in S_{s}}\\displaystyle\\sum_{j\\in S_{s}}p_{s}(i|S_{s},\\xi_{s+1})p_{s}(j|S_{s},\\xi_{s+1})x_{s i}x_{s j}^{\\top}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{i\\in S_{s}\\times\\{0\\}}p_{s}(i|S_{s},\\xi_{s+1})x_{s i}x_{s i}^{\\top}-\\displaystyle\\sum_{i\\in S_{s}\\times\\{0\\}}\\displaystyle\\sum_{j\\in S_{s}\\times\\{0\\}}p_{s}(i|S_{s},\\xi_{s+1})p_{s}(j|S_{s},\\xi_{s+1})x_{s i}x_{s j}^{\\top}}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}_{i\\sim p_{s}(\\cdot|S,\\xi_{s+1})}\\left[x_{s i}x_{s i}^{\\top}\\right]-\\mathbb{E}_{i\\sim p_{s}(\\cdot|S_{s},\\xi_{s+1})}\\left[x_{s i}\\right]\\left(\\mathbb{E}_{i\\sim p_{s}(\\cdot|S_{s},\\xi_{s+1})}\\left[x_{s i}\\right]\\right)^{\\top}}\\\\ &{\\quad\\quad\\quad\\leq\\mathbb{E}_{i\\sim p_{s}(\\cdot|S_{s},\\xi_{s+1})}\\left[x_{s i}x_{s i}^{\\top}\\right]\\leq\\mathbf{I}_{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the third equality holds by setting $x_{s0}=\\mathbf{0}$ for all $s\\geqslant1$ . ", "page_idx": 34}, {"type": "text", "text": "Now, by taking the summation over $s$ and rearranging the terms, we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\mathbf{w}_{t+1}-\\mathbf{w}^{\\star}\\right\\|_{H_{t+1}}^{2}}\\\\ {\\displaystyle\\leqslant\\zeta\\left(\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}^{\\star})-\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}_{s+1})\\right)+\\left\\|\\mathbf{w}_{1}-\\mathbf{w}^{\\star}\\right\\|_{H_{1}}^{2}+6\\sqrt{2}\\zeta\\displaystyle\\sum_{s=1}^{t}\\left\\|\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\right\\|_{2}^{2}}\\\\ {\\displaystyle-\\sum_{s=1}^{t}\\left\\|\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\right\\|_{H_{s}}^{2}}\\\\ {\\displaystyle\\leqslant\\zeta\\left(\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}^{\\star})-\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}_{s+1})\\right)+4\\lambda+6\\sqrt{2}\\zeta\\displaystyle\\sum_{s=1}^{t}\\left\\|\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\right\\|_{2}^{2}-\\sum_{s=1}^{t}\\left\\|\\mathbf{w}_{s+1}-\\mathbf{w}_{s}\\right\\|_{H_{s}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where the last inequality holds since $\\|\\mathbf{w}_{1}-\\mathbf{w}^{\\star}\\|_{H_{1}}^{2}\\leqslant\\lambda\\|\\mathbf{w}_{1}-\\mathbf{w}^{\\star}\\|_{2}^{2}\\leqslant4\\lambda$ . Plugging in $\\zeta=2\\eta$ , we conclude the proof. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "F.2.2 Proof of Lemma F.2 ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Proof of Lemma $F.2$ . Since the norm of $\\tilde{\\textbf{z}}_{s}~=~\\sigma_{s}^{+}\\left(\\mathbb{E}_{\\mathbf{w}\\sim P_{s}}\\left[\\sigma_{s}\\left((x_{s j}^{\\top}\\mathbf{w})_{j\\in S_{s}}\\right)\\right]\\right)$ is unbounded in general, as suggested by Foster et al. [25],\\` we us\u201ce th\\`e smoothed\u02d8 \u2030\u02d8version $\\tilde{\\mathbf{z}}_{s}^{\\mu}\\quad=$ \u201c $\\pmb{\\sigma}_{s}^{+}\\,\\overline{{(\\mathrm{smooth}_{s}^{\\mu}\\,\\mathbb{E}_{\\mathbf{w}\\sim P_{s}}^{\\mathbb{E}}\\left[\\pmb{\\sigma}_{s}\\left((x_{s j}^{\\dagger}\\mathbf{w})_{j\\in S_{s}}\\right)\\right]}})$ as an intermediate-term, where the smooth function is defined by $\\mathrm{smooth}_{s}^{\\mu}(\\mathbf{q})=(1-\\mu)\\mathbf{q}+\\mu\\mathbf{1}/(K_{s}+1)$ , where $\\mathbf{1}\\in\\mathbb{R}^{K_{s}}$ is an all one vector. ", "page_idx": 34}, {"type": "text", "text": "Note that $\\tilde{\\mathbf{z}}_{s}^{\\mu}=\\pmb{\\sigma}_{s}^{+}(\\mathrm{smooth}_{s}^{\\mu}(\\pmb{\\sigma}_{s}(\\tilde{\\mathbf{z}}_{s})))$ by the definition of the pseudo inverse function $\\pmb{\\sigma}_{s}^{+}$ such that $\\pmb{\\sigma}_{s}^{+}(\\pmb{\\sigma}_{s}(\\mathbf{q})\\widetilde{)}=\\mathbf{q}$ for any $\\mathbf{q}\\in\\{\\mathbf{p}\\in[0,1]^{K_{s}}\\mid\\|\\mathbf{p}\\|_{1}<1\\}$ . Then, by Lemma F.7, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\ell(\\tilde{\\mathbf{z}}_{s}^{\\mu},\\mathbf{y}_{s})-\\sum_{s=1}^{t}\\ell(\\tilde{\\mathbf{z}}_{s},\\mathbf{y}_{s})\\leqslant2\\mu t,\\quad\\mathrm{and}\\quad\\|\\tilde{\\mathbf{z}}_{s}^{\\mu}\\|_{\\infty}\\leqslant\\log(1+(K+1)/\\mu).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Hence, to prove the lemma, we need only to bound the gap between the loss of $\\mathbf{w}^{\\star}$ and $\\tilde{\\mathbf{z}}_{s}^{\\mu}$ . To enhance clarity in our presentation, let $\\ell({\\mathbf{z}}_{s}^{\\star},{\\mathbf{y}}_{s})=\\ell_{s}({\\mathbf{w}}^{\\star})$ , where $\\mathbf{z}_{s}^{\\star}=\\left(x_{s j}^{\\top}\\mathbf{w}^{\\star}\\right)_{j\\in S_{s}}\\in\\mathbb{R}^{K_{s}}$ . Then, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}^{\\star})-\\displaystyle\\sum_{s=1}^{t}\\ell(\\tilde{\\mathbf{z}}_{s}^{\\mu},\\mathbf{y}_{s})=\\displaystyle\\sum_{s=1}^{t}\\ell(\\mathbf{z}_{s}^{\\star},\\mathbf{y}_{s})-\\displaystyle\\sum_{s=1}^{t}\\ell(\\tilde{\\mathbf{z}}_{s}^{\\mu},\\mathbf{y}_{s})}\\\\ &{\\displaystyle\\leqslant\\sum_{s=1}^{t}\\langle\\nabla_{z}\\ell(\\mathbf{z}_{s}^{\\star},\\mathbf{y}_{s}),\\mathbf{z}_{s}^{\\star}-\\tilde{\\mathbf{z}}_{s}^{\\mu}\\rangle-\\displaystyle\\sum_{s=1}^{t}\\frac{1}{c_{\\mu}}\\|\\mathbf{z}_{s}^{\\star}-\\tilde{\\mathbf{z}}_{s}^{\\mu}\\|_{\\nabla_{z}^{2}\\ell(\\mathbf{z}_{s}^{\\star},\\mathbf{y}_{s})}^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{s=1}^{t}\\langle\\sigma_{s}(\\mathbf{z}_{s}^{\\star})-\\mathbf{y}_{s},\\mathbf{z}_{s}^{\\star}-\\tilde{\\mathbf{z}}_{s}^{\\mu}\\rangle-\\displaystyle\\sum_{s=1}^{t}\\frac{1}{c_{\\mu}}\\|\\mathbf{z}_{s}^{\\star}-\\tilde{\\mathbf{z}}_{s}^{\\mu}\\|_{\\nabla\\sigma_{s}(\\mathbf{z}_{s}^{\\star})}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $c_{\\mu}=\\log(K+1)+2\\log(1+(K+1)/\\mu)+2$ , the inequality holds by Lemma F.6, and the last equality holds by a direct calculation of the first order and Hessian of the logistic loss as follows: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\nabla_{z}\\ell(\\mathbf{z}_{s},\\mathbf{y}_{s})=\\sigma_{s}(\\mathbf{z}_{s})-\\mathbf{y}_{s},\\quad\\nabla_{z}^{2}\\ell(\\mathbf{z}_{s},\\mathbf{y}_{s})=\\mathrm{diag}(\\sigma_{s}(\\mathbf{z}_{s}))-\\sigma_{s}(\\mathbf{z}_{s})\\sigma_{s}(\\mathbf{z}_{s})^{\\top}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We first bound the first term of the right-hand side. Define $\\mathbf{d}_{s}=(\\mathbf{z}_{s}^{\\star}-\\tilde{\\mathbf{z}}_{s}^{\\mu})/(c_{\\mu}+1)$ . Let $\\mathbf{d}_{s}^{\\prime}$ be ${\\bf d}_{s}$ extended with zero padding. Specifically, we define $\\mathbf{d}_{s}^{\\prime}=[\\mathbf{d}_{s}^{\\top},0,\\dots,0]^{\\top}\\in\\mathbb{R}^{K}$ , where the zeros are appended to increase the dimension of ${\\bf d}_{s}$ to $K$ . Similarly, we also extend $\\pmb{\\sigma}_{s}(\\mathbf{z}_{s}^{\\star})-\\mathbf{y}_{s}$ with zero padding and define $\\pmb{\\varepsilon}_{s}=[(\\pmb{\\sigma}_{s}(\\mathbf{z}_{s}^{\\star})-\\mathbf{y}_{s})^{\\top},0,\\dots,0]^{\\top}\\in\\mathbb{R}^{K}$ . ", "page_idx": 35}, {"type": "text", "text": "Then, one can easily verify that $\\|\\mathbf{d}_{s}^{\\prime}\\|_{\\infty}\\leqslant1$ since $\\|\\mathbf{z}_{s}^{\\star}\\|_{\\infty}\\leqslant\\operatorname*{max}_{i\\in S_{s}}\\|x_{x i}\\|_{2}\\|\\mathbf{w}^{\\star}\\|_{2}\\leqslant1$ and $\\|\\widetilde{\\mathbf z}_{s}^{\\mu}\\|_{\\infty}\\leqslant$ $\\log(1+(K+1)/\\dot{\\mu})$ . On the other hand, $\\mathbf{d}_{s}^{\\prime}$ is ${\\mathcal{F}}_{s}$ -measurable since $\\mathbf{z}_{s}^{\\star}$ and $\\tilde{\\mathbf{z}}_{s}^{\\mu}$ are independent of $\\mathbf{y}_{s}$ . Moreover, we have $\\begin{array}{r}{\\|\\mathbf{d}_{s}^{\\prime}\\|_{\\mathbb{E}[\\varepsilon_{s}\\varepsilon_{s}^{\\top}\\vert\\mathcal{F}_{s}]}^{2}=\\|\\mathbf{d}_{s}\\|_{\\mathbb{E}[(\\sigma_{s}(\\mathbf{z}_{s}^{\\star})-\\mathbf{y}_{s})(\\sigma_{s}(\\mathbf{z}_{s}^{\\star})-\\mathbf{y}_{s})^{\\top}\\vert\\mathcal{F}_{s}]}^{2}=\\|\\mathbf{d}_{s}\\|_{\\nabla\\sigma_{s}(\\mathbf{z}_{s}^{\\star})}^{2}}\\end{array}$ and $\\|\\pmb{\\sigma}_{s}(\\mathbf{z}_{s}^{\\star})-\\mathbf{y}_{s}\\|_{1}\\leqslant2$ . Thus, by Lemma F.5, with probability at least $1-\\delta$ , for any $t\\geqslant1$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\langle\\sigma_{s}(\\mathbf{z}_{s}^{\\star})-\\mathbf{y}_{s},\\mathbf{z}_{s}^{\\star}-\\tilde{\\mathbf{z}}_{s}^{\\mu}\\rangle=(c_{\\mu}+1)\\displaystyle\\sum_{s=1}^{t}\\langle\\sigma_{s}(\\mathbf{z}_{s}^{\\star})-\\mathbf{y}_{s},\\mathbf{d}_{s}\\rangle}\\\\ &{=\\displaystyle(c_{\\mu}+1)\\displaystyle\\sum_{s=1}^{t}\\langle\\varepsilon_{s},\\mathbf{d}_{s}^{\\prime}\\rangle}\\\\ &{\\displaystyle\\leqslant(c_{\\mu}+1)\\sqrt{\\lambda+\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{d}_{s}\\|_{\\nabla\\sigma_{s}(\\mathbf{z}_{s}^{\\star})}^{2}}\\left(\\frac{\\sqrt{\\lambda}}{4}+\\displaystyle\\frac{4}{\\sqrt{\\lambda}}\\log\\cdot\\left(\\frac{2\\sqrt{1+\\frac{1}{\\lambda}\\sum_{s=1}^{t}\\|\\mathbf{d}_{s}\\|_{\\nabla\\sigma_{s}(\\mathbf{z}_{s}^{\\star})}^{2}}}{\\delta}\\right)\\right)}\\\\ &{\\displaystyle\\leqslant(c_{\\mu}+1)\\sqrt{\\lambda+\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{d}_{s}\\|_{\\nabla\\sigma_{s}(\\mathbf{z}_{s}^{\\star})}^{2}}\\cdot\\left(\\frac{\\sqrt{\\lambda}}{4}+4\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second inequality holds because $\\|\\mathbf{d}_{s}\\|_{\\nabla\\sigma_{s}(\\mathbf{z}_{s}^{\\star})}^{2}=\\mathbf{d}_{s}^{\\top}\\nabla\\pmb{\\sigma}_{s}(\\mathbf{z}_{s}^{\\star})\\mathbf{d}_{s}\\leqslant2$ and $\\lambda\\geqslant1$ . Then, combining (F.8) and (F.7), and rearranging the terms, we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=1}^{r}(s^{\\star},\\log^{\\star})-\\sum_{i=1}^{r}(\\xi_{s}^{\\star},\\mathfrak{p}_{s})}}\\\\ &{\\leq(c_{s}+1)\\sqrt{\\lambda+\\displaystyle\\sum_{i=1}^{r}\\prod_{i=1}^{i}\\|\\mathfrak{A}\\|_{\\mathrm{e}^{s},(\\kappa)}^{2}}\\cdot\\left(\\displaystyle\\frac{\\sqrt{\\lambda}}{4}+4\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)-\\sum_{i=1}^{r}\\displaystyle\\frac{1}{c_{s}}\\|\\mathfrak{p}_{s}^{\\star}-\\bar{z}_{s}^{\\star}\\|_{\\mathrm{e}^{s},(\\kappa)}^{2}}\\\\ &{\\leq(c_{s}+1)\\sqrt{\\lambda+\\displaystyle\\sum_{i=1}^{r}\\|\\mathfrak{A}\\|_{\\mathrm{e}^{s},(\\kappa)}^{2}}\\cdot\\left(\\displaystyle\\frac{\\sqrt{\\lambda}}{4}+4\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)-(c_{s}+1)\\sum_{i=1}^{r}\\|\\mathfrak{A}\\|_{\\mathrm{e}^{s},(\\kappa)}^{2}}\\\\ &{\\lesssim(c_{s}+1)\\left(\\lambda+\\displaystyle\\sum_{i=1}^{r}\\|\\mathfrak{A}\\|_{\\mathrm{e}^{s},(\\kappa)}^{2}\\right)+(c_{s}+1)\\left(\\displaystyle\\frac{\\sqrt{\\lambda}}{4}+4\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)^{2}}\\\\ &{-(c_{s}+1)\\displaystyle\\sum_{i=1}^{r}\\|\\mathfrak{A}\\|_{\\mathrm{e}^{s},(\\kappa)}^{2}}\\\\ &{=(c_{s}+1)\\left(\\displaystyle\\frac{1}{16}+2\\sqrt{\\lambda\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)}+16\\left(\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)^{2}\\right),\\qquad\\qquad\\mathrm{(f.S}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the third inequality holds due to the AM-GM inequality. Finally, combining (F.6) and (F.9), by setting $\\mu=1/t$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\left(\\ell_{s}(\\mathbf{w}^{\\star})-\\ell(\\tilde{\\mathbf{z}}_{s},\\mathbf{y}_{s})\\right)}\\\\ &{\\leqslant\\displaystyle\\left(c_{\\mu}+1\\right)\\left(\\frac{17}{16}\\lambda+2\\sqrt{\\lambda}\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)+16\\left(\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)^{2}\\right)+2\\mu t}\\\\ &{\\leqslant\\displaystyle\\left(3\\log(1+(K+1)t)+3\\right)\\left(\\frac{17}{16}\\lambda+2\\sqrt{\\lambda}\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)+16\\left(\\log\\left(\\frac{2\\sqrt{1+2t}}{\\delta}\\right)\\right)^{2}\\right)+2}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the last inequality holds by the definition of $c_{\\mu}=\\log(K+1)+2\\log(1+(K+1)/\\mu)+2.$ This concludes the proof. \u53e3 ", "page_idx": 35}, {"type": "text", "text": "F.2.3 Proof of Lemma F.3 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof of Lemma $F.3$ . The proof with an observation from Proposition 2 in Foster et al. [25], which notes that $\\tilde{\\mathbf{z}}_{s}$ is an aggregation forecaster for the logistic function. Hence, it satisfies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\ell(\\tilde{\\mathbf{z}}_{s},\\mathbf{y}_{s})\\leqslant-\\log\\left(\\mathbb{E}_{\\mathbf{w}\\sim P_{s}}\\left[e^{-\\ell_{s}(\\mathbf{w})}\\right]\\right)=-\\log\\left(\\frac{1}{Z_{s}}\\int_{\\mathbb{R}^{d}}e^{-L_{s}(\\mathbf{w})}\\mathrm{d}\\mathbf{w}\\right),\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\begin{array}{r}{L_{s}(\\mathbf{w}):=\\ell_{s}(\\mathbf{w})+\\frac{1}{2c}\\|\\mathbf{w}-\\mathbf{w}_{s}\\|_{H_{s}}^{2}}\\end{array}$ and $Z_{s}:=\\sqrt{(2\\pi)^{d}c|H_{s}^{-1}|}$ . ", "page_idx": 36}, {"type": "text", "text": "Then, by the quadratic approximation, we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tilde{L}_{s}(\\mathbf{w})=L_{s}(\\mathbf{w}_{s+1})+\\langle\\nabla L_{s}(\\mathbf{w}_{s+1}),\\mathbf{w}-\\mathbf{w}_{s+1}\\rangle+\\frac{1}{2c}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Applying Lemma F.8 and considering the fact that $\\ell_{s}$ is $3\\sqrt{2}$ -self-concordant-like function by Proposition C.1, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\nL_{s}(\\mathbf{w})\\leqslant\\tilde{L}_{s}(\\mathbf{w})+e^{18\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla\\ell_{s}(\\mathbf{w}_{s+1})}^{2}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We define the function $\\tilde{f}_{s}:\\mathcal{W}\\to\\mathbb{R}$ as ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\widetilde{f}_{s+1}(\\mathbf{w})=\\exp\\left(-\\frac{1}{2c}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}-e^{18\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Then, we can establish a lower bound for the expectation in (F.10) as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}\\sim P_{s}}\\left[e^{-\\ell_{s}(\\mathbf{w})}\\right]=\\displaystyle\\frac{1}{Z_{s}}\\int_{\\mathbb{R}^{d}}\\exp(-L_{s}(\\mathbf{w}))\\mathrm{d}\\mathbf{w}}\\\\ &{\\phantom{\\sum}\\geqslant\\displaystyle\\frac{1}{Z_{s}}\\int_{\\mathbb{R}^{d}}\\exp(-\\tilde{L}_{s}(\\mathbf{w})-e^{18\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla\\ell_{s}(\\mathbf{w}_{s+1})}^{2})\\mathrm{d}\\mathbf{w}}\\\\ &{\\phantom{\\sum}=\\frac{\\exp(-L_{s}(\\mathbf{w}_{s+1}))}{Z_{s}}\\int_{\\mathbb{R}^{d}}\\tilde{f}_{s+1}(\\mathbf{w})\\cdot\\exp(-\\langle\\nabla L_{s}(\\mathbf{w}_{s+1}),\\mathbf{w}-\\mathbf{w}_{s+1}\\rangle)\\mathrm{d}\\mathbf{w}_{s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the first inequality holds by (F.12) and the last equality holds by (F.11). We define $\\tilde{Z}_{s+1}\\,=\\,\\int_{\\mathbb{R}^{d}}\\tilde{f}_{s+1}(\\bar{\\mathbf{w}_{})\\mathrm{d}\\mathbf{w}}^{}\\,\\leqslant\\,+\\infty$ . Moreover, we denote the distribution whose density function is $\\tilde{f}_{s+1}(\\mathbf{w})/\\tilde{Z}_{s+1}$ as $\\tilde{P}_{s+1}$ . Then, we can rewrite the equation (F.13) as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}\\sim P_{s}}\\left[e^{-\\ell_{s}(\\mathbf{w})}\\right]\\geqslant\\frac{\\exp\\left(-L_{s}\\left(\\mathbf{w}_{s+1}\\right)\\right)\\tilde{Z}_{s+1}}{Z_{s}}\\mathbb{E}_{\\mathbf{w}\\sim\\tilde{P}_{s+1}}\\left[\\exp\\left(-\\langle\\nabla L_{s}\\big(\\mathbf{w}_{s+1}\\big),\\mathbf{w}-\\mathbf{w}_{s+1}\\rangle\\right)\\right]}\\\\ &{\\qquad\\qquad\\geqslant\\frac{\\exp\\left(-L_{s}\\left(\\mathbf{w}_{s+1}\\right)\\right)\\tilde{Z}_{s+1}}{Z_{s}}\\exp\\left(-\\mathbb{E}_{\\mathbf{w}\\sim\\tilde{P}_{s+1}}\\left[\\langle\\nabla L_{s}\\big(\\mathbf{w}_{s+1}\\big),\\mathbf{w}-\\mathbf{w}_{s+1}\\rangle\\right]\\right)}\\\\ &{\\qquad\\qquad=\\frac{\\exp\\left(-L_{s}\\left(\\mathbf{w}_{s+1}\\right)\\right)\\tilde{Z}_{s+1}}{Z_{s}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the second inequality follows from Jensen\u2019s inequality, and the equality holds because $\\tilde{P}_{s+1}$ is symmetric around ${\\bf w}_{s+1}$ , thus $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{w}\\sim\\tilde{P}_{s+1}}\\left[\\langle\\nabla L_{s}(\\mathbf{w}_{s+1}\\bar{)},\\mathbf{w}-\\mathbf{w}_{s+1}\\rangle\\right]=0.}\\end{array}$ . ", "page_idx": 36}, {"type": "text", "text": "By plugging (F.14) into (F.10), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\ell(\\tilde{\\mathbf{z}}_{s},\\mathbf{y}_{s})\\leqslant L_{s}(\\mathbf{w}_{s+1})+\\log Z_{s}-\\log\\tilde{Z}_{s+1}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In the above, we can bound the last term, $-\\log{\\tilde{Z}_{s+1}}$ , by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\log\\tilde{Z}_{s+1}=-\\log\\left(\\displaystyle\\int_{\\mathbb R^{d}}\\exp\\left(-\\frac{1}{2c}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}-e^{\\mathrm{i}8\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{2}\\right)\\mathrm{d}\\mathbf{w}\\right)}\\\\ &{\\qquad\\qquad=-\\log\\left(\\widehat{Z}_{s+1}\\cdot\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[\\exp\\left(-e^{\\mathrm{i}8\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{2}\\right)\\right]\\right)}\\\\ &{\\qquad\\qquad\\leqslant-\\log\\widehat{Z}_{s+1}+\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[e^{\\mathrm{i}8\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{2}\\right]}\\\\ &{\\qquad=-\\log Z_{s}+\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[e^{\\mathrm{i}8\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{2}\\right],}\\end{array}\\quad\\mathrm{(F.16)}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\hat{P}_{s+1}\\,=\\,\\mathcal{N}(\\mathbf{w}_{s+1},c H_{s}^{-1})$ and $\\begin{array}{r}{\\widehat{Z}_{s+1}\\;=\\;\\int_{\\mathbb{R}^{d}}\\exp\\left(-\\frac{1}{2c}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}\\right)\\,\\mathrm{d}\\mathbf{w},}\\end{array}$ dw. In (F.16), the inequa lipty holds due to Jensen\u2019s ineq upality, and the last inequality is by the fact that $\\hat{Z}_{s+1}\\;=\\;$ $\\begin{array}{r}{\\int_{\\mathbb{R}^{d}}\\!\\exp\\left(-\\frac{1}{2c}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}\\right)\\mathrm{d}\\mathbf{w}=\\sqrt{(2\\pi)^{d}c|H_{s}^{-1}|}=Z_{s}.}\\end{array}$ ", "page_idx": 37}, {"type": "text", "text": "By applying the Cauchy-Schwarz inequality, we can further bound the second term on the right-hand side of (F.16) by ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[e^{18\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}}^{2}(\\mathbf{w}_{s+1})\\right]}\\\\ &{\\qquad\\qquad\\leqslant\\underbrace{\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[e^{36\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\right]}}_{\\mathrm{(a)}\\,-1}\\underbrace{\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}}^{4}(\\mathbf{w}_{s+1})\\right]}}_{\\mathrm{(a)}\\,-2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Note that, since $\\hat{P}_{s+1}=\\mathcal{N}(\\mathbf{w}_{s+1},c H_{s}^{-1})$ , there exist orthogonal bases $\\mathbf{e}_{1},\\ldots,\\mathbf{e}_{d}\\in\\mathbb{R}^{d}$ such that $\\mathbf{w}-\\mathbf{w}_{s+1}$ followps the same distribution as ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{d}\\sqrt{c\\lambda_{j}\\left(H_{s}^{-1}\\right)}X_{j}\\mathbf{e}_{j},\\quad\\mathrm{where}\\;X_{j}\\stackrel{i.i.d.}{\\sim}\\mathcal{N}(0,1),\\forall j\\in[d],\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and $\\lambda_{j}\\left(H_{s}^{-1}\\right)$ denotes the $j$ -th largest eigenvalue of $H_{s}^{-1}$ . Then, we can bound the term $\\left(\\mathtt{a}\\right)-1$ in (F.17\\`) as fo\u02d8llows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[e^{36\\left\\Vert\\mathbf{w}-\\mathbf{w}_{s+1}\\right\\Vert_{2}^{2}}\\right]}=\\sqrt{\\mathbb{E}_{X_{j}}\\left[\\prod_{j=1}^{d}e^{36c\\lambda_{j}\\left(H_{s}^{-1}\\right)X_{j}^{2}}\\right]}\\leqslant\\sqrt{\\prod_{j=1}^{d}\\mathbb{E}_{X_{j}}\\left[e^{\\frac{36c}{\\lambda}X_{j}^{2}}\\right]}}\\\\ &{}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\left(\\mathbb{E}_{X\\sim\\chi^{2}}\\left[e^{\\frac{36c}{\\lambda}X}\\right]\\right)^{\\frac{d}{2}}\\leqslant\\mathbb{E}_{X\\sim\\chi^{2}}\\left[e^{\\frac{18c d}{\\lambda}X}\\right],\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the first inequality holds since $\\lambda_{j}\\,\\left(H_{s}^{-1}\\right)\\,\\leqslant\\,\\frac{1}{\\lambda}$ . In the second equality, $\\chi^{2}$ denotes the chisquare distribution, and the last inequalit\\`y is du\u02d8e to Jensen\u2019s inequality. By setting $\\lambda\\geqslant72c d$ , we get ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[e^{36\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\right]}\\leqslant\\mathbb{E}_{X\\sim\\chi^{2}}\\left[e^{\\frac{X}{4}}\\right]\\leqslant\\sqrt{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the last inequality holds d ue ?to the fact that the moment-generating function for $\\chi^{2}$ -distribution is bounded by $\\mathbb{E}_{X\\sim\\chi^{2}}[\\![e^{t X}]\\!]\\leqslant1/\\sqrt{1-2t}$ for all $t\\leqslant1/2$ . ", "page_idx": 37}, {"type": "text", "text": "Now, we bound the term $\\left(\\mathsf{a}\\right)-2$ in (F.17). ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{4}\\right]}=\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\mathcal{N}(0,c H_{s}^{-1})}\\left[\\|\\mathbf{w}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{4}\\right]}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\mathcal{N}(0,c\\bar{H}_{s}^{-1})}\\left[\\|\\mathbf{w}\\|_{2}^{4}\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $\\bar{H}_{s}\\,=\\,\\,(\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1}))^{-1/2}H_{s}(\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1}))^{-1/2}$ . Let $\\bar{\\lambda}_{j}\\,=\\,\\lambda_{j}\\,\\left(c\\bar{H}_{s}^{-1}\\right)$ be the $j$ -th largest eigenvalue of the matrix $c\\bar{H}_{s}^{-1}$ . Then, conducting an analysis similar to t\\`hat in e\u02d8quation (F.18) yields that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\mathcal{N}(0,c\\widehat{H}_{x}^{-1})}\\left[\\left\\|\\mathbf{w}\\right\\|_{2}^{4}\\right]}=\\sqrt{\\mathbb{E}_{X_{j}\\sim\\mathcal{N}(0,1)}\\left[\\left\\|\\sum_{j=1}^{d}\\sqrt{\\lambda_{j}}X_{j}\\mathbf{e}_{j}\\right\\|_{2}^{4}\\right]}}\\\\ &{\\ =\\sqrt{\\mathbb{E}_{X_{j}\\sim\\mathcal{N}(0,1)}\\left[\\left(\\displaystyle\\sum_{j=1}^{d}\\lambda_{j}X_{j}\\right)^{2}\\right]}}\\\\ &{\\ =\\sqrt{\\displaystyle\\sum_{j=1}^{d}\\sum_{j=1}^{d}\\frac{d}{\\lambda_{j}\\lambda_{j}\\mathbb{E}_{X_{j}\\sim\\mathcal{N}(0,1)}\\left[X_{j}^{2}X_{j}^{2}\\right]}}}\\\\ &{\\ \\ \\leq\\sqrt{3\\displaystyle\\sum_{j=1}^{d}\\sum_{j=1}^{d}\\frac{d}{\\lambda_{j}\\lambda_{j}}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the inequality holds due to $\\mathbb{E}_{X_{j},X_{j^{\\prime}}\\sim\\mathcal{N}(0,1)}[X_{j}^{2}X_{j^{\\prime}}^{2}]\\leqslant3$ for all $j,j^{\\prime}\\in[d]$ , and the last equality holds because $\\begin{array}{r}{\\sum_{j}^{d}\\bar{\\lambda}_{j}=\\mathrm{Tr}\\left(c\\bar{H}_{s}^{-1}\\right).}\\end{array}$ . Here, $\\operatorname{Tr}(A)$ denotes the trace of the matrix $A$ . ", "page_idx": 38}, {"type": "text", "text": "We define the matrix $\\begin{array}{r}{M_{s+1}:=\\lambda\\mathbf{I}_{d}/2+\\sum_{\\tau=1}^{s}\\nabla^{2}\\ell_{\\tau}\\big(\\mathbf{w}_{\\tau+1}\\big)}\\end{array}$ . Under the condition $\\lambda\\geqslant2$ , for any $s\\in[T]$ and $\\mathbf{w}\\in\\mathcal{W}$ , we have $\\begin{array}{r}{\\nabla^{2}\\ell_{s}(\\mathbf{w})\\,\\leq\\,{\\bf I}_{d}\\,\\leqslant\\,\\frac{\\lambda}{2}{\\bf I}_{d}}\\end{array}$ . Thus, we have $H_{s}\\geq M_{s+1}$ . Then, we can bound the trace as follows: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Tr}\\left(\\bar{H}_{s}^{-1}\\right)=\\mathrm{Tr}\\left(H_{s}^{-1}\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})\\right)\\leqslant\\mathrm{Tr}\\left(M_{s+1}^{-1}\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})\\right)}\\\\ &{\\qquad\\qquad=\\mathrm{Tr}\\left(M_{s+1}^{-1}(M_{s+1}-M_{s})\\right)\\leqslant\\log\\frac{\\mathrm{det}\\left(M_{s+1}\\right)}{\\mathrm{det}(M_{s})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where the last inequality holds by Lemma 4.5 of Hazan et al. [27]. Therefore we can bound the term $\\left(\\mathtt{a}\\right)-2$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sqrt{\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{4}\\right]}\\leqslant{\\sqrt{3}}c\\log\\frac{\\operatorname*{det}(M_{s+1})}{\\operatorname*{det}(M_{s})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By plugging (F.19) and (F.20) into (F.17), we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathbf{w}\\sim\\hat{P}_{s+1}}\\left[e^{18\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{s+1}\\|_{\\nabla^{2}\\ell_{s}(\\mathbf{w}_{s+1})}^{2}\\right]\\leqslant{\\sqrt{6}}c\\log\\frac{\\operatorname*{det}(M_{s+1})}{\\operatorname*{det}(M_{s})}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Combining (F.15), (F.16), and (F.21), and taking summation over $s$ , we derive that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{t}\\ell(\\widetilde\\mathbf{z}_{s},\\mathbf{y}_{s})\\leqslant\\displaystyle\\sum_{s=1}^{t}L_{s}(\\mathbf{w}_{s+1})+\\sqrt{6}c\\displaystyle\\sum_{s=1}^{t}\\log\\frac{\\operatorname*{det}(M_{s+1})}{\\operatorname*{det}(M_{s})}}\\\\ {\\displaystyle}&{=\\displaystyle\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}_{s+1})+\\frac{1}{2c}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{w}_{s}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}+\\sqrt{6}c\\displaystyle\\sum_{s=1}^{t}\\log\\frac{\\operatorname*{det}(M_{s+1})}{\\operatorname*{det}(M_{s})}}\\\\ {\\displaystyle}&{=\\displaystyle\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}_{s+1})+\\frac{1}{2c}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{w}_{s}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}+\\sqrt{6}c\\log\\left(\\frac{\\operatorname*{det}(M_{t+1,h})}{\\operatorname*{det}\\binom{\\Delta}{2}\\mathbf{I}_{d}}\\right)}\\\\ {\\displaystyle}&{\\leqslant\\displaystyle\\sum_{s=1}^{t}\\ell_{s}(\\mathbf{w}_{s+1})+\\frac{1}{2c}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{w}_{s}-\\mathbf{w}_{s+1}\\|_{H_{s}}^{2}+\\sqrt{6}c\\cdot d\\log\\left(1+\\frac{t+1}{2\\lambda}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By rearranging the terms, we conclude the proof. ", "page_idx": 38}, {"type": "text", "text": "F.3 Technical Lemmas for Lemma 1 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Lemma F.4 (Proposition 4.1 of Campolongo and Orabona 12). Let the $\\mathbf{w}_{t+1}$ be the solution of the update rule ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{w}_{t+1}=\\arg\\underset{\\mathbf{w}\\in\\mathcal{V}}{\\operatorname*{min}}\\,\\eta\\ell_{t}(\\mathbf{w})+D_{\\psi}(\\mathbf{w},\\mathbf{w}_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\mathcal{V}\\ \\subseteq\\ \\mathcal{W}\\ \\subseteq\\ \\mathbb{R}^{d}$ is a non-empty convex set and ${D_{\\psi}(\\mathbf{w}_{1},\\mathbf{w}_{2})\\;\\;=\\;\\;\\psi(\\mathbf{w}_{1})\\;-\\;\\psi(\\mathbf{w}_{2})\\;-\\;}$ $\\langle\\nabla\\psi(\\mathbf{w}_{2}),\\mathbf{w}_{1}\\,-\\,\\mathbf{w}_{2}\\rangle$ is the Bregman Divergence w.r.t. a strictly convex and continuously differentiable function $\\psi:\\mathcal{W}\\to\\mathbb{R}$ . Further supposing $\\psi(\\mathbf{w})$ is 1-strongly convex w.r.t. a certain norm $\\|\\cdot\\|\\,i n\\,\\mathcal{W}_{;}$ , then there exists a $\\mathbf{g}_{t}^{\\prime}\\in\\partial\\ell_{t}\\!\\left(\\mathbf{w}_{t+1}\\right)$ such that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\langle\\eta_{t}\\mathbf{g}_{t}^{\\prime},\\mathbf{w}_{t+1}-\\mathbf{u}\\rangle\\leqslant\\langle\\nabla\\psi(\\mathbf{w}_{t})-\\nabla\\psi(\\mathbf{w}_{t+1}),\\mathbf{w}_{t+1}-\\mathbf{u}\\rangle\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for any $\\mathbf{u}\\in\\mathcal{W}$ . ", "page_idx": 38}, {"type": "text", "text": "Lemma F.5 (Lemma 15 of Zhang and Sugiyama 54). Let $\\{\\mathcal{F}_{t}\\}_{t=1}^{\\infty}$ be a filtration. Let $\\{{\\bf z}_{t}\\}_{t=1}^{\\infty}$ be $a$ stochastic process in $\\mathcal{B}_{2}(K)=\\{\\bar{\\mathbf{z}}\\in\\mathbb{R}^{K}\\mid\\lVert\\bar{\\mathbf{z}}\\rVert_{\\infty}\\leqslant.1\\}$ such that $\\mathbf{z}_{t}$ is $\\mathcal{F}_{t}$ measurable. Let $\\{\\varepsilon_{t}\\}_{t=1}^{\\infty}$ be a martingale difference sequence such that $\\boldsymbol{\\varepsilon}_{t}\\in\\mathbb{R}^{K}$ is $\\mathcal{F}_{t+1}$ measurable. Furthermore, assume that, conditional on $\\mathcal{F}_{t}$ , we have $\\|\\varepsilon_{t}\\|_{1}\\leqslant2$ almost surely. Let $\\Sigma_{t}=\\mathbb{E}[\\varepsilon_{t}\\varepsilon_{t}^{\\top}|\\mathcal{F}_{t}]$ . and $\\lambda>0$ . Then, for any $t\\geqslant1$ define ", "page_idx": 38}, {"type": "equation", "text": "$$\nU_{t}=\\sum_{s=1}^{t-1}\\langle\\pmb{\\varepsilon}_{s},\\mathbf{z}_{s}\\rangle\\quad a n d\\quad H_{t}=\\lambda+\\sum_{s=1}^{t-1}\\|\\mathbf{z}_{s}\\|_{\\Sigma_{s}}^{2},\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Then, for any $\\delta\\in(0,1]$ , we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\exists t\\geqslant1,U_{t}\\geqslant\\sqrt{H_{t}}\\left(\\frac{\\sqrt{\\lambda}}{4}+\\frac{4}{\\sqrt{\\lambda}}\\log\\left(\\sqrt{\\frac{H_{t}}{\\lambda}}\\right)+\\frac{4}{\\sqrt{\\lambda}}\\log\\left(\\frac{2}{\\delta}\\right)\\right)\\right]\\leqslant\\delta.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Lemma F.6 (Lemma 1 of Zhang and Sugiyama 54). Let $C>0$ , $\\mathbf{a}\\in[-C,C]^{K}$ , $\\mathbf{y}\\in\\mathbb{R}^{K+1}$ be $a$ one-hot vector and $\\mathbf{b}\\in\\mathbb{R}^{K}$ . Then, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\ell({\\mathbf a},{\\mathbf y})\\geqslant\\ell({\\mathbf b},{\\mathbf y})+\\nabla\\ell({\\mathbf b},{\\mathbf y})^{\\top}({\\mathbf a}-{\\mathbf b})+{\\frac{1}{\\log(K+1)+2(C+1)}}({\\mathbf a}-{\\mathbf b})^{\\top}\\nabla^{2}\\ell({\\mathbf b},{\\mathbf y})({\\mathbf a}-{\\mathbf b}).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma F.7 (Lemma 17 of Zhang and Sugiyama 54). Let $\\textbf{z}\\in\\mathbb{R}^{K}$ be a $K$ -dimensional vector. Let $\\begin{array}{r}{\\ell({\\bf z},{\\bf y})~=~\\sum_{k=0}^{K}{\\bf1}\\{y_{i}~=~1\\}~\\cdot\\log\\left(\\frac{1}{[\\sigma({\\bf z})]_{k}}\\right)}\\end{array}$ , where $\\textbf{y}=~\\left[y_{0},\\dots,y_{K}\\right]^{\\top}~\\in~\\mathbb{R}^{K+1}$ , and the softmax functi on $\\pmb{\\sigma}(\\mathbf{z})\\,:\\,\\mathbb{R}^{K}\\,\\rightarrow\\,\\mathbb{R}^{K}$ is define d as $\\begin{array}{r}{[\\sigma(\\mathbf{z})]_{i}~=~\\frac{\\exp([\\mathbf{z}]_{i})}{v_{0}+\\sum_{k=1}^{K}\\exp([\\mathbf{z}]_{k})}}\\end{array}$ for all $i\\;\\in\\;[K],$ and $\\begin{array}{r c l}{[\\pmb{\\sigma}(\\mathbf{z})]_{0}~=~\\frac{v_{0}}{v_{0}+\\sum_{k=1}^{K}\\exp([\\mathbf{z}]_{k})}}\\end{array}$ . Define $\\mathbf{z}^{\\mu}\\ :=\\ \\sigma^{+}\\,(\\mathrm{smooth}_{\\mu}(\\pmb{\\sigma}(\\mathbf{z})))$ , where $\\mathrm{smooth}_{\\mu}({\\bf q})\\;=$ $(1-\\mu)\\mathbf{q}+\\mu\\mathbf{1}/(K+1)$ . Then, for $\\mu\\in[0,1/2]$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\ell(\\mathbf{z}^{\\mu},\\mathbf{y})-\\ell(\\mathbf{z},\\mathbf{y})\\leqslant2\\mu\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We also have $\\|\\mathbf{z}^{\\mu}\\|_{\\infty}\\leqslant\\log(1+(K+1)/\\mu)$ . ", "page_idx": 39}, {"type": "text", "text": "Lemma F.8 (Lemma 18 of Zhang and Sugiyama 54). Let $\\begin{array}{r}{L_{t}(\\mathbf{w})=\\ell_{t}(\\mathbf{w})+\\frac{1}{2c}\\|\\mathbf{w}-\\mathbf{w}_{t}\\|_{H_{t}}^{2}}\\end{array}$ . Assume that $\\ell_{t}$ is a $M$ -self-concordant-like function. Then, for any w, $\\mathbf{w}_{t}\\in\\mathcal{W}$ , the quadratic approximation $\\begin{array}{r}{\\tilde{L}_{t}(\\mathbf{w})=L_{t}(\\mathbf{w}_{t+1})+\\langle\\nabla L_{t}(\\mathbf{w}_{t+1}),\\mathbf{w}-\\mathbf{w}_{t+1}\\rangle+\\frac{1}{2c}\\|\\mathbf{w}-\\mathbf{w}_{t+1}\\|_{H_{t}}^{2}}\\end{array}$ satisfies ", "page_idx": 39}, {"type": "equation", "text": "$$\nL_{t}(\\mathbf{w})\\leqslant\\tilde{L}_{t}(\\mathbf{w})+e^{M^{2}\\|\\mathbf{w}-\\mathbf{w}_{t+1}\\|_{2}^{2}}\\|\\mathbf{w}-\\mathbf{w}_{t+1}\\|_{\\nabla\\ell_{t}(\\mathbf{w}_{t+1})}^{2}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "G Proofs of Theorem 3 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "In this section, we provide the proof of Theorem 3. In addition to the adversarial construction presented in Section D.1, we construct the adversarial non-uniform rewards. ", "page_idx": 39}, {"type": "text", "text": "G.1 Adversarial Rewards Construction ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Under the adversarial construction in Section D.1, we observe that there are $K$ identical context vectors, invariant across rounds $t$ . Therefore, in total, there are $N=K\\cdot\\left({d\\atop d/4}\\right)$ items. Let the rewards be also time-invariant. Given $\\mathbf{w}_{V}$ , we define a unique item $i^{\\star}\\in[N]$ \\`as a\u02d8n item that maximizes $x_{i}^{\\top}\\mathbf{w}_{V}$ , i.e., $x_{i^{\\star}}\\,=\\,x_{V}$ , and has a reward of 1, i.e., $r_{i^{\\star}}=1$ . Then, we construct the non-uniform rewards as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\nr_{i}=\\left\\{{1,\\begin{array}{l l}{{\\mathrm{~for~}i=i^{\\star}}}\\\\ {{\\mathrm{for~}i\\not=i^{\\star},}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where we define $\\gamma$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}_{S\\in S}\\frac{\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}=\\frac{1}{v_{0}+1}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Note that $\\gamma<1$ . ", "page_idx": 39}, {"type": "text", "text": "G.2 Main Proof of Theorem 3 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Proof of Theorem 3. Given the rewards construction as (G.1), any reward in the optimal assortment $S_{t}^{\\star}$ is larger than the expected revenues. ", "page_idx": 39}, {"type": "text", "text": "Lemma G.1. Let $\\begin{array}{r}{R(S^{\\star},\\mathbf{w}_{V})=\\frac{\\sum_{i\\in S^{\\star}}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})r_{i}}{v_{0}+\\sum_{j\\in S^{\\star}}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}}\\end{array}$ \u0159iPS\u2039 exppxiJ wJV qri . Then, we have ", "page_idx": 39}, {"type": "equation", "text": "$$\nr_{i}\\geqslant R(S^{\\star},\\mathbf{w}_{V}),\\quad\\forall i\\in S^{\\star}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Lemma G.1 implies that $S^{\\star}$ contains only one item $i^{\\star}$ . This is because if $S^{\\star}=\\{x_{i^{\\star}}\\}$ , adding any item $i\\neq i^{\\star}$ to the assortment results in lower expected revenue, since $r_{i}=\\gamma\\leqslant R(S^{\\star}=\\{x_{i^{\\star}}\\},\\mathbf{w}_{v})$ . ", "page_idx": 39}, {"type": "text", "text": "Furthermore, we can bound the expected revenue for any assortment as follows: ", "page_idx": 39}, {"type": "text", "text": "Lemma G.2. Under the same parameters and context vectors as those in Section $D$ , if the rewards are constructed according to Equation (G.1), for any $S\\in{\\mathcal{S}}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\nR(S,\\mathbf{w}_{V})\\leqslant\\frac{\\operatorname*{max}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\operatorname*{max}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Let $x_{U_{1}},\\ldots,x_{U_{L}}$ be the distinct feature vectors contained in assortments $S_{t}$ with $U_{1},\\dots,U_{L}\\in$ $\\lambda_{d/4}$ . Let $U^{\\star}$ be the subset among $U_{1},\\ldots,U_{L}$ that maximizes $x_{U}^{\\top}\\mathbf{w}_{V}$ , i.e., $U^{\\star}\\quad\\in$ $\\operatorname*{argmax}_{U\\in\\{U_{1},\\ldots,U_{L}\\}}x_{U}^{\\top}\\mathbf{w}_{V}$ , where $\\mathbf{w}_{V}$ is the underlying parameter. For simplicity, we denote $\\tilde{U}_{t}$ as the unique $U^{\\star}\\in\\mathcal{V}_{d/4}$ in $S_{t}$ . Then, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}R(S^{\\star},\\mathbf{w}_{V})-R(S_{t},\\mathbf{w}_{V})=\\displaystyle\\sum_{t=1}^{T}\\frac{\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}-R(S_{t},\\mathbf{w}_{V})}\\\\ &{\\displaystyle\\geq\\sum_{t=1}^{T}\\frac{\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}-\\frac{\\operatorname*{max}_{i\\in S_{t}}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\operatorname*{max}_{i\\in S_{t}}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}}\\\\ &{\\displaystyle=\\sum_{t=1}^{T}\\frac{\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}-\\frac{\\exp(x_{\\bar{U}_{t}}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\exp(x_{\\bar{U}_{t}}^{\\top}\\mathbf{w}_{V})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the first equality holds becauset $S^{\\star}$ contains only one item $i^{\\star}$ by Lemma G.1 (and recall that $x_{i^{\\star}}=x_{V}$ ), and the inequality holds by Lemma G.2. Hence, the problem is not easier than solving the MNL bandit problems with the assortment size 1, i.e., $K=1$ . By putting $K=1$ and $v_{0}=\\Theta(1)$ in Theorem 1, we derive that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{w}}{\\operatorname*{sup}}\\mathbb{E}_{\\mathbf{w}}^{\\pi}\\left[\\mathbf{Reg}_{T}(\\mathbf{w})\\right]\\geqslant\\frac{1}{|\\mathcal{V}_{d/4}|}\\sum_{V\\in\\mathcal{V}_{d/4}}\\mathbb{E}_{\\mathbf{w}_{V}}^{\\pi}\\sum_{t=1}^{T}R(S^{\\star},\\mathbf{w}_{V})-R(S_{t},\\mathbf{w}_{V})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geqslant\\frac{1}{|\\mathcal{V}_{d/4}|}\\sum_{V\\in\\mathcal{V}_{d/4}}\\mathbb{E}_{\\mathbf{w}_{V}}^{\\pi}\\sum_{t=1}^{T}\\frac{\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}-\\frac{\\exp(x_{U}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\exp(x_{\\bar{U}_{t}}^{\\top}\\mathbf{w}_{V})}}\\\\ &{\\qquad\\qquad\\qquad=\\Omega\\left(d\\sqrt{T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This concludes the proof of Theorem 3. ", "page_idx": 40}, {"type": "text", "text": "G.3 Proofs of Lemmas for Theorem 3 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "G.3.1 Proof of Lemma G.1 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proof of Lemma G.1. We prove by contradiction. Assume that there exists $i\\in S^{\\star}$ such that $r_{i}<$ $R(S^{\\star},{\\bf w}_{V})$ . Then, removing the item $i$ from the assortment $S^{\\star}$ yields higher expected revenue. This contradicts the optimality of $S^{\\star}$ . Thus, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\nr_{i}\\geqslant R(S^{\\star},\\mathbf{w}_{V}),\\quad\\forall i\\in S^{\\star}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 40}, {"type": "text", "text": "G.3.2 Proof of Lemma G.2 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proof of Lemma G.2. We provide a proof by considering the following cases: ", "page_idx": 40}, {"type": "text", "text": "Case 1. $i^{\\star}\\in S$ . ", "page_idx": 40}, {"type": "text", "text": "Recall that, by the construction of rewards, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\gamma=\\operatorname*{min}_{S\\in S}\\frac{\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}{v_{0}+\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}\\leqslant\\frac{\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}{v_{0}+\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}\\leqslant\\frac{\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})}{v_{0}+\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This implies that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\displaystyle\\sum_{i\\in S\\backslash\\{i^{*}\\}}\\exp(x_{i}^{\\top}\\mathbf w_{V})\\right\\}\\gamma\\left(v_{0}+\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})\\right)\\leqslant\\left\\{\\displaystyle\\sum_{i\\in S\\backslash\\{i^{*}\\}}\\exp(x_{i}^{\\top}\\mathbf w_{V})\\right\\}\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})}\\\\ &{\\Leftrightarrow\\left(\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})+\\displaystyle\\sum_{i\\in S\\backslash\\{i^{*}\\}}\\exp(x_{i}^{\\top}\\mathbf w_{V})\\gamma\\right)\\left(v_{0}+\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})\\right)}\\\\ &{\\leqslant\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})\\left(v_{0}+\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})+\\displaystyle\\sum_{i\\in S\\backslash\\{i^{*}\\}}\\exp(x_{i}^{\\top}\\mathbf w_{V})\\right)}\\\\ &{\\Leftrightarrow\\frac{\\exp(x_{i^{*}}^{\\top}\\mathbf w_{V})+\\sum_{i\\in S\\backslash\\{i^{*}\\}}\\exp(x_{i}^{\\top}\\mathbf w_{V})\\gamma}{v_{0}+\\sum_{i\\in S\\backslash\\{i^{*}\\}}(x_{i^{*}}^{\\top}\\mathbf w_{V})}\\leqslant\\frac{\\exp(x_{i}^{\\top}\\mathbf w_{V})}{v_{0}+\\exp(x_{i}^{\\top}\\mathbf w_{V})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Therefore, for all $S\\in S$ , we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(S,\\mathbf{w}_{V})=\\displaystyle\\frac{\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})r_{i}}{v_{0}+\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}=\\frac{\\exp(x_{i^{\\star}}^{\\top}\\mathbf{w}_{V})+\\sum_{i\\in S\\backslash\\{i^{\\star}\\}}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})\\gamma}{v_{0}+\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}}\\\\ &{\\phantom{\\sum_{i\\in S}\\exp(x_{i^{\\star}}^{\\top}\\mathbf{w}_{V})}\\leqslant\\frac{\\exp(x_{i^{\\star}}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\exp(x_{i^{\\star}}^{\\top}\\mathbf{w}_{V})}\\leqslant\\frac{\\operatorname*{max}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})}{v_{0}+\\operatorname*{max}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf{w}_{V})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the first inequality holds by (G.3), and the last inequality holds since fpxq \u201c v0x\\`x is an increasing function. ", "page_idx": 41}, {"type": "text", "text": "Let us return to (G.2). Since v0\\`\u0159iPS exppJxiJ wV q\u011b 1 for any S P S, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\gamma\\leqslant\\frac{\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}{v_{0}+\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}\\leqslant\\frac{\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}{v_{0}+\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}\\cdot\\frac{v_{0}+\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}{\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "which is equivalent to ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\frac{\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})\\gamma}{v_{0}+\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}\\leqslant\\frac{\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}{v_{0}+\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Hence, for all $S\\in S$ , we get ", "page_idx": 41}, {"type": "equation", "text": "$$\nR(S,\\mathbf w_{V})=\\frac{\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})\\gamma}{v_{0}+\\sum_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}\\leqslant\\frac{\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}{v_{0}+\\operatorname*{min}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}\\leqslant\\frac{\\operatorname*{max}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}{v_{0}+\\operatorname*{max}_{i\\in S}\\exp(x_{i}^{\\top}\\mathbf w_{V})}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 41}, {"type": "text", "text": "H Proofs of Theorem 4 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In this section, we provide the proof of Theorem 4. Since we now consider the case of non-uniform rewards, the sizes of both the chosen assortment $S_{t}$ , and the optimal assortment, $S_{t}^{\\star}$ are no longer fixed at $K$ . ", "page_idx": 41}, {"type": "text", "text": "We begin the proof by introducing additional useful lemmas. Lemma H.1 shows that $\\tilde{R}_{t}(S_{t})$ , defined in (6), is an upper bound of the true expected revenue of the optimal assortment, $R_{t}(S_{t}^{\\star},\\mathbf{w}^{\\star})$ . ", "page_idx": 41}, {"type": "text", "text": "Lemma H.1 (Lemma 4 in Oh and Iyengar 42). Let $\\begin{array}{r c l}{{\\tilde{R}_{t}(S)~=~\\frac{\\sum_{i\\in{\\cal S}}\\exp(\\alpha_{t i})r_{t i}}{v_{0}+\\sum_{j\\in{\\cal S}}\\exp(\\alpha_{t j})}}}\\end{array}$ And suppose \u00b7 $S_{t}=\\operatorname*{argmax}_{S\\in S}\\tilde{R}_{t}(S)$ . If for every item $i\\in S_{t}^{\\star}$ , $\\alpha_{t i}\\geqslant x_{t i}^{\\mathsf{T}}\\mathbf{w}^{\\star}$ , then for all $t\\geqslant1$ , the following inequalities hold: ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{t}(S_{t}^{\\star},\\mathbf{w}^{\\star})\\leqslant\\tilde{R}_{t}(S_{t}^{\\star})\\leqslant\\tilde{R}_{t}(S_{t}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Note that Lemma H.1 does not claim that the expected revenue is a monotone function in general. Instead, it specifically states that the value of the expected revenue, when associated with the optimal assortment, increases with an increase in the MNL parameters [8, 42]. ", "page_idx": 42}, {"type": "text", "text": "Lemma H.2 shows that $\\tilde{R}_{t}(S_{t})$ increases as the utilities of items in $S_{t}$ increase. ", "page_idx": 42}, {"type": "text", "text": "Lemma H.2. Let $\\begin{array}{r}{\\tilde{R}_{t}(S)=\\frac{\\sum_{i\\in S}\\exp(\\alpha_{t i})r_{t i}}{v_{0}+\\sum_{j\\in S}\\exp(\\alpha_{t j})}}\\end{array}$ and $S_{t}=\\operatorname{argmax}_{S\\in S}\\tilde{R}_{t}(S)$ . Assume $\\alpha_{t i}^{\\prime}\\geqslant\\alpha_{t i}\\geqslant0$ for all $i\\in[N]$ . Then, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\tilde{R}_{t}(S_{t})\\leqslant\\frac{\\sum_{i\\in S_{t}}\\exp(\\alpha_{t i}^{\\prime})r_{t i}}{v_{0}+\\sum_{j\\in S_{t}}\\exp(\\alpha_{t j}^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Furthermore, we provide a novel elliptical potential Lemma H.3 for the centralized context vectors $\\tilde{x}_{t i}$ . ", "page_idx": 42}, {"type": "text", "text": "Lemma H.3. Let $\\begin{array}{r}{H_{t}\\ =\\ \\lambda\\mathbf{I}_{d}\\,+\\,\\sum_{s=1}^{t-1}\\mathcal{G}_{s}\\big(\\mathbf{w}_{s+1}\\big),}\\end{array}$ , where $\\begin{array}{r}{\\mathcal{G}_{s}(\\mathbf{w})~=~\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w})x_{s i}x_{s i}^{\\top}~-}\\end{array}$ $\\begin{array}{r}{\\sum_{i\\in S_{s}}\\sum_{j\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w})p_{s}(j|S_{s},\\mathbf{w})x_{s i}x_{s j}^{\\top}}\\end{array}$ . Define $\\tilde{x}_{s i}\\,=\\,x_{s i}\\,-\\,\\mathbb{E}_{j\\sim p_{s}(\\cdot|S_{s},\\mathbf{w}_{s+1})}[x_{s j}]$ . Suppose $\\lambda\\geqslant2$ . \u0159Then the following statements hold true: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\rangle\\sum_{s=1}^{t}\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})\\|\\tilde{x}_{s i}\\|_{H_{s}^{-1}}^{2}\\leqslant2d\\log\\big(1+\\frac{t}{d\\lambda}\\big),}\\\\ &{\\rangle\\sum_{s=1}^{t}\\operatorname*{max}_{i\\in S_{s}}\\|\\tilde{x}_{s i}\\|_{H_{s}^{-1}}^{2}\\leqslant\\frac{2}{\\kappa}d\\log\\big(1+\\frac{t}{d\\lambda}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now, we prove the Theorem 4. ", "page_idx": 42}, {"type": "text", "text": "H.1 Proof of Theorem 4 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Proof of Theorem 4. Let $\\alpha_{t i}^{\\prime}=x_{t i}^{\\top}\\mathbf{w}^{\\star}+2\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}$ . If $\\mathbf{w}^{\\star}\\in\\mathcal{C}_{t}(\\delta)$ , then, by Lemma E.1, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\alpha_{t i}\\leqslant x_{t i}^{\\top}\\mathbf{w}^{\\star}+2\\beta_{t}(\\delta)\\|x_{t i}\\|_{H_{t}^{-1}}=\\alpha_{t i}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "We denote $\\begin{array}{r}{\\tilde{\\tilde{R}}_{t}(S_{t})=\\frac{\\sum_{i\\in S_{t}}\\exp(\\alpha_{t i}^{\\prime})r_{t i}}{v_{0}+\\sum_{j\\in S_{t}}\\exp(\\alpha_{t j}^{\\prime})}}\\end{array}$ v\u0159i\\`PSt expepx\u03b11ptipq\u03b1r1tiq. Then, we can bound the regret as follows: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}R_{t}(S_{t}^{\\star},\\mathbf{w}^{\\star})-R_{t}(S_{t},\\mathbf{w}^{\\star})\\leqslant\\sum_{t=1}^{T}\\tilde{R}_{t}(S_{t})-R_{t}(S_{t},\\mathbf{w}^{\\star})\\leqslant\\sum_{t=1}^{T}\\tilde{R}_{t}(S_{t})-R_{t}(S_{t},\\mathbf{w}^{\\star}),\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where the first inequality holds by Lemma H.1 and the last inequality holds by Lemma H.2. ", "page_idx": 42}, {"type": "text", "text": "Now, we define $\\tilde{Q}\\;:\\;\\mathbb{R}^{|S_{t}|}\\;\\rightarrow\\;\\mathbb{R}$ , such that for all $\\mathbf{u}\\ =\\ (u_{1},\\ldots,u_{|S_{t}|})^{\\top}\\ \\in\\ \\mathbb{R}^{|S_{t}|},\\ \\tilde{Q}(\\mathbf{u})$ $\\tilde{Q}(\\mathbf{u})\\ =$ $\\begin{array}{r}{\\sum_{k=1}^{|S_{t}|}\\frac{\\exp(u_{k})r_{t i_{k}}}{v_{0}+\\sum_{j=1}^{|S_{t}|}\\exp(u_{j})}}\\end{array}$ . Let $\\begin{array}{r c l}{S_{t}}&{=}&{\\{i_{1},\\dots,i_{|S_{t}|}\\}}\\end{array}$ . Moreover, for all $t~~\\geqslant~~1$ , let $\\mathbf{u}_{t}\\quad=\\phantom{\\,}$ $(u_{t i_{1}},\\dots\\bar{u_{t i_{|S_{t}|}}})^{\\top}=(\\alpha_{t i_{1}}^{\\prime},\\dots,\\alpha_{t i_{|S_{t}|}}^{\\prime})^{\\top}$ and $\\mathbf{u}_{t}^{\\star}=(u_{t i_{1}}^{\\star},\\star\\star u_{t i_{|S_{t}|}}^{\\star})^{\\top}=(x_{t i_{1}}^{\\top}\\mathbf{w}^{\\star},\\star\\star,x_{t i_{|S_{t}|}}^{\\top}\\mathbf{w}^{\\star})^{\\top}$ . Then, by applying a second order Taylor expansion, we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\tilde{R}_{t}(S_{t})-R_{t}(S_{t},\\mathbf{w}^{\\star})=\\displaystyle\\sum_{t=1}^{T}\\tilde{Q}(\\mathbf{u}_{t})-\\tilde{Q}(\\mathbf{u}_{t}^{\\star})}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{t=1}^{T}\\nabla\\tilde{Q}(\\mathbf{u}_{t}^{\\star})^{\\top}(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})+\\underbrace{\\frac{1}{2}\\sum_{t=1}^{T}(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})^{\\top}\\nabla^{2}\\tilde{Q}(\\bar{\\mathbf{u}}_{t})(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})}_{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $\\bar{\\mathbf{u}}_{t}=\\big(\\bar{u}_{t i_{1}},\\ldots,\\bar{u}_{t i_{|S_{t}|}}\\big)^{\\top}\\in\\mathbb{R}^{|S_{t}|}$ is the convex combination of $\\mathbf{u}_{t}$ and $\\mathbf{u}_{t}^{\\star}$ . ", "page_idx": 42}, {"type": "text", "text": "We first bound the term (C). ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\nabla\\bar{Q}(\\mathbf{u}_{t}^{*})^{T}(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{*})}\\\\ &{\\displaystyle=\\sum_{t=1}^{T}\\sum_{i=1}^{r}\\frac{\\exp(x_{t}^{*}\\mathbf{u}^{*})^{T}r_{t i}}{\\sum_{t=1}^{r}\\sum_{i=1}^{r}\\exp(x_{t}^{*}\\mathbf{u}_{t}^{*})}(u_{t}-u_{t}^{*})-\\displaystyle\\sum_{j\\in\\mathcal{S}_{t}}\\frac{\\exp(x_{t}^{*}\\mathbf{u}^{*})r_{t j}\\sum_{t\\in\\mathcal{S}_{t}}\\exp(x_{t}^{*}\\mathbf{u}_{t}^{*})}{(\\nu_{0}+\\sum_{t<s}\\exp(x_{t}^{*}\\mathbf{u}^{*}))^{2}}(u_{t}-u_{t}^{*})}\\\\ &{\\displaystyle=\\sum_{t=1}^{T}\\sum_{i=1}^{r}p_{t}(i|S_{t},\\mathbf{u}^{*})r_{t i}(u_{t}-u_{t}^{*})-\\displaystyle\\sum_{i\\in\\mathcal{S}_{t}}\\sum_{j\\in\\mathcal{S}_{t}}p_{t}(i|S_{t},\\mathbf{u}^{*})r_{t j}\\rho_{t j}(j|S_{t},\\mathbf{u}^{*})(u_{t}-u_{t}^{*})}\\\\ &{\\displaystyle=\\sum_{t=1}^{T}\\sum_{i=1}^{r}p_{t}(i|S_{t},\\mathbf{u}^{*})r_{t i}\\left((u_{t}-u_{t}^{*})-\\displaystyle\\sum_{j\\in\\mathcal{S}_{t}}p_{t}(j|S_{t},\\mathbf{u}^{*})(u_{t}-u_{t}^{*})\\right)}\\\\ &{\\displaystyle=\\sum_{t=1}^{T}\\sum_{i=1}^{r}p_{t}(i|S_{t},\\mathbf{u}^{*})r_{t i}\\left(2p_{t}(i)|T_{t i}-\\displaystyle\\sum_{j\\in\\mathcal{S}_{t}}p_{t}(j|S_{t},\\mathbf{u}^{*})|_{\\mathcal{I}_{t}}(\\mathbf{u}_{t}^{*})\\right)}\\\\ &{\\displaystyle=\\sum_{t=1}^{T}\\sum_{i=1}^{r}p_{t} \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Let $x_{t0}=\\mathbf{0}$ . Then, we can further bound the right-hand side as follows: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{2\\sum_{t=0}^{\\infty}\\Delta((\\Delta_{t})\\sum_{w=\\infty}^{p}\\gamma_{w}(\\{1_{\\theta},w^{*}\\}_{t}-\\sum_{p=1}^{infty}D_{w}(\\{3,w^{*}\\}_{t})|v_{x}|)_{w}\\Bigg|_{\\mathcal{H}_{\\sigma}}}\\quad}\\\\ &{=2\\sum_{t=0}^{\\infty}\\Delta((\\Delta_{t})\\sum_{w=\\infty}^{p}(\\{1_{\\theta},w^{*}\\}_{t}-\\sum_{p=1}^{\\infty}D_{w}(\\{3,w^{*}\\}_{t})|v_{x}|)_{w}\\Bigg)}\\\\ &{=2\\sum_{t=0}^{\\infty}\\Delta((\\Delta_{t})\\sum_{w=\\infty}^{p}D_{w}(\\{1_{\\theta},w^{*}\\}_{t}-\\sum_{p=1}^{\\infty}D_{w}(\\{3,w^{*}\\}_{t})|v_{x}|)_{w}}\\\\ &{\\leq2\\sum_{t=0}^{\\infty}\\Delta((\\Delta_{t})\\sum_{w=\\infty}^{p}D_{w}(\\{1_{\\theta},w^{*}\\}_{t}-\\sum_{p=p+1}^{\\infty}(\\{1_{\\theta},w^{*}\\}_{t})|v_{x}|)_{w}}\\\\ &{\\leq2\\sum_{t=0}^{\\infty}\\Delta((\\Delta_{t})\\sum_{w=\\infty}^{p}(\\{1_{\\theta},w^{*}\\}_{t})|(\\Delta_{t})|v_{x}|)_{w}-\\sum_{p=p+1}^{\\infty}(\\{1_{\\theta},w^{*}\\}_{t})|v_{x}|\\Big|_{\\mathcal{H}_{\\sigma}}\\Big|_{\\mathcal{H}_{\\sigma}}\\Big|_{\\mathcal{H}_{\\sigma}}}\\\\ &{\\leq2N\\sigma(\\theta)\\sum_{t=\\infty}^{\\infty}\\sum_{w=\\infty}^{p}D_{w}(\\{1_{\\theta},w^{*}\\}_{t}-\\sum_{p=p+1}^{\\infty}(\\{1_{\\theta},w^{*}\\}_{t})|v_{x}|)_{w}\\Big|_{\\mathcal{H}_{\\sigma}}\\Big|_{\\mathcal{H}_{\\sigma}}\\Big|_{\\mathcal{H}_{\\sigma}}}\\\\ &{\\leq2N\\sigma(\\theta)\\sum_{t=\\infty}^{\\infty}\\sum_{w=\\infty}^\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where, in the first inequality, we define $S_{t}^{+}\\subseteq S_{t}$ as the subset of items in $S_{t}$ such that the term $\\left\\|x_{t i}\\right\\|_{H_{t}^{-1}}-\\mathbb{E}_{j\\sim p_{t}(\\cdot|S_{t},\\mathbf{w}^{\\star})}\\bar{\\left[\\left\\|x_{t j}\\right\\|_{H_{t}^{-1}}\\right]}\\geqslant\\bar{0}$ and $r_{t i}\\,\\in\\,[0,1]$ , the second inequality holds because $\\beta_{1}(\\delta)\\leqslant\\cdots\\leqslant\\beta_{T}(\\delta)$ , the third inequality holds due to Jensen\u2019s inequality, and the second-to-last inequality holds due to the fact that $\\|\\mathbf{a}\\|=\\|\\mathbf{a-b+b}\\|\\leqslant\\|\\mathbf{a-b}\\|+\\|\\mathbf{b}\\|$ for any vectors $\\mathbf{a},\\mathbf{b}\\in\\mathbb{R}^{d}$ . ", "page_idx": 43}, {"type": "text", "text": "For simplicity, we denote $\\mathbb{E}_{\\mathbf{w}}\\!\\left[x_{t j}\\right]\\;=\\;\\mathbb{E}_{j\\sim p_{t}(\\cdot|S_{t},\\mathbf{w})}\\!\\left[x_{t j}\\right]$ . Let $\\bar{x}_{t i}\\;=\\;x_{t i}\\;-\\;\\mathbb{E}_{\\mathbf{w}^{\\star}}[x_{t j}]$ and $\\tilde{x}_{t i}\\;=\\;$ $\\boldsymbol{x}_{t i}-\\mathbb{E}_{\\mathbf{w}_{t+1}}[\\boldsymbol{x}_{t j}]$ . Then, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})\\|x_{t i}-\\mathbb{E}_{j\\sim p_{t}(\\cdot|S_{t},\\mathbf w^{\\star})}\\left[x_{t}\\right]\\|_{H_{t}^{-1}}=\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})\\|\\bar{x}_{t i}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle\\leqslant\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})\\|\\bar{x}_{t i}-\\tilde{x}_{t i}\\|_{H_{t}^{-1}}+\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})\\|\\bar{x}_{t i}-\\tilde{x}_{t i}\\|_{H_{t}^{-1}}+\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\left(p_{t}(i|S_{t},\\mathbf w^{\\star})-p_{t}(i|S_{t},\\mathbf w_{t+1})\\right)\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle+\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w_{t+1})\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the inequality holds by the triangle inequality. Now, we bound the terms on the right-hand side of (H.1) individually. For the first term, we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})\\lVert\\bar{x}_{t i}-\\tilde{x}_{t i}\\rVert_{H_{t}^{-1}}}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})\\left\\lVert\\mathbb{E}_{\\mathbf w_{t+1}}[x_{t j}]-\\mathbb{E}_{\\mathbf w^{\\star}}[x_{t j}]\\right\\rVert_{H_{t}^{-1}}}\\\\ {\\displaystyle=\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})\\left\\lVert\\sum_{j\\in S_{t}}\\left(p_{t}(j|S_{t},\\mathbf w_{t+1})-p_{t}(j|S_{t},\\mathbf w^{\\star})\\right)x_{t j}\\right\\rVert_{H_{t}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the last equality holds due to the setting of $x_{t0}=\\mathbf{0}$ . By the mean value theorem, there exists $\\pmb{\\xi}_{t}=(1-c)\\mathbf{w}^{\\star}+c\\bar{\\mathbf{w}}_{t+1}$ for some $c\\in(0,1)$ such that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{\\mathcal{E}_{i}\\in\\mathcal{E}_{s}}}\\\\ &{=\\displaystyle\\sum_{j=1}^{m_{k}}\\Bigg[\\Bigg(P_{j}(\\|S_{j},\\xi_{i})\\pi_{j}-P_{j}(\\|S_{k},\\xi_{i})\\displaystyle\\sum_{k=0}^{n_{k}}P_{k}(|S_{j},\\xi_{k})\\pi_{k i}\\Bigg)^{\\top}(\\mathbf{w}_{i+1}-\\mathbf{w}^{*})\\Bigg|\\left[\\pi_{0}\\lVert\\mathbf{\\hat{z}}_{j}\\rVert_{H_{\\mathbb{T}}}\\right]}\\\\ &{\\le\\displaystyle\\sum_{j=1}^{m_{k}}P_{j}(|S_{j},\\xi_{k}|)\\left[\\pi_{0}^{\\top}(\\mathbf{w}_{i+1}-\\mathbf{w}^{*})\\right]\\left[\\pi_{0}\\lVert\\mathbf{\\hat{z}}_{j}\\rVert_{H_{\\mathbb{T}}},}\\\\ &{\\quad\\displaystyle\\sum_{j=0}^{m_{k}}P_{j}(|S_{k},\\xi_{i}|)\\left[\\pi_{0}\\lVert\\mathbf{\\hat{z}}_{k},\\sum_{k=0}^{n_{k}}P_{j}(|S_{k},\\xi_{k}|)\\right]T_{\\mathbb{R}_{k}}^{\\top}(\\mathbf{w}_{i+1}-\\mathbf{w}^{*})\\right]}\\\\ &{\\quad\\displaystyle\\sum_{j=1}^{m_{k}}P_{j}(|\\delta_{j}|S_{j},\\xi_{k}|)\\left[\\pi_{1}\\mathbf{\\hat{z}}_{j}-\\mathbf{w}^{*}\\right]\\left[\\delta_{j}|S_{j},\\frac{1}{m_{k}}\\right]T_{\\mathbb{R}_{k}}^{\\top}(\\mathbf{\\hat{z}}_{k}^{*})\\left[\\pi_{0}\\lVert\\mathbf{\\hat{z}}_{j}-\\mathbf{w}^{*}\\rVert_{H_{\\mathbb{T}}},}\\\\ &{\\quad\\displaystyle\\sum_{j=0}^{m_{k}}P_{j}(|S_{j},\\xi_{k}|)\\left[\\pi_{1}\\mathbf{\\hat{z}}_{j}-\\mathbf{w}^{*}\\right]\\left[\\delta_{j},\\mathbf{\\hat{z}}_{k}\\right]\\left[\\pi_{0}\\left|\\mathbf{\\hat{z}}_{j}^{*}\\right]\\right]}\\\\ &{\\quad\\displaystyle\\sum_{j=0}^{m_{k}}P_{j}(|\\delta_{j}|)\\left[\\pi_ \n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where the fourth inequality holds by Lemma 1 and the second-to-last inequality holds due to Jensen\u2019s inequality. Hence, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})\\lVert\\bar{x}_{t i}-\\tilde{x}_{t i}\\rVert_{H_{t}^{-1}}\\leqslant2\\displaystyle\\sum_{t=1}^{T}\\beta_{t+1}(\\delta)\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})\\displaystyle\\operatorname*{max}_{j\\in S_{t}}\\lVert x_{t j}\\rVert_{H_{t}^{-1}}^{2}}\\\\ {\\displaystyle}&{\\leqslant2\\beta_{T+1}(\\delta)\\displaystyle\\sum_{t=1}^{T}\\operatorname*{max}_{i\\in S_{t}}\\lVert x_{t i}\\rVert_{H_{t}^{-1}}^{2}}\\\\ &{\\leqslant\\displaystyle\\frac{4}{\\kappa}\\beta_{T+1}(\\delta)d\\log\\left(1+\\displaystyle\\frac{T}{d\\lambda}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last inequality holds by Lemma E.2. Using similar reasoning, we can bound the second term of (H.1). By the mean value theorem, there exists $\\bar{\\xi}_{t}^{\\prime}=(1\\!-c^{\\prime})\\mathbf{w}^{\\star}\\bar{+}c^{\\prime}\\mathbf{w}_{t+1}$ for some $c^{\\prime}\\in(0,1)$ such that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{i\\in S_{t}}\\left(p_{t}(i|S_{t},\\mathbf w^{\\star})-p_{t}(i|S_{t},\\mathbf w_{t+1})\\right)\\left\\|\\tilde{x}_{t}\\right\\|_{H_{t}^{-1}}=\\displaystyle\\sum_{i\\in S_{t}}\\nabla p_{t}(i|S_{t},\\xi_{t}^{\\prime})^{\\top}(\\mathbf w^{\\star}-\\mathbf w_{t+1})\\|\\tilde{x}_{t}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle=\\sum_{i\\in S_{t}}\\left(p_{t}(i|S_{t},\\xi_{t}^{\\prime})x_{t i}-p_{t}(i|S_{t},\\xi_{t}^{\\prime})\\displaystyle\\sum_{k\\in S_{t}}p_{t}(k|S_{t},\\xi_{t}^{\\prime})x_{t k}\\right)^{\\top}\\left(\\mathbf w^{\\star}-\\mathbf w_{t+1}\\right)\\left\\|\\tilde{x}_{t}\\right\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle\\leqslant\\beta_{t+1}(\\delta)\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\xi_{t}^{\\prime})\\|x_{t i}\\|_{H_{t}^{-1}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle+\\ \\beta_{t+1}(\\delta)\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\xi_{t}^{\\prime})\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}\\sum_{k\\in S_{t}}p_{t}(k|S_{t},\\xi_{t}^{\\prime})\\|x_{t k}\\|_{H_{t}^{-1}}}\\\\ {\\displaystyle\\leqslant\\beta_{t+1}(\\delta)\\displaystyle\\operatorname*{lass}_{\\mathrm{tect}}\\|x_{t i}\\|_{H_{t}^{-1}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}+\\beta_{t+1}(\\delta)\\displaystyle\\operatorname*{max}_{\\mathrm{tect}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}\\operatorname*{max}_{k\\in S_{t}}\\|x_{t k}\\|_{H_{t}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then, by applying the AM-GM inequality to each term, we obtain ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{t+1}(\\delta)\\operatorname*{max}_{i}\\|x_{t i}\\|_{H_{t}^{-1}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}+\\beta_{t+1}(\\delta)\\operatorname*{max}_{i\\in S_{t}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}\\operatorname*{max}_{k\\in S_{t}}\\|x_{t k}\\|_{H_{t}^{-1}}}\\\\ &{\\leqslant\\beta_{t+1}(\\delta)\\operatorname*{max}_{i\\in S_{t}}\\frac{\\|x_{t i}\\|_{H_{t}^{-1}}^{2}+\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}}{2}+\\beta_{t+1}(\\delta)\\frac{\\left(\\operatorname*{max}_{i\\in S_{t}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}\\right)^{2}+\\left(\\operatorname*{max}_{k\\in S_{t}}\\|x_{t k}\\|_{H_{t}^{-1}}\\right)^{2}}{2}}\\\\ &{=\\beta_{t+1}(\\delta)\\operatorname*{max}_{i\\in S_{t}}\\frac{\\|x_{t i}\\|_{H_{t}^{-1}}^{2}+\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}}{2}+\\beta_{t+1}(\\delta)\\frac{\\operatorname*{max}_{i\\in S_{t}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}+\\operatorname*{max}_{k\\in S_{t}}\\|x_{t k}\\|_{H_{t}^{-1}}^{2}}{2}}\\\\ &{\\leqslant2\\beta_{t+1}(\\delta)\\operatorname*{max}\\left\\{\\operatorname*{max}_{i\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2},\\operatorname*{max}_{i\\in S_{t}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the equality holds since $(\\operatorname*{max}_{i}a_{i})^{2}\\,=\\,\\operatorname*{max}_{i}a_{i}^{2}$ for any $a_{i}\\geqslant0$ . Thus, by Lemma H.3 (or Lemma E.2), we get ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\left(p_{t}(i|S_{t},\\mathbf{w}^{\\star})-p_{t}(i|S_{t},\\mathbf{w}_{t+1})\\right)\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}}}\\\\ &{}&{\\leqslant2\\beta_{t+1}(\\delta)\\operatorname*{max}\\left\\{\\operatorname*{max}_{i\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2},\\displaystyle\\operatorname*{max}_{i\\in S_{t}}\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}\\right\\}\\leqslant\\frac{4}{\\kappa}\\beta_{T+1}(\\delta)d\\log\\left(1+\\displaystyle\\frac{T}{d\\lambda}\\right),0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Finally, we bound the third term of (H.1). By the Cauchy-Schwarz inequality, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}_{t+1})\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}\\leqslant\\sqrt{\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}_{t+1})}\\sqrt{\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}_{t+1})\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}}}\\\\ &{}&{\\leqslant\\sqrt{T}\\sqrt{2d\\log\\left(1+\\frac{T}{d\\lambda}\\right)},}&{\\mathrm{(H.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last inequality holds by Lemma H.3. Plugging (H.2), (H.3), and (H.4) into (H.1), we get ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})\\|x_{t i}-\\mathbb{E}_{j\\sim p_{t}(\\cdot|S_{t},\\mathbf{w}^{\\star})}\\left[x_{t j}\\right]\\|_{H_{t}^{-1}}}\\\\ &{\\quad\\quad\\quad\\leqslant\\sqrt{T}\\sqrt{2d\\log\\left(1+\\displaystyle\\frac{T}{d\\lambda}\\right)}+\\displaystyle\\frac{8}{\\kappa}\\beta_{T+1}(\\delta)d\\log\\left(1+\\displaystyle\\frac{T}{d\\lambda}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Thus, we can bound the term (c) as follows: ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\nabla\\tilde{Q}(\\mathbf{u}_{t}^{\\star})^{\\top}(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})\\leqslant2\\beta_{T}(\\delta)\\sqrt{T}\\sqrt{2d\\log\\left(1+\\frac{T}{d\\lambda}\\right)}+\\frac{16}{\\kappa}\\beta_{T}(\\delta)\\beta_{T+1}(\\delta)d\\log\\left(1+\\frac{T}{d\\lambda}\\right),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Now, we bound the term (D). Define $Q:\\mathbb{R}^{|S_{t}|}\\rightarrow\\mathbb{R}$ , such that for all $\\mathbf{u}=(u_{1},\\dots,u_{|S_{t}|})\\in\\mathbb{R}^{|S_{t}|}$ , Qpuq \u201c\u0159|iS\u201ct1|v0\\`e|jxS\u201cpt1|p ueixqppujq. Then, we have $\\begin{array}{r}{\\left|\\frac{\\partial^{2}\\tilde{Q}}{\\partial i\\partial j}\\right|\\leqslant\\left|\\frac{\\partial^{2}Q}{\\partial i\\partial j}\\right|}\\end{array}$ since $r_{t i}\\in[0,1]$ . By following the similar r easoning from the equation (E.3) to (E.5\u02c7) in \u02c7Secti\u02c7on E\u02c7.1, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{2}\\sum_{t=1}^{T}(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})^{\\top}\\nabla^{2}\\tilde{Q}(\\bar{\\mathbf{u}}_{t})(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})=\\frac{1}{2}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\sum_{j\\in S_{t}}(u_{t i}-u_{t i}^{\\star})\\frac{\\bar{\\mathcal{O}}^{2}\\tilde{Q}}{\\hat{\\mathcal{O}}i\\hat{\\mathcal{O}}j}(u_{t j}-u_{t j}^{\\star})}}\\\\ &{}&{\\leqslant\\frac{1}{2}\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}\\displaystyle\\sum_{j\\in S_{t}}\\left|u_{t i}-u_{t i}^{\\star}\\right|\\left|\\frac{\\bar{\\mathcal{O}}^{2}Q}{\\hat{\\mathcal{O}}i\\hat{\\mathcal{O}}j}\\right||u_{t j}-u_{t j}^{\\star}\\right|}\\\\ &{}&{\\leqslant10\\beta_{T}(\\delta)^{2}\\displaystyle\\sum_{t=1}^{T}\\displaystyle\\operatorname*{max}_{i\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where the first inequality holds because $\\begin{array}{r}{\\left|\\frac{\\hat{\\sigma}^{2}\\tilde{Q}}{\\hat{\\sigma}i\\hat{\\sigma}j}\\right|\\leqslant\\left|\\frac{\\hat{\\sigma}^{2}Q}{\\hat{\\sigma}i\\hat{\\sigma}j}\\right|}\\end{array}$ . Combining (H.5) and (H.6), we derive that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf R e g}_{T}({\\bf w}^{\\star})\\leqslant2\\beta_{T}(\\delta)\\sqrt{T}\\sqrt{2d\\log\\left(1+\\frac{T}{d\\lambda}\\right)}+\\frac{16}{\\kappa}\\beta_{T}(\\delta)\\beta_{T+1}(\\delta)d\\log\\left(1+\\frac{T}{d\\lambda}\\right)}\\ }\\\\ {{\\displaystyle\\quad\\quad\\quad+10\\beta_{T}(\\delta)^{2}\\sum_{t=1}^{T}\\operatorname*{max}_{i\\in S_{t}}\\|x_{t i}\\|_{H_{t}^{-1}}^{2}}\\ }\\\\ {{\\displaystyle\\quad\\quad=\\tilde{\\mathcal{O}}\\left(d\\sqrt{T}+\\frac{1}{\\kappa}d^{2}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\beta_{T}(\\delta)=\\mathcal{O}\\left(\\sqrt{d}\\log T\\log K\\right)$ . This concludes the proof of Theorem 4. ", "page_idx": 46}, {"type": "text", "text": "Remark H.1. If the boundedness assumption on the parameter is relaxed to $\\|\\mathbf{w}\\|_{2}\\leqslant B$ , since $\\beta_{t}(\\delta)=\\mathcal{O}\\left(B\\sqrt{d}\\log t\\log K\\right)$ , we have $\\begin{array}{r}{\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star})=\\tilde{\\mathcal{O}}\\left(B d\\sqrt{T}+\\frac{1}{\\kappa}d^{2}\\right)}\\end{array}$ . ", "page_idx": 46}, {"type": "text", "text": "H.2 Proofs of Lemmas for Theorem 4 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "H.2.1 Proof of Lemma H.2 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Proof of Lemma $H.2$ . We prove the result by first showing that for any $i\\in S_{t}$ , we have $r_{t i}\\geqslant\\tilde{R}_{t}(S_{t})$ . This can be proven similarly to Lemma G.1. Suppose that there exists $i\\in S_{t}$ for which $r_{t i}<\\tilde{R}_{t}(S_{t})$ . Removing item $i$ from the assortment $S_{t}$ results in a higher expected revenue. Consequently, $S_{t}\\ne\\operatorname{argmax}_{S\\in S}\\tilde{R}_{t}(S)$ , which contradicts the optimality of $S_{t}$ . Thus, we have ", "page_idx": 46}, {"type": "equation", "text": "$$\nr_{t i}\\geqslant\\tilde{R}_{t}(S_{t}),\\quad\\forall i\\in S_{t}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "If we increase $\\alpha_{t i}$ to $\\alpha_{t i}^{\\prime}$ for all $i\\in S_{t}$ , the probability of selecting the outside option decreases. In other words, the sum of probabilities of choosing any $i\\in S_{t}$ increases. Since $r_{t i}\\geqslant\\tilde{R}_{t}(S_{t})$ for all ", "page_idx": 46}, {"type": "text", "text": "$i\\in S_{t}$ , this results in an increase in revenue. Hence, we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\tilde{R}_{t}(S_{t})=\\frac{\\sum_{i\\in S_{t}}\\exp(\\alpha_{t i})r_{t i}}{v_{0}+\\sum_{j\\in S_{t}}\\exp(\\alpha_{t j})}\\leqslant\\frac{\\sum_{i\\in S_{t}}\\exp(\\alpha_{t i}^{\\prime})r_{t i}}{v_{0}+\\sum_{j\\in S_{t}}\\exp(\\alpha_{t j}^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 47}, {"type": "text", "text": "H.2.2 Proof of Lemma H.3 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Proof of Lemma $H.3$ . For notational simplicity, let $\\mathbb{E}_{\\mathbf{w}}[x_{t j}]=\\mathbb{E}_{j\\sim p_{t}(\\cdot|S_{t},\\mathbf{w})}[x_{t j}]$ . Let $x_{t0}=\\mathbf{0}$ . We can rewrite $\\mathcal{G}_{s}(\\mathbf{w})$ in the following way: ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{G}_{s}\\big(\\mathbf{w}_{s+1}\\big)}\\\\ &{~=\\displaystyle\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top}-\\displaystyle\\sum_{i\\in S_{s}}\\sum_{j\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(j|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s j}^{\\top}\\qquad\\qquad\\mathrm{(I)}}\\\\ &{~=\\displaystyle\\sum_{i\\in S_{s}\\times\\{0\\}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s i}^{\\top}-\\displaystyle\\sum_{i\\in S_{s}\\times\\{0\\}}\\sum_{j\\in S_{s}\\in\\{0\\}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})p_{s}(j|S_{s},\\mathbf{w}_{s+1})x_{s i}x_{s j}^{\\top}}\\\\ &{~=\\displaystyle\\mathbb{E}_{\\mathbf{w}_{s+1}\\big[x_{s i}x_{s i}^{\\top}\\big]}-\\mathbb{E}_{\\mathbf{w}_{s+1}\\big[x_{s i}\\big]}\\left(\\mathbb{E}_{\\mathbf{w}_{s+1}\\big[x_{s i}\\big]}\\right)^{\\top}}\\\\ &{~=\\mathbb{E}_{\\mathbf{w}_{s+1}}\\left[(x_{s i}-\\mathbb{E}_{\\mathbf{w}_{s+1}\\big[x_{s m}\\big]})(x_{s i}-\\mathbb{E}_{\\mathbf{w}_{s+1}\\big[x_{s m}\\big]})^{\\top}\\right]}\\\\ &{~=\\mathbb{E}_{\\mathbf{w}_{s+1}\\big[\\Tilde{x}_{s i}\\tilde{x}_{s i}^{\\top}\\big]}=\\displaystyle\\sum_{i\\in S_{s}\\in\\{0\\}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})\\Tilde{x}_{s i}\\Tilde{x}_{s i}^{\\top}\\geq\\displaystyle\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})\\Tilde{x}_{s i}\\Tilde{x}_{s i}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "This means that ", "page_idx": 47}, {"type": "equation", "text": "$$\nH_{t+1}=H_{t}+\\mathcal{G}_{t}(\\mathbf{w}_{t+1})\\geq H_{t}+\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}_{t+1})\\tilde{x}_{t i}\\tilde{x}_{t i}^{\\top}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Hence, we can derive that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(H_{t+1}\\right)\\geqslant\\operatorname*{det}\\left(H_{t}\\right)\\left(1+\\sum_{i\\in S_{t}}p_{t}(i|S_{t},{\\mathbf w}_{t+1})\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Since $\\lambda\\geqslant1$ , for all $t\\geqslant1$ we have $\\begin{array}{r}{\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}_{t+1})\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}\\leqslant\\frac{1}{\\lambda}\\operatorname*{max}_{i\\in S_{t}}\\|\\tilde{x}_{t i}\\|_{2}\\leqslant1}\\end{array}$ . Then, using the fact that $z\\leqslant2\\log(1+z)$ for any $z\\in[0,1]$ , we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\sum_{i\\in S_{s}}p_{s}(i|S_{s},\\mathbf{w}_{s+1})\\|\\tilde{x}_{s,i}\\|_{H_{s}^{-1}}^{2}\\leqslant2\\displaystyle\\sum_{s=1}^{t}\\log\\Big(1+p_{s}(i|S_{s},\\mathbf{w}_{s+1})\\|\\tilde{x}_{s}\\|_{H_{s}^{-1}}^{2}\\Big)}\\\\ {\\displaystyle}&{\\leqslant2\\displaystyle\\sum_{s=1}^{t}\\log\\Big(\\frac{\\mathrm{det}(H_{s+1})}{\\mathrm{det}(H_{s})}\\Big)}\\\\ {\\displaystyle}&{=2\\log\\Big(\\frac{\\mathrm{det}(H_{t+1})}{\\mathrm{det}(H_{1})}\\Big)}\\\\ {\\displaystyle}&{\\leqslant2d\\log\\Big(\\frac{\\mathrm{tr}(H_{t+1})}{d\\lambda}\\Big)\\leqslant2d\\log\\Big(1+\\frac{t}{d\\lambda}\\Big)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "This proves the first inequality. ", "page_idx": 47}, {"type": "text", "text": "To show the second inequality, we come back to equation (H.8). By the definition of $\\kappa$ , we get ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{t+1}=H_{t}+\\mathcal{G}_{t}(\\mathbf{w}_{t+1})=H_{t}+\\displaystyle\\sum_{i\\in S_{t}}p_{t}\\big(i|S_{t},\\mathbf{w}_{t+1}\\big)\\tilde{x}_{t i}\\tilde{x}_{t i}^{\\top}}\\\\ &{\\qquad\\,\\,\\geq H_{t}+\\kappa\\displaystyle\\sum_{i\\in S_{t}}\\tilde{x}_{t i}\\tilde{x}_{t i}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Thus, we obtain that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(H_{t+1}\\right)\\geqslant\\operatorname*{det}\\left(H_{t}\\right)\\left(1+\\kappa\\sum_{i\\in S_{t}}\\left\\Vert\\tilde{x}_{t i}\\right\\Vert_{H_{t}^{-1}}^{2}\\right)\\geqslant\\operatorname*{det}\\left(H_{t}\\right)\\left(1+\\kappa\\operatorname*{max}_{i\\in S_{t}}\\left\\Vert\\tilde{x}_{t i}\\right\\Vert_{H_{t}^{-1}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Since $\\lambda\\geqslant1$ , for all $t\\ \\geqslant\\ 1$ we have $\\begin{array}{r}{\\kappa\\operatorname*{max}_{i\\in S_{t}}\\,\\|\\tilde{x}_{t i}\\|_{H_{t}^{-1}}^{2}\\;\\leqslant\\;\\frac{\\kappa}{\\lambda}\\|\\tilde{x}_{t i}\\|_{2}\\;\\leqslant\\;\\kappa}\\end{array}$ . We then reach the conclusion in the same manner: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{s=1}^{t}\\operatorname*{max}_{i\\in S_{s}}\\|\\tilde{x}_{s i}\\|_{H_{s}^{-1}}^{2}\\leqslant\\frac{2}{\\kappa}\\sum_{s=1}^{t}\\log\\left(1+\\kappa\\operatorname*{max}_{i\\in S_{s}}\\|\\tilde{x}_{s i}\\|_{H_{s}^{-1}}^{2}\\right)}}\\\\ &{\\leqslant\\frac{2}{\\kappa}\\sum_{s=1}^{t}\\log\\left(\\frac{\\operatorname*{det}(H_{s+1})}{\\operatorname*{det}(H_{s})}\\right)}\\\\ &{=\\frac{2}{\\kappa}\\log\\left(\\frac{\\operatorname*{det}(H_{t+1})}{\\operatorname*{det}(H_{1})}\\right)}\\\\ &{\\leqslant\\frac{2}{\\kappa}d\\log\\left(\\frac{\\operatorname{tr}(H_{t+1})}{d\\lambda}\\right)\\leqslant\\frac{2}{\\kappa}d\\log\\left(1+\\frac{t}{d\\lambda}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "This proves the second inequality. ", "page_idx": 48}, {"type": "text", "text": "I Proof of Proposition 1 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "I.1 Main Proof of Proposition 1 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . We construct the parameters and features in the same way as in Section D.1. Recall that, by Proposition D.1, it is sufficient to bound $\\begin{array}{r}{\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\dot{\\sum}_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})}\\end{array}$ . We denote $\\tilde{U}_{t}$ as the unique $U^{\\star}\\in\\mathcal{V}_{d/4}$ in ${\\tilde{S}}_{t}$ . We define  \u0159a function $\\mu:\\mathbb{R}\\to[0,1]$ such that for any $z\\,\\in\\,\\mathbb{R}$ , $\\begin{array}{r}{\\mu(z)\\,=\\,\\frac{K\\exp(z)}{v_{0}+K\\exp(z)}}\\end{array}$ . Since all items in $S^{\\star}$ (and in ${\\tilde{S}}_{t}$ ) are identical, we can express $\\begin{array}{r}{\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})}\\end{array}$ (and $\\begin{array}{r}{\\sum_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V}))}\\end{array}$ as follows: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})=\\frac{K\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}{v_{0}+K\\exp(x_{V}^{\\top}\\mathbf{w}_{V})}=\\mu(x_{V}^{\\top}\\mathbf{w}_{V}),\n$$", "text_format": "latex", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})=\\frac{K\\exp(x_{\\tilde{U}_{t}}^{\\top}\\mathbf{w}_{V})}{v_{0}+K\\exp(x_{\\tilde{U}_{t}}^{\\top}\\mathbf{w}_{V})}=\\mu(x_{\\tilde{U}_{t}}^{\\top}\\mathbf{w}_{V}).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Given $\\mathbf{w}_{V}$ , we define $\\begin{array}{r}{\\kappa_{t}^{\\star}(\\mathbf{w}_{V}):=\\sum_{i\\in S^{\\star}}p_{t}(i|S^{\\star},\\mathbf{w}_{V})p_{t}(0|S^{\\star},\\mathbf{w}_{V})}\\end{array}$ . Since the context vectors $\\{x_{t i}\\}$ are constructed to be invariant a cr\u0159oss rounds $t$ , $\\kappa_{t}^{\\star}({\\bf w}_{V})$ is also independent of $t$ , i.e., $\\kappa_{1}^{\\star}({\\bf w}_{V})\\;=\\;$ $\\mathbf{\\\\\\mu}\\cdot\\mathbf{\\mu}-\\kappa_{T}^{\\star}\\big(\\mathbf{w}_{V}\\big)$ . Therefore, we omit the index $t$ for simplicity. Note that, in our instance construction, $\\kappa^{\\star}({\\bf w}_{V})=\\mu(x_{V}^{\\top}{\\bf w}_{V})\\left(1-\\mu(x_{V}^{\\top}{\\bf w}_{V})\\right)=\\dot{\\mu}(x_{V}^{\\top}{\\bf w}_{V}).$ ", "page_idx": 48}, {"type": "text", "text": "We hypothesize that for all $V\\in\\mathcal{V}_{d/4}$ , the regret is dominated by $d\\sqrt{\\kappa^{\\star}({\\bf w}_{V})T}$ . If this assumption does not hold, then by definition, there exists some $V\\in\\mathcal{V}_{d/4}$ sauch that $\\mathbb{E}_{V}\\left[\\mathbf{Reg}_{T}\\big(\\mathbf{w}_{V}\\big)\\right]=$ $\\Omega\\left(d{\\sqrt{\\kappa^{\\star}(\\mathbf{w}_{V})T}}\\right)$ , thereby completing the proof. ", "page_idx": 48}, {"type": "text", "text": "Hypothesis. There exists a constant $C>0$ such that: ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}_{V}\\left[\\mathbf{Reg}_{T}(\\mathbf{w}_{V})\\right]\\leqslant C\\cdot d\\sqrt{\\kappa^{\\star}(\\mathbf{w}_{V})T},\\quad\\forall V\\in\\mathcal{V}_{d/4}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Additionally, we set $\\mathbf{w}^{\\star}=\\operatorname*{argmax}_{\\mathbf{w}_{V}}\\,\\kappa^{\\star}\\big(\\mathbf{w}_{V}\\big)$ , thus, we have $\\begin{array}{r}{\\kappa^{\\star}=\\kappa^{\\star}(\\mathbf{w}^{\\star})=\\operatorname*{max}_{\\mathbf{w}_{V}}\\kappa^{\\star}(\\mathbf{w}_{V})}\\end{array}$ . ", "page_idx": 48}, {"type": "text", "text": "To establish an instance-dependent lower bound for $\\mu(x_{V}^{\\top}\\mathbf{w}_{V})-\\mu(x_{\\tilde{U}_{t}}^{\\top}\\mathbf{w}_{V})$ , we use the following lemma in place of Lemma D.1: ", "page_idx": 48}, {"type": "text", "text": "Lemma I.1. Suppose $\\epsilon\\in(0,1/d\\sqrt{d})$ and define $\\delta:=d/4-|\\tilde{U}_{t}\\cap V|$ . Then, we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mu(x_{V}^{\\top}\\mathbf{w}_{V})-\\mu(x_{\\tilde{U}_{t}}^{\\top}\\mathbf{w}_{V})\\geqslant\\frac{\\kappa^{\\star}(\\mathbf{w}_{V})}{3}\\cdot\\frac{\\delta\\epsilon}{\\sqrt{d}}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "For any $j\\in V$ , define random variables $\\begin{array}{r}{\\tilde{M}_{j}\\;:=\\;\\sum_{t=1}^{T}\\mathbf{1}\\{j\\;\\in\\;\\tilde{U}_{t}\\}}\\end{array}$ . Then, by Lemma I.1, for all V P Vd{4, we get ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathbb{E}_{V}\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\tilde{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})\\geqslant\\frac{\\kappa^{\\star}(\\mathbf{w}_{V})}{3}\\cdot\\frac{\\epsilon}{\\sqrt{d}}\\left(\\frac{d T}{4}-\\sum_{j\\in V}\\mathbb{E}_{V}[\\tilde{M}_{j}]\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Additionally, we define $\\mathcal{V}_{d/4}^{(j)}:=\\left\\{V\\in\\mathcal{V}_{d/4}:j\\in V\\right\\}$ and $\\mathcal{V}_{d/4-1}:=\\{V\\subseteq[d]:|V|=d/4-1\\}$ By averaging both sides of Equation (I.2) with respect to all $V\\in\\mathcal{V}_{d/4}$ , and by following reasoning similar to that in the proof of Theorem 1, we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{\\left|\\mathcal{V}_{d/4}\\right|}\\sum_{V\\in\\mathcal{V}_{d/4}}\\mathbb{E}_{V}\\sum_{i\\in S^{\\star}}p(i|S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\bar{S}_{t}}p(i|\\tilde{S}_{t},\\mathbf{w}_{V})}}\\\\ &{}&{\\geqslant\\frac{\\kappa^{\\star}\\left(\\mathbf{w}_{V}\\right)}{3}\\cdot\\frac{\\epsilon}{\\sqrt{d}}\\left(\\frac{d T}{6}-\\underset{V\\in\\mathcal{V}_{d/4-1}}{\\operatorname*{max}}\\underset{j\\notin V}{\\sum}\\left|\\mathbb{E}_{V\\cup\\{j\\}}[\\tilde{M}_{j}]-\\mathbb{E}_{V}[\\tilde{M}_{j}]\\right|\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "For simplicity, let $P=\\mathbb{P}_{V}$ and $Q=\\mathbb{P}_{V\\cup\\{j\\}}$ . Then, we can bound the term $\\left|\\mathbb{E}_{V\\cup\\{j\\}}[\\tilde{M}_{j}]-\\mathbb{E}_{V}[\\tilde{M}_{j}]\\right|$ in (I.3) for any $V\\in\\mathcal{V}_{d/4-1}$ . ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}_{P}[\\tilde{M}_{j}]-\\mathbb{E}_{Q}[\\tilde{M}_{j}\\right]|\\leqslant\\sum_{t=0}^{T}t\\cdot\\left|P[\\tilde{M}_{j}=t]-Q[\\tilde{M}_{j}=t]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leqslant T\\cdot\\|P-Q\\|_{\\mathrm{TV}}\\leqslant T\\cdot\\sqrt{\\frac{1}{2}\\operatorname{KL}(P\\|Q)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $\\|P-Q\\|_{\\mathrm{TV}}\\;=\\;\\operatorname*{sup}_{A}\\left|P(A)-Q(A)\\right|\\,|$ is the total variation distance between $P$ and $Q$ , $\\mathrm{KL}(P\\|Q)=\\int(\\log\\mathrm{d}P/\\mathrm{d}Q)\\mathrm{d}P$ is s the Kullback-Leibler (KL) divergence between $P$ and $Q$ , and the last inequality \u015fholds by Pinsker\u2019s inequality. We can derive the instance-dependent bound for the KL divergence term in (I.4) by the following lemma: ", "page_idx": 49}, {"type": "text", "text": "Lemma I.2. For any $V\\in\\mathcal{V}_{d/4-1}$ and $j\\in[d]$ , there exists a positive constant $C_{\\mathrm{KL}}>0$ such that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{d}\\mathrm{KL}(P_{V}\\|Q_{V\\cup\\{j\\}})\\leqslant C_{\\mathrm{KL}}\\cdot\\epsilon^{2}\\kappa^{\\star}(\\mathbf{w}_{V})T.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Plugging (I.4) into (I.3), we get ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\left|V_{d/4}\\right|}\\displaystyle\\sum_{V\\in V_{d/4}}\\mathbb{E}_{V}\\displaystyle\\sum_{i\\in S^{*}}p(i|S^{*},\\mathbf{w}_{V})-\\sum_{i\\in S_{t}}p(i|\\Tilde{S}_{t},\\mathbf{w}_{V})}\\\\ &{\\phantom{\\frac{1}{\\left|V_{d/4}\\right|}}\\geqslant\\frac{K^{*}\\left(\\mathbf{w}_{V}\\right)}{3}\\cdot\\frac{\\epsilon}{\\sqrt{d}}\\left(\\frac{d T}{6}-T\\displaystyle\\sum_{j=1}^{d}\\sqrt{\\frac{1}{2}\\mathrm{KL}(P|Q)}\\right)}\\\\ &{\\phantom{\\frac{1}{\\left|V_{d/4}\\right|}}\\geqslant\\frac{K^{*}\\left(\\mathbf{w}_{V}\\right)}{3}\\cdot\\frac{\\epsilon}{\\sqrt{d}}\\left(\\frac{d T}{6}-T\\sqrt{d}\\cdot\\sqrt{\\frac{1}{2}\\displaystyle\\sum_{j=1}^{d}\\mathrm{KL}(P|Q)}\\right)}\\\\ &{\\phantom{\\frac{1}{\\left|V_{d/4}\\right|}}\\geqslant\\frac{K^{*}\\left(\\mathbf{w}_{V}\\right)}{3}\\cdot\\frac{\\epsilon}{\\sqrt{d}}\\left(\\frac{d T}{6}-T\\sqrt{d}\\cdot\\sqrt{\\frac{1}{2}C\\mathrm{KL}\\cdot\\epsilon^{2}\\kappa^{*}\\left(\\mathbf{w}_{V}\\right)T}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the second inequality is due to the Cauchy-Schwartz inequality, and the last inequality is by Lemma I.2. ", "page_idx": 49}, {"type": "text", "text": "By setting \u03f5 \u201c $\\begin{array}{r}{\\epsilon=\\sqrt{\\frac{d}{72C_{\\mathrm{KL}}\\cdot\\kappa^{\\star}(\\mathbf{w}_{V})T}}}\\end{array}$ b72CKL\u00a8\u03ba\u2039pwV qT , we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{\\lvert\\mathcal{V}_{d/4}\\rvert}\\sum_{V\\in\\mathcal{V}_{d/4}}\\mathbb{E}_{V}\\sum_{i\\in S^{\\star}}p(i\\lvert S^{\\star},\\mathbf{w}_{V})-\\sum_{i\\in\\widetilde{S}_{t}}p(i\\lvert\\tilde{S}_{t},\\mathbf{w}_{V})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\geqslant\\displaystyle\\frac{\\kappa^{\\star}(\\mathbf{w}_{V})}{3}\\cdot\\frac{1}{\\sqrt{d}}\\cdot\\sqrt{\\frac{d}{72C_{\\mathrm{KL}}\\cdot\\kappa^{\\star}(\\mathbf{w}_{V})T}}\\cdot\\frac{d T}{12}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\Omega\\left(d\\sqrt{\\kappa^{\\star}(\\mathbf{w}_{V})T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Recall that by construction, $\\kappa^{\\star}\\,=\\,\\operatorname*{max}_{\\mathbf{w}_{V}}\\,\\kappa^{\\star}\\big(\\mathbf{w}_{V}\\big)$ . Thus, by taking the maximum over $\\mathbf{w}_{V}$ , we complete the proof of Proposition 1. ", "page_idx": 49}, {"type": "text", "text": "I.2 Proofs of Lemmas for Proposition 1 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "I.2.1 Proof of Lemma I.1 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Proof of Lemma I.1. For simplicity, let $x=x_{V}$ and $\\hat{x}=x_{\\tilde{U}_{t}}$ . Then, by the mean value theorem, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mu(x^{\\top}\\mathbf{w}_{V})-\\mu(\\hat{x}^{\\top}\\mathbf{w}_{V})=\\displaystyle\\int_{v=1}^{1}\\mu\\left(x^{\\top}\\mathbf{w}_{V}+v(\\hat{x}-x)^{\\top}\\mathbf{w}_{V}\\right)\\left(x-\\hat{x}\\right)^{\\top}\\mathbf{w}_{V}}&{}\\\\ {\\geqslant\\frac{\\mu\\left(x^{\\top}\\mathbf{w}_{V}\\right)}{1+\\left\\vert(x-\\hat{x})^{\\top}\\mathbf{w}_{V}\\right\\vert}\\left(x-\\hat{x}\\right)^{\\top}\\mathbf{w}_{V}}&{}\\\\ {\\geqslant\\frac{\\dot{\\mu}\\left(x^{\\top}\\mathbf{w}_{V}\\right)}{3}\\left(x-\\hat{x}\\right)^{\\top}\\mathbf{w}_{V}}&{}\\\\ {=\\frac{\\kappa^{\\star}\\left(\\mathbf{w}_{V}\\right)}{3}\\left(x-\\hat{x}\\right)^{\\top}\\mathbf{w}_{V}}&{}\\\\ {\\geqslant\\frac{\\kappa^{\\star}\\left(\\mathbf{w}_{V}\\right)}{3}\\cdot\\frac{\\delta\\epsilon}{\\sqrt{d}},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where in the first equality, $\\textstyle{\\bar{x}}$ is the convex combination of $x$ and $\\hat{x}$ , the first inequality holds by Lemma I.3, the second inequality holds by the bounded assumption (Assumption 1), and the last inequality holds by the definition of $\\delta=d/4-|\\tilde{U}_{t}\\cap V|$ . \u53e3 ", "page_idx": 50}, {"type": "text", "text": "I.2.2 Proof of Lemma I.2 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "Proof of Lemma I.2. Consider a fixed round $t$ , an assortment ${\\tilde{S}}_{t}$ , and the set $\\tilde{U}_{t}$ . Let $U\\;=\\;\\tilde{U}_{t}$ . Define $\\begin{array}{r}{m_{j}(\\tilde{S}_{t}):=\\sum_{x_{U}\\in\\tilde{S}_{t}}{\\bf1}\\{j\\in U\\}/K}\\end{array}$ , which captures the average presence of $j\\in U$ across the assortment S\u02dct. For  simplicity, we denote p \u201cv $\\begin{array}{r}{p=\\frac{K\\exp\\left(x_{U}^{\\top}\\mathbf{w}_{V}\\right)}{v_{0}+K\\exp\\left(x_{U}^{\\top}\\mathbf{w}_{V}\\right)}}\\end{array}$ and q \u201cv0\\`K exppUxUJVw YV tYjutjuq. Then, we get ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(\\mathbb{P}_{V}(\\cdot|\\tilde{S}_{t})\\|\\mathbb{P}_{V\\times\\{j\\}}(\\cdot|\\tilde{S}_{t})\\right)\\leqslant\\chi^{2}\\Big(\\mathbb{P}_{V}(\\cdot|\\tilde{S}_{t})\\|\\mathbb{P}_{V\\times\\{j\\}}(\\cdot|\\tilde{S}_{t})\\Big)}\\\\ &{\\quad=\\chi^{2}(\\mathrm{Bernot})\\|+\\rho\\|\\mathrm{\\,Bernoth}(\\,q))}\\\\ &{\\quad=\\frac{(p-q)^{2}}{q}+\\frac{(p-q)^{2}}{1-q}}\\\\ &{\\quad=\\frac{(p-q)^{2}}{q(1-q)}}\\\\ &{\\quad=\\frac{\\left(\\mu(\\boldsymbol{x}_{U}^{\\top}\\mathbf{w}_{V})-\\mu(\\boldsymbol{x}_{U}^{\\top}\\mathbf{w}_{V\\times\\{j\\}})\\right)^{2}}{\\mu(\\boldsymbol{x}_{U}^{\\top}\\mathbf{w}_{V\\times\\{j\\}})}}\\\\ &{\\quad=\\frac{\\left(\\mu(\\boldsymbol{x}_{U}^{\\top}\\mathbf{\\tilde{w}}_{V\\times\\{j\\}})\\right)^{2}}{\\mu(\\boldsymbol{x}_{U}^{\\top}\\mathbf{w}_{V\\times\\{j\\}})}\\left(\\boldsymbol{x}_{U}^{\\top}\\left(\\mathbf{w}_{V}-\\mathbf{w}_{V\\times\\{j\\}}\\right)\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where in the first inequality, we used $\\mathrm{KL}\\leqslant\\chi^{2}$ (Tsybakov [51], Chapter 2), where $\\chi^{2}$ is a chi-square divergence, the second equality holds by the expression of the chi-square divergence for Bernoulli random variables, and the last equality holds by the mean value theorem, where $\\bar{\\bf w}_{V,j}$ is a convex combination of $\\mathbf{w}_{V}$ and $\\mathbf{w}_{V\\cup\\{j\\}}$ . Then, by Lemma I.4, we can further bound the last term as follows: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}\\left(\\mathbb{P}_{V}(\\cdot|\\tilde{S}_{t})\\|\\mathbb{P}_{V\\setminus\\{j\\}}(\\cdot|\\tilde{S}_{t})\\right)\\leqslant\\dot{\\mu}(x_{U}^{\\top}\\mathbf{w}_{V})e^{3\\left|x_{U}^{\\top}\\left(\\mathbf{w}_{V}-\\mathbf{w}_{V\\setminus\\{j\\}}\\right)\\right|}\\left(x_{U}^{\\top}\\left(\\mathbf{w}_{V}-\\mathbf{w}_{V\\setminus\\{j\\}}\\right)\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant\\dot{\\mu}(x_{U}^{\\top}\\mathbf{w}_{V})e^{3/d^{2}}\\left(x_{U}^{\\top}\\left(\\mathbf{w}_{V}-\\mathbf{w}_{V\\setminus\\{j\\}}\\right)\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\dot{\\mu}(x_{U}^{\\top}\\mathbf{w}_{V})e^{1/4}\\left(x_{U}^{\\top}\\left(\\mathbf{w}_{V}-\\mathbf{w}_{V\\setminus\\{j\\}}\\right)\\right)^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\dot{\\mu}(x_{U}^{\\top}\\mathbf{w}_{V})e^{1/4}\\cdot\\frac{m_{j}\\left(\\tilde{S}_{t}\\right)\\epsilon^{2}}{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where the second-to-the last inequality holds based on the assumption that $d\\geqslant4$ , and the last inequality holds since $m_{j}(\\tilde{S}_{t})\\leqslant\\bar{1}$ . ", "page_idx": 50}, {"type": "text", "text": "Now, let us consider $t$ varying over the round $t\\in[T]$ . Then, we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{j=1}^{d}\\mathbf{K}\\mathbf{L}(P_{V}|\\mathcal{Q}_{V\\setminus\\{j\\}})=}&{\\displaystyle\\sum_{j=1}^{d+}\\sum_{k=1}^{d}\\mathbf{\\hat{Z}}_{V}\\left[\\mathbf{K}\\left(\\mathbf{P}_{V}(\\cdot|\\hat{\\tilde{X}}_{v})|\\mathbb{P}_{V\\setminus\\{j\\}}(\\cdot|\\hat{\\tilde{X}}_{v})\\right)\\right]}\\\\ &{\\le e^{\\displaystyle1/d_{\\hat{Z}}}+\\frac{c^{2}}{d}\\frac{\\hat{Z}}{\\displaystyle\\sum_{j=1}^{d}\\sum_{k=1}^{d}\\mathbf{\\hat{Z}}_{V}}\\left[\\hat{\\mu}(c_{v_{T}}^{\\top}\\mathbf{w}_{V})\\cdot m_{j}(\\hat{\\tilde{X}}_{v})\\right]}\\\\ &{=e^{\\displaystyle1/d_{\\hat{Z}}}+\\frac{c^{2}}{d^{2}}\\mathbf{R}\\left[\\displaystyle\\sum_{j=1}^{T}\\left(\\hat{\\mu}(c_{v_{T}}^{\\top}\\mathbf{w}_{v})\\frac{\\hat{\\mathbf{X}}_{j}}{\\displaystyle\\sum_{j=1}^{d}(\\mathbf{\\hat{X}}_{v})}\\right)\\right]}\\\\ &{\\leq e^{\\displaystyle1/d_{\\hat{Z}}}+\\frac{c^{2}}{d^{2}}\\mathbf{R}\\left[\\displaystyle\\sum_{j=1}^{D}\\hat{\\mu}(c_{v_{T}}^{\\top}\\mathbf{w}_{v})\\right]}\\\\ &{\\leqslant e^{\\displaystyle1/d_{\\hat{Z}}}\\frac{c^{2}}{d^{2}}\\left(\\hat{\\mu}(c_{v}^{\\top}\\mathbf{w}_{v})T+\\mathbb{E}_{V}\\left[\\mathbf{R}\\mathbf{e}_{T}(\\mathbf{w}_{v})\\right]\\right)}\\\\ &{=e^{\\displaystyle1/d_{\\hat{Z}}}+\\frac{c^{2}}{d^{2}}\\left(\\hat{\\mu}(\\mathbf{v}_{v})T+\\mathbb{E}_{V}\\left[\\mathbf{R}\\mathbf{e}_{T}(\\mathbf{w}_{v})\\right]\\right)}\\\\ &{\\leqslant C\\mathbf{R}\\left[\\frac{c}{d}\\left(e^{\\displaystyle1}(\\mathbf{w}_{v})T+d\\sqrt{\\mathbf{r}(\\mathbf{w}_{v})T}\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "the equality follows from the chain rule of relative entropy (cf. Exercise 14.11 of Lattimore and Szepesv\u00e1ri [33]), the second inequality holds because $\\textstyle\\sum_{j=1}^{d}m_{j}(\\tilde{S}_{t})\\,\\leqslant\\,\\frac{d}{4}$ , second-to-the last inequality holds by Lemma I.5, and the last inequality ho ld\u0159s by the hypothesis (Equation (I.1)) with $C_{\\mathrm{KL}}>0$ . ", "page_idx": 51}, {"type": "text", "text": "Furthermore, by the definition of $\\kappa$ (Assumption 2), we know that $\\kappa^{\\star}\\big(\\mathbf{w}_{V}\\big)\\geqslant\\kappa$ . Hence, given the assumption that $T\\geqslant d^{2}/\\kappa\\geqslant d^{2}/\\kappa^{\\star}(\\mathbf{w}_{V})$ , we derive that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{d}\\mathrm{KL}(P_{V}\\|Q_{V\\cup\\{j\\}})\\leqslant C_{\\mathrm{KL}}\\cdot\\epsilon^{2}\\kappa^{\\star}(\\mathbf{w}_{V})T,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 51}, {"type": "text", "text": "I.3 Technical Lemmas for Proposition 1 ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Lemma I.3 (Lemma 7 of Abeille et al. [4]). Let $f$ be a strictly increasing function such that $|{\\ddot{f}}|\\leqslant{\\dot{f}}$ , and let $\\mathcal{Z}$ be any bounded interval of $\\mathbb{R}$ . Then, for all $z_{1},z_{2}\\in{\\mathcal{Z}}$ : ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\int_{v=0}^{1}\\dot{f}\\left(z_{1}+v(z_{2}-z_{1})\\right)d v\\geqslant\\frac{\\dot{f}(z)}{1+|z_{1}-z_{2}|},\\quad f o r\\;z\\in\\{z_{1},z_{2}\\}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma I.4 (Lemma 9 of Abeille et al. [4]). Let $f$ be a strictly increasing function such that $|{\\ddot{f}}|\\leqslant{\\dot{f}}$ , and let $\\mathcal{Z}$ be any bounded interval of $\\mathbb{R}$ . Then, for all $z_{1},z_{2}\\in{\\mathcal{Z}}$ : ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\dot{f}(z_{2})\\exp(-|z_{2}-z_{1}|)\\leqslant\\dot{f}(z_{1})\\leqslant\\dot{f}(z_{2})\\exp(|z_{2}-z_{1}|).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma I.5 (Lemma 11 of Goyal and Perivier [26]). Let $\\begin{array}{r}{\\kappa_{t}^{\\star}:=\\sum_{i\\in S_{t}^{\\star}}p_{t}\\big(i\\vert S_{t}^{\\star},\\mathbf{w}^{\\star}\\big)p_{t}\\big(0\\vert S_{t}^{\\star},\\mathbf{w}^{\\star}\\big).}\\end{array}$ . Then, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\sum_{t=1}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})p_{t}\\big(0|S_{t},\\mathbf{w}^{\\star}\\big)\\leqslant\\sum_{t=1}^{T}\\kappa_{t}^{\\star}+\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star}).\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "J Proof of Proposition 2 ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "In this section, we provide the proof of Proposition 2. We introduce useful lemmas to support the proof. ", "page_idx": 51}, {"type": "text", "text": "Lemma J.1 (Theorem 4 of Tran-Dinh et al. [50]). Let $f:\\mathbb{R}^{K}\\rightarrow\\mathbb{R}$ be a $M_{f}$ -self-concordant-like function and let $x,y\\in\\operatorname{dom}(f)$ , then: ", "page_idx": 52}, {"type": "equation", "text": "$$\ne^{-M_{f}\\|y-x\\|_{2}}\\nabla^{2}f(x)\\le\\nabla^{2}f(y).\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof of Proposition 2. We begin the proof with Equation (E.2) from the proof of Theorem 2: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\nabla Q(\\mathbf{u}_{t}^{\\star})^{\\top}(\\mathbf{u}_{t}-\\mathbf{u}_{t}^{\\star})\\leqslant2\\beta_{T}(\\delta)\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})p_{t}(0|S_{t},\\mathbf{w}^{\\star})\\|x_{t i}\\|_{H_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "We then divide the total rounds into two disjoint sets, $J_{1}$ and $J_{2}$ , such that $J_{1}\\bigcup J_{2}=[T]$ . Specifically, let $\\begin{array}{r}{J_{1}\\;=\\;\\{t\\;\\in\\;[T]|\\sum_{i\\in S_{t}}p_{t}(i|S_{t},{\\bf w}^{\\star})p_{t}(0|S_{t},{\\bf w}^{\\star})\\;\\geqslant\\;\\sum_{i\\in S_{t}}p_{t}(i|S_{t},{\\bf w}_{t+1}^{\\leftarrow})p_{t}(0|\\widetilde{S}_{t}^{\\leftarrow},{\\bf w}_{t+1}^{\\leftarrow})\\}}\\end{array}$ and $J_{2}=[T]\\backslash J_{1}$ . For a b e\u0159tter presentation, we also define th at\u0159: ", "page_idx": 52}, {"type": "equation", "text": "$$\ng_{t}(S;\\mathbf{w}):=\\sum_{i\\in S}p_{t}(i|S,\\mathbf{w})p_{t}(0|S,\\mathbf{w})\\lVert x_{t i}\\rVert_{H_{t}^{-1}}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, we can rewrite the right-hand side of (J.1) as follows: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle2\\beta_{T}(\\delta)\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{\\star})p_{t}(0|S_{t},\\mathbf{w}^{\\star})\\lVert x_{t i}\\rVert_{H_{t}^{-1}}}\\\\ {\\displaystyle=2\\beta_{T}(\\delta)\\sum_{t=1}^{T}g_{t}(S_{t};\\mathbf{w}^{\\star})}\\\\ {\\displaystyle=2\\beta_{T}(\\delta)\\sum_{t\\in J_{1}}g_{t}(S_{t};\\mathbf{w}^{\\star})+2\\beta_{T}(\\delta)\\sum_{t\\in J_{2}}g_{t}(S_{t};\\mathbf{w}^{\\star}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "To bound $\\begin{array}{r}{\\sum_{t\\in J_{1}}g_{t}(S_{t};\\mathbf{w}^{\\star})}\\end{array}$ , by the mean value theorem, we get ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\sum_{i\\in J_{1}}g_{t}(S_{t};\\mathbf{w}^{\\star})=\\sum_{i\\in J_{1}}g_{t}(S_{t};\\mathbf{w}_{t+1})+\\sum_{i\\in J_{1}}\\nabla_{\\mathbf{w}}g_{t}(S_{t};\\bar{\\mathbf{w}}_{t})^{\\top}(\\mathbf{w}^{\\star}-\\mathbf{w}_{t+1}),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\bar{\\bf w}_{t}$ is the convex combination of $\\mathbf{w}^{\\star}$ and $\\mathbf{w}_{t+1}$ . The first term of (J.2) can be bound by ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\in J_{1}}g_{t}(S_{t};\\mathbf w_{t+1})}\\\\ &{\\lesssim\\sqrt{\\displaystyle\\sum_{t\\in J_{1}}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w_{t+1})p_{t}(0|S_{t},\\mathbf w_{t+1})}\\sqrt{\\displaystyle\\sum_{t\\in J_{1}}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w_{t+1})p_{t}(0|S_{t},\\mathbf w_{t+1})\\|x_{t}\\|_{H_{t}^{-1}}^{2}}}\\\\ &{\\leqslant\\sqrt{\\displaystyle\\sum_{t\\in J_{1}}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})p_{t}(0|S_{t},\\mathbf w^{\\star})}\\cdot\\tilde{\\mathcal{O}}\\left(\\sqrt{d}\\right)\\leqslant\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\kappa_{t}^{\\star}+\\mathbf R\\mathbf e\\mathbf g_{T}(\\mathbf w^{\\star})}\\cdot\\tilde{\\mathcal{O}}\\left(\\sqrt{d}\\right),\\quad\\mathrm{~(2)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where the second-to-the last inequality holds by the definition of $J_{1}$ and the elliptical potential lemma (Lemma E.2), and the last inequality holds by the following Lemma: ", "page_idx": 52}, {"type": "text", "text": "Moreover, the second term of (J.2) can be bounded as follows: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i\\in J_{1}}\\nabla_{\\theta}\\mu(S_{i}\\cdot\\mathbf{w}_{i})^{\\top}(\\mathbf{w}^{*}-\\mathbf{w}_{i+1})\\Bigg\\rvert}\\\\ &{=\\displaystyle\\left\\lvert\\sum_{i\\in J_{1}}p_{i}(\\theta)\\boldsymbol{S}_{i}\\cdot\\mathbf{w}_{i}\\right\\rvert\\sum_{\\theta\\in\\mathcal{S}_{i}}p_{i}(\\vert\\boldsymbol{S}_{i}\\cdot\\mathbf{w}_{i}\\vert_{\\theta\\in}^{\\top}\\vert\\mathbf{w}^{*}-\\mathbf{w}_{i+1})\\vert\\boldsymbol{x}_{i}\\vert\\,\\mu_{i}-}\\\\ &{-2p_{i}(\\theta)\\boldsymbol{S}_{i}\\cdot\\boldsymbol{\\Psi}_{i}(i\\vert\\boldsymbol{S}_{i},\\Psi_{i})\\vert\\boldsymbol{x}_{i}\\vert\\,\\mu_{i}-\\displaystyle\\sum_{j\\in\\mathcal{S}_{i}}p_{i}(j\\vert\\boldsymbol{S}_{i},\\tilde{\\mathbf{w}}_{i})\\boldsymbol{\\Sigma}_{i j}^{\\top}(\\mathbf{w}^{*}-\\mathbf{w}_{i+1})\\Bigg\\rvert}\\\\ &{\\leqslant\\delta_{T}(\\delta)\\displaystyle\\sum_{i\\in\\mathcal{S}_{i}}\\sum_{i\\in\\mathcal{S}_{i}}p_{i}(\\vert\\boldsymbol{S}_{i},\\Psi_{i}\\vert)\\boldsymbol{x}_{i}\\vert\\mu_{i}^{-1}+2\\delta_{T}(\\delta)\\displaystyle\\sum_{\\theta\\in\\mathcal{S}_{i}}\\left(\\sum_{i\\in\\mathcal{S}_{i}}p_{i}(\\vert\\boldsymbol{S}_{i},\\Psi_{i}\\vert)\\boldsymbol{x}_{i}\\vert_{\\theta\\iota}^{-1}\\right)^{2}}\\\\ &{\\leqslant\\delta_{T}(\\delta)\\displaystyle\\sum_{i\\in\\mathcal{S}_{i}}\\operatorname*{max}_{i\\in\\mathcal{S}_{i}}\\left\\lvert\\sum_{\\mu=1}^{d}+2\\delta_{T}(\\delta)\\displaystyle\\sum_{\\theta\\in\\mathcal{S}_{i}}\\left(\\sum_{\\mu=1}^{\\operatorname*{max}}\\left(\\sum_{\\theta\\in\\mathcal{S}_{i}}p_{i}\\right)\\left(\\sum_{\\mu=1}^{d}\\right)\\right)^{2}}\\\\ &{\\leqslant\\delta_{T}(\\delta)\\displaystyle\\sum_{i\\in\\mathcal{S}_{i}}\\operatorname*{max}_{i\\in\\mathcal{S}_{i}}\\left\\lvert\\sum_{\\mu=1}^{d}+2\\delta_{T}(\\delta)\\displaystyle\\sum_{\\theta\\in\\mathcal{S}_{i\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the first inequality holds by Lemma 1, and the last inequality holds by Lemma E.2. Combining (J.3) and (J.4), we can bound the term $\\begin{array}{r}{\\sum_{t\\in J_{1}}g_{t}(S_{t};\\mathbf{w}^{\\star})}\\end{array}$ as follows: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\sum_{t\\in J_{1}}g_{t}(S_{t};\\mathbf{w}^{\\star})\\leqslant\\tilde{\\mathcal{O}}\\left(\\sqrt{d}\\cdot\\sqrt{\\sum_{t=1}^{T}\\kappa_{t}^{\\star}+\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star})}+d^{3/2}/\\kappa\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Now, we bound $\\begin{array}{r}{\\sum_{t\\in J_{2}}g_{t}(S_{t};\\mathbf{w}^{\\star})}\\end{array}$ . We define $\\begin{array}{r}{H_{t}^{\\star}:=\\lambda\\mathbf{I}_{d}+\\sum_{s=1}^{t-1}\\mathcal{G}_{s}(\\mathbf{w}^{\\star})}\\end{array}$ . Recall that $\\nabla^{2}\\ell_{s}(\\mathbf{w})=$ $\\mathcal{G}_{s}(\\mathbf{w})$ and $\\ell_{s}$ is $3\\sqrt{2}$ -self-concordant-like function (Proposi ti\u0159on C.1). Then, by Lemma J.1, we get $H_{t}\\geq H_{t}^{\\star}e^{-3\\sqrt{2}}$ . With this fact, we can now bound the term $\\begin{array}{r}{\\sum_{t\\in J_{2}}g_{t}(S_{t};\\mathbf{w}^{\\star})}\\end{array}$ : ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\in J_{2}}g_{t}(S_{t};\\mathbf{w}^{*})\\leqslant\\sqrt{\\displaystyle\\sum_{t\\in J_{2}}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{*})p_{t}(0|S_{t},\\mathbf{w}^{*})}\\sqrt{\\displaystyle\\sum_{t\\in J_{2}}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{*})p_{t}(0|S_{t},\\mathbf{w}^{*})}\\|;}\\\\ &{\\leqslant\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\kappa_{t}^{*}+\\mathbf{Reg}_{T}}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{*})p_{t}(0|S_{t},\\mathbf{w}^{*})\\|x_{t}\\|_{H_{t}^{-1}}^{2}}}\\\\ &{\\leqslant\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\kappa_{t}^{*}+\\mathbf{Reg}_{T}}\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf{w}^{*})p_{t}(0|S_{t},\\mathbf{w}^{*})\\|x_{t}\\|_{(H_{t}^{*})^{-1}}^{2}}}\\\\ &{\\leqslant\\bar{\\mathcal{O}}\\left(\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\kappa_{t}^{*}+\\mathbf{Reg}_{T}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "where the second inequality follows from Lemma I.5, second-to-the last inequality holds because $H_{t}\\,\\geq\\,H_{t}^{\\star}e^{-3\\sqrt{2}}$ , and the last inequality is obtained by a slight modification of Lemma E.2 with $\\mathbf{w}_{1},\\dots,\\mathbf{w}_{T}=\\mathbf{w}^{\\star}$ . ", "page_idx": 53}, {"type": "text", "text": "Now, by plugging (J.5) and (J.6) into (J.1), and using the fact that $\\beta_{T}(\\delta)=\\tilde{\\mathcal{O}}(\\sqrt{d})$ , we obtain that ", "page_idx": 53}, {"type": "equation", "text": "$$\n2\\beta_{T}(\\delta)\\sum_{t=1}^{T}\\sum_{i\\in S_{t}}p_{t}(i|S_{t},\\mathbf w^{\\star})p_{t}(0|S_{t},\\mathbf w^{\\star})\\lVert x_{t i}\\rVert_{H_{t}^{-1}}=\\tilde{O}\\left(d\\sqrt{\\sum_{t=1}^{T}\\kappa_{t}^{\\star}+\\mathbf{Re}\\mathbf{g}_{T}(\\mathbf w^{\\star})}+d^{3/2}/\\kappa\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Furthermore, by applying the same analysis as in the proof of Theorem 2 in order to bound the second-order term in the Taylor expansion (term (B) in (E.1)), we get ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star})=\\tilde{\\mathcal{O}}\\left(d\\sqrt{\\sum_{t=1}^{T}\\kappa_{t}^{\\star}+\\mathbf{Reg}_{T}(\\mathbf{w}^{\\star})}+d^{2}/\\kappa\\right).\n$$", "text_format": "latex", "page_idx": 53}, {"type": "image", "img_path": "Q4NWfStqVf/tmp/c27afff16ae71f8a5ea569452e0fab37805443c0679a037389879ba4127eb780.jpg", "img_caption": ["K Experiment Details and Additional Results ", "Figure K.1: Runtime per round under uniform rewards (first row) and non-uniform rewards (second row). "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "Q4NWfStqVf/tmp/5ce64a17a1579139094fb547e332180610b160fcbdf7e6fbd7869d60ffcd9b33.jpg", "img_caption": ["Figure K.2: Cumulative regret under uniform rewards with $v_{0}=\\Theta(K)$ . "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "For each instance, we sample the true parameter $\\mathbf{w}^{\\star}$ from a uniform distribution in $[-1/\\sqrt{d},1/\\sqrt{d}]^{d}$ . For the context features $x_{t i}$ , we sample each $x_{t i}$ independently and identic?ally dis?tributed (i.i.d.) from a multivariate Gaussian distribution $\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{I}_{d})$ and clip it to range $[-1/\\sqrt{d},1/\\sqrt{d}]^{d}$ . Therefore, we ensure that $\\|\\mathbf{w}^{\\star}\\|_{2}\\leqslant1$ and $\\|{\\boldsymbol x}_{t i}\\|_{2}\\leqslant1$ , satisfying Assumption 1. For each experimental configuration, we conducted 20 independent runs for each instance and reported the average cumulative regret (Figure 1) and runtime per round (Figure K.1) for each algorithm. The error bars in Figure 1 and K.2 represent the standard deviations (1-sigma error). We have omitted the error bars in Figure K.1 because they are minimal. ", "page_idx": 54}, {"type": "text", "text": "In the uniform reward setting where $r_{t i}\\;=\\;1$ , the combinatorial optimization step to select the assortment simply involves sorting items by their utility estimate. In contrast, in the non-uniform reward setting, rewards are sampled from a uniform distribution in each round, i.e., $r_{t i}\\sim\\mathrm{Unif}(0,1)$ . For combinatorial optimization in this setting, we solve an equivalent linear programming (LP) problem that is solvable in polynomial-time [48, 21]. The experiments are run on Xeon(R) Gold 6226R CPU $@$ 2.90GHz (16 cores). ", "page_idx": 54}, {"type": "text", "text": "Figure K.1 presents additional empirical results on the runtime per round. Our algorithm OFU-MNL $^+$ demonstrates a constant computation cost for each round, while the other algorithms exhibit a linear dependence on $t$ . It is also noteworthy that the runtime for uniform rewards is approximately 10 times faster than that for non-uniform rewards. This difference arises because we use linear programming (LP) optimization for assortment selection in the non-uniform reward setting, which is more computationally intensive. ", "page_idx": 54}, {"type": "text", "text": "", "page_idx": 55}, {"type": "text", "text": "Furthermore, Figure K.2 illustrates the cumulative regrets of the proposed algorithm compared to other baseline algorithms under uniform rewards with $\\dot{v}_{0}=K/5$ . Since $v_{0}$ is proportional to $K$ , an increase in $K$ does not improve the regret. This observation is also consistent with our theoretical results. ", "page_idx": 55}, {"type": "text", "text": "L Technical Errors in Agrawal et al. [5] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "In this section, we discuss the technical errors in Agrawal et al. [5]. There are two main significant errors: There are mainly two significant errors: ", "page_idx": 55}, {"type": "text", "text": "1. Equation (16). ", "text_level": 1, "page_idx": 55}, {"type": "equation", "text": "$$\n\\alpha_{i}(\\mathbf{X}_{S_{t}},\\boldsymbol{\\theta}_{t},\\boldsymbol{\\theta}^{\\star})\\boldsymbol{x}_{t i}:=\\mu_{i}(\\mathbf{X}_{S_{t}}^{\\top}\\boldsymbol{\\theta}^{\\star})-\\mu_{i}(\\mathbf{X}_{S_{t}}^{\\top}\\boldsymbol{\\theta}_{t}),\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "where $\\mathbf{X}_{S_{t}}$ is a design matrix whose columns are the attribute vectors $x_{t i}$ of the items in the assortment $S_{t}$ and $\\mu_{i}(\\mathbf{X}_{S_{t}}^{\\top}\\theta)=P_{t}(i|S_{t},\\theta)$ . ", "page_idx": 55}, {"type": "text", "text": "It appears that the authors may have intended to derive this equation using a first-order exact Taylor expansion. However, in MNL bandits, this equation generally does not hold. Consider a counterexample where $x_{t i}\\,=\\,0$ , $x_{t j}\\neq0$ for $j\\neq i$ , and $\\theta^{\\star}\\neq\\theta_{t}$ . Then, the left-hand side of (L.1) is equals to 0 (since $x_{t i}\\,=\\,0.$ ), but the right-hand side of (L.1) is not 0, because the denominators of each $\\mu_{i}(\\mathbf{X}_{S_{t}}^{\\top}\\theta^{\\star})$ and $\\mu_{i}(\\mathbf{X}_{S_{t}}^{\\top}\\theta_{t})$ differ. This equation only holds in special cases, such as when $K=1$ , which corresponds to the logistic bandit case. Equation (16) in Agrawal et al. [5] serves as the foundation for the entire proof in their paper. Consequently, all subsequent results derived from it are also incorrect. ", "page_idx": 55}, {"type": "text", "text": "2. Cauchy-Schwarz inequality in the regret analysis (Page 46) ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "When using the Cauchy-Schwarz inequ?ality on the regret before applying the elliptical potential lemma, they indeed incur an additional $\\sqrt{K}$ factor: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\operatorname*{min}\\left(\\sum_{i\\in S_{t}}\\left\\|x_{t i}\\right\\|_{{\\bf J}_{t}^{-1}},1\\right)\\leqslant\\sqrt{K T}\\sqrt{\\operatorname*{min}\\left(\\sum_{i\\in S_{t}}\\left\\|x_{t i}\\right\\|_{{\\bf J}_{t}^{-1}}^{2},1\\right)}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Hence, their regret should actually be $\\tilde{\\mathcal{O}}(d\\sqrt{K T}+d^{2}/\\kappa)$ , which is worse than the result in our Theorem 2 (upper bound under uniform rewards) by a factor of $K$ . ", "page_idx": 55}, {"type": "text", "text": "M Limitations ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "To determine the optimistic assortment in line 5 of Algorithm 1, we must calculate the optimistic utility $\\alpha_{t i}$ for each item $i\\;\\in\\;[N]$ . Consequently, the computational cost for each round scales polynomially with $N$ , the number of items. If $N$ is very large, or infinite, our proposed algorithm becomes intractable. It is important to note that much of the existing literature on contextual bandits [2, 10, 24, 1, 20, 6\u20138, 16, 41, 42, 4, 45] also requires enumerating all items to select the item for each round. ", "page_idx": 55}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: Abstract and Section 1 ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 56}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Justification: Section M in Appendix ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 56}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: Assumption 1 and 2 ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 57}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: Secion 7 and Section K ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 57}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 57}, {"type": "text", "text": "Answer: [No] ", "page_idx": 58}, {"type": "text", "text": "Justification: We have included the code in the supplementary material. After our paper is accepted, we will provide open access to the code. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 58}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: Section 7 and Section K Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 58}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Justification: Figure 1 and K.2. We omitted the error bars in Figure K.1 because they are too minimal to be significant. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 58}, {"type": "text", "text": "", "page_idx": 59}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: Section K ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 59}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 59}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 59}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 59}, {"type": "text", "text": "Justification: This paper focuses on theoretical results and therefore does not discuss societal impacts. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 59}, {"type": "text", "text": "", "page_idx": 60}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: No risks. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 60}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 60}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 61}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 61}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 61}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 61}]