[{"type": "text", "text": "Online Control with Adversarial Disturbance for Continuous-time Linear Systems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jingwei Li IIIS, Tsinghua University Shanghai Qizhi Institute ljw22@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Jing Dong The Chinese University of Hong Kong, Shenzhen jingdong@link.cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Can Chang IIIS, Tsinghua University cc22@mails.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Baoxiang Wang The Chinese University of Hong Kong, Shenzhen bxiangwang@cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Jingzhao Zhang IIIS, Tsinghua University Shanghai Qi zhi Institute jingzhaoz@mail.tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study online control for continuous-time linear systems with finite sampling rates, where the objective is to design an online procedure that learns under nonstochastic noise and performs comparably to a fixed optimal linear controller. We present a novel two-level online algorithm, by integrating a higher-level learning strategy and a lower-level feedback control strategy. This method offers a practical and robust solution for online control, which achieves sublinear regret. Our work provides the first nonasymptotic results for controlling continuous-time linear systems with finite number of interactions with the system. Moreover, we examine how to train an agent in domain randomization environments from a non-stochastic control perspective. By applying our method to the SAC (Soft Actor-Critic) algorithm, we achieved improved results in multiple reinforcement learning tasks within domain randomization environments. Our work provides new insights into non-asymptotic analyses of controlling continuous-time systems. Furthermore, our work brings practical intuition into controller learning under non-stochastic environments. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A major challenge in robotics is to deploy simulated controllers into real-world. This process, known as sim-to-real transfer, can be difficult due to misspecified dynamics, unanticipated real-world perturbations, and non-stationary environments. Various strategies have been proposed to address these issues, including domain randomization, meta-learning, and domain adaptation [20, 10, 21]. Although they have shown great effectiveness in experimental results, training agents within these setups poses a significant challenge. To accommodate different environments, the strategies developed by agents tend to be overly conservative [26, 4] or lead to suboptimal outcomes [43, 27]. ", "page_idx": 0}, {"type": "text", "text": "In this work, we provide an analysis of the sim-to-real transfer problem from an online control perspective. Online control focuses on iteratively updating the controller after deployment (i.e., online) based on collected trajectories. Significant progress has been made in this field by applying insights from online learning to linear control problems [2, 1, 12, 19, 11, 8, 6, 16]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Following this line of work, we approach the sim-to-real transfer issue for continuous-time linear systems as a non-stochastic control problem, as explored in previous works [19, 11, 8]. These studies provide regret bounds for an online controller that lacks prior knowledge of system perturbations. However, a gap remains as no previous analysis has specifically investigated continuous-time systems, but real world systems often evolve continuously in time. ", "page_idx": 1}, {"type": "text", "text": "Existing literature on online continuous control is limited [42, 22, 13, 32]. Most continuous control research emphasizes the development of model-free algorithms, such as policy iteration, under the assumption of noise absence. Recently, [8] examined online continuous-time linear quadratic control problem and achieves sublinear regret. However, it relies on the assumption of standard Brownian noise instead of non-stochastic noise that may not always hold true in real-world applications. This leads us to the crucial question: ", "page_idx": 1}, {"type": "text", "text": "Is it possible to design an online non-stochastic control algorithm in a continuous-time setting that achieves sublinear regret? ", "page_idx": 1}, {"type": "text", "text": "Our work addresses this question by proposing a two-level online controller. The higher-level controller symbolizes the policy learning process and updates the policy at a low frequency to minimize regret. Conversely, the lower-level controller delivers high-frequency feedback control input to reduce discretization error. Our proposed algorithm results in regret bounds for continuoustime linear control in the face of non-stochastic disturbances. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we implement the ideas from our theoretical analysis and test them in several experiments. Note that the key difference between our algorithm and traditional online policy optimization is that we utilize information from past states with some skips to enable faster adaptation to environmental changes. Although the aforementioned concepts are often adopted experimentally as frame stacking and frame skipping, there is relatively little known about the appropriate scenarios for applying these techniques. Our analysis and experiments demonstrate that these techniques are particularly effective in developing adaptive policies for uncertain environments. We choose the task of training agents in a domain randomization environment to evaluate our method, and the results confirm that these techniques substantially improve the agents\u2019 performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The control theory of linear dynamical systems under disturbances has been thoroughly examined in various contexts, such as linear quadratic stochastic control [7], robust control [37, 23], system identification [17, 24, 9, 25]. However, most of these problems are investigated in non-robust settings, with robust control being the sole exception where adversarial perturbations in the dynamic are permitted. In this scenario, the controller solves for the optimal linear controller in the presence of worst-case noise. Nonetheless, the algorithms designed in this context can be overly conservative as they optimize over the worst-case noise, a scenario that is rare in real-world applications. We will elaborate on the difference between robust control and online non-stochastic control in Section 3. ", "page_idx": 1}, {"type": "text", "text": "Online Control There has been a recent surge of interest in online control, as demonstrate by studies such as [2, 1, 12]. In online control, the player interacts with the environment and updates the policy in each round aiming to achieve sublinear regre\u221at. In scenarios with stochastic Gaussian noise, [12] provides the first efficient algorithm with an $O({\\sqrt{T}})$ regret bound. However, in real-world applications, the assumption of Gaussian distribution is often unfulfilled. ", "page_idx": 1}, {"type": "text", "text": "[3] pioneers research on non-stochastic online control, where the noises can be adversarial. Under general convex costs, they introduce the Disturbance-Action Policy\u221a Class. Using an online convex optimization (OCO) algorithm with memory, they achieve an $O({\\sqrt{T}})$ regret bound. Subsequent studies extend this approach to other scenarios, such as quadratic costs [8], partial observations [36, 35] or unknown dynamical systems [19, 11]. Other works yield varying theoretical guarantees like online competitive ratio [15, 33]. ", "page_idx": 1}, {"type": "text", "text": "Online Continuous Control Compared to online control, there has been relatively little research on model-based continuous-time control. Most continuous control works focus on developing model-free algorithms such as policy iteration (e.g. [42, 22, 32]), typically assuming zero-noise. This is because analyzing the system when transition dynamics are represented by differential equations, rather than recurrence formulas, poses a significant challenge. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Recently, [8] studies online continuous-time linear quadratic control with standard Brownian noise and unknown system dynamics. They propose an algorithm based on the least-square method, which estimates the system\u2019s coefficients and solves the corresponding Riccati equation. The papers [34, 14] also focus on online control s\u221aetups with continuous-time stochastic linear systems and unknown dynamics. They achieve $O(\\sqrt{T}\\,\\mathbf{\\bar{log}}\\,T)$ regret by different approaches. [34] uses the Thompson sampling algorithm to learn optimal actions. [14] takes a randomized-estimates policy to balance exploration and exploitation. The main difference between [8, 34, 14] and our paper is that they consider stochastic noise of Brownian motion which can be quite stringent and may fail in real-world applications, while the noise in our setup is non-stochastic. This makes our analysis completely different from theirs. ", "page_idx": 2}, {"type": "text", "text": "Domain Randomization Domain randomization, which is proposed by [39], is a commonly used technique for training agents to adapt to different (real) environments by training in randomized simulated environments. From the empirical perspective, many previous works focus on designing efficient algorithms for learning in a randomized simulated environment (by randomizing environmental settings, such as friction coefficient) such that the algorithm can adapt well in a new environment, [29, 44, 26, 28, 30]. Other works study how to effectively randomize the simulated environment so that the trained algorithm would generalize well in other environments [43, 27, 38]. However, prior research has not explored how to apply certain theoretical analysis ideas to train agents in domain-randomized environments. Limited previous works, such as [10] and [21], concentrate on theoretically analyzing the sim-to-real gap within specific domain randomization models but they do not test their algorithms in real domain randomization environments. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Setting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we consider the online non-stochastic control for continuous-time linear systems.   \nTherefore, we provide a brief overview below and define our notations. ", "page_idx": 2}, {"type": "text", "text": "3.1 Continuous-time Linear Systems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The Linear Dynamical System can be considered a specific case of a continuous Markov decision process with linear transition dynamics. The state transitions are governed by the following equation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\dot{x}}_{t}=A x_{t}+B u_{t}+w_{t}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{t}$ is the state at time $t$ , $u_{t}$ is the action taken by the controller at time $t$ , and $w_{t}$ represents the disturbance at time $t$ . Follow the setup of [3], we assume $x_{0}=0$ . We do not make any strong assumptions about the distribution of $w_{t}$ , and we also assume that the distribution of $w_{t}$ is unknown to the learner beforehand. This implies that the disturbance sequence $w_{t}$ can be selected adversarially. ", "page_idx": 2}, {"type": "text", "text": "When the action $u_{t}$ is applied to the state $x_{t}$ , a cost $c_{t}(x_{t},u_{t})$ is incurred. Here, we assume that the cost function $c_{t}$ is convex. However, this cost is not known in advance and is only revealed after the action $u_{t}$ is implemented at time $t$ . In the system described above, an online policy $\\pi$ is defined as a function that maps known states to actions, i.e., $u_{t}=\\pi(\\{x_{\\xi}|\\xi\\in[0,t]\\})$ . Our goal, then, is to design an algorithm that determines such an online policy to minimize the cumulative cost incurred. Specifically, for any algorithm $\\boldsymbol{\\mathcal{A}}$ , the cost incurred over a time horizon $T$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ_{T}(A)=\\int_{0}^{T}c_{t}(x_{t},u_{t})d t\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In scenarios where the policy is linear (i.e., a linear controller), such that $u_{t}=-K x_{t}$ , we use $J(K)$ to denote the cost of a policy $K\\in\\kappa$ from a certain class $\\kappa$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Difference between Robust and Online Non-stochastic Control ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "While both robust and online non-stochastic control models incorporate adversarial noise, it\u2019s crucial to understand that their objectives differ significantly. ", "page_idx": 2}, {"type": "text", "text": "The objective function for robust control, as seen in [37, 23], is defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{u_{1}}\\operatorname*{max}_{w_{1:T}}\\operatorname*{min}_{u_{2}}...\\operatorname*{min}_{u_{t}}\\operatorname*{max}_{w_{T}}J_{T}(A)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Meanwhile, the objective function for online non-stochastic control, as discussed in [3], is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A}\\operatorname*{max}_{w_{1:T}}(J_{T}(A)-\\operatorname*{min}_{K\\in K}J_{T}(K))\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that the robust control approach seeks to directly minimize the cost function, while online non-stochastic control targets the minimization of regret, which is the discrepancy between the actual cost and the cost associated with a baseline policy. Additionally, in robust control, the noise at each step can depend on the preceding policy, whereas in online non-stochastic control, all the noise is predetermined (though unknown to the player). ", "page_idx": 3}, {"type": "text", "text": "3.3 Assumptions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We operate under the following assumptions throughout this paper. To be concise, we denote $\\|\\cdot\\|$ as the $L_{2}$ operator norm of the vector and matrix. Firstly, we make assumptions concerning the system dynamics and noise: ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. The matrices that govern the dynamics are bounded, meaning $\\|A\\|\\,\\leq\\,\\kappa_{A}$ and $\\|B\\|\\leq\\kappa_{B}$ , where $\\kappa_{A}$ and $\\kappa_{B}$ are constants. Moreover, the perturbation and its derivative are both continuous and bounded: $\\|w_{t}\\|\\,,\\|\\dot{w}_{t}\\|\\leq W$ , with $W$ being a constant. ", "page_idx": 3}, {"type": "text", "text": "These assumptions ensure that we can bound the states and actions, as well as their first and secondorder derivatives. Next, we make assumptions regarding the cost function: ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. The costs $c_{t}(x,u)$ are convex in $x$ and $u$ . Additionally, if there exists a constant $D$ such that $\\|x\\|,\\|u\\|\\leq\\ D$ , then we have the following inequalities of the costs: $|c_{t}(x,u)|\\;\\leq$ $\\beta D^{2},\\|\\nabla_{x}c_{t}(x,\\bar{u})\\|\\,,\\|\\nabla_{u}c_{t}(x,u)\\|\\le G D,$ , $|c_{t_{1}}(x,u)-\\bar{c_{t_{2}}}(x,\\bar{u})|\\leq L|t_{1}-t_{2}|D^{2}$ , ", "page_idx": 3}, {"type": "text", "text": "where $\\beta,G$ and $L$ are constants corresponding to the cost function. This assumption implies that if the differences between states and actions are small, then the error in their cost will also be relatively small. ", "page_idx": 3}, {"type": "text", "text": "3.4 Strongly Stable Policy ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We next describe our baseline policy class introduced in [12]. Note that the continuous system and the discrete system are different. If we consider the approximation over a relatively small interval $h$ , we get ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{t+h}=x_{t}+\\int_{s=t}^{t+h}\\dot{x}_{s}d s=x_{t}+\\int_{s=t}^{t+h}A x_{s}+B u_{s}+w_{s}d s\\qquad\\qquad\\qquad\\qquad\\quad\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Therefore, if we consider the transition of a discrete system $x_{i+1}\\,=\\,\\tilde{A}x_{i}+\\tilde{B}u_{i}+\\tilde{w}_{i}$ , we get the approximation $\\tilde{A}\\approx I+h A$ , $\\tilde{B}\\approx h B$ . Hence, we extend the definition of a strongly stable policy [12, 3] in the discrete system to the continuous system as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 1. A linear policy $K$ is $(\\kappa,\\gamma)$ -strongly stable if, for any $h>0$ that is sufficiently small, there exist matrices $L_{h},P$ such that $I+\\dot{h}(A-\\bar{B}\\dot{K})=P L_{h}P^{-1}$ , with the following two conditions: ", "page_idx": 3}, {"type": "text", "text": "1. The norm of $L_{h}$ is strictly smaller than unity and dependent on $h$ , i.e., $\\|L_{h}\\|\\leq1-h\\gamma$ .   \n2. The controller and transforming matrices are bounded, i.e., $\\|K\\|\\leq\\kappa$ and $\\|P\\|,\\|P^{-1}\\|\\leq\\kappa$ . ", "page_idx": 3}, {"type": "text", "text": "The above definition ensures the system can be stabilized by a linear controller $K$ . ", "page_idx": 3}, {"type": "text", "text": "3.5 Regret Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To evaluate the designed algorithm, we follow the setup in [12, 3] and use regret, which is defined as the cumulative difference between the cost incurred by the policy of our algorithm and the cost incurred by the best policy in hindsight. Let $\\kappa$ denotes the class of strongly stable linear policies, i.e. $\\begin{array}{r}{\\mathcal{K}=\\{K:K}\\end{array}$ is $(\\kappa,\\gamma)$ -strongly stable $\\}$ . Then we try to minimize the regret of algorithm: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A}\\operatorname*{max}_{w_{1:T}}\\operatorname{Regret}(A)=\\operatorname*{min}_{A}\\operatorname*{max}_{w_{1:T}}(J_{T}(A)-\\operatorname*{min}_{K\\in K}J_{T}(K))\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4 Algorithm Design ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we outline the design of our algorithm and formally define the concepts involved in deriving our main theorem. We summarize our algorithm design as follows: ", "page_idx": 4}, {"type": "text", "text": "First, we discretize the total time period $T$ into smaller intervals of length $h$ . We use the information at each point $x_{h},x_{2h},\\ldots$ and $u_{h},u_{2h},\\ldots$ to approximate the actual cost of each time interval, leveraging the continuity assumption. This process does introduce some discretization errors. ", "page_idx": 4}, {"type": "text", "text": "Next, we employ the Disturbance-Action policy (DAC) [3]. This policy selects the action based on the current time step and the estimations of disturbances from several past steps. This policy can approximate the optimal linear policy in hindsight when we choose suitable parameters. However, the optimal policy $K^{*}$ is unknown, so we cannot directly acquire the optimal choice. To overcome this, we employ the OCO with memory framework [5] to iteratively adjust the DAC policy parameter $M_{t}$ to approximate the optimal solution $M^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "After that, we introduce the concept of the ideal state $y_{t}$ and ideal action $v_{t}$ that approximate the actual state $x_{t}$ and action $u_{t}$ . Note that both the state and policy depend on all DAC policy parameters $M_{1},M_{2},\\ldots,M_{t}$ . Yet, the OCO with memory framework only considers the previous $H$ steps. Therefore, we need to consider ideal state and action. $y_{t}$ and $v_{t}$ represent the state the system would reached if it had followed the DAC policy $\\{M_{t-H},\\ldots,M_{t}\\}$ at all time steps from $t-H$ to $t$ , under the assumption that the state $x_{t-H}$ was 0. ", "page_idx": 4}, {"type": "text", "text": "From all the analysis above, we can decompose the regret as three parts: the discretization error $R_{1}$ , the regret of the OCO with memory $R_{2}$ , and the approximation error between the ideal cost and the actual cost $R_{3}$ . ", "page_idx": 4}, {"type": "text", "text": "Then we will formally introduce out method and define all the concepts. In the subsequent discussion, we use shorthand notation to denote the cost, state, control, and disturbance variables $c_{i h}$ , $x_{i h}$ , $u_{i h}$ , and $w_{i h}$ as $c_{i}$ , $x_{i}$ , $u_{i}$ , and $w_{i}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "First, we need to define the Disturbance-Action Policy Class(DAC) for continuous systems: ", "page_idx": 4}, {"type": "text", "text": "Definition 2. The Disturbance-Action Policy Class(DAC) is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nu_{t}=-K x_{t}+\\sum_{i=1}^{l}M_{t}^{i}\\hat{w}_{t-i}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $K$ is a fixed strongly stable policy, $l$ is a parameter that signifies the dimension of the policy class, $M_{t}=\\{M_{t}^{1},\\cdot\\cdot\\cdot,\\bar{M}_{t}^{l}\\}$ is the weighting parameter of the disturbance at step $t$ , and $\\hat{w}_{t}$ is the estimated disturbance: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{w}_{t}=\\frac{x_{t+1}-x_{t}-h(A x_{t}+B u_{t})}{h}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We note that this definition differs from the DAC policy in discrete systems [3] as we utilize the estimation of disturbance over an interval $[t,t+h]$ instead of only the noise in time $t$ . It counteracts the second-order residue term of the Taylor expansion of $x_{t}$ and is also an online policy as it only requires information from the previous state. ", "page_idx": 4}, {"type": "text", "text": "Our higher-level controller adopts the OCO with memory framework. A technical challenge lies in balancing the approximation error and OCO regret. To achieve a low approximation error, we desire the policy update interval $H$ to be inversely proportional to the sampling distance $h$ . However, this relationship may lead to large OCO regret. To mitigate this issue, we introduce a new parameter $m=\\Theta\\big(\\frac{1}{h}\\big)$ , representing the lookahead window. We update the parameter $M_{t}$ only once every $m$ iterations, further reducing the OCO regret without negatively impacting the approximation error: ", "page_idx": 4}, {"type": "equation", "text": "$$\nM_{t+1}={\\left\\{\\begin{array}{l l}{\\Pi_{M}\\left(M_{t}-\\eta\\nabla g_{t}(M)\\right)}&{{\\mathrm{if~}}t\\bmod m==0\\,,}\\\\ {M_{t}}&{{\\mathrm{otherwise}}\\,.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Where $g_{t}$ is a function corresponding to the loss function $c_{t}$ and we will introduce later in Algorithm 1. For notational convenience and to avoid redundancy, we denote $\\tilde{M}_{[t/m]}=M_{t}$ . We can then define the ideal state and action. Due to the properties of the OCO with memory structure, we need to consider only the previous $H m$ states and actions, rather than all states. As a result, we introduce the definition of the ideal state and action. During the interval $t\\in[i m,(i+1)m-1]$ , the learning policy remains unchanged, so we could define the ideal state and action follow the definition in [3]: ", "page_idx": 4}, {"type": "text", "text": "Definition 3. The ideal state $y_{t}$ and action $v_{t}$ at time $t\\in[i m,(i+1)m-1]$ are defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\ny_{t}=x_{t}(\\tilde{M}_{i-H},...,\\tilde{M}_{i}),v_{t}=-K y_{t}+\\sum_{j=1}^{l}M_{i}^{j}w_{t-i}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the notation indicates that we assume the state $x_{t-H}$ is 0 and that we apply the DAC policy $\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)$ at all time steps from $t-H m$ to $t$ . ", "page_idx": 5}, {"type": "text", "text": "We can also define the ideal cost in this interval follow the definition in [3]: ", "page_idx": 5}, {"type": "text", "text": "Definition 4. The ideal cost function during the interval $t\\in[i m,(i+1)m-1]$ is defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\nf_{i}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)=\\sum_{t=i m}^{(i+1)m-1}c_{t}\\left(y_{t}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right),v_{t}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "With all the concepts presented above, we are now prepared to introduce our algorithm: ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Continuous two-level online control algorithm Input: step size $\\eta$ , sample distance $h$ , policy update parameters $H,m$ , parameters $\\kappa,\\gamma,T$ . Define sample numbers $n=\\lceil T/h\\rceil$ , OCO policy update times $p=\\lceil n/m\\rceil$ . Define DAC policy update class $\\dot{\\mathcal{M}}=\\Big\\{\\tilde{M}=\\Big\\{\\tilde{M}^{\\dot{1}}\\dots\\tilde{M}^{H m}\\Big\\}:\\Big\\|\\tilde{M}^{\\dot{i}}\\Big\\|\\leq2h\\kappa^{3}(1-\\gamma)^{i-1}\\Big\\}.$ Initialize $M_{0}\\in\\mathcal{M}$ arbitrarily. for $k=0,\\ldots,p-1$ do for $s=0,\\ldots,m-1$ do Denote the discretization time $r=k m+s$ . Use the action $\\begin{array}{r}{u_{t}=-K x_{r}+h\\sum_{i=1}^{H m}\\tilde{M}_{k}^{i}\\hat{w}_{r-i}}\\end{array}$ during the time period $t\\in[r h,(r+1)h]$ . Observe the new state $x_{r+1}$ at ti me $(r+1)h$ and record $\\hat{w}_{r}$ according to Equation (1). end for Define the function $g_{k}(M)=f_{k}(M,\\dots,M)$ . Update OCO policy $\\tilde{M}_{k+1}=\\Pi_{\\mathcal{M}}\\left(\\tilde{M}_{k}-\\eta\\nabla g_{k}(\\tilde{M}_{k})\\right)$ . end for ", "page_idx": 5}, {"type": "text", "text": "5 Main Result ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present the primary theorem of online continuous control regret analysis: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Under Assumption 1, 2, a step size of $\\begin{array}{r}{\\eta=\\Theta(\\sqrt{\\frac{m}{T h}})}\\end{array}$ , and a DAC policy update frequency $m=\\Theta(\\frac{1}{h})$ , Algorithm $^{\\,I}$ attains a regret bound of ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ_{T}(A)-\\operatorname*{min}_{K\\in K}J_{T}(K)\\le{\\cal O}(n h(1-h\\gamma)^{\\frac{H}{h}})+{\\cal O}(\\sqrt{n h})+{\\cal O}(T h)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "With the sampling distance $h=\\Theta(\\frac{1}{\\sqrt{T}})$ , and the OCO policy update parameter $H=\\Theta(\\log(T))$ , Algorithm $^{\\,l}$ achieves a regret bound of ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ_{T}(A)-\\operatorname*{min}_{K\\in K}J_{T}(K)\\leq O\\left(\\sqrt{T}\\log\\left(T\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 1 demonstrates a regret that matches the regret of a discrete system [3]. Despite the analysis of a continuous system differing from that of a discrete system, we can balance discretization error, approximation error, and OCO with memory regret by selecting an appropriate update frequency for the policy. Here, $O(\\cdot)$ and $\\Theta(\\cdot)$ are abbreviations for the polynomial factors of universal constants in the assumption. ", "page_idx": 5}, {"type": "text", "text": "While we defer the detailed proof to the appendix, we outline the key ideas and highlight them below. ", "page_idx": 5}, {"type": "text", "text": "Challenge and Proof Sketch We first explain why we cannot directly apply the methods for discrete nonstochastic control from [3] to our work. To utilize Assumption 2, it is necessary first to establish a union bound over the states. In a discrete-time system, it can be easily proved by applying the dynamics inequality $\\|x_{t+1}\\|\\leq a\\|x_{t}\\|+b$ (where $a<1$ ) and the induction method presented in [3]. However, for a continuous-time system, a different approach is necessary because we only have the differential equation instead of the state recurrence formula. ", "page_idx": 6}, {"type": "image", "img_path": "JrYdk3HEnc/tmp/d1e69377b786c21b51e1dd8123b2731e79af2a71204cb594d4d7b44000848f5a.jpg", "img_caption": ["Figure 1: Bounding the states and their derivatives separately. We employ Gronwall\u2019s inequality with the induction method to bound the states. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "To overcome this challenge, we employ Gronwall\u2019s inequality to bound the first and second-order derivatives in the neighborhood of the current state. We then use these bounded properties, in conjunction with an estimation of previous noise, to bound the distance to the next state. Through an iterative application of this method, we can argue that all states and actions are bounded. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Another challenge is that we need to discretize the system but we must overcome the curse of dimensionality caused by discretization. In continuous-time systems, the number of states is inversely proportional to the discretization parameter $h$ , which also determines the size of the OCO memory buffer. Our regret is primarily composed of three components: the error caused by discretization $R_{1}$ , the regret of OCO with memory $R_{2}$ and the difference between the actual cost\u221a and the approximate cost $R_{3}$ . The discretization error $R_{1}$ is $O(h T)$ , therefore if we achieve $O({\\sqrt{T}})$ regret, we must choose h no more than O( \u221a1 ). ", "page_idx": 6}, {"type": "text", "text": "If we update the OCO with memory parameter at eac\u221ah timestep follow the method in [3], we will incur the regret of OCO with memory $R_{2}=O(H^{2.5}{\\sqrt{T}})$ . The difference between the actual cost and the approximate cost $R_{3}\\,=\\,O(T(1-h\\gamma)^{H})$ . To achieve sublinear regret for the third t\u221aerm, we must choose $\\begin{array}{r}{H=O(\\frac{\\log T}{h\\gamma})}\\end{array}$ , but since $h$ is no more t\u221ahan $\\textstyle O({\\frac{1}{\\sqrt{T}}})$ , $H$ will be larger than $\\Theta({\\sqrt{T}})$ , therefore the second term $R_{2}$ will definitely exceed $O(\\sqrt{T})$ . ", "page_idx": 6}, {"type": "text", "text": "Therefore, we adjust the frequency of updating the OCO parameters by introducing a new parameter $m$ , using a two-level approach and update the OCO parameters once in every $m$ steps. This wi\u221all incur the third term $R_{3}={\\cal O}(T(1-h\\gamma)^{H m})$ but keep the OCO with memory reg\u221aret $R_{2}=O(H^{2.5}{\\sqrt{T}})$ , so we can choose $\\begin{array}{r}{H=O(\\frac{\\log T}{\\gamma})}\\end{array}$ and $\\begin{array}{r}{m=O(\\frac{1}{h})}\\end{array}$ . Then the term of $R_{2}$ is $O({\\sqrt{T}}\\log T)$ and we achieve the same regret compare with the discrete system. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we apply our theoretical analysis to the practical training of agents. First we highlight the key difference between our algorithm and traditional online policy optimization. ", "page_idx": 6}, {"type": "text", "text": "1. Stack: While standard online policy optimization learns the optimal policy from the current state $u_{t}=\\phi(x_{t})$ , an optimal non-stochastic controller employs the DAC policy as outlined in Definition 2. Leveraging information from past states aids the agent in adapting to dynamic environments.   \n2. Skip: Different from the analysis in [3], in a continuous-time system we update the state information every few steps, rather than updating it at every step. This solves the curse of dimensionality caused by discretization in continuous-time system. ", "page_idx": 6}, {"type": "text", "text": "The above inspires us with an intuitive strategy for training agents by stacking past observations with some observations to skip. We denote this as Stack & skip for convenience. Stack & skip is frequently used as a heuristic in reinforcement learning, yet little was known about when and why such a technique could boost agent performance. ", "page_idx": 6}, {"type": "text", "text": "How should we evaluate our algorithm in a non-stochastic environment? We opt for learning an optimal policy within a domain randomization environment. In this context, each model\u2019s parameters are randomly sampled from a predetermined task distribution. We train policies to optimize performance across various simulated models [41, 29]. ", "page_idx": 6}, {"type": "text", "text": "We observe that learning in Domain Randomization (DR) significantly differs from stochastic or robust learning problems. In DR, sampling from environmental variables occurs at the beginning of each episode, rather than at every step, distinguishing it from stochastic learning where randomness is step-wise independent and identically distributed. This episodic sampling approach allows agents in DR to exploit environmental conditions and adapt to episodic changes ", "page_idx": 7}, {"type": "image", "img_path": "JrYdk3HEnc/tmp/25ea119c7a04029fb201b8117cb1b3062958699610124a09acf60a5faf94ee63.jpg", "img_caption": ["Figure 2: Leverage past observation of states with some skip. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "within an episode. On the other hand, robust learning focuses on worst-case scenarios depending on an agent\u2019s policy. DR, in contrast, is concerned with the distribution of conditions aimed at broad applicability rather than worst-case perturbations. ", "page_idx": 7}, {"type": "text", "text": "In the context of non-stochastic control, the disturbance, while not disclosed to the learner beforehand, remains fixed throughout the episode and does not adaptively respond to the control policy. This setup in non-stochastic control shows a clear parallel to domain randomization: fixed yet unknown disturbances in non-stochastic control mirror the unknown training environments in DR. As the agent continually interacts with these environments, it progressively adapts, mirroring the adaptive process observed in domain randomization. Therefore, we propose evaluating our algorithm within a domain randomization training task. Subsequently, we introduce the details of our experimental setup: ", "page_idx": 7}, {"type": "text", "text": "Environment Setting We conduct experiments on the hopper, halfcheetah, and walker2d benchmarks using the MuJoCo simulator [40]. The randomized parameters include environmental physical parameters such as damping and friction, as well as the agent properties such as torso size. We set the range of our domain randomization to follow a distribution with default parameters as the mean value, shown in Table 1. When training in the domain randomization environment, the parameter is uniformly sampled from this distribution. To an", "page_idx": 7}, {"type": "table", "img_path": "JrYdk3HEnc/tmp/ba6cc776355c5b1f3d51b72793fec5ef15daaeb5c3711f1b8b704152af1dc200.jpg", "table_caption": ["Table 1: The DR distributions of environment. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "alyze the result of generalization, we only change one of the parameters and keep the other parameters as the mean of its distribution in each test environment. We conducted experiments using NVIDIA A40 graphics card. ", "page_idx": 7}, {"type": "text", "text": "Algorithm Design and Baseline We design a practical meta-algorithm that converts any standard deep RL algorithm into a domain-adaptive algorithm, shown in Figure 2. In this algorithm, we augment the original state observation $o_{t}^{\\mathrm{old}}$ at time $t$ with past observations, resulting in $o_{t}^{\\mathrm{new}}=$ d, otol\u2212dm, . . . , otol\u2212d(h\u22121)m]. Here h is the number of past states we leverage and m is the number of states we skip when we get each of the past states. For clarity in our results, we selected the SAC algorithm for evaluation. We use a variant of Soft Actor-Critic (SAC) [31] and leverage past states with some skip as our algorithm. We compare our algorithm with the standard SAC algorithm training on domain randomization environments as our baseline. ", "page_idx": 7}, {"type": "text", "text": "Impact of Frame Stack and Frame Skip To understand the effects of the frame stack number $h$ and frame skip number $m$ , we carried out experiments in the hopper environment with different $h$ and $m$ . For each parameter we train with 3 random seeds and take the average. Figure 3 shows that the performance increases significantly when the frame stack number is increased from 1 to 3, and remains roughly unchanged when the frame stack number continues to climb up. Figure 4 shows that the optimal frame skip number is 3, while both too large or too small frame skip numbers result in sub-optimal results. Therefore, in the following experiments we fix the parameter $h=3$ , $m=3$ . We train our algorithm with this parameter and standard SAC on hopper and test the performance on more environments. Figure 5 shows that our algorithm outperforms the baseline in all environments. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "JrYdk3HEnc/tmp/3fcca6cf012afab805c260874f708c9a8abf24eb59df86cb0679a4a21066093e.jpg", "img_caption": ["Figure 3: Impact of frame stack number. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "JrYdk3HEnc/tmp/efb9da581a8e9f6cc66aeffc117c1658455d1d1225d2ba435094936e2eaa5371.jpg", "img_caption": ["Figure 4: Impact of frame skip number. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "JrYdk3HEnc/tmp/ecbc0fc32dc4cd391eed374e204af89032cc513bd9b6598638d38ee3a6566826.jpg", "img_caption": ["Figure 5: Agents\u2019 reward in various test environments of hopper. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "JrYdk3HEnc/tmp/b81e16d942b40469677d31b127573122d326c40a787238aabfb6ca016e10e11b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Results on Other Environments Each algorithm was trained using three distinct random seeds in the half-cheetah and walker2d domain randomization (DR) environments. Consistent with previous experiments, we employed a frame stack number of $h\\,=\\,3$ and frame skip number of $m\\,=\\,3$ . The comparative performance of our algorithm and the baseline algorithm, across various domain parameters, is presented in Figure 6. The result clearly demonstrates that our algorithm consistently outperforms the baseline in all evaluated test environments. ", "page_idx": 8}, {"type": "image", "img_path": "JrYdk3HEnc/tmp/23d07b0bb641a9a9b79d26ae9da086527fc44e7e7ef6122fd3df81e4a20af535.jpg", "img_caption": ["Figure 6: Performance in half-cheetah(Top) and walker2d(Bottom). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "7 Conclusion, Limitations and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose a two-level online controller for continuous-time linear systems with adversarial disturbances, aiming to achieve sublinear regret. This approach is grounded in our examination of agent training in domain randomization environments from an online control perspective. At the higher level, our controller employs the Online Convex Optimization (OCO) with memory framework to update policies at a low frequency, thus reducing regret. The lower level uses the DAC policy to align the system\u2019s actual state more closely with the idealized setting. ", "page_idx": 9}, {"type": "text", "text": "In our empirical evaluation, applying our algorithm\u2019s core principles to the SAC (Soft Actor-Critic) algorithm led to significantly improved results in multiple reinforcement learning tasks within domain randomization environments. This highlights the adaptability and effectiveness of our approach in practical scenarios. ", "page_idx": 9}, {"type": "text", "text": "It is important to note that our theoretical analysis depends on the known dynamics of the system and the assumption of convex costs. This reliance could represent a limitation to our method, as it may not adequately address scenarios where these conditions do not hold or where system dynamics are incompletely understood. For future research, there are several promising directions in online non-stochastic control of continuous-time systems. These include extending our methods to systems with unknown dynamics, exploring the impact of assuming strong convexity in cost functions, and shifting the focus from regret to the competitive ratio. Further research can also explore how to utilize historical information more effectively to enhance agent training in domain randomization environments. This might involve employing time series analysis instead of simply incorporating parameters into neural network training. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Yasin Abbasi-Yadkori, Peter Bartlett, and Varun Kanade. Tracking adversarial targets. In International Conference on Machine Learning, 2014.   \n[2] Yasin Abbasi-Yadkori and Csaba Szepesv\u00e1ri. Regret bounds for the adaptive control of linear quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, 2011.   \n[3] Naman Agarwal, Brian Bullins, Elad Hazan, Sham Kakade, and Karan Singh. Online control with adversarial disturbances. In International Conference on Machine Learning, 2019.   \n[4] Artemij Amiranashvili, Max Argus, Lukas Hermann, Wolfram Burgard, and Thomas Brox. Pre-training of deep rl agents for improved learning under domain randomization. arXiv preprint arXiv:2104.14386, 2021.   \n[5] Oren Anava, Elad Hazan, and Shie Mannor. Online learning for adversaries with memory: price of past mistakes. Advances in Neural Information Processing Systems, 2015.   \n[6] Lachlan Andrew, Siddharth Barman, Katrina Ligett, Minghong Lin, Adam Meyerson, Alan Roytman, and Adam Wierman. A tale of two metrics: Simultaneous bounds on competitiveness and regret. In Conference on Learning Theory, pages 741\u2013763. PMLR, 2013.   \n[7] Michael Athans. The role and use of the stochastic linear-quadratic-gaussian problem in control system design. IEEE transactions on automatic control, 16(6):529\u2013552, 1971.   \n[8] Matteo Basei, Xin Guo, Anran Hu, and Yufei Zhang. Logarithmic regret for episodic continuoustime linear-quadratic reinforcement learning over a finite-time horizon. Journal of Machine Learning Research, 2022.   \n[9] Marco C Campi and PR Kumar. Adaptive linear quadratic gaussian control: the cost-biased approach revisited. SIAM Journal on Control and Optimization, 36(6):1890\u20131907, 1998.   \n[10] Xiaoyu Chen, Jiachen Hu, Chi Jin, Lihong Li, and Liwei Wang. Understanding domain randomization for sim-to-real transfer. In International Conference on Learning Representations, 2022.   \n[11] Xinyi Chen and Elad Hazan. Black-box control for linear dynamical systems. In Conference on Learning Theory, 2021.   \n[12] Alon Cohen, Avinatan Hasidim, Tomer Koren, Nevena Lazic, Yishay Mansour, and Kunal Talwar. Online linear quadratic control. In International Conference on Machine Learning, 2018.   \n[13] Tyrone E Duncan, Petr Mandl, and Bo\u02d9zenna Pasik-Duncan. On least squares estimation in continuous time linear stochastic systems. Kybernetika, 28(3):169\u2013180, 1992.   \n[14] Mohamad Kazem Shirani Faradonbeh and Mohamad Sadegh Shirani Faradonbeh. Online reinforcement learning in stochastic continuous-time systems. In The Thirty Sixth Annual Conference on Learning Theory, pages 612\u2013656. PMLR, 2023.   \n[15] Gautam Goel, Naman Agarwal, Karan Singh, and Elad Hazan. Best of both worlds in online control: Competitive ratio and policy regret. arXiv preprint arXiv:2211.11219, 2022.   \n[16] Gautam Goel and Adam Wierman. An online algorithm for smoothed regression and lqr control. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2504\u20132513. PMLR, 2019.   \n[17] Graham C Goodwin, Peter J Ramadge, and Peter E Caines. Discrete time stochastic adaptive control. SIAM Journal on Control and Optimization, 19(6):829\u2013853, 1981.   \n[18] Elad Hazan. Introduction to online convex optimization. CoRR, abs/1909.05207, 2019.   \n[19] Elad Hazan, Sham Kakade, and Karan Singh. The nonstochastic control problem. In Algorithmic Learning Theory, 2020.   \n[20] Sebastian H\u00f6fer, Kostas Bekris, Ankur Handa, Juan Camilo Gamboa, Melissa Mozifian, Florian Golemo, Chris Atkeson, Dieter Fox, Ken Goldberg, John Leonard, et al. Sim2real in robotics and automation: Applications and challenges. IEEE transactions on automation science and engineering, 2021.   \n[21] Jiachen Hu, Han Zhong, Chi Jin, and Liwei Wang. Provable sim-to-real transfer in continuous domain with partial observations. arXiv preprint arXiv:2210.15598, 2022.   \n[22] Yu Jiang and Zhong-Ping Jiang. Computational adaptive optimal control for continuous-time linear systems with completely unknown dynamics. Automatica, 2012.   \n[23] IS Khalil, JC Doyle, and K Glover. Robust and optimal control. Prentice hall, 1996.   \n[24] PR Kumar. Optimal adaptive control of linear-quadratic-gaussian systems. SIAM Journal on Control and Optimization, 21(2):163\u2013178, 1983.   \n[25] Lennart Ljung. System identification. Springer, 1998.   \n[26] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J Pal, and Liam Paull. Active domain randomization. In Conference on Robot Learning, pages 1162\u20131176. PMLR, 2020.   \n[27] Melissa Mozian, Juan Camilo Gamboa Higuera, David Meger, and Gregory Dudek. Learning domain randomization distributions for training robust locomotion policies. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6112\u20136117. IEEE, 2020.   \n[28] Fabio Muratore, Christian Eilers, Michael Gienger, and Jan Peters. Data-efficient domain randomization with bayesian optimization. IEEE Robotics and Automation Letters, 6(2):911\u2013 918, 2021.   \n[29] Fabio Muratore, Michael Gienger, and Jan Peters. Assessing transferability from simulation to reality for reinforcement learning. IEEE transactions on pattern analysis and machine intelligence, 43(4):1172\u20131183, 2019.   \n[30] Fabio Muratore, Theo Gruner, Florian Wiese, Boris Belousov, Michael Gienger, and Jan Peters. Neural posterior domain randomization. In Conference on Robot Learning, pages 1532\u20131542. PMLR, 2022.   \n[31] Evgenii Nikishin, Max Schwarzer, Pierluca D\u2019Oro, Pierre-Luc Bacon, and Aaron Courville. The primacy bias in deep reinforcement learning. In International Conference on Machine Learning. PMLR, 2022.   \n[32] Syed Ali Asad Rizvi and Zongli Lin. Output feedback reinforcement learning control for the continuous-time linear quadratic regulator problem. In 2018 Annual American Control Conference (ACC), 2018.   \n[33] Guanya Shi, Yiheng Lin, Soon-Jo Chung, Yisong Yue, and Adam Wierman. Online optimization with memory and competitive control. Advances in Neural Information Processing Systems, 33:20636\u201320647, 2020.   \n[34] Mohamad Kazem Shirani Faradonbeh, Mohamad Sadegh Shirani Faradonbeh, and Mohsen Bayati. Thompson sampling efficiently learns to control diffusion processes. Advances in Neural Information Processing Systems, 35:3871\u20133884, 2022.   \n[35] Max Simchowitz. Making non-stochastic control (almost) as easy as stochastic. Advances in Neural Information Processing Systems, 33:18318\u201318329, 2020.   \n[36] Max Simchowitz, Karan Singh, and Elad Hazan. Improper learning for non-stochastic control. In Conference on Learning Theory, pages 3320\u20133436. PMLR, 2020.   \n[37] Robert F Stengel. Optimal control and estimation. Courier Corporation, 1994.   \n[38] Gabriele Tiboni, Karol Arndt, and Ville Kyrki. Dropo: Sim-to-real transfer with offilne domain randomization. Robotics and Autonomous Systems, 166:104432, 2023.   \n[39] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23\u201330. IEEE, 2017.   \n[40] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012.   \n[41] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic data: Bridging the reality gap by domain randomization. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 969\u2013977, 2018.   \n[42] Draguna Vrabie, O Pastravanu, Murad Abu-Khalaf, and Frank L Lewis. Adaptive optimal control for continuous-time linear systems based on policy iteration. Automatica, 2009.   \n[43] Quan Vuong, Sharad Vikram, Hao Su, Sicun Gao, and Henrik I Christensen. How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies? arXiv preprint arXiv:1903.11774, 2019.   \n[44] Sergey Zakharov, Wadim Kehl, and Slobodan Ilic. Deceptionnet: Network-driven domain randomization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 532\u2013541, 2019. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "In the appendix we define $n$ as the smallest integer greater than or equal to $\\textstyle{\\frac{T}{h}}$ , and we use the shorthand $c_{i h}$ , $x_{i h}$ , $u_{i h}$ , and $w_{i h}$ as $c_{i}$ , $x_{i}$ , $u_{i}$ , and $w_{i}$ , respectively. First we provide the proof of our main theorem there. ", "page_idx": 12}, {"type": "text", "text": "A Proof of Theorem 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem 1. Under Assumption 1, 2, a step size of $\\eta=\\Theta(\\sqrt{\\frac{m}{T h}})$ , and a DAC policy update frequency $m=\\Theta(\\frac{1}{h})$ , Algorithm $^{\\,I}$ attains a regret bound of ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ_{T}(A)-\\operatorname*{min}_{K\\in K}J_{T}(K)\\le{\\cal O}(n h(1-h\\gamma)^{\\frac{H}{h}})+{\\cal O}(\\sqrt{n h})+{\\cal O}(T h)\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "With the sampling distance $h=\\Theta({\\frac{1}{\\sqrt{T}}})$ , and the OCO policy update parameter $H=\\Theta(\\log(T))$ , Algorithm $^{\\,l}$ achieves a regret bound of ", "page_idx": 12}, {"type": "equation", "text": "$$\nJ_{T}(A)-\\operatorname*{min}_{K\\in K}J_{T}(K)\\leq O\\left(\\sqrt{T}\\log\\left(T\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. We denote $u_{t}^{*}=K^{*}x_{t}^{*}$ as the optimal state and action that follows the policy specified by $K^{*}$ , where $K^{*}=\\arg\\operatorname*{max}_{K\\in K}J_{T}(K)$ . ", "page_idx": 12}, {"type": "text", "text": "We then discretize and decompose the regret as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle J_{T}({\\cal A})-\\operatorname*{min}_{K\\in{\\cal K}}J_{T}(K)=\\displaystyle\\int_{0}^{T}c_{t}(x_{t},u_{t})d t-\\displaystyle\\int_{0}^{T}c_{t}(x_{t}^{*},u_{t}^{*})d t}\\\\ {\\displaystyle=\\sum_{i=0}^{n-1}\\int_{i h}^{(i+1)h}c_{t}(x_{t},u_{t})d t-\\displaystyle\\sum_{i=0}^{n-1}\\int_{i h}^{(i+1)h}c_{t}(x_{t}^{*},u_{t}^{*})d t}\\\\ {\\displaystyle}&{=h\\left(\\sum_{i=0}^{n-1}c_{i}(x_{i},u_{i})-\\sum_{i=0}^{n-1}c_{i}(x_{i}^{*},u_{i}^{*})\\right)+R_{0}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $R_{0}$ represents the discretization error. ", "page_idx": 12}, {"type": "text", "text": "We define $p$ as the smallest integer greater than or equal to $\\frac{n}{m}$ , then the first term can be further decomposed as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{c}{\\displaystyle\\sum_{i=0}^{n-1}c_{i}(x_{i},u_{i})-\\sum_{i=0}^{n-1}c_{i}(x_{i}^{*},u_{i}^{*})}\\\\ {\\displaystyle\\sum_{i=0}^{p-1(i+1)m-1}\\cdots\\sum_{i=0}^{n-1}\\cdots\\sum_{j=i}^{n}c_{i}(x_{i}^{*},u_{i}^{*})}\\\\ {\\displaystyle\\sum_{i=0}^{p-1}\\sum_{j=i}^{p-1}c_{i}(x_{i},u_{i})-\\sum_{i=0}^{p-1}\\sum_{j=i+m}^{n}c_{i}(x_{i}^{*},u_{i}^{*})}\\end{array}}\\\\ &{\\begin{array}{c}{\\displaystyle\\sum_{i=0}^{p-1}\\left(\\sum_{j=i+0}^{(i+1)m-1}c_{i}(x_{i},u_{i})-\\sum_{j=i+m}^{(i+1)m-1}c_{i}(y_{i},v_{i})\\right)+\\sum_{i=0}^{p-1}\\sum_{j=i}^{(i+1)m-1}c_{i}(y_{i},v_{i})-\\sum_{i=0}^{p-1}\\sum_{j=i+m}^{n-1}c_{i}(x_{i}^{*},u_{j}^{*})}\\\\ {\\displaystyle\\sum_{i=0}^{p-1}\\left(\\sum_{j=i+m}^{(i+1)m-1}c_{i}(x_{i},u_{i})-f_{i}(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i})\\right)+\\sum_{i=0}^{p-1}f_{i}(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i})}\\end{array}}\\\\ &{\\begin{array}{c}{\\displaystyle\\sum_{i=0}^{p-1}\\left(\\sum_{j=i+m}^{(i+1)m-1}c_{i}(x_{i},u_{i})-f_{i}(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i})\\right)+\\sum_{i=0}^{p-1}f_{i}(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i})}\\\\ {\\displaystyle-\\sum_{i=0}^{m-1}c_{i}(x_{i}^{*},u_{i}^{*})+\\sum_{i=0}^{n-1}f_{i}(M_{i-1},\\ldots,M)-\\sum_{i\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the last equality is by the definition of the idealized cost function. ", "page_idx": 12}, {"type": "text", "text": "Let us denote ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal R}_{1}=\\displaystyle\\sum_{i=0}^{p-1}\\left(\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i},u_{i})-f_{i}(\\tilde{M}_{i-H},\\dots,\\tilde{M}_{i})\\right)\\,\\mathrm{,}}\\\\ &{{\\cal R}_{2}=\\displaystyle\\sum_{i=0}^{p-1}f_{i}(\\tilde{M}_{i-H},\\dots,\\tilde{M}_{i})-\\sum_{M\\in{\\cal M}}\\sum_{i=0}^{p-1}f_{i}(M,\\dots,M)\\,\\mathrm{,}}\\\\ &{{\\cal R}_{3}=\\displaystyle\\operatorname*{min}_{M\\in{\\cal M}}\\sum_{i=0}^{p-1}f_{i}(M,\\dots,M)-\\sum_{i=0}^{p-1}\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i}^{*},u_{i}^{*})\\,\\mathrm{.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then we have the regret decomposition as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)=h(R_{1}+R_{2}+R_{3})+O(h T)\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We then separately upper bound each of the four terms. ", "page_idx": 13}, {"type": "text", "text": "The term $R_{0}$ represents the error caused by discretization, which decreases as the number of sampling points increases and the sampling distance $h$ decreases. This is because more sampling points make our approximation of the continuous system more accurate. Using Lemma 3, we get the following upper bound: $R_{0}\\leq O(h T)$ . ", "page_idx": 13}, {"type": "text", "text": "The term $R_{1}$ represents the difference between the actual cost and the approximate cost. For a fixed $h$ , this error decreases as the number of sample points looked ahead $m$ increases, while it increases as the sampling distance $h$ decreases. This is because the closer adjacent points are, the slower the convergence after approximation. By Lemma 4 we can bound it as $\\mathring{R}_{1}\\leq\\mathring{O}(n(1-h\\gamma)^{H m})$ . ", "page_idx": 13}, {"type": "text", "text": "The term $R_{2}$ is incurred due to the regret of the OCO with memory algorithm. Note that this term is determined by learning rate $\\eta$ and the policy update frequency $m$ . Choosing suitable parameters and using Lemma 5, we can obtain the following upper bound: $R_{2}\\leq O(\\sqrt{n/h})$ . ", "page_idx": 13}, {"type": "text", "text": "The term $R_{3}$ represents the difference between the ideal optimal cost and the actual optimal cost. Since the accuracy of the DAC policy approximation of the optimal policy depends on its degree of freedom $l$ , a higher degree of freedom leads to a more accurate approximation of the optimal policy. We use Lemma 6 and choose $l=H m$ to bound this error: $R_{3}\\leq\\dot{O}(n(1-h\\gamma)^{H m})$ . ", "page_idx": 13}, {"type": "text", "text": "By summing up these four terms and taking $m=\\Theta(\\frac{1}{h})$ , we get: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\mathrm{Regret}}(T)\\leq O(n h(1-h\\gamma)^{\\frac{H}{h}})+O({\\sqrt{n h}})+O(h T)\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, we choose $\\begin{array}{r}{h=\\Theta\\Big(\\frac{1}{\\sqrt{T}}\\Big),m=\\Theta\\left(\\frac{1}{h}\\right),H=\\Theta(l o g(T))}\\end{array}$ , the regret is bounded by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(T)\\leq O({\\sqrt{T}}\\log(T))\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Key Lemmas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we will primarily discuss the rationale behind the proof of our key lemmas. First, we need to prove all the states and actions are bounded. ", "page_idx": 13}, {"type": "text", "text": "Lemma 2. Under Assumption $^{\\,l}$ and 2, choosing arbitrary $h$ in the interval $[0,h_{0}]$ where $h_{0}$ is a constant only depends on the parameters in the assumption, we have for any $t$ and policy $M_{i}$ , $\\|x_{t}\\|,\\|y_{t}\\|,\\|u_{t}\\|,\\|\\bar{v}_{t}\\|\\leq D_{t}$ , $\\begin{array}{r}{\\|\\dot{x}_{t}\\|\\,\\leq\\,D,\\,\\|x_{t}-y_{t}\\|,\\|u_{t}\\,\\overset{\\cdot}{-}v_{t}\\|\\,\\leq\\,\\kappa^{2}(1+\\dot{\\kappa})(1\\overset{\\cdot}{-}h\\gamma)^{H\\dot{m}+1}\\dot{D}}\\end{array}$ . In particular, taking all the $M_{t}=0$ and $K=K^{*}$ , we can also obtain the inequality of the optimal solution: $\\|\\boldsymbol{x}_{t}^{*}\\|,\\|\\boldsymbol{u}_{t}^{*}\\|\\leq D$ . ", "page_idx": 13}, {"type": "text", "text": "The proof of this Lemma mainly use the Gronwall inequality and the induction method. Then we analyze the discretization error of the system. ", "page_idx": 13}, {"type": "text", "text": "Lemma 3. Under Assumption 2, Algorithm 1 attains the following bound of $R_{0}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathfrak{I}_{0}=\\sum_{i=0}^{n-1}\\int_{i h}^{(i+1)h}(c_{t}(x_{t},u_{t})-c_{t}(x_{t}^{*},u_{t}^{*}))d t-h\\sum_{i=0}^{n-1}(c_{i}(x_{i},u_{i})-c_{i}(x_{i}^{*},u_{i}^{*}))\\le(G+L)D^{2}h T\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This lemma indicates that the discretization error is directly proportional to the sample distance $h$ . In other words, increasing the number of sampling points leads to more accurate estimation of system. ", "page_idx": 14}, {"type": "text", "text": "Then we analysis the difference between ideal cost and actual cost. The following lemma describes the upper bound of the error by approximating the ideal state and action: ", "page_idx": 14}, {"type": "text", "text": "Lemma 4. Under Assumption 1 and 2, Algorithm 1 attains the following bound of $R_{1}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nR_{1}=\\sum_{i=0}^{p-1}\\left(\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i},u_{i})-f_{i}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)\\right)\\le n G D^{2}\\kappa^{2}(1+\\kappa)(1-h\\gamma)^{H m+1}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From this lemma, it is evident that for a fixed sample distance $h$ , the error diminishes as the number of sample points looked ahead $m$ increases. However, as the sampling distance $h$ decreases, the convergence rate of this term becomes slower. Therefore, it is not possible to select an arbitrarily small value for $h$ in order to minimize the discretization error $R_{0}$ . ", "page_idx": 14}, {"type": "text", "text": "We need to demonstrate that the discrepancy between $x_{t}$ and $y_{t}$ , as well as $u_{t}$ and $v_{t}$ , is sufficiently small, given assumption 1. This can be proven by analyzing the state evolution under the DAC policy. ", "page_idx": 14}, {"type": "text", "text": "By utilizing Assumption 2 and Lemma 2, we can deduce the following inequality: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|c_{t}\\left(x_{t},u_{t}\\right)-c_{t}\\left(y_{t},v_{t}\\right)\\right|\\leq\\left|c_{t}\\left(x_{t},u_{t}\\right)-c_{t}\\left(y_{t},u_{t}\\right)\\right|+\\left|c_{t}\\left(y_{t},u_{t}\\right)-c_{t}\\left(y_{t},v_{t}\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq G D\\|x_{t}-y_{t}\\|+G D\\|u_{t}-v_{t}\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Summing over all the terms and use Lemma 2, we can derive an upper bound for $R_{1}$ . ", "page_idx": 14}, {"type": "text", "text": "Next, we analyze the regret of Online Convex Optimization (OCO) with a memory term. To analyze OCO with a memory term, we provide an overview of the framework established by [5] in online convex optimization. The framework considers a scenario where, at each time step $t$ , an online player selects a point $x_{t}$ from a set $\\kappa\\subset\\mathbb{R}^{d}$ . At each time step, a loss function $f_{t}:K^{H\\bar{+}1}\\to\\mathbb{R}$ is revealed, and the player incurs a loss of $f_{t}\\left(x_{t-H},\\ldots,x_{t}\\right)$ . The objective is to minimize the policy regret, which is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{PolicyRegret}=\\sum_{t=H}^{T}f_{t}\\left(x_{t-H},\\ldots,x_{t}\\right)-\\operatorname*{min}_{x\\in K}\\sum_{t=H}^{T}f_{t}(x,\\ldots,x)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this setup, the first term corresponds to the DAC policy we choose, while the second term is used to approximate the optimal strongly stable linear policy. ", "page_idx": 14}, {"type": "text", "text": "Lemma 5. Under Assumption $^{\\,l}$ and 2, choosing $\\begin{array}{r}{m=\\frac{C}{h}}\\end{array}$ and $\\eta=\\Theta(\\frac{m}{T h})$ , Algorithm 1 attains the following bound of $R_{2}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal R}_{2}=\\displaystyle\\sum_{i=0}^{p-1}f_{i}(\\tilde{M}_{i-H},\\dots,\\tilde{M}_{i})-\\operatorname*{min}_{M\\in\\mathcal{M}}\\displaystyle\\sum_{i=0}^{p-1}f_{i}(M,\\dots,M)}\\\\ &{\\quad\\le\\displaystyle\\frac{4a}{\\gamma}\\sqrt{\\frac{G D C^{2}\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}(\\frac{G D C\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}+C^{2}\\kappa^{3}\\kappa_{B}W_{0}H^{2})\\frac{n}{h}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To analyze this term, we can transform the problem into an online convex optimization with memory and utilize existing results presented by [5] for it. By applying their results, we can derive the following bound: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t=H}^{T}f_{t}\\left(x_{t-H},\\dots,x_{t}\\right)-\\operatorname*{min}_{x\\in{\\cal K}}\\sum_{t=H}^{T}f_{t}(x,\\dots,x)\\le{\\cal O}\\left(D\\sqrt{G_{f}\\left(G_{f}+L H^{2}\\right)T}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking into account the bounds on the diameter, Lipschitz constant, and the gradient, we can ultimately derive an upper bound for $R_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "Lastly, we aim to establish a bound on the approximation error between the optimal DAC policy and the unknown optimal linear policy. ", "page_idx": 15}, {"type": "text", "text": "Lemma 6. Under Assumption $^{\\,I}$ and 2, Algorithm 1 attains the following bound of $R_{3}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathfrak{Z}_{3}=\\operatorname*{min}_{M\\in\\mathcal{M}}\\sum_{i=0}^{p-1}f_{i}(M,...,M)-\\sum_{i=0}^{p-1}\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i}^{*},u_{i}^{*})\\le3n(1-h\\gamma)^{H m}G D W_{0}\\kappa^{3}a(l h\\kappa_{B}+1)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The intuition behind this lemma is that the evolution of states leads to an approximation of the optimal linear policy in hindsight, where $u_{t}^{*}=-K^{*}x_{t}$ if we choose $M^{*}=\\mathbf{\\dot{\\left\\{}}\\mathbf{\\dot{}}\\right\\}}$ , where $M^{i}=$ $(\\bar{K}-K^{*})(I+\\bar{h}(A^{\\prime}-B K^{*}))^{i}$ . Although the optimal policy $K^{*}$ is unknown, such an upper bound is attainable because the left-hand side represents the minimum of $M\\in\\mathcal{M}$ . ", "page_idx": 15}, {"type": "text", "text": "C The evolution of the state ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we will prove that using the DAC policy, the states and actions are uniformly bounded. The difference between ideal and actual states and the difference between ideal and actual action is very small. ", "page_idx": 15}, {"type": "text", "text": "We begin with expressions of the state evolution using DAC policy: ", "page_idx": 15}, {"type": "text", "text": "Lemma 7. We have the evolution of the state and action: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle x_{t+1}=Q_{h}^{l+1}x_{t-l}+h\\sum_{i=0}^{2l}\\Psi_{t,i}\\hat{w}_{t-i}}\\,,}\\\\ {~~}\\\\ {{\\displaystyle y_{t+1}=h\\sum_{i=0}^{2H m}\\,\\Psi_{t,i}\\hat{w}_{t-i}}\\,,}\\\\ {~~}\\\\ {{\\displaystyle v_{t}=\\,-\\,K y_{t}+h\\sum_{j=1}^{H m}M_{t}^{j}\\hat{w}_{t-j}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Psi_{t,i}$ represent the coefficients of $\\hat{w}_{t-i}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Psi_{t,i}=Q_{h}^{i}\\mathbf{1}_{i\\leq l}+h\\sum_{j=0}^{l}Q_{h}^{j}B M_{t-j}^{i-j}\\mathbf{1}_{i-j\\in[1,l]}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Define $Q_{h}\\,=\\,I+h(A-B K)$ . Using the Taylor expansion of $x_{t}$ and denoting $r_{t}$ as the second-order residue term, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{t+1}=x_{t}+h\\dot{x}_{t}+h^{2}r_{t}=x_{t}+h(A x_{t}+B u_{t}+w_{t})+h^{2}r_{t}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then we calculate the difference between $w_{i}$ and $\\hat{w}_{i}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{w}_{t}-w_{t}=\\frac{x_{t+1}-x_{t}-h(A x_{t}+B u_{t}+w_{t})}{h}=h r_{t}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the definition of DAC policy and the difference between disturbance, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{x_{t+1}=x_{t}+h\\left(A x_{t}+B\\left(-K x_{t}+h\\displaystyle\\sum_{i=1}^{l}M_{t}^{i}\\bar{w}_{t-i}\\right)+\\hat{w}_{t}-h r_{t}\\right)+h^{2}r_{t}}\\\\ {=(I+h(A-B K))x_{t}+h\\left(B h\\displaystyle\\sum_{i=1}^{l}M_{t}^{i}\\bar{w}_{t-i}+\\hat{w}_{t}\\right)}\\\\ {=Q_{h}x_{t}+h\\left(B h\\displaystyle\\sum_{i=1}^{l}M_{t}^{i}\\bar{w}_{t-i}+\\hat{w}_{t}\\right)}\\\\ {=Q_{h}^{2}x_{t-1}+h\\left(Q_{h}\\left(B h\\displaystyle\\sum_{i=1}^{l}M_{t-1}^{i}\\bar{w}_{t-1-i}+\\hat{w}_{t-1}\\right)\\right)+h\\left(B h\\displaystyle\\sum_{i=1}^{l}M_{t}^{i}\\bar{w}_{t-i}+\\hat{w}_{t}\\right)}\\\\ {=Q_{h}^{4+1}x_{t-l}+h\\displaystyle\\sum_{i=0}^{l}\\Phi_{t,i}\\bar{w}_{t-i}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last equality is by recursion and $\\Psi_{t,i}$ represent the coefficients of $\\hat{w}_{t-i}$ . ", "page_idx": 16}, {"type": "text", "text": "Then we calculate the coefficients of $w_{t-i}$ and get the following result: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Psi_{t,i}=Q_{h}^{i}\\mathbf{1}_{i\\leq l}+h\\sum_{j=0}^{l}Q_{h}^{j}B M_{t-j}^{i-j}\\mathbf{1}_{i-j\\in[1,l]}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the ideal definition of $y_{t+1}$ and $v_{t}$ (only consider the effect of the past $H m$ steps while planning, assume $x_{t-H m}=0,$ ), taking $l=H m$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{{}}&{{}}&{{y_{t+1}=h\\displaystyle\\sum_{i=0}^{2H m}\\Psi_{t,i}\\hat{w}_{t-i},}}\\\\ {{}}&{{}}&{{v_{t}=\\displaystyle-\\,K y_{t}+h\\displaystyle\\sum_{j=1}^{H m}M_{t}^{j}\\hat{w}_{t-j}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we prove the norm of the transition matrix is bounded. ", "page_idx": 16}, {"type": "text", "text": "Lemma 8. We have the following bound of the transition matrix: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\Psi_{t,i}\\|\\leq a(l h\\kappa_{B}+1)\\kappa^{2}(1-h\\gamma)^{i-1}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. By the definition of strongly stable policy, we know ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|Q_{h}^{i}\\|=\\|(P L_{h}P^{-1})^{i}\\|=\\|P(L_{h})^{i}P^{-1}\\|\\leq\\|P\\|\\|L_{h}\\|^{i}\\|P^{-1}\\|\\leq a\\kappa^{2}(1-h\\gamma)^{i}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the definition of $\\Psi_{t,i}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Psi_{t,i}\\|=\\displaystyle\\left\\|Q_{h}^{i}\\mathbf{1}_{i\\leq l}+h\\sum_{j=0}^{l}Q_{h}^{j}B M_{t-j}^{i-j}\\mathbf{1}_{i-j\\in[1,l]}\\right\\|}\\\\ &{\\qquad\\leq\\kappa^{2}(1-h\\gamma)^{i}+a h\\displaystyle\\sum_{j=1}^{l}\\kappa_{B}\\kappa^{2}(1-h\\gamma)^{j}(1-h\\gamma)^{i-j-1}}\\\\ &{\\qquad\\leq\\kappa^{2}(1-h\\gamma)^{i}+a l h\\kappa_{B}\\kappa^{2}(1-h\\gamma)^{i-1}\\leq a(l h\\kappa_{B}+1)\\kappa^{2}(1-h\\gamma)^{i-1}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first inequality is due to equation 2, assumption 1 and the condition of $\\left\\|M_{t}^{i}\\right\\|\\leq a(1-$ $h\\gamma)^{i-1}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "After that, we can uniformly bound the state $x_{t}$ and its first and second-order derivative. ", "page_idx": 16}, {"type": "text", "text": "Lemma 9. For any $t\\in[0,T],$ , choosing arbitrary $h$ in the interval $[0,h_{0}]$ where $h_{0}$ is a constant only depends on the parameters in the assumption, we have $\\|x_{t}\\|\\leq D_{1}$ , $\\|\\dot{\\boldsymbol{x}}_{t}\\|\\leq D_{2}$ , $\\|\\ddot{x}_{t}\\|\\leq D_{3}$ and the estimatation of disturbance is bounded by $\\left\\|\\hat{w}_{t}\\right\\|\\leq W_{0}$ . Moreover, $D_{1}$ , $D_{2}$ , $D_{3}$ are only depend on the parameters in the assumption. ", "page_idx": 17}, {"type": "text", "text": "Proof. We prove this lemma by induction. When $t=0$ , it is clear that $x_{0}$ satisfies this condition. Suppose $\\dot{x_{t}^{\\prime}}\\,\\le\\,D_{1},\\,\\dot{x}_{t}\\,\\le\\,D_{2},\\,\\ddot{x}_{t}\\,\\le\\,D_{3},\\,\\hat{w}_{t}\\,\\le\\,W_{0}$ for any $t\\,\\leq\\,t_{0}$ , where $t_{0}\\,=\\,k h$ is the $k$ -th discretization point. Then for $t\\in[t_{0},t_{0}+h]$ , we first prove that $\\dot{x}_{t}\\le D_{2},\\ddot{x}_{t}\\le D_{3}$ . ", "page_idx": 17}, {"type": "text", "text": "By Assumption 1 and our definition of $u_{t}$ , we know that for any $t\\in[t_{0},t_{0}+h]$ . Thus, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\dot{x}_{t}\\|=\\|A x_{t}+B u_{t}+w_{t}\\|}\\\\ &{\\quad=\\|A x_{t}+B(-K x_{t_{0}}+h\\displaystyle\\sum_{i=1}^{l}M_{k}^{i}\\dot{w}_{k-i})+w_{t}\\|}\\\\ &{\\quad\\leq\\kappa_{A}\\|x_{t}\\|+\\kappa_{B}\\kappa\\|x_{t_{0}}\\|+\\displaystyle h\\displaystyle\\sum_{i=1}^{l}(1-h\\gamma)^{i-1}W_{0}+W}\\\\ &{\\quad\\leq\\kappa_{A}\\|x_{t}\\|+\\kappa_{B}\\kappa D_{1}+\\displaystyle\\frac{W_{0}}{\\gamma}+W\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality is by the induction hypothesis $\\hat{w}_{t}\\;\\leq\\;W_{0}$ for any $t\\,\\leq\\,t_{0}$ and $M_{k}^{i}\\ \\leq$ $(1-h\\gamma)^{i-1}$ , the second inequality is by the induction hypothesis $x_{t}\\leq D_{1}$ for any $t\\leq t_{0}$ . ", "page_idx": 17}, {"type": "text", "text": "For any $t\\in[t_{0},t_{0}+h]$ , because we choose the fixed policy $u_{t}\\equiv u_{t_{0}}$ , so we have $\\dot{u}_{t}=0$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\|{\\ddot{x}}_{t}\\right\\|=\\left\\|A{\\dot{x}}_{t}+B{\\dot{u}}_{t}+{\\dot{w}}_{t}\\right\\|=\\left\\|A{\\dot{x}}_{t}+{\\dot{w}}_{t}\\right\\|\\leq\\kappa_{A}\\|{\\dot{x}}_{t}\\|+W\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By the Newton-Leibniz formula, we have for any $\\zeta\\in[0,h]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\dot{x}_{t_{0}+\\zeta}-\\dot{x}_{t_{0}}=\\int_{0}^{\\zeta}\\ddot{x}_{t_{0}+\\xi}d_{\\xi}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\dot{x}_{t_{0}+\\zeta}\\|\\leq\\|\\dot{x}_{t_{0}}\\|+\\int_{0}^{\\zeta}\\|\\ddot{x}_{t_{0}+\\xi}\\|d_{\\xi}}\\\\ {\\displaystyle\\leq\\|\\dot{x}_{t_{0}}\\|+\\int_{0}^{\\zeta}(\\kappa_{A}\\|\\dot{x}_{t_{0}+\\xi}\\|+W)d_{\\xi}}\\\\ {\\displaystyle=\\|\\dot{x}_{t_{0}}\\|+W\\zeta+\\kappa_{A}\\int_{0}^{\\zeta}\\|\\dot{x}_{t_{0}+\\xi}\\|d_{\\xi}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By Gronwall inequality, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\dot{x}_{t_{0}+\\zeta}\\|\\leq\\|\\dot{x}_{t_{0}}\\|+W\\zeta+\\int_{0}^{\\zeta}(\\|\\dot{x}_{t_{0}}\\|+W\\xi)\\exp(\\kappa_{A}(\\zeta-\\xi))d\\xi\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\dot{x}_{t_{0}+\\zeta}\\|\\leq\\|\\dot{x}_{t_{0}}\\|+W\\zeta+\\int_{0}^{\\zeta}(\\|\\dot{x}_{t_{0}}\\|+W\\zeta)\\exp(\\kappa_{A}\\zeta))d_{\\xi}}\\\\ &{\\qquad=(\\|\\dot{x}_{t_{0}}\\|+W\\zeta)(1+\\zeta\\exp(\\kappa_{A}\\zeta))}\\\\ &{\\qquad\\leq\\bigg(\\kappa_{A}\\|x_{t_{0}}\\|+\\kappa_{B}\\kappa D_{1}+\\displaystyle\\frac{W_{0}}{\\gamma}+W+W h\\bigg)\\left(1+h\\exp(\\kappa_{A}h)\\right)}\\\\ &{\\qquad\\leq\\bigg((\\kappa_{A}+\\kappa_{B}\\kappa)D_{1}+\\displaystyle\\frac{W_{0}}{\\gamma}+W+W h\\bigg)\\left(1+h\\exp(\\kappa_{A}h)\\right)}\\\\ &{\\qquad\\leq\\bigg((\\kappa_{A}+\\kappa_{B}\\kappa)D_{1}+\\displaystyle\\frac{W_{0}}{\\gamma}+2W\\bigg)\\left(1+\\exp(\\kappa_{A})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first inequality is by the relation $\\xi\\le\\zeta$ , the second inequality is by the relation $\\zeta\\leq h$ and the bounding property of first-order derivative, the third inequality is by the induction hypothesis and the last inequality is due to $h\\leq1$ . ", "page_idx": 18}, {"type": "text", "text": "By the relation $\\|\\ddot{\\boldsymbol{x}}_{t}\\|\\leq\\kappa_{A}\\|\\dot{\\boldsymbol{x}}_{t}\\|+W$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\ddot{x}_{t_{0}+\\zeta}\\|\\leq\\kappa_{A}D_{2}+W\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So we choose $D_{3}=\\kappa_{A}D_{2}+W$ . By the equation, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|{\\hat{w}_{t}-w_{t}}\\right|\\left|=\\left|{\\frac{\\displaystyle\\left|x_{t+1}-x_{t}-h(A x_{t}+B u_{t}+w_{t})\\right|}{h}}\\right|\\right|}\\\\ &{\\qquad\\qquad=\\left|{\\frac{\\displaystyle\\left|x_{t+1}-x_{t}-h\\dot{x}_{t}\\right|}{h}}\\right|=\\left\\|\\frac{\\displaystyle\\int_{0}^{h}(\\dot{x}_{t+\\xi}-\\dot{x}_{t})d\\xi}{h}\\right\\|=\\left\\|\\frac{\\displaystyle\\int_{0}^{h}\\int_{0}^{\\xi}\\ddot{x}_{t+\\zeta}d\\zeta d\\xi}{h}\\right\\|}\\\\ &{\\qquad\\qquad\\leq\\frac{\\displaystyle\\int_{0}^{h}\\int_{0}^{\\xi}\\|\\ddot{x}_{t+\\zeta}\\|d\\zeta d\\xi}{h}}\\\\ &{\\qquad\\qquad\\leq h D_{3}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where in the second line we use the Newton-Leibniz formula, the inequality is by the conclusion $\\|\\ddot{x}_{t}\\|\\leq D_{3}$ which we have proved before. By Assumption 1, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\hat{w}_{t}\\|\\leq W+h D_{3}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Choosing $D_{3}=\\kappa_{A}D_{2}+W$ , $W_{0}=W+h D_{3}=W+h(\\kappa_{A}D_{2}+W)$ , we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\dot{x}_{t_{0}+\\zeta}|\\|\\leq\\big((\\kappa_{A}+\\kappa_{B}\\kappa)D_{1}+\\displaystyle\\frac{W_{0}}{\\gamma}+2W\\big)(1+\\exp(\\kappa_{A}))}\\\\ &{\\qquad\\leq\\big((\\kappa_{A}+\\kappa_{B}\\kappa)D_{1}+\\displaystyle\\frac{W+h(\\kappa_{A}D_{2}+W)}{\\gamma}+2W\\big)(1+\\exp(\\kappa_{A}))}\\\\ &{\\qquad\\leq D_{2}\\left(\\displaystyle\\frac{h\\kappa_{A}}{\\gamma}(1+\\exp(\\kappa_{A}))\\right)+\\left((\\kappa_{A}+\\kappa_{B}\\kappa)D_{1}+\\displaystyle\\frac{(1+h+2\\gamma)W}{\\gamma}\\right)(1+\\exp(\\kappa_{A})))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the notation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\beta_{1}=\\frac{h\\kappa_{A}}{\\gamma}(1+\\exp(\\kappa_{A}))\\,,}\\\\ {\\displaystyle\\beta_{2}=\\left((\\kappa_{A}+\\kappa_{B}\\kappa)D_{1}+\\frac{2(1+\\gamma)W}{\\gamma}\\right)(1+\\exp(\\kappa_{A}))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When $\\begin{array}{r}{h<\\frac{\\gamma}{2\\kappa_{A}(1+\\exp(\\kappa_{A}))}}\\end{array}$ , we have $\\beta_{1}<\\frac{1}{2}$ . Taking $D_{2}=2\\beta_{2}$ we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\dot{x}_{t_{0}+\\zeta}\\|\\leq\\beta_{1}D_{2}+\\beta_{2}\\leq D_{2}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So we have proved that for any $t\\in[t_{0},t_{0}+h],\\|\\dot{\\boldsymbol{x}}_{t}\\|\\leq D_{2},\\|\\ddot{\\boldsymbol{x}}_{t}\\|\\leq D_{3},\\|\\hat{\\boldsymbol{w}}_{t}\\|\\leq W_{0}.$ ", "page_idx": 18}, {"type": "text", "text": "Then we choose suitable $D_{1}$ and prove that for any $t\\in[t_{0},t_{0}+h]$ , $\\|x_{t}\\|\\leq D_{1}$ . ", "page_idx": 18}, {"type": "text", "text": "Using Lemma 7, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{t+1}=h\\sum_{i=0}^{t}\\Psi_{t,i}\\hat{w}_{t-i}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the induction hypothesis of bounded state and estimation noise in $[0,t_{0}]$ together with Lemma 8, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x_{t+1}\\|\\leq h\\displaystyle\\sum_{i=0}^{t}(l h\\kappa_{B}+1)\\kappa^{2}(1-h\\gamma)^{i}(W+h D_{3})}\\\\ &{\\qquad\\mathrm\\qquad\\leq\\frac{(l h\\kappa_{B}+1)\\kappa^{2}(W+h D_{3})}{\\gamma}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, by the Taylor expansion and the inequality $\\dot{x}_{t}\\leq D_{2}$ , we have for any $\\zeta\\in[0,h]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|x_{t+1}-x_{t+\\zeta}\\|=\\|\\int_{\\zeta}^{h}\\dot{x}_{t+\\xi}d\\xi\\|\\leq(h-\\zeta)D_{2}\\leq h D_{2}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x_{t+\\zeta}\\|\\leq\\|x_{t+1}\\|+h D_{2}\\leq\\frac{(l h\\kappa_{B}+1)\\kappa^{2}(W+h D_{3})}{\\gamma}+h D_{2}}\\\\ &{\\qquad=\\frac{(l h\\kappa_{B}+1)\\kappa^{2}W(1+h)}{\\gamma}+h D_{2}\\left(\\frac{(l h\\kappa_{B}+1)\\kappa^{2}\\kappa_{A}}{\\gamma}+1\\right)}\\\\ &{\\qquad\\leq\\frac{(l\\kappa_{B}+1)2\\kappa^{2}W}{\\gamma}+h D_{2}\\left(\\frac{(l\\kappa_{B}+1)\\kappa^{2}\\kappa_{A}}{\\gamma}+1\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the last inequality we use $h\\leq1$ . ", "page_idx": 19}, {"type": "text", "text": "By the relation $D_{2}=\\beta_{2}/(1-\\beta_{1})$ and $\\beta_{1}\\leq\\frac{1}{2}$ , we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{2}\\leq2\\left((\\kappa_{A}+\\kappa_{B}\\kappa)D_{1}+\\frac{2(1+\\gamma)W}{\\gamma}\\right)(1+\\exp(\\kappa_{A})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the notation ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\gamma_{1}=2h(\\kappa_{A}+\\kappa_{B}\\kappa)(1+\\exp(\\kappa_{A}))\\,,}\\\\ {\\displaystyle\\gamma_{2}=\\frac{(l\\kappa_{B}+1)2\\kappa^{2}W}{\\gamma}+4\\frac{(1+\\gamma)W}{\\gamma}(1+\\exp(\\kappa_{A}))\\left(\\frac{(l\\kappa_{B}+1)\\kappa^{2}\\kappa_{A}}{\\gamma}+1\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We have $\\|x_{t+\\zeta}\\|\\leq\\gamma_{1}D_{1}+\\gamma_{2}$ . ", "page_idx": 19}, {"type": "text", "text": "From the equation of $\\gamma_{1}$ we know that when $\\begin{array}{r}{h\\leq\\frac{1}{4(\\kappa_{A}+\\kappa_{B}\\kappa)(1+\\exp(\\kappa_{A}))}}\\end{array}$ we have $\\gamma_{1}\\leq\\frac{1}{2}$ . Then we choose $D_{1}=2\\gamma_{2}$ , we finally get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|x_{t+\\zeta}\\|\\leq\\gamma_{1}D_{1}+\\gamma_{2}\\leq D_{1}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, set ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{0}=\\operatorname*{min}\\left\\{1,\\frac{\\gamma}{\\kappa_{A}(1+\\exp(\\kappa_{A}))},\\frac{1}{4(\\kappa_{A}+\\kappa_{B}\\kappa)(1+\\exp(\\kappa_{A}))}\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the relationship $D_{1}=2\\gamma_{2},D_{2}=2\\beta_{2},D_{3}=\\kappa_{A}D_{2}+W,W_{0}=W+h D_{3},$ ", "page_idx": 19}, {"type": "text", "text": "we can verify the induction hypothesis. Moreover, we know that $D_{1},D_{2},D_{3}$ are not depend on $h$ .   \nTherefore we have proved the claim. ", "page_idx": 19}, {"type": "text", "text": "The last step is then to bound the action and the approximation errors of states and actions. ", "page_idx": 19}, {"type": "text", "text": "Lemma 2. Under Assumption $^{\\,l}$ and 2, choosing arbitrary $h$ in the interval $[0,h_{0}]$ where $h_{0}$ is a constant only depends on the parameters in the assumption, we have for any $t$ and policy $M_{i}$ , $\\|x_{t}\\|,\\|y_{t}\\|,\\|u_{t}\\|,\\|\\bar{v}_{t}\\|\\leq D_{t}$ , $\\|\\dot{\\boldsymbol{x}}_{t}\\|\\leq D$ , $\\|x_{t}-y_{t}\\|$ $-\\,y_{t}\\|,\\|u_{t}\\stackrel{\\cdot}{-}v_{t}\\|\\,\\leq\\,\\kappa^{2}(1+\\kappa)(1\\stackrel{\\cdot}{-}h\\gamma)^{H\\!m+1}\\dot{D}$ . In particular, taking all the $M_{t}=0$ and $K=K^{*}$ , we can also obtain the inequality of the optimal solution: $\\|\\boldsymbol{x}_{t}^{*}\\|,\\|\\boldsymbol{u}_{t}^{*}\\|\\leq D$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. By Lemma 8, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\Psi_{t,i}\\|\\leq a(l h\\kappa_{B}+1)\\kappa^{2}(1-h\\gamma)^{i-1}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma 9 we know that for any $h$ in $[0,h_{0}]$ , where ", "page_idx": 19}, {"type": "equation", "text": "$$\nh_{0}=\\operatorname*{min}\\left\\{1,\\frac{\\gamma}{\\kappa_{A}(1+\\exp(\\kappa_{A}))},\\frac{1}{4(\\kappa_{A}+\\kappa_{B}\\kappa)(1+\\exp(\\kappa_{A}))}\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have $\\|x_{t}\\|\\leq D_{1}$ . ", "page_idx": 20}, {"type": "text", "text": "By Lemma 7, Lemma 8 and Lemma 9, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|y_{t+1}\\|=\\|h\\sum_{i=0}^{2H m}\\Psi_{t,i}\\hat{w}_{t-i}\\|}\\\\ &{\\qquad\\qquad\\le h W_{0}\\displaystyle\\sum_{i=0}^{2H m}a(l h\\kappa_{B}+1)\\kappa^{2}(1-h\\gamma)^{i-1}}\\\\ &{\\qquad\\qquad\\le\\frac{a W_{0}(l h\\kappa_{B}+1)\\kappa^{2}}\\gamma=\\tilde{D}_{1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Via the definition of $x_{t},y_{t}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|x_{t}-y_{t}\\|\\leq\\kappa^{2}(1-h\\gamma)^{H m+1}\\,\\|x_{t-H m}\\|\\leq\\kappa^{2}(1-h\\gamma)^{H m+1}D_{1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the actions ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle u_{t}=\\,-\\,K x_{t}+h\\sum_{i=1}^{H m}M_{t}^{i}\\hat{w}_{t-i}\\,,}}\\\\ {{\\displaystyle v_{t}=\\,-\\,K y_{t}+h\\sum_{i=1}^{H m}M_{t}^{i}\\hat{w}_{t-i}\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "we can derive the bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|u_{t}\\|\\le\\|K x_{t}\\|+h\\displaystyle\\sum_{i=1}^{H m}\\|M_{t}^{i}\\hat{w}_{t-i}\\|\\le\\kappa\\,\\|x_{t}\\|+W_{0}h\\displaystyle\\sum_{i=1}^{H m}a(1-h\\gamma)^{i-1}\\le\\kappa D_{1}+\\displaystyle\\frac{a W_{0}}{\\gamma}\\,,}\\\\ &{\\|v_{t}\\|\\le\\,\\|K y_{t}\\|+h\\displaystyle\\sum_{i=1}^{H m}\\|M_{t}^{i}\\hat{w}_{t-i}\\|\\le\\kappa\\,\\|y_{t}\\|+W_{0}h\\displaystyle\\sum_{i=1}^{H m}a(1-h\\gamma)^{i-1}\\le\\kappa\\tilde{D}_{1}+\\displaystyle\\frac{a W_{0}}{\\gamma}\\,,}\\\\ &{\\|u_{t}-v_{t}\\|\\le\\,\\|K\\|\\,\\|x_{t}-y_{t}\\|\\le\\kappa^{3}(1-h\\gamma)^{H m+1}D_{1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Lemma 9, taking $\\begin{array}{r}{D=\\operatorname*{max}\\{D_{1},D_{2},\\tilde{D}_{1},\\kappa D_{1}\\!+\\!\\frac{W_{0}}{\\gamma},\\kappa\\tilde{D}_{1}\\!+\\!\\frac{W_{0}}{\\gamma}\\}}\\end{array}$ , we get the following inequality: $\\|x_{t}\\|,\\|y_{t}\\|,\\|u_{t}\\|,\\|v_{t}\\|\\leq D,\\|{\\dot{x}}_{t}\\|\\leq D$ . ", "page_idx": 20}, {"type": "text", "text": "We also have ", "page_idx": 20}, {"type": "equation", "text": "$$\nx_{t}-y_{t}\\|+\\|u_{t}-v_{t}\\|\\leq\\kappa^{2}(1-h\\gamma)^{H m+1}D_{1}+\\kappa^{3}(1-h\\gamma)^{H m+1}D_{1}\\leq\\kappa^{2}(1+\\kappa)(1-h\\gamma)^{H m+1}D\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, the optimal policy can be recognized as taking the DAC policy with all the $M_{t}$ equal to 0 and the fixed strongly stable policy $K=K^{*}$ . So we also have $\\|\\boldsymbol{x}_{t}^{*}\\|,\\|\\boldsymbol{u}_{t}^{*}\\|\\leq D$ . ", "page_idx": 20}, {"type": "text", "text": "Now we have finished the analysis of evolution of the states. It will be helpful to prove the key lemmas in this paper. ", "page_idx": 20}, {"type": "text", "text": "D Proof of Lemma 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we will prove the following lemma: ", "page_idx": 20}, {"type": "text", "text": "Lemma 3. Under Assumption 2, Algorithm 1 attains the following bound of $R_{0}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathfrak{I}_{0}=\\sum_{i=0}^{n-1}\\int_{i h}^{(i+1)h}(c_{t}(x_{t},u_{t})-c_{t}(x_{t}^{*},u_{t}^{*}))d t-h\\sum_{i=0}^{n-1}(c_{i}(x_{i},u_{i})-c_{i}(x_{i}^{*},u_{i}^{*}))\\le(G+L)D^{2}h T\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. By Assumption 2 and Lemma 2, since we use the unchanged policy $u_{t}$ in the interval $t\\in[i h,(\\dot{i}+1)h]$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\vert c_{t}(x_{t},u_{t})-c_{i h}(x_{i h},u_{i h})\\vert\\leq\\vert c_{t}(x_{t},u_{t})-c_{t}(x_{i h},u_{i h})\\vert+\\vert c_{t}(x_{i h},u_{i h})-c_{i h}(x_{i h},u_{i h})\\vert}&{}\\\\ {\\leq\\underset{x}{\\operatorname*{max}}\\left\\Vert\\nabla_{x}c_{t}(x,u)\\right\\Vert\\left\\Vert x_{t}-x_{i h}\\right\\Vert+L(t-i h)D^{2}}&{}\\\\ {\\leq G D\\Vert\\displaystyle\\int_{i h}^{t}\\dot{x}_{s}d s\\right\\Vert+L(t-i h)D^{2}}&{}\\\\ {\\leq(G+L)D^{2}(t-i h)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|\\sum_{i=0}^{n-1}\\int_{i h}^{(i+1)h}c_{t}(x_{t},u_{t})d t-h\\sum_{i=0}^{n-1}c_{i}(x_{i},u_{i})|}\\\\ {\\displaystyle=|\\sum_{i=0}^{n-1}\\int_{i h}^{(i+1)h}(c_{t}(x_{t},u_{t})-c_{i h}(x_{i h},u_{i h}))d t|}\\\\ {\\displaystyle\\le(G+L)D^{2}\\sum_{i=0}^{n-1}\\int_{i h}^{(i+1)h}(t-i h)d t=\\frac{1}{2}(G+L)D^{2}n h^{2}=\\frac{1}{2}(G+L)D^{2}h T\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "A similar bound can easily be established by lemma 2 about the optimal state and policy: ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\sum_{i=0}^{n-1}\\int_{i h}^{(i+1)h}c_{t}(x_{t}^{*},u_{t}^{*})d t-\\sum_{i=0}^{n-1}c_{i}(x_{i}^{*},u_{i}^{*})|\\leq\\frac{1}{2}(G+L)D^{2}h T\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Taking sum of the two terms we get $R_{0}\\leq(G+L)D^{2}h T$ . ", "page_idx": 21}, {"type": "text", "text": "E Proof of Lemma 4 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we will prove the following lemma: ", "page_idx": 21}, {"type": "text", "text": "Lemma 4. Under Assumption $^{\\,I}$ and 2, Algorithm $^{\\,I}$ attains the following bound of $R_{1}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{1}=\\sum_{i=0}^{p-1}\\left(\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i},u_{i})-f_{i}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)\\right)\\leq n G D^{2}\\kappa^{2}(1+\\kappa)(1-h\\gamma)^{H m+1}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Using Lemma 2 and Assumption 2, have the approximation error between ideal cost and actual cost bounded as, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|c_{t}\\left(x_{t},u_{t}\\right)-c_{t}\\left(y_{t},v_{t}\\right)|\\leq|c_{t}\\left(x_{t},u_{t}\\right)-c_{t}\\left(y_{t},u_{t}\\right)|+|c_{t}\\left(y_{t},u_{t}\\right)-c_{t}\\left(y_{t},v_{t}\\right)|}\\\\ &{\\qquad\\qquad\\qquad\\leq G D\\|x_{t}-y_{t}\\|+G D\\|u_{t}-v_{t}\\|}\\\\ &{\\qquad\\qquad\\qquad\\leq G D^{2}\\kappa^{2}(1+\\kappa)(1-h\\gamma)^{H m+1}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality is by triangle inequality, the second inequality is by Assumption 2, Lemma 2, and the third inequality is by Lemma 2. ", "page_idx": 21}, {"type": "text", "text": "With this, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{1}=\\displaystyle\\sum_{i=0}^{p-1}\\left(\\sum_{j=i:m}^{(i+1)m-1}c_{i}(x_{i},u_{i})-f_{i}(\\tilde{M}_{i-H},...,\\tilde{M}_{i})\\right)}\\\\ &{\\quad=\\displaystyle\\sum_{i=0}^{p-1}\\left(\\sum_{j=i:m}^{(i+1)m-1}c_{i}(x_{i},u_{i})-\\sum_{j=i m}^{(i+1)m-1}c_{i}(y_{i},v_{i})\\right)}\\\\ &{\\quad\\le\\displaystyle\\sum_{i=0}^{p-1}\\sum_{j=i:m}^{(i+1)m-1}G D^{2}\\kappa^{2}(1+\\kappa)(1-h\\gamma)^{H m+1}\\le n G D^{2}\\kappa^{2}(1+\\kappa)(1-h\\gamma)^{H m+1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "F Proof of Lemma 5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Before we start the proof of Lemma 5, we first present an overview of the online convex optimization (OCO) with memory framework. Consider the setting where, for every $t$ , an online player chooses some point $x_{t}\\in\\kappa\\stackrel{.}{\\subset}\\mathbb{R}^{d}$ , a loss function $f_{t}:K^{H+1}\\stackrel{=}{\\mapsto}\\mathbb{R}$ is revealed, and the learner suffers a loss of $f_{t}\\left(x_{t-H},\\ldots,x_{t}\\right)$ . We assume a certain coordinate-wise Lipschitz regularity on $f_{t}$ of the form such that, for any $j\\in\\{1,\\ldots,H\\}$ , for any $x_{1},\\dotsc,x_{H},\\tilde{x}_{j}\\in{\\cal K}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|f_{t}\\left(x_{1},\\ldots,x_{j},\\ldots,x_{H}\\right)-f_{t}\\left(x_{1},\\ldots,\\tilde{x}_{j},\\ldots,x_{H}\\right)\\right|\\leq L\\left\\|x_{j}-\\tilde{x}_{j}\\right\\|\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In addition, we define $\\tilde{f}_{t}(x)=f_{t}(x,\\ldots,x)$ , and we let ", "page_idx": 22}, {"type": "equation", "text": "$$\nG_{f}=\\operatorname*{sup}_{t\\in\\{1,\\ldots,T\\},x\\in{\\cal K}}\\left\\|\\nabla\\tilde{f}_{t}(x)\\right\\|,\\quad D_{f}=\\operatorname*{sup}_{x,y\\in{\\cal K}}\\|x-y\\|\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The resulting goal is to minimize the policy regret, which is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\mathrm{Regret}}=\\sum_{t=H}^{T}f_{t}\\left(x_{t-H},\\ldots,x_{t}\\right)-\\operatorname*{min}_{x\\in K}\\sum_{t=H}^{T}f_{t}(x,\\ldots,x)\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Algorithm 2 Online Gradient Descent with Memory (OGD-M) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "IInniptiuat:li zSet $\\eta$ $\\{f_{t}\\}_{t=m}^{T}$ .y. $x_{0},\\ldots,x_{H-1}\\in{\\mathcal{K}}$ for $t=H,\\dots,T$ do Play $x_{t}$ , suffer loss $f_{t}\\left(x_{t-H},\\ldots,x_{t}\\right)$ . Set $x_{t+1}=\\Pi_{K}\\left(x_{t}-\\eta\\nabla\\tilde{f}_{t}(x)\\right)$ . ", "page_idx": 22}, {"type": "text", "text": "end for ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To minimize this regret, a commonly used algorithm is the Online Gradient descent. By running the Algorithm 2, we may bound the policy regret by the following lemma: ", "page_idx": 22}, {"type": "text", "text": "Lemma 10. Let $\\left\\{f_{t}\\right\\}_{t=1}^{T}$ be Lipschitz continuous loss functions with memory such that $\\tilde{f}_{t}$ are convex. Then by runnning algorithm 2 itgenerates a sequence $\\{x_{t}\\}_{t=1}^{T}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=H}^{T}f_{t}\\left(x_{t-H},\\dots,x_{t}\\right)-\\operatorname*{min}_{x\\in{\\mathcal K}}\\sum_{t=H}^{T}f_{t}(x,\\dots,x)\\leq\\frac{D_{f}^{2}}{\\eta}+T G_{f}^{2}\\eta+L H^{2}\\eta G_{f}T\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, setting \u03b7 = $\\begin{array}{r}{\\eta=\\frac{D_{f}}{\\sqrt{G_{f}(G_{f}+L H^{2})T}}}\\end{array}$ implies that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{PolicyRegret}\\le2D_{f}\\sqrt{G_{f}\\left(G_{f}+L H^{2}\\right)T}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. By the standard OGD analysis [18], we know that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=H}^{T}\\tilde{f}_{t}\\left(x_{t}\\right)-\\operatorname*{min}_{x\\in K}\\sum_{t=H}^{T}\\tilde{f}_{t}(x)\\leq\\frac{D_{f}^{2}}{\\eta}+T G^{2}\\eta.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In addition, we know by the Lipschitz property, for any $t\\geq H$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|f_{t}\\left(x_{t-H},\\ldots,x_{t}\\right)-f_{t}\\left(x_{t},\\ldots,x_{t}\\right)\\right|\\leq L\\displaystyle\\sum_{j=1}^{H}\\|x_{t}-x_{t-j}\\|\\leq L\\displaystyle\\sum_{j=1}^{H}\\sum_{l=1}^{j}\\|x_{t-l+1}-x_{t-l}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq L\\displaystyle\\sum_{j=1}^{H}\\sum_{l=1}^{j}\\eta\\left\\|\\nabla\\tilde{f}_{t-l}\\left(x_{t-l}\\right)\\right\\|\\leq L H^{2}\\eta G,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and so we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\sum_{t=H}^{T}f_{t}\\left(x_{t-H},\\ldots,x_{t}\\right)-\\sum_{t=H}^{T}f_{t}\\left(x_{t},\\ldots,x_{t}\\right)\\right|\\leq T L H^{2}\\eta G.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=H}^{T}f_{t}\\left(x_{t-H},\\dots,x_{t}\\right)-\\operatorname*{min}_{x\\in{\\mathcal K}}\\sum_{t=H}^{T}f_{t}(x,\\dots,x)\\leq\\frac{D_{f}^{2}}{\\eta}+T G_{f}^{2}\\eta+L H^{2}\\eta G_{f}T\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In this setup, the first term corresponds to the DAC policy we make, and the second term is used to approximate the optimal strongly stable linear policy. It is worth noting that the cost of OCO with memory depends on the update frequency $H$ . Therefore, we propose a two-level online controller. The higher-level controller updates the policy with accumulated feedback at a low frequency to reduce the regret, whereas a lower-level controller provides high-frequency updates of the DAC policy to reduce the discretization error. In the following part, we define the update distance of the DAC policy as $l=H m$ , where $m$ is the ratio of frequency between the DAC policy update and OCO memory policy update. Formally, we update the value of $M_{t}$ once every $m$ transitions, where $g_{t}$ represents a loss function. ", "page_idx": 23}, {"type": "equation", "text": "$$\nM_{t+1}={\\left\\{\\begin{array}{l l}{\\Pi_{M}\\left(M_{t}-\\eta\\nabla g_{t}(M)\\right)}&{{\\mathrm{if~}}t{\\mathcal{Y}}_{0}m==0}\\\\ {M_{t}}&{{\\mathrm{otherwise}}\\,.}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "From now on, we denote $\\tilde{M}_{t}=M_{t m}$ for the convenience to remove the duplicate elements. By the definition of ideal cost, we know that it is a well-defined definition. ", "page_idx": 23}, {"type": "text", "text": "By Lemma 7 we know that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle y_{t+1}=h\\sum_{i=0}^{2H m}\\Psi_{t,i}\\hat{w}_{t-i},}\\\\ {\\displaystyle v_{t}=-K y_{t}+h\\sum_{j=1}^{H m}M_{t}^{j}\\hat{w}_{t-j}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Psi_{t,i}=Q_{h}^{i}\\mathbf{1}_{i\\leq l}+h\\sum_{j=0}^{l}Q_{h}^{j}B M_{t-j}^{i-j}\\mathbf{1}_{i-j\\in[1,l]}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So we know that $y_{t}$ and $y_{t}$ are linear combination of $M_{t}$ , therefore ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{i}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)=\\sum_{t=i m}^{(i+1)m-1}c_{t}\\left(y_{t}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right),v_{t}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is convex in $M_{t}$ . So we can use the OCO with memory structure to solve this problem. ", "page_idx": 23}, {"type": "text", "text": "By Lemma 9 we know that $y_{t}$ and $v_{t}$ are bounded by $D$ . Then we need to calculate the diameter, Lipchitz constant, and gradient bound of this function $f_{i}$ . In the following, we choose the DAC policy parameter $l=H m$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma 11. (Bounding the diameter) We have ", "page_idx": 23}, {"type": "equation", "text": "$$\nD_{f}=\\operatorname*{sup}_{M_{i},M_{j}\\in\\mathcal{M}}\\|M_{i}-M_{j}\\|\\leq\\frac{2a}{h\\gamma}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. By the definition of $\\mathcal{M}$ , taking $l=H m$ we know that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{M_{i},M_{j}\\in\\mathcal{M}}\\|M_{i}-M_{j}\\|\\leq\\sum_{k=1}^{H m}\\|M_{i}^{k}-M_{j}^{k}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\sum_{k=1}^{H m}2a(1-h\\gamma)^{k-1}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{2a}{h\\gamma}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 12. (Bounding the Lipschitz Constant) Consider two policy sequences $\\left\\{\\tilde{M}_{i-H}\\cdot\\cdot\\cdot\\tilde{M}_{i-k}\\cdot\\cdot\\cdot\\tilde{M}_{i}\\right\\}$ and $\\left\\{\\tilde{M_{i-H}}\\cdot\\cdot\\cdot\\hat{M_{i-k}}\\cdot\\cdot\\cdot\\tilde{M_{i}}\\right\\}$ which differ in exactly one policy played at a time step $t-k$ for $k\\in\\{0,\\ldots,H\\}$ . Then we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{i}\\left(\\tilde{M}_{i-H}\\ldots\\tilde{M}_{i-k}\\ldots\\tilde{M}_{i}\\right)-f_{i}\\left(\\tilde{M}_{i-H}\\ldots\\hat{M}_{i-k}\\ldots\\tilde{M}_{i}\\right)\\Big|\\le C^{2}\\kappa^{3}\\kappa_{B}W_{0}\\sum_{j=0}^{H m}\\|\\tilde{M}_{i-k}^{j}-\\hat{M}_{i-k}^{j}\\|\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $C$ is a constant. ", "page_idx": 24}, {"type": "text", "text": "Proof. By the definition we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|y_{t}-\\tilde{y}_{t}\\|=\\|h\\sum_{i=0}^{2H m}\\hbar\\sum_{j=0}^{H m}Q_{h}^{j}B(M_{t-j}^{i-j}-\\tilde{M}_{t-j}^{i-j})\\mathbf{1}_{i-j\\in[1,H m]}\\tilde{w}_{t-i}\\|}\\\\ &{\\quad\\quad\\quad\\leq h^{2}\\kappa^{2}\\kappa_{B}W_{0}\\displaystyle\\sum_{i=0}^{2H m}\\sum_{j=0}^{H m}\\|M_{t-j}^{i-j}-\\tilde{M}_{t-j}^{i-j}\\|\\mathbf{1}_{i-j\\in[1,H m]}}\\\\ &{\\quad\\quad\\quad\\leq h^{2}\\kappa^{2}\\kappa_{B}W_{0}\\displaystyle\\sum_{j=0}^{H m}\\|\\tilde{M}_{i-k}^{j}-\\hat{M}_{i-k}^{j}\\|}\\\\ &{\\quad\\quad\\quad\\quad=h C\\kappa^{2}\\kappa_{B}W_{0}\\displaystyle\\sum_{j=0}^{H m}\\|\\tilde{M}_{i-k}^{j}-\\hat{M}_{i-k}^{j}\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Where the first inequality is by $\\|Q_{h}^{j}\\|\\leq\\kappa^{2}(1-h\\gamma)^{j-1}\\leq\\kappa^{2}$ and lemma 9 of bounded estimation disturbance, the second inequality is by the fact that $M_{i-k}$ have taken $m$ times, the last equality is by $\\begin{array}{r}{m=\\frac{C}{h}}\\end{array}$ . Furthermore, we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\Vert v_{t}-\\tilde{v}_{t}\\right\\Vert=\\left\\Vert{}-K\\left(y_{t}-\\tilde{y}_{t}\\right)\\right\\Vert\\leq h C\\kappa^{3}\\kappa_{B}W_{0}\\sum_{j=0}^{H m}\\left\\Vert{}\\tilde{M}_{i-k}^{j}-\\hat{M}_{i-k}^{j}\\right\\Vert\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore using Assumption 2, Lemma 9 and Lemma 2 we immediately get that ", "page_idx": 24}, {"type": "equation", "text": "$$\nf_{i}\\left(\\tilde{M}_{i-H}\\ldots\\tilde{M}_{i-k}\\ldots\\tilde{M}_{i}\\right)-f_{i}\\left(\\tilde{M}_{i-H}\\ldots\\hat{M}_{i-k}\\ldots\\tilde{M}_{i}\\right)\\Big|\\le C^{2}\\kappa^{3}\\kappa_{B}W_{0}\\sum_{j=0}^{H m}\\|\\tilde{M}_{i-k}^{j}-\\hat{M}_{i-k}^{j}\\|\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Lemma 13. (Bounding the Gradient) We have the following bound for the gradient: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\nabla_{M}f_{t}(M\\cdot\\cdot\\cdot M)\\|_{F}\\leq\\frac{G D C\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Since $M$ is a matrix, the $\\ell_{2}$ norm of the gradient $\\nabla_{M}f_{t}$ corresponds to the Frobenius norm of the $\\nabla_{M}f_{t}$ matrix. So it will be sufficient to derive an absolute value bound on $\\nabla_{M_{p,q}^{[r]}}f_{t}(M,\\dots,M)$ for all $r,p,q$ . To this end, we consider the following calculation. Using lemma 9 we get that $y_{t}(M\\,.\\,.\\,.\\,M),v_{t}(M\\,.\\,.\\,.\\,M)\\leq D$ . Therefore, using Assumption 2 we have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\bigg|\\nabla_{M_{p,q}^{[r]}}c_{t}(M\\cdot\\cdot\\cdot M)\\bigg|\\leq G D\\left(\\bigg|\\bigg|\\frac{\\partial y_{t}(M)}{\\partial M_{p,q}^{[r]}}+\\frac{\\partial v_{t}(M\\cdot\\cdot\\cdot M)}{\\partial M_{p,q}^{[r]}}\\bigg|\\bigg|\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We now bound the quantities on the right-hand side: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\frac{\\delta y_{t}\\left(M\\cdot\\dots M\\right)}{\\delta M_{p,q}^{[r]}}\\right\\|=\\left\\|h\\sum_{i=0}^{2H m}h\\sum_{j=1}^{H m}\\left[\\frac{\\partial Q_{h}^{j}B M^{[i-j]}}{\\partial M_{p,q}^{[r]}}\\right]\\hat{w}_{t-i}\\mathbf{1}_{i-j\\in[1,H]}\\right\\|}\\\\ {\\displaystyle\\qquad\\qquad\\leq h^{2}\\sum_{i=r}^{r+H m}\\left\\|\\left[\\frac{\\partial Q_{h}^{i-r}B M^{[r]}}{\\partial M_{p,q}^{[r]}}\\right]w_{t-i}\\right\\|}\\\\ {\\displaystyle\\qquad\\leq h^{2}\\kappa^{2}W_{0}\\kappa_{B}\\frac{1}{h\\gamma}=\\frac{h\\kappa^{2}W_{0}\\kappa_{B}}{\\gamma}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial v_{t}(M\\cdot\\cdot\\cdot M)}{\\partial M_{p,q}^{[r]}}\\right\\|\\leq\\kappa\\left\\|\\frac{\\delta y_{t}(M\\cdot\\cdot\\cdot M)}{\\delta M_{p,q}^{[r]}}\\right\\|\\leq\\kappa\\frac{h\\kappa^{2}W_{0}\\kappa_{B}}{\\gamma}\\leq\\frac{h\\kappa^{3}W_{0}\\kappa_{B}}{\\gamma}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining the above inequalities with ", "page_idx": 25}, {"type": "equation", "text": "$$\nf_{i}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)=\\sum_{t=i m}^{(i+1)m-1}c_{t}\\left(y_{t}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right),v_{t}\\left(\\tilde{M}_{i-H},\\ldots,\\tilde{M}_{i}\\right)\\right)\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "gives the bound that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\nabla_{M}f_{t}(M\\cdot\\cdot\\cdot M)\\|_{F}\\leq\\frac{G D C\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally we prove Lemma 5: ", "page_idx": 25}, {"type": "text", "text": "Lemma 5. Under Assumption $^{\\,l}$ and 2, choosing $\\begin{array}{r}{m=\\frac{C}{h}}\\end{array}$ and $\\eta=\\Theta(\\frac{m}{T h})$ , Algorithm 1 attains the following bound of $R_{2}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal R}_{2}=\\displaystyle\\sum_{i=0}^{p-1}f_{i}(\\tilde{M}_{i-H},\\dots,\\tilde{M}_{i})-\\operatorname*{min}_{M\\in\\mathcal{M}}\\displaystyle\\sum_{i=0}^{p-1}f_{i}(M,\\dots,M)}\\\\ &{\\quad\\le\\displaystyle\\frac{4a}{\\gamma}\\sqrt{\\frac{G D C^{2}\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}(\\frac{G D C\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}+C^{2}\\kappa^{3}\\kappa_{B}W_{0}H^{2})\\frac{n}{h}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. By Lemma 10 we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nR_{2}\\leq2D_{f}\\sqrt{G_{f}\\left(G_{f}+L H^{2}\\right)p}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Lemma 11, Lemma 12, and Lemma 13 we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{2}\\le2D_{f}\\sqrt{G_{f}\\left(G_{f}+L H^{2}\\right)p}}\\\\ &{\\quad\\le2\\frac{2a}{h\\gamma}\\sqrt{\\frac{G D C\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}(\\frac{G D C\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}+C^{2}\\kappa^{3}\\kappa_{B}W_{0}H^{2})\\frac{n}{m}}}\\\\ &{\\quad\\le\\frac{4a}{\\gamma}\\sqrt{\\frac{G D C^{2}\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}(\\frac{G D C\\kappa^{2}(\\kappa+1)W_{0}\\kappa_{B}}{\\gamma}+C^{2}\\kappa^{3}\\kappa_{B}W_{0}H^{2})\\frac{n}{h}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "G Proof of Lemma 6 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we will prove the approximation value of DAC policy and optimal policy is sufficiently small. First, we introduce the following: ", "page_idx": 26}, {"type": "text", "text": "Lemma 14. For any two $(\\kappa,\\gamma)$ -strongly stable matrices $K^{*},K$ , there exists $M=\\left(M^{1},\\cdot\\cdot\\cdot,M^{H m}\\right)$ where ", "page_idx": 26}, {"type": "equation", "text": "$$\nM^{i}=\\left(K-K^{*}\\right)\\left(I+h(A-B K^{*})\\right)^{i-1}\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "such that ", "page_idx": 26}, {"type": "equation", "text": "$$\nc_{t}(x_{t}(M),u_{t}(M))-c_{t}(x_{t}^{*},u_{t}^{*})\\leq G D W_{0}\\kappa^{3}a(l h\\kappa_{B}+1)(1-h\\gamma)^{H m}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Denote $Q_{h}(K)=I+h(A-B K),Q_{h}(K^{*})=I+h(A-B K^{*})$ . By Lemma 7 we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nx_{t+1}^{*}=h\\sum_{i=0}^{t}Q_{h}^{i}(K^{*}){\\hat{w}}_{t-i}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Consider the following calculation for $i\\leq H m$ and $M^{i}=\\left(K-K^{*}\\right)(I+h(A-B K^{*}))^{i-1};$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\Psi_{t,i}\\left(M,\\dots,M\\right)=Q_{h}^{i}(K)+h\\displaystyle\\sum_{j=1}^{i}Q_{h}^{i-j}(K)B M^{j}}}\\\\ {{{}}}\\\\ {{\\qquad=Q_{h}^{i}(K)+h\\displaystyle\\sum_{j=1}^{i}Q_{h}^{i-j}(K)B\\left(K-K^{*}\\right)Q_{h}^{j-1}(K^{*})}}\\\\ {{{}}}\\\\ {{\\qquad=Q_{h}^{i}(K)+\\displaystyle\\sum_{j=1}^{i}Q_{h}^{i-j}(K)(Q_{h}(K^{*})-Q_{h}(K))Q_{h}^{j-1}(K^{*})}}\\\\ {{{}}}\\\\ {{\\qquad=Q_{h}^{i}(K^{*})\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the final equality follows as the sum telescopes. Therefore, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\nx_{t+1}(M)=h\\sum_{i=0}^{H m}Q_{h}^{i}(K^{*})\\hat{w}_{t-i}+h\\sum_{i=H m+1}^{t}\\Psi_{t,i}\\hat{w}_{t-i}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then we obtain that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|x_{t+1}(M)-x_{t+1}^{*}\\right\\|\\leq h W_{0}\\sum_{i=H m+1}^{t}(\\left\\|\\Psi_{t,i}\\left(M_{*}\\right)\\right\\|+\\left\\|Q_{h}^{i}(K^{*})\\right\\|).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Using Definition 1 and Lemma 7 we finally get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|x_{t+1}(M)-x_{t+1}^{*}\\right\\|\\leq h W_{0}(\\displaystyle\\sum_{i=H m+1}^{t}((l h\\kappa_{B}+1)a\\kappa^{2}(1-h\\gamma)^{i-1})+\\kappa^{2}(1-h\\gamma)^{i})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq W_{0}(l h\\kappa_{B}+2)a\\kappa^{2}(1-h\\gamma)^{H m}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We also have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|u_{t}^{*}-u_{t}(M)\\right|=\\left|-\\sqrt{\\kappa}\\frac{\\gamma}{\\varepsilon_{t}+K}+K z_{t}\\left(M\\right)-\\displaystyle\\sum_{k=0}^{K-1}M^{k}{\\widehat u}_{t-\\widehat{\\gamma}-\\varepsilon}\\right|}&{}\\\\ {=\\left\\|\\left(K-K\\right)^{*}\\pi^{*}+K\\left(\\alpha_{t}(M)-z\\right)-h\\displaystyle\\sum_{k=0}^{K-1}M^{k}{\\widehat u}_{t-\\widehat{\\gamma}-\\varepsilon}\\right\\|}&{}\\\\ {=\\left\\|\\left(K-K\\right)^{*}\\displaystyle\\sum_{k=0}^{t-1}\\widehat{\\gamma}\\right\\|_{L^{2}}^{2}\\left|\\widehat{w}_{t-k}^{*}+K\\left(\\alpha_{t}(M)-z\\right)-h\\displaystyle\\sum_{k=0}^{K-1}M^{k}{\\widehat u}_{t-\\widehat{\\gamma}-\\varepsilon}\\right\\|}&{}\\\\ {=\\left\\|K\\left(\\alpha_{t}(M)-z\\right)-h\\displaystyle\\sum_{k=0}^{t-1}\\widehat{\\gamma}\\right\\|_{L^{2}}^{2}\\left(K-K\\right)^{*}\\displaystyle\\sum_{k=0}^{t-1}(K^{-1}){\\widehat u}_{t-k-1}^{*}\\left\\|}&{}\\\\ {=\\left\\|K\\displaystyle\\sum_{i=\\widehat{t}=\\widehat{\\gamma}+1}^{t-1}\\left(\\widehat{w}_{i_{1}}-Q_{k-1}^{*}(K^{*})\\right){\\widehat u}_{t-k-1}^{*}-\\displaystyle\\sum_{k=\\widehat{t}=\\widehat{\\gamma}+1}^{t-1}\\left(K-K^{*}\\right)Q_{k-1}^{*-1}\\left(K^{*}\\right){\\widehat u}_{t-1}\\right\\|}&{}\\\\ {=\\left\\|\\displaystyle\\left|\\frac{\\widehat{\\gamma}}{h}\\displaystyle\\sum_{k=0}^{t-1}K^{*}\\left(Q_{k-1}^{*}(K^{*})+\\psi_{k}\\right)\\right\\|_{L^{2}}+\\displaystyle\\sum_{k=\\widehat{t}=\\widehat{\\gamma}+1}^{t-1}\\left(K-K^{*}\\right)Q_{k-1}^{*-1}\\left(K^{*}\\right){\\widehat u}_{t-k-1}\\right\\|}&{}\\\\ {\\leq\\left\\|\\alpha_{t-1}^{*}\\left(K\\right)\\right\\|^{2}w+\\alpha(H)s+1\\right)s^{2}\\left\\|-1\\right\\|_{L^{2}}^{2}\\left|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the inequality is by Definition 1 and Lemma 8. ", "page_idx": 27}, {"type": "text", "text": "Finally, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|c_{t}\\left(x_{t}(M),u_{t}(M)\\right)-c_{t}\\left(x_{t}^{*},u_{t}^{*}\\right)\\right|}\\\\ &{\\leq\\left|c_{t}\\left(x_{t}(M),u_{t}(M)\\right)-c_{t}\\left(x_{t}^{*},u_{t}(M)\\right)\\right|+\\left|c_{t}\\left(x_{t}^{*},u_{t}(M)\\right)-c_{t}\\left(x_{t}^{*},u_{t}^{*}\\right)\\right|}\\\\ &{\\leq\\!G D|x_{t}(M)-x_{t}^{*}|+G D|u_{t}(M)-u_{t}^{*}|}\\\\ &{\\leq\\!G D W_{0}\\kappa^{3}a(l h\\kappa_{B}+1)(1-h\\gamma)^{H m}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the second inequality is by Assumption 2. ", "page_idx": 27}, {"type": "text", "text": "Then we can prove our main lemma: ", "page_idx": 27}, {"type": "text", "text": "Lemma 6. Under Assumption $^{\\,I}$ and 2, Algorithm $^{\\,I}$ attains the following bound of $R_{3}$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathfrak{Z}_{3}=\\operatorname*{min}_{M\\in\\mathcal{M}}\\sum_{i=0}^{p-1}f_{i}(M,...,M)-\\sum_{i=0}^{p-1}\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i}^{*},u_{i}^{*})\\le3n(1-h\\gamma)^{H m}G D W_{0}\\kappa^{3}a(l h\\kappa_{B}+1)\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. By choosing ", "page_idx": 27}, {"type": "equation", "text": "$$\nM^{i}=\\left(K-K^{*}\\right)\\left(I+h(A-B K^{*})\\right)^{i-1}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We know that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|M^{i}\\|=\\|\\left(K-K^{*}\\right)\\left(I+h(A-B K^{*})\\right)^{i-1}\\|\\leq2\\kappa^{3}(1-\\gamma)^{i-1}\\,.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore choose $a=2\\kappa^{3}$ we have $M=\\{M^{i}\\}$ in the DAC policy update class $\\mathcal{M}$ . ", "page_idx": 27}, {"type": "text", "text": "Then we have the analysis of the regret: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{I}_{3}=\\displaystyle\\operatorname*{min}_{M\\in\\mathcal{M}}\\sum_{i=0}^{p-1}f_{i}(M,...,M)-\\sum_{i=0}^{p-1}\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i}^{*},u_{i}^{*})}\\\\ &{\\quad\\le\\displaystyle\\operatorname*{min}_{M\\in\\mathcal{M}}\\sum_{i=0}^{p-1}\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i}(M),u_{i}(M))-\\sum_{i=0}^{p-1}\\sum_{j=i m}^{(i+1)m-1}c_{i}(x_{i}^{*},u_{i}^{*})+n\\kappa^{2}(1+\\kappa)(1-h\\gamma)^{H m+1}}\\\\ &{\\quad\\le3n(1-h\\gamma)^{H m}G D W_{0}\\kappa^{3}a(l h\\kappa_{B}+1)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the first inequality is by Lemma 2 and the second inequality is by Lemma 14. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We clarify our contributions and basic problem setups in both abstract and introduction. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We discuss the limitation of our paper in Section 7. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the full set of assumptions and a complete proof. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We disclose the experiment details in Section 6. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our code is very simple, just use the traditional SAC algorithm with one line implement. Our main contribution is the theoretical analysis. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We specify all the training and test details in 6. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: For each experiment we use 3 random seeds and take the average. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We specify all the computational resources in 6. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Our work is about the theory on online control, which does not seem to have evident societal impacts. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We add citations for all datasets we used. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 32}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]