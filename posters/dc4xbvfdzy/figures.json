[{"figure_path": "dc4xbVfdzy/figures/figures_3_1.jpg", "caption": "Figure 1: Model Overview. The left: we combine the trajectories T with position embeddings, and then feed the result sequence to the Decision Mamba encoder which has L layers. The middle: a coarse-grained branch and a fine-grained branch are integrated together to capture the trajectory features. The right: visualization of multi-grained scans.", "description": "This figure provides a high-level overview of the Decision Mamba model architecture. The left panel shows the input processing stage, where trajectories are combined with positional embeddings and fed into the encoder. The middle panel details the encoder's core structure, highlighting the integration of coarse-grained and fine-grained branches to capture both global and local trajectory features.  The right panel illustrates the concept of multi-grained scans, visualizing how both global and local features are extracted and processed within the architecture.", "section": "3.2 Decision Mamba"}, {"figure_path": "dc4xbVfdzy/figures/figures_4_1.jpg", "caption": "Figure 1: Model Overview. The left: we combine the trajectories T with position embeddings, and then feed the result sequence to the Decision Mamba encoder which has L layers. The middle: a coarse-grained branch and a fine-grained branch are integrated together to capture the trajectory features. The right: visualization of multi-grained scans.", "description": "This figure provides a high-level overview of the Decision Mamba model architecture.  It shows three key components: the trajectory embedding process (left), the multi-grained state space model (SSM) which combines coarse-grained and fine-grained information (middle), and a visualization of the multi-grained scanning mechanism (right). The left side illustrates how trajectories are processed using position embeddings and fed into the model's encoder. The middle section details the core architecture of Decision Mamba, showing how it uses both coarse-grained and fine-grained SSMS to capture trajectory features at different scales. The right section shows the scanning method across different scales.", "section": "3.2 Decision Mamba"}, {"figure_path": "dc4xbVfdzy/figures/figures_4_2.jpg", "caption": "Figure 2: The process of PSER includes: i) generating action labels with previous step policy, ii) refining target label, iii) computing loss, where the red circle denotes the noise.", "description": "This figure illustrates the Progressive Self-Evolution Regularization (PSER) process.  It shows three steps: 1) The previous step's policy (k-1) generates action labels. 2) These labels are refined by combining them with the current policy's predictions (k), using a weighted average controlled by \u03b2k. The weighting emphasizes the current step's prediction when \u03b2k is closer to 0 and the previous step's prediction when \u03b2k is close to 1. 3) The refined labels are then used to compute a loss function (LPSE,k), guiding the training process to focus more on clean data while mitigating the effects of noisy data points (represented by the red circle).", "section": "3.2.2 Progressive Self-Evolution Regularization"}, {"figure_path": "dc4xbVfdzy/figures/figures_8_1.jpg", "caption": "Figure 3: Impact of Context Lengths. We compare the normalized scores of BC, DT and DM with different context lengths. The DM consistently outperforms other baselines.", "description": "This figure shows the results of an experiment comparing the performance of three different offline reinforcement learning models (BC, DT, and DM) across various context lengths. The experiment was conducted on three different datasets (Hopper-M, Hopper-M-E, Hopper-M-R, Halfcheetah-M, Halfcheetah-M-E, Halfcheetah-M-R).  The results demonstrate that the Decision Mamba (DM) model consistently outperforms the other two models across all datasets and context lengths, indicating its robustness and effectiveness in capturing both local and global information from the trajectory data.  The y-axis represents the normalized score of each model, and the x-axis represents the length of the context window used in the model.", "section": "4.4 Comparison Results with Different Context Lengths"}, {"figure_path": "dc4xbVfdzy/figures/figures_13_1.jpg", "caption": "Figure 4: The visualizations of tasks.", "description": "This figure visualizes the five different tasks used in the experiments: HalfCheetah, Hopper, Walker, Ant, and Antmaze. Each image shows a different robotic agent in its environment, highlighting the diversity of the tasks.", "section": "4.1 Settings"}, {"figure_path": "dc4xbVfdzy/figures/figures_15_1.jpg", "caption": "Figure 5: The normalized scores of DT and DM when conditioned on the specified target returns.", "description": "This figure compares the performance of Decision Transformer (DT) and Decision Mamba (DM) models when predicting actions based on different target returns.  The x-axis represents the target return (normalized), and the y-axis represents the normalized score achieved by each model. The figure shows that DM consistently outperforms DT, especially in the out-of-distribution (OOD) region (returns beyond the range observed in the training data). The dashed lines represent the oracle (optimal) performance and the best trajectory in the training dataset.  The results demonstrate that DM is more robust to unseen return-to-go values than DT.", "section": "4.3 Ablation Study"}, {"figure_path": "dc4xbVfdzy/figures/figures_15_2.jpg", "caption": "Figure 6: The distributions of action.", "description": "This figure visualizes the action distribution of the learned policies trained on different levels of noisy data.  It shows the hidden layer of the predicted action to compare the action distribution learned by Decision Transformer (DT) and Decision Mamba (DM).  DM's distribution is more concentrated, indicating its robustness to varying noise levels in the training data.", "section": "E Visualization of Action Distribution"}]