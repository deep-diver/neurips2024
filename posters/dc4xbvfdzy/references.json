{"references": [{"fullname_first_author": "Lili Chen", "paper_title": "Decision Transformer: Reinforcement learning via sequence modeling", "publication_date": "2021-12-01", "reason": "This paper introduces the Decision Transformer architecture, which is a major inspiration for the proposed Decision Mamba and serves as a key comparison baseline."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-12-01", "reason": "This paper introduces the Conservative Q-learning algorithm, a prominent offline RL method that is used as a comparison baseline and discussed in relation to the proposed method's advantages."}, {"fullname_first_author": "Scott Emmons", "paper_title": "Rvs: What is essential for offline rl via supervised learning?", "publication_date": "2021-12-01", "reason": "This paper explores the effectiveness of supervised learning for offline RL, which is a related approach compared against the proposed method, highlighting the different strengths and weaknesses."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-01-01", "reason": "This paper introduces the Mamba architecture, a core component of Decision Mamba, and its properties are crucial to understanding Decision Mamba's design and capabilities."}, {"fullname_first_author": "Justin Fu", "paper_title": "D4rl: Datasets for deep data-driven reinforcement learning", "publication_date": "2020-01-01", "reason": "This paper introduces the D4RL benchmark, a collection of datasets used for evaluating offline RL algorithms.  The use of this benchmark is central to the experimental validation of the proposed approach."}]}