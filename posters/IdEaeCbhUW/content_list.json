[{"type": "text", "text": "Achieving Precise Control with Slow Hardware: Model-Based Reinforcement Learning for Action Sequence Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Current reinforcement learning (RL) models are often claimed to explain animal   \n2 behavior. However, they are designed for artificial agents that sense, think, and react   \n3 much faster than the brain, and they tend to fail when operating under human-like   \n4 sensory and reaction times. Despite using slow neurons, the brain achieves precise   \n5 and low-latency control through a combination of predictive and sequence learning.   \n6 The basal ganglia is hypothesized to learn compressed representations of action   \n7 sequences, allowing the brain to produce a series of actions for a given input. We   \n8 present the Hindsight-Sequence-Planner (HSP), a model of the basal ganglia and   \n9 the prefrontal cortex that operates under \"brain-like\" conditions: slow information   \n10 processing with quick sensing and actuation. Our \"temporal recall\" mechanism is   \n11 inspired by the prefrontal cortex\u2019s role in sequence learning, where the agent uses   \n12 an environmental model to replay memories at a finer temporal resolution than its   \n13 processing speed while addressing the credit assignment problem caused by scalar   \n14 rewards in sequence learning. HSP employs model-based training to achieve model  \n15 free control, resulting in precise and efficient behavior that appears low-latency   \n16 despite running on slow hardware. We test HSP on various continuous control   \n17 tasks, demonstrating that it not can achieve comparable performance \u2019human-like\u2019   \n18 frequencies by relying on significantly fewer observations and actor calls (actor   \n19 sample complexity). ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Biological and artificial agents must learn behaviors that maximize rewards to thrive in complex   \n22 environments. Reinforcement learning (RL), a class of algorithms inspired by animal behavior,   \n23 facilitates this learning process (1). The connection between neuroscience and RL is profound.   \n24 The Temporal Difference (TD) error, a key concept in RL, effectively models the firing patterns of   \n25 dopamine neurons in the midbrain (2; 3; 4). Additionally, a longstanding goal of RL algorithms is to   \n26 match and surpass human performance in control tasks (5; 6; 7; 8; 9; 10)   \n27 However, most of these successes are achieved by leveraging large amounts of data in simulated   \n28 environments and operating at speeds orders of magnitude faster than biological neurons. For example,   \n29 the default timestep for the Humanoid task in the MuJoCo environment (11) in OpenAI Gym (12)   \n30 is 15 milliseconds. In contrast, human reaction times range from 150 milliseconds (13) to several   \n31 seconds for complex tasks (14). When RL agents are constrained to human-like reaction times, even   \n32 state-of-the-art algorithms struggle to perform in simple environments.   \n33 The primary reason for this difficulty is the implicit assumption in RL that the environment and the   \n34 agent operate at a constant timestep. Consequently, in embodied agents, all components\u2014sensors,   \n35 compute units, and actuators\u2014are synchronized to operate at the same frequency. Typically, this   \n36 frequency is limited by the speed of computation in artificial agents (15). As a result, robots often   \n37 require fast onboard computing hardware (CPU or GPU) to achieve higher control frequencies   \n38 (16; 17; 18).   \n39 In contrast, biological agents achieve precise and seemingly fast control using much slower hard  \n40 ware. This is possible because biological agents effectively decouple the computation frequency   \n41 from the actuation frequency, allowing them to achieve high actuation frequencies even with slow   \n42 computational speeds. Consequently, biological agents demonstrate robust, adaptive, and efficient   \n43 control.   \nTo allow the RL agent to observe and react to changes in the environment quickly, RL algorithms are   \n45 forced to set a high frequency. Even in completely predictable environments, when the agent learns   \n46 to walk or move, a small timestep is required to account for the actuation frequency required for the   \n47 task, but it is not necessary to observe the environment as often or compute new actions as frequently.   \n48 As a result, RL algorithms suffer from many problems such as low sample efficiency, failure to learn   \n49 tasks with sparse rewards, jerky control, high compute cost, and catastrophic failure due to missing   \n50 inputs.   \nIn this work, we propose Hindsight-Sequence-Planner (HSP), a model for sequence learning based on   \n52 the role of the basal ganglia (BG) and the prefrontal cortex (PFC). Our model learns open-loop control   \n53 utilizing a slow hardware and low attention, and hence also low energy. Additionally, the algorithm   \n54 utilizes a simultaneously learned model of the environment during its training but can act without it   \n55 for fast and cheap inference. We demonstrate the algorithm achieves competitive performance on   \n56 difficult continuous control tasks while utilizing a fraction of observations and calls to the policy. To   \n57 the best of our knowledge, HSP is the first to achieve this feat. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "58 2 Neural Basis for Sequence Learning ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "59 Unlike artificial RL agents, learning in the brain does not stop once an optimal solution has been   \n60 found. During initial task learning, brain activity increases as expected, reflecting neural recruitment.   \n61 However, after training and repetition, activity decreases as the brain develops more efficient repre  \n62 sentations of the action sequence, commonly referred to as muscle memory (19). This phenomenon   \n63 is further supported by findings that sequence-specific activity in motor regions evolves based on the   \n64 amount of training, demonstrating skill-specific efficiency and specialization over time (20).   \n65 The neural basis for action sequence learning involves a sophisticated interconnection of different   \n66 brain regions, each making a distinct contribution:   \n67 1. Basal ganglia (BG): Action chunking is a cognitive process by which individual actions are   \n68 grouped into larger, more manageable units or \"chunks,\" facilitating more efficient storage,   \n69 retrieval, and execution with reduced cognitive load (21). Importantly, this mechanism   \n70 allows the brain to perform extremely fast and precise sequences of actions that would be   \n71 impossible if produced individually. The BG plays a crucial role in chunking, encoding   \n72 entire behavioral action sequences as a single action (22; 21; 23; 24; 25; 26). Dysfunction   \n73 in the BG is associated with deficits in action sequences and chunking in both animals   \n74 (27; 28; 29) and humans (30; 31; 21). However, the neural basis for the compression of   \n75 individual actions into sequences remains poorly understood.   \n76 2. Prefrontal cortex (PFC): The PFC is critical for the active unbinding and dismantling   \n77 of action sequences to ensure behavioral flexibility and adaptability (32). This suggests   \n78 that action sequences are not merely learned through repetition; the PFC modifies these   \n79 sequences based on context and task requirements. Recent research indicates that the PFC   \n80 supports memory elaboration (33) and maintains temporal context information (34) in action   \n81 sequences. The prefrontal cortex receives inputs from the hippocampus.   \n82 3. Hippocampus (HC) replays neuronal activations of tasks during subsequent sleep at speeds   \n83 six to seven times faster. This memory replay may explain the compression of slow actions   \n84 into fast chunks. The replayed trajectories from the HC are consolidated into long-term   \n85 cortical memories (35; 36). This phenomenon extends to the motor cortex, which replays   \n86 motor patterns at accelerated speeds during sleep (37). ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "87 3 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "88 3.1 Model-Based Reinforcement Learning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "89 Model-Based Reinforcement Learning (MBRL) algorithms leverage a model of the environment,   \n90 which can be either learned or known, to enhance RL performance (38). Broadly, MBRL algorithms   \n91 have been utilized to:   \n92 1. Improve Data Efficiency: By augmenting real-world data with model-generated data, MBRL   \n93 can significantly enhance data efficiency (39; 40; 41).   \n94 2. Enhance Exploration: MBRL aids in exploration by using models to identify potential or   \n95 unexplored states (42; 43; 44).   \n96 3. Boost Performance: Better learned representations from MBRL can lead to improved   \n97 asymptotic performance (45; 46).   \n98 4. Transfer Learning: MBRL supports transfer learning, enabling knowledge transfer across   \n99 different tasks or environments (47; 48).   \n100 5. Online Planning: Models can be used for online planning with a single-step policy (49).   \n101 However, this approach increases model complexity as each online planning step requires an   \n102 additional call to the model, making it nonviable for energy and computationally constrained   \n103 agents like the brain and robots.   \n104 Compared to online planning, our algorithm maintains a model complexity of zero after training, elim  \n105 inating the need for any model calls post-training. This significantly reduces the computational and   \n106 energy requirements, making it more suitable for practical applications in constrained environments.   \n107 Additionally, the performance of online planning algorithms relies heavily on the accuracy of the   \n108 model. In contrast, our approach can leverage even an inaccurate model to learn a better-performing   \n109 policy than online planning, using the same model. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "110 3.2 Macro-Actions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 Reinforcement Learning (RL) algorithms that utilize macro-actions demonstrate many benefits,   \n112 including improved exploration and faster learning (50). However, identifying effective macro  \n113 actions is a challenging problem due to the curse of dimensionality, which arises from large action   \n114 spaces. To address this issue, some approaches have employed genetic algorithms (51) or relied on   \n115 expert demonstrations to extract macro-actions (52). However, these methods are not scalable and   \n116 lack biological plausibility.   \n117 In contrast, our approach learns macro-actions using the principles of RL, thus requiring little   \n118 overhead while combining the flexibility of primitive actions with the efficiency of macro-actions. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "119 3.3 Action Repetition and Frame-skipping ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 To overcome the curse of dimensionality while gaining the beneftis of macro-actions, many approaches   \n121 utilize frame-skipping and action repetition, where macro-actions are restricted to a single primitive   \n122 action that is repeated. Frame-skipping and action repetition serve as a form of partial open-loop   \n123 control, where the agent selects a sequence of actions to be executed without considering the   \n124 intermediate states. Consequently, the number of actions is linear in the number of time steps   \n125 (53; 54; 55; 56; 57).   \n126 For instance, FiGaR (56) shifts the problem of macro-action learning to predicting the number of   \n127 steps that the outputted action can be repeated. TempoRL (55) improved upon FiGaR by conditioning   \n128 the number of repetitions on the selected actions. However, none of these algorithms can scale to   \n129 continuous control tasks with multiple action dimensions, as action repetition forces all actuators   \n130 and joints to be synchronized in their repetitions, leading to poor performance for longer action   \n131 sequences. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "IdEaeCbhUW/tmp/1d32cba46d0171f0cdd8b52589421f80698fa592400e034f6bbcdfe92af16091.jpg", "img_caption": ["Figure 1: The Hindsight-Sequence-Planner (HSP) model. The HSP takes inspiration from the function of the basal ganglia (BG) (Top/Orange) and the prefrontal cortex (PFC) (Bottom/Blue). We train an actor with a gated recurrent unit that can produce sequences of arbitrary lengths given a single state. This is achieved by utilizing a critic and a model that acts at a finer temporal resolution during training/replay to provide an error signal to each primitive action of the action sequence. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "132 4 Hindsight Sequence Planner ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "133 Based on the insights presented in Section 2, we introduce a novel reinforcement learning model   \n134 capable of learning sequences of actions (macro-actions) by replaying memories at a finer temporal   \n135 resolution than the action generation, utilizing a model of the environment during training. ", "page_idx": 3}, {"type": "text", "text": "136 Components ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "137 The Hindsight-Sequence-Planner (HSP) algorithm learns to plan \"in-the-mind\" using a model during   \n138 training, allowing the learned action-sequences to be executed without the need for model-based   \n139 online planning. This is achieved using an actor-critic setting where the actor and critic operate at   \n140 different frequencies, representing the observation/computation and actuation frequencies, respec  \n141 tively. Essentially, the critic is only used during training/replay and can operate at any temporal   \n142 resolution, while the actor is constrained to the temporal resolution of the slowest component in the   \n143 sensing-compute-actuation loop. Denoting the actor\u2019s timestep as $t^{\\prime}$ and the critic\u2019s timestep as $t$ , our   \n144 algorithm includes three components: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathrm{Model}\\ :s_{t+1}={\\mathbf{m}}_{\\phi}(s_{t},a_{t})}}\\\\ {{\\mathrm{Critic}\\ :q_{t}={\\mathbf{q}}_{\\psi}(s_{t},a_{t})}}\\\\ {{\\mathrm{Actor}\\ :a_{t^{\\prime}}=a_{t},a_{t^{\\prime}+t},a_{t^{\\prime}+2t}..\\sim\\pi_{\\omega}(s_{t^{\\prime}})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 We denote individual actions in the action sequence generated by actor using the notation $\\pi_{\\omega}(s_{t^{\\prime}})_{t}$ to   \n146 represent the action $a_{t^{\\prime}+t}$   \n147 1. Model: Learns the dynamics of the environment, predicting the next state $s_{t+1}$ given the   \n148 current state $s_{t}$ and primitive action $a_{t}$ .   \n149 2. Critic: Takes the same input as the model but predicts the Q-value of the state-action pair.   \n150 3. Actor: Produces a sequence of actions given an observation at time $t^{\\prime}$ . Observations from   \n151 the environment can occur at any timestep $t$ or $t^{\\prime}$ , where we assume $t^{\\prime}>t$ . Specifically, in   \n152 our algorithm, $t^{\\prime}=J t$ where $J>1;J\\in\\mathbb{Z}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "153 Each component of our algorithm is trained in parallel, demonstrating competitive learning speeds. ", "page_idx": 3}, {"type": "text", "text": "154 We follow the Soft-Actor-Critic (SAC) algorithm (58) for learning the actor-critic. Exploration and   \n155 uncertainty are critical factors heavily influenced by timestep size and planning horizon. Many   \n156 model-free algorithms like DDPG (59) and TD3 (60) explore by adding random noise to each action   \n157 during training. However, planning a sequence of actions over a longer timestep can result in additive   \n158 noise, leading to poor performance during training and exploration. The SAC algorithm addresses this   \n159 by maximizing the entropy of each action in addition to the expected return, allowing our algorithm   \n160 to automatically lower entropy for deeper actions farther from the observation. ", "page_idx": 3}, {"type": "text", "text": "161 Learning the Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "162 The model is trained to minimize the Mean Squared Error of the predicted states. For a trajectory $\\tau=$   \n163 $(s_{t},a_{t},s_{t+1})$ drawn from the replay buffer $\\mathcal{D}$ , the predicted state is taken from $\\tilde{s}_{t+1}\\sim{\\mathbf{m}}\\phi(s_{t},a_{t})$ .   \n164 The loss function is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\phi}=\\mathbb{E}_{\\tau\\sim\\mathcal{D}}\\big(\\tilde{s}_{t+1}-s_{t+1}\\big)^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "165 For this work, the model is a feed-forward neural network with two hidden layers. In addition to the   \n166 current model $\\mathbf{m}_{\\phi}$ , we also maintain a target model ${\\bf{m}}_{\\phi^{-}}$ that is the exponential moving average of   \n167 the current model. ", "page_idx": 4}, {"type": "text", "text": "168 Learning Critic ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "169 The critic is trained to predict the Q-value of a given state-action pair $\\tilde{q}_{t}=\\mathbf{q}_{\\psi}\\big(\\boldsymbol{s}_{t},\\boldsymbol{a}_{t}\\big)$ using the target   \n170 value from the modified Bellman equation: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{q}_{t}=r_{t}+\\gamma\\mathbb{E}_{a_{t+1}\\sim\\pi_{\\omega}(s_{t+1})_{0}}[\\mathbf{q}_{\\psi^{-}}(s_{t+1},a_{t+1})-\\alpha\\log\\pi_{\\omega}(a_{t+1}|s_{t+1})]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "171 Here, $\\mathbf{q}_{\\psi^{-}}$ is the target critic, which is the exponential moving average of the critic. Following the   \n172 SAC algorithm, we train two critics and use the minimum of the two $\\mathbf{q}_{\\psi^{-}}$ values to train the current   \n173 critics. The loss function is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\psi}=\\mathbb{E}_{\\tau\\sim\\mathcal{D}}[(\\tilde{q}_{t\\,k}-\\hat{q}_{t})^{2}]\\forall k\\in1,2\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "174 Both critics are feed-forward neural networks with two hidden layers. ", "page_idx": 4}, {"type": "text", "text": "175 Learning Policy ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "176 The HSP policy utilizes two hidden layers followed by a Gated-Recurrent-Unit (GRU) (61) that takes   \n177 as input the previous action in the action sequence, followed by two linear layers that output the mean   \n178 and standard deviation of the Gaussian distribution of the action. This design allows the policy to   \n179 produce action sequences of arbitrary length given a single state and the last action.   \n180 A naive approach to training a sequence of actions would be to augment the action space to include   \n181 all possible actions of the sequence length. However, this quickly leads to the curse of dimensionality,   \n182 as each sequence is considered a unique action, dramatically increasing the policy\u2019s complexity.   \n183 Additionally, such an approach ignores the temporal information of the action sequence and faces the   \n184 difficult problem of credit assignment, with only a single scalar reward for the entire action sequence.   \n185 To address these problems, we use different temporal scales for the actor and critic. The critic assigns   \n186 value to each segment of the action sequence, bypassing the credit assignment problem caused by the   \n187 single scalar reward. However, using collected transitions to train the action sequence is impractical,   \n188 as changing the first action in the sequence would render all future states inaccurate. Thus, the model   \n189 populates intermediate states, which the critic then uses to assign value to each primitive action in the   \n190 sequence.   \n191 Therefore, given a trajectory $\\tau=\\left(a_{t-1},s_{t},a_{t},s_{t+1}\\right)$ , we first produce the $J$ -step action sequence   \n192 using the policy: $\\tilde{a}_{t:t+J}=\\pi_{\\phi}\\big(s_{t}\\big)$ . We then iteratively apply the target model to get the intermediate   \n193 states $\\tilde{s}_{t+1:t+J-1}$ . Finally, we use the critic to calculate the loss for the actor as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\omega}=\\mathbb{E}_{\\tau\\sim\\mathcal{D}}\\left[\\alpha\\log\\pi_{\\omega}(\\tilde{a}_{t}|s_{t})-\\mathbf{q}_{\\psi}(s_{t},\\tilde{a}_{t})+\\sum_{j=1}^{J}\\alpha\\log\\pi_{\\omega}(\\tilde{a}_{t+j}|\\tilde{s}_{t+j})-\\mathbf{q}_{\\psi}(\\tilde{s}_{t+j},\\tilde{a}_{t+j})\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "194 5 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "195 Overview ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "196 We evaluate our HSP approach on several continuous control tasks, comparing it against the SAC   \n197 baseline and the TempoRL algorithm (55). Our focus is on environments with multi-dimensional   \n198 actions, ranging from the simple LunarLanderContinuous (2 action dimensions) to the complex   \n199 Humanoid environment (17 action dimensions). This allows us to highlight the beneftis of HSP over   \n200 traditional action repetition approaches. We utilize the OpenAI gym (62) implementation of the   \n201 MuJoCo environments (11). ", "page_idx": 4}, {"type": "text", "text": "202 Experiemental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "203 We train HSP with four different action sequence lengths (ASL), $J=2,4,8,16$ , referred to as HSP- $J$ .   \n204 During training, HSP is evaluated based on its $J$ value, processing states only after every $J$ actions.   \n205 All hyperparameters are identical between HSP and SAC, except for the actor update frequency: HSP   \n206 updates the actor every 4 steps, while SAC updates every step. Thus, SAC has four more actor update   \n207 steps compared to HSP. Additionally, HSP learns a model in parallel with the actor and critic.   \n209 Figure 6 presents the learning curves of HSP and SAC across six continuous control tasks. We observe   \n210 that HSP outperforms SAC in four out of six tasks (excluding Ant and HalfCheetah). Notably, HSP-16   \n211 achieves competitive performance on LunarLander and Hopper tasks, showcasing the algorithm\u2019s   \n212 capability to learn long action sequences from scratch. Surprisingly, HSP also outperforms SAC in   \n213 the Humanoid environment with fewer inputs and actor updates while concurrently learning a model,   \n214 demonstrating the efficacy of the algorithm on environments with higher action dimensions. ", "page_idx": 5}, {"type": "image", "img_path": "IdEaeCbhUW/tmp/bd49a8783af73d2568912b1d2790fea56c52ddd7595bfcd624265cfb49326087.jpg", "img_caption": ["208 Learning Curves ", "Figure 2: Learning curves of HSP- $J$ and Soft-Actor Critic (SAC) (58) over continuous control tasks. HSP and SAC are evaluated under different settings: SAC receives input after every primitive action, while HSP receives input after $J$ primitive actions. Yet it demonstrates competitive performance on all environments, even outperforming SAC on LunarLander, Hopper and Humanoid environments. HSP demonstrates stable learning even with the added model and generative replay training. All curves are averaged over 5 trials, with shaded regions representing standard error. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "215 Action Sequence Length (ASL) Performance ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "216 Learning curves alone do not fully capture HSP\u2019s performance and benefits. For instance, HSP-16   \n217 shows poor performance on Ant in the learning curve, yet it demonstrates competitive performance   \n218 when tested on shorter action sequences. Figure 3 presents the performance of trained algorithms   \n219 across different action sequence lengths (ASL).   \n220 We select the largest $J$ that shows competitive performance (greater than $75\\%$ of the SAC when   \n221 evaluated on primitive actions) for each environment and test it for sequence lengths up to 30. For   \n222 SAC and HSP, we fix the length of action sequences while TempoRL is designed to dynamically pick   \n223 the best ASL, therefore we report the avg. action sequence length for TempoRL. HSP demonstrates   \n224 competitive performance on longer action sequences, approaching human-like reaction times in   \n225 some environments. Unlike SAC, which fails with action sequences of 2 or 3, HSP shows a gradual   \n226 degradation of performance. Additionally, HSP generalizes well in environments like LunarLander   \n227 and Ant, even though the actor is trained only on sequence lengths of 16.   \n228 Comparing HSP to TempoRL, we find that TempoRL prefers shorter repetitions and struggles in   \n229 more difficult environments. TempoRL does not incentivize longer actions and suffers from the   \n230 curse of dimensionality to some extent, as it needs to learn the number of repetitions for each unique   \n231 state-action pair. Furthermore, action repetitions are not suitable for multi-dimensional actions, as   \n232 they force synchronized repetition across all actuators resulting in poor performance in environments   \n233 with high action dimensions like Ant and HalfCheetah environments. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "IdEaeCbhUW/tmp/164b1ab02ed556d922efd3cbc5be47fa08646a57858c1d004474f66e9e44e92d.jpg", "img_caption": ["Figure 3: Performance of HSP, SAC, and TempoRL (55) at different Action Sequence Lengths (ASL). SAC and TempoRL repeat the same action for the duration, while HSP can perform a sequence of actions. Since it implements dynamic action repetition, we present the average ASL for TempoRL instead of a range of ASL. HSP demonstrates robust performance even at human-like reaction times $(>\\!150\\mathrm{ms})$ . All markers are averaged over 5 trials, with the error bars representing standard error. Going from left to right then top to bottom, the selected training ASL $J$ for HSP are: 16, 16, 4, 16, 4, 8. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "234 Comparison to Model-based Online Planning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "235 In addition to action repetition, model-based online planning is another approach that allows the RL   \n236 agent to reduce its observational frequency. However, it often requires a highly accurate model of   \n237 the environment and incurs increased model complexity due to the use of the model during control.   \n238 Despite these challenges, comparing HSP to model-based online planning is essential since it is useful   \n239 when the actor cannot produce long sequences of actions and does not require the hyper-parameter $J$ .   \n240 With access to an accurate model of the environment, the agent\u2019s performance might generalize to   \n241 arbitrary ASL.   \n242 Since HSP incorporates a model of the environment that is learned in parallel, we compare the   \n243 performance of the HSP actor utilizing the actor-generated action sequences against model-based   \n244 online planning, where the actor produces only a single action between each simulated state.   \n245 Figure 4 shows the performance of online planning using the model in HSP versus the action   \n246 sequences generated by the HSP policy. We see that HSP can learn action sequences that perform   \n247 better than model-based online planning using the same model. Thus, HSP can leverage inaccurate   \n248 models to learn accurate action sequences, further reducing the required computational complexity   \n249 during training. We hypothesize that this superior performance is due to the fact that the actor learns   \n250 a $J$ -step action sequence concurrently, while online planning only produces one action at a time.   \n251 Consequently, HSP is able to learn and produce long, coherent action sequences, whereas single-step   \n252 predictions tend to drift, similar to the \"hallucination\" phenomenon observed in transformer-based   \n253 language models. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "IdEaeCbhUW/tmp/0705f149aa2cd3db4e072ae04685dadede5d5ba32ab399b0d08ef164e6aee1a4.jpg", "img_caption": ["Figure 4: Performance of HSP and model based online planning on different ASL. Both HSP and Online Planning utilize the same actor and model. HSP utilizes the actor to generate a sequence of actions while online planning utilizes the actor and the model to generate a sequence of actions. The same model is used to train the HSP action sequences. Yet, we find that while the model is not accurate enough to sustain performance for longer sequences, it can train the actor to produce accurate action sequences. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "254 Generative Replay in Latent Space ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "IdEaeCbhUW/tmp/9daacef756d9aa5babe760092fa9d9c189f7d4bee6d4a111b73db956269fc748.jpg", "img_caption": ["Figure 5: Left: Learning curve of HSP with latent state-space on the Walker2d-v2 environment. Right: Performance of latent HSP-16 on different ASL, compared to SAC and TempoRL. Utilizing a latent representation for state space is especially beneficial for the Walker2d environment so that it outperforms SAC even when training upto sequence lengths of $J=16$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "255 Previous studies have shown that generative replay benefits greatly from latent representations (63).   \n256 Recently, Simplified Temporal Consistency Reinforcement Learning (TCRL) (64) demonstrated   \n257 that learning a latent state-space improves not only model-based planning but also model-free RL   \n258 algorithms. Building on this insight, we introduced an encoder to encode the observations in our   \n259 algorithm. We provide the complete implementation details in the Appendix.   \n260 We did not observe any benefits of using the encoder and temporal consistency for HSP in most   \n261 environments (results in the appendix). However, for the Walker environment, utilizing the latent   \n262 space for generative replay significantly improved performance, making it competitive even at 16   \n263 steps (128ms) (Figure 5). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "264 6 Discussion, Limitations and Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "265 We introduce the Hindsight-Sequence-Planner (HSP) algorithm, a biologically plausible model for   \n266 sequence learning. It represents a significant step towards achieving robust control at brain-like   \n267 speeds. The key contributions of HSP include its ability to generate long sequences of actions from a   \n268 single state, its resilience to reduced input frequency, and its lower computational complexity per   \n269 primitive action.   \n270 The current RL framework encourages synchrony between the environment and the components   \n271 of the agent. However, the brain utilizes components that act at different frequencies and yet is   \n272 capable of robust and accurate control. HSL provides an approach to reconcile this difference   \n273 between neuroscience and RL, while remaining competitive on current RL benchmarks. HSP offers   \n274 substantial beneftis over traditional RL algorithms, particularly in the context of autonomous agents   \n275 such as self-driving cars and robots. By enabling operation at slower observational frequencies and   \n276 providing a gradual decay in performance with reduced input frequency, HSP addresses critical   \n277 issues related to sensor failure and occlusion, and energy consumption. Additionally, HSP generates   \n278 long sequences of actions from a single state, which can enhance the explainability of the policy   \n279 and provide opportunities to override the policy early in case of safety concerns. HSP also learns   \n280 a latent representation of the action sequence, which could be used in the future to interface with   \n281 large language models for multimodal explainability and even hierarchical reinforcement learning   \n282 and transfer learning. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "283 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "284 Despite its advantages, HSP has some limitations. It shows slightly reduced performance in the   \n285 Ant and HalfCheetah environments, which we believe can be mitigated through improved models   \n286 and hyperparameter tuning. HSP also requires more computational resources during training due   \n287 to the parallel training of an environment model and introduces more hyperparameters, particularly   \n288 the training ASL $(J)$ . In this work, we do not optimize the neural network architecture of the actor   \n289 to reduce the compute, as a result, the total compute per primitive action is still larger than SAC.   \n290 However, we believe producing a sequence of actions will be more efficient than producing a single   \n291 primitive action per state after optimization. Larger ASL values may not perform well in stochastic   \n292 environments. Moreover, HSP currently uses a constant ASL, but ideally, the ASL should adapt   \n293 based on the environment\u2019s predictability. ", "page_idx": 8}, {"type": "text", "text": "294 Future Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "295 We believe the HSP model contributes to both artificial agents and the study of biological control.   \n296 Future work will incorporate biological features like attention mechanisms and knowledge transfer.   \n297 Additionally, HSP can benefit from existing Model-Based RL approaches as it naturally learns a   \n298 model of the world. In deterministic environments, a capable agent should achieve infinite horizon   \n299 control for tasks like walking and hopping from a single state. This is an important research direction   \n300 that is currently underexplored, as many environments are partially observable or have some degree   \n301 of stochasticity. Current approaches rely on external information at every state, which increases   \n302 energy consumption and vulnerability to adversarial or missing inputs. Truly autonomous agents will   \n303 need to impl ement multiple policies simultaneously, and simple tasks like walking can be performed   \n304 without input states if learned properly. Our future work will focus on extending the action sequence   \n305 horizon until deterministic tasks can be performed using a single state and implementing a mechanism   \n306 to dynamically pick the action sequence horizon based on context and predictability of the state.   \n307 Serotonin is an important neuromodulator that has been demonstrated to signal the availability of   \n308 time and resources in the brain to enable the decision on the planning horizon and the use of compute   \n309 (65). In the future, we hope to introduce a mechanism to replicate the effect of serotonin in HSP. ", "page_idx": 8}, {"type": "text", "text": "310 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "311 [1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.   \n312 [2] W. Schultz, P. Dayan, and P. R. Montague, \u201cA neural substrate of prediction and reward,\u201d   \n313 Science, vol. 275, pp. 1593\u20131599, 1997.   \n314 [3] W. Schultz, \u201cNeuronal reward and decision signals: From theories to data,\u201d Physiological   \n315 Reviews, vol. 95, pp. 853\u2013951, 7 2015.   \n316 [4] J. Y. Cohen, S. Haesler, L. Vong, B. B. Lowell, and N. Uchida, \u201cNeuron-type-specific signals   \n317 for reward and punishment in the ventral tegmental area,\u201d Nature 2012 482:7383, vol. 482,   \n318 pp. 85\u201388, 1 2012.   \n319 [5] OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. D\u02dbebiak, C. Dennison, D. Farhi,   \n320 Q. Fischer, S. Hashme, C. Hesse, R. J\u00f3zefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov,   \n321 H. P. d. O. Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang,   \n322 F. Wolski, and S. Zhang, \u201cDota 2 with large scale deep reinforcement learning,\u201d 12 2019.   \n323 [6] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock  \n324 hart, D. Hassabis, T. Graepel, T. Lillicrap, and D. Silver, \u201cMastering atari, go, chess and shogi   \n325 by planning with a learned model,\u201d Nature 2020 588:7839, vol. 588, pp. 604\u2013609, 12 2020.   \n326 [7] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. M\u00fcller, V. Koltun, and D. Scaramuzza,   \n327 \u201cChampion-level drone racing using deep reinforcement learning,\u201d Nature 2023 620:7976,   \n328 vol. 620, pp. 982\u2013987, 8 2023.   \n329 [8] P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian, T. J. Walsh, R. Capo  \n330 bianco, A. Devlic, F. Eckert, F. Fuchs, L. Gilpin, P. Khandelwal, V. Kompella, H. C. Lin,   \n331 P. MacAlpine, D. Oller, T. Seno, C. Sherstan, M. D. Thomure, H. Aghabozorgi, L. Barrett,   \n332 R. Douglas, D. Whitehead, P. D\u00fcrr, P. Stone, M. Spranger, and H. Kitano, \u201cOutracing cham  \n333 pion gran turismo drivers with deep reinforcement learning,\u201d Nature 2022 602:7896, vol. 602,   \n334 pp. 223\u2013228, 2 2022.   \n335 [9] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi,   \n336 R. Powell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre,   \n337 T. Cai, J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard,   \n338 D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring,   \n339 D. Yogatama, D. W\u00fcnsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu,   \n340 D. Hassabis, C. Apps, and D. Silver, \u201cGrandmaster level in starcraft ii using multi-agent   \n341 reinforcement learning,\u201d Nature 2019 575:7782, vol. 575, pp. 350\u2013354, 10 2019.   \n342 [10] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,   \n343 M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou,   \n344 H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, \u201cHuman-level control through   \n345 deep reinforcement learning,\u201d Nature 2015 518:7540, vol. 518, pp. 529\u2013533, 2 2015.   \n346 [11] E. Todorov, T. Erez, and Y. Tassa, \u201cMujoco: A physics engine for model-based control,\u201d in   \n347 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033,   \n348 IEEE, 2012.   \n349 [12] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d. Cola, T. Deleu, M. Goul\u00e3o,   \n350 A. Kallinteris, A. KG, M. Krimmel, R. Perez-Vicente, A. Pierr\u00e9, S. Schulhoff, J. J. Tai, A. T. J.   \n351 Shen, and O. G. Younis, \u201cGymnasium,\u201d Mar. 2023.   \n352 [13] A. Jain, R. Bansal, A. Kumar, and K. Singh, \u201cA comparative study of visual and auditory   \n353 reaction times on the basis of gender and physical activity levels of medical first year students,\u201d   \n354 International journal of applied and basic medical research, vol. 5, no. 2, pp. 124\u2013127, 2015.   \n355 [14] R. Limpert, Brake design and safety. SAE international, 2011.   \n356 [15] B. Katz, J. D. Carlo, and S. Kim, \u201cMini cheetah: A platform for pushing the limits of dynamic   \n357 quadruped control,\u201d Proceedings - IEEE International Conference on Robotics and Automation,   \n358 vol. 2019-May, pp. 6295\u20136301, 5 2019.   \n359 [16] G. B. Margolis, G. Yang, K. Paigwar, T. Chen, and P. Agrawal, \u201cRapid locomotion via re  \n360 inforcement learning,\u201d International Journal of Robotics Research, vol. 43, pp. 572\u2013587, 4   \n361 2024.   \n362 [17] Q. Li, G. Dong, R. Qin, J. Chen, K. Xu, and X. Ding, \u201cQuadruped reinforcement learning   \n363 without explicit state estimation,\u201d in 2022 IEEE International Conference on Robotics and   \n364 Biomimetics (ROBIO), pp. 1989\u20131994, IEEE, 2022.   \n365 [18] T. Haarnoja, B. Moran, G. Lever, S. H. Huang, D. Tirumala, J. Humplik, M. Wulfmeier,   \n366 S. Tunyasuvunakool, N. Y. Siegel, R. Hafner, et al., \u201cLearning agile soccer skills for a bipedal   \n367 robot with deep reinforcement learning,\u201d arXiv preprint arXiv:2304.13653, 2023.   \n368 [19] T. Wiestler and J. Diedrichsen, \u201cSkill learning strengthens cortical representations of motor   \n369 sequences,\u201d Elife, vol. 2, p. e00801, 2013.   \n370 [20] N. F. Wymbs and S. T. Grafton, \u201cThe human motor system supports sequence-specific rep  \n371 resentations over multiple training-dependent timescales,\u201d Cerebral cortex, vol. 25, no. 11,   \n372 pp. 4213\u20134225, 2015.   \n373 [21] N. Favila, K. Gurney, and P. G. Overton, \u201cRole of the basal ganglia in innate and learned   \n374 behavioural sequences,\u201d Reviews in the Neurosciences, vol. 35, no. 1, pp. 35\u201355, 2024.   \n375 [22] X. Jin, F. Tecuapetla, and R. M. Costa, \u201cBasal ganglia subcircuits distinctively encode the   \n376 parsing and concatenation of action sequences,\u201d Nature neuroscience, vol. 17, no. 3, pp. 423\u2013   \n377 430, 2014.   \n378 [23] X. Jin and R. M. Costa, \u201cShaping action sequences in basal ganglia circuits,\u201d Current opinion   \n379 in neurobiology, vol. 33, pp. 188\u2013196, 2015.   \n380 [24] G. S. Berns and T. J. Sejnowski, \u201cHow the basal ganglia make decisions,\u201d in Neurobiology of   \n381 decision-making, pp. 101\u2013113, Springer, 1996.   \n382 [25] G. S. Berns and T. J. Sejnowski, \u201cA computational model of how the basal ganglia produce   \n383 sequences,\u201d Journal of cognitive neuroscience, vol. 10, no. 1, pp. 108\u2013121, 1998.   \n384 [26] E. Garr, \u201cContributions of the basal ganglia to action sequence learning and performance,\u201d   \n385 Neuroscience & Biobehavioral Reviews, vol. 107, pp. 279\u2013295, 2019.   \n386 [27] A. J. Doupe, D. J. Perkel, A. Reiner, and E. A. Stern, \u201cBirdbrains could teach basal ganglia   \n387 research a new song,\u201d Trends in neurosciences, vol. 28, no. 7, pp. 353\u2013363, 2005.   \n388 [28] X. Jin and R. M. Costa, \u201cStart/stop signals emerge in nigrostriatal circuits during sequence   \n389 learning,\u201d Nature, vol. 466, no. 7305, pp. 457\u2013462, 2010.   \n390 [29] M. Matamales, Z. Skrbis, M. R. Bailey, P. D. Balsam, B. W. Balleine, J. G\u00f6tz, and J. Bertran  \n391 Gonzalez, \u201cA corticostriatal deficit promotes temporal distortion of automatic action in ageing,\u201d   \n392 ELife, vol. 6, p. e29908, 2017.   \n393 [30] J. G. Phillips, E. Chiu, J. L. Bradshaw, and R. Iansek, \u201cImpaired movement sequencing in   \n394 patients with huntington\u2019s disease: a kinematic analysis,\u201d Neuropsychologia, vol. 33, no. 3,   \n395 pp. 365\u2013369, 1995.   \n396 [31] L. Boyd, J. Edwards, C. Siengsukon, E. Vidoni, B. Wessel, and M. Linsdell, \u201cMotor sequence   \n397 chunking is impaired by basal ganglia stroke,\u201d Neurobiology of learning and memory, vol. 92,   \n398 no. 1, pp. 35\u201344, 2009.   \n399 [32] C. F. Geissler, C. Frings, and B. Moeller, \u201cIlluminating the prefrontal neural correlates of   \n400 action sequence disassembling in response\u2013response binding,\u201d Scientific Reports, vol. 11, no. 1,   \n401 p. 22856, 2021.   \n402 [33] M. A. Immink, M. Pointon, D. L. Wright, and F. E. Marino, \u201cPrefrontal cortex activation during   \n403 motor sequence learning under interleaved and repetitive practice: a two-channel near-infrared   \n404 spectroscopy study,\u201d Frontiers in Human Neuroscience, vol. 15, p. 644968, 2021.   \n405 [34] D. Shahnazian, M. Senoussi, R. M. Krebs, T. Verguts, and C. B. Holroyd, \u201cNeural representa  \n406 tions of task context and temporal order during action sequence execution,\u201d Topics in Cognitive   \n407 Science, vol. 14, no. 2, pp. 223\u2013240, 2022.   \n408 [35] M. C. Zielinski, W. Tang, and S. P. Jadhav, \u201cThe role of replay and theta sequences in mediating   \n409 hippocampal-prefrontal interactions for memory and cognition,\u201d Hippocampus, vol. 30, no. 1,   \n410 pp. 60\u201372, 2020.   \n411 [36] P. Malerba, K. Tsimring, and M. Bazhenov, \u201cLearning-induced sequence reactivation during   \n412 sharp-wave ripples: a computational study,\u201d in Advances in the Mathematical Sciences: AWM   \n413 Research Symposium, Los Angeles, CA, April 2017, pp. 173\u2013204, Springer, 2018.   \n414 [37] D. B. Rubin, T. Hosman, J. N. Kelemen, A. Kapitonava, F. R. Willett, B. F. Coughlin, E. Halgren,   \n415 E. Y. Kimchi, Z. M. Williams, J. D. Simeral, et al., \u201cLearned motor patterns are replayed in   \n416 human motor cortex during sleep,\u201d Journal of Neuroscience, vol. 42, no. 25, pp. 5007\u20135020,   \n417 2022.   \n418 [38] T. M. Moerland, J. Broekens, A. Plaat, C. M. Jonker, et al., \u201cModel-based reinforcement   \n419 learning: A survey,\u201d Foundations and Trends\u00ae in Machine Learning, vol. 16, no. 1, pp. 1\u2013118,   \n420 2023.   \n421 [39] D. Yarats, A. Zhang, I. Kostrikov, B. Amos, J. Pineau, and R. Fergus, \u201cImproving sample   \n422 efficiency in model-free reinforcement learning from images,\u201d in Proceedings of the AAAI   \n423 Conference on Artificial Intelligence, vol. 35, pp. 10674\u201310681, 2021.   \n424 [40] M. Janner, J. Fu, M. Zhang, and S. Levine, \u201cWhen to trust your model: Model-based policy   \n425 optimization,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n426 [41] J. Wang, W. Li, H. Jiang, G. Zhu, S. Li, and C. Zhang, \u201cOffline reinforcement learning with   \n427 reverse model-based imagination,\u201d Advances in Neural Information Processing Systems, vol. 34,   \n428 pp. 29420\u201329432, 2021.   \n429 [42] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, \u201cCuriosity-driven exploration by self  \n430 supervised prediction,\u201d in International conference on machine learning, pp. 2778\u20132787,   \n431 PMLR, 2017.   \n432 [43] B. C. Stadie, S. Levine, and P. Abbeel, \u201cIncentivizing exploration in reinforcement learning   \n433 with deep predictive models,\u201d arXiv preprint arXiv:1507.00814, 2015.   \n434 [44] N. Savinov, A. Raichuk, R. Marinier, D. Vincent, M. Pollefeys, T. Lillicrap, and S. Gelly,   \n435 \u201cEpisodic curiosity through reachability,\u201d arXiv preprint arXiv:1810.02274, 2018.   \n436 [45] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,   \n437 M. Lai, A. Bolton, et al., \u201cMastering the game of go without human knowledge,\u201d nature,   \n438 vol. 550, no. 7676, pp. 354\u2013359, 2017.   \n439 [46] S. Levine and V. Koltun, \u201cGuided policy search,\u201d in International conference on machine   \n440 learning, pp. 1\u20139, PMLR, 2013.   \n441 [47] A. Zhang, H. Satija, and J. Pineau, \u201cDecoupling dynamics and reward for transfer learning,\u201d   \n442 arXiv preprint arXiv:1804.10689, 2018.   \n443 [48] R. Sasso, M. Sabatelli, and M. A. Wiering, \u201cMulti-source transfer learning for deep model-based   \n444 reinforcement learning,\u201d arXiv preprint arXiv:2205.14410, 2022.   \n445 [49] A. Fickinger, H. Hu, B. Amos, S. Russell, and N. Brown, \u201cScalable online planning via   \n446 reinforcement learning fine-tuning,\u201d Advances in Neural Information Processing Systems,   \n447 vol. 34, pp. 16951\u201316963, 2021.   \n448 [50] A. McGovern, R. S. Sutton, and A. H. Fagg, \u201cRoles of macro-actions in accelerating reinforce  \n449 ment learning,\u201d 1997.   \n450 [51] Y.-H. Chang, K.-Y. Chang, H. Kuo, and C.-Y. Lee, \u201cReusability and transferability of macro   \n451 actions for reinforcement learning,\u201d ACM Transactions on Evolutionary Learning and Opti  \n452 mization, vol. 2, no. 1, pp. 1\u201316, 2022.   \n453 [52] H. Kim, M. Yamada, K. Miyoshi, T. Iwata, and H. Yamakawa, \u201cReinforcement learning in   \n454 latent action sequence space,\u201d in 2020 IEEE/RSJ International Conference on Intelligent Robots   \n455 and Systems (IROS), pp. 5497\u20135503, IEEE, 2020.   \n456 [53] S. Kalyanakrishnan, S. Aravindan, V. Bagdawat, V. Bhatt, H. Goka, A. Gupta, K. Kr  \n457 ishna, and V. Piratla, \u201cAn analysis of frame-skipping in reinforcement learning,\u201d ArXiv,   \n458 vol. abs/2102.03718, 2021.   \n459 [54] A. Srinivas, S. Sharma, and B. Ravindran, \u201cDynamic action repetition for deep reinforcement   \n460 learning,\u201d in AAAI, 2017.   \n461 [55] A. Biedenkapp, R. Rajan, F. Hutter, and M. Lindauer, \u201cTemporl: Learning when to act,\u201d in   \n462 International Conference on Machine Learning, pp. 914\u2013924, PMLR, 2021.   \n463 [56] S. Sharma, A. Srinivas, and B. Ravindran, \u201cLearning to repeat: Fine grained action repetition   \n464 for deep reinforcement learning,\u201d ArXiv, vol. abs/1702.06054, 2017.   \n465 [57] H. Yu, W. Xu, and H. Zhang, \u201cTaac: Temporally abstract actor-critic for continuous control,\u201d   \n466 Advances in Neural Information Processing Systems, vol. 34, pp. 29021\u201329033, 2021.   \n467 [58] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu,   \n468 A. Gupta, P. Abbeel, et al., \u201cSoft actor-critic algorithms and applications,\u201d arXiv preprint   \n469 arXiv:1812.05905, 2018.   \n470 [59] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra,   \n471 \u201cContinuous control with deep reinforcement learning,\u201d arXiv preprint arXiv:1509.02971, 2015.   \n472 [60] S. Fujimoto, H. Hoof, and D. Meger, \u201cAddressing function approximation error in actor-critic   \n473 methods,\u201d in International conference on machine learning, pp. 1587\u20131596, PMLR, 2018.   \n474 [61] K. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and   \n475 Y. Bengio, \u201cLearning phrase representations using rnn encoder-decoder for statistical machine   \n476 translation,\u201d arXiv preprint arXiv:1406.1078, 2014.   \n477 [62] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba,   \n478 \u201cOpenai gym,\u201d 2016.   \n479 [63] G. M. Van de Ven, H. T. Siegelmann, and A. S. Tolias, \u201cBrain-inspired replay for continual   \n480 learning with artificial neural networks,\u201d Nature communications, vol. 11, no. 1, p. 4069, 2020.   \n481 [64] Y. Zhao, W. Zhao, R. Boney, J. Kannala, and J. Pajarinen, \u201cSimplified temporal consistency   \n482 reinforcement learning,\u201d in International Conference on Machine Learning, pp. 42227\u201342246,   \n483 PMLR, 2023.   \n484 [65] K. Doya, K. W. Miyazaki, and K. Miyazaki, \u201cSerotonergic modulation of cognitive computa  \n485 tions,\u201d Current Opinion in Behavioral Sciences, vol. 38, pp. 116\u2013123, 2021.   \n486 [66] D. Yarats and I. Kostrikov, \u201cSoft actor-critic (sac) implementation in pytorch.\u201d https://   \n487 github.com/denisyarats/pytorch_sac, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "488 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "489 1. Claims   \n490 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n491 paper\u2019s contributions and scope?   \n492 Answer: [Yes]   \n493 Justification: The abstract and introduction accurately reflect the paper\u2019s contributions and   \n494 scope.   \n495 Guidelines:   \n496 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n497 made in the paper.   \n498 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n499 contributions made in the paper and important assumptions and limitations. A No or   \n500 NA answer to this question will not be perceived well by the reviewers.   \n501 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n502 much the results can be expected to generalize to other settings.   \n503 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n504 are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "09 Guidelines:   \n10 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n11 the paper has limitations, but those are not discussed in the paper.   \n12 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n13 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n14 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n15 model well-specification, asymptotic approximations only holding locally). The authors   \n16 should reflect on how these assumptions might be violated in practice and what the   \n17 implications would be.   \n18 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n19 only tested on a few datasets or with a few runs. In general, empirical results often   \n20 depend on implicit assumptions, which should be articulated.   \n21 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n22 For example, a facial recognition algorithm may perform poorly when image resolution   \n23 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n24 used reliably to provide closed captions for online lectures because it fails to handle   \n25 technical jargon.   \n26 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n27 and how they scale with dataset size.   \n28 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n29 address problems of privacy and fairness.   \n30 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n31 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n32 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n33 judgment and recognize that individual actions in favor of transparency play an impor  \n34 tant role in developing norms that preserve the integrity of the community. Reviewers   \n35 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 13}, {"type": "text", "text": "536 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "537 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n538 a complete (and correct) proof? ", "page_idx": 13}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 13}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "552 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We provide a complete algorithm and list of hyperparameters in the appendix.   \nAdditionally we also released the code and trained models. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "595 Answer: [Yes]   \n596 Justification: We include code to reproduce the results.   \n597 Guidelines:   \n598 \u2022 The answer NA means that paper does not include experiments requiring code.   \n599 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n600 public/guides/CodeSubmissionPolicy) for more details.   \n601 \u2022 While we encourage the release of code and data, we understand that this might not be   \n602 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n603 including code, unless this is central to the contribution (e.g., for a new open-source   \n604 benchmark).   \n605 \u2022 The instructions should contain the exact command and environment needed to run to   \n606 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n607 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n608 \u2022 The authors should provide instructions on data access and preparation, including how   \n609 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n610 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n611 proposed method and baselines. If only a subset of experiments are reproducible, they   \n612 should state which ones are omitted from the script and why.   \n613 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n614 versions (if applicable).   \n615 \u2022 Providing as much information as possible in supplemental material (appended to the   \n616 paper) is recommended, but including URLs to data and code is permitted.   \n617 6. Experimental Setting/Details   \n618 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n619 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n620 results?   \n621 Answer: [Yes]   \n622 Justification: We provide code, and list of all hyperparameters in appendix   \n623 Guidelines:   \n624 \u2022 The answer NA means that the paper does not include experiments.   \n625 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n626 that is necessary to appreciate the results and make sense of them.   \n627 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n628 material.   \n629 7. Experiment Statistical Significance   \n630 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n631 information about the statistical significance of the experiments?   \n632 Answer: [Yes]   \n633 Justification: Error bars are reported for all results presented.   \n634 Guidelines:   \n635 \u2022 The answer NA means that the paper does not include experiments.   \n636 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n637 dence intervals, or statistical significance tests, at least for the experiments that support   \n638 the main claims of the paper.   \n639 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n640 example, train/test split, initialization, random drawing of some parameter, or overall   \n641 run with given experimental conditions).   \n642 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n643 call to a library function, bootstrap, etc.)   \n644 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n645 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n646 of the mean.   \n647 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n648 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n649 of Normality of errors is not verified.   \n650 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n651 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n652 error rates).   \n653 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n654 they were calculated and reference the corresponding figures or tables in the text.   \n655 8. Experiments Compute Resources   \n656 Question: For each experiment, does the paper provide sufficient information on the com  \n657 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n658 the experiments?   \n659 Answer: [Yes]   \n660 Justification: A description of compute resources used is provided in the appendix   \n661 Guidelines:   \n662 \u2022 The answer NA means that the paper does not include experiments.   \n663 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n664 or cloud provider, including relevant memory and storage.   \n665 \u2022 The paper should provide the amount of compute required for each of the individual   \n666 experimental runs as well as estimate the total compute.   \n667 \u2022 The paper should disclose whether the full research project required more compute   \n668 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n669 didn\u2019t make it into the paper).   \n670 9. Code Of Ethics   \n671 Question: Does the research conducted in the paper conform, in every respect, with the   \n672 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n673 Answer: [Yes]   \n674 Justification: the research conducted in the paper conforms, in every respect, with the   \n675 NeurIPS Code of Ethics.   \n676 Guidelines:   \n677 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n678 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n679 deviation from the Code of Ethics.   \n680 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n681 eration due to laws or regulations in their jurisdiction).   \n682 10. Broader Impacts   \n683 Question: Does the paper discuss both potential positive societal impacts and negative   \n684 societal impacts of the work performed?   \n685 Answer: [Yes]   \n686 Justification: See section 6   \n687 Guidelines:   \n688 \u2022 The answer NA means that there is no societal impact of the work performed.   \n689 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n690 impact or why the paper does not address societal impact.   \n691 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n692 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n693 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n694 groups), privacy considerations, and security considerations.   \n695 \u2022 The conference expects that many papers will be foundational research and not tied   \n696 to particular applications, let alone deployments. However, if there is a direct path to   \n697 any negative applications, the authors should point it out. For example, it is legitimate   \n698 to point out that an improvement in the quality of generative models could be used to   \n699 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n700 that a generic algorithm for optimizing neural networks could enable people to train   \n701 models that generate Deepfakes faster.   \n702 \u2022 The authors should consider possible harms that could arise when the technology is   \n703 being used as intended and functioning correctly, harms that could arise when the   \n704 technology is being used as intended but gives incorrect results, and harms following   \n705 from (intentional or unintentional) misuse of the technology.   \n706 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n707 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n708 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n709 feedback over time, improving the efficiency and accessibility of ML).   \n710 11. Safeguards   \n711 Question: Does the paper describe safeguards that have been put in place for responsible   \n712 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n713 image generators, or scraped datasets)?   \n714 Answer: [NA]   \n715 Justification: the paper poses no such risks.   \n716 Guidelines:   \n717 \u2022 The answer NA means that the paper poses no such risks.   \n718 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n719 necessary safeguards to allow for controlled use of the model, for example by requiring   \n720 that users adhere to usage guidelines or restrictions to access the model or implementing   \n721 safety filters.   \n722 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n723 should describe how they avoided releasing unsafe images.   \n724 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n725 not require this, but we encourage authors to take this into account and make a best   \n726 faith effort.   \n727 12. Licenses for existing assets   \n728 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n729 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n730 properly respected?   \n731 Answer: [Yes]   \n732 Justification: We cite original papers for each algorithm and envrionment used   \n733 Guidelines:   \n734 \u2022 The answer NA means that the paper does not use existing assets.   \n735 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n736 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n737 URL.   \n738 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n739 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n740 service of that source should be provided.   \n741 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n742 package should be provided. For popular datasets, paperswithcode.com/datasets   \n743 has curated licenses for some datasets. Their licensing guide can help determine the   \n744 license of a dataset.   \n745 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n746 the derived asset (if it has changed) should be provided.   \n747 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n748 the asset\u2019s creators.   \n749 13. New Assets   \n750 Question: Are new assets introduced in the paper well documented and is the documentation   \n751 provided alongside the assets?   \n752 Answer: [Yes]   \n753 Justification: The documentation for the released code is provided   \n754 Guidelines:   \n755 \u2022 The answer NA means that the paper does not release new assets.   \n756 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n757 submissions via structured templates. This includes details about training, license,   \n758 limitations, etc.   \n759 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n760 asset is used.   \n761 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n762 create an anonymized URL or include an anonymized zip file.   \n763 14. Crowdsourcing and Research with Human Subjects   \n764 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n765 include the full text of instructions given to participants and screenshots, if applicable, as   \n766 well as details about compensation (if any)?   \n767 Answer: [NA]   \n768 Justification: paper does not involve crowdsourcing nor research with human subjects.   \n769 Guidelines:   \n770 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n771 human subjects.   \n772 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n773 tion of the paper involves human subjects, then as much detail as possible should be   \n774 included in the main paper.   \n775 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n776 or other labor should be paid at least the minimum wage in the country of the data   \n777 collector.   \n778 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n779 Subjects   \n780 Question: Does the paper describe potential risks incurred by study participants, whether   \n781 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n782 approvals (or an equivalent approval/review based on the requirements of your country or   \n783 institution) were obtained?   \n784 Answer: [NA]   \n785 Justification: the paper does not involve crowdsourcing nor research with human subjects.   \n786 Guidelines:   \n787 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n788 human subjects.   \n789 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n790 may be required for any human subjects research. If you obtained IRB approval, you   \n791 should clearly state this in the paper.   \n792 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n793 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n794 guidelines for their institution.   \n795 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n796 applicable), such as the institution conducting the review. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "IdEaeCbhUW/tmp/2d28f909a337e427f6e4b2710be61f69c9ff9fdb7f28270d253bc3966710a289.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "800 Hyperparameters ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "801 The table below lists the hyperparameters that are common between every environment used for all   \n802 our experiments for the SAC and HSP algorithms: ", "page_idx": 19}, {"type": "text", "text": "803 A.2 Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "804 Due to its added complexity during training, HSP requires longer wall clock time for training when   \n805 compared to SAC. We performed a minimal hyperparameter search over the actor update frequency   \n806 parameter on the Hopper environment (tested values: 1, 2, 4, 8, 16). All the other hyperparamters   \n807 were picked to be equal to the SAC implementation. We also did not perform a hyerparameter search   \n808 over the size of GRU for the actor. It was picked to have the same size as the hidden layers of the feed   \n809 forward network of the actor in SAC. The neural network for the model was also picked to have the   \n810 same architecture as the actor from SAC, thus it has two hidden layers with 256 neurons. Similarly   \n811 the encoder for the latent HSP implementation was also picked to have the same architecture. For the   \n812 latent HSP implementation we also add an additional replay buffer to store transitions of length 5,   \n813 to implement the temporal consistency training for the model. This was done for simplicity of the   \n814 implementation, and it can be removed since it is redundant to save memory.   \n815 All experiments were performed on a GPU cluster the Nvidia 1080ti GPUs. Each run was performed   \n816 using a single GPU, utilizing 8 CPU cores of Intel(R) Xeon(R) Silver 4116 (24 core) and 16GB of   \n817 memory.   \n818 We utilize the pytorch implementation of SAC (https://github.com/denisyarats/pytorch_   \n819 sac) (66). ", "page_idx": 19}, {"type": "table", "img_path": "IdEaeCbhUW/tmp/d2d554644610cc4da78d5381c04d10fbc6a787ba1d67bb5ccc68ea206f5ff28e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "IdEaeCbhUW/tmp/714dc86c1f8dbfc10aa1ea9f43e2db769086de5c96d6e5001e61a34dd1179a15.jpg", "table_caption": ["Table 1: List of Common hyperparameters "], "table_footnote": ["Table 2: List of environment-specific hyperparameters "], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "820 A.3 Latent State Space Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "821 Following the TCRL implementation, we use two encoders: an online encoder $\\mathbf{e}_{\\theta}$ and a target encoder   \n822 $\\mathbf{e}_{\\theta^{-}}$ , which is the exponential moving average of the online encoder: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathtt{E n c o d e r:}e_{t}=\\mathbf{e}_{\\theta}\\big(s_{t}\\big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "823 Thus, the model predicts the next state in the latent space. Additionally, we introduce multi-step   \n824 model prediction for temporal consistency. Following the TCRL work, we use a cosine loss for model   \n825 prediction. The model itself predicts only a single step forward, but we enforce temporal consistency   \n826 by rolling out the model $H$ -steps forward to predict $\\tilde{e}_{t+1:t+1+H}$ .   \n827 Specifically, for an $H$ -step trajectory $\\tau=(z_{t},a_{t},z_{t+1})_{t:t+H}$ drawn from the replay buffer $\\mathcal{D}$ , we   \n828 use the online encoder to get the first latent state $e_{t}={\\bf e}_{\\theta}(o_{t})$ . Then conditioning on the sequence of   \n829 actions $a_{t:t+H}$ , the model is applied iteratively to predict the latent states $\\widetilde{e}_{t+1}=\\mathbf{m}_{\\phi}(\\widetilde{e}_{t},a_{t})$ . Finally,   \n830 we use the target encoder to calculate the target latent states $\\hat{e}_{t+1:t+H+1}=\\mathbf{e}_{\\theta^{-}}\\big(o_{t+1:t+1+H}\\big)$ . The   \n831 Loss function is defined as: ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\theta,\\phi}=\\mathbb{E}_{\\tau\\sim\\mathcal{D}}\\left[\\sum_{h=0}^{H}-\\gamma^{h}\\left(\\frac{\\tilde{e}_{t+h}}{\\lvert\\lvert\\tilde{e}_{t+h}\\rvert\\rvert_{2}}\\right)^{T}\\left(\\frac{\\hat{e}_{t+h}}{\\lvert\\lvert\\hat{e}_{t+h}\\rvert\\rvert_{2}}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "832 We set $H=5$ for our experiments. Both the encoder and the model are feed-forward neural networks   \n833 with two hidden layers. ", "page_idx": 20}, {"type": "image", "img_path": "IdEaeCbhUW/tmp/813c95541bb2326fb34a19b0700ee58e3309ade1aacc6a13851b15266bf33955.jpg", "img_caption": ["Here, we provide complete learning curves for the latent space HSP. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 6: Learning curves of Latent HSP- ${\\mathbf{\\nabla}}n$ and Soft-Actor Critic (SAC) over continuous control tasks. ", "page_idx": 21}]