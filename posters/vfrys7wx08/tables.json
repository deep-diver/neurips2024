[{"figure_path": "VFRyS7Wx08/tables/tables_9_1.jpg", "caption": "Table 1: Offline RL results obtained by combining PAGAR with RECOIL averaged over 4 seeds.", "description": "This table presents the average performance results of offline reinforcement learning experiments using two different methods: RECOIL and PAGAR-RECOIL.  The results are shown for four different continuous control tasks from the MuJoCo environment.  Each result represents the average performance across four independent experimental runs (seeds), providing a measure of the methods' reliability and stability. PAGAR-RECOIL consistently shows comparable or better performance compared to RECOIL across all tasks.", "section": "7.2 Continuous Control Tasks"}, {"figure_path": "VFRyS7Wx08/tables/tables_33_1.jpg", "caption": "Table 2: Hyperparameters used in the training processes", "description": "This table lists the hyperparameters used in the training processes for both continuous control domain and partially observable domain.  For each domain, it specifies the policy training batch size, discount factor, GAE parameter, PPO clipping parameter, lambda_0 (lambda zero), sigma (\u03c3), ic (bottleneck constraint), beta (\u03b2), mu (\u00b5), and delta (\u03b4).  Note that some hyperparameters have different values depending on the specific task (e.g., VAIL and GAIL).", "section": "C.1 Experimental Details"}]