[{"figure_path": "VFRyS7Wx08/figures/figures_2_1.jpg", "caption": "Figure 1: (a) The two bars respectively represent the policy utility spaces of a task-aligned reward function r+ and a task-misaligned reward function r\u00af. The white color indicates the utilities of acceptable policies, and the blue color indicates the unacceptable ones. Within the utility space of r\u207a, the utilities of all acceptable policies are higher (> U+) than those of the unacceptable ones, and the policies with utilities higher than Ur+ have higher orders than those of utilities lower than Ur+. Within the utility space of r\u00af, acceptable and unacceptable policies' utilities are mixed together, leading to a low U,- and an even lower \u016a- . (b) IRL-based IL relies solely on IRL's optimal reward function r* which can be task-misaligned and lead to an unacceptable policy \u03c0\u03c1* \u2208 \u03a0\nacc while PAGAR-based IL learns an acceptable policy \u03c0* \u2208 \u03a0acc from a set RE,8 of reward functions.", "description": "This figure illustrates the core concept of the paper, comparing conventional IRL-based imitation learning with the proposed PAGAR method. (a) shows how task-aligned reward functions (r+) clearly separate acceptable policies from unacceptable ones in their utility space, while misaligned ones (r-) do not. (b) highlights PAGAR's advantage by learning from multiple candidate reward functions to arrive at an acceptable policy.", "section": "4 Task-Reward Alignment"}, {"figure_path": "VFRyS7Wx08/figures/figures_2_2.jpg", "caption": "Figure 1: (a) The two bars respectively represent the policy utility spaces of a task-aligned reward function r+ and a task-misaligned reward function r\u00af. The white color indicates the utilities of acceptable policies, and the blue color indicates the unacceptable ones. Within the utility space of r\u207a, the utilities of all acceptable policies are higher (> U+) than those of the unacceptable ones, and the policies with utilities higher than Ur+ have higher orders than those of utilities lower than Ur+. Within the utility space of r\u00af, acceptable and unacceptable policies' utilities are mixed together, leading to a low U,- and an even lower \u016a- . (b) IRL-based IL relies solely on IRL's optimal reward function r* which can be task-misaligned and lead to an unacceptable policy \u03c0\u03c1* \u2208 \u03a0\nacc while PAGAR-based IL learns an acceptable policy \u03c0* \u2208 \u03a0acc from a set RE,8 of reward functions.", "description": "This figure illustrates the difference between conventional IRL-based imitation learning and the proposed PAGAR-based approach.  Panel (a) shows the policy utility spaces for task-aligned (r+) and misaligned (r-) reward functions.  Task alignment means acceptable policies have higher utilities than unacceptable ones, and higher utility implies higher task performance.  Panel (b) highlights that conventional IRL can yield unacceptable policies (\u03c0\u03c1*) due to misaligned reward functions (r*), while PAGAR learns an acceptable policy (\u03c0*) by leveraging a set (RE,\u03b4) of candidate reward functions.", "section": "4 Task-Reward Alignment"}, {"figure_path": "VFRyS7Wx08/figures/figures_8_1.jpg", "caption": "Figure 2: Comparing Algorithm 1 with baselines in partial observable navigation tasks. The suffix after each 'PAGAR-' indicates which IRL technique is used in Algorithm 1. The y axis indicates the average return per episode. The x axis indicates the number of time steps.", "description": "This figure compares the performance of the proposed PAGAR-based imitation learning algorithm (with GAIL and VAIL) against standard baselines (GAIL, VAIL, and IQ-Learn) on two partially observable navigation tasks (DoorKey-6x6 and SimpleCrossingS9N1) from the MiniGrid environment.  The results show the average return per episode over the number of time steps (frames) for both scenarios with 10 and 1 expert demonstrations.  The plots illustrate the superior performance of PAGAR-based methods, particularly in scenarios with limited data.", "section": "7 Experiments"}, {"figure_path": "VFRyS7Wx08/figures/figures_8_2.jpg", "caption": "Figure 3: PAGAR-GAIL in different reward spaces", "description": "The figure compares the performance of PAGAR-GAIL and GAIL using different reward function hypothesis sets (Sigmoid and Categorical).  It shows that PAGAR-GAIL outperforms GAIL in both cases using fewer samples, highlighting its robustness and efficiency.", "section": "7.1 Discrete Navigation Tasks"}, {"figure_path": "VFRyS7Wx08/figures/figures_9_1.jpg", "caption": "Figure 2: Comparing Algorithm 1 with baselines in partial observable navigation tasks. The suffix after each 'PAGAR-' indicates which IRL technique is used in Algorithm 1. The y axis indicates the average return per episode. The x axis indicates the number of time steps.", "description": "This figure compares the performance of the proposed PAGAR-based imitation learning algorithm (with two variants, PAGAR-GAIL and PAGAR-VAIL) against several baselines (GAIL, VAIL, and IQ-Learn) on two partially observable navigation tasks (DoorKey-6x6 and SimpleCrossingS9N1) and a transfer learning setting.  The results are shown for different numbers of expert demonstrations (1 and 10). The plots show the average return per episode over the number of timesteps (frames).  The results highlight PAGAR's improved sample efficiency and ability to generalize to unseen environments.", "section": "7 Experiments"}, {"figure_path": "VFRyS7Wx08/figures/figures_23_1.jpg", "caption": "Figure 1: (a) The two bars respectively represent the policy utility spaces of a task-aligned reward function r+ and a task-misaligned reward function r\u00af. The white color indicates the utilities of acceptable policies, and the blue color indicates the unacceptable ones. Within the utility space of r\u207a, the utilities of all acceptable policies are higher (> U+) than those of the unacceptable ones, and the policies with utilities higher than Ur+ have higher orders than those of utilities lower than Ur+. Within the utility space of r\u00af, acceptable and unacceptable policies' utilities are mixed together, leading to a low U,- and an even lower \u016a- . (b) IRL-based IL relies solely on IRL's optimal reward function r* which can be task-misaligned and lead to an unacceptable policy \u03c0\u03c1* \u2208 \u03a0\\Hacc while PAGAR-based IL learns an acceptable policy \u03c0* \u2208 \u03a0acc from a set RE,8 of reward functions.", "description": "This figure illustrates the core idea of the paper: task alignment vs. data alignment in inverse reinforcement learning.  (a) shows how a task-aligned reward function (r+) clearly separates acceptable and unacceptable policies based on their utility, while a task-misaligned reward function (r-) does not. (b) contrasts conventional IRL-based imitation learning (which focuses on data alignment and might produce an unacceptable policy) with the proposed PAGAR framework (which prioritizes task alignment and aims to produce an acceptable policy using multiple reward functions).", "section": "4 Task-Reward Alignment"}, {"figure_path": "VFRyS7Wx08/figures/figures_33_1.jpg", "caption": "Figure 2: Comparing Algorithm 1 with baselines in partial observable navigation tasks. The suffix after each 'PAGAR-' indicates which IRL technique is used in Algorithm 1. The y axis indicates the average return per episode. The x axis indicates the number of time steps.", "description": "This figure compares the performance of the proposed PAGAR-based imitation learning algorithm (with two variants, PAGAR-GAIL and PAGAR-VAIL) against several baselines (GAIL, VAIL, and IQ-Learn) on two partial observable navigation tasks (DoorKey-6x6 and SimpleCrossingS9N1).  The results are shown separately for experiments with 1 and 10 expert demonstrations.  The plots show the average return per episode over the number of training timesteps. This helps illustrate the effectiveness of PAGAR, especially in scenarios with limited expert data.", "section": "7 Experiments"}, {"figure_path": "VFRyS7Wx08/figures/figures_34_1.jpg", "caption": "Figure 2: Comparing Algorithm 1 with baselines in partial observable navigation tasks. The suffix after each 'PAGAR-' indicates which IRL technique is used in Algorithm 1. The y axis indicates the average return per episode. The x axis indicates the number of time steps.", "description": "This figure compares the performance of the proposed PAGAR-based imitation learning algorithm (with two variants, PAGAR-GAIL and PAGAR-VAIL) against several baselines (GAIL, VAIL, and IQ-Learn) on two partially observable navigation tasks (DoorKey-6x6 and SimpleCrossingS9N1).  It showcases the algorithm's performance with both 10 and 1 expert demonstrations, highlighting its ability to learn effectively even with limited data.  The x-axis represents the number of timesteps (training frames), and the y-axis shows the average return (reward) achieved per episode. The results illustrate that PAGAR outperforms the baselines, particularly when fewer demonstrations are available.", "section": "7 Experiments"}, {"figure_path": "VFRyS7Wx08/figures/figures_34_2.jpg", "caption": "Figure 3: PAGAR-GAIL in different reward spaces", "description": "This figure compares the performance of PAGAR-GAIL and GAIL using different reward function hypothesis sets (Sigmoid vs. Categorical).  It shows that PAGAR-GAIL outperforms GAIL in both cases, even with fewer samples. The x-axis represents the number of frames (data points), and the y-axis shows the average return per episode.", "section": "7.1 Discrete Navigation Tasks"}, {"figure_path": "VFRyS7Wx08/figures/figures_34_3.jpg", "caption": "Figure 2: Comparing Algorithm 1 with baselines in partial observable navigation tasks. The suffix after each 'PAGAR-' indicates which IRL technique is used in Algorithm 1. The y axis indicates the average return per episode. The x axis indicates the number of time steps.", "description": "This figure compares the performance of the proposed PAGAR algorithm (with two variants using GAIL and VAIL) against standard baselines (GAIL, VAIL, and IQ-Learn) on two partially observable navigation tasks (DoorKey-6x6 and SimpleCrossingS9N1) with varying numbers of expert demonstrations.  The plots show the average return per episode over the number of time steps or frames, illustrating the learning curves for each algorithm. The results demonstrate PAGAR's improved sample efficiency and performance, especially when limited demonstrations are available.  Transfer learning performance on related tasks is also depicted.", "section": "7 Experiments"}]