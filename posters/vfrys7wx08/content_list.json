[{"type": "text", "text": "Rethinking Inverse Reinforcement Learning: from Data Alignment to Task Alignment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weichao Zhou Boston University Boston, MA 02215 zwc662@bu.edu ", "page_idx": 0}, {"type": "text", "text": "Wenchao Li Boston University Boston, MA 02215 wenchao@bu.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many imitation learning (IL) algorithms use inverse reinforcement learning (IRL) to infer a reward function that aligns with the demonstrations. However, the inferred reward function often fails to capture the underlying task objective. In this paper, we propose a novel framework for IRL-based IL that prioritizes task alignment over conventional data alignment. Our framework is a semi-supervised approach that leverages expert demonstrations as weak supervision signals to derive a set of candidate reward functions that align with the task rather than only with the data. It adopts an adversarial mechanism to train a policy with this set of reward functions to gain a collective validation of the policy\u2019s ability to accomplish the task. We provide theoretical insights into this framework\u2019s ability to mitigate task-reward misalignment and present a practical implementation. Our experimental results show that our framework outperforms conventional IL baselines in complex and transfer learning scenarios. The complete code are available at https://github.com/zwc662/PAGAR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Inverse reinforcement learning (IRL) $\\mathrm{Ng}$ and Russell [2000], Finn et al. [2017] has become a popular method for imitation learning $\\mathrm{(IL)}$ , allowing policies to be trained by learning reward functions from expert demonstrations Abbeel and $\\mathrm{Ng}$ [2004], Ho and Ermon [2016]. Despite its widespread use, IRL-based IL faces significant challenges that often stem from overemphasizing data alignment rather than task alignment. For instance, reward ambiguity, where multiple reward functions can be consistent with the expert demonstrations, makes it difficult to identify the correct reward function. This problem persists even when there are infinite data $\\mathrm{Ng}$ and Russell [2000], Cao et al. [2021], Skalse et al. [2022a,b]. Additionally, limited availability of demonstrations can further exacerbate this problem, as the data may not fully capture the nuances of the task. Misaligned reward functions can lead to policies that optimize the wrong objectives, resulting in poor performance and even reward hacking Hadfield-Menell et al. [2017], Amodei et al. [2016], Pan et al. [2022], a phenomenon where the policy exploits loopholes in the inferred reward function. These challenges highlight the limitation of exclusively pursuing data alignment in solving real-world tasks. ", "page_idx": 0}, {"type": "text", "text": "In light of these considerations, this paper advocates for a paradigm shift from a narrow focus on data alignment to a broader emphasis on task alignment. Grounded in a general formalism of task objectives, we propose identifying the task-aligned reward functions that more accurately reflect the underlying task objectives in their policy utility spaces. Expanding on this concept, we explore the intrinsic relationship between the task objective, reward, and expert demonstrations. This relationship leads us to a novel perspective where expert demonstrations can serve as weak supervision signals for identifying a set of candidate task-aligned reward functions. Under these reward functions, the expert achieves high -\u2014 but not necessarily optimal \u2014- performance. The rationale is that achieving high performance under a task-aligned reward function is often adequate for real-world applications. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Building on this premise, we leverage IRL to derive the set of candidate task-aligned reward functions and propose Protagonist Antagonist Guided Adversarial Reward (PAGAR), a semi-supervised framework designed to mitigate task-reward misalignment by training a policy with this candidate reward set. PAGAR adopts an adversarial training mechanism between a protagonist policy and an adversarial reward searcher, iteratively improving the policy learner to attain high performance across the candidate reward set. This method moves beyond relying on deriving a single reward function from data, enabling a collective validation of the policy\u2019s similarity to expert demonstrations in terms of effectiveness in accomplishing tasks. Experimental results show that our algorithm outperforms baselines on complex IL tasks with limited demonstrations and in challenging transfer environments. We summarize our contributions below. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Introduction of Task Alignment in IRL-based IL: We present a novel perspective that shifts the focus from data alignment to task alignment, addressing the root causes of reward misalignment in IRL-based IL. \u2022 Protagonist Antagonist Guided Adversarial Reward (PAGAR): We propose a new semi-supervised framework that leverages adversarial training to improve the robustness of the learned policy. \u2022 Practical Implementation: We present a practical implementation of PAGAR, including the adversarial reward searching mechanism and the iterative policy-improving process. Experimental results demonstrate superior performances in complex and transfer learning environments. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "IRL-based IL circumvents many challenges of traditional IL such as compounding error Ross and Bagnell [2010], Ross et al. [2011], Zhou et al. [2020] by learning a reward function to interpret the expert behaviors $\\mathrm{Ng}$ et al. [1999], $\\mathrm{Ng}$ and Russell [2000] and then learning a policy from the reward function via reinforcement learning (RL)Sutton and Barto [2018]. However, the learned reward function may not always align with the underlying task, leading to reward misspecification Pan et al. [2022], Skalse and Abate [2022], reward hacking Skalse et al. [2022b], and reward ambiguity $\\mathrm{Ng}$ and Russell [2000], Cao et al. [2021]. The efforts on alleviating reward ambiguity include Max-Entropy IRL Ziebart et al. [2008], Max-Margin IRL Abbeel and $\\mathrm{Ng}$ [2004], Ratliff et al. [2006], and Bayesian IRL Ramachandran and Amir [2007]. GAN-based methods Ho and Ermon [2016], Jeon et al. [2018], Finn et al. [2016], Peng et al. [2019], Fu et al. [2018] use neural networks to learn reward functions from limited demonstrations. However, these efforts that aim to address reward ambiguity fall short of mitigating the general impact of reward misalignment which can be caused by various reasons such as IRL making false assumptions about the relationship between expert policy and expert reward function Skalse et al. [2022a], Hong et al. [2023]. Other attempts to mitigate reward misalignment involve external information other than expert demonstrations Hejna and Sadigh [2023], Zhou and Li [2018, 2022a,b]. Our work adopts the generic setting of IRL-based IL without needing additional information. The idea of considering a reward set instead of focusing on a single reward function is supported by Metelli et al. [2021] and Lindner et al. [2022]. However, these works target reward ambiguity instead of reward misalignment. Our protagonist and antagonist setup is inspired by the concept of unsupervised environment design (UED) Dennis et al. [2020]. In this paper, we develop novel theories in the context of reward learning. ", "page_idx": 1}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Reinforcement Learning (RL) models the environment as a Markov Decision Process $\\mathcal{M}\\,=$ $\\langle\\mathbb{S},\\mathbb{A},\\mathcal{P},d_{0}\\rangle$ where $\\mathbb{S}$ is the state space, A is the action space, $\\mathcal{P}$ is the transition probability, $d_{0}$ is the initial state distribution. A policy $\\pi(a|s)$ determines the probability of an RL agent performing an action $a$ at state $s$ . By successively performing actions for $T$ steps from an initial state $s^{(0)}\\sim$ $d_{0}$ , a trajectory $\\tau\\,=\\,{s^{(\\bar{0})}a^{(0)}s^{(1)}a^{(1)}}\\,.\\,.\\,.\\,s^{(T)}$ is produced. A state-action based reward function is a mapping $r\\,:\\,\\mathbb{S}\\,\\times\\,\\mathbb{A}\\,\\rightarrow\\,\\mathbb{R}$ . The soft $Q\\cdot$ -value function of $\\pi$ is $\\mathcal{Q}_{\\pi}(s,a)\\;=\\;r(s,a)\\:+\\:\\gamma\\ .$ $\\bar{\\mathbb{E}}_{s^{\\prime}\\sim\\mathcal{P}(\\cdot|s,a)}^{-\\bar{\\mathbb{E}}}\\left[\\bar{\\mathcal{V}_{\\pi}}(s^{\\prime})\\right]$ where $\\gamma\\,\\in\\,(0,1]$ is a discount factor, $\\mathcal{V}_{\\pi}$ is the soft state-value function of $\\pi$ defined as $\\begin{array}{r}{\\mathcal{V}_{\\pi}(s):=\\underset{a\\sim\\pi(\\cdot\\vert s)}{\\mathbb{E}}\\left[\\mathcal{Q}_{\\pi}(s,a)\\right]+\\mathcal{H}(\\pi(\\cdot\\vert s))}\\end{array}$ , and $\\mathcal{H}(\\pi(\\cdot|s))$ is the entropy of $\\pi$ at state $s$ . ", "page_idx": 1}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/b47aa3e9032a9c8f55551baeca2f22d6e9e40440074bb1131f009a1b17f1b3d8.jpg", "img_caption": ["(a) Task-Reward Alignment "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/e19d1d923613abcfb6ace7fc0649bf7ff4d2fdc94a7716294f688cfe0fafb5bd.jpg", "img_caption": ["(b) PAGAR-Based IL "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: (a) The two bars respectively represent the policy utility spaces of a task-aligned reward function $r^{+}$ and a task-misaligned reward function $r^{-}$ . The white color indicates the utilities of acceptable policies, and the blue color indicates the unacceptable ones. Within the utility space of $r^{+}$ , the utilities of all acceptable policies are higher $\\left(\\geq U_{r^{+}}\\right)$ ) than those of the unacceptable ones, and the policies with utilities higher than $\\overline{{U}}_{r^{+}}$ have higher orders than those of utilities lower than $\\overline{{U}}_{r^{+}}$ . Within the utility space of $r^{-}$ , acceptable and unacceptable policies\u2019 utilities are mixed together, leading to a low $\\underline{{U}}_{r^{-}}$ and an even lower $\\overline{{U}}_{r^{-}}$ . (b) IRL-based IL relies solely on IRL\u2019s optimal reward function $r^{*}$ which can be task-misaligned and lead to an unacceptable policy $\\pi_{r^{*}}\\in\\Pi\\backslash\\Pi_{a c c}$ while PAGAR-based IL learns an acceptable policy $\\pi^{*}\\in\\Pi_{a c c}$ from a set $R_{E,\\delta}$ of reward functions. ", "page_idx": 2}, {"type": "text", "text": "The soft advantage of performing action $a$ at state $s$ and then following a policy $\\pi$ afterwards is $A_{\\pi}(s,a)\\,=\\,\\mathcal{Q}_{\\pi}(\\bar{s},a)\\,\\,\\bar{-\\,}\\mathcal{V}_{\\pi}(s)$ . The expected return of $\\pi$ under a reward function $r$ is given as $\\begin{array}{r}{U_{r}(\\pi)=\\underset{\\tau\\sim\\pi}{\\mathbb{E}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\cdot r(s^{(t)},a^{(t)})]}\\end{array}$ . With a slight abuse of notations, we denote the entropy of a policy as $\\begin{array}{r}{\\mathcal{H}(\\pi)\\,:=\\,\\underset{\\tau\\sim\\pi}{\\mathbb{E}}[\\sum_{t=0}^{\\infty}\\gamma^{t}\\cdot\\mathcal{H}(\\pi(\\cdot|s^{(t)}))]}\\end{array}$ . The standard RL learns an optimal policy by maximizing $U_{r}(\\pi)$ . The entropy regularized RL learns a soft-optimal policy by maximizing the objective function $\\mathcal{J}_{R L}(\\pi;r):=U_{r}(\\pi)+\\mathcal{H}(\\pi)$ . ", "page_idx": 2}, {"type": "text", "text": "Inverse Reinforcement Learning (IRL) assumes that a set $E=\\{\\tau_{1},\\dots,\\tau_{N}\\}$ of expert demonstrations are sampled from the roll-outs of the expert\u2019s policy $\\pi_{E}$ and aims to learn the expert reward function $r_{E}$ . IRL $\\mathrm{Ng}$ and Russell [2000] assumes that $\\pi_{E}$ is optimal under $r_{E}$ and learns $r_{E}$ by maximizing the margin $U_{r}(\\bar{E})-\\operatorname*{max}_{\\pi}U_{r}(\\pi)$ while Maximum Entropy IRL (MaxEnt IRL) Ziebart et al. [2008] maximizes an entropy regularized objective function $\\mathcal{I}_{I R L}(r)=U_{r}(E)-\\left(\\operatorname*{max}_{\\pi}U_{r}(\\pi)+\\mathcal{H}(\\pi)\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "Generative Adversarial Imitation Learning (GAIL) Ho and Ermon [2016] draws a connection between IRL and Generative Adversarial Nets (GANs) as shown in Eq.1, where a discriminator $D:\\mathbb{S}\\times\\mathbb{A}\\to[0,1]$ is trained by minimizing Eq.1 so that $D$ can accurately identify any $(s,a)$ generated by the agent. Meanwhile, an agent policy $\\pi$ is trained as a generator to maximize Eq.1 so that $D$ cannot discriminate $\\tau\\sim\\pi$ from $\\tau_{E}$ . Adversarial inverse reinforcement learning (AIRL) $\\mathrm{F}\\mathbf{u}$ et al. [2018] uses a neural-network reward function r to represent D(s, a) := exp(r(s\u03c0,(aa)|)s+)\u03c0(a|s), rewrites $\\mathcal{I}_{I R L}$ as minimizing Eq.1, and proves that the optimal reward satisfies $r^{*}\\equiv\\log\\pi_{E}\\equiv\\mathcal{A}_{\\pi_{E}}$ . By training $\\pi$ with $r^{*}$ until optimality, $\\pi$ will behave just like $\\pi_{E}$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{(s,a)\\sim\\pi}{\\mathbb{E}}\\left[\\log D(s,a))\\right]+\\underset{(s,a)\\sim\\pi_{E}}{\\mathbb{E}}\\left[\\log(1-D(s,a))\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "4 Task-Reward Alignment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we formalize the concept of task-reward misalignment in IRL-based IL. We start by defining a notion of task based on the framework from Abel et al. [2021]. ", "page_idx": 2}, {"type": "text", "text": "Definition 1 (Task). Given the policy hypothesis set $\\Pi$ , a task $(\\Pi,\\preceq_{t a s k},\\Pi_{a c c})$ is specified by a partial order $\\preceq_{t a s k}$ over $\\Pi$ and a non-empty set of acceptable policies $\\Pi_{a c c}\\subseteq\\Pi$ such that $\\forall\\pi_{1}\\in\\Pi_{a c c}$ and $\\forall\\pi_{2}\\notin\\Pi_{a c c}$ , $\\pi_{2}\\preceq_{t a s k}\\pi_{1}$ always hold. ", "page_idx": 2}, {"type": "text", "text": "Remark: The notions of policy acceptance and order allow the definition of task to accommodate a broad range of real-world tasks1 including the standard RL tasks (learning the optimal policy from a reward function $r$ ): given a reward function $r$ and a policy hypothesis set $\\Pi$ , the standard RL task can be written as a tuple $(\\Pi,\\preceq_{t a s k},\\Pi_{a c c})$ where $\\preceq_{t a s k}$ satisfies $\\bar{\\forall}\\pi_{1},\\pi_{2}\\in\\Pi,\\pi_{1}\\preceq_{t a s k}\\pi_{2}\\Leftrightarrow U_{r}(\\pi_{1})\\leq$ $U_{r}(\\pi_{2})$ , and $\\Pi_{a c c}=\\{\\pi\\mid\\forall\\pi^{\\prime}\\in\\Pi.\\pi^{\\prime}\\preceq_{t a s k}\\pi\\}$ contains all the optimal policies. ", "page_idx": 2}, {"type": "text", "text": "Designing reward function(s) that align with the underlying task is essential in RL. Whether a designed reward aligns with the task hinges on how policies are ordered by the task and the utilities of the policies under the reward function. Therefore, we define the task-reward alignment by examining the utility spaces of the reward functions. If the acceptable policy set $\\Pi_{a c c}$ of the task is given, we let $\\underline{{U}}_{r}:=\\operatorname*{inin}_{\\pi\\in\\Pi_{a c c}}U_{r}(\\pi)$ be the minimal utility achieved by any acceptable policy under $r$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Task-Aligned Reward Functions). A reward function is a task-aligned reward function (denoted as $r^{+}$ ) if and only if $\\forall\\pi\\in\\Pi\\backslash\\Pi_{a c c},U_{r^{+}}(\\pi)<\\underline{{U}}_{r^{+}}(\\pi)$ . Conversely, if this condition is not met, it is a task-misaligned reward function (denoted as $r^{-}$ ). ", "page_idx": 3}, {"type": "text", "text": "The definition suggests that under a task-aligned reward function $r^{+}$ , all acceptable policies for the task yield higher utilities than unacceptable ones. It also suggests that a policy is deemed acceptable as long as its utility is greater than $\\underline{{U}}_{r^{+}}$ for some task-aligned reward function $r^{+}$ , even if this policy is not optimal. We also examine whether high utility under a reward function $r$ suggests a higher order under $\\preceq_{t a s k}$ . We define $\\overline{{U}}_{r}:=\\operatorname*{max}_{\\pi\\in\\Pi}\\bar{U_{r}}(\\pi)\\;s.\\dot{t}.\\;\\forall\\pi_{1},\\pi_{2}\\in\\Pi,U_{r}(\\pi_{1})<U_{r}(\\bar{\\pi)}\\overset{\\cdot}{\\leq}U_{r}(\\pi_{2}\\big)\\Rightarrow$ $(\\pi_{1}\\preceq_{t a s k}\\pi)\\wedge(\\pi_{1}\\preceq_{t a s k}\\pi_{2})$ , which is the highest utility threshold such that any policy achieving a higher utility than ${\\overline{{U}}}_{r}$ has a higher order than those achieving lower utilities than $\\overline{{U}}_{r}$ . In Figure 1(a) we illustrate how ${\\overline{{U}}}_{r}$ and $U_{r}$ vary between task-aligned and misaligned reward functions. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Given the policy order $\\preceq_{t a s k}$ of a task, for any two reward functions $r_{1},r_{2}$ , if $\\{\\pi\\mid\\bigcap U_{r_{1}}(\\pi)\\geq\\overline{{{U}}}_{r_{1}}\\}\\subseteq\\{\\pi\\mid U_{r_{2}}(\\pi)\\geq\\overline{{{U}}}_{r_{2}}\\}$ , then there must exist policies $\\pi_{1}\\in\\{\\pi\\mid U_{r_{1}}(\\pi)\\geq$ $\\overline{{U}}_{r_{1}}\\},\\pi_{2}\\in\\{\\pi\\mid U_{r_{2}}(\\pi)\\geq\\overline{{U}}_{r_{2}}\\}$ such that $U_{r_{1}}(\\pi_{2})\\leq U_{r_{1}}(\\pi_{1})$ and $\\pi_{2}\\preceq_{t a s k}\\pi_{1}$ while $U_{r_{2}}(\\pi_{2})\\geq$ $U_{r_{2}}(\\pi_{1})$ . ", "page_idx": 3}, {"type": "text", "text": "This proposition implies that a high threshold $\\overline{{U}}_{r}$ indicates that a high utility corresponds to a high order in terms of $\\preceq_{t a s k}$ . In particular, for any task-aligned reward function $r^{+}$ , $\\left\\{\\pi\\mid U_{r^{+}}(\\pi)\\stackrel{\\cdot}{=}$ $\\overline{{U}}_{r^{+}}\\}\\subseteq\\Pi_{a c c}\\equiv\\{\\pi\\mid U_{r^{+}}(\\pi)\\geq\\underline{{U}}_{r^{+}}\\}$ (see proof in Appendix A.2). Thus, a small $\\{\\pi\\mid U_{r^{+}}(\\pi)\\geq$ $\\overline{{U}}_{r^{+}}\\}$ leads to a large $\\{\\pi\\mid U_{r^{+}}(\\pi)\\in[\\underline{{U}}_{r^{+}},\\overline{{U}}_{r^{+}}]\\}$ . Hence, a task-aligned reward function $r^{+}$ is more likely to be aligned with the task if it has a wide $[\\underline{{U}}_{r^{+}},\\overline{{U}}_{r^{+}}]$ and a narrow $[\\overline{{U}}_{r^{+}},\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{+}}(\\pi)]$ . ", "page_idx": 3}, {"type": "text", "text": "4.1 Mitigate Task-Reward Misalignment in IRL-Based IL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In IRL-based $\\mathrm{IL}$ , a key challenge is that the underlying task is unknown, making it difficult to assert if a learned policy is acceptable. We denote the optimal reward function learned from the demonstration set $E$ as $r^{*}$ , and the optimal policy under $r^{*}$ as $\\pi_{r^{*}}$ . When $\\pi_{r^{*}}$ has a poor performance under $r_{E}$ , it is considered to have a high $R e g r e t(\\pi_{r^{*}},r_{E})$ which is defined in Eq.2. If $R e g r e t(\\pi_{r^{*}},r_{E})>$ $\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r_{E}}(\\pi^{\\prime})-\\underline{{U}}_{r_{E}}$ , then $\\pi_{r^{*}}$ is unacceptable and $r^{*}$ is task-misaligned. ", "page_idx": 3}, {"type": "equation", "text": "$$\nR e g r e t(\\pi,r):=\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r}(\\pi^{\\prime})-U_{r}(\\pi)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Several factors can lead to a high $R e g r e t(\\pi_{r^{*}},r_{E})$ . For instance, Viano et al. [2021] shows that when expert demonstrations are collected in an environment whose dynamical function differs from that of the learning environment, $|R e g r e t(\\pi_{r^{*}},r_{E})|$ can be positively related to the discrepancy between those dynamical functions. Additionally, we prove in Appendix A.1 that learning from only a few representative expert trajectories can also result in a large $|R e g r e t(\\pi_{r^{*}},r_{E})|$ with a high probability. ", "page_idx": 3}, {"type": "text", "text": "Our insight for mitigating such potential task-reward misalignment in IRL-based $\\mathrm{IL}$ is to shift our focus from learning an optimal policy that maximizes the intrinsic $r_{E}$ to learning an acceptable policy $\\pi^{*}$ that achieves a utility higher than $U_{r^{+}}$ under any task-aligned reward function $r^{+}$ . Our approach is to treat the expert demonstrations as weak supervision signals based on the following. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $\\mathcal{T}$ be an indicator function. For any $\\begin{array}{r}{k\\geq\\big\\{\\underset{r^{+}}{\\operatorname*{min}}\\,\\sum_{\\pi\\in\\Pi}{\\mathcal L}\\{U_{r^{+}}(\\pi)\\geq U_{r^{+}}(\\pi_{E})\\}\\big\\},}\\end{array}$ , $i f\\pi^{*}$ satisfies $\\begin{array}{r}{\\left\\{\\sum_{\\pi\\in\\Pi}{\\mathcal{I}}\\{U_{r}(\\pi)\\geq U_{r}(\\pi^{*})\\}\\right\\}<|\\Pi_{a c c}|}\\end{array}$ for all $r\\in R_{E,k}:=\\{r\\mid\\textstyle\\sum_{\\pi\\in\\Pi}{\\mathcal{Z}}\\{U_{r}(\\pi)\\geq\\}$ $U_{r}(\\pi_{E})\\}\\leq k\\}$ , then $\\pi^{*}$ is an acceptable policy, i.e., $\\pi^{*}\\in\\Pi_{a c c}$ . Additionally, i $f k<|\\Pi_{a c c}|,$ , such an acceptable policy $\\pi^{*}$ is guaranteed to exist. ", "page_idx": 3}, {"type": "text", "text": "The statement suggests that we can obtain an acceptable policy by training it to attain high performance across a reward function set $R_{E,k}$ that includes all the reward functions where, for each reward function at most $k$ policies outperform the expert policy $\\pi_{E}$ . The minimal value of $k$ is determined by all the task-aligned reward functions in the reward hypothesis set. Appendix A.2 provides the proof. ", "page_idx": 3}, {"type": "text", "text": "How to build $R_{E,k}?$ Building $R_{E,k}$ involves setting the parameter $k$ . If $r_{E}$ is a task-aligned reward function and $\\pi_{E}$ is optimal solely under $r_{E}$ , then the minimal $k\\,=\\,0$ , and $R_{E,0}$ only contains $r_{E}$ . However, relying on a singleton $R_{E,0}$ equates to applying vanilla IRL, which is susceptible to misalignment issues, as noted earlier. It is crucial to recognize that $r_{E}$ might not meet the task-aligned reward function criteria specified in Definition 2, even though its optimal policy $\\pi_{E}$ is acceptable. This situation necessitates a positive $k$ , thereby expanding $R_{E,k}$ beyond a single function and changing the role of expert demonstrations from strong supervision to weak supervision. Note that we suggest letting $k\\leq|\\Pi_{a c c}|$ instead of allowing $k\\rightarrow\\infty$ because $R_{E,\\infty}$ would then encompass all possible reward functions, and it is impractical to identify a policy capable of achieving high performance across all reward functions. Letting $k\\leq|\\Pi_{a c c}|$ guarantees there exists a feasible policy $\\pi^{*}$ , e.g., $\\pi_{E}$ itself. As the task alignment of each reward function typically remains unknown in IRL settings, this paper proposes treating $k$ as an adjustable parameter \u2013 starting with a small $k$ and adjusting based on empirical learning outcome, allowing for iterative refinement for alignment with task requirements. ", "page_idx": 4}, {"type": "text", "text": "In practice, $\\Pi$ can be uncountable, e.g., a Gaussian policy. Hence, we adapt the concept of $k$ in $R_{E,k}$ to a hyperparameter $\\delta\\leq\\delta^{*}:=\\operatorname*{max}_{r}\\mathcal{I}_{I R L}(r)$ , leading us to redefine $R_{E,k}$ as a $\\delta$ -optimal reward function set $R_{E,\\delta}:=\\{r\\mid\\mathcal{T}_{I R L}(r)\\geq\\delta\\}$ . This superlevel set includes all the reward functions under which the optimal policies outperform the expert by at most $-\\delta$ . If $\\delta$ is appropriately selected such that $R_{E,\\delta}$ includes task-aligned reward functions, we can mitigate reward misalignment by satisfying the conditions outlined in Definition 3, which are closely related to Definition 2 and Proposition 1. ", "page_idx": 4}, {"type": "text", "text": "Definition 3 (Mitigation of Task-Reward Misalignment). Assuming that the reward function set $R_{E,\\delta}$ contains task-aligned reward function $r^{+}$ \u2019s, the mitigation of task-reward misalignment in IRL-based $\\mathrm{IL}$ is to learn a policy $\\pi^{*}$ such that (i) (Weak Acceptance) $\\forall r^{+}\\in R_{E,\\delta}$ , $U_{r^{+}}(\\pi^{*})\\geq\\underline{{U}}_{r^{+}}$ , or (ii) (Strong Acceptance) $\\forall r^{+}\\in R_{E,\\delta},U_{r^{+}}(\\pi^{*})\\geq\\overline{{U}}_{r^{+}}$ . ", "page_idx": 4}, {"type": "text", "text": "While condition (i) states that $\\pi^{*}$ is acceptable for the task, i.e., $\\pi^{*}\\in\\Pi_{a c c}$ , condition (ii) further states that $\\pi^{*}$ have a high order in terms of $\\preceq_{t a s k}$ . Hence, condition (i) is weaker than (ii) because a policy $\\pi^{*}$ satisfying (ii) automatically satisfies (i) according to Definition 2. Given the uncertainty in identifying which reward function is aligned, our solution is to train a policy to achieve high utilities under all reward functions in $R_{E,\\delta}$ to satisfy the conditions in Definition 3. We explain this approach in the following semi-supervised paradigm, PAGAR. ", "page_idx": 4}, {"type": "text", "text": "5 Protagonist Antagonist Guided Adversarial Reward (PAGAR) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "PAGAR is an adversarial reward searching paradigm which iteratively searches for a reward function to challenge a policy learner by incurring a high regret as defined in Eq.2. We refer to the policy to be learned as the protagonist policy and re-write it as $\\pi_{P}$ . We then introduce a second policy, dubbed antagonist policy $\\pi_{A}$ , as a proxy of the arg max $U_{r}(\\pi^{\\prime})$ for Eq.2. For each reward function $r$ , we call $\\pi^{\\prime}\\!\\in\\!\\Pi\\qquad\\quad$   \nthe regret of $\\pi_{P}$ under $r$ , i.e., $R e g r e t(\\pi_{P},r)=\\operatorname*{max}_{\\pi_{A}\\in\\Pi}U_{r}(\\pi_{A})-U_{r}(\\pi_{P})$ , the Protagonist Antagonist Induced Regret. We then formally define PAGAR in Definition 4. ", "page_idx": 4}, {"type": "text", "text": "Definition 4 (Protagonist Antagonist Guided Adversarial Reward (PAGAR)). Given a candidate reward function set $R$ and a protagonist policy $\\pi_{P}$ , PAGAR searches for a reward function $r$ within $R$ to maximize the Protagonist Antagonist Induced Regret, i.e., maxRegret $(\\pi_{P},r)$ . r\u2208R ", "page_idx": 4}, {"type": "text", "text": "PAGAR-based IL learns a policy from $R_{E,\\delta}$ by minimizing the worst-case Protagonist Antagonist Induced Regret via MinimaxRegret $\\left(R_{E,\\delta}\\right)$ as defined in Eq.3 where $R$ can be any input reward function set and is set as $R=R_{E,\\delta}$ in PAGAR-based IL. ", "page_idx": 4}, {"type": "equation", "text": "$$\nM i n i m a x R e g r e t(R):=\\arg\\operatorname*{min}_{\\pi_{P}\\in\\Pi}\\operatorname*{max}_{r\\in R}R e g r e t(\\pi_{P},r)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our subsequent discussion will focus on identifying the sufficient conditions for PAGAR-based $\\mathrm{IL}$ to mitigate task-reward misalignment as described in Definition 3. In particular, we consider the case where $\\mathcal{I}_{I R L}(r):=U_{r}(E)-\\operatorname*{max}_{\\pi}U_{r}(\\pi)$ . We use $L_{r}$ to denote the Lipschitz constant of $r(\\tau)$ , and $W_{E}$ to denote the smallest Wasserstein 1-distance $W_{1}(\\pi,E)$ between $\\tau\\sim\\pi$ of any $\\pi$ and $\\tau\\sim E$ , i.e., $W_{E}\\triangleq\\operatorname*{min}_{\\pi\\in\\Pi}W_{1}(\\pi,E)$ . Then, we have Theorem 2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Weak Acceptance). If the following conditions $(I)$ (2) hold for $R_{E,\\delta}$ , then the optimal protagonist policy $\\pi_{P}:=M i n i m a x R e g r e t(R_{E,\\delta})$ satisfies $\\forall r^{+}\\in R_{E,\\delta}$ , $U_{r^{+}}(\\pi_{P})\\geq U_{r^{+}}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T h e r e\\;e x i s t s\\;r^{+}\\in R_{E,\\delta}s,\\;a n d\\underset{r^{+}\\in R_{E,\\delta}}{\\operatorname*{max}}\\;\\{\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\;U_{r^{+}}(\\pi)-\\overline{{U}}_{r^{+}}\\}<\\underset{r^{+}\\in R_{E,\\delta}}{\\operatorname*{min}}\\;\\{\\overline{{U}}_{r^{+}}-U_{r^{+}}\\};}\\\\ &{\\forall r^{+}\\in R_{E,\\delta},\\;L_{r^{+}}\\cdot W_{E}-\\delta\\leq\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\;U_{r^{+}}(\\pi)-\\overline{{U}}_{r^{+}}\\;a n d\\;\\forall r^{-}\\;\\in R_{E,\\delta},\\;L_{r^{-}}\\cdot W_{E}-\\delta<0}\\\\ &{\\ \\ \\operatorname*{min}\\quad\\{\\overline{{U}}_{r^{+}}-U_{r^{+}}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This statement shows the conditions for PAGAR-based $\\mathrm{IL}$ to attain the \u2018Weak Acceptance\u2019 goal described in Definition 3. The condition (1) states that the task-aligned reward functions in $R_{E,\\delta}$ all have a high level of alignment in matching $\\preceq_{t a s k}$ within their high utility ranges. The condition (2) requires that for the policy $\\pi^{*}=\\arg\\operatorname*{min}_{\\pi\\in\\Pi}W_{1}(\\pi,E)$ , the performance difference between $E$ and $\\pi^{*}$ is small enough under all $r\\in R_{E,\\delta}$ . Since for each reward function $r\\in R_{E,\\delta}$ , the performance difference between $E$ and the optimal policy under $r$ is bounded by $\\delta$ , condition (2) implicitly requires that $\\pi^{*}$ not only performs well under any task-aligned reward function $r^{+}$ (thus being acceptable in the task) but also achieve relatively low regret under task-misaligned reward function $r^{-}$ . However, the larger the rage $[\\overline{{U}}_{r^{+}},\\underline{{U}}_{r^{+}}]$ is across the task-aligned reward function $r^{+}$ , the less strict the requirement for low regret under $r^{-}$ becomes. The proof can be found in Appendix A.5. The following theorem further suggests that a $\\delta$ close to its upper-bound $\\delta^{*}:=\\operatorname*{max}_{r}\\mathcal{I}_{I R L}(r)$ can help MinimaxRegret $\\left(R_{E,\\delta}\\right)$ gain a better chance of finding an acceptable policy for the underlying task and attain the \u2018Strong Acceptance\u2019 goal described in Definition 3. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3 (Strong Acceptance). Assume that the condition $(I)$ in Theorem 2 holds for $R_{E,\\delta}$ . If for any $r\\in R_{E,\\delta;}$ , $L_{r}\\cdot W_{E}-\\delta\\leq\\operatorname*{min}_{r^{+}\\in R_{E,\\delta}}{\\{\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{+}}(\\pi)-\\overline{{U}}_{r^{+}}\\}},$ , then the optimal protagonist policy \u03c0P = MinimaxRegret $\\left(R_{E,\\delta}\\right)$ satisfies $\\forall r^{+}\\in R_{E,\\delta}$ , $U_{r^{+}}(\\pi_{P})\\geq\\overline{{U}}_{r^{+}}$ . ", "page_idx": 5}, {"type": "text", "text": "When do these assumptions hold? The condition (1) in Theorem 2 requires all the task-aligned reward functions in $R_{E,\\delta}$ exhibit a high level of conformity with the policy order $\\preceq_{t a s k}$ . Being task-aligned already sets a strong premise for satisfying this condition. We further posit that this condition is more easily satisfied when the task has a binary outcome, such as in reach-avoid tasks so that the aligned and misaligned reward functions tend to have higher discrepancy than tasks with quantitative outcomes. In the experimental section, we validate this hypothesis by evaluating tasks of this kind. Regarding condition (2) of Theorem 2 and the assumptions of Theorem 3, which basically require the existence of a policy with low regret across $R_{E,\\delta}$ set, it is reasonable to assume that expert policy meets this criterion. ", "page_idx": 5}, {"type": "text", "text": "5.1 Comparing PAGAR-Based IL with IRL-Based IL ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We illustrate the difference between IRL-based $\\mathrm{IL}$ and PAGAR-based IRL in Fig.1(b). While IRLbased $\\mathrm{IL}$ aims to learn the optimal policy $\\pi_{r^{*}}$ under the IRL-optimal reward $r^{*}$ , PAGAR-based IL learns a policy $\\pi^{*}$ from the reward function set $R_{E,\\delta}$ . Both PAGAR-based IL and IRL-based IL are zero-sum games between a policy learner and a reward learner. However, while IRL-based IL only aims to reach equilibrium at a single reward function under strong assumptions, e.g., sufficient demonstrations, convex reward and policy spaces, etc., PAGAR-based $\\mathrm{IL}$ can reach equilibrium with a mixture of reward functions without those assumptions. ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. Given arbitrary reward function set $R_{s}$ , there exists a constant $c$ and a distribution ${\\mathcal{R}}_{\\pi}$ over $R$ such that MinimaxRegret $(R)$ yields the same pol$\\begin{array}{r l}&{i c y\\;\\;\\,a s\\;\\;\\arg\\operatorname*{max}_{\\pi\\in\\Pi}\\ \\;\\;\\bigg\\{\\frac{R e g r e t(\\pi,r_{\\pi}^{*})}{c-U_{r_{\\pi}^{*}}(\\pi)}\\cdot U_{r_{\\pi}^{*}}(\\pi)+\\underset{r\\sim\\mathcal{R}_{\\pi}(r)}{\\mathbb{E}}[(1-\\frac{R e g r e t(\\pi,r)}{c-U_{r}(\\pi)})\\cdot U_{r}(\\pi)]\\bigg\\}}\\\\ &{\\arg\\underset{r\\in R}{\\operatorname*{max}}\\;U_{r}(\\pi)\\;s.t.\\;r\\in\\arg\\underset{r^{\\prime}\\in R}{\\operatorname*{max}}\\;R e g r e t(\\pi,r^{\\prime}).}\\end{array}$ where $\\begin{array}{r l}{r_{\\pi}^{*}}&{{}=}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "A detailed derivation can be found in Theorem 6 in Appendix A.4. In a nutshell, $\\mathcal{R}_{\\pi}(r)$ is a baseline distribution over $R$ such that (i) $c\\equiv\\underset{r\\sim\\mathcal{R}_{\\pi}}{\\mathbb{E}}\\left[U_{r}(\\pi)\\right]$ holds for all the $\\pi$ \u2019s that do not always perform worse than any other policy under $r\\in R$ , (ii) among all the $R_{\\pi}$ \u2019s that satisfy the condition (i), we pick the one with the minimal $c$ ; and (iii) for any other policy $\\pi$ , ${\\mathcal{R}}_{\\pi}$ uniformly concentrates on a $\\operatorname{rg\\,max}_{r\\in R}\\,U_{r}(\\pi)$ . Note that in PAGAR-based $\\mathrm{IL}$ , where $R_{E,\\delta}$ is used in place of arbitrary $R$ , ${\\mathcal{R}}_{\\pi}$ is a distribution over $R_{E,\\delta}$ and $r_{\\pi}^{*}$ is constrained to be within $R_{E,\\delta}$ . Essentially, the mixed reward functions dynamically assign weights to $r\\sim\\mathcal{R}_{\\pi}$ and $r_{\\pi}^{*}$ depending on $\\pi$ . If $\\pi$ performs worse under $r_{\\pi}^{*}$ than under many other reward functions $(U_{r_{\\pi}^{*}}(\\pi)$ falls below $c$ ), a higher weight will be allocated to using $r_{\\pi}^{*}$ to train $\\pi$ . Conversely, if $\\pi$ performs better under $r_{\\pi}^{*}$ than under many other reward functions ( $c$ falls below $U r_{\\pi}^{*}(\\pi)$ ), a higher weight will be allocated to reward functions drawn from ${\\mathcal{R}}_{\\pi}$ . Furthermore, we prove in Appendix A.7 that the MinimaxRegret objective function defined in Eq.3 is a convex optimization w.r.t the protagonist policy $\\pi_{P}$ . ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We also prove in Appendix A.8 that when there is no misalignment issue, i.e., under the ideal conditions for IRL, PAGAR-based $\\mathrm{IL}$ can either guarantee inducing the same results as IRL-based $\\mathrm{IL}$ with $\\delta=\\operatorname*{max}_{r}\\mathcal{I}_{I R L}(r)$ , or guarantee inducing an acceptable $\\pi_{P}$ by making max $\\mathcal{I}_{I R L}(r)-\\delta$ no greater than $\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{+}}(\\pi)-\\overline{{U}}_{r^{+}}$ for $r^{+}\\in R_{E,\\delta}$ . ", "page_idx": 6}, {"type": "text", "text": "6 A Practical Approach to Implementing PAGAR-based IL ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we introduce a practical approach to solving MinimaxRegret $\\boldsymbol{\\cdot}(R_{E,\\delta})$ based on IRL and RL. In a nutshell, this approach alternates between policy learning and reward searching. We first explain how we optimize the policies; then we derive from Eq.3 two reward improvement bounds for searching for the adversarial reward. We will also discuss how to enforce the constraint $r\\in R_{E,\\delta}$ . ", "page_idx": 6}, {"type": "text", "text": "6.1 Policy Optimization with On-and-Off Policy Samples ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given an intermediate learned reward function $r$ , we use RL to train $\\pi_{P}$ to minimize the regret $\\operatorname*{min}_{\\pi_{P}}R e g r e t(\\pi_{P},r)=\\operatorname*{min}_{\\pi_{P}}\\left\\{\\operatorname*{max}_{\\pi_{A}}U_{r}(\\pi_{A})\\right\\}-U_{r}(\\pi_{P})$ as indicated by Eq.3 where $\\pi_{A}$ is trained to serve as the optimal policy under $r$ as noted in Section 5. Since we have to sample trajectories with $\\pi_{A}$ and $\\pi_{P}$ , we propose to combine off-policy and on-policy samples to optimize $\\pi_{P}$ so that we can leverage the samples maximally. Off-Policy: We leverage the Theorem 1 in Schulman et al. [2015] to derive a bound for the utility subtraction: $U_{r}(\\pi_{P})-{\\bar{U_{r}}}(\\pi_{A})\\leq\\sum_{s\\in\\mathbb{S}}\\rho_{\\pi_{A}}(s)\\sum_{a\\in\\mathbb{A}}\\pi_{P}(a|s)A_{\\pi_{A}}(s,a)\\,+$ $C\\cdot\\operatorname*{max}_{s}D_{T V}(\\pi_{A}(\\cdot|s),\\pi_{P}(\\cdot|s))^{2}$ where $\\begin{array}{r}{\\rho_{\\pi_{A}}(s)\\,=\\,\\sum_{t=0}^{T}\\gamma^{t}P r o b(s^{(t)}\\,=\\,s|\\pi_{A})}\\end{array}$ is the discounted visitation frequency of $\\pi_{A}$ , $A_{\\pi_{A}}$ is the advantage function without considering the entropy, and $C$ is some constant. Then we follow the derivation in Schulman et al. [2017], which is based on Theorem 1 in Schulman et al. [2015], to derive from the inequality an importance sampling-based objective function $\\mathcal{I}_{\\pi_{A}}(\\pi_{P};r)\\;:=\\;\\mathbb{E}_{s\\sim\\pi_{A}}[\\mathrm{min}(\\xi(s,a)\\cdot A_{\\pi_{A}}\\!\\left(s,a\\right),c l i p(\\bar{\\xi}(s,a),1-\\bar{\\sigma_{,1}}\\cdot\\bar{\\sigma_{,2}}\\cdot\\lambda_{\\pi_{A}}\\!\\left(s,a\\right)\\right]$ where $\\sigma$ is a clipping threshold, $\\begin{array}{r}{\\xi(s,a)=\\frac{\\pi_{P}(a|s)}{\\pi_{A}(a|s)}}\\end{array}$ is an importance sampling rate. The details can be found in Appendix B.1. This objective function allows us to train $\\pi_{P}$ by using the trajectories of $\\pi_{A}$ . On-Policy: We also optimize $\\pi_{P}$ with the standard RL objective function $\\mathcal{I}_{R L}(\\pi_{P};r)$ by using the trajectories of $\\pi_{P}$ itself. As a result, the objective function for optimizing $\\pi_{P}$ is $\\bar{\\mathcal{T}}_{\\pi_{A}}(\\pi_{P};r)+\\mathcal{J}_{R L}(\\pi_{P};r)$ . As for $\\pi_{A}$ , we only use the standard RL objective function, i.e., $\\pi_{P}\\!\\in\\!\\Pi$   \nmax $\\mathcal{J}_{R L}(\\pi_{A};r)$ . Although the computational complexity equals the sum of the complexities of RL \u03c0A\u2208\u03a0   \nupdate steps for $\\pi_{A}$ and $\\pi_{P}$ , these two RL update steps can be executed in parallel. ", "page_idx": 6}, {"type": "text", "text": "6.2 Regret Maxmization with On-and-Off Policy Samples ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Given the intermediate learned protagonist and antagonist policy $\\pi_{P}$ and $\\pi_{A}$ , according to MinimaxRegret in Eq.3, we need to optimize $r$ to maximize $U_{r}(\\pi_{A})\\mathrm{~-~}U_{r}(\\pi_{P})$ . In practice, we found that the subtraction between the estimated $U_{r}(\\pi_{A})$ and $U_{r}(\\pi_{P})$ can have a high variance. To resolve this issue, we derive two reward improvement bounds to approximate this subtraction. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4. Suppose policy $\\pi_{2}\\in\\Pi$ is the optimal solution for $\\mathcal{I}_{R L}(\\pi;r)$ . Then , the inequalities Eq.4 and 5 hold for any policy $\\pi_{1}\\in\\Pi$ , where $\\alpha=\\operatorname*{max}_{s}D_{T V}\\big(\\pi_{1}(\\cdot|s),\\pi_{2}(\\cdot|s)\\big)$ , $\\epsilon=\\operatorname*{max}_{s,a}|A_{\\pi_{2}}(s,a)|$ , an ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{l\\,\\Delta A(s)=\\underset{a\\sim\\pi_{1}}{\\mathbb{E}}[A_{\\pi_{2}}(s,a)]-\\underset{a\\sim\\pi_{2}}{\\mathbb{E}}[A_{\\pi_{2}}(s,a)].}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\left|U_{r}(\\pi_{1})-U_{r}(\\pi_{2})-\\sum_{t=0}^{\\infty}\\gamma_{\\ s^{(t)}\\sim\\pi_{1}}^{t}\\Big[\\Delta A(s^{(t)})\\Big]\\right|\\leq\\frac{2\\alpha\\gamma\\epsilon}{(1-\\gamma)^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\left|U_{r}(\\pi_{1})-U_{r}(\\pi_{2})-\\sum_{t=0}^{\\infty}\\gamma_{\\ s^{(t)}\\sim\\pi_{2}}^{t}\\Big[\\Delta A(s^{(t)})\\Big]\\right|\\leq\\frac{2\\alpha\\gamma(2\\alpha+1)\\epsilon}{(1-\\gamma)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 An On-and-Off-Policy Algorithm for Imitation Learning with PAGAR ", "page_idx": 7}, {"type": "text", "text": "Input: Expert demonstration $E$ , IRL loss bound $\\delta$ , initial protagonist policy $\\pi_{P}$ , antagonist policy $\\pi_{A}$ , reward function $r$ , Lagrangian parameter $\\lambda\\geq0$ , maximum iteration number $N$ . ", "page_idx": 7}, {"type": "text", "text": "Output: $\\pi_{P}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: for iteration $i=0,1,\\ldots,N$ do   \n2: Sample trajectory sets $\\mathbb{D}_{A}\\sim\\pi_{A}$ and $\\mathbb{D}_{P}\\sim\\pi_{P}$   \n3: Optimize $\\pi_{A}$ : estimate $\\mathcal{I}_{R L}(\\pi_{A};r)$ with $\\mathbb{D}_{A}$ ; update $\\pi_{A}$ to maximize $\\mathcal{J}_{R L}(\\pi_{A};r)$   \n4: Optimize $\\pi_{P}$ : estimate $\\mathcal{I}_{R L}(\\pi_{P};r)$ with $\\mathbb{D}_{P}$ ; estimate $\\mathcal{I}_{\\pi_{A}}(\\pi_{P};r)$ with $\\mathbb{D}_{A}$ ; update $\\pi_{A}$ to maximize $\\mathcal{J}_{R L}(\\pi_{P};r)+\\mathcal{J}_{\\pi_{A}}(\\pi_{P};r)$   \n5: Optimize $r$ : estimate $\\mathcal{I}_{P A G A R}(r;\\pi_{P},\\pi_{A})$ with $\\mathbb{D}_{P}$ and $\\mathbb{D}_{A}$ ; estimate $\\mathcal{I}_{I R L}(r)$ with $\\mathbb{D}_{A}$ and $E$ ; update $r$ to minimize $\\mathcal{I}_{P A G A R}(r;\\pi_{P},\\pi_{A})+\\lambda\\cdot(\\delta-\\mathcal{J}_{I R L}(r))$ ; then update $\\lambda$ to maximize $\\delta-\\mathcal{I}_{I R L}(r)$   \n6: end for ", "page_idx": 7}, {"type": "text", "text": "By letting $\\pi_{P}$ be $\\pi_{1}$ and $\\pi_{A}$ be $\\pi_{2}$ , Theorem 4 enables us to bound $U_{r}(\\pi_{A})-U_{r}(\\pi_{P})$ by using either only the samples of $\\pi_{A}$ or only those of $\\pi_{P}$ . Following $\\mathrm{Fu}$ et al. [2018], we let $r$ be a proxy of $A_{\\pi_{2}}$ in Eq.4 and 5. Then we derive two loss functions $\\mathcal{I}_{R,1}(r;\\pi_{P},\\pi_{A})$ and $\\mathcal{I}_{R,2}(r;\\pi_{P},\\pi_{A})$ for $r$ as shown in Eq.6 and 7 where $C_{1}$ and $C_{2}$ are constants proportional to the estimated maximum KL divergence between $\\pi_{A}$ and $\\pi_{P}$ (to bound $\\alpha$ Schulman et al. [2015]). The objective function for $r$ is then $\\mathcal{J}_{P A G A R}:=\\mathcal{J}_{R,1}+\\mathcal{J}_{R,2}$ . The complexity equals that of computing the reward along the trajectories sampled from $\\pi_{A}$ and $\\pi_{P}$ . ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{I}_{R,1}(r;\\pi_{P},\\pi_{A}):=\\underset{\\tau\\sim\\pi_{A}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(\\xi(s^{(t)},a^{(t)})-1\\right)\\cdot r(s^{(t)},a^{(t)})\\right]+C_{1}\\cdot\\underset{(s,a)\\sim\\pi_{A}}{\\operatorname*{max}}|r(s,a)|\\in\\pi_{B},}\\\\ &{\\mathcal{I}_{R,2}(r;\\pi_{P},\\pi_{A}):=\\underset{\\tau\\sim\\pi_{P}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(1-\\frac{1}{\\xi(s^{(t)},a^{(t)})}\\right)\\cdot r(s^{(t)},a^{(t)})\\right]+C_{2}\\cdot\\underset{(s,a)\\sim\\pi_{P}}{\\operatorname*{max}}|r(s,a)|\\in\\pi_{P}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "6.3 Algorithm for Solving PAGAR-Based IL ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We enforce the constraint $r\\,\\in\\,R_{E,\\delta}$ by adding to $\\mathcal{I}_{P A G A R}(r;\\pi_{P},\\pi_{A})$ a penalty term $\\lambda\\cdot(\\delta-$ ${\\mathcal{I}}_{I R L})$ , where $\\lambda$ is a Lagrangian parameter. Then, the objective function for optimizing $r$ is $\\operatorname*{min}_{r\\in R}\\,\\mathcal{J}_{P A G A R}(r;\\pi_{P},\\pi_{A})+\\lambda\\cdot(\\delta-\\mathcal{J}_{I R L}(r))$ . We initialize with a large $\\lambda$ to emphasize satisfying the constraint and update $\\lambda$ based on $\\delta-\\mathcal{I}_{I R L}$ (the details can be found in Appendix B.4). Algorithm 1 describes our approach to PAGAR-based IL. The algorithm alternates between optimizing the policies and the rewards function. It first trains $\\pi_{A}$ in line 3. Then, it employs the on-and-off policy approach to train $\\pi_{P}$ in line 4, where the off-policy objective ${\\mathcal{I}}_{\\pi_{A}}$ is estimated based on $\\mathbb{D}_{A}$ . In line 5, while $\\mathcal{T}_{P A G A R}$ is estimated based on both $\\mathbb{D}_{A}$ and $\\mathbb{D}_{P}$ , the ${\\mathcal{I}}_{I R L}$ is only based on $\\mathbb{D}_{A}$ . ", "page_idx": 7}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The goal of our experiments is to assess whether using PAGAR-based IL can efficiently mitigate reward misalignment under conditions that are not ideal for IRL. We present the main results below and provide details and additional results in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "7.1 Discrete Navigation Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Benchmarks: We consider a maze navigation environment where the task objective is compatible with Definition 1. Our benchmarks include two discrete domain tasks from the Mini-Grid environments Chevalier-Boisvert et al. [2023]: DoorKey-6x6- $\\nu\\o{O}$ , and SimpleCrossingS9N1- $\\cdot\\nu\\theta$ . In both tasks, the agent needs to interact with the environmental objects which are randomly positioned in every episode while the agent can only observe a small, unblocked area in front of it. The default reward, which is always zero unless the agent reaches the target, is used to evaluate the performance of learned policies. Due to partial observability and the implicit hierarchical nature of the task, these environments are considered challenging for RL and $\\mathrm{IL}$ , and have been extensively used for benchmarking curriculum RL and exploration-driven RL. ", "page_idx": 7}, {"type": "text", "text": "Baselines: We compare our approach with two standard baselines: GAIL Ho and Ermon [2016] and VAIL Peng et al. [2019]. GAIL has been introduced in Section 3. VAIL is based on GAIL but additionally optimizes a variational discriminator bottleneck (VDB) objective. Our approach uses the IRL techniques behind those two baseline algorithms, resulting in two versions of Algorithm 1, denoted as PAGAR-GAIL and PAGAR-VAIL, respectively. More specifically, if the baseline optimizes a $J_{I R L}$ objective, we use the same $J_{I R L}$ objective in Algorithm 1. Also, we extract the reward function $r$ from the discriminator $D$ as mentioned in Section 3. More details are in Appendix C.1. PPO Schulman et al. [2017] is used for policy training in GAIL, VAIL, and ours with a replay buffer of size 2048. Additionally, we compare our algorithm with a state-of-the-art (SOTA) IL algorithm, IQ-Learn Garg et al. [2021], which, however, is not compatible with our algorithm because it does not explicitly optimize a reward function. The policy and the reward functions are all approximated using convolutional networks. ", "page_idx": 8}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/f2d2460e46e2a689a9cf8afc3532cb267e797374e10a5b9175d2bb3a9fb92c4c.jpg", "img_caption": ["Figure 2: Comparing Algorithm 1 with baselines in partial observable navigation tasks. The suffix after each \u2018PAGAR-\u2019 indicates which IRL technique is used in Algorithm 1. The $y$ axis indicates the average return per episode. The $x$ axis indicates the number of time steps. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "IL with Limited Demonstrations. By learning from 10 expert-demonstrated trajectories with high returns, PAGAR-based IL produces high-performance policies with high sample efficiencies as shown in Figure 2(a) and (c). Furthermore, we compare PAGAR-VAIL with VAIL by reducing the number of demonstrations from 10 to 1. As shown in Figure 2(b) and (d), PAGAR-VAIL produces high-performance policies with significantly higher sample efficiencies. ", "page_idx": 8}, {"type": "text", "text": "IL under Dynamics Mismatch. We demonstrate that PAGAR enables the agent to infer and accomplish the objective of a task even in environments that are substantially different from the one observed during expert demonstrations. As shown in Figure 2(e), we collect 10 expert demonstrations from the SimpleCrossingS9N1- $\\cdot\\nu\\theta$ environment. Then we apply Algorithm 1 and the baselines, GAIL, VAIL, and IQ-learn to learn policies in SimpleCrossingS9N2-v0, SimpleCrossingS9N3- $\\cdot\\nu\\theta$ and FourRooms- $\\nu O$ . The results in Figure 2(f)-(g) show that PAGAR-based IL outperforms the baselines in these challenging zero-shot settings. ", "page_idx": 8}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/74ad979837130340552160b2548ac4f546b3a70cae3158ff82b5f93d40617697.jpg", "img_caption": ["Figure 3: PAGAR-GAIL in different reward spaces "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "IL with Different Reward Hypothesis Sets. The foundational theories of GAIL and AIRL indicate that different reward function hypothesis sets can affect the equilibrium of their GAN frameworks. We study whether choosing different reward hypothesis sets can influence the performance of Algorithm 1. We compare using a Sigmoid function with a Categorical distribution in the output layer of the discriminator networks in GAIL and PAGAR-GAIL. When using the Sigmoid function, the outputs of $D$ are not normalized, i.e., $\\begin{array}{r}{\\sum_{a\\in\\mathbb{A}}D(s,\\bar{a})\\neq1}\\end{array}$ . When using a Categorical distribution, $\\textstyle\\sum_{a\\in\\mathbb{A}}D(s,a)=1$ . We test GAIL and PAGAR-GAIL in DoorKey-6x6- $\\nu O$ environment. As shown in Figure 3, PAGAR-GAIL outperforms GAIL in both cases by using fewer samples. ", "page_idx": 8}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/137814aa9040be8dd37065521f1d152c79c073a3322297a4900a162b0e456ec9.jpg", "img_caption": ["Figure 4: Comparing Algorithm 1 with f-IRL in continuous control tasks. \u2018PAGAR-fIRL\u2019 indicates f-IRL is used as the inverse RL algorithm in Algorithm 1. The $y$ axis indicates the average return per episode. The $x$ axis indicates the number of time steps in the environment. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "VFRyS7Wx08/tmp/f6f8db2290c807542dd94c8dd3f2da5ae968deb6c15a9e7881c2917f41eb3adc.jpg", "table_caption": ["Table 1: Offline RL results obtained by combining PAGAR with RECOIL averaged over 4 seeds. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "7.2 Continuous Control Tasks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We evaluate PAGAR-based IL on continuous control tasks in both online and offline RL settings, demonstrating its ability to improve IRL-based IL performance across different types of tasks. ", "page_idx": 9}, {"type": "text", "text": "Benchmarks: We use four continuous control environments from Mujoco. In the online RL setting, both protagonist and antagonist policies are permitted to explore the environment. In the offline RL setting, exploration by these policies is restricted. Especially, for offline RL we use the D4RL\u2019s \u2018expert\u2019 datasets as the expert demonstrations and the \u2018random\u2019 datasets as the offline suboptimal dataset. Policy performance is evaluated online in both settings by using the default reward function of the environment. ", "page_idx": 9}, {"type": "text", "text": "Baselines: We compare PAGAR-based IL against f-IRL Ni et al. [2021] in the online RL setting and compare with RECOIL Sikchi et al. [2024] in the offline RL setting. When comparing with f-IRL, we use f-IRL as the IRL algorithm in Algorithm 1. When comparing with RECOIL, as RECOIL does not directly learn the reward function but learns the Q and V functions, we develop another algorithm for the offline RL setting to combine PAGAR with RECOIL by explicitly learning a reward function and using the reward function and V function to represent the Q function. The details can be found in Appendix B.4. ", "page_idx": 9}, {"type": "text", "text": "Results: As shown in Figure 4, PAGAR-based IL achieves equivalent performance to the baselines with fewer iterations. Furthermore, on the Ant and Walker2d tasks, Algorithm 1 matches the performance level of f-IRL using significantly less iterations. Additional results of PAGAR with GAIL and VAIL across other continuous control benchmarks are provided in Appendix C.3. Table 1 further shows that when combined with RECOIL, PAGAR-based IL achieves higher performance in most of the tasks than the baseline. These results demonstrate the broader applicability of PAGARbased IL in both online and offilne settings and its effectiveness across different types of environments, further reinforcing the robustness of our approach. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we propose to prioritize task alignment over conventional data alignment in IRL-based $\\mathrm{IL}$ by treating expert demonstrations as weak supervision signals to derive a set of candidate reward functions that align with the task rather than only with the data. Our PAGAR-based IL adopts an adversarial mechanism to train a policy with this set of reward functions. Experimental results demonstrate that our algorithm can mitigate reward misalignment in challenging environments. Our future work will focus on employing the PAGAR paradigm to other task alignment problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the U.S. National Science Foundation under grant CCF-2340776. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the Twenty-First International Conference on Machine Learning, ICML \u201904, page 1, New York, NY, USA, 2004. Association for Computing Machinery. ISBN 1581138385. doi: 10.1145/1015330. 1015430.   \nD. Abel, W. Dabney, A. Harutyunyan, M. K. Ho, M. Littman, D. Precup, and S. Singh. On the expressivity of markov reward. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \nD. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man\u00e9. Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.   \nH. Cao, S. Cohen, and L. Szpruch. Identifiability in inverse reinforcement learning. Advances in Neural Information Processing Systems, 34:12362\u201312373, 2021.   \nM. Chevalier-Boisvert, B. Dai, M. Towers, R. de Lazcano, L. Willems, S. Lahlou, S. Pal, P. S. Castro, and J. Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.   \nM. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 13049\u201313061. Curran Associates, Inc., 2020.   \nC. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International conference on machine learning, pages 49\u201358. PMLR, 2016.   \nC. Finn, T. Yu, J. Fu, P. Abbeel, and S. Levine. Generalizing skills with semi-supervised reinforcement learning. In International Conference on Learning Representations, 2017.   \nJ. Fu, K. Luo, and S. Levine. Learning robust rewards with adverserial inverse reinforcement learning. In International Conference on Learning Representations, 2018.   \nD. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon. IQ-learn: Inverse soft-q learning for imitation. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \nD. Hadfield-Menell, S. Milli, P. Abbeel, S. J. Russell, and A. Dragan. Inverse reward design. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   \nJ. Hejna and D. Sadigh. Inverse preference learning: Preference-based RL without a reward function. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id $=$ gAP52Z2dar.   \nJ. Ho and S. Ermon. Generative adversarial imitation learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.   \nJ. Hong, K. Bhatia, and A. Dragan. On the sensitivity of reward inference to misspecified human models. In The Eleventh International Conference on Learning Representations, 2023.   \nW. Jeon, S. Seo, and K.-E. Kim. A bayesian approach to generative adversarial imitation learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \nD. Lindner, A. Krause, and G. Ramponi. Active exploration for inverse reinforcement learning. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.   \nA. M. Metelli, G. Ramponi, A. Concetti, and M. Restelli. Provably efficient learning of transferable rewards. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 7665\u20137676. PMLR, 18\u201324 Jul 2021.   \nA. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML \u201900, pages 663\u2013670, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1-55860-707-2.   \nA. Y. Ng, D. Harada, and S. J. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning, ICML \u201999, pages 278\u2013287, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. ISBN 1-55860-612-2.   \nT. Ni, H. Sikchi, Y. Wang, T. Gupta, L. Lee, and B. Eysenbach. f-irl: Inverse reinforcement learning via state marginal matching. In Conference on Robot Learning, pages 529\u2013551. PMLR, 2021.   \nA. Pan, K. Bhatia, and J. Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id $\\equiv$ JYtwGwIL7ye.   \nX. B. Peng, A. Kanazawa, S. Toyer, P. Abbeel, and S. Levine. Variational discriminator bottleneck: Improving imitation learning, inverse RL, and GANs by constraining information flow. In International Conference on Learning Representations, 2019.   \nD. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI\u201907, page 2586\u20132591, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.   \nN. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In Proceedings of the 23rd international conference on Machine learning, pages 729\u2013736, 2006.   \nS. Ross and D. Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 661\u2013668, 2010.   \nS. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627\u2013635, 2011.   \nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR, 2015.   \nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.   \nH. Sikchi, Q. Zheng, A. Zhang, and S. Niekum. Dual RL: Unification and new methods for reinforcement and imitation learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\equiv$ xt9Bu66rqv.   \nJ. Skalse and A. Abate. Misspecification in inverse reinforcement learning. arXiv preprint arXiv:2212.03201, 2022.   \nJ. Skalse, M. Farrugia-Roberts, S. Russell, A. Abate, and A. Gleave. Invariance in policy optimisation and partial identifiability in reward learning. arXiv preprint arXiv:2203.07475, 2022a.   \nJ. M. V. Skalse, N. H. R. Howe, D. Krasheninnikov, and D. Krueger. Defining and characterizing reward gaming. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022b.   \nR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. 2018.   \nL. Viano, Y.-T. Huang, P. Kamalaruban, A. Weller, and V. Cevher. Robust inverse reinforcement learning under transition dynamics mismatch. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id $=$ t8HduwpoQQv.   \nW. Zhou and W. Li. Safety-aware apprenticeship learning. In H. Chockler and G. Weissenbacher, editors, Computer Aided Verification, pages 662\u2013680, Cham, 2018. Springer International Publishing. ISBN 978-3-319-96145-3.   \nW. Zhou and W. Li. Programmatic reward design by example. Proceedings of the AAAI Conference on Artificial Intelligence, 36(8):9233\u20139241, Jun. 2022a. doi: 10.1609/aaai.v36i8.20910.   \nW. Zhou and W. Li. A hierarchical Bayesian approach to inverse reinforcement learning with symbolic reward machines. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 27159\u201327178. PMLR, 17\u201323 Jul 2022b.   \nW. Zhou, R. Gao, B. Kim, E. Kang, and W. Li. Runtime-safety-guided policy repair. In Runtime Verification: 20th International Conference, RV 2020, Los Angeles, CA, USA, October 6\u20139, 2020, Proceedings 20, pages 131\u2013150. Springer, 2020.   \nB. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3, AAAI\u201908, pages 1433\u20131438. AAAI Press, 2008. ISBN 978-1-57735-368-3. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Reward Design with PAGAR ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "This paper does not aim to resolve the ambiguity problem in IRL but provides a way to circumvent it so that reward ambiguity does not lead to reward misalignment in IRL-based IL. PAGAR, the semisupervised reward design paradigm proposed in this paper, tackles this problem from the perspective of semi-supervised reward design. But the nature of PAGAR is distinct from IRL and IL: assume that a set of reward functions is available for some underlying task, where some of those reward functions align with the task while others are misaligned, PAGAR provides a solution for selecting reward functions to train a policy that successfully performs the task, without knowing which reward function aligns with the task. Our research demonstrates that policy training with PAGAR is equivalent to learning a policy to maximize an affine combination of utilities measured under a distribution of the reward functions in the reward function set. With this understanding of PAGAR, we integrate it with $\\mathrm{IL}$ to illustrate its advantages. ", "page_idx": 13}, {"type": "text", "text": "A.1 Motivation: Failures in IRL-Based IL ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For readers\u2019 convenience, we put the theorem from Abel et al. [2021] here for reference. ", "page_idx": 13}, {"type": "text", "text": "Theorem 5. Viano et al. [2021] If the demonstration environment has a dynamics function $\\mathcal{P}_{d e m o}$ different from the dynamics function $\\mathcal{P}$ in the learning environment, the performance gap between the policies $\\pi_{E}$ and $\\pi_{r^{*}}$ under the ground true reward function $r_{E}$ satisfies $\\mid U_{r_{E}}(\\pi_{E})-U_{r_{E}}(\\pi_{r^{*}})\\mid\\;\\leq$ $\\begin{array}{r l}&{\\frac{2\\cdot\\gamma\\cdot\\operatorname*{max}{|\\mathbf{\\sigma}_{S,a}|}\\mid r_{E}(s,a)\\mid}{(1-\\gamma)^{2}}\\cdot\\underset{s,a}{\\operatorname*{max}}\\;D_{T V}(\\mathcal{P}(\\cdot\\mid s,a),\\mathcal{P}_{d e m o}(\\cdot\\mid s,a)).}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Then, we prove that a limited number of demonstrations can also lead to the performance gap as described in Theorem 5. Essentially, we can construct a demonstration dynamics $\\mathcal{P}_{d e m o}$ such that all the state transition probability estimated in $E$ match $\\mathcal{P}_{d e m o}$ with zero error. Then a similar performance gap as that caused by dynamics mismatch can be derived. ", "page_idx": 13}, {"type": "text", "text": "Proposition 3. Assume that the expert demonstration contains m state-action pairs by only selecting the optimal actions, i.e., select arg max $\\pi_{E}(a|s)$ at each s. Then $\\begin{array}{r l}{\\mid U_{r_{E}}(\\bar{\\pi}_{E})-\\dot{U}_{r_{E}}(\\pi_{r^{*}})\\mid}&{{}\\leq}\\end{array}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2\\cdot d\\cdot\\gamma\\cdot\\operatorname*{max}{\\mid r\\in(s,a)\\mid}}{(1-\\gamma)^{2}}~~~w h e r e~P r o b(d\\ge\\epsilon)~\\le~2\\cdot\\exp(-2\\cdot m\\cdot\\epsilon^{2})~f o r~a n y~\\epsilon\\in~[\\underline{{p}},1]~w h e r e~\\underline{{p}}=~0,}\\\\ &{\\frac{1}{\\gamma}~\\cdot\\frac{\\operatorname*{min}{r}_{E}(s,a)-\\operatorname*{min}{r}_{E}(s,a)\\operatorname*{max}{\\pi}_{E}(a|s))}{\\operatorname*{max}{r}_{s,a}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. We translate the limited demonstration case into a dynamics mismatch case where we will leverage the demonstrated state transitions to construct a dynamics $\\mathcal{P}_{d e m o}(\\cdot\\mid s,a)$ and compare it with $\\mathcal{P}(\\cdot\\,\\vert\\,\\boldsymbol{s},\\boldsymbol{a})$ . ", "page_idx": 13}, {"type": "text", "text": "Assume that for each state-action pair $s,a$ in $E$ , i.e., $m_{s,a}\\in[m]$ , the $m_{s,a}$ transition instances are $(s,a,s_{1}),(s,a,s_{2}),\\ldots,(s,a,s_{m_{s,a}})$ . For each ${\\hat{s}}\\in\\mathbb{S}$ , we let $\\begin{array}{r}{\\dot{\\mathcal{P}}_{d e m o}(\\hat{s}\\mid s,a)=\\frac{1}{m_{s,a}}\\sum_{i=1}^{m_{s,a}}\\mathcal{Z}[s_{i}=}\\end{array}$ $\\hat{s}]$ . ", "page_idx": 13}, {"type": "text", "text": "Then, we can view $\\begin{array}{r}{D_{T V}(\\mathcal{P}(\\cdot\\mid s,a),\\mathcal{P}_{d e m o}(\\cdot\\mid s,a))\\,=\\,\\frac{1}{2}\\,\\left||\\mathcal{P}(\\cdot\\mid s,a)-\\mathcal{P}_{d e m o}(\\cdot\\mid s,a))|\\right|_{1}}\\end{array}$ as a function of $m$ independent random variables, i.e., $(s_{1},a_{1}),(s_{2},a_{2}),\\ldots,(s_{m_{s,a},a_{m_{s,a}}})$ , sampled from the $\\mathbb{S}$ domain. If one of the variables, $s_{i}$ , is changed from $\\hat{s}$ to another state $\\hat{s}^{\\prime}\\ \\neq\\ \\hat{s}$ , Pdemo(s\u02c6 | s, a) and Pdemo(s\u02c6\u2032 | s, a) should decrease and increase by m1s,a respectively. Then, DT V (P(\u00b7 | s, a), Pdemo(\u00b7 | s, a)) changes at most bym1s,a . ", "page_idx": 13}, {"type": "text", "text": "We can also view the $D_{T V}$ at each $(s,a)\\ \\in\\ E$ as functions of the $m$ sampled state-action pairs in $E$ , with a little abuse of notations, denoted as $(s_{1},a_{1}),(s_{2},a_{2}),\\ldots,(s_{m},a_{m})$ by flattening all the demonstrated trajectories. If a state-action pair, $(s_{i},a_{i})$ , in one of the trajectories, is changed from $({\\hat{s}},{\\hat{a}})$ to another state $(\\hat{s}^{\\prime},\\hat{a}^{\\prime})$ , then for the state-action $\\left(s_{i-1},a_{i-1}\\right)$ that precedes $s_{i}$ if $s_{i}$ is not the initial state, $D_{T V}(\\mathcal{P}(\\cdot\\mid s,a),\\mathcal{P}_{d e m o}(\\cdot\\mid s,a))$ changes at most by $\\frac{1}{m_{s_{i-1},a_{i-1}}}$ which can be as large as 1 since it is possible that $m_{s_{i-1},a_{i-1}}~=~1$ . Also, $D_{T V}(\\mathcal{P}(\\cdot\\mid\\hat{s},\\hat{a}),\\mathcal{P}_{d e m o}(\\cdot\\mid\\hat{s},\\hat{a}))$ and $D_{T V}(\\mathcal{P}(\\cdot\\mid\\hat{s}^{\\prime},\\hat{a}^{\\prime}),\\mathcal{P}_{d e m o}(\\cdot\\mid\\hat{s}^{\\prime},\\hat{a}^{\\prime}))$ change at most by $\\frac{1}{m_{\\hat{s},\\hat{a}}}$ and 1 $\\frac{1}{m_{\\hat{s}^{\\prime},\\hat{a}^{\\prime}}}$ . Note that if $m_{\\hat{s}^{\\prime},\\hat{a}^{\\prime}}\\;=\\;0$ , i.e., $(\\hat{s}^{\\prime},\\hat{a}^{\\prime})\\;\\notin\\;E$ , changing ${\\hat{s}},{\\hat{a}}$ into $\\hat{s}^{\\prime},\\hat{a}^{\\prime}$ equivalently leads to taking $D_{T V}(\\mathcal{P}(\\cdot\\mid\\hat{s},\\hat{a},\\mathcal{P}_{d e m n o}(\\cdot\\mid\\hat{s}^{\\prime},\\hat{a}^{\\prime}))$ into consideration when computing ms,aax DT V (P(\u00b7 | s, a), Pdemo(\u00b7 | s, a)). And DT V (P(\u00b7 | s\u02c6, \u02c6a, Pdemno(\u00b7 | s\u02c6\u2032, \u02c6a\u2032)) \u22641\u2212P(si+12|s\u02c6\u2032,a\u02c6\u2032)) where $s_{i+1}$ is the state that succeeds $s_{i}$ in the demonstrated trajectroy. Therefore, if changing a state-action pair in $E$ into another state-action pair, $\\operatorname*{max}_{(s,a)\\in E}D_{T V}(\\mathcal{P}(\\cdot\\mid s,a),\\mathcal{P}_{d e m o}(\\cdot\\mid s,a))$ can be changed as large as by 1. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "According to McDiarmid\u2019s Inequality, Prob(| max DT V (P(\u00b7 | s, a), Pdemo(\u00b7 | s, a)) \u2212 $\\begin{array}{r l}&{\\mathrm{~wcontmg~\\gamma~w~ive~}_{1}\\mathrm{and}\\mathrm{mut}=\\flat\\mod\\lim_{(s,a)\\in E}\\ \\fint w_{\\varepsilon}(\\mathrm{\\boldmath~\\operatorname{\\tt~mode}~}_{(s,a)\\in E}\\mod^{2}\\gamma V^{\\prime}\\{\\mathrm{\\boldmath~\\beta~}^{*},\\mathrm{\\boldmath~\\alpha~}^{*},u\\},r\\,d e m\\circ\\{\\mathrm{\\boldmath~\\beta~}^{*},\\mathrm{\\boldmath~\\alpha~}^{*},u\\})=}\\\\ &{\\{\\big[\\operatorname*{max}_{(s,a)\\in E}\\ D_{T V}(\\mathcal P(\\cdot\\ |\\ s,a),\\mathcal P_{d e m o}(\\cdot\\ |\\ s,a))\\big]\\ \\ge\\ \\epsilon\\ \\}\\ \\le\\ 2\\cdot\\exp(-\\frac{2\\cdot\\epsilon^{2}}{\\sum_{i=1}^{m}(1)^{2}})\\ =\\ 2\\cdot\\exp(-\\frac{2\\cdot\\epsilon^{2}}{m}).}\\end{array}$ Note that $\\mathbb{E}[\\operatorname*{max}_{s,a}D_{T V}(\\mathcal{P}(\\cdot\\mid s,a),\\mathcal{P}_{d e m o}(\\cdot\\mid s,a))]=0$ since the estimation is un-biased. Hence, $\\begin{array}{r}{P r o b\\big(\\underset{(s,a)\\in E}{\\operatorname*{max}}D_{T V}(\\mathcal{P}(\\cdot\\mid s,a),\\mathcal{P}_{d e m o}(\\cdot\\mid s,a))\\ge\\epsilon\\big)\\le2\\cdot\\exp(-\\frac{2\\epsilon^{2}}{m})\\,\\mathrm{for}\\,(s,a)\\in E.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "For state-action pairs that are outside of $E$ , we need to construct $\\mathcal{P}_{d e m o}$ at those state-action pairs such that the action selection in $E$ is always optimal. To start with, we let $\\mathcal{P}_{d e m o}(\\cdot\\mid a,s)\\equiv\\mathcal{P}(\\cdot\\mid a,s)$ for any state-action pairs that do not appear in $E$ . For some state-action pairs, the states are not reached in $E$ . We can disregard whether maintains optimal at those states and focus on the states that do appear in $E$ . By denoting the Q-value of $\\pi_{E}$ under $\\mathcal{P}_{d e m o}$ as $Q_{\\pi_{E}}^{d e m o}$ , we need to make sure that the $Q_{\\pi_{E}}^{d e m o}(s,a^{\\prime})$ for $a^{\\prime}\\neq\\arg\\operatorname*{max}_{a}\\pi_{E}(a|s)$ is no greater than $Q_{\\pi_{E}}^{d e m o}(s,\\arg\\operatorname*{max}_{a}\\pi_{E}(a|s))$ . Since the state-actions in $E$ may have extremely low rewards stochastic nature of $\\mathcal{P}$ , we consider the worst case Q-value for $a^{*}=\\arg\\operatorname*{max}_{a}\\pi_{E}(a|s)$ i.e., $\\begin{array}{r}{Q_{\\pi_{E}}^{d e m o}(s,a^{*})\\,\\ge\\,\\frac{1}{1-\\gamma}\\cdot\\underset{s}{\\operatorname*{min}}\\,\\,r_{E}\\big(s,\\arg\\underset{a}{\\operatorname*{max}}\\,\\pi_{E}(a|s)\\big)}\\end{array}$ . Meanwhile, for $a^{\\prime}\\neq\\arg\\operatorname*{max}_{a}\\pi_{E}(a|s),$ , we consider the best-case $\\mathrm{^Q}$ -value, i.e., Qd\u03c0emo(s, a\u2032) \u2264 $\\frac{1}{1\\!-\\!\\gamma}\\,\\cdot\\operatorname*{max}_{(s,a)}\\,r_{E}(s,a)$ . We add a dummy, absorbing state $\\underline{s}$ that is unreachable from any state-action under the dynamics $\\mathcal{P}$ and also unreachable from any demonstrated state-action under the constructed dynamics $\\mathcal{P}_{d e m o}$ . But we let $\\mathcal{P}_{d e m o}(\\underline{{s}}|s,a^{\\prime})>0$ if $Q_{\\pi_{E}}^{d e m o}(s,a^{\\prime})>Q_{\\pi_{E}}^{d e m o}(s,a^{*})$ . In other words, we add probability density on transitioning from such $(s,a^{\\prime})$ to $\\underline{s}$ so that $Q_{\\pi_{E}}^{d e m o}(s,a^{\\prime})$ drops below $Q_{\\pi_{E}}^{d e m o}(s,a^{*})$ . We denote this density as $\\underline{{p}}$ . Then $\\mathcal{P}_{d e m o}(\\underline{{s}}|s,a^{\\prime})\\;=\\;p$ and $\\mathcal{P}_{d e m o}(s^{\\prime}|s,a^{\\prime})\\;=$ $(1-\\underline{{p}})\\cdot\\mathcal{P}(s^{\\prime}|s,a^{\\prime})$ for any other $s^{\\prime}\\in\\mathbb{S}$ . As a result, $D_{T V}(\\mathcal{P}(\\cdot\\mid s,a^{\\prime}),\\mathcal{P}_{d e m o}(\\cdot\\mid s,a^{\\prime})=p$ . Note find the that adding $\\underline{{p}}$ to ensure $\\underline{s}$ does not affect the Q-values of the state-action pairs that appear in $Q_{\\pi_{E}}^{d e m o}(s,a^{\\prime})\\stackrel{\\textstyle{-}}{\\leq}Q_{\\pi_{E}}^{d e m o}(s,a^{*})$ . Considering the worst-case, $\\underline{{p}}\\cdot(\\operatorname*{max}_{s,a}\\boldsymbol{r}_{E}(s,a)+$ $E$ . Then we aim to $\\begin{array}{r l}&{\\frac{\\gamma}{1-\\gamma}\\cdot\\underset{s,a}{\\operatorname*{min}}\\ r_{E}(s,a))+(1-\\underline{{p}})\\cdot\\frac{1}{1-\\gamma}\\cdot\\underset{s,a}{\\operatorname*{max}}r_{E}(s,a)\\leq\\frac{1}{1-\\gamma}\\cdot\\underset{s}{\\operatorname*{min}}\\ r_{E}(s,\\arg\\operatorname*{max}_{a}\\ \\pi_{E}(a|s))\\ g}\\\\ &{\\underline{{p}}\\leq\\frac{1}{\\gamma}\\cdot\\frac{\\underset{s,a}{\\operatorname*{max}}r_{E}(s,a)-\\underset{s}{\\operatorname*{min}}\\ r_{E}(s,a)\\underset{r_{E}(s,a)}{\\operatorname*{max}}\\ \\pi_{E}(a|s))}{\\underset{s,a}{\\operatorname*{max}}r_{E}(s,a)-\\underset{s,a}{\\operatorname*{min}}\\ r_{E}(s,a)}.}\\end{array}$ gives ", "page_idx": 14}, {"type": "text", "text": "Combining the analysis on the state-action pairs that appear in $E$ and not in $E$ , we can conclude that   \nfor $\\epsilon>\\underline{{p}}$ , we can extend the confidence bound $P r o\\bar{b}\\big(\\operatorname*{max}_{(s,a)\\in E}D_{T V}(\\mathcal P(\\cdot\\mid s,a),\\mathcal P_{d e m o}(\\cdot\\mid s,a))\\big)\\geq$   \n$\\begin{array}{r}{\\epsilon)\\leq2\\cdot\\exp(-\\frac{2\\epsilon^{2}}{m})}\\end{array}$ for $(s,a)\\in E$ from $(s,a)\\in E$ to all state-action pairs in $\\mathbb{S}\\times\\mathbb{A}$ . By using a   \nvariable $d$ to represent max $D_{T V}(\\mathcal{P}(\\cdot\\mid s,a)-\\mathcal{P}_{d e m o}(\\cdot\\mid s,a))$ , we can use the conclusion drawn s,a   \nfrom Theorem 5. Note that adding the dummy $\\underline{s}$ does not affect the max $|r_{E}(s,a)|$ since we let s,a   \n$r_{E}(\\underline{{s}},a)\\equiv\\operatorname*{min}_{s,a}r_{E}(s,a)$ . Our proof is complete. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "A.2 Task-Reward Alignment ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide proof of the properties of the task-reward alignment concept that we define in the main text. For readers\u2019 convenience, we include our definition of task for reference. ", "page_idx": 14}, {"type": "text", "text": "Definition 1 (Task) Given the policy hypothesis set $\\Pi$ , a task $(\\Pi,\\preceq_{t a s k},\\Pi_{a c c})$ is specified by a partial order over $\\Pi$ and a non-empty set of acceptable policies $\\Pi_{a c c}\\subseteq\\Pi$ such that $\\forall\\pi_{1}\\in\\Pi_{a c c}$ and $\\bar{\\forall}\\pi_{2}\\notin\\Pi_{a c c}$ , $\\pi_{2}\\preceq_{t a s k}\\pi_{1}$ always hold. ", "page_idx": 14}, {"type": "text", "text": "We have defined in the main text that $\\underline{{U}}_{r}:=\\operatorname*{min}_{\\pi\\in\\Pi_{a c c}}\\,U_{r}(\\pi)$ is the lowest utility achieved by any acceptable policies under $r$ , and $\\overline{{U}}_{r}:=\\operatorname*{max}_{\\pi\\in\\Pi}U_{r}(\\pi)\\;s.t.\\;\\forall\\pi_{1},\\pi_{2}\\in\\Pi,U_{r}(\\pi_{1})\\leq U_{r}(\\pi)\\leq U_{r}(\\pi_{2})\\Rightarrow$ $(\\pi_{1}\\preceq_{t a s k}\\pi)\\wedge(\\pi_{1}\\preceq_{t a s k}\\pi_{2})$ is the highest utility threshold such that any policy achieving a utility higher than $\\overline{{U}}_{r}$ also has a higher order than those of which utilities are lower than ${\\overline{{U}}}_{r}$ . We call a reward function $r$ a $(\\overline{{U}}_{r},\\underline{{U}}_{r})$ -aligned with the task. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Lemma 1. For any task-aligned reward function $r^{+}$ , $\\overline{{U}}_{r^{+}}\\geq\\underline{{U}}_{r^{+}}$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. If $\\overline{{U}}_{r^{+}}\\;<\\;\\underline{{U}}_{r^{+}}$ , then for any policy $\\pi$ with $U_{r}(\\pi)~\\in~[\\overline{{U}}_{r^{+}},\\underline{{U}}_{r^{+}})$ , it is guaranteed that $\\pi\\in\\Pi\\backslash\\Pi_{a c c}$ by definition of $U_{r^{+}}$ . Then, $U_{r^{+}}$ is a better solution for max $U_{r}(\\pi)\\ s.t.\\ \\forall\\pi_{1},\\pi_{2}\\in$ $\\pi\\!\\in\\!\\Pi$ $\\Pi,U_{r}(\\pi_{1})\\leq U_{r}(\\pi)\\leq U_{r}(\\pi_{2})\\Rightarrow(\\pi_{1}\\preceq_{t a s k}\\pi)\\wedge(\\pi_{1}\\preceq_{t a s k}\\pi_{2})$ than $\\overline{{U}}_{r^{+}}$ . Hence, $\\overline{{U}}_{r^{+}}\\geq\\underline{{U}}_{r^{+}}$ must hold. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. If $\\pi_{1}$ \u2aaftask $\\pi_{2}\\,\\Leftrightarrow\\,U_{r}(\\pi_{2})\\,\\leq\\,U_{r}(\\pi_{1})$ holds for any $\\pi_{1},\\pi_{2}\\,\\in\\,\\Pi$ , then $\\overline{{U}}_{r}\\,=\\,\\underline{{U}}_{r}\\,=$ $\\operatorname*{min}_{\\pi\\in\\Pi}U_{r}(\\pi)$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. Since the policy that has the highest order achieves the lowest utility $\\operatorname*{min}_{\\pi\\in\\Pi}U_{r}(\\pi)$ , no other acceptable policy can achieve even lower utility. Hence, by definition $\\underline{{U}}_{r}=\\operatorname*{min}_{\\pi\\in\\Pi}U_{r}(\\pi)$ . ", "page_idx": 15}, {"type": "text", "text": "Another example is that when optimal policy under $r$ has the lowest order in terms of $\\preceq_{t a s k}$ , and the highest-order policy has the lowest utility under $r$ , ${\\overline{{U}}}_{r}=U_{r}$ . In those extreme cases, the reward function can be considered completely misaligned. In the non-extreme cases, if a reward function $r$ exhibits misalignment with the task \u2013 the threshold ${\\overline{{U}}}_{r}$ is lower than the utility of an unacceptable policy \u2013 $\\overline{{U}}_{r}$ then must be also lower than the utilities of all the acceptable policies, which forms a lower-bound for the size of $\\underline{{U}}_{r}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 3. For any reward function $r$ $;\\,i f\\,\\exists\\pi\\not\\in\\Pi_{a c c},U_{r}(\\pi)\\geq\\overline{{U}}_{r},\\,t h e n\\,\\overline{{U}}_{r}\\leq U_{r}.$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Since $\\pi\\notin\\Pi_{a c c}$ and $U_{r}(\\pi)\\geq\\overline{{U}}_{r}$ , then $\\forall\\pi^{\\prime}\\in\\Pi_{a c c},U_{r}(\\pi^{\\prime})\\geq\\overline{{U}}_{r}$ must be true, otherwise $\\exists\\pi^{\\prime}\\in\\Pi_{a c c},\\pi^{\\prime}\\preceq_{t a s k}\\pi$ , which contradicts the definition of $\\Pi_{a c c}$ and $\\overline{{U}}_{r}$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma 4. If a reward function $r$ has $\\overline{{U}}_{r}\\leq\\underline{{U}}_{r}$ , $r$ is a task-misaligned reward function. ", "page_idx": 15}, {"type": "text", "text": "Proof. If $\\overline{{U}}_{r}\\,\\leq\\,\\underline{{U}}_{r}$ , then there must exists two policies $\\pi_{1},\\pi_{2}$ where $\\pi_{1}\\,\\in\\,\\Pi\\backslash\\Pi_{a c c},U_{r}(\\pi_{1})\\,\\in$ $[\\overline{{U}}_{r},\\underline{{U}}_{r}]$ and $U_{r}(\\pi_{2})\\geq\\underline{{{U}}}_{r}\\land\\pi_{2}\\preceq_{t a s k}\\pi_{1}$ . Such $\\pi_{2}$ must not be an acceptable policy. Otherwise, it contradicts the definition of $\\Pi_{a c c}$ . Hence, it must be unacceptable, leading to $r$ being a task-misaligned reward function. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proposition 1 For any two reward functions $r_{1},r_{2}$ , if $\\{\\pi:U_{r_{1}}(\\pi)\\geq\\overline{{U}}_{r_{1}}\\}\\subseteq\\{\\pi:U_{r_{2}}(\\pi)\\geq\\overline{{U}}_{r_{2}}\\}$ , then there must exist a $\\pi_{1}\\,\\in\\,\\{\\pi\\,:\\,U_{r_{1}}(\\pi)\\,\\geq\\,\\overline{{U}}_{r_{1}}\\}$ and a $\\pi_{2}\\,\\in\\,\\{\\pi\\,:\\,U_{r_{2}}(\\pi)\\,\\geq\\,\\overline{{U}}_{r_{2}}\\}$ that satisfy $U_{r_{1}}(\\pi_{2})\\leq U_{r_{1}}(\\pi_{1})$ and $\\pi_{2}$ \u2aaftask $\\pi_{1}$ while $U_{r_{2}}(\\pi_{1})\\leq U_{r_{2}}(\\pi_{2})$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. According to the definition of $\\overline{{U}}_{r_{1}}$ , $\\forall\\pi_{1}\\in\\{\\pi:U_{r_{2}}(\\pi)\\geq\\overline{{U}}_{r_{1}}\\}$ and $\\forall\\pi_{2}\\in\\{\\pi:U_{r_{2}}(\\pi)\\geq$ $\\overline{{U}}_{r_{2}}\\}/\\{\\pi:U_{r_{1}}(\\pi)\\,\\geq\\,\\overline{{U}}_{r_{1}}\\}$ , $U_{r_{1}}(\\pi_{2})\\,\\leq\\,U_{r_{1}}(\\pi_{1})$ and $\\pi_{2}\\preceq_{t a s k}\\pi_{1}$ must be true. Furthermore, if for all pairs of $\\pi_{2}\\,\\in\\,\\{\\pi:U_{r_{2}}(\\pi)\\,\\geq\\,\\overline{{U}}_{r_{2}}\\}/\\{\\pi:U_{r_{1}}(\\pi)\\,\\geq\\,\\overline{{U}}_{r_{1}}\\}$ and $\\pi_{1}\\in\\{\\pi:U_{r_{1}}(\\pi)\\geq\\overline{{U}}_{r_{1}}\\}$ , $U_{r_{2}}(\\pi_{1})\\,>\\,U_{r_{2}}(\\pi_{2})$ is true, then $\\{U_{r_{2}}(\\pi)\\,\\mid\\,U_{r_{1}}(\\pi)\\,\\geq\\,\\overline{{{U}}}_{r_{1}}\\}\\,\\subset\\,\\{\\overline{{{U}}}_{r_{2}},\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\,\\,U_{r_{2}}(\\pi)\\}$ is a smaller non-empty interval than $[\\overline{{U}}_{r_{2}},\\operatorname*{max}_{\\pi\\in\\Pi}\\,U_{r_{2}}(\\pi)]$ , contradicting the fact that $\\overline{{U}}_{r_{2}}$ is the highest utility threshold under $r_{2}$ . Hence, there must exist a $\\pi_{2}\\,\\in\\,\\{\\pi:U_{r_{2}}(\\pi)\\,\\geq\\,\\overline{{U}}_{r_{2}}\\}/\\{\\pi:U_{r_{1}}(\\pi)\\,\\geq\\,\\overline{{U}}_{r_{1}}\\}$ and a $\\pi_{1}\\in\\{\\pi:U_{r_{1}}(\\pi)\\geq\\overline{{U}}_{r_{1}}\\}$ that satisfy $U_{r_{1}}(\\pi_{2})\\,\\leq\\,U_{r_{1}}(\\pi_{1})$ and $U_{r_{2}}(\\pi_{1})\\,\\leq\\,U_{r_{2}}(\\pi_{2})$ while $\\pi_{2}\\preceq_{t a s k}\\pi_{1}$ as aforementioned. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Note that from now on, we use the notation $r^{+}$ to denote task-aligned reward functions and $r^{-}$ to denote task-misaligned reward functions for short. Furthermore, if a reward function $r$ satisfies $U_{r}(\\pi_{1})\\leq U_{r}(\\pi_{2})\\Leftrightarrow\\pi_{1}\\preceq_{t a s k}\\pi_{2}$ , we call this reward function the ground true reward function, and denote it as $r_{t a s k}$ for simplicity. Apparently, any $r_{t a s k}$ is a task-aligned reward function, and it has the most trivial misalignment. ", "page_idx": 15}, {"type": "text", "text": "Proof. By definition, arg \u03c0m\u2208a\u03a0x $U_{r_{t a s k}}(\\pi)$ has higher order and higher utility than any other policies. ", "page_idx": 16}, {"type": "text", "text": "Lemma 6. If $\\pi_{r_{t a s k}}$ is optimal under $r_{t a s k}$ , for any reward function $r$ , $U_{r}(\\pi_{r_{t a s k}})\\geq\\overline{{U}}_{r}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. If ${U_{r}(\\Pi_{a c c})<\\overline{{U}}_{r}}$ , then there exists a $\\pi$ with its utility $U_{r}(\\pi)\\geq\\overline{{U}}_{r}$ , thus $\\pi_{r_{t a s k}}\\preceq_{t a s k}\\pi$ , which contradicts the assumption that $\\pi_{r_{t a s k}}$ is the highest-order policy. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Furthermore, we have the following property. ", "page_idx": 16}, {"type": "text", "text": "Theorem 1 Let $\\mathcal{T}$ be an indicator function. For any $\\begin{array}{r}{k\\geq\\big\\{\\underset{r^{+}}{\\operatorname*{min}}\\,\\sum_{\\pi\\in\\Pi}{\\mathcal{Z}}\\{U_{r^{+}}(\\pi)\\geq U_{r^{+}}(\\pi_{E})\\}\\big\\}}\\end{array}$ , if $\\pi^{*}$ satisfies $\\begin{array}{r}{\\left\\{\\sum_{\\pi\\in\\Pi}\\mathcal{Z}\\{U_{r}(\\pi)\\geq U_{r}(\\pi^{*})\\}\\right\\}<|\\Pi_{a c c}|}\\end{array}$ for all $r\\in R_{E,k}:=\\{r\\mid\\textstyle\\sum_{\\pi\\in\\Pi}{\\mathcal{Z}}\\{U_{r}(\\pi)\\geq\\}$ $U_{r}(\\pi_{E})\\}\\leq k\\}$ , then $\\pi^{*}$ is an acceptable policy, i.e., $\\pi^{*}\\in\\Pi_{a c c}$ . Additionally, if $k<|\\Pi_{a c c}|$ , such an acceptable policy $\\pi^{*}$ is guaranteed to exist. ", "page_idx": 16}, {"type": "text", "text": "Proof. Since $\\pi_{E}$ is an acceptable policy, there must be at least one task-aligned reward function $r^{+}$ that satisfies $|\\{\\pi:U_{r}(\\bar{\\pi)}\\geq U_{r}(\\pi_{E})\\}|\\leq k$ since $k\\geq\\operatorname*{min}_{r^{+}}\\,|\\{\\pi:U_{r^{+}}(\\pi)\\geq U_{r^{+}}(\\pi_{E})\\}|$ . The greater $k$ is, the more task-aligned reward functions tend to be included. If $\\pi^{*}$ achieves $\\mid\\{\\pi~:$ $\\bar{U}_{r^{+}}(\\pi)\\geq U_{r^{+}}(\\pi^{*})\\}<|\\Pi_{a c c}|$ under any task-aligned reward function $r^{+}$ , $\\pi^{*}$ must be acceptable policy. Because if $\\pi^{*}$ is unacceptable, there must be an acceptable policy performing worse than the unacceptable $\\pi^{*}$ under $r^{+}$ , contradicting the definition of task-aligned reward function. Hence, $\\pi^{*}$ must be acceptable policy. Furthermore, for any $\\begin{array}{r}{k\\in[\\underset{r^{+}}{\\operatorname*{min}}\\;\\sum_{\\pi\\in\\Pi}\\overbar{\\mathcal{Z}}\\{U_{r^{+}}(\\pi)\\geq U_{r^{+}}(\\pi_{E})\\},|\\Pi_{a c c}|)}\\end{array}$ , the policy $\\pi_{E}$ itself satisfied $\\begin{array}{r}{\\sum_{\\pi\\in\\Pi}{\\mathcal{Z}}\\{U_{r}(\\pi)\\ \\geq\\ U_{r}(\\pi_{E})\\}\\ <\\ |\\Pi_{a c c}|}\\end{array}$ for all $r\\ \\in\\ R_{E,k}$ , which guarantees the existence of a f easible $\\pi^{*}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "A.3 Semi-supervised Reward Design ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Designing a reward function can be thought as deciding an ordering of policies. We adopt a concept, called total domination, from unsupervised environment design Dennis et al. [2020], and re-interpret this concept in the context of reward design. In this paper, we suppose that the function $U_{r}(\\pi)$ is given to measure the performance of a policy and it does not have to be the utility function. While the measurement of policy performance can vary depending on the free variable $r$ , total dominance can be viewed as an invariance regardless of such dependency. ", "page_idx": 16}, {"type": "text", "text": "Definition 5 (Total Domination). A policy, $\\pi_{1}$ , is totally dominated by some policy $\\pi_{2}$ w.r.t a reward function set $R$ , if for every pair of reward functions $r_{1},r_{2}\\in R$ , $U_{r_{1}}(\\pi_{1})<U_{r_{2}}(\\pi_{2})$ . ", "page_idx": 16}, {"type": "text", "text": "If $\\pi_{1}$ totally dominate $\\pi_{2}$ w.r.t $R$ , $\\pi_{2}$ can be regarded as being unconditionally better than $\\pi_{1}$ . In other words, the two sets $\\{U_{r}(\\pi_{1})\\mid r\\in R\\}$ and $\\{U_{r}(\\pi_{2})\\mid r\\in R\\}$ are disjoint, such that $\\operatorname*{sup}\\{U_{r}(\\pi_{1})\\mid r\\in$ $R\\}<\\operatorname*{inf}\\{U_{r}(\\pi_{2})\\mid{\\dot{r}}\\in R\\}$ . Conversely, if a policy $\\pi$ is not totally dominated by any other policy, it indicates that for any other policy, say $\\pi_{2}$ , $\\operatorname*{sup}\\{U_{r}(\\pi_{1})\\;|\\;r\\in R\\}\\stackrel{\\cdot}{\\geq}\\operatorname*{inf}\\{U_{r}(\\pi_{2})\\;|\\;r\\in R\\}$ . ", "page_idx": 16}, {"type": "text", "text": "Definition 6. A reward function set $R$ aligns with an ordering $\\prec_{R}$ among policies such that $\\pi_{1}\\prec_{R}\\pi_{2}$ if and only if $\\pi_{1}$ is totally dominated by $\\pi_{2}$ w.r.t. $R$ . ", "page_idx": 16}, {"type": "text", "text": "Especially, designing a reward function $r$ is to establish an ordering $\\prec\\{r\\}$ among policies. Total domination can be extended to policy-conditioned reward design, where the reward function $r$ is selected by following a decision rule $\\omega(\\pi)$ such that $\\begin{array}{r}{\\sum_{r\\in R}\\omega(\\pi)(r)\\,=\\,1}\\end{array}$ . We let $U_{\\omega}(\\pi)\\;=\\;$ $\\sum_{r\\in R}\\omega(\\pi)(r)\\cdot U_{r}(\\pi)$ be an affine combination of $U_{r}(\\pi)$ \u2019s with its coefficients specified by $\\omega(\\pi)$ . ", "page_idx": 16}, {"type": "text", "text": "Definition 7. A policy conditioned decision rule $\\omega$ is said to prefer a policy $\\pi_{1}$ to another policy $\\pi_{2}$ , which is notated as $\\pi_{1}\\prec^{\\omega}\\pi_{2}$ , if and only if $U_{\\omega}(\\pi_{1})<U_{\\omega}(\\pi_{2})$ . ", "page_idx": 16}, {"type": "text", "text": "Making a decision rule for selecting reward functions from a reward function set to respect the total dominance w.r.t this reward function set is an unsupervised learning problem, where no additional external supervision is provided. If considering expert demonstrations as a form of supervision and using it to constrain the set $R_{E}$ of reward function via IRL, the reward design becomes semisupervised. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A.4 Solution to the MinimaxRegret ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Without loss of generality, we use $R$ instead of $R_{E,\\delta}$ in our subsequent analysis because solving MinimaxRegret $(R)$ does not depend on whether there are constraints for $R$ . In order to show such an equivalence, we follow the same routine as in Dennis et al. [2020], and start by introducing the concept of weakly total domination. ", "page_idx": 17}, {"type": "text", "text": "Definition 8 (Weakly Total Domination). A policy $\\pi_{1}$ is weakly totally dominated w.r.t a reward function set $R$ by some policy $\\pi_{2}$ if and only if for any pair of reward function $r_{1},r_{2}\\in R,U_{r_{1}}(\\pi_{1})\\leq$ $U_{r_{2}}(\\pi_{2})$ . ", "page_idx": 17}, {"type": "text", "text": "Note that a policy $\\pi$ being totally dominated by any other policy is a sufficient but not necessary condition for $\\pi$ being weakly totally dominated by some other policy. A policy $\\pi_{1}$ being weakly totally dominated by a policy $\\pi_{2}$ implies that $\\operatorname*{sup}\\{U_{r}(\\pi_{1})\\mid r\\in R\\}\\stackrel{}{\\leq}\\operatorname*{inf}\\{U_{r}(\\pi_{2})\\mid r\\in\\bar{R}\\}$ . We assume that there does not exist a policy $\\pi$ that weakly totally dominates itself, which could happen if and only if $U_{r}(\\pi)$ is a constant. We formalize this assumption as the following. ", "page_idx": 17}, {"type": "text", "text": "Assumption 1. For the given reward set $R$ and policy set $\\Pi$ , there does not exist a policy $\\pi$ such that for any two reward functions $r_{1},r_{2}\\in R$ , $U_{r_{1}}(\\pi)=U_{r_{2}}(\\pi)$ . ", "page_idx": 17}, {"type": "text", "text": "This assumption makes weak total domination a non-reflexive relation. It is obvious that weak total domination is transitive and asymmetric. Now we show that successive weak total domination will lead to total domination. ", "page_idx": 17}, {"type": "text", "text": "Lemma 7. for any three policies $\\pi_{1},\\pi_{2},\\pi_{3}\\,\\in\\,\\Pi_{i}$ , if $\\pi_{1}$ is weakly totally dominated by $\\pi_{2},\\ \\pi_{2}$ is weakly totally dominated by $\\pi_{3}$ , then $\\pi_{3}$ totally dominates $\\pi_{1}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. According to the definition of weak total domination, $\\operatorname*{max}_{r\\in R}\\,U_{r}(\\pi_{1})\\;\\leq\\;\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{2})$ and 1 $\\operatorname*{nax}_{\\cdot\\in R}U_{r}(\\pi_{2})\\leq\\operatorname*{min}_{r\\in R}U_{r}(\\pi_{3})$ . If $\\pi_{1}$ is weakly totally dominated but not totally dominated by $\\pi_{3}$ , then 7 $\\operatorname*{max}_{r\\in R}U_{r}(\\pi_{1})=\\operatorname*{min}_{r\\in R}U_{r}(\\pi_{3})$ must be true. However, it implies $\\operatorname*{min}_{r\\in R}U_{r}(\\pi_{2})=\\operatorname*{max}_{r\\in R}U_{r}(\\pi_{2})$ , which violates Assumption 1. We finish the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 8. For the set $\\Pi_{\\neg w t d}\\subseteq\\Pi$ of policies that are not weakly totally dominated by any other policy in the whole set of policies w.r.t a reward function set $R,$ , there exists a range $U\\subseteq\\mathbb{R}$ such that for any policy $\\in\\Pi_{\\neg w t d,\\atop r\\in R}U\\subseteq[\\operatorname*{min}_{r\\in R}U_{r}(\\pi),\\operatorname*{mix}_{r\\in R}U_{r}(\\pi)]$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. For any two policies $\\pi_{1},\\pi_{2}\\in\\Pi_{\\neg w t d}$ , it cannot be true that $\\operatorname*{nax}_{r\\in R}\\,U_{r}(\\pi_{1})\\,=\\,\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{2})$   \nnor $\\operatorname*{min}_{r\\in R}{U_{r}(\\pi_{1})}=\\operatorname*{max}_{r\\in R}{U_{r}(\\pi_{2})}$ , because otherwise one of the policies weakly totally dominates   \nthe other. Without loss of generalization, we assume that $\\operatorname*{nax}_{\\cdot\\in R}\\,U_{r}(\\pi_{1})\\;>\\;\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{2})$ . In this   \ncase, m $\\operatorname*{ax}_{\\bar{\\lambda}}\\,U_{r}(\\pi_{2})\\,>\\,\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{1})$ must also be true, otherwise $\\pi_{1}$ weakly totally dominates $\\pi_{2}$ . r   \nInductively, min max $U_{r}(\\pi)\\,>\\,\\operatorname*{max}_{\\pi\\in\\Pi_{\\neg w t d}}\\,\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi)$ . Letting $u b=\\operatorname*{min}_{\\pi\\in\\Pi_{\\rightarrow w t d}}\\operatorname*{max}_{r\\in R}U_{r}(\\pi)$ and $\\pi\\!\\in\\!\\Pi_{\\neg w t d}$ r\u2208R   \n$l b=\\operatorname*{max}_{\\pi\\in\\Pi_{\\rightarrow w t d}}\\operatorname*{min}_{r\\in R}U_{r}(\\pi)$ , any $U\\subseteq[l b,u b]$ shall support the assertion. We finish the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Lemma 9. For a reward function set $R$ , if a policy $\\pi\\in\\Pi$ is weakly totally dominated by some other policy in $\\Pi$ and there exists a subset $\\Pi_{\\neg w t d}\\subseteq\\Pi$ of policies that are not weakly totally dominated by any other policy in $\\pi$ , then $\\operatorname*{max}_{r\\in R}U_{r}(\\pi)<\\operatorname*{min}_{\\pi^{\\prime}\\in\\Pi_{\\neg w t d}}\\,\\operatorname*{max}_{r\\in R}U_{r}(\\pi^{\\prime})$ ", "page_idx": 17}, {"type": "text", "text": "Proof. If $\\pi_{1}$ is weakly totally dominated by a policy $\\pi_{2}\\,\\in\\,\\Pi$ , then $\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{2})\\,=\\,\\operatorname*{max}_{r\\in R}\\,U_{r}(\\pi)$ . $\\operatorname*{max}_{r\\in R}\\,U_{r}(\\pi)\\;\\geq\\;\\operatorname*{min}_{\\pi^{\\prime}\\in\\Pi_{\\to w t d}}\\;\\operatorname*{max}_{r\\in R}\\,U_{r}(\\pi^{\\prime})$ , then $\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{2})\\;\\geq\\;\\operatorname*{min}_{\\pi^{\\prime}\\in\\Pi_{-w t d}}\\,\\operatorname*{max}_{r\\in R}\\,U_{r}(\\pi^{\\prime})$ , making at least one of the policies in $\\Pi_{\\neg w t d}$ being weakly totally dominated by $\\pi_{2}$ . Hence, rm\u2208aRx $U_{r}(\\pi)<$ $\\operatorname*{min}_{\\pi^{\\prime}\\in\\Pi_{\\neg w t d}}\\operatorname*{max}_{r\\in R}U_{r}(\\pi^{\\prime})$ must be true. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Given a policy $\\pi$ and a reward function $r$ , the regret is represented as Eq.8 ", "page_idx": 18}, {"type": "equation", "text": "$$\nR e g r e t(\\pi,r)\\;\\;:=\\;\\;\\operatorname*{max}_{\\pi^{\\prime}}U_{r}(\\pi^{\\prime})-U_{r}(\\pi)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then we represent the MinimaxRegret $:(R)$ problem in Eq.9. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{M i n i m a x R e g r e t(R)}&{:=}&{\\arg\\underset{\\pi\\in\\Pi}{\\operatorname*{min}}\\left\\{\\underset{r\\in R}{\\operatorname*{max}}\\;R e g r e t(\\pi,r)\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We denote as $r_{\\pi}^{*}\\in R$ the reward function that maximizes $U_{r}(\\pi)$ among all the $r$ \u2019s that achieve the maximization in Eq.9. Formally, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{r_{\\pi}^{*}}&{{}\\in}&{\\arg\\underset{r\\in R}{\\operatorname*{max}}\\;U_{r}(\\pi)\\qquad s.t.\\;r\\in\\arg\\underset{r^{\\prime}\\in R}{\\operatorname*{max}}\\;R e g r e t(\\pi,r^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then MinimaxRegret can be defined as minimizing the worst-case regret as in Eq.9. Next, we want to show that for some decision rule $\\omega$ , the set of optimal policies that maximize $U_{\\omega}$ are the solutions to MinimaxRegret $(R)$ . Formally, ", "page_idx": 18}, {"type": "equation", "text": "$$\nM i n i m a x R e g r e t(R)=\\arg\\operatorname*{max}_{\\pi\\in\\Pi}U_{\\omega}(\\pi)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We design $\\omega$ by letting $\\omega(\\pi)\\,:=\\,\\overline{{\\omega}}(\\pi)\\,\\cdot\\,\\delta_{r_{\\pi}^{*}}\\,+\\,(1-\\overline{{\\omega}}(\\pi))\\,\\cdot\\,\\mathcal{R}_{\\pi}$ where $\\mathcal{R}_{\\pi}\\,\\in\\,\\Delta(R)$ is a policy conditioned distribution over reward functions, $\\delta_{{r}_{\\pi}^{*}}$ be a delta distribution centered at $r_{\\pi}^{*}$ , and $\\overline{{\\omega}}(\\pi)$ is a coefficient. We show how to design $\\mathcal{R}$ by using the following lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 10. Given that the reward function set is $R,$ , there exists a decision rule $\\mathcal{R}:\\Pi\\to\\Delta(R)$ which guarantees that: $^{\\,l}$ ) for any policy $\\pi$ that is not weakly totally dominated by any other policy in \u03a0, i.e., $\\pi\\in\\Pi_{\\lnot w t d}\\subseteq\\Pi_{\\lnot}$ , $U_{\\mathcal{R}}(\\pi)\\equiv c$ where $c=\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi_{\\rightarrow w t d}}\\operatorname*{min}_{r\\in R}U_{r}(\\pi^{\\prime});2)$ for any $\\pi$ that is weakly totally dominated by some policy but not totally dominated by any policy, $U_{\\mathcal{R}}(\\pi)=\\operatorname*{max}_{r\\in R}U_{r}(\\pi);\\,3)$ $i f\\pi$ is totally dominated by some other policy, $\\overline{{\\omega}}(\\pi)$ is a uniform distribution. ", "page_idx": 18}, {"type": "text", "text": "Proof. Since the description of $\\mathcal{R}$ for the policies in condition 2) and 3) are self-explanatory, we omit the discussion on them. For the none weakly totally dominated policies in condition 1), having a constant $U_{\\mathcal{R}}(\\pi)\\equiv c$ is possible if and only if for any policy $\\pi\\in\\Pi_{\\neg w e d}$ $\\mathbf{\\Sigma}_{l},\\,c\\in[\\operatorname*{min}_{r\\in R}U_{r}(\\pi^{\\prime}),\\operatorname*{max}_{r\\in R}\\bar{U}_{r}(\\pi^{\\prime})]$ . As mentioned in the proof of Lemma 8, $c$ can exist within $[\\operatorname*{min}_{r\\in R}~U_{r}(\\pi),\\operatorname*{max}_{r\\in R}~U_{r}(\\pi)]$ . Hence, $c=\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi_{\\rightarrow w t d}}\\operatorname*{min}_{r\\in R}U_{r}(\\pi^{\\prime})$ is a valid assignment. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Then by letting $\\begin{array}{r}{\\overline{{\\omega}}(\\pi):=\\frac{R e g r e t(\\pi,r_{\\pi}^{*})}{c-U_{r_{\\pi}^{*}}(\\pi)}}\\end{array}$ , we have the following theorem. ", "page_idx": 18}, {"type": "text", "text": "Theorem 6. By letting $\\omega(\\pi):=\\overline{{\\omega}}(\\pi)\\cdot\\delta_{r_{\\pi}^{*}}+(1-\\overline{{\\omega}}(\\pi))\\cdot\\mathcal{R}_{\\pi}$ with $\\begin{array}{r}{\\overline{{\\omega}}(\\pi):=\\frac{R e g r e t(\\pi,r_{\\pi}^{*})}{c-U_{r_{\\pi}^{*}}(\\pi)}}\\end{array}$ and any $\\mathcal{R}$ that satisfies Lemma $I O$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\nM i n i m a x R e g r e t(R)=\\arg\\operatorname*{max}_{\\pi\\in\\Pi}U_{\\omega}(\\pi)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. If a policy $\\pi\\,\\in\\,\\Pi$ is totally dominated by some other policy, since there exists another policy with larger $U_{\\omega}$ , $\\pi$ cannot be a solution to arg max $U_{\\omega}(\\pi)$ . Hence, there is no need for further \u03c0\u2208\u03a0 discussion on totally dominated policies. We discuss the none weakly totally dominated policies and the weakly totally dominated but not totally dominated policies (shortened to \"weakly totally dominated\" from now on) respectively. First we expand $\\arg\\operatorname*{max}_{\\pi\\in\\Pi}U_{\\omega}(\\pi)$ as in Eq.13. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\arg\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\,U_{\\omega}(\\pi)}\\\\ {=}&{\\arg\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\sum_{e\\in\\pi}(\\pi)(r)\\cdot U_{r}(\\pi)}\\\\ {=}&{\\arg\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\frac{R e g r e t\\left(\\pi,r_{\\pi}^{*}\\right)\\cdot U_{r_{\\pi}^{*}}\\left(\\pi\\right)+\\left(U_{\\mathcal{R}}\\left(\\pi\\right)-U_{r_{\\pi}^{*}}\\left(\\pi\\right)-R e g r e t\\left(\\pi,r_{\\pi}^{*}\\right)\\right)\\cdot U_{\\mathcal{R}}\\left(\\pi\\right)}{c-U_{r_{\\pi}^{*}}\\left(\\pi\\right)}}\\\\ {=}&{\\arg\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\frac{\\left(U_{\\mathcal{R}}\\left(\\pi\\right)-U_{r_{\\pi}^{*}}\\left(\\pi\\right)\\right)\\cdot U_{\\mathcal{R}}\\left(\\pi\\right)-\\left(U_{\\mathcal{R}}\\left(\\pi\\right)-U_{r_{\\pi}^{*}}\\left(\\pi\\right)\\right)\\cdot R e g r e t\\left(\\pi,r_{\\pi}^{*}\\right)}{c-U_{r_{\\pi}^{*}}\\left(\\pi\\right)}}\\\\ {=}&{\\arg\\underset{\\pi\\in\\Pi}{\\operatorname*{max}}\\frac{U_{\\mathcal{R}}\\left(\\pi\\right)-U_{r_{\\pi}^{*}}\\left(\\pi\\right)}{c-U_{r_{\\pi}^{*}}\\left(\\pi\\right)}\\cdot U_{\\mathcal{R}}\\left(\\pi\\right)-R e g r e t\\left(\\pi,r_{\\pi}^{*}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "1) For the none weakly totally dominated policies, since by design $U_{\\mathcal{R}}\\equiv c$ , Eq.13 is equivalent to arg $\\operatorname*{max}_{\\pi\\in\\Pi_{1}}{-R e g r e t(\\pi,r_{\\pi}^{*})}$ which exactly equals MinimaxRegret $\\cdot(R)$ . Hence, the equivalence holds among the none weakly totally dominated policies. Furthermore, if a none weakly totally dominated policy $\\pi\\in\\Pi_{\\neg w t d}$ achieves optimality in MinimaxRegret $(R)$ , its $U_{\\omega}(\\pi)$ is also no less than any weakly totally dominated policy. Because according to Lemma 9, for any weakly totally dominated policy $\\pi_{1}$ , its $U_{\\mathcal{R}}(\\pi_{1})\\leq c$ , hence $\\begin{array}{r}{\\frac{U_{\\mathcal{R}}(\\pi)-U_{r_{\\pi}^{*}}(\\pi)}{c-U_{r_{\\pi}^{*}}(\\pi)}\\cdot U_{\\mathcal{R}}(\\pi_{1})\\leq c}\\end{array}$ . Since $R e g r e t(\\pi,r_{\\pi}^{*})\\leq$ $R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})$ , $U_{\\omega}(\\pi)\\geq U_{\\omega}(\\pi_{1})$ . Therefore, we can assert that if a none weakly totally dominated policy $\\pi$ is a solution to MinimaxRegret $(R)$ , it is also a solution to arg max $U_{\\omega}(\\pi)$ . Additionally, \u03c0\u2208\u03a0   \nto prove that if a none weakly totally dominated policy $\\pi$ is a solution to arg max $U_{\\omega}(\\pi^{\\prime})$ , it is also a \u03c0\u2032\u2208\u03a0   \nsolution to MinimaxRegret $(R)$ , it is only necessary to prove that $\\pi$ achieve no larger regret than all the weakly totally dominated policies. But we delay the proof to 2). ", "page_idx": 19}, {"type": "text", "text": "2) If a policy $\\pi$ is weakly totally dominated and is a solution to MinimaxRegret $(R)$ , we show that it is also a solution to arg max $U_{\\omega}(\\pi)$ , i.e., its $U_{\\omega}(\\pi)$ is no less than that of any other policy. \u03c0\u2208\u03a0 ", "page_idx": 19}, {"type": "text", "text": "We start by comparing with non weakly totally dominated policy. for any weakly totally dominated policy $\\begin{array}{l l l l}{\\pi_{1}}&{\\in}&{M i n i m a x R e g r e t(R)}\\end{array}$ , it must hold true that $\\begin{array}{r l}{R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})}&{{}\\leq}\\end{array}$ $R e g r e t(\\pi_{2},r_{\\pi_{2}}^{*})$ for any $\\pi_{2}\\mathrm{~\\ensuremath~{~\\in~}~}\\pi$ that weakly totally dominates $\\pi_{1}$ . However, it also holds that $R e\\bar{g}r e t(\\pi_{2},r_{\\pi_{2}}^{*})\\ \\leq\\ R e g r e t(\\pi_{1},r_{\\pi_{2}}^{*})$ due to the weak total domination. Therefore, $R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})\\ =\\ R e g r e t(\\pi_{2},r_{\\pi_{2}}^{*})\\ =\\ R e g r e t(\\pi_{1},r_{\\pi_{2}}^{*})$ , implying that $\\pi_{2}$ is also a solution to $M i n i m a x R e g r e t(R)$ . It also implies that $U_{r_{\\pi_{2}}^{*}}(\\pi_{1}\\tilde{)}~=~U_{r_{\\pi_{2}}^{*}}(\\pi_{2})~\\geq~U_{r_{\\pi_{1}}^{*}}(\\pi_{1})$ due to the weak total domination. However, by definition $U_{r_{\\pi_{1}}^{*}}(\\pi_{1})~\\geq~U_{r_{\\pi_{2}}^{*}}(\\pi_{1})$ . Hence, $U_{r_{\\pi_{1}}^{*}}(\\pi_{1})~=$ $U_{r_{\\pi_{2}}^{*}}(\\pi_{1})\\,=\\,U_{r_{\\pi_{2}}^{*}}(\\pi_{2})$ must hold. Now we discuss two possibilities: a) there exists another policy $\\pi_{3}$ that weakly totally dominates $\\pi_{2}$ ; b) there does not exist any other policy that weakly totally dominates $\\pi_{2}$ . First, condition a) cannot hold. Because inductively it can be derived $U_{r_{\\pi_{1}}^{*}}(\\pi_{1})=U_{r_{\\pi_{2}}^{*}}(\\pi_{1})=U_{r_{\\pi_{2}}^{*}}(\\pi_{2})=U_{r_{\\pi_{3}}^{*}}(\\pi_{3})$ , while Lemma 7 indicates that $\\pi_{3}$ totally dominates $\\pi_{1}$ , which is a contradiction. Hence, there does not exist any policy that weakly totally dominates $\\pi_{2}$ , meaning that condition b) is certain. We note that $U_{r_{\\pi_{1}}^{*}}(\\pi_{1})=U_{r_{\\pi_{2}}^{*}}(\\pi_{1})=U_{r_{\\pi_{2}}^{*}}(\\pi_{2})$ and the weak total domination between $\\pi_{1},\\pi_{2}$ imply that $r_{\\pi_{1}}^{*},r_{\\pi_{2}}^{*}\\in$ arg $U_{r}(\\pi_{1})$ , $r_{\\pi_{2}}^{*}\\ \\stackrel{-}{\\in}\\ \\arg\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{2}),$ $r\\!\\in\\!R$ and thus $\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{2})\\;\\leq\\;\\operatorname*{max}_{\\pi\\in\\Pi_{-w t d}}\\,\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi)\\;=\\;c$ . Again, $\\pi_{1}\\ \\in\\ M i n i m a x R e g r e t(R)$ makes $R e g r e t(\\pi_{1},r_{\\pi}^{*})\\,\\le\\,R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})\\,\\le\\,R e g r e t(\\pi,r_{\\pi}^{*})$ not only hold for $\\pi\\,=\\,\\pi_{2}$ but also for any other policy $\\pi\\;\\in\\;\\Pi_{\\neg w t d}$ , then for any policy $\\pi\\;\\in\\;\\Pi_{\\neg w t d}$ , $U_{r_{\\pi}^{*}}(\\pi_{1})\\;\\geq\\;U_{r_{\\pi}^{*}}(\\pi)\\;\\geq\\;\\operatorname*{min}_{r\\in R}\\:U_{r}(\\pi)$ . Hence, $U_{r_{\\pi}^{*}}(\\pi_{1})~\\geq~\\operatorname*{max}_{\\pi\\in\\Pi_{\\rightarrow w t d}}~\\operatorname*{min}_{r\\in R}~U_{r}(\\pi)~=~c$ . Since $U_{r_{\\pi}^{*}}(\\pi_{1})\\ =\\ \\operatorname*{min}_{r\\in R}\\ U_{r}(\\pi_{2})$ as aforementioned, $\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi_{2})\\;>\\;\\operatorname*{max}_{\\pi\\in\\Pi_{\\rightarrow w t d}}\\,\\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi)$ will cause a contradiction. Hence, $\\operatorname*{min}_{r\\in R}~U_{r}(\\pi_{2})~=$ $\\operatorname*{max}_{\\pi\\in\\Pi_{\\neg w t d}}\\;\\operatorname*{min}_{r\\in R}\\;U_{r}(\\pi)\\;=\\;c$ . As a result, $U_{\\mathcal{R}}(\\pi)\\;=\\;U_{r_{\\pi}^{*}}(\\pi)\\;=\\;\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi_{\\rightarrow w t d}}\\ \\operatorname*{min}_{r\\in R}\\,U_{r}(\\pi^{\\prime})\\;=\\;c$ , and $\\begin{array}{r}{U_{\\omega}(\\pi)=c-R e g r e t(\\pi,r_{\\pi}^{*})\\geq\\underset{\\pi^{\\prime}\\in\\Pi_{-w t d}}{\\operatorname*{max}}c-R e g r e t(\\pi^{\\prime},r_{\\pi^{\\prime}}^{*})=\\underset{\\pi^{\\prime}\\in\\Pi_{-w t d}}{\\operatorname*{max}}U_{\\omega}(\\pi^{\\prime}).}\\end{array}$ In other words, if a weakly totally dominated policy $\\pi$ is a solution to $M i n i m a x R e g r e t(R)$ , then its $U_{\\omega}(\\pi)$ is no less than that of any non weakly totally dominated policy. This also complete the proof at the end of ", "page_idx": 19}, {"type": "text", "text": "1), because if a none weakly totally dominated policy $\\pi_{1}$ is a solution to $\\arg\\operatorname*{max}_{\\pi\\in\\Pi}U_{\\omega}(\\pi)$ but not a solution to MinimaxRegret $(R)$ , then $R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})>0$ and a weakly totally dominated policy $\\pi_{2}$ must be the solution to MinimaxRegret $(R)$ . Then, $\\begin{array}{r}{U_{\\omega}(\\pi_{2})=c>c\\!-\\!R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})=U_{\\omega}(\\pi_{1}),}\\end{array}$ , which, however, contradicts $\\pi_{1}\\in\\arg\\operatorname*{max}_{\\pi\\in\\Pi}U_{\\omega}(\\pi).$ . ", "page_idx": 20}, {"type": "text", "text": "It is obvious that a weakly totally dominated policy $\\pi\\in M i n i m a x R e g r e t(R)$ has a $U_{\\omega}(\\pi)$ no less than any other weakly totally dominated policy. Because for any other weakly totally dominated policy $\\pi_{1}$ , $U_{\\mathcal{R}}(\\pi_{1})\\leq c$ and $\\bar{R e g r e t}(\\pi_{1},r_{\\pi_{1}}^{*})\\stackrel{<}{\\leq}R e g r e t(\\pi,r_{\\pi}^{*})$ , hence $U_{\\omega}(\\pi_{1})\\stackrel{.}{\\leq}U_{\\omega}(\\dot{\\pi})$ according to Eq.13. ", "page_idx": 20}, {"type": "text", "text": "So far we have shown that if a weakly totally dominated policy $\\pi$ is a solution to   \nMinimaxRegret ${\\bf\\nabla}\\!\\cdot\\!(R)$ , it is also a solution to arg max $U_{\\omega}(\\pi^{\\prime})$ . Next, we need to show that the $\\pi^{\\prime}\\!\\in\\!\\Pi$   \nreverse is also true, i.e., if a weakly totally dominated policy $\\pi$ is a solution to arg max $U_{\\omega}(\\pi)$ , it \u03c0\u2208\u03a0   \nmust also be a solution to MinimaxRegret $:(R)$ . In order to prove its truthfulness, we need to show   \nthat if $\\pi\\not\\in M i n i m a x R e g r e t(R)$ , whether there exists: a) a none weakly totally dominated policy   \n$\\pi_{1}$ , or b) another weakly totally dominated policy $\\pi_{1}$ , such that $\\pi_{1}\\,\\in\\,M i n i m a x R e g r e t(R)$ and   \n$U_{\\omega}(\\pi_{1})\\,\\le\\,U_{\\omega}(\\pi)$ . If neither of the two policies exists, we can complete our proof. Since it has   \nbeen proved in 1) that if a none weakly totally dominated policy achieves MinimaxRegret $(R)$ , it   \nalso achieves arg max $U_{\\omega}(\\pi^{\\prime})$ , the policy described in condition a) does not exist. Hence, it is only \u03c0\u2032\u2208\u03a0   \nnecessary to prove that the policy in condition b) also does not exist. ", "page_idx": 20}, {"type": "text", "text": "If such weakly totally dominated policy $\\pi_{1}$ exists, $\\pi\\;\\;\\notin\\;\\;M i n i m a x R e g r e t(R)$ and $\\pi_{1}~~\\in$ MinimaxRegret ${\\bf\\nabla}\\!\\cdot\\!(R)$ indicates $R e g r e t(\\pi,r_{\\pi}^{*})\\ >\\ R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})$ . Since $U_{\\omega}(\\pi_{1})~\\geq~U_{\\omega}(\\pi)$ , according to Eq.13, $\\begin{array}{r}{U_{\\omega}(\\pi_{1})~=~c-~R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})~\\le~U_{\\omega}(\\pi)~=~\\frac{U_{\\mathcal R}(\\pi)-U_{r_{\\pi}^{*}}(\\pi)}{c-U_{r_{\\pi}^{*}}(\\pi)}\\,\\cdot\\,U_{\\mathcal R}(\\pi)~-~}\\end{array}$ $R e g r e t(\\pi,r_{\\pi}^{*})$ . Thus URc(\u2212\u03c0)U\u2212r\u2217U(r\u03c0\u2217\u03c0 )(\u03c0)(\u03c0) \u00b7 UR \u2265c + Regret(\u03c0, r\u2217\u03c0) \u2212Regret(\u03c01, r\u2217\u03c01) > c, which is impossible due to $U_{\\mathcal{R}}~\\leq~c$ . Therefore, such $\\pi_{1}$ also does not exist. In fact, this can be reasoned from another perspective. If there exists a weakly totally dominated policy $\\pi_{1}$ with $U_{r_{\\pi_{1}}^{*}}(\\pi_{1})=c=U_{r_{\\pi}^{*}}(\\pi)$ but $\\pi_{1}\\not\\in M i n i m a x R e g r e t(R)$ , then $R e g r e t(\\pi,r_{\\pi}^{*})>R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*})$ . It also indicates max \u03c0\u2032\u2208\u03a0 $U_{r_{\\pi}^{*}}(\\pi^{\\prime})>\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r_{\\pi_{1}}^{*}}(\\pi^{\\prime})$ . Meanwhile, $R e g r e t(\\pi_{1},r_{\\pi}^{*}):=\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r_{\\pi}^{*}}(\\pi^{\\prime})\\,-$ $U_{r_{\\pi}^{*}}(\\pi_{1})\\le R e g r e t(\\pi_{1},r_{\\pi_{1}}^{*}):=\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r_{\\pi_{1}}^{*}}(\\pi^{\\prime})-U_{r_{\\pi_{1}}^{*}}(\\pi_{1}):=\\operatorname*{max}_{r\\in R}\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r}(\\pi^{\\prime})-U_{r}(\\pi_{1})$ ) indicates $\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r_{\\pi}^{*}}(\\pi^{\\prime})-\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r_{\\pi_{1}}^{*}}(\\pi^{\\prime})\\le U_{r_{\\pi}^{*}}(\\pi_{1})-U_{r_{\\pi_{1}}^{*}}(\\pi_{1}).$ However, we have proved that, for a weakly totally dominated policy, $\\pi_{1}\\in\\mathit{M i n i m a x R e g r e t}(R)$ indicates $U_{r_{\\pi_{1}}^{*}}(\\pi_{1})=\\operatorname*{max}_{r\\in R}U_{r}(\\pi_{1})$ . Hence, $\\pi^{\\prime}\\!\\in\\!\\Pi$ $U_{r_{\\pi}^{*}}(\\pi^{\\prime})-\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}\\,U_{r_{\\pi_{1}}^{*}}(\\pi^{\\prime})\\,\\le\\,U_{r_{\\pi}^{*}}(\\pi_{1})-U_{r_{\\pi_{1}}^{*}}(\\pi_{1})\\,\\le\\,0$ and it contradicts \u03c0m\u2032\u2208a\u03a0x $U_{r_{\\pi}^{*}}(\\pi^{\\prime})>$ $\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r_{\\pi_{1}}^{*}}(\\pi^{\\prime})$ . Therefore, such $\\pi_{1}$ does not exist. In summary, we have exhausted all conditions and can assert that for any policies, being a solution to MinimaxRegret $(R)$ is equivalent to a solution to arg $\\operatorname*{max}_{\\pi\\in\\Pi}U_{\\omega}(\\pi)$ . We complete our proof. ", "page_idx": 20}, {"type": "text", "text": "A.5 Collective Validation of Similarity Between Expert and Agent ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Definition 2 and our definition of Regret in Eq.2, we use the utility function $U_{r}$ to measure the performance of a policy. We now show that we can replace $U_{r}$ with other functions. ", "page_idx": 20}, {"type": "text", "text": "Lemma 11. The solution of MinimaxRegret $(R_{E,\\delta^{*}})$ does not change when $U_{r}$ in MinimaxRegret is replace with $U_{r}(\\pi)-f(r)$ where $f$ can be arbitrary function of $r$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. When using $U_{r}(\\pi)\\,-\\,f(r)$ instead of $U_{r}(\\pi)$ to measure the policy performance, solving MinimaxRegret $(R)$ is to solve Eq. 14, which is the same as Eq.9. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{M i m i m a x R e g r e t(R)}&{=}&{\\underset{\\pi\\in\\Pi}{\\arg\\operatorname*{max}}\\ \\underset{r\\in R}{\\operatorname*{min}}\\ R e g r e t(\\pi,r)}\\\\ &{=}&{\\underset{\\pi\\in\\Pi}{\\arg\\operatorname*{max}}\\ \\underset{r\\in R}{\\operatorname*{min}}\\ \\underset{\\pi^{\\prime}\\in\\Pi}{\\operatorname*{max}}\\left\\{U_{r}(\\pi^{\\prime})-f(r)\\right\\}-\\left(U_{r}(\\pi)-f(r)\\right)}\\\\ &{=}&{\\underset{\\pi\\in\\Pi}{\\arg\\operatorname*{max}}\\ \\underset{r\\in R}{\\operatorname*{min}}\\ \\underset{\\pi^{\\prime}\\in\\Pi}{\\operatorname*{max}}\\ U_{r}(\\pi^{\\prime})-U_{r}(\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma 11 implies that we can use the policy-expert margin $U_{r}(\\pi)-U_{r}(E)$ as a measurement of policy performance. This makes the rationale of using PAGAR-based IL for collective validation of similarity between $E$ and $\\pi$ more intuitive. ", "page_idx": 21}, {"type": "text", "text": "A.6 Criterion for Successful Policy Learning ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To analyze the sufficient conditions for MinimaxRegret to mitigate task-reward misalignment, we start by analyzing the general properties of MinimaxRegret on arbitrary input $R$ . ", "page_idx": 21}, {"type": "text", "text": "Proposition 4. If the following conditions $(I)$ (2) hold for $R$ , then the optimal protagonist policy   \n$\\pi_{P}:=M i n i m a x R e g r e t(R)$ satisfies $\\forall r^{+}\\in R,U_{r^{+}}(\\pi_{P})\\geq U_{r^{+}}$ . (1) There exists $r^{+}\\in R,$ , and $\\mathrm{~\\underset{}{'\\ m a x}~}\\{\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{+}}(\\pi)-\\bar{U}_{r^{+}}\\}<\\operatorname*{min}_{r^{+}\\in R}\\{\\bar{U}_{r^{+}}-U_{r^{+}}\\};$ (2) There exists a policy $\\pi^{*}$ such that $\\forall r^{+}\\ \\ \\in\\ \\ R$ , $U_{r^{+}}(\\pi^{*})~~\\geq~~\\bar{U}_{r^{+}}$ , and $\\forall r^{-}\\:\\:\\in\\:\\:R,$ , $R e g r e t(\\pi^{*},r^{-})<\\operatorname*{min}_{r^{+}\\in R}\\left\\{\\bar{U}_{r^{+}}-U_{r^{+}}\\right\\}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Suppose the conditions are met, and a policy $\\pi_{1}$ satisfies the property described in conditions 2). Then for any policy $\\pi_{2}\\,\\in\\,M i n i m a x R e g r e t(R)$ , if $\\pi_{2}$ does not satisfy the mentioned property, there exists a task-aligned reward function $r^{+}~\\in~R$ such that $U_{r^{+}}(\\bar{\\pi}_{2})\\;\\leq\\;\\underline{{{U}}}_{r^{+}}$ . In this case $R e g r e t(\\pi_{2},r^{+})\\;=\\;\\operatorname*{max}_{\\pi\\in\\Pi}\\,U_{r^{+}}(\\pi)\\,-\\,U_{r^{+}}(\\pi_{2})\\;\\ge\\;\\overline{{U}}_{r^{+}}\\,-\\,\\underline{{U}}_{r^{+}}\\;\\ge\\;\\operatorname*{min}_{r^{+}\\!\\prime\\in{\\cal R}}\\,\\overline{{U}}_{r^{+}}\\,-\\,\\underline{{U}}_{r^{+}},$ . However, for $\\pi_{1}$ , it holds for any task-aligned reward function ${\\hat{r}}^{+}\\,\\in\\,R$ that $R e g r e t(\\pi_{1},\\hat{r}^{+})\\leq$ 7 $\\operatorname*{max}_{\\tau\\in\\Pi}U_{\\widehat{r}^{+}}\\left(\\pi\\right)-\\overline{{U}}_{\\widehat{r}^{+}}\\le\\operatorname*{max}_{r^{+}\\in R}\\,\\widetilde{\\{\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{+}}(\\pi)-\\overline{{U}}_{r^{+}}\\}}<\\operatorname*{min}_{r^{+}\\!\\pi^{\\prime}\\in R}\\,\\{\\overline{{U}}_{r^{+}}\\!\\cdot\\!-\\!\\underline{{U}}_{r^{+}}\\}\\le\\bar{R e g r e t}(\\pi_{2},r^{+}),$ and it also holds for any misaligned reward function $r^{-}\\in R$ that $R e g r e t(\\pi_{1},r^{-})<\\operatorname*{min}_{r^{+^{\\prime}}\\in R}\\left\\{\\overline{{U}}_{r^{+^{\\prime}}}-\\right.$ $\\underline{{{U}}}_{r^{+}},\\underline{{{\\O}}}\\;\\le\\;R e g r e t(\\pi_{2},\\hat{r}^{+})$ . Hence, $R e g r e t(\\pi_{1},r^{+})\\ <\\ R e g r e t(\\pi_{2},r^{+})$ , contradicting $\\pi_{2}~~\\in$ MinimaxRegret $(R)$ . We complete the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "In Proposition 4, condition (1) states that the task-aligned reward functions in $R$ all have a low extent of misalignment while condition (2) states that there exists a $\\pi^{*}$ that not only performs well under all $r^{+}$ \u2019s (thus being acceptable in the task) but also achieves relatively low regret under all $r^{-}$ \u2019s. Note that the more aligned the $r^{+}$ \u2019s, the more forgiving the tolerance for high regret on $r^{-}$ . Furthermore, Proposition 5 shows that, under a stronger condition on the existence of a policy $\\pi^{*}$ performing well under all reward functions in $R$ , MinimaxRegret $(R)$ can guarantee to induce an acceptable policy, i.e., satisfying the condition (2) in Definition 3. ", "page_idx": 21}, {"type": "text", "text": "Proposition 5 (Strong Acceptance). Assume that condition $(I)$ in Proposition $^{4}$ is satisfied. In addition, if there exists a policy $\\pi^{*}$ such that $\\forall r\\in R$ , Regret(\u03c0\u2217, r) < max r+\u2208R  \u03c0\u2208\u03a0 $\\{\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{+}}(\\pi)-\\bar{U}_{r^{+}}\\}$ , then the optimal protagonist policy $\\pi_{P}:=M i n i m a x R e g r e t(R)$ satisfies $\\forall r^{+}\\in R_{:}$ , $U_{r^{+}}(\\pi_{P})\\geq$ U\u00afr+. ", "page_idx": 21}, {"type": "text", "text": "Proof. Since maxmax $U_{r}(\\pi)-U_{r}(\\pi_{P})\\leq m a x$ maxUr(\u03c0) \u2212Ur(\u03c0\u2217) < max{maxUr+(\u03c0) \u2212 r\u2208R \u03c0 \u03c0 r+\u2208R  \u03c0\u2208\u03a0 $\\overline{{U}}_{r^{+}}\\}$ , we can conclude that for any $r^{+}\\in R$ , $U_{r^{+}}(\\pi_{P})\\geq\\overline{{U}}_{r^{+}}$ . The proof is complete. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Note that the assumptions in Proposition 4 and 5 are not trivially satisfiable for arbitrary $R$ , e.g., if $R$ contains two reward functions with opposite signs, i.e., $r,-r\\in R$ , no policy can perform well under both $r$ and $-r$ . However, in PAGAR-based $\\mathrm{IL}$ , using $R_{E,\\delta}$ in place of arbitrary $R$ is equivalent to using $E$ and $\\delta$ to constrain the selection of reward functions, which can lead to additional implications. ", "page_idx": 21}, {"type": "text", "text": "Theorem 2. (Weak Acceptance) If the following conditions (1) (2) hold for $R_{E,\\delta}$ , then the optimal   \nprotagonist policy $\\pi_{P}:=M i n i m a x R e g r e t(R_{E,\\delta})$ satisfies $\\forall r^{+}\\in R_{E,\\delta}$ , $U_{r^{+}}(\\pi_{P})\\geq\\underline{{U}}_{r^{+}}$ . (1) The condition (1) in Proposition 4 holds (2) $\\begin{array}{r l}&{\\forall r^{+}\\,\\in\\,R_{E,\\delta},\\,L_{r^{+}}\\cdot W_{E}-\\delta\\,\\leq\\displaystyle\\operatorname*{max}_{\\pi\\in\\Pi}\\,U_{r^{+}}(\\pi)-\\overline{{U}}_{r^{+}}\\mathrm{~and~}\\forall r^{-}\\,\\in\\,R_{E,\\delta},\\,L_{r^{-}}\\cdot W_{E}-\\delta\\,<\\,0}\\\\ &{\\underset{r^{+}\\in R_{E,\\delta}}{\\operatorname*{min}}\\,\\ \\ \\{\\overline{{U}}_{r^{+}}-\\underline{{U}}_{r^{+}}\\}.}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. We consider $U_{r}(\\pi)=\\mathbb{E}_{\\tau\\sim\\pi}[r(\\tau)]$ . Since $\\begin{array}{r}{W_{E}\\triangleq\\underset{\\pi\\in\\Pi}{\\mathrm{min}}\\,W_{1}(\\pi,E)=\\frac{1}{K}\\underset{|r|_{L}\\leq K}{\\operatorname*{sup}}\\,U_{r}(E)-U_{r}(\\pi)}\\end{array}$ for any $K>0$ , let $\\pi^{*}$ be the policy that achieves the minimality in $W_{E}$ . Then for any $r^{+}\\in R$ , the $\\dot{L}_{r^{+}}\\cdot W_{E}-\\delta\\geq L_{r^{+}}\\cdot\\frac{\\star_{1}}{L_{r^{+}}}\\operatorname*{sup}_{|r|_{L}\\leq L_{r^{+}}}U_{r}(E)-U_{r}(\\pi)-\\delta\\stackrel{\\star}{\\geq}U_{r^{+}}(\\overline{{E}})-U_{r^{+}}(\\pi)\\stackrel{\\star}{-}(U_{r^{+}}(E)-U_{r^{+}}(E))$ max $U_{r^{+}}(\\pi^{\\prime}))\\,=\\,\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}\\,U_{r^{+}}(\\pi^{\\prime})\\,-\\,U_{r^{+}}(\\pi)$ . Hence, for all $r^{+}\\,\\in\\,R$ , max $U_{r^{+}}(\\pi^{\\prime})\\,-\\,U_{r^{+}}(\\pi)\\,<$ < \u03c0\u2032\u2208\u03a0 \u03c0\u2032\u2208\u03a0 max $U_{r^{+}}(\\pi^{\\prime})-\\bar{U}_{r^{+}}$ , i.e., $U_{r^{+}}(\\pi^{*})\\geq\\bar{U}_{r^{+}}$ . Likewise, $L_{r^{-}}\\cdot W_{E}-\\delta<\\operatorname*{min}_{r^{+}\\in R_{E,\\delta}}\\overline{{U}}_{r^{+}}-\\underline{{U}}_{r^{+}}$ indicates \u03c0\u2032\u2208\u03a0 that for all $r^{-}\\in R,\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r^{+}}(\\pi^{\\prime})-U_{r^{+}}(\\pi)<\\operatorname*{min}_{r^{+}\\in R_{E,\\delta}}\\overline{{U}}_{r^{+}}-\\underline{{U}}_{r^{+}}$ . Then, we have recovered the condition (2) in Proposition 4. As a result, we deliver the same guarantees in Proposition 4. \u518f\u53e3 ", "page_idx": 22}, {"type": "text", "text": "Theorem 2 delivers the same guarantee as that of Proposition 4 but differs from Proposition 4 in that Condition (2) implicitly requires that for the policy $\\pi^{*}=\\arg\\operatorname*{min}_{\\pi\\in\\Pi}W_{1}(\\pi,E)$ , the performance difference between $E$ and $\\pi^{*}$ is small enough under all $r\\in R_{E,\\delta}$ . ", "page_idx": 22}, {"type": "text", "text": "Theorem 3. (Strong Acceptance) Assume that the condition (1) in Theorem 4 holds for $R_{E,\\delta}$ . If for any $r\\in R_{E,\\delta}$ , $,L_{r}\\cdot W_{E}-\\delta\\leq\\operatorname*{min}_{r^{+}\\in R_{E,\\delta}}\\{\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{+}}(\\pi)-\\overline{{U}}_{r^{+}}\\}$ , then the optimal protagonist policy $\\pi_{P}=M i n i m a x R e g r e t(R_{E,\\delta})$ satisfies $\\forall r^{+}\\in R_{E,\\delta},U_{r^{+}}(\\pi_{P})\\geq\\overline{{U}}_{r^{+}}.$ ", "page_idx": 22}, {"type": "text", "text": "Proof. Again, we let $\\pi^{*}$ be the policy that achieves the minimality in $W_{E}$ . Then, we have $L_{r}\\cdot W_{E}-$ $\\delta\\geq L_{r}\\cdot\\frac{\\daleth}{L_{r}}\\operatorname*{sup}_{\\substack{\\mid r\\mid\\,\\,L\\leq L_{r}}}U_{r}(E)-\\bar{U}_{r}(\\pi^{*})-(U_{r^{+}}(E)-\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r^{+}}(\\bar{\\pi^{\\prime}}))\\geq\\operatorname*{max}_{\\pi^{\\prime}\\in\\Pi}U_{r^{+}}(\\pi^{\\prime})-U_{r^{+}}(\\pi^{*})$ for any $r\\in R_{E,\\delta}$ . We have recovered the condition in Proposition 5. The proof is complete. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "A.7 Stationary Solutions ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we show that MinimaxRegret is convex for $\\pi_{P}$ . ", "page_idx": 22}, {"type": "text", "text": "Proposition 6. $\\operatorname*{max}_{\\boldsymbol{r\\in R}}R e g r e t(\\pi_{P},\\boldsymbol{r})$ is convex in $\\pi_{P}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. For any $\\alpha\\in[0,1]$ and $\\pi_{P,1},\\pi_{P,2}$ , there exists a $\\pi_{P,3}=\\alpha\\pi_{P,1}+(1-\\alpha)\\pi_{P,2}$ . Let $r_{1},\\pi_{A,1}$ and $r_{2},\\pi_{A,2}$ be the optimal reward and antagonist policy for $\\pi_{P,1}$ and $\\pi_{P,2}$ Then $\\alpha\\cdot(\\operatorname*{max}_{r\\in R}\\,\\operatorname*{max}_{\\pi_{A}\\in\\Pi}\\,U_{r}(\\pi_{A})-$ $\\begin{array}{r}{J_{r}(\\pi_{P,1}))\\,+\\,(1\\,-\\,\\alpha)\\,\\cdot\\,(\\underset{r\\,\\in{\\cal R}}{\\operatorname*{max}}\\,\\underset{\\pi_{\\triangleq}\\,\\in\\,\\Pi}{\\operatorname*{max}}\\,\\,U_{r}(\\pi_{A})\\,-\\,U_{r}(\\pi_{P,2}))\\,=\\,\\alpha(U_{r_{1}}(\\pi_{A,1})\\,-\\,\\hat{U}_{r_{1}}(\\tilde{\\pi}_{P,1}))\\,+\\,(1\\,-\\,\\alpha)\\,(\\beta_{1}\\,-\\,\\alpha)\\,(\\beta_{1}\\,-\\,\\alpha)\\,\\cdot\\,(\\pi_{P,2})\\,=\\,\\alpha(U_{r_{1}}(\\pi_{A,1})\\,-\\,\\alpha)\\,.}\\end{array}$ \u03b1)(Ur2(\u03c0A,2) \u2212Ur2(\u03c0P,2)) \u2265\u03b1(Ur3(\u03c0A,3) \u2212Ur3(\u03c0P,1)) + (1 \u2212\u03b1)(Ur2(\u03c0A,3) \u2212Ur3(\u03c0P,2)) = $U_{r_{3}}(\\pi_{A,3})-U_{r_{3}}(\\pi_{P,3})$ . Therefore, $\\operatorname*{max}_{r\\in R}\\,\\operatorname*{max}_{\\pi_{A}\\in\\Pi}\\,U_{r}(\\pi_{A})-U_{r}(\\pi_{P})$ is convex in $\\pi_{P}$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "A.8 Compare PAGAR-Based IL with IRL-Based IL ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Assumption 2. max $\\mathcal{I}_{I R L}(r)$ can reach Nash Equilibrium at an optimal reward function $r^{*}$ and its optimal policy $\\pi_{r^{*}}$ . ", "page_idx": 22}, {"type": "text", "text": "We make this assumption only to demonstrate how PAGAR-based $\\mathrm{IL}$ can prevent performance degradation w.r.t IRL-based $\\mathrm{IL}$ , which is preferred when IRL-based $\\mathrm{IL}$ does not have a reward misalignment issue under ideal conditions. We draw two assertions from this assumption. The first one considers Maximum Margin IRL-based IL and shows that if using the optimal reward function set $R_{E,\\delta^{*}}$ as input to MinimaxRegret, PAGAR-based $\\mathrm{IL}$ and Maximum Margin IRL-based IL have the same solutions. ", "page_idx": 22}, {"type": "text", "text": "Proposition 7. $\\pi_{r^{*}}=M i n i m i a x R e g r e t(R_{E,\\delta^{*}}).$ ", "page_idx": 22}, {"type": "text", "text": "Proof. The reward function set $R_{E,\\delta^{*}}$ and the policy set $\\Pi_{a c c}$ achieving Nash Equilibrium for $\\arg\\operatorname*{min}_{r\\in R}\\,J_{I R L}(r)$ indicates that for any $r\\in R_{E,\\delta^{*}},\\pi\\in\\Pi_{a c c},\\pi\\in\\arg\\operatorname*{max}_{\\pi\\in\\Pi}U_{r}(\\pi)-U_{r}(E)$ . Then $\\Pi_{a c c}$ will be the solution to ar $\\operatorname*{max}_{\\pi_{P}\\in\\Pi}\\operatorname*{min}_{r\\in R_{E,\\delta^{*}}}\\ \\left\\{\\operatorname*{max}_{\\pi_{A}\\in\\Pi}U_{r}(\\pi_{A})-U_{r}(E)\\right\\}-\\left(U_{r}(\\pi_{P})-U_{r}(E)\\right)$ ", "page_idx": 22}, {"type": "text", "text": "because the policies in $\\Pi_{a c c}$ achieve zero regret. Then Lemma 11 states that $\\Pi_{a c c}$ will also be the solution to arg max min $\\left\\{\\operatorname*{max}_{\\pi_{A}\\in\\Pi}U_{r}(\\pi_{A})\\right\\}-U_{r}(\\pi_{P})$ . We finish the proof. \u53e3 \u03c0P \u2208\u03a0r\u2208RE,\u03b4\u2217 ", "page_idx": 23}, {"type": "text", "text": "The proof can be found in Appendix A.6. The second assertion shows that if IRL-based $\\mathrm{IL}$ can learn a policy to succeed in the task, MinimaxRegret $\\left(R_{E,\\delta}\\right)$ with $\\delta<\\delta^{*}$ can also learn a policy that succeeds in the task under certain condition. The proof can be found in Appendix A.6. This assertion also suggests that the designer should select a $\\delta$ smaller than $\\delta^{*}$ while making $\\delta^{*}-\\delta$ no greater than the expected size of the high-order policy utility interval. ", "page_idx": 23}, {"type": "text", "text": "Proposition 8. If $r^{*}$ is a task-aligned reward function and $\\delta\\,\\geq\\,\\delta^{*}\\,-\\,(\\operatorname*{max}_{\\pi\\in\\Pi}\\,U_{r^{*}}(\\pi)\\,-\\,\\overline{{U}}_{r^{*}})$ , the optimal protagonist policy $\\pi_{P}=M i n i m i a x R e g r e t(R_{E,\\delta})$ is guaranteed to be acceptable for the task. ", "page_idx": 23}, {"type": "text", "text": "Proof. If $\\pi_{r^{*}}\\,\\in\\,M i n i m i a x R e g r e t(R_{E,\\delta})$ , then $\\pi_{r^{*}}$ can succeed in the task by definition. Now   \nassume that $\\pi_{P}\\neq\\pi_{r^{*}}$ . Since $J_{I R L}$ achieves Nash Equilibrium at $r^{*}$ and $\\pi_{r^{*}}$ , for any other reward   \nfunction $r$ we have max $U_{r}(\\pi)-U_{r}(\\pi_{r^{*}})\\,\\leq\\,\\delta^{*}\\,-^{'}(U_{r}(E)\\,-\\,\\operatorname*{max}_{\\pi\\in\\Pi}\\,U_{r}(\\pi))\\,\\leq\\,\\delta^{*}\\,-\\,\\delta$ . We also $\\pi\\!\\in\\!\\Pi$   \nhave max Regre $:\\!(r^{\\prime},\\pi_{P})\\leq\\operatorname*{max}_{r^{\\prime}\\in R_{E,\\delta}}R e g r e t(r^{\\prime},\\pi_{r^{*}})\\leq\\delta^{*}-\\delta$ . Furthermore, $R e g r e(r^{*},\\pi_{P})\\leq$ $r^{\\prime}\\!\\in\\!R_{E,\\delta}$   \n$\\operatorname*{max}_{r^{\\prime}\\in R_{E,\\delta}}R e g r e t(r^{\\prime},\\pi_{P})$ . Hence, $R e g r e(r^{*},\\pi_{P})\\leq\\delta-\\delta^{*}\\leq\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{+}}(\\pi)-\\overline{{U}}_{r^{+}}$ . In other words,   \n$U_{r^{*}}(\\pi_{P})\\in[\\overline{{U}}_{r^{*}},\\operatorname*{max}_{\\pi\\in\\Pi}U_{r^{*}}(\\pi)]$ , indicating $\\pi_{P}$ can succeed in the task. The proof is complete. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "A.9 Example 1 ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/2e20ff47158d675a3b802a0331b9000be893ecafe043e5c1d121e888e1d2cf10.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 5: Left: Consider an MDP where there are two available actions $a_{1},a_{2}$ at initial state $s_{0}$ . In other states, actions make no difference: the transition probabilities are either annotated at the transition edges or equal 1 by default. States $s_{3}$ and $s_{6}$ are terminal states. Expert demonstrations are in $E$ . Middle: $\\mathbf{X}$ -axis indicates the MaxEnt IRL loss bound $\\delta$ for $R_{E,\\delta}$ as defined in Section A.3. The y-axis indicates the probability of the protagonist policy learned via MinimaxRegret $\\left(R_{E,\\delta}\\right)$ choosing $a_{2}$ at $s_{0}$ . The red curve shows how different $\\delta$ \u2019s lead to different protagonist policies. The blue dashed curve is for reference, showing the optimal policy under the optimal reward learned via MaxEnt IRL. Right: The curve shows how the MaxEnt IRL Loss changes with $\\omega$ . ", "page_idx": 23}, {"type": "text", "text": "Example 1. Figure 5 Left shows an illustrative example of how PAGAR-based IL mitigates reward misalignment in IRL-based IL. The task requires that a policy must visit $s_{2}$ and $s_{6}$ with probabilities no less than 0.5 within 5 steps, i.e. $P r o b(\\bar{s_{2}}\\mid\\pi)\\ge0.5\\land P r o b(s_{6}\\mid\\pi)\\ge0.5$ where $P r o b(s\\mid\\pi)$ is the probability of $\\pi$ generating a trajectory that contains $s$ within the first 5 steps. It can be derived adnerailvyattiicoanll iys  tahsa ft oal lsouwcsc.essful policy must choose $a_{2}$ at $s_{0}$ with a probability within $\\left[{\\frac{1}{2}},{\\frac{125}{188}}\\right]$ . The ", "page_idx": 23}, {"type": "text", "text": "The trajectories that reach $s_{6}$ after choosing $a_{2}$ at $s_{0}$ include: $(s_{0},a_{2},s_{2},s_{6}),(s_{0},a_{2},s_{2},s_{2},s_{6}),(s_{0},a_{2},s_{2},s_{2},s_{2},s_{6})$ . The total probability equals $\\begin{array}{r l r}{P r o b(s_{6}\\mid\\pi;s_{0},a_{2})}&{{}\\!\\!=\\!\\!}&{{\\frac{1}{5}}\\,+\\,{\\frac{1}{5}}^{2}\\,+\\,{\\frac{1}{5}}^{3}\\ =\\ {\\frac{31}{125}}}\\end{array}$ 5. Then the total probability of reaching $s_{6}$ equals $\\begin{array}{r}{{P r o b}(s_{6}\\mid\\pi)=(1-\\pi(a_{2}\\mid s_{0}))+\\frac{31}{125}\\cdot\\pi(a_{2}\\mid s_{0}).}\\end{array}$ . For $P r o b(s_{6}\\mid\\pi)$ to be no less than 0.5, $\\pi(a_{2}\\mid s_{0})$ must be no greater than $\\frac{125}{188}$ . ", "page_idx": 23}, {"type": "text", "text": "The reward function hypothesis space is $[r_{\\omega}\\;|\\;r_{\\omega}(s,a)=\\omega\\cdot r_{1}(s,a)+(1-\\omega)\\cdot r_{2}(s,a)\\}$ where $\\omega\\in[0,1]$ is a parameter, $r_{1},r_{2}$ are two features. Specifically, $r_{1}(s,a)$ equals 1 if $s=s_{2}$ and equals 0 otherwise, and $r_{2}(s,a)$ equals 1 if $s=s_{6}$ and equals 0 otherwise. Given the demonstrations and the MDP, the maximum negative MaxEnt IRL loss $\\delta^{*}\\approx2.8$ corresponds to the optimal parameter $\\omega^{\\ast}=1$ . This is computed based on Eq.6 in Ziebart et al. [2008]. The discount factor is $\\gamma=0.99$ . When computing the normalization term $Z$ in Eq.4 of Ziebart et al. [2008], we only consider the trajectories within 5 steps. The optimal policy under $r_{\\omega^{*}}$ chooses $a_{2}$ at $s_{0}$ with probability 1 and reaches $s_{6}$ with probability less than 0.25, thus failing to accomplish the task. The optimal protagonist policy $\\pi_{P}=M i n i m a x R e g r e t(R_{E,\\delta})$ can succeed in the task as indicated by the grey dashed lines in Figure 5 Middle. It chooses $a_{2}$ at $s_{0}$ with probability 1 when $\\delta$ is close to its maximum $\\delta^{*}$ . However, $\\pi_{P}(a_{2}\\mid s_{2})$ decreases as $\\delta$ decreases. It turns out that for any $\\delta<1.1$ the optimal protagonist policy can succeed in the task. In Figure 5 Right, we further show how the MaxEnt IRL loss changes with $\\omega$ . ", "page_idx": 24}, {"type": "text", "text": "B Approach to Solving MinimaxRegret ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we show how we derive the off-policy RL objective function for $\\pi_{P}$ . Also, we develop a series of theories that lead to two bounds of the Protagonist Antagonist Induced Regret. By using those bounds, we formulate objective functions for solving Imitation Learning problems with PAGAR. ", "page_idx": 24}, {"type": "text", "text": "B.1 Off-Policy Objective Function for Protagonist Policy Training ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For reader\u2019s convenience, we put the Theorem 1 in Schulman et al. [2015] here. ", "page_idx": 24}, {"type": "text", "text": "Theorem 7 (Schulman et al. [2015]). Let $\\alpha_{\\mathrm{~\\tiny~\\textnormal~{~\\rightmoon~}~}}=\\operatorname*{\\tiny~{\\mathrm{\\tiny~max}}}_{s}$ $D_{T V}(\\pi_{o l d},\\pi_{n e w})$ , and let $\\epsilon\\mathrm{~\\ensuremath~{~\\beta~}~}$ max $\\mathbb{E}_{a\\sim\\pi_{n e w}}[A_{\\pi_{o l d}}(s,a)]$ , then Eq.15 holds. ", "page_idx": 24}, {"type": "equation", "text": "$$\nU_{r}(\\pi_{n e w})\\le U_{r}(\\pi_{o l d})+\\sum_{s\\in\\mathbb{S}}\\rho_{\\pi_{o l d}}(s)\\sum_{a\\in\\mathbb{A}}\\pi_{n e w}(a|s)A_{\\pi_{o l d}}(s,a)+\\frac{2\\epsilon\\gamma}{(1-\\gamma)^{2}}\\alpha^{2}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho_{\\pi_{o l d}}(s)=\\sum_{t=0}^{T}\\gamma^{t}P r o b(s^{(t)}=s|\\pi_{o l d})}\\end{array}$ is the discounted visitation frequency of $\\pi_{o l d}$ , $A_{\\pi_{o l d}}$ is the advantage f unction without considering the entropy. ", "page_idx": 24}, {"type": "text", "text": "Algorithm 1 in Schulman et al. [2015] learns $\\pi_{n e w}$ by maximizing the r.h.s of the inequality Eq.15, which only involves the trajectories and the advantage function of $\\pi_{o l d}$ . By moving $U_{r}(\\pi_{o l d})$ from r.h.s of Eq.15 to the left, and replacing $\\pi_{n e w}$ with $\\pi_{P}$ and $\\pi_{o l d}$ with $\\pi_{A}$ , we obtain a bound for $U_{r}(\\pi_{P})-\\stackrel{\\cdot}{U}_{r}(\\pi_{A})$ as mentioned in Section 6.1. The PPO in Schulman et al. [2017] further simplifies the r.h.s of the inequality Eq.15 with a clipped importance sampling rate. We derived $J_{\\pi_{A}}(\\pi_{P})$ by using the same trick. ", "page_idx": 24}, {"type": "text", "text": "B.2 Protagonist Antagonist Induced Regret Bounds ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Our theories are inspired by the on-policy policy improvement methods in Schulman et al. [2015]. The theories in Schulman et al. [2015] are under the setting where entropy regularizer is not considered. In our implementation, we always consider entropy regularized RL of which the objective is to learn a policy that maximizes $J_{R L}(\\pi;\\dot{r})=U_{r}(\\pi)+\\mathcal{H}\\bar{(\\pi)}$ . Also, since we use GAN-based IRL algorithms, the learned reward function $r$ as proved by Fu et al. [2018] is a distribution. Moreover, it is also proved in Fu et al. [2018] that a policy $\\pi$ being optimal under $r$ indicates that $\\log\\pi\\equiv r\\equiv A_{\\pi}$ . We omit the proof and let the reader refer to $\\mathrm{Fu}$ et al. [2018] for details. Although all our theories are about the relationship between the Protagonist Antagonist Induced Regret and the soft advantage function $A_{\\pi}$ , the equivalence between $A_{\\pi}$ and $r$ allows us to use the theories to formulate our reward optimization objective functions. To start off, we denote the reward function to be optimized as $r$ . Given the intermediate learned reward function $r$ , we study the Protagonist Antagonist Induced Regret between two policies $\\pi_{1}$ and $\\pi_{2}$ . ", "page_idx": 24}, {"type": "text", "text": "Lemma 12. Given a reward function $r$ and a pair of policies $\\pi_{1}$ and $\\pi_{2}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nU_{r}(\\pi_{1})-U_{r}(\\pi_{2})=\\underset{\\tau\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]+\\underset{\\tau\\sim\\pi}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{H}\\left(\\pi_{2}(\\cdot|s^{(t)})\\right)\\right]\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. This proof follows the proof of Lemma 1 in Schulman et al. [2015] where RL is not entropy-regularized. For entropy-regularized RL, since $\\begin{array}{r l}{A_{\\pi}(s,a^{(t)})}&{{}=}\\end{array}$ $\\mathbb{E}_{s^{\\prime}\\sim\\mathcal{T}(\\cdot|s,a^{(t)})}\\left[r(s,a^{(\\bar{t})})+\\bar{\\gamma}\\mathcal{V}_{\\pi}(s^{\\prime})-\\mathcal{V}_{\\pi}(s)\\right]$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}A_{2}(s^{(i)},a^{(i)})\\right]}\\\\ {=}&{\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}\\left(r^{(s^{(i+1)},a^{(i+1)})}+\\gamma\\nu_{\\sigma_{2}}(s^{(i+1)})-\\nu_{\\sigma_{2}}(s^{(i)})\\right)\\right]}\\\\ {=}&{\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}(s^{(i)},a^{(i)})-\\nu_{\\sigma_{2}}(s^{(i)})\\right]}\\\\ {=}&{\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}(s^{(i)},a^{(i)})\\right]-\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\nu_{\\sigma_{2}}(s^{(i)})\\right]}\\\\ {=}&{\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}(s^{(i)},a^{(i)})\\right]-\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}(s^{(i)},a^{(i)})+\\mathcal{H}\\left(\\pi_{2}(\\cdot|s^{(i)})\\right)\\right]}\\\\ {=}&{\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}(s^{(i)},a^{(i)})\\right]-\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}(s^{(i)},a^{(i)})+\\mathcal{H}\\left(\\pi_{2}(\\cdot|s^{(i)})\\right)\\right]}\\\\ {=}&{U_{r}(\\pi_{1})-U_{r}(\\pi_{2})-\\underset{r\\in\\mathbb{Z}_{0}}{\\mathbb{E}}\\left[\\sum_{i=0}^{\\infty}r^{i}\\left(\\pi_{2}(\\cdot|s^{(i)})\\right)\\right]}\\\\ {=}&{U_{r}(\\pi_{1})-U_{r}(\\pi_{2\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Remark 1. Lemma 12 confirms that $\\begin{array}{r}{\\underset{\\tau\\sim\\pi}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathcal{A}_{\\pi}(s^{(t)},a^{(t)})\\right]=U_{r}(\\pi)\\!-\\!U_{r}(\\pi)\\!+\\!\\mathcal{H}(\\pi)=\\mathcal{H}(\\pi)}\\end{array}$ . We follow Schulman et al. [2015] and denote $\\Delta{\\cal A}(s)=\\underset{a\\sim\\pi_{1}(\\cdot\\vert s)}{\\mathbb{E}}\\left[{\\cal A}_{\\pi_{2}}(s,a)\\right]-\\underset{a\\sim\\pi_{2}(\\cdot\\vert s)}{\\mathbb{E}}\\left[{\\cal A}_{\\pi_{2}}(s,a)\\right]$ ] as the difference between the expected advantages of following $\\pi_{2}$ after choosing an action respectively by following policy $\\pi_{1}$ and $\\pi_{2}$ at any state $s$ . Although the setting of Schulman et al. [2015] differs from ours by having the expected advantage $\\underset{a\\sim\\pi_{2}(\\cdot|s)}{\\mathbb{E}}\\left[\\mathcal{A}_{\\pi_{2}}(s,a)\\right]$ equal to 0 due to the absence of entropy regularization, the following definition and lemmas from Schulman et al. [2015] remain valid in our setting. ", "page_idx": 25}, {"type": "text", "text": "Definition 9. Schulman et al. [2015], the protagonist policy $\\pi_{1}$ and the antagonist policy $\\pi_{2}$ ) are $\\alpha$ -coupled if they defines a joint distribution over $(a,\\tilde{a})\\in\\mathbb{A}\\times\\mathbb{A}$ , such that $P r o b(a\\neq\\tilde{a}|s)\\le\\dot{\\alpha}$ for all $s$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma 13. Schulman et al. [2015] Given that the protagonist policy $\\pi_{1}$ and the antagonist policy $\\pi_{2}$ are $\\alpha$ -coupled, then for all state $s$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\Delta A(s)|\\leq2\\alpha\\mathrm{max}|A_{\\pi_{2}}(s,a)|\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 14. Schulman et al. [2015] Given that the protagonist policy $\\pi_{1}$ and the antagonist policy $\\pi_{2}$ are $\\alpha$ -coupled, then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\biggl|\\underset{s(t)\\sim\\pi_{1}}{\\mathbb{E}}\\Bigl[\\Delta A(s^{(t)})\\Bigr]-\\underset{s^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\Bigl[\\Delta A(s^{(t)})\\Bigr]\\biggr|\\leq4\\alpha(1-(1-\\alpha)^{t})\\underset{s,a}{\\operatorname*{max}}|A_{\\pi_{2}}(s,a)|\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 15. Given that the protagonist policy $\\pi_{1}$ and the antagonist policy $\\pi_{2}$ are $\\alpha$ -coupled, then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]-\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\le2(1-(1-\\alpha)^{t})\\underset{(s,a)}{\\operatorname*{max}}|A_{\\pi_{2}}(s,a)|\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The proof is similar to that of Lemma 14 in Schulman et al. [2015]. Let $n_{t}$ be the number of times that $a^{(t^{\\prime})}\\sim\\pi_{1}$ does not equal $a^{(t^{\\prime})}\\sim\\pi_{2}$ for $t^{\\prime}<t$ , i.e., the number of times that $\\pi_{1}$ and $\\pi_{2}$ ", "page_idx": 25}, {"type": "text", "text": "disagree before timestep $t$ . Then for $s^{(t)}\\sim\\pi_{1}$ , we have the following. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{s^{(t)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]}\\\\ {=}&{P(n_{t}=0)\\underset{s^{(t)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]+P(n_{t}>0)\\underset{s^{(t)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The expectation decomposes similarly for $s^{(t)}\\sim\\pi_{2}$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{s^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]}\\\\ {=}&{}&{P(n_{t}=0)\\underset{s^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]+P(n_{t}>0)\\underset{s^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]}\\\\ &{}&{\\qquad\\qquad\\quad a^{(t)}\\sim\\pi_{2}}&{\\underset{n_{t}>0}{\\overset{a^{(t)}\\sim\\pi_{2}}{\\prod}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "$\\mathrm{When\\;computing}\\underset{s^{(t)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}\\big(s^{(t)},a^{(t)}\\big)\\right]\\right]-\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}\\big(s^{(t)},a^{(t)}\\big)\\right],$ , the terms with $n_{t}=$ 0 cancel each other because $n_{t}=0$ indicates that $\\pi_{1}$ and $\\pi_{2}$ agreed on all timesteps less than $t$ . That leads to the following. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\underset{s^{(t)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]-\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]}\\\\ &{=}&{P(n_{t}>0)\\underset{s_{t}^{(t)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]-P(n_{t}>0)\\underset{s^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]}\\\\ &{\\quad}&{a_{t}^{(t)}\\sim\\overset{-\\alpha}{0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By definition of $\\alpha$ , the probability of $\\pi_{1}$ and $\\pi_{2}$ agreeing at timestep $t^{\\prime}$ is no less than $1-\\alpha$ . Hence, $\\bar{P(n_{t}>0)}\\leq1-(1-\\alpha^{t})^{t}$ . Hence, we have the following bound. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{s^{(1)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{u_{1}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]-\\underset{s^{(1)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right|}\\\\ {=}&{\\ \\left|P(n_{t}>0)\\underset{s^{(1)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{u_{1}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]-P(n_{t}>0)\\underset{s^{(1)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right|}\\\\ {\\leq}&{\\ P(n_{t}>0)\\left(\\left|\\underset{s^{(1)}\\sim\\pi_{1}}{\\underbrace{\\mathbb{E}}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right|+\\left|\\underset{u_{1}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right|\\right)}\\\\ {\\leq}&{\\ 2(1-(1-\\alpha)^{1})\\underset{s^{(1)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The preceding lemmas lead to the proof for Theorem 4 in the main text. ", "page_idx": 26}, {"type": "text", "text": "Theorem 4. Suppose that $\\pi_{2}$ is the optimal policy in terms of entropy regularized RL under $r$ . Let $\\alpha\\,=\\,\\operatorname*{max}_{s}\\,D_{T V}\\bigl(\\pi_{1}(\\cdot|s),\\pi_{2}(\\cdot|s)\\bigr)$ , $\\epsilon\\,=\\operatorname*{max}_{s,a}\\,|\\mathcal{A}_{\\pi_{2}}(s,a^{(t)})|$ , and $\\Delta\\!\\!\\,\\mathcal{A}(s)\\,=\\,\\underset{a\\sim\\pi_{1}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s,a)\\right]\\,-$ ", "page_idx": 26}, {"type": "text", "text": "$\\underset{a\\sim\\pi_{2}}{\\mathbb{E}}\\left[\\mathcal{A}_{\\pi_{2}}(s,a)\\right]$ . For any policy $\\pi_{1}$ , the following bounds hold. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\displaystyle U_{r}(\\pi_{1})-U_{r}(\\pi_{2})-\\sum_{t=0}^{\\infty}\\gamma^{t}\\underset{s^{(t)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\Delta A(s^{(t)})\\right]\\Bigg\\vert}&{\\leq}&{\\displaystyle\\frac{2\\alpha\\gamma\\epsilon}{(1-\\gamma)^{2}}}\\\\ {\\displaystyle U_{r}(\\pi_{1})-U_{r}(\\pi_{2})-\\sum_{t=0}^{\\infty}\\gamma^{t}\\underset{s^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[\\Delta A(s^{(t)})\\right]\\Bigg\\vert}&{\\leq}&{\\displaystyle\\frac{2\\alpha\\gamma(2\\alpha+1)\\epsilon}{(1-\\gamma)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. We first leverage Lemma 12 to derive Eq.23. Note that since $\\pi_{2}$ is optimal under $r$ , Remark 1 confirmed that $\\begin{array}{r}{\\mathcal{H}(\\pi_{2})=-\\sum_{t=0}^{\\infty}\\gamma_{\\ s^{(t)}\\sim\\pi_{2}}^{t}\\bigg[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\Big[\\mathcal{A}_{\\pi_{2}}\\big(s^{(t)},a^{(t)}\\big)\\Big]\\bigg].}\\end{array}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{\\mathcal{N}_{n}}(\\mathbf{r},|\\mathbf{r}_{1})-U_{\\mathcal{N}_{n}}(\\mathbf{z})}\\\\ {=}&{(U_{n}^{*}(\\mathbf{r}_{1})-U_{\\mathcal{N}_{n}}(\\mathbf{z})-\\mathcal{H}(\\mathbf{r}_{2}))+\\mathcal{H}(\\mathbf{r}_{2})}\\\\ {=}&{\\displaystyle\\operatorname*{max}_{r=\\infty}\\left[\\sum_{i=0}^{n}r^{\\lambda}A_{\\lambda,i}(\\mathbf{z}_{s}(s^{(i)},a^{(i)}))\\right]+\\mathcal{H}(\\mathbf{r}_{2})}\\\\ {=}&{\\displaystyle\\operatorname*{max}_{r=\\infty}\\left[\\sum_{i=0}^{n}r^{\\lambda}A_{\\lambda,i}(\\mathbf{z}_{s^{(i)}}(s^{(i)},a^{(i)}))\\right]-\\sum_{i=0}^{n}r^{\\lambda}\\sum_{i=0}^{n}\\left[\\sum_{a^{(i)}=\\lambda_{m}}\\left[A_{a_{n}}(s^{(i)},a^{(i)})\\right]\\right]}\\\\ {=}&{\\displaystyle\\sum_{r=\\infty}^{\\infty}r^{\\lambda}\\sum_{i,j=0}^{n}\\left[\\sum_{i=0}^{n}\\left[A_{a_{2}}(s^{(i)},a^{(i)})\\right]-\\sum_{a^{(i)}=\\lambda_{m}}\\left[A_{a_{1}}(s^{(i)},a^{(i)})\\right]\\right]+}\\\\ &{\\displaystyle\\sum_{s=0}^{n}r^{\\lambda}\\left(\\sum_{a^{(i)}=\\lambda_{m}}\\left[\\sum_{i=0}^{n}\\left[A_{a_{2}}(s^{(i)},a^{(i)})\\right]\\right]-\\sum_{a^{(i)}=\\lambda_{m}}\\left[A_{a_{1}}(s^{(i)},a^{(i)})\\right]\\right]}\\\\ {=}&{\\displaystyle\\sum_{s=0}^{n}r^{\\lambda}\\left(\\sum_{a^{(i)}=\\lambda_{m}}\\left[\\sum_{i=0}^{n}\\left[A_{a_{2}}(s^{(i)},a^{(i)})\\right]-\\sum_{a^{(i)}=\\lambda_{m}}\\left[\\sum_{a^{(i)}=\\lambda_{m}}\\left[A_{a_{2}}(s^{(i)},a^{(i)})\\right]\\right]\\right)}\\\\ {\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We switch terms between Eq.23 and $U_{r}(\\pi_{1})-U_{r}(\\pi_{2})$ , then use Lemma 15 to derive Eq.24. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\displaystyle\\left|U_{r}(\\pi_{1})-U_{r}(\\pi_{2})-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma_{\\phantom{t}_{s}(t)\\sim\\pi_{1}}^{t}\\left[\\Delta A(s^{(t)})\\right]\\right|}\\\\ {=}&{\\displaystyle\\left|\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(\\underset{s^{(t)}\\sim\\pi_{1}}{\\mathbb{E}}\\left[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]-\\underset{s^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}(s^{(t)},a^{(t)})\\right]\\right]\\right)\\right|}\\\\ {\\le}&{\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\cdot\\operatorname*{\\mathcal{Q}\\mathrm{max}}_{(s,a)}\\lvert A_{\\pi_{2}}(s,a)\\rvert\\cdot(1-(1-\\alpha)^{t})\\le\\frac{2\\alpha\\gamma\\underset{(s,a)}{\\operatorname*{max}}\\lvert A_{\\pi_{2}}(s,a)\\rvert}{(1-\\gamma)^{2}}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Alternatively, we can expand $U_{r}(\\pi_{2})-U_{r}(\\pi_{1})$ into Eq.25. During the process, $\\mathcal{H}(\\pi_{2})$ is converted ", "page_idx": 28}, {"type": "text", "text": "$\\begin{array}{r}{\\mathrm{into}-\\sum_{t=0}^{\\infty}\\gamma_{\\phantom{t}s^{(t)}\\sim\\pi_{2}}^{t}\\bigg[\\underset{a^{(t)}\\sim\\pi_{2}}{\\mathbb{E}}\\left[A_{\\pi_{2}}\\big(s^{(t)},a^{(t)}\\big)\\right]\\bigg].}\\end{array}$ $\\begin{array}{r l}{\\ }&{U_{i,\\{n\\}}=-U_{i,\\{n\\}}\\sum_{\\ell=\\{n\\}}^{n}\\mathcal{D}_{i}(x_{i})}\\\\ {=}&{\\left(U_{i,\\{n\\}}^{\\top}\\mathcal{D}_{i}(x_{i})-U_{i}(x_{i})\\right)+\\mathcal{H}(x_{i})}\\\\ {=}&{\\ \\operatorname*{max}\\left[\\sum_{j=\\lceil n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{\\ell\\rfloor-1}\\omega_{j,\\ell}(u_{j},u_{i})^{(n)}\\right]+\\mathcal{H}(x_{i})}\\\\ {=}&{\\displaystyle\\sum_{j=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{n\\rfloor}\\left\\{\\sum_{\\ell=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{n\\rfloor}\\Big|\\sum_{\\ell=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{n\\rfloor}\\sum_{\\ell=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{n\\rfloor}\\Big|\\sum_{\\ell=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\}^{n\\rfloor}}\\\\ {=}&{\\displaystyle\\sum_{j=\\lfloor n\\}^{n\\rfloor}\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\bigg[\\sum_{\\ell=\\lfloor n\\}^{n}\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\Big|\\sum_{\\ell=\\lfloor n\\}^{n\\rfloor}\\sum_{\\ell=\\lfloor n\\rfloor-\\lfloor n\\rfloor}^{n}\\Big|\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\Big|\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\Big|\\sum_{\\ell=\\lfloor n\\rfloor-\\lfloor n\\rfloor}^{n}\\Big|}\\\\ {=}&{\\displaystyle\\sum_{j=\\lfloor n\\}^{n\\rfloor}\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\left[\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\Big|\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\Big|\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\sum_{\\ell=\\lfloor n\\rfloor-\\lfloor n\\rfloor}^{n}\\sum_{\\ell=\\lfloor n\\rfloor}^{n}\\left[A_{\\alpha_{1}\\langle\\alpha_{1}^{(j)},\\alpha^{(j)}\\rangle\\right]\\right]+}\\\\ &{\\quad\\quad\\:\\:\\prod_{\\ell=\\lfloor n\\rfloor}^{n$ )) ) ", "page_idx": 28}, {"type": "text", "text": "We switch terms between Eq.25 and $U_{r}(\\pi_{1})-U_{r}(\\pi_{2})$ , then base on Lemma 14 and 15 to derive the inequality in Eq.26. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left[\\sigma_{r}(n_{1})-E_{t}(n_{2})-\\frac{\\gamma}{\\alpha_{0}^{t}},\\frac{\\gamma}{n_{1}\\alpha_{1}^{t}n_{3}}\\right]\\right|}\\\\ {=}&{\\left|\\left[\\sigma_{r}(n_{1})-E_{t}(n_{2})-\\left[\\sigma_{0}\\right]\\right|^{2}\\right|}\\\\ &{\\qquad\\sum_{j=1}^{n_{1}}\\left(\\sigma_{j,j}\\nabla_{x_{j-1}}\\left[A_{x_{j}}(n_{1}^{(n_{1})},n^{(j)})\\right]-\\sigma_{x_{j-1}}\\nabla_{x_{j-1}}\\left[A_{x_{j}}(n_{2}^{(n_{2})},n^{(j)})\\right]\\right)\\right|}\\\\ {=}&{\\left|\\left[\\frac{\\gamma}{\\alpha_{0}^{t}},\\frac{\\gamma}{n_{1}\\alpha_{1}^{t}n_{3}}\\right]\\left(\\lambda_{1}\\alpha_{1}^{(n_{1})}\\right)-\\frac{\\gamma}{\\alpha_{0}^{t}n_{1}}\\left[\\lambda_{2}\\alpha_{1}^{(n_{1})}\\left(n_{1}^{(n_{2})}\\right)\\right]\\right|}\\\\ &{\\qquad\\sum_{j=1}^{n_{1}}\\left(\\sigma_{j,j}\\nabla_{x_{j-1}}\\left[A_{x_{j}}(n_{1}^{(n_{2})},n_{2}^{(n_{1})})\\right]-\\sigma_{x_{j-1}}\\nabla_{x_{j-1}}\\left[A_{x_{j}}(n_{1}^{(n_{1})},n^{(n_{1})})\\right]\\right)\\right|}\\\\ &{\\qquad\\left|\\left[\\frac{\\gamma}{\\alpha_{0}^{t}n_{1}}\\right]\\left(\\int_{-\\infty}^{\\infty}\\left[\\sigma_{x_{j-1}}\\left[A_{x_{j}}(n_{2}^{(n)},n^{(n_{2})})\\right]\\right)+\\frac{\\gamma}{\\alpha_{0}^{t}n_{1}}\\left[\\sigma_{x_{j-1}}\\left[A_{x_{j}}(n_{2}^{(n_{2})},n^{(n_{1})})\\right]\\right)\\right]\\right|}\\\\ &{\\qquad\\left|\\left[\\frac{\\gamma}{\\alpha_{0}^{t}n_{1}},\\frac{\\\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It is stated in Schulman et al. [2015] that max $D_{T V}\\bigl(\\pi_{2}(\\cdot|s),\\pi_{1}(\\cdot|s)\\bigr)\\,\\le\\,\\alpha$ . Hence, by letting $\\alpha:=\\operatorname*{max}_{s}D_{T V}\\big(\\pi_{2}(\\cdot|s),\\pi_{1}(\\cdot|s)\\big)$ , Eq.23 and 26 still hold. Then, we have proved Theorem 4. ", "page_idx": 29}, {"type": "text", "text": "B.3 Objective Functions of Reward Optimization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "To derive $J_{R,1}$ and $J_{R,2}$ , we let $\\pi_{1}=\\pi_{P}$ and $\\pi_{2}=\\pi_{A}$ . Then based on Eq.21 and 22 we derive the following upper-bounds of $U_{r}(\\pi_{P})-U_{r}(\\pi_{A})$ . ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{{U_{r}(\\pi_{P})-U_{r}(\\pi_{A})}}&{{\\le}}&{{\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}{}_{s^{(t)}\\sim\\pi_{P}}^{\\mathbb{E}}\\left[\\Delta A(s^{(t)})\\right]+\\frac{2\\alpha\\gamma(2\\alpha+1)\\epsilon}{(1-\\gamma)^{2}}}}\\\\ {{U_{r}(\\pi_{P})-U_{r}(\\pi_{A})}}&{{\\ge}}&{{\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}{}_{s^{(t)}\\sim\\pi_{A}}\\left[\\Delta A(s^{(t)})\\right]-\\frac{2\\alpha\\gamma\\epsilon}{(1-\\gamma)^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By our assumption that $\\pi_{A}$ is optimal under $r$ , we have ${\\mathcal{A}}_{\\pi_{A}}\\equiv r$ Fu et al. [2018]. This equivalence enables us to replace $A_{\\pi_{A}}$ \u2019s in $\\Delta\\mathcal{A}$ with $r$ . As for the $\\textstyle{\\frac{2\\alpha\\gamma(2\\alpha+1)\\epsilon}{(1-\\gamma)^{2}}}$ and $\\frac{2\\alpha\\gamma\\epsilon}{(1\\!-\\!\\gamma)^{2}}$ terms, since the objective is to maximize $U_{r}(\\pi_{A})-U_{r}(\\pi_{B})$ , we heuristically estimate the $\\epsilon$ in Eq.27 by using the samples from $\\pi_{P}$ and the $\\epsilon$ in Eq.28 by using the samples from $\\pi_{A}$ . As a result we have the objective functions defined as Eq.29 and 30 where \u03be1(s, a) = \u03c0\u03c0PA ((aa((tt))||ss((tt)))) and \u03be2 = \u03c0A(a((tt))|s((tt))) are the importance sampling probability ratio derived from the definition of $\\begin{array}{r}{\\Delta\\mathcal{A};C_{1}\\propto-\\frac{\\gamma\\hat{\\alpha}}{(1-\\gamma)}}\\end{array}$ and $\\begin{array}{r}{C_{2}\\propto\\frac{\\gamma\\hat{\\alpha}}{(1-\\gamma)}}\\end{array}$ where $\\hat{\\alpha}$ is either an estimated maximal KL-divergence between $\\pi_{A}$ and $\\pi_{B}$ since $D_{K L}\\geq D_{T V}^{2}$ according to Schulman et al. [2015], or an estimated maximal $D_{T V}^{2}$ depending on whether the reward function is Gaussian or Categorical. We also note that for finite horizon tasks, we compute the average rewards instead of the discounted accumulated rewards in Eq.30 and 29. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{R,1}(r;\\pi_{P},\\pi_{A}):=\\underset{\\tau\\sim\\pi_{A}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(\\xi_{1}(s^{(t)},a^{(t)})-1\\right)\\cdot r(s^{(t)},a^{(t)})\\right]+C_{1}\\underset{(s,a)\\sim\\pi_{A}}{\\operatorname*{max}}|r(s,a)|(\\pi_{A})|,}\\\\ &{J_{R,2}(r;\\pi_{P},\\pi_{A}):=\\underset{\\tau\\sim\\pi_{P}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(1-\\xi_{2}(s^{(t)},a^{(t)})\\right)\\cdot r(s^{(t)},a^{(t)})\\right]+C_{2}\\underset{(s,a)\\sim\\pi_{P}}{\\operatorname*{max}}|r(s,a)|(\\pi_{A})|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Beside $J_{R,1},J_{R,2}$ , we additionally use two more objective functions based on the derived bounds. W $J_{R,r}(r;\\pi_{A},\\pi_{P})$ . By denoting the optimal policy under $r$ as $\\pi^{*}$ , $\\alpha^{*}=\\operatorname*{max}_{s\\in\\mathbb{S}}D_{T V}(\\pi^{*}(\\cdot|s),\\pi_{A}(\\cdot|s)$ , $\\epsilon^{*}=\\operatorname*{max}_{(s,a^{(t)})}|A_{\\pi^{*}}(s,a^{(t)})|$ , and $\\Delta\\mathcal{A}_{A}^{\\ast}(s)=\\underset{a\\sim\\pi_{A}}{\\mathbb{E}}[\\mathcal{A}_{\\pi^{\\ast}}(s,a)]-\\underset{a\\sim\\pi^{\\ast}}{\\mathbb{E}}[\\mathcal{A}_{\\pi^{\\ast}}(s,a)],$ , we have the following. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{}}&{{U_{r}\\left(\\pi_{P}\\right)-U_{r}\\left(\\pi_{S}^{*}\\right)}}\\\\ {{=}}&{{U_{r}\\left(\\pi_{P}\\right)-U_{r}\\left(\\pi_{A}\\right)+U_{r}\\left(\\pi_{A}\\right)-U_{r}\\left(\\pi^{*}\\right)}}\\\\ {{\\ }}&{{\\le}}&{{U_{r}\\left(\\pi_{P}\\right)-U_{r}\\left(\\pi_{A}\\right)+\\displaystyle\\sum_{t=0}^{\\infty}\\gamma_{s}^{t}\\frac{\\mathbb{E}}{|\\pi_{B}^{t}|\\log_{s}}\\left[\\Delta A_{A}^{*}(s^{(t)})\\right]+\\displaystyle\\frac{2\\alpha^{*}\\gamma_{t}^{*}c^{*}}{(1-\\gamma)^{2}}}}\\\\ {{}}&{{=}}&{{U_{r}\\left(\\pi_{P}\\right)-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma_{s}^{t}\\frac{\\mathbb{E}}{|\\pi_{B}^{t}|\\log_{s}}\\left[\\biggr.\\mathbb{E}\\left(s^{(t)},a^{(t)}\\right)\\right]\\biggr]+}}\\\\ {{}}&{{\\displaystyle\\sum_{t=0}^{\\infty}\\gamma_{s}^{t}\\frac{\\mathbb{E}}{|\\pi_{B}^{t}|\\log_{s}}\\left[\\biggr.\\mathbb{E}\\left(a_{r}^{*},(s^{(t)},a^{(t)})\\right]-\\displaystyle\\sum_{t=0}^{\\infty}\\mathbb{E}_{\\pi^{*}}\\left[A_{s}\\left(\\pi^{*},a^{(t)},a^{(t)}\\right)\\right]\\right]+\\displaystyle\\frac{2\\alpha^{*}\\gamma_{t}^{*}c^{*}}{(1-\\gamma)^{2}}}}\\\\ {{=}}&{{U_{r}\\left(\\pi_{P}\\right)-\\displaystyle\\sum_{t=0}^{\\infty}\\gamma_{s}^{t}\\frac{\\mathbb{E}}{|\\pi_{B}^{t}|\\log_{s}}\\left[\\biggr.\\mathbb{E}_{\\pi^{*}}\\left(s^{(t)},a^{(t)}\\right)\\right]+\\displaystyle\\frac{2\\alpha^{*}\\gamma_{t}^{*}c^{*}}{(1-\\gamma)^{2}}}}\\\\ {{=}}&{{\\displaystyle\\sum_{t=\\pi_{P}}\\left[\\sum_{t=0}^{\\infty}\\gamma_{s}^{t}r(s^{(t)},a^{(t)})\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $\\begin{array}{r}{\\xi_{3}=\\frac{\\exp(r(s^{(t)},a^{(t)}))}{\\pi_{A}(a^{(t)}|s^{(t)})}}\\end{array}$ be the importance sampling probability ratio. It is suggested in Schulman et al. [2017] that instead of directly optimizing the objective function Eq.31, optimizing a surrogate ", "page_idx": 29}, {"type": "text", "text": "objective function as in Eq.32, which is an upper-bound of Eq.31, with some small $\\delta\\in(0,1)$ can be much less expensive and still effective. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{R,3}(r;\\pi_{P},\\pi_{A}):=\\underset{\\tau\\sim\\pi_{P}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s^{(t)},a^{(t)})\\right]-}\\\\ &{\\qquad\\qquad\\underset{\\tau\\sim\\pi_{A}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\operatorname*{min}\\big(\\xi_{3}\\cdot r(s^{(t)},a^{(t)}),c l i p(\\xi_{3},1-\\delta,1+\\delta)\\cdot r(s^{(t)},a^{(t)})\\big)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Alternatively, we let $\\Delta\\mathcal{A}_{P}^{*}(s)=\\underset{a\\sim\\pi_{P}}{\\mathbb{E}}\\left[\\mathcal{A}_{\\pi^{*}}(s,a)\\right]-\\underset{a\\sim\\pi^{*}}{\\mathbb{E}}\\left[\\mathcal{A}_{\\pi^{*}}(s,a)\\right]$ . The according to Eq.27, we have the following. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U_{r}\\big(\\pi_{P}\\big)-U_{r}\\big(\\pi^{*}\\big)}\\\\ {\\le}&{\\displaystyle\\sum_{t=0}^{\\infty}\\gamma_{\\phantom{t}s^{(t)}\\sim\\pi_{P}}^{t}\\left[\\Delta A_{P}^{*}\\big(s^{(t)}\\big)\\right]+\\frac{2\\alpha^{*}\\gamma(2\\alpha^{*}+1)\\epsilon^{*}}{(1-\\gamma)^{2}}}\\\\ {=}&{\\displaystyle\\sum_{t=0}^{\\infty}\\gamma_{\\phantom{t}s^{(t)}\\sim\\pi_{P}}^{t}\\left[\\underset{a^{(t)}\\sim\\pi_{P}}{\\mathbb{E}}\\left[A_{\\pi^{*}}\\big(s^{(t)},a^{(t)}\\big)\\right]-\\underset{a^{(t)}\\sim\\pi^{*}}{\\mathbb{E}}\\left[A_{\\pi^{*}}\\big(s^{(t)},a\\big)^{(t)}\\right]\\right]+\\frac{2\\alpha^{*}\\gamma(2\\alpha^{*}+1)\\epsilon^{*}}{(1-\\gamma)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then a new objective function JR,4 is formulated in Eq.34 where \u03be4 = ex\u03c0pP( r((as((tt))|,sa(t()t))) ). ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{J_{R,4}(r;\\pi_{P},\\pi_{A}):=\\underset{\\tau\\sim\\pi_{P}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\bigl(s^{(t)},a^{(t)}\\bigr)\\right]-}\\\\ &{\\qquad\\qquad\\underset{\\tau\\sim\\pi_{P}}{\\mathbb{E}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\operatorname*{min}\\left(\\xi_{4}\\cdot r\\bigl(s^{(t)},a^{(t)}\\bigr),c l i p\\bigl(\\xi_{4},1-\\delta,1+\\delta\\bigr)\\cdot r\\bigl(s^{(t)},a^{(t)}\\bigr)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "B.4 Incorporating IRL Algorithms ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Online RL Setting. In our implementation, we combine PAGAR with GAIL, VAIL, and f-IRL, respectively. In this section, we use $J_{I R L}$ to indicate the IRL loss to be minimized in place of the notation ${\\mathcal{I}}_{I R L}$ in the main text. Accordingly, $\\delta$ is the target IRL loss. As f-IRL is an inverse RL algorithm that explicitly learns a reward function, when PAGAR is combined with f-IRL, the metaalgorithm Algorithm 1 can be directly implemented without changes. When PAGAR is combined with GAIL, the meta-algorithm Algorithm 1 becomes Algorithm 2. When PAGAR is combined with VAIL, it becomes Algorithm 3. Both of the two algorithms are GAN-based IRL, indicating that both algorithms use Eq.1 as the IRL objective function. In these two cases, we use a neural network to approximate $D$ , the discriminator in Eq.1. To get the reward function $r$ , we follow $\\mathrm{Fu}$ et al. [2018] and denote $\\begin{array}{r}{r(s,a)=\\log\\left(\\frac{\\pi_{A}(a|s)}{D(s,a)}-\\pi_{A}(a|s)\\right)}\\end{array}$ as mentioned in Section 1. Hence, the only difference between Algorithm 2 and Algorithm 1 is in the representation of the reward function. Regarding VAIL, since it additionally learns a representation for the state-action pairs, a bottleneck constraint $J_{I C}(D)\\leq i_{c}$ is added where the bottleneck $J_{I C}$ is estimated from policy roll-outs. VAIL introduces a Lagrangian parameter $\\beta$ to integrate $J_{I C}(D)-i_{c}$ in the objective function. As a result its objective function becomes $J_{I R L}(r)+\\beta\\cdot(J_{I C}(D)-i_{c})$ . VAIL not only learns the policy and the discriminator but also optimizes $\\beta$ . In our case, we utilize the samples from both protagonist and antagonist policies to optimize $\\beta$ as in line 10 following Peng et al. [2019]. ", "page_idx": 30}, {"type": "text", "text": "In our implementation, depending on the difficulty of the benchmarks, we choose to maintain $\\lambda$ as a constant or update $\\lambda$ with the IRL loss $J_{I R L}(r)$ in most of the continuous control tasks. In HalfCheetah- $\\nu2$ and all the maze navigation tasks, we update $\\lambda$ by introducing a hyperparameter $\\mu$ . As described in the maintext, we treat $\\delta$ as the target IRL loss of $J_{I R L}(r)$ , i.e., $\\bar{J}_{I R L}(r)\\leq\\delta$ . In all the maze navigation tasks, we initialize $\\lambda$ with some constant $\\lambda_{0}$ and update $\\lambda$ by $\\lambda:=$ $\\lambda\\cdot\\exp(\\mu\\cdot(J_{I R L}(r)\\,\\bar{-\\,}\\delta))$ after every iteration. In HalfCheetah- $\\cdot\\nu2$ , we update $\\lambda$ by $\\lambda:=m a x(\\lambda_{0},\\lambda\\,.$ $\\exp(\\mu\\cdot(J_{I R L}(r)-\\delta)))$ to avoid $\\lambda$ being too small. Besides, we use PPO Schulman et al. [2017] to train all policies in Algorithm 2 and 3. ", "page_idx": 30}, {"type": "text", "text": "Offilne RL Setting. We incorporate PAGAR with RECOIL Sikchi et al. [2024]. The original RECOIL algorithm does not learn a reward function but learns a $Q,V$ value functions and a policy $\\pi$ with neural networks. In order to combine PAGAR with RECOIL, we made the following modification to RECOIL: ", "page_idx": 30}, {"type": "text", "text": "Input: Expert demonstration $E$ , discriminator loss bound $\\delta$ , initial protagonist policy $\\pi_{P}$ , antagonist policy $\\pi_{A}$ , discriminator $D$ (representing $\\begin{array}{r}{r(s,a)=\\log\\left(\\frac{\\pi_{A}(a|s)}{D(s,a)}-\\pi_{A}(a|s)\\right)}\\end{array}$ , Lagrangian parameter $\\lambda$ , iteration number $i=0$ , maximum iteration number $N$ ", "page_idx": 31}, {"type": "text", "text": "Output: $\\pi_{P}$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1: while iteration number $i<N$ do   \n2: Sample trajectory sets $\\mathbb{D}_{A}\\sim\\pi_{A}$ and $\\mathbb{D}_{P}\\sim\\pi_{P}$   \n3: Estimate $J_{R L}(\\pi_{A};r)$ with $\\mathbb{D}_{A}$   \n4: Optimize $\\pi_{A}$ to maximize $J_{R L}(\\pi_{A};r)$ .   \n5: Estimate $J_{R L}(\\pi_{P};r)$ with $\\mathbb{D}_{P}$ ; $J_{\\pi_{A}}(\\pi_{P};\\pi_{A},r)$ with $\\mathbb{D}_{P}$ and $\\mathbb{D}_{A}$ ;   \n6: Optimize $\\pi_{P}$ to maximize $J_{R L}(\\pi_{P};r)+J_{\\pi_{A}}(\\pi_{P};\\pi_{A},r)$ .   \n7: Estimate $J_{P A G A R}(r;\\pi_{P},\\pi_{A})$ with $\\mathbb{D}_{P}$ and $\\mathbb{D}_{A}$   \n8: Estimate $J_{I R L}(\\pi_{A};r)$ with $\\mathbb{D}_{A}$ and $E$ by following the IRL algorithm   \n9: Optimize $D$ to minimize $J_{P A G A R}(r;\\pi_{P},\\pi_{A})+\\lambda\\cdot m a x(J_{I R L}(r)+\\delta,0)$   \n10: end while ", "page_idx": 31}, {"type": "text", "text": "", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "11: return $\\pi_{P}$ ", "page_idx": 31}, {"type": "text", "text": "Algorithm 3 VAIL w/ PAGAR ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Input: Expert demonstration $E$ , discriminator loss bound $\\delta$ , initial protagonist policy $\\pi_{P}$ , antagonist policy $\\pi_{A}$ , discriminator $D$ (representing $\\begin{array}{r}{r(s,a)=\\log\\left(\\frac{\\pi_{A}(a|s)}{D(s,a)}-\\pi_{A}(a|s)\\right)}\\end{array}$ , Lagrangian parameter $\\lambda$ for PAGAR, iteration number $i=0$ , maximum iteration number $N$ , Lagrangian parameter $\\beta$ for bottleneck constraint, bounds on the bottleneck penalty $i_{c}$ , learning rate $\\mu$ . ", "page_idx": 31}, {"type": "text", "text": "Output: $\\pi_{P}$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1: while iteration number $i<N$ do   \n2: Sample trajectory sets $\\mathbb{D}_{A}\\sim\\pi_{A}$ and $\\mathbb{D}_{P}\\sim\\pi_{P}$   \n3: Estimate $J_{R L}(\\pi_{A};r)$ with $\\mathbb{D}_{A}$   \n4: Optimize $\\pi_{A}$ to maximize $J_{R L}(\\pi_{A};r)$ .   \n5: Estimate $J_{R L}(\\pi_{P};r)$ with $\\mathbb{D}_{P}$ ; $J_{\\pi_{A}}(\\pi_{P};\\pi_{A},r)$ with $\\mathbb{D}_{P}$ and $\\mathbb{D}_{A}$ ;   \n6: Optimize $\\pi_{P}$ to maximize $J_{R L}(\\pi_{P};r)+J_{\\pi_{A}}(\\pi_{P};\\pi_{A},r)$ .   \n7: Estimate $J_{P A G A R}(r;\\pi_{P},\\pi_{A})$ with $\\mathbb{D}_{P}$ and $\\mathbb{D}_{A}$   \n8: Estimate $J_{I R L}(\\pi_{A};r)$ with $\\mathbb{D}_{A}$ and $E$ by following the IRL algorithm   \n9: Estimate $J_{I C}(D)$ with $\\mathbb{D}_{A},\\mathbb{D}_{P}$ and $E$   \n10: Optimize $D$ to minimize $\\dot{J}_{P A G A R}(r;\\pi_{P},\\pi_{A})+\\lambda\\cdot m a x(J_{I R L}(r)-\\delta,0)+\\beta\\cdot J_{I C}(D)$   \n11: Update $\\begin{array}{r}{\\beta:=\\operatorname*{max}\\left(0,\\beta-\\mu\\cdot\\left(\\frac{J_{I C}(D)}{3}-i_{c}\\right)\\right)}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "12: end while ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "13: return $\\pi_{P}$ ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 Instead of learning the $Q$ value function, we explicitly learn a reward function $r:\\mathbb{S}\\!\\times\\!\\mathbb{A}\\!\\times\\!\\mathbb{S}\\rightarrow$ $\\mathbb{R}$ , which takes the current state $s$ , action $a$ and the next state $s^{\\prime}$ as input, and outputs a real number as the reward ", "page_idx": 31}, {"type": "text", "text": "\u2022 We use the same loss function as that for optimizing $Q$ in RECOIL to optimize $r$ by replacing $Q(s,a)$ with $r(s,a,s^{\\prime})+\\gamma V(s^{\\prime})$ for every $(s,a,s^{\\prime})$ sampled from an offilne dataset $\\mathbb{D}$ . We denote this loss function for $r$ as $\\mathcal{I}_{I R L}(r)$ . ", "page_idx": 31}, {"type": "text", "text": "\u2022 We use the same loss function for optimizaing the value function $V$ as in RECOIL to still optimize $V$ , except for replacing the target $Q(s,a)$ with target $r(s,a,s^{\\prime})+\\gamma V(s^{\\prime})$ for every $(s,a,s^{\\prime})$ experience sampled from the offline dataset. ", "page_idx": 31}, {"type": "text", "text": "\u2022 Instead of learning a single policy as in RECOIL, we learn a protagonist and antagonist policies $\\pi_{P}$ and $\\pi_{A}$ by using the same SAC-like policy update rule as in RECOIL, except for replacing $Q(s,a)$ with $r(s,a,s^{\\prime})+\\gamma V(s^{\\prime})$ for every $(s,a,s^{\\prime})$ experience sampled from the offline dataset ", "page_idx": 31}, {"type": "text", "text": "\u2022 With some heuristic, we construct a PAGAR-loss as follows. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathcal{I}_{P A G A R}(r;\\pi_{P},\\pi_{A})}&{:=}&{\\mathbb{E}_{(s,a,s^{\\prime})\\sim E}\\left[r(s,a,s^{\\prime})\\cdot m a x(0,\\frac{\\pi_{P}(a|s)}{\\pi_{A}(a|s)})\\right]+}\\\\ &{}&{\\mathbb{E}_{(s,a,s^{\\prime})\\sim\\mathbb{D}}\\left[r(s,a,s^{\\prime})\\cdot m i n(0,\\frac{\\pi_{P}(a|s)}{\\pi_{A}(a|s)})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "\u2022 For simplicity, we multiply this PAGAR-loss $\\mathcal{T}_{P A G A R}$ with a fixed Lagrangian parameter $\\lambda=1e-3$ and add it to the aforementioned loss ${\\mathcal{I}}_{I R L}$ for optimizing $r$ . ", "page_idx": 32}, {"type": "text", "text": "C Experiment Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "This section presents some details of the experiments and additional results. ", "page_idx": 32}, {"type": "text", "text": "C.1 Experimental Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Hardware. All experiments are carried out on a quad-core i7-7700K processor running at $3.6\\:\\mathrm{GHz}$ with a NVIDIA GeForce GTX 1050 Ti GPU and a $16\\:\\mathrm{GB}$ of memory. Network Architectures. Our algorithm involves a protagonist policy $\\pi_{P}$ , and an antagonist policy $\\pi_{A}$ . In our implementation, the two policies have the same structures. Each structure contains two neural networks, an actor network, and a critic network. When associated with GAN-based IRL, we use a discriminator $D$ to represent the reward function as mentioned in Appendix B.4. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Protagonist and Antagonist policies. We prepare two versions of actor-critic networks, a fully connected network (FCN) version, and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version, the actor and critic networks have 3 layers. Each hidden layer has 100 neurons and a tanh activation function. The output layer output the mean and standard deviation of the actions. In the CNN version, the actor and critic networks share 3 convolutional layers, each having 5, 2, 2 filters, $2\\times2$ kernel size, and $R e L U$ activation function. Then 2 FCNs are used to simulate the actor and critic networks. The FCNs have one hidden layer, of which the sizes are 64. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Discriminator $D$ for PAGAR-based GAIL in Algorithm 2. We prepare two versions of discriminator networks, an FCN version and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version has 3 linear layers. Each hidden layer has 100 neurons and a tanh activation function. The output layer uses the Sigmoid function to output the confidence. In the CNN version, the actor and critic networks share 3 convolutional layers, each having 5, 2, 2 filters, $2\\times2$ kernel size, and $R e L U$ activation function. The last convolutional layer is concatenated with an FCN with one hidden layer with 64 neurons and tanh activation function. The output layer uses the Sigmoid function as the activation function. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Discriminator $D$ for PAGAR-based VAIL in Algorithm 3. We prepare two versions of discriminator networks, an FCN version and a CNN version, respectively, for the Mujoco and Mini-Grid benchmarks. The FCN version uses 3 linear layers to generate the mean and standard deviation of the embedding of the input. Then a two-layer FCN takes a sampled embedding vector as input and outputs the confidence. The hidden layer in this FCN has 100 neurons and a tanh activation function. The output layer uses the Sigmoid function to output the confidence. In the CNN version, the actor and critic networks share 3 convolutional layers, each having 5, 2, 2 filters, $2\\times2$ kernel size, and $R e L U$ activation function. The last convolutional layer is concatenated with a two-layer FCN. The hidden layer has 64 neurons and uses tanh as the activation function. The output layer uses the Sigmoid function as the activation function. ", "page_idx": 32}, {"type": "text", "text": "Hyperparameters The hyperparameters that appear in Algorithm 3 and 3 are summarized in Table 2 where we use $\\mathtt{N}/\\mathtt{A}$ to indicate using $\\delta^{*}$ , in which case we let $\\mu=0$ . Otherwise, the values of $\\mu$ and $\\delta$ vary depending on the task and IRL algorithm. The parameter $\\lambda_{0}$ is the initial value of $\\lambda$ as explained in Appendix B.4. ", "page_idx": 32}, {"type": "text", "text": "Expert Demonstrations. Our expert demonstrations all achieve high rewards in the task. The number of trajectories and the average trajectory total rewards are listed in Table 3. ", "page_idx": 32}, {"type": "table", "img_path": "VFRyS7Wx08/tmp/e40e2ca9b12c6c4201b1d1978896e3acb04262c69916b7ce8f6d7d94082a5909.jpg", "table_caption": [], "table_footnote": ["Table 3: The number of demonstrated trajectories and the average trajectory rewards "], "page_idx": 33}, {"type": "text", "text": "C.2 Additional Results ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Continuous Tasks with Non-Binary Outcomes We test PAGAR-based IRL in 5 Mujuco tasks where the task objectives do not have binary outcomes. We append the results in three Mujoco benchmarks: Walker2d-v2, HalfCheeta- $_{\\nu2}$ , Hopper-v2, InvertedPendulum- $_{\\nu2}$ and Swimmer- $_{\\nu2}$ in Figure 6 and 7. Algorithm 1 performs similarly to VAIL and GAIL in those two benchmarks. The results show that PAGAR-based IL takes fewer iterations to achieve the same performance as the baselines. In particular, in the HalfCheetah-v2 task, Algorithm 1 achieves the same level of performance compared with GAIL and VAIL by using only half the numbers of iterations. IQ-learn does not perform well in Walker2d-v2 but performs better than ours and other baselines by a large margin. ", "page_idx": 33}, {"type": "text", "text": "C.3 Influence of Reward Hypothesis Space ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In addition to the DoorKey-6x6-v0 environment, we also tested PAGAR-GAIL and GAIL in SimpleCrossingS9N2- $\\cdot\\nu\\theta$ environment. The results are shown in Figure 6 and 8. ", "page_idx": 33}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/41c2c4e93a60863485289ee75a774653a9aa307822a9e30bb3b37d95159828da.jpg", "img_caption": ["Figure 6: (Left: Walker2d-v2. Right: HalfCheeta-v2) The $y$ axis indicates the average return per episode. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/374118428212522e5ec44dc3208971893a2780f9b6daba52863bf9f25da05a74.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 7: Comparing Algorithm 1 with baselines. The suffix after each \u2018PAGAR-\u2019 indicates which IRL algorithm is utilized in Algorithm 1. The $y$ axis is the average return per step. The $x$ axis is the number of iterations in GAIL, VAIL, and ours. The policy is executed between each iteration for 2048 timesteps for sample collection. One exception is that IQ-learn updates the policy at every timestep, making its actual number of iterations 2048 times larger than indicated in the figures. ", "page_idx": 34}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/73c16a980f24a7d38ecfdc36e170a41059ed30573b42e3a6b5ca72817d151282.jpg", "img_caption": ["(a) MiniGrid-DoorKey-6x6-v0 "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "VFRyS7Wx08/tmp/3d5afab3f6cec2783450040e5b5be27ac038afa4827631b8a1135152b59687a9.jpg", "img_caption": ["(b) MiniGrid-SimpleCrossingS9N2-v0 "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "Figure 8: Comparing Algorithm 1 with baselines. The prefix \u2018protagonist_GAIL\u2019 indicates that the IRL algorithm utilized in Algorithm 1 is the same as in GAIL. The \u2018_Sigmoid\u2019 and \u2018_Categ\u2019 suffixes indicate whether the output layer of the discriminator is using the Sigmoid function or Categorical distribution. The $x$ axis is the number of sampled frames. The $y$ axis is the average return per episode. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We have discussed the task-aligned reward function is hardly predictable in IRL-based IL setting. As mentioned in Section 5, \"How to build $R_{E,k}\"$ , it is designer\u2019s decision to set value for the parameter $k$ . ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Every lemma, proposition and theorem have the set of assumptions included in them. The proves are included in Appendix. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We disclosed all the information in Appendix C.1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 36}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide the source code in our submission, including the instructions on how to reproduce the results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 37}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We disclosed all the information in Appendix C.1. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 37}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The curves in the plots are smoothed. They reflect how the policies\u2019 average returns change as the learning episodes increase. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We disclosed all the information in Appendix C.1. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 38}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We conform with NeurIPS Code of Ethics. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 38}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not discuss and is not intended to cause significant societal impacts. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper does not release data or models that have a high risk for misuse. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 39}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 40}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]