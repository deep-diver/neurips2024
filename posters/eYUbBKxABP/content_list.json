[{"type": "text", "text": "Formalising Anti-Discrimination Law in Automated Decision Systems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the legal challenges in automated decision-making by analysing conven  \n2 tional algorithmic fairness approaches and their alignment with anti-discrimination   \n3 law in the United Kingdom and other jurisdictions based on English common law.   \n4 By translating principles of anti-discrimination law into a decision-theoretic frame  \n5 work, we formalise discrimination and propose a new, legally informed approach   \n6 to developing systems for automated decision-making. Our investigation reveals   \n7 that while algorithmic fairness approaches have adapted concepts from legal theory,   \n8 they can confilct with legal standards, highlighting the importance of bridging the   \n9 gap between automated decisions, fairness, and anti-discrimination doctrine. ", "page_idx": 0}, {"type": "text", "text": "10 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "11 Automated decision-making using predictive models is becoming increasingly important in many   \n12 areas of society, including lending [60, 100, 107], criminal justice [31, 14, 151], hiring [64, 59, 25],   \n13 and welfare eligibility [41, 56, 113]. Instances of large-scale failures, from disproportionately harming   \n14 vulnerable people in welfare eligibility assessments [113] to bias in consumer lending [80], highlight   \n15 the need for lawful implementation. Scrutiny of ML-based decisions is heightened by concerns about   \n16 replicating human biases and historical inequality [97, 41, 93].   \n17 Concerns about algorithmic bias have spurred research into fair ML. Early discourse on fairness   \n18 in ML was relatively narrow due to technical constraints [29, 61]. More recently, researchers have   \n19 developed formal definitions of fairness in algorithmic decisions and methods to measure fairness   \n20 in predictive models [49, 31, 27, 147, 87, 85]. Algorithmic fairness definitions generally measure   \n21 prediction disparities across groups with different legally protected characteristics [90, 136, 87, 17].   \n22 This research has resulted in several proposals, including statistical metrics to assess the fairness of   \n23 individual predictive models [136, 111, 22, 24], fairness for model auditing [70, 103, 63, 89, 98], and   \n24 fairness constraints on models [31, 148, 50, 145, 12].   \n25 These criteria simplify fairness into measurements of disparity that do not inherently map to unlawful   \n26 discrimination. The usefulness of these metrics in practice is limited as incomplete or even irrelevant   \n27 measures for legal investigations. There have been important efforts to bridge the gap between legal   \n28 and technical approaches to fair ML [81, 55, 51, 144, 139, 1, 46]. Lawyers have highlighted the   \n29 challenges of the narrow construction of fairness metrics focusing on disparity in predictions rather   \n30 than more nuanced definitions of discriminatory conduct and the broader context of the automated   \n31 decision-making process [55, 51, 144, 1]. We aim to contextualise and formalise legal concepts of   \n32 algorithmic discrimination beyond the narrow construction of statistical disparity.   \n33 The predominance of US analysis of fairness and discrimination in ML, lack of non-US ML   \n34 datasets [78], and the limited legal scholarship translating these concepts, has inadvertently fos  \n35 tered a series of misconceptions that pervade the field. However, very few papers have engaged with   \n36 anti-discrimination laws outside of the United States [143, 139, 1, 140, 67, 76]. We aim to introduce   \n37 new principles and methods to deal with the issues identified in this literature. By avoiding the   \n38 nuanced legal realities of other jurisdictions, models designed to comply with US laws may breach   \n39 UK laws or those in comparable jurisdictions. Our paper addresses this gap by providing a rigorous   \n40 analysis of UK discrimination law, correcting some mischaracterisations, and establishing a more   \n41 accurate foundation for developing fair ML in the UK and its related jurisdictions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "42 1.1 Automated Decision-Making ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "43 Let $x_{i}\\in\\mathbb{R}^{p}$ be a vector of observed attributes for individual $i$ . A decision-maker must choose a   \n44 decision $a\\in A$ , where $\\boldsymbol{\\mathcal{A}}$ is closed. Further, we assume that the decision-maker wants to decide   \n45 based on a future outcome $y_{i}\\in\\mathcal{V}$ for individual $i$ . Here, we assume $\\mathcal{V}=\\mathbb{N}$ , which can be relaxed.   \n46 Decision-making under uncertainty has long been studied in statistical decision theory [108, 32, 13,   \n47 99]. Let $u(y,a)$ be a utility function that summarises the utility for the decision-maker. The optimal   \n48 decision is then ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\na^{\\star}=\\underset{a\\in\\mathcal{A}}{\\arg\\operatorname*{max}}\\sum_{y\\in\\mathcal{Y}}u(a,y)p(y|a)\\,.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "49 The decision-maker usually neither knows $y_{i}$ nor $p(y|a)$ at the time of the decision. Hence, the   \n50 decision must be based solely on $x_{i}$ . In an SML setting, a prediction model ${\\hat{p}}(y|x)$ is trained to   \n51 compute the predicted probability distribution (pmf) $\\hat{\\pmb{\\pi}}_{i}=\\hat{p}(y|x_{i})$ for individual $i$ , with the support   \n52 on $\\boldsymbol{\\wp}$ . Further, let $\\hat{y}(\\hat{\\pmb{\\pi}}_{i}\\bar{)}\\in\\mathcal{Y}$ be the classification made based on $\\hat{\\pi}_{i}$ . In simple settings, the decision   \n53 can be formulated as a decision function $d(\\hat{\\pmb{\\pi}}_{i})\\in\\mathcal{A}$ that is used to choose an appropriate action   \n54 based on $\\hat{\\pi}_{i}$ . In the binary $y$ and $a$ case, it reduces to a simple threshold $\\tau$ , i.e., $\\bar{d}(\\hat{\\pi})\\,\\bar{=}\\,I(\\hat{\\pi}\\leq\\tau)$ ,   \n55 where $I$ is the indicator function and $\\hat{\\pi}_{i}\\,=\\,\\hat{p}(y\\,=\\,1\\,\\mid\\,x_{i})$ . We often train a model ${\\hat{p}}(y|x)$ based   \n56 on previous data $D=\\left(\\mathbf{y},\\mathbf{X}\\right)$ , drawn from a population $p(y,x)$ , where both $x_{i}$ and $y_{i}$ are known.   \n57 Replacing $p(\\boldsymbol{y}|\\boldsymbol{x}_{i})$ with the predictive model ${\\hat{p}}(y|x_{i})$ in Eq. 1 gives an optimal decision. ", "page_idx": 1}, {"type": "text", "text": "58 1.2 Algorithmic Fairness ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "59 To define algorithmic fairness, we separate $x_{i}$ into protected and legitimate features $\\boldsymbol{x}_{i}=\\left(x_{p i},x_{l i}\\right)$ ;   \n60 we drop $i$ to simplify notation. Here, $x_{p}\\in{\\mathcal{C}}$ indicates protected attributes, with $\\mathcal{C}$ being the set of   \n61 different groups. Legally protected characteristics commonly identified in datasets include gender,   \n62 race, and age. Many fairness metrics aim to evaluate the fairness of an SML model for commonly   \n63 identified protected characteristics in datasets, including gender and race [49, 31, 82, 85].   \n64 Statistical parity, or demographic parity, is one of the central algorithmic fairness metrics [31, 136,   \n65 90, 82]. For statistical parity to hold, it requires that ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}\\left[\\hat{p}(y|x)~|~x_{p}\\right]=\\mathbb{E}_{x}\\left[\\hat{p}(y|x)\\right]\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "66 such that the model predictions, in expectation over $x$ , need to be the same for the different groups [31,   \n67 136]. Given that the decision function $d(\\pi)$ is the same for the different groups, statistical parity results   \n68 in equal decisions for the different groups. However, we discuss later in this paper that, in practice,   \n69 statistical parity may exacerbate inequality or even result in unlawful discrimination [10, 74, 63].   \n70 Conditional statistical parity extends statistical parity to account for legitimate features $x_{l}$ . The   \n71 model predictions should only differ across protected groups to the extent that the difference is   \n72 conditional on legitimate factors [31, 136, 23]. This can be formalised as, ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol{x}}\\left[\\hat{p}(y|\\boldsymbol{x})~|~\\boldsymbol{x}_{l},\\boldsymbol{x}_{p}\\right]=\\mathbb{E}_{\\boldsymbol{x}}\\left[\\hat{p}(y|\\boldsymbol{x})~|~\\boldsymbol{x}_{l}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "73 so that, conditional on legitimate features $x_{l}$ , there should not be any difference in predictions between   \n74 groups given by the protected attribute. Below, we discuss the legitimacy of variables that correlate   \n75 to protected attributes [34, 77].   \n76 Other similar group comparison metrics have been proposed, such as error parity, balanced clas  \n77 sification rate, and equalised odds [49, 31, 136, 90, 82, 30]. Also, more individual approaches   \n78 to parity have considered whether otherwise identical individuals are treated differently if they   \n79 have different protected attributes [34, 68]. Finally, ideas from causal inference and counterfactual   \n80 analysis have also been proposed to measure outcome consistency for individuals across protected   \n81 groups [75, 69, 106, 149, 26, 142, 92, 6]   \n83 The algorithmic fairness literature largely identifies statistical disparities in predicted outcomes for   \n84 binary marginalised groups. Legally, discrimination is both broader and more detailed. Not all   \n85 actions perceived as discriminatory are unlawful, and some non-obvious actions may be prohibited.   \n86 Anti-discrimination law only applies to select duty-bearers in certain conditions [65]. Individuals\u2019   \n87 friendship choices being based on race are not legally regulated, despite sometimes seeming unfair [36,   \n88 65]. It only applies to protected attributes. An algorithm that rejects a loan application because the   \n89 applicant uses an Android phone rather than an iOS device may seem unfair because it does not   \n90 reflect the true default risk but is a proxy for the applicant\u2019s income [2, 76]. However, in isolation, this   \n91 would not be unlawful discrimination under UK law because poverty is not a protected attribute [96].   \n92 The prohibition on discrimination traces its legal roots to the Universal Declaration of Human Rights,   \n93 which established equality and freedom from discrimination as fundamental human rights, further   \n94 advanced in several international treaties [132, 88], and enacted as legislation worldwide spurred by   \n95 the Civil Rights Movement [86, 65]. The United Kingdom implemented several anti-discrimination   \n96 laws in the 20th century [118, 119, 116], which were consolidated in the Equality Act 2010 [40].   \n97 The Equality Act protects \u201cage; disability; gender reassignment; marriage and civil partnership;   \n98 pregnancy and maternity; race; religion or belief; sex; sexual orientation\u201d [40, s 4]. Algorithmic   \n99 fairness literature has often oversimplified these protected characteristics as simply identifying visible   \n100 traits when each has complex social meanings [57]. One complexity is, for example, the difference   \n101 between a person with a protected attribute by biological fact or by identifying with a protected   \n102 group [73]. UK anti-discrimination law distinguishes between direct discrimination and indirect   \n103 discrimination. While analogous to the US disparate treatment and disparate impact doctrine, there   \n104 are important distinctions, meaning they should not be so easily elided [1].   \n105 Direct discrimination occurs when an individual is treated less favourably than another based on   \n106 a protected characteristic [40, s 13]. To establish direct discrimination, it is necessary to identify   \n107 the specific protected characteristic involved, demonstrate the less favourable treatment (by real or   \n108 hypothetical comparison), and prove that this treatment was caused \u201cbut for\u201d the protected attribute.   \n109 The intention of the decision-maker is not required or necessary [123, 131].   \n110 Indirect discrimination refers to a policy, criterion, or practice (PCP) that disproportionately   \n111 disadvantages a group with a particular protected attribute compared to those without [40, s 19]. To   \n112 prove indirect discrimination, one must identify such a PCP, show that it puts a group defined by its   \n113 protected attribute at a particular disadvantage compared to those without such attribute, and evaluate   \n114 whether it is justifiable as a proportionate means of achieving a legitimate aim.   \n115 English common law is either in force or is the dominant influence in 80 legal systems that govern   \n116 approximately 2.8 billion people, not including the US [28]. UK anti-discrimination law is very similar   \n117 to numerous Commonwealth and common law jurisdictions, including Australia [3], Canada [20],   \n118 India [47], New Zealand [91], South Africa [110], and the pending bill in Bangladesh [9]. European   \n119 Union law also has broadly the same principles and discrimination case law evolved in parallel during   \n120 the UK\u2019s membership [45]. It is increasingly important to gain a nuanced understanding of unlawful   \n121 discrimination in AI systems as new laws aim to prevent future harms [44, 16]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "122 1.4 Contributions and Limitations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "123 This paper makes four core contributions at the intersection of automated decision-making, fairness,   \n124 and anti-discrimination doctrine.   \n125 1. We formalise critical aspects of anti-discrimination doctrine into decision-theoretic formalism.   \n126 2. We analyse the legal role of the data-generating process (DGP) and develop the DGP as a   \n127 theoretical framework to formalise the legitimacy of the prediction target $y$ and the features $x$ in   \n128 supervised models for automated decisions.   \n129 3. Further, we consider the legal and practical effects of approximating the DGP in supervised   \n130 models. We propose conditional estimation parity as a new, legally informed target.   \n131 4. Finally, we provide recommendations on creating SML models that minimise the risk of unlawful   \n132 discrimination in automated decision-making.   \n133 Our paper is formally limited to analysing and providing novel recommendations for the UK. While   \n134 we discuss related jurisdictions that are functionally similar and based on English common law,   \n135 specific legal advice should be followed with respect to different jurisdictions. Accountability varies   \n136 by jurisdiction and context, which is why our paper underscores the importance of careful, informed   \n137 classification by experts with appropriate legal advice. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "138 2 Automated Decisions and Discrimination ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "139 2.1 Legitimacy of True Differences ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 In SML, it is crucial to differentiate unlawful discrimination from mere statistical disparities and con  \n141 cepts of algorithmic fairness. While formal equality may map to statistical parity, anti-discrimination   \n142 laws in the UK and related jurisdictions aim to achieve substantive equality. Despite the general   \n143 rule that individuals should not receive less favourable treatment based on their protected attributes,   \n144 courts acknowledge that treating all groups the same can actually disadvantage a protected group   \n145 and minimise important structural and true differences [137, 143]. Therefore, substantive equal  \n146 ity may sometimes require legitimate differential treatment because of the true differences among   \n147 individuals [137, 52, 144, 139].   \n148 For instance, insurance decisions that might otherwise be construed as discriminatory \u2013 specifically   \n149 concerning gender reassignment, marriage, civil partnership, pregnancy, and sex discrimination \u2013   \n150 are permissible if they are based on reliable actuarial data and executed reasonably [40, Sch 9. s   \n151 20]. Financial services can also \u201cuse age as a criterion for pricing risk, as it is a key risk factor   \n152 associated with for example, medical conditions, ability to drive, likelihood of making an insurance   \n153 claim and the ability to repay a loan\u201d [117, para. 7.6]. These exemptions highlight legal recognition   \n154 that certain group distinctions, particularly those involving risk assessment, are relevant and necessary   \n155 for the equitable operation of such services. Similar statutory exemptions are found in other similar   \n156 anti-discrimination laws, including the European Union [43, art 2], Australia [8, s 30-47], Canada   \n157 [20, s 15], New Zealand [91, s 24-60] and South Africa [110, s 14]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "158 2.2 True Data Generating Process ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "159 Therefore, an important aspect from the legal perspective that is overlooked in the existing literature   \n160 is the distinction between a \u201ctrue data-generating\u201d process (DGP) and the estimated model ${\\hat{p}}(y|x)$ .   \n161 To formalise, we assume that there exists a true DGP, $D\\sim p(y,x)$ , where $D_{i}=(y_{i},x_{i})$ . Further, we   \n162 use $p(\\boldsymbol{y}|\\boldsymbol{x}_{i}^{\\mathrm{true}})$ to denote the true probability (pmf) for individual $i$ , given the true features $x_{i}^{\\mathrm{{true}}}$ .   \n163 We make multiple observations on the role of the \u201ctrue\u201d model and its use in connecting predictive   \n164 modelling and legal reasoning.   \nFirst, understanding the limits of predictive models is crucial to explore inherent uncertainties and   \n166 limitations in predictions. The true model is, in practice, never observed or known. When developing   \n167 ${\\hat{p}}(y|x)$ , the target is often to select the model with the best predictive performance, which is closely   \n168 connected to the role of the true DGP [15, 134, 135, 133]. For this reason, the \u201ctrue\u201d model may   \n169 include features in $x_{i}^{\\mathrm{{true}}}$ that are not observed in the data, sometimes referred to as an $\\mathcal{M}$ -open setting   \n170 when the \u201ctrue\u201d model is not included in the set of candidate models [15, 135].   \n171 Second, we assume that $p(\\boldsymbol{y}|\\boldsymbol{x}_{i})$ is a probability distribution over $\\boldsymbol{\\wp}$ , introducing some level of   \n172 aleatoric uncertainty in the true underlying process [95, 58, 114]. This means that perfect prediction   \n173 of $y_{i}$ may not be possible, even with knowledge of the true DGP. The distinction between aleatoric   \n174 and epistemic uncertainty is important from a legal perspective. The reason is simple: the uncertainty   \n175 coming from estimation is the (legal) responsibility of the modeller, while the aleatoric uncertainty   \n176 can instead be considered a true underlying general risk.   \n177 Third, the true DGP connects to judicial legal reasoning. Courts must engage theoretically with   \n178 legal and normative conceptions of what is justifiable and what constitutes unlawful discrimination.   \n179 Judges consider legitimacy, proportionality, and necessity when evaluating actions, and hypothetical   \n180 alternatives, that led to less favourable treatment. Although, courts are not oracles. Discrimination   \n181 case law may not pinpoint what the perfect decision should have been. However, courts will engage   \n182 in a similar theoretical process of reasoning about the decision-making process to the true DGP to   \n183 understand whether the actions were justified or unlawful. We explain legal reasoning within this   \n184 framework throughout the paper and in a real-world case on unlawful discrimination in algorithmic   \n185 decision-making (see Appendix A). ", "page_idx": 3}, {"type": "text", "text": "186 2.3 Estimation Parity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "187 Legally, distinguishing between a true difference and an estimated one is important. We approximate   \n188 the true DGP with a model ${\\hat{p}}(y|x)$ based on training data when training an SML model. The   \n189 approximation introduces estimation error ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\epsilon_{i}=\\hat{\\pmb{\\pi}}_{i}-\\pmb{\\pi}_{i}=\\hat{p}(y_{i}|x_{i})-p(y_{i}|x_{i}^{\\mathrm{true}})\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "190 Algorithmic fairness literature often assumes the absence of estimation error [see e.g., 49] or assumes   \n191 that the true causal structure is known [150, 68, 26, 21]. In practice, this is rarely the case. Hence,   \n192 it is crucial, both practically and legally, to distinguish between the true underlying probabilities   \n193 $\\pi_{i}$ and the estimated probabilities $\\hat{\\pi}_{i}$ . While the true underlying probability may sometimes be   \n194 defensible (Section 2.2), introducing an estimation error that disadvantages individuals based on   \n195 protected attributes invokes discrimination liability.   \n196 As the model will try to approximate the true data-generating process, modellers\u2019 expectations are   \n197 difficult to ascertain. The law is unlikely to set a deterministic standard that any adverse effects of   \n198 estimation will make a modeller liable. The modeller should try to approximate the true model as   \n199 much as possible [see 4, 141, 135, 133, for discussions on model misspecification]. However, where   \n200 an estimation disparity reaches a threshold for discriminatory effects, the legal evaluation would   \n201 require analysing the steps taken to test and mitigate estimation disparity (even though the intent is   \n202 immaterial).   \n203 The potential bias in training data presents a risk that the estimation model will introduce bias against   \n204 individuals with protected attributes (Section 2.6). Historical discriminatory lending practices, for   \n205 example, could be perpetuated through biased training data [18, 104]. Such biased estimations   \n206 may introduce biased outcomes that are not reflective of true differences, potentially leading to   \n207 discriminatory outcomes. Therefore, we introduce \u201cConditional Estimation Parity\u201d to formalise the   \n208 legal context of estimation.   \n209 Conditional Estimation Parity is the difference in estimation error between groups with a protected   \n210 attribute, given legitimate features, i.e., ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol{x}}[\\epsilon\\mid x_{p},x_{l}]=\\mathbb{E}_{\\boldsymbol{x}}[\\epsilon\\mid x_{l}]\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "211 Reducing the error in Eq. 4 is expected to diminish the risk of conditional estimation disparity.   \n212 However, assessing conditional estimation parity is complex due to inherent challenges in evaluating   \n213 estimation error.   \n214 It is crucial to examine both mathematical and legal causal theories of why certain differences are   \n215 legitimate bases to make classification distinctions [71]. We examine the mathematical basis for   \n216 identifying statistical disparities in the context of unlawful discrimination. In Section 2.5, 2.7, and 2.6   \n217 we consider the causal relationships between legitimate differentiation and unlawful discrimination. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "218 2.4 Statistical Disparities and Prima Facie Discrimination ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "219 To initiate a claim for discrimination, a claimant must establish a prima facie case [37, 40, s 136].   \n220 Sufficient evidence must be produced to show that unlawful discrimination may have occurred,   \n221 including by showing discriminatory effects or harm against an individual or group caused by the   \n222 decision-maker\u2019s action [37, 65]. Statistical evidence can be used to prove less favourable treatment   \n223 or particular disadvantage, but by design, it shows correlations, and \u201ca correlation is not the same   \n224 as a causal link\u201d [130, para. 28]. We explain the threshold for legal causation at the trial stage in   \n225 Section 2.5. Although, at this stage, a mere correlation between the adverse effect on the person   \n226 and the decision-maker\u2019s action will suffice [65]. The size of the disparity is relevant. Smaller   \n227 disparities are less likely to trigger legal inquiry under anti-discrimination laws [127]. Courts will   \n228 compare statistical evidence showing the different effects and outcomes between a disadvantaged   \n229 group compared to a group without the protected attribute. The significance of the statistical disparity   \n230 hinges on the specifics of the case [127, 124]. The thresholds for statistical significance are flexible   \n231 and often resisted by courts to avoid excessive dependence on data [138]. The UK has specifically   \n232 avoided thresholds like those used to measure statistically significant disparity in the US [105, 10].   \n233 Statistical disparities, as identified through algorithmic fairness metrics, may indicate a reason to   \n234 consider whether discrimination has arisen. However, without taking context and potential true and   \n235 legitimate differences into account, these disparities hold little legal weight (see Section 2.1). We can   \n236 formalise this as the legal target being to minimise the conditional estimation disparity ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\omega=||\\mathbb{E}_{x}\\left[\\epsilon_{i}\\mid x_{l},x_{p}\\right]-\\mathbb{E}_{x}\\left[\\epsilon_{i}\\mid x_{l}\\right]\\mid\\mid_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "237 where $||\\cdot||_{2}$ is the euclidean norm. This target generalises the idea of minimising conditional   \n238 statistical parity. If we assume true conditional statistical parity, i.e. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\boldsymbol{x}}\\left[p(y_{i}|x_{i}^{\\mathrm{true}})\\mid x_{l},x_{p}\\right]=\\mathbb{E}_{\\boldsymbol{x}}\\left[p(y_{i}|x_{i}^{\\mathrm{true}})\\mid x_{l}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "239 then the target in Eq. 6 will be reduced to minimise the conditional statistical parity (see Eq. 3).   \n240 Although, this is only true as long as there are no true differences.   \n241 Hence, if true statistical parity does not hold, it is explained by true differences between groups. If   \n242 there is a true difference, such as age in financial services, forcing conditional statistical parity would   \n243 harm the protected group, most likely resulting in unlawful discrimination. This result aligns with   \n244 previous observations about the risks of forcing parity metrics [31, 144, 54]. Courts may need to be   \n245 more flexible in the type of statistical data they consider to establish a prima facie case by considering   \n246 non-comparative adverse effects in their assessment. Therefore, deferring to conditional estimation   \n247 parity provides an avenue for a contextually informed assessment. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "248 2.5 Legal Causation and the Utility Function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "249 To lawyers, causation is the relationship between an act, i.e., an action or decision, and its effect,   \n250 which requires two questions: (1) factually, but for the act, would the consequences have occurred;   \n251 (2) is the act a substantial cause of the consequence to apply responsibility. We are concerned with   \n252 the first question. Direct discrimination \u201crequires a causal link between the less favourable treatment   \n253 and the protected characteristic\u201d; indirect discrimination \u201crequires a causal link between the PCP and   \n254 the particular disadvantage suffered by the group and individual\u201d [130, para. 25]. In an algorithmic   \n255 context, this causal link requires asking whether $i$ would have received the same action or decision   \n256 a, but for their protected attribute $x_{p}$ or the PCP that indirectly relates to their protected attribute   \n257 $x_{p}$ [122, 123]. For instance, whether an individual would have suffered the disadvantage but for the   \n258 protected attribute would be discriminatory regardless of the decision-maker\u2019s intention [1]. This is a   \n259 notable distinction from certain aspects of US discrimination doctrine.   \n260 From a decision-theoretic perspective, the protected attribute $x_{p}$ can affect the decision $a$ either   \n261 through the utility function $u(a,y)$ or through the model $\\hat{p}(y|\\bar{x})$ . Discrimination may occur if   \n262 the utility function in Eq. 1 differs for different groups defined by the protected attribute. Such a   \n263 difference would mean that an individual or whole group with a protected attribute is treated less   \n264 favourably than those without a protected attribute given the same model ${\\hat{p}}(y|x)$ . Such a difference in   \n265 the utility function would risk unlawful discrimination. Specifically, if $u(a,y)$ is changed for different   \n266 persons, either directly based on a protected attribute or indirectly has the effect of disproportionately   \n267 disadvantaging a group with a protected characteristic without justification (see Section 2.6).   \n268 Having different ${\\hat{p}}(y|x)$ , on the other hand, would mean that there is a legal causation between the   \n269 decision $a$ and $x_{p}$ . This might either be motivated by true differences (see Section 2.1) or a result of   \n270 conditional estimation disparity. In the latter case, this might be a case of legal causation, i.e., that the   \n271 model is poor, and hence, the modelling has resulted in disadvantaging a protected group. Therefore,   \n272 we can view the causal structure of ${\\hat{p}}(y|x)$ as central to avoiding unlawful discrimination. However,   \n273 not considering causal structures could lead to conditional estimation disparity, and potentially result   \n274 in unlawful discrimination.   \n275 Legal causation focuses on the legal causal link between $x_{p}$ and the decision $a$ . In addition, legal   \n276 causation is less formal than common definitions of causal effects in ML. Courts, at least outside of   \n277 the US, are effects-orientated, and a wide range of forms of a \u201clegal causal link\u201d could be identified   \n278 [109, 65]. Much of the causal-based fairness literature formulates \u201ccausation\u201d on the true causal   \n279 model structure in ${\\hat{p}}(y|x)$ , i.e., the study of the causal effect of $x$ , due to outside interventions on $y$   \n280 [101, 10, 149, 21]. However, this formulation is not the same as that of legal causation.   \n281 In this discussion, the parallels to other discrimination studies become evident in how it would   \n282 affect automated decision-making, particularly taste-based and statistical discrimination. Taste-based   \n283 discrimination[11], could arise if only the utility function $u(a,y)$ unjustifiably disfavours a group   \n284 based on protected attributes $x_{p}$ . Statistical discrimination, on the other hand, arises when decision  \n285 makers use group-level statistics as proxies for individual characteristics due to imperfect information   \n286 [7, 102]. Statistical discrimination parallels the disadvantaging of a group due to having different   \n287 $\\hat{p}(x|y)$ . While these types of discrimination are generally prohibited, statistical discrimination can be   \n288 legally permissible in some circumstances (see Section 2.1). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "289 2.6 Legitimate aim and $y$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "290 Decision-makers must consider the legitimacy of using an SML model by explicitly defining its   \n291 purpose and the outcome variable $y$ . In algorithm design, social implications should be considered [87,   \n292 57, 63]. Additionally, this aligns the model\u2019s use with legal expectations.   \n293 If the court believes sufficient evidence of discrimination exists, the burden shifts to the respondent to   \n294 disprove allegations of unlawful discrimination [38]. Indirect discrimination can be justified if the   \n295 PCP is a proportionate means of achieving a legitimate aim [40, s 19(2)(d)]. Identifying a legitimate   \n296 aim is closely connected to the choice of $y$ , the unknown entity used for decision-making. If the   \n297 choice of $y$ is legitimate based on context and the benefit outweighs any potential harm, there is a   \n298 lower risk of unlawful discrimination [35].   \n299 The legitimacy of the aim depends on the decision-makers\u2019 raison d\u2019\u00eatre [65]. In Homer, the Court   \n300 established a legitimate aim must \u201ccorrespond to a real need and the means used must be appropriate   \n301 with a view to achieving the objective and be necessary to that end\u201d [128, 35, 39]. In lending, it is   \n302 a legitimate aim to protect the repayment of their loans or at least secure their loans. In fact, \u201cthe   \n303 mortgage market could not survive without that aim being realised\u201d [126, para. 79].   \n304 For a legitimate $y$ to be an exception to indirect discrimination, the PCP must be a proportionate   \n305 means of achieving the legitimate $y$ [40, s 19(2)(d)]. To be proportionate, it must be an appropriate   \n306 means of achieving the legitimate aim and (reasonably) necessary to do so [128]. Such analysis   \n307 will turn on the facts of each case. However, it will require evaluating whether the design choices   \n308 were \u201cappropriate with a view to achieving the objective and be necessary\u201d by weighing the need   \n309 against the seriousness of detriment to the disadvantaged group [39, para. 151]. This will require   \n310 considering whether non-discriminatory alternatives were available [128]. Measures to improve   \n311 accuracy, maximise beneftis over costs, minimise estimation error, or condition for protected attributes   \n312 may all be relevant considerations for whether the modeller\u2019s choices were proportionate means of   \n313 achieving a legitimate $y$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "314 If the estimated outcome $\\tilde{y}$ approximates the true outcome $y$ , this can lead to biased predictions. Let ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma_{i}=||p(\\tilde{y}_{i}|x_{i}^{\\mathrm{{true}}})-p(y_{i}|x_{i}^{\\mathrm{{true}}})||_{2}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "315 then, if the expectation of $\\gamma$ condition on $x_{l}$ shows a disparity, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x}[\\gamma\\mid x_{p},x_{l}]\\neq\\mathbb{E}_{x}[\\gamma\\mid x_{l}]\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "316 it suggests the use of $\\tilde{y}$ is inappropriate and might be discriminatory. ", "page_idx": 6}, {"type": "text", "text": "317 To illustrate with an example, if a bank\u2019s training data is outdated or sourced from a different country,   \n318 it may not accurately represent the current population relevant to the model. This discrepancy can   \n319 lead to biased estimations, particularly if the data reflects historical prejudices. For instance, the   \n320 model might unjustly associate certain demographics with higher default risk, not because of true   \n321 differences but biased historical data [as warned in 33]. ", "page_idx": 6}, {"type": "text", "text": "322 2.7 Legitimate $x$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "323 One of the more crucial aspects of SML for automated decision-making is the choice of features $x$ .   \n324 The aim and $y$ will help inform the choice of features to include in the model. We can separate three   \n325 types of features from a legal perspective: features with protected attributes $x_{p}$ , legitimate features $x_{l}$ ,   \n326 and non-legitimate or illegitimate features $x_{n}$ . The distinction between $x_{l}$ and $x_{n}$ depends on whether   \n327 the feature can be considered legitimately related to $y$ (see Section 2.5). Causal fairness literature   \n328 has engaged with questions of discriminatory variables through the lens of proxy discrimination   \n329 [69, 115]. Proxy discrimination has a specific legal meaning under UK law that relates to direct   \n330 discrimination, unlike much of the US literature on proxy discrimination that relates to indirect forms   \n331 of discrimination. Here, we explain the UK legal implications of such causal relationships between   \n332 variables and we provide a real-world example in Appendix A.   \n334 Direct discrimination in automated decisions may arise when members of, or an entire protected   \n335 group, is affected. Where a model ${\\hat{p}}(y|x)$ uses a protected attribute $x_{p}$ , and there is a difference   \n336 in predictions between the protected groups defined by $x_{p}$ , this risk arises. Models have directly   \n337 used protected characteristics, giving rise to direct discrimination [94, see discussion in Appendix].   \n338 Direct discrimination may arise when a feature is an exact proxy for a protected attribute. In $L e e\\nu$   \n339 Ashers, Lady Hale explained that the risk of direct discrimination also arises if a decision is based on   \n340 a feature that \u201cis not the protected characteristic itself but some proxy for it\u201d [129]. Therefore, direct   \n341 discrimination can arise even where $x_{p}$ has been removed because there is a feature which is an exact   \n342 proxy that is \u201cindissociable\u201d or has an \u201cexact correspondence\u201d to $x_{p}$ [130, 129]. Formally, we can   \ndefine an exact proxy as a feature $\\tilde{x}_{p}$ with a perfect or almost perfect correlation with $x_{p}$ [115].   \n344 UK courts have accepted that an exact proxy would be pregnancy because \u201cpregnancy is unique   \nto the female sex\u201d [121, 125]. If a model uses pregnancy or maternity leave as a feature, collected   \n346 from CV information, for example, it would have the effect of using an exact proxy $\\tilde{x}_{p}$ that could   \n347 hypothetically be the basis for a direct discrimination claim.   \n348 Given the relevance of $x_{p}$ to direct discrimination, modellers have been encouraged to remove pro  \n349 tected attributes when designing ML models [105, 62, 48]. These claims are usually based on the US   \n350 Equal Protection Clause, which subjects classifications based on certain protected characteristics, such   \n351 as race, to strict scrutiny [146]. The focus on excluding certain data inputs is one form of discrimina  \n352 tion prevention [146, 46], but not under UK law. Further, simply removing protected characteristics   \n353 reduces accuracy and utility [150, 66], and does not remove the risk of discrimination [34, 79, 72].   \nThis reasoning connects to the true DGP. If a protected attribute like gender is inherent in the DGP,   \n355 removing it does not eliminate discrimination but instead may introduce it. Taking a gender-neutral   \n356 approach to recidivism predictions may have the adverse effect of discrimination against women who   \n357 would otherwise have received lower risk scores [31]. In Loomis, the Court accepted that in recidivism   \n358 algorithms, \u201cif the inclusion of gender promotes the accuracy, it serves the interests of institutions and   \n359 defendants, rather than a discriminatory purpose\u201d [112, 766]. Hence, if the inclusion of $x_{p}$ improves   \n360 the accuracy and beneftis the protected group, it may avoid the risk of discriminatory purposes. There   \n361 is an absence of any legal guidance in the UK on the relationship between true probabilities and   \n362 protected attributes in automated decision-making. Pending further legal guidance, it is important to   \n363 carefully consider whether including $x_{p}$ is relevant to promote accuracy and conditional estimation   \n364 parity. Removing protected attributes often ignores the true probabilities for the legitimate differences   \n365 between protected groups, affecting the lawfulness of its outcomes.   \n366 Therefore, removing $x_{p}$ will not avoid liability for unlawful direct discrimination by itself. Even   \n367 if a model ignores $x_{p}$ , in practice, it may rely on other data points acting as proxies with \u201cexact   \ncorrespondence\u201d to a protected characteristic $\\tilde{x}_{p}$ . Importantly, this diverges from US law and   \n369 highlights that intention is immaterial to UK direct discrimination [Cf. e.g., 5, 115]. UK law focuses   \n370 on the discriminatory effects rather than a formalistic view of whether $x_{p}$ is considered or not. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "371 2.7.2 Defining $x_{l}$ and $x_{n}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "372 Indirect discrimination may arise if a PCP appears to apply equally to everyone but disadvantages   \n373 members of a protected group. Both forms of discrimination can arise using an exact proxy or a   \n374 weak proxy in a PCP. Therefore, identifying legitimate features is challenging when many features   \n375 correlate to protected groups. We define non-legitimate features $x_{n}$ as features not legitimate in   \n376 the context of the true DGP (Section 2.2). In practice, this means a non-legitimate feature is one   \n377 that, if included, would not contribute to the predictive performance of the optimal model, i.e., the   \n378 one with the lowest estimation error (Section 2.3). Therefore, $x_{n}$ would not improve the predictive   \n379 performance if a modeller had the true features.   \n380 For example, hair length strongly correlates to gender in many cultural contexts but is unlikely to   \n381 contribute to the consumers\u2019 true default risk. Boyarskaya et al. explain the absence of a \u201ccausal   \n382 story\u201d between hair length and loan repayment because hair length would not be part of a true model   \n383 for the risk of default [19]. Therefore, hair length is an example of $x_{n}$ in a lending context.   \n384 For comparison, the legitimacy of zip codes illustrates the nuanced nature of legitimate features.   \n385 While a zip code may correlate with race in some contexts, it might be a legitimate variable in   \n386 other situations. For example, in an application for home insurance covering flood risk, zip codes   \n387 are invaluable proxies for granular information such as geographical features, land topography and   \n388 historical flooding. Therefore, in the best model for property flood insurance decisions, zip code will   \n389 improve the predictive performance as a legitimate proxy for data within the true DGP. However, in a   \n390 university application, there should be no predictive or causal relationship to merit for acceptance. In   \n391 such cases, zip code likely acts as a proxy for race or the unprotected characteristic of socio-economic   \n392 status and would be $x_{n}$ . So, in some circumstances, the zip code would be legitimate $x_{l}$ , but in others,   \n393 it may not be $x_{n}$ . It will also be relevant to consider whether a less discriminatory feature is available,   \n394 i.e., one with less correlation to a protected attribute that is equally predictive.   \n395 As explored in Appendix A, in lending, information about income and debts are likely to be legitimate   \n396 features $x_{l}$ . Credit scores can be a proxy for a person\u2019s financial position, as well as protected   \n397 attributes [18, 60]. However, the complexity of calculating credit scores means it is more valuable for   \n398 inferring income, debt repayments, and history of credit. Credit scores, or related features, would   \n399 have a material impact on the true model for default, and then would be a legitimate feature $x_{l}$ .   \n400 Given that nearly, all features may contain some information on protected attributes, even legitimate   \n401 factors [30], this approach explains the need to assess the strength of this dependence and whether the   \n402 feature contributes significantly to the model\u2019s prediction and can be argued to be part of a true DGP. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "403 2.7.3 Feature construction from $x$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "404 The distinction between $x_{l}$ and $x_{n}$ also gives rise to problems in automatic feature construction, such   \n405 as using deep neural networks. If features are constructed automatically using a combination of $x_{l}$   \n406 and $x_{n}$ , indirect and direct discrimination are risks. As an example, an applicant\u2019s resume contains   \n407 legitimate features $x_{l}$ for recruitment prediction. However, the detailed granularity of many resumes   \n408 also gives rise to the problem of non-legitimate information, such as maternity leave or women-only   \n409 sports or other information that may contain information on other protected attributes. Hence, there   \n410 needs to be an active choice of only including legitimate features $x_{l}$ from available data in the model. ", "page_idx": 8}, {"type": "text", "text": "411 3 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "412 Minimising unlawful discrimination in automated decision-making requires a nuanced and contextual   \n413 approach. While it is beyond our scope to offer specific legal advice, our findings underscore several   \n414 key considerations to identify and mitigate potential discrimination effectively:   \n415 1. Assess data legitimacy. Carefully examine if the data, both the target variable $(y)$ and features $(x)$ ,   \n416 are legitimate for the specific context (Sections 2.6 and 2.7). Legal analysis should inform what is   \n417 legitimate in a specific setting.   \n418 2. Build an accurate model. Strive to approximate the true DGP $p(y|x)$ , using only legitimate   \n419 features $x_{l}$ . Reasonable, necessary, and proportionate steps must be taken to minimise estimation   \n420 error and aim for estimation parity (Section 2.3). This may entail model inference, interrogating   \n421 social biases in the data, and scrutinising the estimated model.   \n422 3. Evaluate statistical disparity. Given the best model ${\\hat{p}}(y|x)$ , assess for conditional statistical parity   \n423 by examining outcomes across groups with protected characteristics (Section 2.4). If a model\u2019s   \n424 performance improves by including protected attributes, consider:   \n425 (a) Identify whether conditional statistical parity is unattainable or undesirable based on true   \n426 group differences. This requires stringent analysis into whether differences stem from prior   \n427 injustice or legitimate variation.   \n428 (b) Incorporate further legitimate features $x_{l}$ that could minimise statistical disparities by \u201cex  \n429 plaining away\u201d the performance gained by the protected attribute with legitimate features.   \n430 (c) Avoid using the model due to unmitigated discrimination risks.   \n431 While these guidelines cannot guarantee lawful automated decisions, they provide meaningful   \n432 recommendations and abstractions to help identify and mitigate unlawful discrimination risks.   \n433 In conclusion, this work bridges a critical gap between the technical aspects of automated decisions   \n434 and the complexities of anti-discrimination law. By translating these nuanced legal concepts into   \n435 decision theory, we underscore the importance of accurately modelling true data-generating processes   \n436 and the innovative concept of estimation parity. This interdisciplinary approach enhances the   \n437 understanding of automated decision-making and sets a foundation for future research that aligns   \n438 technological advancements with legal and ethical standards. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "439 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "440 [1] Jeremias Adams-Prassl, Reuben Binns, and Aislinn Kelly-Lyth. Directly Discriminatory Algorithms. The   \n441 Modern Law Review, 86(1):144\u2013175, 2022.   \n442 [2] Nikita Aggarwal. The Norms of Algorithmic Credit Scoring. Cambridge Law Journal, 80(1):42\u201373,   \n443 2021.   \n444 [3] AHRC. A Quick Guide to Australian Discrimination Laws. Technical report, Australian Human Rights   \n445 Commission, 2014.   \n446 [4] Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle. 1973.   \n447 [5] Larry Alexander and Kevin Cole. Discrimination by Proxy. Constitutional Commentary, 14:453\u2013463,   \n448 1997.   \n449 [6] Jose Manuel Alvarez and Salvatore Ruggieri. Counterfactual Situation Testing: Uncovering Discrimi  \n450 nation under Fairness given the Difference. In Proceedings of the 3rd ACM Conference on Equity and   \n451 Access in Algorithms, Mechanisms, and Optimization, Boston, MA, USA\u201e 2023. ACM.   \n452 [7] Kenneth Arrow. The Theory of Discrimination. In Discrimination in Labor Markets, pages 3\u201333.   \n453 Princeton University Press, 1971.   \n454 [8] Australian Parliament. Sex Discrimination Act 1984.   \n455 [9] Bangladesh Parliament. Anti-Discrimination Bill 2022.   \n456 [10] Solon Barocas and Andrew Selbst. Big Data\u2019s Disparate Impact. California Law Review, 104(3):671\u2013732,   \n457 2016.   \n458 [11] Gary Becker. The Economics of Discrimination. University of Chicago Press, 1957.   \n459 [12] Ruben Becker, Gianlorenzo D\u2019Angelo, and Sajjad Ghobadi. On the cost of demographic parity in   \n460 influence maximization, June 2023.   \n461 [13] James Berger. Statistical Decision Theory and Bayesian Analysis. New York: Springer, 1985.   \n462 [14] Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in Criminal   \n463 Justice Risk Assessments: The State of the Art. Sociological Methods & Research, 50(1):3\u201344, 2018.   \n464 [15] Jos\u00e9 M Bernardo and Adrian FM Smith. Bayesian theory. John Wiley & Sons, 1994.   \n465 [16] J. R. Biden. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial   \n466 Intelligence. The White House, 2023. Executive Order 14110.   \n467 [17] Reuben Binns. Fairness in Machine Learning: Lessons from Political Philosophy. Proceedings of   \n468 Machine Learning Research, 81:149\u2013159, 2018.   \n469 [18] Harold Black, Robert L. Schweitzer, and Lewis Mandell. Discrimination in Mortgage Lending. The   \n470 American Economic Review, 68(2):186\u2013191, 1978.   \n471 [19] Margarita Boyarskaya, Solon Barocas, Hanna Wallach, and Michael Carl Tschantz. What Is a Proxy and   \n472 Why Is It a Problem? Proceedings of the Conference on Fairness, Accountability, and Transparency,   \n473 2022.   \n474 [20] Canadian Parliament. Human Rights Act. R.S.C. (c.H-6), 1985.   \n475 [21] Alycia N. Carey and Xintao Wu. The causal fairness field guide: perspectives from social and formal   \n476 sciences. Frontiers in Big Data, 5:892837, 2022.   \n477 [22] Alycia N. Carey and Xintao Wu. The statistical fairness field guide: perspectives from social and formal   \n478 sciences. AI and Ethics, 3(1):1\u201323, 2023.   \n479 [23] Alessandro Castelnovo, Riccardo Crupi, Greta Greco, Daniele Regoli, Ilaria Giuseppina Penco, and   \n480 Andrea Claudio Cosentini. A clarification of the nuances in the fairness metrics landscape. Scientific   \n481 Reports, 12(1), 2022.   \n482 [24] Simon Caton and Christian Haas. Fairness in Machine Learning: A Survey. ACM Computing Surveys,   \n483 2023. Just Accepted.   \n484 [25] Zhisheng Chen. Ethics and discrimination in artificial intelligence-enabled recruitment practices. Human  \n485 ities and Social Sciences Communications, 10(1), 2023.   \n486 [26] S. Chiappa. Path-specific counterfactual fairness. In Proceedings of the AAAI Conference on Artificial   \n487 Intelligence, pages 7801\u20137808, 2019.   \n488 [27] Alexandra Chouldechova. Fair Prediction with Disparate Impact: A Study of Bias in Recidivism   \n489 Prediction Instruments. Big Data, 5(2):153\u2013163, 2017.   \n490 [28] CIA. Legal System - The World Factbook. Technical report, Central Intelligence Agency.   \n491 [29] Nancy S. Cole. Bias in Selection. Journal of Educational Measurement, 10(4):237\u2013255, 1973.   \n492 [30] Sam Corbett-Davies, Johann D. Gaebler, Hamed Nilforoshan, Ravi Shroff, and Sharad Goel. The Measure   \n493 and Mismeasure of Fairness. Journal of Machine Learning Research, 24(312):1\u2013117, 2023.   \n494 [31] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic Decision   \n495 Making and the Cost of Fairness. In Proceedings of the 23rd ACM SIGKDD International Conference on   \n496 Knowledge Discovery and Data Mining, pages 797\u2013806, Halifax, Canada, 2017.   \n497 [32] M. H. DeGroot. Optimal Statistical Decisions. McGraw-Hill, 1970.   \n498 [33] DFS. Report on Apple Card Investigation. New York State Department of Financial Services, 2021.   \n499 [34] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Rich Zemel. Fairness Through   \n500 Awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages   \n501 214\u2013226, 2012.   \n502 [35] ECJ. C-170/84, Bilka Kaufhaus GmbH $\\nu$ Weber von Hartz. European Court of Justice, 1986. ECR 1607.   \n503 [36] Elizabeth Emens. Intimate Discrimination: The State\u2019s Role in the Accidents of Sex and Love. Harvard   \n504 Law Review, 22(5):1307\u20131402, 2009.   \n505 [37] England and Wales Court of Appeal. Igen ltd $\\nu$ Wong. [2005] EWCA Civ 142.   \n506 [38] England and Wales Court of Appeal. Madarassy v Nomura International plc. [2007] EWCA Civ 33.   \n507 [39] England and Wales Court of Appeal. Secretary of State for Defence v Elias. (2006) IRLR 934.   \n508 [40] Equality Act. 2010 (UK).   \n509 [41] Virginia Eubanks. Automating Inequality. St. Martin\u2019s Press, 2018.   \n510 [42] European Court of Justice. C-236/09 Association belge des Consommateurs Test-Achats ASBL v Conseil   \n511 des ministres. (2011) ECR I-00773.   \n512 [43] European Parliament. Directive 2002/73/EC of the European Parliament and of the Council of 23   \n513 September 2002 amending Council Directive 76/207/EEC on the implementation of the principle of equal   \n514 treatment for men and women as regards access to employment, vocational training and promotion, and   \n515 working conditions. 2002. OJ L 269.   \n516 [44] European Parliament. Amendments adopted by the European Parliament on 14 June 2023 on the proposal   \n517 for a regulation of the European Parliament and of the Council on laying down harmonised rules on   \n518 artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. 2023.   \n519 (COM(2021)0206 \u2013 C9-0146/2021 \u2013 2021/0106(COD).   \n520 [45] European Union. Charter of Fundamental Rights of the European Union. 2009. OJ 2012/C 326/02.   \n521 [46] Talia Gillis. The Input Fallacy. Minnesota Law Review, 106:1175, 2022.   \n522 [47] Government of India. Constitution of India, 1950.   \n523 [48] Przemyslaw A. Grabowicz, Nicholas Perello, and Aarshee Mishra. Marrying Fairness and Explainability   \n524 in Supervised Learning. In Proceedings of the Conference on Fairness, Accountability, and Transparency,   \n525 page 1905\u20131916, Seoul, Republic of Korea, 2022.   \n526 [49] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of Opportunity in Supervised Learning. In   \n527 Proceedings of the 30th Conference on Neural Information Processing Systems, Barcelona, Spain, 2016.   \n528 [50] Hoda Heidari, Michele Loi, Krishna P. Gummadi, and Andreas Krause. A moral framework for under  \n529 standing fair ml through economic models of equality of opportunity. In Proceedings of the Conference   \n530 on Fairness, Accountability, and Transparency, page 181\u2013190, Atlanta, GA, 2019.   \n531 [51] Deborah Hellman. Measuring Algorithmic Fairness. Virginia Law Review, 106(4):811\u2013866, 2020.   \n532 [52] Deborah Hellman. Sex, Causation, and Algorithms: How Equal Protection Prohibits Compounding Prior   \n533 Injustice. Washington University Law Review, 98:481\u2013523, 2020.   \n534 [53] Anne Hellum, Ingunn Ikdahl, Vibeke Strand, and Eva-Maria Svensson. Nordic Equality and Anti  \n535 Discrimination Laws in the Throes of Change: Legal developments in Sweden, Finland, Norway, and   \n536 Iceland. Routledge, 2023.   \n537 [54] Corinna Hertweck, Christoph Heitz, and Michele Loi. On the Moral Justification of Statistical Parity. In   \n538 Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 747\u2013757, 2021.   \n539 [55] Daniel E Ho and Alice Xiang. Affirmative algorithms: The legal grounds for fairness as awareness.   \n540 University of Chicago Law Review Online, pages 134\u2013154, 2020.   \n541 [56] Sally Ho and Garance Burke. An Algorithm that Screens for Child Neglect Raises Concerns. Associated   \n542 Press, 2022.   \n543 [57] Lily Hu and Issa Kohler-Hausmann. What\u2019s sex got to do with machine learning? In Proceedings of the   \n544 Conference on Fairness, Accountability, and Transparency, page 513, Barcelona, Spain, 2020.   \n545 [58] Eyke H\u00fcllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning: an   \n546 introduction to concepts and methods. Machine Learning, 110(3):457\u2013506, March 2021.   \n547 [59] Anna Lena Hunkenschroer and Alexander Kriebitz. Is AI Recruiting (un)ethical? A Human Rights   \n548 Perspective on the Use of AI for Hiring. AI and Ethics, 3(1):199\u2013213, 2022.   \n549 [60] Mikella Hurley and Julius Adebayo. Credit Scoring in the Era of Big Data. Yale Journal of Law and   \n550 Technology, 18(1):148\u2013216, 2017.   \n551 [61] Ben Hutchinson and Margaret Mitchell. 50 Years of Test (Un)fairness: Lessons for Machine Learning. In   \n552 Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 49\u201358, Atlanta, GA,   \n553 2019.   \n554 [62] James E. Johndrow and Kristian Lum. An Algorithm For Removing Sensitive Information: Application   \n555 To Race-independent Recidivism Prediction. The Annals of Applied Statistics, 13(1):pp. 189\u2013220, 2019.   \n556 [63] Maximilian Kasy and Rediet Abebe. Fairness, Equality, and Power in Algorithmic Decision-Making. In   \n557 Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 576\u2013586, 2021.   \n558 [64] Aislinn Kelly-Lyth. Challenging Biased Hiring Algorithms. Oxford Journal of Legal Studies, 41(4):899\u2013   \n559 928, 2021.   \n560 [65] Tarunabh Khaitan. A Theory of Discrimination Law. Oxford University Press, 2015.   \n561 [66] Fereshte Khani and Percy Liang. Removing Spurious Features Can Hurt Accuracy and Affect Groups   \n562 Disproportionately. In Proceedings of the Conference on Fairness, Accountability, and Transparency,   \n563 page 196\u2013205, 2021.   \n564 [67] Elif Kiesow Cortez and Nestor Maslej. Adjudication of Artificial Intelligence and Automated Decision  \n565 Making Cases in Europe and the USA. European Journal of Risk Regulation, 14(3):457\u2013475, 2023.   \n566 [68] Niki Kilbertus, Adria Gascon, Matt Kusner, Michael Veale, Krishna Gummadi, and Adrian Weller. Blind   \n567 Justice: Fairness with Encrypted Sensitive Attributes. In Proceedings of the 35th International Conference   \n568 on Machine Learning, pages 2630\u20132639, Stockholm, Sweden, 2018.   \n569 [69] Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and   \n570 Bernhard Sch\u00f6lkopf. Avoiding discrimination through causal reasoning. In Advances in Neural Informa  \n571 tion Processing Systems, volume 30, page 656\u2013666, 2017.   \n572 [70] Pauline Kim. Auditing Algorithms for Discrimination. University of Pennsylvania Law Review Online,   \n573 166(1), 2017.   \n574 [71] Barbara Kiviat. The Moral Affordances of Construing People as Cases: How Algorithms and the Data   \n575 They Depend on Obscure Narrative and Noncomparative Justice. Sociological Theory, 41(3):175\u2013200,   \n576 2023.   \n577 [72] Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Cass Sunstein. Discrimination in the Age of   \n578 Algorithms. Journal of Legal Analysis, 10:113\u2013174, 2019.   \n579 [73] Issa Kohler-Hausmann and Robin Dembroff. Supreme Confusion About Causality at the Supreme Court.   \n580 City University of New York Law Review, 25(1):57\u201392, 2022.   \n581 [74] Joshua Kroll, Joanna Huey, Solon Barocas, Edward Felten, Joel Reidenberg, David Robinson, and Harlan   \n582 Yu. Accountable Algorithms. University of Pennsylvania Law Review, 165(3):633, 2017.   \n583 [75] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual Fairness. Advances in   \n584 Neural Information Processing Systems, 30:4069\u20134079, 2017.   \n585 [76] Katja Langenbucher. Consumer Credit in The Age of AI \u2013 Beyond Anti-Discrimination Law. Law   \n586 Working Paper No. 663/2022, 2023.   \n587 [77] Finn Lattimore, Simon O\u2019Callaghan, Zoe Paleologos, Alistair Reid, Edward Santow, Holli Sargeant,   \n588 and Andrew Thomsen. Using Artificial Intelligence to Make Decisions: Addressing the Problem of   \n589 Algorithmic Bias. Technical Paper, Australian Human Rights Commission, 2020.   \n590 [78] Tai Le Quy, Arjun Roy, Vasileios Iosifidis, Wenbin Zhang, and Eirini Ntoutsi. A survey on datasets for   \n591 fairness-aware machine learning. WIREs Data Mining and Knowledge Discovery, 12(3):e1452, 2022.   \n592 [79] Zachary Lipton, Julian McAuley, and Alexandra Chouldechova. Does Mitigating ML\u2019s Impact Disparity   \n593 Require Treatment Disparity? Advances in Neural Information Processing Systems, 31, 2018.   \n594 [80] Emmanuel Martinez and Lauren Kirchner. The Secret Bias Hidden in Mortgage-Approval Algorithms.   \n595 The Markup, https://perma.cc/U6W9-MECE, 2021.   \n596 [81] Sandra G. Mayson. Bias In, Bias Out. Yale Law Journal, 128(8):2122\u20132473, 2019.   \n597 [82] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A Survey on   \n598 Bias and Fairness in Machine Learning. ACM Computing Surveys, 54(6), 2021.   \n599 [83] Ministry of Justice, Finland. Government Porposal for the Equality Act and Related Laws HE 19/2014 vp   \n600 (Hallituksen esitys eduskunnalle yhdenvertaisuuslaiksi ja er\u00e4iksi siihen liittyviksi laeiksi).   \n601 [84] Ministry of Justice, Finland. Non-Discrimination Act (Yhdenvertaisuuslaki) (1325/2014).   \n602 [85] Shira Mitchell, Eric Potash, Solon Barocas, Alexander D\u2019Amour, and Kristian Lum. Algorithmic Fairness:   \n603 Choices, Assumptions, and Definitions. Annual Review of Statistics and Its Application, 8(1):141\u2013163,   \n604 2021.   \n605 [86] Sophia Moreau. What Is Discrimination? Philosophy & Public Affairs, 38(2):143\u2013179, 2010.   \n606 [87] Deirdre Mulligan, Joshua Kroll, Nitin Kohli, and Richmond Wong. This Thing Called Fairness: Disci  \n607 plinary Confusion Realizing a Value in Technology. In Proceedings of the ACM on Human-Computer   \n608 Interaction, volume 3, pages 1\u201336, 2019.   \n609 [88] Mpoki Mwakagali. International Human Rights Law and Discrimination Protections. Brill, 2018.   \n610 [89] Jakob M\u00f6kander. Auditing of AI: Legal, Ethical and Technical Approaches. Digital Society, 2(3):49,   \n611 2023.   \n612 [90] Arvind Narayanan. Tutorial: 21 Fairness Definition and their Politics. Proceedings of the Conference on   \n613 Fairness, Accountability, and Transparency, 2018.   \n614 [91] New Zealand Parliament. Human Rights Act, 1993.   \n615 [92] Hamed Nilforoshan, Johann D Gaebler, Ravi Shroff, and Sharad Goel. Causal conceptions of fairness   \n616 and their consequences. In International Conference on Machine Learning, pages 16848\u201316887. PMLR,   \n617 2022.   \n618 [93] Safiya Umoja Noble. Algorithms of Oppression. New York University Press, 2018.   \n619 [94] Finland National Non-Discrimination and Equality Tribunal. Decision 216/2017. 2018.   \n620 [95] Tony O\u2019Hagan. Dicing with the Unknown. Significance, 1(3):132\u2013133, 2004.   \n621 [96] OHCHR. Banning Discrimination on Grounds of Socioeconomic Disadvantage: An Essential Tool in the   \n622 Fight Against Poverty. Thematic Report A/77/157, Special Rapporteur on Extreme Poverty and Human   \n623 Rights, United Nations Office of the High Commissioner for Human Rights, 2022.   \n624 [97] Cathy O\u2019Neil. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens   \n625 Democracy. Penguin Books, 2016.   \n626 [98] Cathy O\u2019Neil, Holli Sargeant, and Jacob Appel. Explainable Fairness in Regulatory Algorithmic Auditing,   \n627 2023.   \n628 [99] Giovanni Parmigiani and Lurdes Inoue. Decision Theory. Wiley, 2010.   \n629 [100] Frank Pasquale. The Black Box Society. Harvard University Press, 2019.   \n630 [101] Judea Pearl. An Introduction to Causal Inference. The International Journal of Biostatistics, 6(2), 2010.   \n631 [102] Edmund Phelps. The Statistical Theory of Racism and Sexism. The American Economic Review,   \n632 62(4):659\u2013661, 1972.   \n633 [103] Inioluwa Deborah Raji, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben   \n634 Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker Barnes. Closing the AI Accountability Gap:   \n635 Defining an End-to-End Framework for Internal Algorithmic Auditing. In Proceedings of the Conference   \n636 on Fairness, Accountability, and Transparency, page 33\u201344, Barcelona, Spain, 2020.   \n637 [104] Lisa Rice and Deidre Swesnik. Discriminatory Effects of Credit Scoring on Communities of Color.   \n638 Suffolk University Law Review, 46(935):935\u2013966, 2013.   \n639 [105] Andrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. The   \n640 Knowledge Engineering Review, 29(5):582\u2013638, 2014.   \n641 [106] Chris Russell, Matt J Kusner, Joshua Loftus, and Ricardo Silva. When Worlds Collide: Integrating   \n642 Different Counterfactual Assumptions in Fairness. In Advances in Neural Information Processing Systems,   \n643 volume 30, page 6417\u20136426, 2017.   \n644 [107] Holli Sargeant. Algorithmic decision-making in financial services: economic and normative outcomes in   \n645 consumer credit. AI and Ethics, 3(4):1295\u20131311, 2023.   \n646 [108] Leonard Savage. The Foundations of Statistics. Operations Research, 4(2):254\u2013258, 1956.   \n647 [109] Patrick Shin. Is there a unitary concept of discrimination? In Deborah Hellman and Sophia Rei  \n648 betanz Moreau, editors, Philosophical foundations of discrimination law, page 172. Oxford University   \n649 Press, 2013.   \n650 [110] South African Parliament. Promotion of Equality and Prevention of Unfair Discrimination Act, 2000.   \n651 [111] Till Speicher, Hoda Heidari, Nina Grgic-Hlaca, Krishna P. Gummadi, Adish Singla, Adrian Weller, and   \n652 Muhammad Bilal Zafar. A unified approach to quantifying algorithmic unfairness: Measuring individual   \n653 & group unfairness via inequality indices. In Proceedings of the 24th International Conference on   \n654 Knowledge Discovery & Data Mining, page 2239\u20132248, London, United Kingdom, 2018.   \n655 [112] Supreme Court of Wisconsin. State v. Loomis. 881 N.W.2d 749, 2016.   \n656 [113] Adrien S\u00e9n\u00e9cat. The use of opaque algorithms facilitates abuses within public services. Le Monde, 2023.   \n657 [114] Anique Tahir, Lu Cheng, and Huan Liu. Fairness through aleatoric uncertainty. In Proceedings of the 32nd   \n658 International Conference on Information and Knowledge Management, page 2372\u20132381, Birmingham,   \n659 United Kingdom, 2023.   \n660 [115] Michael Carl Tschantz. What is proxy discrimination? In Proceedings of the Conference on Fairness,   \n661 Accountability, and Transparency, pages 1993\u20132003, Seoul, Republic of Korea, June 2022.   \n662 [116] UK Parliament. Disability Discrimination Act 1995.   \n663 [117] UK Parliament. Explanatory Memorandum to the Equality Act 2010 (Age Exceptions Order). 2012.   \n664 [118] UK Parliament. Race Relations Act 1965.   \n665 [119] UK Parliament. Sex Discrimination Act 1975.   \n666 [120] United Kingdom Employment Appeals Tribunal. Dziedziak $\\nu$ Future Electronics Ltd. [2012]   \n667 UKEAT/0270/11.   \n668 [121] United Kingdom Employment Appeals Tribunal. O\u2019Neil v Governors of St Thomas More Roman Catholic   \n669 School. (1996) IRLR 372.   \n670 [122] United Kingdom House of Lords. Equal Opportunities Commission, R (on the application of) v Birming  \n671 ham City Council. (1989) 1 AC 1155.   \n672 [123] United Kingdom House of Lords. James v Eastleigh Borough Council. (1990) 2 AC 751.   \n673 [124] United Kingdom House of Lords. Secretary of State For Employment, Ex Parte Seymour Smith and   \n674 Another, R v. (2000) 1 All ER 857.   \n675 [125] United Kingdom House of Lords. Webb v EMO Air Cargo (UK) Ltd (No. 2). (1995) IRLR 645.   \n676 [126] United Kingdom Supreme Court. Akerman-Livingstone v Aster Communities Ltd. (2015) 1 AC 1399.   \n677 [127] United Kingdom Supreme Court. Essop v Home Office (UK Border Agency). (2017) IRLR 558.   \n678 [128] United Kingdom Supreme Court. Homer v Chief Constable of West Yorkshire Police. (2012) IRLR 601.   \n679 [129] United Kingdom Supreme Court. Lee v Ashers. (2018) AC 413.   \n680 [130] United Kingdom Supreme Court. R (Coll) v Secretary of State for Justice. (2017) 1 WLR 2093.   \n681 [131] United Kingdom Supreme Court. R (on the application of E) v JFS Governing Body. (2009) 1 WLR   \n682 2353.   \n683 [132] United Nations. Universal Declaration of Human Rights. 1948.   \n684 [133] Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-one-out   \n685 cross-validation and waic. Statistics and computing, 27:1413\u20131432, 2017.   \n686 [134] Aki Vehtari and Jouko Lampinen. Bayesian model assessment and comparison using cross-validation   \n687 predictive densities. Neural computation, 14(10):2439\u20132468, 2002.   \n688 [135] Aki Vehtari and Janne Ojanen. A survey of Bayesian predictive methods for model assessment, selection   \n689 and comparison. Statistics Surveys, 6:142 \u2013 228, 2012.   \n690 [136] Sahil Verma and Julia Rubin. Fairness Definitions Explained. In Proceedings of the International   \n691 Workshop on Software Fairness, pages 1\u20137, Gothenburg, Sweden, 2018.   \n692 [137] Marc De Vos. The European Court of Justice and the March Towards Substantive Equality in European   \n693 Union Anti-Discrimination Law. International Journal of Discrimination and the Law, 20(1):62\u201387,   \n694 2020.   \n695 [138] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Why Fairness Cannot Be Automated: Bridging the   \n696 Gap Between EU Non-discrimination Law and AI. Computer Law & Security Review, 41:105567, 2021.   \n697 [139] Sandra Wachter, Brent Daniel Mittelstadt, and Chris Russell. Bias Preservation in Machine Learning:   \n698 The Legality of Fairness Metrics Under EU Non-Discrimination Law. West Virginia Law Review,   \n699 123(3):735\u2013790, 2021.   \n700 [140] Hilde Weerts, Rapha\u00eble Xenidis, Fabien Tarissan, Henrik Palmer Olsen, and Mykola Pechenizkiy.   \n701 Algorithmic unfairness through the lens of eu non-discrimination law: Or why the law is not a decision   \n702 tree. In Proceedings of the Conference on Fairness, Accountability, and Transparency, page 805\u2013816,   \n703 Chicago, IL, USA, 2023. ACM.   \n704 [141] Halbert White. Maximum likelihood estimation of misspecified sodels. Econometrica, 50(1):1\u201325, 1982.   \n705 [142] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. Pc-fairness: A unified framework for   \n706 measuring causality-based fairness. In Advances in Neural Information Processing Systems, volume 32,   \n707 page 3404\u20133414, 2019.   \n708 [143] Rapha\u00eble Xenidis. Tuning EU equality law to algorithmic discrimination: Three pathways to resilience.   \n709 Maastricht Journal of European and Comparative Law, 27(6):736\u2013758, 2020.   \n710 [144] Alice Xiang. Reconciling Legal and Technical Approaches to Algorithmic Bias. Tennessee Law Review,   \n711 88(3):649, 2021.   \n712 [145] Renzhe Xu, Peng Cui, Kun Kuang, Bo Li, Linjun Zhou, Zheyan Shen, and Wei Cui. Algorithmic decision   \n713 making with conditional fairness. Proceedings of the 26th International Conference on Knowledge   \n714 Discovery & Data Mining, 2020.   \n715 [146] Crystal Yang and Will Dobbie. Equal Protection Under Algorithms: A New Statistical and Legal   \n716 Framework. Michigan Law Review, 119:291, 2020.   \n717 [147] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna Gummadi. Fairness   \n718 beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In   \n719 Proceedings of the 26th International Conference on World Wide Web, pages 1171\u20131180, Perth, Australia,   \n720 2017.   \n721 [148] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. Gummadi. Fairness   \n722 constraints: A flexible approach for fair classification. Journal of Machine Learning Research, 20(75):1\u2013   \n723 42, 2019.   \n724 [149] Junzhe Zhang and Elias Bareinboim. Fairness in decision-making \u2014 the causal explanation formula.   \n725 Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), April 2018.   \n726 [150] Lu Zhang, Yongkai Wu, and Xintao Wu. A Causal Framework for Discovering and Removing Direct and   \n727 Indirect Discrimination. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial   \n728 Intelligence, pages 3929\u20133935, Melbourne, Australia, 2017.   \n729 [151] Miri Zilka, Holli Sargeant, and Adrian Weller. Transparency, Governance and Regulation of Algorithmic   \n730 Tools Deployed in the Criminal Justice System: A UK Case Study. In Proceedings of the Conference on   \n731 AI, Ethics, and Society, page 880\u2013889, Oxford, United Kingdom, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "733 Overview of Finnish Anti-Discrimination Law ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "734 Finnish anti-discrimination law bears many similarities to UK and EU laws. We briefly set out the   \n735 relevant provisions that show the similarities to the Equality Act set out in Section 1.3.   \n736 Section 8(1) of the Non-Discrimination [84] defines the protected characteristics as:1   \n737 No one may be discriminated against on the basis of age, origin, nationality, language, religion,   \n738 belief, opinion, political activity, trade union activity, family relationships, state of health,   \n739 disability, sexual orientation or other personal characteristics. Discrimination is prohibited,   \n740 regardless of whether it is based on a fact or assumption concerning the person him/herself or   \n741 another.   \n742 Section 3(1) of the Non-Discrimination Act provides that: \u201cProvisions on prohibition of discrimination   \n743 based on gender and the promotion of gender equality are laid down in the Act on Equality between   \n744 Women and Men (609/1986).\u201d The Non-Discrimination Act can be applied in cases of multiple   \n745 discrimination, even if gender is one of the grounds of discrimination [83, 84, s 3(1)].   \n746 It is worth noting that this definition is broader than in the UK Equality Act. Some protected   \n747 characteristics are outlined more explicitly; for example, a person discriminated against on the basis   \n748 of language may be able to bring a claim based on racial discrimination [120]. Unlike many Nordic   \n749 countries, the Equality Act does not explicitly protect political activity, trade union activity, and does   \n750 not include \u201cor other personal characteristics\u201d [53].   \n751 Direct discrimination is defined in Section 10:2   \n752 Discrimination is direct if a person, on the grounds of personal characteristics, is treated less   \n753 favourably than another person was treated, is treated or would be treated in a comparable   \n754 situation.   \n755 Indirect discrimination is defined in Section 13:3   \n756 Discrimination is indirect if an apparently neutral rule, criterion or practice puts a person at a   \n757 disadvantage compared with others as on the grounds of personal characteristics, unless the rule,   \n758 criterion or practice has a legitimate aim and the means for achieving the aim are appropriate   \n759 and necessary.   \n760 Section 11(1) defines justifications for different treatment as:4   \n761 Different treatment does not constitute discrimination if the treatment is based on legislation and   \n762 it otherwise has an acceptable objective and the measures to attain the objective are proportionate. ", "page_idx": 16}, {"type": "text", "text": "763 Overview of Finnish National Non-Discrimination and Equality Tribunal Decision 216/2017 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "764 The first case regarding automated decision-making and discrimination was in Finland. The person,   \n765 referred to as A, was denied credit for online purchases based on a credit rating system employed   \n766 by a bank. Person A reported the case to the Non-Discrimination Ombudsman (Yhdenvertaisuus  \n767 valtuutettu), who brought the case before the National Non-Discrimination and Equality Tribunal   \n768 (Yhdenvertaisuus- ja tasa-arvolautakunta). The Tribunal found that the bank\u2019s statistical scoring   \n769 model resulted in direct discrimination based on multiple protected characteristics and was not   \n770 justified by an acceptable objective achieved by proportionate measures. Consequently, the Tri  \n771 bunal prohibited the bank from continuing this practice and imposed a conditional fine to enforce   \n772 compliance.   \n773 The decision-making system in question is for online store financing, which is a purchase-bound, fast   \n774 and automated credit type very different from regular consumer credit. The credit applied for by the   \n775 consumer in each situation is also always bound to the purchase and its value, which means that it is   \n776 more difficult, or even impossible, to undertake detailed requests for information and background   \n777 checks. The individual investigation of the creditworthiness of customers using personal information   \n778 and documents, such as salary and tax certificates, may not be suitable for this type of credit. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "779 Decision-making Model and Data ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "780 The company made credit decisions based on data from the internal records of the credit company,   \n781 information from the credit file, and the score from the company\u2019s internal scoring system.   \n782 The bank\u2019s scoring system assessed creditworthiness. The scoring system used population statistics   \n783 and personal attributes to calculate the percentage of people in certain groups with bad credit history   \n784 and awarded points proportionate to how common bad credit records were in the group in question.   \n785 The variables used included race, first language, age, and place of residence. The company did not   \n786 require or investigate the applicant\u2019s income or financial situation. ", "page_idx": 17}, {"type": "text", "text": "787 True Data Generating Process and Estimation Error ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "788 The bank\u2019s scoring model was based on statistical correlations calculated population and groups,   \n789 including gender, language, age and place of residence, meaning the model is more or less ${\\hat{p}}(y|x_{p})$ .   \n790 This model cannot be said to have attempted to model the true underlying data-generating process   \n791 and instead relied on data that was available regarding protected attributes. It is reasonable to expect   \n792 that the bank was aware of other legitimate factors that could explain the credit score. Therefore, the   \n793 model introduces epistemic uncertainty stemming from the lack of information that could have been   \n794 used to make better predictions, i.e. reasonable legitimate features $x_{l}$ .   \n795 By solely using the data available, rather than identifying what data would be best to reduce estimation   \n796 error, the modellers built an automated decision-making system that unlawfully discriminated. We   \n797 now evaluate how the Tribunal came to those conclusions about the legitimacy of $y$ and $x$ for such a   \n798 model. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "799 Legitimate $y$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "800 The bank argued that the \u201cdifferent treatment does not constitute discrimination if the treatment   \n801 is based on legislation and has an otherwise acceptable objective and the measures to attain the   \n802 objective are proportionate.\u201d The Tribunal agreed that \u201cthe provision of credit to customers is a   \n803 business, the purpose of which is to gain profit\u201d and that \u201cthe investigation of creditworthiness is as   \n804 such based on law and that it has the acceptable and justified objective as defined in section 11 of the   \n805 Non-Discrimination Act\u201d. Therefore, creditworthiness assessment is a legitimate $y$ .   \n806 However, the Tribunal clarified that \u201cthe individual assessment required by the legislation means   \n807 expressly the assessment of an individual\u2019s credit behaviour, credit history, income level and assets,   \n808 and not the extension of the impact of models formed on the basis of probability assessments created   \n809 with statistical methods using the behaviour and characteristics of others, to the individual applying   \n810 for the credit in the credit decision in such a way that assessment is solely based on such models.\u201d   \n811 Therefore, to be appropriate and necessary to achieve that aim, the model must consider legitimate   \n812 features $x_{l}$ . ", "page_idx": 17}, {"type": "text", "text": "813 Protected, Legitimate, and Non-Legitimate Variables $x$ ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "814 Four protected attributes were used as variables in this model $x_{p}$ : age, language, other personal   \n815 characteristic (place of residence), and gender.   \n816 The Tribunal acknowledged that age may be a legitimate variable if it had been used in the assessment   \n817 of creditworthiness mainly when applied to young persons. However, it was not justified in this   \n818 assessment, given the age of the credit applicant.   \n819 The Tribunal agreed with the position under European law that gender is prohibited from being used   \n820 as an actuarial factor in financial services [42].   \n821 Therefore, these features did not contribute to the accuracy of the model\u2019s prediction in a way   \n822 that could be argued as part of the true DGP. Therefore, in this case, these $x_{p}$ variables are also   \n823 non-legitimate variables $x_{n}$ .   \n824 As explained by the Tribunal, to achieve the legitimate $y$ of undertaking an individual assessment   \n825 of creditworthiness and ability to repay, the model should have considered, for example, income,   \n826 expenditure, debt, assets, security and guarantee liabilities, employment and type of employment   \n827 contract (i.e., permanent or temporary). These features would have been legitimate variables $x_{l}$ by   \n828 improving the predictive performance of the model to achieve more accurate decisions. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "829 Conditional Estimation Parity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "830 Using the legitimate variables identified above, we can now consider conditional estimation parity,   \n831 the difference in estimation error between groups with a protected attribute, given legitimate features.   \nReducing the error in Eq. 4 is expected to diminish the risk of conditional estimation disparity.   \n833 However, assessing conditional estimation parity is complex due to inherent challenges in evaluating   \n834 estimation error.   \nJudges engage this type of reasoning through statistical or theoretical means. In this case, the   \n836 Ombudsman brought evidence of the effects of the protected characteristics $x_{p}$ on the true prediction.   \n837 Person A was negatively affected by his age. He was in the age group of 31-40 years old, but if he had   \n838 been at least 51 years old, he would have received a higher score sufficient for the credit application.   \n839 If person A spoke Swedish as his first language, he would have received a sufficient score for granting   \n840 the loan. Finnish-speaking residents received a lower score compared to Swedish-speaking residents.   \n841 Further, ethnic minorities with an official first language other than Finnish or Swedish were put in an   \n842 unfavourable position.   \n843 A would have earned more points based on his residential area if he had lived in a population   \n844 centre. The bank\u2019s statistical method, which is based on a grid of residential areas, gave A the lowest   \n845 score because he lives in a sparsely populated area that has not yielded any statistically significant   \n846 information.   \n847 Gender impacted the model, where women received a higher score than men. The Tribunal agreed   \n848 that if the person A had been a woman, he would have been granted the credit. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "849 Conclusions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "850 This case study demonstrates the intersection between judicial reasoning and our formalisation. To   \n851 avoid liability for unlawful multiple direct discrimination in this algorithmic decision-making process,   \n852 the company should have:   \n853 1. Assessed data legitimacy. While the Tribunal agreed with the target variable $(y)$ as a legitimate   \n854 aim, they did not believe the features $(x)$ were legitimate for the specific context (Section 2.7).   \n855 2. Built an accurate model. The bank did not strive to approximate the true DGP $p(y|x)$ , and did not   \n856 use legitimate features $x_{l}$ . Reasonable, necessary, and proportionate steps should have been taken   \n857 to minimise estimation error and aim for estimation parity (Section 2.3).   \n858 3. Evaluate differences. The bank should have considered whether there were true and legitimate   \n859 differences based on protected characteristics and whether they could have been \u201cexplained away\u201d   \n860 by legitimate features $\\left(\\boldsymbol{x}_{l}\\right)$ to minimise statistical disparities.   \n861 These recommendations should be used to help identify and mitigate unlawful discrimination within   \n862 the specific context of each jurisdiction. ", "page_idx": 18}, {"type": "text", "text": "863 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "866 paper\u2019s contributions and scope?   \n867 Answer: [Yes]   \n868 Justification: The abstract and introduction clearly state the claims, contributions, assump  \n869 tions and limitations of the paper.   \n870 Guidelines:   \n871 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n872 made in the paper.   \n873 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n874 contributions made in the paper and important assumptions and limitations. A No or   \n875 NA answer to this question will not be perceived well by the reviewers.   \n876 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n877 much the results can be expected to generalize to other settings.   \n878 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n879 are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Jademes   \n5 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n6 the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n8 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n9 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n0 model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n3 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often   \n5 depend on implicit assumptions, which should be articulated.   \n6 \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution   \n8 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n9 used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n2 and how they scale with dataset size.   \n3 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n4 address problems of privacy and fairness.   \n5 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n6 reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n8 judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "911 3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "912 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n913 a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper presents formalisations, which all include relevant assumptions and formatting, but no theoretical results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "28 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not include experiments. Guidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "970 Answer: [NA]   \n971 Justification: The paper does not include experiments.   \n972 Guidelines:   \n973 \u2022 The answer NA means that paper does not include experiments requiring code.   \n974 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n975 public/guides/CodeSubmissionPolicy) for more details.   \n976 \u2022 While we encourage the release of code and data, we understand that this might not be   \n977 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n978 including code, unless this is central to the contribution (e.g., for a new open-source   \n979 benchmark).   \n980 \u2022 The instructions should contain the exact command and environment needed to run to   \n981 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n982 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n983 \u2022 The authors should provide instructions on data access and preparation, including how   \n984 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n985 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n986 proposed method and baselines. If only a subset of experiments are reproducible, they   \n987 should state which ones are omitted from the script and why.   \n988 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n989 versions (if applicable).   \n990 \u2022 Providing as much information as possible in supplemental material (appended to the   \n991 paper) is recommended, but including URLs to data and code is permitted.   \n992 6. Experimental Setting/Details   \n993 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n994 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n995 results?   \n996 Answer: [NA]   \n997 Justification: The paper does not include experiments.   \n998 Guidelines:   \n999 \u2022 The answer NA means that the paper does not include experiments.   \n1000 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n1001 that is necessary to appreciate the results and make sense of them.   \n1002 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n1003 material.   \n1004 7. Experiment Statistical Significance   \n1005 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n1006 information about the statistical significance of the experiments?   \n1007 Answer: [NA] .   \n1008 Justification: The paper does not include experiments.   \n1009 Guidelines:   \n1010 \u2022 The answer NA means that the paper does not include experiments.   \n1011 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n1012 dence intervals, or statistical significance tests, at least for the experiments that support   \n1013 the main claims of the paper.   \n1014 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n1015 example, train/test split, initialization, random drawing of some parameter, or overall   \n1016 run with given experimental conditions).   \n1017 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n1018 call to a library function, bootstrap, etc.)   \n1019 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n1020 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1021 of the mean.   \n1022 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1023 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1024 of Normality of errors is not verified.   \n1025 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1026 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1027 error rates).   \n1028 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1029 they were calculated and reference the corresponding figures or tables in the text.   \n1030 8. Experiments Compute Resources   \n1031 Question: For each experiment, does the paper provide sufficient information on the com  \n1032 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1033 the experiments?   \n1034 Answer: [NA] .   \n1035 Justification: The paper does not include experiments.   \n1036 Guidelines:   \n1037 \u2022 The answer NA means that the paper does not include experiments.   \n1038 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1039 or cloud provider, including relevant memory and storage.   \n1040 \u2022 The paper should provide the amount of compute required for each of the individual   \n1041 experimental runs as well as estimate the total compute.   \n1042 \u2022 The paper should disclose whether the full research project required more compute   \n1043 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1044 didn\u2019t make it into the paper).   \n1045 9. Code Of Ethics   \n1046 Question: Does the research conducted in the paper conform, in every respect, with the   \n1047 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1048 Answer: [Yes]   \n1049 Justification: Our research rigorously addresses the ethical code outlined by the conference,   \n1050 particularly focusing on issues related to safety, security, discrimination, and fairness. We   \n1051 have proactively identified and discussed potential harmful outcomes, particularly those   \n1052 involving discrimination and misuse in the contexts of legal and ethical standards. Further  \n1053 more, we provide recommendations to mitigate these risks, underscoring our commitment   \n1054 to the responsible development and application of technology that respects human rights   \n1055 and societal values. The paper does not contain research involving human subjects or   \n1056 participants, it does not conduct experiments or have data-related concerns.   \n1057 Guidelines:   \n1058 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1059 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1060 deviation from the Code of Ethics.   \n1061 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1062 eration due to laws or regulations in their jurisdiction).   \n1063 10. Broader Impacts   \n1064 Question: Does the paper discuss both potential positive societal impacts and negative   \n1065 societal impacts of the work performed?   \n1066 Answer: [Yes]   \n1067 Justification: Our research contributes to bridging the gap between legal standards and   \n1068 algorithmic fairness, aiming to enhance the integrity and fairness of automated decision  \n1069 making systems. This has significant implications for improving equity in critical areas   \n1070 where algorithmic decisions are increasingly prevalent. We also the risks of unfair treatment   \n1071 based on model or data biases or misinterpretation of the legal doctrines we study. We   \n1072 explore the potential for unintended consequences even when the technology functions   \n1073 as intended, such as the reinforcement of existing societal biases under the guise of legal   \n1074 compliance. To mitigate these risks, we propose specific safeguards to prevent unlawful   \n1075 discrimination in systems.   \n1076 Guidelines:   \n1077 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1078 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1079 impact or why the paper does not address societal impact.   \n1080 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1081 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1082 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1083 groups), privacy considerations, and security considerations.   \n1084 \u2022 The conference expects that many papers will be foundational research and not tied   \n1085 to particular applications, let alone deployments. However, if there is a direct path to   \n1086 any negative applications, the authors should point it out. For example, it is legitimate   \n1087 to point out that an improvement in the quality of generative models could be used to   \n1088 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1089 that a generic algorithm for optimizing neural networks could enable people to train   \n1090 models that generate Deepfakes faster.   \n1091 \u2022 The authors should consider possible harms that could arise when the technology is   \n1092 being used as intended and functioning correctly, harms that could arise when the   \n1093 technology is being used as intended but gives incorrect results, and harms following   \n1094 from (intentional or unintentional) misuse of the technology.   \n1095 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1096 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1097 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1098 feedback over time, improving the efficiency and accessibility of ML).   \n1099 11. Safeguards   \n1100 Question: Does the paper describe safeguards that have been put in place for responsible   \n1101 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1102 image generators, or scraped datasets)?   \n1103 Answer: [NA] .   \n1104 Justification: The paper does not release data or models.   \n1105 Guidelines:   \n1106 \u2022 The answer NA means that the paper poses no such risks.   \n1107 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1108 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1109 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1110 safety filters.   \n1111 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1112 should describe how they avoided releasing unsafe images.   \n1113 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1114 not require this, but we encourage authors to take this into account and make a best   \n1115 faith effort.   \n1116 12. Licenses for existing assets   \n1117 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1118 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1119 properly respected?   \n1120 Answer: [NA] .   \n1121 Justification: The paper does not use existing assets.   \n1122 Guidelines:   \n1123 \u2022 The answer NA means that the paper does not use existing assets.   \n1124 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1125 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1126 URL.   \n1127 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n128 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n129 service of that source should be provided.   \n1130 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1131 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1132 has curated licenses for some datasets. Their licensing guide can help determine the   \n1133 license of a dataset.   \n1134 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1135 the derived asset (if it has changed) should be provided.   \n1136 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1137 the asset\u2019s creators.   \n138 13. New Assets   \n139 Question: Are new assets introduced in the paper well documented and is the documentation   \n140 provided alongside the assets?   \n141 Answer: [NA] .   \n142 Justification: The paper does not release new assets.   \n143 Guidelines:   \n144 \u2022 The answer NA means that the paper does not release new assets.   \n1145 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n146 submissions via structured templates. This includes details about training, license,   \n147 limitations, etc.   \n148 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n149 asset is used.   \n150 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1151 create an anonymized URL or include an anonymized zip file.   \n1152 14. Crowdsourcing and Research with Human Subjects   \n153 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n154 include the full text of instructions given to participants and screenshots, if applicable, as   \n1155 well as details about compensation (if any)?   \n156 Answer: [NA] .   \n157 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n158 Guidelines:   \n159 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n160 human subjects.   \n161 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n162 tion of the paper involves human subjects, then as much detail as possible should be   \n163 included in the main paper.   \n164 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n165 or other labor should be paid at least the minimum wage in the country of the data   \n166 collector.   \n167 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n168 Subjects   \n169 Question: Does the paper describe potential risks incurred by study participants, whether   \n1170 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1171 approvals (or an equivalent approval/review based on the requirements of your country or   \n172 institution) were obtained?   \n173 Answer: [NA] .   \n1174 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n1175 Guidelines:   \n1176 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n177 human subjects.   \n1178   \n1179   \n1180   \n1181   \n1182   \n1183   \n1184   \n1185 ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]