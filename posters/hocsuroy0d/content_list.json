[{"type": "text", "text": "Attack-Aware Noise Calibration for Differential Privacy ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bogdan Kulynych\u2217 Juan Felipe Gomez\u2217 Lausanne University Hospital (CHUV) Harvard University Georgios Kaissis Flavio du Pin Calmon Carmela Troncoso Technical University Munich Harvard University EPFL ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Differential privacy (DP) is a widely used approach for mitigating privacy risks when training machine learning models on sensitive data. DP mechanisms add noise during training to limit the risk of information leakage. The scale of the added noise is critical, as it determines the trade-off between privacy and utility. The standard practice is to select the noise scale to satisfy a given privacy budget $\\varepsilon$ . This privacy budget is in turn interpreted in terms of operational attack risks, such as accuracy, sensitivity, and specificity of inference attacks aimed to recover information about the training data records. We show that first calibrating the noise scale to a privacy budget $\\varepsilon$ , and then translating $\\varepsilon$ to attack risk leads to overly conservative risk assessments and unnecessarily low utility. Instead, we propose methods to directly calibrate the noise scale to a desired attack risk level, bypassing the step of choosing $\\varepsilon$ . For a given notion of attack risk, our approach significantly decreases noise scale, leading to increased utility at the same level of privacy. We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than $\\varepsilon$ , when training privacy-preserving ML models substantially improves model accuracy for the same risk level. Our work provides a principled and practical way to improve the utility of privacy-preserving ML without compromising on privacy. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning and statistical models can leak information about individuals in their training data, which can be recovered by membership inference, attribute inference, and reconstruction attacks (Fredrikson et al., 2015; Shokri et al., 2017; Yeom et al., 2018; Balle et al., 2022). The most common defenses against these attacks are based on differential privacy (DP) (Dwork et al., 2014). Differential privacy introduces noise to either the data, the training algorithm, or the model parameters (Chaudhuri et al., 2011). This noise provably limits the adversary\u2019s ability to run successful attacks at the cost of reducing the utility of the model. ", "page_idx": 0}, {"type": "text", "text": "In DP, the parameters $\\varepsilon$ and $\\delta$ control the privacy-utility trade-off. These parameters determine the scale (e.g., variance) of the noise added during training: Smaller values of these parameters correspond to larger noise. Larger noise provides stronger privacy guarantees but reduces the utility of the trained model. Typically, $\\delta$ is set to a small fixed value (usually between $10^{-8}$ and $10^{-5}$ ), leaving $\\varepsilon$ as the primary tunable parameter. Without additional analyses, the values of parameters $(\\varepsilon,\\delta)$ alone do not provide a tangible and intuitive operational notion of privacy risk (Nanayakkara et al., 2023). This begs the question: how should practitioners, regulators, and data subjects decide on acceptable values of $\\varepsilon$ and $\\delta$ and calibrate the noise scale to achieve a desired level of protection? ", "page_idx": 0}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/0481694c167145f14c89924f2c62395655b5f177c8676db1802d41b80291e7e8.jpg", "img_caption": ["Figure 1: Test accuracy ( $\\mathbf{\\dot{X}}$ -axis) of a privately finetuned GPT-2 on SST-2 text sentiment classification dataset (top) and a convolutional neural network on CIFAR-10 image classification dataset (bottom). The DP noise is calibrated to guarantee at most a certain level of privacy attack sensitivity (y-axis) at three possible attack false-positive rates $\\alpha\\in\\{0.01,0.05,0.1\\}$ . See Section 4 for details. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "A standard way of assigning operational meaning to DP parameters is mapping them to attack risks. One common approach is computing attacker\u2019s posterior belief (or equivalently, accuracy or advantage) of membership inference attacks, that concrete values of $(\\varepsilon,\\delta)$ allow (Wood et al., 2018). An alternative is to compute the trade-off between sensitivity and specificity of feasible membership inference attacks (Wasserman and Zhou, 2010; Kairouz et al., 2015; Dong et al., 2022), which was recently shown to also be directly related to success of record reconstruction attacks (Hayes et al., 2024; Kaissis et al., 2023a). Such approaches map $(\\varepsilon,\\delta)$ to a quantifiable level of risk for individuals whose data is present in the dataset. Studies have shown that such risk-based measures are the most useful way to interpret the guarantees afforded by DP for practitioners and data subjects (Cummings et al., 2021; Franzen et al., 2022; Nanayakkara et al., 2023). ", "page_idx": 1}, {"type": "text", "text": "In this work, we show that directly calibrating the level of noise to satisfy a given level of attack risk, as opposed to satisfying a certain $\\varepsilon$ , enables a significant increase in utility (see Figure 1). We enable this direct calibration to attack risk by working under $f$ -DP (Dong et al., 2022), a hypothesis testing interpretation of DP. In particular, we extend the tight privacy analysis method by Doroshenko et al. (2022) to directly estimate operational privacy risk notions in $f$ -DP. Then, we use our extended algorithm to directly calibrate the level of noise to satisfy a given level of attack risk. Concretely, our contributions are: ", "page_idx": 1}, {"type": "text", "text": "1. We provide efficient methods for calibrating noise to (a) maximum accuracy (equivalently, advantage), (b) sensitivity and specificity of membership inference attacks, in any DP mechanism, including DP-SGD (Abadi et al., 2016) with arbitrarily many steps.   \n2. We empirically show that our calibration methods reduce the required noise scale for a given level of privacy risk, up to $2\\times$ as compared to standard methods for choosing DP parameters. In a private language modeling task with GPT-2 (Radford et al., 2019), we demonstrate that the decrease in noise can translate to a 18 p.p. gain in classification accuracy.   \n3. We demonstrate that relying on membership inference accuracy as an interpretation of privacy risk, as is common practice, can increase attack power in privacy-critical regimes, and that calibration for sensitivity and specificity does not suffer from this drawback. ", "page_idx": 1}, {"type": "text", "text": "4. We provide a Python package which implements our algorithms for analyzing DP mechanisms in terms of the interpretable $f$ -DP guarantees, and calibrating to operational risks: ", "page_idx": 2}, {"type": "text", "text": "github.com/Felipe-Gomez/riskcal ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Ultimately, we advocate for practitioners to calibrate the noise level in privacy-preserving machine learning algorithms to a sensitivity and specificity constraint under $f$ -DP as outlined in Section 3.2. ", "page_idx": 2}, {"type": "text", "text": "Related Work. Prior work has studied methods for communicating the privacy guarantees afforded by differential privacy (Nanayakkara et al., 2023, 2022; Franzen et al., 2022; Mehner et al., 2021; Wood et al., 2018), and introduced various principled methods for choosing the privacy parameters (Abowd and Schmutte, 2015; Nissim et al., 2014; Hsu et al., 2014). Unlike our approach, these works assume that the mechanisms are calibrated to a given $\\varepsilon$ privacy budget parameter, and do not aim to directly set the privacy guarantees in terms of operational notions of privacy risk. ", "page_idx": 2}, {"type": "text", "text": "Cherubin et al. (2024); Ghazi and Issa (2023); Izzo et al. (2024); Mahloujifar et al. (2022) use variants of DP that directly limit the advantage of membership inference attacks. We show that calibrating noise to a given level of advantage can increase privacy risk in security-critical regimes and provide methods that mitigate this issue. Leemann et al. (2024) provide methods for evaluating the success of membership inference attacks under a weaker threat model than in DP. Unlike their work, we preserve the standard strong threat model in differential privacy but set and report the privacy guarantees in terms of an operational notion of risk under $f$ -DP as opposed to the $\\varepsilon$ parameter. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Setup and notation. Let $\\mathbb{D}^{n}$ denote the set of all datasets of size $n$ over a space $\\mathbb{D}$ , and let $S\\simeq S^{\\prime}$ denote a neighboring relation, e.g. $S,S^{\\prime}$ that differ by one datapoint. We study randomized algorithms (mechanisms) $M(S)$ that take as input a dataset $S\\in2^{\\mathbb{D}}$ , and output the result of a computation, e.g., statistical queries or an ML model. We denote the output domain of the mechanism by $\\Theta$ . For ease of presentation, we mainly consider randomized mechanisms that are parameterized by a single noise parameter $\\omega\\in{\\Omega}$ , but our results extend to mechanisms with multiple parameters. For example, in the Gaussian mechanism (Dwork et al., 2014), $M(S)=q(S)+Z$ , where $Z\\sim\\mathcal{N}(0,\\sigma^{2})$ and $q(S)$ is a non-private statistical algorithm, the parameter is $\\omega=\\sigma$ with $\\Omega=\\mathbb{R}^{\\geq0}$ . We denote a parameterized mechanism by $M_{\\omega}(S)$ . We summarize the notation in Table 1 in the Appendix. ", "page_idx": 2}, {"type": "text", "text": "Differential Privacy. For any $\\gamma\\geq0$ , we define the hockey-stick divergence from distribution $P$ to $Q$ over a domain $\\scriptscriptstyle\\mathcal{O}$ by ", "page_idx": 2}, {"type": "equation", "text": "$$\nD_{\\gamma}(P\\parallel Q)\\triangleq\\operatorname*{sup}_{E}Q(E)-\\gamma P(E)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the supremum is taken over all measurable sets $E\\subseteq{\\mathcal{O}}$ . We define differential privacy (DP) (Dwork et al., 2006) as follows: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1. A mechanism $M(\\cdot)$ satisfies $(\\varepsilon,\\delta)$ -DP iff $\\mathrm{sup}_{S\\simeq S^{\\prime}}\\,D_{e^{\\varepsilon}}(M(S)\\parallel M(S^{\\prime}))\\leq\\delta.$ ", "page_idx": 2}, {"type": "text", "text": "Lower values of $\\varepsilon$ and $\\delta$ mean more privacy which in turn requires more noise, and vice versa. In the rest of the paper we assume that a larger value of the parameter $\\omega\\in\\Omega$ for $\\Omega\\subseteq\\mathbb{R}$ , e.g., standard deviation of Gaussian noise $\\omega=\\sigma$ in the Gaussian mechanism, means that the mechanism $M_{\\omega}(\\cdot)$ is more noisy, which translates into a higher level of privacy (smaller $\\varepsilon,\\delta)$ , but lower utility. ", "page_idx": 2}, {"type": "text", "text": "Most DP algorithms satisfy a collection of $(\\varepsilon,\\delta)$ -DP guarantees. We define the privacy proflie (Balle and Wang, 2018), or privacy curve (Gopi et al., 2021; Alghamdi et al., 2023) of a mechanism as: ", "page_idx": 2}, {"type": "text", "text": "Definition 2.2. A parameterized mechanism $M_{\\omega}(\\cdot)$ has a privacy proflie $\\varepsilon_{\\omega}:[0,1]\\to\\mathbb{R}$ if for every $\\delta\\in[0,1]$ , $M_{\\omega}(\\cdot)$ is $(\\varepsilon(\\delta),\\delta)$ -DP. ", "page_idx": 2}, {"type": "text", "text": "We refer to the function $\\delta_{\\omega}(\\varepsilon)$ , defined analogously, also as the privacy profile. ", "page_idx": 2}, {"type": "text", "text": "DP-SGD. A common algorithm for training neural networks with DP guarantees is DP-SGD (Abadi et al., 2016). The basic building block of DP-SGD is the subsampled Gaussian mechanism, defined as $M(S)=q({\\mathsf{P o i s s o n S a m p l e}}_{p}\\circ S)+Z$ , where $Z\\sim\\mathcal{N}(0,\\Delta_{2}^{2}\\:\\bar{\\cdot}\\:\\sigma^{2}\\cdot I_{d})$ , and PoissonSamplep is a procedure which subsamples a dataset $S$ such that every record has the same probability $p\\in(0,1)$ to be in the subsample. DP-SGD, parameterized by $p,\\sigma$ , and $T\\geq1$ , is a repeated application of the subsampled Gaussian mechanism: $M^{(1)}\\circ M^{(2)}\\circ\\cdot\\cdot\\cdot\\circ M^{(T)}(S)$ , where $q^{(i)}(\\cdot)$ is a single step of gradient descent with per-record gradient clipping to $\\Delta_{2}$ Euclidean norm. In line with a standard practice (Ponomareva et al., 2023), we regard all parameters but $\\sigma$ as fixed, thus $\\omega=\\sigma$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Privacy profiles for mechanisms such as DP-SGD are computed via numerical algorithms called accountants (see, e.g., Abadi et al., 2016; Gopi et al., 2021; Doroshenko et al., 2022; Alghamdi et al., 2023). These algorithms compute the achievable privacy profile to accuracy nearly matching the lower bound of a privacy audit where the adversary is free to choose the entire (pathological or realistic) training dataset (Nasr et al., 2021, 2023). Given these results, we regard the analyses of these accountants as tight, and use them for calibration to a particular $(\\varepsilon,\\delta)$ -DP constraint. ", "page_idx": 3}, {"type": "text", "text": "Standard Calibration. The procedure of choosing the parameter $\\omega\\in\\Omega$ to satisfy a given level of privacy is called calibration. In standard calibration, one chooses $\\omega$ given a target DP guarantee $\\varepsilon^{\\star}$ and an accountant that supplies a privacy proflie $\\varepsilon_{\\omega}(\\delta)$ for any noise parameter $\\omega\\in\\Omega$ , to ensure that $M_{\\omega}(S)$ satisfies $(\\varepsilon^{\\star},\\delta^{\\star})$ -DP: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\;\\omega\\;\\;\\;\\;\\mathrm{s.t.}\\;\\varepsilon_{\\omega}(\\delta^{\\star})\\geq\\varepsilon^{\\star},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\delta^{\\star}$ set by convention to $\\delta^{\\star}={^1\\!/\\!c\\!\\cdot\\!n}$ , where $n$ is the dataset size, and $c>1$ (see, e.g., Ponomareva et al., 2023; Near et al., 2023). The parameter $\\varepsilon^{\\star}$ is also commonly chosen by convention between 2 and 10 for privacy-persevering ML algorithms with practical utility (Ponomareva et al., 2023). In Eq. (2) and the rest the paper we denote by \u22c6the target value of privacy risk. ", "page_idx": 3}, {"type": "text", "text": "After calibration, the $(\\varepsilon,\\delta)$ parameters are often mapped to some operational notation of privacy attack risk for interpretability. In the next section, we introduce the hypothesis testing framework of DP, $f$ -DP, and the notions of risk that $(\\varepsilon,\\delta)$ parameters are often mapped to. In contrast to standard calibration, in Section 2.3, we calibrate $\\omega$ to directly minimize these privacy risks. ", "page_idx": 3}, {"type": "text", "text": "2.2 Operational Privacy Risks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We can interpret differential privacy through the lens of membership inference attacks (MIAs) in the so-called strong-adversary model (see, e.g., Nasr et al., 2021). In this framework, the adversary aims to determine whether a given output $\\theta\\in\\Theta$ came from $M(S)$ or $M(S^{\\prime})$ , where $S^{\\prime}=S\\cup\\dot{\\{z\\}}$ for some target example $z\\in\\mathbb{D}$ .\u2020 The adversary has access to the mechanism $M(\\cdot)$ , the dataset $S$ , and the target example $z\\in\\mathbb{D}$ . Such an attack is equivalent to a binary hypothesis test (Wasserman and Zhou, 2010; Kairouz et al., 2015; Dong et al., 2022): ", "page_idx": 3}, {"type": "equation", "text": "$$\nH_{0}:\\theta\\sim M(S),\\quad H_{1}:\\theta\\sim M(S^{\\prime}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the MIA is modelled as a test $\\phi:\\Theta\\to[0,1]$ that maps a given mechanism output $\\theta$ to the probability of the null hypothesis $H_{0}$ being rejected. We can analyze this hypothesis test through the trade-off between the achievable false positive rate (FPR) $\\alpha_{\\phi}\\overset{\\Delta}{=}\\mathbb{E}_{M(S)}[\\dot{\\phi}]$ and false negative rate (FNR) $\\beta_{\\phi}\\triangleq1-\\mathbb{E}_{M(S^{\\prime})}[\\phi]$ , where the expectations are taken over the coin filps in the mechanism.\u2021 Dong et al. (2022) formalize the trade-off function and define $f{\\mathrm{-}}D P$ as follows: ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3. A trade-off function $T(M(S),M(S^{\\prime})):[0,1]\\rightarrow[0,1]$ outputs the FNR of the most powerful attack at any given level $\\alpha\\in[0,1]$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nT(M(S),M(S^{\\prime}))(\\alpha)=\\operatorname*{inf}_{\\phi:\\;\\Theta\\to[0,1]}\\{\\beta_{\\phi}\\mid\\alpha_{\\phi}\\leq\\alpha\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "See Figure 5 in the Appendix for an illustration. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4. A mechanism $M(\\cdot)$ satisfies $f$ -DP, where $f$ is the trade-off curve for some other mechanism, if for all $\\alpha\\in[0,1]$ , we have $\\operatorname*{inf}_{S\\subseteq S^{\\prime}}T(M(S),\\dot{M}(S^{\\prime}))(\\alpha)\\geq f(\\alpha)$ . ", "page_idx": 3}, {"type": "text", "text": "Next, we state the equivalence between $(\\varepsilon,\\delta)$ -DP guarantees and $f$ -DP guarantees. ", "page_idx": 3}, {"type": "text", "text": "Proposition 2.1 (Dong et al. (2022)). If a mechanism $M(\\cdot)$ is $(\\varepsilon,\\delta)$ -DP, then it is $f$ -DP with ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\alpha)=\\operatorname*{max}\\{0,\\;1-\\delta-e^{\\varepsilon}\\alpha,\\;e^{-\\varepsilon}\\cdot(1-\\delta-\\alpha)\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, a mechanism $M(\\cdot)$ satisfies $(\\varepsilon(\\delta),\\delta)$ -DP for all $\\delta\\in[0,1]$ iff it is $f$ -DP with ", "page_idx": 3}, {"type": "equation", "text": "$$\nf(\\alpha)=\\operatorname*{sup}_{\\delta\\in[0,1]}\\operatorname*{max}\\{0,\\,1-\\delta-e^{\\varepsilon(\\delta)}\\alpha,\\,e^{-\\varepsilon(\\delta)}\\cdot(1-\\delta-\\alpha)\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We overview three particular notions of attack risk: advantage/accuracy of MIAs, FPR/FNR of MIAs, and reconstruction robustness. These risks can be thought of as summary statistics of the $f$ curve. ", "page_idx": 4}, {"type": "text", "text": "Advantage/Accuracy. Wood et al. (2018) proposed\u00a7 to measure the attack risk as the maximum achievable attack accuracy. To avoid confusion with task accuracy, we use advantage over random guessing, which is the difference between the attack TPR $1-\\beta_{\\phi}$ and FNR $\\alpha_{\\phi}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta\\triangleq\\operatorname*{sup}_{S\\simeq S^{\\prime}\\,\\phi:\\,\\Theta\\rightarrow[0,1]}1-\\beta_{\\phi}-\\alpha_{\\phi}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The advantage $\\eta$ is a linear transformation of the maximum attack accuracy s $\\mathrm{up\\,^{1}/2\\cdot(1-\\beta_{\\phi})+1/2}$ \u00b7 $(1-\\alpha_{\\phi})$ , where supremum is over $S\\simeq S^{\\prime}$ and $\\phi:\\Theta\\to[0,1]$ . Moreover, $\\eta$ can be obtained from a fixed point $\\alpha^{*}=f(\\alpha^{*})$ of the $f$ curve as $1-2\\alpha^{*}$ , and it is bounded given an $(\\varepsilon,\\delta)$ -DP guarantee: ", "page_idx": 4}, {"type": "text", "text": "Proposition 2.2 (Kairouz et al. (2015)). If a mechanism $M(\\cdot)\\,i s\\,(\\varepsilon,\\delta)\\!-\\!I$ DP, then we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\eta\\leq\\frac{e^{\\varepsilon}-1+2\\delta}{e^{\\varepsilon}+1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "FPR/FNR Risk. Recent work (Carlini et al., 2022; Rezaei and Liu, 2021) has argued that MIAs are a relevant threat only when the attack true positive rate $1-\\beta_{\\phi}$ is high at low enough $\\alpha_{\\phi}$ . As a concrete notion of risk, we thus consider minimum level of attack FNR $\\beta^{\\star}$ within an FPR region $\\alpha\\in[0,\\alpha^{\\star}]$ , where $\\alpha^{\\star}$ is a low value. This approach is similar to the statistically significant p-values often used in the sciences. Following the scientific standards and Carlini et al. (2022), we consider $\\alpha^{\\star}\\in\\{0.01,0.05,0.1\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Reconstruction Robustness. Another privacy threat is the reconstruction of training data records (see, e.g., Balle et al., 2022). Denoting by $R(\\theta;z)$ an attack that aims to reconstruct $z$ , its success probability can be formalized as $\\rho\\triangleq\\operatorname*{Pr}[\\ell(\\dot{z},R(\\theta;\\bar{z}))\\leq\\gamma]$ over $\\theta\\sim M(S\\cup\\{z\\}),z\\sim\\pi$ for some loss function $\\ell:\\mathbb{D}^{2}\\rightarrow\\mathbb{R}$ and prior $\\pi$ . Kaissis et al. (2023a) showed that MIA error rates bound reconstruction success as $\\rho\\le1-f(\\kappa_{\\gamma})$ for an appropriate choice of $\\kappa_{\\gamma}$ . Therefore, the FPR/FNR trade-off curve can also be thought as a notion of robustness to reconstruction attacks. ", "page_idx": 4}, {"type": "text", "text": "2.3 Our Objective: Attack-Aware Noise Calibration ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The standard practice in DP is to calibrate the noise scale $\\omega$ of a mechanism $M_{\\omega}(\\cdot)$ to some target $(\\varepsilon^{\\star},\\delta^{\\star})$ -DP guarantee, with $\\varepsilon^{\\star}$ from a recommended range, e.g., $\\varepsilon^{\\star}\\,\\in\\,[2,10]$ , and $\\delta^{\\star}$ fixed to $\\delta^{\\star}<{}^{1}\\!/n$ , as in Eq. (2). Then, the privacy guarantees provided by the chosen $(\\varepsilon^{\\dot{\\star}},\\delta^{\\star})$ are obtained by mapping these values to bounds on sensitivity and specificity (by Proposition 2.1) or advantage (by Proposition 2.2) of membership inference attacks. In this work, we show that if the goal is to provide an operational and interpretable guarantee such as attack advantage or FPR/FNR, this approach leads to unnecessarily pessimistic noise requirements and a deterioration in utility due to the intermediate step of setting $(\\varepsilon^{\\star},\\delta^{\\star})$ . We show it is possible to skip this intermediate step by using the hypothesis-testing interpretation of DP to directly calibrate noise to operational notions of privacy risk. In practice, this means replacing the constraint in Eq. (2) with an operational notion of risk: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\;\\omega\\;\\;\\;\\;\\mathrm{s.t.}\\;\\tau\\mathrm{i}\\,\\mathbf{s}\\mathbf{k}_{\\omega}\\leq\\mathrm{threshol}\\mathbf{d}^{\\star}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Solving this optimization problem requires two components. First, a way to optimize $\\omega$ given a method to compute $\\mathtt{r i s k}_{\\omega}$ . As we assume that risk is monotonic in $\\omega$ , Eq. (9) can be solved via binary search (see, e.g., Paszke et al., 2019) using calls to the $\\tt r i s k_{\\omega}$ function to an arbitrary precision. Second, we need a way to compute $\\mathtt{r i s k}_{\\omega}$ for any value $\\omega$ . In the next section, we provide efficient methods for doing so for general DP mechanisms, including composed mechanisms such as DP-SGD, by extending the tight privacy analysis from Doroshenko et al. (2022) to computing $f$ -DP. Having these methods, we instantiate Eq. (9) for the notions of risks introduced in Section 2.2. ", "page_idx": 4}, {"type": "text", "text": "3 Numeric Calibration to Attack Risks ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide methods for calibrating DP mechanisms to the notions of privacy risk in Section 2.2. As a first step, we introduce the core technical building blocks of our calibration method: methods for evaluating advantage $\\eta_{\\omega}$ and the trade-off curve $f_{\\omega}(\\alpha)$ for a given value of $\\omega$ . ", "page_idx": 4}, {"type": "text", "text": "Dominating Pairs and PLRVs. We make use of two concepts, originally developed in the context of computing tight privacy profiles under composition: dominating pairs (Zhu et al., 2022a) and privacy loss random variables (PLRV) (Dwork and Rothblum, 2016). ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1. We say that a pair of distributions $(P,Q)$ is a dominating pair for a mechanism $M(\\cdot)$ if for every $\\varepsilon\\in\\mathbb R$ , we have $\\begin{array}{r}{\\operatorname*{sup}_{S\\simeq S^{\\prime}}D_{e^{\\varepsilon}}(M(S)\\parallel M(S^{\\prime}))\\le D_{e^{\\varepsilon}}(P\\parallel Q)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Importantly, a dominating pair also provides a lower bound on the trade-off curve of a mechanism: Proposition 3.1. If $(P,Q)$ is a dominating pair for a mechanism $M$ , then for $\\alpha\\in[0,1]$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{S\\geq S^{\\prime}}T(M(S),M(S^{\\prime}))(\\alpha)\\geq T(P,Q)(\\alpha).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proofs of this and all the following statements are in Appendix E. Proposition 3.1 implies that a mechanism $M(\\cdot)$ is $f$ -DP with $f=\\bar{T}(P,Q)$ . Next, we introduce privacy loss random variables, which provide a natural parameterization of the curve $T(P,Q)$ . ", "page_idx": 5}, {"type": "text", "text": "Definition 3.2. Suppose that a mechanism $M(\\cdot)$ has a discrete-valued dominating pair $(P,Q)$ . Then, we define the privacy loss random variables (PLRVs) $(X,Y)$ as $Y\\triangleq\\,\\log Q(o){\\big/}P(o)$ , with $o\\sim Q$ , and $X\\triangleq\\,\\log Q(o^{\\prime}){\\big/}P(o^{\\prime})$ with $o^{\\prime}\\sim P$ . ", "page_idx": 5}, {"type": "text", "text": "We can now state the result which serves as a main building block for our calibration algorithms, and forms the main theoretical contribution of our work. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3 (Accounting for advantage and $f$ -DP with PLRVs). Suppose that a mechanism $M(\\cdot)$ has a discrete-valued dominating pair $(P,Q)$ with associated PLRVs $(X,Y)$ . The attack advantage $\\eta$ for this mechanism is bounded: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\eta\\leq\\operatorname*{Pr}[Y>0]-\\operatorname*{Pr}[X>0].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, for any $\\tau\\in\\mathbb{R}\\cup\\{\\infty,-\\infty\\}$ and $\\gamma\\in[0,1].$ , define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta^{*}(\\tau,\\gamma)=\\mathrm{Pr}[Y\\leq\\tau]-\\gamma\\,\\mathrm{Pr}[Y=\\tau].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For any level $\\alpha\\in[0,1]$ , choosing $\\tau=(1-\\alpha)$ -quantile of $X$ and $\\begin{array}{r}{\\gamma=\\frac{\\alpha-\\operatorname*{Pr}[X>\\tau]}{\\operatorname*{Pr}[X=\\tau]}}\\end{array}$ guarantees that $T(P,Q)(\\alpha)=\\beta^{*}(\\tau,\\gamma)$ . ", "page_idx": 5}, {"type": "text", "text": "To show this, we use the Neyman-Pearson lemma to explicitly parameterize the most powerful attack at level $\\alpha$ in terms the threshold $\\tau$ on the Neyman-Pearson test statistic and the probability $\\gamma$ of guessing when the test statistic exactly equals the threshold. See Appendix E.2 for the detailed proof. ", "page_idx": 5}, {"type": "text", "text": "We remark that similar results for the trade-off curve appear in (Zhu et al., 2022a) without the $\\gamma$ terms, as Zhu et al. assume continuous PLRVs $(X,Y)$ . In our work, we rely on the technique due to Doroshenko et al. (2022), summarized in Appendix D, which discretizes continuous mechanisms such as the subsampled Gaussian in DP-SGD, and provides a dominating pair that is discrete and finitely supported over an evenly spaced grid. As the dominating pairs are discrete, the $\\gamma$ terms are non-zero, thus are necessary to fully reconstruct the trade-off curve. ", "page_idx": 5}, {"type": "text", "text": "3.1 Calibration to Advantage ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "First, we show how to instantiate Eq. (9) to calibrate noise to a target advantage $\\eta^{\\star}\\in[0,1]$ . Let $\\eta_{\\omega}$ denote the advantage of the mechanism $M_{\\omega}(\\cdot)$ as defined in Eq. (7): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\omega\\quad\\mathrm{~s.t.~}\\quad\\eta_{\\omega}\\leq\\eta^{\\star}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given the PLRVs $(X_{\\omega},Y_{\\omega})$ , we can obtain a substantially tighter bound than converting $(\\varepsilon,\\delta)$ guarantees using Proposition 2.2 under standard calibration. Specifically, Theorem 3.3 provides the following way to solve the problem: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\omega\\quad\\mathrm{~s.t.~}\\quad\\operatorname*{Pr}[Y_{\\omega}>0]-\\operatorname*{Pr}[X_{\\omega}>0]\\leq\\eta^{\\star}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We call this approach advantage calibration, and show how to practically implement it in Algorithms 3 and 4 in the Appendix. Given a method for obtaining valid PLRVs $X_{\\omega},Y_{\\omega}$ for any $\\omega$ , such as the one by Doroshenko et al. (2022), advantage calibration is guaranteed to ensure bounded advantage, which follows by combining Proposition 3.1 and Theorem 3.3: ", "page_idx": 5}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/d705aa0fb856f3af3219d535c72ae4ecce4c3c4d2825068eec4fed65883e53d6.jpg", "img_caption": ["(a) Calibrating noise to attack advantage significantly reduces the required noise scale compared to the standard approach. y axis is logarithmic. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/cd20ba3e38853dac2fa428a21aa9616cf135ed53f20365714f5249c04b870e27.jpg", "img_caption": ["Figure 2: Benefits and pitfalls of advantage calibration. ", "(b) Optimal calibration for advantage comes with a pitfall: it allows for $\\Delta\\beta$ higher attack power in the low FPR regime compared to standard calibration. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Proposition 3.2. Given $P L R V s$ $(X_{\\omega},Y_{\\omega})$ of a discrete-valued dominating pair of a mechanism $M_{\\omega}(\\cdot)$ , choosing $\\omega^{*}$ using Eq. (14) ensures $\\eta_{\\omega^{*}}\\leq\\eta^{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "Utility Benefits. We demonstrate how calibration for a given level of attack advantage can increase utility. As a mechanism to calibrate, we consider DP-SGD with $p=0.001$ subsampling rate, $T=10{,}000$ iterations, and assume that $\\delta^{\\star}=10^{-5}$ . Our goal is to compare the noise scale $\\sigma$ obtained via advantage calibration to the standard approach. ", "page_idx": 6}, {"type": "text", "text": "As a baseline, we choose $\\sigma$ using standard calibration in Eq. (2), and convert the resulting $(\\varepsilon,\\delta)$ guarantees to advantage using Proposition 2.2. We detail this procedure in Algorithm 2 in the Appendix. We consider target values of advantage $\\eta^{\\star}\\in[0.01,0.25]$ . As we show in Figure 2a, our direct calibration procedure enables to reduce the noise scale by up to $3.5\\times$ . ", "page_idx": 6}, {"type": "text", "text": "Pitfalls of Calibrating for Advantage. Calibration to a given level of membership advantage is a compelling idea due to the decrease in noise required to achieve better utility at the same level of risk as with the standard approach. Despite this increase in utility, we caution that this approach comes with a deterioration of privacy guarantees other than maximum advantage compared to standard calibration. Concretely, it allows for increased attack $T P R$ in the privacy-critical regime of low attack FPR (see Section 2.2). The next result quantifies this pitfall: ", "page_idx": 6}, {"type": "text", "text": "Proposition 3.3 (Cost of advantage calibration). Fix a dataset size $n>1$ , and a target level of attack advantage $\\eta^{\\star}\\in(\\delta^{\\star},1)$ , where $\\delta^{\\star}={^1\\!/\\!c\\!\\cdot\\!n}$ for some $c>1$ . For any $\\begin{array}{r}{0\\,<\\,\\alpha\\,<\\,\\frac{1-\\overset{\\smile}{\\eta}^{\\star}}{2}}\\end{array}$ , there exists $a$ $D P$ mechanism for which the gap in FNR $f_{\\mathsf{s t a n d a r d}}(\\alpha)$ obtained with standard calibration for $\\varepsilon^{\\star}$ that ensures $\\eta\\leq\\eta^{\\star}$ , and FNR $f_{\\mathsf{a d v}}(\\alpha)$ obtained with advantage calibration is lower bounded: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta\\beta(\\alpha)\\triangleq f_{\\mathsf{s t a n d a r d}}(\\alpha)-f_{\\mathsf{a d v}}(\\alpha)\\geq\\eta^{\\star}-\\delta^{\\star}+2\\alpha\\frac{\\eta^{\\star}}{\\eta^{\\star}-1}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For example, if we aim to calibrate a mechanism to at most $\\eta^{\\star}=0.5$ (or, $75\\%$ attack accuracy), we could potentially increase attack sensitivity by $\\Delta\\beta(\\alpha)\\approx30$ p.p. at FPR $\\alpha=0.1$ compared to standard calibration with $\\delta^{\\star}=10^{-5}$ (see the illustration in Figure 2b). Note that the difference $\\Delta\\beta$ in Proposition 3.3 is an overestimate in practice: the increase in attack sensitivity can be significantly lower for mechanisms such as the Gaussian mechanism (see Figure 6 in the Appendix). ", "page_idx": 6}, {"type": "text", "text": "3.2 Safer Choice: Calibration to FNR within a Given FPR Region ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we show how to calibrate the noise in any practical DP mechanism to a given minimum level of attack FNR $\\beta^{\\star}$ within an FPR region $\\alpha\\in[0,\\alpha^{\\star}]$ , which enables to avoid the pitfalls of advantage calibration. We base this notion of risk off the previous work (Carlini et al., 2022; Rezaei and Liu, 2021) which argued that MIAs are a relevant threat only when the achievable TPR $1-\\beta$ is high at low FPR $\\alpha$ . We instantiate the calibration problem in Eq. (9) as follows, assuming $M_{\\omega}(\\cdot)$ satisfies $f_{\\omega}(\\alpha)$ -DP: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\;\\omega\\mathrm{~s.t.~}\\operatorname*{inf}_{0\\leq\\alpha\\leq\\alpha^{\\star}}f_{\\omega}(\\alpha)\\geq\\beta^{\\star}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "To solve Eq. (16), we begin by showing that such calibration is in fact equivalent to requiring a given level of attack FNR $\\beta^{\\star}$ and FPR $\\alpha^{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "Require: PMF $\\operatorname*{Pr}[X_{\\omega}=x_{i}]$ over grid $\\{x_{1},x_{2},\\ldots,x_{k}\\}$ with $x_{1}<x_{2}<...<x_{k}$   \nRequire: PMF $\\mathrm{Pr}[Y_{\\omega}=y_{j}]$ over grid $\\{y_{1},y_{2},\\dots,y_{l}\\}$ with $y_{1}<y_{2}<...<y_{l}$   \n1: procedure COMPUTEBETA $.(\\omega;\\alpha^{\\star};X_{\\omega},Y_{\\omega})$   \n2: $t\\gets\\operatorname*{min}\\{i\\in\\{0,1,\\dots,k\\}\\ |\\ \\mathrm{Pr}[X_{\\omega}>x_{i}]\\leq\\alpha^{\\star}\\}$ , where $x_{0}\\triangleq-\\infty$   \n3: $\\begin{array}{r}{\\gamma\\leftarrow\\frac{\\alpha^{\\star}-\\operatorname*{ipr}[X_{\\omega}>x_{t}]}{\\operatorname*{Pr}[X_{\\omega}=x_{t}]}}\\end{array}$   \n4: return $f_{\\omega}(\\alpha^{\\star})\\stackrel{}{=}\\operatorname*{Pr}[Y_{\\omega}\\leq x_{t}]-\\gamma\\operatorname*{Pr}[Y_{\\omega}=x_{t}]$ ", "page_idx": 7}, {"type": "text", "text": "Proposition 3.4. For any $\\alpha^{\\star}\\geq0,\\beta^{\\star}\\geq0$ such that $\\alpha^{\\star}+\\beta^{\\star}\\leq1$ , and any $f$ -DP mechanism $M(\\cdot)$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{0\\leq\\alpha\\leq\\alpha^{\\star}}f(\\alpha)\\geq\\beta^{\\star}\\,i\\!f\\!\\!f\\,f(\\alpha^{\\star})\\geq\\beta^{\\star}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This follows directly by monotonicity of the trade-off function $f$ (Dong et al., 2022). The optimization problem becomes: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\,\\omega\\,\\operatorname{s.t.}\\,f_{\\omega}(\\alpha^{\\star})\\geq\\beta^{\\star}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Unlike advantage calibration to $\\eta^{\\star}$ , the approach in Eq. (18) limits the adversary\u2019s capabilities without increasing the risk in the privacy-critical low-FPR regime, as we can explicitly control the acceptable attack sensitivity for a given low FPR. ", "page_idx": 7}, {"type": "text", "text": "To obtain $f_{\\omega}(\\alpha)$ , we use the PLRVs $X_{\\omega},Y_{\\omega}$ along with Theorem 3.3 to compute $f=T(P,Q)^{\\mathnormal{\\mathnormal{\\mathnormal{\\mathnormal{\\mathnormal{\\mathnormal{\\mathnormal{\\mathnormal{\\mathnormal{\\Lambda}}}}}}}}}}$ (see Algorithm 1), and solve Eq. (18) using binary search over $\\omega\\in{\\Omega}$ . We provide the precise procedure in Algorithm 6 in the Appendix. This approach guarantees the desired level of risk: ", "page_idx": 7}, {"type": "text", "text": "Proposition 3.5. Given $P L R V s$ $(X_{\\omega},Y_{\\omega})$ of a discrete-valued dominating pair of a mechanism $M_{\\omega}\\bar{(}\\cdot)$ , choosing $\\omega^{*}$ using Eq. (18) and Algorithm $^{\\,l}$ to compute $f_{\\omega}(\\alpha)$ ensures $f_{\\omega^{*}}(\\alpha^{\\star})\\geq\\beta^{\\star}$ . ", "page_idx": 7}, {"type": "text", "text": "3.3 Other Approaches to Trade-Off Curve Accounting ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we first contextualize the proposed method within existing work. Then, we discuss settings in which alternatives to PLRV-based procedures could be more suitable. ", "page_idx": 7}, {"type": "text", "text": "Benefits of PLRV-based Trade-Off Curve Accounting. Computational efficiency is important when estimating $f_{\\omega}(\\alpha)$ , as the calibration problem requires evaluating this function multiple times for different values of $\\omega$ as part of binary search. Algorithm 1 computes $f_{\\omega}(\\alpha)$ for a single $\\omega$ in $\\approx500\\mathrm{ms}$ , enabling fast calibration, e.g., in $\\approx1$ minute for DP-SGD with $T=10{,}000$ steps on commodity hardware (see Appendix H). Existing methods for estimating $f_{\\omega}(\\alpha)$ , on the contrary, either provide weaker guarantees than Proposition 3.5 or are substantially less efficient. In particular, Dong et al. (2022) introduced $\\mu$ -GDP, an asymptotic expression for $f_{\\omega}(\\alpha)$ as $T\\,\\rightarrow\\,\\infty$ , that overestimates privacy (Gopi et al., 2021), and thus leads to mechanisms that do not satisfy the desired level of attack resilience when calibrating to it. Nasr et al. (2023); Zheng et al. (2020) introduced a discretizationbased approach to approximate $f_{\\omega}(\\alpha)$ (discussed next) that can be orders of magnitude less efficient than the direct estimation in Algorithm 1, e.g., 1\u20136 minutes $(\\approx100\u2013700\\times$ slower) for a single evaluation of $f_{\\omega}(\\alpha)$ in the same setting as before, depending on the coarseness of discretization. ", "page_idx": 7}, {"type": "text", "text": "Calibration using Black-Box Accountants. Most DP mechanisms are accompanied by $(\\varepsilon,\\delta)$ -DP accountants, i.e., methods to compute their privacy proflie $\\varepsilon_{\\omega}(\\delta)$ or $\\delta_{\\omega}(\\varepsilon)$ . Black-box access to these accountants enables to estimate $\\eta_{\\omega}$ and $f_{\\omega}(\\alpha)$ . In particular, Proposition 2.2 tells us that $(0,\\delta)$ -DP mechanisms bound advantage as $\\eta\\leq\\delta$ . Thus, advantage calibration can also be performed with any $\\varepsilon_{\\omega}(\\delta)$ accountant by calibrating noise to ensure $\\varepsilon_{\\omega}(\\bar{\\eta}^{\\star})=0$ . Estimating $f_{\\omega}(\\bar{\\alpha)}$ , as mentioned previously, is less straightforward. Existing numeric approaches (Nasr et al., 2023; Zheng et al., 2020) are equivalent to approximating Eq. (6) on a discrete grid over $\\delta\\in\\{\\delta_{1},\\ldots,\\delta_{u}\\}$ . This requires $u$ calls to the accountant $\\varepsilon_{\\omega}(\\delta)$ , thus quickly becomes inefficient for estimating $f_{\\omega}(\\alpha)$ to high precision. We provide a detailed discussion of such black-box approaches in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "Calibration of Mechanisms with Known Trade-Off Curves. An important feature of our calibration methods is that they enable calibration of mechanisms whose privacy proflie is unknown in the exact form, e.g., DP-SGD for $T>1$ . Simpler mechanisms, such as the Gaussian mechanism, which are used for simpler statistical analyses, e.g., private mean estimation, admit exact analytical solutions to the calibration problems in Eqs. (13) and (18). In Appendix G, we provide such solutions for the standard Gaussian mechanism, which enable efficient calibration without needing Algorithm 1. ", "page_idx": 7}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/666bfb120e885e8d6dca280c763b0efcc67c2f227c4d71d9bfe47e2f1177c662.jpg", "img_caption": ["Figure 3: Calibration to attack TPR (i.e., 1\u2212FNR) significantly reduces the noise scale in low FPR regimes. Unlike calibration for attack advantage, this approach does not come with a deterioration of privacy for low FPR, as it directly targets this regime. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/13b69460e13f464d8571cdce338019c54150f84b5fef915b5948bfdf607092cd.jpg", "img_caption": ["Figure 4: Trade-off curves obtained via our method in Algorithm 1 provide a significantly tighter analysis of the attack risks, compared to the standard method of interpreting the privacy risk for a given $(\\varepsilon,\\delta)$ with fixed $\\delta<{^1}/n$ via Eq. (5). The trade-off curves are shown for three runs of DP-SGD with different noise multipliers in the language modeling experiment with GPT-2. The dotted line - - shows the trade-off curve which corresponds to perfect privacy. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we empirically evaluate the utility improvement of our calibration method over traditional approaches. We do so in simulations as well as in realistic applications of DP-SGD. In Appendix H, we also evaluate the utility gain when performing simpler statistical analyses. ", "page_idx": 8}, {"type": "text", "text": "Simulations. First, we demonstrate the noise reduction when calibrating the DP-SGD algorithm for given error rates using the setup in Section 3.1. We fix three low FPR values: $\\alpha^{\\star}\\in\\{0.01,0.05,0.1\\}$ , and vary maximum attack sensitivity $1-\\beta^{\\star}$ from 0.1 to 0.5 in each FPR regime. We show the results in Figure 3. We observe a significant decrease in the noise scale for all values. Although the decrease is smaller than with calibration for advantage (see Figure 2a), calibrating directly for risk in the low FPR regime avoids the pitfall of advantage calibration: inadvertently increasing risk in this regime. ", "page_idx": 8}, {"type": "text", "text": "Language Modeling and Image Classification. We showed that FPR/FNR calibration enables to significantly reduce the noise scale. Next, we study how much of this reduction in noise translates into actual utility improvement in downstream applications. We evaluate our method for calibrating noise in private deep learning on two tasks: text sentiment classification using the SST-2 dataset (Socher et al., 2013), and image classification using the CIFAR-10 dataset (Krizhevsky et al., 2009). ", "page_idx": 8}, {"type": "text", "text": "For sentiment classification, we fine-tune GPT-2 (small) (Radford et al., 2019) using a DP version of LoRA (Yu et al., 2021). For image classification, we follow the approach of Tramer and Boneh (2021) of training a convolutional neural network on top of ScatterNet features (Oyallon and Mallat, 2015) with DP-SGD (Abadi et al., 2016). See additional details in Appendix H. For each setting, by varying the noise scale, we obtain several models at different levels of privacy. For each of the models we compute the guarantees in terms of TPR $1-\\beta$ at three fixed levels of FPR $\\alpha^{\\star}\\in\\{0.01,0.05,0.1\\}$ that would be obtained under standard calibration, and using our Algorithm 1. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Figure 1 shows that FPR/FNR calibration significantly increases task accuracy (a notion of utility; not to confuse with attack accuracy, a notion of privacy risk) at the same level of $1-\\beta$ for all values of $\\alpha^{\\star}$ . For instance, for GPT-2, we see the accuracy increase of $18.3\\,\\mathrm{p.p}$ . at the same level of privacy risk (top leftmost plot). To illustrate the reasons behind such a large difference between the methods, in Figure 4, we show the trade-off curves obtained with our Algorithm 1, and with the standard method of deriving the FPR/FNR curve from a single $(\\varepsilon,\\delta)$ pair for a fixed $\\delta<{^1}/n$ via Eq. (5). We can see that the latter approach drastically overestimates the attack risks, which translates to significantly higher noise and lower task accuracy when calibrating with standard calibration. ", "page_idx": 9}, {"type": "text", "text": "5 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we proposed novel methods for calibrating noise in differentially private learning targeting a given level of operational privacy risk: advantage and FPR/FNR of membership inference attacks. We introduced an accounting algorithm which directly and tightly estimates privacy guarantees in terms of $f$ -DP, which characterizes these operational risks. Using simulations and end-to-end experiments on common use cases, we showed that our attack-aware noise calibration significantly decreases the required level of noise compared to the standard approach at the same level of operational risk. In the case of calibration for advantage, we also showed that the noise decrease could be harmful as it could allow for increased attack success in the low FPR regime compared to the standard approach, whereas calibration for a given level of FPR/FNR mitigates this issue. Next, we discuss limitations and possible directions for future work. ", "page_idx": 9}, {"type": "text", "text": "Choice of Target FPR/FNR. We leave open the question on how to choose the target FPR $\\alpha^{\\star}$ and FNR $\\beta^{\\star}$ , e.g., whether standard significance levels in sciences such as $\\alpha^{\\star}=0.05$ are compatible with data protection regulation and norms. Further work is needed to develop concrete guidance on the choice of target FPR and FNR informed by legal and practical constraints. ", "page_idx": 9}, {"type": "text", "text": "Catastrophic Failures. It is possible to construct pathological DP mechanisms which admit catastrophic failures (see, e.g., Ponomareva et al., 2023), i.e., mechanisms which allow non-trivial attack TPR at FPR $\\alpha=0$ so that their trade-off curve is such that $T(M(S),M(S^{\\prime}))(0)<1$ for some $S\\simeq S^{\\prime}$ . A classical example in the context of private data release is a mechanism that releases a data record in the clear with probability $\\delta>0$ , in which case we have $T(M(S),M(S^{\\prime}))(0)=1-\\delta$ See the proof of Proposition 3.3 in Appendix E for a concrete construction. In the case that such a pathological mechanism is used in practice, one should use standard calibration to $(\\varepsilon,\\delta)$ with $\\delta\\ll{^1}/n$ to directly limit the chance of catastrophic failures. Fortunately, practical mechanisms such as DP-SGD do not admit catastrophic failures, as they ensure $T(M(S),\\dot{M}(S^{\\prime}))(0)=1.$ . ", "page_idx": 9}, {"type": "text", "text": "Tight Bounds for Privacy Auditing. Multiple prior works on auditing the privacy properties of ML algorithms (Nasr et al., 2021; Liu et al., 2021; Jayaraman and Evans, 2019; Erlingsson et al., 2019) used conversions between $(\\varepsilon,\\delta)$ and operational risks like in Proposition 2.1, which we have shown to significantly overestimate the actual risks. Beyond calibrating noise, our methods provide bounds on attack success rates for audits in a more precise and computationally efficient way than a recent similar approach by Nasr et al. (2023). ", "page_idx": 9}, {"type": "text", "text": "Accounting in the Relaxed Threat Models. Although we have focused on DP, our methods apply to any notion of privacy that is also formalized as a hypothesis test. In particular, our method can be used as is to compute privacy guarantees of DP-SGD in a relaxed threat model (RTM) proposed by Kaissis et al. (2023b). Previously, there was no efficient method for accounting in the RTM. ", "page_idx": 9}, {"type": "text", "text": "Applications Beyond Privacy. Our method can be applied to ensure provable generalization guarantees in deep learning. Indeed, prior work has shown that advantage $\\eta$ bounds generalization gaps of ML models (Kulynych et al., 2022a,b). Thus, even though advantage calibration can exacerbate certain risks, it can be a useful tool for ensuring a desired level of generalization in models that usually do not come with non-vacuous generalization guarantees, e.g., deep neural networks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Priyanka Nanayakkara for the helpful suggestions. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, 2016.   \nJohn M Abowd and Ian M Schmutte. Revisiting the economics of privacy: Population statistics and confidentiality protection as public goods. Economics, (1/20), 2015.   \nWael Alghamdi, Juan Felipe Gomez, Shahab Asoodeh, Flavio Calmon, Oliver Kosut, and Lalitha Sankar. The saddle-point method in differential privacy. In International Conference on Machine Learning, pages 508\u2013528. PMLR, 2023.   \nBorja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. In International Conference on Machine Learning. PMLR, 2018.   \nBorja Balle, Giovanni Cherubin, and Jamie Hayes. Reconstructing training data with informed adversaries. In 2022 IEEE Symposium on Security and Privacy (SP). IEEE, 2022.   \nBarry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.   \nNicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP), 2022.   \nKamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 2011.   \nGiovanni Cherubin, Boris Ko\u00a8pf, Andrew Paverd, Shruti Tople, Lukas Wutschitz, and Santiago Zanella B\u00b4eguelin. Closed-form bounds for DP-SGD against record-level inference. In 33rd USENIX Security Symposium (USENIX Security 2024), 2024.   \nRachel Cummings, Gabriel Kaptchuk, and Elissa M Redmiles. \u201cI need a better description\u201d\u2019: An investigation into user expectations for differential privacy. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, 2021.   \nJinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. Journal of the Royal Statistical Society Series B: Statistical Methodology, 2022.   \nVadym Doroshenko, Badih Ghazi, Pritish Kamath, Ravi Kumar, and Pasin Manurangsi. Connect the dots: Tighter discrete approximations of privacy loss distributions. Proceedings on Privacy Enhancing Technologies, 2022.   \nCynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint arXiv:1603.01887, 2016.   \nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proceedings of the Theory of Cryptography Conference, 2006.   \nCynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 2014.   \nU\u00b4lfar Erlingsson, Ilya Mironov, Ananth Raghunathan, and Shuang Song. That which we call private. arXiv preprint arXiv:1908.03566, 2019.   \nDaniel Franzen, Saskia Nun\u02dcez von Voigt, Peter So\u00a8rries, Florian Tschorsch, and Claudia Mu\u00a8ller-Birn. Am i private and if so, how many? communicating privacy guarantees of differential privacy with risk communication formats. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, 2022.   \nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the ACM SIGSAC conference on computer and communications security, 2015.   \nMarco Gaboardi, Michael Hay, and Salil Vadhan. A programming framework for OpenDP. Manuscript, May, 2020.   \nElena Ghazi and Ibrahim Issa. Total variation with differential privacy: Tighter composition and asymptotic bounds. In IEEE International Symposium on Information Theory (ISIT), 2023.   \nSivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. Advances in Neural Information Processing Systems (NeurIPS), 2021.   \nCharles R. Harris, K. Jarrod Millman, St\u00b4efan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00b4andez del R\u00b4\u0131o, Mark Wiebe, Pearu Peterson, Pierre G\u00b4erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 2020.   \nJamie Hayes, Borja Balle, and Saeed Mahloujifar. Bounding training data reconstruction in DP-SGD. Advances in Neural Information Processing Systems, 2024.   \nJustin Hsu, Marco Gaboardi, Andreas Haeberlen, Sanjeev Khanna, Arjun Narayan, Benjamin C Pierce, and Aaron Roth. Differential privacy: An economic method for choosing epsilon. In 2014 IEEE 27th Computer Security Foundations Symposium, 2014.   \nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.   \nZachary Izzo, Jinsung Yoon, Sercan O Arik, and James Zou. Provable membership inference privacy. Transactions on Machine Learning Research, 2024.   \nBargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice. In 28th USENIX Security Symposium (USENIX Security 19), 2019.   \nBargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, and David Evans. Revisiting membership inference under realistic assumptions. Proceedings on Privacy Enhancing Technologies, 2021.   \nRicheng Jin, Zhonggen Su, Caijun Zhong, Zhaoyang Zhang, Tony Quek, and Huaiyu Dai. Breaking the communication-privacy-accuracy tradeoff with f-differential privacy. Advances in Neural Information Processing Systems (NeurIPS), 2023.   \nPeter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In International Conference on Machine Learning. PMLR, 2015.   \nGeorgios Kaissis, Jamie Hayes, Alexander Ziller, and Daniel Rueckert. Bounding data reconstruction attacks with the hypothesis testing interpretation of differential privacy. arXiv preprint arXiv:2307.03928, 2023a.   \nGeorgios Kaissis, Alexander Ziller, Stefan Kolek, Anneliese Riess, and Daniel Rueckert. Optimal privacy guarantees for a relaxed threat model: Addressing sub-optimal adversaries in differentially private machine learning. Advances in Neural Information Processing Systems (NeurIPS), 2023b.   \nThomas Kluyver, Benjamin Ragan-Kelley, Fernando P\u00b4erez, Brian Granger, Matthias Bussonnier, Jonathan Frederic, Kyle Kelley, Jessica Hamrick, Jason Grout, Sylvain Corlay, Paul Ivanov, Dami\u00b4an Avila, Safia Abdalla, Carol Willing, and Jupyter development team. Jupyter notebooks - a publishing format for reproducible computational workflows. In Positioning and Power in Academic Publishing: Players, Agents and Agendas. IOS Press, 2016.   \nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, 2009.   \nBogdan Kulynych, Mohammad Yaghini, Giovanni Cherubin, Michael Veale, and Carmela Troncoso. Disparate vulnerability to membership inference attacks. Proceedings on Privacy Enhancing Technologies, 2022a.   \nBogdan Kulynych, Yao-Yuan Yang, Yaodong Yu, Jaroslaw Blasiok, and Preetum Nakkiran. What you see is what you get: Principled deep learning via distributional generalization. Advances in Neural Information Processing Systems (NeurIPS), 2022b.   \nTobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Gaussian membership inference privacy. Advances in Neural Information Processing Systems, 36, 2024.   \nErich L Lehmann and Joseph P Romano. Testing statistical hypotheses. Springer Science & Business Media, 2006.   \nJiaxiang Liu, Simon Oya, and Florian Kerschbaum. Generalization techniques empirically outperform differential privacy against membership inference. arXiv preprint arXiv:2110.05524, 2021.   \nSaeed Mahloujifar, Alexandre Sablayrolles, Graham Cormode, and Somesh Jha. Optimal membership inference bounds for adaptive composition of sampled gaussian mechanisms. arXiv preprint arXiv:2204.06106, 2022.   \nLuise Mehner, Saskia Nu\u02dcnez von Voigt, and Florian Tschorsch. Towards explaining epsilon: A worst-case study of differential privacy risks. In 2021 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW), 2021.   \nPriyanka Nanayakkara, Johes Bater, Xi He, Jessica Hullman, and Jennie Rogers. Visualizing privacy-utility trade-offs in differentially private data releases. Proceedings on Privacy Enhancing Technologies, 2:601\u2013618, 2022.   \nPriyanka Nanayakkara, Mary Anne Smart, Rachel Cummings, Gabriel Kaptchuk, and Elissa M Redmiles. What are the chances? explaining the epsilon parameter in differential privacy. In 32nd USENIX Security Symposium (USENIX Security 23), 2023.   \nMilad Nasr, Shuang Songi, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini. Adversary instantiation: Lower bounds for differentially private machine learning. In IEEE Symposium on security and privacy (SP), 2021.   \nMilad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Trame\\`r, Matthew Jagielski, Nicholas Carlini, and Andreas Terzis. Tight auditing of differentially private machine learning. In 32nd USENIX Security Symposium (USENIX Security 23), 2023.   \nJoseph P Near, David Darais, Naomi Lefkovitz, Gary Howarth, et al. Guidelines for evaluating differential privacy guarantees. National Institute of Standards and Technology, Tech. Rep, 2023.   \nKobbi Nissim, Salil Vadhan, and David Xiao. Redrawing the boundaries on purchasing data from privacy-sensitive individuals. In Proceedings of the conference on Innovations in theoretical computer science, 2014.   \nEdouard Oyallon and St\u00b4ephane Mallat. Deep roto-translation scattering for object classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015.   \nThe pandas development team. pandas-dev/pandas: Pandas, 2020.   \nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nNatalia Ponomareva, Hussein Hazimeh, Alex Kurakin, Zheng Xu, Carson Denison, H. Brendan McMahan, Sergei Vassilvitskii, Steve Chien, and Abhradeep Guha Thakurta. How to DP-fy ML: A practical guide to machine learning with differential privacy. Journal of Artificial Intelligence Research, 2023.   \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019.   \nShahbaz Rezaei and Xin Liu. On the difficulty of membership inference attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In IEEE symposium on security and privacy $(S P)$ . IEEE, 2017. ", "page_idx": 13}, {"type": "text", "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew $\\mathrm{Ng}$ , and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2013.   \nFlorian Tramer and Dan Boneh. Differentially private learning needs better features (or much more data). In International Conference on Learning Representations, 2021.   \nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00b4efan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, I\u02d9lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u02c6onio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 2020.   \nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2018.   \nMichael L. Waskom. seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 2021. doi: 10.21105/joss.03021. URL https://doi.org/10.21105/joss.03021.   \nLarry Wasserman and Shuheng Zhou. A statistical framework for differential privacy. Journal of the American Statistical Association, 2010.   \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, et al. Huggingface\u2019s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.   \nAlexandra Wood, Micah Altman, Aaron Bembenek, Mark Bun, Marco Gaboardi, James Honaker, Kobbi Nissim, David R O\u2019Brien, Thomas Steinke, and Salil Vadhan. Differential privacy: A primer for a non-technical audience. Vand. J. Ent. & Tech. L., 2018.   \nLukas Wutschitz, Huseyin A. Inan, and Andre Manoel. dp-transformers: Training transformer models with differential privacy. https://www.microsoft.com/en-us/research/project/ dp-transformers, August 2022.   \nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF). IEEE, 2018.   \nAshkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv preprint arXiv:2109.12298, 2021.   \nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning of language models. In International Conference on Learning Representations, 2021.   \nQinqing Zheng, Jinshuo Dong, Qi Long, and Weijie Su. Sharp composition bounds for gaussian differential privacy via Edgeworth expansion. In International Conference on Machine Learning. PMLR, 2020.   \nYuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang. Optimal accounting of differential privacy via characteristic function. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, 2022a.   \nYuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang. Optimal accounting of differential privacy via characteristic function. In International Conference on Artificial Intelligence and Statistics. PMLR, 2022b. ", "page_idx": 13}, {"type": "table", "img_path": "hOcsUrOY0D/tmp/770f407302a773e427b7f7d3603a63edddd98438fa8a77c091948c62306a3df9.jpg", "table_caption": ["Table 1: Notation summary "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Attack-Aware Noise Calibration with Black-box DP Accountants ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Advantage Calibration. Proposition 2.2 implies that $(0,\\delta)$ -DP mechanisms ensure bounded advantage $\\eta\\le\\delta$ . Therefore, given access to a black-box accountant $\\varepsilon_{\\omega}(\\delta)$ or $\\delta_{\\omega}(\\varepsilon)$ we can calibrate to a given level of advantage $\\eta^{\\star}$ by ensuring $(0,\\eta^{\\star})$ -DP: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\omega\\in\\Omega}\\omega\\quad\\mathrm{~s.t.~}\\quad\\varepsilon_{\\omega}(\\eta^{\\star})=0\\quad\\mathrm{~or~}\\quad\\delta_{\\omega}(0)=\\eta^{\\star}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "This is a more generic way to perform advantage calibration using an arbitrary black-box accountant.   \nIt is equivalent to our procedure in Section 3.1 when using Doroshenko et al. (2022) accountant. ", "page_idx": 14}, {"type": "text", "text": "FPR/FNR Calibration with Grid Search. Given a black-box DP accountant, i.e., a method which computes the privacy proflie $\\varepsilon_{\\omega}(\\delta)$ of a mechanism $M_{\\omega}(\\cdot)$ , we can approximate $f_{\\omega}(\\alpha)$ by discretizing the range of $\\delta\\in[0,1]$ and solving Eq. (6) as: ", "page_idx": 14}, {"type": "equation", "text": "$$\nf_{\\omega}(\\alpha)\\geq\\operatorname*{sup}_{\\delta\\in\\{\\delta_{1},\\delta_{2},\\ldots,\\delta_{u}\\}}\\operatorname*{max}\\{0,\\,1-\\delta-e^{\\varepsilon_{\\omega}(\\delta)}\\alpha,\\,e^{-\\varepsilon_{\\omega}(\\delta)}\\cdot(1-\\delta-\\alpha)\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $0\\ \\leq\\ \\delta_{1}\\ <\\ \\delta_{2}\\ <\\ .\\ldots\\ <\\ \\delta_{u}\\ \\leq\\ 1$ . It is possible to perform an analogous discretization using $\\delta_{\\omega}(\\varepsilon)$ and Proposition 2.1, in which case we have to additionally choose a bounded subspace $\\varepsilon\\in[\\varepsilon_{\\mathrm{min}},\\varepsilon_{\\mathrm{max}}]\\subset\\mathbb{R}$ . Equivalent procedures to Eq. (20) have previously appeared in Nasr et al. (2023); Zheng et al. (2020). ", "page_idx": 14}, {"type": "text", "text": "Plugging in Eq. (20) into the problem in Eq. (18), we can calibrate mechanisms to a given $\\alpha^{\\star},\\beta^{\\star}$ using binary search (see Section 2.3) in a space $[\\omega_{\\mathrm{min}},\\omega_{\\mathrm{max}}]\\subseteq\\Omega$ to additive error $\\omega_{\\mathrm{err}}>0$ . Denoting by $\\nu$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\nu\\triangleq\\frac{\\omega_{\\mathrm{max}}-\\omega_{\\mathrm{min}}}{\\omega_{\\mathrm{err}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "the calibration requires $u\\cdot\\lceil\\log_{2}\\nu\\rceil$ evaluations of $\\varepsilon_{\\omega}(\\delta)$ . For instance, a single evaluation of the bound in Eq. (20) takes approximately one minute with $u=100$ , and six minutes with $u=1\\small{,}000$ for DP-SGD with $T=10{,}000$ using Gopi et al. (2021) accountant as an instantiation of $\\varepsilon_{\\omega}(\\delta)$ on commodity hardware (see Appendix H). In contrast, evaluating $f_{\\omega}(\\cdot)$ using Algorithm 1 in the same settings takes approximately $500\\mathrm{ms}$ at the default discretization level $\\Delta=10^{-4}$ (see Appendix D). ", "page_idx": 14}, {"type": "text", "text": "Although this approach is substantially less computationally efficient than our direct procedure in Section 3.2, its strength is that it can be used to calibrate noise in any DP algorithm which provides a way to compute its $(\\varepsilon,\\delta)$ guarantees. ", "page_idx": 14}, {"type": "text", "text": "B Detailed Calibration Algorithms ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Advantage calibration. The standard advantage calibration first finds $\\varepsilon^{\\star}$ for a given $\\delta^{\\star}<{}^{1}\\!/n$ which provides the desired advantage guarantee via Eq. (8), then calibrates noise to the derived $(\\varepsilon^{\\star},\\delta^{\\star})$ -DP guarantee using the privacy profile $\\varepsilon_{\\omega}(\\delta)$ function: ", "page_idx": 14}, {"type": "text", "text": "Algorithm 2 Standard advantage calibration ", "page_idx": 15}, {"type": "table", "img_path": "hOcsUrOY0D/tmp/97123b431cbf87bfe807f474f4065082336537b976c7d3aceab666ade8bd6bcf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "For direct calibration to advantage, we first show how to practically use the expression in Theorem 3.3 to evaluate advantage using PLRVs: ", "page_idx": 15}, {"type": "text", "text": "Algorithm 3 Compute advantage using PLRVs $(X,Y)$   \nRequire: PMF $\\mathrm{Pr}[X_{\\omega}=\\tau]$ over grid $\\{x_{1},x_{2},\\ldots,x_{k}\\}$ with $x_{1}<x_{2}<...<x_{k}$   \nRequire: PMF $\\mathrm{Pr}[Y_{\\omega}=\\tau]$ over grid $\\{y_{1},y_{2},\\dots,y_{l}\\}$ with $y_{1}<y_{2}<...<y_{l}$   \n1: procedure COMPUTEADV $\\mathbf{\\Theta}^{\\prime}(\\omega;X_{\\omega},Y_{\\omega})$   \n2: $t_{X}\\leftarrow\\operatorname*{min}\\{i\\in[k]\\mid x_{i}>0\\}$ , $t_{Y}\\gets\\operatorname*{min}\\{i\\in[l]\\ |\\ y_{i}>0\\}$   \n3: return $\\begin{array}{r}{\\sum_{i=t_{Y}}^{l}\\operatorname*{Pr}[Y_{\\omega}=y_{i}]-\\sum_{i=t_{X}}^{k}\\operatorname*{Pr}[X_{\\omega}=x_{i}]}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Given Algorithm 3, direct calibration to advantage amounts to, e.g., binary search: ", "page_idx": 15}, {"type": "table", "img_path": "hOcsUrOY0D/tmp/bd36bd58cd78351796d04b5a838cf76453e60efa59c7f8b68b5fadcd71eb0402.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "FPR/FNR Calibration. The standard approach to FPR/FNR calibration proceeds analogously to advantage calibration. First, the algorithm solves Eq. (5) to obtain the value of $\\varepsilon^{\\star}$ which ensures that a mechanism satisfies $f(\\alpha^{\\star})=\\beta^{\\star}$ . Then, the algorithm calibrates the noise to the obtained $(\\varepsilon^{\\star},\\delta^{\\star})$ pair using the privacy profile function $\\varepsilon_{\\omega}(\\delta)$ : ", "page_idx": 15}, {"type": "text", "text": "Algorithm 5 Standard FPR/FNR calibration ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Require: $\\alpha^{\\star},\\beta^{\\star},\\delta^{\\star}$ , where $\\delta^{\\star}<{\\textstyle\\frac{1}{n}}$ , privacy profile $\\varepsilon_{\\omega}(\\delta)$ . 1: Find $\\varepsilon^{\\star}$ by solving Eq. (5) for $\\varepsilon$ with fixed $\\delta=\\delta^{\\star}$ and $f(\\alpha^{\\star})=\\beta^{\\star}$ 2: Find noise parameter $\\omega^{*}$ , e.g., using binary search: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\omega^{*}\\gets\\underset{\\omega\\in\\Omega}{\\mathrm{argmin}}\\ \\mathrm{s.t.}\\ \\varepsilon_{\\omega}(\\delta^{\\star})\\geq\\varepsilon^{\\star}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "3: return $\\omega^{*}$ ", "page_idx": 15}, {"type": "text", "text": "Direct calibration to FPR/FNR amounts to, e.g., binary search, using calls to Algorithm 1: ", "page_idx": 15}, {"type": "text", "text": "Algorithm 6 Direct FPR/FNR calibration using PLRVs $(X,Y)$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Require: $\\alpha^{\\star}$ , $\\beta^{\\star}$ , PLRVs $X_{\\omega},Y_{\\omega}$ (see Algorithm 1 for a more detailed specification) 1: Find noise parameter $\\omega^{*}$ , e.g., using binary search: $\\omega^{*}\\gets\\underset{\\omega\\in\\Omega}{\\mathrm{argmin~s.t.~COMPUTEBETA}}(\\omega;\\alpha^{\\star};X_{\\omega},Y_{\\omega})\\geq\\beta^{\\star}$ ", "page_idx": 15}, {"type": "text", "text": "2: return $\\omega^{*}$ ", "page_idx": 15}, {"type": "table", "img_path": "hOcsUrOY0D/tmp/215846c7c79ac4afc8f669270ebb4f02a06bcc5545ddc7dead6b505140181026.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Calibration to Other Risk Notions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Noise calibration for a given FPR/FNR level can be seen as a basic building block to calibrate for other operational measures of risk that are functions of FPR $\\alpha$ and FNR $\\beta$ . ", "page_idx": 16}, {"type": "text", "text": "For instance, Rezaei and Liu (2021) propose to measure the risks of membership inference attacks in terms of accuracy acc and FPR $\\alpha$ , where: $\\mathsf{a c c}(\\alpha,\\beta)\\triangleq1/2\\cdot\\left((1-\\alpha)+(1-\\bar{\\beta})\\right)$ . We can calibrate for a given level of accuracy $\\mathsf{a c c}^{\\star}$ and FPR $\\alpha^{\\star}$ using the method in Section 3.2 by solving the expression for accuracy for a given $\\beta^{\\star}$ . ", "page_idx": 16}, {"type": "text", "text": "Jayaraman et al. (2021) propose to measure positive predictive value, or precision, of attacks: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathtt{p p v}(\\alpha,\\beta)\\triangleq\\frac{1-\\beta}{1-\\beta+\\alpha}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Although precision alone is not sufficient to determine the level of privacy, like with accuracy, we can calibrate for a given level of precision $\\mathsf{p p v^{\\star}}$ and FPR $\\alpha^{\\star}$ by deriving the corresponding $\\beta^{\\star}$ . ", "page_idx": 16}, {"type": "text", "text": "We provide the exact conversions in Table 2. These enable practitioners to use the calibration method in Section 3.2 while reporting technically equivalent but potentially more interpretable measures, e.g., attack accuracy at a given FPR. ", "page_idx": 16}, {"type": "text", "text": "Although throughout the paper we have assumed that the hypotheses $H_{0}$ and $H_{1}$ both have probability $1/2$ , our results and conversions can be easily extended to settings where the hypotheses are not equiprobable, as proposed by Jayaraman et al. (2021). ", "page_idx": 16}, {"type": "text", "text": "D Dominating Pairs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Constructing Discrete Dominating Pairs and their PLRVs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We summarize the technique from Doroshenko et al. (2022) to construct a dominating pair from a composed mechanism $M(\\bar{S})=M^{(1)}\\circ M^{(2)}\\circ\\cdots\\circ M^{(T)}(S)$ . This models the common use case in privacy-preserving ML where a simple mechanism, such as the subsampled Gaussian in DP-SGD, is applied $T$ times. We assume that each sub-mechanism $M^{(i)},i\\in[T]$ , has a known privacy curve $\\delta_{i}(\\varepsilon)$ . Given an input discretization parameter $\\Delta$ , a size $k$ , and a starting $\\varepsilon_{1}$ , (Doroshenko et al., 2022) creates a grid $\\{\\varepsilon_{1},\\varepsilon_{1}+\\Delta,\\ldots,\\varepsilon_{1}+k\\Delta\\}$ . Then, they compute the privacy curve on this grid $\\{\\delta_{i}(\\varepsilon_{1}),\\delta_{i}(\\varepsilon_{1}+\\dot{\\Delta}),\\dots,\\delta_{i}(\\varepsilon_{1}+k\\Delta)\\}$ , and append the values of $\\delta(-\\infty)=0$ and $\\delta(\\infty)$ . The dominating pair for the $i^{\\mathrm{th}}$ mechanism is constructed using Algorithm 7. Note that Algorithm 7 is identical to Algorithm 1 in Doroshenko et al. (2022), with the notation modified to be consistent with the notation in this paper. ", "page_idx": 16}, {"type": "text", "text": "This process is repeated for every mechanism. As long as the discretization parameter $\\Delta$ is the same for all $T$ mechanisms, the resulting collection of PLRVs can can be composed via the Fast Fourier Transform. The dominating pair for the composed mechanism $M$ is simply the distribution of $(X_{1}+X_{2}\\ldots+X_{T},Y_{1}+Y_{2}\\ldots+X_{T})$ . ", "page_idx": 16}, {"type": "text", "text": "We remark that the discretization parameter $\\Delta$ is user-defined, and the choice for the size $k$ and starting $\\varepsilon$ for each grid is mechanism-specific. For further implementation details, we point the reader to the code documentation flie and the code itself, which can be found in the dp accounting Python library,. In particular, we note that while the PLRVs $X,Y$ have the same support except for atoms at $\\pm\\infty$ , the support of the composed PLRV $X_{1}+X_{2}\\dots+X_{T}$ need not be the same as the the support of $Y_{1}+Y_{2}\\dots+Y_{T}$ . This is because in the convolution part of the implementation of Doroshenko et al. (2022), the code discards any tail probabilities smaller than some truncation parameter. This is why we allow for $X$ and $Y$ to have different support in Algorithm 1, and why we make no assumptions on the distributions of $(P,Q)$ or of $(X,Y)$ in the proof for Theorem 3.3. ", "page_idx": 16}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/f7213b503a56409a0bff7958273eb5d3b748a0691b943dcdbcbc41ee27ef76ea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D.2 Some Properties of the Trade-Off Curves of Discrete Dominating Pairs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide several observations on the trade-off curve of discrete dominating pairs. In particular, these observations hold for the trade-off curve described Theorem 3.3. ", "page_idx": 17}, {"type": "text", "text": "Connecting the Dots. From the proof of Theorem 3.3 (see Appendix E.2), we know that when the level $\\alpha$ happens to equal a point in the reverse CDF of $X$ , i.e. when $\\alpha=\\operatorname*{Pr}[X>x_{i}]$ for some $i$ , that the corresponding FNR $\\bar{T}(P,Q)(\\alpha)$ is simply the CDF of $Y$ evaluated at the same point, i.e. $T(P,Q)(\\alpha)=\\bar{\\mathrm{Pr}}[Y\\leq^{\\stackrel{\\cdot}{}}x_{i}]$ . Since the reverse CDF of $X$ can take on $k+1$ values, it follows that there are $k+1$ values of $\\alpha$ where the trade-off curve is fully characterized by the CDF of the PLRVs. ", "page_idx": 17}, {"type": "text", "text": "Next, we observe a special structure of the trade-off curve on the points outside of these $k+1$ values. For fixed $\\tau$ , Eq. (34) implies $\\alpha^{*}(\\tau,\\gamma)$ is increasing linearly in $\\gamma$ and Eq. (37) implies $\\beta^{*}(\\tau,\\gamma)$ is decreasing linearly in $\\gamma$ . This implies that the trade-off curve \u201cin between\u201d the $k+1$ points that correspond to the CDFs of the PLRVs is $a$ linear interpolation, where one \u201cconnects the dots\u201d. Hence, the trade-off curve is piece-wise linear, continuous everywhere, and not differentiable at the $k+1$ points where $\\alpha$ happens to be on the reverse CDF of $X$ . ", "page_idx": 17}, {"type": "text", "text": "This observation provides an interesting connection to Doroshenko et al. (2022), who showed that \u201cconnecting the dots\u201d between finite points on the privacy profile $\\delta(e^{\\varepsilon})^{\\parallel}$ yields a valid pessimistic estimate to the privacy proflie. Could \u201cconnecting the dots\u201d in trade-off curve space also yield a valid pessimistic estimate? The answer is clearly no: \u201cconnecting the dots\u201d on finite samples from a tradeoff curve corresponds to an optimistic bound on the trade-off curve. Nevertheless, it is interesting to note that the class of discrete and finitely supported privacy loss random variables simultaneously achieve a pessimistic bound in privacy proflie space and an optimistic bound in trade-off curve space. Further exploration of this phenomena, specifically in the context of constructing optimal optimistic privacy estimates, is left as future work. ", "page_idx": 17}, {"type": "text", "text": "Behavior at the Edges. The trade-off curve of discrete dominating $(P,Q)$ in general does not satisfy $T(P,Q)(0)\\stackrel{!}{=}1$ . Indeed, the point $\\alpha\\,=\\,0$ corresponds to $\\tau\\,=\\,x_{\\mathrm{max}}$ and $\\gamma=0$ , in which case $T(P,Q)(0)=\\mathrm{Pr}[Y\\leq x_{\\mathrm{max}}]=1-\\mathrm{Pr}[Y>x_{\\mathrm{max}}]$ . Whether or not this equals 1 depends on the details of the PLRV $Y$ , though we note that in our experiments, $T(P,Q)(0)$ is usually 1 to within a margin of $10^{-10}$ . Moreover, we have that $T(P,Q)(\\bar{\\alpha})=0$ for any $\\alpha\\in[\\mathrm{Pr}[X>-\\infty],1]$ . Indeed, for any $\\alpha\\in[\\operatorname*{Pr}[X>-\\infty],1]$ , we have that $\\tau=-\\infty$ , meaning that $\\beta^{*}(\\bar{\\tau},\\bar{\\gamma})=\\mathrm{Pr}[Y\\leq-\\infty]=0$ for any choice of $\\gamma$ . ", "page_idx": 17}, {"type": "text", "text": "The observation that $T(P,Q)(0)\\neq1$ , that $T(P,Q)$ is piece-wise linear, and that $T(P,Q)(\\alpha)=0$ for any sufficiently large $\\alpha$ , are all consistent with the findings of Jin et al. (2023), who characterized the trade-off curves of discrete-valued mechanisms. ", "page_idx": 17}, {"type": "text", "text": "E Omitted Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Omitted Proofs in Section 3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "First, let us define the notion of the convex conjugate that we use in the proofs. For a given function $f:[0,1]\\rightarrow[0,1]$ , its convex conjugate $f^{*}$ is: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf^{*}(y)=\\operatorname*{sup}_{0\\leq x\\leq1}y x-f(x),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we can show the omitted proofs. ", "page_idx": 18}, {"type": "text", "text": "Proposition 3.1. If $(P,Q)$ is a dominating pair for a mechanism $M$ , then for $\\alpha\\in[0,1]$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{S\\geq S^{\\prime}}T(M(S),M(S^{\\prime}))(\\alpha)\\geq T(P,Q)(\\alpha).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. The proof follows from taking the convex conjugate of both sides of the following result from Zhu et al. (2022b): ", "page_idx": 18}, {"type": "text", "text": "Proposition E.1 (Lemma 20 from Zhu et al. (2022b) restated in our notation). If a mechanism is $(\\varepsilon,\\bar{D}_{e^{\\varepsilon}}(P\\parallel Q)){\\cdot}D_{\\varepsilon}$ P, then it is $f$ -DP for $f$ such that the following holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{e^{\\varepsilon}}(P\\parallel Q)=1+f^{*}(-e^{\\varepsilon}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking the convex conjugate of the equation above reveals that $f$ follows exactly the structure of the trade-off curve implied by the Neyman-Pearson optimal test, which is exactly $T(P,Q)$ . See Appendix E.2.2 for more details on the Neyman-Pearson lemma. ", "page_idx": 18}, {"type": "text", "text": "Proposition 3.3 (Cost of advantage calibration). Fix a dataset size $n>1$ , and a target level of attack advantage $\\eta^{\\star}\\in(\\delta^{\\star},1)$ , where $\\delta^{\\star}={^1\\!/\\!c\\!\\cdot\\!n}$ for some $c>1$ . For any $\\begin{array}{r}{0\\,<\\,\\alpha\\,<\\,\\frac{1-\\bar{\\eta}^{\\star}}{2}}\\end{array}$ , there exists $a$ $D P$ mechanism for which the gap in FNR $f_{\\mathsf{s t a n d a r d}}(\\alpha)$ obtained with standard calibration for $\\varepsilon^{\\star}$ that ensures $\\eta\\leq\\eta^{\\star}$ , and FNR $f_{\\mathsf{a d v}}(\\alpha)$ obtained with advantage calibration is lower bounded: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Delta\\beta(\\alpha)\\triangleq f_{\\mathsf{s t a n d a r d}}(\\alpha)-f_{\\mathsf{a d v}}(\\alpha)\\geq\\eta^{\\star}-\\delta^{\\star}+2\\alpha\\frac{\\eta^{\\star}}{\\eta^{\\star}-1}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let us fix a pair of datasets $S\\simeq S^{\\prime}$ . Suppose that we have a mechanism $M:2^{\\mathbb{D}}\\rightarrow\\{0,1,2,3\\}$ which satisfies $(\\varepsilon,\\delta)$ -DP. Further, assume that for the specific fixed pair $S,S^{\\prime}$ it is defined as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(M(S)=0)=0}\\\\ &{P(M(S)=1)=(1-\\delta)\\cdot\\frac{e^{\\epsilon}}{e^{\\epsilon}+1}\\quad P(M(S^{\\prime})=1)=(1-\\delta)\\cdot\\frac{1}{e^{\\epsilon}+1}}\\\\ &{P(M(S)=2)=(1-\\delta)\\cdot\\frac{1}{e^{\\epsilon}+1}\\quad P(M(S^{\\prime})=2)=(1-\\delta)\\cdot\\frac{e^{\\epsilon}}{e^{\\epsilon}+1}}\\\\ &{P(M(S)=3)=\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The defining feature of this mechanism is that its trade-off curve $T(M(S),M(S^{\\prime}))$ for $S,S^{\\prime}$ exactly matches the $f(\\cdot)$ curve for generic $(\\varepsilon,\\delta)$ -DP mechanisms in Eq. (5) (Kairouz et al., 2015). Thus, for this mechanism we can use $f$ and $T(M(S),M(S^{\\prime}))$ interchangeably. In the rest of the proof, we assume that we are calibrating this mechanism. ", "page_idx": 18}, {"type": "text", "text": "We want to derive (1) $f_{\\tt s t a n d a r d}$ under standard calibration with $\\delta^{\\star}={^1\\!/\\!c\\!\\cdot\\!n}$ and $\\varepsilon^{\\star}$ chosen such that we have $\\eta\\leq\\eta^{\\star}$ , (2) $f_{\\mathsf{a d v}}$ under advantage calibration for ensuring $\\eta^{\\star}$ , and find their difference. ", "page_idx": 18}, {"type": "text", "text": "For this, we first solve Eq. (8) for $\\varepsilon$ to derive the corresponding $\\varepsilon^{\\star}$ that would satisfy the required level of $\\eta^{\\star}$ under standard calibration with $\\begin{array}{r}{\\delta^{\\star}=\\frac{1}{c\\cdot n}}\\end{array}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varepsilon^{\\star}=\\log\\biggl(\\frac{2\\delta^{\\star}-\\eta^{\\star}-1}{\\eta^{\\star}-1}\\biggr)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As we are interested in the low $\\alpha$ regime, let us only consider the following form of the DP trade-off curve from Proposition 2.1: ", "page_idx": 18}, {"type": "equation", "text": "$$\n~f(\\alpha)=1-\\delta-e^{\\varepsilon}\\alpha.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is easy to verify that this this form holds for $\\begin{array}{r}{0\\leq\\alpha\\leq\\frac{1-\\delta}{1+e^{\\varepsilon}}}\\end{array}$ . In the case of $(\\varepsilon^{\\star},\\delta^{\\star})$ -DP with $\\varepsilon^{\\star}$ defined by Eq. (25), a simple computation shows that this holds for $\\begin{array}{r}{0\\leq\\alpha\\leq\\frac{1-\\eta^{\\star}}{2}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "To get $f_{\\tt s t a n d a r d}$ , we plug $(\\varepsilon^{\\star},\\delta^{\\star})$ into the form in Eq. (26). Recall that by Eq. (8) advantage calibration for generic DP mechanisms is equivalent to calibrating noise to $(0,\\eta^{\\star})$ -DP. Thus, to get $f_{\\mathsf{a d v}}(\\alpha)$ , we plug into $\\varepsilon=0,\\delta=\\eta^{\\star}$ to Eq. (26). Subtracting the two, we get: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta\\beta=\\eta^{\\star}-\\delta^{\\star}+2\\alpha\\frac{\\eta^{\\star}-\\delta^{\\star}}{\\eta^{\\star}-1},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "from which we get the sought form. ", "page_idx": 19}, {"type": "text", "text": "Proposition 3.5. Given $P L R V s$ $(X_{\\omega},Y_{\\omega})$ of a discrete-valued dominating pair of a mechanism $M_{\\omega}(\\cdot)$ , choosing $\\omega^{*}$ using Eq. (18) and Algorithm $^{\\,l}$ to compute $f_{\\omega}(\\alpha)$ ensures $f_{\\omega^{*}}(\\alpha^{\\star})\\geq\\beta^{\\star}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Observe that Algorithm 1 computes the intermediate values of $\\tau$ and $\\gamma$ considered in the four cases of $\\alpha$ values in the proof of Theorem 3.3 given in Appendix E.2, and thus computes the valid trade-off curve $T(P,Q)(\\alpha)$ as defined in Eq. (12). By Proposition 3.1, $M_{\\omega}(\\cdot)$ satisfies $f$ -DP with $f=T(P,Q)$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "E.2 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Theorem 3.3 (Accounting for advantage and $f$ -DP with PLRVs). Suppose that a mechanism $M(\\cdot)$ has a discrete-valued dominating pair $(P,Q)$ with associated PLRVs $(X,Y)$ . The attack advantage $\\eta$ for this mechanism is bounded: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta\\leq\\operatorname*{Pr}[Y>0]-\\operatorname*{Pr}[X>0].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Moreover, for any $\\tau\\in\\mathbb{R}\\cup\\{\\infty,-\\infty\\}$ and $\\gamma\\in[0,1].$ , define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\beta^{*}(\\tau,\\gamma)=\\mathrm{Pr}[Y\\leq\\tau]-\\gamma\\,\\mathrm{Pr}[Y=\\tau].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For any level $\\alpha\\in[0,1]$ , choosing $\\tau=(1-\\alpha)$ -quantile of $X$ and $\\begin{array}{r}{\\gamma=\\frac{\\alpha-\\operatorname*{Pr}[X>\\tau]}{\\operatorname*{Pr}[X=\\tau]}}\\end{array}$ guarantees that $T(P,Q)(\\alpha)=\\beta^{*}(\\tau,\\gamma).$ . ", "page_idx": 19}, {"type": "text", "text": "Eq. (11) is an implication of a result by Gopi et al. (2021), which states: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta(\\varepsilon)=\\operatorname*{Pr}[Y>\\varepsilon]-e^{\\varepsilon}\\operatorname*{Pr}[X>\\varepsilon].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We get Eq. (11) by observing that $(0,\\delta)$ -DP bounds $\\eta\\leq\\delta$ from Proposition 2.2. ", "page_idx": 19}, {"type": "text", "text": "In the remainder of the proof, we show Eq. (12) and why choosing the threshold $\\tau$ and coin filp probability $\\gamma$ in the way specified in the theorem guarantees $T(P,Q)(\\alpha)=\\beta(\\tau,\\gamma)$ . In Appendix E.2.1, we establish the notation necessary for the remainder of the proof along with all the assumptions made. In Appendix E.2.2, we introduce the Neyman-Pearson lemma and use it to construct Eq. (12). Finally, in Appendix E.2.3, we prove the final statement of the theorem. ", "page_idx": 19}, {"type": "text", "text": "E.2.1 Setup, Notation, and Assumptions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Let the domain of $(P,Q)$ be $\\scriptscriptstyle\\mathcal{O}$ , which we assume to be countable. We refer to the probability mass function of $P$ as $P(\\cdot)$ and similarly for $Q$ . We allow for multiple atoms $o$ where $P(o)>{\\dot{0}}$ and $Q(o)=0$ , and also multiple atoms $o^{\\prime}$ where $Q(o^{\\prime})>0$ and $P(o^{\\prime})\\,=\\,0$ . We make no further assumptions on $(P,Q)$ . ", "page_idx": 19}, {"type": "text", "text": "Since $(P,Q)$ dominate the mechanism $M(\\cdot)$ , we know from Proposition 3.1 that the hypothesis test: ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{0}:o\\sim P,\\quad H_{1}:o\\sim Q\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is easier (the trade-off curve is less than or equal to) that the standard DP hypothesis test: ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{0}:\\theta\\sim M(S),\\quad H_{1}:\\theta\\sim M(S^{\\prime})\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $S\\simeq S^{\\prime}$ . In Appendix E.2.2, we use the Neyman-Pearson Lemma to tightly characterize the trade-off curve implied by (29). The notion of privacy loss random variables (PLRVs) $(X,Y)$ , which were defined in Def. 3.2 as $Y\\triangleq\\,\\log Q(o){\\big/}P(o)$ with $o\\sim Q$ , and $X\\triangleq\\,\\log\\dot{Q}(o^{\\prime})\\big/P(o^{\\prime})$ with $o^{\\prime}\\sim P$ , appear naturally and play a central role in the proof. ", "page_idx": 19}, {"type": "text", "text": "As such, we establish more notation on them. Let $\\tau$ denote the finite values that the PLRVs can take ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}=\\big\\{\\log Q(o)\\big/P(o)\\;\\;\\big|\\;\\;o\\in\\mathcal{O},\\;P(o)>0,\\;Q(o)>0\\big\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We let the support of $X$ be ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{X}=\\left\\{\\{-\\infty\\}\\cup\\mathcal{T}\\subsetneq\\mathcal{\\mathrm{~if~}}\\operatorname*{sup}\\mathcal{T}\\in\\mathcal{T}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and we set $\\operatorname*{Pr}[X=\\operatorname*{sup}\\mathcal{T}]=0$ if we manually append $\\operatorname{sup}\\tau$ to $X$ . We do this to make the quantile of $X$ well-defined on all countable domains. Moreover, let $x_{\\operatorname*{max}}=\\operatorname*{sup}\\mathbb{X}=\\operatorname*{sup}\\mathcal{T}$ . We will often refer to elements in the support of $X$ via $\\mathbb{X}=\\left\\{-\\infty,x_{1},x_{2},\\ldots,x_{\\mathrm{max}}\\right\\}$ . ", "page_idx": 20}, {"type": "text", "text": "E.2.2 Applying the Neyman-Pearson Lemma ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "According to the Neyman-Pearson Lemma (see, e.g., Lehmann and Romano., 2006; Dong et al., 2022), the most powerful attack at level $\\alpha$ for the hypothesis test (29) is a threshold test $\\phi^{*}:{\\mathcal{O}}\\to[0,1]$ parameterized by two numbers $\\tau\\in\\mathbb{R}\\cup\\{-\\infty,\\infty\\},\\gamma\\in[0,1],$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\phi_{\\tau,\\gamma}^{*}(o)=\\left\\{{\\begin{array}{l l}{1}&{\\mathrm{if}\\ Q(o)>e^{\\tau}P(o)}\\\\ {\\gamma}&{\\mathrm{if}\\ Q(o)=e^{\\tau}P(o)}\\\\ {0}&{\\mathrm{if}\\ Q(o)<e^{\\tau}P(o).}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which we can equivalently write as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\phi_{\\tau,\\gamma}^{*}(o)=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if}\\ \\log\\frac{Q(o)}{P(o)}>\\tau}\\\\ {\\gamma}&{\\mathrm{if}\\ \\log\\frac{Q(o)}{P(o)}=\\tau}\\\\ {0}&{\\mathrm{if}\\ \\log\\frac{Q(o)}{P(o)}<\\tau.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This threshold test works by filpping a coin and rejecting the null hypothesis (equivalently, guessing that o came from Q) with probability \u03d5\u03c4\u2217,\u03b3(o). Here, log PQ ((oo)) is the Neyman-Pearson test statistic, and $\\tau$ is the threshold for this test statistic. If the test statistic is less (greater) than the threshold, the test always rejects (accepts) the null hypothesis, and if the test statistic equals the threshold, the test flips a coin with probability $\\gamma$ to reject the null hypothesis. ", "page_idx": 20}, {"type": "text", "text": "The false positive rate of $\\phi_{\\tau,\\gamma}^{*}$ , which we denote by $\\alpha$ , is the probability that the null hypothesis is rejected $(\\phi_{\\tau,\\gamma}^{*}>0)$ ) when the null hypothesis is true $(o\\sim P)$ , and has the following form: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{*}(\\tau,\\gamma)\\triangleq\\underset{o\\sim P}{\\mathbb{E}}[\\phi_{\\tau,\\gamma}^{*}(o)]}\\\\ &{\\qquad\\qquad=\\operatorname*{Pr}[X>\\tau]+\\gamma\\operatorname*{Pr}[X=\\tau].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, the false negative rate of $\\phi_{\\tau,\\gamma}^{*}$ , which we denote $\\beta$ , is the probability that the null hypothesis is accepted $(1-\\phi_{\\tau,\\gamma}^{*}>0)$ ) when the null hypothesis is false $(o\\sim Q)$ ), and has the following form: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta^{*}(\\tau,\\gamma)\\triangleq1-\\underset{\\theta\\sim Q}{\\mathbb{E}}[\\phi_{\\tau,\\gamma}^{*}(\\theta)]}\\\\ &{\\qquad\\quad=1-(\\operatorname*{Pr}[Y>\\tau]+\\gamma\\operatorname*{Pr}[Y=\\tau])}\\\\ &{\\qquad\\quad=\\operatorname*{Pr}[Y\\leq\\tau]-\\gamma\\operatorname*{Pr}[Y=\\tau].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We have thus shown the correctness of the construction of Eq. (12). In Appendix E.2.3, we prove the final statement in Theorem 3.3. ", "page_idx": 20}, {"type": "text", "text": "E.2.3 Construction of the Trade-Off Curve of a Dominating Pair ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The goal of this section is to prove the following statement made in Theorem 3.3: ", "page_idx": 20}, {"type": "table", "img_path": "hOcsUrOY0D/tmp/98bda59be9596be6f465f5deb19d421c02967319de1776eb675bcdd15f156fe3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "where $T(P,Q)(\\alpha)$ outputs the false negative rate of the most powerful attack at level $\\alpha$ . From Appendix E.2.2, we know that the most powerful attack takes the form $\\phi_{\\tau,\\gamma}^{*}$ as defined in Eq. (32). ", "page_idx": 20}, {"type": "text", "text": "One should think of the level $\\alpha$ as a constraint on the attack $\\phi_{\\tau,\\gamma}^{*}$ . In particular, the constraint $\\alpha^{*}(\\tau,\\gamma)=\\alpha$ (where $\\alpha^{*}$ is the false positive rate of $\\phi_{\\tau,\\gamma}^{*}$ and is defined in Eq. (33)) yields a family of possible tests that all achieve the level $\\alpha$ . If $(P,Q)$ were continuous distributions, the constraint $\\alpha^{*}(\\tau,\\gamma)=\\alpha$ would uniquely determine the optimal test. This does not hold in the discrete case, and hence we must identify the most powerful test within this family. ", "page_idx": 21}, {"type": "text", "text": "Below, we list out 4 different regimes for the value of the level $\\alpha$ , identify the family of possible tests in each regime and the most powerful test, and finally give the false negative rate of the respective most powerful test. ", "page_idx": 21}, {"type": "text", "text": "1 Case $\\alpha=1$ : Recall that $X$ has a finite probability of being $-\\infty$ , meaning that the only way to have $\\alpha^{*}(\\tau,\\gamma)=1$ is to set $\\tau=-\\infty$ and $\\gamma=1$ . The corresponding false negative rate is given by $\\beta^{*}(-\\infty,1)=\\operatorname*{Pr}[Y\\leq-\\infty]-\\operatorname*{Pr}[Y=-\\infty]=0$ . ", "page_idx": 21}, {"type": "text", "text": "2 Case $\\alpha=0$ : If we choose the threshold $\\tau=x_{\\mathrm{max}}$ and the coin flip probability $\\gamma=0$ , then we have that the false positive rate of this test is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\alpha^{*}(\\tau=x_{\\operatorname*{max}},\\gamma=0)=\\operatorname*{Pr}(X>x_{\\operatorname*{max}})+\\gamma\\operatorname*{Pr}[X=x_{\\operatorname*{max}}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, any test with $\\tau>x_{\\mathrm{max}}$ has $\\alpha^{*}(\\tau,\\gamma)=0$ . However, increasing the threshold above $x_{\\mathrm{max}}$ can never decrease $\\beta^{*}$ . Moreover, a test with a threshold $\\tau<x_{\\mathrm{max}}$ cannot achieve $\\alpha=0$ . It follows that choosing $(\\tau=x_{\\operatorname*{max}},\\gamma=0)$ yields the most powerful test, which has a false negative rate of $\\beta^{*}(x_{\\operatorname*{max}},0)=\\operatorname*{Pr}[Y\\leq x_{\\operatorname*{max}}]$ . ", "page_idx": 21}, {"type": "text", "text": "3 Case $\\alpha\\,=\\,\\operatorname*{Pr}[X>x_{t}]$ for some $x_{t}\\,\\in\\,\\mathbb{X}{:}$ If we choose the threshold $\\tau\\ =\\ x_{t}$ and coin flip probability $\\gamma=0$ , then we have that the false positive rate of this test is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\alpha^{*}(\\tau=x_{t},\\gamma=0)=\\mathrm{Pr}(X>x_{t})+0}\\\\ {=\\alpha.\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, the test $\\phi_{x_{t+1},1}^{*}$ and any test with $\\tau\\in\\left(x_{t},x_{t+1}\\right)$ has $\\alpha^{*}(\\tau,\\gamma)=\\alpha$ . It is straightforward to see that all these tests are equivalent to outputting 1 if log PQ ((oo)) > $\\textstyle{\\frac{Q(o)}{P(o)}}>x_{t}$ and 0 otherwise, making them all equivalent to $\\phi_{x_{t},0}^{*}$ . Note that no other test can achieve the level $\\alpha$ , since decreasing the threshold below $x_{t}$ or above $x_{t+1}$ makes it impossible to achieve level $\\alpha$ . For fixed threshold $\\tau=x_{t}\\left(x_{t+1}\\right)$ , only a coin filp probability of $\\gamma=0(1)$ achieves level $\\alpha$ . We conclude that all the tests that achieve level $\\alpha$ have a false negative rate of $\\beta^{*}=\\operatorname*{Pr}[Y\\leq x_{t}]$ . ", "page_idx": 21}, {"type": "text", "text": "4 Otherwise: If we choose the threshold ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\tau=\\operatorname*{inf}\\{x\\in\\mathbb{X}\\mid\\alpha\\geq\\operatorname*{Pr}[X>x]\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and choose the coin flip probability $\\gamma$ to exactly satisfy the constraint that $\\alpha^{*}(\\tau,\\gamma)=\\alpha$ , i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma={\\frac{\\alpha-\\operatorname*{Pr}[X>x_{t}]}{\\operatorname*{Pr}[X=x_{t}]}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then this test achieves a false positive rate of $\\alpha$ . It is easy to see that this is the only test that achieves level $\\alpha$ , and has a false negative rate of $\\beta^{*}=\\operatorname*{Pr}[Y\\leq x_{t}]-\\gamma\\operatorname*{Pr}[Y=x_{t}]$ . ", "page_idx": 21}, {"type": "text", "text": "Note that in all regimes, there is one unique test that achieves a level $\\alpha$ and is the most powerful test. However, in some regimes of $\\alpha\\in[0,1]$ , namely regime 3, there are many different parameterizations for the same test. In these cases, we are free to choose any parameterization. For each regime, the very first test we list is the parameterization we choose. To summarize, we have the following most powerful tests: ", "page_idx": 21}, {"type": "text", "text": "1 when $\\alpha=1$ , choose $\\tau=-\\infty,\\gamma=1$   \n2 when $\\alpha=0$ , choose $\\tau=x_{\\operatorname*{max}},\\gamma=0$   \n3 when $\\alpha=\\operatorname*{Pr}[X>x_{t}]$ , choose $\\tau=x_{t},\\gamma=0$   \n4 else, choose $\\tau$ via Eq. (42), and $\\begin{array}{r}{\\gamma=\\frac{\\alpha-\\operatorname*{Pr}[X>\\tau]}{\\operatorname*{Pr}[X=\\tau]}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "It is clear from the list above that for distributions with finite support, the most powerful test can be concisely written as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\tau=\\operatorname*{inf}\\{x\\in\\mathbb{X}\\mid\\alpha\\geq\\operatorname*{Pr}[X>x]\\}}\\\\ {\\gamma=\\cfrac{\\alpha-\\operatorname*{Pr}[X>\\tau]}{\\operatorname*{Pr}[X=\\tau]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we recognize $\\tau$ as the $(1-\\alpha)$ -quantile of $X$ . ", "page_idx": 22}, {"type": "text", "text": "Note that for distributions with countably infinite support, Eq. (45) does not capture Case 2, since $\\operatorname*{Pr}[X=x_{\\operatorname*{max}}]=0$ . So, we define $\\gamma=0$ whenever $\\alpha=0$ , and $\\gamma=\\mathrm{Eq}$ . (45) otherwise. Since this work focuses on using PLRVs from Doroshenko et al. (2022), which are always finitely supported, we report Eq. (44) and Eq. (45) without this edge case in the main body. ", "page_idx": 22}, {"type": "text", "text": "We remark that similar results regarding the trade-off curve between two discrete mechanisms can be found in Jin et al. (2023). We differ from this work by parameterizing the trade-off curve using PLRVs, in contrast to Jin et al., who parameterized the trade-off curve in terms of the discrete distributions $P$ and $Q$ . Our parameterization lends itself more naturally to composition, as the PLRVs sum under composition. ", "page_idx": 22}, {"type": "text", "text": "F Practical Considerations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The algorithm of Doroshenko et al. (2022), which is implemented in the dp accounting Python library,\\*\\* handles Poisson subsampling under composition (i.e. accounting for DP-SGD) by analyzing the removal and add relations separately. This approach, to the authors knowledge, was first advocated for by Zhu et al. (2022b) (see the discussion in their Appendix). ", "page_idx": 22}, {"type": "text", "text": "In particular, instead of the algorithm outputting a dominating pair $(P,Q)$ that dominates for the symmetric add/remove relation under composition, it outputs one dominating pair for the asymmetric remove relation $(P_{\\mathrm{remove}},Q_{\\mathrm{remove}})$ and one for the asymmetric add relation $\\left(P_{\\mathrm{add}},Q_{\\mathrm{add}}\\right)$ . This means that naively applying Theorem 3.3 to, for example, $\\dot{\\left(P_{\\mathrm{add}},Q_{\\mathrm{add}}\\right)}$ , will return a trade-off curve that is only valid for DP-SGD under the asymmetric add relation. ", "page_idx": 22}, {"type": "text", "text": "To handle the case when Theorem 3.3 is applied to a dominating pair $(P,Q)$ (equivalently, the PLRVS $(X,Y))$ that only dominate a mechanism under an asymmetric neighboring relation, a more sophisticated technique is needed to map $T(P,Q)$ to the target symmetric neighboring relation. In particular, a result from (Dong et al., 2022) explains how to handle this case: ", "page_idx": 22}, {"type": "text", "text": "Proposition F.1 (Proposition F.2 from Dong et al. (2022)). Let $f:[0,1]\\,\\rightarrow\\,[0,1]$ be a convex, continuous, non-increasing function with $f(x)\\leq1-x$ for $x\\in[0,1]$ . Suppose a mechanism $M$ is $(\\varepsilon,1+f^{*}(-e^{\\varepsilon})){\\cdot}D P$ for all $\\varepsilon\\ge0$ , then it is Symm $(f)$ -DP with the symmetrization operator Symm $(f)$ defined as: ", "page_idx": 22}, {"type": "equation", "text": "$$\nS y m m(f)(x)=\\left\\{\\!\\!\\left\\{f,f^{-1}\\right\\}^{**}\\!,\\!\\!\\!\\begin{array}{l}{{i f{\\bar{x}}\\leq f({\\bar{x}}),}}\\\\ {{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ${\\bar{x}}=\\operatorname*{inf}\\{x\\in[0,1]\\mid:-1\\in\\partial f(x)\\},$ , and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\{f,f^{-1}\\}^{**}(x)=\\left\\{\\!\\!\\begin{array}{l l}{{f(x),}}&{{i f x\\leq\\bar{x},}}\\\\ {{f(\\bar{x}),}}&{{i f\\bar{x}<x\\leq f(\\bar{x}),}}\\\\ {{f^{-1}(x),}}&{{i f x>f(\\bar{x}).}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Though not explicitly stated, the proposition does assume the mechanism $M(\\cdot)$ has a symmetric neighboring relation. By letting $f$ be unspecified however, the proposition allows for the input function $f$ to correspond to an asymmetric neighboring relation. In this case, the proposition returns a trade-off curve that holds for the symmetric neighboring relation. ", "page_idx": 22}, {"type": "text", "text": "We can hence apply this proposition to the problem at hand by recalling that given a dominating pair $(P,Q)$ , we have that the mechanism is $\\left(\\varepsilon,D_{e^{\\varepsilon}}(P\\parallel Q)\\right)$ -DP. Moreover, Theorem 3.3 outputs the tradeoff function $f=T(P,Q)$ , which is exactly the function $f$ such that $D_{e^{\\varepsilon}}(P\\parallel Q)=\\mathbf{\\bar{1}}+f^{*}(-e^{\\varepsilon})$ . We can thus restate Proposition F.1 in more familiar form as: ", "page_idx": 22}, {"type": "text", "text": "Proposition F.2 (Proposition F.2 from Dong et al. (2022) restated). Suppose that $(P,Q)$ is a dominating pair for a mechanism $M(\\cdot)$ under either the add or remove relation. Then, the mechanism is Symm $:\\!(T(P,Q))$ -DP with respect to the add/remove relation. ", "page_idx": 23}, {"type": "text", "text": "Proposition F.2 allows us to, for example, use a dominating pair for the asymmetric add relation to obtain a trade-off curve for the symmetric add/remove relation. Moreover, the operator $\\mathrm{Symm}(T(P,Q))$ turns out to be straightforward to implement in practice. ", "page_idx": 23}, {"type": "text", "text": "Appendix E.2.3 details how to explicitly construct $T(P,Q)$ . It is well known that $T(Q,P)(\\alpha)=$ $\\bar{T(P,Q)}^{-1}(\\alpha)$ , hence the order of $(P,Q)$ can be easily swapped in Appendix E.2.3 to get the inverse function $T(P,Q)^{-1}$ . The only obstacle remaining is in determining $\\bar{x}=\\operatorname*{inf}\\{x\\in[\\bar{0},1]\\mid:-1\\in$ $\\partial f(x)\\}$ . Due to the structure of $T(P,Q)$ , namely that it is a piece-wise linear function parameterized by Eq. (34) and Eq. (37), it turns out that the subdifferential $\\partial f(x)$ are of the form $\\{e^{\\tau}\\}$ , where $\\tau$ are the allowable thresholds of the Neyman-Pearson lemma at level $x$ identified in each of the 4 cases of the proof laid out in Appendix E.2.3. As an example, a unique threshold of $-\\infty$ at $\\alpha=1$ implies that the derivative of $T(P,Q)$ at $\\alpha=1$ is 0, meaning the trade-off curve is flat there. ", "page_idx": 23}, {"type": "text", "text": "It follows that the constraint ${\\bar{x}}=\\operatorname*{inf}\\{x\\in[0,1]\\mid:-1\\in\\partial f(x)\\}$ implies that $\\textstyle{\\bar{x}}$ is the smallest level $\\alpha$ where the threshold switches signs, i.e. $\\bar{x}=\\overset{\\cdot}{\\alpha^{*}}(\\tau=0,\\gamma=0)=\\operatorname*{Pr}[X>0]$ and $f(\\bar{x})=\\beta^{*}(\\tau=$ $0,\\gamma=0)=\\operatorname*{Pr}[Y\\leq0]$ . This gives us all the information needed to implement the Symm operator. ", "page_idx": 23}, {"type": "text", "text": "G Calibrating Gaussian Mechanism ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In the case where the trade-off curve of the mechanism has a closed form, we can solve the calibration problems in Eqs. (13) and (18) exactly without resorting to the numerical procedures in Sections 3.1 and 3.2. ", "page_idx": 23}, {"type": "text", "text": "Definition G.1. For a given non-private algorithm $q:2^{\\mathbb{D}}\\to\\mathbb{R}^{d}$ , a Gaussian mechanism (GM) is defined as $M(S)=q(S)+\\xi$ , where $\\xi\\sim\\mathcal{N}(0,\\Delta_{2}\\cdot\\sigma^{2}\\cdot I_{d})$ and $\\Delta_{2}\\triangleq\\operatorname*{sup}_{S\\simeq S^{\\prime}}\\|q(S)-q(S^{\\prime})\\|_{2}$ is the sensitivity of $q(S)$ . ", "page_idx": 23}, {"type": "text", "text": "For the Gaussian mechanism, we can exactly compute the relevant adversary\u2019s error rates: ", "page_idx": 23}, {"type": "text", "text": "Proposition G.1 (Balle and Wang (2018); Dong et al. (2022)). Suppose that $M_{\\sigma}(S)$ is GM with sensitivity $\\Delta_{2}$ and noise variance $\\sigma^{2}$ . Denote by $\\mu=\\left.{\\Delta_{2}}\\right/\\sigma$ and by $\\Phi(t)$ the CDF of the standard Gaussian distribution $\\mathcal{N}(0,1)$ . Then, ", "page_idx": 23}, {"type": "text", "text": "\u2022 The mechanism satisfies $(\\varepsilon,\\delta)$ -DP if the following holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\delta=\\Phi\\left(\\frac{\\mu}{2}-\\frac{\\varepsilon}{\\mu}\\right)-e^{\\varepsilon}\\Phi\\left(-\\frac{\\mu}{2}-\\frac{\\varepsilon}{\\mu}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "\u2022 It satisfies $f$ -DP with: ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(\\alpha)=\\Phi\\left(\\Phi^{-1}(1-\\alpha)-\\mu\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With these closed-form expressions, we can solve the calibration problems exactly: ", "page_idx": 23}, {"type": "text", "text": "Corollary G.2 (Advantage calibration for GM). For a GM $M_{\\sigma}(S)$ and target ${\\eta}^{\\star}>0$ , choosing $\\sigma$ as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sigma=\\frac{\\Delta_{2}}{2\\Phi^{-1}\\left(\\frac{\\eta^{\\star}+1}{2}\\right)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "ensures that adversary\u2019s advantage is upper bounded by $\\eta^{\\star}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Corollary G.2. It is sufficient to ensure $(0,\\eta^{\\star})$ -DP. Plugging in $\\varepsilon\\,=\\,0$ and $\\delta\\,=\\,\\eta^{\\star}$ into Eq. (48), we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta^{\\star}=\\Phi\\left(\\frac{\\mu}{2}\\right)-\\Phi\\left(-\\frac{\\mu}{2}\\right)=2\\Phi\\left(\\frac{\\mu}{2}\\right)-1,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "from which we can derive $\\begin{array}{r}{\\mu=\\frac{\\Delta_{2}}{\\sigma}=2\\Phi^{-1}\\left(\\frac{\\eta^{\\star}+1}{2}\\right)}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "By solving Eq. (49) for $\\alpha$ , we also have an exact expression for calibrating to a given level of $\\alpha^{\\star},\\beta^{\\star}$ : ", "page_idx": 23}, {"type": "text", "text": "Corollary G.3 (FPR/FNR calibration for GM). For a Gaussian mechanism $M_{\\sigma}(S)$ , and target $\\alpha^{\\star}\\geq0$ , $\\beta^{\\star}\\geq0$ such that $\\alpha^{\\star}+\\beta^{\\star}\\leq1,$ , choosing $\\sigma$ as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sigma=\\frac{\\Delta_{2}}{\\Phi^{-1}(1-\\alpha^{\\star})-\\Phi^{-1}(\\beta^{\\star})}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "ensures that adversary\u2019s FNR and FPR rates are lower bounded by $\\alpha^{\\star}$ and $\\beta^{\\star}$ , respectively. ", "page_idx": 24}, {"type": "text", "text": "Note that using the exact expressions above to calibrate Gaussian mechanism offer only computational advantages compared the method in the main body. In terms of resulting noise scale $\\sigma$ , the results are the same as with generic PLRV-based calibration up to a numerical approximation error. ", "page_idx": 24}, {"type": "text", "text": "H Additional Experiments, Details, and Figures ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "H.1 Computing Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We use a commodity machine with AMD Ryzen 5 2600 six-core CPU, 16GB of RAM, and an Nvidia GeForce RTX 4070 GPU with 16GB of VRAM to run our experiments. All experiments with deep learning take up to four hours to finish. ", "page_idx": 24}, {"type": "text", "text": "H.2 Experimental Setup ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In all our experimental results, the neighborhood relation $S\\simeq S^{\\prime}$ is the add-remove relation, i.e., $S\\simeq S^{\\prime}$ iff $|\\bar{S}\\,\\Delta\\,S^{\\prime}|=1$ , which is the standard relation used by modern DP-SGD accountants. See more on implementation details related to the neighborhood relation in Appendix F. ", "page_idx": 24}, {"type": "text", "text": "Text Sentiment Classification. We follow Yu et al. (2021) to finetune a GPT-2 (small) (Radford et al., 2019) using LoRA (Hu et al., 2021) with DP-SGD on the SST-2 sentiment classification task (Socher et al., 2013) from the GLUE benchmark (Wang et al., 2018). We use the Poisson subsampling probability $p\\,\\approx\\,0.004$ corresponding to expected batch size of 256, gradient clipping norm of $\\Delta_{2}=1.0$ , and finetune for three epochs with LoRA of dimension 4 and scaling factor of 32. We vary the noise multiplier $\\sigma\\in\\{0.5715,\\stackrel{\\cdot}{0.6072,}0.6366,0.6945,0.7498\\}$ approximately corresponding to $\\varepsilon\\,\\in\\,\\{3.95,3.2,2.7,1.9,1.45\\}$ , respectively, at $\\delta=10^{-5}$ . We use the default training split of the SST-2 dataset containing 67,348 examples for finetuning, and the default validation split containing 872 examples as a test set. ", "page_idx": 24}, {"type": "text", "text": "Image Classification. We follow Tramer and Boneh (2021) to train a convolutional neural network (Tramer and Boneh, 2021, Table 9, Appendix) over the ScatterNet features (Oyallon and Mallat, 2015) on the CIFAR-10 (Krizhevsky et al., 2009) image classification dataset. We use the Poisson subsampling probability of $p\\approx0.16$ corresponding to expected batch size of 8192, learning rate of 4, Nesterov momentum of 0.9, and gradient clipping norm of $\\Delta_{2}=0.1$ . We train for up to 100 epochs. We vary the gradient noise multiplier $\\sigma/\\bar{\\Delta_{2}}\\in\\bar{\\{4,5,6,8,10\\}}$ , corresponding to $\\varepsilon\\in\\{5,3.86,3.15,2.31,1.63\\}$ , respectively, at $\\delta=1\\bar{0}^{-5}$ . We use the default 50K/10K train/test split of CIFAR-10. ", "page_idx": 24}, {"type": "text", "text": "H.3 Additional Experiments with Histogram Release ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Histogram release is a simple but common usage of DP, appearing as a building block, e.g., in private query interfaces (Gaboardi et al., 2020). To evaluate attack-aware noise calibration for histogram release, we use the well-known ADULT dataset (Becker and Kohavi, 1996) comprising a small set of US Census data. We simulate the release of the histogram of the \u2018Education\u2019 attribute (with 16 distinct values, e.g., \u201cHigh school\u201d, \u201cBachelor\u2019s\u201d, etc.) using the standard Gaussian mechanism with post-processing to ensure that the counts are positive integers. To measure utility, we use the $L_{1}$ distance (error) between the original histogram and the released private histogram. ", "page_idx": 24}, {"type": "text", "text": "Figure 7 shows the increase in utility if we calibrate the noise of the mechanism using the direct calibration algorithm to a given level of FPR $\\alpha^{\\star}$ and FNR $\\beta^{\\star}$ vs. standard calibration over 100 simulated releases with different random seeds. In certain cases, e.g., for $\\alpha^{\\star}=0.1$ and $\\beta^{\\star}=0.75$ , our approach decreases the error by approx. $3\\times$ from three erroneous counts on average to one. ", "page_idx": 24}, {"type": "text", "text": "H.4 Software ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We use the following key open-source software: ", "page_idx": 25}, {"type": "text", "text": "\u2022 PyTorch (Paszke et al., 2019) for implementing neural networks.   \n\u2022 huggingface (Wolf et al., 2019) suite of packages for training language models.   \n\u2022 opacus (Yousefpour et al., 2021) for training PyTorch neural networks with DP-SGD.   \n\u2022 dp-transformers (Wutschitz et al., 2022) for differentially private finetuning of language models.   \n\u2022 numpy (Harris et al., 2020), scipy (Virtanen et al., 2020), pandas (pandas development team, 2020), and jupyter (Kluyver et al., 2016) for numeric analyses.   \n\u2022 seaborn (Waskom, 2021) for visualizations. ", "page_idx": 25}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/1a06e777b76732d7be02a4f8c1c86157c241960793214168ef265f9ffe7af16d.jpg", "img_caption": ["Figure 5: Trade-off curves of a Gaussian mechanism that satisfies $(\\varepsilon,\\delta)$ -DP. Each curve shows a boundary of the feasible region (greyed out) of possible membership inference attack FPR $(\\alpha)$ and FNR $(\\beta)$ pairs. The solid curve shows the limit of the feasible region guaranteed by DP via Eq. (5), which is a conservative overestimate of attack success rates compared to the exact trade-off curve (dotted). The maximum advantage $\\eta$ is achieved with FPR and FNR at the point closest to the origin. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/6bda6b2f7b62573c44de3936f9dacc408779c942ec04df98bb4e3bd202339dcb.jpg", "img_caption": ["Figure 6: The increase in attack sensitivity due to calibration for advantage is less drastic for Gaussian mechanism than for a generic $(\\varepsilon,\\delta)$ -DP mechanism. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "hOcsUrOY0D/tmp/0f25c8c780d04ef4038a6e83aa7dd71f512bf0f000ae569b67d838b9778c709b.jpg", "img_caption": ["Figure 7: Direct calibration to attack FNR/FPR reduces average $L_{1}$ error in histogram release with Gaussian mechanism. The confidence bands are $95\\%$ CI over 100 simulated releases. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Claims in the abstract/intro succinctly represent the claims in the main body. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Section 3.1 discusses in detail the limitations of advantage calibration. Section 5 discusses limitations and future work for the whole paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201cLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The theoretical results are within the standard setup of differential privacy detailed in Section 2.1. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We provide the detailed information on reproducing the experimental results in Appendix H. Moreover, we link the code along with the instructions for reproducing. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We use common openly available benchmark datasets. We have published the code on the Github platform. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide the information on the machine learning details in the main body as well as in Appendix H. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: In our setting, we can directly approximate the theoretical quantities of interest (i.e., the level of privacy) without the need for empirical statistical methods and the corresponding uncertainty estimation. For the empirically evaluated model accuracy values, we only use one seed in the main suite of experiments for computational reasons. In the additional experiments in Appendix H, we provide $95\\%$ confidence bands. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \u201cYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our experiments only require commodity hardware. We detail the requirements in Appendix H. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Neither the research process itself nor the outcomes of the research carry significant potential for harm. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The topic of our paper is concerned with a social issue of privacy in machine learning and statistical analyses, and our work aims to improve the state of the art in the area. Although our work is mostly technical, we take a broader look in Sections 1 and 5. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We cite the dataset sources as well the sources for the key pieces of software used for the experimental evaluations and analyses in the main body and Appendix H. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]