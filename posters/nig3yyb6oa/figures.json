[{"figure_path": "niG3Yyb6oA/figures/figures_3_1.jpg", "caption": "Figure 1: Comparison of the exact Fisher information matrix F and our approximation FLNGD. We use LNGD to train MNIST on a fully-connected neural network, whose architecture is 196-20-20-20-20-10. We show the results of the Fisher information matrix of the first layer with 20 units in top, which is a 400 \u00d7 400 matrix. The bottom portion displays partially enlarged parts of the top marked with red square, which is a 40 \u00d7 40 matrix. Within both the top and bottom sections, on the left is the exact Fisher information matrix F, in the middle is our approximation FLNGD, and on the right is the absolute error between them. The brightness levels correspond to the sizes of the absolute values.", "description": "This figure compares the exact Fisher information matrix (F) with the approximation (FLNGD) proposed by the authors.  The top row shows the entire 400x400 matrix for the first layer of a neural network trained on MNIST. The bottom row shows a zoomed-in 40x40 section of the top matrices. The left column shows the exact matrix F, the middle column displays the approximation FLNGD, and the right column shows the absolute error between them, with brighter colors indicating larger errors.", "section": "3.1 Layer-Wise Sample Approximation"}, {"figure_path": "niG3Yyb6oA/figures/figures_8_1.jpg", "caption": "Figure 2: Numerical performance on ResNet-18 with CIFAR-10.", "description": "This figure compares the performance of four different optimizers (SGD, Adam, KFAC, and LNGD) on the CIFAR-10 dataset using ResNet-18 architecture.  The plots show the training loss and testing accuracy over epochs and training time (in seconds). LNGD demonstrates faster convergence and higher accuracy than the other optimizers.", "section": "4 Experiments"}, {"figure_path": "niG3Yyb6oA/figures/figures_8_2.jpg", "caption": "Figure 2: Numerical performance on ResNet-18 with CIFAR-10.", "description": "This figure displays the training and testing results of the ResNet-18 model trained on the CIFAR-10 dataset using four different optimizers: SGD, Adam, KFAC, and LNGD. The plots show the changes in training loss and testing accuracy over time (in seconds) and epochs.  LNGD demonstrates faster convergence and higher testing accuracy compared to the other optimizers.", "section": "4 Experiments"}, {"figure_path": "niG3Yyb6oA/figures/figures_9_1.jpg", "caption": "Figure 4: Numerical performance on Transformer with WMT.", "description": "The figure shows the training loss and testing BLEU score (a common metric for machine translation) over training steps and time in seconds for four different optimizers: SGD, Adam, KFAC, and LNGD.  The plots illustrate the convergence speed and performance of each optimizer on the WMT English-German machine translation task.  LNGD demonstrates faster convergence and better performance compared to the other optimizers.", "section": "4.3 Transformer Training"}, {"figure_path": "niG3Yyb6oA/figures/figures_14_1.jpg", "caption": "Figure 5: Comparison of the exact Fisher information matrix and the approximated Fisher information matrix of KFAC and LNGD. On the left is the exact Fisher information matrix, in the middle is the approximated Fisher information matrix, and on the right is the absolute error of these. The first row shows the result of KFAC, and the second row shows the results of LNGD.", "description": "This figure compares the exact Fisher Information Matrix (FIM) with approximations from KFAC and LNGD methods.  Each row represents a comparison for a single layer. The left column shows the true FIM; the middle shows the approximation by KFAC (top row) and LNGD (bottom row); and the right shows the absolute difference between the true FIM and the approximation.  The visualization helps to understand how well each approximation captures the true FIM, specifically highlighting the diagonal elements as they are particularly emphasized by LNGD.", "section": "B.2 Comparisons Between KFAC and LNGD"}, {"figure_path": "niG3Yyb6oA/figures/figures_15_1.jpg", "caption": "Figure 6: Illustration of Gaussian distribution.", "description": "The figure shows four histograms visualizing the distributions of sample representation vectors' values in some dimensions and Euclidean norm of two layers of ResNet-18 network on CIFAR-10.  The distributions are shown to be approximately Gaussian, supporting the Gaussian distribution assumption used in the layer-wise sample approximation of the Fisher information matrix.", "section": "B.3 Illustration of the Gaussian Distribution Assumption"}, {"figure_path": "niG3Yyb6oA/figures/figures_22_1.jpg", "caption": "Figure 1: Comparison of the exact Fisher information matrix F and our approximation FLNGD. We use LNGD to train MNIST on a fully-connected neural network, whose architecture is 196-20-20-20-20-10. We show the results of the Fisher information matrix of the first layer with 20 units in top, which is a 400 \u00d7 400 matrix. The bottom portion displays partially enlarged parts of the top marked with red square, which is a 40 \u00d7 40 matrix. Within both the top and bottom sections, on the left is the exact Fisher information matrix F, in the middle is our approximation FLNGD, and on the right is the absolute error between them. The brightness levels correspond to the sizes of the absolute values.", "description": "This figure compares the exact Fisher information matrix (F) with the approximation used in the proposed LNGD method (FLNGD).  It visualizes the matrices for the first layer of a neural network trained on the MNIST dataset, showcasing both the full 400x400 matrix and a zoomed-in 40x40 section. The rightmost column in each row displays the absolute error between the exact and approximated matrices, highlighting the accuracy of the approximation.", "section": "3 LNGD: A Layer-Wise Second-Order Optimizer"}]