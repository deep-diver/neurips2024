{"importance": "This paper is crucial for researchers working with large vision-language models (LVLMs). It addresses critical issues in current evaluation methods, proposing new metrics and a benchmark to ensure fair and accurate assessment of model performance. This will significantly impact future research in this rapidly advancing field, as well as improve the reliability of model development and comparison.", "summary": "MMStar benchmark tackles flawed LVLMs evaluation by focusing on vision-critical samples, minimizing data leakage, and introducing new metrics for fair multi-modal gain assessment.", "takeaways": ["Current LVLMs benchmarks suffer from visual content irrelevancy and data leakage issues.", "MMStar, a new benchmark with 1500 meticulously curated samples, addresses these issues.", "New metrics (multi-modal gain & leakage) provide more accurate LVLMs performance evaluation."], "tldr": "Large vision-language models (LVLMs) evaluation is currently hindered by two major problems: many samples don't require visual understanding, and unintentional data leakage occurs during training.  This leads to misjudgments about actual multi-modal improvements and misguides research. \nTo address this, the authors introduce MMStar, a new benchmark with 1500 carefully selected samples.  These samples are designed to be purely vision-dependent, minimize data leakage, and challenge the advanced multi-modal capabilities of LVLMs.  Furthermore, they propose two new metrics: multi-modal gain and multi-modal leakage, to provide more accurate measurements of LVLMs' true performance and data leakage during training.", "affiliation": "University of Science and Technology of China", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "evP9mxNNxJ/podcast.wav"}