[{"figure_path": "evP9mxNNxJ/figures/figures_1_1.jpg", "caption": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.", "description": "This figure shows four examples highlighting issues in existing multi-modal benchmark datasets.  Panel (a) demonstrates cases where LLMs can answer questions using only their existing knowledge without any need for visual input. Panel (b) shows examples where the question itself contains the answer, rendering the image irrelevant.  Panel (c) illustrates unintentional data leakage in LLM training datasets. Panel (d) shows how LVLMs can answer questions without visual input, suggesting data leakage in their training.", "section": "Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_4_1.jpg", "caption": "Figure 2: LLM hit rate across various benchmarks.", "description": "The figure shows the percentage of questions in several popular multi-modal benchmarks that could be answered correctly by most LLMs without using any visual input.  It illustrates the prevalence of samples where visual content is unnecessary, highlighting a shortcoming in the design of these benchmarks.  The high hit rates (ScienceQA at 57.2%, AI2D at 46.2%) indicate that a significant portion of these benchmarks primarily assess LLMs' textual capabilities rather than true multi-modal reasoning.", "section": "3 Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_6_1.jpg", "caption": "Figure 3: Details of MMStar benchmark. (a) Statics of the data sources during the data curation process. After applying the coarse filter process and manual review, we narrow down from a total of 22,401 samples to 11,607 candidate samples and finally select 1,500 high-quality samples to construct our MMStar benchmark. (b) We display 6 core capabilities in the inner ring, with 18 detailed axes presented in the outer ring. The middle ring showcases the number of samples for each detailed dimension.", "description": "This figure shows the process of curating the MMStar benchmark dataset.  The left panel (a) illustrates the data reduction steps from 22,401 initial samples to the final 1,500 samples through a coarse filter and manual review process. The right panel (b) displays the composition of MMStar, showing its six core capabilities (inner ring), eighteen detailed axes (outer ring), and the number of samples per axis (middle ring).", "section": "4 MMStar"}, {"figure_path": "evP9mxNNxJ/figures/figures_8_1.jpg", "caption": "Figure 4: LLMs perform close to random guessing (the dashed line) on MMStar.", "description": "The figure is a scatter plot showing the average performance of various LLMs on the MMStar benchmark against the number of parameters in each model.  The plot demonstrates that, despite differences in model size and architecture, most LLMs achieve performance very close to that of a random baseline (indicated by the dashed horizontal line). This suggests that visual content is crucial for solving the MMStar benchmark, as the LLMs are unable to leverage their language understanding capabilities alone to provide accurate answers. The best-performing LLM, Qwen1.5-1.8B, still only achieves a score only slightly above the random baseline. ", "section": "5.1 Results Analysis of MMStar"}, {"figure_path": "evP9mxNNxJ/figures/figures_17_1.jpg", "caption": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.", "description": "This figure shows four examples highlighting issues with existing multi-modal benchmarks.  Panel (a) demonstrates samples answerable by LLMs using only text. Panel (b) shows cases where the question implies the answer, making the image irrelevant. Panel (c) illustrates samples unintentionally present in LLM training data, allowing LLMs to answer without visual input. Panel (d) shows samples that LVLMs can solve without images, suggesting leakage into LVLM training data.", "section": "Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_18_1.jpg", "caption": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.", "description": "This figure shows four examples of problems found in existing multi-modal benchmark datasets. The problems highlighted are: 1. Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. 2. Unintentional data leakage exists in LLM and LVLM training. LLMs and LVLMs could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. ", "section": "Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_19_1.jpg", "caption": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.", "description": "This figure shows four examples from existing multi-modal benchmarks to illustrate the issues of visual content not being necessary and unintentional data leakage in LLM and LVLM training.  Each subfigure demonstrates one aspect of these problems.  Specifically, it illustrates instances where:\n(a) LLMs can answer questions using only world knowledge.\n(b) The question already contains the answer; images are irrelevant.\n(c) LLMs can answer questions because the questions and answers were in the LLMs' training data.\n(d) LVLMs solve questions without visual input, suggesting data leakage in the LVLM training data.", "section": "Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_20_1.jpg", "caption": "Figure 5: Illustration of data leakage during LVLMs' multi-modal training processes. We showcase samples that LLMs cannot answer correctly but LVLMs without accessing images (LVLM-text) can. Each LLM-LVLMtext pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs. The chart in the center tallies the number of samples in existing benchmarks hit by more than half of the LLM-LVLMtext pairs, underscoring the issue of data leakage during the multi-modal training process.", "description": "This figure demonstrates cases where LLMs fail to answer correctly, but LVLMs (even without visual input) succeed. This highlights unintentional data leakage from LLMs and LVLMs during training, where models memorize samples from training data instead of using multi-modal reasoning. The central chart shows the percentage of samples in various benchmarks that were correctly answered by more than half of the LLMs tested, illustrating the prevalence of data leakage.", "section": "Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_21_1.jpg", "caption": "Figure 5: Illustration of data leakage during LVLMs' multi-modal training processes. We showcase samples that LLMs cannot answer correctly but LVLMs without accessing images (LVLM-text) can. Each LLM-LVLMtext pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs. The chart in the center tallies the number of samples in existing benchmarks hit by more than half of the LLM-LVLMtext pairs, underscoring the issue of data leakage during the multi-modal training process.", "description": "This figure shows examples where LLMs fail to answer questions correctly, but LVLMs (without visual input) succeed. This highlights the problem of data leakage in LVLMs' training data.  The chart summarizes the number of samples across benchmarks where more than half of the tested LLMs and LVLMs could answer correctly without visual input, thus demonstrating how widespread data leakage is within existing multimodal benchmarks.", "section": "3 Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_22_1.jpg", "caption": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.", "description": "This figure shows four examples of issues found in existing multi-modal benchmarks. These issues include questions that can be answered without visual information, questions where the answers are already present in the question, and unintended data leakage in LLM and LVLM training data. These issues lead to inaccurate evaluation results and potentially misleading research directions.", "section": "Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_22_2.jpg", "caption": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.", "description": "This figure shows four examples of issues with current multi-modal benchmarks. The first two examples show questions that LLMs can answer without visual input, because the answers are found in the question or can be derived from world knowledge.  The second two examples show questions that LVLMs can answer even without visual input, because these questions were present in the training data for the LLMs and LVLMs. These issues highlight the necessity of improved benchmarks that are free of data leakage and truly require multi-modal reasoning to be answered.", "section": "Introduction"}, {"figure_path": "evP9mxNNxJ/figures/figures_22_3.jpg", "caption": "Figure 5: Illustration of data leakage during LVLMs' multi-modal training processes. We showcase samples that LLMs cannot answer correctly but LVLMs without accessing images (LVLM-text) can. Each LLM-LVLMtext pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs. The chart in the center tallies the number of samples in existing benchmarks hit by more than half of the LLM-LVLMtext pairs, underscoring the issue of data leakage during the multi-modal training process.", "description": "This figure demonstrates unintentional data leakage in existing multi-modal benchmarks by showing examples where LLMs fail to answer questions correctly, but LVLMs (without visual input) succeed, indicating that these samples were memorized during training.  The central chart further quantifies this issue by showing the percentage of samples in various benchmarks that LLMs could answer without visual input.", "section": "Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_22_4.jpg", "caption": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.", "description": "The figure showcases four examples illustrating the two main issues the paper identifies with existing multi-modal benchmark datasets.  These problems are: (a) Visual content is unnecessary for many samples (the answer can be inferred from text alone). (b) The question itself contains the answer making visual input redundant. (c) Unintentional data leakage in LLM/LVLM training - LLMs/LVLMs can answer visual-necessary questions without image content, indicating memorization of training data. (d) Data leakage in LVLM training - LVLMs can answer questions that LLMs cannot, suggesting memorization of multi-modal training data.", "section": "Introduction"}, {"figure_path": "evP9mxNNxJ/figures/figures_22_5.jpg", "caption": "Figure 5: Illustration of data leakage during LVLMs' multi-modal training processes. We showcase samples that LLMs cannot answer correctly but LVLMs without accessing images (LVLM-text) can. Each LLM-LVLMtext pair represents an LLM and its corresponding LVLM without accessing images, totaling 16 pairs. The chart in the center tallies the number of samples in existing benchmarks hit by more than half of the LLM-LVLMtext pairs, underscoring the issue of data leakage during the multi-modal training process.", "description": "This figure shows examples of data leakage in existing multi-modal benchmarks.  It illustrates how LLMs without visual input are able to answer questions that require visual understanding, demonstrating that some benchmark questions unintentionally appeared in the training data of LLMs and LVLMs. The chart summarizes this phenomenon by showing the percentage of samples correctly answered by more than half of the evaluated LLMs.", "section": "Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/figures/figures_22_6.jpg", "caption": "Figure 1: We highlight cases in existing multi-modal benchmarks where evaluation samples either lack visual dependency or have unintentionally leaked into the training data of LLMs and LVLMs. (a) Some samples can be answered by LLMs using only text-based world knowledge; (b) For some instances, the question itself contains the answer, making images superfluous; (c) Some samples are leaked into LLMs' training corpora can be \"recalled\" with the textual questions and answers directly; (d) Some samples indiscernible to LLMs but solved by LVLMs without accessing images suggest leakage into LVLMs' multi-modal training data.", "description": "This figure shows four examples illustrating issues in existing multimodal benchmarks.  (a) shows examples where LLMs can answer questions using only world knowledge, (b) shows examples where questions themselves contain the answers, making the images unnecessary. (c) shows examples of unintentional data leakage in LLM training data, where LLMs can 'recall' answers without visual input, and (d) demonstrates data leakage in LVLM training, where LVLMs can answer questions without visual input that LLMs cannot answer.", "section": "Introduction"}]