[{"figure_path": "evP9mxNNxJ/tables/tables_3_1.jpg", "caption": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2-shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [64]), MMB (MMBench-EN-Dev [34]), ScienceQA (ScienceQA-Test [38]), AI2D (AI2D-Test [26]), SEED (SEED-Image [27]), and MathVista (MathVista-Mini [37]). The best results are highlighted in bold and underlined.", "description": "This table presents the performance of 22 large language models (LLMs) on six popular multi-modal benchmarks using a 2-shot inference strategy.  It compares the performance of both closed-source and open-source LLMs across different benchmarks, highlighting the best results for each benchmark.  The table aims to illustrate the varying capabilities of different LLMs in answering questions that may or may not require visual input, setting the stage for the subsequent analysis of multi-modal models.", "section": "3 Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/tables/tables_5_1.jpg", "caption": "Table 2: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0-shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.", "description": "This table presents a comparison of the performance of several Large Vision-Language Models (LVLMs) and their corresponding Large Language Models (LLMs) across six popular multi-modal benchmarks.  The \"strategy\" column indicates whether the model used only text (\"LLM\") or both text and images (\"LVLM\") during the evaluation.  Results are shown for multiple metrics across different models to evaluate multi-modal capabilities and data leakage during training.  Due to space limitations, only a subset of models are presented in the table.", "section": "2 Related Work"}, {"figure_path": "evP9mxNNxJ/tables/tables_7_1.jpg", "caption": "Table 3: Evaluation of various LVLMs on MMStar. We report the results of 2 closed-source LVLMs and 14 open-source LVLMs with varying sizes and architectures. We report the detailed results of the CP (coarse perception), FP (fine-grained perception), IR(instance reasoning), LR (logical reasoning), ST (science & technology), and MA (mathematics) core capabilities. The best results are highlighted in bold and underlined. The worst results of multi-modal gain (MG) and multi-modal leakage (ML) metrics are in italic red.", "description": "This table presents the performance of 16 Large Vision-Language Models (LVLMs) on the MMStar benchmark.  It breaks down the results by six core multimodal capabilities (coarse perception, fine-grained perception, instance reasoning, logical reasoning, science & technology, and mathematics), showing the average score for each and the multi-modal gain (MG) and multi-modal leakage (ML). The best performing models for each capability are highlighted.", "section": "5.1 Results Analysis of MMStar"}, {"figure_path": "evP9mxNNxJ/tables/tables_9_1.jpg", "caption": "Table 2: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0-shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.", "description": "This table presents the performance comparison of various Large Vision-Language Models (LVLMs) and their corresponding Large Language Models (LLMs) across six popular multi-modal benchmarks.  The results are shown for two evaluation strategies: using the full LVLM with visual input and using only the LLM text component (without visual input).  The table highlights the best performance in the \"LVLM-text\" setting (using only the text part of the model, without images) to illustrate data leakage issues. It helps in understanding the actual multimodal capabilities of LVLMs beyond the LLM backbone and reveals unintentional data leakage.", "section": "2 Related Work"}, {"figure_path": "evP9mxNNxJ/tables/tables_16_1.jpg", "caption": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2-shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [64]), MMB (MMBench-EN-Dev [34]), ScienceQA (ScienceQA-Test [38]), AI2D (AI2D-Test [26]), SEED (SEED-Image [27]), and MathVista (MathVista-Mini [37]). The best results are highlighted in bold and underlined.", "description": "This table presents the performance of 22 LLMs (2 closed-source and 20 open-source) on six popular multi-modal benchmarks using a 2-shot inference strategy.  The benchmarks assess different aspects of multimodal understanding. The table highlights the best-performing LLMs for each benchmark and provides an average performance score across all benchmarks.  It demonstrates that even powerful LLMs can solve many questions without needing the visual information, highlighting a potential problem in existing multi-modal benchmark design.", "section": "3 Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/tables/tables_23_1.jpg", "caption": "Table 1: Evaluation of various LLMs on six popular multi-modal benchmarks under 2-shot. We employ a 2-shot inference strategy for evaluating all LLMs to reduce instances of refusal to answer and align the answer formats. We report the results of 2 closed-source LLMs and 20 open-source LLMs with varying sizes and architectures. The evaluated benchmarks include MMMU (MMMU-Val [64]), MMB (MMBench-EN-Dev [34]), ScienceQA (ScienceQA-Test [38]), AI2D (AI2D-Test [26]), SEED (SEED-Image [27]), and MathVista (MathVista-Mini [37]). The best results are highlighted in bold and underlined.", "description": "This table presents the performance of 22 large language models (LLMs) on six popular multi-modal benchmarks using a 2-shot inference strategy.  It compares the performance of 2 closed-source LLMs and 20 open-source LLMs with different model sizes and architectures, highlighting the best-performing model for each benchmark. The table showcases the limitations of existing benchmarks, as LLMs can achieve high scores without needing visual input.", "section": "3 Two Overlooked Issues for Evaluating LVLMs"}, {"figure_path": "evP9mxNNxJ/tables/tables_24_1.jpg", "caption": "Table 2: Evaluation of various LVLMs on six popular multi-modal benchmarks. For the \"strategy\" column, \"LLM\" refers to evaluating using the corresponding LLM base of the LVLM, while \"LVLM-text\" denotes evaluating LVLMs without accessing images. We employ the 0-shot inference strategy for LLMs to align the evaluation protocols of LVLMs. We only report the results of 2 closed-source LVLMs and 8 open-source LVLMs due to space limits. For the entire LVLMs' results, please refer to the appendix. The highest results of the LVLM-text setting across the models are highlighted in bold and underlined.", "description": "This table presents the performance of various Large Vision-Language Models (LVLMs) on six popular multi-modal benchmarks.  It compares the performance of each LVLM using both its full capabilities (\"LVLM\") and only its underlying Large Language Model (\"LLM\") to assess the impact of visual information. The table highlights the best \"LVLM-text\" (no visual input) scores for each benchmark.", "section": "2 Related Work"}]