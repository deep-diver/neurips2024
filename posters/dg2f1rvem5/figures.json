[{"figure_path": "DG2f1rVEM5/figures/figures_2_1.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure showcases the model's ability to generate various 3D objects and avatars.  The top three rows demonstrate unconditional and class-conditional object generation, highlighting detailed textures and complex geometries. The fourth row shows image-conditioned digital avatar generation, demonstrating the model's capacity to create realistic avatars from single portraits.  The bottom row displays text-to-3D generation, illustrating how the model produces high-quality 3D models based on text descriptions.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_3_1.jpg", "caption": "Figure 2: Overall framework. Our framework comprises two main stages of representation construction and 3D diffusion. In the representation construction stage, given multi-view renderings of a 3D asset, we perform densification-constrained fitting to obtain 3D Gaussians with constant numbers. Subsequently, the Gaussians are structured into GaussianCube via Optimal Transport. In the 3D diffusion stage, our 3D diffusion model is trained to generate GaussianCube from Gaussian noise.", "description": "This figure illustrates the two-stage framework of the proposed method. The first stage is representation construction, which involves densification-constrained fitting of 3D Gaussians to a 3D asset from multi-view images using 3D Gaussian Splatting (3DGS) and then structuring these Gaussians into a GaussianCube using Optimal Transport. The second stage is 3D diffusion, where a 3D U-Net is used to generate a GaussianCube from noise, conditioned on class, image, or text information.", "section": "3 Method"}, {"figure_path": "DG2f1rVEM5/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of representation construction. First, we perform densification-constrained fitting to yield a fixed number of Gaussians, as shown in (a). We then employ Optimal Transport to organize the resultant Gaussians into a voxel grid. A 2D illustration of this process is presented in (b).", "description": "This figure illustrates the two main steps in creating the GaussianCube representation.  (a) shows the densification-constrained fitting process, where the algorithm aims to obtain a fixed number of Gaussians while maintaining high accuracy.  This involves identifying, sampling and adding Gaussians where needed (densification) and removing redundant ones (pruning). (b) demonstrates how the resulting Gaussians are organized into a structured grid using Optimal Transport, ensuring a spatially coherent arrangement for efficient 3D diffusion modeling.", "section": "3 Method"}, {"figure_path": "DG2f1rVEM5/figures/figures_5_1.jpg", "caption": "Figure 4: Qualitative results of object fitting.", "description": "This figure shows a qualitative comparison of the object fitting results obtained using different methods: Instant-NGP, Gaussian Splatting, Voxel, Triplane, and the proposed GaussianCube.  The results demonstrate the superior performance of GaussianCube in terms of detail preservation and overall accuracy compared to existing methods.", "section": "Experiments"}, {"figure_path": "DG2f1rVEM5/figures/figures_6_1.jpg", "caption": "Figure 5: Qualitative comparison of unconditional 3D generation on ShapeNet Car and Chair datasets. Our model is capable of generating results of complex geometry with rich details.", "description": "This figure compares the results of unconditional 3D generation of cars and chairs using four different methods: EG3D, GET3D, DiffTF, and the GaussianCube method proposed in the paper.  Each method's output is shown as a series of images depicting generated objects from different viewpoints. The GaussianCube method demonstrates the generation of objects with more complex geometry and finer details compared to the other methods.", "section": "4 Experiments"}, {"figure_path": "DG2f1rVEM5/figures/figures_6_2.jpg", "caption": "Figure 6: Qualitative comparison of class-conditioned 3D generation on large-vocabulary OmniObject3D [64]. Our model is able to handle diverse distribution with semantically accurate results.", "description": "This figure compares the results of class-conditioned 3D object generation using the proposed GaussianCube method against the DiffTF method on the OmniObject3D dataset.  It showcases the ability of GaussianCube to generate objects with more complex geometries and detailed textures compared to DiffTF, demonstrating its superior performance in handling diverse object categories.", "section": "Experiments"}, {"figure_path": "DG2f1rVEM5/figures/figures_7_1.jpg", "caption": "Figure 7: Qualitative comparison of 3D avatar creation conditioned on single frontal portraits.", "description": "The figure compares the quality of 3D avatar generation from single frontal portraits using three methods: a reference image, Rodin [59], and the proposed GaussianCube method.  The results show that GaussianCube produces avatars with higher fidelity and more detail, particularly in hair and accessories, compared to Rodin.  The reference image is provided for comparison to show the level of detail achievable.", "section": "4.2 Implementation Details"}, {"figure_path": "DG2f1rVEM5/figures/figures_8_1.jpg", "caption": "Figure 8: Qualitative comparison of text-to-3D generation on Objaverse [14]. Our model is able to generate high-quality samples according to the given text prompts.", "description": "This figure compares the text-to-3D generation results of different methods on the Objaverse dataset.  It showcases samples generated by DreamGaussian, VolumeDiffusion, Shape-E, LGM, and the authors' method (Ours). The comparison highlights the superior quality and fidelity of the 3D objects generated by the authors' approach, which closely match the given text descriptions.", "section": "Experiments"}, {"figure_path": "DG2f1rVEM5/figures/figures_9_1.jpg", "caption": "Figure 9: Qualitative ablation of representation fitting.", "description": "The figure shows a qualitative comparison of the object fitting results obtained using different representation construction methods. The methods compared are: (A) Voxel grid without offset; (B) Voxel grid with offset; (C) Our method without Optimal Transport; and (D) Our method (GaussianCube). The results demonstrate that the proposed GaussianCube method (D) significantly improves the fitting quality compared to the other methods.", "section": "3 Method"}, {"figure_path": "DG2f1rVEM5/figures/figures_18_1.jpg", "caption": "Figure 9: Qualitative ablation of representation fitting.", "description": "This figure shows the qualitative results of ablation studies on representation fitting.  It compares the results of several methods: a voxel grid without offsets, a voxel grid with offsets, the method without optimal transport (OT), and the full GaussianCube method (ours).  Each image shows the fitting result for a car model.  The figure visually demonstrates that the GaussianCube method (ours), which includes densification-constrained fitting and optimal transport, produces a significantly better fitting of the car model compared to other methods.", "section": "3.1 Representation Construction"}, {"figure_path": "DG2f1rVEM5/figures/figures_18_2.jpg", "caption": "Figure 12: Visualization of nearest neighbor search on ShapeNet Car and Chair.", "description": "This figure shows the top-3 nearest neighbors for several generated samples using CLIP similarity. This demonstrates the model's capability to generate novel objects with unique shapes and textures instead of simply memorizing training data.", "section": "Experiments"}, {"figure_path": "DG2f1rVEM5/figures/figures_18_3.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure shows examples of 3D objects generated by the GaussianCube model.  The top three rows demonstrate the model's ability to generate diverse objects with complex geometry and rich texture details from various conditions. The fourth row shows high-fidelity digital avatars generated from single portrait images. The fifth row showcases high-quality 3D assets generated from text prompts.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_18_4.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure showcases the versatility of the GaussianCube method in generating various 3D objects. The top three rows demonstrate the model's ability to create objects with intricate details and textures from different categories (cars, chairs, and various other objects). The fourth row highlights the method's capacity for high-fidelity digital avatar generation based on single input images. Finally, the bottom row showcases the successful generation of high-quality 3D models from text prompts, confirming the model's ability to handle text-to-3D generation tasks.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_19_1.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure showcases the capabilities of the GaussianCube model in generating high-quality 3D assets.  It presents examples of unconditional object generation, class-conditioned object generation, image-conditioned avatar generation, and text-to-3D generation. The results demonstrate the model's ability to create diverse objects with complex geometries and rich details.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_19_2.jpg", "caption": "Figure 7: Qualitative comparison of 3D avatar creation conditioned on single frontal portraits.", "description": "This figure compares the results of 3D avatar generation from a single frontal portrait image using three different methods: a reference image, the Rodin model, and the GaussianCube model proposed in the paper.  The comparison shows that GaussianCube produces avatars with better details, particularly in hair and clothing textures, compared to the Rodin model.  The GaussianCube method seems to generate more realistic results and preserve the identity better than Rodin.", "section": "4.2 Implementation Details"}, {"figure_path": "DG2f1rVEM5/figures/figures_20_1.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure showcases the capabilities of the GaussianCube-based diffusion model.  It presents several example outputs of the model conditioned on different inputs: unconditional generation (top three rows), image-conditioned avatar generation (fourth row), and text-conditioned 3D asset generation (fifth row). The figure highlights the model's ability to generate diverse 3D objects with fine details and realistic textures, demonstrating its high accuracy and versatility.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_20_2.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure shows examples of 3D objects generated by the proposed GaussianCube method.  The top three rows demonstrate the model's ability to generate diverse objects with complex geometry and rich textures, based on different conditions (unconditional, text-to-3D, image-conditioned). The fourth row showcases the generation of high-fidelity digital avatars, conditioned on single portrait images.  The bottom row illustrates the model's success in creating high-quality 3D assets from text prompts.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_20_3.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure showcases the versatility of the GaussianCube model in generating various 3D objects. The top three rows demonstrate the generation of objects with complex geometries and rich details.  The fourth row shows the creation of high-fidelity digital avatars conditioned on portrait images. Finally, the last row highlights the model's capability in producing high-quality 3D assets from text prompts.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_20_4.jpg", "caption": "Figure 20: Example of text-guided 3D editing.", "description": "This figure demonstrates the text-guided 3D editing capabilities of the proposed GaussianCube model.  Starting with a source object (a red pickup truck), the model successfully modifies the object's attributes based on text prompts, generating variations such as a green pickup truck, a burnt and rusted pickup truck, and a pickup truck with a colorful paint job. This showcases the model's ability to not only generate new 3D objects, but also to precisely control and manipulate existing ones through text-based instructions.", "section": "4 Experiments"}, {"figure_path": "DG2f1rVEM5/figures/figures_21_1.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure showcases the results of the GaussianCube model on various 3D generation tasks.  The top three rows demonstrate unconditional and text-to-3D object generation, highlighting the model's ability to create diverse objects with intricate details and realistic textures. The fourth row displays its capability for high-fidelity digital avatar generation conditioned on single portrait images. Finally, the bottom row showcases the creation of high-quality 3D assets from text prompts, underlining the model's versatility in handling different types of input and generating diverse outputs.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_22_1.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure showcases the model's ability to generate a wide variety of 3D objects with high fidelity. The top three rows show examples of diverse objects with complex geometries and rich textures, generated unconditionally (ShapeNet Car, ShapeNet Chair, OmniObject3D) or conditioned on text prompts. The fourth row displays high-fidelity digital avatars generated from single portrait images, demonstrating the model's ability to perform image-to-3D translation.  The final row showcases text-to-3D generation, where high-quality 3D assets are generated based solely on text descriptions. This demonstrates the model's versatility and capability in generating detailed and realistic 3D content across different modalities.", "section": "1 Introduction"}, {"figure_path": "DG2f1rVEM5/figures/figures_23_1.jpg", "caption": "Figure 1: Our diffusion model is able to create diverse objects with complex geometry and rich texture details (top three rows). Our method also supports creating high-fidelity digital avatars (the forth row) conditioned on single portrait images (visualized in dashed boxes) and high-quality 3D assets given text prompts (the fifth row).", "description": "This figure showcases various examples of 3D objects generated using the GaussianCube method.  The top three rows demonstrate the model's ability to generate diverse objects with intricate details and textures. The fourth row shows high-fidelity digital avatars generated from single portrait images. The bottom row displays high-quality 3D models generated from text prompts.", "section": "1 Introduction"}]