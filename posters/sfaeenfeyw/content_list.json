[{"type": "text", "text": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shuai Li Zhao Song Shanghai Jiao Tong University Simons Institute for the Theory of Computing, UC Berkeley shuaili8@sjtu.edu.cn magic.linuxkde@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Yu Xia Tong Yu Tianyi Zhou University of California, San Diego Adobe Research University of Southern California yux078@ucsd.edu tyu@adobe.com tzhou029@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) are known for their exceptional performance in natural language processing, making them highly effective in many human liferelated tasks. The attention mechanism in the Transformer architecture is a critical component of LLMs, as it allows the model to selectively focus on specific input parts. The softmax unit, which is a key part of the attention mechanism, normalizes the attention scores. Hence, the performance of LLMs in various NLP tasks depends significantly on the crucial role played by the attention mechanism with the softmax unit. ", "page_idx": 0}, {"type": "text", "text": "In-context learning is one of the celebrated abilities of recent LLMs. Without further parameter updates, Transformers can learn to predict based on few in-context examples. However, the reason why Transformers becomes in-context learners is not well understood. Recently, in-context learning has been studied from a mathematical perspective with simplified linear self-attention without softmax unit. Based on a linear regression formulation $\\operatorname*{min}_{x}\\|A x-b\\|_{2}$ , existing works show linear Transformers\u2019 capability of learning linear functions in context. The capability of Transformers with softmax unit approaching full Transformers, however, remains unexplored. ", "page_idx": 0}, {"type": "text", "text": "In this work, we study the in-context learning based on a softmax regression formulation $\\begin{array}{r}{\\operatorname*{min}_{x}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)\\stackrel{\\circ}-b\\|_{2}}\\end{array}$ . We show the upper bounds of the data transformations induced by a single self-attention layer with softmax unit and by gradient-descent on a $\\ell_{2}$ regression loss for softmax prediction function. Our theoretical results imply that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, there has been a significant increase in research and development in the field of Artificial Intelligence, with large language models (LLMs) emerging as an effective way to tackle complex tasks. Transformers have achieved state-of-the-art results in various NLP tasks, such as machine translation [PCR19, $\\mathrm{GHG^{+}20]}$ and text generation $[\\mathrm{LSX}^{+}22]$ . As a result, they have become the preferred architecture for NLP, where BERT [DCLT18], GPT-3 $[{\\mathbf{B}}{\\mathbf{M}}{\\mathbf{R}}^{+}20]$ , PaLM $[\\mathsf{C N D}^{+}22]$ were proposed. They have demonstrated remarkable learning and reasoning capabilities and have proven to be more efficient than traditional models when processing natural language. ", "page_idx": 0}, {"type": "text", "text": "Additionally, LLMs can be fine-tuned for multiple purposes without requiring a new build from scratch, making them a versatile tool for AI applications. Moreover, recent studies on the in-context learning abilities of LLMs have demonstrated that even without further fine-tuning, LLMs can generalize to new tasks with only a few demonstration examples in the prompt. To understand how LLMs become in-context learners, recent works have studied the problem and provided mathematical explanations from the Transformer architecture perspective, showing a simplified linear self-attention layer of Transformer can learn linear functions similarly as a step of gradient descent $[{\\mathrm{ONR}}^{+}22$ , $\\mathrm{ASA}^{+}22$ , GTLV22, $\\mathrm{CLL}^{+}24]$ . While such linear approximation of full Transformers is overly simplistic, studies on more complex Transformer architecture are needed to further explain the in-context learning phenomenon. ", "page_idx": 1}, {"type": "text", "text": "Transformers have a specific type of sequence-to-sequence neural network architecture. They utilize the attention mechanism $[\\mathrm{VSP}^{\\bar{+}}17\\$ , $\\mathbf{R}\\mathbf{N}\\bar{\\mathbf{S}}^{+}18$ , DCLT18, $\\mathrm{BMR}^{+}20]$ that allows them to capture longrange dependencies and context from input data effectively. The core of the attention mechanism is the attention matrix which is comprised of rows and columns, corresponding to individual words or \u201ctokens\u201d. The attention matrix represents the relationships within the given text. It measures the importance of each token in a sequence as it relates to the desired output. During the training process, the attention matrix is learned and optimized to improve the accuracy of the model\u2019s predictions. Through the attention mechanism, each input token is evaluated based on its relevance to the desired output by assigning a token score. This score is determined by a similarity function that compares the current output state with input states. ", "page_idx": 1}, {"type": "text", "text": "Theoretically, the attention matrix is comprised of the query matrix $Q\\in\\mathbb{R}^{n\\times d}$ , the key matrix $K\\in$ $\\mathbb{R}^{n\\times d}$ and the value matrix $V\\in\\mathbb{R}^{n\\times d}$ . Following [ZHDK23, AS23, BSZ24, AS24b, AS24c, AS24a], the computation of the normalized attention function is defined as $D^{-1}\\exp(Q K^{\\top})V$ . Following the transformer literature, we apply exp to a matrix entry-wise way. Here $D\\in\\mathbb{R}^{n\\times n}$ is diagonal matrix that defined as $D=\\mathrm{diag}(\\bar{\\exp}(Q\\bar{K}^{\\top}){\\bf1}_{n})$ . Intuitively, $D$ denotes the softmax normalization matrix. A more general computation formulation can be written as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\underbrace{D^{-1}}_{n\\times n{\\mathrm{~diagonal~matrix}}}\\underbrace{\\mathrm{exp}(X Q K^{\\top}X^{\\top})}_{n\\times n}\\underbrace{X}_{n\\times d}\\underbrace{V}_{d\\times d},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where ", "page_idx": 1}, {"type": "equation", "text": "$$\nD:=\\operatorname{diag}(\\exp(X Q K^{\\top}X^{\\top})\\mathbf{1}_{n}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In the above setting, we treat $Q,K,V\\,\\in\\,\\mathbb{R}^{d\\times d}$ as weights and $X$ is the input sentence data that has length $n$ and each word embedding size is $d$ . In the remaining of the part, we will switch $X$ to notation $A$ and use $A$ to denote sentence. Mathematically, the attention computation problem can be formulated as a regression problem in the following sense ", "page_idx": 1}, {"type": "text", "text": "Definition 1.1. We consider the following problem ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{X\\in\\mathbb{R}^{d\\times d}}\\|D^{-1}\\exp(A X A^{\\top})-B\\|_{F}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $A\\in\\mathbb{R}^{n\\times d}$ can be treated as a length- $n$ document and each word has length- $d$ embedding size. Here $D=\\operatorname{diag}(A X A^{\\top}\\mathbf{1}_{n})$ . For any given $A\\in\\mathbb{R}^{n\\times d}$ and $B\\in\\mathbb{R}^{n\\times n}$ , the goal is to find some weight $X$ to optimize the above objective function. ", "page_idx": 1}, {"type": "text", "text": "In contrast to the formulation in [ZHDK23, AS23, BSZ24], the parameter $X$ in Definition 1.1 is equivalent to the $Q K^{\\top}\\,\\in\\,\\mathbb{R}^{d\\times\\bar{d}}$ in the generalized version of [ZHDK23, AS23, BSZ24] (e.g. replacing $Q\\in\\mathbb{R}^{n\\times d}$ by $X Q$ where $X\\in\\,\\breve{\\mathbb{R}}^{n\\times d}$ and $Q\\in\\mathbb{R}^{d\\times d}$ . Similarly for $K$ and $V$ . In such scenario, $X$ can be viewed as a matrix representation of a length- ${\\cdot n}$ sentence.). ", "page_idx": 1}, {"type": "text", "text": "A number of work $[\\mathrm{ASA}^{+}22$ , GTLV22, $\\mathrm{ONR}^{+}22]$ ] study the in-context learning from mathematical perspective in a much simplified setting than Definition 1.1, which is linear regression formulation as in Definition 1.2. They show linear Transformer without softmax unit in its attention layer can mimic the ability of gradient descent in learning linear functions in context. While the softmax unit plays an important role in attention computations of full Transformers, their simplification of the softmax unit leaves a gap in explaining LLMs\u2019 in-context learning abilities. ", "page_idx": 1}, {"type": "text", "text": "Definition 1.2. Given a matrix $A\\in\\mathbb{R}^{n\\times d}$ and $b\\in\\mathbb{R}^{n}$ , the goal is to solve ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\|A x-b\\|_{2}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Several theoretical transformer work have studied either exponential regression [GMS23, LSZ23] or softmax regression problem [DLS23, LLSS24a]. In this work, to take a step forward to understand the softmax unit in the attention scheme in LLMs. We consider the following softmax regression and study the in-context learning phenomena and its closeness to gradient descent from the data transformation perspective. ", "page_idx": 2}, {"type": "text", "text": "Definition 1.3 (Softmax Regression). Given a $A\\in\\mathbb{R}^{n\\times d}$ and a vector $b\\in\\mathbb{R}^{n}$ , the goal is to solve ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d}}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-b\\|_{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We remark that the Definition 1.3 of Softmax Regression is a formulation in between Definition 1.2 and Definition 1.1. ", "page_idx": 2}, {"type": "text", "text": "We state our major result as follows: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.4 (Bounded shift for in-context learning, informal version of the combination of Theorem 4.2 and Theorem 4.3). If the following conditions hold: Let $A~\\in~\\mathbb{R}^{n\\times d}$ . Let $b\\ \\in\\ \\mathbb{R}^{n}$ . $\\|A\\|\\leq R.$ $\\leq R.\\ L e t\\|x\\|_{2}\\leq R.\\ \\|A(x_{t+1}-x_{t})\\|_{\\infty}<0.01.\\ \\|(A_{t+1}-A_{t})x\\|_{\\infty}<0.01$ . Let $R\\geq4$ . Let $M:=n^{1.5}\\exp(10R^{2})$ . We consider the softmax regression (Definition 1.3) problem ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-b\\|_{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "\u2022 Part 1. If we move the $x_{t}$ to $x_{t+1}$ , then we\u2019re solving a new softmax regression with $\\begin{array}{r}{\\operatorname*{min}_{x}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-\\widetilde{b}\\|_{2}}\\end{array}$ where $\\|\\widetilde{b}-b\\|_{2}\\leq M\\cdot\\|x_{t+1}-x_{t}\\|_{2}$ \u2022 Part 2. If we move the $A_{t}$ to $A_{t+1}$ , then we\u2019re solving a new softmax regression with $\\begin{array}{r}{\\operatorname*{min}_{x}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-\\widehat{b}\\|_{2}}\\end{array}$ where $\\|\\widehat{b}-b\\|_{2}\\leq M\\cdot\\|A_{t+1}-A_{t}\\|$ ", "page_idx": 2}, {"type": "text", "text": "Recall that $A\\in\\mathbb{R}^{n\\times d}$ denotes a length- $\\cdot n$ document and each word has the length- $d$ embedding size and $x$ denotes the simplified version of $Q K^{\\top}$ . One-step gradient descent can be treated as an update to the model\u2019s weight $x$ . Thus, part 1 of our result (Theorem 1.4) implies that the data transformation of $b$ induced by gradient-descent on the $\\ell_{2}$ regression loss is bounded by $M\\cdot\\|x_{t+1}-x_{t}\\|_{2}$ . ", "page_idx": 2}, {"type": "text", "text": "According to $[{\\mathrm{ONR}}^{+}22]$ , to do in-context learning, a self-attention layer update can be treated as an update to the tokenized document $A$ . For detailed derivation, please refer to $[{\\mathrm{ONR}}^{+}22]$ . Thus, part 2 of our result (Theorem 1.4) implies that the data transformation of $b$ induced by a single self-attention layer is bounded by $M\\cdot\\|A_{t+1}-A_{t}\\|$ . ", "page_idx": 2}, {"type": "text", "text": "We remark that the data transformation of $b$ induced by 1) a single self-attention layer and by 2) gradient-descent on the $\\ell_{2}$ regression loss are both bounded. The bounded transformation of $b$ implies that when training self-attention-only Transformers for fundamental regression tasks, the models learned by gradient-descent and Transformers show great similarity. ", "page_idx": 2}, {"type": "text", "text": "Roadmap. In Section 2, we give some preliminaries. In Section 3, we compute the gradient of the loss function with softmax function with respect to $x$ . Those functions include $\\alpha(\\overline{{x}})^{-1}$ , $\\alpha(x)$ and $f(x)$ . In Section 4, we give our formal theoretical results, validated by numerical experiments presented in Section 5. In Section 6, we conclude our paper. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Section 2.1, we introduce the notations used in this paper. In Section 2.2, we give some facts about the basic algebra. In Section 2.3, we propose the lower bound on $\\langle\\exp(A x),{\\mathbf{1}}_{n}^{{\\phantom{\\dagger}}}\\rangle$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Notations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For a positive integer $n$ , we use $[n]$ to denote $\\{1,2,\\cdots\\,,n\\}$ , for any positive integer $n$ . We use $\\mathbb{E}[\\cdot]$ to denote expectation. We use $\\mathrm{Pr}[\\cdot]$ to denote probability. We use ${\\bf1}_{n}$ to denote the vector where all entries are one. We use ${\\bf0}_{0}$ to denote the vector where all entries are zero. The identity matrix of size $n\\times n$ is represented by $I_{n}$ for a positive integer $n$ . The symbol $\\mathbb{R}$ refers to real numbers and $\\mathbb{R}_{\\geq0}$ represents non-negative real numbers. For any vector $x\\in\\mathbb{R}^{n}$ , $\\exp(x)\\in\\mathbb{R}^{n}$ denotes a vector where the $i$ -th entry is $\\exp(x_{i})$ and $\\|{\\boldsymbol{x}}\\|_{2}$ represents its $\\ell_{2}$ norm, that is, $\\begin{array}{r}{\\|x\\|_{2}:=(\\sum_{i=1}^{n}x_{i}^{2})^{1/2}}\\end{array}$ . We use $\\|x\\|_{\\infty}$ to denote $\\operatorname*{max}_{i\\in[n]}|x_{i}|$ . For any vector $x\\in\\mathbb{R}^{n}$ and vector $y\\in\\mathbb{R}^{d}$ , we use $\\langle x,y\\rangle$ to denote the inner product of vector $x$ and $y$ . The notation $B_{i}$ is used to indicate the $i$ -th row of matrix $B$ . If $a$ and $b$ are two column vectors in $\\mathbb{R}^{n}$ , then $a\\circ b$ denotes a column vector where $(a\\circ b)_{i}=a_{i}b_{i}$ . For a square and full rank matrix $B$ , we use $B^{-1}$ to denote the true inverse of $B$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "2.2 Basic Algebras ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Fact 2.1. For vectors $\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$ , we have $\\begin{array}{r l}&{\\bullet\\ \\|x\\circ y\\|_{2}\\leq\\|x\\|_{\\infty}\\cdot\\|y\\|_{2}}\\\\ &{\\bullet\\ \\|x\\|_{\\infty}\\leq\\|x\\|_{2}\\leq\\sqrt{n}\\|x\\|_{\\infty}}\\\\ &{\\bullet\\ \\|\\exp(x)\\|_{\\infty}\\leq\\exp(\\|x\\|_{2})}\\end{array}$ \u2022 For any $\\|x-y\\|_{\\infty}\\leq0.01$ , we have $\\|\\exp(x)-\\exp(y)\\|_{2}\\leq\\|\\exp(x)\\|_{2}\\cdot2\\|x-y\\|_{\\infty}$ ", "page_idx": 3}, {"type": "text", "text": "Fact 2.2. For matrices $X,Y$ , we have ", "page_idx": 3}, {"type": "text", "text": "${\\begin{array}{r l}&{\\bullet\\ \\|X^{\\top}\\|=\\|X\\|}\\\\ &{\\bullet\\ \\|X\\|\\geq\\|Y\\|-\\|X-Y\\|}\\\\ &{\\bullet\\ \\|X+Y\\|\\leq\\|X\\|+\\|Y\\|}\\\\ &{\\bullet\\ \\|X\\cdot Y\\|\\leq\\|X\\|\\cdot\\|Y\\|}\\end{array}}$ \u2022 If $X\\preceq\\alpha\\cdot Y$ , then $\\|X\\|\\leq\\alpha\\cdot\\|Y\\|$ ", "page_idx": 3}, {"type": "text", "text": "2.3 Lower bound on $\\beta$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Lemma 2.3. If the following conditions holds ", "page_idx": 3}, {"type": "text", "text": "$\\begin{array}{l}{\\bullet\\ \\|A\\|\\leq R}\\\\ {\\bullet\\ \\|x\\|_{2}\\leq R}\\end{array}$ \u2022 Let $\\beta$ be lower bound on $\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle$ ", "page_idx": 3}, {"type": "text", "text": "Then we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta\\geq\\exp(-R^{2})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Proof. We have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{\\displaystyle\\left\\langle\\exp(A x),\\mathbf{1}_{n}\\right\\rangle=\\sum_{i=1}^{n}\\exp((A x)_{i})}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\geq\\operatorname*{min}_{i\\in[n]}\\exp((A x)_{i})}\\\\ {\\displaystyle}\\\\ {\\geq\\operatorname*{min}_{i\\in[n]}\\exp(-|(A x)_{i}|)}\\\\ {\\displaystyle}\\\\ {\\displaystyle}&{=\\exp(-\\operatorname*{max}_{i\\in[n]}|(A x)_{i}|)}\\\\ {\\displaystyle}&{=\\exp(-\\|A x\\|_{\\infty})}\\\\ {\\displaystyle}&{\\geq\\exp(-\\|A x\\|_{2})}\\\\ {\\displaystyle}&{\\geq\\exp(-R^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "the 1st step follows from simple algebra, the 2nd step comes from simple algebra, the 3rd step follows from the fact that $\\exp(x)\\geq\\exp(-|x|)$ , the 4th step follows from the fact that $\\exp(-x)$ is monotonically decreasing, the 5th step comes from definition of $\\ell_{\\infty}$ norm, the 6th step follows from Fact 2.1, the 7th step follows from the assumption on $A$ and $x$ . \u53e3 ", "page_idx": 3}, {"type": "text", "text": "3 Softmax Function with Respect to $x$ ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Section 3.1, we give the definitions used in the computation. In Section 3.2, we compute the gradient of the loss function with softmax function with respect to $x$ . Those functions includes $\\bar{\\alpha}(x)^{-1},\\alpha(x)$ and $f(x)$ . ", "page_idx": 4}, {"type": "text", "text": "3.1 Definitions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We define function softmax $f$ as follows ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 (Function $f$ , Definition 5.1 in [DLS23]). Given a matrix $A\\in\\mathbb{R}^{n\\times d}$ . Let ${\\mathbf{1}}_{n}$ denote $a$ length- $n$ vector that all entries are ones. We define prediction function $f:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}^{n}}$ as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(x):=\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\cdot\\exp(A x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Definition 3.2 (Loss function $L_{\\mathrm{exp}}$ , Definition 5.3 in [DLS23]). Given a matrix $A\\in\\mathbb{R}^{n\\times d}$ and $a$ vector $b\\in\\mathbb{R}^{n}$ . We define loss function $L_{\\mathrm{exp}}:\\mathbb R^{d}\\to\\mathbb R$ as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{exp}}(x):=0.5\\cdot\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-b\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For convenient, we define two helpful notations $\\alpha$ and $c$ ", "page_idx": 4}, {"type": "text", "text": "Definition 3.3 (Normalized coefficients, Definition 5.4 in [DLS23]). We define $\\alpha:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha(x):=\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, we can rewrite $f(x)$ (see Definition 3.1) and $L_{\\exp}(x)$ (see Definition 3.2) as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(x)=\\alpha(x)^{-1}\\cdot\\exp(A x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{exp}}(x)=0.5\\cdot\\|f(x)-b\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Definition 3.4 (Definition 5.5 in [DLS23]). We define function $c:\\mathbb{R}^{d}\\in\\mathbb{R}^{n}$ as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\nc(x):=f(x)-b.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then we can rewrite $L_{\\exp}(x)$ (see Definition 3.2) as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{\\mathrm{exp}}(x)=0.5\\cdot\\|c(x)\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2 Gradient Computations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We state a lemma from previous work, ", "page_idx": 4}, {"type": "text", "text": "Lemma 3.5 (Gradient, Lemma 5.6 in [DLS23]). If the following conditions hold ", "page_idx": 4}, {"type": "text", "text": "\u2022 Given matrix $A\\in\\mathbb{R}^{n\\times d}$ and a vector $b\\in\\mathbb{R}^{n}$ \u2022 Let $\\alpha(x)$ be defined in Definition 3.3.   \n\u2022 Let $f(x)$ be defined in Definition 3.1.   \n\u2022 Let $c(x)$ be defined in Definition 3.4.   \n\u2022 Let $L_{\\exp}(x)$ be defined in Definition 3.2. ", "page_idx": 4}, {"type": "text", "text": "For each $i\\in[d]$ , we have ", "page_idx": 4}, {"type": "text", "text": "\u2022 Part 1. ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{\\operatorname{d}\\exp(A x)}{\\operatorname{d}x_{i}}}=\\exp(A x)\\circ A_{*,i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "\u2022 Part 2. ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\frac{\\operatorname{d}\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle}{\\operatorname{d}x_{i}}}=\\langle\\exp(A x),A_{*,i}\\rangle\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "\u2022 Part 3. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\alpha(x)^{-1}}{\\mathrm{d}x_{i}}=-\\alpha(x)^{-1}\\cdot\\langle f(x),A_{*,i}\\rangle\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "\u2022 Part 4. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}f(x)}{\\mathrm{d}x_{i}}=\\frac{\\mathrm{d}c(x)}{\\mathrm{d}x_{i}}=\\,-\\left\\langle f(x),A_{*,i}\\right\\rangle\\cdot f(x)\\,+\\,f(x)\\circ A_{*,i}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "\u2022 Part 5. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}L_{\\mathrm{exp}}(x)}{\\mathrm{d}x_{i}}=\\underbrace{A_{*,i}^{\\top}}_{1\\times n}\\cdot\\Big(\\underbrace{f(x)}_{n\\times1}\\underbrace{\\langle c(x),f(x)\\rangle}_{\\mathrm{scalar}}+\\underbrace{\\mathrm{diag}(f(x))}_{n\\times n}\\underbrace{c(x)}_{n\\times1}\\Big)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4 Main Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In Section 4.1, we show the lipschitz bound of function $f$ . In Section 4.2, we show our upper bound result of $\\delta_{b}$ with respect to $x$ . In Section 4.3, we show our upper bound result of $\\delta_{b}$ with respect to $A$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 Lipschitz Bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To bound the shift of $b$ , we first show the Lipschitz property for the basic functions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\ \\|\\exp(A x)-\\exp(A y)\\|_{2}\\leq2{\\sqrt{n}}R\\exp(R^{2})\\cdot\\|x-y\\|_{2}}\\\\ &{\\bullet\\ |\\alpha(x)-\\alpha(y)|\\leq\\|\\exp(A x)-\\exp(A y)\\|_{2}\\cdot\\sqrt{n}}\\\\ &{\\bullet\\ |\\alpha(x)^{-1}-\\alpha(y)^{-1}|\\leq\\beta^{-2}\\cdot|\\alpha(x)-\\alpha(y)|}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We can show that ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.1. If the following conditions hold ", "page_idx": 5}, {"type": "text", "text": "$:L e t\\,\\beta\\in(0,1).$ .   \n\u2022 Let $\\delta_{b,1}\\in\\mathbb{R}^{n}$ be defined as Definition B.3. \u2022 Let $\\delta_{b,2}\\in\\mathbb{R}^{n}$ be defined as Definition B.3. $\\begin{array}{l}{{\\bullet\\,\\,L e t\\,\\delta_{b}=\\delta_{b,1}+\\delta_{b,2}.}}\\\\ {{\\bullet\\,\\,L e t\\,R\\geq4.}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "We have \u2022 Part 1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\delta_{b,1}\\|_{2}\\leq2\\beta^{-2}n^{1.5}\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "\u2022 Part 2. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\delta_{b,2}\\|_{2}\\leq2\\beta^{-1}\\sqrt{n}R\\exp(R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "\u2022 Part 3. ", "page_idx": 5}, {"type": "equation", "text": "$$\n||\\underbrace{f(x_{t+1})-f(x_{t})}_{\\delta_{b}}||_{2}\\leq4\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot||x_{t+1}-x_{t}||_{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. Proof of Part 1. We have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\delta_{b,1}\\|_{2}\\leq|\\alpha(x_{t+1})^{-1}-\\alpha(x_{t})^{-1}|\\cdot\\|\\exp(A x_{t+1})\\|_{2}}\\\\ &{\\qquad\\leq|\\alpha(x_{t+1})^{-1}-\\alpha(x_{t})^{-1}|\\cdot\\sqrt{n}\\cdot\\exp(R^{2})}\\\\ &{\\qquad\\leq\\beta^{-2}\\cdot|\\alpha(x_{t+1})-\\alpha(x_{t})|\\cdot\\sqrt{n}\\cdot\\exp(R^{2})}\\\\ &{\\qquad\\leq\\beta^{-2}\\cdot\\sqrt{n}\\cdot\\|\\exp(A x_{t+1})-\\exp(A x_{t})\\|_{2}\\cdot\\sqrt{n}\\cdot\\exp(R^{2})}\\\\ &{\\qquad\\leq\\beta^{-2}\\cdot\\sqrt{n}\\cdot2\\sqrt{n}R\\exp(R^{2})\\|x_{t+1}-x_{t}\\|_{2}\\cdot\\sqrt{n}\\cdot\\exp(R^{2})}\\\\ &{\\qquad=2\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the first step follows from definition, the second step follows from assumption on $A$ and $x$ , the third step follows Lemma B.7, the forth step follows from Lemma B.6, the fifth step follows from Lemma B.5. ", "page_idx": 6}, {"type": "text", "text": "Proof of Part 2. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\delta_{b,2}\\|_{2}\\leq|\\alpha(x_{t+1})^{-1}|\\cdot\\|\\exp(A x_{t+1})-\\exp(A x_{t})\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\beta^{-1}\\cdot\\|\\exp(A x_{t+1})-\\exp(A x_{t})\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\beta^{-1}\\cdot2\\sqrt{n}R\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the first step follows from definition, the 2nd step comes from Lemma B.5. ", "page_idx": 6}, {"type": "text", "text": "Proof of Part 3. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\delta_{b}\\|_{2}=\\|\\delta_{b,1}+\\delta_{b,2}\\|_{2}}\\\\ &{\\qquad\\leq\\|\\delta_{b,1}\\|_{2}+\\|\\delta_{b,2}\\|_{2}}\\\\ &{\\qquad\\leq2\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}+2\\beta^{-1}n^{0.5}R\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\\\ &{\\qquad\\leq2\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}+2\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\\\ &{\\qquad\\leq4\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the 1st step follows from the definition of $\\delta_{b}$ , the 2nd step follows from triangle inequality, the 3rd step follows from the results in Part 1 and Part 2, the 4th step follows from the fact that $n\\geq1$ and $\\beta^{-1}\\geq1$ , the 5th step follows from simple algebra. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Similarly, we can show the Lipschitz property of function $f$ with respect to $A$ as the following ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\lVert f(A_{t+1})-f(A_{t})\\rVert_{2}}\\\\ {\\leq4\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\lVert A_{t+1}-A_{t}\\rVert_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Due to space limitation, we defer formal lemma and proof to D.2. ", "page_idx": 6}, {"type": "text", "text": "4.2 Shifting Weight Parameter $x$ ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Theorem 4.2 (Bounded shift for shifting the weight parameter, formal of Theorem 1.4). If the following conditions hold ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;L e t\\;A\\in\\mathbb{R}^{n\\times d}}\\\\ &{\\bullet\\;\\|A\\|\\leq R}\\\\ &{\\bullet\\;\\|A(x_{t+1}-x_{t})\\|_{\\infty}<0.01}\\\\ &{\\bullet\\;L e t\\;R\\geq4}\\\\ &{\\bullet\\;L e t\\;M:=n^{1.5}\\exp(10R^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We consider the softmax regression problem ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-b\\|_{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "If we move the $x_{t}$ to $x_{t+1}$ , then we\u2019re solving a new softmax regression problem with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-\\widetilde{b}\\|_{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\widetilde{\\boldsymbol{b}}-\\boldsymbol{b}\\|_{2}\\leq M\\cdot\\|\\boldsymbol{x}_{t+1}-\\boldsymbol{x}_{t}\\|_{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof. We have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widetilde b-b\\|_{2}\\le4\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\\\ &{\\qquad\\qquad\\le4n^{1.5}R\\exp(2R^{2})\\exp(2R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\\\ &{\\qquad\\quad\\le n^{1.5}(4R)\\exp(4R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\\\ &{\\qquad\\quad\\le n^{1.5}\\exp(6R^{2})\\exp(4R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\\\ &{\\qquad\\quad\\le n^{1.5}\\exp(10R^{2})\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\\\ &{\\qquad\\quad\\le M\\cdot\\|x_{t+1}-x_{t}\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the 1st step follows from Lemma 4.1, the 2nd step comes from Lemma 2.3, the 3rd step comes from simple algebra, the 4th step follows from simple algebra, the 5th step follows from simple algebra and the 6th step follows from the definition of $M$ . \u53e3 ", "page_idx": 7}, {"type": "text", "text": "4.3 Shifting Sentence Data $A$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Theorem 4.3 (Bounded shift for in-context learning, formal of Theorem 1.4). If the following conditions hold ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;L e t\\;A\\in\\mathbb{R}^{n\\times d}}\\\\ &{\\bullet\\;\\|A\\|\\leq R}\\\\ &{\\bullet\\;\\|(A_{t+1}-A_{t})x\\|_{\\infty}<0.01}\\\\ &{\\bullet\\;L e t\\;R\\geq4}\\\\ &{\\bullet\\;L e t\\;M:=n^{1.5}\\exp(10R^{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We consider the softmax regression problem ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-b\\|_{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "If we move the $A_{t}$ to $A_{t+1}$ then we\u2019re solving a new softmax regression problem with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x}\\|\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-\\widetilde{b}\\|_{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\widetilde{b}-b\\|_{2}\\le M\\cdot\\|A_{t+1}-A_{t}\\|.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof. We have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widetilde b-b\\|_{2}\\le4\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\le4n^{1.5}R\\exp(2R^{2})\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\\\ &{\\quad\\quad\\quad\\le n^{1.5}(4R)\\exp(4R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\\\ &{\\quad\\quad\\quad\\le n^{1.5}\\exp(6R^{2})\\exp(4R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\\\ &{\\quad\\quad\\quad\\le n^{1.5}\\exp(10R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\\\ &{\\quad\\quad\\quad\\le M\\cdot\\|A_{t+1}-A_{t}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where the 1st step follows from Lemma D.5, the 2nd step follows from Lemma 2.3, the 3rd step follows from simple algebra, the 4th step comes from simple algebra, the 5th step comes from simple algebra and the 6th step follows from the definition of $M$ . \u53e3 ", "page_idx": 7}, {"type": "text", "text": "5 Numerical Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we present our numerical experiments to validate our theoretical results that when training self-attention-only Transformers for softmax regression tasks, the models learned by gradientdescent and Transformers show great similarity. ", "page_idx": 8}, {"type": "text", "text": "5.1 Experiments Setup ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "According to Definition 1.3, we construct the synthetic softmax regression tasks consists of randomly sampled length- ${\\mathbf{\\nabla}}n$ documents $A\\in\\mathbb{R}^{n\\times d}$ where each word has the $d$ -dimensional embedding and targets $b\\in\\mathbb{R}^{n}$ . Each document is generated from a unique random seed. In our experiments, we choose a set of different document length $n$ and a set of different embedding size $d$ . ", "page_idx": 8}, {"type": "text", "text": "Following $[{\\mathrm{ONR}}^{+}22]$ , we compare the following two models in our experiment ", "page_idx": 8}, {"type": "text", "text": "\u2022 a trained single self-attention (SA) layer with softmax unit approximating full Transformers.   \n\u2022 a softmax regression model trained with one-step gradient descent (GD). ", "page_idx": 8}, {"type": "text", "text": "The training objective for both models is defined as in Definition 1.3. For the single self-attention layer with a softmax unit, we choose the learning rate $\\eta_{\\mathrm{SA}}\\,=\\,0.005$ . For the softmax regression model, we determine the optimal learning rate $\\eta_{\\mathrm{GD}}$ by minimizing the $\\ell_{2}$ regression loss over a training set of $10^{3}$ tasks through line search. ", "page_idx": 8}, {"type": "text", "text": "To compare the trained single self-attention layer with a softmax unit and the softmax regression model trained with one-step gradient descent, we sample $10^{3}$ tasks and record the losses of two models. In addition, we follow $[{\\mathrm{ONR}}^{+}22]$ to record ", "page_idx": 8}, {"type": "text", "text": "\u2022 Pred Diff: the predictions difference measured with the $\\ell_{2}$ norm: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|\\widehat{y}_{\\mathrm{SA}}(A)-\\widehat{y}_{\\mathrm{GD}}(x)\\|_{2}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\widehat{y}_{\\mathrm{SA}}(A)$ corresponds to the $\\widetilde{b}$ in Theorem 4.2, and $\\widehat{y}_{\\mathrm{GD}}(x)$ corresponds to the $\\widetilde{b}$ in Theorem 4.3. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Model Cos: the cosine similarity between the sensitivities of two models: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\cos\\mathrm{Sim}\\big(\\frac{\\partial\\widehat{y}_{\\mathrm{GD}}(x)}{\\partial x},\\frac{\\partial\\widehat{y}_{\\mathrm{SA}}(A)}{\\partial A}\\big)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "\u2022 Model Diff: the model sensitivity difference measured with the $\\ell_{2}$ norm: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|\\frac{\\partial\\widehat{y}_{\\mathrm{GD}}(x)}{\\partial x}-\\frac{\\partial\\widehat{y}_{\\mathrm{SA}}(A)}{\\partial A}\\|_{2}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "All experiments run on a single NVIDIA RTX2080Ti GPU with 10 independent repetitions. ", "page_idx": 8}, {"type": "text", "text": "5.2 Different Document Lengths ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "For synthetic softmax regression tasks of document length $n\\in\\{200,1000\\}$ and word embedding size $d=20$ , the comparison results between a trained single self-attention layer and one-step gradient descent are shown in Figure 1 and Figure 2. Due to space limitation, we present more results with different document length $n\\in\\{25,50,100,200,400,1000\\}$ in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "We compare two models\u2019 losses over training steps of Transformers in Figure 1a and Figure 2a. In Figure 1b and Figure 2b, we show the differences and similarities of two models over the training steps. From the results, we find identical performances of the two models measured in losses. We also observe considerable alignment of the two models across tasks of different document lengths, indicated by decreasing prediction and model difference and increasing cosine similarity between models. Besides, comparing results with different $n$ , we observe that with larger document length, which is common in practical NLP tasks, more training steps are required for Transformers to exhibit such similarities. This shows the crucial role of pretraining stage of Transformers for their in-context learning ability. ", "page_idx": 8}, {"type": "image", "img_path": "SFaEENfEyw/tmp/38bd24dc07407a0241723e3f67d9a03839f6f6b3af2e2d12af47ee63432ae869.jpg", "img_caption": ["(a) Losses over training (b) Difference and similarity (a) Losses over training (b) Difference and similarity steps of Transformer over training steps steps of Transformer over training steps ", "Figure 1: Comparison on softmax regression tasks Figure 2: Comparison on softmax regression tasks of document length $n=200$ . of document length $n=1000$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.3 Different Word Embedding Sizes ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We also compare trained single self-attention layer and one-step gradient descent on synthetic softmax regression tasks of different word embedding sizes and document length $n=200$ . Similarly, we measure two models\u2019 losses and similarities over training steps on each set of tasks. Due to space limitation, we follow $[{\\mathrm{ONR}}^{+}22]$ to show in Figure 3 the loss comparisons at the end of training over different embedding size $d\\in\\{5,10,20,35,50\\}$ . The complete loss curves and measurements of model difference and similarity are presented in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "From the results, we again observe similar performances and close alignment of the two models with different word embedding sizes. ", "page_idx": 9}, {"type": "text", "text": "To summarize, our numerical results validate our theoretical results in Section 4, showing that when training self-attentiononly Transformers for softmax regression tasks, the models learned by gradient-descent and Transformers show great similarity. Note that due to the non-linearity of softmax regression, it is not expected for models to match exactly as implied in our Figure 3: Loss comparisons with theoretical results in Section 4, which is also observed in our different word embedding sizes $d$ . numerical findings. ", "page_idx": 9}, {"type": "image", "img_path": "SFaEENfEyw/tmp/369ffb9d6d829c4728f7cf264141aa0ae243ac7883ee620a30213db62e018db2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The attention mechanism that incorporates the softmax unit is a crucial aspect of Large Language Models (LLMs) and significantly contributes to their extraordinary performance in various Natural Language Processing (NLP) tasks. The ability to learn in-context is highly valued in recent LLMs, and comprehending this concept is vital when querying LLMs. In this study, taking a step further from prior works\u2019 studies on linear Transformer\u2019s ability of learning linear functions, we examined the in-context learning process from a softmax regression perspective of Transformer\u2019s attention mechanism. We established the bound on the data transformations brought about by a single selfattention layer with softmax unit and gradient descent on an L2 regression loss. Our findings suggest that the update acquired through gradient descent and in-context learning are highly similar when training self-attention-only Transformers for softmax regression tasks, which is also validated through our preliminary experimental results. These results offer insights into the theoretical underpinnings of in-context learning in Transformers and can aid in improving the understanding and performance of LLMs in various NLP tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The authors would like to thank Jerry Yao-Chieh Hu, Zhenmei Shi, Lichen Zhang and Yufa Zhou for helping preparing for camera-ready version of this paper. For more information related to the paper and adjacent topics, see https://www.youtube.com/@zhaosong2031 and https:// space.bilibili.com/3546587376650961. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[Ano24a] Anonymous. Fundamental limits of prompt tuning transformers: Universality, capacity and efficiency. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. under review.   \n[Ano24b] Anonymous. On statistical rates of conditional diffusion transformer: Approximation and estimation. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. under review. [AS23] Josh Alman and Zhao Song. Fast attention requires bounded entries. In NeurIPS, 2023. [AS24a] Josh Alman and Zhao Song. Fast rope attention: Combining the polynomial method and fast fourier transform. In manuscript, 2024. [AS24b] Josh Alman and Zhao Song. The fine-grained complexity of gradient computation for training large language models. In NeurIPS. arXiv preprint arXiv:2402.04497, 2024. [AS24c] Josh Alman and Zhao Song. How to capture higher-order correlations? generalizing matrix softmax attention to kronecker computation. In ICLR, 2024.   \n$[\\mathrm{ASA}^{+}22]$ Ekin Akyu\u00a8rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. [BAG20] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the Ability and Limitations of Transformers to Recognize Formal Languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096\u20137116, Online, November 2020. Association for Computational Linguistics. [Bel22] Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207\u2013219, March 2022.   \n$[{\\mathbf{B}}{\\mathbf{M}}{\\mathbf{R}}^{+}20]$ Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [BP66] Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554\u20131563, 1966. [BPG20] Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 455\u2013475, Online, November 2020. Association for Computational Linguistics. [BSZ24] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algorithm and hardness for dynamic attention maintenance in large language models. In ICML. arXiv preprint arXiv:2304.02207, 2024.   \n$[\\mathrm{CDL}^{+}22]$ Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In International Conference on Learning Representations (ICLR), 2022.   \n$[\\mathrm{CDW}^{+}21]$ Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00b4e. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems (NeurIPS), 34:17413\u201317426, 2021.   \n[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.   \n$[\\mathrm{CLL}^{+}24]$ Bo Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. Bypassing the exponential dependency: Looped transformers efficiently learn in-context by multi-step gradient descent. arXiv preprint arXiv:2410.11268, 2024.   \n$[\\mathbf{CLS}^{+}24]$ Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Hsr-enhanced sparse attention acceleration. arXiv preprint arXiv:2410.10165, 2024.   \n$[\\mathsf{C N D}^{+}22]$ Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.   \n$[\\mathrm{DCL}^{+}21]$ Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint arXiv:2112.00029, 2021.   \n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n[DGS23] Yichuan Deng, Yeqi Gao, and Zhao Song. Solving tensor low cycle rank approximation. arXiv preprint arXiv:2304.06594, 2023.   \n$\\mathrm{[DGV^{+}18]}$ ] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.   \n[DKOD20] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrfefficient attention using asymmetric clustering. Advances in Neural Information Processing Systems (NeurIPS), 33:6476\u20136489, 2020. [DLS23] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regression. arXiv preprint arXiv:2304.10411, 2023.   \n[DMS23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension. arxiv preprint: arxiv 2304.03426, 2023.   \n[EGKZ21] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. arXiv preprint arXiv:2110.10090, 2021. [EGZ20] Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. How can self-attention networks recognize Dyck-n languages? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4301\u20134306, Online, November 2020. Association for Computational Linguistics.   \n$[\\mathrm{GHG}^{+}20]$ Peng Gao, Chiori Hori, Shijie Geng, Takaaki Hori, and Jonathan Le Roux. Multi-pass transformer for machine translation. arXiv preprint arXiv:2009.11382, 2020.   \n[GMS23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over-parameterized exponential regression. arXiv preprint arXiv:2303.16504, 2023.   \n[GSYZ24] Yeqi Gao, Zhao Song, Xin Yang, and Yufa Zhou. Differentially private attention computation. In Neurips Safe Generative AI Workshop 2024, 2024.   \n[GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. arXiv preprint arXiv:2208.01066, 2022.   \n$[\\mathrm{HCL}^{+}24]$ Jerry Yao-Chieh Hu, Pei-Hsuan Chang, Haozheng Luo, Hong-Yu Chen, Weijian Li, Wei-Po Wang, and Han Liu. Outlier-efficient hopfield layers for large transformerbased models. In Forty-first International Conference on Machine Learning (ICML), 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "$[\\mathrm{HCW}^{+}24]$ Jerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, and Han Liu. Nonparametric modern hopfield models. arXiv preprint arXiv:2404.03900, 2024. ", "page_idx": 12}, {"type": "text", "text": "[HL19] John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733\u20132743, Hong Kong, China, November 2019. Association for Computational Linguistics. [HLSL24] Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits of modern hopfield models: A fine-grained complexity analysis. In Forty-first International Conference on Machine Learning (ICML), 2024.   \n[HWL24a] Jerry Yao-Chieh Hu, Dennis Wu, and Han Liu. Provably optimal memory capacity for modern hopfield models: Transformer-compatible dense associative memories as spherical codes. In Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS), 2024.   \n$[\\mathrm{HWL}^{+}24\\mathrm{b}]$ Jerry Yao-Chieh Hu, Weimin Wu, Zhuoru Li, Sophia Pi, , Zhao Song, and Han Liu. On statistical rates and provably efficient criteria of latent diffusion transformers (dits). In Thirty-eighth Conference on Neural Information Processing Systems (NeurIPS), 2024.   \n$[\\mathrm{HYW}^{+}23]$ Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023. [KKL20] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. [KS23] Tokio Kajitsuka and Issei Sato. Are transformers with one layer self-attention using low-rank weight matrices universal approximators? arXiv preprint arXiv:2307.14023, 2023. [KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc\u00b8ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156\u20135165. PMLR, 2020.   \n$[\\mathrm{LLS}^{+}24\\mathrm{a}]$ Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Tianyi Zhou. Fourier circuits in neural networks and transformers: A case study of modular arithmetic with multiple inputs. arXiv preprint arXiv:2402.09469, 2024.   \n$[\\mathrm{LLS}^{+}24\\mathrm{b}]$ Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Fine-grained attention i/o complexity: Comprehensive analysis for backward passes. arXiv preprint arXiv:2410.09397, 2024.   \n$[\\mathrm{LLS}^{+}24\\mathrm{c}]$ Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Zhuoyan Xu, and Junze Yin. Conv-basis: A new paradigm for efficient attention inference and gradient computation in transformers. arXiv preprint arXiv:2405.05219, 2024.   \n[LLS+24d] Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, and Yufa Zhou. Beyond linear approximations: A novel pruning approach for attention matrix. arXiv preprint arXiv:2410.11261, 2024.   \n[LLSS24a] Chenyang Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. Exploring the frontiers of softmax: Provable optimization, applications in diffusion model, and beyond. arXiv preprint arXiv:2405.03251, 2024.   \n[LLSS24b] Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. A tighter complexity analysis of sparsegpt. arXiv preprint arXiv:2408.12151, 2024. $[\\mathrm{LSS}^{+}24\\mathrm{a}]$ Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Looped relu mlps may be all you need as practical programmable computers. arXiv preprint arXiv:2410.09375, 2024.   \n$\\mathrm{LSS}^{+}24\\mathrm{b}]$ Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Multi-layer transformers gradient can be approximated in almost linear time. arXiv preprint arXiv:2408.13233, 2024.   \n[LSSY24] Yingyu Liang, Zhenmei Shi, Zhao Song, and Chiwun Yang. Toward infinite-long prefix in transformer. arXiv preprint arXiv:2406.14036, 2024.   \n[LSSZ24a] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Differential privacy of cross-attention with provable guarantee. arXiv preprint arXiv:2407.14717, 2024.   \n[LSSZ24b] Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. Tensor attention training: Provably efficient learning of higher-order transformers. arXiv preprint arXiv:2405.16411, 2024.   \n$[\\mathrm{LSX}^{+}22]$ ] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6), 2022. [LSZ23] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh regression problems. arXiv preprint, 2303.15725, 2023.   \n$[\\mathrm{LWD}^{+}23]$ Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and Beidi Chen. Deja vu: Contextual sparsity for efficient llms at inference time. In Manuscript, 2023.   \n$[{\\mathrm{ONR}}^{+}22]$ Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joa\u02dco Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022. [PCR19] Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh. Fully quantized transformer for machine translation. arXiv preprint arXiv:1910.10485, 2019.   \n[PMB19] Jorge P\u00b4erez, Javier Marinkovi\u00b4c, and Pablo Barcel\u00b4o. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019.   \n$[\\mathrm{RNS}^{+}18]$ ] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.   \n$[\\mathrm{SMN}^{+}24]$ Zhenmei Shi, Yifei Ming, Xuan-Phi Nguyen, Yingyu Liang, and Shafiq Joty. Discovering the gems in early layers: Accelerating long-context llms with 1000x input token reduction. arXiv preprint arXiv:2409.17422, 2024.   \n$[\\mathrm{SSZ}^{+}24\\mathrm{a}]$ Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Yanyu Li, Yifan Gong, Kai Zhang, Hao Tan, Jason Kuen, Henghui Ding, Zhihao Shu, Wei Niu, Pu Zhao, Yanzhi Wang, and Jiuxiang Gu. Lazydit: Lazy learning for the acceleration of diffusion transformers, 2024.   \n$[\\mathbf{S}\\mathbf{S}\\mathbf{Z}^{+}24\\mathbf{b}]$ Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Jing Liu, Ruiyi Zhang, Ryan A. Rossi, Hao Tan, Tong Yu, Xiang Chen, Yufan Zhou, Tong Sun, Pu Zhao, Yanzhi Wang, and Jiuxiang Gu. Numerical pruning for efficient autoregressive models, 2024.   \n[SZKS21] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. arXiv preprint arXiv:2103.07601, 2021. [TDP19] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u20134601, Florence, Italy, July 2019. Association for Computational Linguistics. [VB19] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63\u201376, Florence, Italy, August 2019. Association for Computational Linguistics. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "[VBC20] James Vuckovic, Aristide Baratin, and Remi Tachet des Combes. A mathematical theory of attention. arXiv preprint arXiv:2007.02876, 2020.   \n$[\\mathrm{VSP^{+}17}]$ Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[WCM21] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. arXiv preprint arXiv:2107.13163, 2021.   \n[WHHL24] Dennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, and Han Liu. Uniform memory retrieval with larger capacity for modern hopfield models. In Forty-first International Conference on Machine Learning (ICML), 2024.   \n$[\\mathrm{WHL}^{+}24]$ Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo-Yu Chen, and Han Liu. STanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. In The Twelfth International Conference on Learning Representations (ICLR), 2024.   \n$[\\mathbf{W}\\mathbf{L}\\mathbf{K}^{+}20]$ Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.   \n$[X\\mathrm{HH}^{+}24]$ Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, HsiSheng Goan, and Han Liu. Bishop: Bi-directional cellular learning for tabular data with generalized sparse modern hopfield model. In Forty-first International Conference on Machine Learning (ICML), 2024.   \n[XRLM21] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. [XSL24] Zhuoyan Xu, Zhenmei Shi, and Yingyu Liang. Do large language models have compositional ability? an investigation into limitations and scalability. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024.   \n$[\\mathrm{YBR}^{+}20]$ Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2020.   \n[YPPN21] Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Selfattention networks can process bounded hierarchical languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770\u20133785, Online, August 2021. Association for Computational Linguistics.   \n$[Z\\mathbf{B}\\mathbf{B}^{+}22]$ Yi Zhang, Arturs Backurs, Se\u00b4bastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task, 2022.   \n[ZHDK23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. arXiv preprint arXiv:2302.02451, 2023.   \n$[Z\\mathrm{KV}^{+}20]$ Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33:15383\u201315393, 2020.   \n[ZPGA23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? arXiv preprint arXiv:2303.08117, 2023. ", "page_idx": 14}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Roadmap. In Section A, we introduce some related works. In Section B, we compute the Lipschitz with respect to $x$ . In Section C, we give some definitions related to the softmax function of $A$ . In Section D, we compute the Lipschitz with respect to $A$ . In Section E, we show our complete numerical experiments that support our theoretical results. ", "page_idx": 15}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 In-Context Learning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$[\\mathrm{ASA}^{+}22]$ indicate that Transformer-based in-context learners are able to perform traditional learning algorithms implicitly. This is achieved by encoding smaller models within their internal activations. These smaller models are updated by the given context. They theoretically investigate the learning algorithms that Transformer decoders can implement. They demonstrate that Transformers need only a limited number of layers and hidden units to implement various linear regression algorithms. For $d$ -dimensional regression problems, a $O(d)$ -hidden-size Transformer can perform a single step of gradient descent. They also demonstrate its ability to update a ridge regression problem. The study reveals that Transformers theoretically have the ability to perform multiple linear regression algorithms. ", "page_idx": 15}, {"type": "text", "text": "[GTLV22] concentrate on training Transformer to learn certain functions, under in-context conditions. The goal is to have a more comprehensive understanding of in-context learning and determine if Transformers can learn the majority of functions within a given class after training. They found that in-context learning is possible even when there is a distribution shift between training and inference data or between in-context examples and query inputs. In addition, they find out that Transformers can learn more complex function classes such as sparse linear functions, two-layer neural networks, and decision trees. These trained Transformers have comparable performance to task-specific learning algorithms. ", "page_idx": 15}, {"type": "text", "text": "$[{\\mathrm{ONR}}^{+}22]$ demonstrate the similarity between the training process of Transformers in in-context tasks and some meta-learning formulations based on gradient descent. During training Transformers for auto-regressive tasks, the implementation of in-context learning in the Transformer forward pass is carried out through gradient-based optimization of an implicit auto-regressive inner loss that is constructed from the in-context data. ", "page_idx": 15}, {"type": "text", "text": "Formally speaking, they consider the following problem $\\operatorname*{min}_{x}\\|A x-b\\|_{2}$ defined in Definition 1.2. They show that one step of gradient descent carries out data transformation as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|A(x+\\delta_{x})-b\\|_{2}=\\|A x-(b-\\delta_{b})\\|_{2}}\\\\ {=\\|A x-\\widetilde{b}\\|_{2}\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\delta_{x}$ denotes the one-step gradient descent on $x$ and $\\delta_{b}$ denotes the corresponding data transformation on $b$ . They also show that a self-attention layer is in principle capable of exploiting statistics in the current training data samples. Concretely, let $Q,K,\\mathbf{\\dot{V}}\\in\\mathbf{\\dot{R}}^{d\\times d}$ denotes the weights for the query matrix, key matrix, and value matrix respectively. The linear self-attention layer updates an input sample by doing the following data transformation: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{b}_{j}=b_{j}+P V K^{\\top}Q_{j}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\widehat{b}$ denotes the updated $b$ and $P$ denotes the projection matrix such that a Transformer step $\\widehat{b}_{j}$ on every $j$ is identical to the gradient-induced dynamics $\\widetilde{b}_{j}$ . This equivalence implies that when training linear-self-attention-only Transformers for fundamental regression tasks, the models learned by GD and Transformers show great similarity. ", "page_idx": 15}, {"type": "text", "text": "[XRLM21] explores the occurrence of in-context learning during pre-training when documents exhibit long-range coherence. The Language Model (LLM) develops the ability to generate coherent next tokens by deducing a latent document-level concept. During testing, in-context learning is observed when the LLM deduces a shared latent concept between examples in a prompt. They demonstrate that in-context learning happens even when there is a distribution mismatch between prompts and pretraining data, especially when the pretraining distribution is a mixture of Hidden ", "page_idx": 15}, {"type": "text", "text": "Markov Models [BP66]. Theoretically, they show that the error of the in-context predictor is optimal when a distinguishability condition holds. In cases where this condition does not hold, the expected error still reduces as the length of each example increases. This finding highlights the importance of both input and input-output mapping for in-context learning. ", "page_idx": 16}, {"type": "text", "text": "A.2 Transformer Theory ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The advancements of Transformers have been noteworthy, however, their learning mechanisms are not completely comprehensible yet. Although these models have performed remarkably well in structured and reasoning activities, our comprehension of their mathematical foundations lags significantly behind. Past research has indicated that the outstanding performance of Transformer-based models can be attributed to the information within their components, such as multi-head attention. Various studies [TDP19, VB19, HL19, Bel22, $\\mathrm{LLS^{+}24a}$ , XSL24] have presented empirical proof that these components carry a substantial amount of information, which can help resolve different probing tasks. ", "page_idx": 16}, {"type": "text", "text": "Recent research has investigated the potential of Transformers through both theoretical and experimental methods, including Turing completeness [BPG20], function approximation $[\\mathbf{Y}\\mathbf{B}\\mathbf{R}^{+}20$ , $\\bar{\\mathrm{CDW}}^{+}21]$ , formal language representation [BAG20, EGZ20, YPPN21], and abstract algebraic operation learning $[Z\\mathbf{B}\\mathbf{B}^{+}22]$ . Some of these studies have indicated that Transformers may act as universal approximators for sequence-to-sequence operations $[\\mathrm{YBR}^{+}20$ , KS23, Ano24a] and emulate Turing machines [PMB19, BPG20]. $[\\mathrm{LWD}^{+}23]$ demonstrate the existence of contextual sparsity in LLM, which can be accurately predicted. They exploit the sparsity to speed up LLM inference without degrading the performance from both a theoretical perspective and an empirical perspective. $[\\mathrm{DCL}^{+}21]$ proposed the Pixelated Butterfly model that uses a simple fixed sparsity pattern to speed up the training of Transformer. Other studies have focused on the expressiveness of attention within Transformers $\\mathrm{[DGV^{+}18}$ , VBC20, $Z\\mathrm{KV}^{+}20$ , EGKZ21, SZKS21, WCM21, LSSY24, $\\mathrm{LSS}^{+}24\\mathrm{a}]$ and differentially private attention mechanisms [GSYZ24, LSSZ24a]. Recently, modern Hopfield models $[\\mathrm{HYW}^{+}23$ , HLSL24, WHHL24, $\\mathrm{HCL}^{+}24$ , HWL24a, $\\mathrm{HCW}^{+}24]$ have introduced Hopfield layers as powerful alternatives for transformer attention, offering solid theoretical guarantees and strong empirical performance $[X\\mathrm{HH}^{+}24$ , $\\mathrm{WHL}^{+}24]$ . Additionally, the statistical and computational theory of transformer-based diffusion models, specifically Diffusion Transformers (DiTs), has been studied in depth $[\\mathrm{HWL}^{+}24\\mathrm{b}$ , Ano24b]. ", "page_idx": 16}, {"type": "text", "text": "Furthermore, [ZPGA23] has demonstrated that moderately sized masked language models may effectively parse and recognize syntactic information that helps in the partial reconstruction of a parse tree. Inspired by the language grammar model studied by [ZPGA23], [DGS23] consider the tensor cycle rank approximation problem. [GMS23] consider the exponential regression in neural tangent kernel over-parameterization setting. [LSZ23] studied the computation of regularized version of the exponential regression problem but they ignore the normalization factor. [DLS23] consider the softmax regression which considers the normalization factor compared to exponential regression problems [GMS23, LSZ23]. The majority of LLMs can perform attention computations in an approximate manner during inference, as long as there are sufficient guarantees of precision. This perspective has been studied by various research, including [CGRS19, KKL20, $\\mathrm{WLK}^{\\bar{+}}20$ , DKOD20, KVPF20, $\\mathrm{CDW}^{+}21$ , $\\mathrm{CDL}^{+}\\dot{22}$ , LLSS24b, $\\mathrm{LLS}^{+}24\\mathrm{d}$ , $\\mathrm{LSS}^{+}24\\mathrm{b}$ , $\\mathrm{SMN}^{+}24$ , $L{\\mathrm{L}}S^{+}24\\mathrm{c}$ , ${\\mathrm{SSZ}}^{+}24\\mathrm{b}$ , $\\mathrm{SSZ}^{+}24\\mathrm{a}]$ . With this in mind, [ZHDK23, AS23, BSZ24, DMS23, $\\mathrm{HYW}^{+}23$ , $\\mathrm{LLS}^{+}24\\mathrm{b}$ , $\\mathrm{CLS^{+}24}$ , LSSZ24b, HLSL24, $\\mathrm{HWL}^{+}24\\mathrm{b}$ , HWL24a] have studied the attention matrix computation from the hardness perspective and developed faster algorithms. ", "page_idx": 16}, {"type": "text", "text": "B Lipschitz with respect to $x$ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Section B.1, we give the preliminary to compute the Lipschitz. In Section B.2, we compute the Lipschitz of function $\\exp(A x)$ with respect to $x$ . In Section B.3, we compute the Lipschitz of the function $\\alpha$ with respect to $x$ . In Section B.4, we compute the Lipschitz of function $\\alpha^{-\\dagger}$ with respect to $x$ . ", "page_idx": 16}, {"type": "text", "text": "B.1 Preliminary ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We can compute ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}L}{\\mathrm{d}x}}=g(x)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\eta>0$ denote the learning rate. ", "page_idx": 17}, {"type": "text", "text": "We update ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{t+1}=x_{t}+\\eta\\cdot g(x_{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Definition B.1. We define $\\delta_{b}\\in\\mathbb{R}^{n}$ to be the vector that satisfies the following conditions ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\langle\\exp(A x_{t+1}),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x_{t+1})-b\\|_{2}^{2}=\\|\\langle\\exp(A x_{t}),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x_{t})-b+\\delta_{b}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\{-1,+1\\}^{n}$ denote a vector that each entry can be either $-1$ or $+1$ . In the worst case, there are $2^{n}$ possible solutions, e.g., ", "page_idx": 17}, {"type": "equation", "text": "$$\n(\\langle\\exp(A x_{t+1}),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x_{t+1})-\\langle\\exp(A x_{t}),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x_{t}))\\circ\\{-1,+1\\}^{n}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The norm of all the choices are the same. Thus, it is sufficient to only consider one solution as follows. ", "page_idx": 17}, {"type": "text", "text": "Claim B.2. We can write $\\delta_{b}$ as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{b}=\\underbrace{\\langle\\exp(A x_{t+1}),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x_{t+1})}_{f(x_{t+1})}-\\underbrace{\\langle\\exp(A x_{t}),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x_{t})}_{f(x_{t})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The proof directly follows from Definition B.1. ", "page_idx": 17}, {"type": "text", "text": "For convenience, we split $\\delta_{b}$ into two terms, and provide the following definitions ", "page_idx": 17}, {"type": "text", "text": "Definition B.3. We define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{b,1}:=(\\langle\\exp(A x_{t+1}),\\mathbf{1}_{n}\\rangle^{-1}-\\langle\\exp(A x_{t}),\\mathbf{1}_{n}\\rangle^{-1})\\cdot\\exp(A x_{t+1})}\\\\ &{\\delta_{b,2}:=\\langle\\exp(A x_{t}),\\mathbf{1}_{n}\\rangle^{-1}\\cdot(\\exp(A x_{t+1})-\\exp(A x_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we have ", "page_idx": 17}, {"type": "text", "text": "Lemma B.4. We have ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{b}=\\delta_{b,1}+\\delta_{b,2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "\u2022 We can rewrite $\\delta_{b,1}$ as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{b,1}=(\\alpha(x_{t+1})^{-1}-\\alpha(x_{t})^{-1})\\cdot\\exp(A x_{t+1}),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "\u2022 We can rewrite $\\delta_{b,2}$ as follows ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{b,2}=\\alpha(x_{t})^{-1}\\cdot(\\exp(A x_{t+1})-\\exp(A x_{t})).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{b}=\\delta_{b,1}+\\delta_{b,2}}\\\\ &{\\quad=\\alpha(x_{t+1})^{-1}\\exp(A x_{t+1})-\\alpha(x_{t})^{-1}\\exp(A x_{t+1})+}\\\\ &{\\quad\\quad\\alpha(x_{t})^{-1}\\exp(A x_{t+1})-\\alpha(x_{t})^{-1}\\exp(A x_{t})}\\\\ &{\\quad=\\alpha(x_{t+1})^{-1}\\exp(A x_{t+1})-\\alpha(x_{t})^{-1}\\exp(A x_{t})}\\\\ &{\\quad=\\langle\\exp(A x_{t+1}),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x_{t+1})-\\langle\\exp(A x_{t}),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x_{t}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the 1st step follows from the definitions of $\\delta_{b}$ , the 2nd step follows from the definitions of $\\delta_{b,1}$ and $\\delta_{b,2}$ , the 3rd step follows from simple algebra, the 4th step comes from the definition of $\\alpha$ . ", "page_idx": 17}, {"type": "text", "text": "B.2 Lipschitz for function $\\exp(A x)$ with respect to $x$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Lemma B.5. If the following conditions holds ", "page_idx": 18}, {"type": "text", "text": "\u2022 Let A \u2208Rn\u00d7d   \n$\\begin{array}{l}{\\cdot\\;L e t\\:\\|A(y-x)\\|_{\\infty}<0.01}\\\\ {\\cdot\\;L e t\\:\\|A\\|\\leq R}\\end{array}$   \n\u2022 Let $x,y$ satisfy that $\\|x\\|_{2}\\leq R$ and $\\|y\\|_{2}\\leq R$ ", "page_idx": 18}, {"type": "text", "text": "Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\exp(A x)-\\exp(A y)\\|_{2}\\leq2{\\sqrt{n}}R\\exp(R^{2})\\cdot\\|x-y\\|_{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. We have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\exp(A x)-\\exp(A y)\\|_{2}\\leq\\|\\exp(A x)\\|_{2}\\cdot2\\|A(x-y)\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{n}\\cdot\\exp(\\|A x\\|_{2})\\cdot2\\|A(x-y)\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{n}\\exp(R^{2})\\cdot2\\|A(x-y)\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{n}\\exp(R^{2})\\cdot2\\|A\\|\\cdot\\|x-y\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\sqrt{n}R\\exp(R^{2})\\cdot\\|x-y\\|_{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the 1st step follows from $\\|A(y-x)\\|_{\\infty}<0.01$ and Fact 2.1, the 2nd step comes from Fact 2.1, the 3rd step follows from Fact 2.2, the 4th step follows from Fact 2.2, the last step follows from $\\|A\\|\\leq R$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B.3 Lipschitz for function $\\alpha(x)$ with respect to $x$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We state a tool from previous work [DLS23]. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.6 (Lemma 7.2 in [DLS23]). If the following conditions hold ", "page_idx": 18}, {"type": "text", "text": "\u2022 Let $\\alpha(x)$ be defined as Definition 3.3 ", "page_idx": 18}, {"type": "text", "text": "Then we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\alpha(x)-\\alpha(y)|\\leq\\|\\exp(A x)-\\exp(A y)\\|_{2}\\cdot\\sqrt{n}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.4 Lipschitz for function $\\alpha(x)^{-1}$ with respect to $x$ ", "page_idx": 18}, {"type": "text", "text": "We state a tool from previous work [DLS23]. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.7 (Lemma 7.2 in [DLS23]). If the following conditions hold ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\;L e t\\left\\langle\\exp(A x),\\mathbf{1}_{n}\\right\\rangle\\geq\\beta}\\\\ {\\bullet\\;L e t\\left\\langle\\exp(A y),\\mathbf{1}_{n}\\right\\rangle\\geq\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\alpha(x)^{-1}-\\alpha(y)^{-1}|\\leq\\beta^{-2}\\cdot|\\alpha(x)-\\alpha(y)|.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C Softmax Function with respect to $A$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we consider the function with respect to $A$ . We define function softmax $f$ as follows Definition C.1 (Function $f$ , Reparameterized $x$ by $A$ in Definition 3.1). Given a matrix $A\\in\\mathbb{R}^{n\\times d}$ . Let ${\\mathbf{1}}_{n}$ denote a length-n vector that all entries are ones. We define prediction function $f:\\mathbb{R}^{n\\times d}\\rightarrow$ $\\mathbb{R}^{n}$ as follows ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(A):=\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle^{-1}\\cdot\\exp(A x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we reparameterized $x$ by $A$ for our loss function $L$ . We define loss function $L$ as follows ", "page_idx": 19}, {"type": "text", "text": "Definition C.2 (Loss function $L_{\\mathrm{exp}}$ , Reparameterized $x$ by $A$ in Definition 3.2). Given a matrix $A\\in\\mathbb{R}^{n\\times d}$ and a vector $b\\in\\mathbb{R}^{n\\times d}$ . We define loss function $L_{\\mathrm{exp}}:\\mathbb{R}^{n\\times d}\\rightarrow\\mathbb{R}$ as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\nL_{\\mathrm{exp}}(A):=0.5\\cdot\\Vert\\langle\\mathrm{exp}(A x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A x)-b\\Vert_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For convenience, we define two helpful notations $\\alpha$ and $c$ with respect to $A$ as follows: ", "page_idx": 19}, {"type": "text", "text": "Definition C.3 (Normalized coefficients, Reparameterized $x$ by $A$ in Definition 3.3). We define $\\alpha:\\mathbb{R}^{n\\times d}\\rightarrow\\mathbb{R}$ as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\alpha(A):=\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we can rewrite $f(A)$ (see Definition C.1) and $L_{\\mathrm{exp}}(A)$ (see Definition C.2) as follows ", "page_idx": 19}, {"type": "text", "text": "\u2022 $f(A)=\\alpha(A)^{-1}\\cdot\\exp(A x).$ . \u2022 $L_{\\mathrm{exp}}(A)=0.5\\cdot\\|\\alpha(A)^{-1}\\cdot\\exp(A x)-b\\|_{2}^{2}.$ \u2022 $L_{\\mathrm{exp}}(A)=0.5\\cdot\\|f(A)-b\\|_{2}^{2}.$ ", "page_idx": 19}, {"type": "text", "text": "Definition C.4 (Reparameterized $x$ by $A$ in Definition 3.4). We define function $c:\\mathbb{R}^{n\\times d}\\in\\mathbb{R}^{n}$ as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\nc(A):=f(A)-b.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then we can rewrite $L_{\\mathrm{exp}}(A)$ (see Definition C.2) as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\nL_{\\mathrm{exp}}(A)=0.5\\cdot\\|c(A)\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "D Lipschitz with respect to $A$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Section D.1, we give the preliminary to compute the Lipschitz. In Section D.2, we show the upper bound of $\\delta_{b}$ with respect to $A$ . In Section D.3, we compute the Lipschitz of function $\\exp(A x)$ with respect to $A$ . In Section D.4, we compute the Lipschitz of the function $\\alpha$ with respect to $A$ . In Section D.5, we compute the Lipschitz of function $\\alpha^{-1}$ with respect to $A$ . ", "page_idx": 19}, {"type": "text", "text": "D.1 Preliminary ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We define $\\delta_{b}$ as follows ", "page_idx": 19}, {"type": "text", "text": "Definition D.1 (Reparameterized $x$ by $A$ in Definition B.1). We define $\\delta_{b}\\in\\mathbb{R}^{n}$ to be the vector that satisfies the following conditions ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\langle\\exp(A_{t+1}x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A_{t+1}x)-b\\|_{2}^{2}=\\|\\langle\\exp(A_{t}x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A_{t}x)-b+\\delta_{b}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Claim D.2 (Reparameterized $x$ by $A$ in Definition B.2). We can write $\\delta_{b}$ as follows ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\delta_{b}=\\underbrace{\\langle\\exp(A_{t+1}x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A_{t+1}x)}_{f(A_{t+1})}-\\underbrace{\\langle\\exp(A_{t}x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A_{t}x)}_{f(A_{t})}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. The proof directly follows from Definition D.1. ", "page_idx": 19}, {"type": "text", "text": "For convenient, we split $\\delta_{b}$ into two terms, and provide the following definitions ", "page_idx": 19}, {"type": "text", "text": "Definition D.3 (Reparameterized $x$ by $A$ in Definition B.3). We define ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{b,1}:=(\\langle\\exp(A_{t+1}x),\\mathbf{1}_{n}\\rangle^{-1}-\\langle\\exp(A_{t}x),\\mathbf{1}_{n}\\rangle^{-1})\\cdot\\exp(A_{t+1}x)}\\\\ &{\\delta_{b,2}:=\\langle\\exp(A_{t}x),\\mathbf{1}_{n}\\rangle^{-1}\\cdot(\\exp(A_{t+1}x)-\\exp(A_{t}x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, we have ", "page_idx": 19}, {"type": "text", "text": "Lemma D.4 (Reparameterized $x$ by $A$ in Lemma B.4). We have ", "page_idx": 19}, {"type": "text", "text": "\u2022 We can rewrite $\\delta_{b}\\in\\mathbb{R}^{n}$ as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta_{b}=\\delta_{b,1}+\\delta_{b,2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 We can rewrite $\\delta_{b,1}\\in\\mathbb{R}^{n}$ as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta_{b,1}=(\\alpha(A_{t+1})^{-1}-\\alpha(A_{t})^{-1})\\cdot\\exp(A_{t+1}x),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 We can rewrite $\\delta_{b,2}\\in\\mathbb{R}^{n}$ as follows ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\delta_{b,2}=\\alpha(A_{t})^{-1}\\cdot(\\exp(A_{t+1}x)-\\exp(A_{t}x)).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{b}=\\delta_{b,1}+\\delta_{b,2}}\\\\ &{\\quad=\\alpha(A_{t+1})^{-1}\\exp(A_{t+1}x)-\\alpha(A_{t})^{-1}\\exp(A_{t+1}x)+}\\\\ &{\\quad\\quad\\alpha(A_{t})^{-1}\\exp(A_{t+1}x)-\\alpha(A_{t})^{-1}\\exp(A_{t}x)}\\\\ &{\\quad=\\alpha(A_{t+1})^{-1}\\exp(A_{t+1}x)-\\alpha(A_{t})^{-1}\\exp(A_{t}x)}\\\\ &{\\quad=\\langle\\exp(A_{t+1}x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A_{t+1}x)-\\langle\\exp(A_{t}x),\\mathbf{1}_{n}\\rangle^{-1}\\exp(A_{t}x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the 1st step follows from the definitions of $\\delta_{b}$ , the 2nd step follows from the definitions of $\\delta_{b,1}$ and $\\delta_{b,2}$ , the 3rd step comes from simple algebra, the 4th step comes from the definition of $\\alpha$ . ", "page_idx": 20}, {"type": "text", "text": "D.2 Upper Bounding $\\delta_{b}$ with respect to $A$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We can show that Lemma D.5 (Reparameterized $x$ by $A$ in Lemma 4.1). If the following conditions hold ", "page_idx": 20}, {"type": "text", "text": "\u2022 Let $\\beta\\in(0,1)$ .   \n\u2022 Let $\\delta_{b,1}\\in\\mathbb{R}^{n}$ be defined as Definition D.3. \u2022 Let $\\delta_{b,2}\\in\\mathbb{R}^{n}$ be defined as Definition $D.3$ . $\\begin{array}{l}{{\\bullet\\,\\,L e t\\,\\delta_{b}=\\delta_{b,1}+\\delta_{b,2}.}}\\\\ {{\\bullet\\,\\,L e t\\,R\\geq4.}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "We have \u2022 Part 1. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\delta_{b,1}\\|_{2}\\leq2\\beta^{-2}n^{1.5}\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|_{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Part 2. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\delta_{b,2}\\|_{2}\\leq2\\beta^{-1}\\sqrt{n}R\\exp(R^{2})\\cdot\\|A_{t+1}-A_{t}\\|_{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Part 3. ", "page_idx": 20}, {"type": "equation", "text": "$$\n||\\underbrace{f(A_{t+1})-f(A_{t})}_{\\delta_{b}}||_{2}\\leq4\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot||A_{t+1}-A_{t}||_{2}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Proof of Part 1. We have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\delta_{b,1}\\|_{2}\\leq|\\alpha(A_{t+1})^{-1}-\\alpha(A_{t})^{-1}|\\cdot\\|\\exp(A_{t+1}x)\\|_{2}}\\\\ &{\\leq|\\alpha(A_{t+1})^{-1}-\\alpha(A_{t})^{-1}|\\cdot\\sqrt{n}\\cdot\\exp(R^{2})}\\\\ &{\\leq\\beta^{-2}\\cdot|\\alpha(A_{t+1})-\\alpha(A_{t})|\\cdot\\sqrt{n}\\cdot\\exp(R^{2})}\\\\ &{\\leq\\beta^{-2}\\cdot\\sqrt{n}\\cdot\\|\\exp(A_{t+1}x)-\\exp(A_{t}x)\\|_{2}\\cdot\\sqrt{n}\\cdot\\exp(R^{2})}\\\\ &{\\leq\\beta^{-2}\\cdot\\sqrt{n}\\cdot2\\sqrt{n}R\\exp(R^{2})\\|A_{t+1}-A_{t}\\|\\cdot\\sqrt{n}\\cdot\\exp(R^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n=2\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first step follows from definition, the second step follows from assumption on $A$ and $x$ , the third step follows Lemma D.8, the forth step follows from Lemma D.7, the fifth step follows from Lemma D.6. ", "page_idx": 21}, {"type": "text", "text": "Proof of Part 2. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\delta_{b,2}\\|_{2}\\leq|\\alpha(A_{t+1})^{-1}|\\cdot\\|\\exp(A_{t+1}x)-\\exp(A_{t}x)\\|_{2}}\\\\ &{\\qquad\\quad\\leq\\beta^{-1}\\cdot\\|\\exp(A_{t+1}x)-\\exp(A_{t}x)\\|_{2}}\\\\ &{\\qquad\\quad\\leq\\beta^{-1}\\cdot2\\sqrt{n}R\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Part 3. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\delta_{b}\\|_{2}=\\|\\delta_{b,1}+\\delta_{b,2}\\|_{2}}\\\\ &{\\qquad\\leq\\|\\delta_{b,1}\\|_{2}+\\|\\delta_{b,2}\\|_{2}}\\\\ &{\\qquad\\leq2\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|+2\\beta^{-1}n^{0.5}R\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\\\ &{\\qquad\\leq2\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|+2\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\\\ &{\\qquad\\leq4\\beta^{-2}n^{1.5}R\\exp(2R^{2})\\cdot\\|A_{t+1}-A_{t}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the 1st step follows from the definition of $\\delta_{b}$ , the 2nd step comes from triangle inequality, the 3rd step comes from the results in Part 1 and Part 2, the 4th step follows from the fact that $n\\geq1$ and $\\beta^{-1}\\geq1$ , the 5th step follows from simple algebra. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D.3 Lipschitz for function $\\exp(A x)$ with respect to $A$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma D.6 (Reparameterized $x$ by $A$ in Lemma B.5). If the following conditions holds ", "page_idx": 21}, {"type": "text", "text": "$\\begin{array}{l}{\\mathord{\\left.\\cdot\\right.}L e t\\,A,B\\in\\mathbb{R}^{n\\times d}}\\\\ {\\mathord{\\left.\\cdot\\right.}L e t\\,\\|(A-B)x\\|_{\\infty}<0.01}\\\\ {\\mathord{\\left.\\cdot\\right.}L e t\\,\\|A\\|\\le R}\\end{array}$ \u2022 Let $x$ satisfy that $\\|x\\|_{2}\\leq R$ ", "page_idx": 21}, {"type": "text", "text": "Then we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\exp(A x)-\\exp(B x)\\|_{2}\\leq2{\\sqrt{n}}R\\exp(R^{2})\\cdot\\|A-B\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\exp(A x)-\\exp(B x)\\|_{2}\\leq\\|\\exp(A x)\\|_{2}\\cdot2\\|(A-B)x\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{n}\\cdot\\exp(\\|A x\\|_{2})\\cdot2\\|(A-B)x\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{n}\\exp(R^{2})\\cdot2\\|(A-B)x\\|_{2}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{n}\\exp(R^{2})\\cdot2\\|A-B\\|\\cdot\\|x\\|_{2}}\\\\ &{\\qquad\\qquad\\leq2\\sqrt{n}R\\exp(R^{2})\\cdot\\|A-B\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the 1st step follows from $\\|A(y\\mathrm{~-~}x)\\|_{\\infty}\\,<\\,0.01$ and Fact 2.1, the 2nd step follows from Fact 2.1, the 3rd step follows from Fact 2.2, the 4th step comes from Fact 2.2, the last step follows from $\\|A\\|\\leq R$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D.4 Lipschitz for function $\\alpha(A)$ with respect to $A$ ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma D.7 (Reparameterized $x$ by $A$ in Lemma B.6). If the following conditions hold ", "page_idx": 21}, {"type": "text", "text": "\u2022 Let $\\alpha(A)$ be defined as Definition C.3 ", "page_idx": 21}, {"type": "text", "text": "Then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\alpha(A)-\\alpha(B)|\\leq\\|\\exp(A x)-\\exp(B x)\\|_{2}\\cdot{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\alpha(A)-\\alpha(B)|=|\\langle\\exp(A x)-\\exp(B x),\\mathbf{1}_{n}\\rangle|}\\\\ &{\\phantom{|}\\leq\\|\\exp(A x)-\\exp(B x)\\|_{2}\\cdot\\sqrt{n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the 1st step comes from the definition of $\\alpha(x)$ , the 2nd step follows from Cauchy-Schwarz inequality (Fact 2.1). \u53e3 ", "page_idx": 22}, {"type": "text", "text": "D.5 Lipschitz for function $\\alpha(A)^{-1}$ with respect to $A$ ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma D.8 (Reparameterized $x$ by $A$ in Lemma B.7). If the following conditions hold ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bullet\\;L e t\\;\\langle\\exp(A x),\\mathbf{1}_{n}\\rangle\\geq\\beta}\\\\ {\\bullet\\;L e t\\;\\langle\\exp(B x),\\mathbf{1}_{n}\\rangle\\geq\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n|\\alpha(A)^{-1}-\\alpha(B)^{-1}|\\leq\\beta^{-2}\\cdot|\\alpha(A)-\\alpha(B)|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We can show that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\alpha(A)^{-1}-\\alpha(B)^{-1}|=\\alpha(A)^{-1}\\alpha(B)^{-1}\\cdot|\\alpha(A)-\\alpha(B)|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\beta^{-2}\\cdot|\\alpha(A)-\\alpha(B)|}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the 1st step follows from simple algebra, the 2nd step follows from $\\alpha(A)\\geq\\beta,\\alpha(B)\\geq\\beta$ . ", "page_idx": 22}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we show the complete numerical experimental results supporting our theoretical results that when training self-attention-only Transformers for softmax regression tasks, the models learned by gradient-descent and Transformers show great similarity. ", "page_idx": 22}, {"type": "text", "text": "Experiments setup. According to Definition 1.3, we construct the synthetic softmax regression tasks consists of randomly sampled length- $n$ documents $A\\ \\in\\ \\mathbb{R}^{n\\times\\check{d}}$ where each word has the $d$ -dimensional embedding and targets $b\\,\\in\\,\\mathbb{R}^{n}$ . In our experiments we choose a set of different value of document length $n\\,\\in\\,\\{25,50,100,200,400\\}$ and a set of different embedding size $d\\in$ $\\{5,10,20,35,50\\}$ . Following $[{\\mathrm{ONR}}^{+}22]$ , we compare the two models in our experiment: a trained single self-attention (SA) layer with a softmax unit approximating the full Transformers, and a softmax regression model trained with one-step gradient descent. The training objective for both models is defined as in Definition 1.3. For the single self-attention layer with a softmax unit, we choose the learning rate $\\eta_{\\mathrm{SA}}=0.005$ . For the softmax regression model, we determine the optimal learning rate $\\eta_{\\mathrm{GD}}$ by minimizing the $\\ell_{2}$ regression loss over a training set of $10^{3}$ tasks through line search. ", "page_idx": 22}, {"type": "text", "text": "To compare the trained single self-attention layer with a softmax unit and the softmax regression model trained with one-step gradient descent, we sample $10^{3}$ tasks and record the losses of two models. In addition, we follow $[{\\mathrm{ONR}}^{+}22]$ to record ", "page_idx": 22}, {"type": "text", "text": "\u2022 Pred Diff: the predictions difference measured with the $\\ell_{2}$ norm: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\widehat{y}_{\\mathrm{SA}}(A)-\\widehat{y}_{\\mathrm{GD}}(x)\\|_{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\widehat{y}_{\\mathrm{SA}}(A)$ is corresponding to the $\\widetilde{b}$ in Theorem 4.2, and $\\widehat{y}_{\\mathrm{GD}}(x)$ is corresponding to the $\\widetilde{b}$ in Theorem 4.3. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Model Cos: the cosine similarity between the sensitivities of two models: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\cos\\mathrm{Sim}\\big(\\frac{\\partial\\widehat{y}_{\\mathrm{GD}}(x)}{\\partial x},\\frac{\\partial\\widehat{y}_{\\mathrm{SA}}(A)}{\\partial A}\\big)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "\u2022 Model Diff: the model sensitivity difference measured with the $\\ell_{2}$ norm: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\frac{\\partial\\widehat{y}_{\\mathrm{GD}}(x)}{\\partial x}-\\frac{\\partial\\widehat{y}_{\\mathrm{SA}}(A)}{\\partial A}\\|_{2}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "All experiments run on a single NVIDIA RTX2080Ti GPU with 10 independent repetitions. ", "page_idx": 23}, {"type": "text", "text": "Results on tasks of different document lengths. The results of the comparisons between a trained single self-attention layer and one-step gradient descent on synthetic softmax regression tasks of document length $n\\in\\{25,50,100,200,400,1000\\}$ and word embedding size $d=20$ are shown in Figure 4-9. We measure two models\u2019 losses and similarities over the training steps of the SA layer for each set of tasks. From the results, we observe identical performances of the two models measured in losses. We also observe considerable alignment of the two models across tasks of different document lengths, indicated by decreasing prediction and model difference and increasing cosine similarity between models. ", "page_idx": 23}, {"type": "text", "text": "Results on tasks of different word embedding sizes. The results of the comparisons between a trained single self-attention layer and one-step gradient descent on synthetic softmax regression tasks of document length $n=200$ and word embedding size $d\\in\\{5,10,\\dot{2}0,35,50\\}$ are shown in Figure 7 and 10-13. Similarly, we measure two models\u2019 losses and similarities over training steps of the SA layer for each set of tasks. We again observe similar performances and close alignment of the two models. ", "page_idx": 23}, {"type": "text", "text": "In conclusion, our experimental results empirically validate our theoretical results in Section 4, showing that when training self-attention-only Transformers for softmax regression tasks, the models learned by gradient-descent and Transformers show great similarity. Due to the non-linearity of softmax regression, it is not expected for models to match exactly as implied in our theoretical results in Section 4, which is also observed in our experimental findings. ", "page_idx": 23}, {"type": "image", "img_path": "SFaEENfEyw/tmp/db5a1c9d0e7495bfadfcfa52eb53b5fbeceb7899ea16e6eadc51e21a630b8a89.jpg", "img_caption": ["(b) Difference and similarity over training steps "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "SFaEENfEyw/tmp/62102f65fc0b376d452ac4bbd5f2359d53b3468b222b4b404824191acd51ce70.jpg", "img_caption": ["Figure 4: Comparison between trained single-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=25$ and embedding size $d=20$ . ", "(a) Losses over training steps of Transformer "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "F Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our findings are restricted to small Transformer and simple regression problems. One interesting direction for further investigation is to acquire a comprehensive perception of in-context learning in larger models. To our best knowledge, we believe this work does not have any negative societal impact. ", "page_idx": 23}, {"type": "text", "text": "G Impact Statements ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 23}, {"type": "image", "img_path": "SFaEENfEyw/tmp/10b583751a0d18f240893ede35ea86d3eac2a769f0ab6e61d3e623d19504b059.jpg", "img_caption": ["(a) Losses over training steps of Transformer "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "SFaEENfEyw/tmp/ce7e7d59074f7ece664527d883f8f8f15d5836e3ab7e7c7154290bc0404313c8.jpg", "img_caption": ["(b) Difference and similarity over training steps "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 5: Comparison between trained single-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=50$ and embedding size $d=20$ . ", "page_idx": 24}, {"type": "image", "img_path": "SFaEENfEyw/tmp/440634c110ac2ffe07fcd269c48d4bbd59d16beef34e978c638bbad11fcf0bea.jpg", "img_caption": ["Figure 6: Comparison between trained one-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=100$ and embedding size $d=20$ . ", "(a) Losses over training steps of Transformer "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "SFaEENfEyw/tmp/b706c88b68c6da8dcfaf06e80e29e29bfe145431213ddd4f86fba55aa5715240.jpg", "img_caption": ["(b) Difference and similarity over training steps "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "SFaEENfEyw/tmp/0e9ab0d032e2aa9f0042c70b3f8c43c7abf661637dc953d49f2b1b1750c55b02.jpg", "img_caption": ["(b) Difference and similarity over training steps "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "SFaEENfEyw/tmp/0bd8f8a70d321506e7e935aaa39124f424b54b3993da7653b3bc14fabd8edfda.jpg", "img_caption": ["Figure 7: Comparison between trained one-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=200$ and embedding size $d=20$ . ", "(a) Losses over training steps of Transformer "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "SFaEENfEyw/tmp/35a90d51cc5cd7d0f365b0c4274440e66c2bc1647a0ba8da507d060e3dbcde3c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 8: Comparison between trained one-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=400$ and embedding size $d=20$ . ", "page_idx": 25}, {"type": "image", "img_path": "SFaEENfEyw/tmp/83507981b0483b450369b2258cd223316c2e5f949f3c8e4585f6a9ef49e1f4d1.jpg", "img_caption": ["Figure 9: Comparison between trained one-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=1000$ and embedding size $d=20$ . "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "SFaEENfEyw/tmp/57a549a8c6f383cec71840acb509bbdf7ba240b373637ae116426ad9353902c6.jpg", "img_caption": ["(a) Losses over training steps of Transformer "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "SFaEENfEyw/tmp/3540855ec667dbdb6c01fa738bf64ee6a5cdb3d8afc82c1729c80c10961a9052.jpg", "img_caption": ["(b) Difference and similarity over training steps "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 10: Comparison between trained one-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=200$ and embedding size $d=5$ . ", "page_idx": 25}, {"type": "image", "img_path": "SFaEENfEyw/tmp/68504cda87d4eabdb58a54129ea86b11ce5401da533c472f05a50c6b48010231.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 11: Comparison between trained one-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=200$ and embedding size $d=10$ . ", "page_idx": 26}, {"type": "image", "img_path": "SFaEENfEyw/tmp/21a04c30bc286379cc5e18350f0325161f08763c9aebf0aebc3c5e430ee7ec86.jpg", "img_caption": ["Figure 12: Comparison between trained one-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=200$ and embedding size $d=35$ . "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "SFaEENfEyw/tmp/5fbfcccb84eddd423cd0452c0c1081cfa6495753aac07d5edef964338f663260.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 13: Comparison between trained one-SA-layer Transformer and one-step GD on softmax regression tasks of document length $n=200$ and embedding size $d=50$ . ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We propose our main results at the end of the introduction. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The limitations is discussed in Section F. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "\u2022 For Theorem 4.2, the proof is in Section 4.2.   \n\u2022 For Theorem 4.3, the proof is in Section 4.3. ", "page_idx": 28}, {"type": "text", "text": "We have carefully checked the correctness and the time complexity proof that shown in this paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The experimental setup is described in Section 5.1 and Appendix E. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: The data and code are planned to be released upon acceptance and approval. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The experimental setup is described in Section 5.1 and Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The results plotted in Section 5 and Appendix E include error bars. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The compute resource information is provided in Section 5.1 and Appendix E. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The authors have read the NeurIPS Code of Ethies and made sure the paper followsthe NeurIPS Code of Ethics in every aspect. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The potential societal impact is discussed in Section G. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new dataset or model. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The assets are properly mentioned and cited in Section 5 and Appendix E. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]