[{"figure_path": "SFaEENfEyw/figures/figures_9_1.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "This figure compares the performance of two models, namely a trained single self-attention (SA) layer with a softmax unit and a softmax regression model trained with one-step gradient descent (GD), on synthetic softmax regression tasks.  The tasks are designed with document length n=200.  The figure presents plots showing the losses of both models over training steps (a), and their differences and similarities over the training steps (b), including the prediction difference measured by the l2 norm, the model sensitivity difference, and the cosine similarity between the sensitivities of both models.", "section": "5 Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_9_2.jpg", "caption": "Figure 3: Loss comparisons with different word embedding sizes d.", "description": "This figure compares the loss of two models (Gradient Descent and Trained Transformer) on synthetic softmax regression tasks with varying word embedding sizes (d). The x-axis represents the embedding size, and the y-axis represents the loss.  The plot shows that the losses of the two models are similar across different embedding sizes.", "section": "5.3 Different Word Embedding Sizes"}, {"figure_path": "SFaEENfEyw/figures/figures_23_1.jpg", "caption": "Figure 4: Comparison between trained single-SA-layer Transformer and one-step GD on softmax regression tasks of document length n = 25 and embedding size d = 20.", "description": "This figure compares the performance of a single self-attention layer with softmax unit (approximating a full Transformer) and a softmax regression model trained with one-step gradient descent.  It shows the losses over training steps, along with the prediction difference, model sensitivity difference, and cosine similarity between the two models.  The results demonstrate the similarity in performance and alignment between the two models, particularly in terms of cosine similarity, which increases over training steps.", "section": "5.2 Different Document Lengths"}, {"figure_path": "SFaEENfEyw/figures/figures_23_2.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "The figure compares the performance of the softmax regression model trained by gradient descent and the single self-attention layer with a softmax unit for document length n = 200. It shows the losses of two models and their difference and similarity over training steps. The results indicate similar performance of the two models.", "section": "5 Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_24_1.jpg", "caption": "Figure 4: Comparison between trained single-SA-layer Transformer and one-step GD on softmax regression tasks of document length n = 25 and embedding size d = 20.", "description": "This figure compares the performance of a single self-attention layer with a softmax unit (approximating a full Transformer) and a one-step gradient descent method on synthetic softmax regression tasks.  The document length is 25 and embedding size is 20.  The plots show the losses over training steps for both models, along with their prediction difference, model difference, and cosine similarity. This illustrates how closely the two models' performance aligns.", "section": "Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_24_2.jpg", "caption": "Figure 4: Comparison between trained single-SA-layer Transformer and one-step GD on softmax regression tasks of document length n = 25 and embedding size d = 20.", "description": "This figure compares the performance of a single self-attention layer with a softmax unit (approximating a full Transformer) and a softmax regression model trained with one-step gradient descent.  The comparison is done on synthetic softmax regression tasks with document length n=25 and word embedding size d=20.  The figure shows the losses over training steps for both models, and also displays the difference and similarity metrics between the two: Preds Diff (prediction difference), Model Diff (model sensitivity difference), and Model Cos (cosine similarity of model sensitivities).  The results illustrate the similarity in performance and behavior between the two approaches.", "section": "5.2 Different Document Lengths"}, {"figure_path": "SFaEENfEyw/figures/figures_24_3.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "This figure compares the performance of a trained single self-attention layer with softmax unit and a softmax regression model trained with one-step gradient descent on softmax regression tasks with document length n=200.  Subfigures (a) show the losses over training steps, while subfigures (b) show the differences and similarities between the two models (prediction difference, model sensitivity difference and cosine similarity).", "section": "5 Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_24_4.jpg", "caption": "Figure 4: Comparison between trained single-SA-layer Transformer and one-step GD on softmax regression tasks of document length n = 25 and embedding size d = 20.", "description": "This figure compares the performance of a single self-attention layer with softmax unit (approximating full Transformers) and a softmax regression model trained with one-step gradient descent on a synthetic softmax regression task. The document length (n) is 25, and the word embedding size (d) is 20.  The plot shows the losses over training steps for both models, as well as the difference and similarity between their predictions and model sensitivities. The shaded areas represent the standard deviation across multiple independent repetitions. This figure demonstrates the similarity in performance and alignment between the two models on this specific task.", "section": "5.2 Different Document Lengths"}, {"figure_path": "SFaEENfEyw/figures/figures_24_5.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "This figure compares the performance of a trained single self-attention layer with a softmax unit and a softmax regression model trained with one-step gradient descent on softmax regression tasks with a document length of 200.  The figure shows three subplots: (a) Losses over training steps, (b) Difference and similarity over training steps. The results show that the two models exhibit similar performance and close alignment.", "section": "5 Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_24_6.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "This figure compares the performance of a softmax regression model trained with one-step gradient descent (GD) and a trained single self-attention (SA) layer with a softmax unit on synthetic softmax regression tasks.  The document length (n) is fixed at 200.  The figure likely shows training loss curves for both models across training steps, perhaps also showing metrics like prediction difference and cosine similarity between the models' predictions to illustrate their closeness.", "section": "5 Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_25_1.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "This figure compares the performance of a trained single self-attention layer with a softmax unit and a softmax regression model trained with one-step gradient descent on synthetic softmax regression tasks with a document length of 200.  The left panel (a) shows the losses over training steps for both models. The right panel (b) displays the difference and similarity between the two models over the training steps, including prediction difference, model sensitivity difference, and cosine similarity of model sensitivities.", "section": "5 Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_25_2.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "This figure compares the performance of a softmax regression model trained with one-step gradient descent and a single self-attention layer with a softmax unit (approximating full Transformers) on synthetic softmax regression tasks.  The document length is fixed at 200 words. The figure shows (a) the losses over training steps for both models and (b) their differences and similarities over training steps, measured by prediction differences, model sensitivity differences, and cosine similarity between the models' sensitivities.  This demonstrates the closeness in performance between gradient descent and the Transformer-based approach.", "section": "5 Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_25_3.jpg", "caption": "Figure 4: Comparison between trained single-SA-layer Transformer and one-step GD on softmax regression tasks of document length n = 25 and embedding size d = 20.", "description": "This figure compares the performance of a single self-attention layer with softmax unit (approximating full Transformers) and a softmax regression model trained with one-step gradient descent.  The comparison is shown across training steps for a synthetic softmax regression task with a document length of 25 and a word embedding size of 20.  The subfigures show the losses over training steps, the differences and similarities between the two models' performances over the training steps.", "section": "5.2 Different Document Lengths"}, {"figure_path": "SFaEENfEyw/figures/figures_25_4.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "This figure compares the performance of a single self-attention layer with softmax unit and one-step gradient descent on softmax regression tasks with a document length of 200. The figure shows that the two models have similar performances in terms of loss over training steps, and that the prediction and model differences decrease while the cosine similarity between the models increases over training steps. This indicates that the model learned by gradient descent and the Transformer show great similarity.", "section": "5 Numerical Experiments"}, {"figure_path": "SFaEENfEyw/figures/figures_26_1.jpg", "caption": "Figure 4: Comparison between trained single-SA-layer Transformer and one-step GD on softmax regression tasks of document length n = 25 and embedding size d = 20.", "description": "This figure presents a comparison of the performance of a trained single self-attention layer with a softmax unit (approximating full transformers) and a softmax regression model trained with one-step gradient descent.  The comparison is done across training steps, showing the losses for both models, and also the prediction difference, model sensitivity difference and cosine similarity between the two models. This allows for a visual assessment of how similar the models' behavior is.", "section": "5.2 Different Document Lengths"}, {"figure_path": "SFaEENfEyw/figures/figures_26_2.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "The figure shows the comparison results between a trained single self-attention layer with a softmax unit and a softmax regression model trained with one-step gradient descent. The left panel shows the losses over training steps for both models on synthetic softmax regression tasks with document length n=200. The right panel shows the differences and similarities of the two models over training steps measured by Prediction Difference (Preds Diff), Model Difference (Model Diff), and Model Cosine Similarity (Model Cos).", "section": "5.1 Experiments Setup"}, {"figure_path": "SFaEENfEyw/figures/figures_26_3.jpg", "caption": "Figure 1: Comparison on softmax regression tasks of document length n = 200.", "description": "This figure compares the performance of a trained single self-attention layer with softmax unit and a softmax regression model trained with one-step gradient descent on synthetic softmax regression tasks with a document length of 200. The left subplot shows losses over training steps, and the right subplot displays the differences and similarities between the two models over the training steps. The figure helps to visualize the closeness of in-context learning and weight shifting for softmax regression.", "section": "5 Numerical Experiments"}]