{"references": [{"fullname_first_author": "Josh Alman", "paper_title": "Fast attention requires bounded entries", "publication_date": "2023", "reason": "This paper provides a theoretical foundation for understanding the computational complexity of attention mechanisms, a core component of LLMs and crucial to the paper's analysis of in-context learning."}, {"fullname_first_author": "Ekin Aky\u00fcrek", "paper_title": "What learning algorithm is in-context learning? Investigations with linear models", "publication_date": "2022", "reason": "This paper offers a simplified mathematical perspective on in-context learning using linear models, providing a foundational framework for the current paper's investigation."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020", "reason": "This seminal work established the remarkable few-shot learning capabilities of LLMs, a phenomenon central to the concept of in-context learning explored in this paper."}, {"fullname_first_author": "Johannes von Oswald", "paper_title": "Transformers learn in-context by gradient descent", "publication_date": "2022", "reason": "This paper directly connects in-context learning to gradient descent, a key concept that this paper builds upon and expands upon to consider the impact of softmax."}, {"fullname_first_author": "Shivam Garg", "paper_title": "What can transformers learn in-context? A case study of simple function classes", "publication_date": "2022", "reason": "This paper investigates the in-context learning capabilities of Transformers through a series of experiments on simplified tasks, providing valuable context for this paper's theoretical analysis."}]}