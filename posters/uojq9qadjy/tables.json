[{"figure_path": "uoJQ9qadjY/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of IPRM with videoQA methods on STAR (left) and AGQAv2 (right). All methods operate on 32 frames unless otherwise mentioned in (). *Not directly compared as utilizes additional surrogate tasks / benchmarks and num. of frames not reported.", "description": "This table compares the performance of the proposed Iterative and Parallel Reasoning Mechanism (IPRM) against other state-of-the-art video question answering (VQA) methods on two benchmark datasets:  STAR and AGQAv2.  The table shows the accuracy achieved by each method across different metrics, including the overall accuracy and accuracy for various question subtypes (e.g., feasibility, interaction, prediction, sequencing).  It highlights IPRM's superior performance, particularly on the STAR and AGQAv2 datasets. Note that some methods are not directly compared due to differences in the number of frames used or the inclusion of additional surrogate tasks.", "section": "3 Experiments"}, {"figure_path": "uoJQ9qadjY/tables/tables_6_1.jpg", "caption": "Table 2: Comparison of methods for CLEVRER-Humans [51] (Opt. is per option acc. and Qs. is per question acc.). IPRM achieves state-of-art across settings.", "description": "This table compares the performance of different methods on the CLEVRER-Humans benchmark for video reasoning.  The methods are evaluated using two metrics: per-option accuracy (Opt.) and per-question accuracy (Qs.).  The results are shown for three different training settings: zero-shot, finetuned, and scratch.  The table highlights that IPRM achieves state-of-the-art performance across all three training settings.", "section": "3 Experiments"}, {"figure_path": "uoJQ9qadjY/tables/tables_6_2.jpg", "caption": "Table 2: Comparison of methods for CLEVRER-Humans [51] (Opt. is per option acc. and Qs. is per question acc.). IPRM achieves state-of-art across settings.", "description": "This table compares the performance of IPRM against other methods on the CLEVRER-Humans benchmark for video reasoning.  The benchmark tests the model's ability to determine causal links between events in synthetic videos.  The table shows the per-option accuracy (Opt.) and per-question accuracy (Qs.) for various methods, categorized by zero-shot (no training on the specific task), finetuned (trained on the task), and scratch (trained from scratch) settings.  The results highlight that IPRM achieves state-of-the-art performance across all settings.", "section": "3 Experiments"}, {"figure_path": "uoJQ9qadjY/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of IPRM with videoQA methods on STAR (left) and AGQAv2 (right). All methods operate on 32 frames unless otherwise mentioned in (). *Not directly compared as utilizes additional surrogate tasks / benchmarks and num. of frames not reported.", "description": "This table compares the performance of the Iterative and Parallel Reasoning Mechanism (IPRM) against other video question answering (VQA) methods on two benchmark datasets: STAR and AGQAv2.  The table shows the accuracy achieved by each method on various subtasks within each benchmark.  Note that some methods are not directly comparable due to differences in their experimental setup, such as the use of additional surrogate tasks or different numbers of video frames.", "section": "3 Experiments"}, {"figure_path": "uoJQ9qadjY/tables/tables_16_1.jpg", "caption": "Table 2: Comparison of methods for CLEVRER-Humans [51] (Opt. is per option acc. and Qs. is per question acc.). IPRM achieves state-of-art across settings.", "description": "This table compares the performance of IPRM with other methods on the CLEVRER-Humans benchmark for video reasoning.  The benchmark evaluates the model's ability to determine causal links between events in synthetic videos. The table shows that IPRM achieves state-of-the-art performance across different evaluation settings (zero-shot, finetuned, and from scratch).", "section": "3 Experiments"}, {"figure_path": "uoJQ9qadjY/tables/tables_16_2.jpg", "caption": "Table 1: Comparison of IPRM with videoQA methods on STAR (left) and AGQAv2 (right). All methods operate on 32 frames unless otherwise mentioned in (). *Not directly compared as utilizes additional surrogate tasks / benchmarks and num. of frames not reported.", "description": "This table compares the performance of the proposed Iterative and Parallel Reasoning Mechanism (IPRM) against other state-of-the-art video question answering (VQA) methods on two benchmark datasets: STAR and AGQAv2.  The table shows the accuracy achieved by each method on different aspects of video reasoning, such as interaction, prediction, and feasibility.  It highlights that IPRM outperforms other methods on both datasets.", "section": "3 Experiments"}, {"figure_path": "uoJQ9qadjY/tables/tables_17_1.jpg", "caption": "Table 8: Left: Comparison of IPRM with prominent vision-language attention mechanisms with CLIP VIT-L/14 backbones on CLEVR-Humans, GQA and NLVRv2 benchmarks (\u20184L\u2019 indicates 4 att layers; \u2018x\u2019 indicates model did not converge). Right: Results with other CLIP variants VIT-B and VIT-L@ 336 on GQA and NLVRv2.", "description": "This table compares the performance of IPRM against other prominent vision-language attention mechanisms (Cross-Att, Concat-Att, and Wt-Proj-Fusion) using different CLIP backbones (VIT-L/14, VIT-B/16, and VIT-L/14@336).  The comparison is done across three benchmarks: CLEVR-Humans, GQA, and NLVR2.  The table shows the number of additional parameters (+Param), GFLOPs, and the accuracy (TestD for GQA, Test for NLVR2, and Zero-shot/Finetuned for CLEVR-Humans). It highlights that IPRM achieves superior performance with fewer parameters and GFLOPs compared to the other methods.", "section": "3.3 CLIP Integration Results"}, {"figure_path": "uoJQ9qadjY/tables/tables_17_2.jpg", "caption": "Table 1: Comparison of IPRM with videoQA methods on STAR (left) and AGQAv2 (right). All methods operate on 32 frames unless otherwise mentioned in (). *Not directly compared as utilizes additional surrogate tasks / benchmarks and num. of frames not reported.", "description": "This table compares the performance of the proposed Iterative and Parallel Reasoning Mechanism (IPRM) against other video question answering (VQA) methods on two benchmark datasets:  STAR and AGQAv2.  The table shows the accuracy achieved by each method on several sub-tasks within each dataset, highlighting IPRM's superior performance, especially on tasks involving complex reasoning such as prediction and sequencing. Note that some methods are not directly compared because they use different numbers of frames or additional benchmark tasks.", "section": "3 Experiments"}]