[{"type": "text", "text": "Dense Associative Memory Through the Lens of Random Features ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Benjamin Hoover IBM Research & Georgia Tech benjamin.hoover@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Duen Horng Chau Georgia Tech polo@gatech.edu ", "page_idx": 0}, {"type": "text", "text": "Hendrik Strobelt IBM Research & MIT-IBM hendrik.strobelt@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Parikshit Ram IBM Research parikshit.ram@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Dmitry Krotov IBM Research krotov@ibm.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dense Associative Memories are high storage capacity variants of the Hopfield networks that are capable of storing a large number of memory patterns in the weights of the network of a given size. Their common formulations typically require storing each pattern in a separate set of synaptic weights, which leads to the increase of the number of synaptic weights when new patterns are introduced. In this work we propose an alternative formulation of this class of models using random features, commonly used in kernel methods. In this formulation the number of network's parameters remains fixed. At the same time, new memories can be added to the network by modifying existing weights. We show that this novel network closely approximates the energy function and dynamics of conventional Dense Associative Memories and shares their desirable computational properties. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hopfield network of associative memory is an elegant mathematical model that makes it possible to store a set of memory patterns in the synaptic weights of the neural network [1]. For a given prompt $\\sigma_{i}(t=0)$ , which serves as the initial state of that network, the neural update equations drive the dynamical flow towards one of the stored memories. For a system of $K$ memory patterns in the $D$ dimensional binary space the network's dynamics can be described by the temporal trajectory $\\sigma_{i}(t)$ which descends the energy function ", "page_idx": 0}, {"type": "equation", "text": "$$\nE=-\\sum_{\\mu=1}^{K}\\Big(\\sum_{i=1}^{D}\\xi_{i}^{\\mu}\\sigma_{i}\\Big)^{2}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Here $\\xi_{i}^{\\mu}$ (index $\\mu=1...K$ ,and index $i=1...D)$ represent memory vectors. The neural dynamical equations describe the energy descent on this landscape. In this formulation, which we call the memory representation, the geometry of the energy landscape is encoded in the weights of the network $\\xi_{i}^{\\mu}$ , which coincide with the memorised patterns. Thus, in situations when the set of the memories needs to be expanded by introducing new patterns one must introduce additional weights. ", "page_idx": 0}, {"type": "text", "text": "Alternatively, one could rewrite the above energy in a different form, which is more commonly used in the literature. Specifically, the sum over the memories can be computed upfront and the energy can be written as ", "page_idx": 0}, {"type": "equation", "text": "$$\nE=-\\sum_{i,j=1}^{D}T_{i j}\\sigma_{i}\\sigma_{j},\\quad\\mathrm{where}\\quad T_{i j}=\\sum_{\\mu=1}^{K}\\xi_{i}^{\\mu}\\xi_{j}^{\\mu}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "image", "img_path": "164QnJsYjF/tmp/a2a377855fbbd329d8ee06f650d60a10b3469fc37bc43dc025f85ea99e98fe8e.jpg", "img_caption": ["A) Distributed Representation (DrDAM) ", "MemoryRepresentation(MrDAM) Energy computed from similarity between query and stored patterns "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "164QnJsYjF/tmp/37c36656df8f7d8bf162509e06e6a6450ab6c42b51db721182db8fd17c38a667.jpg", "img_caption": ["B) DrDAMReconstructions CloselyMatch MrDAM's "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: The Distributed Representation for Dense Associative Memory (DrDAM) approximates both the energy and fixed-point dynamics of the traditional Memory Representation for Dense Associative Memory (MrDAM) while having a parameter space of constant size. A) Diagram of DrDAM using a $\\otimes$ basis function parameterized by random features (e.g., see eq. (8). In the distributed representation, adding new memories does not change the size of the memory tensor. B) Comparing energy descent dynamics between DrDAM and MrDAM on $3\\!\\times\\!64\\!\\times\\!64$ images from Tiny Imagenet [11]. Both models are initialized on queries where the bottom two-thirds of pixels are occluded with zeros; dynamics are run while clamping the visible pixels and their collective energy traces shown. DrDAM achieves the same fixed points as MrDAM, and these final fixed points have the same energy. The energy decreases with time for both MrDAM and DrDAM, although the dependence of the energy relaxation towards the fixed point is sometimes different between the two representations. Experimental setup is described in appendix D. ", "page_idx": 1}, {"type": "text", "text": "In this form one can think about weights of the network being the symmetric tensor $T_{i j}$ instead of $\\xi_{i}^{\\mu}$ . One advantage of formulating the model this way is that the tensor $T_{i j}$ does not require adding additional parameters when new memories are introduced. Additional memories are stored in the already existing set of weights by redistributing the information about new memories across the already existing network parameters. We refer to this formulation as distributed representation. ", "page_idx": 1}, {"type": "text", "text": "A known problem of the network (eqs. (1) and (2)) is that it has a small memory storage capacity, which scales at best linearly as the size of the network $D$ is increased [1]. This limitation has been resolved with the introduction of Dense Associative Memories (DenseAMs), also known as Modern Hopfield Networks [2]. This is achieved by strengthening the non-linearities (interpreted as neural activation functions) in eq. (1), which can lead to the super-linear and even exponentially large memory storage capacity [2, 3]. Using continuous variables $\\mathbf{x}\\in\\mathbb{R}^{D}$ , the energy is defined as1 ", "page_idx": 1}, {"type": "equation", "text": "$$\nE=-Q\\Big[\\sum_{\\mu=1}^{K}F\\Big(S\\big[\\pmb{\\xi}^{\\mu},\\mathbf{g}(\\mathbf{x})\\big]\\Big)\\Big],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the function $\\mathbf{g}:\\mathbb{R}^{D}\\,\\rightarrow\\,\\mathbb{R}^{D}$ is a vector function (e.g., a sigmoid, a linear function, or a layernorm), the function $F(\\cdot)$ is a rapidly growing separation function (e.g., power $F(\\cdot)=(\\cdot)^{n}$ or exponent), $S[\\mathbf{x},\\mathbf{x}^{\\prime}]$ is a similarity function (e.g., a dot product or a Euclidean distance), and $Q$ is a scalar monotone function (e.g., linear or logarithm). For instance, in order to describe the classical Hopfield network with binary variables (eq. (1)) one could take: linear $Q$ , quadratic $F(\\cdot)\\,=\\,(\\cdot)^{2}$ dot product $S$ , and a sign function for $g_{i}=s i g n(x_{i})=\\sigma_{i}$ . There are many possible combinations of various functions $\\mathbf{g},\\bar{F}(\\cdot),S(\\cdot,\\cdot)$ that lead to different models from the DenseAM family [2-7]; many of the resulting models have proven useful for various problems in AI and neuroscience [8]. Diffusion models have been linked to even more sophisticated forms of the energy landscape [9, 10]. ", "page_idx": 1}, {"type": "text", "text": "From the perspective of the information storage capacity DenseAMs are significantly superior compared to the classical Hopfield networks. At the same time, most? of the models from the DenseAM family are typically formulated using the memory representation, and for this reason ", "page_idx": 1}, {"type": "image", "img_path": "164QnJsYjF/tmp/589e24f188619ad13ad0f379cb97488065b8454419b9688f99408f3ed70b2d42.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: DrDAM achieves parameter compression over MrDAM, successfully storing 20 different $64\\mathrm{x}64\\mathrm{x}3$ images from TinyImagenet [11] and retrieving them when occluding the lower $40\\%$ of each query. The memory matrix of MrDAM is of shape (20, 12288) while the memory tensor of DrDAM is of shape $Y=2\\cdot10^{5}$ ,a ${\\sim}20\\%$ reduction in the number of parameters compared to MrDAM; all other configurations for this experiment match those in appendix D. Further compression can be achieved with a higher tolerance for DrDAM's retrieval error, smaller $\\beta$ , and fewer occluded pixels, see $\\S\\ 4$ Top: Occluded query images. Middle: Fixed-point retrievals from DrDAM. Bottom: (ground truth) Fixed-point retrievals of MrDAM. ", "page_idx": 2}, {"type": "text", "text": "require introduction of new weights when additional memory patterns are added to the network. The main question that we ask in our paper is: how can we combine superior memory storage properties of DenseAMs with the distributed (across synaptic weights) formulation of these models in the spirit of classical Hopfeld networks (eq. (2))? If such a formulation is found, it would allow us to add memories to the existing network by simply recomputing already existing synaptic weights, without adding new parameters. ", "page_idx": 2}, {"type": "text", "text": "A possible answer to this question is offered by the theory of random features and kernel machines. Given an input domain $\\mathcal{X}$ , kernel machines leverage a positive definite Mercer kernel function $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}_{+}$ that measures the similarity between pairs of inputs. The renowned \u201ckernel trick' allows one to compute the inner-product ", "page_idx": 2}, {"type": "equation", "text": "$$\nk(\\mathbf{x},\\mathbf{x}^{\\prime})=\\langle\\varphi(\\mathbf{x}),\\varphi(\\mathbf{x}^{\\prime})\\rangle=\\sum_{\\alpha=1}^{Y}\\varphi_{\\alpha}(\\mathbf{x})\\varphi_{\\alpha}(\\mathbf{x}^{\\prime})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "between two inputs $\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathcal{X}$ in a rich feature space defined by the feature map $\\varphi(\\mathbf x)$ without ever explicitly realizing the feature map $\\varphi(\\mathbf x)$ . Various machine learning models (such as support vector machines [12], logistic regression, and various others [13, 14] can be learned with just access to pairwise inner-products, and thus, the kernel trick allows one to learn such models in an extremely expressive feature space. Kernel functions have been developed for various input domains beyond the Euclidean space such as images, documents, strings (such as protein sequences [15]), graphs (molecules [16], brain neuron activation paths) and time series (music, financial data) [17]. Common kernels for Euclidean data are the radial basis function or RBF kernel $k(\\mathbf{x},\\mathbf{x}^{\\prime})=\\exp(-\\gamma\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}^{2})$ and the polynomial kernel $k(\\mathbf{x},\\mathbf{x}^{\\prime})=(\\langle\\mathbf{x},\\mathbf{x}^{\\prime}\\rangle+b)^{p}$ . To appreciate the expressivity of these kernel machines, note that, for input domain $\\mathbb{R}^{D}$ , the RBF kernel corresponds to an infinite dimensional feature space ( $Y=\\infty$ ) and the polynomial kernel to a $O(D^{p})$ dimensional feature space. ", "page_idx": 2}, {"type": "text", "text": "Interpreting the composition of the separation and similarity functions in eq. (3) as the left hand side of the kernel trick eq. (4) we can map the energy into the feature space, using appropriately chosen feature maps. Subsequently, the order of the summations over memories and features can be swapped, and the sum over memories can be computed explicitly. This makes it possible to encode all the memories in a tensor $T_{\\alpha}$ , which we introduce in section $\\S\\ 3$ , that contains all the necessary information about the memories. The energy function then becomes defined in terms of this tensor only, as opposed to individual memories. This functionality is summarized in fig. 1. Additionally, we show examples of retrieved Tiny ImageNet images that have been memorised using the original DenseAM model, which we call MrDAM, and the \u201cfeaturized\" version of the same model, which we call DrDAM (please see the explanations of these names in the caption to fig. 1). These examples visually illustrate that mapping the problem into the feature space preserves most of the desirable computational properties of DenseAMs, which normally are defined in the kernel space\". ", "page_idx": 2}, {"type": "text", "text": "Contributions: ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose a novel approximation of a DenseAM network utilizing random features commonly used in kernel machines. This novel architecture does not require the storage of the original memories, and can incorporate new memories without increasing the size of the network. ", "page_idx": 2}, {"type": "text", "text": "We precisely characterize the approximation introduced in the energy descent dynamics by this architecture, highlighting the different critical factors that drive the difference between the exact energy descent and the proposed approximate one.   \n$\\blacktriangleright$ We validate our theoretical guarantee with empirical evaluations. ", "page_idx": 3}, {"type": "text", "text": "In the past, kernel trick has been used for optimizing complexity of the attention mechanism in Transformers [18], and those results have been recently applied to associative memory [19], given the various connections between Transformers and DenseAMs [4, 20]. Existing studies [18, 19] focus on settings when attention operation or associative memory retrieval is done in a single step update. This is different from our goals here, which is to study the recurrent dynamics of the associative memory updates and convergence of that dynamics to the attractor fixed points. Iatropoulos et al. [21] propose kernel memory networks which are a recurrent form of a kernel support vector machine, and highlight that DenseAM networks are special cases of these kernel memory networks. Making a connection between nonparametric kernel regression and associative memory, Hu et al. [22] propose a family of provably efficient sparse Hopfield networks [23, 24], where the dynamics of any given input are explicitly driven by a subset of the memories due to various entropic regularizations on the energy. DenseAMs have been also used for sequences [25, 26, 24]. To reduce the complexity of computing all the pairs of $F(S[\\pmb{\\xi},\\mathbf{x}])$ for a given set of memories and queries, Hu et al. [27] leverage a low-rank approximation of this separation-similarity matrix using polynomial expansions. The kernel trick has also recently been used to increase separation between memories (with an additional learning stage to learn the kernel), thereby improving memory capacity [28]. There are also very recent theoretical analysis of the random feature Hopfield networks [29, 30], where their focus in on the construction of memories using random features. Kernels are also related to density estimation [31], and recent works have leveraged a connection between mixtures of Gaussians and DenseAMs for clustering [32, 33]. Lastly, random features have been used for biological implementations of both Transformers and DenseAMs [34, 35]. ", "page_idx": 3}, {"type": "text", "text": "To the best of our knowledge there is no rigorous theoretical and empirical comparison of DenseAMs and their distributed (featurized) variants in recurrent memory storage and retrieval settings, as well as results pertaining to the recovery of the fixed points of the energy descent dynamics. This is the mainfocus of ourwork. ", "page_idx": 3}, {"type": "text", "text": "2  Technical background ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given the energy function in eq. (3), a variable $\\mathbf{x}$ is updated in the forward pass through the \u201clayers\"' of this recurrent model such that its energy decreases with each update. If the energy is bounded from below, this ensures that the input will (approximately) converge to a local minimum. This can be achieved by performing a \u201cgradient descent' in the energy landscape. Considering the continuous dynamics, updating the input x over time with $d\\mathbf{x}/d t$ , we need to ensure that $d E/{\\bar{d t}}<0$ . This can be achieved by setting $d\\mathbf{x}/d t\\propto-\\nabla_{\\mathbf{x}}E$ ", "page_idx": 3}, {"type": "text", "text": "Discretizing the above dynamics, the update of an input $\\mathbf{x}$ at the $t$ -th recurrent layer is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{x}^{(t)}\\leftarrow\\mathbf{x}^{(t-1)}-\\eta^{(t-1)}\\nabla_{\\mathbf{x}}E^{(t-1)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\eta^{(t)}$ is a (step dependent step-size for the energy gradient descent, $E^{(t)}$ is the energy of the input after the $t$ -th layer, and the input to the first layer $\\mathbf{x}^{(0)}\\leftarrow\\mathbf{x}$ . The final output of the associative memory network after $L$ layers is $\\bar{\\mathbf{x}}^{(L)}$ ", "page_idx": 3}, {"type": "text", "text": "DenseAMs significantly improve the memory capacity of the associative memory network by utilizing rapidly growing nonlinearity-based separation-similarity compositions such as $F(S[{\\bf x},\\dot{\\xi}^{\\mu}])=$ $\\exp(\\bar{\\beta}\\,\\langle{\\bf x},\\bar{\\xi}^{\\bar{\\mu}}\\rangle)$ Oor $\\bar{F}(S[{\\bf x},\\pmb{\\xi}^{\\mu}])\\stackrel{!}{=}\\exp(-\\beta\\bar{/}2\\|{\\bf x}-\\pmb{\\xi}^{\\mu}\\|_{2})$ Oor $\\bar{F}(S[{\\bf x},\\bar{\\xi}^{\\mu}])=(\\langle{\\bf x},\\xi^{\\mu}\\rangle)^{p}\\,,p>2$ among other choices,with $\\beta>0$ corresponding to the inverse temperature that controls how rapidly the separation-similarity function grows. However, these separation-similarity compositions do not allow for the straightforward simplifications as in eq. (2), except for the power composition. For a general similarity function, the update based on gradient descent over the energy in eq. (3) is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}}E=-\\left.\\frac{d Q(y)}{d y}\\right|_{y=\\sum_{\\mu}F(S[\\xi^{\\mu},\\mathbf{g}(\\mathbf{x})])}\\cdot\\sum_{\\mu=1}^{K}\\left(\\left.\\frac{d F(s)}{d s}\\right|_{s=S[\\xi^{\\mu},\\mathbf{g}(\\mathbf{x})]}.\\cdot\\frac{d S(\\xi^{\\mu},\\mathbf{z})}{d\\mathbf{z}}\\right|_{\\mathbf{z}=\\mathbf{g}(\\mathbf{x})}.\\cdot\\frac{d\\mathbf{g}(\\mathbf{x})}{d\\mathbf{x}}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For example, with $Q(\\cdot)=(1/\\beta)\\log(\\cdot),\\,F(\\cdot)=\\exp(\\beta\\cdot),\\,S[\\pmb{\\xi}^{\\mu},\\mathbf{x}]=\\langle\\pmb{\\xi}^{\\mu},\\mathbf{x}\\rangle$ and $\\mathbf{g}(\\mathbf{x})=\\mathbf{x}/\\|\\mathbf{x}\\|_{2}$ the energy function and the corresponding update? are: ", "page_idx": 4}, {"type": "equation", "text": "$$\nE(\\mathbf{x})=-\\frac{1}{\\beta}\\log\\sum_{\\mu=1}^{K}\\exp(\\beta\\left\\langle\\pmb{\\xi}^{\\mu},\\mathbf{g}(\\mathbf{x})\\right\\rangle),\\;\\nabla_{\\mathbf{x}}E(\\mathbf{x})=-\\frac{\\sum_{\\mu=1}^{K}\\exp(\\beta\\left\\langle\\pmb{\\xi}^{\\mu},\\mathbf{g}(\\mathbf{x})\\right\\rangle)\\xi^{\\mu}}{\\sum_{\\mu=1}^{K}\\exp(\\beta\\left\\langle\\pmb{\\xi}^{\\mu},\\mathbf{g}(\\mathbf{x})\\right\\rangle)}\\cdot\\frac{d\\mathbf{g}(\\mathbf{x})}{d\\mathbf{x}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This form does not directly admit itself to a distributed storage of memories as in eq. (2), and thus, in order to perform the gradient descent on the energy, it is necessary to keep all the memories in their original form. We will try to address this issue by taking inspiration from the area of kernel machines[36]. ", "page_idx": 4}, {"type": "text", "text": "2.1 Random Features for Kernel Machines ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The expressivity of kernel learning usually comes with increased computational complexity both during training and inference, taking time quadratic and linear in the size of the training set respectively. The groundbreaking work of Rahimi and Recht [37] introduced random features to generate explicit featuremaps $\\varphi:\\mathbb{R}^{\\breve{D}}\\to\\mathbb{R}^{Y}$ for the RBF and other shift-invariant kernels4 that approximate the true kernel function - that is $\\langle\\varphi(\\mathbf{x}),\\varphi(\\mathbf{x}^{\\prime})\\rangle\\approx k(\\mathbf{x},\\mathbf{x}^{\\prime})$ . Various such random maps have been developed for shift-invariant kernels [18, 19, 38] and polynomials kernels [39-41]. ", "page_idx": 4}, {"type": "text", "text": "For the RBF kernel and the exponentiated dot-product or EDP kernel $k(\\mathbf{x},\\mathbf{x^{\\prime}})=\\exp(\\langle\\mathbf{x},\\mathbf{x^{\\prime}}\\rangle)$ ,there are usually two classes of random features - trigonometric features and exponential features. For the RBFkernel $k(\\mathbf{x},\\mathbf{x}^{\\prime})=\\exp(-\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}^{2}/2)$ , the trigonometric features [37] are given on the left and the exponential features [18] are on the right: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varphi({\\bf x})=\\frac{1}{\\sqrt{Y}}\\left[\\begin{array}{l}{\\cos(\\langle\\omega^{1},{\\bf x}\\rangle)}\\\\ {\\sin(\\langle\\omega^{1},{\\bf x}\\rangle)}\\\\ {\\cdots,}\\\\ {\\cos(\\langle\\omega^{Y},{\\bf x}\\rangle)}\\\\ {\\sin(\\langle\\omega^{Y},{\\bf x}\\rangle)}\\end{array}\\right],\\qquad\\qquad\\varphi({\\bf x})=\\frac{\\exp(-\\|{\\bf x}\\|_{2}^{2})}{\\sqrt{2Y}}\\left[\\begin{array}{l}{\\exp(+\\langle\\omega^{1},{\\bf x}\\rangle)}\\\\ {\\exp(-\\langle\\omega^{1},{\\bf x}\\rangle)}\\\\ {\\cdots,}\\\\ {\\exp(+\\langle\\omega^{Y},{\\bf x}\\rangle)}\\\\ {\\exp(-\\langle\\omega^{Y},{\\bf x}\\rangle)}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\omega^{\\alpha}\\sim\\mathcal{N}(0,I_{D})\\forall\\alpha\\in\\{1,\\ldots,Y\\}$ are the random projection vectors.5 A random feature map $\\varphi$ for the RBF kernel can be used for the EDP kernel by scaling $\\varphi(\\mathbf x)$ with $\\exp(\\|\\mathbf{x}\\|_{2}^{2}/2)$ . While the trigonometric features ensure that $k(\\mathbf{x},\\mathbf{x})=\\langle\\varphi(\\mathbf{x}),\\varphi(\\mathbf{x})\\rangle=1$ , the exponential features ensure that $\\varphi(\\mathbf{x})\\in\\mathbb{R}_{+}^{2Y}$ whichisei while the random samples $\\pmb{\\omega}^{\\alpha}\\sim\\mathcal{N}(\\mathbf{0},I_{D})$ are supposed to be independent, Choromanski et al. [42] show that the $\\{\\omega^{\\bar{1}},\\ldots,\\omega^{Y}\\}$ can be entangled to be exactly orthogonal to further reduce the variance of the approximation while maintaining unbiasedness. In general, the approximation of the random feature map is $O(\\sqrt{D/Y})$ , implying that a feature space with $Y\\sim O(D/\\epsilon^{2})$ random features will ensure, with high probability, for any x. $\\mathbf{x}^{\\prime}\\in\\mathbb{R}^{D}$ \uff0c $|k(\\mathbf{x},\\mathbf{x}^{\\prime})-\\langle\\varphi(\\mathbf{x}),\\varphi(\\mathbf{x}^{\\prime})\\rangle\\,|\\,\\leq\\,\\epsilon$ Scaling in the kernel functions such as $\\exp(-\\beta\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}^{2}/2)$ or $\\exp(\\beta\\left\\langle\\mathbf{x},\\mathbf{x}^{\\prime}\\right\\rangle)$ can be handled with the aforementioned random feature maps $\\varphi$ by applying them to $\\sqrt{\\beta}\\mathbf{x}$ with $\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\varphi(\\sqrt{\\beta}\\mathbf{x}^{\\prime})\\right\\rangle\\approx$ $\\exp(-\\beta\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}^{2}/2)$ ", "page_idx": 4}, {"type": "text", "text": "3  DrDAM with Random Features ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Revisting the general energy function in eq. (3), if we have available an explicit mapping $\\varphi:\\mathbb{R}^{D}\\to$ $\\mathbb{R}^{Y}$ such that $\\langle\\varphi(\\xi^{\\mu}),\\varphi(\\bar{\\bf x})\\rangle\\approx F(S[\\xi^{\\mu},\\bar{\\bf x}])$ , then we can simplify the general energy function in eq. (3) to ", "page_idx": 4}, {"type": "equation", "text": "$$\nE(\\mathbf{x})\\approx\\hat{E}(\\mathbf{x})=-Q\\left(\\sum_{\\mu=1}^{K}\\left\\langle\\varphi(\\pmb{\\xi}^{\\mu}),\\varphi(\\mathbf{g}(\\mathbf{x}))\\right\\rangle\\right)=-Q\\left(\\left\\langle\\sum_{\\mu=1}^{K}\\varphi(\\pmb{\\xi}^{\\mu}),\\varphi(\\mathbf{g}(\\mathbf{x}))\\right\\rangle\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denoting $\\textstyle\\mathbf{T}=\\sum_{\\mu}\\varphi({\\boldsymbol{\\xi}}^{\\mu})$ , we can write a simplified general update step for any input $\\mathbf{x}$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}}{\\hat{E}}=-\\left.{\\frac{d Q(s)}{d s}}\\right|_{s=\\langle\\varphi(\\mathbf{g}(\\mathbf{x})),\\mathbf{T}\\rangle}\\cdot\\left(\\left.{\\frac{\\varphi(\\mathbf{z})}{d\\mathbf{z}}}\\right|_{\\mathbf{z}=\\mathbf{g}(\\mathbf{x})}^{\\top}\\mathbf{T}\\right)\\cdot{\\frac{d\\mathbf{g}(\\mathbf{x})}{d\\mathbf{x}}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d\\varphi(\\mathbf{x})/d\\mathbf{x}\\in\\mathbb{R}^{Y\\times D}$ is the gradient of the feature map with respect to its input. In the presence of such an explicit map $\\varphi$ , we can distribute the memory in a MrDAM into the single $Y$ -dimensional vector $\\mathbf{T}$ , and be able to apply the update in eq. (10). We can then use the random feature based energy gradient $\\nabla_{\\mathbf{x}}\\hat{E}(\\mathbf{x})$ instead of the true energy gradient $\\nabla_{\\mathbf{x}}E(\\mathbf{x})$ in the energy gradient descent step in eq. (5).6 We name this scheme \u201cDistributed representation for Dense Associative Memory\"\" or DrDAM, and we compare the computational costs of DrDAM with the \\*Memory representation of Dense Associative Memory\u201d or MrDAM in the following: ", "page_idx": 5}, {"type": "text", "text": "Proposition 1. With access to the $K$ memories $\\{\\pmb{\\xi}^{\\mu}\\in\\mathbb{R}^{D},\\mu\\in[K]\\}$ \uff0cMrDAMtakes $O(L K D)$ time and $O(K D)$ peak memory for $L$ energy gradient descent steps (orlayers) as defined in eq. (5) with the true energy gradient $\\nabla_{\\mathbf{x}}E(\\mathbf{x})$ ", "page_idx": 5}, {"type": "text", "text": "Naively, the random feature based DrDAM would require $O(D Y)$ memory to store the random vectors and the $\\nabla_{\\mathbf{x}}\\varphi(\\mathbf{x})$ matrix. However, we can show that we can generate the random vectors on demand to reduce the overall peak memory to just $O(Y)$ . The various procedures in DrDAM are detailed in Algorithm 1. The RF subroutine generates the random feature for any memory or input. The ProcMems subroutine consolidates all the memories into a single $\\mathbf{T}\\in\\mathbb{R}^{Y}$ vector. The GradComp subroutine compute the gradient $\\nabla_{\\mathbf x}\\hat{E}$ . The following are the computational complexities of these procedures: ", "page_idx": 5}, {"type": "text", "text": "Proposition 2. The RF subroutine in Algorithm 1 takes $O(\\bar{D}Y)$ timeand $O(D+Y)$ peakmemory. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3. ProcMems in Algorithm 1 takes $O(D Y K)$ timeand $O(D+Y)$ peakmemory. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4. GradComp in Algorithm 1 takes $O(\\bar{D}(Y+D))$ timeand $O(D+Y)$ peakmemory. ", "page_idx": 5}, {"type": "text", "text": "Thus, the computational complexities of DrDAM neural dynamics are (see appendix F.1 for proof and discussions): ", "page_idx": 5}, {"type": "table", "img_path": "164QnJsYjF/tmp/52477d80b7ff12827d31c097a87d59100269aa4675e805da0cf18feeb1fc98b8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Ineorem 1.wutn aranaom jeature map $\\varphi$ utiz  \ning. $Y$ random projections $\\{\\varphi_{\\alpha},\\alpha\\;\\;\\in\\;\\;\\{1,\\dots,Y\\}\\}$   \nand $K$ memories $\\{\\xi^{\\mu}\\;\\;\\in\\;\\;\\|\\dot{\\mathbb{R}}^{D},\\mu\\;\\;\\in\\;\\;\\{1,\\ldots,K\\}\\}$ .the random-feature based DrDAM takes $O\\left(D\\left(Y K+L(Y+D)\\right)\\right)$ time and $O(Y+D)\\,,$ peakmemoryfor $L$ energy gradient descent steps (or layers) as defined in eq. (5) with the random feature based approximation gradient $\\nabla_{\\mathbf{x}}\\hat{E}(\\mathbf{x})$ defined in eq. (10). ", "page_idx": 5}, {"type": "text", "text": "However, note that the memory encoding only needs to be done once, while the same $\\mathbf{T}$ canbe utilized for $L$ steps of energy gradient steps for multiple input, and the cost of ProcMems is amortized over these multiple inputs. We also show that the computational costs of the inclusion of a new memories $\\xi$ ", "page_idx": 5}, {"type": "text", "text": "Proposition 5. The inclusion of a new memory ${\\boldsymbol{\\xi}}\\in\\mathbb{R}^{D}$ toaDrDAMwith $K$ memoriesdistributedin $\\mathbf{T}\\in\\mathbb{R}^{Y}$ takes $O(D Y)$ timeand $O(D+Y)$ peakmemory. ", "page_idx": 5}, {"type": "text", "text": "The above result shows that inclusion of new memories correspond to constant time and memory irrespective of the number of memories in the current DenseAM. Next, we study the divergence between the output of a $L$ -layered MrDAM using the energy descent in eq. (5) with the true gradient in eq. (6) and that of DrDAM using the random feature based gradient in eq. (10). ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Consider the following energy function with $K$ memories $\\{\\pmb{\\xi}^{\\mu}\\in\\mathbb{R}^{D},\\mu\\in\\{1,\\ldots,K\\}\\}$ andinversetemperature $\\beta>0$ ", "page_idx": 6}, {"type": "equation", "text": "$$\nE(\\mathbf{x})=-\\frac{1}{\\beta}\\log\\left(\\sum_{\\mu=1}^{K}\\exp(-\\beta/2\\|\\pmb{\\xi}^{\\mu}-\\mathbf{x}\\|_{2}^{2})\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We further make the following assumptions: $(A l)A l l$ memories $\\xi^{\\mu}$ and inputs $\\mathbf{x}$ lie in $\\mathcal{X}=[0,{^1}/\\sqrt{D}]^{D}$ (A2) Using a random feature map $\\varphi:\\mathbb{R}^{D}\\to\\mathbb{R}^{Y}$ using $Y$ random feature maps, for any $\\mathbf{x},\\mathbf{x^{\\prime}}\\in\\mathbb{R}^{d}$ there is a constant $C_{1}>0$ such that $\\left|\\exp(\\|\\mathbf{x}-\\mathbf{x}^{\\prime}\\|_{2}^{2}/2)-\\langle\\varphi(\\mathbf{x}),\\varphi(\\mathbf{x}^{\\prime})\\rangle\\right|\\leq C_{1}\\sqrt{D/Y}$ .Given an input $\\mathbf{x}\\in\\mathcal{X}$ ,let $\\mathbf{x}^{(L)}$ be the output of the MrDAM defined by the energy function in eq. (11) using the true energy gradient in eq. (6) and $\\hat{\\mathbf{x}}^{(L)}$ be the output of DrDAM with approximate gradient in eq. (10) using the random feature map $\\varphi$ using a constant step-size of $\\eta>0$ in (5). Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\Vert\\mathbf{x}^{(L)}-\\hat{\\mathbf{x}}^{(L)}\\right\\Vert_{2}\\leq2\\eta L C_{1}K e^{\\beta E(\\mathbf{x})}\\sqrt{D/Y}\\left(\\frac{1-\\left(\\eta L(1+2K\\beta e^{\\beta/2})\\right)^{L}}{1-\\eta L(1+2K\\beta e^{\\beta/2})}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Assumption (A1) just ensures that all the memories and inputs have bounded norm, and can be achieved via translating and scaling the memories and inputs. Assumption (A2) pertains to the approximation introduced in the kernel function evaluation with the random feature map, and is satisfied (with high probability) based on results such as Rahimi and Recht [37, Claim 1] and Choromanski et al. [18, Theorem 4]. The above result precisely characterizes the effect on the divergence $\\lVert\\mathbf{x}^{(L)}-\\hat{\\mathbf{x}}^{(L)}\\rVert$ of the (i) initial energy of the input $E(\\mathbf{x})$ - lower is beter, (ii) the inverse temperature $\\beta-$ lower is better, (ii) the number of memories $K$ - lower is better, (iv) the ambient data dimensionality $D$ - lower is better, (v) the number of random features $Y$ -- higher is better, and (vi) the number of layers $L-$ lower is better. The proof and further discussion are provided in appendix F.2. Note that theorem 2 analyzes the discretized system, but as the step-size $\\eta\\rightarrow0$ we approach the fully contracting continuous model. An appropriate choice for the energy descent stepsize $\\eta$ simplifies the above result, bounding the divergence to $O(\\sqrt{D/Y})$ ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Under the conditions and definitions of theorem 2, if we set the step size $\\eta\\:=\\:$ L(1+2K Bes/2) with C2 < 1, the divergence is bounded as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{x}^{(L)}-\\hat{\\mathbf{x}}^{(L)}\\right\\|_{2}\\leq\\frac{C_{1}C_{2}e^{\\beta(E(\\mathbf{x})-1/2)}}{\\beta(1-C_{2})}\\sqrt{D/Y}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "These  above  results  can  be  extended  to  the  EDP  based  energy  function $\\begin{array}{r l}{E(\\mathbf{x})}&{{}=}\\end{array}$ $\\begin{array}{r}{-1/\\beta\\log\\sum_{\\mu}\\exp(\\beta\\left\\langle\\pmb{\\xi}^{\\mu},\\mathbf{x}\\right\\rangle)+1/2\\|\\mathbf{x}\\|_{2}^{2}}\\end{array}$ using the same proof technique. ", "page_idx": 6}, {"type": "text", "text": "4  Empirical evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To be an accurate approximation of the traditional MrDAM, DrDAM must empirically satisfy the following desiderata for all possible queries and at all configurations for inverse temperature $\\beta$ and pattern dimension $D$ ", "page_idx": 6}, {"type": "text", "text": "$({\\bf D1})$ for the same query, DrDAM must predict similar energies and energy gradients as MrDAM; and $\\left(\\mathbf{D}2\\right)$ for the same initial query, DrDAM must retrieve similar fixed points as MrDAM. ", "page_idx": 6}, {"type": "text", "text": "However, in our experiments we observed that the approximation quality of DrDAM is strongly affected by the choice of $\\beta$ and that the approximation quality decreases the further the query patterns are from the stored memory patterns, as predicted by theorem 2. We characterize this behavior in the following experiments using the trigonometric \u201cSinCos\" basis function, which performed best in our ablation experiments (see appendix C), but note that the choice of the random features do play a significant role in the interpretations of these results. ", "page_idx": 6}, {"type": "text", "text": "4.1  (D1) How accurate are the energies and gradients of DrDAM? ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Figure 3 evaluates how well DrDAM, configured at different feature sizes $Y$ , approximates the energy and energy gradients of MrDAM configured with different inverse temperatures $\\beta$ and storing random binary patterns of dimension $D$ . The experimental setup is described below. ", "page_idx": 6}, {"type": "image", "img_path": "164QnJsYjF/tmp/bd65f4895d0c2107ae198d53c5136721136fdc5fd16fac67629405d0843b3285.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "164QnJsYjF/tmp/2620b9f0ea5473312b70769d6b6950d47a62800a55910df8c160e5922c92b19d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: DrDAM produces better approximations to the energies and gradients of MrDAM when the queries are closer to the stored patterns. Approximation quality improves with larger feature dimension $Y$ , but decreases with higher $\\beta$ and higher pattern dimension $D$ . Approximation error is computed on 500 stored binary patterns normalized between $\\{0,\\frac{1}{\\sqrt{D}}\\}$ . The Mean Approximation Errors (MAE, eq. (14)) is taken over 500 queries initialized: at stored patterns (i.e., queries equal the stored patterns), near stored patterns (i.e., queries equal the stored patterns where $10\\%$ of the bits have been flipped), and randomly (i.e., queries are random and far from stored patterns). Error bars represent the standard error of the mean but are visible only at poor approximations. Red horizontal lines represent the expected error of random energies and gradients. The theoretical error upper bounds of eq. (13) (dark curves on the gradient errors in the right plot only) show a tight fit to empirical results at low $\\beta$ and $D$ and are only shown if predictions are \u201cbetter than random\". The shaded area shows the difference between the theoretical bound and the empirical results. ", "page_idx": 7}, {"type": "text", "text": "We generated $2K=1000$ unique, binary patterns (where each value is normalized to be $\\{0,\\frac{1}{\\sqrt{D}}\\})$ and stored $K=500$ of them into the memory matrix $\\Xi$ of MrDAM. We denote these stored patterns as $\\xi^{\\mu}\\in$ $\\{0,\\frac{1}{\\sqrt{D}}\\}^{D}$ \uff0c $\\mu\\in[|K|]$ where $D$ is a hyperparameter controlled by the experiment. For a given $\\beta$ the memory matrix is converted into the featurized memory vector $\\begin{array}{r}{T_{\\alpha}:=\\sum_{\\mu}\\varphi_{\\alpha}(\\xi^{\\mu})}\\end{array}$ from eq. (9), where $\\alpha\\in[2Y]$ . The remaining paterns are treated as the \u201crandom queries\" $\\mathbf{x}_{\\mathrm{far}}^{b}$ $b\\in\\mathbb{[K]}$ (i.e., queries that are far from the stored patterns). Finally, in addition to evaluating the energy at these random queries and at the stored patterns, we also want to evaluate the energy at queries $\\mathbf{\\bar{x}}_{\\mathrm{near}}^{\\bar{b}}$ that are \"near' the stored patterns; thus, we take each stored pattern $\\xi^{\\mu}$ and perform bit-flips on $0.1D$ of its entries. ", "page_idx": 7}, {"type": "text", "text": "For each set of queries $\\mathbf{x}^{b}\\in\\{\\pmb{\\xi}^{b},\\mathbf{x}_{\\mathrm{near}}^{b},\\mathbf{x}_{\\mathrm{far}}^{b}\\}$ $b\\in[\\![K]\\!]$ , and choice of $\\beta,Y$ and $D$ , we compute the Mean Approximation Error (MAE) between MrDAM's energy $E_{b}:=E(\\mathbf{x}^{b};\\beta,\\Xi)$ (whose gradient matrix is denoted $\\nabla_{\\mathbf{x}}E_{b},$ and DrDAM's energy $\\hat{E}_{b}:=\\hat{E}(\\mathbf{x}^{b};\\beta,\\mathbf{T})$ (whose gradient is denoted $\\nabla_{\\mathbf{x}}\\hat{E}_{b}\\grave{,}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{MAE}_{\\mathrm{Energy}}=\\frac{1}{K}\\sum_{b\\in[K]}\\left|E_{b}-\\hat{E_{b}}\\right|,~\\mathrm{and}~~\\mathrm{MAE}_{\\mathrm{Gradient}}=\\frac{1}{K}\\sum_{b\\in[K]}\\left\\|\\nabla_{\\mathbf{x}}E_{b}-\\nabla_{\\mathbf{x}}\\hat{E_{b}}\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We found it useful to visualize the results using log-scale and to compare the errors against the expected error of a \u201crandom guess\" of the energy/gradients (horizontal red dashed line in each plot of fig. 3). The \u201crandom guess error\u2019 was empirically computed by sampling a new set of random queries $\\mathbf{x}_{\\mathrm{guess}}^{b}$ $b\\in[\\![K]\\!]$ (independent of thereference queries) and computing theMAEbetween the standard energy on the reference queries vs. the approximate energies on the random queries. This error was averaged across $Y$ for each $\\beta$ ; the highest average error across all $\\beta\\mathrm{s}$ is plotted. ", "page_idx": 7}, {"type": "text", "text": "Observation 1: DrDAM approximations are best for queries near stored patterns DrDAM approximations for both the energy and energy gradients are better the closer the query patterns are to the stored patterns. In this regime, approximation accuracy predictably improves when increasing ", "page_idx": 7}, {"type": "image", "img_path": "164QnJsYjF/tmp/5133523864451906985baf917d8877f82afbbecc6191f170334ae53b0f1710c6.jpg", "img_caption": ["A) Hamming Error\\* of retrievals when queries are.. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: A) Retrieval errors predictably follow the approximation quality of fig. 3. Error is lowest at/near stored patterns but is completely random when energy and gradient approximations are poor, i.e., at high values of $\\beta$ and $D$ . Note that error improves across $Y$ but follows a different (and noisier) trace than the corresponding approximations for energy and gradient in fig. 3 due to error accumulating over multiple update steps. B) DrDAM's approximation quality improves as $Y$ increases (visible at low $\\beta$ ), but larger $Y$ 's are needed for good approximations to the DAM's fixed points at higher $\\beta$ 's. (Left) The same corrupted query from CIFAR-10 where bottom $50\\%$ is masked is presented to DAM's with different $\\beta$ 's. (Middle) The fixed points of DrDAM for each $\\beta$ at different sizes $Y$ of the feature space. (Right) The \u201cground truth\u201d fixed point of MrDAM. The top $50\\%$ of pixels are clamped throughout the dynamics. ", "page_idx": 8}, {"type": "text", "text": "the value for $Y$ within \u201creasonable\u201d values (i.e., values corresponding into sizes of featurized queries and memories that can operate within 46GB of GPU memory). ", "page_idx": 8}, {"type": "text", "text": "Observation 2: DrDAM approximations worsen as inverse temperature $\\beta$ increasesAcross nearly all experiments, DrDAM approximations worsen as $\\beta$ increases. At queries near the stored patterns, $\\beta=50$ has an energy error approximately $10\\times$ that of $\\beta=30$ and $100\\times$ that of $\\beta=10$ across all $Y$ . At high $D$ and when queries are far from the patterns, the error of $\\beta=50$ approaches $1000\\times$ the error of $\\beta=10$ . This observation similarly holds for the errors of corresponding gradients, corroborating the statement of theorem 2. ", "page_idx": 8}, {"type": "text", "text": "Observation 3: DrDAM approximations break at sufficiently high values of $D$ and $\\beta$ In general, DrDAM's approximation errors remain the same across choices for $D$ , especially when the queries are near the stored patterns. However, when both $\\beta$ and $D$ are sufficiently large (e.g., $\\beta\\geq40$ and $D\\ge100$ in fig. 3), increasing the value of $Y$ does not improve the approximation quality: DrDAM continues to return almost random gradients and energies. We explore this phenomenon more in $\\S\\,^{4.2}$ in the context of the retrievability of stored patterns. ", "page_idx": 8}, {"type": "text", "text": "4.2  (D2) How accurate are the memory retrievals using DrDAM? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Memory retrieval is the process by which an initial query. $\\mathbf{x}^{(0)}$ descends the energy function and is transformed into a fixed point of the energy dynamics. This process can be described by the discrete update rule in eq. (5), where $E$ can represent either MrDAM's energy or the approximate energy of DrDAM. A memory is said to be \u201cretrieved\" when $|E(\\mathbf{x}^{(L)})-E(\\mathbf{x}^{(L-1)})|<\\varepsilon$ for some small $\\varepsilon>0$ at which point $\\mathbf{x}^{(\\bar{L}-1)}\\approx\\mathbf{x}^{(L)}=:\\mathbf{x}^{\\star}$ is declared to be the retrieved memory after $L$ iterations because $\\mathbf{x}^{\\star}$ lives at a local minimum of the energy function $E$ ", "page_idx": 8}, {"type": "text", "text": "Quantifying retreal error  Given the same intial queries $\\mathbf{x}^{(0)}\\in\\{0,\\frac{1}{\\sqrt{D}}\\}^{D}$ we want to quantify the difference between the fixed points $\\hat{\\mathbf{x}}^{\\star}$ retrieved by descending DrDAM's approximate energy and the fixed points $\\mathbf{x}^{\\star}$ retrieved by descending the energy of MrDAM. We follow the experimental setup of $\\S\\,{4.1}$ , only this time we run full memory retrieval dynamics until convergence. ", "page_idx": 8}, {"type": "text", "text": "Note that since energy uses an L2-similarity kernel, memory retrieval is not guaranteed to return binary values. Thus, we binarize $\\mathbf{x}^{\\star}$ by assigning each entry to its nearest binary value before computing the normalized Hamming approximation error $\\Delta_{H}$ ,i.e., ", "page_idx": 9}, {"type": "equation", "text": "$$\n[x]:=\\left\\{{\\frac{1}{\\sqrt{D}}},\\begin{array}{l l}{x\\geq{\\frac{1}{2{\\sqrt{D}}}}}\\\\ {\\quad}&{{\\mathrm{otherwise}}}\\end{array}}\\right.,{\\mathrm{and}}\\quad\\quad\\Delta_{H}:={\\frac{1}{\\sqrt{D}}}\\sum_{i\\in[D]}\\left|\\left[\\mathbf{x}_{i}^{\\star}\\right]-\\left[{\\hat{\\mathbf{x}}}_{i}^{\\star}\\right]\\right|.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The choice of normalized Hamming approximation error $\\Delta_{H}$ on our binary data is equivalent to the squared L2 erorn thelf sideofourboud inq. 13 uptoalinear salng of. ", "page_idx": 9}, {"type": "text", "text": "Figure 4A shows the results of this experiment. Many observations from $\\S\\ 4.1$ translate to these experiments: we notice that retrieval is random at high $\\beta$ and $D$ , and that retrievals are of generally higher accuracy nearer the stored patterns. However, we notice that high $\\beta$ values can retrieve better approximations than lower values of $\\beta$ when the queries are at or near stored patterns. Additionally, for sufficiently high $\\beta$ (e.g., see $D\\,=\\,1000$ $\\beta\\,=\\,50$ near stored patterns), this accompanies an interesting \u201cthresholding\u201d behavior for $Y$ where retrieval error starts to improve rapidly once $Y$ reaches a minimal threshold. This behavior is corroborated in the high $D$ regime in fig. 4B. ", "page_idx": 9}, {"type": "text", "text": "Visualizing retrieval error Figure 4B shows what retrieval errors look like qualitatively. We stored $K=10$ random images from CIFAR10 [43] into the memory matrix of MrDAM, resulting in patterns of size $D=3\\times32\\times32=3072$ , and compared retrievals using $\\beta\\mathrm{s}$ that produced meaningful image results with MrDAM. To keep $\\beta$ values consistent with our previous experiments, each pixel was normalized to the continuous range between O and $\\textstyle{\\frac{1}{\\sqrt{D}}}$ s.t. $\\xi_{i}^{\\mu}\\overset{\\cdot}{\\in}[0,\\frac{1}{\\sqrt{D}}]$ with $\\mu\\in[|K|]$ and $i\\in[D]$ ", "page_idx": 9}, {"type": "text", "text": "From $\\S\\ 4.1$ and fig. 4A, we know that approximate retrievals are inaccurate at high $\\beta$ and high $D$ if the query is far from the stored patterns. However, this is exactly the regime we test when retrieving images in fig. 4B. The visible pixels (top half of the image) are clamped while running the dynamics until convergence. Retrieved memories at different configurations for DrDAM are plotted against their corresponding MrDAM retrievals in fig. 4B. ", "page_idx": 9}, {"type": "text", "text": "As $\\beta$ increases, insufficiently large values of $Y$ fail to retrieve meaningful approximations to the dynamics of MrDAM. We observe that image completions generally become less noisy as $Y$ increases, but with diminishing improvement in perceptible quality after some threshold where DrDAM goes from predicting noise to predicting meaningful image completions. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our study is explicitly designed to characterize where DrDAM is a good approximation to the energies and dynamics of MrDAM. In pushing the limits of the distributed representation, we discovered that DrDAM is most accurate when: (1) query patterns are nearer to the stored patterns; (2) $\\beta$ is lower; and (3) $Y$ is large. Error bounds for these situations are explicitly derived in theorem 2 and empirically tested in $\\S\\ 4$ ", "page_idx": 9}, {"type": "text", "text": "We have explored the use of distributed representations via random feature maps in DenseAMs. We have demonstrated how this can be done efficiently, and we precisely characterized how it performs the neural dynamics relative to the memory representation DenseAMs. Our theoretical results highlight the factors playing a role in the approximation introduced by the distributed representations, and our experiments validate these theoretical insights. As future work, we intend to explore how such distributed representations can be leveraged in hierarchical associative memory networks [44, 45], which can have useful inductive biases (e.g., convolutions, attention), and allow extensions with multiple hidden layers. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1]  John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554-2558, 1982.   \n[2]  Dmitry Krotov and John J Hopfield. Dense associative memory for pattern recognition. Advances in neural information processing systems, 29, 2016.   \n[3] Mete Demircigil, Judith Heusel, Matthias Lowe, Sven Upgang, and Franck Vermet. On a model of associative memory with huge storage capacity. Journal of Statistical Physics, 168(2):288- 299, 2017.   \n[4]  Hubert Ramsauer, Bernhard Schaf, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Ader, David Kreil, Michael K Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield networks is all you need. In International Conference on Learning Representations, 2021. URL https : //openreview.net/forum? id=tL89RnzIiCd.   \n[5]  Dmitry Krotov and John J Hopfield. Large associative memory problem in neurobiology and machine learning. In International Conference on Learning Representations, 2021.   \n[6] Beren Millidge, Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, and Rafal Bogacz. Universal hopfield networks: A general framework for single-shot associative memory models. In International Conference on Machine Learning, pages 15561-15583. PMLR, 2022.   \n[7]  Thomas F Burns and Tomoki Fukai. Simplicial hopfield networks. In The Eleventh International Conference on Learning Representations, 2022.   \n[8]  Dmitry Krotov. A new frontier for hopfield networks. Nature Reviews Physics, 5(7):366-367, 2023.   \n[9]  Benjamin Hoover, Hendrik Strobelt, Dmitry Krotov, Judy Hoffman, Zsolt Kira, and Duen Horng Chau. Memory in Plain Sight: Surveying the Uncanny Resemblances of Associative Memories and Diffusion Models, 2023. URL https : //arxiv. org/abs/2309 .16750.   \n[10] Luca Ambrogioni. In Search of Dispersed Memories: Generative Diffusion Models Are Associative Memory Networks. Entropy, 26(5), 2024. ISSN 1099-4300. doi: 10.3390/ e26050381. URL https: //www.mdpi.com/1099-4300/26/5/381.   \n[11]  Ya Le and Xuan S. Yang. Tiny imagenet visual recognition challenge. 2015. URL https : //api.semanticscholar.org/CorpusID:16664790.   \n[12]  Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20:273-297, 1995. URL https://link.springer.com/content/pdf/10.1007/bf00994018.pdf.   \n[13] Ryan R Curtin, Parikshit Ram, and Alexander G Gray. Fast exact max-kernel search. In Proceedings of the 2013 SIAM International Conference on Data Mining, pages 1-9. SIAM, 2013. URL https : //doi .org/10.1137/1.9781611972832.1.   \n[14]  Ryan R Curtin and Parikshit Ram. Dual-tree fast exact max-kernel search. Statistical Analysis and Data Mining: The ASA Data Science Journal, 7(4):229-253, 2014. URL https : //doi . org/10.1002/sam.11218.   \n[15]  Christina Leslie, Eleazar Eskin, and William Stafford Noble. The spectrum kernel: A string kernel for svm protein classification. In Biocomputing 2002, pages 564-575. World Scientific, 2001. URL https: //www.worldscientific.com/doi/abs/10.1142/9789812799623_ 0053.   \n[16] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47-i56, 2005. URL https: //dl.acm.org/doi/abs/10.1093/ bioinformatics/bti1007.   \n[17] K-R Muller, Alexander J Smola, Gunnar Ratsch, Bernhard Scholkopf, Jens Kohlmorgen, and Vladimir Vapnik. Predicting time series with support vector machines. In International conference on artificial neural networks, pages 99-1004. Springer, 1997. URL https : //link.springer.com/chapter/10.1007/BFb0020283.   \n[18] Krzysztof Choromanski, Valeri Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. Proceedings of ICLR, 2020. URL https : //arxiv . org/pdf /2009. 14794.pdf.   \n[19] Deepali Jain, Krzysztof Marcin Choromanski, Kumar Avinava Dubey, Sumeet Singh, Vikas Sindhwani, Tingnan Zhang, and Jie Tan. Mnemosyne: Learning to train transformers with transformers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id $=$ Fdfyga5i0A.   \n[20] Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen Homg Chau, Mohammed Zaki, and Dmitry Krotov. Energy transformer. Advances in Neural Information Processing Systems, 36, 2024. URL https://proceedings.neurips cc/paper_ files/paper/2023/file/57a9b97477b67936298489e3c1417b0a-Paper-Conference. pdf.   \n[21] Georgios Iatropoulos, Johanni Brea, and Wulfram Gerstner. Kernel memory networks: A unifying framework for memory modeling. Advances in neural information processing systems, 35:35326-353382022.URLhttps://proceedings.neurips.cc/paper_files/paper/ 2022/file/e55d081280e79e714debf2902e18eb69-Paper-Conference.pdf.   \n[22] Jerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, and Han Liu. Nonparametric modern hopfield models. arXiv preprint arXiv:2404.03900, 2024. URL https:/ /arxiv . org/pdf / 2404.03900.pdf.   \n[23] Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On sparse modern hopfield model. Advances in Neural Information Processing Systems, 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 57bc0a850255e2041341bf74c7e2b9fa-Paper-Conference.pdf.   \n[24] Saul Jose Rodrigues Dos Santos, Vlad Niculae, Daniel C Mcnamee, and Andre Martins. Sparse and structured hopfield networks. In Proceedings of the 41st International Conference on Machine Learning,volume 235 of Proceedings of Machine Learning Research, pages 43368-43388. PMLR, 21-27 Jul 2024. URL https: //proceedings .mlr.press/v235/ santos24a.html.   \n[25] Hamza Chaudhry, Jacob Zavatone-Veth, Dmity Krotov, and Cengiz Pehlevan. Long sequence hopfield memory. Advances in Neural Information Processing Systems, 36, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ aa32ebcdd2ce1bed4ef7f456fc8fa5c1-Paper-Conference.pdf.   \n[26] Dennis Wu, Jerry Yao-Chieh Hu, Weijian Li, Bo- Yu Chen, and Han Liu. STanhop: Sparse tandem hopfield model for memory-enhanced time series prediction. In The Twelfth International Conference on Learning Representations, 2024. URL https : //openreview.net/forum? id=6iwg437cZs.   \n[27] Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, and Han Liu. On computational limits of modern hopfield models: A fine-grained complexity analysis. In Proceedings of the 4lst International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 19327-19343. PMLR, 2i-27 Jul 2024. URL https: //proceedings .mir.press/ v235/hu24j.html.   \n[28] Dennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, and Han Liu. Uniform memory retrieval with larger capacity for modern hopfield models. arXiv preprint arXiv:2404.03900, 2024. URL https://arxiv.org/pdf/2404.03827.pdf.   \n[29] Matteo Negri, Clarissa Lauditi, Gabriele Perugini, Carlo Lucibello, and Enrico Maria Malatesta. Random feature hopfield networks generalize retrieval to previously unseen examples. In Associative Memory & Hopfeld Networks in 2023, 2023.   \n[30] Matteo Negri, Clarissa Lauditi, Gabriele Perugini, Carlo Lucibello, and Enrico Malatesta. Storage and learning phase transitions in the random-features hopfield model. Physical Review Letters, 131(25):257301, 2023.   \n[31] Bermard W Silverman.  Density estimation for statistics and data analysis. Chapman & Hall/CRC, 1998.   \n[32] Bishwajit Saha, Dmitry Krotov, Mohammed J Zaki, and Parikshit Ram. End-to-end differentiable clustering with associative memories. In Proceedings of the 4oth International ConferenceonMacheLaingolm20fProcdingfMacheLaningRearch 29649-29670. PMLR, 23-29 Jul 2023. URL https ://proceedings .mlr.press/v202/ saha23a.html.   \n[33] Rylan Schaeffer, Nika Zahedi, Mikail Khona, Dhruv Pai, Sang Truong, Yilun Du, Mitchell Ostrow, Sarthak Chandra, Andres Carranza, Ila Rani Fiete, Andrey Gromov, and Sanmi Koyejo. Bridging associative memory and probabilistic modeling, 2024. URL https : //arxiv . org/ abs/2402.10202.   \n[34] Leo Kozachkov, Ksenia V Kastanenka, and Dmitry Krotov. Building transformers from neurons and astrocytes. Proceedings of the National Academy of Sciences, 120(34):e2219150120, 2023.   \n[35] Leo Kozachkov, Jean-Jacques Slotine, and Dmitry Krotov. Neuron-astrocyte associative memory. arXiv preprint arXiv:2311.08135, 2023.   \n[36] L\u00e9on Bottou. Large-scale kernel machines. MIT press, 2007.   \n[37]  Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in neural information processing systems, 2007. URL https : //proceedings . neurips . cc/ paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf.   \n[38]  Valeri Likhosherstov, Krzysztof Marcin Choromanski, Kumar Avinava Dubey, Frederick Liu, Tamas Sarlos, and Adrian Weller. Dense-exponential random features: Sharp positive estimators of the gaussian kernel. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id $\\equiv$ SOxrBMFihS.   \n[39] Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels. In Artificial intelligence and statistics, pages 583-591. PMLR, 2012. URL https : //proceedings .mlr . press/v22/kar12/kar12.pdf.   \n[40] Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,pages 239-247, 2013. URL https: //di.acm.org/doi/pdf/10.1145/ 2487575.2487591.   \n[41] Raffay Hamid, Ying Xiao, Alex Gittens, and Dennis DeCoste. Compact random feature maps. In International conference on machine learning, pages 19-27. PMLR, 2014. URL https://proceedings.mlr.press/v32/hamid14.pdf.   \n[42] Krzysztof M Choromanski, Mark Rowland, and Adrian Weller. The unreasonable effectiveness of structured random orthogonal embeddings. Advances in neural information processing systems, 30, 2017. URL https://proceedings .neurips .cc/paper/2017/file/ bf8229696f7a3bb4700cfddef19fa23f-Paper.pdf.   \n[43]  Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[44] Dmitry Krotov. Hierarchical associative memory, 2021. URL https : //arxiv. org/abs/ 2107.06446.   \n[45]  Benjamin Hoover, Duen Horng Chau, Hendrik Strobelt, and Dmitry Krotov. A universal abstraction for hierarchical hopfield networks. In The Symbiosis of Deep Learning and Differential Equations Il, 2022. URL https : //openreview.net /forum?id=SAv3nhzNWhw. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[46]  Peter W Frey and David J Slate. Letter recognition using holland-style adaptive classifiers. Machine learning,6:161-182, 1991. ", "page_idx": 13}, {"type": "text", "text": "[47] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http: //github.com/google/jax.   \n[48]  Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 4Oth International Conference on Machine Learning,volume 202 of Proceedings of Machine Learning Research, pages 19565-19594. PMLR, 23-29 Jul 2023. URL https: //proceedings.mlr.press/v202/li23l.html. ", "page_idx": 13}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this paper, we have explored the use of distributed representations via random feature maps in DenseAMs. However, we are only scratching the surface of opportunities that such distributed representations bring to DenseAMs. There are various aspects we do not cover: (i) We do not cover the ability of these distributed representations to provide (probably lossy) compression. (i) We do not study the properties of DrDAM relative to MrDAM when DrDAM is allowed to have different step sizes and number of layers than MrDAM. A further limitation of our work is the limited number of datasets on which we have characterized the performance of DrDAM. ", "page_idx": 14}, {"type": "text", "text": "B  Approximation error when increasing the number of stored patterns in DrDAM ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "$\\S\\ 4.1$ validated eq. (13), confirming that approximation error decreases as the number of random features $Y$ increases under constant number of stored patterns $K$ . We can also consider a related but different question: under constant number of random features $Y$ , how does approximation error behave when increasing the number of stored patterns $K?$ Intuitively, DrDAM's approximation should be good when a small number of patterns are stored in the network, and this approximation should worsen as we increase the number of stored patterns. ", "page_idx": 14}, {"type": "text", "text": "Figure 5 validates this intuition empirically, with the caveat that random queries generally improve in accuracy because the probability of being near a stored patterns (a regime that generally leads to higher accuracy of retrievals, see $\\S\\ 4$ ) increases as we store more patterns into the network. For this experiment, $Y=2e5$ was held constant across all experiments and each plotted approximation error is averaged over a number of queries equal to the number of stored patterns $K$ . The experimental design otherwise exactly replicates that of fig. 3. ", "page_idx": 14}, {"type": "table", "img_path": "164QnJsYjF/tmp/c350dce167cf26e33679b60bc4be9e62128353e2be12a1ca7b570666adac34cc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "", "img_caption": ["Figure 5: Mean Approximation Error (MAE, eq. (14)) increases as the number of stored patterns $K$ increases (except at random starting positions, where more stored patterns increases the probability that a random query is closer to a memory, a regime that leads to higher accuracy of the retrievals, see fig. 3),keeping $Y=2e5$ constant across all experiments. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "C  Ablation study: Comparing choices for basis function ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Different basis functions can be used to approximate the RBF kernel used in the energy of the memory representation of the DAM in eq. (7). We considered the following kernels (\"Cos\", \u201c\"SinCos\", \u201cExp\", \u201cExpExp\"),rewritten here as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\phi_{\\operatorname{Tw}}(x)}&{=\\phantom{-}\\sqrt{\\frac{2}{V}}\\left[\\begin{array}{l}{\\cos\\left(\\phi_{r}^{(1)},x\\right)+h}\\\\ {\\cos\\left(\\phi_{r}^{(2)},x\\right)+h}\\\\ {\\cos\\left(\\phi_{r}^{(3)},x\\right)+h}\\\\ {\\sin\\left(\\phi_{r}^{(4)},x\\right)+h}\\end{array}\\right]\\,,}\\\\ {\\phi_{\\operatorname{Tw}}(x)}&{=\\frac{\\displaystyle1}{\\sqrt{V}}\\left[\\begin{array}{l}{\\cos\\left(\\phi_{r}^{(1)},x\\right)}\\\\ {\\sin\\left(\\phi_{r}^{(2)},x\\right)}\\\\ {\\sin\\left(\\phi_{r}^{(3)},x\\right)}\\\\ {\\sin\\left(\\phi_{r}^{(4)},x\\right)+h}\\end{array}\\right]\\,,}\\\\ {\\phi_{\\operatorname{Tw}}(x)}&{=\\frac{\\displaystyle1}{\\sqrt{V}}\\left[\\begin{array}{l}{\\cos\\left(\\phi_{r}^{(1)},x\\right)}\\\\ {\\sin\\left(\\phi_{r}^{(2)},x\\right)+h}\\\\ {\\sin\\left(\\phi_{r}^{(3)},x\\right)+h}\\end{array}\\right]\\,,}\\\\ {\\phi_{\\operatorname{Tw}}(x)}&{=\\frac{\\displaystyle1}{\\sqrt{V}}\\left[\\begin{array}{l}{\\cos\\left(\\phi_{r}^{(1)},x\\right)+h}\\\\ {\\sin\\left(\\phi_{r}^{(2)},x\\right)+h}\\\\ {\\cos\\left(\\phi_{r}^{(3)},x\\right)+h}\\end{array}\\right]\\,,}\\\\ {\\phi_{\\operatorname{Tw}}(x)}&{=\\frac{\\displaystyle1}{\\sqrt{V}}\\left[\\begin{array}{l}{\\cos\\left(\\phi_{r}^{(1)},x\\right)+h}\\\\ {\\sin\\left(\\phi_{r}^{(2)},x\\right)+h}\\\\ {\\cos\\left(\\phi_{r}^{(3)},x\\right)+h}\\end{array}\\right]\\,,}\\\\ {\\phi_{\\operatorname{Tw}}(x)}&{=\\frac{\\displaystyle1}{\\sqrt{V}}\\left[\\begin{array}{l}{\\cos\\left(\\phi_{r}^{(1)},x\\right)+h}\\\\ {\\sin\\left(\\phi_{r}^{(2)},x\\right)+h}\\\\ {\\cos\\left(\\phi_{r}^{(3)},x\\right)+h}\\end{array}\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\pmb{\\omega}^{\\alpha}\\sim\\mathcal{N}(\\mathbf{0},I_{D})$ \uff0c $\\alpha\\in[\\![Y]\\!]$ are the random projection vectors and $b^{\\alpha}\\sim\\mathcal{U}(0,2\\pi)$ are random \"biases\"' or shifts in the basis function. ", "page_idx": 15}, {"type": "text", "text": "Figure 6 shows how well the above basis functions approximated the true energy and energy gradient at different values for $\\beta$ and size of feature dimension $Y$ . Specifically, given the Letter dataset [46] which consists of 16-dimensional continuous vectors whose values were normalized to be between $[0,{\\frac{1}{\\sqrt{D}}}]$ we randomly elected900 unique data poins,storing 500 patterns intothe memory and choosing the remaining 400 to serve as new patterns. We then compared how well the energy and energy gradients of the chosen basis function approximates the predictions of the original DAM. ", "page_idx": 15}, {"type": "text", "text": "We observe that the trigonometric basis functions (i.e., either Cos or SinCos) provide the most accurate approximations for the energy and gradients of the standard MrDAM, especially in the regime Of high $\\beta$ which is required for the high memory storage capacity of DenseAMs. Surprisingly, the Positive Random Features (PRFs) of [18] do not perform well in the dense (high $\\beta$ ) regime; in general, trigonometric features always provide better approximations than the PRFs. ", "page_idx": 15}, {"type": "text", "text": "We conclude that the SinCos basis function is the best approximation for use in the experiments reported in the main paper, as this choice consistently produces the best approximations for the energy gradients across all values of $\\beta$ ", "page_idx": 15}, {"type": "text", "text": "D  TinyImagenet Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In performing the qualitative reconstructions shown in fig. 1, we used a standard MrDAM energy (eq. (7)) configured with inverse temperature $\\beta=60$ . We approximated this energy in a DrDAM using the trigonometric \u201cSinCos\" basis function shown in eq. (8) configured with feature dimension $Y=1.8e5$ . The four images shown were selected from the Tiny Imagenet [11] dataset, rasterized into a vector, and stored in the memory matrix a MrDAM, resulting in a memory of shape (4, 12288). Energy descent for both MrDAM and DrDAM used standard gradient descent at a step size of 0.1 until the dynamics of all images converged (for fig. 1 after 300 steps, see energy traces). Visible pixels are \"clamped\" throughout the duration of the dynamics by zeroing out the energy gradients on the visible top one-third of the image. ", "page_idx": 15}, {"type": "image", "img_path": "164QnJsYjF/tmp/6d1c76f2543b9ef43003c5cc8b5db8459fe0d6bc587ab8c72aaa29b0580e297c.jpg", "img_caption": ["Figure 6: Trigonometric basis functions significantly outperform Positive Random Features, especially in the regime of large $\\beta$ . We end up choosing the SinCos function to analyze in the main paper, as this choice of basis function always produced the best approximations to the energy gradient. Experiments performed on the 16-dimensional Letters dataset [46]. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "In MrDAM, the memory matrix necessarily grows linearly when storing new patterns $\\xi^{\\mu}$ . However, the distributed memory tensor $\\mathbf{T}$ of DrDAM does not grow when new patterns are stored. This means it is possible to compress the memories into a smaller tensor $\\mathbf{T}$ where $Y<\\mathrm{NumPix}$ els, provided that we operate in a regime that allows larger approximation errors in the retrieval and smaller initial occlusions. Figure 2 shows a variation of the setting of fig. 1 where stored patterns are actually compressed into DrDAM's memory tensor, successfully storing $20*12288$ pixels from a distributed tensor of size $Y\\,=\\,2e5$ and retrieving the memories with $40\\%$ initial occlusion of the queries, a ${\\sim}20\\%$ reduction in the number of parameters compared to MrDAM. All other hyperparameters are the same as was used to generate fig. 1, and convergence on all images occurs after 1000 steps. ", "page_idx": 16}, {"type": "text", "text": "E  Details on Computational Environment for the Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All experiments are performed on a single L40s GPU equipped with 46GB VRAM. Experiments were written and performed using the JAX [47] library for tensor manipulations. Unless otherwise noted, gradient computation was performed using JAX's powerful autograd mechanism. Experimental code with instructions to replicate the results in this paper are made available at this GitHub repository (https: //github. com/bhoov/distributed_DAM), complete with instructions to setup the coding environment and run all experiments. ", "page_idx": 16}, {"type": "text", "text": "F  Detailed Proofs and Discussions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1Details for theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1.1 Proof of theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of theorem 1. The proof above involves noting that, first we need to encode all the memories with ProcMems, which takes $O(D Y K)$ time and $O\\bar{(}Y+D)$ peak memory using proposition 3. ", "page_idx": 17}, {"type": "text", "text": "Then we compute $L$ gradients with GradComp for $L$ iterations of energy gradient descent, taking $O(L D(Y+\\Bar{D}))$ time and $O(D+Y)$ peak memory using proposition 4. ", "page_idx": 17}, {"type": "text", "text": "Putting the runtimes together, and using the maximum of the peak memories gives us the statement of the theorem. ", "page_idx": 17}, {"type": "text", "text": "F.1.2  Comparing computational complexities of MrDAM and DrDAM ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Note that, comparing the computational complexities of MrDAM in proposition 1 to that of DrDAM in theorem 1 does not directly provide any computational improvements as it would depend on the choicesof $D,K,L,Y$ . The main point of these results is to highlight, that once the memories are processed via ProcMems, the energy descent with DrDAM requires computation and memory that only depends on $D$ and $Y$ . And together with theorem 2 and corollary 1, we characterize situations where the energy descent divergence between MrDAM and DrDAM can be bounded with a choice of $Y$ that only depends on $D$ (and other parameters in the energy function) but not $K$ ", "page_idx": 17}, {"type": "text", "text": "While we do not claim or highlight computational gains over MrDAM, note that the peak memory complexity of MrDAM is $O(K D)$ compared to $O(\\bar{Y}+D)$ for DrDAM. Given that in the interesting regime of $Y\\sim O(D/\\epsilon^{2})$ which upperbounds the energy descent divergence between DrDAM and MrDAM in corollary 1 to at most some $\\epsilon>0$ , DrDAM is more memory efficient than MrDAM if the number of memories $K>C/\\epsilon^{2}$ for some sufficiently large positive constant $C$ . Ignoring the time required to encode the memories into the distributed representation in DrDAM using ProcMems, the runtime complexities are $O(L K D)$ for MrDAM compared to $O(L D(Y+D))$ for DrDAM. Again, considering the interesting regime of $Y\\sim O(D/\\epsilon^{2})$ , DrDAM will be computationally more efficient than MrDAM if the number of memories $K>\\widetilde{C}D/\\epsilon^{2}$ for some sufficiently large positive constant C'. ", "page_idx": 17}, {"type": "text", "text": "F.2 Details for theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.2.1 Proof of theorem 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Here we will make use of the following result from Li et al. [48]: ", "page_idx": 17}, {"type": "text", "text": "Lemma 1 (adapted from Liet al. [48] Lemma B.1). For x $\\mathbf{z}\\in\\mathbb{R}^{K}$ with maxi,je[K] (x; - xj) \u2264 8 and $\\begin{array}{r}{\\operatorname*{max}_{i,j\\in[\\![K]\\!]}\\!\\left(\\mathbf{z}_{i}-\\mathbf{z}_{j}\\right)\\leq\\delta_{i}}\\end{array}$ we have the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathsf{s o f t m a x}(\\mathbf{x})\\|_{\\infty}\\leq\\frac{e^{\\delta}}{K},\\quad\\|\\mathsf{s o f t m a x}(\\mathbf{x})-\\mathsf{s o f t m a x}(\\mathbf{z})\\|_{1}\\leq\\frac{e^{\\delta}}{K}\\|\\mathbf{x}-\\mathbf{z}\\|_{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We now develop the following results: ", "page_idx": 17}, {"type": "text", "text": "Lemma 2. Under the conditions and notation of theorem 2, for $\\mathbf{x},\\mathbf{z}\\in\\mathcal{X}$ ,wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf x}E(\\mathbf x)-\\nabla_{\\mathbf x}E(\\mathbf z)\\|\\le(1+2K\\beta e^{\\beta/2})\\|\\mathbf x-\\mathbf z\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Given the energy function in eq. (11), we can write the energy gradient $\\nabla_{\\mathbf{x}}E(\\mathbf{x})$ as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}}E(\\mathbf{x})=\\mathsf{s o f}\\tan\\mathbf{x}(-\\beta/2\\lVert\\mathbf{x}-\\Xi\\rVert_{2}^{2})(\\mathbf{x}-\\Xi)=\\mathbf{x}-\\mathsf{s o f}\\tan\\mathbf{x}(-\\beta/2\\lVert\\mathbf{x}-\\Xi\\rVert_{2}^{2})\\Xi,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Xi\\;=\\;[\\xi^{1},\\ldots,\\xi^{K}]$ \uff0c $\\lVert\\mathbf{x}-\\Xi\\rVert_{2}^{2}$ denotes $[||\\mathbf{x}-\\xi^{1}||_{2}^{2},\\dots||\\mathbf{x}-\\xi^{K}||_{2}^{2}]$ and $\\left(\\mathbf{x}\\mathrm{~-~}\\Xi\\right)$ denotes $[(\\mathbf{x}-\\xi^{1}),\\dots\\overdot{,}(\\mathbf{x}-\\xi^{K})]$ Then we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\mathbf x}E(\\mathbf x)-\\nabla_{\\mathbf x}E(\\mathbf z)\\|_{2}}\\\\ &{\\quad=\\|\\mathbf x-\\mathbf s\\circ\\mathbf f\\operatorname*{tmax}(-\\beta/2\\|\\mathbf x-\\Xi\\|_{2}^{2})\\Xi-\\mathbf z+\\mathbf s\\circ\\mathbf f\\operatorname*{tmax}(-\\beta/2\\|\\mathbf z-\\Xi\\|_{2}^{2})\\Xi\\|_{2}}\\\\ &{\\quad\\le\\|\\mathbf x-\\mathbf z\\|+\\|(\\mathbf s\\circ\\mathbf f\\operatorname*{tmax}(-\\beta/2\\|\\mathbf x-\\Xi\\|_{2}^{2})-\\mathbf s\\circ\\mathbf f\\operatorname*{tmax}(-\\beta/2\\|\\mathbf z-\\Xi\\|_{2}^{2}))\\Xi\\|_{2}}\\\\ &{\\quad\\le\\|\\mathbf x-\\mathbf z\\|+\\|(\\mathbf s\\circ\\mathbf f\\operatorname*{tmax}(-\\beta/2\\|\\mathbf x-\\Xi\\|_{2}^{2})-\\mathbf s\\circ\\mathbf f\\operatorname*{tmax}(-\\beta/2\\|\\mathbf z-\\Xi\\|_{2}^{2}))\\|_{1}\\|\\Xi\\|_{2}}\\\\ &{\\quad\\le\\|\\mathbf x-\\mathbf z\\|+\\frac{e^{\\beta/2}}{K}\\|\\Xi\\|_{2}\\,\\|\\beta/2(\\|\\mathbf z-\\Xi\\|_{2}^{2}-\\|\\mathbf x-\\Xi\\|_{2}^{2})\\|_{1}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we applied lemma 1 to the softmax term in the right hand side of eq. (22) with $\\delta=\\beta/2$ since all pairwise distances in $\\mathcal{X}$ are in $[0,1]$ ", "page_idx": 18}, {"type": "text", "text": "Now we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\big\\|\\beta/2(\\|\\mathbf z-\\Xi\\|_{2}^{2}-\\|\\mathbf x-\\Xi\\|_{2}^{2})\\big\\|_{1}=\\frac{\\beta}{2}\\sum_{\\mu=1}^{K}\\big|\\|\\mathbf z-\\xi^{\\mu}\\|_{2}^{2}-\\|\\mathbf x-\\xi^{\\mu}\\|_{2}^{2}\\big|}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=\\frac{\\beta}{2}\\sum_{\\mu=1}^{K}|\\langle\\mathbf z+\\mathbf x,\\mathbf z-\\mathbf x\\rangle+2\\,\\langle\\xi^{\\mu},\\mathbf x-\\mathbf z\\rangle|}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\beta}{2}\\sum_{\\mu=1}^{K}\\|\\mathbf z-\\mathbf x\\|(\\|\\mathbf z+\\mathbf x\\|+2\\|\\xi^{\\mu}\\|)\\leq\\frac{\\beta}{2}\\sum_{\\mu=1}^{K}4\\|\\mathbf z-\\mathbf x\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since $\\|\\pmb{\\xi}^{\\mu}\\|\\leq1$ and $\\|\\mathbf{x}+\\mathbf{z}\\|\\leq\\|\\mathbf{x}\\|+\\|\\mathbf{z}\\|\\leq2$ Putting eq. (26) in eq. (23), and using the fact that $\\|\\Xi\\|_{2}\\leq K$ ,we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf x}E(\\mathbf x)-\\nabla_{\\mathbf x}E(\\mathbf z)\\|_{2}\\le\\|\\mathbf x-\\mathbf z\\|+\\frac{e^{\\beta/2}}{K}K2\\beta\\sum_{\\mu=1}^{K}\\|\\mathbf z-\\mathbf x\\|=(1+2K\\beta e^{\\beta/2})\\|\\mathbf x-\\mathbf z\\|,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "giving is eq. (16) in the statement of the lemma. ", "page_idx": 18}, {"type": "text", "text": "Given the structure of the energy gradient in eq. (18) of the energy function in eq. (11), we consider a specific energy gradient for this specific energy function instead of the generic energy gradient in eq. (10). We can rewrite the exact energy gradient as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{x}}E(\\mathbf{x})=\\mathbf{x}-\\sum_{\\mu=1}^{K}\\frac{\\exp(-\\beta/2\\Vert\\mathbf{x}-\\pmb{\\xi}^{\\mu}\\Vert_{2}^{2})\\pmb{\\xi}^{\\mu}}{\\sum_{\\mu^{\\prime}=1}^{K}\\exp(-\\beta/2\\Vert\\mathbf{x}-\\pmb{\\xi}^{\\mu^{\\prime}}\\Vert_{2}^{2})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using random feature maps, we can write the approximate gradient as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{x}}\\hat{E}(\\mathbf{x})=\\mathbf{x}-\\displaystyle\\sum_{\\mu=1}^{K}\\frac{\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\rangle\\xi^{\\mu}}{\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\rangle}}\\\\ &{\\qquad=\\frac{\\displaystyle\\sum_{\\mu=1}^{K}\\varphi(\\sqrt{\\beta}\\mathbf{x})\\cdot\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\cdot\\xi^{\\mu\\tau}}{\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\sum_{\\mu^{\\prime}=1}^{K}\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\right\\rangle}}\\\\ &{\\qquad=\\frac{\\displaystyle\\varphi(\\sqrt{\\beta}\\mathbf{x})\\cdot\\sum_{\\mu=1}^{K}\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\cdot\\xi^{\\mu\\tau}}{\\displaystyle\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\sum_{\\mu}\\mathbf{x}\\rangle},\\quad\\mathrm{wher~}\\mathbf{T}=\\displaystyle\\sum_{\\mu^{\\prime}=1}^{K}\\varphi(\\sqrt{\\beta}\\xi^{\\mu^{\\prime}})}\\\\ &{\\qquad=\\frac{\\displaystyle\\varphi(\\sqrt{\\beta}\\mathbf{x})\\cdot\\mathbf{R}}{\\displaystyle\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\mathbf{T}\\rangle},\\quad\\mathrm{where~}\\mathbf{R}=\\displaystyle\\sum_{\\mu=1}^{K}\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\cdot\\xi^{\\mu\\tau},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we again just need to store $\\mathbf{T}$ and $\\mathbf{R}$ as defined above and do not need to maintain the original memory matrix $\\Xi$ ", "page_idx": 18}, {"type": "text", "text": "Lemma 3. Under the conditions and notation of theorem 2, and assuming that $\\langle\\varphi(\\mathbf{x}),\\varphi(\\mathbf{x}^{\\prime})\\rangle\\geq$ $0\\forall\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathcal{X}$ for the approximate gradient $\\nabla_{\\mathbf{x}}\\hat{E}(\\mathbf{x})$ in eq. (32), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf{x}}E(\\mathbf{x})-\\nabla_{\\mathbf{x}}\\hat{E}(\\mathbf{x})\\|\\le2C_{1}K e^{\\beta E(\\mathbf{x})}\\sqrt{\\frac{D}{Y}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We can expand out the left-hand side of eq. (33) as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf{x}}E(\\mathbf{x})-\\nabla_{\\mathbf{x}}\\hat{E}(\\mathbf{x})\\|=\\left\\|\\frac{\\sum_{\\mu=1}^{K}\\exp(-\\beta/2\\|\\mathbf{x}-\\pmb{\\xi}^{\\mu}\\|^{2})\\xi^{\\mu}}{\\sum_{\\mu=1}^{K}\\exp(-\\beta/2\\|\\mathbf{x}-\\pmb{\\xi}^{\\mu}\\|^{2})}-\\frac{\\sum_{\\mu=1}^{K}\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\right\\rangle\\xi^{\\mu}}{\\sum_{\\mu=1}^{K}\\left\\langle\\varphi(\\sqrt{\\beta}q),\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\right\\rangle}\\right\\|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by reversing the simplifying steps made above to arrive at eq. (32). ", "page_idx": 19}, {"type": "text", "text": "Let use denote $\\epsilon=C_{1}\\sqrt{D/Y}$ the approximation in the kernel value induced by the random feature map $\\varphi$ . Then considering the terms in the denominator above, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1/K)\\left|\\displaystyle\\sum_{\\mu}\\exp(-\\beta/2\\|\\mathbf x-\\pmb\\xi^{\\mu}\\|^{2})-\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf x),\\displaystyle\\sum_{\\mu}\\varphi(\\sqrt{\\beta}\\pmb\\xi^{\\mu})\\right\\rangle\\right|}\\\\ &{\\quad=(1/K)\\left|\\displaystyle\\sum_{\\mu}\\left(\\exp(-\\beta/2\\|\\mathbf x-\\pmb\\xi^{\\mu}\\|^{2})-\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf x),\\varphi(\\sqrt{\\beta}\\pmb\\xi^{\\mu})\\right\\rangle\\right)\\right|}\\\\ &{\\quad\\leq(1/K)\\displaystyle\\sum_{\\mu}\\left|\\left(\\exp(-\\beta/2\\|\\mathbf x-\\pmb\\xi^{\\mu}\\|^{2})-\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf x),\\varphi(\\sqrt{\\beta}\\pmb\\xi^{\\mu})\\right\\rangle\\right)\\right|\\leq(1/K)\\sum_{\\mu}\\epsilon=\\epsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Considering the terms in the numerators, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(1/K\\right)\\left\\|\\displaystyle\\sum_{\\mu}\\exp(-\\beta/2)\\|\\mathbf x-\\xi^{\\mu}\\|^{2})\\xi^{\\mu}-\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf x),\\displaystyle\\sum_{\\mu}\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\right\\rangle\\xi^{\\mu}\\right\\|}\\\\ &{\\quad=(1/K)\\left\\|\\displaystyle\\sum_{\\mu}\\left(\\exp(-\\beta/2)\\|\\mathbf x-\\xi^{\\mu}\\|^{2}\\right)-\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf x),\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\right\\rangle\\xi^{\\mu}\\right\\|}\\\\ &{\\quad\\leq(1/K)\\displaystyle\\sum_{\\mu}\\left\\|\\left(\\exp(-\\beta/2)\\|\\mathbf x-\\xi^{\\mu}\\|^{2}\\right)-\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf x),\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\right\\rangle\\right)\\xi^{\\mu}\\right\\|}\\\\ &{\\quad\\leq(1/K)\\displaystyle\\sum_{\\mu}\\left\\|\\left(\\exp(-\\beta/2)\\|\\mathbf x-\\xi^{\\mu}\\|^{2}\\right)-\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf x),\\varphi(\\sqrt{\\beta}\\xi^{\\mu})\\right\\rangle\\right)\\|\\xi^{\\mu}\\|}\\\\ &{\\quad\\leq(1/K)\\displaystyle\\sum_{\\mu}\\epsilon\\|\\xi^{\\mu}\\|=\\epsilon\\qquad\\cdot:\\|\\xi^{\\mu}\\|\\leq1.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Let us define the following terms for convenience: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\succ a=1/\\kappa\\sum_{\\mu}\\exp(-\\beta/2\\|\\mathbf{x}-\\pmb{\\xi}^{\\mu}\\|^{2})\\pmb{\\xi}^{\\mu}}\\\\ &{\\succ b=1/\\kappa\\sum_{\\mu}\\exp(-\\beta/2\\|\\mathbf{x}-\\pmb{\\xi}^{\\mu}\\|^{2})}\\\\ &{\\succ\\hat{a}=1/\\kappa\\sum_{\\mu}\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\varphi(\\sqrt{\\beta}\\pmb{\\xi}^{\\mu})\\right\\rangle\\pmb{\\xi}^{\\mu}}\\\\ &{\\succ\\hat{b}=1/\\kappa\\sum_{\\mu}\\left\\langle\\varphi(\\sqrt{\\beta}\\mathbf{x}),\\varphi(\\sqrt{\\beta}\\pmb{\\xi}^{\\mu})\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, based on our previous bounds, we know that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|a-\\hat{a}\\|\\leq\\epsilon,\\quad|b-\\hat{b}|\\leq\\epsilon,\\quad\\|\\nabla_{\\mathbf x}E(\\mathbf x)-\\nabla_{\\mathbf x}\\hat{E}(\\mathbf x)\\|=\\left\\|\\frac{a}{b}-\\frac{\\hat{a}}{\\hat{b}}\\right\\|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\mathbf x}E(\\mathbf x)-\\nabla_{\\mathbf x}\\hat{E}(\\mathbf x)\\|=\\left\\|\\frac{a}{b}-\\frac{\\hat{a}}{\\hat{b}}\\right\\|=\\left\\|\\frac{a-\\hat{a}}{b}+\\frac{\\hat{a}}{\\hat{b}}\\frac{\\hat{b}}{b}-\\frac{\\hat{a}}{\\hat{b}}\\right\\|\\leq\\left\\|\\frac{a-\\hat{a}}{b}\\right\\|+\\left\\|\\frac{\\hat{a}}{\\hat{b}}\\right\\|\\left|\\frac{\\hat{b}}{b}-1\\right|}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{b}\\|a-\\hat{a}\\|+\\left\\|\\frac{\\hat{a}}{\\hat{b}}\\right\\|\\frac{1}{b}|\\hat{b}-b|\\leq\\frac{1}{b}\\left(\\epsilon+\\left\\|\\frac{\\hat{a}}{\\hat{b}}\\right\\|\\epsilon\\right)}\\\\ &{\\qquad\\qquad\\leq\\epsilon\\frac{1}{b}\\left(1+\\left\\|\\frac{\\hat{a}}{\\hat{b}}\\right\\|\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $({\\hat{a}}/{\\hat{b}})$ is in the convex hull of the memories since this is a weighted sum of the memories where the weights are positive and add up to 1. Thus within $(\\hat{a}/\\hat{b})\\in[0,1/\\sqrt{d}]^{d}$ ,thus $\\|\\hat{a}/\\hat{b}\\|\\leq1$ Now $b=(1/\\bar{K})\\exp(-\\bar{\\beta}E({\\bf x}))$ .Thus ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mathbf{x}}E(\\mathbf{x})-\\nabla_{\\mathbf{x}}\\hat{E}(\\mathbf{x})\\|\\le2\\epsilon K\\exp(\\beta E(\\mathbf{x}))=2C_{1}K\\exp(\\beta E(\\mathbf{x}))\\sqrt{\\frac{D}{Y}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "giving us the right-hand side of eq. (33). ", "page_idx": 20}, {"type": "text", "text": "Proof of theorem 2. Expanding out the divergence $D^{(L)}$ after $L$ energy descent steps, and using the fact that $\\mathbf{x}^{(0)}=\\hat{\\mathbf{x}}^{(0)}=\\mathbf{\\bar{x}}$ wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D^{(L)}\\triangleq\\Vert\\mathbf{x}^{(L)}-\\hat{\\mathbf{x}}^{(L)}\\Vert}&{}\\\\ &{=\\left\\Vert\\left(\\mathbf{x}^{(0)}-\\underset{t\\in[L]}{\\sum}\\eta\\nabla_{\\mathbf{x}}E(\\mathbf{x}^{(t-1)})\\right)-\\left(\\hat{\\mathbf{x}}^{(0)}-\\underset{t\\in[L]}{\\sum}\\eta\\nabla_{\\mathbf{x}}\\hat{E}(\\hat{\\mathbf{x}}^{(t-1)})\\right)\\right\\Vert}\\\\ &{=\\left\\Vert\\underset{t\\in[L]}{\\sum}-\\eta\\left(\\nabla_{\\mathbf{x}}E(\\mathbf{x}^{(t-1)})-\\nabla_{\\mathbf{x}}\\hat{E}(\\hat{\\mathbf{x}}^{(t-1)})\\right)\\right\\Vert}\\\\ &{=\\left\\Vert\\underset{t\\in[L]}{\\sum}\\eta\\left(\\nabla_{\\mathbf{x}}E(\\mathbf{x}^{(t-1)})-\\nabla_{\\mathbf{x}}\\hat{E}(\\hat{\\mathbf{x}}^{(t-1)})\\right)\\right\\Vert}\\\\ &{\\leq\\underset{t\\in[L]}{\\sum}\\left\\Vert\\eta\\left(\\nabla_{\\mathbf{x}}E(\\mathbf{x}^{(t-1)})-\\nabla_{\\mathbf{x}}\\hat{E}(\\hat{\\mathbf{x}}^{(t-1)})\\right)\\right\\Vert}\\\\ &{\\underset{t\\in[L]}{\\leq}\\left\\Vert\\eta\\left(\\nabla_{\\mathbf{x}}E(\\mathbf{x}^{(t-1)})-\\nabla_{\\mathbf{x}}\\hat{E}(\\hat{\\mathbf{x}}^{(t-1)})\\right)\\right\\Vert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Letusdete tdividatmsbs $d^{(t)}$ with $\\begin{array}{r}{D^{(L)}=\\sum_{t\\in[L]}d^{(t)}}\\end{array}$ Als, Iet us denote by $A=2C_{1}K e^{\\beta E(\\mathbf{x})}\\sqrt{D/Y}$ and by $B=(1+2K\\beta e^{\\beta/2})$ Then writing out the $t$ -th term ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d^{(t)}=\\left\\|\\eta\\left(\\nabla_{\\mathbf{x}}E(\\mathbf{x}^{(t-1)})-\\nabla_{\\mathbf{x}}\\hat{E}(\\hat{\\mathbf{x}}^{(t-1)})\\right)\\right\\|}\\\\ &{\\quad\\quad\\leq\\left\\|\\eta\\left(\\nabla_{\\mathbf{x}}E(\\mathbf{x}^{(t-1)})-\\nabla_{\\mathbf{x}}E(\\hat{\\mathbf{x}}^{(t-1)})\\right)\\right\\|+\\left\\|\\eta\\left(\\nabla_{\\mathbf{x}}\\hat{E}(\\hat{\\mathbf{x}}^{(t-1)})-\\nabla_{\\mathbf{x}}\\hat{E}(\\hat{\\mathbf{x}}^{(t-1)})\\right)\\right\\|}\\\\ &{\\quad\\quad\\leq\\eta B\\|\\mathbf{x}^{(t-1)}-\\hat{\\mathbf{x}}^{(t-1)}\\|+\\eta A,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first term is bounded using lemma 2 and the definition of $B$ , and the second term is bounded using lemma 3 and the definition of $A$ ", "page_idx": 20}, {"type": "text", "text": "Note that tis givs usthr $d^{(t)}\\,\\leq\\,\\eta A+\\eta B D^{(t-1)}$ , and thus, $\\begin{array}{r}{D^{(L)}\\,\\leq\\,\\sum_{t\\in[L]}\\eta(A\\,+\\,}\\end{array}$ $B D^{(t-1)})$ ", "page_idx": 20}, {"type": "text", "text": "Writing out the recursion using induction, we can show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\eta A\\left(\\displaystyle\\sum_{t\\in[L]}1+\\displaystyle\\sum_{t\\in[L]}t(\\eta B)+\\displaystyle\\sum_{t\\in[L]}\\sum_{t_{1}\\in[L]}t_{1}(\\eta B)^{2}+\\displaystyle\\sum_{t\\in[L]}\\sum_{t_{1}\\in[L]}\\sum_{t_{2}\\in[t_{1}]}t_{2}(\\eta B)^{3}\\right.}\\\\ &{\\qquad\\qquad\\left.+\\cdots+\\displaystyle\\sum_{t\\in[L]}\\sum_{t_{1}\\in[L]}\\cdots\\sum_{t_{L-1}\\in[t_{L-2}]}t_{L-1}(\\eta B)^{L-1}\\right)}\\\\ &{\\leq\\eta A\\left(L+L(\\eta B L)+L(\\eta B L)^{2}+\\cdots+L(\\eta B L)^{L-1}\\right)}\\\\ &{=\\eta A L\\displaystyle\\frac{1-(\\eta B L)^{L}}{1-\\eta B L}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Replacing the values of $A$ and $B$ above gives us the statement of the theorem. ", "page_idx": 21}, {"type": "text", "text": "F.2.2   Dependence on the initial energy $E(\\mathbf{x})$ of the input. ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The divergence upper bound in eq. (12) (in theorem 2) depends on the term $\\exp(\\beta E(\\mathbf{x}))$ .However, note that, for the energy function defined in eq. (11), assuming that all memories and the initial queries are in a ballof diameter 1 (which is the assumption A1 in theorem 2), $\\begin{array}{r}{E(\\mathbf{x})\\leq\\frac{1}{2}-\\frac{\\log K}{\\beta}}\\end{array}$ implying that $\\exp(\\beta E(\\mathbf{x}))\\leq\\exp(\\beta/2)/K$ , and we can replace this in the upper bound and remove the dependence on $E({\\bf x})$ ", "page_idx": 21}, {"type": "text", "text": "However, an important aspect of our analysis is that the bound is input specific, and depends on the initial energy $E({\\bf x})$ . As discussed above, this can be upper bounded uniformly, but our bound is more adaptive to the input $\\mathbf{x}$ ", "page_idx": 21}, {"type": "text", "text": "For example, if the input is initialized near one of the memories, while being suficiently far from the remaining $(K-1)$ memories, then $\\exp(\\beta E(\\mathbf{x}))$ term can be relatively small. More precisely, with all memories and queries lying in a ball of diameter 1, let the query be at a distance $r<1\\AA$ to its closest memories, and as far as possible from the remaining $(K-1)$ memories. In this case, the initial energy $E({\\bf x})\\approx-(1/\\beta)\\log(\\exp(-\\beta r/2)+(K-1)\\exp(-\\beta/2))$ , implying that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\exp(\\beta E(\\mathbf{x}))\\approx\\frac{\\exp(\\beta r/2)}{(1+\\exp(-\\beta(1-r)/2)}\\leq\\exp(\\beta r/2).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For sufficiently small $r<1$ , the above quantity can be relatively small. If, for example, $r\\sim O(\\log K)$ then $\\exp(\\beta E(\\mathbf{x}))\\,\\sim\\,{\\cal O}(K^{\\beta})$ , while $r\\,\\rightarrow\\,0$ gives us $\\exp(\\beta E(\\mathbf{x}))\\,\\to\\,{\\cal O}(1)$ . This highlights the adaptive input-dependent nature of our analysis. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We lay out clearly our contribution of a novel approximation of the dynamical process for Dense Associative Memory that uses random features. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n$\\blacktriangleright$ The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n$\\blacktriangleright$ The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n$\\blacktriangleright$ It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Limitations are characterized theoretically in $\\S\\ 3$ , empirically in $\\S\\ 4$ , and summarized in appendix A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n$\\blacktriangleright$ The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n$\\blacktriangleright$ The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n$\\blacktriangleright$ The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n$\\blacktriangleright$ The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n$\\blacktriangleright$ The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n$\\blacktriangleright$ If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n$\\blacktriangleright$ While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Assumptions, propositions, and theorems are given in $\\S\\ 3$ , with additional proofs in appendix F. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not include theoretical results.   \n$\\blacktriangleright$ All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n$\\blacktriangleright$ All assumptions should be clearly stated or referenced in the statement of any theorems.   \n$\\blacktriangleright$ The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n$\\blacktriangleright$ Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n$\\blacktriangleright$ Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Extensive descriptions of the main experimental results are provided in $\\S\\ 4$ experimental details that were not appropriate for the main paper are explained in appendix D and appendix E. Our proposed method itself is explained in detail in Algorithm 1. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not include experiments.   \n$\\blacktriangleright$ If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n$\\blacktriangleright$ If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n$\\blacktriangleright$ Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \nWhile NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d)  We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Code and instructions to run all experiments in this paper is provided at this anonymous GitHub repository (https : //anonymous . 4open.science/r/drdam), which is included in appendix E of our paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that paper does not include experiments requiring code.   \n$\\blacktriangleright$ Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \nThe instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n$\\blacktriangleright$ The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n$\\blacktriangleright$ The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n$\\blacktriangleright$ At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \nProviding as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All details are provided alongside the results reported in $\\S\\ 4$ .Computational environment for the experiments is described in appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not include experiments.   \n$\\blacktriangleright$ The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n$\\blacktriangleright$ The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Error bars representing the \u201cstandard error of the mean\" (SEM) are shown on all quantitative plots (i.e., figs. 3, 4 and 6). We evaluated on large sample sizes s.t. the SEM is small, but still visible when the predictions have high variability. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not include experiments.   \n$\\blacktriangleright$ The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n$\\blacktriangleright$ The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \nThe method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n$\\blacktriangleright$ The assumptions made should be given (e.g., Normally distributed errors).   \n$\\blacktriangleright$ It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n$\\blacktriangleright$ It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n$\\blacktriangleright$ For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n$\\blacktriangleright$ If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We use the same computational environment for all experiments, which is described in appendix E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not include experiments.   \n$\\blacktriangleright$ The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n$\\blacktriangleright$ The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n$\\blacktriangleright$ The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our work does not require human subjects. Datasets used are public and the nature of the evaluation is not dependent on any inherent biases. The work does not have unexpected anticipated societal harms or other harmful consequences requiring mitigation. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n$\\blacktriangleright$ If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n$\\blacktriangleright$ The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not anticipate any additional societal harms or other harmful consequences requiring mitigation beyond the well-known risks. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that there is no societal impact of the work performed. $\\blacktriangleright$ If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 25}, {"type": "text", "text": "Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \nThe conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n$\\blacktriangleright$ The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n$\\blacktriangleright$ If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: No proprietary datasets were used. The models developed are not anticipated to have a high risk potential for misuse. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper poses no such risks.   \n$\\blacktriangleright$ Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n$\\blacktriangleright$ Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n$\\blacktriangleright$ We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All owners of assets used in this paper are properly cited. No licensed work wasused. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not use existing assets.   \n$\\blacktriangleright$ The authors should cite the original paper that produced the code package or dataset.   \n$\\blacktriangleright$ The authors should state which version of the asset is used and, if possible, include a URL.   \n$\\blacktriangleright$ The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n$\\blacktriangleright$ For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n$\\blacktriangleright$ If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode .com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license ofadataset.   \n\u2192 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n$\\blacktriangleright$ If this information is not available online, the authors are encouraged to reach out to the asset'screators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper mainly contributes a new algorithm for approximating Dense Associative Memories. This is documented throughout the paper. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not release new assets.   \n$\\blacktriangleright$ Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n$\\blacktriangleright$ The paper should discuss whether and how consent was obtained from people whose assetisused.   \n$\\blacktriangleright$ At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our research makes no use of crowdsourcing or human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n$\\blacktriangleright$ Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2192 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "5. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our research makes no use of crowdsourcing or human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "$\\blacktriangleright$ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n$\\blacktriangleright$ Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n$\\blacktriangleright$ We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u2192 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]