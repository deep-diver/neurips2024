[{"figure_path": "bc1qt1sZsW/figures/figures_1_1.jpg", "caption": "Figure 1: The distribution of multimodal data in different scenarios. (a) exhibits the complete modality, and (b) exhibits the incomplete modality.", "description": "This figure illustrates the difference between theoretical and real-world scenarios in multimodal sleep staging.  In the theoretical scenario (a), all modalities (e.g., EEG, EOG, EMG) are complete, allowing for robust sleep staging.  The real-world scenario (b), however, shows incomplete modalities due to sensor malfunctions or detachment, which significantly impacts the performance of sleep staging algorithms.", "section": "1 Introduction"}, {"figure_path": "bc1qt1sZsW/figures/figures_3_1.jpg", "caption": "Figure 2: The overall framework of CMISleepNet. It consists of three main components: MAIM, SMCCL and MCTA mechanism. Two incomplete modalities, X\u00b9 and X2 are taken as examples for illustration. In the missing modality imputation phase, MAIM learns multimodal shared representations from the available modal distribution to recover complete modalities X\u00b9 and X2. Meanwhile, X\u00b9 and X2 are fed into SMCCL to perform distribution alignment, making the recovered modal data closer to the real data distribution. Furthermore, temporal CNN is utilized to performer feature extraction of X\u00b9 and X2 and obtain the multimodal fusion representation F. After that, F is fed into a Transformer containing MCTA for temporal context modeling to obtain the temporal representation F, which is then used for prediction of sleep stage scores. CMISleepNet also includes three objective functions: (1) for missing modality imputation, l(s) for distribution alignment, (c) for sleep staging.", "description": "The figure illustrates the architecture of CIMSleepNet, a framework for robust sleep staging with incomplete multimodal physiological signals. It comprises three main modules: MAIM (Missing Modality Imputation), SMCCL (Semantic & Modal Calibration Contrastive Learning), and MCTA (Multi-level Cross-branch Temporal Attention). MAIM recovers missing modalities by learning shared representations, SMCCL aligns the recovered data distribution with real data using contrastive learning with semantic and modality information, and MCTA captures temporal context across different scales using a combination of temporal CNN and Bi-GRU. The framework aims to improve the accuracy of sleep staging even when data from certain modalities is missing.", "section": "3 Methodology"}, {"figure_path": "bc1qt1sZsW/figures/figures_6_1.jpg", "caption": "Figure 3: Design of the multi-level cross-branch temporal attention (MCTA) mechanism. D and T are the number of channels of temporal CNN at different levels; the values of D/2 and T/2 are rounded down; k is the kernel size; st is the stride. M and N are the neuron counts of Bi-GRU at different levels, where M = C/S and N = D\u00b7C/S.", "description": "This figure shows the architecture of the Multi-level Cross-branch Temporal Attention (MCTA) mechanism, a key component of the CIMSleepNet model.  MCTA is designed to capture both intra-epoch (within a single epoch of sleep data) and inter-epoch (across multiple epochs) temporal relationships in the data. It uses two parallel branches, one leveraging temporal convolutional networks (CNNs) and the other using bidirectional gated recurrent units (Bi-GRUs), to model temporal information at different scales. The intra-epoch branch captures short-term dependencies, while the inter-epoch branch handles long-term dependencies and transitions between sleep stages.  The outputs of both branches are then combined to generate a comprehensive temporal representation, which improves the sleep staging accuracy.", "section": "3.4 Feature Extraction and Temporal Context Modeling"}, {"figure_path": "bc1qt1sZsW/figures/figures_8_1.jpg", "caption": "Figure 4: Impact of various missing rates. The shaded area represents the range of upper and lower standard deviations.", "description": "This figure presents the performance comparison of CIMSleepNet and other competitive methods under various missing rates for four datasets: Sleep-EDF-20, Sleep-EDF-78, SVUH-UCD, and MHR.  The x-axis represents the missing rate, and the y-axis shows the accuracy, macro F1-score, and Cohen Kappa scores for each method. The shaded region around each line indicates the standard deviation, representing the variability in the results. CIMSleepNet consistently outperforms other methods across different missing rates and evaluation metrics.", "section": "4 Experiments"}, {"figure_path": "bc1qt1sZsW/figures/figures_8_2.jpg", "caption": "Figure 5: Visualization of the recovered modalities by ICL, SCL and SMCCL.", "description": "This figure visualizes the results of modality recovery using three different contrastive learning methods: ICL, SCL, and SMCCL.  It uses t-SNE to project the original and recovered EEG and EOG data into a 2D space, allowing for a visual comparison of the data distributions.  The plot helps to demonstrate the effectiveness of SMCCL in maintaining the original data distribution during the recovery process, unlike ICL and SCL which show less consistency.", "section": "3.3 Distribution Alignment"}, {"figure_path": "bc1qt1sZsW/figures/figures_9_1.jpg", "caption": "Figure 6: Visualization of latent features of different methods on Sleep-EDF-20.", "description": "This figure visualizes the latent features extracted by different methods on the Sleep-EDF-20 dataset with a missing rate of 0.5.  It uses t-SNE to project the high-dimensional feature representations into a 2D space for visualization. Each point represents a data sample, and the color indicates the sleep stage. The figure aims to demonstrate the discriminative ability of different methods in extracting features from incomplete data. CIMSleepNet shows a more distinguishable and clustered distribution compared to other methods, suggesting its robustness in handling missing modalities.", "section": "Ablation studies"}, {"figure_path": "bc1qt1sZsW/figures/figures_14_1.jpg", "caption": "Figure 2: The overall framework of CMISleepNet. It consists of three main components: MAIM, SMCCL and MCTA mechanism. Two incomplete modalities, X\u00b9 and X2 are taken as examples for illustration. In the missing modality imputation phase, MAIM learns multimodal shared representations from the available modal distribution to recover complete modalities X\u00b9 and X2. Meanwhile, X\u00b9 and X2 are fed into SMCCL to perform distribution alignment, making the recovered modal data closer to the real data distribution. Furthermore, temporal CNN is utilized to performer feature extraction of X\u00b9 and X2 and obtain the multimodal fusion representation F. After that, F is fed into a Transformer containing MCTA for temporal context modeling to obtain the temporal representation F, which is then used for prediction of sleep stage scores. CMISleepNet also includes three objective functions: (1) for missing modality imputation, l(s) for distribution alignment, (c) for sleep staging.", "description": "This figure presents the architecture of the CIMSleepNet model, which is composed of three main modules: MAIM for handling missing modalities, SMCCL for aligning data distributions, and MCTA for capturing temporal context.  The figure shows the flow of data through these modules, highlighting the interaction between them and the final sleep stage prediction.", "section": "3.2 Missing Modality Imputation"}, {"figure_path": "bc1qt1sZsW/figures/figures_17_1.jpg", "caption": "Figure 8: Hyperparameters, \u03b1 and \u03b2, analysis on Sleep-EDF-20.", "description": "This figure shows the impact of hyperparameters \u03b1 and \u03b2 on the performance of CIMSleepNet using the Sleep-EDF-20 dataset.  It presents 3D plots visualizing how Accuracy, Macro F1-score, and Cohen Kappa change across different combinations of \u03b1 and \u03b2 values. The plots show that the model's performance is sensitive to changes in both hyperparameters, with \u03b2 exhibiting a greater degree of sensitivity than \u03b1.  The optimal values for \u03b1 and \u03b2 appear to lie within specific ranges.", "section": "Parameter Sensitivity Analysis"}, {"figure_path": "bc1qt1sZsW/figures/figures_17_2.jpg", "caption": "Figure 9: Training dynamics of modal imagination loss, distribution alignment loss and classification loss on Sleep-EDF-20. Among them, modal imagination loss is presented in two modalities: EEG and EOG respectively.", "description": "This figure shows the training dynamics of three different loss functions: modal imputation loss (for EEG and EOG), distribution alignment loss, and classification loss.  The x-axis represents the number of training iterations, and the y-axis represents the loss value.  The plot illustrates how these losses change during the training process.  Observe that modal imputation loss and distribution alignment loss decrease initially at a faster rate compared to the classification loss, indicating an initial focus on recovering the missing modality and aligning the data distributions before refining the model's ability to classify sleep stages accurately.", "section": "J Training Process Analysis"}]