[{"type": "text", "text": "Robust Sleep Staging over Incomplete Multimodal Physiological Signals via Contrastive Imagination ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qi Shen1, Junchang $\\mathbf{Xin}^{2}$ , Bing Tian Dai3, Shudi Zhang1, Zhiqiong Wang1 ", "page_idx": 0}, {"type": "text", "text": "1College of Medicine and Biological Information Engineering, Northeastern University, China 2College of Computer Science and Engineering, Northeastern University, China 3School of Information Systems, Singapore Management University, Singapore   \n2210521@stu.neu.edu.cn, xinjunchang@mail.neu.edu.cn, btdai@smu.edu.sg, 2310535@stu.neu.edu.cn, wangzq@bmie.neu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Multimodal physiological signals, such as EEG, EOG and EMG, provide rich and reliable physiological information for automated sleep staging (ASS). However, in the real world, the completeness of various modalities is difficult to guarantee, which seriously affects the performance of ASS based on multimodal learning. Furthermore, the exploration of temporal context information within PSs is also a serious challenge. To this end, we propose a robust multimodal sleep staging framework named contrastive imagination modality sleep network (CIMSleepNet). Specifically, CIMSleepNet handles the issue of arbitrary modal missing through the combination of modal awareness imagination module (MAIM) and semantic & modal calibration contrastive learning (SMCCL). Among them, MAIM can capture the interaction among modalities by learning the shared representation distribution of all modalities. Meanwhile, SMCCL introduces prior information of semantics and modalities to check semantic consistency while maintaining the uniqueness of each modality. Utilizing the calibration of SMCCL, the data distribution recovered by MAIM is aligned with the real data distribution. We further design a multi-level cross-branch temporal attention mechanism, which can facilitate the mining of cross-scale temporal context representations at both the intra-epoch and inter-epoch levels. Extensive experiments on five multimodal sleep datasets demonstrate that CIMSleepNet remarkably outperforms other competitive methods under various missing modality patterns. The source code is available at: https://github.com/SQAIYY/CIMSleepNet. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Automated sleep staging (ASS) is essential to promote sleep quality assessment and sleep disorder diagnosis, providing convenience for the public in the daily monitoring of sleep within their home environment. Many machine learning algorithms, including feature engineering and deep learning, have been proposed for ASS [1, 2, 3, 4, 5]. In particular, deep learning methods represented by convolutional neural network (CNN) have achieved remarkable results in the field of ASS [6]. Compared with feature engineering, deep learning does not require the guidance of prior knowledge and has the advantage of automatically extracting physiological signals (PSs) features. ", "page_idx": 0}, {"type": "text", "text": "In clinical applications, due to the complexity of human physiological states, subjects usually need to wear multiple sensors to obtain more comprehensive and integrated physiological information from multimodal PSs collected from different sources [7]. Hence, several multimodal fusion algorithms [8, 9, 10, 11] based on deep learning have been developed to cope with the challenges of multimodal ASS. Although various multimodal fusion algorithms provide guarantees for automated processing and analysis of these multimodal PSs, they still have some limitations. As illustrated in the Fig. 1 (a), existing methods are almost all conducted under the assumption that all modal data are complete. However, in real scenarios, the modal data will be incomplete due to sensor malfunctions or detachment, as shown in the Fig. 1 (b). Unfortunately, the second scenario will seriously affect the reasoning process of algorithms, resulting in a sharp decline in performance [12]. ", "page_idx": 0}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/5970ee98d91a5a45c14966c8ad1824df4c8c656a9ad9a65a1b37563416313925.jpg", "img_caption": ["Figure 1: The distribution of multimodal data in different scenarios. (a) exhibits the complete modality, and (b) exhibits the incomplete modality. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Further, how to mine dynamic temporal changes and complex stage-transitioning patterns in PSs is another challenge for ASS. Most sleep staging works [13, 14, 15, 16, 17] utilize recurrent neural network (RNN) and its variants to model temporal dependencies within learnable hidden states. Recently, due to its efficient parallel computing ability and powerful global context modeling ability, Transformer has gradually become the preferred alternative to RNN in the ASS field [18, 19, 20]. However, Transformer lacks the recurrent modeling abilities of RNN, which is crucial for mining the structural representations and positional embedding of input sequences [21, 22]. Meanwhile, most methods are limited to mining temporal correlations at a single level in PSs, i.e., intra-epoch level or inter-epoch level. These issues make it difficult for existing temporal models to fully understand the complex variability patterns in PTS, thereby affecting the performance of sleep staging. ", "page_idx": 1}, {"type": "text", "text": "Considering the above challenges, we propose a robust multimodal sleep staging framework named contrastive imagination modality sleep network (CIMSleepNet), suitable for scenarios with incomplete modalities. The core contributions of CIMSleepNet are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We first design a modal awareness imagination module (MAIM), which can realize the imputation of missing modalities to restore the completeness of the various modalities. MAIM leverages the distribution of available modalities as prior conditions to learn multimodal shared representations and enhance the inter-modal correlation, thereby improving the recovery process of missing modalities. \u2022 We provide a novel insight into the impact of the intrinsic connection between semantic and modality on data distribution. Hence, a semantic & modal calibration contrastive learning (SMCCL) is presented to modify the restored data distribution. It can utilize bidirectional guidance of semantic and modality to align the restored data with the real distribution. \u2022 We further explore a multi-level cross-branch temporal attention (MCTA) mechanism that enables interactive modeling of recurrent features and self-attention weights from the intraepoch and inter-epoch levels to yield more comprehensive temporal representations. \u2022 Extensive experiments on five multimodal sleep datasets exhibit that CIMSleepNet can significantly improve multimodal ASS performance under various missing modality patterns. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Multimodal Learning for Sleep Staging: In the ASS field, several pioneering studies have been devoted to exploring how to utilize multimodal PSs acquired from various sensors to improve ASS performance. Andreotti et al. [8] selected three polysomnography (PSG) signals related to sleep, electroencephalogram (EEG), electrooculogram (EOG) and electromyogram (EMG), as input to CNN to improve ASS accuracy. Similarly, Jia et al. [23] effectively mined salient waves form multimodal sleep PSs with a multimodal salient wave detection network. Lin et al. [11] designed a cross-link fusion module to eliminate redundant information in multimodal PSs. Huy et al. [8] focused on the training mode of the deep model, and proposed an adaptive gradient blending strategy, which improves the joint learning representation ability of multimodal PSs in different views. Furthermore, multimodal PSs collected by some consumer electronic devices have gradually been applied in ASS field. For instance, Walch et al. [24] utilized feature engineering methods to analyze human motion signals and heart rate (HR) signals collected by Apple Watch, and verified their relevance to the sleep stage. Then, Zhai et al. [9] and Mads et al. [25] further improved multimodal sleep staging performance based on consumer electronic devices by constructing a feature fusion method based on deep learning. However, these studies have largely neglected the impact of incomplete modalities scenarios, which are more representative of real-world data distributions. Kontras et al. [26] ingeniously combined self-attention and cross-attention mechanisms to extract coordinating representations for multimodal PSs, thereby mitigating the interference caused by missing modalities on neural network. Nevertheless, this method was developed to handle the complete absence of one or more modalities, whereas it is impractical in real-life clinical applications. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Contrastive Learning Under Missing Modalities: Invariant contrastive learning (ICL) and semantic contrastive learning (SCL) are currently promising choices for solving the modality missing issue. For instance, Lin et al. [27] proposed a cross-modal ICL, aiming to utilize available modalities to achieve prediction of missing modalities. Similarly, Liu et al. [28] narrowed the gap between heterogeneous modalities through ICL for reconstructing missing modalities. SCL introduces category information on the basis of the former to achieve semantic structure preservation in missing modal cases [29, 30]. These studies focus on learning multimodal consistency representations, i.e., only recovering the multimodal shared information to deal with multimodal missing issues. However, this strategy leads to the loss of specific information unique to each modality, thereby failing to exploit inter-modal complementarity. ", "page_idx": 2}, {"type": "text", "text": "Temporal Context Learning in sequence modeling: It has achieved rapid development driven by the sequence-to-sequence models. For instance, Supratak et al. [13] introduced bidirectional long short-term memory (Bi-LSTM) to learn transition rules during sleep stages. Phan et al. [14] applied bidirectional gated recurrent unit (Bi-GRU) to model contextual information of sequence representations. Phyo et al. [16] provided a Bi-LSTM equipped with two auxiliary tasks to explicitly learn periodic transition patterns. Besides, Qu et al. [18] employed Transformer to improve the ability to mine context information in a parallel optimization manner. Eldele et al. [19] deployed temporal CNN to Transformer, further improving its ability to capture temporal features. Although Transformer has advantages over RNN and its variants in terms of computational efficiency and context learning, it lacks recurrent modeling ability, resulting in the omission of some important temporal attribute information [21, 22]. Furthermore, studies [22, 31, 32, 33] have proved that the features learned by RNN and Transformer are complementary. The above optimization perspective provides valuable inspiration for us to design novel temporal context architectures. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first define a complete multimodal PSs dataset $\\mathbb{D}=\\{(\\mathbf{X}_{i},\\boldsymbol{y}_{i})\\}_{i=1}^{N}$ where $\\mathbf{X}_{i}$ is the $i$ th multimodal epoch (sample), ${\\mathrm{y}}_{i}$ is the sleep stage label of the ith epoch and $N$ is total number of epochs. Suppose $\\mathbf{X}_{i}$ contains $M$ modities, i.e., $\\mathbf{X}_{i}=\\{\\mathbf{x}_{i}^{j}\\}_{j=1}^{M}$ , $\\bar{\\mathbf{x}}_{i}^{j}\\in\\mathbb{R}^{C_{j}\\times L_{j}}$ , where $C_{j}$ and $L_{j}$ are the number of channels and sampling points of the $j$ th modality, respectively. Furthermore, $y_{i}~\\in$ $\\{0,1,\\cdot\\cdot\\cdot,K-1\\}$ , where $K$ is the number of sleep stage categories. Different from the complete modality missing issue of Kontras et al. [26], we mainly focus on the chunk-based missing pattern, i.e., random missing in units of multiple epochs, which is a common situation in biomedical research [34]. This is mainly due to the fact that subjects tend to be interrupted for an extended period of time during the data collection. To construct incomplete modal dataset, we define a mask matrix $\\mathbf{Z}=\\{\\{Z_{i}^{j}\\}_{i=1}^{\\overline{{N}}}\\}_{j=1}^{M}\\in\\mathbb{R}^{N\\times M}$ at the epoch level to track the missing status of modalities. If $\\mathbf{x}_{i}^{j}$ is observed, $Z_{i}^{j}\\!=1$ ; otherwise, $Z_{i}^{j}=0$ . Note that, $Z_{i}^{0}\\wedge Z_{i}^{1}\\wedge,\\cdot\\cdot\\cdot\\ ,\\wedge Z_{i}^{M-1}\\not=0$ , i.e., each $\\mathbf{X}_{i}$ must have at least one available modality. According to the mask matrix, the missing rate of the dataset can be defined as $\\begin{array}{r}{\\rho=1-\\frac{1}{N\\cdot M}\\sum_{i=1}^{N}\\!\\sum_{j=1}^{M}Z_{i}^{j}}\\end{array}$ . Then, we define the incomplete multimodal PSs dataset $\\tilde{\\mathbb{D}}=\\{(\\tilde{\\mathbf{X}}_{i},y_{i})\\}_{i=1}^{N}$ , where $\\tilde{\\mathbf{X}}_{i}$ and $\\mathbf{X}_{i}$ have the same shape, i.e., $\\tilde{\\mathbf{X}}_{i}=\\{\\tilde{\\mathbf{x}}_{i}^{j}\\}_{j=1}^{M},\\tilde{\\mathbf{x}}_{i}^{j}\\in\\mathbb{R}^{C_{j}\\times L_{j}}$ . After that, we reorganize the dataset $\\tilde{\\mathbb D}$ with a new shape, i.e., $\\mathbf{x}^{j}\\in\\mathbb{R}^{\\vec{N}\\times T\\times C_{j}\\,\\check{\\times}\\,L_{j}}$ , to perform temporal context modeling. Among them, $\\vec{N}=\\lfloor N/T\\rfloor$ and $T$ is the length of contextual information. Finally, we also define a modality matrix $\\mathbf{S}=\\{\\mathbf{\\beta}\\{\\mathbf{\\sigma}_{s_{i}}^{j}\\}_{.\\;i=1}^{H}\\}_{\\;j=1}^{\\;M}\\in R^{H\\times M}$ to provide information about the modalities involved in each epoch, where $s_{i}^{\\jmath}\\,\\in\\,\\{0,1,\\cdot\\cdot\\cdot\\,,M-1\\}$ is the modal label of the $j$ th modality of the ith epoch and $\\dot{H}=\\vec{N}\\cdot T$ . ", "page_idx": 2}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/f39dbe4e64017bef721b9a019d4ce99dd8a22550c5daa629b4a286698d98e54f.jpg", "img_caption": ["Figure 2: The overall framework of CMISleepNet. It consists of three main components: MAIM, SMCCL and MCTA mechanism. Two incomplete modalities, $\\tilde{\\bf X}^{1}$ and $\\tilde{\\mathbf{X}}^{2}$ are taken as examples for illustration. In the missing modality imputation phase, MAIM learns multimodal shared representations from the available modal distribution to recover complete modalities $\\bar{\\bf X}^{1}$ and $\\bar{\\mathbf{X}}^{2}$ . Meanwhile, $\\bar{\\bf X}^{1}$ and $\\bar{\\mathbf{X}}^{2}$ are fed into SMCCL to perform distribution alignment, making the recovered modal data closer to the real data distribution. Furthermore, temporal CNN is utilized to performer feature extraction of $\\bar{\\bf X}^{1}$ and $\\bar{\\mathbf{X}}^{2}$ and obtain the multimodal fusion representation $\\tilde{\\mathbf{F}}$ . After that, $\\tilde{\\mathbf{F}}$ is fed into $\\stackrel{\\mathrm{a}}{\\leftrightarrow}$ Transformer containing MCTA for temporal context modeling to obtain the temporal representation $\\mathbf{F}$ , which is then used for prediction of sleep stage scores. CMISleepNet also includes three objective functions: $\\ell^{(I)}$ for missing modality imputation, $\\bar{\\ell}^{(s)}$ for distribution alignment, $\\ell^{(c)}$ for sleep staging. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "As schematized in Fig. 2, we present CIMSleepNet, which aims to cope with the issues of modality missing and temporal context modeling in multimodal ASS. Given incomplete multimodal PSs, we first employ MAIM to impute the missing modal data (Sec. 3.2). Meanwhile, SMCCL is utilized to modify the distribution of the recovered data (Sec. 3.3). Then, we leverage temporal CNN and MCTA embedded in the Transformer structure to perform feature extraction and temporal context modeling on the recovered complete multimodal data, respectively (Sec. 3.4). Finally, the model parameters are optimized by combining various objective functions to achieve sleep staging (Sec. 3.5). ", "page_idx": 3}, {"type": "text", "text": "3.2 Missing Modality Imputation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To impute missing modalities, we design MAIM, which mainly consists of $M$ $M{=}2$ in Fig. 2) modality-specific encoders $\\mathbf{E}(\\mathbf{\\xi}\\cdot)=\\{E_{j}(\\bar{\\mathbf{\\xi}}\\cdot)\\}_{j=1}^{M}$ and decoders $\\mathbf{D}({\\widetilde{\\mathbf{\\Gamma}}}\\cdot)=\\{D_{j}(\\mathbf{\\Gamma}\\cdot)\\}_{j=1}^{M}$ . Each encoder and decoder is implemented via separable temporal CNN [16] to reduce the parameter redundancy. ", "page_idx": 3}, {"type": "text", "text": "Modality-specific encoder. As as depicted in Fig. 2, incomplete multimodal PSs $\\tilde{\\mathbf{X}}$ and mask matrix $\\mathbf{Z}$ are transmitted in MAIM through multimodal data flow and mask matrix flow respectively. Firstly, multiple encoders are utilized to project multimodal PSs into the latent space, and the latent representations of all modalities are fused in a multiply add operation, formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{f}_{i}=\\frac{1}{\\sum_{j=1}^{M}Z_{i}^{j}}\\sum_{j=1}^{N}Z_{i}^{j}E_{j}\\left(\\tilde{\\mathbf{x}}_{i}^{j}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{f}_{i}$ denotes the multimodal shared representation obtained from $\\mathbf{X}_{i}$ . Since the modalities in the training set are also incomplete, the best choice for guiding the missing data in the reconstruction process is other available data of the same modality [30]. However, the data recovered by this way loses the diversity of the original data and cannot retain the original semantic structure. To improve the data diversity, we drew inspiration from multimodal variational autoencoder (MVAE) [35] to learn not the shared representations of multimodal PSs but their distributions. Learning diverse data ensures that the generated data is not limited to the data that guide it, making it easier for SMCCL to perform calibration. We first utilize multilayer perceptron (MLP) to obtain two vectors, $\\mu_{i}$ and $\\sigma_{i}$ , which are used to describe the mean and variance in the distribution from the $\\mathbf{f}_{i}$ . Then, $\\mathbf{f}_{i}$ is subjected to reparameterization to obtain the latent representation $\\hat{\\bf f}_{i}$ . Formally, $\\begin{array}{r}{\\hat{\\mathbf{f}}_{i}=\\mu_{i}+\\exp\\left(\\frac{\\sigma_{i}}{2}\\right)\\odot\\varepsilon_{i}}\\end{array}$ , where $\\odot$ is element-wise multiplication and $\\varepsilon_{i}$ is a random variable sampled from the distribution of $\\mathbf{f}_{i}$ . After that, $\\hat{\\bf f}_{i}$ is mapped back into the input space of $\\mathbf{f}_{i}$ to get multimodal shared representation $\\overline{{\\mathbf{f}}}_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Modality-specific decoder. In the decoding stage, $\\overline{{\\mathbf{f}}}_{i}$ is fed into each decoder for reconstructing modality-specific data, i.e., {\u00afxij}jM=1 $\\{\\bar{\\mathbf{x}}_{i}^{j}\\}_{j=1_{\\circ}}^{M}=\\{D_{j}(\\bar{(\\mathbf{f}}_{i})\\}_{j=1}^{\\mathcal{M}}$ . Similar to M(mVseA) E, the parameters of MAIM are optimized guided by the joint of mean square error (MSE) $\\ell^{(m s e)}$ and Kullback-Leibler $(\\mathrm{KL})$ divergence $\\ell^{(K\\mathbb{Z})}$ . We refer to the overall loss function as the modal imagination loss function $\\ell^{(I)}$ . Suppose the batchsize is $h$ , $\\ell^{(I)}$ can be denoted as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\ell^{(I)}=\\frac{1}{M}\\sum_{j}^{M}\\ell_{j}^{(m s e)}+\\eta\\ell^{(K L)}}\\ ~}\\\\ {{\\displaystyle=\\frac{1}{M\\cdot B}\\sum_{j=1}^{M}\\sum_{i=1}^{B}\\left\\|\\tilde{\\mathbf{x}}_{i}^{j}-\\bar{\\mathbf{x}}_{i}^{j}\\right\\|^{2}-\\frac{\\eta}{2B}\\sum_{i=1}^{B}\\sum_{k=1}^{D}\\left(1+\\ln\\left(\\sigma_{i}^{k}\\right)-\\left(\\mu_{i}^{k}\\right)^{2}-\\left(\\sigma_{i}^{k}\\right)^{2}\\right)}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $B=h\\cdot T$ , $\\bar{D}$ is the dimension of $\\overline{{\\mathrm{f}}}_{i}$ , $\\eta$ is the loss weight, $\\tilde{\\mathbf{x}}_{i}^{j}$ is the real sample (if $\\tilde{\\mathbf{x}}_{i}^{j}$ is missing, $\\tilde{\\mathbf{x}}_{i}^{j}$ is the random sampling of the available data in the same modality). We found that the value of $\\eta$ is not sensitive, but removing $\\ell^{(K L)}$ results in a significant decrease in performance of CMISleepNet. Hence, we set $\\eta$ to 1. In particular, $\\ell^{(K L)}$ is used to constrain how close the latent variable distribution is to the prior distribution, prompting the decoder to generate more diverse samples. Then, the mask matrix is utilized to judge whether all recovered data is in a missing state before. if $\\tilde{\\mathbf{x}}_{i}^{j}$ is missing, $\\overline{{\\mathbf{x}}}_{i}^{j}$ will be used as the recovered modality; otherwise, $\\tilde{\\mathbf{x}}_{i}^{j}$ itself will be used. It can be expressed by mask matrix as $\\bar{\\mathbf{x}}_{i}^{j}=Z_{i}^{j}\\tilde{\\mathbf{x}}_{i}^{j}+(1-Z_{i}^{j})\\bar{\\mathbf{x}}_{i}^{j}$ . ", "page_idx": 4}, {"type": "text", "text": "3.3 Distribution Alignment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Different from contrastive learning based on modality consistency [27, 28, 30, 29], our SMCCL introduces semantic and modal information, which not only preserves the semantic structure but also restores the specific modality information to a great extent. As illustrated in Fig. 2, SMCCL covers three similarity levels. The first-level similarity is applied to narrow the distance between different samples with two identical patterns, i.e., the same category and the same modality. Second-level and third-level similarities are utilized to correct the distribution between samples with any of the same single patterns. Note that, the constraint strength of the first-level similarity should be higher than that of the other two levels of similarity because it can be dual-guided in semantics and modality. The latter two levels of similarity are meaningful, and samples that meet these similarity criteria should not be repelled. Because these data still have semantic similarity or modal similarity. Furthermore, contrastive learning is performed within a batch, and the original complete data that meets the first-level similarity standard with the restored data may not necessarily exist in a batch, which further reflects the necessity of the latter two levels of similarity. ", "page_idx": 4}, {"type": "text", "text": "Supposing that a batch contains $B$ epochs, we divide the above similarity levels by constructing similarity weight matrix $\\mathbf{W}=\\{\\{w_{i}^{j}\\}_{i=1}^{\\mathbf{2}\\times M}\\}_{j=1}^{B\\times M}$ . To divide the similarity levels of all sample pairs, we use the label set $\\left\\{y_{i}\\right\\}_{i=1}^{B}$ and modality matrix S to introduce both semantic and modal information for each sample. We first replicate the label set, increasing its modality dimension, to obtain label two eeigxhpta $\\mathbf{Y}=\\mathbf{\\^{\\prime}}\\{\\{\\tilde{y}_{i}^{j}\\}_{i=1}^{B}\\}_{j=1}^{M}$ $R=B\\cdot M$ . e Frleadtteefnin tew tow om amtraitcri sa nads $\\underline{{\\bar{\\mathbf{Y}}}}^{\\star}\\!=\\!\\{\\{\\bar{y}_{i}^{j}\\}_{i=1}^{R}\\}_{j=1}^{R}$ aanndd $\\bar{\\mathbf{S}}=\\{\\{\\bar{s}_{i}^{j}\\}_{i=1}^{R}\\}_{j=1}^{R}$ $\\bar{\\mathbf Y}$ $\\bar{\\bf S}$ $\\mathbf{V}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{U}=\\{\\{u_{i}^{j}\\}_{i=1}^{R}\\}_{j=1}^{R},u_{i}^{j}=\\left\\{\\begin{array}{l l}{1,}&{\\bar{y}_{i}^{j}=\\dot{y}_{i}^{j}}\\\\ {0,}&{\\bar{y}_{i}^{j}\\neq\\dot{y}_{i}^{j}}\\end{array}\\right.\\mathbf{V}=\\{\\{v_{i}^{j}\\}_{i=1}^{R}\\}_{j=1}^{R},v_{i}^{j}=\\left\\{\\begin{array}{l l}{1,}&{\\bar{s}_{i}^{j}=\\dot{s}_{i}^{j}}\\\\ {0,}&{\\bar{s}_{i}^{j}\\neq\\dot{s}_{i}^{j}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where \"1\" is a positive pair and $\"0\"$ is a negative pair. Besides, $\\dot{y}_{i}^{j}$ and $\\dot{s}_{i}^{j}$ are the elements in $\\bar{\\mathbf{Y}}^{\\mathrm{T}}$ and $\\bar{\\bf S}^{\\mathrm{T}}$ respectively. Further, the similarity weight matrix $\\mathbf{W}$ can be constructed by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{W}=\\underbrace{\\mathbf{U}\\odot\\mathbf{V}}_{\\mathrm{the~lth~level}}+\\underbrace{(1.\\Theta)(\\mathbf{U}-\\mathbf{U}\\odot\\mathbf{V})}_{\\mathrm{the~2th~level}}+\\underbrace{\\Theta(\\mathbf{V}-\\mathbf{U}\\odot\\mathbf{V})}_{\\mathrm{the~3th~level}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\odot$ denotes element-wise multiplication and $\\Theta=\\{\\{\\Theta_{i}^{j}\\}_{i=1}^{R}\\}_{j=1}^{R}$ is used to set the weights for the second-level and third-level similarity. We refer $\\mathrm{to}\\,\\Theta$ as the modality consistency matrix and \u0398ij as the modality consistency score. In \u0398, {{\u0398ij}ik\u00b7=B(k\u22121)\u00b7B+1}jR=1 i s the modality consistency score of the $k\\mathrm{th}$ modality and other modalities, which contain all the same $\\Theta_{i}^{j}$ values. We rename $\\Theta_{i}^{j}$ in $\\{\\{\\Theta_{i}^{j}\\}_{i=(k-1)\\cdot B+1}^{k\\cdot B}\\}_{j=1}^{R}$ to $\\theta_{k}$ and calculate it by the inter-modal mutual information under information theory [36]. Taking the $k$ th modality as an example, we use the projector $\\operatorname{g}_{k}(\\,\\cdot\\,)$ composed of MLP to map the reconstructed complete modality data into a low-dimensional feature space and activate it by the Softmax function $\\delta_{k}({\\bf\\nabla}\\cdot{\\bf\\xi})$ , i.e., $\\phi^{k}\\overset{\\cdot}{=}\\delta_{k}\\big(\\mathrm{g}_{k}\\big(\\bar{\\mathrm{x}}^{k}\\big)\\big)$ . Formally, ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\theta_{k}=\\frac{1}{M-1}\\sum_{i=1}^{M}\\mathbb{1}_{i\\neq k}\\cdot\\frac{I(\\phi^{k};\\phi^{i})}{H(\\phi^{k},\\phi^{i})}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbb{1}_{x}$ is an indicator, when $x$ is true, the result is $\"1\"$ , otherwise it is $\"0\"$ , $I(\\phi^{k};\\phi^{i})$ is the mutual information of $\\phi^{k}$ and $\\phi^{i}$ , $H(\\phi^{k},\\phi^{i})$ is the joint entropy of $\\phi^{k}$ and $\\phi^{i}$ . The value range of $\\theta_{k}$ is between 0 and 1, and it can automatically adjust the ratio of the second-level and third-level similarity according to the modal consistency. For instance, if the value of $\\theta_{k}$ is larger, it means that the inter-modal consistency is higher, but the amount of specific modal information is lower. Hence, it is necessary to increase the introduction of modal information, i.e., to increase the weight of the third-level similarity of formula (4). Vice versa. To more intuitively represent the construction process of the similarity weight W, we provide an example in Appendix C. To formulate IH((\u03d5\u03d5kk;,  \u03d5\u03d5ii)), we define a discrete joint probability distribution $\\mathcal{P}(m,n)$ and two discrete marginal probability distributions $\\mathcal{P}(m)$ and $\\mathcal{P}(n)$ . Since $\\phi^{\\tilde{k}}$ and $\\phi^{i}$ are activated by Softmax function, $\\bar{\\phi}^{k}$ and $\\phi^{i}$ can be regarded as the distribution of two discrete cluster assignment variables $m$ and $n$ on $\\dot{D}$ categories like [29, 37]. Among them, $\\dot{D}$ is the feature dimension of $\\phi^{k}.$ and $\\phi^{i}$ . Hence, we redefine $\\mathcal{P}(m,n)$ , $\\mathcal{P}(m)$ and P(n) as P = 12(\u03d5k(\u03d5i)T D\u02d9+ $\\begin{array}{r}{\\mathbf{P}_{-}^{-}\\frac{1}{2}(\\phi_{,\\cdot}^{k}(\\phi_{,\\cdot}^{i})^{\\mathrm{T}}\\vec{r}+\\phi_{,\\cdot}^{i}(\\underline{{\\phi}}^{k})^{\\mathrm{T}})\\in\\mathbb{R}_{-\\mathrm{~\\tiny~\\textnormal~{\\tiny~-~}~},\\mathrm{~\\tiny~\\textnormal~{\\tiny~{~D}~}~},\\mathrm{~\\tiny~{~P~}~}_{m}}^{B\\times\\dot{D}\\times\\dot{D}}=\\mathrm{Expand}(\\frac{1}{\\bar{D}}\\sum_{d_{n}=1}^{D}\\mathbf{P}_{\\bullet,\\bullet,d_{n}})\\in\\mathbb{R}^{B\\times\\dot{D}\\times\\dot{D}}}\\end{array}$ and $\\begin{array}{r}{{\\bf P}_{n}=\\mathrm{Expand}(\\frac{1}{\\hat{D}}\\sum_{d_{m}=1}^{\\nu}{\\bf P_{\\bullet,d_{m},\\bullet}})\\in\\mathbb{R}^{B\\times D\\times D}}\\end{array}$ , respectively. D\u02d9As ad nr=es1ult, then discrete form of $\\frac{I(\\phi^{k};\\phi^{i})}{H(\\phi^{k},\\phi^{i})}$ can be expressed as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{I(\\phi^{k};\\,\\phi^{i})}{H(\\phi^{k},\\,\\phi^{i})}=\\log_{\\frac{1}{\\mathbf{P}}}\\left(\\frac{\\mathbf{P}}{\\mathbf{P}_{m}\\mathbf{P}_{n}}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The theoretical result of formula (6) are demonstrated in Appendix D. To match the dimensions of   \ntcdhoaetn att rtwaoso t oirvbeteda eliofns $\\bar{\\mathbf Y}$ .mi  naAt ocf claaotntroedtnih neogrp  teporr ,cn t oworen ofbsoaert  tcah he n oocfv oreelm ccpoountnsatttrriauoscntti evodef $\\dot{\\bf X}=\\{\\bar{\\bf x}_{i}\\}_{i=1}^{B\\cdot M}$ $\\dot{\\mathbf X}$ $\\bar{g}(\\,\\cdot\\,)$ $\\boldsymbol{\\psi}\\stackrel{\\smile}{=}\\bar{\\boldsymbol{g}}(\\dot{\\mathbf{X}}),\\boldsymbol{\\psi}=\\{\\varphi_{i}\\}_{i=1}^{B\\cdot M}$ $\\mathbf{W}$   \nlearning, SMCCL,which can be defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\ell^{(s)}=\\frac{-1}{N_{w_{i}^{j}>0}-1}\\sum_{i=1}^{B\\cdot M}\\sum_{j=1}^{B\\cdot M}\\mathbb{1}_{i\\neq j}\\cdot\\mathbb{1}_{w_{i}^{j}>0}\\cdot w_{i}^{j}\\cdot\\log\\frac{\\exp(\\varphi_{i}\\cdot\\varphi_{j}/\\tau)}{\\sum_{k=1}^{B\\cdot M}\\mathbb{1}_{i\\neq k}\\cdot\\exp(\\varphi_{i}\\cdot\\varphi_{k}/\\tau)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\ell^{(s)}$ is named distribution alignment loss, $N_{w_{i}^{j}>0}-1$ is the number of $w_{i}^{j}>0$ in a batch and $\\tau$ is a temperature coefficient, which is set to 0.07 liike [38]. In SMCCL, $\\ell^{(s)}$ adjusts the attention given to different sample pairs based on $\\mathbf{W}$ , achieving more fine-grained distribution calibration. ", "page_idx": 5}, {"type": "text", "text": "3.4 Feature Extraction and Temporal Context Modeling ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As illustrated in Fig. 2, the recovered complete modal dataset $\\bar{\\mathbf{X}}=\\{\\{\\mathrm{x}_{i}^{j}\\}_{i=1}^{B}\\}_{j.}^{M}$ is also fed into the temporal CNN for feature extraction and concatenation to obtain multimodal fusion temporal representation $\\tilde{\\mathbf{F}}\\in\\mathbb{R}^{B\\times D\\times C}$ , $B=h\\cdot T$ during the distribution calibration process. Among them, $C$ is the number of channels, $D$ is the feature dimension, $h$ is the batch size and $T$ is the context length. Then, we utilize a Transformer composed of layer normalization (LN), MCTA, and MLP for temporal context modeling, thereby obtaining temporal representation $\\stackrel{\\leftrightarrow}{\\mathbf{F}}\\in\\vec{\\mathbb{R}}^{B\\times D\\times C}$ . We focus on introducing MCTA, with its single-head structure depicted in Fig. 3. Firstly, the fusion representation after the first LN is divided into $S$ heads, i.e., $\\dot{\\mathbf{F}}=\\dot{\\{\\mathrm{f}_{s}\\}}_{s=1}^{S}$ , where $\\dot{\\mathrm{f}}_{s}\\in\\dot{\\mathbb{R}^{B\\times D\\times(C/S)}}$ . After that, $\\dot{\\mathrm{f}}_{s}$ is fed into MCTA. It has two branches and includes intra-epoch and inter-epoch levels, which can fully mine the temporal context information of latent features. ", "page_idx": 5}, {"type": "text", "text": "Intra-epoch level: In the 1th branch, temporal CNN is adopted to generate the query $Q_{s}$ and the key $K_{s}^{(T)}$ and value $V_{s}^{(T)}$ . Related study [39] have proven that temporal CNN exhibits efficiency beyond linear operations, while also eliminating the requirement for positional encoding. In the 2th branch, we use Bi-GRU to learn the recurrent representation of $\\dot{\\mathrm{f}}_{s}$ . Similarly, key $\\bar{K}_{s}^{(B)}$ and value $\\bar{V}_{s}^{(B)}$ from $\\overline{{\\mathbf{f}}}_{s}^{(B)}$ are obtained via temporal CNN. To achieve cross-branch interaction, we splice $K_{s}^{(T)}$ and $V_{s}^{(T)}$ with K\u00afs(B) and $\\bar{V}_{s}^{(B)}$ . As a result, the intra-epoch cross-branch attention can be calculated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\dot{\\mathbf{f}}_{s}^{(T)}=\\operatorname{Intra\\_CA}_{s}=\\operatorname{Softmax}(\\frac{Q_{s}\\cdot K_{s}^{\\mathrm{T}}}{\\sqrt{C/S}})V_{s},K_{s}=\\left[K_{s}^{(T)}||\\bar{K}_{s}^{(B)}\\right],V_{s}=\\left[V_{s}^{(T)}||\\bar{V}_{s}^{(B)}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/e781fd057b08e0b72020499861d3853cf723f5955c40f02491ab0a324e8e8034.jpg", "img_caption": ["Figure 3: Design of the multi-level cross-branch temporal attention (MCTA) mechanism. $D$ and $T$ are the number of channels of temporal CNN at different levels; the values of $D/2$ and $T/2$ are rounded down; $\\mathbf{k}$ is the kernel size; st is the stride. $M$ and $N$ are the neuron counts of Bi-GRU at different levels, where $M=C/S$ and $N=D\\cdot C/S$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "In the interactive process, MCTA can effectively integrate recurrent bias into self-attention weights to improve the shortcomings of traditional Transformer recurrent modeling ability. ", "page_idx": 6}, {"type": "text", "text": "Inter-epoch level: As shown in Fig. 3, the 1th branch and the 2th branch of the inter-epoch level exhibit a reversed pattern compared to the intra-epoch level. This design enables MCTA to not only realize the interaction of cross-branch in parallel manner, but also capture rich temporal representations layer by layer. In this level, $\\mathbf{\\dot{f}}_{s}^{(T)}\\in\\mathbb{R}^{\\mathbf{\\dot{h}}\\times T\\times(D\\cdot C/S)}$ and $\\mathbf{\\overline{{f}}}_{s}^{(B)}\\in\\mathbb{R}^{h^{\\star}T\\times(D\\cdot C/S)}$ serve as the input of the two branches respectively. In the 1th branch, similar to the 2th branch at the intraepoch level, $\\dot{\\mathbf{f}}_{s}^{(T)}$ is mapped to $\\dot{\\mathbf{f}}_{s}^{(B)}$ , to obtain $K_{s}^{(B)}$ and $V_{s}^{(B)}$ . In the 2th branch, $\\overline{{\\mathbf{f}}}_{s}^{(B)}$ is mapped to $\\bar{Q}_{s}$ , $\\dot{K}_{s}$ and $\\bar{V}_{s}$ by temporal CNN. Likewise, the inter-epoch cross-branch attention can be calculated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\bar{\\mathbf{f}}_{s}^{(T)}=\\mathrm{Inter}_{-}\\mathbf{C}\\mathbf{A}_{s}=\\mathrm{Softmax}(\\frac{\\bar{Q}_{s}\\cdot\\bar{K}_{s}^{\\mathrm{T}}}{\\sqrt{C/S}})\\bar{V}_{s},\\bar{K}_{s}=\\left[\\bar{K}_{s}^{(T)}||K_{s}^{(B)}\\right],V_{s}=\\left[\\bar{V}_{s}^{(T)}||V_{s}^{(B)}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "After that, we concatenate $\\mathbf{\\dot{f}}_{s}^{(B)}\\in\\mathbb{R}^{h\\times T\\times(D\\cdot C/S)}$ and $\\mathbf{\\overline{{f}}}_{s}^{(T)}\\in\\mathbb{R}^{h\\times T\\times(D\\cdot C/S)}.$ , and perform dimensionality reduction via temporal CNN to obtain the fused representation $\\ddot{\\mathbf{f}}_{s}\\in\\mathbb{R}^{B\\times(\\ddot{D}/\\dot{S})}$ . Finally, extending single-head MCTA to multiple heads can be expressed as \u00a8F = [f\u00a81||f\u00a82|| \u00b7 \u00b7 \u00b7 ||f\u00a8S] \u2208RB\u00d7 D\u00a8. ", "page_idx": 6}, {"type": "text", "text": "3.5 Optimization Objective ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We utilize temporal representation $\\stackrel{\\leftrightarrow}{\\mathbf{F}}\\in\\mathbb{R}^{B\\times D\\times C}$ to perform sleep staging. Meanwhile, cross entropy loss $\\ell^{(c)}$ is regarded as a good choice to guide the learning of model parameters, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\ell^{(c)}=-\\frac{1}{B}\\sum_{i=1}^{B}\\sum_{j=1}^{K}\\tilde{\\mathbf{W}}_{j}\\left(y_{i,j}\\mathrm{ln}\\left(\\tilde{y}_{i,j}\\right)+\\left(1-y_{i,j}\\right)\\mathrm{ln}\\left(1-\\tilde{y}_{i,j}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $B$ is the batch size, $K$ is the number of categories, $\\tilde{\\bf W}$ is the category weight, $y$ is the real label and $\\tilde{y}$ is the predicted label. After that, we construct the total objective loss for CIMSleepNet. Formally, $\\overset{\\vartriangle}{\\boldsymbol{\\ell}}=\\ell^{(c)^{\\star}}+\\alpha\\ell^{(I)}+\\beta\\ell^{(s)}$ , where $\\alpha$ and $\\beta$ are the weight of the loss term. ", "page_idx": 6}, {"type": "text", "text": "4 EXPERIMENTS ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Datasets and Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets: Five multimodal sleep datasets, Sleep-EDF-20 [40, 41], Sleep-EDF-78 [40, 41], SVUHUCD [40], Motion and heart rate (MHR) [24] and SHHS [42, 43] are used for the effectiveness of CIMSleepNet. The first four datasets are used to verify the performance of CIMSleepNet when the modality is randomly partially missing, and the last dataset is used to verify its performance when the modality is completely missing. We choose EEG and EOG, for Sleep-EDF-20, Sleep-EDF-78 and SHHS; EEG, EOG and EMG, for SVUH-UCD; motion signal and HR, for MHR. We provide detailed introduction and preprocessing methods of all datasets in Appendix E. ", "page_idx": 6}, {"type": "table", "img_path": "bc1qt1sZsW/tmp/b0d6b087cd3c2e586f91613b4b71bdee809e31894fe079a36ad5edeb7d78c0b1.jpg", "table_caption": ["Table 1: Performance comparison for complete and incomplete modalities in randomly partially missing case. Here \"incomplete\" means the maximum missing rate. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Implementation Details: In the first four datasets, CIMSleepNet is trained and tested using $k$ -fold cross-validation, with a total of five repetitions of this procedure. Each result is the average of five results. In the last dataset, the training strategy refers to [26]. The detailed experimental settings and important hyperparameter settings are in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "4.2 Comparison with the state-of-the-arts ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In randomly partially missing case, ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "bc1qt1sZsW/tmp/75a0572c5be0cef5b7cee04cdd29cddfc846e3c06d5689527230ef27ad79bd38.jpg", "table_caption": ["Table 2: Performance comparison in completely missing case. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "we compare our CIMSleepNet with 8 ASS methods that can support multimodal learning: FeatConcat, MultitaskCNN [8], SalientSleepNet [23], MM-Net [11], TransSleep [16], XSleepNet [10], MLP [24] and DeepCNN [9]. We leverage mask matrix Z and the public code of these methods to simulate the incomplete modality case. Then, we compare CIMSleepNet with them under different missing rate $\\rho$ . According to the calculation ", "page_idx": 7}, {"type": "text", "text": "formula of $\\rho$ , for two modalities, the missing rate ranges from [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]; for three modalities, the missing rate ranges from [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], where 0.7 is an approximate value of $2/3$ . In the completely missing case, we compare CIMSleepNet with CoRe-Sleep [26], the only existing ASS method that can handle complete missing of one or more modalities. We employ accuracy (Acc), macro F1-score (MF1) and Cohen Kappa $(K)$ [44] to quantitatively analyze all methods. We also compare the data recovery performance of SMCCL with ICL [28] and SCL [30]. All methods are described in Appendix G. ", "page_idx": 7}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/77eb2bbcaaf3fba4b323e81b3ef905f0bbbfd2fa478cc37c0a96f8ac110c787e.jpg", "img_caption": ["Figure 4: Impact of various missing rates.The shaded area represents the range of upper and lower standard deviations. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Quantitative results: As shown in Tab 1, CIMSleepNet achieves performance comparable to the state-of-the-arts in the complete modality. In the incomplete modalities, compared to the performance on complete modalities, all models exhibit a decrease in performance on the four datasets. Fortunately, CIMSleepNet has the least performance degradation and performs the best. As schematized in Fig. 4, we further evaluate the performance of CIMSleepNet and other methods under different missing rates. We observe that CIMSleepNet outperforms other methods in almost all datasets and missing rates. As the missing rate increases, the performance of other methods begins to decline significantly. Relatively speaking, CIMSleepNet exhibits a more stable trend. Further, Tab 2 exhibits the performance of CIMSleepNet trained with $\\rho\\:=\\:0.5$ (maximizing the model\u2019s robustness to missing modalities) and tested under complete modality absence. We observe that CIMSleepNet ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/9e84aaece642e6e0f7c542105e5bff299cb7f8bf8f5deb16ecb73c7517cc9a9d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Visualization of the recovered modalities by ICL, SCL and SMCCL. ", "page_idx": 8}, {"type": "text", "text": "outperforms CoRe-Sleep in terms of performance across different testing modalities. ", "page_idx": 8}, {"type": "text", "text": "Qualitative results: We substitute ICL and SCL with SMCCL on CIMSleepNet to compare the performance of these three contrastive learning methods in data recovery (when $\\rho=0.5)$ ). As depicted in Fig. 5, we randomly selected 500 recovered missing samples (500 EEG epochs and 500 EOG epochs) in Sleep-EDF-20 and projected them into 2D space via t-SNE [45]. ICL only focuses on the inter-modal consistency and ignores the recovery of semantic information. SCL retains semantic information based on ICL, thereby improving data matching. However, ICL and SCL tend to learn the inter-modal consistency, i.e., utilize multimodal shared information to guide the recovery of missing data. This strategy easily leads to the loss of modality-specific information, thus failing to exploit the inter-modal complementarity. Different from ICL and SCL, our SMCCL explores the intrinsic connection between semantic and modal information under mutual information theory. Hence, compared to ICL and SCL, the data recovered by SMCCL exhibits a more consistent distribution with the original data, further demonstrating its effectiveness in handling missing modalities. We also visualize the features extracted by each method. Specifically, we randomly select the data of one subject (9th) among the Sleep-EDF-20. As illustrated in Fig. 6, we use t-SNE [45] to visualize the distribution of features generated by all methods at $\\rho=0.5$ , which are extracted before the final decision head. Compared with other methods, our CIMSleepNet can extract more discriminative representations in incomplete modalities, further demonstrating its robustness. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/c6cd47670470f49d2b19e90d583926aca9fbfb26945ad95d6ff7a8085c7434c8.jpg", "img_caption": ["Figure 6: Visualization of latent features of different methods on Sleep-EDF-20. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Ablation studies: We conduct ablation studies for CIMSleepNet on Sleep-EDF-20 under the condition of missing rate $\\rho=0.5$ . It can be observed from Tab 3 that no matter which component is deleted, each evaluation metric of the results will decrease. It is particularly noteworthy that in the absence of both MAIM and SMCCL, the performance drops significantly, further demonstrating their importance in dealing with the missing modality issue. Furthermore, we find that although the two components designed to mitigate modality missing issue (MAIM and SMCCL) introduce additional parameters, the increase is much less than that introduced by the sequence modeling component (MCTA). However, sequence modeling is crucial for capturing the temporal information of PSs and improving model performance [10, 46]. The ablation experiment of MCTA, parameter sensitivity analysis and training process analysis are detailed in Appendix H, I and J, respectively. ", "page_idx": 9}, {"type": "table", "img_path": "bc1qt1sZsW/tmp/4ecd0c96ba0c616bee3e26fecf56c7b1d0172951236b8d324fe8e004fa8ed09d.jpg", "table_caption": ["Table 3: Ablation study of CIMSleepNet on Sleep-EDF-20. \u201c\u2713\u201d indicates the use of this component. MCTA indicates the Transformer equipped with MCTA. The context length of single inference is 25. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We try to challenge multimodal ASS under incomplete modalities by proposing CIMSleepNet. In CIMSleepNet, MAIM reconstructs missing modality data by establishing interactions among modalities, which allows for the provision of complete modality data support for subsequent components. Meanwhile, SMCCL ingeniously leverages semantic information and modal information to subdivide similarity into three levels, thereby simulating real data distribution. Then, MCTA mechanism accomplishes comprehensive temporal context modeling, further improving the expressive ability of latent temporal representations. Extensive experiments demonstrate that the effectiveness of CIMSleepNet in various incomplete modalities. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the National Natural Science Foundation of China (62072089) and the Fundamental Research Funds for the Central Universities of China (N2116016, N2224001-10). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S. G\u00fcne\u00b8s, K. Polat, and $\\mathrm{S}$ . Yosunkaya, \u201cEfficient sleep stage recognition system based on eeg signal using $\\mathbf{k}$ -means clustering based feature weighting,\u201d Expert Systems with Applications, vol. 37, no. 12, pp. 7922\u20137928, 2010.   \n[2] M. Perslev, M. Jensen, S. Darkner, P. J. Jennum, and C. Igel, \u201cU-time: A fully convolutional network for time series segmentation applied to sleep staging,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.   \n[3] Z. Jia, Y. Lin, J. Wang, R. Zhou, X. Ning, Y. He, and Y. Zhao, \u201cGraphsleepnet: Adaptive spatial-temporal graph convolutional networks for sleep stage classification.\u201d in IJCAI, vol. 2021, 2020, pp. 1324\u20131330.   \n[4] J. Wang, S. Zhao, H. Jiang, S. Li, T. Li, and G. Pan, \u201cGeneralizable sleep staging via multi-level domain alignment,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 1, 2024, pp. 265\u2013273.   \n[5] S. Ma, Y. Zhang, Y. Chen, T. Xie, S. Song, and Z. Jia, \u201cExploring structure incentive domain adversarial learning for generalizable sleep stage classification,\u201d ACM Transactions on Intelligent Systems and Technology, vol. 15, no. 1, pp. 1\u201330, 2024.   \n[6] T. R. Sri, J. Madala, S. L. Duddukuru, R. Reddipalli, P. K. Polasi et al., \u201cA systematic review on deep learning models for sleep stage classification,\u201d in 2022 6th International Conference on Trends in Electronics and Informatics (ICOEI). IEEE, 2022, pp. 1505\u20131511.   \n[7] G. Muhammad, F. Alshehri, F. Karray, A. El Saddik, M. Alsulaiman, and T. H. Falk, \u201cA comprehensive survey on multimodal medical signals fusion for smart healthcare systems,\u201d Information Fusion, vol. 76, pp. 355\u2013375, 2021.   \n[8] H. Phan, F. Andreotti, N. Cooray, O. Y. Ch\u00e9n, and M. De Vos, \u201cJoint classification and prediction cnn framework for automatic sleep stage classification,\u201d IEEE Transactions on Biomedical Engineering, vol. 66, no. 5, pp. 1285\u20131296, 2018.   \n[9] B. Zhai, Y. Guan, M. Catt, and T. Pl\u00f6tz, \u201cUbi-sleepnet: Advanced multimodal fusion techniques for three-stage sleep classification using ubiquitous sensing,\u201d Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 5, no. 4, pp. 1\u201333, 2021.   \n[10] H. Phan, O. Y. Ch\u00e9n, M. C. Tran, P. Koch, A. Mertins, and M. De Vos, \u201cXsleepnet: Multi-view sequential model for automatic sleep staging,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 5903\u20135915, 2021.   \n[11] Y. Lin, M. Wang, F. Hu, X. Cheng, and J. Xu, \u201cMultimodal polysomnography based automatic sleep stage classification via multiview fusion network,\u201d IEEE Transactions on Instrumentation and Measurement, 2023.   \n[12] D. Ramachandram and G. W. Taylor, \u201cDeep multimodal learning: A survey on recent advances and trends,\u201d IEEE signal processing magazine, vol. 34, no. 6, pp. 96\u2013108, 2017.   \n[13] A. Supratak, H. Dong, C. Wu, and Y. Guo, \u201cDeepsleepnet: A model for automatic sleep stage scoring based on raw single-channel eeg,\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 25, no. 11, pp. 1998\u20132008, 2017.   \n[14] H. Phan, F. Andreotti, N. Cooray, O. Y. Ch\u00e9n, and M. De Vos, \u201cSeqsleepnet: end-to-end hierarchical recurrent neural network for sequence-to-sequence automatic sleep staging,\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 27, no. 3, pp. 400\u2013410, 2019.   \n[15] H. Seo, S. Back, S. Lee, D. Park, T. Kim, and K. Lee, \u201cIntra-and inter-epoch temporal context network (iitnet) using sub-epoch features for automatic sleep scoring on raw single-channel eeg,\u201d Biomedical signal processing and control, vol. 61, p. 102037, 2020.   \n[16] J. Phyo, W. Ko, E. Jeon, and H.-I. Suk, \u201cTranssleep: Transitioning-aware attention-based deep neural network for sleep staging,\u201d IEEE Transactions on Cybernetics, 2022.   \n[17] H. W. Loh, C. P. Ooi, J. Vicnesh, S. L. Oh, O. Faust, A. Gertych, and U. R. Acharya, \u201cAutomated detection of sleep stages using deep learning techniques: A systematic review of the last decade (2010\u20132020),\u201d Applied Sciences, vol. 10, no. 24, p. 8963, 2020.   \n[18] W. Qu, Z. Wang, H. Hong, Z. Chi, D. D. Feng, R. Grunstein, and C. Gordon, \u201cA residual based attention model for eeg based sleep staging,\u201d IEEE journal of biomedical and health informatics, vol. 24, no. 10, pp. 2833\u20132843, 2020.   \n[19] E. Eldele, Z. Chen, C. Liu, M. Wu, C.-K. Kwoh, X. Li, and C. Guan, \u201cAn attention-based deep learning approach for sleep stage classification with single-channel eeg,\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 29, pp. 809\u2013818, 2021.   \n[20] Y. Dai, X. Li, S. Liang, L. Wang, Q. Duan, H. Yang, C. Zhang, X. Chen, L. Li, X. Li et al., \u201cMultichannelsleepnet: A transformer-based model for automatic sleep stage classification with psg,\u201d IEEE Journal of Biomedical and Health Informatics, 2023.   \n[21] A. Bulatov, Y. Kuratov, and M. Burtsev, \u201cRecurrent memory transformer,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 11 079\u201311 091, 2022.   \n[22] W. Zhou, S.-I. Kamata, H. Wang, and X. Xue, \u201cMultiscanning-based rnn-transformer for hyperspectral image classification,\u201d IEEE Transactions on Geoscience and Remote Sensing, 2023.   \n[23] Z. Jia, Y. Lin, J. Wang, X. Wang, P. Xie, and Y. Zhang, \u201cSalientsleepnet: Multimodal salient wave detection network for sleep staging,\u201d arXiv preprint arXiv:2105.13864, 2021.   \n[24] O. Walch, Y. Huang, D. Forger, and C. Goldstein, \u201cSleep stage prediction with raw acceleration and photoplethysmography heart rate data derived from a consumer wearable device,\u201d Sleep, vol. 42, no. 12, p. zsz180, 2019.   \n[25] M. Olsen, J. M. Zeitzer, R. N. Richardson, P. Davidenko, P. J. Jennum, H. B. S\u00f8rensen, and E. Mignot, \u201cA flexible deep learning architecture for temporal sleep stage classification using accelerometry and photoplethysmography,\u201d IEEE Transactions on Biomedical Engineering, vol. 70, no. 1, pp. 228\u2013237, 2022.   \n[26] K. Kontras, C. Chatzichristos, H. Phan, J. Suykens, and M. De Vos, \u201cCore-sleep: A multimodal fusion framework for time series robust to imperfect modalities.\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, 2024.   \n[27] Y. Lin, Y. Gou, Z. Liu, B. Li, J. Lv, and X. Peng, \u201cCompleter: Incomplete multi-view clustering via contrastive prediction,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 11 174\u201311 183.   \n[28] R. Liu, H. Zuo, Z. Lian, B. W. Schuller, and H. Li, \u201cContrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities,\u201d IEEE Transactions on Affective Computing, 2024.   \n[29] Y. Lin, Y. Gou, X. Liu, J. Bai, J. Lv, and X. Peng, \u201cDual contrastive prediction for incomplete multi-view representation learning,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4447\u20134461, 2022.   \n[30] S. Qian and C. Wang, \u201cCom: Contrastive masked-attention model for incomplete multimodal learning,\u201d Neural Networks, vol. 162, pp. 443\u2013455, 2023.   \n[31] D. Chen, S. Yongchareon, E. M.-K. Lai, J. Yu, Q. Z. Sheng, and Y. Li, \u201cTransformer with bidirectional gru for nonintrusive, sensor-based activity recognition in a multiresident environment,\u201d IEEE Internet of Things Journal, vol. 9, no. 23, pp. 23 716\u201323 727, 2022.   \n[32] R. Pramanik, R. Sikdar, and R. Sarkar, \u201cTransformer-based deep reverse attention network for multi-sensory human activity recognition,\u201d Engineering Applications of Artificial Intelligence, vol. 122, p. 106150, 2023.   \n[33] Q. Luo, S. He, X. Han, Y. Wang, and H. Li, \u201cLsttn: A long-short term transformer-based spatiotemporal neural network for traffic flow forecasting,\u201d Knowledge-Based Systems, vol. 293, p. 111637, 2024.   \n[34] K. T. Ahmed, S. Baul, Y. Fu, and W. Zhang, \u201cAttention-based multi-modal missing value imputation for time series data with high missing rate,\u201d in Proceedings of the 2023 SIAM International Conference on Data Mining (SDM). SIAM, 2023, pp. 469\u2013477.   \n[35] D. Khattar, J. S. Goud, M. Gupta, and V. Varma, \u201cMvae: Multimodal variational autoencoder for fake news detection,\u201d in The world wide web conference, 2019, pp. 2915\u20132921.   \n[37] J. Huang, S. Gong, and X. Zhu, \u201cDeep semantic clustering by partition confidence maximisation,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 8849\u2013 8858.   \n[38] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan, \u201cSupervised contrastive learning,\u201d Advances in neural information processing systems, vol. 33, pp. 18 661\u2013 18 673, 2020.   \n[39] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan, \u201cEnhancing the locality and breaking the memory bottleneck of transformer on time series forecasting,\u201d Advances in neural information processing systems, vol. 32, 2019.   \n[40] A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley, \u201cPhysiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals,\u201d circulation, vol. 101, no. 23, pp. e215\u2013e220, 2000.   \n[41] B. Kemp, A. H. Zwinderman, B. Tuk, H. A. Kamphuisen, and J. J. Oberye, \u201cAnalysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg,\u201d IEEE Transactions on Biomedical Engineering, vol. 47, no. 9, pp. 1185\u20131194, 2000.   \n[42] S. F. Quan, B. V. Howard, C. Iber, J. P. Kiley, F. J. Nieto, G. T. O\u2019Connor, D. M. Rapoport, S. Redline, J. Robbins, J. M. Samet et al., \u201cThe sleep heart health study: design, rationale, and methods,\u201d Sleep, vol. 20, no. 12, pp. 1077\u20131085, 1997.   \n[43] G.-Q. Zhang, L. Cui, R. Mueller, S. Tao, M. Kim, M. Rueschman, S. Mariani, D. Mobley, and S. Redline, \u201cThe national sleep research resource: towards a sleep data commons,\u201d Journal of the American Medical Informatics Association, vol. 25, no. 10, pp. 1351\u20131358, 2018.   \n[44] J. Cohen, \u201cA coefficient of agreement for nominal scales,\u201d Educational and psychological measurement, vol. 20, no. 1, pp. 37\u201346, 1960.   \n[45] L. Van der Maaten and G. Hinton, \u201cVisualizing data using t-sne.\u201d Journal of machine learning research, vol. 9, no. 11, 2008.   \n[46] H. Phan, K. P. Lorenzen, E. Heremans, O. Y. Ch\u00e9n, M. C. Tran, P. Koch, A. Mertins, M. Baumert, K. B. Mikkelsen, and M. De Vos, \u201cL-seqsleepnet: Whole-cycle long sequence modeling for automatic sleep staging,\u201d IEEE Journal of Biomedical and Health Informatics, vol. 27, no. 10, pp. 4748\u20134757, 2023.   \n[47] B. Yang, W. Wu, Y. Liu, and H. Liu, \u201cA novel sleep stage contextual refinement algorithm leveraging conditional random fields,\u201d IEEE Transactions on Instrumentation and Measurement, vol. 71, pp. 1\u201313, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our work also has some limitations. To deal with the missing modality issue and temporal dependency, we introduce an additional architecture, which will incur additional computational overhead. It is worth mentioning that the lack of labeling information is also a common phenomenon in real-world applications. Hence, we expect to develop an unsupervised or semi-supervised learning multimodal approach in our forthcoming study, which can simultaneously address the challenges of modality missing and label missing. ", "page_idx": 13}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our work facilitates the daily sleep quality assessment and sleep disorder diagnosis for the public, and lays the foundation for promoting personalized treatment of sleep disorders. However, the multimodal physiological data required for model training may involve sensitive personal health data, which may bring potential social privacy and security issues. ", "page_idx": 13}, {"type": "text", "text": "C Similarity Weight Matrix Construction Example ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Supposing that a batch contains 3 multimodal epochs and the number of modal types is 3, the calculation process and results of the similarity weight matrix W are illustrated in Fig. 7. Specifically, the redefinition of matrices $\\mathbf{Y}$ and S is realized by using flatten and replicate operations. Moreover, calculate $\\bar{\\mathbf Y}$ and $\\bar{\\bf S}$ separately to obtain the mask matrices $\\mathbf{U}$ and $\\mathbf{V}$ by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{U}=\\{\\{u_{i}^{j}\\}_{i=1}^{R}\\}_{j=1}^{R},u_{i}^{j}=\\left\\{\\begin{array}{l l}{1,}&{\\bar{y}_{i}^{j}=\\dot{y}_{i}^{j}}\\\\ {0,}&{\\bar{y}_{i}^{j}\\neq\\dot{y}_{i}^{j}}\\end{array}\\right.\\mathbf{V}=\\{\\{v_{i}^{j}\\}_{i=1}^{R}\\}_{j=1}^{R},v_{i}^{j}=\\left\\{\\begin{array}{l l}{1,}&{\\bar{s}_{i}^{j}=\\dot{s}_{i}^{j}}\\\\ {0,}&{\\bar{s}_{i}^{j}\\neq\\dot{s}_{i}^{j}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\"1\"$ is a positive pair and $\"0\"$ is a negative pair. Besides, $\\dot{y}_{i}^{j}$ and $\\dot{s}_{i}^{j}$ are the elements in $\\bar{\\mathbf{Y}}^{\\mathrm{T}}$ and $\\bar{\\bf S}^{\\mathrm{T}}$ respectively. Further, combining modality consistency matrix $\\Theta$ , the similarity weight matrix $\\mathbf{W}$ can be constructed by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{W}=\\underbrace{\\mathbf{U}\\odot\\mathbf{V}}_{\\mathrm{the~lth~level}}+\\underbrace{(1\\!-\\!\\Theta)(\\mathbf{U}-\\mathbf{U}\\odot\\mathbf{V})}_{\\mathrm{the~2th~level}}+\\underbrace{\\Theta(\\mathbf{V}-\\mathbf{U}\\odot\\mathbf{V})}_{\\mathrm{the~3th~level}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\odot$ denotes element-wise multiplication and $\\Theta$ in Fig. 7 is composed of the set consisting of $\\theta_{1}$ , $\\theta_{2}$ and $\\theta_{3}$ . ", "page_idx": 13}, {"type": "text", "text": "D Theoretical Proof ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We simplify IH((\u03d5\u03d5kk;,  \u03d5\u03d5ii)) in continuous random variables. It can be expressed as follows ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{I(\\phi^{k},\\phi^{k})}{H(\\phi^{k},\\phi^{k})}=\\frac{H(\\phi^{k})+H(\\phi^{l})-H(\\phi^{k},\\phi^{l})}{H(\\phi^{k},\\phi^{k})}}\\\\ &{=\\frac{\\int P(x)\\ln\\frac{1}{\\mu x}d x+\\int P(\\phi)\\ln\\frac{1}{\\mu x}d y-\\int P(x,y)\\ln\\frac{1}{\\mu x}d x d y}{\\int\\int P(x)\\ln\\frac{1}{\\mu x}d x d y}}\\\\ &{=\\frac{\\int\\int P(x,y)\\ln\\frac{1}{\\mu x}d x d y+\\int\\int p(x,y)\\ln\\frac{1}{\\mu x}d x d y-\\int\\int p(x,y)\\ln\\frac{1}{\\mu x}d x d y}{\\int\\int p(x,y)\\ln\\frac{1}{\\mu x}d x d y}}\\\\ &{=\\frac{\\int\\int P(x,y)\\ln\\frac{1}{\\mu x}d x d y-\\int\\int p(x,y)\\ln\\frac{1}{\\mu x}d x d y}{\\int\\int P(x,y)\\ln\\frac{1}{\\mu x}d x d y}}\\\\ &{=\\frac{\\int\\int P(x,y)\\ln\\frac{1}{\\mu x}d x d y}{\\int\\int P(x,y)\\ln\\frac{1}{\\mu x}d x d y}d x d y}\\\\ &{=\\frac{\\int\\int P(x,y)\\ln\\frac{\\mu x}{\\mu x})d x d y}{\\int\\int P(x,y)\\ln\\frac{1}{\\mu x}d x d y}}\\\\ &{=\\int\\int\\log\\frac{1}{\\mu x}\\left(\\frac{\\int P(x,y)}{\\rho(x,y)\\ln\\frac{1}{\\mu x}d y}\\right)d x d y}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/db56cebd71ece03e71456a23dc5f2c897a1395a8617d1650ca15e2b527fc4c3d.jpg", "img_caption": ["Figure 7: Example of similarity weight matrix W construction. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "where $p(x)$ and $p(y)$ are the marginal probability distributions of $\\phi^{k}$ and $\\phi^{i}$ , respectively, i.e., the continuous form of $\\mathcal{P}(m)$ and $\\mathcal{P}(n)$ in formula (6). $p(x,y)$ is their joint probability distribution, i.e., the continuous form of $\\mathcal P(m,n)$ in formula (6). Therefore, the expression of formula (6) is obtained. ", "page_idx": 14}, {"type": "text", "text": "E Data and Preprocessing ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1) Sleep-EDF-20 [40, 41]: The dataset has been widely applied in sleep study, comprising 39 nights of PSG recordings from 20 subjects. The subjects are aged between 25 and 34 years old, with 10 males and 10 females. Each recording are divided into epochs in units of 30 seconds. The data preprocessing method draws on the previous work [10]. After preprocessing, the context length $T$ is set to 25, and the redundant segments at the front end are discarded.. These epochs are classified into five different categories, including wake (W), rapid eye movement (REM), and three types of non-REM (N1, N2, and N3). Then, two modalities, electroencephalogram (EEG) (Fpz-Cz channel) and electrooculogram (EOG) (ROC-LOC channel), are utilized to evaluate CIMSleepNet. Among them, the sampling frequency of EEG and EOG is both $100\\mathrm{Hz}$ . ", "page_idx": 14}, {"type": "text", "text": "2) Sleep-EDF-78 [40, 41]: The dataset includes 153 recordings from 78 subjects. The subjects have a wide age range, from 25 to 101 years old, and included 41 males and 37 females. Similarly, the length of each epoch is 30 seconds, and the method [10] is utilized to preprocess Sleep-EDF-78 data. We set the context length $T$ to 25 for each modality. The choice of modalities, categories and sampling frequency are the same as for Sleep-EDF-20. ", "page_idx": 15}, {"type": "text", "text": "3) SVUH-UCD [40]: The dataset focuses on sleep staging study with sleep disorders. It includes 25 PSG recordings from 25 sleep apnea patients. Their ages ranged from 28 to 68, including 21 males and 4 females. Following previous study [47], we choose EEG (C3-A2 channel, $128\\,\\mathrm{Hz}$ ), EOG (horizontal channel, $64\\:\\mathrm{Hz}$ ) and EMG $(64\\,\\mathrm{Hz})$ , and resample these recordings to $100\\mathrm{Hz}$ . Furthermore, we also set the context length $T$ to 25. This dataset are also divided into five sleep stages. ", "page_idx": 15}, {"type": "text", "text": "4) MHR [24]: This is a public sleep dataset based on wearable devices, which contains overnight sleep recordings from 31 subjects. Subjects are allowed 8 hours of sleep monitoring opportunities and each epoch is 30 seconds in length. We preprocess this dataset using the method described in previous work [24]. After the preprocessing is completed, we set the context length $T$ to 20 and exclude redundant epochs. Following the usage rules of this dataset, we employ two modalities, the motion signals composed of three-axis accelerometry and the heart rate signals, to perform classification tasks for the three categories: W, non-REM, and REM. In these two modalities, the sampling frequency of motion signal and heart rate signal is $50\\mathrm{Hz}$ and 1Hz respectively. ", "page_idx": 15}, {"type": "text", "text": "5) SHHS [42, 43]: This dataset is a large sleep dataset collected from multiple sleep centers, which contains two sub-datasets, namely SHHS-1 and SHHS-2. Following previous studies [10, 26], we choose SHHS-1 for our experiment. SHHS-1 consists of 5,791 subjects aged between 39 and 90 years old. We employ the EEG (C4-A1 channel, $125\\mathrm{Hz})$ ) and EOG (L-R channel, 50Hz), and resample them to $100\\,\\mathrm{Hz}$ . Furthermore, the context length $T$ is set to 25. We also divide it into five sleep stages. ", "page_idx": 15}, {"type": "text", "text": "F Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We choose the programming language based on Python 3.8 and the deep learning framework based on PyTorch 1.13 to build and train the model. All experiments are conducted on a server containing an RTX 4090 GPU (24GB) and an Intel(R) Xeon(R) Gold 6430 processor (120GB) equipped with 16 virtual CPUs. The total objective loss is mainly optimized through the Adam optimizer. In the first four dataset, CMISleepNet is trained (Held-out validation set 4 subjects for Sleep-EDF-20, SVUHUCD and MHR; 7 subjects for Sleep-EDF-78) and tested by $k$ -fold cross-validation. Specifically, CMISleepNet performs five random samplings and five $k$ -fold cross-validations for each missing rate in every dataset. After each $k$ -fold cross-validation, the prediction results from the test sets of all folds are combined as one time result. Each missing rate result is the average of five results. In the last dataset, the training strategy refers to [26], i.e., using a random split of 0.7 and 0.3 for the train (Held-out 100 subjects for validation) and test set. ", "page_idx": 15}, {"type": "text", "text": "The important hyperparameters on different datasets can be described as: We set the learning rate of all datasets to 0.001 and 0.0001 before and after the 10th iteration, respectively. The weight decay is set to 0.0001 for all datasets. The maximum number of iterations is set to 100 for all datasets. The number of intra-epoch heads $S_{1}=4$ and the number of inter-epoch heads $S_{2}=8$ for all datasets. The number of cross-validation folds, $k$ , is set to 20 for Sleep-EDF-20; 20 for Sleep-EDF-78; 25 for SVUH-UCD and 15 for MHR. The coefficient set $\\tilde{\\bf W}$ utilized to adjust category weights are set to [1.5, 2.5, 1.5,1.0, 1.5] for Sleep-EDF-20; [1.5, 2.2, 1.5,1.0, 1.5] for Sleep-EDF-78; [1.5, 2.0, 1.5,1.0, 1.5] for SVUH-UCD; [ 2.0, 1.0, 2.0] for MHR; [2.0, 3.0, 1.5,1.0, 1.5] for SHHS. Further, we set the coefficients $\\alpha$ and $\\beta$ of the total objective function to 0.001 and 0.01, respectively for all datasets. ", "page_idx": 15}, {"type": "text", "text": "G Compared Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In randomly partially missing case, we compare our CIMSleepNet with 8 ASS methods that can support multimodal learning: FeatConcat, MultitaskCNN [8], SalientSleepNet [23], MM-Net [11], TransSleep [16], XSleepNet [10], MLP [24] and DeepCNN [9]. In the completely missing case, we compare CIMSleepNet with CoRe-Sleep [26], the only existing ASS method that can handle complete missing of one or more modalities. We also compare the data recovery performance of SMCCL with ICL [28] and SCL [30]. All methods are described as follows. ", "page_idx": 15}, {"type": "text", "text": "1) FeatConcat: The temporal CNNs are utilized as the feature extractor of each modality data, and the features of different modalities are directly fused. Moreover, the fused features are sent to MLP for classification. ", "page_idx": 16}, {"type": "text", "text": "2) MultitaskCNN [8]: Based on the sleep staging task, the task of predicting adjacent epochs is added to improve the multimodal sleep staging performance. ", "page_idx": 16}, {"type": "text", "text": "3) SalientSleepNet [23]: A dual-branch U2-Net structure is proposed to improve the feature extraction of salient waves of multimodal physiological signals. ", "page_idx": 16}, {"type": "text", "text": "4) MM-Net [11]: A cross-link fusion module is exploited to reduce redundant information of multi-modality and multi-view. ", "page_idx": 16}, {"type": "text", "text": "5) TransSleep [16]: Two auxiliary tasks, segment confusion stage estimation and stage-transition detection, are designed to address stage transitions during sleep. We make it capable of handling multimodal data by increasing the number of channels at the input head of TransSleep. ", "page_idx": 16}, {"type": "text", "text": "6) XSleepNet [10]: An adaptive gradient blending strategy is designed to improve the joint representation ability of the original signal and the corresponding time-frequency image. ", "page_idx": 16}, {"type": "text", "text": "7) MLP [24]: Combining motion features and heart rate features, and using MLP to implement sleep staging (Wake/ NREM/ REM) based on wearable devices. ", "page_idx": 16}, {"type": "text", "text": "8) DeepCNN [9]: A deep CNN is constructed to explore the impact of early-stage fusion, late-stage fusion and hybrid fusion. We chose the late-stage fusion solution because it has the best performance. ", "page_idx": 16}, {"type": "text", "text": "9) CoRe-Sleep [26]: The ingenious combination of self-attention and cross-attention improves the robustness of the model under imperfect data. ", "page_idx": 16}, {"type": "text", "text": "10) ICL [28]: Improving modal consistency by bringing different modalities of the same instance closer together. ", "page_idx": 16}, {"type": "text", "text": "11) SCL [30]: The introduction of semantic information improves the ability to recover the semantic structure information of data. ", "page_idx": 16}, {"type": "text", "text": "H Ablation studies of MCTA ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We also explore the internal details of Transformer equipped with MCTA. The six baseline models in Tab 4 are substructures of Transformer equipped with MCTA. Among them, Intra- $\\cal X$ denotes using two $X$ layers for intra-epoch temporal dependency modeling. Inter- $X$ denotes using two $X$ layers for inter-epoch context modeling. Intra $\\&$ Inter- $X$ denotes using four $X$ layers for temporal context modeling, with the first two layers capturing intra-epoch temporal dependency and the last two layers capturing inter-epoch context. $X$ refers to GRU or Transformer. It can be observed from the results that both Intra & Inter-GRU and Intra & Inter-Transformer outperform their respective single-level models. Further, CIMSleepNet performs the best when using Transformer equipped with MCTA, which proves the effectiveness of multi-level cross-branch representation fusion. ", "page_idx": 16}, {"type": "table", "img_path": "bc1qt1sZsW/tmp/794e7e3c9857656c471174a621f00a196cfd7e0d12004112fead1c40bd1f0eb5.jpg", "table_caption": ["Table 4: Ablation study of Transformer equipped with MCTA on Sleep-EDF-20. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "I Parameter Sensitivity Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We explore the impact of $\\alpha$ and $\\beta$ on the performance of CIMSleepNet on the Sleep-EDF-20. As shown in Fig. 8, as the values of $\\alpha$ and $\\beta$ change, the performance of CIMSleepNet fluctuates to varying degrees. We also observe that $\\beta$ has a greater sensitivity to CIMSleepNet compared to $\\alpha$ . ", "page_idx": 17}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/ef4e02794f0043cec8ac0bb6bad8cd7eb6e04fd0eb19af6d739fbfda8efcf83a.jpg", "img_caption": ["Figure 8: Hyperparameters, $\\alpha$ and $\\beta$ , analysis on Sleep-EDF-20. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "J Training Process Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As schematized in Fig. 9, we provide visualizations of different validation loss curves to explore their real-time changes during training. We can observed: During the entire training process, modal imagination loss and distribution alignment loss will decrease as the number of iterations increases, which shows that the data imputation ability and distribution ftiting ability of the model are gradually improving. It is worth mentioning that, in the early stages of training, the rate of decrease in distribution alignment loss is greater than that of modal imagination loss. This phenomenon occurs because the data generated during the initial training stage deviates significantly from the real distribution, requiring substantial adjustments through distribution alignment loss function. When the modal imagination loss and distribution alignment loss are in a stationary state, the classification loss continues to decrease, which indicates that the data generated in the stationary state will further improve the classification performance of the model. ", "page_idx": 17}, {"type": "image", "img_path": "bc1qt1sZsW/tmp/9ee9d36df93e18f849f9d2745e1257767554810630f5e43e82436adec0334906.jpg", "img_caption": ["Figure 9: Training dynamics of modal imagination loss, distribution alignment loss and classification loss on Sleep-EDF-20. Among them, modal imagination loss is presented in two modalities: EEG and EOG respectively. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main claims contained in the abstract and introduction accurately reflect the contribution and scope of our study. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We create a separate section in Appendix A to discuss the limitations of this study. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The manuscript contains proofs of theoretical results, and all theorems, formulas and proofs are supported by corresponding theories and references. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: This manuscript fully presents all the information necessary to reproduce the main experimental results of the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The datasets used in this study are all public datasets, and we provide their citations. In addition, we open the corresponding codes. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: All experimental details are described in detail in Appendix F. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our experimental results are the average of training under multiple random seeds. The error margins are included in the results for various missing modalities. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide detailed information about the computational resources in the Appendix F. We also provide experiment on computational efficiency in Tab 3. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Our manuscript complies with the NeurIPS ethical guidelines both in terms of research area as well as methods and datasets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We create a separate section in Appendix B to discuss the broader impacts of this study. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our study poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The creators or original owners of all assets used in this research are duly acknowledged, and the licenses and terms of use are clearly mentioned and properly respected. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The new assets introduced in the paper are well documented, and documentation is provided along with the assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We use public datasets and do not involve crowdsourced experiments and research on humans. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our study does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]