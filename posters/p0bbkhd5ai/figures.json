[{"figure_path": "p0BBKhD5aI/figures/figures_3_1.jpg", "caption": "Figure 1: Schematic representations of the transformer architecture we model. (a) The forward pass through the residual stream is an alternation of MHSA and MLP blocks scaled by \u03b2oL-\u03b1\u03b9. (b) The MHSA block computes keys, queries, values, and attention variables to produce a concatenated output of dimension dmodel = NH.", "description": "This figure schematically shows the architecture of the transformer model used in the paper. Panel (a) illustrates the forward pass through the residual stream, highlighting the alternating MHSA and MLP blocks, which are scaled by the factor \u03b2oL-\u03b1\u03b9 (\u03b2o: constant, L: depth, \u03b1L: scaling exponent). Panel (b) details the MHSA block, which computes keys, queries, values, and attention variables to generate a concatenated output with a dimension of dmodel = NH (N: key/query dimension, H: number of heads).", "section": "2 Parameterizations with Feature Learning Limits"}, {"figure_path": "p0BBKhD5aI/figures/figures_4_1.jpg", "caption": "Figure 2: Increasing dimension-per-head N with heads fixed for \u03b1A = {1, 1/2}. (a) Both \u03b1A = 1 and \u03b1A = 1/2 exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite N at H = 16. (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For \u03b1A = 1 the variance of attention variables decays at rate O(N\u22122) and for \u03b1A = 1/2 the variance does not decay with N.", "description": "This figure shows the results of experiments investigating the effect of scaling the dimension per head (N) on hyperparameter transfer and the variance of attention variables across different heads.  Panel (a) demonstrates that with \u00b5P scaling (\u03b1A = 1), optimal learning rates transfer well across different values of N. Panel (b) shows that the variance of attention variables across heads decreases with increasing N under \u00b5P scaling, suggesting a collapse towards single-head behavior; however, this variance remains high when \u03b1A = 1/2.", "section": "3.2 Scaling Dimension-Per-Head N"}, {"figure_path": "p0BBKhD5aI/figures/figures_6_1.jpg", "caption": "Figure 3: The initial kernels converge as H \u2192 \u221e and are determined by (possibly non-Gaussian) distributions of Ah over heads in each layer. (a) Convergence of Hss\u2032(x, x\u2032) = h(x)h(x\u2032) in a L = 8, N = 4 vision transformer at initialization at rate O(H\u22121). (b) The density of Ah entries over heads at fixed spatial location converges as H \u2192 \u221e but is non-Gaussian for small N. (c) As N \u2192 \u221e the initial density of A approaches a Gaussian with variance of order O(N1\u22122\u03b1A).", "description": "This figure shows the convergence of the initial kernels and the distribution of attention variables (Ah) as the number of heads (H) approaches infinity.  Subfigure (a) demonstrates the convergence of the residual stream kernel Hss\u2032(x, x\u2032) at a rate of O(H\u22121) in an 8-layer, 4-dimensional vision transformer.  Subfigures (b) and (c) illustrate the distribution of Ah for different values of N (dimension per head). Subfigure (b) (N=1) shows a non-Gaussian distribution for small N, while (c) (N=16) shows that as N gets larger, the distribution approaches a Gaussian.  The results illustrate the impact of scaling parameters on the initial state of the transformer network.", "section": "Scaling Number of Heads"}, {"figure_path": "p0BBKhD5aI/figures/figures_6_2.jpg", "caption": "Figure 2: Increasing dimension-per-head N with heads fixed for \u03b1A = {1, 1/2}. (a) Both \u03b1A = 1 and \u03b1A = 1/2 exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite N at H = 16. (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For \u03b1A = 1 the variance of attention variables decays at rate O(N\u22122) and for \u03b1A = 1/2 the variance does not decay with N.", "description": "This figure explores the impact of scaling the dimension per head (N) while keeping the number of heads (H) constant, using two different scaling exponents for the key/query inner product (\u03b1A = 1 and \u03b1A = 1/2).  The left panel (a) demonstrates hyperparameter transfer, showing consistent performance across different values of N for both \u03b1A settings. The right panel (b) illustrates the variance of attention variables across heads, revealing a decay for \u03b1A = 1 but no decay for \u03b1A = 1/2 as N increases, which highlights the impact of the parameterization choice on attention head behavior during training.", "section": "3.2 Scaling Dimension-Per-Head N"}, {"figure_path": "p0BBKhD5aI/figures/figures_7_1.jpg", "caption": "Figure 5: Depth scaling in a vision transformer on CIFAR-5M with \u03b1L \u2208 {1/2,1}. (a) The key and query weights move by 1/\u221aL. (b) The compute scaling laws with models at fixed width N, H and varying depth L. At large L, the \u03b1L = 1 (dashed) models perform better at fixed compute.", "description": "This figure shows the effects of depth scaling on the performance of a vision transformer model trained on CIFAR-5M dataset.  The left panel (a) displays how the key and query weights change with increasing depth L, specifically showing they scale by 1/\u221aL, indicating a scaling law. The right panel (b) shows the compute scaling laws for the models with \u03b1L values of 1/2 and 1, demonstrating that models with \u03b1L = 1 perform better at a fixed compute budget as depth L increases.", "section": "Infinite Depth Limits"}, {"figure_path": "p0BBKhD5aI/figures/figures_8_1.jpg", "caption": "Figure 6: Initial and final representations are converging as model scale increases after one pass of training on the full CIFAR-5M with SGD+momentum. The base model is a (N, H, L) = (16, 16, 4) and (\u03b1A, \u03b1L, \u03b20, \u03b30) = (1, 1, 4, 0.1). (a) The test loss dynamics for one pass through CIFAR-5M. The dynamics are very similar across different head-counts H but the early dynamics are changed for large depth L, consistent with our theory. (b) The initial and final feature kernels after spatial pooling at the last layer of the residual stream. The initial kernel at large L is quite different for \u03b1A = 1 due to suppression of Brownian motion on the forward pass, which we explain in Section 3.4. (c) The residual stream kernel across pairs of spatial positions for a single randomly chosen input sample. (d) The distribution of attention entries across heads at a fixed pair of spatial locations and data point. The initial variance of A decreases for \u03b1A = 1 but the update is roughly consistent across N. For \u03b1A = \u00bd both initial and final distributions for Ah are consistent across N.", "description": "This figure visualizes the convergence of initial and final representations as the model scales increase after one training pass on the CIFAR-5M dataset.  It shows how test loss, residual stream pooled kernels, spatial kernels for a single sample, and attention distributions evolve across various model sizes (N, H, L) and scaling parameters (\u03b1A, \u03b1L, \u03b20, \u03b30).  The plots demonstrate convergence as model size increases, although the initial kernel at large L exhibits some differences related to Brownian motion suppression under certain parameterizations.  The figure corroborates the theoretical analysis presented in the paper, highlighting the impact of different scaling strategies on training dynamics.", "section": "3.2 Scaling Dimension-Per-Head N"}, {"figure_path": "p0BBKhD5aI/figures/figures_9_1.jpg", "caption": "Figure 7: Training dynamics and initial/final representations of decoder only language models trained on C4 converge with increasing model scale. The base model has (N, H, L) = (8,8, 4) and (\u03b1\u2081, \u03b2\u2080, \u03b3\u2080) = (1, 4, 0.25) and \u03b1 \u2208 {1, \u00bd}. (a) Train loss dynamics after 10000 steps on C4 using Adam optimizer. The dynamics improve consistently when scaling H for both values of \u03b1, with slight benefit to \u03b1 = 1. Scaling N reveals a significant advantage to setting \u03b1 = \u00bd. Scaling L provides little improvement for either parameterization of \u03b1. (b) Initial and final residual stream kernels for the final token across samples for Base, H = 128, N = 128, and L = 64 models. The first row is at initialization. The second and third rows are after training with \u03b1 \u2208 {1, \u00bd} respectively. (c) Initial and final feature kernels across pairs of tokens for a single randomly chosen input sample. Note both types of kernels are identical across \u03b1 except for a slight difference at large N.", "description": "Figure 7 shows the results of experiments on a causal language model trained on the C4 dataset. It demonstrates how the training dynamics and learned representations change as the model's size (number of heads, key/query dimension, and depth) increases.  The plots highlight differences in performance when scaling model dimensions using different scaling parameters (\u03b1 = 1 and \u03b1 = 1/2), revealing the best scaling strategies for optimizing performance at various model sizes.", "section": "Experiments in Realistic Settings"}, {"figure_path": "p0BBKhD5aI/figures/figures_9_2.jpg", "caption": "Figure 7: Training dynamics and initial/final representations of decoder only language models trained on C4 converge with increasing model scale. The base model has (N, H, L) = (8,8, 4) and (\u03b1A, \u03b20, \u03b30) = (1, 4, 0.25) and \u03b1A \u2208 {1, 1/2}. (a) Train loss dynamics after 10000 steps on C4 using Adam optimizer. The dynamics improve consistently when scaling H for both values of \u03b1A, with slight benefit to \u03b1A = 1. Scaling N reveals a significant advantage to setting \u03b1A = 1/2. Scaling L provides little improvement for either parameterization of \u03b1A. (b) Initial and final residual stream kernels for the final token across samples for Base, H = 128, N = 128, and L = 64 models. The first row is at initialization. The second and third rows are after training with \u03b1A \u2208 {1, 1/2} respectively. (c) Initial and final feature kernels across pairs of tokens for a single randomly chosen input sample. Note both types of kernels are identical across \u03b1A except for a slight difference at large N.", "description": "This figure shows the results of experiments on a decoder-only language model trained on the C4 dataset.  It demonstrates the effects of scaling the model's dimensions (number of heads H, key/query dimension N, and depth L) on training dynamics and learned representations (kernels).  Subfigures (a), (b), and (c) show the training loss, the final token kernels across samples and tokens within a sample respectively, for various model sizes. The results suggest that scaling N significantly improves performance, while scaling H leads to modest improvements and scaling L has limited effect.  The different \u03b1A parameterizations (1 and 1/2) yield similar overall trends but some differences in detail are observed.", "section": "Experiments in Realistic Settings"}, {"figure_path": "p0BBKhD5aI/figures/figures_13_1.jpg", "caption": "Figure 8: One pass training on CIFAR-5M with vision transformers with the setting of Figure 6.", "description": "This figure shows the training curves for vision transformers trained on the CIFAR-5M dataset.  It demonstrates how the test loss changes over training steps when varying the number of heads (H), the number of dimensions per head (N), and the depth (L) of the transformer model. Each subplot shows the training curves under different parameterizations of these three hyperparameters, highlighting the relationship between model scale and training dynamics.", "section": "A Additional Figures"}, {"figure_path": "p0BBKhD5aI/figures/figures_13_2.jpg", "caption": "Figure 10: Spatial kernels for a single test point before and after training across H, N, L values.", "description": "This figure visualizes the spatial kernels before and after training of a vision transformer model.  The kernels are shown for various values of H (number of heads), N (dimension per head), and L (depth). The heatmap shows kernel values across different spatial locations, revealing how these change during training and how model parameters affect them.  Specifically, it illustrates the effect of different hyperparameter scalings on the learned representations.", "section": "Additional Figures"}, {"figure_path": "p0BBKhD5aI/figures/figures_14_1.jpg", "caption": "Figure 10: Spatial kernels for a single test point before and after training across H, N, L values.", "description": "This figure visualizes spatial kernels before and after training a vision transformer on the CIFAR-5M dataset. It shows how these kernels change across different model sizes by varying the number of heads (H), the dimension per head (N), and the depth (L) of the network. Each subplot presents a heatmap representing the kernel, where the color intensity represents the kernel value.  The top row shows the initial kernels, and the bottom row shows the kernels after training.  The pattern changes suggest how the model's representation of spatial relationships evolves during training and how this evolution depends on the architectural choices for H, N and L.", "section": "Additional Figures"}, {"figure_path": "p0BBKhD5aI/figures/figures_14_2.jpg", "caption": "Figure 2: Increasing dimension-per-head N with heads fixed for \u03b1A = {1, 1/2}. (a) Both \u03b1A = 1 and \u03b1A = 1/2 exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite N at H = 16. (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For \u03b1A = 1 the variance of attention variables decays at rate O(N\u22122) and for \u03b1A = 1/2 the variance does not decay with N.", "description": "This figure shows the results of experiments investigating the impact of increasing the dimension per head (N) on vision transformer training.  Subfigure (a) demonstrates hyperparameter transfer, showing that models with different values of N maintain similar performance when scaled appropriately (\u03b1A = 1, 1/2). Subfigure (b) examines the variance of attention across different heads. With \u03b1A = 1, the variance decreases as N increases, suggesting that the network effectively collapses to a single-head attention mechanism in the large N limit. However, with \u03b1A = 1/2, this variance does not decrease, indicating head diversity is maintained.", "section": "3.2 Scaling Dimension-Per-Head N"}, {"figure_path": "p0BBKhD5aI/figures/figures_15_1.jpg", "caption": "Figure 12: Performance of language models trained on C4 in main text Figure 7(a) as a function of compute, estimated as FLOPs = 6 \u00d7 Params. The base model has size (N, H, L) = (8,8, 4) and we examine scaling up N, H, L with either \u03b1A = 1/2 or \u03b1A = 1. The \u03b1A = 1 models perform better at fixed compute for either N or H scaling. Increasing L does not significantly increase compute in this regime since the embedding and decoding layers contribute most of the parameters.", "description": "The figure shows the performance of language models trained on the C4 dataset, in terms of loss, as a function of compute (estimated as FLOPs = 6 * number of parameters).  Different model sizes were tested, varying the key/query dimension (N), number of heads (H), and depth (L), while using two different scaling exponents (\u03b1A = 1/2 and \u03b1A = 1). The results indicate that using \u03b1A = 1 leads to better performance at a fixed compute, particularly when scaling N or H. Scaling L did not significantly increase compute due to the dominant contribution of embedding and decoding layers to the total number of parameters.", "section": "Experiments in Realistic Settings"}, {"figure_path": "p0BBKhD5aI/figures/figures_22_1.jpg", "caption": "Figure 2: Increasing dimension-per-head N with heads fixed for a\u03bb = {1, 1/2}. (a) Both a\u03bb = 1 and a\u03bb = 1/2 exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite N at H = 16. (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For a\u03bb = 1 the variance of attention variables decays at rate O(N\u22122) and for a\u03bb = 1/2 the variance does not decay with N.", "description": "This figure shows the results of experiments investigating the effect of increasing the dimension per head (N) on hyperparameter transfer and attention variance in vision transformers.  Panel (a) demonstrates that with either key/query scaling exponent (a\u03bb), similar hyperparameters work across different values of N, exhibiting hyperparameter transfer. Panel (b) shows that only with the mean field scaling exponent (a\u03bb = 1) does the variance of attention variables across heads decay with increasing N, while it remains constant for a\u03bb = 1/2.", "section": "3.2 Scaling Dimension-Per-Head N"}, {"figure_path": "p0BBKhD5aI/figures/figures_22_2.jpg", "caption": "Figure 2: Increasing dimension-per-head N with heads fixed for \u03b1A = {1, 1/2}. (a) Both \u03b1A = 1 and \u03b1A = 1/2 exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite N at H = 16. (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For \u03b1A = 1 the variance of attention variables decays at rate O(N\u22122) and for \u03b1A = 1/2 the variance does not decay with N.", "description": "This figure shows the results of experiments investigating the effect of increasing the dimension per head (N) on hyperparameter transfer and attention variance in vision transformers.  Subfigure (a) demonstrates that with either scaling of the key-query inner product (\u03b1A = 1 or \u03b1A = 1/2), similar hyperparameter transfer occurs across different values of N. Subfigure (b) shows that when \u03b1A = 1, attention variance decays with increasing N, suggesting that the heads of the network converge towards similar behaviour; however, when \u03b1A = 1/2, attention variance does not decrease, indicating that heads maintain diversity.", "section": "3.2 Scaling Dimension-Per-Head N"}, {"figure_path": "p0BBKhD5aI/figures/figures_22_3.jpg", "caption": "Figure 2: Increasing dimension-per-head N with heads fixed for aA = {1,}. (a) Both aA = 1 and aA = exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite N at H = 16. (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For aA = 1 the variance of attention variables decays at rate O(N\u22122) and for aA = the variance does not decay with N.", "description": "This figure shows the impact of increasing the dimension per head (N) on hyperparameter transfer and attention variance across heads.  Panel (a) demonstrates that using the \u00b5P scaling (aA = 1) leads to similar performance across varying N, while Panel (b) shows that the attention variance decays with N only when aA = 1, implying that identical dynamics only occurs when \u00b5P scaling is used.", "section": "3.2 Scaling Dimension-Per-Head N"}, {"figure_path": "p0BBKhD5aI/figures/figures_22_4.jpg", "caption": "Figure 2: Increasing dimension-per-head N with heads fixed for \u03b1\u2084 = {1, 3}. (a) Both \u03b1\u2084 = 1 and \u03b1\u2084 = 1/2 exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite N at H = 16. (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For \u03b1\u2084 = 1 the variance of attention variables decays at rate O(N\u207b\u00b2) and for \u03b1\u2084 = 1/2 the variance does not decay with N.", "description": "This figure demonstrates the impact of scaling the dimension per head (N) on hyperparameter transfer and attention variance across heads in vision transformers.  Panel (a) shows that with the \u00b5P scaling (\u03b1\u2084 = 1), similar performance is achieved across different values of N, indicating hyperparameter transfer.  In contrast, with \u03b1\u2084 = 1/2, this transferability is lost.  Panel (b) visualizes the variance of attention variables across different heads, showing that it decays rapidly with increasing N for \u03b1\u2084 = 1 but remains relatively constant for \u03b1\u2084 = 1/2.", "section": "3.2 Scaling Dimension-Per-Head N"}, {"figure_path": "p0BBKhD5aI/figures/figures_33_1.jpg", "caption": "Figure 2: Increasing dimension-per-head N with heads fixed for \u03b1A = {1, 3}. (a) Both \u03b1A = 1 and \u03b1A = 1/2 exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite N at H = 16. (b) The variance of attention variables across the different heads of a vision transformer after training for 2500 steps on CIFAR-5M. For \u03b1A = 1 the variance of attention variables decays at rate O(N\u22122) and for \u03b1A = 1/2 the variance does not decay with N.", "description": "This figure shows the results of experiments investigating the effect of scaling the dimension per head (N) on hyperparameter transfer and attention variance across multiple heads in vision transformers.  Part (a) demonstrates that with the appropriate scaling (\u03b1A=1 and \u03b1A=1/2), similar hyperparameter performance is observed across different values of N. Part (b) shows that for \u03b1A=1 (\u03bcP scaling), the variance of attention across heads decreases quadratically with increasing N while for \u03b1A=1/2 this is not the case.", "section": "3.2 Scaling Dimension-Per-Head N"}]