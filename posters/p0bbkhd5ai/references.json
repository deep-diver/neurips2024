{"references": [{"fullname_first_author": "Song Mei", "paper_title": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit", "publication_date": "2019-06-01", "reason": "This paper provides foundational mean-field theory for analyzing neural networks, which is the theoretical basis for the current work's approach."}, {"fullname_first_author": "Greg Yang", "paper_title": "Tensor programs IV: Feature learning in infinite-width neural networks", "publication_date": "2021-07-01", "reason": "This paper introduces the Tensor Programs framework, which the current work uses to derive theoretical limits of transformer models."}, {"fullname_first_author": "Blake Bordelon", "paper_title": "Self-consistent dynamical field theory of kernel evolution in wide neural networks", "publication_date": "2022-12-01", "reason": "This paper develops the DMFT framework used to analyze the training dynamics of the transformers."}, {"fullname_first_author": "Nikhil Vyas", "paper_title": "Feature-learning networks are consistent across widths at realistic scales", "publication_date": "2023-04-01", "reason": "This paper provides empirical evidence supporting the theoretical results found in the current work, showing that the learned features are consistent across different scales."}, {"fullname_first_author": "Jiri Hron", "paper_title": "Infinite attention: NNGP and NTK for deep attention networks", "publication_date": "2020-06-01", "reason": "This paper provides a theoretical treatment of multi-head self-attention at initialization, which is extended in the current work to analyze training dynamics."}]}