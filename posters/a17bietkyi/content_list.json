[{"type": "text", "text": "Improving self-training under distribution shifts via anchored confidence with theoretical guarantees ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Taejong Joo & Diego Klabjan Department of Industrial Engineering & Management Sciences Northwestern University Evanston, IL, USA {taejong.joo,d-klabjan}@northwestern.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Self-training often falls short under distribution shifts due to an increased discrepancy between prediction confidence and actual accuracy. This typically necessitates computationally demanding methods such as neighborhood or ensemble-based label corrections. Drawing inspiration from insights on early learning regularization, we develop a principled method to improve self-training under distribution shifts based on temporal consistency. Specifically, we build an uncertainty-aware temporal ensemble with a simple relative thresholding. Then, this ensemble smooths noisy pseudo labels to promote selective temporal consistency. We show that our temporal ensemble is asymptotically correct and our label smoothing technique can reduce the optimality gap of self-training. Our extensive experiments validate that our approach consistently improves self-training performances by $8\\%$ to $16\\%$ across diverse distribution shift scenarios without a computational overhead. Besides, our method exhibits attractive properties, such as improved calibration performance and robustness to different hyperparameter choices. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we address the challenge of adapting pre-trained neural networks at test time under distribution shifts, a problem known as test-time adaptation (TTA) or source-free domain adaptation (SFDA). Distribution shifts\u2014where a model trained on one distribution is then tested on a different one\u2014are ubiquitous in many practical scenarios due to demographic subpopulation shift [1] and changes in data collection environments [2\u20134]. Despite the robust performance of neural networks under independent and identically distributed (i.i.d.) settings, they often suffer from substantial performance degradation under such shifts [5, 6]. Recently, TTA and SFDA have proven their effectiveness in resolving these critical issues by effectively leveraging information about the distribution shifts contained in unlabeled samples given at the test time. ", "page_idx": 0}, {"type": "text", "text": "Self-training is the basis of many state-of-the-art methods in TTA and SFDA [7\u20139], utilizing pseudo labels generated from the model\u2019s own predictions to train a model on unlabeled samples [10]. Since pseudo labels are regarded as true labels in self-training, the success of self-training methods highly depends on how to filter incorrect pseudo labels to prevent self-confirmation bias [11]. This issue has been effectively handled via simple confidence-based thresholding in i.i.d. settings [12\u201314]. However, the distribution shifts make it hard to filter incorrect pseudo labels due to high noise rates even under high threshold [15]. Thus, sophisticated methods fliter incorrect pseudo labels based on a neighborhood structure of the data [16\u201318] and consistency of multiple predictions under different models [19] or augmentations [20](cf. Section 5), which are computationally intensive by nature. ", "page_idx": 0}, {"type": "text", "text": "Recent insights in [8] suggest an alternative strategy can be also effective: promoting temporal consistency can enhance self-training performance in SFDA without the computational burden of previous methods. The temporal consistency regularizer, so called early learning regularization (ELR) [21], was originally developed to address neural networks\u2019 tendency to learn clean information first and then gradually memorize noisy labels [22, 23]; this setting is naturally connected to self-training scenarios when we regard the pseudo labels as random noisy labels. However, the impacts of ELR on self-training have not been fully understood. Also, since ELR does not consider unique characteristics of distribution shifts, we aim to answer the following question: Is there any principled way to improve the way of memorizing all past predictions tailored self-training under distribution shifts? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In this work, we show that the answer is affirmative by proposing Anchored Confidence (AnCon) that uses confident predictions to support a generalized notion of temporal consistency. Specifically, we construct a generalized temporal ensemble, which weighs predictions based on predictive uncertainty, and then use the ensemble as a smoothing vector in label smoothing [24]. Then, through rigorous theoretical analyses, we show that our simple heuristic for the generalized temporal ensemble is asymptotically correct and that the label smoothing formulation can reduce the optimality gap. As a result, AnCon can correct wrong pseudo labels without expensive computations and can be easily applied to self-training methods by replacing one-hot pseudo labels with smooth pseudo labels, unlike neighborhood-based or centroid methods. Through extensive experiments, we show that AnCon improves self-training under diverse distribution shift scenarios and posses many attractive properties. ", "page_idx": 1}, {"type": "text", "text": "Our contribution can be summarized as follows: 1) We develop AnCon, which is the first algorithm that attempts to improve self-training under distribution shifts by generalizing a notion of temporal consistency with theoretical guarantees; 2) Without any additional forward passes or neighborhood search, AnCon improves self-training performances $8\\%$ and $16\\%$ under domain shifts and image corruptions, respectively; 3) Remarkably, we also show that AnCon significantly improves calibration performance and is robust with respect to model selection methods and hyperparameter choices. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Notation and setup For an input space $\\mathcal{X}\\subseteq\\mathbb{R}^{d}$ and a label space $\\mathcal{Y}=[K]:=\\{1,2,\\cdot\\cdot\\cdot,K\\}$ , we define $X$ and $Y$ to be random variables of input and output with probability densities $p_{X}$ and $p_{Y}$ , respectively. We also define a neural network $\\bar{f}(\\cdot;\\theta):\\mathcal{X}\\,\\bar{\\to}\\,\\triangle^{K-\\bar{1}}$ parameterized by a parameter $\\theta\\in\\mathbb{R}^{p}$ where $\\triangle^{K-1}$ is the probability simplex with $K$ elements. Our goal is to minimize the cross-entropy loss $\\begin{array}{r}{\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{p}}l(\\theta):=\\mathbb{E}_{X Y}[\\dot{H}(f(\\bar{X};\\theta),p_{Y|X})]}\\end{array}$ where $\\begin{array}{r l}{H(f(\\bar{x;\\theta}),p_{Y\\mid X=x})\\,:=\\,-\\sum_{k\\in[K]}p(Y\\stackrel{\\cdot\\cdot}{=}}&{{}}\\end{array}$ $k|x)\\log f_{k}(x;\\theta)$ . In the SFDA setting, we are given an initial parameter $\\theta_{0}$ that is trained on a different data generating distribution $(X^{\\prime},Y^{\\prime})$ , e.g., $\\begin{array}{r}{\\theta_{0}\\in\\arg\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{p}}\\mathbb{E}_{X^{\\prime}Y^{\\prime}}[H(f(X^{\\prime};\\theta),p_{Y^{\\prime}|X^{\\prime}})]}\\end{array}$ . We can think about this setting as either (1) $(X^{\\prime},Y^{\\prime})$ being pre-training data and $\\theta_{0}$ the foundation model with the task of fine-tuning the model on unlabeled $X$ or (2) a transfer learning problem with $(X^{\\prime},Y^{\\prime})$ being the source domain data and $X$ the target domain. Here, we assume only covariate shift without concept shift; that is, $p_{X}\\neq^{D}p_{X^{\\prime}}$ but $p_{Y|X}=^{D}p_{Y^{\\prime}|X^{\\prime}}$ . Even in this case, we note that suboptimality under the distribution shift, i.e., $\\mathrm{min}_{\\theta\\in\\mathbb{R}^{p}}\\,l(\\theta)-l(\\theta_{0})$ , can be large. ", "page_idx": 1}, {"type": "text", "text": "Self-training In this work, we tackle the distribution shifts by using the self-training method that replaces the true label $Y(x)$ by the pseudo label, i.e., $\\begin{array}{r}{\\operatorname*{min}_{\\theta_{n e w}}\\hat{l}(\\theta_{n e w};\\theta)=\\mathbb{E}_{X}[-\\log f_{\\hat{Y}(x;\\theta)}(x;\\theta_{n e w})]}\\end{array}$ where $\\hat{Y}(x;\\theta):=\\arg\\operatorname*{max}_{k\\in[K]}f_{k}(x;\\theta)$ is a pseudo label under $\\theta$ . Specifically, the algorithmic framework of self-training is as follows: given $\\theta_{0}$ , we iteratively find model parameter $\\theta_{m+1}$ for $m\\,=\\,0,1,\\cdot\\cdot\\cdot$ with $\\theta_{m+1}\\,\\in\\,\\mathrm{arg}\\,\\mathrm{min}_{\\theta_{n\\in w}}\\,\\hat{l}(\\theta_{n e w};\\theta_{m})$ . For later use, we also define a prediction confidence $c(x;\\theta):=\\operatorname*{max}_{k\\in[K]}f_{k}(x;\\tilde{\\theta})$ and $\\theta_{0:m}:=(\\theta_{0},\\cdot\\cdot\\cdot,\\theta_{m})$ . ", "page_idx": 1}, {"type": "text", "text": "Early learning regularization In the learning from noisy labels (LFN) scenario, [21] identified an \"early-learning phenomenon\" where neural networks initially learn information contained in clean labels before gradually memorizing noisy labels, leading to a performance deterioration as training progresses. To mitigate this issue, ELR penalizes predictions that deviate from earlier predictions by defining a target network from the past predictions: $\\begin{array}{r}{\\bar{f}_{E L R}(x;\\theta_{0:m}):=\\sum_{j=0}^{m}(1-\\gamma)\\cdot\\dot{\\gamma}^{m-j}f(x;\\theta_{j})}\\end{array}$ . Then, adding an auxiliary loss of $L_{E L R}(\\theta;\\theta_{0:m})\\;=\\;\\mathbb{E}_{X}[\\mathrm{log}(1\\,-\\,f(X;\\theta)^{T}\\,\\bar{f}_{E L R}(X;\\theta_{0:m}))]$ to $\\hat{l}(\\theta;\\theta_{m})$ can prevent memorization of noisy labels while preserving correct patterns. ", "page_idx": 1}, {"type": "text", "text": "Notably, this insight has recently been confirmed to be applicable in the SFDA setting by [8]. This observation is appealing because ELR can be efficiently implemented by reusing past predictions without additional forward passes or neighborhood searching, unlike dominant methods in SFDA [16\u201320]. Nevertheless, given that ELR stems from a general property of the neural network training in the i.i.d. setting, herein we aim to step towards a more principled approach to encourage temporal consistency tailored for distribution shift scenarios. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Anchored confidence ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce AnCon, which promotes the temporal consistency on selectively chosen predictions via label smoothing. In Section 3.1, we first explain the idea of promoting selective temporal consistency based on confident predictions via label smoothing [24] with a temporal ensemble. Then, in Section 3.2, we explain how to effectively construct temporal ensemble for improving self-training under distribution shifts. Finally, we theoretically analyze the efficacy of AnCon by drawing connection between our method and knowledge distillation in Section 3.3. ", "page_idx": 2}, {"type": "text", "text": "3.1 Selective temporal consistency via label smoothing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we utilize label smoothing [24] to promote the selective temporal consistency instead of using an auxiliary loss function like ELR. Specifically, given a generalized temporal ensemble $\\bar{f}(x;\\bar{\\theta_{0:m}},\\mathbf{w}_{0:m})$ with $\\mathbf{w}_{0:m}:=(w_{0},\\cdot\\cdot\\cdot,w_{m})$ which will be specified in Section 3.2, we construct a regularized pseudo label $\\tilde{Y}(X;\\theta_{0:m},\\mathbf{w}_{0:m})$ by using $\\bar{f}(x;\\theta_{0:m},\\mathbf{w}_{0:m})$ as a smoothing vector for the pseudo label $\\hat{Y}(x;\\theta_{m})$ . That is, we perform self-training by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{sin}_{a}\\mathbb{E}_{X}[H(f(X;\\theta),\\tilde{Y}(X;\\theta_{0:m},{\\mathbf w}_{0:m}))],\\quad\\tilde{Y}(X;\\theta_{0:m},{\\mathbf w}_{0:m})=(1-\\lambda)E_{1}(\\hat{Y}(x;\\theta_{m}))+\\lambda\\bar{f}(x;\\theta_{0:m},{\\mathbf w}_{0:m}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\lambda\\in[0,1]$ is a coefficient, $E_{1}(\\cdot)$ is the one-hot encoding, and $\\bar{f}(x):=(\\bar{f}_{1}(x),\\cdot\\cdot\\cdot\\,,\\bar{f}_{K}(x))$ is the $K$ -dimensional output of the generalized temporal ensemble (cf. (2)). ", "page_idx": 2}, {"type": "text", "text": "Thus, AnCon can control the usage of potentially noisy information in $\\hat{Y}(x;\\theta_{m})$ based on its consistency with $\\bar{f}(x;\\theta_{0:m},\\mathbf{w}_{0:m})$ . Not only can this approach preserve the early learning phenomenon as in ELR (cf. Section 2), but the label smoothing formulation also significantly stabilizes the selftraining performance under different hyperparameter choices due to the fact that the optimal values of hyperparameters are less problem dependent compared to its equivalent auxiliary regularization [25]. Beyond removing the burden of hyperparameter search, this robustness is a particularly intriguing property under distribution shift scenarios where the model selection becomes a challenging task. ", "page_idx": 2}, {"type": "text", "text": "Further, encouraging the temporal consistency through label smoothing enables us to connect our method with knowledge distillation (KD) [26], which can provide a wide range of principled techniques and theoretical results developed in KD. As a concrete example, we will show when and how AnCon can reduce the optimality gap of self-training, i.e., a case with $\\lambda=0$ , in Section 3.3. ", "page_idx": 2}, {"type": "text", "text": "3.2 Constructing an effective generalized temporal ensemble with prediction confidences ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Next, we construct the generalized temporal ensemble that makes the selective temporal consistency in (1) work effectively in self-training under distribution shifts. Specifically, given $\\theta_{0:m}$ and weights $\\mathbf{w}_{0:m}(x)\\in\\triangle^{m}$ for each $x\\in\\mathscr{X}$ , the prediction by the generalized temporal ensemble is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\bar{f}_{k}(x;\\theta_{0:m},\\mathbf{w}_{0:m}):=\\sum_{i=0}^{m}w_{i}(x)\\cdot p(y=k|x,\\theta_{i}),\\quad k\\in[K]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p(\\boldsymbol{y}|\\boldsymbol{x},\\boldsymbol{\\theta}_{i})$ is the prediction made by $f(x;\\theta_{i})$ , which can be either soft $(p(y\\,=\\,j|x,\\theta_{i})\\,=$ $f_{j}(x;\\theta_{i}))$ or hard $~\\langle p(y\\,=\\,j|x,\\theta_{i})\\,=\\,1$ if $j\\,=\\,\\arg\\operatorname*{max}_{k\\in[K]}\\,f_{k}(x;\\theta_{i})$ and $p(y\\,=\\,j|x,\\theta_{i})\\,=\\,0$ otherwise). In this work, we use hard prediction because soft prediction puts more weights on recent predictions since self-training tends to keep increasing the prediction confidence during training. ", "page_idx": 2}, {"type": "text", "text": "Surprisingly, we will show that the following simple relative thresholding for determining $\\mathbf{w}_{0:m}$ gives the asymptotic optimal weights achieving the minimum worst-case optimality gap of self-training: ", "page_idx": 2}, {"type": "equation", "text": "$$\nw_{m}(x)\\propto{\\bf1}(c(x;\\theta_{m})>\\delta_{m}^{(\\beta)}),\\quad\\delta_{m}^{(\\beta)}:=\\sum_{i=0}^{m}(1-\\beta)\\beta^{m-i}\\hat{\\mathbb{E}}_{X}[c(X;\\theta_{i})]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\delta_{m}^{(\\beta)}$ is an exponential moving average (EMA) of prediction confidence with hyperparameter $\\beta$ and $\\hat{\\mathbb{E}}[c(X;\\theta_{i})]$ is a Monte-Carlo approximation of $\\mathbb{E}[c(X;\\theta_{i})]$ with mini-batch samples. ", "page_idx": 2}, {"type": "text", "text": "Intuitively, our weighting mechanism aggregates only relatively confident predictions with a uniform weight. Given the observation that relative ordering of confidence is highly correlated with accuracy even under distribution shifts [27, 28], our thresholding rule would tend to put non-zero weights on correct predictions. Besides, by employing the relative criterion, the thresholding does not suffer from the problems that neglect predictions obtained in the early stage of training. ", "page_idx": 3}, {"type": "text", "text": "We also remark that AnCon has almost the same computational cost as ELR. Specifically, (2) can be implemented by $\\bar{f}_{k}(x;\\theta_{0:m},{\\bf w}_{0:m})=\\bar{f}_{k}(x;\\theta_{0:m-1},\\dot{\\bf w}_{0:m-1})+w_{m}(x)p(y=\\dot{k}|x,\\theta_{m})$ that requires to store the weighted sum of previous predictions without additional forward passes or storing previous parameters, which is the same as storing the previous logit vector in ELR. Similarly, (3) can be efficiently implemented by $\\delta_{m}^{(\\beta)}=\\beta\\delta_{m-1}^{(\\beta)}\\!+\\!(1\\!-\\!\\beta)\\hat{\\mathbb{E}}_{X}[c(X;\\theta_{m})]$ that requires constant additional computational costs compared to vanilla self-training with the constant being small. Therefore, AnCon shares the same computational beneftis as ELR compared to other state-of-the-art methods in SFDA. ", "page_idx": 3}, {"type": "text", "text": "On optimality of the relative thresholding in (3) From the optimization perspective, the optimal weights w0:m correspond to the weights under which self-training with $\\tilde{Y}(X;\\theta_{0:m},\\mathbf{w}_{0:m})$ can minimize the expected loss: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{w}_{0:m}^{\\dagger}\\in\\underset{\\mathbf{w}_{0:m}}{\\arg\\operatorname*{min}}l(\\theta_{\\mathbf{w}_{0:m}}^{\\dagger}),\\quad\\theta_{\\mathbf{w}_{0:m}}^{\\dagger}\\in\\underset{\\theta}{\\arg\\operatorname*{min}}\\,\\hat{\\mathbb{E}}_{X}[H(f(X;\\theta),\\tilde{Y}(X;\\theta_{0:m},\\mathbf{w}_{0:m}))].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Unfortunately, $l(\\boldsymbol{\\theta}_{\\mathbf{w}_{0:m}}^{\\dagger})$ , or its empirical counterpart, is not available in self-training due to the absence of labels. Further, even if labels are given, solving (4) is intractable due to non-smoothness of $l(\\boldsymbol{\\theta}_{\\mathbf{w}_{0:m}}^{\\dagger})$ with respect to $\\mathbf{w}_{0:m}$ and the cost of finding $\\bar{\\theta_{\\mathbf{w}_{0:m}}^{\\dagger}}$ . ", "page_idx": 3}, {"type": "text", "text": "To circumvent this issue, we show in Section 3.3 that (4) can be relaxed to the problem of finding ensemble weights that give a maximum likelihood estimation (MLE) solution under certain conditions. As a result, instead of solving the intractable optimization in (4), we find the optimal weights by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{w}_{0:m}^{\\dagger}\\in\\underset{\\mathbf{w}_{0:m}}{\\arg\\operatorname*{max}}\\,\\mathbb{E}_{X Y}[\\log\\bar{f}_{Y}(X;\\theta_{0:m},\\mathbf{w}_{0:m})].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the following theorem which is proven in Appendix B.2, we show that the simple relative thresholding in (3) can make $\\bar{f}(x;\\theta_{0:m},\\mathbf{w}_{0:m})$ asymptotically correct for samples where the neural network tends to be relatively confident during self-training and thus our simple weighting mechanism in (3) to be the solution of (5) in the asymptotic region. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. Let $A_{i}(c)\\ :=\\ \\{x\\ \\in\\ x|c(x;\\theta_{i})\\ >\\ c\\}$ , $\\begin{array}{r l r}{Q(x;\\mathbf{c}_{0:m})\\!\\!}&{{}:=}&{\\!\\!\\sum_{i=0}^{m}{\\bf1}(x\\!\\!}&{{}\\in\\;A_{i}(c_{i})),}\\end{array}$ , and $\\begin{array}{r}{\\bar{p}(x;\\mathbf{c}_{0:m})\\;=\\;\\frac{1}{Q(x;\\mathbf{c}_{0:m})}\\sum_{i=0}^{m}\\mathbb{E}_{Y|X=x}[\\mathbf{1}(Y(x)\\;=\\;\\hat{Y}(x;\\theta_{i}))]\\mathbf{1}(x\\;\\in\\;A_{i}(c_{i}))}\\end{array}$ for $x$ such that $Q(x;\\mathbf{c}_{0:m})>O.$ . Let us assume that random events $\\mathbf{1}(Y(x)=\\hat{Y}(x;\\theta_{i}))$ and $\\mathbf{1}(Y(x)={\\hat{Y}}(x;\\theta_{j}))$ are conditionally independent given $X\\in A_{i}(c)$ for $j\\in\\{0,\\cdot\\cdot\\cdot,i-1\\}$ , $x\\in\\mathscr{X}$ , $c\\in[0,1)$ . If $x\\in\\mathscr{X}$ such that $\\bar{p}(x;\\mathbf{c}_{0:m})>1/2$ , then for the generalized temporal ensemble in (3), it holds that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\underset{k\\in[K]}{\\arg\\operatorname*{max}}\\,\\bar{f}_{k}(x;\\theta_{0:m},\\mathbf{w}_{0:m})\\neq Y(x))\\leq\\exp\\left(-\\frac{Q(x;\\mathbf{c}_{0:m})}{2}\\cdot\\xi(\\bar{p}(x;\\mathbf{c}_{0:m}))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\xi(z):=2z-1-\\log(2z)$ is a positive increasing function in $z\\in[0.5,1]$ . ", "page_idx": 3}, {"type": "text", "text": "The result states that as long as the average accuracy for relatively confident predictions over iterations exceeds $50\\%$ , the error rate of the generalized temporal ensemble monotonically decreases as $Q(x;\\mathbf{c}_{0:m})$ increases. Furthermore, $\\bar{f}\\bar{(}x;{\\theta_{0:m}},\\mathbf{w}_{0:m})$ is asymptotically correct on $x$ such that $Q(x;\\mathbf{c}_{0:m})\\ \\to\\ \\infty$ as $m\\ \\rightarrow\\ \\infty$ . AnCon aims to achieve these desirable properties through the uncertainty-aware temporal consistency that helps to satisfy the condition $\\bar{p}(\\bar{x};\\bar{\\mathbf{c}_{0:m}})>0.5.$ . Specifically, as shown in Figure 4a in Appendix, our generalized temporal ensemble\u2019s accuracy tends to significantly increase as the number of confident samples increases, being consistent with our theory. We note that this monotonic improvement would not be the case for the temporal ensemble without uncertainty-awareness and vanilla self-training (cf. Figure 4a). ", "page_idx": 3}, {"type": "text", "text": "Finally, we emphasize that the assumption $\\bar{p}(x;\\mathbf{c}_{0:m})\\,>\\,0.5$ applies only to relatively confident predictions which are averaged over iterations. This is significantly weaker than requiring a lower bound of an expected accuracy of each sample for every iteration, which is the case when LFN methods are directly applied to the self-training scenario. Also, due to its dependency on the choice of the confidence thresholds $\\mathbf{c}_{0:m}$ , the assumption can hold by controlling $\\mathbf{c}_{0:m}$ at the expense of loosening the upper bound in (6) (e.g., selecting 90th-quantile as in Figure 4b in Appendix). Specifically, increasing the thresholds can improve $\\xi(\\bar{\\bar{p}}(x;{\\mathbf c}_{0:m}))$ and enhance the chance of satisfying $\\bar{p}\\bar{(x;{\\mathbf c}_{0:m})}>1/2$ but reducing $Q(x;\\mathbf{c}_{0:m})$ . While this trade-off necessitates a proper choice of $\\mathbf{c}_{0:m}$ , our extensive experiments show that setting the threshold $c_{m}$ by the EMA of prediction confidence, i.e., $\\delta_{m}^{(\\beta)}$ in (3), works effectively. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3 Theoretical insights from knowledge distillation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present a novel connection between AnCon and KD for addressing intractability of (4). KD is a framework for training a small student network $f$ , e.g., ResNet-50 [29], with an additional supervision from a large teacher network $f^{(t)}$ , e.g., ResNet-152. Specifically, a cross-entropy under KD is $l_{K D}(\\theta)\\,=\\,\\mathbb{E}_{X Y}\\left[H\\big(f(X;\\theta),(1-\\lambda_{K D})E_{1}(Y(X))+\\lambda_{K D}f^{(t)}(X)\\big)\\right]$ with $\\lambda_{K D}\\,\\in\\,[0,1]$ , which bears a significant similarity with AnCon (cf. (1)). Indeed, AnCon can be understood as a special case of KD called self-distillation when $f$ and $f^{(t)}$ have the same architecture, where the generalized temporal ensemble $\\bar{f}$ corresponds to the teacher network $f^{(t)}$ with a notable difference that the pseudo label $\\hat{Y}$ is used instead of the true label $Y$ . Based on this connection, we perform a convergence analysis of AnCon by modifying the partial variance reduction theory [30], as given below. We note that the usage of $\\tilde{Y}$ and $\\bar{f}$ results in an inherently biased gradient estimator, which requires special treatments for the convergence analysis unlike the typical self-distillation setting. ", "page_idx": 4}, {"type": "text", "text": "Setup Following [30], we assume a linear model $\\begin{array}{r}{f_{k}(x;\\theta)=\\exp(\\Theta_{k}^{T}x)/\\sum_{i\\in[K]}\\exp(\\Theta_{i}^{T}x)}\\end{array}$ with $\\Theta_{i}\\ \\in\\ \\mathbb{R}^{d}$ for $i\\ \\in\\ [K]$ , $\\theta\\,:=\\,\\mathrm{Concat}(\\Theta_{1},\\cdot\\cdot\\cdot\\,,\\Theta_{K})\\,\\in\\,\\mathbb{R}^{d K}$ , and $\\mathrm{Concat}(\\cdot)$ is the concatenation operation. Also, we assume a bounded support for $X$ ; that is, $\\|\\boldsymbol{x}\\parallel\\leq C$ for all $x$ where $p_{X}(x)>0$ . Under this setting, we repeat the following steps starting from a given $\\theta_{0}$ (i.e., $m=0$ ): ", "page_idx": 4}, {"type": "text", "text": "1. Outer temporal ensemble update: Update $\\mathbf{w}_{0:m}$ by (3) to obtain $\\bar{f}(x;\\theta_{0:m},\\mathbf{w}_{0:m})$ (cf. (2)). 2. Inner parameter update: With $\\theta_{m,0}\\,:=\\,\\theta_{m}$ and $(\\theta_{0:m},\\mathbf{w}_{0:m})$ , solve (1) with stochastic gradient descent $\\theta_{m,t+1}=\\theta_{m,t}-\\gamma g_{\\xi}^{(m,t)}$ g\u03be(m,t)for t \u2208{0, 1, \u00b7 \u00b7 \u00b7 , T \u22121} and set \u03b8m+1 = \u03b8m,T . 3. Iteration number update: Set $m=m+1$ and terminate if $m={\\hat{T}}$ . ", "page_idx": 4}, {"type": "text", "text": "Here, $\\gamma$ is the learning rate, $\\xi$ contains $b$ random samples from $p_{X}$ , and the stochastic gradiFor th $\\begin{array}{r l r l}&{\\mathrm{ler~\\r{AnCon\\is\\deffned\\as\\}~}g_{\\xi}^{(m,t)}}&{:=\\nabla_{\\theta_{m},t}\\frac{1}{b}\\sum_{X_{i}\\in\\xi}H\\big(f\\big(X_{i};\\theta_{m,t}\\big),\\dot{Y}\\big(X_{i};\\theta_{0:m},{\\bf w}_{0:m}\\big)\\big).}\\\\ &{\\mathrm{se\\boldmath\\large\\linear\\quad\\wodel,~}\\quad\\mathrm{note\\boldmath\\large\\what}\\quad g_{\\xi}^{(m,t)}}&{=}&{\\nabla\\tilde{l}_{\\xi}(\\theta_{m,t})\\quad-\\mathrm{\\boldmath\\large\\\\}\\lambda\\hat{g}_{\\xi}(\\theta_{0:m},{\\bf w}_{0:m})}\\\\ &{\\tilde{l}_{\\xi}(\\theta_{m,t})\\quad}&{=}&{\\frac{1}{b}\\sum_{X_{i}\\in\\xi}H\\big(f\\big(X_{i};\\theta_{m,t}\\big),\\dot{Y}\\big(X_{i};\\theta_{m}\\big)\\big)\\quad\\mathrm{and\\boldmath\\large\\}\\hat{g}_{\\xi}(\\theta_{0:m},{\\bf w}_{0:m})}&{:=}\\\\ &{\\hat{g}_{1,\\xi}(\\theta_{0:m},{\\bf w}_{0:m}),\\cdots,\\hat{g}_{K,\\xi}(\\theta_{0:m},{\\bf w}_{0:m})\\big)\\quad\\quad\\mathrm{with}\\quad\\quad\\hat{g}_{k,\\xi}(\\theta_{0:m},{\\bf w}_{0:m})}&{:=}\\\\ &{\\tilde{\\mathfrak{c}}_{\\xi}\\big[\\big(\\bar{f}_{k}(X_{i};\\theta_{0:m},{\\bf w}_{0:m})-\\hat{Y}_{k}(X_{i};\\theta_{m})\\big)X_{i}\\big]\\,\\mathrm{for}\\,k\\in[K].}\\end{array}$ (m,t) where   \nConcat ", "page_idx": 4}, {"type": "text", "text": "In Theorem 3.2 which is proven in Appendix B.3, we analyze the convergence of the inner parameter update step under AnCon and vanilla self-training. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. Let us assume $l(\\theta)$ satisfies $L$ -smoothness, $\\mathcal{L}$ -expected smoothness, and $\\mu$ -PolyakLojasiewicz $(P L)$ condition (cf. Assumptions B.1, B.2, and $B.3$ in Appendix B.1). For $\\begin{array}{r}{\\gamma\\leq\\frac{\\mu}{4\\mathscr{L}\\cdot L}}\\end{array}$ , $a$ carefully chosen $\\lambda$ (cf. $\\begin{array}{r}{\\lambda=\\lambda_{m}^{\\dagger}:=\\frac{\\mathbb{E}_{\\boldsymbol\\xi}[\\langle\\nabla\\tilde{l}_{\\boldsymbol\\xi}(\\boldsymbol\\theta^{*}),\\hat{g}_{\\boldsymbol\\xi}(\\boldsymbol\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]}{\\mathbb{E}_{\\boldsymbol\\xi}\\|\\hat{g}_{\\boldsymbol\\xi}(\\boldsymbol\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2}+\\frac{2}{L\\gamma}\\|\\hat{g}(\\boldsymbol\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2}}}\\end{array}$ in Lemma $B.6$ in Appendix B.5), and any realization of $\\theta_{m}$ , it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underbrace{\\mathbb{E}[l(\\theta_{m,t})-l(\\theta^{*})|\\theta_{m}]}_{\\mathrm{\\normalfont~\\leftmoon~}}\\leq\\underbrace{(1-\\gamma\\mu)^{t}(l(\\theta_{m})-l(\\theta^{*}))}_{\\mathrm{\\normalfont~\\leftmoon~}}+\\underbrace{\\frac{8C^{2}}{\\mu}g^{\\varepsilon}(\\theta_{m})}_{\\mathrm{\\normalfont~\\leftmoon~}}}&{+\\underbrace{\\frac{2}{\\mu}N(\\lambda_{m}^{\\dagger};\\theta_{0:m},{\\bf w}_{0:m})}_{\\mathrm{\\normalfont~\\leftmoon~}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\emph{w h e r e}\\emph{l*}\\emph{:=}\\emph{l}(\\theta^{*})\\emph{\\ w i t h}\\emph{\\theta^{*}}\\emph{\\in}\\,\\mathrm{~arg\\,min}_{\\theta\\in\\Theta}l(\\theta),\\ \\ g^{\\mathcal{E}}(\\theta)\\emph{\\theta}:=\\emph{\\mathbb{E}}[\\mathbf{1}(\\hat{Y}(X;\\theta)\\emph{\\theta}\\emph{\\forall}Y(\\theta))],}\\\\ &{N(\\lambda;\\theta_{0:m},\\mathbf{w}_{0:m})\\emph{=}\\,\\lambda^{2}\\emph{\\|}\\hat{\\ y}(\\theta_{0:m},\\mathbf{w}_{0:m})\\emph{\\|2+}\\frac{L\\gamma}{2}\\mathbb{E}_{\\xi}\\left[\\|\\nabla\\tilde{l}_{\\xi}(\\theta^{*})-\\lambda\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2}\\right],}\\\\ &{N(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m})\\emph{\\leq}N(0)\\ w h e r e\\ N(0):=N(0;\\theta_{0:m},\\mathbf{w}_{0:m})f o r\\,a n y\\ (\\theta_{0:m},\\mathbf{w}_{0:m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Further, when $\\mathbf{w}_{0:m}$ is such that $\\mathbb{E}_{\\xi}[\\langle\\nabla\\tilde{l}_{\\xi}(\\theta^{*}),\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]\\ge0,$ , i.e., the teacher has a sufficiently good performance, it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{N(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m})}{N(0)}\\leq\\operatorname*{min}\\left(1,2\\operatorname*{min}\\left(1,\\frac{C^{2}g^{K L}(\\theta_{0:m},\\mathbf{w}_{0:m})}{\\sigma_{*}\\sigma_{(\\theta_{0:m},\\mathbf{w}_{0:m})}}\\right)+\\frac{2C^{2}g^{C}(\\theta_{0:m},\\mathbf{w}_{0:m})}{L\\gamma\\sigma_{(\\theta_{0:m},\\mathbf{w}_{0:m})}^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\sigma_{*}^{2}:=\\mathbb{E}_{\\xi}\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\parallel^{2},\\ \\sigma_{(\\theta_{0:m},\\mathbf{w}_{0:m})}^{2}:=\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2},\\ g^{K L}(\\theta_{0:m},\\mathbf{w}_{0:m})=0$ $\\mathbb{E}_{X}[D_{K L}(f(X;\\theta^{*})\\ \\Vert\\ \\bar{f}(X;\\theta_{0:m},\\dot{\\mathbf{w}_{0:m}}))]$ with $D_{K L}(p\\parallel q)$ is the KL-divergence between $p$ and $q$ , an $\\begin{array}{r}{\\emph{l g}^{C}(\\theta_{0:m},\\mathbf{w}_{0:m})=\\parallel\\mathbb{E}_{X}[\\bar{f}(X;\\theta_{0:m},\\mathbf{w}_{0:m})]-\\mathbb{E}_{X}[\\hat{Y}(X;\\theta_{m})]\\parallel^{2}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "In Theorem 3.2, (7) characterizes the optimality gap in the inner loop optimization. Specifically, the first term is about reducing the initial optimality gap over iterations and motivates why we need adaptation, e.g., by self-training, if the performance deteriorates under severe distribution shifts. The second term is about the bias of the pseudo label and motivates the challenges of self-training under poorly performing pseudo labels in the case of severe distribution shifts. Crucially, these two terms can be fully characterized by the quality of initial model $\\theta_{m}$ and do not depend on $\\mathbf{w}_{0:m}$ . Thus, we concentrate on the impacts of $\\mathbf{w}_{0:m}$ designed in (1)-(3) on $N(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m})$ to show that AnCon\u2019s effectiveness on improving self-training performance under distribution shifts. ", "page_idx": 5}, {"type": "text", "text": "First, Theorem 3.2 shows the effectiveness of our label smoothing formulation in (1) under properly chosen $\\lambda_{m}^{\\dagger}$ . Specifically, compared to vanilla self-training, AnCon results in the smaller neighborhood size of the stochastic gradient descent; $N(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m})\\leq N(0)$ . That is, the result suggests that AnCon is at least better than vanilla self-training under the mild regularity conditions. ", "page_idx": 5}, {"type": "text", "text": "Further, under the additional assumption of a sufficiently good performance temporal ensemble, (8) motivates AnCon\u2019s weighting mechanism as a relaxed solution of the intractable optimization problem in (4). Specifically, if the marginal distribution of the pseudo labels does not change quickly over outer iterations (which is the case especially for the later training stages as shown in Figure 5 in Appendix), changing $\\mathbf{w}_{0:m}$ would have only a marginal impact on $g^{C}(\\bar{\\theta}_{0:m},\\mathbf{w}_{0:m})$ . Thus, the weighting mechanism that minimizes $g^{K L}(\\theta_{0:m},\\mathbf{w}_{0:m})$ would minimize the worst-case optimality gap, which justifies our approach of circumventing intractability of (4) with (5). In this regard, AnCon\u2019s weighting mechanism in (3) could be thought of as a relaxed solution of (4) as it minimizes $g^{K L}(\\theta_{0:m},\\Bar{\\mathbf{w}}_{0:m})$ in the asymptotic region (cf. Theorem 3.1). ", "page_idx": 5}, {"type": "text", "text": "We conclude this section by analyzing the three iterative steps where the pseudo labels and the temporal ensembles keep updating. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.2.1. Let us assume $l(\\theta)$ satisfies $L$ -smoothness, $\\mathcal{L}$ -expected smoothness, and $\\mu{-}P L$ condition. For $\\begin{array}{r}{\\gamma\\leq\\frac{\\mu}{4\\mathcal{L}\\cdot L}}\\end{array}$ , $\\lambda$ carefully adjusted at each outer temporal ensemble update (cf. $\\lambda=\\lambda_{j}^{\\dagger}$ in Lemma B.6 for each outer iteration $j\\in\\{0,\\cdots\\,,\\hat{T}-1\\})$ , and any initial parameter $\\theta_{0}$ , it holds that $\\begin{array}{r}{\\mathbb{E}[l(\\theta_{\\hat{T}})-l^{*}]\\leq(1-\\mu\\gamma)^{T\\cdot(\\hat{T}-1)}(l(\\theta_{0})-l^{*})+\\zeta_{\\hat{T}}\\mathbb{E}_{j\\sim I^{(\\hat{T})}}[\\frac{8C^{2}}{\\mu}g^{\\mathcal{E}}(\\theta_{j})+\\frac{2}{\\mu}N(\\lambda_{j}^{\\dagger};\\theta_{0:j},{\\mathbf w}_{0:j})]}\\end{array}$ (9) where $p(I^{(\\hat{T})}=j)\\propto(1-\\mu\\gamma)^{T\\cdot(\\hat{T}-1-j)}$ for $j\\in\\{0,\\cdot\\cdot\\cdot,\\hat{T}-1\\}$ and $\\begin{array}{r}{\\zeta_{\\hat{T}}=\\sum_{i=0}^{\\hat{T}-1}(1-\\mu\\gamma)^{T\\cdot i}.}\\end{array}$ . Corollary 3.2.1 is proved in Appendix B.4 and gives a whole picture of the optimality gap under AnCon. We first remark the trade-off associated with $\\hat{T}$ on the suboptimality $\\mathbb{E}[l(\\theta_{\\hat{T}})-l^{*}]$ , which characterize the early-learning phenomenon observed in the biased gradient settings (e.g., self-training [31] and LFN [32]). Specifically, in (9), increasing $\\hat{T}$ reduces the first term $\\big(1\\!-\\!\\mu\\gamma\\big)^{T\\cdot(\\hat{T}-1)}\\big(l(\\theta_{0})\\!-\\!l^{*}\\big)$ but increases the coefficient of the second term $\\zeta_{\\hat{T}}$ . Therefore, a longer training with a large $\\hat{T}$ may not enhance the self-training performance especially under a large second term due to inaccurate pseudo labels or the temporal ensemble. ", "page_idx": 5}, {"type": "text", "text": "Nevertheless, for each outer loop iteration, $g^{\\mathcal{E}}({\\boldsymbol{\\theta}}_{j})$ would be smaller under AnCon than its value under vanilla self-training as $\\mathbb{E}[l(\\theta_{j})^{\\mathsf{\\bar{\\theta}}}-l^{*}]$ has a tighter upper bound under AnCon due to Theorem 3.2. Therefore, with the guarantee $N(\\lambda_{j}^{\\dagger};\\theta_{0:j},{\\bf w}_{0:j})\\le N(0)$ , AnCon would achieve a tighter upper bound of (9) than the vanilla self-training method, enabling longer training with smaller value of the second term as observed in Figure 1b. Finally, we remark that this theoretical superiority of $\\mathtt{A n C o n}$ can be extended to the self-training methods with other weighting mechanisms in the asymptotic region when $\\mathbb{E}_{\\xi}[\\langle\\nabla\\tilde{l}_{\\xi}(\\theta^{*}),\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{\\bar{w}}_{0:m})\\rangle]\\geq0$ (cf. Theorem 3.2). ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Goal and baselines Part of the experiments shows that AnCon surpasses the vanilla self-training method that uses ${\\hat{Y}}(x;\\theta)$ as a pseudo label, which serves as a strong baseline, under different types of distribution shifts (Sections 4.1-4.2). We also show that AnCon achieves a better performance than ELR with $\\lambda_{E L R}^{*}\\in\\{1,3,7,12,25\\}$ , which is the coefficient multiplied to the auxiliary loss, to show effectiveness of uncertainty aware temporal ensemble. Finally, we assess the integration of AnCon with generalized cross-entropy (GCE) [33, 31] and neighborhood reciprocity clustering (NRC) [17], which are frequently cited as state-of-the-art methods [20, 34, 31]. We note that AnCon can be seamlessly applied to GCE and NRC by replacing one-hot pseudo labels by AnCon\u2019s regularized pseudo labels $\\tilde{Y}(X;\\theta_{0:m},\\mathbf{w}_{0:m})$ . For descriptions of the training configurations which we adopt from literature, see Appendix D. ", "page_idx": 5}, {"type": "table", "img_path": "a17biETKyI/tmp/b688c3dd036a1d81d60ed5cf5b1a70b2fd710d19d2653ad6da2b090fd17e949e.jpg", "table_caption": ["Table 1: SFDA benchmark results. The numbers indicate the mean test accuracy across three repetitions. We present half of the domain pairs for Office-31 and OfficeHome in the main body and the rest of the pairs are presented in Appendix (cf. Tables 3, 4, and 6). "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Evaluation In our self-training settings (SFDA and TTA), we should determine the best checkpoint without labeled samples, i.e. to select a model $\\theta_{0},\\theta_{1},\\cdot\\cdot\\cdot,\\theta_{I}$ which is used for evaluation on test. We use information maximization [35], I $M(\\theta)\\,=\\,H_{E}\\bigl(\\mathbb{E}_{X}[f(X;\\theta)]\\bigr)\\,-\\,\\mathbb{E}_{X}[H_{E}\\bigl(f(X;\\theta)\\bigr)]$ when $H_{E}$ is the entropy, which is proven to be effective in unsupervised domain adaptation (UDA) model selection literature [36, 37]. Specifically, we evaluate $I M(\\theta_{m})$ on the hold-out unlabeled samples at the end of each epoch $m$ for $I$ number of training epochs and select the checkpoint with $\\theta^{*}\\in\\arg\\operatorname*{max}_{m\\in[I]}I M(\\theta_{m})$ as the best one. This $\\theta^{*}$ is used for final evaluation on the test data. ", "page_idx": 6}, {"type": "text", "text": "Implementation details of AnCon For each (outer) iteration $m$ , we set $w_{j}(x)={\\bf1}(c(x;\\theta_{j})>\\delta_{j}^{(\\beta)})$ for $j\\in\\{0,\\cdots,m\\}$ , instead of $\\begin{array}{r}{w_{j}(x)=\\frac{\\mathbf{1}(c(x;\\theta_{j})>\\delta_{j}^{(\\beta)})}{\\sum_{i=0}^{m}\\mathbf{1}(c(x;\\theta_{i})>\\delta_{i}^{(\\beta)})}}\\end{array}$ im1=(0c (1x(;c\u03b8(jx);\u03b8>i\u03b4)j>\u03b4i)(\u03b2)). Note that the former with \u03bb can be thought of as the latter with instance-dependent coefficient $\\boldsymbol{\\lambda}\\!\\cdot\\!\\boldsymbol{Q}(x;\\mathbf{c}_{0:m})$ , which assigns more weight on the generalized temporal ensemble for $x$ with large $Q(x;\\mathbf{c}_{0:m})$ . In addition, we consider the online update scenario of pseudo labels, i.e., $T=1$ . Finally, given the challenging nature of hyperparameter selection in self-training under distribution shifts, we perform all experiments by using a single configuration of hyperparameters of AnCon ( $\\lambda=0.3$ and $\\beta=0.9_{\\cdot}$ ). These hyperparameters result in the maximum value of $\\operatorname*{max}_{m\\in[\\hat{T}]}I M(\\theta_{m})$ on the hold-out unlabeled samples on Office-31. ", "page_idx": 6}, {"type": "text", "text": "4.1 Self-training under domain shifts ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first consider SFDA that adapts a model trained in one domain by performing self-training in the distribution shifted domain. For the network architecture, we use the modified ResNet used in [16], which includes batch normalization [38] after the bottleneck layer and weight normalization [39] in the last linear layer, which is used in [17, 8] for stabilizing the learning process in SFDA. ", "page_idx": 6}, {"type": "text", "text": "Datasets We evaluate AnCon on the following datasets: Office-31 [40] with 4,000 images of 31 categories from three domains (amazon, dslr, webcam); OfficeHome [41] with 15,000 images of 65 categories from four domains (art, clipart, product, real-world); VisDa [42] with 280,000 images of 31 categories from two domains (synthetic, real). ", "page_idx": 6}, {"type": "text", "text": "Results AnCon consistently improves self-training in diverse domain pairs across the three datasets (Table 1). Specifically, it reduces the average self-training test error by $5\\%$ in Office-31, $6\\%$ in OfficeHome, and $13\\%$ in VisDa. In addition, compared to ELR with its dataset-dependent optimal hyperparameter value, AnCon shows comparable performance to ELR $(2\\%,\\,3\\%$ , and $-3\\%$ performance differences in Office-31, OfficeHome, and VisDa, respectively). Despite its slightly inferior performance in VisDa, we note that AnCon achieves significantly better accuracy for low-performing classes: for \u201cSkateboard\u201d and \u201cTruck\u201d classes, AnCon achieves $50\\%$ and $16\\%$ of accuracies, while ELR achieves $39\\%$ and $0.43\\%$ in these classes. Thus, AnCon can be as effective as ELR for improving the self-training performances under distribution shifts without needing to adjust hyperparameter values for each dataset unlike ELR. ", "page_idx": 6}, {"type": "image", "img_path": "a17biETKyI/tmp/90515431aecaca8bf8fa1ce5cde41dbbbe7844bc2ef865258627ec0ca718f364.jpg", "img_caption": ["Figure 1: Section 4.2: (a) Test accuracy for each intensity level in ImageNet-C. (b) Performance degeneration in the defocus blur corruption with intensity 4. Section 4.3.1: (c) Maximum performance changes under different model selection methods. We present performances for individual corruptions in Appendix. For all boxplots used in the paper, the box represents interquantile range with whiskers as $\\pm\\,1.5$ interquantile range and the horizontal line inside the box represents the median. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Notably, AnCon significantly improves performances of both GCE and NRC via fundamentally different mechanisms for handling noisy pseudo labels (e.g., reducing test accuracy of $3\\%$ and $8\\%$ on average in OfficeHome, respectively). Specifically, NRC filters incorrect predictions based on local consistency, while AnCon uses temporal consistency. Combining NRC and AnCon leverages pseudo labels that are both locally and temporally consistent, resulting in significant performance improvements over NRC or Self-Training $^+$ AnCon (cf. Table 1). In addition, GCE reduces the impact of wrong pseudo labels rather than finding them. Applying GCE to AnCon minimizes the effects of potentially wrong but temporally consistent pseudo labels, which can be implied by the performance of $\\mathrm{GCE}+\\mathtt{A n C o n}$ compared to GCE or Self-Training $+\\;{\\tt A n C o n}$ (cf. Table 1). Thus, the impressive performance gains from AnCon, which would be orthogonal to the gains from state-of-the-art methods in SFDA, show its significant practical implications. ", "page_idx": 7}, {"type": "text", "text": "4.2 Self-training under synthetic corruption operations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While we have considered the domain shift, e.g., adaptation of a model trained on synthetic images to real images, in Section 4.1, this section examines the self-training\u2019s ability to adapt to distribution shifts by synthetic image corruptions. This setting has been used to measure the robustness of neural networks with respect to a general out-of-distribution setting. To this end, we consider ImageNet-C [6], which consists of 50,000 images drawn from a validation set of ImageNet [43] where each image is corrupted by 15 types of synthetic corruptions related to noise, blur, weather and digital. ", "page_idx": 7}, {"type": "text", "text": "Result Consistent with the findings under the domain shift, AnCon outperforms the average performances of self-training and ELR under varying levels of corruption intensities (cf. Figure 1a and Tables 7-11 in Appendix), improving the self-training method\u2019s accuracy by $16\\%$ on average. Further, the gains from AnCon is significant when the distribution shifts are intense (e.g., improving accuracies by $20\\%$ and $52\\%$ on average in intensities of 4 and 5) where the initial model trained on the source domain significantly deteriorates. Specifically, for Shot, Impulse, and Gaussian corruptions with the most extreme shift intensity of 5, where the initial model achieves accuracies of $(3.04\\%$ , $1.76\\%$ , $2.12\\%$ ), AnCon achieves $22.56\\%$ , $26.56\\%$ , $25.85\\%$ ) (cf. Table 11). This striking improvement compared to vanilla self-training with performances $3.26\\%$ , $1.72\\%$ , $1.04\\%$ and ELR with performances $(8.00\\%$ , $14.12\\%$ , $16.00\\%)$ ), underscores the importance of the AnCon\u2019s uncertainty-aware temporal consistency scheme, as shown in Corollary 3.2.1. We note that this impressive result is also explained by AnCon\u2019s ability to prevent the gradual performance degradation during the course of training with the extremely noisy pseudo labels (cf. Figure 1b). Combined with previous results in domain shift scenarios, we expect that AnCon would work effectively in various out-of-distribution settings. ", "page_idx": 7}, {"type": "text", "text": "4.3 Versatility of AnCon ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In previous sections, we have shown the universality of AnCon by evaluating it on diverse distribution shift scenarios. In this section, we show versatility of AnCon by analyzing its attractive properties in robustness and uncertainty representation. ", "page_idx": 7}, {"type": "text", "text": "4.3.1 Robustness to model selection ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "There is no universally agreed model selection criterion, such as cross-validation in the i.i.d. setting, in self-training under distribution shifts. This is partly due to the variety of distribution shift scenarios, where an effective criterion in one may be ineffective or inapplicable in another; for instance, a principled criteria called importance-weighted cross validation [44] in UDA cannot be applied to SFDA. In this regard, it would be an important characteristic of a self-training method under distribution shift to be robust with respect to different choices of model selection criteria. Therefore, we evaluate robustness with respect to the following different model selection criteria: InfoMax [35], Corr-C [36], and Ent [45] (see Appendix D.3 for the description). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Figure 1c shows that AnCon\u2019s maximum performance change due to different model selection methods is much lower than that of other methods, especially under severe distribution shifts. This valuable advantage can be contributed to the property of AnCon that can prevent performance degeneration (cf. Figure 1b). Given that, in practice, we barely know when the model collapse happens and which model selection criteria are the best, the results highlight a significant practical value of AnCon. ", "page_idx": 8}, {"type": "text", "text": "4.3.2 Robustness to the choice of hyperparameters ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Throughout this paper, we have shown that our single config  \nuration of parameters $(\\lambda\\,=\\,0.3,\\beta\\,=\\,0.9)$ work well across a   \nwide range of benchmark problems. In this section, we aim   \nto show our findings can be preserved when the hyperparam  \neter values deviate from the default setting by performing a   \nsensitivity analysis for values $\\lambda\\,\\in\\,\\{0.1,0.{\\bar{3}},0.5,\\bar{0}.7,0.9\\}$ and   \n$\\beta\\,\\in\\,\\{0.1,0.3,0.5,0.7,0.9\\}$ . We also test two frequently used   \nannealing schedules that $\\dot{\\lambda_{m}}=m/I$ and $\\lambda_{m}=\\operatorname*{min}(1,2m/I)$ ,   \ncalled full and half, respectively. Figure 2 shows that AnCon is sta  \nble even under extreme values of hyperparameters. Specifically,   \nfor both hyperparameters, the maximum average performance   \nchange is less than $1\\%$ , and $\\beta$ barely impacts the performance   \nof AnCon. Indeed, our analysis suggests to increase $\\lambda$ from our   \ndefault setting; that is, to put a higher weight on the general   \ntemporal ensemble\u2019s prediction. Here, we note that our subop  \ntimal choices of hyperparameters are due to our rigorous and   \npractical hyperparameter choice. Given the challenging nature   \nof hyperparameter optimization under distribution shifts, the sta  \nble performances of AnCon under arbitrary choices of hyperparameters would enable AnCon to be seamlessly applied to diverse practical settings. ", "page_idx": 8}, {"type": "image", "img_path": "a17biETKyI/tmp/430c43d5604f8056155473cccf9cb092e00a4152a5f19d660efb9c90cce7b49c.jpg", "img_caption": ["(b) "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 2: Sensitivity analysis with respect to $\\lambda$ and $\\beta$ on four domain pairs (Ar-Pr, Pr-Cl, RwCl, Rw-Pr) in OfficeHome. Here, green triangles are means. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.3.3 Improved calibration performance ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We have shown that all self-training methods significantly improve the performance of the baseline method after the adaptation period. However, it is widely known that these noticeable improvements come with the price of sacrificing an uncertainty representation ability which is critical in real-world decision-making scenarios [46, 47]. Specifically, the calibration performance, which is the gap between the prediction confidence and accuracy, usually monotonically increases as self-training keeps reducing the uncertainty for all predictions during the course of training. In this regard, we analyze the calibration performance with respect to the expected calibration error (ECE; see Appendix for definition). Here, a lower ECE means a lower gap between confidence and accuracy. ", "page_idx": 8}, {"type": "text", "text": "As shown in Figure 3a and Table 5 in Appendix, AnCon gives much lower ECE compared to other methods. Considering ELR and GCE both have regularization effects, we conjecture that this phenomenon is due to selective regularization in AnCon that increases prediction confidences of samples only if the past confident predictions are consistent with the current prediction. Especially, in ", "page_idx": 8}, {"type": "image", "img_path": "a17biETKyI/tmp/ae420ba89fc32eccae28cdd85fd8421a93fca9f1b206b79dd5f12b9cd945ab38.jpg", "img_caption": ["Figure 3: (a) ECEs under five levels of intensities in ImageNet-C; (b) Accuracy and ECE changes during the course of training in VisDa. ", ""], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3b which confirms the accuracy-calibration dilemma in VisDa, AnCon is shown to limit the ECE increases during training compared to all other methods. That is, AnCon helps to significantly reduce the price of the calibration performance we need to pay for improving accuracy, which are both important measures in practice. ", "page_idx": 9}, {"type": "text", "text": "4.4 Algorithmic design choices ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Recall that we define $\\begin{array}{r}{\\bar{f}(x;\\theta_{0:m},\\mathbf{w}_{0:m})=\\sum_{i=0}^{m}w_{i}(x)\\cdot p(y|x,\\theta_{i})}\\end{array}$ with our simple design choices: the relative thresholding for weighting scheme $w_{i}(x)\\propto{\\bf1}(c(x;\\theta_{i})>\\delta_{i}^{(\\beta)})$ and hard prediction for $p(\\boldsymbol{y}|\\boldsymbol{x},\\boldsymbol{\\theta}_{i})$ . In Appendix C.3, we found that our simple design choices are more appropriate for the distribution shift settings than several more sophisticated alternatives, which can be summarized as follows. ", "page_idx": 9}, {"type": "text", "text": "\u2022 More sophisticated weighting schemes (e.g., Entropy $(w_{i}(x)\\,\\propto\\,\\exp\\left\\{-H_{E}[f(x;\\theta_{i})]\\right\\}))$ reduce the self-training performance, despite being a more accurate measure of prediction uncertainty. We conjecture that the poor calibration performance of the neural network in self-training under distribution shifts prevents the sophisticated weighting schemes from accurately reflecting the goodness of the prediction.   \n\u2022 Various soft prediction schemes, which can give more information about the non-leading entry values, leads to performance reductions. We conjecture that the continuously increasing confidence in the later stage of self-training would make soft prediction ignore early-stage predictions which may be valuable to memorize. ", "page_idx": 9}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Filtering incorrect pseudo labels Popular confidence-based thresholding methods [13, 14, 9] fall short under distribution shifts since even high confident predictions can be highly incorrect. Therefore, recent advances in SFDA and TTA utilize higher order information to filter incorrect pseudo labels. For instance, based on the intuition that true labels of adjacent samples would be same, centroids for each predicted class can be maintained in the feature space and then the pseudo label for each input is corrected by the adjacent centroid [16]. The idea of using per-class centroids has been extended to incorporate more general clustering structures [18, 17]. However, the neighborhood structure-based methods are computationally demanding due to storage of memory banks in the feature space and nearest neighbors search. Such computational complexity persists in other approaches, which are based on the consistency of multiple predictions from different augmentations [20] and models trained with different loss functions [19]. Compared to these solutions, AnCon can efficiently estimate correct labels with only limited extra memory overhead of storing past predictions. ", "page_idx": 9}, {"type": "text", "text": "Learning from noisy labels Treating pseudo labels as inherently noisy, techniques from the LFN literature have been integrated to self-training. For instance, the LFN literature has proposed robust loss functions that reduce impacts of random noisy labels [48, 33], and a recent large-scale experimental study shows the applicability of the generalized cross-entropy in the SFDA setting [31]. The effectiveness of ELR on SFDA [8] bears a similar idea because ELR was developed to regularize the neural networks\u2019 tendencies to memorize incorrect labels [21]. Despite their effectiveness, by nature, these approaches do not consider important characteristics of the unbounded and instancedependent noise rates inherent in self-training under distribution shifts, which results in significant suboptimality in both theory and practice. However, by considering the unique characteristics of self-training under distribution shift, AnCon relaxes the conditions required to achieve optimality as well as boosts the self-training performance in diverse scenarios. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces AnCon, which effectively improves self-training performances under diverse distribution shift scenarios by promoting selective temporal consistency based on confident predictions. As a result, AnCon effectively mitigates the detrimental effects of noisy pseudo labels without much computational overhead, unlike the previous methods. We show that AnCon not only advances our theoretical understanding of a generalized notion of temporal consistency in self-training but also can be a practical asset as a simple and effective self-training method with attractive properties. In Appendix C.4, we present limitations and future directions, such as adaptive determination of $\\lambda$ , combining local and temporal consistency, and extending the selective temporal consistency in the sequential decision making problems. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank Jihyeon Hyeong, Yuchen Lou, Jiezhong Wu, and anonymous reviewers for their valuable discussions and constructive suggestions during the preparation of this manuscript. We declare that there was no funding received for this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on Fairness, Accountability and Transparency, 2018.   \n[2] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do Imagenet classifiers generalize to Imagenet? In International Conference on Machine Learning, 2019.   \n[3] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon. Combining satellite imagery and machine learning to predict poverty. Science, 353 (6301):790\u2013794, 2016.   \n[4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In European Conference on Computer Vision, 2018.   \n[5] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In Advances in Neural Information Processing Systems, 2020. [6] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019.   \n[7] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-training for domain adaptation. In Advances in Neural Information Processing Systems, 2021.   \n[8] Li Yi, Gezheng Xu, Pengcheng Xu, Jiaqi Li, Ruizhi Pu, Charles Ling, Ian McLeod, and Boyu Wang. When source-free domain adaptation meets learning with noisy labels. In International Conference on Learning Representations, 2023. [9] David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain adaptation. arXiv preprint arXiv:2106.04732, 2021.   \n[10] Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, 2013.   \n[11] Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Pseudolabeling and confirmation bias in deep semi-supervised learning. In International Joint Conference on Neural Networks, 2020.   \n[12] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semisupervised learning with consistency and confidence. In Advances in Neural Information Processing Systems, 2020.   \n[13] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In Advances in Neural Information Processing Systems, 2021.   \n[14] Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, et al. Freematch: Self-adaptive thresholding for semi-supervised learning. arXiv preprint arXiv:2205.07246, 2022.   \n[15] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems, 2019.   \n[16] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International Conference on Machine Learning, 2020.   \n[17] Shiqi Yang, Joost van de Weijer, Luis Herranz, Shangling Jui, et al. Exploiting the intrinsic neighborhood structure for source-free domain adaptation. In Advances in Neural Information Processing Systems, 2021.   \n[18] Shiqi Yang, Yaxing Wang, Joost Van De Weijer, Luis Herranz, and Shangling Jui. Generalized source-free domain adaptation. In IEEE/CVF International Conference on Computer Vision, 2021.   \n[19] Hao-Wei Yeh, Thomas Westfechtel, Jia-Bin Huang, and Tatsuya Harada. Boosting source-free domain adaptation via confidence-based subsets feature alignment. In International Conference on Pattern Recognition, 2022.   \n[20] Nazmul Karim, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-pang Chiu, Supun Samarasekera, and Nazanin Rahnavard. C-sfda: A curriculum learning aided self-training framework for efficient source free domain adaptation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.   \n[21] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Earlylearning regularization prevents memorization of noisy labels. In Advances in Neural Information Processing Systems, 2020.   \n[22] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3): 107\u2013115, 2021.   \n[23] Devansh Arpit, Stanis\u0142aw Jastrz\u02dbebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International Conference on Machine Learning, 2017.   \n[24] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.   \n[25] Gabriel Pereyra, George Tucker, Jan Chorowski, \u0141ukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.   \n[26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[27] Yaodong Yu, Stephen Bates, Yi Ma, and Michael Jordan. Robust calibration with multi-domain temperature scaling. In Advances in Neural Information Processing Systems, 2022.   \n[28] Taejong Joo and Diego Klabjan. IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation. In International Conference on Machine Learning, 2024.   \n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.   \n[30] Mher Safaryan, Alexandra Peste, and Dan Alistarh. Knowledge distillation performs partial variance reduction. In Advances in Neural Information Processing Systems, 2023.   \n[31] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. If your data distribution shifts, use selflearning. Transactions on Machine Learning Research, 2022.   \n[32] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In International Conference on Artificial Intelligence and Statistics, 2020.   \n[33] Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise for deep neural networks. In AAAI Conference on Artificial Intelligence, 2017.   \n[34] Ori Press, Steffen Schneider, Matthias K\u00fcmmerer, and Matthias Bethge. Rdumb: A simple approach that questions our progress in continual test-time adaptation. In Advances in Neural Information Processing Systems, 2023.   \n[35] Yuan Shi and Fei Sha. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation. In International Coference on International Conference on Machine Learning, 2012.   \n[36] Weijie Tu, Weijian Deng, Tom Gedeon, and Liang Zheng. Assessing model out-of-distribution generalization with softmax prediction probability baselines and a correlation method, 2023. URL https://openreview.net/forum?id $=$ 1maXoEyeqx.   \n[37] Dapeng Hu, Jian Liang, Jun Hao Liew, Chuhui Xue, Song Bai, and Xinchao Wang. Mixed samples as probes for unsupervised model selection in domain adaptation. In Advances in Neural Information Processing Systems, 2023.   \n[38] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015.   \n[39] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in Neural Information Processing Systems, 2016.   \n[40] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European Conference on Computer Vision, 2010.   \n[41] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.   \n[42] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.   \n[43] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, 2009.   \n[44] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M\u00fcller. Covariate shift adaptation by importance weighted cross validation. Journal of Machine Learning Research, 8(5), 2007.   \n[45] Pietro Morerio, Jacopo Cavazza, and Vittorio Murino. Minimal-entropy correlation alignment for unsupervised deep domain adaptation. arXiv preprint arXiv:1711.10288, 2017.   \n[46] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.   \n[47] John D Lee and Katrina A See. Trust in automation: Designing for appropriate reliance. Human Factors, 46(1):50\u201380, 2004.   \n[48] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In International Conference on Machine Learning, 2020.   \n[49] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 3(4):643\u2013653, 1963.   \n[50] Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In International Conference on Machine Learning, 2018.   \n[51] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, 2018.   \n[52] Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. The Annals of Mathematical Statistics, pages 493\u2013507, 1952.   \n[53] Taejong Joo, Uijung Chung, and Min-Gwan Seo. Being Bayesian about categorical probability. In International Conference on Machine Learning, 2020.   \n[54] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning, 2016.   \n[55] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael Jordan. Conditional adversarial domain adaptation. In Advances in Neural Information Processing Systems, 2018.   \n[56] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "a17biETKyI/tmp/580b494d7ec816118e03c158e31d55595f1943335c27d4b49f470d89adffdc4a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B Proof of claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Assumptions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let us recall that $l(\\theta)=\\mathbb{E}_{X Y}[H(f(X;\\theta),p_{Y|X})]$ and $\\begin{array}{r}{l_{\\xi}(\\theta)=\\frac{1}{b}\\sum_{X_{i}\\in\\xi}H(f(X_{i};\\theta),p_{Y\\mid X_{i}})}\\end{array}$ where $\\xi$ contains $b$ number of random samples from $p_{X}$ . We also denote $\\theta^{*}\\in\\arg\\operatorname*{min}_{\\theta\\in\\mathbb{R}^{p}}l(\\theta)$ . In this work, we assume the following three regularity conditions on $l(\\theta)$ which is differentiable with respect to $\\theta$ , which are mild but essential for most theoretical studies with convergence analyses. ", "page_idx": 14}, {"type": "text", "text": "Assumption B.1 ( $L$ -smoothness). $l(\\cdot)$ is $L$ -smooth for some constant $L>0$ ; that is, ", "page_idx": 14}, {"type": "equation", "text": "$$\nl(\\theta^{\\prime})\\leq l(\\theta)+\\langle\\nabla l(\\theta),\\theta^{\\prime}-\\theta\\rangle+\\frac{L}{2}\\parallel\\theta^{\\prime}-\\theta\\parallel^{2},\\quad\\forall\\theta\\in\\mathbb{R}^{p}\\:a n d\\:\\theta^{\\prime}\\in\\mathbb{R}^{p}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assumption B.2 $\\mathcal{L}$ -expected smoothness [30]). $l(\\cdot)$ is $\\mathcal{L}$ -smooth in expectation with respect to $\\xi\\sim\\mathcal{D}$ ; that is, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\xi\\sim\\mathcal{D}}[\\Vert\\ \\nabla l_{\\xi}(\\theta)-\\nabla l_{\\xi}(\\theta^{*})\\ \\Vert^{2}]\\le2\\mathcal{L}(l(\\theta)-l(\\theta^{*}))),\\quad\\forall\\theta\\in\\mathbb{R}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Assumption B.3 $\\lvert\\mu$ -Polyak-Lojasiewicz (PL) condition [49]). $l(\\cdot)$ satisfies the $\\mu$ -PL condition for some constant $\\mu>0$ ; that is, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\parallel\\nabla l(\\theta)\\parallel^{2}\\geq2\\mu(l(\\theta)-l(\\theta^{*})),\\quad\\forall\\theta\\in\\mathbb{R}^{p}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We remark that Assumptions B.1 and B.2 can trivially hold under bounded parameter values that can be guaranteed by optimizing neural networks with finite iterations under a gradient or weight clipping. Assumption B.3 holds for infinite-width neural networks, i.e., the neural tangent kernel (NTK) regime [50]. Given that the gradient descent training dynamics of neural networks can be well approximated by NTK [51], the PL condition can be generally regarded as a mild assumption. ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. For $x$ such that $Q(x;\\mathbf{c}_{0:m})>0$ , let us consider subsequence of index $l$ such that $x\\in A_{l}(c_{l})$ ; that is, $\\big(j_{1},\\cdot\\cdot\\cdot\\;,j_{Q(x;\\mathbf{c}_{0:m})}\\big)$ where $j_{l}=\\operatorname*{min}\\{l\\ge j_{l-1}|x\\in\\bar{A}_{l}(c_{l})\\}$ and $j_{0}=0$ . Let $S_{Q(x;\\mathbf{c}_{0:m})}=$ $\\begin{array}{r}{\\sum_{i=1}^{Q(x;\\mathbf{c}_{0:m})}\\mathbf{1}(Y(x)=\\hat{Y}(x;\\theta_{j_{i}}))}\\end{array}$ . Then, we have the following inequality ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p\\left(\\underset{k}{\\arg\\operatorname*{max}}\\,\\bar{f}_{k}(x;\\theta_{0:m},\\mathbf{w}_{0:m})\\neq Y(x)\\right)\\leq p\\left(S_{Q(x;\\mathbf{c}_{0:m})}\\leq\\frac{Q(x;\\mathbf{c}_{0:m})}{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\exp\\left(-\\frac{Q(x;\\mathbf{c}_{0:m})}{2}\\cdot(2\\bar{p}(x;\\mathbf{c}_{0:m})-1-\\log(2\\bar{p}(x;\\mathbf{c}_{0:m})))\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the first inequality holds because $\\arg\\operatorname*{max}_{k}\\bar{f}_{k}(x;\\theta_{0:m},\\mathbf{w}_{0:m})=Y(x)$ if $S_{Q(x;\\mathbf{c}_{0:m})}~>$ $Q(x;\\mathbf{c}_{0:m})/2$ and the second inequality holds due to Lemma B.4 given later in Appendix B.5 with $p_{i}=p(Y(x)=\\hat{Y}(x;\\theta_{i}))$ , $o=Q(x;\\mathbf{c}_{0:m})$ , and $q=1/2$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B.3 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. By Lemma B.5, any realization of $\\theta_{m}$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[l(\\theta_{m,t})-l(\\theta^{*})|\\theta_{m}]\\leq(1-\\gamma\\mu)^{t}(l(\\theta_{m})-l(\\theta^{*}))+\\displaystyle\\frac{8C^{2}}{\\mu}g^{\\varepsilon}(\\theta_{m})}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{2}{\\mu}\\left(\\lambda^{2}\\parallel\\hat{g}(\\theta_{0:m},{\\bf w}_{0:m})\\parallel^{2}+\\frac{L\\gamma}{2}\\mathbb{E}_{\\xi}\\left[\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})-\\lambda\\hat{g}_{\\xi}(\\theta_{0:m},{\\bf w}_{0:m})\\parallel^{2}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, from the definition $N(\\lambda;\\theta_{0:m},\\mathbf{w}_{0:m})\\quad\\quad=\\quad\\quad\\lambda^{2}\\qquad\\parallel\\qquad\\hat{g}(\\theta_{0:m},\\mathbf{w}_{0:m})$ $\\parallel^{2}$ $\\begin{array}{r}{+\\frac{L\\gamma}{2}\\mathbb{E}_{\\xi}\\left[\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})-\\lambda\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}\\right]}\\end{array}$ , we get (7). To show $N(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m})\\leq N(0)$ , we apply Lemma B.6 as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\nabla(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m})}{N(0)}=1-\\frac{\\left(\\mathbb{E}_{\\xi}[\\langle\\nabla\\tilde{l}_{\\xi}(\\theta^{*}),\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]\\right)^{2}}{\\left(\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}+\\frac{2}{L\\gamma}\\parallel\\hat{g}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}\\right)\\left(\\mathbb{E}_{\\xi}\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\parallel^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To show (8), let us assume $\\begin{array}{r}{\\mathbb{E}_{\\xi}[\\langle\\nabla\\tilde{l}_{\\xi}(\\theta^{*}),\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]\\ge0.}\\end{array}$ . Then, we get the following inequality: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{E}_{\\xi}[\\langle\\nabla\\tilde{l}_{\\xi}(\\theta^{*}),\\,\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]}{(\\mathbb{E}_{\\xi}\\,\\|\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\|^{2})^{1/2}\\cdot(\\mathbb{E}_{\\xi}\\,\\|\\,\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2})^{1/2}}}\\\\ &{=\\frac{\\mathbb{E}_{\\xi}\\,\\|\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\|^{2}+\\mathbb{E}_{\\xi}\\,\\|\\,\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2}-\\mathbb{E}_{\\xi}\\,\\|\\,\\nabla\\tilde{l}_{\\xi}(\\theta^{*})-\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2}}{2(\\mathbb{E}_{\\xi}\\,\\|\\,\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\|^{2})^{1/2}\\cdot(\\mathbb{E}_{\\xi}\\,\\|\\,\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2})^{1/2}}}\\\\ &{\\ge1-\\operatorname*{min}\\Bigg(1,\\frac{\\mathbb{E}_{\\xi}\\,\\|\\,\\nabla\\tilde{l}_{\\xi}(\\theta^{*})-\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\,\\|^{2}}{2(\\mathbb{E}_{\\xi}\\,\\|\\,\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\|^{2})^{1/2}\\cdot(\\mathbb{E}_{\\xi}\\,\\|\\,\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2})^{1/2}}\\Bigg)}\\\\ &{\\ge1-\\operatorname*{min}\\Bigg(1,\\frac{C^{2}\\mathbb{E}_{\\mathcal{X}}D_{\\mathcal{X}}L(f(X;\\theta^{*})\\|\\,\\|\\,\\tilde{f}(X;\\theta_{0:m},\\mathbf{w}_{0:m}))}{(\\mathbb{E}_{\\xi}\\,\\|\\,\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\|^{2})^{1/2}\\cdot(\\mathbb{E}_{\\\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (18) holds due to $a^{2}+b^{2}\\geq2a b$ ; (19) holds due to the assumption $\\parallel x\\parallel\\leq C,\\parallel\\cdot\\parallel_{2}\\leq\\parallel\\cdot\\parallel_{1}$ , and the Pinsker\u2019s inequality applied as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\xi}\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})-\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}\\le C^{2}\\mathbb{E}_{\\xi}\\frac{1}{b}\\displaystyle\\sum_{X_{i}\\in\\xi}\\parallel f(X_{i};\\theta^{*})-\\bar{f}(X_{i};\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le C^{2}\\mathbb{E}_{\\xi}\\displaystyle\\frac{1}{b}\\displaystyle\\sum_{X_{i}\\in\\xi}\\parallel f(X_{i};\\theta^{*})-\\bar{f}(X_{i};\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel_{1}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le2C^{2}\\mathbb{E}_{X}D_{K L}\\big(f(X;\\theta^{*}),\\bar{f}(X;\\theta_{0:m},\\mathbf{w}_{0:m})\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, we get our desired result by combining the following inequality with (15): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{\\nabla(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m})}{N(0)}=1-\\frac{\\left(\\mathbb{E}_{\\xi}[\\langle\\nabla\\tilde{l}_{\\xi}(\\theta^{*}),\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]\\right)^{2}}{\\left(\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}+\\frac{2}{L\\gamma}\\parallel\\hat{g}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}\\right)\\left(\\mathbb{E}_{\\xi}\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\parallel^{2}\\right)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n=1-\\left(\\frac{\\left(\\mathbb{E}_{\\xi}[\\langle\\nabla\\tilde{l}_{\\xi}(\\theta^{*}),\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]\\right)^{2}}{(\\mathbb{E}_{\\xi}\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\parallel^{2})^{1/2}\\cdot(\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2})^{1/2}}\\right)^{2}\\frac{1}{1+\\frac{2}{L\\gamma}\\frac{\\|\\hat{g}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2}}{\\mathbb{E}_{\\xi}\\|\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\leq1-\\left(1-\\operatorname*{min}\\left(1,\\frac{C^{2}\\mathbb{E}_{X}D_{K L}(f(X;\\theta^{*})\\parallel\\bar{f}(X;\\theta_{0:m},{\\mathbf{w}}_{0:m}))}{(\\mathbb{E}_{\\xi}\\parallel\\nabla\\bar{l}_{\\xi}(\\theta^{*})\\parallel^{2})^{1/2}\\cdot(\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}(\\theta_{0:m},{\\mathbf{w}}_{0:m})\\parallel^{2})^{1/2}}\\right)\\right)^{2}\\frac{1}{1+\\frac{2}{L\\gamma}\\frac{\\|\\bar{g}(\\theta_{0:m},{\\mathbf{w}}_{0:m})\\|^{2}}{\\mathbb{E}_{\\xi}\\|\\hat{g}_{\\xi}(\\theta_{0:m},{\\mathbf{w}}_{0:m})\\|^{2}}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\leq2\\operatorname*{min}\\left(1,\\frac{C^{2}\\mathbb{E}_{X}D_{K L}(f(X;\\theta^{*})\\parallel\\bar{f}(X;\\theta_{0:m},\\mathbf{w}_{0:m}))}{(\\mathbb{E}_{\\xi}\\parallel\\nabla\\bar{l}_{\\xi}(\\theta^{*})\\parallel^{2})^{1/2}\\cdot(\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2})^{1/2}}\\right)+\\frac{2}{L\\gamma}\\frac{\\parallel\\hat{g}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}}{\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\le2\\operatorname*{min}\\left(1,\\frac{C^{2}\\mathbb{E}_{X}D_{K L}\\left(f\\left(X;\\theta^{*}\\right)\\parallel\\bar{f}\\left(X;\\theta_{0:m},\\mathbf{w}_{0:m}\\right)\\right)}{\\left(\\mathbb{E}_{\\xi}\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\parallel^{2}\\right)^{1/2}\\cdot\\left(\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}\\left(\\theta_{0:m},\\mathbf{w}_{0:m}\\right)\\parallel^{2}\\right)^{1/2}}\\right)}}\\\\ &{}&{+\\frac{2C^{2}}{L\\gamma}\\frac{\\parallel\\mathbb{E}_{X}\\left[\\bar{f}\\left(X;\\theta_{0:m},\\mathbf{w}_{0:m}\\right)\\right]-\\mathbb{E}_{X}\\left[\\hat{Y}\\left(X;\\theta_{m}\\right)\\right]\\parallel^{2}}{\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}\\left(\\theta_{0:m},\\mathbf{w}_{0:m}\\right)\\parallel^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (24) holds due to 1 \u2212(1\u2212u)2 $\\begin{array}{r}{1-\\frac{(1-u)^{2}}{1+v}\\leq2u+v}\\end{array}$ [30]. ", "page_idx": 16}, {"type": "text", "text": "B.4 Proof of Corollary 3.2.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. For any realization of $\\theta_{m}$ for $m\\in\\{0,1,\\cdots\\,,\\hat{T}-1\\}$ , applying Theorem 3.2 with $\\lambda=\\lambda_{m}^{\\dagger}$ (cf. Lemma B.6) gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[l(\\theta_{m+1})-l^{*}|\\theta_{m}]=\\mathbb{E}[l(\\theta_{m,T})-l^{*}|\\theta_{m}]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le(1-\\mu\\gamma)^{T}(l(\\theta_{m})-l^{*})+\\displaystyle\\frac{8C^{2}}{\\mu}g^{\\mathcal{E}}(\\theta_{m})+\\frac{2}{\\mu}N(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, by recursively applying (27) to $\\theta_{0}$ , we get our desired result as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathfrak{L}[l(\\theta_{\\bar{T}})-l^{*}]\\leq(1-\\mu\\gamma)^{T\\cdot(\\hat{T}-1)}(l(\\theta_{0})-l^{*})+\\sum_{i=0}^{\\hat{T}-1}(1-\\mu\\gamma)^{T\\cdot(\\hat{T}-1-i)}\\left(\\frac{8C^{2}}{\\mu}g^{\\varepsilon}(\\theta_{i})+\\frac{2}{\\mu}N(\\lambda_{i}^{\\dagger};\\theta_{0:i},{\\mathbf{w}}_{0:i},}\\end{array}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.5 Supporting lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma B.4. Let $A_{i}\\,\\sim\\,B e r n(p_{i})$ with $p_{i}\\,\\in\\,[0,1]$ and $\\begin{array}{r}{S_{o}\\,=\\,\\sum_{i\\in[o]}A_{i}}\\end{array}$ , where $A_{i}$ and $A_{j}$ are independent for $i\\neq j$ . Then for any $\\begin{array}{r}{q<\\bar{p}:=\\frac{1}{o}\\sum_{i\\in[o]}p_{i}}\\end{array}$ , we have the tail probability bound ", "page_idx": 16}, {"type": "equation", "text": "$$\np(S_{o}\\le q\\cdot o)\\le\\exp\\left(o\\left(q-\\bar{p}-q\\log\\frac{q}{\\bar{p}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. For any $t$ , the moment generating function of $S_{m}$ is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M(t)=\\mathbb{E}_{S_{o}}\\left[\\exp(t S_{o})\\right]=\\displaystyle\\prod_{i\\in[o]}\\mathbb{E}_{A_{i}}\\left[\\exp(t A_{i})\\right]=\\exp\\left(\\displaystyle\\sum_{i\\in[o]}\\log(\\mathbb{E}_{A_{i}}[\\exp(t A_{i})])\\right)}\\\\ &{=\\exp\\left(\\displaystyle\\sum_{i\\in[o]}\\log(p_{i}(\\exp(t)-1)+1)\\right)\\leq\\exp\\left(\\displaystyle\\sum_{i\\in[o]}p_{i}(\\exp(t)-1)\\right)=\\exp(o\\bar{p}(\\exp(t)-1))}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the first equality comes from the independence and the inequality is from $\\log x\\leq x-1$ . Then, for $t<0$ and $q<\\bar{p}$ , the Chernoff bound [52] gives ", "page_idx": 16}, {"type": "equation", "text": "$$\np(S_{o}\\le q\\cdot o)\\leq M(t)\\exp(-t\\cdot q\\cdot o)=\\exp(o\\bar{p}(\\exp(t)-1)-t\\cdot q\\cdot o).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, setting $t=\\log\\left(q/\\bar{p}\\right)<0$ gives our desired result. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.5 (Modification from [30]). Let us assume that $l(\\theta)$ satisfies the $L$ -smoothness, the $\\mathcal{L}$ -expected smoothness, and the $\\mu{-}P L$ condition. Also, we assume a linear model $f_{k}(x;\\theta)\\;=$ $\\{\\exp(\\theta^{T}x)\\}_{k}/\\sum_{i\\in[K]}\\{\\exp(\\theta^{T}x)\\}_{i}$ with $\\theta\\;\\in\\;\\mathbb{R}^{d\\times K}$ . Then, for the stochastic gradient descent $\\theta_{m,t+1}=\\theta_{m,t}-\\gamma g_{\\xi}^{(m,t)}$ g(m,t)with g\u03be(m,t)= \u2207\u02c6EXi\u2208\u03be[H(f(Xi; \u03b8m,t), Y\u02dc (Xi; \u03b80:m, w0:m)], \u03b8m,0 := \u03b8m, and a constant learning rate $\\begin{array}{r}{\\gamma\\leq\\frac{\\mu}{4\\mathcal{L}\\cdot L}}\\end{array}$ , any realization of $\\theta_{m}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathbb{E}[l({\\boldsymbol{\\theta}}_{m,t})-l({\\boldsymbol{\\theta}}^{*})|{\\boldsymbol{\\theta}}_{m}]\\leq(1-\\gamma\\mu)^{t}(l({\\boldsymbol{\\theta}}_{m})-l({\\boldsymbol{\\theta}}^{*}))+\\frac{8C^{2}}{\\mu}g^{\\mathcal{E}}({\\boldsymbol{\\theta}}_{m})}}\\\\ {{\\displaystyle\\qquad\\qquad+\\left.\\frac{1}{\\mu}\\left(2\\lambda^{2}\\parallel\\hat{g}({\\boldsymbol{\\theta}}_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}+L\\gamma\\mathbb{E}_{\\xi}\\left[\\parallel\\nabla\\tilde{l}_{\\xi}({\\boldsymbol{\\theta}}^{*})-\\lambda\\hat{g}_{\\xi}({\\boldsymbol{\\theta}}_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}\\right]\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. For any realization of $\\theta_{m,t}$ , we have the following bound: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\xi}[l(\\theta_{m,t+1})-l(\\theta^{*})|\\theta_{m,t}]\\le(l(\\theta_{m,t})-l(\\theta^{*}))-\\gamma\\langle\\nabla l(\\theta_{m,t}),g^{(m,t)}\\rangle+\\frac{L\\gamma^{2}}{2}\\mathbb{E}_{\\xi}\\left[\\Vert\\;g_{\\xi}^{(m,t)}\\;\\Vert^{2}\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad-(\\mu(\\nu_{\\mathrm{ex}})-\\nu_{\\mathrm{e}})(\\nu_{\\mathrm{ex}})-\\gamma\\nabla\\left[\\left(\\eta\\right)\\psi_{\\mathrm{ex}}\\right]\\mid^{2}+\\gamma\\nabla\\left[\\left(\\eta\\right)\\psi_{\\mathrm{ex}}\\right],}\\\\ &{\\quad+\\frac{\\lambda}{2}\\frac{\\mathcal{G}^{2}}{2}\\mathcal{G}\\left[\\left(\\nabla c_{\\mathrm{f}}(\\nu_{\\mathrm{ex}})-\\lambda\\right)\\phi_{\\mathrm{ex}}(\\nu_{\\mathrm{ex}})\\nabla c_{\\mathrm{f}}\\right]\\mid^{2}}\\\\ &{\\quad+\\frac{\\mathcal{G}^{2}}{3}\\mathcal{G}\\left[\\left(\\eta\\right)\\phi_{\\mathrm{ex}}-\\lambda\\right)(\\eta^{2})-\\frac{\\mathcal{G}^{2}}{2}(\\eta(\\nu_{\\mathrm{e}})-\\nu_{\\mathrm{e}})(\\nu_{\\mathrm{e}})\\nabla c_{\\mathrm{f}}\\right]}\\\\ &{\\quad+\\frac{\\mathcal{G}^{2}}{3}\\mathcal{G}\\left[\\left(\\eta\\right)\\phi_{\\mathrm{ex}}-\\lambda\\right](\\eta^{2})-\\frac{\\mathcal{G}^{2}}{2}(\\eta(\\nu_{\\mathrm{e}})-\\lambda)(\\eta^{2})+\\gamma\\nabla^{2}\\left[\\left(\\nu_{\\mathrm{e}}^{2}-\\nu_{\\mathrm{e}}\\right)^{2}\\right]}\\\\ &{\\quad+\\frac{\\mathcal{G}^{2}}{3}\\mathcal{G}\\left[\\left(\\nabla c_{\\mathrm{f}}(\\nu_{\\mathrm{ex}})-\\lambda\\right)\\phi_{\\mathrm{ex}}(\\nu_{\\mathrm{e}})-\\nabla c_{\\mathrm{f}}\\right]\\mid^{2}+\\gamma\\nabla^{2}\\left[\\left(\\nu_{\\mathrm{e}}^{2}-\\lambda\\right)\\right]}\\\\ &{\\quad+\\frac{\\mathcal{G}^{2}}{3}\\mathcal{G}\\left[\\left(\\eta\\right)\\phi_{\\mathrm{ex}}-\\nabla c_{\\mathrm{f}}(\\nu_{\\mathrm{e}})-\\nabla c_{\\mathrm{f}}(\\nu_{\\mathrm{e}})\\nabla c_{\\mathrm{f}}\\right]\\mid^{2}+\\mathcal{F}\\left[\\left(\\nu_{\\mathrm{e}}^{2}-\\lambda\\right)\\phi_{\\mathrm{ex}}(\\nu_{\\mathrm{ex}})\\nabla c_{\\mathrm{f}}\\right]}\\\\ &{\\quad+\\frac{\\mathcal{G}^{2}}{3}\\mathcal{G}\\left[\\left(\\eta\\right)\\phi_{\\mathrm{ex}}-\\nabla c_ \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where (33) holds due to $L$ -smoothness; (34) holds due to $g_{\\xi}^{(m,t)}=\\nabla l_{\\xi}(\\theta_{m,t})\\!-\\!b_{\\xi}^{(m,t)}=\\nabla\\tilde{l}_{\\xi}(\\theta_{m,t})-$ $\\lambda\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})$ ; (36) holds due to $\\begin{array}{r}{\\langle a,b\\rangle\\leq\\frac{1}{4}\\parallel a\\parallel^{2}+\\parallel b\\parallel^{2}}\\end{array}$ ; (37) holds due to the inequality $\\parallel x+y\\parallel^{2}\\leq2\\parallel x\\parallel^{2}+2\\parallel y\\parallel^{2}$ ; (38) holds due to the expected smoothness assumption; (39) holds due to the condition $\\begin{array}{r}{\\gamma\\leq\\frac{1}{4\\mathscr{L}}\\frac{\\mu}{L}}\\end{array}$ ; (41) holds due to the law of total expectation with the random event $\\mathbf{1}(Y(X)={\\hat{Y}}(X;\\theta_{m}))$ and then applying the bounded support assumption. ", "page_idx": 17}, {"type": "text", "text": "By recursively applying the above bound to any realization of $\\theta_{m}$ , we get our desired result as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}[l(\\theta_{m,t})-l(\\theta^{*})|\\theta_{m}]\\leq(1-\\gamma\\mu)^{t}(l(\\theta_{m})-l(\\theta^{*}))+\\frac{8C^{2}}{\\mu}\\cdot g^{\\mathcal{E}}(\\theta_{m})}\\\\ {\\displaystyle\\qquad+\\,\\frac{1}{\\gamma\\mu}\\left(2\\gamma\\lambda^{2}\\parallel\\hat{g}(\\theta_{0:m},{\\bf w}_{0:m})\\parallel^{2}+L\\gamma^{2}\\mathbb{E}_{\\xi}\\left[\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})-\\lambda\\hat{g}_{\\xi}(\\theta_{0:m},{\\bf w}_{0:m})\\parallel^{2}\\right]\\right)}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=(1-\\gamma\\mu)^{t}(l(\\theta_{m})-l(\\theta^{*}))+\\frac{8C^{2}}{\\mu}g^{\\mathcal{E}}(\\theta_{m})}\\\\ {\\displaystyle\\qquad\\qquad+\\,\\frac{1}{\\mu}\\left(2\\lambda^{2}\\parallel\\hat{g}(\\theta_{0:m},{\\bf w}_{0:m})\\parallel^{2}+L\\gamma\\mathbb{E}_{\\xi}\\left[\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})-\\lambda\\hat{g}_{\\xi}(\\theta_{0:m},{\\bf w}_{0:m})\\parallel^{2}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "1 $\\begin{array}{r}{\\begin{array}{r l}{\\textbf{\\emph{e m m a}}\\textbf{\\emph{B6}}(\\mathrm{Lemma~}1\\quad\\mathrm{in}\\ \\ [30]).\\ L e t\\ \\ N(\\lambda;\\theta_{0:m},\\mathbf{w}_{0:m})}&{:=\\quad\\lambda^{2}\\underbrace{\\|}\\hat{\\textbf{\\i}}\\hat{\\theta}(\\theta_{0:m},\\mathbf{w}_{0:m})}\\\\ {\\frac{L\\gamma}{2}\\mathbb{E}_{\\xi}\\left[\\|\\nabla\\hat{l}_{\\xi}(\\theta^{*})-\\lambda\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\ \\|^{2}\\right].T h e n,f o r\\ \\lambda_{m}^{\\dagger}=\\frac{\\mathbb{E}_{\\xi}[\\langle\\nabla\\hat{l}_{\\xi}(\\theta^{*}),\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]}{\\mathbb{E}_{\\xi}[\\|\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|^{2}+\\frac{2}{L\\gamma}\\|\\hat{g}(\\theta_{0:m},\\mathbf{w}_{0:m})\\|]},}\\end{array}}\\end{array}$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\nabla(\\lambda_{m}^{\\dagger};\\theta_{0:m},\\mathbf{w}_{0:m})}{N(0)}=1-\\frac{\\left(\\mathbb{E}_{\\xi}[\\langle\\nabla\\tilde{l}_{\\xi}(\\theta^{*}),\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\rangle]\\right)^{2}}{\\left(\\mathbb{E}_{\\xi}\\parallel\\hat{g}_{\\xi}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}+\\frac{2}{L\\gamma}\\parallel\\hat{g}(\\theta_{0:m},\\mathbf{w}_{0:m})\\parallel^{2}\\right)\\left(\\mathbb{E}_{\\xi}\\parallel\\nabla\\tilde{l}_{\\xi}(\\theta^{*})\\parallel^{2}\\right)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C Additional results and discussions ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Empirical evidence for Theorem 3.1 ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "a17biETKyI/tmp/6d1f4bd21245239788a2904d426faf8d5f6383625d3950c178331e7132853473.jpg", "img_caption": ["Figure 4: (a) The accuracy of the generalized temporal ensemble along with the number of confident samples under different degrees of distribution shifts. Here, the temporal ensemble is constructed by averaging all predictions over iterations. (b) On-average accuracies per the number of confident samples over iterations under different thresholding rules. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.2 Marginal distribution of pseudo labels over epochs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.3 Algorithmic design choices ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Here, we examine effectiveness of our design choices\u2013the relative thresholding for weighting scheme $w_{i}(x)$ and hard prediction for $p(\\boldsymbol{y}|\\boldsymbol{x},\\boldsymbol{\\theta}_{i})$ . To this end, we replace our design choices with several alternatives, keeping other features of AnCon the same. Then, we analyze the changes in the performance compared to the default setting of AnCon in four domain pairs of OfficeHome. ", "page_idx": 18}, {"type": "text", "text": "Sophisticated weighting scheme for $w_{i}(x)$ We compare our relative thresholding with sophisticated weighting schemes called Entropy $(\\dot{w_{i}}(x)\\propto\\exp\\left\\{-H_{E}[f(x;\\theta_{i})]\\right\\})$ and Maxprob $(w_{i}\\bar{(}x)\\propto$ $\\mathrm{max}_{k\\in[K]}\\;f_{k}(x;\\theta_{i}))$ . These weighting schemes aim to more precisely weight the predictions based on the uncertainty of the prediction. However, in Figure 6a, the simple relative thresholding works better than these sophisticated weighting schemes. We conjecture that the poor calibration performance of the neural network in self-training under distribution shifts prevents the estimated uncertainty of a prediction from accurately reflecting the goodness of the prediction. Further, the ineffectiveness of these sophisticated weighting schemes persists even when we improve the calibration performance with last-layer Dirichlet [53] and Monte-Carlo Dropout [54] (see Appendix D.3 for detail). Therefore, our relative thresholding is simple yet effective in self-training under distribution shift. ", "page_idx": 18}, {"type": "image", "img_path": "a17biETKyI/tmp/9bf2670f762b829f772827a91834dc64fb823231f47c451803d1c7756521aba4.jpg", "img_caption": ["Figure 5: (a) Counting the number of pseudo labels for each class with 5,000 training samples in ImageNet-C over 100 training epochs, which shows that the marginal distribution of pseudo labels barely changes during training. (b) Changes in the total variation distance of the marginal distributions of the pseudo labels for each two consecutive epochs. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "a17biETKyI/tmp/9304329568d6f3528018821d596e68fab53ec06871948dff179c0e764aabb561.jpg", "img_caption": ["Figure 6: Ablation study of (a) weighting and (b) prediction schemes. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Soft prediction for $p(\\boldsymbol{y}|\\boldsymbol{x},\\boldsymbol{\\theta}_{i})$ We compare hard prediction vs soft prediction with a softmax temperature parameter $T\\in\\{0.5,0.75,1.0,1.25,1.5\\}$ (cf. Appendix D.3 for the definition). By design, soft prediction is more informative, giving values in non-leading entries. However, in Figure 6b, soft prediction turns out to underperform hard prediction for various values of $T$ . We conjecture that the continuously increasing confidence in the later stage of self-training would make soft prediction ignore early-stage predictions which may be valuable to memorize. Thus, we believe that hard prediction would be more appropriate for AnCon. ", "page_idx": 19}, {"type": "text", "text": "C.4 Limitations and future directions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Albeit AnCon proving its effectiveness with a fixed value of $\\lambda$ , Theorem 3.1 suggests that its optimal value depends on the temporal ensemble performance. Thus, future works could aim for adaptively determining $\\lambda$ . In addition, we observe that all self-training based methods fall short without sophisticated model designs tailored for distribution shifts [16, 31] (cf. Appendix E). In this regard, it would be valuable to rigorously understand properties of function classes that enable successful self-training under distribution shifts. Also, while AnCon significantly relaxes the conditions required to achieve optimality compared to LFN techniques, the on-average correct prediction condition still can be violated in challenging self-training scenarios. Given the compatibility of AnCon and NRC (cf. Table 1), efficiently combining local and temporal consistencies could be a step toward further relaxing the optimality condition. Finally, we remark that the idea of selective temporal consistency in sequential decision-making scenarios could be a notable future direction of research. Extending AnCon to sequential decision-making scenarios presents significant challenges as they involve the fundamentally different mechanisms. In sequential decision-making, leveraging observed rewards and balancing exploitation and exploration are core aspects (e.g., constructing the upper confidence bound of the reward in the bandit problem), unlike in self-training. Therefore, the main challenges for applying AnCon to this setting would be defining rewards and incorporating exploration strategies into pseudo label generation. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "D Additional details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 SFDA training configurations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our training configuration is based on [16]. Specifically, we train ResNet-50 [29] as a source classifier, which is used as an initial point for SFDA, for 50 epochs, with minibatch stochastic gradient descent with Nesterov momentum by using the initial learning rate of 0.01, the momentum parameter of 0.9, and the minibatch size of 64 in the Office Home dataset. The learning rate is decayed by $[1+10$ \u00b7 current iteration number / maximum number of iterations) $-0.75$ , and for the pre-trained layers we use a 10 times smaller learning rate. For Office-31, we change the number of epochs by 100 and other configurations are the same as those used in the setting in Office Home. Similarly, we only change the number of epochs to 10, the learning rate to 0.001, and the architecture to ResNet-101 for VisDa. We split dataset into $90\\%$ of the training set and $10\\%$ of the validation set, and the best model is chosen based on the lowest error rate on the validation set. For adapting the pre-trained model in the target domain, we use the exactly same configurations, except reducing the number of epochs to 30 for Office-31 and OfficeHome and to 15 for VisDa. ", "page_idx": 20}, {"type": "text", "text": "D.2 ImageNet-C training configuration ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For applying self-training, we follow the training configuration used in [31]; specifically, given ResNet-50 pretrained on ImageNet, we perform self-training without threshold by running the stochastic gradient descent optimizer with the learning rate of 0.001 and the minibatch size of 100 for 20 epochs. We remark that other techniques such as momentum and learning rate scheduling are not used. ", "page_idx": 20}, {"type": "text", "text": "D.3 Further experimental details in analyses ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Definition of ECE ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\nE C E(\\theta;\\mathcal{D})=\\sum_{g=1}^{G}\\frac{|\\mathcal{G}_{g}^{\\theta}|}{|\\mathcal{D}|}|A c c(\\mathcal{G}_{g}^{\\theta})-C o n f(\\mathcal{G}_{g}^{\\theta}))|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{G}_{g}^{\\theta}:=\\{x\\in\\mathcal{D}|\\frac{g}{G}\\leq c(x;\\theta)<\\frac{g+1}{G}\\}}\\end{array}$ , $A c c(\\mathcal{G}_{g}^{\\theta})$ is the average accuracy in $\\mathcal{G}_{g}^{\\theta}$ , and $C o n f(\\mathcal{G}_{g}^{\\theta})$ is the average confidence in $\\mathcal{G}_{g}^{\\theta}$ . ", "page_idx": 20}, {"type": "text", "text": "Configuration of the sophisticated uncertainty representation methods We used the regularization coefficient of 0.001 for the belief matching loss [53] and 20 forward passes to compute the posterior predictive distribution for MC-dropout with dropout probabilities 0.2 and 0.5 [54]. ", "page_idx": 20}, {"type": "text", "text": "Definitions of model selection criteria ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 InfoMax (higher is better) ", "page_idx": 20}, {"type": "equation", "text": "$$\nI M(\\theta)=H(\\mathbb{E}_{X}[f(X;\\theta)])-\\mathbb{E}_{X}[H(f(X;\\theta))]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{H(p)\\~=~-\\sum_{k\\in[K]}p_{i}\\log p_{i}}\\end{array}$ for $p~\\in~\\triangle^{K-1}$ . Note that we abuse the notation $H(\\cdot)$ with the definition of the cross-entropy; that is, $\\begin{array}{r}{H(q,p)\\,=\\,-\\sum_{k\\in[K]}p_{i}\\log q_{i}}\\end{array}$ and $\\begin{array}{r}{H(p)=-\\sum_{k\\in[K]}p_{i}\\log p_{i}}\\end{array}$ . ", "page_idx": 20}, {"type": "table", "img_path": "a17biETKyI/tmp/2967e7562a62808ccdba2815b9ae644c27b376135e6dbb867286f9543a868010.jpg", "table_caption": ["Table 2: Accuracy across different domain pairs for each UDA method in OfficeHome. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "\u2022 Corr-C (lower is better) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{Corr.C}(\\theta)=C/(\\parallel C\\parallel_{F}/\\sqrt{K}),\\quad C_{i j}=\\sum_{n=1}^{b}f_{i}(x_{n};\\theta)f_{j}(x_{n};\\theta)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $C\\in\\mathbb{R}^{K\\times K}$ and $\\Vert\\cdot\\Vert_{F}$ is the Frobenius norm. Ent (lower is better) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{Ent}(\\theta)=H(\\mathbb{E}_{X}[f(X;\\theta)]).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Softmax with a temperature parameter", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\np(y=j\\vert x,\\theta,T)=\\phi_{j}(f^{-\\phi}(x;\\theta)/T)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where f \u2212\u03d5(x; \u03b8) are the logits of the neural network such that fi(x; \u03b8) =  k\u2208e[xKp] (ef xi\u2212p\u03d5((f xk\u2212;\u03b8\u03d5)()x;\u03b8)) for $i\\in[K]$ and $\\phi(x)$ is the softmax function. ", "page_idx": 21}, {"type": "text", "text": "D.4 Computational resources for experiments ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this work, we use multiple servers which consist of multiple GPUs including TITAN XP (12GB), RTX 8000 (50GB), and A100 (40GB). AnCon takes 1.16 seconds on average for each iteration. ", "page_idx": 21}, {"type": "text", "text": "E Result in UDA ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "While SFDA does not assume any information, unlabeled target domain samples as domain shift information can be available during the source domain training process in some practical scenarios, e.g., when labeling target domain samples is costly. In this case, UDA methods are used to minimize the potential impact of distribution shifts by matching input marginal probability distributions of source and target domains. Given the prevalence of UDA scenarios in practice, we aim to show whether AnCon and other self-training methods can be effective for the UDA methods (conditional domain adversarial network (CDAN) [55] and maximum mean discrepancy (MDD) [56]) in the adaptation stage. However, as shown in Table 2, all methods decrease the performance of both CDAN and MDD for most cases, albeit the reduction rates in AnCon are smaller than self-training and ELR. We conjecture that this is because the base methods (CDAN and MDD) do not contain the sophisticated tricks, such as adding weight normalization to the final linear layer, which are used in the SFDA literature [16]. ", "page_idx": 21}, {"type": "text", "text": "F Additional tables ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 3: Benchmark results in Office-31. The numbers indicate the mean test accuracy across three repetitions with boldface for the best score. ", "page_idx": 22}, {"type": "table", "img_path": "a17biETKyI/tmp/01537953025f4495df6ad04d3917510d23104887df5e630b89047553bec52341.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "a17biETKyI/tmp/4529c17dae3a9aafa6be4ddc2227598ad4c1251282d9e96a4482bcbbd4662679.jpg", "table_caption": ["Table 4: Benchmark results in OfficeHome. The numbers indicate the mean test accuracy across three repetitions with boldface for the best score. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "a17biETKyI/tmp/ea3ba90c513b12ef5ee04d917a2d489412f6c72613c27c1cb1ac8d0ce36cd3a7.jpg", "table_caption": ["Table 5: ECE benchmark results in OfficeHome. The numbers indicate the mean test ECE across three repetitions with boldface for the best score (lower is better). "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "a17biETKyI/tmp/1831ca7320c160f6409afb5dcda5bd4bb59152b8a46d44d55a70c570cf84798a.jpg", "table_caption": ["Table 6: Benchmark results in VisDa. The numbers indicate the mean test accuracy across three repetitions with boldface for the best score. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 7: ImageNet-C experiments with corruption intensity 1. The numbers indicate the mean test accuracy across three repetitions with boldface for the best score. We only present the first five letter for each corruption type. The full names are available in [6]. ", "page_idx": 23}, {"type": "table", "img_path": "a17biETKyI/tmp/e8c57698382efee85d8e9abf3de98f75e2e18b788682a0ac5c1b7fdf71316d00.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "a17biETKyI/tmp/7e109e0a21ee4aac69cb652b4b7f508e2b1b4ac5479ea3b8dc5fe0cba7e41625.jpg", "table_caption": ["Table 8: ImageNet-C experiments with corruption intensity 2. The numbers indicate the mean test accuracy across three repetitions with boldface for the best score. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "a17biETKyI/tmp/f9da3495ae0eceb04b8f0e6c4a8eb6ebf1fcdb9bd13bbbaf53439089f7d2bdf3.jpg", "table_caption": ["Table 9: ImageNet-C experiments with corruption intensity 3. The numbers indicate the mean test accuracy across three repetitions with boldface for the best score. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "a17biETKyI/tmp/af5162c3d8f57020dec9a6cbf0e111a4dc2b494d157e6c2e5ce947c5e0378809.jpg", "table_caption": ["Table 10: ImageNet-C experiments with corruption intensity 4. The numbers indicate the mean test accuracy across three repetitions with boldface for the best score. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "a17biETKyI/tmp/eabeba39c8fbdf5d80ea35784abaa3835568e01c4b0e19359b03d8cbfd638ed7.jpg", "table_caption": ["Table 11: ImageNet-C experiments with corruption intensity 5. The numbers indicate the mean test accuracy across three repetitions with boldface for the best score. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: In Section 1, we clearly state this work\u2019s scope and contributions, which are shown in Theorem 3.1, Theorem 3.2, and extensive experiments in Section 4. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We include \"Limitations and future directions\" in Section 6 and discuss implications of the assumptions for theoretical results. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Justification: We have added all assumptions in the statements of Theorems 3.1 and 3.2 with proofs in Appendix. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We include training configurations of all experiments in Appendix D ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] Justification: We release our code at https://github.com/tjoo512/ancon. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Justification: We include training configurations of all experiments in Appendix D. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have included error bars with box plots for experiments that have high variance across different random seeds. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide computational resources used in the research in the Appendix. ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We thoroughly read the NeurIPS Code of Ethics and have confirmed that our research does not violate any of them. ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative   \nsocietal impacts of the work performed?   \nAnswer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: This work considers general principle of promoting temporal consistency for improving self-training under distribution shifts. Therefore, it would not have direct societal impacts, which are contributed particularly by our work. ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?   \nAnswer: [NA]   \nJustification: Our experiments concern only standard models with publicly available benchmarks. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?   \nAnswer: [Yes]   \nJustification: We cite papers related to code, data, and models used in our experiments. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?   \nAnswer: [NA]   \nJustification: This paper does not include the new assets. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA]   \nJustification: This paper does not include crowdsourcing or research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] Justification: This paper does not include crowdsourcing or research with human subjects. ", "page_idx": 25}]