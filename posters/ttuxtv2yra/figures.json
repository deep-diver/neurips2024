[{"figure_path": "ttUXtV2YrA/figures/figures_1_1.jpg", "caption": "Figure 1: Existing integration schemes, e.g., ACMix [40], apply MHSAs and Convs at the same granularity (top). In contrast, we affirm that by offloading the burden of extracting fine-grained features to lightweight Convs, MHSAs can be aggressively applied to coarse semantic slots to make spatial mixing more efficient (bottom).", "description": "This figure compares two different approaches for integrating convolutional and multi-head self-attention mechanisms in vision backbones. The top part illustrates the existing approach (ACMix), where both operations are applied at the same fine-grained level.  The bottom part shows the proposed GLMix approach, which uses MHSAs at a coarser granularity (on semantic slots) and Convs at a finer granularity (on a regular grid), leading to improved efficiency.", "section": "1 Introduction"}, {"figure_path": "ttUXtV2YrA/figures/figures_1_2.jpg", "caption": "Figure 2: IN1k top-1 accuracy vs. FLOPS. While several recent state-of-the-art models (i.e., MaxViT [47], CSWin [14] and SG-Former [43], SMT [34]) lie in almost the same Pareto frontier, our GLNet models move the frontier further to the upper-right with a clear margin.", "description": "This figure shows a comparison of the trade-off between top-1 accuracy on ImageNet-1k and FLOPS (floating point operations per second) for several state-of-the-art models, including the proposed GLNet models.  It demonstrates that GLNet achieves higher accuracy at similar or better throughput compared to other models, representing an improvement in efficiency.", "section": "1 Introduction"}, {"figure_path": "ttUXtV2YrA/figures/figures_3_1.jpg", "caption": "Figure 4: Structure of our GLMix block. At the core is a pair of conjugated soft clustering and dispatching modules to bridge the set and grid representations and enable local-global fusion.", "description": "This figure illustrates the architecture of the GLMix block, a core component of the proposed GLNet model.  The GLMix block integrates convolutional and multi-head self-attention operations at different granularities.  It takes as input both a fine-grained grid representation (from convolutions) and a coarse-grained set representation (semantic slots). A pair of fully differentiable soft clustering and dispatching modules are used to bridge these representations, enabling the fusion of local and global features. The clustering module groups the fine-grained features into semantic slots, while the dispatching module redistributes the processed features from the slots back to the grid.  This architecture aims to combine the strengths of both local (convolution) and global (self-attention) feature processing, achieving efficient local-global feature fusion.", "section": "3 Methodology"}, {"figure_path": "ttUXtV2YrA/figures/figures_3_2.jpg", "caption": "Figure 3: The semantic slots correspond to \"soft\" irregular semantic regions (left). Compared to using hard-divided regular patches (as adopted by plain ViTs [15, 46]) on the right, our formulation is closer to tokenization in NLP.", "description": "This figure illustrates the difference between the proposed method's representation of an image and the traditional approach used in Vision Transformers. The left side shows how the proposed method represents an image using soft irregular semantic regions as semantic slots.  The right side displays how traditional Vision Transformers divide an image into hard-divided regular patches. The caption highlights that the proposed method's approach is more similar to tokenization used in natural language processing (NLP).", "section": "3.1 Convs and MHSAs at Different Granularities"}, {"figure_path": "ttUXtV2YrA/figures/figures_8_1.jpg", "caption": "Figure 5: Visualization of semantic slots. For each sample, we show the input image (left), assignment maps of all semantic slots (middle), and four representative slots (right). We use the k-medoids algorithm to select the four representative slots automatically.", "description": "This figure visualizes how the semantic slots, which are used in the GLMix block, correspond to regions of the input image.  The left column shows the original input images. The middle column displays the assignment maps, showing how each pixel in the input image is assigned to one of the 64 semantic slots.  The right column shows four representative slots selected by a k-medoids algorithm, providing a visual representation of the learned semantic features.", "section": "4.4 Visualization of Semantic Slots"}, {"figure_path": "ttUXtV2YrA/figures/figures_16_1.jpg", "caption": "Figure 6: Visualization of semantic slots of blocks at different depths. The setting is the same as in Figure 5, except that we add the visualizations for a shallower block (columns 2-3) and a deeper block (columns 6-7).", "description": "This figure visualizes the semantic slots at different depths (2nd, 5th, and 10th blocks) of the GLMix integration scheme. Each row shows an input image and the corresponding semantic slots for each block.  The visualization uses color-coding to represent the clustering weights, highlighting the regions that each semantic slot focuses on. By comparing across different depths, we observe the shift in the semantic grouping from color-based patterns (shallow block) to object-level groupings (deeper block).", "section": "4.4 Visualization of Semantic Slots"}, {"figure_path": "ttUXtV2YrA/figures/figures_17_1.jpg", "caption": "Figure 5: Visualization of semantic slots. For each sample, we show the input image (left), assignment maps of all semantic slots (middle), and four representative slots (right). We use the k-medoids algorithm to select the four representative slots automatically.", "description": "This figure visualizes the semantic slots generated by the GLMix module. Each row represents a different image. The leftmost column displays the input image. The central columns show heatmaps representing the assignment of each pixel to the semantic slots. Finally, the rightmost columns display the four most representative semantic slots for each image, selected automatically using the k-medoids algorithm.  This visualization demonstrates the semantic grouping effect achieved by the soft clustering module, even without dense supervision.", "section": "4.4 Visualization of Semantic Slots"}, {"figure_path": "ttUXtV2YrA/figures/figures_17_2.jpg", "caption": "Figure 5: Visualization of semantic slots. For each sample, we show the input image (left), assignment maps of all semantic slots (middle), and four representative slots (right). We use the k-medoids algorithm to select the four representative slots automatically.", "description": "This figure visualizes the semantic slots learned by the model.  Each row shows an example image, followed by a grid of activation maps representing the different semantic slots. The selected semantic slots are visualized on the right.  This helps understand how the model groups image regions into meaningful semantic concepts.", "section": "4.4 Visualization of Semantic Slots"}]