{"references": [{"fullname_first_author": "Angelica Chen", "paper_title": "Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs", "publication_date": "2024", "reason": "This paper is highly relevant because it demonstrates a similar phenomenon of sudden learning in masked language models, providing a crucial comparison for the study's findings on matrix completion."}, {"fullname_first_author": "Emmanuel J. Cand\u00e8s", "paper_title": "Exact matrix completion via convex optimization", "publication_date": "2009", "reason": "This paper is foundational to the matrix completion problem, providing the theoretical underpinnings and convex optimization approach used as a baseline comparison in the current study."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-00", "reason": "BERT is the core model used in this research, its architecture and pre-training method forming the foundation of the experimental design and analysis."}, {"fullname_first_author": "Guillaume Alain", "paper_title": "Understanding intermediate layers using linear classifier probes", "publication_date": "2017", "reason": "This paper introduces the linear probe methodology used to analyze the model's hidden layers, crucial for interpretability and understanding the model's internal representations."}, {"fullname_first_author": "Steven Diamond", "paper_title": "CVXPY: A Python-embedded modeling language for convex optimization", "publication_date": "2016", "reason": "This paper describes CVXPY, the software used for solving the nuclear norm minimization problem that is compared against the BERT model's performance, serving as a key tool in the experimental comparisons."}]}