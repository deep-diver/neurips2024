{"references": [{"fullname_first_author": "Martin Abadi", "paper_title": "TensorFlow: A system for large-scale machine learning", "publication_date": "2016-00-00", "reason": "This paper introduces TensorFlow, a crucial deep learning framework used extensively in the experiments."}, {"fullname_first_author": "Alekh Agarwal", "paper_title": "Distributed delayed stochastic optimization", "publication_date": "2011-00-00", "reason": "This foundational work provides theoretical groundwork for understanding asynchronous SGD algorithms with delays, which are central to the paper's core contributions."}, {"fullname_first_author": "Yossi Arjevani", "paper_title": "A tight convergence analysis for stochastic gradient descent with delayed updates", "publication_date": "2020-00-00", "reason": "This paper offers a rigorous convergence analysis of SGD with delayed updates, directly relevant to the theoretical analysis of the proposed OrMo algorithm."}, {"fullname_first_author": "Jeffrey Dean", "paper_title": "Large scale distributed deep networks", "publication_date": "2012-00-00", "reason": "This seminal paper describes large-scale distributed deep learning, the context in which asynchronous methods, including the proposed OrMo, are most relevant."}, {"fullname_first_author": "Aaron Defazio", "paper_title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "publication_date": "2014-00-00", "reason": "This paper introduces SAGA, a significant optimization algorithm.  The comparison with OrMo in the experiments highlights its relevance as a state-of-the-art method."}]}