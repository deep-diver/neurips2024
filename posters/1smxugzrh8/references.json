{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "CLIP, a model for evaluating image-text similarity, is a core component of the experimental setup."}, {"fullname_first_author": "Huiwen Chang", "paper_title": "Muse: Text-to-image generation via masked generative transformers", "publication_date": "2023-01-01", "reason": "The core text-to-image generation model used in the paper is Muse."}, {"fullname_first_author": "Kihyuk Sohn", "paper_title": "Styledrop: Text-to-image generation in any style", "publication_date": "2023-06-01", "reason": "FineStyle builds upon StyleDrop, a prior method for style-personalized text-to-image generation."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Classifier-free diffusion guidance", "publication_date": "2022-07-01", "reason": "Classifier-free guidance is used to improve the quality of generated images."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-06-01", "reason": "High-resolution image synthesis is achieved through techniques in latent diffusion models."}]}