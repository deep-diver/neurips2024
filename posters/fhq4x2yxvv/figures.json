[{"figure_path": "fHq4x2YXVv/figures/figures_1_1.jpg", "caption": "Figure 1: The pipeline diagram of AlphaPruning. Our post-training layer-wise pruning method involves the following steps: (i) Performing ESD analysis on all weight matrices of a base LLM and (ii) employing PL fitting to derive the layer-wise metric values (that measures the HT exponent). Then, (iii) using the layer-wise metric values, we assign layer-wise pruning ratios to each layer through a linear assignment function.", "description": "This figure illustrates the AlphaPruning pipeline, a post-training layer-wise pruning method for LLMs.  It shows three main stages: 1) ESD analysis of the weight matrices in each layer of the LLM to assess their heavy-tailed properties (measured by the HT exponent); 2) PL fitting to quantify the heavy-tailedness of each layer's ESD; 3) A linear mapping function that transforms the layer-wise quality metrics (derived from PL fitting) into layer-wise pruning ratios, which are then applied to prune the LLM.", "section": "3 Alpha-Pruning"}, {"figure_path": "fHq4x2YXVv/figures/figures_6_1.jpg", "caption": "Figure 2: The WikiText validation perplexity for LLaMA-7B (left) and LLaMA-13B (right) pruned with different sparsities using Wanda.", "description": "This figure compares the performance of LLaMA-7B and LLaMA-13B models pruned using the Wanda method with different sparsity levels.  The x-axis represents the sparsity percentage (how many parameters were removed), and the y-axis shows the WikiText validation perplexity, a measure of how well the model predicts the next word in a sequence. Lower perplexity indicates better performance.  The figure shows that AlphaPruning consistently outperforms the uniform sparsity approach across different sparsity levels for both models.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/figures/figures_8_1.jpg", "caption": "Figure 4: ImageNet-1K accuracy (\u2191) of the sparse ConvNext model pruned to various sparsity levels by AlphaPruning and other baseline methods, without fine-tuning.", "description": "This figure compares the performance of AlphaPruning against other baseline methods in pruning a ConvNext model on the ImageNet-1K dataset. The x-axis represents the sparsity level (percentage of weights removed), and the y-axis shows the ImageNet-1K accuracy.  The results demonstrate the superior performance of AlphaPruning in maintaining accuracy even at high sparsity levels, outperforming uniform sparsity and OWL methods.", "section": "4.4 Analyzing LLM pruning via HT-SR perspective"}, {"figure_path": "fHq4x2YXVv/figures/figures_9_1.jpg", "caption": "Figure 5: Analyzing the heavy-tail metric PL_Alpha_Hill (lower the better by HT-SR theory) and performance metric WikiText validation perplexity (lower the better) before and after pruning by baseline uniform pruning and AlphaPruning. (a) The metric value is reported by averaging over all layers within each model. The dashed lines represent the perplexity and the histograms represent the PL_Alpha_Hill value. (b) The metric is reported by averaging all the matrices within each LLM layer.", "description": "This figure shows the results of analyzing the heavy-tailed self-regularization (HT-SR) properties of LLMs before and after pruning using both uniform and AlphaPruning methods.  Subfigure (a) compares the average PL_Alpha_Hill metric (a measure of model quality) and perplexity across different LLM sizes (7B, 13B, 30B parameters).  It demonstrates that AlphaPruning maintains better model quality (lower PL_Alpha_Hill) compared to uniform pruning, resulting in lower perplexity. Subfigure (b) provides a layer-wise analysis of the PL_Alpha_Hill metric for LLaMA-7B and LLaMA-13B models, showing AlphaPruning's more targeted pruning approach.", "section": "4.4 Analyzing LLM pruning via HT-SR perspective"}, {"figure_path": "fHq4x2YXVv/figures/figures_16_1.jpg", "caption": "Figure 5: Analyzing the heavy-tail metric PL_Alpha_Hill (lower the better by HT-SR theory) and performance metric WikiText validation perplexity (lower the better) before and after pruning by baseline uniform pruning and AlphaPruning. (a) The metric value is reported by averaging over all layers within each model. The dashed lines represent the perplexity and the histograms represent the PL_Alpha_Hill value. (b) The metric is reported by averaging all the matrices within each LLM layer.", "description": "This figure shows a comparison of the heavy-tailed metric PL_Alpha_Hill and the WikiText validation perplexity before and after applying uniform pruning and AlphaPruning.  Subfigure (a) provides a model-wise comparison, averaging the PL_Alpha_Hill across all layers for each model. The dashed line shows the perplexity, while the histogram displays the PL_Alpha_Hill distribution. Subfigure (b) offers a layer-wise comparison, with the PL_Alpha_Hill averaged across all matrices within each LLM layer.  The lower the PL_Alpha_Hill, the better the model quality according to HT-SR theory.", "section": "4.4 Analyzing LLM pruning via HT-SR perspective"}, {"figure_path": "fHq4x2YXVv/figures/figures_17_1.jpg", "caption": "Figure 7: Comparing layer-wise sparsity distributions allocated by AlphaPruning (blue, ours) and OWL (orange). While both methods show similar overall trends, AlphaPruning generates a more granular distribution with distinct differences between consecutive layers.", "description": "This figure compares the layer-wise sparsity distributions obtained from AlphaPruning and OWL methods.  Both methods show a general trend of lower sparsity in earlier layers and higher sparsity in later layers, which is intuitive given the typical importance of early layers in LLMs. However, AlphaPruning shows a more fine-grained distribution with more distinct differences in sparsity between adjacent layers compared to OWL.", "section": "4.4 Analyzing LLM pruning via HT-SR perspective"}, {"figure_path": "fHq4x2YXVv/figures/figures_17_2.jpg", "caption": "Figure 5: Analyzing the heavy-tail metric PL_Alpha_Hill (lower the better by HT-SR theory) and performance metric WikiText validation perplexity (lower the better) before and after pruning by baseline uniform pruning and AlphaPruning. (a) The metric value is reported by averaging over all layers within each model. The dashed lines represent the perplexity and the histograms represent the PL_Alpha_Hill value. (b) The metric is reported by averaging all the matrices within each LLM layer.", "description": "This figure compares the heavy-tailed metric PL_Alpha_Hill and the WikiText validation perplexity before and after pruning using uniform pruning and AlphaPruning methods.  Panel (a) shows model-wide comparison by averaging PL_Alpha_Hill across all layers, demonstrating that AlphaPruning maintains better model quality than uniform pruning, as reflected by lower perplexity and PL_Alpha_Hill values. Panel (b) provides a layer-wise analysis showing that AlphaPruning preserves the quality of model layers more effectively.", "section": "4.4 Analyzing LLM pruning via HT-SR perspective"}, {"figure_path": "fHq4x2YXVv/figures/figures_18_1.jpg", "caption": "Figure 9: Analyzing ESD properties and assignment strategies for LRA. (a) Stable_Rank and PL_Alpha_Hill show a similar pattern (the more heavy-tailed, the more low-ranked) across different ESDs sampled from Pareto distribution. (b) The layer-wise PL_Alpha_Hill and Stable_Rank of the LLaMA-7B model exhibit a similar trend. (c) Comparing two assignment strategies for LRA, and \u201cCompress more on HTed layers\u201d is better. This finding is opposite to pruning-based methods, which find that \u201cCompress less on HTed layers\u201d is better.", "description": "This figure compares the heavy-tailed (HT) properties and low-rank properties of weight matrices in LLMs.  It shows a strong positive correlation between these properties: more heavy-tailed matrices are also more low-ranked.  The figure further explores layer-wise patterns in these properties within the LLaMA-7B model, demonstrating a similarity between heavy-tailed and low-rank structures across layers. Finally, the figure contrasts two LRA assignment strategies, revealing that prioritizing compression on heavier-tailed layers yields superior results, contrasting with the findings of pruning-based methods.", "section": "E Further analysis of results"}, {"figure_path": "fHq4x2YXVv/figures/figures_19_1.jpg", "caption": "Figure 10: Comparison of pruning models with varying model-wise HT measures (Alpha). Models with higher HT measures are more prunable using both Uniform pruning and AlphaPruning. Relative accuracy is calculated as the post-pruning accuracy divided by the pre-pruning accuracy. The experiments used FCNs trained on CIFAR-10.", "description": "This figure shows the results of an experiment comparing the performance of two pruning methods (uniform pruning and AlphaPruning) on fully connected neural networks (FCNs) trained on the CIFAR-10 dataset.  The x-axis represents the pruning ratio, and the y-axis shows the relative test accuracy (post-pruning accuracy / pre-pruning accuracy).  Different colored lines represent FCNs with varying heavy-tailed (HT) measures (Alpha), a metric used to characterize the heavy-tailedness of the weight matrix eigenspectrum, which is related to the model's training quality and generalization capabilities. The results demonstrate that models with higher HT measures are generally more prunable, meaning that a larger fraction of their parameters can be removed without significant performance degradation, using both uniform and AlphaPruning methods.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/figures/figures_19_2.jpg", "caption": "Figure 10: Comparison of pruning models with varying model-wise HT measures (Alpha). Models with higher HT measures are more prunable using both Uniform pruning and AlphaPruning. Relative accuracy is calculated as the post-pruning accuracy divided by the pre-pruning accuracy. The experiments used FCNs trained on CIFAR-10.", "description": "This figure shows the result of comparing two pruning strategies (uniform pruning and AlphaPruning) on fully connected networks (FCNs) trained with different model-wise heavy-tailed (HT) measures (Alpha).  The x-axis represents the pruning ratio and the y-axis represents the relative test accuracy, calculated by dividing the post-pruning test accuracy by the pre-pruning test accuracy.  The results demonstrate that models with higher HT measures are more easily pruned using both uniform and AlphaPruning methods. The experiments were conducted using FCNs and the CIFAR-10 dataset.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/figures/figures_20_1.jpg", "caption": "Figure 7: Comparing layer-wise sparsity distributions allocated by AlphaPruning (blue, ours) and OWL (orange). While both methods show similar overall trends, AlphaPruning generates a more granular distribution with distinct differences between consecutive layers.", "description": "This figure compares the layer-wise sparsity distributions produced by AlphaPruning and OWL.  Both methods show a general trend of lower sparsity in earlier layers and higher sparsity in later layers, but AlphaPruning demonstrates a more nuanced distribution. AlphaPruning's distribution is more granular, with more noticeable differences in sparsity between consecutive layers, suggesting a more refined allocation of sparsity based on the model's structure and training characteristics.", "section": "4.4 Analyzing LLM pruning via HT-SR perspective"}, {"figure_path": "fHq4x2YXVv/figures/figures_21_1.jpg", "caption": "Figure 5: Analyzing the heavy-tail metric PL_Alpha_Hill (lower the better by HT-SR theory) and performance metric WikiText validation perplexity (lower the better) before and after pruning by baseline uniform pruning and AlphaPruning. (a) The metric value is reported by averaging over all layers within each model. The dashed lines represent the perplexity and the histograms represent the PL_Alpha_Hill value. (b) The metric is reported by averaging all the matrices within each LLM layer.", "description": "This figure analyzes the heavy-tailed self-regularization (HT-SR) metric PL_Alpha_Hill and its correlation with the WikiText validation perplexity before and after pruning using uniform pruning and AlphaPruning. It shows that AlphaPruning not only outperforms uniform pruning in terms of perplexity but also leads to a more favorable distribution (lower mean) of PL_Alpha_Hill among the layers, minimizing the damage caused by pruning.", "section": "4.4 Analyzing LLM pruning via HT-SR perspective"}, {"figure_path": "fHq4x2YXVv/figures/figures_21_2.jpg", "caption": "Figure 1: The pipeline diagram of AlphaPruning. Our post-training layer-wise pruning method involves the following steps: (i) Performing ESD analysis on all weight matrices of a base LLM and (ii) employing PL fitting to derive the layer-wise metric values (that measures the HT exponent). Then, (iii) using the layer-wise metric values, we assign layer-wise pruning ratios to each layer through a linear assignment function.", "description": "This figure shows the pipeline of AlphaPruning, a post-training layer-wise pruning method.  It consists of three main steps: 1) analyzing the empirical spectral densities (ESDs) of all weight matrices in the base large language model (LLM), 2) fitting power laws (PL) to the ESDs to obtain layer-wise metric values (representing the heavy-tailed exponent), and 3) using these metric values to linearly map the layer-wise pruning ratios.  The diagram visually depicts the flow of information from the weight matrices through ESD analysis and PL fitting to the final pruning ratios allocated to each layer.", "section": "3 Alpha-Pruning"}, {"figure_path": "fHq4x2YXVv/figures/figures_24_1.jpg", "caption": "Figure 1: The pipeline diagram of AlphaPruning. Our post-training layer-wise pruning method involves the following steps: (i) Performing ESD analysis on all weight matrices of a base LLM and (ii) employing PL fitting to derive the layer-wise metric values (that measures the HT exponent). Then, (iii) using the layer-wise metric values, we assign layer-wise pruning ratios to each layer through a linear assignment function.", "description": "This figure illustrates the pipeline of AlphaPruning, a post-training layer-wise pruning method.  It starts with analyzing the empirical spectral densities (ESDs) of the weight matrices of a large language model (LLM). Then, power-law (PL) fitting is used to extract metric values that represent the heavy-tailed property of each layer.  Finally, a linear mapping function applies these metric values to determine layer-wise pruning ratios, which means each layer of the LLM is pruned with a different ratio.", "section": "3 Alpha-Pruning"}, {"figure_path": "fHq4x2YXVv/figures/figures_26_1.jpg", "caption": "Figure 16: Using AlphaPruning to determine layerwise sparsity for OSSCAR. The x-axis pruning ratio represents the fraction of pruned parameters relative to the total parameters in the linear sublayer of multi-head attention and the second sublayer of the feed-forward network, any other type of sublayers are not included. The model used is OPT-6.7B, and perplexity (\u2193) is evaluated on WikiText.", "description": "This figure compares the performance of OSSCAR with and without AlphaPruning.  OSSCAR is a structured pruning method that prunes parameters within specific layers (linear sublayer of multi-head attention and second sublayer of the feed-forward network). AlphaPruning allocates sparsity non-uniformly across layers, while OSSCAR uses uniform pruning. The x-axis represents the pruning ratio (fraction of pruned parameters relative to the total number in the specified layers), and the y-axis is the perplexity of the OPT-6.7B model evaluated on WikiText.  Lower perplexity indicates better performance. The graph shows that AlphaPruning combined with OSSCAR consistently achieves lower perplexity than OSSCAR alone across different pruning ratios, demonstrating its effectiveness in improving the performance of this structured pruning method.", "section": "More results on semi-structured and structured pruning"}]