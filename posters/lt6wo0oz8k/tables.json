[{"figure_path": "Lt6wO0oZ8k/tables/tables_15_1.jpg", "caption": "Table 1: Hyperparameters", "description": "This table lists the hyperparameters used in the experiments for the three different reinforcement learning algorithms (D3QN, IQL, PPO) and the opponent model.  It breaks down parameters such as hidden units, activation functions, optimizers, learning rates, and other key settings, allowing for reproducibility of the results.", "section": "5 Experiments"}, {"figure_path": "Lt6wO0oZ8k/tables/tables_15_2.jpg", "caption": "Table 2: Test performance on 8m", "description": "This table shows the win rates of QMIX and OMG-optim in the 8m map of the SMAC environment when facing different types of opponents: 7 non-homologue, 6 homologue, and 7 homologue.  The results demonstrate that OMG-optim significantly outperforms QMIX, especially against non-homologous opponents (those trained with different algorithms).", "section": "5.3 Generalization to Unknown Opponents"}, {"figure_path": "Lt6wO0oZ8k/tables/tables_16_1.jpg", "caption": "Table 3: The proportion of each s<sub>t+k</sub> in N<sup>H</sup>.", "description": "This table shows the frequency distribution of subgoals selected from the set of future states N<sup>H</sup> = {s<sub>t+k</sub>|1 \u2264 k \u2264 H}.  The proportion indicates how frequently each state (s<sub>t+1</sub>, s<sub>t+2</sub>, s<sub>t+3</sub>, s<sub>t+4</sub>, s<sub>t+5</sub>) was chosen as a subgoal across 100 trajectories.  The data suggests a preference for selecting subgoals within the first few steps of the future trajectory.", "section": "D s<sub>g</sub> selection frequency"}]