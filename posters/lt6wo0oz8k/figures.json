[{"figure_path": "Lt6wO0oZ8k/figures/figures_0_1.jpg", "caption": "Figure 1: Infer the goal of others", "description": "This figure illustrates how humans can infer the goals of others by observing their actions over several steps.  It depicts multiple agents, possibly in a competitive scenario, moving towards different goals. By observing the trajectories of other agents, an agent can infer their likely goals, improving its decision-making in multi-agent environments. This concept is central to the proposed Opponent Modeling based on Subgoal Inference (OMG) method.", "section": "1 Introduction"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_2_1.jpg", "caption": "Figure 2: Diagram of OMG. During the interaction phase, OMG infers the subgoal from the historical trajectory. The CVAE & acts as an inference model, deducing the opponent subgoal denoted as \u011d. The inferred subgoal serves as input for the policy model Q(s, \u011d, a). In the update phase, OMG examines the entire trajectory in a hindsight manner to select subgoals \u011f as priors for training the inference model. The subgoal selector employs a value-based heuristic to choose a state from the next few steps and then encodes it into a subgoal using the pre-trained VAE \u03c8.", "description": "This figure illustrates the architecture of the Opponent Modeling based on subgoals inference (OMG) model.  It shows two main phases: the interaction phase where the model infers the opponent's subgoal using a conditional variational autoencoder (CVAE) and the update phase where the model refines its subgoal inference based on the entire trajectory and a value-based subgoal selection heuristic. The interaction phase uses the inferred subgoal to inform the agent's policy, while the update phase uses a selected subgoal to improve the inference model.", "section": "4 Method"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_4_1.jpg", "caption": "Figure 3: Learned Q-values using tabular Q-learning in an 11 \u00d7 11 gridworld. The agent and the opponent start from the S\u2081 and S2, respectively. The two rewarding grids are D\u2081 and D2, and the reward will only be given to the agent who arrives first. The opponent executes one of policies \u03c0\u2081 and \u03c0\u2082, which target D\u2081 and D\u2082, respectively. The g and a\u00af\u00b2 are obtained from an oracle.", "description": "This figure shows the learned Q-values for an agent using tabular Q-learning in a gridworld scenario.  Two scenarios are shown, one after 1e5 training steps and the other after 1e7 training steps. The goal is for the agent to reach either destination D1 or D2 before the opponent, who has a fixed policy targeting either D1 or D2. The heatmaps illustrate the Q-values (expected rewards) at different states (s), comparing using opponent's actions (a\u00afi) vs opponent's subgoal (g) in policy learning. The improvement in Q-value distribution as training progresses is evident, demonstrating that subgoal inference enhances learning compared to using opponent actions directly.", "section": "4.1 Policy Learning with Opponent's Subgoals"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_6_1.jpg", "caption": "Figure 4: (a) Performance in Foraging. The red bar shows the total score obtained by the agent. The blue bar illustrates the number of steps in each episode. The results show that OMG can converge to the same score as the baselines but end the episode in fewer steps because it predicts the opponent\u2019s goal. (b) Performance in Predator-Prey. The results show the score obtained by the agent as a predator with two other uncontrolled predators, and OMG outperforms the baselines.", "description": "This figure shows the performance comparison of different methods in two environments: Foraging and Predator-Prey.  In (a) Foraging, OMG achieves similar scores to the baselines but requires fewer steps per episode. In (b) Predator-Prey, OMG significantly outperforms the baselines.  The results highlight OMG's ability to predict opponent goals and adapt its strategy accordingly.", "section": "5 Experiments"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_7_1.jpg", "caption": "Figure 5: Test performance of cooperation with unseen opponents in 8m (a), 3s_vs_5z (b) and 2c_vs_64zg (c) maps of SMAC. The X-axis represents the opponent's policies, and \"homologue\" refers to the policy learned by the same algorithm, while \"non-homologue\" represents different ones; e.g. 7 homologue refers to 7 opponents from 8 agents trained by the same algorithm (QMIX, VDN or IQL) on the 8m, and 7 non-homologue involves 7 opponents from different runs of those algorithms. The results show that OMG-optimistic outperforms all baselines. The results are averaged over collaborating with 30 opponents of different policies, with 95% confidence intervals.", "description": "This figure displays the results of testing the cooperation performance of the OMG algorithm against unseen opponents in three different StarCraft II maps (8m, 3s_vs_5z, and 2c_vs_64zg) of the SMAC environment.  The x-axis represents different opponent policy types, categorized as 'homologue' (policies trained by the same algorithm) and 'non-homologue' (policies trained by different algorithms).  The y-axis shows the win rate. The results demonstrate the superior performance of the OMG-optimistic variant compared to other methods (IQL, LIAM, Naive OM, OMG-conservative) across all maps and opponent types.", "section": "5.3 Generalization to Unknown Opponents"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_8_1.jpg", "caption": "Figure 6: Ablation study in Foraging. In (a), methods on the X-axis labeled with \u201csupv\u201d indicate that the inference model uses an MLP instead of a CVAE. In (b) OMG-random, OMG-1s, and OMG-3s represent subgoals selected from the opponent\u2019s future states: randomly, at the next step, and at the third step, respectively.", "description": "This figure shows the ablation study results for the proposed Opponent Modeling based on subgoal inference (OMG) method in the Foraging environment.  Panel (a) compares the performance of OMG using a Conditional Variational Autoencoder (CVAE) for subgoal inference against a simpler Multilayer Perceptron (MLP) architecture. Panel (b) contrasts different subgoal selection strategies within OMG: random selection, selecting the next timestep's state, and selecting the state three timesteps ahead.  The results illustrate the impact of both the inference model architecture and the subgoal selection strategy on the overall performance of the OMG method.", "section": "5.4 Ablation Study"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_8_2.jpg", "caption": "Figure 7: Ablation study of OMG in Foraging. (a) and (b) compares OMGs with different subgoal inputs for policy learning. (c) and (d) show ablation study for the hyperparameter horizon H.", "description": "This figure presents the ablation study results of the Opponent Modeling based on subgoal inference (OMG) method in the Foraging environment.  Parts (a) and (b) compare the performance of OMG with different subgoal input methods for policy learning, showing the impact of using inferred subgoals (\u011d), prior subgoals (\u011f), and randomly selected subgoals on the agent's score and episode length.  Parts (c) and (d) explore the effect of varying the hyperparameter H (horizon of future states considered for subgoal selection) on the agent's performance, demonstrating the influence of this parameter on both score and episode length.", "section": "5.4 Ablation Study"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_9_1.jpg", "caption": "Figure 8: Subgoal analysis of OMG in Foraging. The subgoal hit rates for OMG-conservative and OMG-optimistic are shown in Figure 8(a). In Figure 8(b), the agent controls player 1 (white), and the opponent controls player 2 (blue). The blue circle represents the state obtained through the reconstruction of the subgoal inferred by the agent. The figure illustrates the difference between OMG-conservative and OMG-optimistic under the same initial state and opponent policy.", "description": "This figure analyzes the performance of the Opponent Modeling based on subgoal inference (OMG) method in a Foraging environment.  Panel (a) shows the subgoal hit ratio (the percentage of times the model correctly predicts the opponent's future state based on their subgoal) for both the optimistic and conservative versions of OMG. Panel (b) visually demonstrates, using a sample scenario, how the inferred subgoal (blue circle) helps the agent anticipate the opponent's future movement. The difference between the optimistic and conservative versions of OMG is highlighted in this visualization, showing how their different subgoal selection strategies lead to different predictions.", "section": "5.5 Inferred Subgoal Analysis"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_13_1.jpg", "caption": "Figure 9: Illustration of opponent's decision tree. Circles, edges, and squares represent state nodes, action, and goal nodes respectively.", "description": "This figure illustrates a decision tree representing the opponent's action sequences.  Non-leaf nodes represent states, edges represent actions taken by the opponent, and leaf nodes represent the goal states.  The tree is used to analyze and compare the number of state-action pairs (s, a) versus the number of state-subgoal pairs (s, g) in opponent modeling. This comparison helps demonstrate the efficiency gains of OMG's subgoal-based approach over action-prediction methods.", "section": "A Analysis of (s, g)"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_14_1.jpg", "caption": "Figure 5: Test performance of cooperation with unseen opponents in 8m (a), 3s_vs_5z (b) and 2c_vs_64zg (c) maps of SMAC. The X-axis represents the opponent's policies, and \"homologue\" refers to the policy learned by the same algorithm, while \"non-homologue\" represents different ones; e.g. 7 homologue refers to 7 opponents from 8 agents trained by the same algorithm (QMIX, VDN or IQL) on the 8m, and 7 non-homologue involves 7 opponents from different runs of those algorithms. The results show that OMG-optimistic outperforms all baselines. The results are averaged over collaborating with 30 opponents of different policies, with 95% confidence intervals.", "description": "This figure shows the test performance of the OMG algorithm in three different scenarios of the StarCraft II Multi-Agent Challenge (SMAC) environment against opponents with unseen policies.  The x-axis represents different opponent policies; homologue indicates opponents trained using the same algorithm and parameters, non-homologue indicates opponents trained differently. The y-axis shows the win rate.  The results demonstrate that the OMG-optimistic approach outperforms the baselines (IQL, LIAM, Naive OM) across the different scenarios and opponent types. The improved performance highlights the generalization capabilities of the OMG-optimistic method.", "section": "5.3 Generalization to Unknown Opponents"}, {"figure_path": "Lt6wO0oZ8k/figures/figures_16_1.jpg", "caption": "Figure 8: Subgoal analysis of OMG in Foraging. The subgoal hit rates for OMG-conservative and OMG-optimistic are shown in Figure 8(a). In Figure 8(b), the agent controls player 1 (white), and the opponent controls player 2 (blue). The blue circle represents the state obtained through the reconstruction of the subgoal inferred by the agent. The figure illustrates the difference between OMG-conservative and OMG-optimistic under the same initial state and opponent policy.", "description": "This figure analyzes the performance of the Opponent Modeling based on subgoal inference (OMG) method in the Foraging environment.  Specifically, it shows the subgoal hit rate (the percentage of times the opponent actually visits the predicted subgoal state) for both the optimistic and conservative versions of the OMG algorithm.  Additionally, it illustrates a sample trajectory for both versions to visually demonstrate how their different subgoal selection strategies influence the predicted path and overall performance.", "section": "5.5 Inferred Subgoal Analysis"}]