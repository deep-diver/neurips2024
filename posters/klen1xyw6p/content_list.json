[{"type": "text", "text": "On the Robustness of Spectral Algorithms for Semirandom Stochastic Block Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aditya Bhaskara \u2217 Agastya Vibhuti Jha \u2020 Michael Kapralov \u2021 Naren Sarayu Manoj \u00a7 Davide Mazzali\u00b6 Weronika Wrzos-Kaminska \u2225 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In a graph bisection problem, we are given a graph $G$ with two equally-sized unlabeled communities, and the goal is to recover the vertices in these communities. A popular heuristic, known as spectral clustering, is to output an estimated community assignment based on the eigenvector corresponding to the second smallest eigenvalue of the Laplacian of $G$ . Spectral algorithms can be shown to provably recover the cluster structure for graphs generated from certain probabilistic models, such as the Stochastic Block Model (SBM). However, spectral clustering is known to be non-robust to model mis-specification. Techniques based on semidefinite programming have been shown to be more robust, but they incur significant computational overheads. ", "page_idx": 0}, {"type": "text", "text": "In this work, we study the robustness of spectral algorithms against semirandom adversaries. Informally, a semirandom adversary is allowed to \u201chelpfully\u201d change the specification of the model in a way that is consistent with the ground-truth solution. Our semirandom adversaries in particular are allowed to add edges inside clusters or increase the probability that an edge appears inside a cluster. Semirandom adversaries are a useful tool to determine the extent to which an algorithm has overfit to statistical assumptions on the input. ", "page_idx": 0}, {"type": "text", "text": "On the positive side, we identify classes of semirandom adversaries under which spectral bisection using the unnormalized Laplacian is strongly consistent, i.e., it exactly recovers the planted partitioning. On the negative side, we show that in these classes spectral bisection with the normalized Laplacian outputs a partitioning that makes a classification mistake on a constant fraction of the vertices. Finally, we demonstrate numerical experiments that complement our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Graph partitioning or clustering is a fundamental unsupervised learning primitive. In a graph partitioning problem, one seeks to identify clusters of vertices that are highly internally connected and sparsely connected to the outside. This task is of particular significance when the given graph presents a latent community structure. In this setting, the goal is to recover the communities as accurately as possible. Various statistical models that attempt to capture this situation have been proposed and studied in the literature. Perhaps the most popular of these is the Symmetric Stochastic Block Model (SSBM) [HLL83]. ", "page_idx": 0}, {"type": "text", "text": "Following the notation of previous works [AFWZ20; DLS21], in this paper we describe an SSBM with specifications $n,P_{1},P_{2},p,q$ , where $n$ is an even positive integer, $P_{1}$ and $P_{2}$ are a partitioning of the vertex set $V=\\{1,\\ldots,n\\}$ into subsets of equal size, and $p$ and $q$ are probabilities. Without loss of generality, we may assume that the partitions $P_{1}$ and $P_{2}$ consist of vertices $1,\\ldots,n/2$ and $n/2+$ $1,\\ldots,n$ , respectively. Hence, with a mild abuse of notation, we write an SSBM with parameters $n,p,q$ only and write it as ${\\mathsf{S S B M}}(n,p,q)$ . Now, let ${\\mathsf{S S B M}}(n,p,q)$ be a distribution over random undirected graphs $G=(V,E)$ where each edge $(v,w)\\in P_{1}\\times P_{1}$ and $(v,w)\\in P_{2}\\times P_{2}$ (which we refer to as \u201cinternal edges\u201d) appears independently with probability $p$ , and each edge $(v,w)\\in$ $P_{1}\\times P_{2}$ (which we refer to as \u201ccrossing edges\u201d) appears independently with probability $q$ . When $p\\gg\\ q$ , there should be many more internal edges than crossing edges. Hence, we expect the community structure to become more evident as $p$ tends away from $q$ . ", "page_idx": 1}, {"type": "text", "text": "In such scenarios, our general algorithmic goal is to efficiently identify $P_{1}$ and $P_{2}$ when given $G$ without any community labels. This task is hereafter referred to as the graph bisection problem. In this work, we will be interested in exact recovery, also known as strong consistency, in which we want an algorithm that, with probability at least $1-1/n$ over the randomness of the instance, exactly returns the partition $\\{P_{1},P_{2}\\}$ for all $n$ sufficiently large. Other approximate notions of recovery (such as almost exact, partial, and weak recovery) are also well-studied but are beyond the scope of this work. ", "page_idx": 1}, {"type": "text", "text": "Although the ${\\mathsf{S S B M}}(n,p,q)$ distribution over graphs is a useful starting point for algorithm design and has led to a deep theory about when recovery is possible and of what nature [Abb18], it may not be representative of all scenarios in which we should expect our algorithms to succeed. To remedy this, researchers have proposed several different random graph models that may be more reflective of properties satisfied by real-world networks. These include the geometric block model [GNW24], the Gaussian mixture block model [LS24], and others. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we take a different perspective to graph generation by considering various semirandom models. At a high level, a semirandom model for a statistical problem interpolates between an average-case input (for example produced by a model such as the SSBM) and a worst-case input, in a way that still allows for a meaningful notion of ground-truth solution. In our context of graph bisection, this can be achieved by an adversary adding internal edges or by the distribution of internal edges itself being nonhomogeneous (i.e., every internal edge $(v,w)$ appears independently with probability $p_{v w}\\geq p$ , where the $p_{v w}$ may be chosen adversarially for each internal edge). Researchers have studied similar semirandom models for graph bisection [FK01; MMV12; MPW16; Moi21] and other statistical problems such as classification under Massart noise [MN06], detecting a planted clique in a random graph [FK01; CSV17; MMT20; BKS23], sparse recovery [KLLST23], and top- $K$ ranking [YCOM24]. ", "page_idx": 1}, {"type": "text", "text": "These modeling modifications are not necessarily meant to capture a real-world data generation process. Rather, they are a useful testbed with which we can determine whether commonly used algorithms have overfit to statistical assumptions present in the model. In particular, observe that these changes in model specification are ostensibly helpful, in that increasing the number of internal edges should only enhance the community structure. Perhaps surprisingly, it is known that a number of natural algorithms that succeed in the SSBM setting no longer work under such helpful modifications [Moi21]. Therefore, it is natural to ask which algorithms for graph bisection are robust in semirandom models. ", "page_idx": 1}, {"type": "text", "text": "At this point, the performance of approaches based on convex programming is well-understood in various semirandom models [FK01; MMV12; MPW16; Moi21]. However, in practice, it may be undesirable to run such an algorithm due to computational costs. Another class of algorithms, that we call spectral algorithms, is more widely used in practice. Loosely speaking, a spectral algorithm constructs a matrix M that is a function of the graph $G$ and outputs a clustering arising from the embedding of the vertices determined by the eigenvectors of M. Popular choices of matrices include the unnormalized Laplacian $\\mathbf{L}_{G}$ and the normalized Laplacian $\\mathcal{L}_{G}$ (we will formally define and intuit these notions in the sequel) [Von07]. This is because structural properties of both $\\mathbf{L}_{G}$ and $\\mathcal{L}_{G}$ imply that the second smallest eigenvalue of each, denoted as $\\lambda_{2}(\\mathbf{L}_{G})$ and $\\lambda_{2}(\\mathcal{L}_{G})$ , serves as a continuous proxy for connectivity, and the corresponding eigenvector, ${\\bf\\nabla}u_{2}({\\bf L}_{G})$ and $u_{2}(\\mathcal{L}_{G})$ , has entries whose signs reveal a lot of information about the underlying community structure. This motivates Algorithm 1. It can be run, for example, with ${\\mathsf{M a t r i x}}(G):={\\mathbf{L}}_{G}$ or $\\mathring{\\mathsf{M a t r i x}}(G):=\\mathcal{L}_{G}$ . Following this discussion, we arrive at the question we study in this paper. ", "page_idx": 1}, {"type": "table", "img_path": "kLen1XyW6P/tmp/9f32c5dea412744de3acec796d38b4f3922c7027967f33ab26982728733f3cd7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Question 1. Under which semirandom models do the Laplacian-based spectral algorithms, using the second eigenvector of $\\mathbf{L}_{G}$ or $\\mathcal{L}_{G}$ , exactly recover the ground-truth communities $P_{1}$ and $P_{2}$ ? ", "page_idx": 2}, {"type": "text", "text": "Main contributions. Our results show a surprising difference in the robustness of spectral bisection when considering the normalized versus the unnormalized Laplacian. We summarize our results below: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Consider a nonhomogeneous symmetric stochastic block model with parameters $q<p<$ $\\overline{{p}}$ , where every internal edge appears independently with probability $p_{u v}\\in[p,\\overline{{p}}]$ and every crossing edge appears independently with probability $q$ . We show that under an appropriate spectral gap condition, the spectral algorithm with the unnormalized Laplacian exactly recovers the communities $P_{1}$ and $P_{2}$ . Moreover, this holds even if an adversary plants $\\ll n p$ internal edges per vertex prior to the edge sampling phase.   \n\u2022 Consider a stronger semirandom model where the subgraphs on the two communities $P_{1}$ and $P_{2}$ are adversarially chosen and the crossing edges are sampled independently with probability $q$ . We show that if the graph is sufficiently dense and satisfies a spectral gap condition, then the spectral algorithm with the unnormalized Laplacian exactly recovers the communities $P_{1}$ and $P_{2}$ .   \n\u2022 We show that there is a family of instances from a nonhomogeneous symmetric stochastic block model in which the spectral algorithm achieves exact recovery with the unnormalized Laplacian, but incurs a constant error rate with the normalized Laplacian. This is surprising because it contradicts conventional wisdom that normalized spectral clustering should be favored over unnormalized spectral clustering [Von07]. ", "page_idx": 2}, {"type": "text", "text": "We also numerically complement our findings via experiments on various parameter settings. ", "page_idx": 2}, {"type": "text", "text": "Outline. The rest of this paper is organized as follows. In Section 2, we more formally define our semirandom models, the Laplacians $\\mathbf{L}$ and $\\mathcal{L}$ , and formally state our results. In Section 3, we give sketches of the proofs of our results. In Section 4, we show results from numerical trials suggested by our theory. In Appendices A.1 and A.5 we prove important auxiliary lemmas we need for our results. In Appendix A.6, we prove our robustness results for the unnormalized Laplacian. In Appendix A.8, we prove our inconsistency result for the normalized Laplacian. In Appendix B, we give additional numerical trials and discussion. ", "page_idx": 2}, {"type": "text", "text": "2 Models and main results ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we study unnormalized and normalized spectral clustering in several semirandom SSBMs. These models permit a richer family of graphs than the SSBM alone. ", "page_idx": 2}, {"type": "text", "text": "Matrices related to graphs. Throughout this paper, all graphs are to be interpreted as being undirected, and we assume that the vertices of an $n$ -vertex graph coincide with the set $\\{1,\\ldots,n\\}$ . With this in mind, we begin with defining various matrices associated with graphs, building up to the unnormalized and normalized Laplacians, which are central to the family of algorithms we analyze (Algorithm 1). ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Adjacency matrix). Let $G=(V,E)$ be a graph. The adjacency matrix $\\mathbf{A}_{G}\\in\\mathbb{R}^{V\\times V}$ of $G$ is the matrix with entries defined as $\\mathbf{A}_{G}[v,w]=\\mathbb{1}\\left\\{(v,w)\\in E\\right\\}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.2 (Degree matrix). Let $G=(V,E)$ be a graph. The degree matrix $\\mathbf{D}_{G}\\in\\mathbb{R}^{V\\times V}$ of $G$ is the diagonal matrix with entries defined as ${\\bf D}_{G}[v,v]=d_{G}[v],$ , where $d_{G}[v]$ is the degree of $v$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 2.3 (Unnormalized Laplacian). Let $G=(V,E)$ be a graph. The unnormalized Laplacian $\\mathbf{L}_{G}\\,\\in\\,\\mathbb{R}^{V\\times V}$ of $G$ is the matrix defined as $\\begin{array}{r}{\\mathbf{L}_{G}:=\\mathbf{D}_{G}-\\mathbf{A}_{G}=\\sum_{(v,w)\\in E}(e_{v}-e_{w})(e_{v}-e_{w})^{\\top}}\\end{array}$ , where $e_{i}$ denotes the $i$ -th standard basis vector. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4 (Normalized Laplacians). Let $G\\,=\\,(V,E)$ be a graph. The symmetric normalized Laplacian $\\mathcal{L}_{G,\\mathsf{s y m}}\\in\\mathbb{R}^{V\\times V}$ and the random walk Laplacian $\\mathcal{L}_{G,\\mathtt{r w}}\\in\\mathbb{R}^{V\\times V}$ of $G$ are defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}_{G,\\mathrm{sym}}:=\\mathbf{I}-\\mathbf{D}_{G}^{-1/2}\\mathbf{A}_{G}\\mathbf{D}_{G}^{-1/2}\\,,\\qquad\\qquad}&{{}\\mathcal{L}_{G,\\mathrm{rw}}:=\\mathbf{I}-\\mathbf{D}_{G}^{-1}\\mathbf{A}_{G}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For all notions above, when the graph $G$ is clear from context, we omit the subscript $G$ . Furthermore, when we discuss normalized Laplacians, we intend its symmetric version $\\mathcal{L}_{\\mathrm{sym}}$ unless otherwise stated. So, we omit this subscript as well and simply write $\\mathcal{L}$ . ", "page_idx": 3}, {"type": "text", "text": "Next, we define the spectral bisection algorithms. We will discuss some intuition for why these algorithms are reasonable heuristics in Section 3. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.5 (Unnormalized and normalized spectral bisection). Let $G\\;=\\;(V,E)$ be a graph, and let its unnormalized and normalized Laplacians be L and $\\mathcal{L}$ , respectively. We refer to the algorithm resulting from running Algorithm 1 on $G$ with Matrix $\\mathbf{\\Sigma}:(G):=\\mathbf{L}_{G}$ as unnormalized spectral bisection. We refer to the algorithm resulting from running Algorithm $^{\\,l}$ on $G$ with Matrix $(G)={\\mathcal{L}}_{G}$ as normalized spectral bisection. ", "page_idx": 3}, {"type": "text", "text": "Our goal is to understand when the above algorithms, applied to a graph with a latent community structure, achieve exact recovery or strong consistency, defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.6. Let $\\{P_{1},P_{2}\\}$ be a partitioning of $V=\\{1,\\ldots,n\\}$ , and let $\\mathcal{D}:=\\mathcal{D}(\\{P_{1},P_{2}\\})$ be $a$ distribution over $n$ -vertex graphs $G=(V,E)$ . We say that an algorithm is strongly consistent or achieves exact recovery on $\\mathcal{D}$ if given a graph $G\\sim\\mathcal{D}$ it outputs the correct partitioning $\\{P_{1},P_{2}\\}$ with probability at least $1-1/n$ over the randomness of $G$ . ", "page_idx": 3}, {"type": "text", "text": "2.1 Nonhomogeneous symmetric stochastic block model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our first model is a family of nonhomogeneous symmetric stochastic block models, defined below. ", "page_idx": 3}, {"type": "text", "text": "Model 1 (Nonhomogeneous symmetric stochastic block model). Let n be an even positive integer, $V\\,=\\,\\{1,\\ldots,n\\}$ , $\\{P_{1},P_{2}\\}$ be a partitioning of $V$ into two equally-sized subsets, and $q\\,<\\,p\\,\\leq\\,{\\overline{{p}}}$ be probabilities. Let $\\mathcal{D}$ be any probability distribution over graphs $G=(V,E)$ such that for every $(v,w)\\in P_{1}\\times P_{1}$ and $(v,w)\\,\\in\\,P_{2}\\,\\times\\,P_{2}$ , the edge $(v,w)$ appears in $E$ independently with some probability $p_{v w}\\in[p,\\overline{{p}}]$ , and for every $(v,w)\\in P_{1}\\times P_{2}$ , the edge $(v,w)$ appears in $E$ independently with probability $q$ . We call such $\\mathcal{D}$ a nonhomogeneous symmetric stochastic block model (which we will abbreviate as NSSBM). We call the set of all such $\\mathcal{D}$ the family of nonhomogeneous stochastic block models with parameters $p,\\overline{{p}},q$ , written as $\\mathsf{N S S B M}(n,p,\\overline{{p}},q)$ . ", "page_idx": 3}, {"type": "text", "text": "To visualize Model 1, consider the expected adjacency matrix of some NSSBM distribution. We then have the relations ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left[\\frac{p\\cdot\\mathbf{J}_{n/2}\\,\\left|\\,\\mathbf{\\nabla}q\\cdot\\mathbf{J}_{n/2}\\,\\right.}{q\\cdot\\mathbf{J}_{n/2}\\,\\left|\\,\\mathbf{\\nabla}p\\cdot\\mathbf{J}_{n/2}\\,\\right.}\\right]\\leq\\left[\\frac{\\mathbf{P}_{P_{1}}}{q\\cdot\\mathbf{J}_{n/2}\\,\\left|\\,\\mathbf{\\nabla}\\mathbf{P}_{P_{2}}\\,\\right.}\\right]\\leq\\left[\\frac{\\overline{{p}}\\cdot\\mathbf{J}_{n/2}\\,\\left|\\,\\mathbf{\\nabla}q\\cdot\\mathbf{J}_{n/2}\\,\\right.}{q\\cdot\\mathbf{J}_{n/2}\\,\\left|\\,\\overline{{p}}\\cdot\\mathbf{J}_{n/2}\\,\\right.}\\right]\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the leftmost matrix denotes the expected adjacency matrix of ${\\mathsf{S S B M}}(n,p,q)$ , the rightmost matrix denotes the expected adjacency matrix of $\\mathsf{S S B M}(n,\\overline{{p}},q),\\mathbf{J}_{l}$ k denotes the $k\\!\\times\\!k$ all-ones matrix, and ${\\bf P}_{P_{1}}$ and ${\\bf P}_{P_{2}}$ denote the edge probability matrices for edges internal to $P_{1}$ and $P_{2}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "The above also shows that the rank of the expected adjacency matrix for ${\\mathsf{S S B M}}(n,p,q)$ is 2. However, the rank for the expected adjacency matrix for some NSSBM distribution may be as large as $\\Omega(n)$ . Perhaps surprisingly, this will turn out to be unimportant for our entrywise eigenvector perturbation analysis. In particular, the tools we use were originally designed for low-rank signal matrices or spiked low-rank signal matrices [AFWZ20; DLS21; BV24], but we will see that they can be adapted to the signal matrices we consider. ", "page_idx": 3}, {"type": "text", "text": "The NSSBM family generalizes the symmetric stochastic block model described in the previous section \u2013 this is attained by setting $p_{v w}\\,=\\,p$ for all internal edges $(v,w)$ . However, it can also encode biases for certain graph properties. For instance, a distribution from the NSSBM family may encode the idea that certain subsets of $P_{1}$ are expected to be denser than $P_{1}$ as a whole. ", "page_idx": 3}, {"type": "text", "text": "With this definition in hand, we are ready to formally state our first technical result in Theorem 1. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Let $p,\\overline{{p}},q$ be probabilities such that $q\\,<\\,p\\,\\leq\\,{\\overline{{p}}}$ and such that $\\alpha:=\\overline{{p}}/(p-q)$ is an arbitrary constant. Let $\\mathcal D\\,\\in\\,\\mathsf{N S S B M}(n,p,\\overline{{p}},q)$ . Let $n\\,\\geq\\,N(\\alpha)$ where the function $N(\\alpha)$ only depends on $\\alpha$ . There exists a universal constant $C>0$ such that $i f$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nn(p-q)\\geq C\\left({\\sqrt{n{\\overline{{p}}}\\log n}}+\\log n\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then unnormalized spectral bisection is strongly consistent on $\\mathcal{D}$ . ", "page_idx": 4}, {"type": "text", "text": "We prove Theorem 1 in Appendix A.7.1. In fact, we show a somewhat stronger statement \u2013 in addition to the process described above, we also allow the adversary to, before sampling the graph, set a small number of the $p_{v w}$ to 1 (at most $n{\\overline{{p}}}/\\log\\log n$ edges per vertex). We detail this further in Appendix A.7.1. ", "page_idx": 4}, {"type": "text", "text": "We now remark on the tightness of our gap condition in Theorem 1. A work of Abbe, Bandeira, and Hall [ABH16] identifies an exact information-theoretic threshold above which exact recovery with high probability is possible and below which no algorithm can be strongly consistent. In particular, the threshold states that for any $p$ and $q$ satisfying $\\sqrt{p}-\\sqrt{q}>\\sqrt{2\\log n/n}$ , exact recovery is possible, and when $p$ and $q$ do not satisfy this, exact recovery is information-theoretically impossible. Furthermore, Feige and Kilian [FK01] prove that the information-theoretic threshold does not change in a somewhat stronger semirandom model that includes the NSSBM family. Additionally, Deng, Ling, and Strohmer [DLS21] show that unnormalized spectral bisection is strongly consistent all the way to this threshold in the special case where the graph is drawn from ${\\mathsf{S S B M}}(n,p,q)$ . By contrast, our gap condition holds in the same critical degree regime as in the information-theoretic threshold (namely, $p=\\Theta(\\log n/n))$ but our constant is not optimal. We incur this constant loss because for the sake of presentation, we opt for a cleaner argument that can handle the nonhomogeneity and generalizes more readily across degree regimes. To our knowledge, none of these features are present in prior work analyzing spectral methods in an SSBM setting [AFWZ20; DLS21]. ", "page_idx": 4}, {"type": "text", "text": "2.2 Deterministic clusters model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given Theorem 1, it is natural to ask what happens if we allow the adversary full control over the structure of the graphs in $P_{1}$ and $P_{2}$ instead of simply allowing the adversary to perturb the edge probabilities. In this section, we answer this question. We first describe a more adversarial semirandom model than the NSSBM family. We call this model the deterministic clusters model, defined as follows. ", "page_idx": 4}, {"type": "text", "text": "Model 2 (Deterministic clusters model). Let n be an even positive integer, $V\\ =\\ \\{1,\\ldots,n\\},$ , $\\{P_{1},P_{2}\\}$ be a partitioning of $V$ into two equally-sized subsets, q be a probability, and $d_{\\mathsf{i n}}$ be an integer degree lower bound. Consider a graph $G\\,=\\,(V,E)$ generated according to the following process. ", "page_idx": 4}, {"type": "text", "text": "1. The adversary chooses arbitrarily graphs $G[P_{1}]$ and $G[P_{2}]$ with minimum degree $d_{\\mathrm{in}}$ ; 2. Nature samples every edge $(v,w)\\in P_{1}\\times P_{2}$ to be in $E$ independently with probability $q$ . 3. The adversary arbitrarily adds edges $(v,w)\\in P_{1}\\times P_{1}$ and $(v,w)\\in P_{2}\\times P_{2}$ to $E$ after observing the edges sampled by nature. ", "page_idx": 4}, {"type": "text", "text": "We call a distribution $\\mathcal{D}$ of graphs generated according to the above process a deterministic clusters model (DCM). We call the set of all such $\\mathcal{D}$ the family of deterministic clusters models with parameters $d_{\\mathrm{in}}$ and $q$ , written as $\\mathsf{D C M}(n,d_{\\mathsf{i n}},q)$ . ", "page_idx": 4}, {"type": "text", "text": "The DCM graph generation process is heavily motivated by the one studied by Makarychev, Makarychev, and Vijayaraghavan [MMV12]. This model is much more flexible than the SSBM and NSSBM settings in that the graphs the adversary draws on $P_{1}$ and $P_{2}$ are allowed to look very far from random graphs. This means the DCM is a particularly good benchmark for algorithms to ensure they are not implicitly using properties of random graphs that might not hold in the worst case. ", "page_idx": 4}, {"type": "text", "text": "Within the DCM setting, we have Theorem 2. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let q be a probability and $d_{\\mathrm{in}}$ be an integer, and let $D\\in\\mathsf{D C M}(n,d_{\\mathsf{i n}},q)$ . For $G\\sim\\mathcal{D}$ , letL denote the expectation of $\\mathbf{L}$ after step (2) but before step (3) in Model 2. There exists constants ", "page_idx": 4}, {"type": "text", "text": "$C_{1},C_{2},C_{3}>0$ such that for all n sufficiently large, if ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{\\mathrm{in}}\\geq C_{1}\\cdot\\left(\\frac{n q}{2}+\\sqrt{n}\\right)\\quad a n d\\quad\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\widehat{\\mathbf{L}})\\geq\\sqrt{n}+C_{2}n q+C_{3}\\left(\\sqrt{n q\\log n}+\\log n\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "then unnormalized spectral bisection is strongly consistent on $\\mathcal{D}$ . ", "page_idx": 5}, {"type": "text", "text": "We prove Theorem 2 in Appendix A.7.2. We remark that, as in Theorem 1, the constants that appear in Theorem 2 are somewhat arbitrary. They are chosen to make our proofs cleaner and can likely be optimized. ", "page_idx": 5}, {"type": "text", "text": "As a basic application of Theorem 2, note that in the SSBM, if $p=\\omega(1/\\sqrt{n})$ and $q=1/\\sqrt{n}$ , then for $n$ sufficiently large, with high probability, the resulting graph satisfies the conditions needed to apply Theorem 2. For a more interesting example, let $P_{1}$ and $P_{2}$ be two $d$ -regular spectral expanders with $d\\,=\\,\\omega({\\sqrt{n}})$ and let $q\\,\\leq\\,1/{\\sqrt{n}}$ . On top of both of these two graph classes, one can further allow arbitrary edge insertions inside $P_{1}$ and $P_{2}$ while still being guaranteed exact recovery from unnormalized spectral bisection. ", "page_idx": 5}, {"type": "text", "text": "2.3 Inconsistency of normalized spectral clustering ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Notice that in Theorem 1 and Theorem 2, we only address the strong consistency of the unnormalized Laplacian in our nonhomogeneous and semirandom models. But what happens when we run spectral bisection with the normalized Laplacian? ", "page_idx": 5}, {"type": "text", "text": "In Theorem 3, we prove that there is a subfamily of instances belonging to $\\mathsf{N S S B M}(n,p,\\overline{{p}},q)$ with $\\overline{{p}}=6p,q=p/2$ on which unnormalized spectral bisection is strongly consistent (following from Theorem 1) but normalized spectral clustering is inconsistent in a rather strong sense. Thus, one cannot obtain results similar to Theorem 1 and Theorem 2 for normalized spectral bisection. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. For all n sufficiently large, there exists a nonhomogeneous stochastic block model such that unnormalized spectral bisection is strongly consistent whereas normalized spectral bisection (both symmetric and random-walk) incurs a misclassification rate of at least $24\\%$ with probability $1-1/n$ . ", "page_idx": 5}, {"type": "text", "text": "We prove Theorem 3 in Appendix A.8. Furthermore, we expect that it is straightforward to adapt the example in Theorem 3 to prove an analogous result for our DCM setting. ", "page_idx": 5}, {"type": "text", "text": "The result of Theorem 3 may run counter to conventional wisdom, which suggests that normalized spectral clustering should be favored over the unnormalized variant [Von07]. Perhaps a more nuanced view in light of Theorem 1 and Theorem 2 is to acknowledge that the normalized Laplacian and its eigenvectors enjoy stronger concentration guarantees [SB15; DLS21], but the unnormalized Laplacian\u2019s second eigenvector is more robust to monotone adversarial changes. ", "page_idx": 5}, {"type": "text", "text": "2.4 Open problems ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Perhaps the most natural follow-up question inspired by our results is to determine whether the restriction that every internal edge probability $p_{v w}\\leq\\overline{{p}}$ can be lifted entirely while still maintaining strong consistency of the unnormalized Laplacian (Theorem 2). Another exciting direction for future work is to lower the degree and/or spectral gap requirement present in our results in the DCM setting (Theorem 2). Finally, we only study insertion-only monotone adversaries, as crossing edge deletions change the second eigenvector of the expected Laplacian. It would be illuminating to understand the robustness of Laplacian-based spectral algorithms against a monotone adversary that is also allowed to delete crossing edges. We are optimistic that the answers to one or more of these questions will further improve our understanding of the robustness of spectral clustering to \u201chelpful\u201d model misspecification. ", "page_idx": 5}, {"type": "text", "text": "3 Analysis sketch ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "First, let us give some intuition as to why one may expect that unnormalized spectral bisection is robust against our monotone adversaries. Here and in the sequel, let $\\pmb{u}_{2}^{\\star}=[\\mathbb{1}_{n/2}^{\\star}\\oplus-\\mathbb{1}_{n/2}]/\\sqrt{n}$ , where ${\\mathbb I}_{k}$ denotes the all-1s vector in $k$ dimensions and $\\oplus$ denotes vector concatenation. Let $\\mathbf{L}$ be the unnormalized Laplacian of the graph we want to partition, $\\mathbf{L}^{\\star}:=\\,\\mathbb{E}\\left[\\mathbf{L}\\right]$ , $\\mathbf{E}:=\\mathbf{L}-\\mathbf{L}^{\\star}$ , and $\\lambda_{i}^{\\star}:=\\,\\lambda_{i}(\\mathbf{L}^{\\star})$ for $1\\,\\leq\\,i\\,\\leq\\,n$ . For an edge $(v,w)$ , let $\\pmb{e}_{v w}\\,:=\\,\\pmb{e}_{v}\\,-\\,\\pmb{e}_{w}$ , so that $e_{v w}$ is an edge incidence vector corresponding to the edge $(v,w)$ . Let $p_{v w}$ be the probability that the edge $(v,w)$ ", "page_idx": 5}, {"type": "text", "text": "appears in $G$ and observe that $\\mathbf{L}^{\\star}$ can be written as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{L}^{\\star}=\\sum_{(v,w)\\in E_{\\mathrm{interal}}}p_{v w}\\cdot e_{v w}e_{v w}^{T}+\\sum_{(v,w)\\in E_{\\mathrm{crosing}}}q\\cdot e_{v w}e_{v w}^{T}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $E_{\\mathrm{internal}}\\,=\\,(P_{1}\\,\\times\\,P_{1})\\cup(P_{2}\\,\\times\\,P_{2})$ and $E_{\\mathrm{crossing}}\\,=\\,P_{1}\\,\\times\\,P_{2}$ . We can verify that $\\pmb{u}_{2}^{\\star}$ is an eigenvector of $\\mathbf{L}^{\\star}$ \u2013 indeed, we do so in Lemma A.14. And, for now, assume that $\\pmb{u}_{2}^{\\star}$ does correspond to the second smallest eigenvalue of $\\mathbf{L}^{\\star}$ (in our NSSBM family, this is easily ensured by enforcing $p>q,$ ). Moreover, for every internal edge $(v,w)\\in E_{\\mathrm{internal}}$ , we have $\\left\\langle\\boldsymbol{e}_{v w},\\boldsymbol{u}_{2}^{\\star}\\right\\rangle=0$ . Hence, any changes in internal edges do not change the fact that $\\pmb{u}_{2}^{\\star}$ is an eigenvector of the perturbed matrix. Thus, if the sampled $\\mathbf{L}$ is close enough to $\\mathbf{L}^{\\star}$ , then it is plausible that the second eigenvector of $\\mathbf{L}$ , denoted as $\\pmb{u}_{2}$ , is pretty close to $\\pmb{u}_{2}^{\\star}$ . In fact, the following conceptually stronger statement holds. If the subgraph formed by selecting just the crossing edges of $G$ is regular, then $\\pmb{u}_{2}^{\\star}$ is an eigenvector of L. This follows from the fact that $\\pmb{u}_{2}^{\\star}$ is an eigenvector of the unnormalized Laplacian of any regular bipartite graph where both sides have size $n/2$ and the previous observation that every internal edge is orthogonal to $\\pmb{u}_{2}^{\\star}$ . ", "page_idx": 6}, {"type": "text", "text": "To make this perturbation idea more formal, we recall the Davis-Kahan Theorem. Loosely, it states that $\\|\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}\\|_{2}\\lesssim\\|(\\mathbf{L}-\\mathbf{L}^{\\star})\\boldsymbol{u}_{2}^{\\star}\\|_{2}\\,/(\\lambda_{3}^{\\star}-\\lambda_{2}^{\\star})$ (we give a more formal statement in Lemma A.15). Expanding the entrywise absolute value $|(\\mathbf{L}-\\mathbf{L}^{\\star})\\mathbf{\\boldsymbol{u}}_{2}^{\\star}|$ reveals that its entries can be expressed as $2\\left\\lvert\\bar{d}_{\\mathrm{out}}[v]-\\mathbb{E}\\left[d_{\\mathrm{out}}[v]\\right]\\right\\rvert/\\sqrt{n}$ , where $d_{\\mathrm{out}}[v]$ denotes the number of edges incident to $v$ crossing to the opposite community as $v$ . This is unaffected by any increase in the number of edges incident to $v$ that stay within the same community as $v$ , denoted as $d_{\\mathrm{in}}[v]$ . Hence, regardless of how many internal edges we add before sampling or what substructures they encourage/create, if we have $\\lambda_{2}^{\\star}\\ll\\lambda_{3}^{\\star}$ , then we get $\\lVert\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}\\rVert_{2}\\leq o(1)$ . This immediately implies that $\\pmb{u}_{2}$ is a correct classifier on all but an $o(1)$ fraction of the vertices. ", "page_idx": 6}, {"type": "text", "text": "Entrywise analysis of $\\pmb{u}_{2}$ and NSSBM strong consistency. In order to achieve strong consistency, we need that for all $n$ sufficiently large, $\\pmb{u}_{2}$ is a perfect classifier. Unfortunately, the above argument does not immediately give that. In particular, in the density and spectral gap regimes we consider, the bound of $o(1)$ yielded by the Davis-Kahan theorem is not sufficiently small to directly yield $\\lVert\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\rVert_{2}\\ll1/\\sqrt{n}$ . Instead, we carry out an entrywise analysis of $\\pmb{u}_{2}$ . A general framework for doing so is given by Abbe, Fan, Wang, and Zhong [AFWZ20] and is adapted to the unnormalized and normalized Laplacians by Deng, Ling, and Strohmer [DLS21]. ", "page_idx": 6}, {"type": "text", "text": "At a high level, we adapt the analysis of Deng, Ling, and Strohmer [DLS21] to our setting. We consider the intermediate estimator vector $\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}u_{2}^{\\star}$ . This is a natural choice because we can verify $(\\mathbf{D}\\,-\\,\\lambda_{2}\\mathbf{I})^{-1}\\mathbf{A}\\mathbf{u}_{2}\\,=\\,\\mathbf{u}_{2}$ . We will see that it is enough to show that this intermediate estimator correctly classifies all the vertices while satisfying $|(\\bar{\\mathbf{D}}-\\lambda_{2}\\mathbf{I})^{-1}\\mathbf{A}(\\mathbf{{u}}_{2}^{\\star}-\\mathbf{{u}}_{2})|\\leq$ $\\left|\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}u_{2}^{\\star}\\right|$ (again, the absolute value is taken entrywise). With this in mind, taking some entry indexed by $v\\in V$ and multiplying both sides by ${\\pmb d}[v]-\\lambda_{2}$ (which we will show is positive with high probability), we see that it is enough to show ", "page_idx": 6}, {"type": "equation", "text": "$$\n|\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\rangle|\\leq|\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}\\rangle|=\\frac{|\\pmb{d}_{\\mathsf{i n}}[v]-\\pmb{d}_{\\mathsf{o u t}}[v]|}{\\sqrt{n}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{\\nabla}a_{v}$ denotes the $v$ -th row of $\\mathbf{A}$ . The advantage of this rewrite is that the right hand side can be uniformly bounded, so it is enough to control the left hand side. ", "page_idx": 6}, {"type": "text", "text": "To argue about the left hand side of (1), it may be tempting to use the fact that $\\scriptstyle\\mathbf{a}_{v}$ is a Bernoulli random vector and use Bernstein\u2019s inequality to argue about the sum of rescalings of these Bernoulli random variables. Unfortunately, we cannot do this since $\\pmb{u}_{2}$ and $\\scriptstyle\\mathbf{a}_{v}$ are dependent. To resolve this, we use a leave-one-out trick [AFWZ20; BV24]. We can think of this as leaving out the vertex $v$ corresponding to the entry we want to analyze and sampling the edges incident to the rest of the vertices. The second eigenvector of the resulting $\\mathbf{L}^{\\left(v\\right)}$ , denoted as $u_{2}^{(\\check{v})}$ , is a very good proxy for $\\pmb{u}_{2}$ and is independent from $\\scriptstyle\\mathbf{a}_{v}$ . Hence, we may complete the proof of Theorem 1. ", "page_idx": 6}, {"type": "text", "text": "One of our main observations is that although this style of analysis was originally built for low-rank signal matrices [AFWZ20; BV24], it can be adapted to handle the nonhomogeneity inside $P_{1}$ and $P_{2}$ . In particular, the nonhomogeneity we permit in the NSSBM family may make $\\mathbf{L}^{\\star}$ look very far from a spiked low-rank signal matrix. Furthermore, our entrywise analysis of eigenvectors under perturbations is one of the first that we are aware of that moves beyond analyzing low-rank signal matrices or spiked low-rank signal matrices. ", "page_idx": 6}, {"type": "image", "img_path": "kLen1XyW6P/tmp/40e2fa182488c12209c7958e8187507f95d2b45071899f7ef702ee285e4795da.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Extension to deterministic clusters. To prove Theorem 2, we start again at (1). An alternate way to upper bound the left hand side is to use the Cauchy-Schwarz inequality. A variant of the Davis-Kahan theorem gives us control over $\\lVert\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}\\rVert_{2}$ while $\\|\\mathbf{a}_{v}\\|_{2}=\\sqrt{\\pmb{d}[v]}$ . The advantage of this is that we get a worst-case upper bound on the left hand side of (1) \u2013 it holds no matter what edges orthogonal to $\\pmb{u}_{2}^{\\star}$ are inserted before or after nature samples the crossing edges (which are precisely the internal edges). Combining these and using the fact that the right hand side of (1) is increasing in $d_{\\mathrm{in}}[v]$ (and increases faster than $\\|\\pmb{a}_{v}\\|_{2}=\\sqrt{\\pmb{d}[v]})$ allows us to complete the proof of Theorem 2. ", "page_idx": 7}, {"type": "text", "text": "Inconsistency of normalized spectral bisection. Finally, we describe the family of hard instances we use to prove Theorem 3. To motivate this family of instances, recall that by the graph version of Cheeger\u2019s inequality, the second eigenvalue of $\\mathcal{L}$ and the corresponding eigenvector can be used to find a sparse cut in $G$ . Thus, if we create sparse cuts inside $P_{1}$ that are sparser than the cut formed by separating $P_{1}$ and $P_{2}$ , then conceivably the normalized Laplacian\u2019s second eigenvector may return the new sparser cut. ", "page_idx": 7}, {"type": "text", "text": "To make this formal, consider the following graph structure. Let $n$ be a multiple of 4. Let $L_{1}$ consist of indices $1,\\dots,n/4$ , $L_{2}$ consist of indices $n/4+1,\\ldots,n/2$ , and $R$ consist of indices $n/2+1,\\ldots,n$ . Consider the block structure induced by the matrix $\\mathbf{A}^{\\star}=\\mathbb{E}\\left[\\mathbf{A}\\right]$ shown in Table 1. ", "page_idx": 7}, {"type": "text", "text": "Intuitively, as $K$ gets larger, the cut separating $L_{1}$ from $V\\setminus L_{1}$ becomes sparser. From Cheeger\u2019s inequality, this witnesses a small $\\lambda_{2}(\\mathcal{L})$ and therefore the corresponding $u_{2}(\\mathcal{L})$ may return the cut $L_{1},V\\setminus L_{1}$ . We formally prove that this is indeed what happens when $K$ is a sufficiently large constant and then Theorem 3 follows. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical trials ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We programmatically generate synthetic graphs that help illustrate our theoretical findings using the libraries NetworkX 3.3 (BSD 3-Clause license), SciPy 1.13.0 (BSD 3-Clause License), and NumPy 1.26.4 (modified BSD license) [HSS08; ${\\mathrm{VGO}}^{+}20$ ; HMvdW $^{+}20$ ]. We ran all our experiments on a free Google Colab instance with the CPU runtime, and each experiment takes under one hour to run. In this section we focus on a setting that allows relating Theorem 1 and Theorem 3, and defer more experiments that investigate both NSSBM and DCM graphs to Appendix B. ", "page_idx": 7}, {"type": "text", "text": "To put Theorem 1 and Theorem 3 in perspective, we consider graphs generated following the process outlined in the proof of Theorem 3, which gives rise to the following benchmark distribution. ", "page_idx": 7}, {"type": "text", "text": "Benchmark distribution. Let $n$ be divisible by 4 and let $\\{P_{1},P^{\\,}{-}2\\}$ be a partitioning of $V=[n]$ into two equally-sized subsets. Let $\\{L_{1},L_{2}\\}$ be a bipartition of $P_{1}$ such that $|L_{1}|=|L_{2}|=n/4$ and call $L=P_{1}$ , $R=P_{2}$ for convenience as in the proof of Theorem 3. Then, for some $p,\\overline{{p}},q\\in[0,1]$ such that $q\\leq p\\leq\\overline{{p}}$ , consider the distribution $\\mathcal{D}_{p,\\overline{{p}},q}$ over graphs $G=(V,E)$ obtained by sampling every edge $(u,v)\\in(L_{1}\\times L_{1})\\cup(L_{2}\\times L_{2})$ independently with probability $\\overline{{p}}$ , every edge $(u,v)\\in$ $(L_{1}\\times L_{2})\\cup(R\\times R)$ independently with probability $p$ , and every edge $(u,v)\\in L\\times R$ independently with probability $q$ . One can see that $\\mathcal{D}_{p,\\overline{{p}},q}$ is in fact in the set $\\mathsf{N S S B M}(n,p,\\overline{{p}},q)$ . ", "page_idx": 7}, {"type": "text", "text": "Setup. Let us fix $n\\,=\\,2000$ , $p\\,=\\,24\\log n/n$ , $q\\,=\\,8\\log n/n$ . For varying values of $\\overline{{p}}$ in the range $[p,1]$ , we sample $t=10$ independent draws $G$ from $\\mathcal{D}_{p,\\overline{{p}},q}$ . For each of them, we run spectral bisection (i.e. Algorithm 1) with matrices $\\mathbf{L},\\mathcal{L}_{\\mathsf{s y m}},\\mathcal{L}_{\\mathsf{r w}},\\mathbf{A}$ . Then, we compute the agreement of the bipartition hence obtained (with respect to the planted bisection), that is the fraction of correctly classified vertices. We average the agreement across the $t$ independent draws. The results are shown in the top left plot of Fig. 1. Another natural way to get a bipartition of $V$ from the eigenvector is a sweep cut. In a sweep cut, we sort the entries of $\\pmb{u}_{2}$ and take the vertices corresponding to the smallest $n/2$ entries to be on one side of the bisection and put the remaining on the other side. The average agreement obtained in this other fashion is shown in the bottom left plot of Fig. 1. ", "page_idx": 7}, {"type": "text", "text": "Theoretical framing. As per Theorem 1, we expect unnormalized spectral bisection to achieve exact recovery (i.e. agreement equal to 1) whenever $\\overline{{p}}\\leq\\overline{{p}}_{\\mathtt{m a x}}$ , where ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\overline{{p}}_{\\mathtt{m a x}}=\\frac{\\left(n(p-q)-\\log n\\right)^{2}}{n\\log n}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "is obtained by rearranging the precondition of Theorem 1, ignoring the constants and disregarding the fact that $\\alpha$ should be $O(1)$ . On the contrary, the proof of Theorem 3 shows that normalized spectral bisection misclassifies a constant fraction of vertices provided that $p/q\\ \\geq\\ 2$ (which our choice of parameters satisfies) and $\\overline{{p}}\\geq\\overline{{p}}_{\\mathsf{t h r}}$ , where ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\overline{{p}}_{\\mathrm{thr}}=3\\cdot p^{2}/q\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "In Fig. 1, the solid vertical line corresponds to the value of $\\overline{{p}}_{\\mathsf{t h r}}$ on the $x$ -axis, and the dashed vertical line corresponds to the value of $\\overline{{p}}_{\\mathsf{m a x}}$ on the $x$ -axis. In particular, observe that in our setting $\\overline{{p}}_{\\mathsf{t h r}}<$ $\\overline{{p}}_{\\mathsf{m a x}}$ , so there is an interval of values for $\\overline{{p}}$ where we expect Theorem 1 and Theorem 3 to apply simultaneously. ", "page_idx": 8}, {"type": "text", "text": "Empirical evidence: consistency. One can see from the top left plot in Fig. 1 that the agreement of unnormalized spectral bisection is $100\\%$ for all values of $\\overline{{p}}$ , even beyond $\\overline{{p}}_{\\mathsf{t h r}}$ and $\\overline{{p}}_{\\mathsf{m a x}}$ . On the other hand, the agreement of the bipartition obtained from all other matrices (hence including normalized spectral bisection) drops below $70\\%$ well before the threshold $\\overline{{p}}_{\\mathsf{t h r}}$ predicted by Theorem 3. From the right plot in Fig. 1, we see that computing the bipartition by taking a sweep cut of $n/2$ vertices does not change the results \u2013 $\\pmb{u}_{2}$ of the unnormalized Laplacian continues to achieve $100\\%$ agreement, while for all other matrices the corresponding $\\pmb{u}_{2}$ remains inconsistent. ", "page_idx": 8}, {"type": "text", "text": "Empirical evidence: embedding variance. From the setting of the experiment we just illustrated, observe that as we increase $\\overline{{p}}$ , we expect the subgraph $G[L]$ to have increasing volume. As illustrated in Fig. 1, this seems to correlate with a decrease in the \u201cvariance\u201d of the second eigenvector $\\pmb{u}_{2}$ of the unnormalized Laplacian with respect to the ideal second eigenvector $\\pmb{u}_{2}^{\\star}$ . More precisely, we compute the average distance squared of the embedding of a vertex in $\\pmb{u}_{2}$ from its ideal embedding in $\\pmb{u}_{2}^{\\star}$ , i.e. the quantity ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{s\\in\\{\\pm1\\}}\\frac{1}{n}\\left\\|\\pmb{u}_{2}-s\\cdot\\pmb{u}_{2}^{\\star}\\right\\|_{2}^{2}\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "This suggests that not only does the second eigenvector of the unnormalized Laplacian remain robust to monotone adversaries, but it actually concentrates more strongly around the ideal embedding $\\pmb{u}_{2}^{\\star}$ . ", "page_idx": 8}, {"type": "text", "text": "Empirical evidence: example embedding. Let us fix the value $\\overline{{p}}=\\,\\overline{{p}}_{\\mathsf{t h r}}$ , for which we see in Fig. 3 that all matrices except the unnormalized Laplacian fail to recover the planted bisection. We generate a graph from $\\mathcal{D}_{p,\\overline{{p}},q}$ , and plot how the vertices are embedded in the real line by the second eigenvector of all the matrices we consider. The result is shown in Fig. 1, where the three horizontal dashed lines, from top to bottom, respectively correspond to the value of $1/\\sqrt{n},0,-1/\\sqrt{n}$ on the $y$ -axis. ", "page_idx": 8}, {"type": "text", "text": "4.1 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Community detection. Community detection has garnered significant attention in theoretical computer science, statistics, and data science. For a general overview of recent progress and related literature, see the survey by Abbe [Abb18]. In what follows, we discuss the works we believe are most related to what we study in this paper. ", "page_idx": 8}, {"type": "text", "text": "As mentioned in the introduction, perhaps the most fundamental and well-studied model is the symmetric stochastic block model (SSBM), due to [HLL83]. The celebrated work of Abbe, Bandeira, and Hall [ABH16] gives sharp bounds on the threshold for exact recovery for the SSBM setting. They complement their result by showing that SDP based methods can achieve the information theoretic lower bound for the planted bisection problem, even with a monotone adversary [Moi21]. A line of work [AFWZ20; DLS21] demonstrates that natural spectral algorithms achieve exact recovery for the SSBM all the way to the information-theoretic threshold. ", "page_idx": 8}, {"type": "text", "text": "Generalizations of the symmetric stochastic block model. Since the introduction of SBMs [HLL83], numerous variants have been proposed. These variants are designed to better reflect realworld graph properties that are unlikely to appear in graphs sampled from the SSBM. For instance, real-life social networks are likely to contain triangles. To address this, Sankararaman and Baccelli ", "page_idx": 8}, {"type": "image", "img_path": "kLen1XyW6P/tmp/2b0a805d3dfcb2b34d04d0f9570490ba364a0ebab9f77471bbdca4fca92f0b09.jpg", "img_caption": ["Figure 1: Top left, bottom left: Agreement with the planted bisection of the bipartition obtained from several matrices associated with an input graph generated from a distribution in $\\mathsf{N S S B M}(n,p,\\overline{{p}},q)$ for fixed values of $n,p,q$ and varying values of $\\overline{{p}}$ . In the top left plot, the bipartition is the 0-cut of the second eigenvector, as in Algorithm 1. In the bottom left plot, the bipartition is the sweep cut of the first $n/2$ vertices in the second eigenvector. The dashed vertical line corresponds to $\\overline{{p}}_{\\mathtt{m a x}}=\\overline{{p}}_{\\mathtt{m a x}}(n,p,q)$ (see (2)), and the solid vertical line corresponds to $\\overline{{p}}_{\\mathtt{t h r}}=\\overline{{p}}_{\\mathtt{t h r}}(n,p,q)$ (see (3)). Top middle, top right, bottom middle: Embedding of the vertices given by the second eigenvector $\\pmb{u}_{2}$ of several matrices associated with a graph sampled from $\\mathcal{D}_{p,\\overline{{p}},q}$ with $\\overline{{p}}=\\overline{{p}}_{\\mathtt{t h r}}$ . Horizontal dashed lines, from top to bottom, correspond to ${\\bar{1}}/{\\sqrt{n}},{\\bar{0}},-1/{\\sqrt{n}}$ respectively. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Bottom right: Variance of the embedding in the second eigenvector $\\pmb{u}_{2}$ of the unnormalized Laplacian with respect to the ideal eigenvector $\\pmb{u}_{2}^{\\star}$ (see (4)), for input graphs generated from a distribution in NSSBM $(n,p,\\overline{{p}},q)$ with fixed values of $n,p,q$ and varying values of $\\overline{{p}}$ . ", "page_idx": 9}, {"type": "text", "text": "[SB17] introduced a spatial stochastic block model, sometimes known as the geometric stochastic block model (GSBM). Other variations were introduced in the works of [GPMS18; GMPS19]. Subsequent work studies the performance of spectral algorithms on certain Gaussian or Geometric Mixture block models [ABRS20; ABD21; LS24; GNW24]. ", "page_idx": 9}, {"type": "text", "text": "Studying community detection with a semirandom model approaches this modeling question differently. Rather than implicitly encouraging a particular structure within the clusters like the models just mentioned, a semirandom adversary (including the ones we study in this paper) can more directly test the robustness of the algorithm to specially designed substructures. ", "page_idx": 9}, {"type": "text", "text": "Semirandom and monotone adversaries. As far as we are aware, Blum and Spencer [BS95] were the first to introduce a semirandom model. Within this model, they studied graph coloring problems. Feige and Kilian [FK01] demonstrated that semidefinite programming methods can accurately recover communities up to a certain threshold, even in the semi-random setting. Other problems, such as detecting a planted clique [Jer92; $\\mathrm{Ku}95$ ; BHKKMP19], have also been studied in the semi-random model of [FK01]. In the setting of planted clique, a natural spectral algorithm fails against monotone adversaries [MMT20; BKS23]. Monotone adversaries and semirandom models have also been extensively studied for other statistical and algorithmic problems [VA18; KLLST23; GC23; BGLMSY24]. Finally, [SL17] shows that a spectral heuristic due to Boppana [Bop87] is robust under a monotone adversary that is allowed to both insert internal edges and delete crossing edges. However, as far as we are aware, this algorithm does not fit in the framework of Algorithm 1. ", "page_idx": 9}, {"type": "text", "text": "We remark that the models we study in this paper are most closely related to models studied by [MN06] and [MMV12]. In particular, allowing increased internal edge probabilities is analogous to Massart noise in classification problems, and our model with adversarially chosen internal edges can be seen as the same model as that studied in [MMV12] (although without allowing crossing edge deletions). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. AB was partially supported by the National Science Foundation under Grant Nos. CCF-2008688 and CCF-2047288. NSM was supported by a National Science Foundation Graduate Research Fellowship. We thank Avrim Blum and Yury Makarychev for helpful discussions. We thank Nirmit Joshi for pointing us to the reference [DLS21]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[Abb18] Emmanuel Abbe. Community detection and stochastic block models: recent developments. Journal of Machine Learning Research, 18(177):1\u201386, 2018. arXiv: 1703.10146 [math.PR]. URL: http://jmlr.org/papers/v18/16- 480. html (cited on pages 2, 9).   \n[ABH16] Emmanuel Abbe, Afonso S. Bandeira, and Georgina Hall. Exact recovery in the stochastic block model. IEEE Transactions on Information Theory, 62(1):471\u2013487, 2016. DOI: 10.1109/TIT.2015.2490670. arXiv: 1405.3267 [cs.SI] (cited on pages 5, 9, 34).   \n[ABRS20] Emmanuel Abbe, Enric Boix-Adser\u00e0, Peter Ralli, and Colin Sandon. Graph powering and spectral robustness. SIAM Journal on Mathematics of Data Science, 2(1):132\u2013157, 2020. DOI: 10.1137/19M1257135. arXiv: 1809.04818 [cs.DS]. URL: https://doi.org/10.1137/19M1257135 (cited on page 10).   \n[AFWZ20] Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector analysis of random matrices with low expected rank. Annals of statistics, 48(3):1452, 2020. arXiv: 1709.09565 [math.ST] (cited on pages 2, 4, 5, 7, 9, 22, 23).   \n[ABD21] Konstantin Avrachenkov, Andrei Bobu, and Maximilien Dreveton. Higher-order spectral clustering for geometric graphs. Journal of Fourier Analysis and Applications, 27(2):22, March 2021. DOI: 10.1007/s00041-021-09825-2. arXiv: 2009.11353 [cs.LG]. URL: https://doi.org/10.1007/s00041- 021- 09825-2 (cited on page 10).   \n[BHKKMP19] Boaz Barak, Samuel Hopkins, Jonathan Kelner, Pravesh K Kothari, Ankur Moitra, and Aaron Potechin. A nearly tight sum-of-squares lower bound for the planted clique problem. SIAM Journal on Computing, 48(2):687\u2013735, 2019. arXiv: 1604. 03084 [cs.CC] (cited on page 10).   \n[BV24] Abhinav Bhardwaj and Van Vu. Matrix perturbation: davis-kahan in the infinity norm. In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). January 2024, pages 880\u2013934. arXiv: 2304.00328 [math.PR] (cited on pages 4, 7).   \n[BGLMSY24] Avrim Blum, Meghal Gupta, Gene Li, Naren Sarayu Manoj, Aadirupa Saha, and Yuanyuan Yang. Dueling optimization with a monotone adversary. In Proceedings of Thirty Fifth Conference on Algorithmic Learning Theory (ALT), February 2024. arXiv: 2311.11185 [cs.DS] (cited on page 10).   \n[BS95] Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. Journal of Algorithms, 19(2):204\u2013234, 1995. ISSN: 0196-6774. DOI: https : / / doi . org / 10 . 1006 / jagm . 1995 . 1034. URL: https : / / www . sciencedirect.com/science/article/pii/S0196677485710346 (cited on page 10).   \n[Bop87] Ravi B. Boppana. Eigenvalues and graph bisection: an average-case analysis. In 28th Annual Symposium on Foundations of Computer Science (sfcs 1987), pages 280\u2013285, 1987. DOI: 10.1109/SFCS.1987.22 (cited on page 10).   \n[BKS23] Rares-Darius Buhai, Pravesh K. Kothari, and David Steurer. Algorithms approaching the threshold for semi-random planted clique. In Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, pages 1918\u2013 1926, Orlando, FL, USA. Association for Computing Machinery, 2023. ISBN: 9781450399135. arXiv: 2212.05619 [cs.DS] (cited on pages 2, 10).   \n[CSV17] Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pages 47\u201360, Montreal, Canada. Association for Computing Machinery, 2017. ISBN: 9781450345286. arXiv: 1611.02315 [cs.LG] (cited on page 2).   \n[DLS21] Shaofeng Deng, Shuyang Ling, and Thomas Strohmer. Strong consistency, graph laplacians, and the stochastic block model. Journal of Machine Learning Research, 22(117):1\u201344, 2021. arXiv: 2004.09780 [stat.ML] (cited on pages 2, 4\u20137, 9, 11, 26, 33).   \n[FK01] Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. J. Comput. Syst. Sci., 63(4):639\u2013671, December 2001. ISSN: 0022-0000. DOI: 10.1006/jcss. 2001.1773. URL: https://doi.org/10.1006/jcss.2001.1773 (cited on pages 2, 5, 10).   \n[GMPS19] Sainyam Galhotra, Arya Mazumdar, Soumyabrata Pal, and Barna Saha. Connectivity of Random Annulus Graphs and the Geometric Block Model. In Dimitris Achlioptas and L\u00e1szl\u00f3 A. V\u00e9gh, editors, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM 2019), volume 145 of Leibniz International Proceedings in Informatics (LIPIcs), 53:1\u2013 53:23, Dagstuhl, Germany. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik, 2019. ISBN: 978-3-95977-125-2. arXiv: 1804.05013 [cs.DM] (cited on page 10).   \n[GPMS18] Sainyam Galhotra, Soumyabrata Pal, Arya Mazumdar, and Barna Saha. The geometric block model and applications. In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1147\u20131150, 2018. DOI: 10.1109/ALLERTON.2018.8635938 (cited on page 10).   \n[GC23] Xing Gao and Yu Cheng. Robust matrix sensing in the semi-random model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL: https://openreview.net/forum?id $=$ nSr2epejn2 (cited on page 10).   \n[GNW24] Julia Gaudio, Xiaochun Niu, and Ermin Wei. Exact community recovery in the geometric sbm. In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). 2024, pages 2158\u20132184. DOI: 10.1137/1.9781611977912. 78. arXiv: 2307.11196 [cs.SI]. URL: https://epubs.siam.org/doi/abs/ 10.1137/1.9781611977912.78 (cited on pages 2, 10).   \n[HSS08] Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using NetworkX. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008 (cited on page 8).   \n[HMvdW+20] Charles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez del R\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357\u2013362, September 2020. DOI: 10. 1038/s41586-020-2649-2. URL: https://doi.org/10.1038/s41586-020- 2649-2 (cited on page 8).   \n[HLL83] Paul W. Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: first steps. Social Networks, 5(2):109\u2013137, 1983. ISSN: 0378-8733. DOI: https://doi.org/10.1016/0378-8733(83)90021-7. URL: https: //www.sciencedirect.com/science/article/pii/0378873383900217 (cited on pages 1, 9).   \n[Jer92] Mark Jerrum. Large cliques elude the metropolis process. Random Struct. Algorithms, 3(4):347\u2013360, 1992. DOI: 10 . 1002 / RSA . 3240030402. URL: https : //doi.org/10.1002/rsa.3240030402 (cited on page 10).   \n[KLLST23] Jonathan Kelner, Jerry Li, Allen X. Liu, Aaron Sidford, and Kevin Tian. Semirandom sparse recovery in nearly-linear time. In Gergely Neu and Lorenzo Rosasco, editors, Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 2352\u20132398. PMLR, July 2023. arXiv: 2203.04002 [cs.DS]. URL: https://proceedings.mlr. press/v195/kelner23a.html (cited on pages 2, 10).   \n[Ku95] Ludk Kuera. Expected complexity of graph partitioning problems. Discrete Applied Mathematics, 57(2):193\u2013212, 1995. ISSN: 0166-218X. DOI: https : / / doi . org / 10 . 1016 / 0166 - 218X(94 ) 00103 - K. URL: https : / / www . sciencedirect.com/science/article/pii/0166218X9400103K. Combinatorial optimization 1992 (cited on page 10).   \n[LLV17] Can M Le, Elizaveta Levina, and Roman Vershynin. Concentration and regularization of random graphs. Random Structures & Algorithms, 51(3):538\u2013561, 2017. arXiv: 1506.00669 [math.PR] (cited on page 17).   \n[LS24] Shuangping Li and Tselil Schramm. Spectral clustering in the gaussian mixture block model, 2024. arXiv: 2305.00979 [stat.ML] (cited on pages 2, 10).   \n[MMV12] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approximation algorithms for semi-random partitioning problems. In Proceedings of the Forty-Fourth Annual ACM Symposium on Theory of Computing, STOC \u201912, pages 367\u2013384, New York, New York, USA. Association for Computing Machinery, 2012. ISBN: 9781450312455. arXiv: 1205.2234 [cs.DS] (cited on pages 2, 5, 10).   \n[MN06] Pascal Massart and \u00c9lodie N\u00e9d\u00e9lec. Risk bounds for statistical learning. The Annals of Statistics, 34(5), October 2006. ISSN: 0090-5364. DOI: 10 . 1214 / 009053606000000786. URL: http : / / dx . doi . org / 10 . 1214 / 009053606000000786 (cited on pages 2, 10).   \n[MMT20] Theo McKenzie, Hermish Mehta, and Luca Trevisan. A new algorithm for the robust semi-random independent set problem. In Proceedings of the Thirty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA \u201920, pages 738\u2013 746, Salt Lake City, Utah. Society for Industrial and Applied Mathematics, 2020. arXiv: 1808.03633 [cs.DS] (cited on pages 2, 10).   \n[Moi21] Ankur Moitra. Semirandom stochastic block models. In Beyond the Worst-Case Analysis of Algorithms. Tim Roughgarden, editor. Cambridge University Press, 2021, pages 212\u2013233. DOI: 10.1017/9781108637435.014 (cited on pages 2, 9).   \n[MPW16] Ankur Moitra, William Perry, and Alexander S. Wein. How robust are reconstruction thresholds for community detection? In Proceedings of the FortyEighth Annual ACM Symposium on Theory of Computing, STOC \u201916, pages 828\u2013 841, Cambridge, MA, USA. Association for Computing Machinery, 2016. ISBN: 9781450341325. DOI: 10 . 1145 / 2897518 . 2897573. arXiv: 1511 . 01473 [cs.DS]. URL: https://doi.org/10.1145/2897518.2897573 (cited on page 2).   \n[SB17] Abishek Sankararaman and Fran\u00e7ois Baccelli. Community detection on euclidean random graphs. In 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 510\u2013517, 2017. DOI: 10.1109/ALLERTON. 2017.8262780 (cited on page 9).   \n[SB15] Purnamrita Sarkar and Peter J. Bickel. Role of normalization in spectral clustering for stochastic blockmodels. The Annals of Statistics, 43(3), June 2015. ISSN: 0090- 5364. DOI: 10.1214/14-aos1285. arXiv: 1310.1495 [stat.ML]. URL: http: //dx.doi.org/10.1214/14-AOS1285 (cited on page 6).   \n[SL17] Martin R. Schuster and Maciej Liskiewicz. New Abilities and Limitations of Spectral Graph Bisection. In Kirk Pruhs and Christian Sohler, editors, 25th Annual European Symposium on Algorithms (ESA 2017), volume 87 of Leibniz International Proceedings in Informatics (LIPIcs), 66:1\u201366:15, Dagstuhl, Germany. Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik, 2017. ISBN: 978-3-95977-049-1. DOI: 10.4230/LIPIcs.ESA.2017.66. URL: https://drops.dagstuhl.de/ entities/document/10.4230/LIPIcs.ESA.2017.66 (cited on page 10).   \n[Ver18] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018 (cited on page 14).   \n[VA18] Aravindan Vijayaraghavan and Pranjal Awasthi. Clustering semi-random mixtures of Gaussians. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5055\u20135064. PMLR, July 2018. arXiv: 1711 . 08841 [cs.DS]. URL: https : / / proceedings . mlr . press / v80 / vijayaraghavan18a.html (cited on page 10). ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[VGO+20] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020. DOI: 10.1038/s41592-019- 0686-2 (cited on page 8). ", "page_idx": 13}, {"type": "text", "text": "[Von07] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17:395\u2013416, 2007. arXiv: 0711.0189 [cs.DS] (cited on pages 2, 3, 6). ", "page_idx": 13}, {"type": "text", "text": "[YCOM24] Yuepeng Yang, Antares Chen, Lorenzo Orecchia, and Cong Ma. Top- $\\mathcal{K}$ ranking with a monotone adversary. In Shipra Agrawal and Aaron Roth, editors, Proceedings of Thirty Seventh Conference on Learning Theory, volume 247 of Proceedings of Machine Learning Research, pages 5123\u20135162. PMLR, July 2024. arXiv: 2402.07445 [stat.ML]. URL: https://proceedings.mlr.press/v247/ yang24b.html (cited on page 2). ", "page_idx": 13}, {"type": "text", "text": "A Deferred proofs ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we build the tools we need to prove Theorem 1, Theorem 2, andTheorem 3. Throughout, it will be helpful to refer to the overview (Section 3) for a proof roadmap. ", "page_idx": 13}, {"type": "text", "text": "Notation in the proofs. In all proofs, we adopt the notation used in the technical overview (Section 3). Additionally, for a vertex $v\\in V$ , let $P(v)$ denote the community that $v$ belongs to. ", "page_idx": 13}, {"type": "text", "text": "A.1 Concentration inequalities ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our proof strategy for Theorem 1 and Theorem 2 is to appeal to Lemma A.23, which guarantees strong consistency provided that $d[v]-\\lambda_{2}>0$ , $d_{\\mathsf{i n}}[v]>d_{\\mathsf{o u t}}[v]$ , and $|\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\rangle|\\leq(\\pmb{d}_{\\mathsf{i n}}[v]-$ $d_{\\mathrm{out}}[\\bar{v}])/\\sqrt{n}$ for all vertices $v$ . Proving that the first two conditions hold is relatively easy. In the setting of Theorem 1, it essentially follows from concentration of the degrees, which is proved in Appendix A.2. In the setting of Theorem 2, it follows from the assumptions of the Theorem. Proving that the third condition holds is the main technical challenge. ", "page_idx": 13}, {"type": "text", "text": "For all three parts, our proofs rely on several auxiliary concentration results. We prove these in Appendix A.3 and Appendix A.4. ", "page_idx": 13}, {"type": "text", "text": "We extensively use the following variants of Bernstein\u2019s Inequality, which can be derived from [Ver18, Theorem 2.8.4]. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1. Let $\\textstyle X=\\sum_{i=1}^{m}X_{i}$ , where $X_{i}=1$ with probability $p_{i}$ and $X_{i}=0$ with probability $1-p_{i}$ and all the $X_{i}$ ar e independent. Let $\\mu=\\operatorname{\\mathbb{E}}\\left[X\\right]$ . Then, for all $t>0$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathsf{P r}\\left[\\left|X-\\mu\\right|\\geq t\\right]\\leq2\\mathsf{e x p}\\left(-\\operatorname*{min}\\left\\{\\frac{t^{2}}{4\\sum_{i=1}^{m}p_{i}(1-p_{i})},\\frac{3t}{4}\\right\\}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From this, we get the following very useful corollary. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. Let $\\textstyle X=\\sum_{i=1}^{m}X_{i}$ , where $X_{i}=1$ with probability $p_{i}$ and $X_{i}=0$ with probability $1-p_{i}$ and all the $X_{i}$ ar e independent. Let $\\mu=\\operatorname{\\mathbb{E}}\\left[X\\right]$ . Then, for all $t>0$ , with probability at least $1-\\delta$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n|X-\\mu|\\leq\\sqrt{4\\sum_{i=1}^{m}p_{i}(1-p_{i})\\log\\left(2/\\delta\\right)}+4/3\\log\\left(2/\\delta\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "A.2 Concentration of degrees ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this Section, we give concentration statements regarding the number of internal vertices incident to each vertex and the number of crossing edges incident to each vertex. We then compare these against $\\lambda_{2}$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.3. Suppose the crossing edges are sampled identically and independently with probability $q$ . Then, for some universal constant $C>0$ , with probability at least $1-\\delta$ we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall v\\in V,\\quad|d_{\\mathrm{out}}[v]-\\mathbb{E}\\left[d_{\\mathrm{out}}[v]\\right]|\\leq C\\left(\\sqrt{n q\\log\\left(^{n/\\delta}\\right)}+\\log\\left({^n/\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.3. Choose some $\\ensuremath{v}\\in\\mathsf{\\Omega}_{V}$ . Consider the random variable $d_{\\mathrm{out}}[v]$ . Using Lemma A.2, we have that there is a constant $C\\,>\\,0$ such that with probability at least $1-\\delta/n$ one has ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|d_{\\mathsf{o u t}}[v]-\\mathbb{E}\\left[d_{\\mathsf{o u t}}[v]\\right]|\\leq C\\left(\\sqrt{4n q/2\\log\\left({^{2n}}/{^{6}}\\right)}+\\log\\left({^{2n}}/{^{6}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking a union bound over all $n$ vertices completes the proof of Lemma A.3. ", "page_idx": 14}, {"type": "text", "text": "Note that Lemma A.3 above applies in both the settings of Theorem 1 and Theorem 2. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.4. Suppose the internal edges are sampled independently with probabilities $p_{v w}$ such that $p\\leq p_{v w}\\leq\\overline{{p}}$ . Then, for some universal constant $C>0$ , with probability $\\geq1-\\delta$ we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\forall v\\in V,\\quad|d_{\\mathrm{in}}[v]-\\mathbb{E}\\left[d_{\\mathrm{in}}[v]\\right]|\\leq C\\left(\\sqrt{\\sum_{w\\in P(v)\\backslash\\{v\\}}p_{v w}(1-p_{v w})\\log\\left({n/\\delta}\\right)}+\\log\\left({n/\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.4. As before, choose some $v\\,\\in\\,V$ and consider the random variable $d_{\\mathrm{in}}[v]$ . By Lemma A.2, we have that there is a constant $C>0$ such that with probability at least $1-\\delta/n$ one has ", "page_idx": 14}, {"type": "equation", "text": "$$\n|d_{\\mathrm{in}}[v]-\\mathbb{E}\\left[d_{\\mathrm{in}}[v]\\right]|\\leq C\\left(\\sqrt{4\\sum_{w\\in P(v)\\backslash\\{v\\}}p_{v w}(1-p_{v w})\\log\\left({2^{n}}/{\\delta}\\right)}+\\log\\left({2^{n}}/{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Taking a union bound over all $n$ vertices completes the proof of Lemma A.4. ", "page_idx": 14}, {"type": "text", "text": "Combining the above two lemmas, we obtain a lower-bound on $d_{\\mathrm{in}}[v]-d_{\\mathrm{out}}[v]$ . In particular, the following lemma implies that in the setting of Theorem 1, we have $d_{\\mathrm{in}}[v]>\\bar{d_{\\mathrm{out}}}[v]$ . This will be required for applying Lemma A.23. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.5. There exists a universal constant $C>0$ such that with probability $\\geq1-\\delta,$ , in the same settings as Lemma A.3 and Lemma $A.4$ and assuming the gap condition in Theorem $^{\\,l}$ , if $p\\geq q$ , then for all $v\\in V$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{\\mathsf{i n}}[v]-d_{\\mathsf{o u t}}[v]\\geq\\frac{n(p-q)}{2}-C\\left(\\sqrt{n p\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.5. Let $v\\in V$ . First, we call Lemma A.3 with a failure probability of $\\delta/(2n)$ to conclude that ", "page_idx": 14}, {"type": "equation", "text": "$$\nd_{\\mathsf{o u t}}[v]\\leq\\frac{n q}{2}+C_{A.3}\\left(\\sqrt{\\frac{n q}{2}\\log\\left({2n^{2}}/{\\delta}\\right)}+\\log\\left({2n^{2}}/{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Next, we call Lemma A.4 with a failure probability of $\\delta/(2n)$ to conclude that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{\\mathrm{in}}[v]\\geq\\displaystyle\\sum_{w\\in P(v)\\setminus\\{v\\}}p_{v w}-C_{A.4}\\left(\\sqrt{\\displaystyle\\sum_{w\\in P(v)\\setminus\\{v\\}}p_{v w}(1-p_{v w})\\log\\left(2^{n^{2}}/\\delta\\right)}+\\log\\left(2^{n^{2}}/\\delta\\right)\\right)}\\\\ &{\\qquad\\geq\\displaystyle\\sum_{w\\in P(v)\\setminus\\{v\\}}p_{v w}-C_{A.4}\\left(\\sqrt{\\displaystyle\\sum_{w\\in P(v)\\setminus\\{v\\}}p_{v w}\\log\\left(2^{n^{2}}/\\delta\\right)}+\\log\\left(2^{n^{2}}/\\delta\\right)\\right)}\\\\ &{\\qquad\\geq\\displaystyle\\frac{n p}{2}-2C_{A.4}\\left(\\sqrt{\\displaystyle\\frac{n p}{2}\\log\\left(n^{2}/\\delta\\right)}+\\log\\left(2^{n^{2}}/\\delta\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the last line uses the fact that $x-c{\\sqrt{x}}$ is increasing in $x$ whenever $x\\ge c^{2}/4$ and $c>0$ . We subtract and conclude the proof of Lemma A.5 by a union bound. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "The following lemma will be useful for lower-bounding ${\\pmb d}[v]-\\lambda_{2}$ in Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.6. Suppose every crossing edge appears independently with probability $q$ . Then, with probability $\\geq1-\\delta,$ , for all $v\\in V$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\lambda_{2}\\leq2d_{\\mathsf{o u t}}[v]+C\\left(\\sqrt{n q\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.6. Observe that with probability at least $1\\ -\\ \\delta$ , $d_{\\mathrm{out}}[w]\\ -\\ \\mathbb{E}\\left[d_{\\mathrm{out}}[v]\\right]\\ \\ \\leq$ $\\sqrt{2n q\\log(^{2n}/\\delta)}+2\\log(2n/\\delta)$ for all $w\\in V$ by Lemma A.2. Then, for every $v\\in V$ we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac2n\\sum_{w\\in P(v)}d_{\\mathrm{out}}[w]-d_{\\mathrm{out}}[v]=\\left(\\frac2n\\sum_{w\\in P(v)}d_{\\mathrm{out}}[w]-\\mathbb{E}\\left[d_{\\mathrm{out}}[v]\\right]\\right)+\\left(\\mathbb{E}\\left[d_{\\mathrm{out}}[v]\\right]-d_{\\mathrm{out}}[v]\\right)}}\\\\ &{}&{\\le\\left|\\frac2n\\sum_{w\\in P(v)}d_{\\mathrm{out}}[w]-\\mathbb{E}\\left[d_{\\mathrm{out}}[v]\\right]\\right|+\\left|\\mathbb{E}\\left[d_{\\mathrm{out}}[v]\\right]-d_{\\mathrm{out}}[v]\\right|\\right.}\\\\ &{}&{\\le\\sqrt{2n q\\log\\left(2^{n}/\\delta\\right)}+\\sqrt{2n q\\log\\left(2^{n}/\\delta\\right)}+4\\log\\left(\\vphantom{\\left(\\left|\\frac2n\\right|\\right)}\\right.}\\\\ &{}&{\\le3\\sqrt{n q\\log\\left(n/\\delta\\right)}+10\\log\\left(n/\\delta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, by the min-max principle, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda_{2}\\leq\\sum_{(w,w^{\\prime})\\in E}\\left(u_{2}^{\\star}[w]-u_{2}^{\\star}[w^{\\prime}]\\right)^{2}=\\frac{4}{n}\\sum_{w\\in P(v)}d_{\\mathsf{o u t}}[w].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining everything, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda_{2}\\leq2\\left(\\frac{2}{n}\\sum_{w\\in P\\left(v\\right)}d_{\\mathsf{o u t}}[w]\\right)\\leq2\\left(d_{\\mathsf{o u t}}[v]+3\\sqrt{n q\\log\\left({n}/{\\delta}\\right)}+10\\log\\left({n}/{\\delta}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "completing the proof of Lemma A.6. ", "page_idx": 15}, {"type": "text", "text": "We can now lower-bound ${\\pmb d}[v]-\\lambda_{2}$ . Note that the following lower bound implies that ${d[v]>\\lambda_{2}}$ , as required by Lemma A.23. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.7. In the setting of Theorem $^{\\,l}$ , with probability $\\geq~1~-~\\delta,$ , for all $v~\\in~V$ , we have $d[v]-\\lambda_{2}>n(p-q)/4$ . ", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma A.7. Recall that the gap condition in Theorem 1 tells us that $p$ and $q$ are such that for a universal constant $C$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nn(p-q)\\geq C\\left(\\sqrt{n p\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have for all $n$ sufficiently large (specifically, $n\\geq N(\\alpha,\\delta)$ for some $N$ that is a function only of the constant $\\alpha$ , and we take $\\delta\\geq1/n^{O(1)},$ ) that with probability at least $1-\\delta$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d[v]-\\lambda_{2}=d_{\\mathsf{i n}}[v]-d_{\\mathsf{o u t}}[v]+(2d_{\\mathsf{o u t}}[v]-\\lambda_{2})}\\\\ &{\\qquad\\qquad\\geq d_{\\mathsf{i n}}[v]-d_{\\mathsf{o u t}}[v]-C_{A.6}\\left(\\sqrt{n q\\log\\left({n/\\delta}\\right)}+\\log\\left({n/\\delta}\\right)\\right)}\\\\ &{\\qquad\\qquad\\geq\\frac{n(p-q)}{2}-(C_{A.5+C_{A.6}})\\left(\\sqrt{n p\\log\\left({n/\\delta}\\right)}+\\log\\left({n/\\delta}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "so insisting ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{n(p-q)}{4}\\geq(C_{A.5+C_{A.6}})\\left(\\sqrt{n p\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right)+1\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "gives the condition required to complete the proof of Lemma A.7. ", "page_idx": 15}, {"type": "text", "text": "The following technical lemma will be useful for upper-bounding $\\|\\pmb{u}_{2}\\|_{\\infty}$ in Lemma A.22. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.8. In the setting of Theorem $^{\\,l}$ , there exists a universal constant $C$ such that with probability $\\geq1-\\delta,$ , for all $v\\in V$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{n\\overline{{{p}}}+\\log\\left(\\scriptscriptstyle{n}/\\delta\\right)}{d[v]-\\lambda_{2}}\\le4\\alpha+C.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma A.8. By Lemma A.7, we have with probability $\\geq1-\\delta$ that for all $v\\in V$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\pmb d}[v]-\\lambda_{2}\\geq\\frac{n(p-q)}{4}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{n\\overline{{{p}}}+\\log{\\binom{n/\\delta}{\\delta}}}{d[v]-\\lambda_{2}}\\leq\\frac{4(n\\overline{{{p}}}+\\log{\\binom{n/\\delta}{\\delta}})}{n(p-q)}=\\frac{4\\overline{{{p}}}}{p-q}+\\frac{4\\log{\\binom{n/\\delta}{n(p-q)}}}{n(p-q)}\\leq4\\alpha+C.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This completes the proof of Lemma A.8. ", "page_idx": 16}, {"type": "text", "text": "A.3 Concentration of Laplacian and eigenvalue perturbations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the matrix concentration lemmas, we need a result due to Le, Levina, and Vershynin [LLV17].   \nWe reproduce it below. ", "page_idx": 16}, {"type": "text", "text": "Lemma A.9 ([LLV17, Theorem 2.1]). Consider a random graph from the model $G(n,\\{p_{i j}\\})$ . Let $d=\\operatorname*{max}_{i j}\\,n p_{i j}$ . For any $r\\geq1$ , the following holds with probability at least $1-n^{-r}$ for a universal constant $C$ . Consider any subset consisting of $10n/d$ vertices, and reduce the weights of the edges incident to those vertices in an arbitrary way. Let $d^{\\prime}$ be the maximal degree of the resulting graph. Then, the adjacency matrix $\\mathbf{A}^{\\prime}$ of the new weighted graph satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|\\mathbf{A}^{\\prime}-\\mathbb{E}\\left[\\mathbf{A}\\right]\\|_{\\mathrm{op}}\\leq C r^{3/2}\\left(\\sqrt{d}+\\sqrt{d^{\\prime}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Moreover, the same holds for $d^{\\prime}$ being the maximal $\\ell_{2}$ norm of the rows of $\\mathbf{A}^{\\prime}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma A.10. Let L be a Laplacian sampled from the nonhomogeneous Erd\u02ddos-R\u00e9nyi model where each edge $(i,j)$ is present independently with probability $p_{i j}$ . Then, there exists a universal constant $C$ such that for all $n$ sufficiently large, with probability $\\geq1-\\delta$ for any $\\delta\\geq n^{-10}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\mathbf{L}-\\mathbb{E}\\left[\\mathbf{L}\\right]\\right\\|_{\\mathrm{op}}\\leq C\\left(\\sqrt{n\\sum_{\\scriptstyle(i,j):\\ p_{i j}\\neq1}p_{i j}\\log\\left({n/\\delta}\\right)}+\\log\\left({n/\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma $A.I O$ . Without loss of generality, for all $p_{i j}$ that are 1, reset their probabilities to 0. To see that this is valid, let $\\mathbf{L}^{\\prime}$ be a Laplacian sampled from this modified distribution and notice that $\\mathbf{L}^{\\prime}-\\mathbb{E}\\left[\\mathbf{L}^{\\prime}\\right]=\\mathbf{L}-\\mathbb{E}$ [L]. ", "page_idx": 16}, {"type": "text", "text": "By Lemma A.9 and Lemma A.2, we have with probability $\\geq1-\\delta/2$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{A}-\\mathbb{E}\\left[\\mathbf{A}\\right]\\|_{\\mathrm{op}}\\leq200C_{A.9}\\sqrt{2n\\underset{i j}{\\operatorname*{max}}\\,p_{i j}+C_{A.2}\\left(\\sqrt{n\\underset{i j}{\\operatorname*{max}}\\,p_{i j}\\log\\left(8n/\\delta\\right)}+\\log(8n/\\delta)\\right)}}\\\\ &{\\qquad\\qquad\\leq400C_{A.9}C_{A.2}\\sqrt{n\\underset{i j}{\\operatorname*{max}}\\,p_{i j}+\\log\\left(8n/\\delta\\right)}}\\\\ &{\\qquad\\qquad\\leq400C_{A.9}C_{A.2}\\left(\\sqrt{n\\underset{i j}{\\operatorname*{max}}\\,p_{i j}\\log\\left(8n/\\delta\\right)}+\\log\\left(^{8n}/\\delta\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and by Lemma A.3 and Lemma A.4, we have with probability $1-\\delta/2$ that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{D}-\\mathbb{E}\\left[\\mathbf{D}\\right]\\right\\|_{\\mathrm{op}}\\leq\\underset{v\\in V}{\\operatorname*{max}}\\left|d_{\\mathsf{o u t}}[v]-\\mathbb{E}\\left[d_{\\mathsf{o u t}}[v]\\right]\\right|+\\underset{v\\in V}{\\operatorname*{max}}\\left|d_{\\mathsf{i n}}[v]-\\mathbb{E}\\left[d_{\\mathsf{i n}}[v]\\right]\\right|}\\\\ &{\\qquad\\qquad\\leq2\\operatorname*{max}\\left\\{C_{A.3},C_{A.4}\\right\\}\\left(\\sqrt{n\\underset{i j}{\\operatorname*{max}}p_{i j}\\log\\left(2^{n}/\\delta\\right)}+\\log\\left(2^{n}/\\delta\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, observe that with probability $\\geq1-\\delta$ (following from a union bound), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathbf{L}-\\mathbb{E}\\left[\\mathbf{L}\\right]\\right\\|_{\\mathrm{op}}=\\left\\|\\mathbf{D}-\\mathbb{E}\\left[\\mathbf{D}\\right]-\\left(\\mathbf{A}-\\mathbb{E}\\left[\\mathbf{A}\\right]\\right)\\right\\|_{\\mathrm{op}}\\leq\\left\\|\\mathbf{D}-\\mathbb{E}\\left[\\mathbf{D}\\right]\\right\\|_{\\mathrm{op}}+\\left\\|\\mathbf{A}-\\mathbb{E}\\left[\\mathbf{A}\\right]\\right\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\qquad\\leq800C_{A,9}C_{A.2}\\operatorname*{max}\\left\\{C_{A.3},C_{A.4}\\right\\}\\left(\\sqrt{n\\operatorname*{max}_{i j}p_{i j}\\log\\left(\\mathrm{}^{8n/\\delta}\\right)}+\\log\\left(^{8n/\\delta}\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "completing the proof of Lemma A.10. ", "page_idx": 17}, {"type": "text", "text": "By applying the above lemma, we can show that there is a gap between $\\lambda_{3}$ and ${\\boldsymbol{\\lambda}}_{2}^{\\star}$ , which will allow us to apply Davis-Kahan style bounds. More concretely, Lemma A.11 and Lemma A.12, together with Lemma A.16, show that $\\lVert u_{2}-u_{2}^{\\star}\\rVert_{2}$ is small. This will be useful for proving that in the context for Theorem 1, the condition $|\\langle a_{v},u_{2}^{\\star}-\\overline{{\\boldsymbol{u}}}_{2}\\rangle|\\leq(d_{\\mathsf{i n}}[v]-d_{\\mathsf{o u t}}[v])/\\sqrt{n}$ in Lemma A.23 is satisfied. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.11. In the setting of Theorem $^{\\,l}$ , there exists a universal constant $C$ such that the following holds. ", "page_idx": 17}, {"type": "text", "text": "Let p and q be such that we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nn(p-q)\\geq C\\left(\\sqrt{n\\bar{p}\\log{(^{n}\\!/\\!\\delta)}}+\\log({^{n}\\!/\\!\\delta})\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, for any $\\delta\\geq n^{-10}$ , with probability $\\geq1-\\delta$ , we have $\\lambda_{3}-\\lambda_{2}^{\\star}\\geq n(p-q)/4$ . ", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma A.11. By Weyl\u2019s inequality and Lemma A.10, we have with probability $\\geq1-\\delta$ that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda_{3}-\\lambda_{2}^{\\star}\\geq\\lambda_{3}^{\\star}-\\lambda_{2}^{\\star}-\\|\\mathbf{L}-\\mathbf{L}^{\\star}\\|_{\\mathrm{op}}\\geq\\frac{n(p-q)}{2}-C_{A.10}\\left(\\sqrt{n\\bar{p}\\log{(n/\\delta)}}+\\log(n/\\delta)\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $C\\geq4C_{A.10}$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{n(p-q)}{4}\\geq C_{A.10}\\left(\\sqrt{n\\bar{p}\\log{(^{n}\\!/\\!\\delta)}}+\\log(^{n}\\!/\\!\\delta)\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Subtracting completes the proof of Lemma A.11. ", "page_idx": 17}, {"type": "text", "text": "Next, we bound $\\lVert\\mathbf{E}u_{2}^{\\star}\\rVert_{2}$ , which we will need in order to apply our Davis-Kahan style bound in Lemma A.16. We remark that Lemma A.12 below holds both in the setting of Theorem 1 and of Theorem 2. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.12. Suppose each crossing edge in our graph appears independently with probability $q$ . There exists a universal constant $C$ such that for all n sufficiently large, with probability $\\geq1-\\delta$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mathbf{E}u_{2}^{\\star}\\|_{2}\\leq C\\left(\\frac{\\log{(1/\\delta)}}{\\log{n}}\\right)^{3/2}\\left(\\sqrt{n q}+(n q\\log{(n/\\delta)})^{1/4}+\\sqrt{\\log{(n/\\delta)}}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma A.12. Observe that $|\\mathbf{E}u_{2}^{\\star}|=2\\left|d_{\\mathrm{out}}-\\mathbb{E}\\left[d_{\\mathrm{out}}\\right]\\right|/\\sqrt{n}$ . By Lemma A.3, for all $v\\in$ $V$ , with probability $\\geq1-\\delta/2$ , we have $\\begin{array}{r}{d_{\\mathsf{o u t}}[v]\\leq n q/2+C_{A,3}\\left(\\sqrt{n q\\cdot\\log\\left(2n/\\delta\\right)}+\\log\\left({2n}/{\\delta}\\right)\\right)}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "So, if we let $\\mathbf{A}_{\\mathsf{o u t}}$ and $\\mathbf{A}_{\\mathrm{out}}^{\\star}$ denote the adjacency matrices consisting only of the crossing edges and the expected value of that, respectively, then invoking Lemma A.9, with probability $\\geq1-\\delta$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{E}u_{2}^{\\star}\\|_{2}=\\frac{2\\,\\|d_{\\mathrm{out}}-\\mathbb{E}\\,[d_{\\mathrm{out}}]\\|_{2}}{\\sqrt{n}}=\\frac{2\\,\\|\\left(\\mathbf{A}_{\\mathrm{out}}-\\mathbf{A}_{\\mathrm{out}}^{\\star}\\right)\\,\\mathbb{1}\\|_{2}}{\\sqrt{n}}\\leq2\\,\\|\\mathbf{A}_{\\mathrm{out}}-\\mathbf{A}_{\\mathrm{out}}^{\\star}\\|_{\\mathrm{op}}}\\\\ &{\\qquad\\qquad\\leq2C_{A,9}\\left(\\frac{\\log{(2/\\delta)}}{\\log{n}}\\right)^{3/2}\\left(\\sqrt{\\frac{n q}{2}}+\\sqrt{C_{A,3}}\\sqrt{n q+\\sqrt{n q\\log{(2n/\\delta)}}+\\log{(2n/\\delta)}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "completing the proof of Lemma A.12. ", "page_idx": 17}, {"type": "text", "text": "Finally, we apply Lemma A.9 in order to bound bound $\\lVert\\pmb{a}_{v}-\\pmb{a}_{v}^{\\star}\\rVert_{2}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma A.13. In the setting of Theorem $^{\\,l}$ , with probability $\\geq1-\\delta$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|a_{v}-a_{v}^{\\star}\\|_{2}\\leq C\\left(\\frac{\\log{(1/\\delta)}}{\\log{n}}\\right)^{3/2}\\left(\\sqrt{n\\bar{p}}+(n\\bar{p}\\log{(n/\\delta)})^{1/4}+\\sqrt{\\log{(n/\\delta)}}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma A.13. We use a similar proof to that of Lemma A.12. Indeed, invoke Lemma A.9 (observe that we can set $p_{i j}$ for the deterministic internal edges to 0 as they do not affect $\\mathbf{A}-\\mathbb{E}\\left[\\mathbf{A}\\right])$ and notice that ", "page_idx": 18}, {"type": "equation", "text": "$$\n|a_{v}-a_{v}^{\\star}|\\|_{2}\\leq\\|\\mathbf{A}-\\mathbf{A}^{\\star}\\|_{\\mathrm{op}}\\leq C_{A,9}\\left(\\frac{\\log{(2/\\delta)}}{\\log{n}}\\right)^{3/2}\\left(\\sqrt{n\\overline{{p}}}+(n\\overline{{p}}\\log{(2n/\\delta)})^{1/4}+\\sqrt{\\log{(2n/\\delta)}}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used $d^{\\prime}\\leq n(\\overline{{p}}+q)/2+2\\operatorname*{max}\\left\\{C_{A.3},C_{A.4}\\right\\}\\left(\\sqrt{n\\overline{{p}}\\log\\left(2n/\\delta\\right)}+\\log\\left(2n/\\delta\\right)\\right)$ from combining Lemma A.3 and Lemma A.4. This completes the proof of Lemma A.13. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "A.4 Eigenvector perturbations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this Appendix, we give our Euclidean norm eigenvector perturbation bounds. ", "page_idx": 18}, {"type": "text", "text": "First, we verify that $\\pmb{u}_{2}^{\\star}$ is indeed the second eigenvector of $\\mathbf{L}^{\\star}$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma A.14. In the setting of Theorem $^{\\,l}$ , we have $\\mathbf{L}^{\\star}\\pmb{u}_{2}^{\\star}=\\lambda_{2}(\\mathbf{L}^{\\star})\\pmb{u}_{2}^{\\star}=n q\\pmb{u}_{2}^{\\star}$ , where $\\mathbf{L}^{\\star}=\\mathbb{E}$ [L]. In the setting of Theorem 2, we have $\\mathbf{L}^{\\star}\\pmb{u}_{2}^{\\star}=\\lambda_{2}(\\mathbf{L}^{\\star})\\pmb{u}_{2}^{\\star}=n q\\pmb{u}_{2}^{\\star}$ , where $\\mathbf{L}^{\\star}$ denotes the Laplacian matrix that agrees with L on all internal edges and agrees with $\\bar{\\mathbb{E}}\\left[\\mathbf{L}\\right]$ on all crossing edges. ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma A.14. In both cases, one can check that $\\pmb{u}_{2}^{\\star}$ is an eigenvector of $\\mathbf{L}^{\\star}$ with eigenvalue nq: for any $v\\in P_{2}$ (i.e. ${\\pmb u}_{2}^{\\star}[v]=-1/\\sqrt{n}$ without loss of generality), one has ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathbf{L}^{\\star}u_{2}^{\\star})_{v}=\\frac{1}{\\sqrt{n}}\\left(-(d_{\\mathsf{i n}}[v]+n q/2)-\\sum_{w\\in P_{1}:\\{v,w\\}\\in E}(-1)+\\sum_{w\\in P_{2}}(-q)\\right)=-\\frac{n q}{\\sqrt{n}}=n q\\cdot u_{2}^{\\star}[v]\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By virtue of the above observations, it suffices to argue that $n q<\\lambda_{3}(\\mathbf{L}^{\\star})\\leq\\cdot\\cdot\\leq\\lambda_{n}(\\mathbf{L}^{\\star})$ . ", "page_idx": 18}, {"type": "text", "text": "In the setting of Theorem 1, we claim $\\begin{array}{r}{\\lambda_{3}^{\\star}\\geq\\frac{n(p+q)}{2}>n q}\\end{array}$ . This is because because $p_{v w}\\geq p$ , which implies that if we consider $\\mathbf{L}_{1}^{\\star}$ to be the expected Laplacian for SSBM $(n,p,q)$ and $\\mathbf{L}_{2}^{\\star}$ to be the expected Laplacian for $\\mathsf{N S S B M}(n,p,\\overline{{p}},q)$ , then $\\mathbf{L}_{2}^{\\star}\\succeq\\mathbf{L}_{1}^{\\star}$ .. ", "page_idx": 18}, {"type": "text", "text": "In the setting of Theorem 2, we have $\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\widehat{\\mathbf{L}})>n q$ , by the theorem assumption. Since $\\mathbf{L}^{\\star}$ is obtained from $\\widehat{\\bf L}$ by adding the adversari al edges,  we have $\\bar{\\lambda_{i}}(\\mathbf{L}^{\\star})\\geq\\lambda_{i}(\\widehat\\mathbf{L})$ for all $i$ . In particular, we have $\\lambda_{3}(\\mathbf{L}^{\\star})\\,\\geq\\,\\dot{\\lambda}_{3}(\\widehat\\mathbf{L})\\,=\\,\\lambda_{2}(\\widehat\\mathbf{L})+(\\lambda_{3}(\\widehat\\mathbf{L})\\,\\overset{\\,\\cdot\\,}{-}\\,\\lambda_{2}(\\widehat\\mathbf{L}))\\,>\\,n q$ , where the  last inequality is using the fact $\\lambda_{2}(\\widehat{\\mathbf{L}})\\geq0$ . Th erefore, $n q$ must be  the secon d eigenvalue of $\\mathbf{L}^{\\star}$ , completing the proof of Lemma A.14 . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Next, we prove a general Davis-Kahan style bound. ", "page_idx": 18}, {"type": "text", "text": "Lemma A.15. Let L and $\\widehat{\\bf L}$ be two weighted Laplacian matrices. Let $\\pmb{u}_{2}$ and $\\widehat{\\pmb{u}_{2}}$ be the second eigenvectors of $\\mathbf{L}$ andL, re s pectively. Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|u_{2}-\\widehat{u_{2}}\\|_{2}\\leq\\sqrt{2}\\cdot\\operatorname*{min}\\left\\{\\frac{\\left\\|(\\widehat{\\mathbf{L}}-\\mathbf{L})u_{2}\\right\\|_{2}}{\\left|\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})\\right|},\\frac{\\left\\|(\\widehat{\\mathbf{L}}-\\mathbf{L})\\widehat{u_{2}}\\right\\|_{2}}{\\left|\\lambda_{3}(\\mathbf{L})-\\lambda_{2}(\\widehat{\\mathbf{L}})\\right|}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma A.15. One can get this sort of guarantee from variants of the Davis-Kahan theorem, but it is more illuminating to write an eigenvalue decomposition and observe it from there. Without ", "page_idx": 18}, {"type": "text", "text": "loss of generality, assume that $\\langle\\widehat{\\mathbf{u}_{2}},\\mathbf{u}_{2}\\rangle\\ge0$ (indeed, otherwise we can always negate $\\widehat{\\pmb{u}_{2}}$ if this is not the case). Notice that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|(\\widehat{\\mathbf{L}}-\\mathbf{L})u_{2}\\right\\|_{2}^{2}=\\left\\|\\left(\\widehat{\\mathbf{L}}-\\lambda_{2}(\\mathbf{L})\\mathbf{I}\\right)u_{2}\\right\\|_{2}^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\left(\\lambda_{2}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})\\right)^{2}\\left\\langle\\widehat{u_{2}},u_{2}\\right\\rangle^{2}+\\displaystyle\\sum_{i=3}^{n}\\left(\\lambda_{i}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})\\right)^{2}\\left\\langle\\widehat{u_{i}},u_{2}\\right\\rangle^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\displaystyle\\sum_{i=3}^{n}\\left(\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})\\right)^{2}\\left\\langle\\widehat{u_{i}},u_{2}\\right\\rangle^{2}=\\left(\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})\\right)^{2}\\left(1-\\left\\langle\\widehat{u_{2}},u_{2}\\right\\rangle^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which rearranges to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle\\widehat{\\pmb{u}_{2}},\\pmb{u}_{2}\\rangle^{2}\\geq1-\\left(\\frac{\\left\\|(\\widehat{\\mathbf{L}}-\\mathbf{L})\\pmb{u}_{2}\\right\\|_{2}}{\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now, if $\\left\\|(\\widehat{\\mathbf{L}}-\\mathbf{L})\\mathbf{\\boldsymbol{u}}_{2}\\right\\|_{2}\\geq|\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})|$ , then the condition $\\begin{array}{r}{\\|u_{2}-\\widehat{\\pmb{u}_{2}}\\|_{2}\\leq\\sqrt{2}\\cdot\\frac{\\left\\|(\\widehat{\\bf L}-{\\bf L}){\\pmb u}_{2}\\right\\|_{2}}{\\left|\\lambda_{3}(\\widehat{\\bf L})-\\lambda_{2}({\\bf L})\\right|}}\\end{array}$ is trivially satisfied, since $\\|\\pmb{u}_{2}-\\widehat{\\pmb{u}_{2}}\\|_{2}\\leq\\sqrt{2-2\\left\\langle\\widehat{\\pmb{u}_{2}},\\pmb{u}_{2}\\right\\rangle}\\leq\\sqrt{2}$ . Otherwise, taking t he s quare ro ots of both sides, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle\\widehat{\\pmb{u}_{2}},\\pmb{u}_{2}\\rangle\\geq\\sqrt{1-\\left(\\frac{\\left\\|(\\widehat{\\bf L}-{\\bf L})\\pmb{u}_{2}\\right\\|_{2}}{\\lambda_{3}(\\widehat{\\bf L})-\\lambda_{2}({\\bf L})}\\right)^{2}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{u_{2}}-u_{2}\\right\\|_{2}^{2}=2-2\\left\\langle\\widehat{u_{2}},u_{2}\\right\\rangle\\leq2-2\\sqrt{1-\\left(\\frac{\\left\\|(\\widehat{\\mathbf{L}}-\\mathbf{L})u_{2}\\right\\|_{2}}{\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})}\\right)^{2}}\\leq2\\cdot\\left(\\frac{\\left\\|(\\widehat{\\mathbf{L}}-\\mathbf{L})u_{2}\\right\\|_{2}}{\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\mathbf{L})}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking the square root of both sides and repeating this argument by exchanging the roles of $\\mathbf{L}$ and $\\widehat{\\bf L}$ yields the statement of Lemma A.15. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "This immediately implies the following upper-bound on $\\lVert\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}\\rVert_{2}$ . We will use it repeatedly, both in Theorem 1 and Theorem 2. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.16. We have ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\pmb{u}_{2}-\\pmb{u}_{2}^{\\star}\\|_{2}\\leq\\sqrt{2}\\cdot\\frac{\\|\\mathbf{E}\\pmb{u}_{2}^{\\star}\\|_{2}}{|\\lambda_{3}-\\lambda_{2}^{\\star}|}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Lemma A.16 immediately follows from Lemma A.15 by letting $\\widehat{\\mathbf{L}}=\\mathbf{L}^{\\star}$ . ", "page_idx": 19}, {"type": "text", "text": "Combining with Lemma A.11 and Lemma A.12, we can now upper-bound $\\lVert\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}\\rVert_{2}$ in the setting of Theorem 1. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.17. In the setting of Theorem $^{\\,l}$ , there exists a universal constant $C$ such that, for $\\delta\\geq$ $3n^{-10}$ , with probability $\\geq1-\\delta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\vert\\vert\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}\\vert\\vert_{2}\\leq\\frac{C}{\\sqrt{\\log\\left({n/\\delta}\\right)}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma A.17. Using Lemma A.16, Lemma A.11 and Lemma A.12, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|u_{2}-u_{2}^{\\star}\\|_{2}\\leq\\frac{400\\sqrt{2}C_{A.12}\\left(\\sqrt{n q}+(n q\\log{(n/\\delta)})^{1/4}+\\sqrt{\\log{(n/\\delta)}}\\right)}{n(p-q)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At this point, it is enough to show that there exists a universal constant $C$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\nC n(p-q)\\geq4\\sqrt{2}C_{A.12}\\left(\\sqrt{n q\\log{(^{n}\\!/\\!\\delta)}}+(n q)^{1/4}\\left(\\log{(^{n}\\!/\\!\\delta)}\\right)^{3/4}+\\log{(^{n}\\!/\\!\\delta)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To see this, note that for any two nonnegative real numbers we have $2a^{1/4}b^{1/4}\\leq\\sqrt{b}+\\sqrt{a}$ , which implies $2a^{1/4}b^{3/4}\\leq b+{\\sqrt{a b}}$ . Let $a=n q$ and $b=\\log\\left(n/\\delta\\right)$ , and we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad4\\sqrt{2}C_{A.12}\\left(\\sqrt{n q\\log\\left(\\mathfrak{n}/\\delta\\right)}+\\left(n q\\right)^{1/4}\\left(\\log\\left(\\mathfrak{n}/\\delta\\right)\\right)^{3/4}+\\log\\left(\\mathfrak{n}/\\delta\\right)\\right)}\\\\ &{\\leq8\\sqrt{2}C_{A.12}\\left(\\sqrt{n q\\log\\left(\\mathfrak{n}/\\delta\\right)}+\\log\\left(\\mathfrak{n}/\\delta\\right)\\right)}\\\\ &{\\leq8\\sqrt{2}C_{A.12}\\left(\\sqrt{n\\overline{{p}}\\log\\left(\\mathfrak{n}/\\delta\\right)}+\\log\\left(\\mathfrak{n}/\\delta\\right)\\right)\\leq\\mathfrak{n}(p-q),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows from the assumption we gave in Theorem 1. We therefore conclude the proof of Lemma A.17. ", "page_idx": 20}, {"type": "text", "text": "Next, we prove $\\ell_{1}$ norm concentration for the rows of $\\mathbf{A}$ and for the rows of $\\mathbf{L}$ in the setting of Theorem 1. We will use this in Lemma A.19, where we will bound $\\left\\|\\pmb{u}_{2}^{(v)}-\\pmb{u}_{2}\\right\\|_{2}$ . Here u2 $u_{2}^{(v)}$ denotes the second eigenvector of the leave-one-out Laplacian $\\mathbf{L}^{\\left(v\\right)}$ . ", "page_idx": 20}, {"type": "text", "text": "Lemma A.18. In the setting of Theorem $^{\\,l}$ , there exists a universal constant $C$ such that with probability $\\geq1-\\delta$ , for all $v\\in V$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pmb{a}_{v}-\\pmb{a}_{v}^{\\star}\\|_{1}\\leq C\\left(n\\overline{{p}}+\\sqrt{n\\overline{{p}}\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right)}\\\\ {\\|\\pmb{l}_{v}-\\mathbb{E}\\left[\\pmb{l}_{v}\\right]\\|_{1}\\leq C\\left(n\\overline{{p}}+\\sqrt{n\\overline{{p}}\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma A.18. It is easy to see that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|l_{v}-\\mathbb{E}\\left[l_{v}\\right]\\|_{1}=|d[v]-\\mathbb{E}\\left[d[v]\\right]|+\\|a_{v}-a_{v}^{\\star}\\|_{1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Let us consider the second term above. By Lemma A.4 and Lemma A.3, we have with probability $\\geq1-\\delta/2$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|a_{v}-a_{v}^{\\star}\\|_{1}\\leq\\|a_{v}\\|_{1}+\\|a_{v}^{\\star}\\|_{1}}\\\\ &{\\qquad\\qquad\\leq2\\left(\\frac{n\\overline{{p}}}{2}+\\operatorname*{max}\\left\\{C_{A.3},C_{A.4}\\right\\}\\left(\\sqrt{n\\overline{{p}}\\log\\left(4n/\\delta\\right)}+\\log\\left(4n/\\delta\\right)\\right)\\right)+n\\overline{{p}}}\\\\ &{\\qquad\\qquad=2n\\overline{{p}}+2\\operatorname*{max}\\left\\{C_{A.3},C_{A.4}\\right\\}\\left(\\sqrt{n\\overline{{p}}\\log\\left(2/\\delta\\right)}+\\log\\left(4n/\\delta\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Finally, by Lemma A.3 and Lemma A.4, we have with probability $1-\\delta/2$ that for all $v\\in V$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|d[v]-\\mathbb{E}\\left[d[v]\\right]|\\leq\\operatorname*{max}_{v\\in V}\\left|d_{\\mathsf{o u t}}[v]-\\mathbb{E}\\left[d_{\\mathsf{o u t}}[v]\\right]\\right|+\\operatorname*{max}_{v\\in V}\\left|d_{\\mathsf{i n}}[v]-\\mathbb{E}\\left[d_{\\mathsf{i n}}[v]\\right]\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\operatorname*{max}\\left\\{C_{A.3},C_{A.4}\\right\\}\\left(\\sqrt{n\\overline{{p}}\\log\\left({4n/\\delta}\\right)}+\\log\\left({4n/\\delta}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Adding everything up means that with probability $\\geq1-\\delta$ , for all $v\\in V$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|l_{v}-\\mathbb{E}\\left[l_{v}\\right]\\|_{1}\\le2n\\overline{{p}}+4\\operatorname*{max}\\left\\{C_{A.4},C_{A.3}\\right\\}\\left(\\sqrt{n\\overline{{p}}\\log\\left(4n/\\delta\\right)}+\\log\\left(4n/\\delta\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which completes the proof of Lemma A.18. ", "page_idx": 20}, {"type": "text", "text": "Having established Lemma A.18, we can now upper-bound ${\\left\\|\\pmb{u}_{2}^{(v)}-\\pmb{u}_{2}\\right\\|}_{2}$ ", "page_idx": 20}, {"type": "text", "text": "Lemma A.19. In the setting of Theorem $^{\\,l}$ , with probability $\\geq1-\\delta,$ , for all $v\\in V$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|u_{2}^{(v)}-u_{2}\\right\\|_{2}\\leq\\|u_{2}\\|_{\\infty}\\cdot\\frac{C\\left(\\overline{{p}}+\\sqrt{\\overline{{p}}\\log\\left(^{n/\\delta}\\right)/n}+\\log\\left(^{n/\\delta}\\right)/n\\right)}{p-q}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma A.19. Recall that the gap condition in Theorem 1 means that $p$ and $q$ are such that for a universal constant $C$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\nn(p-q)\\geq C\\left(\\sqrt{n\\overline{{p}}\\log{(^{n/\\delta})}}+\\log{(^{n/\\delta})}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To appeal to Lemma A.15, we need to understand the entries of the matrix ${\\bf L}-{\\bf L}^{(v)}$ . It is easy to see that this matrix only has nonzero entries on the diagonal and in the $\\boldsymbol{v}$ th row and column. There, the $v$ th row and column of ${\\bf L}-{\\bf L}^{(v)}$ are exactly equal to those of ${\\bf L}-{\\bf L}^{\\star}$ . Moreover, the $w\\ne v\\mathrm{th}$ diagonal entry of ${\\bf L}-{\\bf L}^{(v)}$ is exactly $\\mathbb{1}\\left\\{(v,w)\\in E\\right\\}-p_{v w}$ . ", "page_idx": 21}, {"type": "text", "text": "Hence, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\left(\\mathbf{L}-\\mathbf{L}^{(\\nu)}\\right)u_{z}\\right\\|_{2}}\\\\ &{=\\left(\\displaystyle\\sum_{n=1}^{\\infty}\\left\\langle\\left(\\mathbf{L}-\\mathbf{L}^{(\\nu)}\\right)_{w},u_{z}\\right\\rangle^{2}\\right)^{1/2}}\\\\ &{=\\left(\\left((\\mathbf{L}-\\mathbf{L}^{\\nu})_{\\nu},\\,u_{2}\\right)^{2}+\\displaystyle\\sum_{n\\neq\\nu}\\left(\\left(a_{n}|w|-p_{n w}\\right)u_{2}|w|-\\left(a_{\\nu}|w|-p_{n w}\\right)u_{2}|\\nu\\right)^{2}\\right)^{1/2}}\\\\ &{\\leq|\\left((\\mathbf{L}-\\mathbf{L}^{\\nu})_{\\nu},\\,u_{2}\\right)|+\\left(\\displaystyle\\sum_{n\\neq\\nu}\\left(\\left(a_{n}|w|-p_{n w}\\right)u_{2}|w|-\\left(a_{\\nu}|w|-p_{n w}\\right)u_{2}|\\nu\\right)^{2}\\right)^{1/2}}\\\\ &{\\leq\\left(\\|t_{\\nu}-\\mathbb{L}|_{\\nu}\\right)\\|_{1}+2\\|a_{\\nu}-a_{\\nu}^{\\alpha}\\|_{2}\\cdot\\|u_{2}\\|_{\\infty}}\\\\ &{\\leq\\left(\\|t_{\\nu}-\\mathbb{L}[t_{\\nu}]\\|_{1}+2\\|a_{\\nu}-a_{\\nu}^{\\alpha}\\|_{2}\\right)\\cdot\\|u_{2}\\|_{\\infty}}\\\\ &{\\leq\\|(t_{\\nu}-\\mathbb{L}[t_{\\nu}]\\|_{1}+2\\|a_{\\nu}-a_{\\nu}^{\\alpha}\\|_{2})\\cdot\\|u_{2}\\|_{\\infty}}\\\\ &{\\leq\\|a_{\\nu}\\|_{\\infty}\\cdot3C_{A,\\nu}(\\,n\\neq\\sqrt{n}w)(\\nu(\\nu/\\delta)+\\log(\\nu/\\delta))\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, let $C\\geq8C_{A.10}$ . Using Lemma A.10 to understand the concentration of sampling the graph except edges incident to $v$ , along with Weyl\u2019s inequality, we have with probability $\\geq1-\\delta$ that for all $v\\in V$ and for all $n$ sufficiently large, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\lambda_{3}^{\\left(v\\right)}-\\lambda_{2}\\right|\\geq\\left(\\lambda_{3}^{\\left(v\\right)}-\\lambda_{3}^{\\star}\\right)-\\left(\\lambda_{2}-\\lambda_{2}^{\\star}\\right)+\\left(\\lambda_{3}^{\\star}-\\lambda_{2}^{\\star}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\geq-2\\left(C_{A.10}\\sqrt{n\\overline{{p}}\\log\\left({n/\\delta}\\right)}+\\log\\left({n/\\delta}\\right)\\right)+\\frac{n(p-q)}{2}\\geq\\frac{n(p-q)}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, using Lemma A.15, we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|u_{2}^{(v)}-u_{2}\\right\\|_{2}\\leq\\frac{\\left\\|\\left(\\mathbf{L}-\\mathbf{L}^{\\left(v\\right)}\\right)u_{2}\\right\\|_{2}}{\\left|\\lambda_{3}^{\\left(v\\right)}-\\lambda_{2}\\right|}\\leq\\left\\|u_{2}\\right\\|_{\\infty}\\cdot\\frac{12C_{A,18}\\displaystyle\\left(n\\overline{{p}}+\\sqrt{n\\overline{{p}}\\log\\left(n/\\delta\\right)}+\\log\\left(n/\\delta\\right)\\right)}{n\\left(p-q\\right)}}\\\\ &{\\qquad\\qquad\\leq\\left\\|u_{2}\\right\\|_{\\infty}\\cdot\\frac{12C_{A,18}\\displaystyle\\left(\\overline{{p}}+\\sqrt{\\overline{{p}}\\log\\left(n/\\delta\\right)/n}+\\log\\left(n/\\delta\\right)/n\\right)}{p-q},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "completing the proof of Lemma A.19. ", "page_idx": 21}, {"type": "text", "text": "A.5 Leave-one-out and bootstrap ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The main goal of this section is to establish an upper-bound on $|\\langle\\pmb{a}_{v}-\\pmb{a}_{v}^{\\star},\\pmb{u}_{2}-\\pmb{u}_{2}^{\\star}\\rangle|$ in the setting of Theorem 1. To this end, we will need the following concentration inequality from [AFWZ20]. ", "page_idx": 21}, {"type": "text", "text": "Lemma A.20 (Lemma 7 from [AFWZ20]). Let $\\pmb{w}\\,\\in\\,\\mathbb{R}^{n}$ and $X_{i}\\,\\sim\\,{\\mathsf{B e r}}(p_{i})$ . Let $p\\geq p_{i}$ for all $i\\in[n]$ . Let $X\\in\\mathbb{R}^{n}$ be the vector formed by stacking the $X_{i}$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathsf{P r}\\left[|\\langle w,X-\\mathbb{E}\\left[X\\right]\\rangle|\\ge\\frac{(2+a)p n}{\\operatorname*{max}\\left(1,\\log\\left(\\frac{\\sqrt{n}\\|w\\|_{\\infty}}{\\|w\\|_{2}}\\right)\\right)}\\cdot\\left\\|w\\right\\|_{\\infty}\\right]\\le2\\mathsf{e x p}\\left(-a n p\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma A.21. In the setting of Theorem $^{\\,l}$ , suppose $\\mathbf{\\nabla}a_{v}$ is such that $\\mathbf{a}_{v}[w]\\sim\\mathsf{B e r n o u l l i}(p_{v w})$ and let $\\overline{{p}}\\geq\\operatorname*{max}_{w\\,:\\ p_{v w}\\neq1}p_{v w}$ . With probability $\\geq1-\\delta$ for $\\delta\\geq1/n^{2}$ , for all $v\\in V$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\langle a_{v}-a_{v}^{\\star},u_{2}-u_{2}^{\\star}\\rangle\\right|\\leq C\\left(n\\overline{{p}}+\\log\\left(1/\\delta\\right)\\right)\\left(\\frac{\\left\\|u_{2}\\right\\|_{\\infty}}{\\log\\log n}+\\frac{1}{\\sqrt{n}\\log\\log n}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma A.21. Ideally, one would treat $\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}$ as fixed and then apply Bernstein\u2019s inequality to argue that the sum of centered Bernoulli random variables as written above concentrates well. Unfortunately, since $\\pmb{u}_{2}$ depends on $\\mathbf{a}_{v}\\mathrm{~-~}\\mathbf{a}_{v}^{\\star}$ , we cannot express this inner product as the sum of independent random variables. ", "page_idx": 22}, {"type": "text", "text": "To resolve this, we use the leave-one-out method. Let $u_{2}^{(v)}$ be the second eigenvector of the leaveone-out Laplacian $\\mathbf{L}^{\\left(v\\right)}$ of $\\mathbf{A}^{\\left(v\\right)}$ , where $\\mathbf{A}^{\\left(v\\right)}$ is chosen to agree with $\\mathbf{A}$ everywhere except for the vth row and $v$ th column. The $v$ th row and $v$ th column of $\\mathbf{A}^{(v)}$ are replaced with those of $\\mathbf{A}^{\\star}$ . Now, $\\scriptstyle\\mathbf{a}_{v}$ does not depend on $\\mathbf{L}^{\\left(v\\right)}$ and therefore $u_{2}^{(v)}$ . ", "page_idx": 22}, {"type": "text", "text": "We therefore write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle a_{v}-a_{v}^{\\star},u_{2}-u_{2}^{\\star}\\rangle|\\leq\\left|\\left\\langle a_{v}-a_{v}^{\\star},u_{2}-u_{2}^{(v)}\\right\\rangle\\right|+\\left|\\left\\langle a_{v}-a_{v}^{\\star},u_{2}^{(v)}-u_{2}^{\\star}\\right\\rangle\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|a_{v}-a_{v}^{\\star}\\|_{2}\\cdot\\left\\|u_{2}^{(v)}-u_{2}\\right\\|_{2}+\\left|\\left\\langle a_{v}-a_{v}^{\\star},u_{2}^{(v)}-u_{2}^{\\star}\\right\\rangle\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\|a_{v}-a_{v}^{\\star}\\|_{2}\\cdot\\frac{C_{A.19}\\overline{{p}}}{p-q}\\|u_{2}\\|_{\\infty}+\\left|\\left\\langle a_{v}-a_{v}^{\\star},u_{2}^{(v)}-u_{2}^{\\star}\\right\\rangle\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To bound the rightmost term of the RHS, we use Lemma 7 of [AFWZ20], reproduced in Lemma A.20. In that, let w := u2 $\\pmb{w}\\ :=\\ \\pmb{u}_{2}^{(v)}\\ -\\ \\pmb{u}_{2}^{\\star}$ \u2212u2\u22c6. Let a = n1p $\\begin{array}{r}{a\\,=\\,\\frac{1}{n\\overline{{p}}}\\log\\big(2/\\delta\\big)}\\end{array}$ so that $2\\exp(-2a n\\overline{{{p}}})\\:\\leq\\:\\delta$ . Note that for the deterministic entries, we have $\\pmb{a}_{v}-\\pmb{a}_{v}^{\\star}=1-1=0$ , so in Lemma A.20, we can set $X_{w}\\sim\\mathsf{B e r}(0)$ for these entries. Now, by Lemma A.20, with probability $\\geq1-\\delta$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\left\\langle\\mathbf{u}_{2}^{(v)}-\\mathbf{u}_{2}^{\\star},\\mathbf{a}_{v}-\\mathbf{a}_{v}^{\\star}\\right\\rangle\\right|\\leq\\frac{2n\\overline{{p}}+\\log\\left(\\frac{2}{\\delta}\\right)}{\\operatorname*{max}\\left(1,\\log\\left(\\frac{\\sqrt{n}\\|w\\|_{\\infty}}{\\|w\\|_{2}}\\right)\\right)}\\cdot\\left\\|\\mathbf{w}\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let us first bound $\\|\\pmb{w}\\|_{\\infty}=\\left\\|\\pmb{u}_{2}^{(v)}-\\pmb{u}_{2}^{\\star}\\right\\|_{\\infty}$ . We write ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\vert\\left\\vert\\boldsymbol{u}_{2}^{(v)}-\\boldsymbol{u}_{2}^{\\star}\\right\\vert\\right\\vert_{\\infty}\\leq\\left\\vert\\left\\vert\\boldsymbol{u}_{2}^{(v)}-\\boldsymbol{u}_{2}\\right\\vert\\right\\vert_{\\infty}+\\left\\vert\\left\\vert\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}\\right\\vert\\right\\vert_{\\infty}}\\\\ {\\qquad\\qquad\\leq\\left\\vert\\left\\vert\\boldsymbol{u}_{2}^{(v)}-\\boldsymbol{u}_{2}\\right\\vert\\right\\vert_{2}+\\left\\vert\\left\\vert\\boldsymbol{u}_{2}\\right\\vert\\right\\vert_{\\infty}+\\left\\vert\\left\\vert\\boldsymbol{u}_{2}^{\\star}\\right\\vert\\right\\vert_{\\infty}}\\\\ {\\qquad\\qquad\\leq2\\operatorname*{max}\\left\\{C_{A.19}(\\alpha,\\delta)\\left\\vert\\left\\vert\\boldsymbol{u}_{2}\\right\\vert\\right\\vert_{\\infty},\\frac{1}{\\sqrt{n}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In what follows, we omit the arguments $\\alpha$ and $\\delta$ in mentions of $C_{A.19}$ . Next, using Lemma A.17, the triangle inequality, and $\\delta\\geq\\bar{1}/n^{3}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\pmb{w}\\|_{2}=\\left\\|\\pmb{u}_{2}^{(v)}-\\pmb{u}_{2}^{\\star}\\right\\|_{2}\\leq C_{A.19}\\left\\|\\pmb{u}_{2}\\right\\|_{\\infty}+\\frac{4C_{A.17}}{\\sqrt{\\log n}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now have two cases based on the value of ${\\sqrt{n}}\\cdot{\\frac{\\left|\\left|\\mathbf{\\boldsymbol{u}}_{2}^{(v)}-\\mathbf{\\boldsymbol{u}}_{2}^{\\star}\\right|\\right|_{\\infty}}{\\left|\\left|\\mathbf{\\boldsymbol{u}}_{2}^{(v)}-\\mathbf{\\boldsymbol{u}}_{2}^{\\star}\\right|\\right|_{2}}}$ ", "page_idx": 22}, {"type": "text", "text": "Case ${\\bf1}-{\\bf w}$ is not too \u201cflat.\u201d Let us first handle the case where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{n}\\cdot\\left\\|\\pmb{u}_{2}^{(v)}-\\pmb{u}_{2}^{\\star}\\right\\|_{\\infty}}{\\left\\|\\pmb{u}_{2}^{(v)}-\\pmb{u}_{2}^{\\star}\\right\\|_{2}}\\geq\\sqrt{\\log n}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We plug this into (5) and get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle u_{2}^{(v)}-u_{2}^{\\star},a_{v}-a_{v}^{\\star}\\right\\rangle\\right|\\leq\\frac{2n\\overline{{p}}+\\log\\left(\\frac{2}{\\delta}\\right)}{\\operatorname*{max}\\left(1,\\log\\left(\\frac{\\sqrt{n}\\|w\\|_{\\infty}}{\\|w\\|_{2}}\\right)\\right)}\\cdot\\|\\pmb{w}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\leq4\\cdot\\frac{n\\overline{{p}}+\\log\\left(1/\\delta\\right)}{\\log\\log n}\\left(C_{A.19}\\left\\|u_{2}\\right\\|_{\\infty}+\\frac{1}{\\sqrt{n}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality follows from (8). ", "page_idx": 23}, {"type": "text", "text": "Case $2-w$ is \u201cflat.\u201d We now assume ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{n}\\cdot\\left\\|\\pmb{u}_{2}^{(v)}-\\pmb{u}_{2}^{\\star}\\right\\|_{\\infty}}{\\left\\|\\pmb{u}_{2}^{(v)}-\\pmb{u}_{2}^{\\star}\\right\\|_{2}}\\leq\\sqrt{\\log n}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can easily check that the function ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{x}{\\operatorname*{max}\\left(1,\\log x\\right)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "is increasing, so its maximum will be attained at the largest value of $x$ in the domain. Let $x=$ $\\sqrt{n}\\left\\|\\pmb{w}\\right\\|_{\\infty}\\bar{/}\\left\\|\\pmb{w}\\right\\|_{2}$ and write ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{2\\eta\\bar{p}+\\log\\left(\\frac{\\delta}{\\delta}\\right)}{\\operatorname*{max}\\big(1,\\log\\left(\\frac{\\sqrt{n}\\|w\\|_{\\infty}}{\\|w\\|_{2}}\\right)\\big)}\\cdot\\|w\\|_{\\infty}}\\\\ &{=\\frac{2\\eta\\bar{p}+\\log\\big(\\frac{2}{\\delta}\\big)}{\\operatorname*{max}\\big(1,\\log\\left(\\frac{\\sqrt{n}\\|w\\|_{\\infty}}{\\sqrt{n}}\\right)\\big)}\\cdot\\frac{\\sqrt{n}\\,\\|w\\|_{\\infty}}{\\|w\\|_{2}}\\cdot\\frac{\\|w\\|_{2}}{\\sqrt{n}}}\\\\ &{\\leq\\frac{2\\eta\\bar{p}+\\log\\big(\\frac{2}{\\delta}\\big)}{\\log\\log n}\\cdot\\sqrt{\\frac{\\log n}{n}}\\cdot\\|w\\|_{2}}\\\\ &{\\leq\\frac{2\\eta\\bar{p}+\\log\\big(\\frac{\\delta}{\\delta}\\big)}{\\log\\log n}\\cdot\\sqrt{\\frac{\\log n}{n}}\\cdot C_{A\\log}\\left(\\|u_{2}\\|_{\\infty}+\\frac{1}{\\sqrt{\\log\\left(n/\\delta\\right)}}\\right)}\\\\ &{=C_{A,\\log}\\left(\\frac{2\\eta\\bar{p}+\\log\\big(\\frac{\\delta}{\\delta}\\big)}{\\log\\log n}\\cdot\\sqrt{\\frac{\\log n}{n}}\\,\\|u_{2}\\|_{\\infty}+\\frac{2n\\bar{p}+\\log\\big(\\frac{\\delta}{\\delta}\\big)}{\\sqrt{n}\\cdot\\log\\log n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "All of this tells us that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\left\\langle a_{v}-a_{v}^{\\star},u_{2}^{(v)}-u_{2}^{\\star}\\right\\rangle\\right|\\leq4C_{A.19}\\cdot(n\\overline{{p}}+\\log{(1/\\delta)})\\left(\\frac{\\|{\\boldsymbol u}_{2}\\|_{\\infty}}{\\log\\log n}+\\frac{1}{\\sqrt{n}\\log\\log n}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It remains to handle the term ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\lVert\\mathbf{a}_{v}-\\mathbf{a}_{v}^{\\star}\\right\\rVert_{2}\\cdot\\left\\lVert\\mathbf{u}_{2}\\right\\rVert_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Indeed, using Lemma A.13, we have with probability $\\geq1-\\delta$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|a_{v}-a_{v}^{\\star}\\|_{2}\\cdot\\|u_{2}\\|_{\\infty}\\leq C_{A.13}\\left(\\frac{\\log\\left(1/\\delta\\right)}{\\log n}\\right)^{3/2}\\sqrt{n\\overline{{p}}}\\cdot\\|u_{2}\\|_{\\infty}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining everything tells us that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle a_{v}-a_{v}^{\\star},u_{2}-u_{2}^{\\star}\\rangle|\\leq C_{A.13}\\left(\\displaystyle\\frac{\\log\\left(1/\\delta\\right)}{\\log n}\\right)^{3/2}\\sqrt{n\\overline{{p}}}\\cdot\\|u_{2}\\|_{\\infty}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+4C_{A.19}\\cdot(n\\overline{{p}}+\\log\\left(1/\\delta\\right))\\left(\\displaystyle\\frac{\\|u_{2}\\|_{\\infty}}{\\log\\log n}+\\displaystyle\\frac{1}{\\sqrt{n}\\log\\log n}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C\\left(n\\overline{{p}}+\\log\\left(1/\\delta\\right)\\right)\\left(\\displaystyle\\frac{\\|u_{2}\\|_{\\infty}}{\\log\\log n}+\\displaystyle\\frac{1}{\\sqrt{n}\\log\\log n}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking a union bound over all $v\\in V$ concludes the proof of Lemma A.21. ", "page_idx": 23}, {"type": "text", "text": "Finally, we establish an upper-bound on $\\|\\pmb{u}_{2}\\|_{\\infty}$ . This will be used repeatedly in the proof of Theorem 1. ", "page_idx": 23}, {"type": "text", "text": "Lemma A.22. In the same setting as Theorem $^{\\,l}$ , with probability $\\geq1\\!-\\!\\delta$ , we have for some constant $C(\\alpha,\\delta)$ that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|u_{2}\\right\\|_{\\infty}\\leq{\\frac{C(\\alpha,\\delta)}{\\sqrt{n}}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Lemma A.22. First, observe that ", "page_idx": 24}, {"type": "equation", "text": "$$\n({\\bf D}-{\\bf A}){\\pmb u}_{2}=\\lambda_{2}{\\pmb u}_{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which means that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}\\mathbf{u}_{2}=\\mathbf{u}_{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma A.7, with probability $\\geq1-\\delta$ , for all $v\\in V$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\pmb d}[v]-\\lambda_{2}\\geq\\frac{n(p-q)}{4}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining with Lemma A.6, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d_{\\mathrm{in}}[v]-d_{\\mathrm{out}}[v]}{d_{\\mathrm{in}}[v]-d_{\\mathrm{out}}[v]+(2d_{\\mathrm{out}}[v]-\\lambda_{2})}=1-\\frac{2d_{\\mathrm{out}}[v]-\\lambda_{2}}{d_{\\mathrm{in}}[v]-d_{\\mathrm{out}}[v]+(2d_{\\mathrm{out}}[v]-\\lambda_{2})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le1+\\frac{C_{A,6}\\,\\Bigl(\\sqrt{n q\\log\\left(n/\\delta\\right)}+\\log\\left(n/\\delta\\right)\\Bigr)}{d_{\\mathrm{in}}[v]-d_{\\mathrm{out}}[v]+(2d_{\\mathrm{out}}[v]-\\lambda_{2})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le1+\\frac{4C_{A,6}\\,\\Bigl(\\sqrt{n q\\log\\left(n/\\delta\\right)}+\\log\\left(n/\\delta\\right)\\Bigr)}{n(p-q)}\\le C^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for some constant $C^{\\prime}\\,>\\,0$ , where the penultimate line follows from Lemma A.7 and the last line follows from the gap assumption in Theorem 1. Furthermore, by Lemma A.8 and Lemma A.17, we have with probability $\\geq1-\\delta$ that for all $v\\in V$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{|\\langle a_{v}^{\\star},u_{2}^{\\star}-u_{2}\\rangle|}{d[v]-\\lambda_{2}}\\leq\\frac{\\overline{{p}}\\sqrt{n}}{d[v]-\\lambda_{2}}\\cdot\\frac{C_{A.17}}{\\sqrt{\\log\\left(n/\\delta\\right)}}\\leq\\frac{C_{A.8}(\\alpha)\\cdot C_{A.17}}{\\sqrt{n\\log\\left(n/\\delta\\right)}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, using Lemma A.8 (and using Lemma A.7 to ensure that ${\\pmb d}[v]-\\lambda_{2}>0$ for all $v\\in V$ ), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\|u_{2}\\right\\|_{\\infty}}&{=\\left\\|\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{J}\\right)^{-1}\\mathbf{A}u_{2}\\right\\|_{\\infty}}\\\\ &{=\\left\\|\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{J}\\right)^{-1}\\mathbf{A}u_{2}-(\\mathbf{D}-\\lambda_{2}\\mathbf{I})^{-1}\\mathbf{A}u_{2}^{*}+(\\mathbf{D}-\\lambda_{2}\\mathbf{J})^{-1}\\mathbf{A}u_{2}^{*}\\right\\|_{\\infty}}\\\\ &{\\leq\\left\\|\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{J}\\right)^{-1}\\mathbf{A}u_{2}^{*}\\right\\|_{\\infty}+\\left\\|\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}(u_{2}^{*}-u_{2})\\right\\|_{\\infty}}\\\\ &{=\\frac{\\operatorname*{max}_{1}}{1+\\varepsilon^{\\frac{\\alpha_{1}}{\\alpha_{1}}}}\\frac{\\left|\\left(u_{1},u_{2}^{*}\\right)\\right|}{d\\left|v_{1}-\\lambda_{2}\\right|}+\\frac{\\operatorname*{max}_{1}}{1+\\varepsilon^{\\frac{\\alpha_{1}}{\\alpha_{1}}}}\\frac{\\left|\\left(u_{1},u_{2}^{*}-u_{2}\\right)\\right|}{d\\left|v_{1}-\\lambda_{2}\\right|}}\\\\ &{=\\frac{1}{\\sqrt{m}}\\left(\\operatorname*{max}_{1}\\frac{\\left|\\left(u_{1}^{*}\\right)\\right|-\\left(d_{1}\\left|v_{1}^{*}\\right)\\right|}{d\\left|v_{1}-\\lambda_{2}\\right|}+\\frac{\\operatorname*{max}_{1}}{1+\\varepsilon^{\\frac{\\alpha_{1}}{\\alpha_{1}}}}\\frac{\\left|\\left(u_{1},u_{2}^{*}-u_{2}\\right)\\right|}{d\\left|v_{1}\\right|-\\lambda_{2}}}\\\\ &{\\leq\\frac{C}{\\sqrt{m}}+\\frac{\\operatorname*{max}_{1}}{1+\\varepsilon^{\\frac{\\alpha_{1}}{\\alpha_{1}}}}\\frac{\\left|\\left(u_{1}-u_{2}^{*},u_{2}^{*}-u_{2}\\right)\\right|}{d\\left|v_{1}\\right|-\\lambda_{2}}+\\frac{\\operatorname*{max}_{1}}{1+\\varepsilon^\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that any $n$ large enough ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{C_{A.21}\\cdot C_{A.8}(\\alpha)\\cdot\\|\\pmb{u}_{2}\\|_{\\infty}}{\\log\\log n}\\leq\\frac{\\|\\pmb{u}_{2}\\|_{\\infty}}{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, rearranging and solving for $\\|u_{2}\\|_{\\infty}$ yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\|u_{2}\\right\\|_{\\infty}\\leq2\\left(\\frac{C}{\\sqrt{n}}+C_{A.21}\\cdot C_{A.8}(\\alpha)\\cdot\\left(\\frac{1}{\\sqrt{n}\\log\\log n}\\right)+\\frac{C_{A.8}(\\alpha)\\cdot C_{A.17}}{\\sqrt{n\\log\\left(n/\\delta\\right)}}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "completing the proof of Lemma A.22. ", "page_idx": 24}, {"type": "text", "text": "A.6 Strong consistency of unnormalized spectral bisection ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we prove our main positive results Theorem 1 and Theorem 2. It will be helpful to recall the proof sketches given in Section 3 while reading this section. ", "page_idx": 25}, {"type": "text", "text": "At a high level, the proof plan is as follows. ", "page_idx": 25}, {"type": "text", "text": "1. We first establish a sufficient condition for a particular vertex to be classified correctly. We can think of this as simultaneously showing that the intermediate estimator $({\\bf D}\\ -\\ \\lambda_{2}{\\bf I})^{-1}{\\bf A}u_{2}^{\\star}$ is strongly consistent and that the corresponding \u201cnoise\u201d term $(\\mathbf{D}-\\lambda_{2}\\mathbf{I})^{-1}\\mathbf{A}(\\pmb{u}_{2}^{\\star}-\\Bar{\\pmb{u}}_{2})$ is a lower-order term in comparison to this. For a more formal way to see this, see Lemma A.23.   \n2. For the proof of Theorem 1, the main technical challenge in showing that the noise term above is small amounts to analyzing the random quantity $|\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\rangle|$ . This is where we will have to use the leave-one-out method to decouple the dependence between $\\pmb{a}_{v}$ and $\\pmb{u}_{2}$ . The relevant lemmas for the leave-one-out analysis are Lemma A.21 and Lemma A.22.   \n3. Finally, for the proof of Theorem 2, we again appeal to Lemma A.23 but use a different approach to show that the noise term is small. ", "page_idx": 25}, {"type": "text", "text": "A.6.1 A sufficient condition for exact recovery and proof ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The main result of this subsection is Lemma A.23, which gives a general condition under which a particular vertex will be classified correctly. The proofs of Theorem 1 and Theorem 2 will follow by invoking Lemma A.23. We remark that the point of this lemma is mostly conceptual; the crux of the analysis lies in establishing that these conditions are satisfied our models. ", "page_idx": 25}, {"type": "text", "text": "Lemma A.23. Let $v\\,\\in\\,V$ be some vertex. If ${d[w]\\,-\\,\\lambda_{2}\\,>\\,0}$ for all $w\\ \\in\\ V$ , $d_{\\mathsf{i n}}[v]\\;>\\;d_{\\mathsf{o u t}}[v]$ , and $|\\langle a_{v},u_{2}^{\\star}-u_{2}\\rangle|\\leq(d_{\\mathsf{i n}}[v]-d_{\\mathsf{o u t}}[v])/\\sqrt{n}$ , then sign $\\begin{array}{r}{(\\boldsymbol{u}_{2}[v])=\\mathsf{s i g n}\\,(\\boldsymbol{u}_{2}^{\\star}[v]).}\\end{array}$ , i.e., $\\pmb{u}_{2}$ correctly classifies vertex $v$ . ", "page_idx": 25}, {"type": "text", "text": "The goal of the rest of this section is to prove Lemma A.23. ", "page_idx": 25}, {"type": "text", "text": "Our approach is to study the intermediate estimator ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}\\mathbf{u}_{2}^{\\star}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "At a high level, our goal is to show that this correctly classifies all the vertices with high probability and also is very close to $\\pmb{u}_{2}$ in $\\ell_{\\infty}$ norm with high probability. Deng, Ling, and Strohmer [DLS21] used this intermediate estimator to prove the strong consistency of unnormalized spectral bisection for ${\\mathsf{S B M}}(n,p,q)$ instances. ", "page_idx": 25}, {"type": "text", "text": "Next, we show that this estimator is consistent and prove Lemma A.23. ", "page_idx": 25}, {"type": "text", "text": "Proof of Lemma A.23. Observe that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{u}}_{2}=\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}\\ensuremath{\\boldsymbol{u}}_{2}^{\\star}-\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}\\left(\\ensuremath{\\boldsymbol{u}}_{2}^{\\star}-\\ensuremath{\\boldsymbol{u}}_{2}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Without loss of generality, suppose $v\\in P_{1}$ . In particular, this means that $u_{2}^{\\star}[v]=1/\\sqrt{n}$ . Our goal is to show that $\\bar{\\pmb{u}}_{2}[v]>\\bar{0}$ . And, as per the above, this means that it is enough to show that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}\\pmb{u}_{2}^{\\star}\\right)\\left[\\boldsymbol{v}\\right]\\ge\\left(\\left(\\mathbf{D}-\\lambda_{2}\\mathbf{I}\\right)^{-1}\\mathbf{A}\\left(\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\right)\\right)\\left[\\boldsymbol{v}\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "or equivalently, using the fact that ${\\pmb d}[v]-\\lambda_{2}>0$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}\\rangle\\geq\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\pmb{a}_{v}$ denotes the $v$ -th row of $A$ . To see that the above holds, use the fact that we know that $d_{\\mathsf{i n}}[v]-d_{\\mathsf{o u t}}[v]>0$ , which gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\langle a_{v},u_{2}^{\\star}\\rangle=\\frac{d_{\\mathsf{i n}}[v]-d_{\\mathsf{o u t}}[v]}{\\sqrt{n}}\\geq|\\langle a_{v},u_{2}^{\\star}-u_{2}\\rangle|\\geq\\langle a_{v},u_{2}^{\\star}-u_{2}\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This is exactly what we needed, and we conclude the proof of Lemma A.23. ", "page_idx": 25}, {"type": "text", "text": "A.7 Proofs of main results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "At this point, we are ready to prove our main results. ", "page_idx": 25}, {"type": "text", "text": "A.7.1 Nonhomogeneous symmetric stochastic block model (Proof of Theorem 1) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We are finally ready to prove Theorem 1. For convenience, we reproduce its statement here. ", "page_idx": 26}, {"type": "text", "text": "Theorem 1. Let $p,\\overline{{p}},q$ be probabilities such that $q\\,<\\,p\\,\\leq\\,{\\overline{{p}}}$ and such that $\\alpha:=\\overline{{p}}/(p-q)$ is an arbitrary constant. Let $\\mathcal D\\,\\in\\,\\mathsf{N S S B M}(n,p,\\overline{{p}},q)$ . Let $n\\,\\geq\\,N(\\alpha)$ where the function $N(\\alpha)$ only depends on $\\alpha$ . There exists a universal constant $C>0$ such that $i f$ ", "page_idx": 26}, {"type": "equation", "text": "$$\nn(p-q)\\geq C\\left({\\sqrt{n{\\overline{{p}}}\\log n}}+\\log n\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "then unnormalized spectral bisection is strongly consistent on $\\mathcal{D}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 1. As mentioned in Section 2, we actually prove a slightly stronger statement \u2013 we will allow the adversary to set at most $n{\\overline{{p}}}/\\log\\log n$ of the $p_{v w}$ to 1 per vertex $v$ (in other words, the adversary can commit to at most $n{\\overline{{p}}}/\\log\\log n$ edges per vertex that are guaranteed to appear in the final graph). ", "page_idx": 26}, {"type": "text", "text": "Our plan is to apply Lemma A.23. In order to do so, we start with showing that for all $v$ , we have $d_{\\mathsf{i n}}[v]>d_{\\mathsf{o u t}}[v]$ . By Lemma A.5, with probability $\\geq1-\\delta$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nd_{\\mathrm{in}}[v]-d_{\\mathrm{out}}[v]\\geq\\frac{n(p-q)}{2}-C_{A.5}\\left(\\sqrt{n p\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right)>0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Additionally, by Lemma A.7, we have for all $v$ that ${\\pmb d}[v]>\\lambda_{2}$ . ", "page_idx": 26}, {"type": "text", "text": "The final item we need is to show that for all $v\\in V$ , we have $|\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\rangle|\\leq|\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}\\rangle|$ . Observe that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left|\\left\\langle a_{v},u_{2}^{\\star}-u_{2}\\right\\rangle\\right|\\leq\\left|\\left\\langle a_{v}^{\\star},u_{2}^{\\star}-u_{2}\\right\\rangle\\right|+\\left|\\left\\langle a_{v}-a_{v}^{\\star},u_{2}^{\\star}-u_{2}\\right\\rangle\\right|,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\pmb{a}_{v}^{\\star}$ denotes the $v$ -th row of $\\mathbb{E}\\left[\\mathbf{A}\\right]$ . We handle the terms one at a time. First, note that by Lemma A.11, with probability $\\geq1-\\delta$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\lambda_{3}-\\lambda_{2}^{\\star}\\geq\\frac{n(p-q)}{4}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, let $\\mathbf{E}:=\\mathbf{L}-\\mathbb{E}\\left[\\mathbf{L}\\right]$ , and let $\\mathbf{a}_{v}^{\\star}[\\mathsf{r a n d}]\\,\\in\\,\\mathbb{R}^{V}$ correspond to the vector that entrywise agrees with $\\pmb{a}_{v}^{\\star}$ wherever $\\pmb{a}_{v}^{\\star}$ is not 1 and is zero elsewhere. This corresponds to the edges incident to $v$ that will be sampled randomly from the distribution over graphs. This means that for all $n\\ge N(\\delta)$ and choosing $\\delta\\stackrel{\\star}{\\geq}1/n^{3}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\langle a_{v}^{\\star}[\\mathsf{r a n d}],u_{2}^{\\star}-u_{2}\\rangle|\\leq\\|a_{v}^{\\star}[\\mathsf{r a n d}]\\|_{2}\\cdot\\frac{\\sqrt{2}\\,\\|\\mathbf{E}u_{2}^{\\star}\\|_{2}}{|\\lambda_{3}-\\lambda_{2}^{\\star}|}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\overline{{p}}\\sqrt{n}\\cdot\\frac{4\\sqrt{2}C_{A,12}\\,\\left(\\sqrt{n q}+\\left(n q\\log n\\right)^{1/4}+\\sqrt{\\log n}\\right)}{n(p-q)}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{100C_{A,12}n\\overline{{p}}}{\\sqrt{n}\\log\\log n}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To handle the oblivious insertions, let $\\pmb{d}_{\\mathrm{det}}\\in\\mathbb{R}^{V}$ denote the degree vector that counts the number of deterministic edges inserted incident to $v$ , for all $v\\in V$ . Under this notation, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\langle a_{v}^{\\star}-a_{v}^{\\star}[\\mathsf{r a n d}],u_{2}^{\\star}-u_{2}\\rangle|\\leq d_{\\mathsf{d e t}}[v]\\cdot\\|u_{2}-u_{2}^{\\star}\\|_{\\infty}\\leq\\frac{n\\overline{{p}}}{\\sqrt{n}\\log\\log n}+\\frac{n\\overline{{p}}\\,\\|u_{2}\\|_{\\infty}}{\\log\\log n}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality follows from using $\\|\\pmb{u}_{2}-\\pmb{u}_{2}^{\\star}\\|_{\\infty}\\leq\\|\\pmb{u}_{2}\\|_{\\infty}+\\|\\pmb{u}_{2}^{\\star}\\|_{\\infty}$ . Combining yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\langle\\pmb{a}_{v}^{\\star},\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\rangle|\\leq C^{\\prime}\\frac{n\\overline{{p}}}{\\sqrt{n}\\log\\log n}+\\frac{n\\overline{{p}}\\,\\|\\pmb{u}_{2}\\|_{\\infty}}{\\log\\log n},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for some constant $C^{\\prime}>0$ . Now, notice that for all $n$ sufficiently large, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|\\langle a_{v}-a_{v}^{\\star},u_{2}^{\\star}-{\\mathbf{u}}_{2}\\rangle|}\\\\ &{\\leq C_{A,21}\\,(n\\overline{{p}}+\\log\\left(1/\\delta\\right))\\left(\\frac{\\|{\\mathbf{u}}_{2}\\|_{\\infty}}{\\log\\log n}+\\frac{1}{\\sqrt{n}\\log\\log n}\\right)}\\\\ &{\\leq C_{A,21}\\,(n\\overline{{p}}+\\log\\left(1/\\delta\\right))\\left(\\frac{\\frac{C_{A,22}(\\alpha,\\delta)}{\\sqrt{n}}}{\\log\\log n}+\\frac{1}{\\sqrt{n}\\log\\log n}\\right)}\\\\ &{\\leq\\frac{C_{1}(\\alpha,\\delta)\\cdot\\,(n\\overline{{p}}+\\log\\left(1/\\delta\\right))}{\\sqrt{n}\\log\\log n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Adding yields for $n\\geq N(\\alpha,\\delta)$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\left\\langle a_{v},u_{2}^{\\star}-u_{2}\\right\\rangle\\right|\\leq\\left|\\left\\langle a_{v}^{\\star},u_{2}^{\\star}-u_{2}\\right\\rangle\\right|+\\left|\\left\\langle a_{v}-a_{v}^{\\star},u_{2}^{\\star}-u_{2}\\right\\rangle\\right|}&{}\\\\ {\\leq\\cfrac{C_{2}\\left(\\alpha,\\delta\\right)\\cdot\\left(n\\overline{{p}}+\\log\\left(1/\\delta\\right)\\right)}{\\sqrt{n}\\log\\log n}}\\\\ {\\leq\\cfrac{1}{\\sqrt{n}}\\cdot\\left(\\cfrac{n\\left(p-q\\right)}{2}-C_{A.5}\\left(\\sqrt{n p\\log\\left(n/\\delta\\right)}+\\log\\left(n/\\delta\\right)\\right)\\right)}&{\\mathrm{(gap~conding~}}\\\\ {\\leq\\cfrac{d_{\\mathrm{in}}\\left[v\\right]-d_{\\mathrm{out}}\\left[v\\right]}{\\sqrt{n}}=\\left|\\left\\langle a_{v},u_{2}^{\\star}\\right\\rangle\\right|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which means we satisfy the conditions required by Lemma A.23. Taking a union bound over all our (constantly many) probabilistic statements, setting $\\delta=\\Theta(1/n)$ , and rescaling completes the proof of Theorem 1. \u53e3 ", "page_idx": 27}, {"type": "text", "text": "A.7.2 Deterministic clusters model ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "For convenience, we reproduce the statement of Theorem 2 here. ", "page_idx": 27}, {"type": "text", "text": "Theorem 2. Let q be a probability and $d_{\\mathrm{in}}$ be an integer, and let $D\\in\\mathsf{D C M}(n,d_{\\mathsf{i n}},q)$ . For $G\\sim\\mathcal{D}$ , let $\\widehat{\\bf L}$ denote the expectation of $\\mathbf{L}$ after step (2) but before step (3) in Model 2. There exists constants $C_{1},C_{2},C_{3}>0$ such that for all n sufficiently large, $i f$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{\\mathrm{in}}\\geq C_{1}\\cdot\\left(\\frac{n q}{2}+\\sqrt{n}\\right)\\quad a n d\\quad\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\widehat{\\mathbf{L}})\\geq\\sqrt{n}+C_{2}n q+C_{3}\\left(\\sqrt{n q\\log n}+\\log n\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "then unnormalized spectral bisection is strongly consistent on $\\mathcal{D}$ . ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 2. In this proof, let $\\mathbf{L}^{\\star}$ be the Laplacian matrix that agrees with $\\mathbf{L}$ on all internal edges and agrees with $\\mathbb{E}\\left[\\mathbf{L}\\right]$ on all crossing edges. Let $\\mathbf{L}^{\\mathrm{(cross)}}$ denote the Laplacian matrix corresponding to the cross edges, so we can write $\\mathbf{L}^{\\star}=\\mathbf{L}-\\mathbf{L}^{\\mathrm{(cross)}}+\\mathbb{E}\\left[\\mathbf{L}^{\\mathrm{(cross)}}\\right]$ . Although $\\mathbf{L}^{\\star}\\neq\\mathbb{E}\\left[\\mathbf{L}\\right]$ due to the adaptive adversary, by Lemma A.14, we still have $\\mathbf{L}^{\\star}\\pmb{u}_{2}^{\\star}=\\lambda_{2}^{\\star}\\bar{\\pmb{u}}_{2}^{\\star}=n q\\pmb{u}_{2}^{\\star}$ . Moreover, $(\\mathbf{L}-\\mathbf{L}^{\\star})\\mathbf{{u}}_{2}^{\\star}$ is the vector whose entries are of the form $2(d_{\\mathrm{out}}[v]-\\mathbb{E}\\left[d_{\\mathrm{out}}[v]\\right])/\\sqrt{n}$ . Thus, we will be able to apply Lemma A.16 and Lemma A.12 later on. Finally, observe that $\\lambda_{i}(\\mathbf{L}^{\\star})\\geq\\lambda_{i}(\\widehat{\\mathbf{L}})$ for all $i\\geq3$ and $\\lambda_{2}(\\widehat{\\mathbf{L}})=\\lambda_{2}(\\mathbf{L}^{\\star})=n q$ . Thus, one can use the spectral gap $\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\widehat{\\mathbf{L}})$ to r eason about $\\lambda_{3}^{\\star}-\\lambda_{2}^{\\star}$ . ", "page_idx": 27}, {"type": "text", "text": "Let $\\delta~\\geq~1/n^{3}$ . We will apply Lemma A.23 to get strong consistency. First, let us verify that ${\\pmb d}[v]>\\lambda_{2}$ for all $v$ . Applying Lemma A.10 to the matrix $\\mathbf{L}^{\\mathrm{(cross)}}$ gives ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left\\lVert\\mathbf{L}-\\mathbf{L}^{\\star}\\right\\rVert_{\\mathrm{op}}=\\left\\lVert\\mathbf{L}^{(\\mathrm{cross})}-\\mathbb{E}\\left[\\mathbf{L}^{(\\mathrm{cross})}\\right]\\right\\rVert_{\\mathrm{op}}\\leq C_{A.10}\\left(\\sqrt{n q\\log\\left(^{n/\\delta}\\right)}+\\log(n/\\delta)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, using Weyl\u2019s inequality, for $n>N(\\delta)$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle d[v]-\\lambda_{2}\\geq d_{\\mathrm{in}}[v]-\\lambda_{2}^{\\star}-\\|{\\bf L}-{\\bf L}^{\\star}\\|_{\\mathrm{op}}}\\ ~}\\\\ {{\\displaystyle\\geq C_{1}\\frac{n q}{2}+C_{1}\\sqrt{n}-n q-C_{A.10}\\left(\\sqrt{n q\\log\\left(n/\\delta\\right)}+\\log(n/\\delta)\\right)>0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we verify that $d_{\\mathsf{i n}}[v]>d_{\\mathsf{o u t}}[v]$ for all $v$ . By Lemma A.3, with probability $\\geq1-\\delta$ , for all $v\\in V$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left|d_{\\mathsf{o u t}}[v]-\\frac{n q}{2}\\right|\\leq C_{A.3}\\left(\\sqrt{n q\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "So for $n>N(\\delta)$ , we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\nd_{\\mathrm{in}}[v]-d_{\\mathrm{out}}[v]\\geq C_{1}\\frac{n q}{2}+C_{1}\\sqrt{n}-\\frac{n q}{2}-C_{A.3}\\left(\\sqrt{n q\\log\\left({n}/{\\delta}\\right)}+\\log\\left({n}/{\\delta}\\right)\\right)>0.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here, in the last inequality we used the fact that $\\sqrt{n q\\log\\left(n/\\delta\\right)}\\leq\\operatorname*{max}\\{n q,\\log(n/\\delta)\\}$ . Finally, we need to show that for all $v\\in V$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n|\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\rangle|\\leq|\\langle\\pmb{a}_{v},\\pmb{u}_{2}^{\\star}\\rangle|=\\frac{\\pmb{d}_{\\mathsf{i n}}[v]-\\pmb{d}_{\\mathsf{o u t}}[v]}{\\sqrt{n}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Cauchy-Schwarz, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\langle a_{v},u_{2}^{\\star}-u_{2}\\rangle|\\leq\\|a_{v}\\|_{2}\\cdot\\|u_{2}^{\\star}-u_{2}\\|_{2}=\\sqrt{d_{\\mathrm{in}}[v]+d_{\\mathrm{out}}[v]}\\cdot\\|u_{2}^{\\star}-u_{2}\\|_{2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, it is enough to show that for all $v\\in V$ we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sqrt{n}\\,\\|\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\|_{2}\\leq\\frac{\\pmb{d}_{\\mathsf{i n}}[\\upsilon]-\\pmb{d}_{\\mathsf{o u t}}[\\upsilon]}{\\sqrt{\\pmb{d}_{\\mathsf{i n}}[\\upsilon]+\\pmb{d}_{\\mathsf{o u t}}[\\upsilon]}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Observe that the RHS above is a decreasing function in $d_{\\mathrm{out}}[v]$ and an increasing function in $d_{\\mathrm{in}}[v]$ . Now, by Lemma A.16 and Lemma A.12, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sqrt{n}\\left\\|u_{2}^{\\star}-u_{2}\\right\\|_{2}\\leq\\frac{\\sqrt{n}\\left\\|\\mathbf{E}u_{2}^{\\star}\\right\\|_{2}}{\\left|\\lambda_{3}-\\lambda_{2}^{\\star}\\right|}\\leq\\frac{6C_{A,12}\\sqrt{n}\\left(\\sqrt{n q}+(n q\\log\\left(n/\\delta\\right))^{1/4}+\\sqrt{\\log\\left(^{n/\\delta}\\right)}\\right)}{\\left|\\lambda_{3}-\\lambda_{2}^{\\star}\\right|}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now do casework on the value of $q$ . ", "page_idx": 28}, {"type": "text", "text": "Case 1: $q\\leq\\log\\left(n/\\delta\\right)/n$ . Carrying on from (9) and applying Lemma A.10 (we can set $p_{i j}$ for the deterministic internal edges to 0 as they do not affect $\\mathbf{L}-\\mathbb{E}\\left[\\mathbf{L}\\right]\\!,$ ) along with Weyl\u2019s inequality, for all $n\\geq N(\\delta)$ we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n}\\left\\|u_{2}^{\\star}-u_{2}\\right\\|_{2}\\leq\\frac{18C_{A,12}\\sqrt{n\\log\\left({n}/{\\delta}\\right)}}{\\left|\\lambda_{3}-\\lambda_{2}^{\\star}\\right|}\\leq\\frac{18C_{A,12}\\sqrt{n\\log\\left({n}/{\\delta}\\right)}}{\\sqrt{n}-3C_{A,10}\\log\\left({n}/{\\delta}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\leq C\\sqrt{\\log\\left({n}/{\\delta}\\right)}\\ll\\frac{d_{\\mathrm{in}}\\left[v\\right]-d_{\\mathrm{out}}\\left[v\\right]}{\\sqrt{d_{\\mathrm{in}}\\left[v\\right]+d_{\\mathrm{out}}\\left[v\\right]}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "as required. Here the last inequality follows using the fact that $\\begin{array}{r}{d_{\\mathsf{i n}}[v]\\ \\geq\\ C_{1}\\left(\\frac{n q}{2}+\\sqrt{n}\\right)}\\end{array}$ and $\\begin{array}{r}{d_{\\mathrm{out}}[v]\\le\\frac{n q}{2}+2C_{A.3}\\log(n/\\delta)}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "Case 2: $\\log\\left(n/\\delta\\right)/n\\leq q.$ . Similar to the previous case, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{n}\\left\\|\\pmb{u}_{2}^{\\star}-\\pmb{u}_{2}\\right\\|_{2}\\leq\\frac{18C_{A.12}\\sqrt{n}\\cdot\\sqrt{n q}}{\\left|\\lambda_{3}-\\lambda_{2}^{\\star}\\right|}\\leq\\frac{18C_{A.12}\\sqrt{n}\\cdot\\sqrt{n q}}{\\sqrt{n}+\\left(C_{2}-2C_{A.10}\\right)n q}}\\\\ &{\\qquad\\qquad\\qquad\\leq18C_{A.12}\\cdot\\operatorname*{max}\\left\\{\\sqrt{n q},\\frac{1}{\\left(C_{2}-2C_{A.10}\\right)\\sqrt{q}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Additionally, we can use the conclusion of Lemma A.3 to write with probability $\\geq1-\\delta$ for all $v\\in V$ and $n\\ge N(\\delta)$ that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d_{\\mathrm{in}}[v]-d_{\\mathrm{out}}[v]}{\\sqrt{d_{\\mathrm{in}}[v]+d_{\\mathrm{out}}[v]}}\\ge\\frac{(C_{1}-2C_{A.3}-1/2)n q+C_{1}\\sqrt{n}}{\\sqrt{(C_{1}+2C_{A.3}+1/2)n q}}}\\\\ &{\\qquad\\qquad\\qquad\\ge\\frac{C_{1}-2C_{A.3}-1/2}{\\sqrt{C_{1}+2C_{A.3}+1/2}}\\operatorname*{max}\\left\\{\\sqrt{n q},\\sqrt{\\frac{1}{q}}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "From this, it is clear that one can choose constants $C_{1}$ and $C_{2}$ such that (11) is at most (13). This means we may conclude the proof of Theorem 2. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "A.8 Inconsistency of normalized spectral bisection ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we design a family of problem instances on which unnormalized spectral bisection is strongly consistent whereas normalized spectral bisection is inconsistent. Specifically, our goal is to prove Theorem 3. ", "page_idx": 29}, {"type": "text", "text": "Theorem 3. For all n sufficiently large, there exists a nonhomogeneous stochastic block model such that unnormalized spectral bisection is strongly consistent whereas normalized spectral bisection (both symmetric and random-walk) incurs a misclassification rate of at least $24\\%$ with probability $1-1/n$ . ", "page_idx": 29}, {"type": "text", "text": "A.8.1 The nested block example ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We first state the family of instances on which we will prove our inconsistency results. Let $n$ be a multiple of 4. Let $L_{1}$ consist of indices $1,\\dots,n/4$ , $L_{2}$ consist of indices $n/4+1,\\ldots,n/2$ , and $R$ consist of indices $n/2+1,\\ldots,n$ . ", "page_idx": 29}, {"type": "text", "text": "As mentioned in Section 3, consider the following block structure determined by the $\\mathbf{A}^{\\star}$ written below, where $q<p$ and $K\\geq3p/q$ . ", "page_idx": 29}, {"type": "table", "img_path": "kLen1XyW6P/tmp/4b45f649bee74a86079e5c234901c4c9ed6bdcbce5d33affce0f856b8d3c08cb.jpg", "table_caption": ["Table 2: $\\mathbf{A}^{\\star}$ is defined to have the above block structure. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "We will draw our instances from the nonhomogeneous stochastic block model according to the probabilities prescribed above. Note that within the two clusters $L:=L_{1}\\cup L_{2}$ and $R$ , each edge appears with probability at least $p$ . Moreover, each edge in $L\\times R$ appears with probability exactly $q$ . However, there are also two subcommunities $L_{1}$ and $L_{2}$ that appear within $L$ . Furthermore, observe that unnormalized spectral bisection is consistent on this family of examples with probability $\\geq1-1/n$ by Theorem 1. ", "page_idx": 29}, {"type": "text", "text": "A.8.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "First, we construct $\\mathcal{L}^{\\star}$ . ", "page_idx": 29}, {"type": "text", "text": "Lemma A.24. Let $\\mathcal{L}^{\\star}$ be the normalized Laplacian constructed from $\\mathbf{A}^{\\star}$ and the corresponding $\\mathbf{D}^{\\star}$ .   \nThen, ${\\mathbf I}-{\\mathcal L}^{\\star}$ has the following block structure. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\int_{L_{1}}\\frac{L_{1}}{K p}}{L_{1}}\\left[\\begin{array}{l l}{\\frac{K p}{\\frac{R}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}\\cdot\\mathbb{I}_{n/4\\times n/4}}&{\\frac{p}{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}\\cdot\\mathbb{I}_{n/4\\times n/4}}\\\\ {\\frac{\\frac{p}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}{2}\\cdot\\mathbb{I}_{n/4\\times n/4}}&{\\frac{\\frac{p}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}{\\frac{p}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}\\cdot\\mathbb{I}_{n/4\\times n/4}}\\end{array}\\right]\\frac{R}{\\sqrt{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\cdot\\frac{n}{2}\\cdot\\left(p+q\\right)}\\cdot\\mathbb{I}_{n/2\\times n/2}}}}\\\\ {R}&{\\frac{\\eta}{\\sqrt{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}\\cdot\\frac{q}{\\sqrt{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}\\cdot\\mathbb{I}_{n/2\\times n/2}}}\\cdot\\mathbb{I}_{n/2\\times n/2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof of Lemma A.24. It is easy to see that for any $v\\in L$ , we have $\\begin{array}{r}{d^{\\star}[v]=\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}\\end{array}$ and for any $v\\in R$ , we have $\\begin{array}{r}{d^{\\star}[v]=\\frac{n}{2}\\cdot(p+q)}\\end{array}$ . Lemma A.24 follows by noting that every element of ${\\bf I}-{\\mathcal{L}}^{\\star}$ is of the form $\\pmb{a}_{i}^{\\star}[j]/\\sqrt{\\pmb{d}^{\\star}[i]\\pmb{d}^{\\star}[j]}$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Next, we analyze the eigenvalues and eigenvectors of $\\mathcal{L}^{\\star}$ ", "page_idx": 29}, {"type": "text", "text": "Lemma A.25. Up to normalization and sign, the eigenvector-eigenvalue pairs of $\\mathbf{\\ddot{I}}\\!-\\!\\mathcal{L}^{\\star}$ corresponding to the nonzero eigenvalues of ${\\bf I}-{\\mathcal{L}}^{\\star}$ are ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle(\\lambda_{1}^{\\star},{\\pmb u}_{1}^{\\star})=\\left(1,\\left[\\mathbb{1}_{n/4}\\oplus\\mathbb{1}_{n/4}\\oplus y_{+}\\cdot\\mathbb{1}_{n/4}\\oplus y_{+}\\cdot\\mathbb{1}_{n/4}\\right]\\right)}}\\\\ {{\\displaystyle(\\lambda_{2}^{\\star},{\\pmb u}_{2}^{\\star})=\\left(\\frac{(K-1)p}{2\\left(p\\cdot\\frac{K+1}{2}+q\\right)},\\left[\\mathbb{1}_{n/4}\\oplus-\\mathbb{1}_{n/4}\\oplus\\mathbb{0}_{n/4}\\oplus\\mathbb{0}_{n/4}\\right]\\right)}}\\\\ {{\\displaystyle(\\lambda_{3}^{\\star},{\\pmb u}_{3}^{\\star})=\\left(-1+p\\left(\\frac{1}{p+q}+\\frac{K+1}{p(K+1)+2q}\\right),\\left[\\mathbb{1}_{n/4}\\oplus\\mathbb{1}_{n/4}\\oplus y_{-}\\cdot\\mathbb{1}_{n/4}\\oplus y_{-}\\cdot\\mathbb{1}_{n/4}\\right]\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $y_{+}$ and $y_{-}$ are chosen according to the formulas ", "page_idx": 30}, {"type": "equation", "text": "$$\ny_{+}=\\sqrt{\\frac{2(p+q)}{p(K+1)+2q}}\\qquad\\qquad\\qquad y_{-}=-\\sqrt{\\frac{p(K+1)+2q}{2(p+q)}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Moreover, we have $\\lambda_{1}^{\\star}>\\lambda_{2}^{\\star}>\\lambda_{3}^{\\star}>0$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\lambda_{2}^{\\star}-\\lambda_{3}^{\\star}\\geq1-\\frac{p^{2}(K+3)+4p q}{p^{2}(K+3)+4p q+2q^{2}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma A.25. As we can see from Lemma A.24, ${\\mathbf{I}}-{\\mathcal{L}}^{\\star}$ is a matrix whose rank is at most 3, since it can be constructed by carefully repeating 3 distinct column vectors. Thus, it can have at most 3 nonzero eigenvalues. In what follows, we consider the case where $K>1$ so that there are exactly 3 nonzero eigenvalues. ", "page_idx": 30}, {"type": "text", "text": "The next step is to confirm that the stated eigenvalue-eigenvector pairs are in fact valid. We begin with $\\pmb{u}_{1}^{\\star}$ . Every entry in the first $n/2$ entries of $(\\mathbf{I}-\\mathcal{L}^{\\star})\\mathbf{u}_{1}^{\\star}$ can be expressed as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\frac{n}{4}\\cdot\\frac{K p}{\\frac{n}{2}\\cdot\\frac{K+1}{2}+(p\\cdot\\frac{K+1}{2}+q)}+\\frac{n}{4}\\cdot\\frac{p}{\\frac{n}{2}\\cdot(p\\cdot\\frac{K+1}{2}+q)}+\\frac{n}{2}\\cdot\\left(\\frac{q\\cdot\\sqrt{\\frac{2(p+q)}{p(K+1)+2q}}}{\\sqrt{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\cdot\\frac{n}{2}\\cdot(p+q)}}\\right)}}\\\\ {{=\\frac{(K+1)p}{(K+1)p+2q}+\\frac{q\\cdot\\sqrt{\\frac{2(p+q)}{p(K+1)+2q}}}{\\sqrt{\\left(p\\cdot\\frac{K+1}{2}+q\\right)(p+q)}}=\\frac{(K+1)p}{(K+1)p+2q}+\\frac{q\\cdot\\sqrt{\\frac{2}{p(K+1)+2q}}}{\\sqrt{\\left(p\\cdot\\frac{K+1}{2}+q\\right)}}}}\\\\ {{=\\frac{(K+1)p}{(K+1)p+2q}+\\frac{2q}{(K+1)p+2q}=1,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and every entry in the second $n/2$ entries of $(\\mathbf{I}-\\mathcal{L}^{\\star})\\mathbf{u}_{1}^{\\star}$ can be expressed as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{n}{2}\\cdot\\frac{q}{\\sqrt{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\cdot\\frac{n}{2}\\cdot\\left(p+q\\right)}}+\\frac{n}{2}\\cdot\\frac{p}{\\frac{n}{2}\\cdot\\left(p+q\\right)}\\cdot\\sqrt{\\frac{2\\left(p+q\\right)}{p\\left(K+1\\right)+2q}}}\\\\ &{=\\frac{q}{\\sqrt{\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\left(p+q\\right)}}+\\frac{p}{\\left(p+q\\right)}\\cdot\\sqrt{\\frac{2\\left(p+q\\right)}{p\\left(K+1\\right)+2q}}}\\\\ &{=\\frac{q}{\\sqrt{\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\left(p+q\\right)}}+p\\cdot\\sqrt{\\frac{1}{\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\left(p+q\\right)}}}\\\\ &{=\\frac{\\sqrt{p+q}}{\\sqrt{p\\cdot\\frac{K+1}{2}+q}}=\\sqrt{\\frac{2\\left(p+q\\right)}{p\\left(K+1\\right)+2q}}=y_{+}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For $\\pmb{u}_{2}^{\\star}$ , we can use the block structure and easily verify ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left(\\mathbf{I}-\\mathcal{L}^{\\star}\\right)\\boldsymbol{u}_{2}^{\\star}=\\frac{n}{4}\\cdot\\frac{(K-1)p}{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}\\left[\\mathbb{1}_{n/4}\\oplus-\\mathbb{1}_{n/4}\\oplus\\mathbb{0}_{n/4}\\oplus\\mathbb{0}_{n/4}\\right]=\\lambda_{2}^{\\star}\\boldsymbol{u}_{2}^{\\star}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now address $\\pmb{u}_{3}^{\\star}$ . The first $n/2$ entries of $(\\mathbf{I}-\\mathcal{L}^{\\star})\\pmb{u}_{3}^{\\star}$ are ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{n}{4}\\cdot\\frac{K p}{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}+\\frac{n}{4}\\cdot\\frac{p}{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)}+\\frac{n}{2}\\cdot\\left(\\frac{q\\cdot-\\sqrt{\\frac{p(K+1)+2q}{2(p+q)}}}{\\sqrt{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\cdot\\frac{n}{2}\\cdot\\left(p+q\\right)}}\\right)}\\\\ &{=\\frac{(K+1)p}{(K+1)p+2q}+\\left(\\frac{q\\cdot-\\sqrt{\\frac{1}{p+q}}}{\\sqrt{p+q}}\\right)=\\frac{(K+1)p}{(K+1)p+2q}-\\frac{q}{p+q}=\\lambda_{3}^{\\star},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "and the second $n/2$ entries of $(\\mathbf{I}-\\mathcal{L}^{\\star})\\pmb{u}_{3}^{\\star}$ are ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{n}{2}\\cdot\\frac{q}{\\sqrt{\\frac{n}{2}\\cdot\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\cdot\\frac{n}{2}\\cdot\\left(p+q\\right)}}+\\frac{n}{2}\\cdot\\frac{p}{\\frac{n}{2}\\cdot\\left(p+q\\right)}\\cdot-\\sqrt{\\frac{p(K+1)+2q}{2(p+q)}}}\\\\ &{=\\frac{q}{\\sqrt{\\left(p\\cdot\\frac{K+1}{2}+q\\right)\\left(p+q\\right)}}-\\frac{p}{\\left(p+q\\right)}\\cdot\\sqrt{\\frac{p(K+1)+2q}{2(p+q)}}}\\\\ &{=-\\sqrt{\\frac{p(K+1)+2q}{2(p+q)}}\\left(\\frac{-2q}{p(K+1)+2q}+\\frac{p}{p+q}\\right)=y_{-}\\cdot\\lambda_{3}^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Finally, it remains to check that $1>\\lambda_{2}^{\\star}>\\lambda_{3}^{\\star}>0$ . The fact that $\\lambda_{2}^{\\star}<1$ easily follows from using $p+q>0$ . To prepare to bound $\\lambda_{2}^{\\star}-\\lambda_{3}^{\\star}$ , we first use $p\\geq q$ to establish ", "page_idx": 31}, {"type": "equation", "text": "$$\np^{2}-p q+2q^{2}=p(p-q)+2q^{2}\\geq2q^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This implies ", "page_idx": 31}, {"type": "equation", "text": "$$\np q(K-1)+2q^{2}\\ge3p^{2}-p q+2q^{2}=2p^{2}+(p^{2}-p q+2q^{2})\\ge2p^{2}+2q^{2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which rearranges to ", "page_idx": 31}, {"type": "equation", "text": "$$\np^{2}(K+1)+p q(K+3)+2q^{2}\\geq p^{2}(K+3)+4p q+2q^{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Next, we write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{2}^{\\star}-\\lambda_{3}^{\\star}=\\Bigg(\\cfrac{(K-1)p}{2\\,\\big(p\\cdot\\frac{K+1}{2}+q\\big)}\\Bigg)-\\Bigg(-1+p\\left(\\cfrac{1}{p+q}+\\cfrac{K+1}{p(K+1)+2q}\\right)\\Bigg)}\\\\ &{\\qquad=1-\\cfrac{p}{p+q}-\\cfrac{2p}{p(K+1)+2q}=1-\\bigg(\\cfrac{p^{2}(K+1)+2p q+2p^{2}+2p q}{(p+q)(p(K+1)+2q)}\\bigg)}\\\\ &{\\qquad=1-\\cfrac{p^{2}(K+3)+4p q}{p^{2}(K+1)+p q(K+3)+2q^{2}}\\geq1-\\cfrac{p^{2}(K+3)+4p q}{p^{2}(K+3)+4p q+2q^{2}}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Finally, to show $\\lambda_{3}^{\\star}>0$ , we write ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\lambda_{3}^{\\star}+1=\\frac{p}{p+q}+\\frac{p(K+1)}{p(K+1)+2q}>\\frac{2p}{p+q}>1,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which allows us to complete the proof of Lemma A.25. ", "page_idx": 31}, {"type": "text", "text": "We are now ready to prove the inconsistency of normalized spectral bisection on the nested block examples. ", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem 3. Let $G$ be a graph drawn from the nested block example. We choose $p$ and $q$ such that $p\\,\\gtrsim\\,\\log n/n$ and $p/q\\,=\\,\\alpha\\,\\geq\\,2$ where $\\alpha$ is some constant and such that $p$ and $q$ both satisfy the conditions of Theorem 1. Let $K\\geq3\\alpha$ . Observe that the true communities are $L$ and $R$ We will show that bisection based on $\\pmb{u}_{2}$ of ${\\bf I}-{\\mathcal{L}}$ (corresponding to the eigenvector associated with the second smallest eigenvalue of $\\mathcal{L}$ ) will attain a large misclassification rate. In particular, based on our calculation in Lemma A.25, we expect that $\\pmb{u}_{2}$ will output a bisection that places $L_{1}$ and $L_{2}$ into separate clusters. On the other hand, by Theorem 1, for all $n$ large enough, the unnormalized spectral bisection algorithm will be strongly consistent. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "First, observe that it is enough to prove the inconsistency result just for the symmetric normalized Laplacian. Indeed, observe that if $\\pmb{u}_{2}$ is an eigenvector of $\\mathbf{I}-\\mathcal{L}=\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}$ , then we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\lambda_{2}\\mathbf{D}^{-1/2}u_{2}=\\mathbf{D}^{-1}\\mathbf{A}\\mathbf{D}^{-1/2}u_{2}=\\mathbf{D}^{-1}\\mathbf{A}(\\mathbf{D}^{-1/2}u_{2}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which shows that $\\mathbf{D}^{-1/2}\\mathbf{u}_{2}$ must be the eigenvector of the random-walk normalized Laplacian ${\\bf I}-$ $\\mathbf{D}^{-1}\\mathbf{A}$ corresponding to eigenvalue $\\lambda_{2}$ . Since $\\mathbf{D}$ is a positive diagonal matrix, it does not change the signs of $\\pmb{u}_{2}$ and therefore the output of the normalized spectral bisection algorithm is the same. ", "page_idx": 32}, {"type": "text", "text": "Our general approach to prove the inconsistency is to use the Davis-Kahan Theorem, a bound on $\\|\\mathcal{L}-\\mathcal{L}^{\\star}\\|_{\\mathrm{op}}$ , and a bound on the gap $\\lambda_{2}^{\\star}-\\lambda_{3}$ . Let $d_{\\mathrm{min}}$ be the minimum degree of the graph given by adjacency matrix A and let $d_{\\mathrm{min}}^{\\star}$ be the minimum weighted degree of the graph given by the adjacency matrix $\\mathbf{A}^{\\star}$ . First, using [DLS21, Theorem 3.1], we have with probability $1-n^{-r}$ for some constant $r\\geq1$ and constants $C(r)$ and $C$ (the latter of which does not depend on $r$ ), for all $n$ sufficiently large, ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\mathcal{L}-\\mathcal{L}^{\\star}\\right\\|_{\\mathrm{op}}\\leq\\frac{C(r)\\,\\big(n\\operatorname*{max}_{(i,j)}p_{i j}\\big)^{5/2}}{\\operatorname*{min}\\big\\{d_{\\operatorname*{min}},d_{\\operatorname*{min}}^{\\star}\\big\\}}}\\\\ &{\\qquad\\qquad\\leq\\frac{C(r)\\,(n\\cdot K p)^{5/2}}{\\operatorname*{min}\\big\\{n(p+q)/2,\\,n(p+q)/2-C\\sqrt{n(p+q)\\log n}\\big\\}^{3}}}\\\\ &{\\qquad\\qquad\\leq\\frac{C_{1}(r,\\alpha)K^{5/2}(n p)^{5/2}}{(n p)^{3}}=\\frac{C_{1}(r,\\alpha)K^{5/2}}{\\sqrt{n p}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Next, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{2}^{\\star}-\\lambda_{3}=(\\lambda_{2}^{\\star}-\\lambda_{3}^{\\star})+(\\lambda_{3}^{\\star}-\\lambda_{3})}\\\\ &{\\qquad\\quad\\ge\\left(1-\\frac{p^{2}(K+3)+4p q}{p^{2}(K+3)+4p q+2q^{2}}\\right)-\\frac{C_{1}(r,\\alpha)K^{5/2}}{\\sqrt{n p}}\\ge C_{g}(\\alpha,K),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last line denotes a positive constant depending on $q$ and $K$ (this constant will always be positive for sufficiently large $n$ , as we showed that $\\lambda_{2}^{\\star}-\\lambda_{3}^{\\star}>0$ in Lemma A.25). ", "page_idx": 32}, {"type": "text", "text": "Putting everything together, we get by the Davis-Kahan theorem that some signing of $\\pmb{u}_{2}$ satisfies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|u_{2}-u_{2}^{\\star}\\|_{2}\\leq\\frac{\\|\\mathcal{L}-\\mathcal{L}^{\\star}\\|_{\\mathrm{op}}}{\\operatorname*{min}\\left\\{\\left|\\lambda_{2}^{\\star}-\\lambda_{3}\\right|,1-\\lambda_{2}^{\\star}\\right\\}}\\leq\\frac{C_{2}(r)K^{5/2}}{C_{g}(\\alpha,K)\\sqrt{n p}}\\leq\\frac{C(r,\\alpha,K)}{\\sqrt{n p}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Now, consider the subset of coordinates of $\\pmb{u}_{2}$ belonging to $L_{1}$ . Suppose $m$ of these coordinates do not agree in sign with $\\pmb{u}_{2}^{\\star}$ . To maximize $m$ , each of these coordinates in $\\pmb{u}_{2}$ should be 0, which means the total $\\ell_{2}$ error can be bounded as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\boldsymbol{u}_{2}-\\boldsymbol{u}_{2}^{\\star}\\|_{2}^{2}=m\\left(\\frac{1}{\\sqrt{n/2}}\\right)^{2}\\leq\\frac{C(r,\\alpha,K)^{2}}{n p}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "This means the number of coordinates $m$ on which $\\pmb{u}_{2}$ and $\\pmb{u}_{2}^{\\star}$ disagree on is at most ", "page_idx": 32}, {"type": "equation", "text": "$$\n{\\frac{n\\cdot C(r,\\alpha,K)^{2}}{2n p}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "and therefore the misclassification rate of $\\pmb{u}_{2}$ with respect to the true labeling induced by $L$ and $R$ must be at least ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\frac{n}{4}-\\frac{n\\cdot C(r,\\alpha,K)^{2}}{2n p}}{n}=\\frac{1}{4}-\\frac{C(r,\\alpha,K)^{2}}{2n p}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Since $p\\gtrsim\\log n/n$ , this completes the proof of Theorem 3. ", "page_idx": 32}, {"type": "image", "img_path": "kLen1XyW6P/tmp/637bb583d0876b40081e025d5a46c2f9047641dd64512fdc6fa62ca7259f16da.jpg", "img_caption": ["Figure 2: Agreement with the planted bisection of the bipartition obtained from unnormalized spectral bisection, for graphs generated from a distribution in $\\mathsf{N S S B M}(n,p,\\overline{{p}},q)$ for fixed values of $n,\\overline{{p}}$ and varying values of $p\\,>\\,q$ . The left plot uses $\\overline{{p}}\\,=\\,1/2$ , the right plot uses $\\overline{{p}}\\,=\\,1$ . The solid red curves plot the function $p_{\\mathtt{t h r}}(q)$ (see (14)), and the dashed red curves plot the function $p_{\\mathsf{i n f o}}(q)$ (see (15)). "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "B Additional experiments ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section, we show more numerical trials that complement those discussed in Section 4. ", "page_idx": 33}, {"type": "text", "text": "B.1 Varying edge probabilities in an NSSBM ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In Section 4, we investigated the behavior of an NSSBM model by fixing the values of $p,q$ and varying the largest edge probability $\\overline{{p}}$ . Here, we take an alternative approach, and instead fix $\\overline{{p}}$ and vary the values of $p$ and $q$ . ", "page_idx": 33}, {"type": "text", "text": "Setup. Let us fix $n\\:=\\:2000$ , $\\overline{{p}}~\\in~\\{1/2,1\\}$ . For varying $p,q$ in the range $[1/n,9/20]$ such that $p\\,>\\,q$ , we sample $t\\,=\\,3$ independent draws $G$ from the same benchmark distribution $\\mathcal{D}_{p,\\overline{{p}},q}$ used in Section 4. For each of them, we compute the agreement of the bipartition obtained by unnormalized spectral bisection with respect to the planted bisection. For each $(p,q)$ , we plot the average agreement across the $t$ independent draws. The results are shown in Fig. 2, where in the left and right plot we ran the experiments with $\\overline{{p}}=1/2$ and $\\overline{{p}}=1$ respectively. The lower diagonal of these plots, where $p\\leq q$ , is artificially set to 0. ", "page_idx": 33}, {"type": "text", "text": "Theoretical framing. According to Theorem 1, fixing the value of $\\overline{{p}}\\in\\{1/2,1\\}$ , we obtain that unnormalized spectral bisection achieves exact recovery provided that for $q\\in[1/n,9/20]$ one has $p\\geq p_{\\mathrm{thr}}(q)$ where ", "page_idx": 33}, {"type": "equation", "text": "$$\np_{\\mathsf{t h r}}(q)=\\frac{\\sqrt{\\overline{{p}}\\log n}}{\\sqrt{n}}+q\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "is obtained by rearranging the precondition of Theorem 1, ignoring the constants, and disregarding the fact that $\\alpha$ should be $O(1)$ . The solid red curve in Fig. 2 plots $p_{\\mathtt{t h r}}(q)$ as a function of $q$ . For comparison, the information-theoretic threshold for SSBM [ABH16] demands that $p\\ \\geq\\ p_{\\mathsf{i n f o}}(q)$ where ", "page_idx": 33}, {"type": "equation", "text": "$$\np_{\\mathsf{i n f o}}(q)=\\left(\\sqrt{2}\\sqrt{\\frac{\\log n}{n}}+\\sqrt{q}\\right)^{2}\\,.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The dashed red curve in Fig. 2 plots $p_{\\mathsf{i n f o}}(q)$ as a function of $q$ . ", "page_idx": 33}, {"type": "text", "text": "Empirical evidence. From Fig. 2, one can see that our experiments reflect the behavior predicted by Theorem 1 quite closely, although empirically we achieve $100\\%$ agreement slightly above $p_{\\mathtt{t h r}}(q)$ (i.e. the solid red curve). However, this is likely due to the constant factors from Theorem 1 that we ignored, and also $n=2000$ is plausibly too small to show asymptotic behaviors. Nevertheless, we do achieve $100\\%$ agreement consistently as soon as we surpass the information-theoretic threshold $p_{\\mathsf{i n f o}}(q)$ : in the regime of our experiment, it appears that the unnormalized Laplacian is robust all the way to the optimal threshold for exact recovery in the SSBM. ", "page_idx": 33}, {"type": "image", "img_path": "kLen1XyW6P/tmp/c7e5d46c9371a24ca08cb842ce887a38203f937e534b4d0b778f6cf26a9422e1.jpg", "img_caption": ["Figure 3: Agreement with the planted bisection of the bipartition obtained from several matrices associated with an input graph generated from a distribution $\\mathcal{D}_{q}^{G_{1},G_{2}}\\,\\in\\,\\mathsf{D C M}(n,d_{\\mathsf{i n}},q)$ for fixed values of $n,q$ and varying the size of the planted clique $S$ . In the left plot, the bipartition is the 0-cut of the second eigenvector, as in Algorithm 1. In the right plot, the bipartition is the sweep cut of the first $n/2$ vertices in the second eigenvector. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "B.2 Varying the size of a planted clique in a DCM ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In some sense, the experiments from Section 4 and Appendix B.1 can be thought of as experiments for the deterministic clusters model too. This is because each realization of the internal edges gives rise to a different DCM distribution (see Section 2). We complement our previous discussion by illustrating the behavior of certain families of DCM distributions that are conceptually different than those considered in Section 4. ", "page_idx": 34}, {"type": "text", "text": "Benchmark distribution. Let $n$ be divisible by 4 and let $\\{P_{1},P^{\\,}{-}2\\}$ be a partitioning of $V=[n]$ into two equally-sized subsets. Fix $p\\in[0,1]$ . For some set $S\\subseteq P_{1}$ such that $S=\\{1,\\ldots,|S|\\}$ (for simplicity), let $G_{2}=(P_{2},E_{2})\\sim\\mathsf E\\mathsf{R}(n/2,\\bar{p})$ be a graph drawn from the Erdo\u02dds-R\u00e9nyi distribution with sampling rate $p$ , and let $G_{1}=(P_{1},E_{1})\\sim\\mathsf E\\mathsf{R P C}(n/2,p,S)$ be also a graph drawn from the Erdo\u02dds-R\u00e9nyi distribution with sampling rate $p$ where we additionally plant a clique on the vertices $S$ . Fixing $G_{1},G_{2}$ , for $q~\\in~[0,1]$ we consider the distribution $\\mathcal{D}_{q}^{\\check{G}_{1},\\check{G}_{2}}$ over graphs $G\\,=\\,(V,E)$ where $G[P_{1}]=G_{1}$ , $G[P_{2}]=G_{2}$ , and every edge $(u,v)\\in P_{1}\\times P_{2}^{\\mathrm{u}}$ is sampled independently with probability $q$ . One can see that $\\mathcal{D}_{q}^{G_{1},G_{2}}$ is in fact in the set $\\mathsf{D C M}(n,d_{\\mathsf{i n}},q)$ for some $d_{\\mathrm{in}}$ . ", "page_idx": 34}, {"type": "text", "text": "Setup. Let us fix $n\\,=\\,2000$ , $p\\,=\\,9/\\sqrt{n},\\,q\\,=\\,1/\\sqrt{n}$ . For varying values of $|S|$ in the range $[|P_{1}|\\bar{/}10,|P_{1}|]$ , we sample $G_{1}=(P_{1},E_{1})\\sim\\mathsf E\\mathsf{R P C}(n/2,p,S)$ and $\\tilde{G_{2}}=(P_{2},E_{2})\\stackrel{.}{\\sim}\\mathsf{E R}(n/2,\\bar{p})$ , and then draw $t=10$ independent samples $G$ from $\\vec{D}_{q}^{G_{1},G_{2}}$ . For each sample $G$ , we run spectral bisection (i.e. Algorithm 1) with matrices $\\mathbf{L},\\mathcal{L}_{\\mathsf{s y m}},\\mathcal{L}_{\\mathsf{r w}},\\mathbf{A}$ . Then, we compute the agreement of the bipartition hence obtained with respect to the planted bisection, and average it out across the $t$ independent draws. The results are shown in the left plot of Fig. 3. Again, another natural way to get a bipartition of $V$ from the eigenvector is a sweep cut, and the average agreements that this results in are shown in the right plot of Fig. 3. ", "page_idx": 34}, {"type": "text", "text": "Theoretical framing. Ignoring the constants, Theorem 2 guarantees that exact recovery is achieved by unnormalized spectral bisection as long as $d_{\\mathsf{i n}}\\geq n q+{\\sqrt{n}}$ and $\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\widehat{\\mathbf{L}})\\geq\\sqrt{n}\\,\\dot{+}\\,n q+\\sqrt{n q\\log n}+\\^{\\!\\!1}\\!\\log n.$ , where $\\widehat{\\bf L}$ is the expected Laplacian of $\\mathcal{D}_{q}^{G_{1},G_{2}}$ . For  each cliq ue size that we consider, Fig. 4 shows the  m inimum in-cluster degree of the graphs $G_{1},G_{2}$ that we draw (in the left plot), and the spectral gap $\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\widetilde{\\mathbf{L}})$ . The red horizontal lines in the left and right plot respectively correspond to t he value  of $n q\\,\\pm\\,{\\sqrt{n}}$ and ${\\sqrt{n}}+n q+{\\sqrt{n q\\log n}}+\\log n$ on the $y$ -axis, indicating the lower bound on $d_{\\mathrm{in}}$ and $\\lambda_{3}^{\\widehat{}}(\\widehat{\\mathbf{L}})\\stackrel{\\cdot}{-}\\lambda_{2}(\\widehat{\\mathbf{L}})$ demanded by Theorem 2. ", "page_idx": 34}, {"type": "text", "text": "Empirical evidence: consistency. From Fig. 4, one can see that all the distributions $\\mathcal{D}_{a}^{G_{1},G_{2}}$ that we use roughly meet the requirement of Theorem 2. Indeed, in the left plot of Fig. 3 one sees that unnormalized spectral bisection consistently achieves exact recovery for all clique sizes. On the contrary, the bipartition obtained by running spectral bisection with the adjacency matrix A misclassifies a fraction of the vertices for certain sizes of the planted clique. Nevertheless, the sweep cut obtained from all the matrices recovers the planted bisection exactly. ", "page_idx": 34}, {"type": "image", "img_path": "kLen1XyW6P/tmp/b092b3cd4faa09edf47d62bf0712fa1392ff4373cb7d48a2b8ca3630ce9c532e.jpg", "img_caption": ["Figure 4: The minimum in-cluster degree $d_{\\mathrm{in}}$ and the spectral gap $\\lambda_{3}(\\widehat{\\mathbf{L}})-\\lambda_{2}(\\widehat{\\mathbf{L}})$ of distributions $\\mathcal{D}_{q}^{\\boxtimes,G_{2}}\\in\\mathsf{D C M}(n,d_{\\mathsf{i n},\\,q})$ with fixed values of $n,q$ and varying the size  of the pla nted clique $S$ . The red horizontal line on the left corresponds to the value $n q+{\\sqrt{n}}$ , the red horizontal line on the right corresponds to the value ${\\sqrt{n}}+n q+{\\sqrt{n q\\log n}}+\\log n$ . "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "Empirical evidence: example embedding. Let us fix the value $\\vert{\\cal S}\\vert\\,=\\,800$ for the size of the planted clique, for which we see in Fig. 3 that the adjacency matrix fails to recover the planted bisection. We generate a graph from a distribution $\\mathscr{D}_{q}^{\\check{G}_{1},G_{2}}$ with clique size $\\vert{\\cal S}\\vert\\,=\\,800$ , and plot how the vertices are embedded in the real line by the second eigenvector of all the matrices we consider. The result is shown in Fig. 5, where the three horizontal dashed lines, from top to bottom, respectively correspond to the value of $1/\\sqrt{n},0,-1/\\sqrt{n}$ on the $y$ -axis. Graphically, one can see that the embedding in the unnormalized Laplacian is indeed the one that moves the least away from the values $\\pm1/\\dot{\\sqrt{n}}$ , and in fact the vertices $\\{1,\\dots,800\\}\\subseteq P_{1}$ where we plant the clique concentrate even more around $1/\\sqrt{n}$ . This is a phenomenon related to the one illustrated by Fig. 1. Finally, one can see from the embedding that splitting vertices around 0 does result in misclassifying a fraction of the vertices for the adjacency matrix. However, taking a sweep cut that splits the vertices into two equally sized parts recovers the planted bisection for all matrices. This reflects the results shown in Fig. 3. ", "page_idx": 35}, {"type": "image", "img_path": "kLen1XyW6P/tmp/a1d8c7d2f372173d0ddceb4a96d305e2a888ac5ccfae3d0b28d455c9c8636fb2.jpg", "img_caption": ["Figure 5: Embedding of the vertices given by the second eigenvector $\\pmb{u}_{2}$ of several matrices associated with a graph sampled from a distribution $\\overline{{D_{q}^{G_{1},G_{2}}\\in\\mathsf{D C M}(n,d_{\\mathsf{i n}},q)}}$ , with the size of the planted clique set to $|S|\\,\\stackrel{\\cdot}{=}\\,2/5\\,\\cdot\\,n$ . Horizontal dashed lines, from top to bottom, correspond to $\\bar{1}/\\sqrt{n},0,-\\bar{1}/\\sqrt{n}$ respectively. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "C NeurIPS paper checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Please see Theorem 1, Theorem 2, and Theorem 3 for formal theoretical results. Please see Section 4 and Appendix B for numerical results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Please see Theorem 1, Theorem 2, and Theorem 3 for formal theoretical results that include all assumptions and a corresponding discussion. Please see Section 4.1 for a set of open questions that we do not address in this work. Please see Section 4 and Appendix B for a numerical evaluation of our theoretical results, where we test our theory beyond the statements of our theoretical results. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: Please see the proofs of Theorem 1 (Appendix A.7.1), Theorem 2 (Appendix A.7.2), and Theorem 3 (Appendix A.8), along with all lemmas referenced therein in the appendices. Please also see Section 3 for a proof sketch of all the main results. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Please see Section 4 and Appendix B for details. We have also attached our code. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 38}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We have uploaded our experimental code with our submission. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Please see Section 4 and Appendix B. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: It would be computationally expensive to do so, especially for large graph sizes $n$ . Decreasing $n$ is not feasible because for small values of $n$ , the asymptotic convergence of the algorithms is not evident. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Please see the top of Section 4. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We attest that the research conducted through the course of this work adheres to the NeurIPS Code of Ethics. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This is a theoretical paper on the robustness of a common unsupervised learning algorithm. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: This is a theoretical paper on the robustness of a common unsupervised learning algorithm. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Please see the top of Section 4. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] Justification: NA Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}]