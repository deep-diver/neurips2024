[{"figure_path": "OX4yll3X53/figures/figures_1_1.jpg", "caption": "Figure 1: Gradient flow dynamics and initialization effect for single-layer transformers. (p, q) are Markov switching probabilities, and (e, w) are the embedding and weight parameters (Sec. 2). (a), (c): The flow is aligned along energy contour lines, converging to local or global optima. (b), (d): Ix is the basin of convergence for global minima, Imin for the local minima, and yellow asymptotes for the saddle point. Notice the contrasting behavior for Gaussian initialization around origin for p + q \u2264 1.", "description": "This figure visualizes the gradient flow dynamics and the effect of initialization on the convergence of single-layer transformers trained on next-token prediction loss.  It shows how the convergence to either global or local minima depends on the Markov switching probabilities (p, q) and the initialization of the embedding and weight parameters (e, w). The gradient flow is shown to align with energy contour lines, leading to different optima depending on the initial conditions. The convergence basins for global and local minima, as well as saddle points, are also illustrated. The figure highlights that the behavior near the origin differs significantly depending on whether the sum of the Markov switching probabilities (p+q) is less than or equal to 1.", "section": "3.1 Loss Landscape with Canonical Parameterization"}, {"figure_path": "OX4yll3X53/figures/figures_6_1.jpg", "caption": "Figure 2: Gradient flow dynamics for the canonical parameters \u03b8 = (e, w, a) \u2208 R\u00b3 with the attention scalar a. Notice the contrasting behavior for Gaussian initialization around origin for p + q smaller and greater than one. For an enhanced view of the flow near the origin, please refer to Fig. 5.", "description": "This figure visualizes the gradient flow dynamics for a single-layer transformer with canonical parameters (e, w, a) \u2208 R\u00b3, where 'a' represents the attention scalar.  Panels (a) and (c) show the gradient flow for p+q < 1 and p+q > 1, respectively, illustrating how the flow converges to either local or global minima depending on the initialization and the Markov data properties. Panels (b) and (d) display the corresponding energy manifold and minima locations, further highlighting the influence of initialization and data properties on the convergence behavior.", "section": "4.1 Gradient Flow with Attention"}, {"figure_path": "OX4yll3X53/figures/figures_8_1.jpg", "caption": "Figure 1: Gradient flow dynamics and initialization effect for single-layer transformers. (p, q) are Markov switching probabilities, and (e, w) are the embedding and weight parameters (Sec. 2). (a), (c): The flow is aligned along energy contour lines, converging to local or global optima. (b), (d): Ix is the basin of convergence for global minima, Imin for the local minima, and yellow asymptotes for the saddle point. Notice the contrasting behavior for Gaussian initialization around origin for p + q \u2264 1.", "description": "This figure shows the gradient flow dynamics and the effect of initialization for single-layer transformers.  It visualizes how the parameters (embedding 'e' and weight 'w') converge to either global or local minima depending on the Markov switching probabilities ('p' and 'q'). Panels (a) and (c) show the gradient flow aligned with energy contours, while panels (b) and (d) illustrate the basins of convergence for different optima.  A key observation is the contrasting behavior of Gaussian initialization around the origin for cases where p+q is less than or equal to 1.", "section": "Learning Dynamics"}, {"figure_path": "OX4yll3X53/figures/figures_8_2.jpg", "caption": "Figure 4: Comparison between the average loss curve for the standard gaussian initialization around 0 and our initialization, for p = 0.5 and q = 0.8. Starting from the standard initialization, the model converges to a local minimum corresponding to the unigram model. With our initialization, it converges to the global minimum corresponding to the bigram model.", "description": "This figure compares the training loss curves for a single-layer transformer model trained on a first-order Markov chain with p=0.5 and q=0.8, using two different initialization methods: standard Gaussian initialization and the initialization strategy proposed in the paper. The results demonstrate that the standard initialization leads to convergence to a local minimum representing a unigram model, while the proposed initialization converges to the global minimum corresponding to the true bigram model.", "section": "Empirical Results"}, {"figure_path": "OX4yll3X53/figures/figures_32_1.jpg", "caption": "Figure 2: Gradient flow dynamics for the canonical parameters \u03b8 = (e, w, a) \u2208 R\u00b3 with the attention scalar a. Notice the contrasting behavior for Gaussian initialization around origin for p + q smaller and greater than one. For an enhanced view of the flow near the origin, please refer to Fig. 5.", "description": "This figure shows the gradient flow dynamics for a single-layer transformer with canonical parameters (e, w, a) \u2208 R\u00b3, where 'a' represents the attention scalar.  The left two subfigures (a and b) illustrate the gradient flow for p+q < 1 and the right two subfigures (c and d) illustrate the flow for p+q >1. The contrasting behavior of Gaussian initialization around the origin for these two cases is highlighted. For a more detailed view of the flow near the origin, the reader is referred to Figure 5.", "section": "4.1 Gradient Flow with Attention"}, {"figure_path": "OX4yll3X53/figures/figures_32_2.jpg", "caption": "Figure 2: Gradient flow dynamics for the canonical parameters \u03b8 = (e, w, a) \u2208 R\u00b3 with the attention scalar a. Notice the contrasting behavior for Gaussian initialization around origin for p + q smaller and greater than one. For an enhanced view of the flow near the origin, please refer to Fig. 5.", "description": "This figure shows gradient flow dynamics for single-layer transformers with first-order Markov chains.  The parameters are (e, w, a), representing embedding, weight, and attention, respectively. The plots illustrate how the gradient flow behaves differently depending on the switching factor (p+q) and initial parameter values. Notably, the behavior differs significantly for Gaussian initialization near the origin; it contrasts between p+q > 1 and p+q < 1.  Figure 5 provides a closer look at the dynamics near the origin.", "section": "4.1 Gradient Flow with Attention"}, {"figure_path": "OX4yll3X53/figures/figures_33_1.jpg", "caption": "Figure 1: Gradient flow dynamics and initialization effect for single-layer transformers. (p, q) are Markov switching probabilities, and (e, w) are the embedding and weight parameters (Sec. 2). (a), (c): The flow is aligned along energy contour lines, converging to local or global optima. (b), (d): Ix is the basin of convergence for global minima, Imin for the local minima, and yellow asymptotes for the saddle point. Notice the contrasting behavior for Gaussian initialization around origin for p + q \u2264 1.", "description": "This figure shows the gradient flow dynamics and the effect of initialization on the convergence of single-layer transformers trained on next-token prediction loss.  The plots visualize the gradient flow (a, c) and convergence basins (b, d) in the parameter space (embedding parameter 'e' and weight parameter 'w') for different Markov switching probabilities ('p' and 'q'). The behavior of the system changes depending on whether the sum of switching probabilities (p+q) is less than or greater than 1. For p+q \u2264 1, Gaussian initialization around the origin leads to convergence to global minima; for p+q > 1, Gaussian initialization can lead to local minima.", "section": "Learning Dynamics"}, {"figure_path": "OX4yll3X53/figures/figures_34_1.jpg", "caption": "Figure 1: Gradient flow dynamics and initialization effect for single-layer transformers. (p, q) are Markov switching probabilities, and (e, w) are the embedding and weight parameters (Sec. 2). (a), (c): The flow is aligned along energy contour lines, converging to local or global optima. (b), (d): Ix is the basin of convergence for global minima, Imin for the local minima, and yellow asymptotes for the saddle point. Notice the contrasting behavior for Gaussian initialization around origin for p + q \u2264 1.", "description": "This figure shows the gradient flow dynamics and the effect of initialization on single-layer transformers' convergence to either local or global minima. The plots visualize how the gradient flow (aligned along energy contour lines) converges depending on the Markov switching probabilities (p,q) and the transformer parameters (embedding (e) and weight (w)).  Panels (a) and (c) illustrate gradient flow for different (p,q) values. Panels (b) and (d) show the convergence basins (regions of initialization leading to specific optima) for the same (p,q) values, highlighting the role of initialization in reaching either global or local minima.", "section": "Learning Dynamics"}]