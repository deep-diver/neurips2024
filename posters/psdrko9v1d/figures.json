[{"figure_path": "psDrko9v1D/figures/figures_1_1.jpg", "caption": "Figure 1: The heat diffusion optimization (HeO) framework. The efficiency of searching a key in a dark room is significantly improved by employing navigation that utilizes heat emission from the key. In our framework, heat diffusion transforms the target function of a combinatorial optimization problem into different versions while preserving the location of the optima. Therefore, the gradient information of these transformed functions cooperatively help to optimize the original target function.", "description": "This figure illustrates the core idea of the HeO framework using an analogy.  Finding a key in a dark room represents a combinatorial optimization problem.  Without additional information (like light), searching is inefficient.  HeO introduces heat diffusion, analogous to the key emitting heat.  This heat spreads, providing gradient information from distant areas of the solution space to guide the search, improving efficiency. The figure visually represents how the heat diffuses over time, guiding the search towards the solution.  The method's cooperative optimization is highlighted.", "section": "1 Introduction"}, {"figure_path": "psDrko9v1D/figures/figures_5_1.jpg", "caption": "Figure 2: Performance of HeO (Alg. 3, Appendix), Monte Carlo gradient estimation (MCGE), Hopfield neural network (HNN) and simulated annealing (SA) on minimizing the output of a neural network (Eq. (13)). Top panel: the target function. Bottom panel: the uncertainty V(\u03b8) (Eq. (4)).", "description": "This figure compares the performance of the proposed Heat Diffusion Optimization (HeO) method against three other methods: Monte Carlo Gradient Estimation (MCGE), Hopfield Neural Network (HNN), and Simulated Annealing (SA) in minimizing a neural network's output.  The top panel shows the energy (target function) over time steps, and the bottom panel displays the uncertainty of the solution over time steps. HeO demonstrates superior performance in both minimizing the target function and reducing uncertainty.", "section": "Failure of gradient-based combinatorial optimization"}, {"figure_path": "psDrko9v1D/figures/figures_5_2.jpg", "caption": "Figure 3: a, Illustration of the max-cut problem. b, Performance of HeO (Alg. 1) and representative iterative approximation methods including LQA [10], aSB [12], bSB [13], dSB [13], CIM [35] and SIM-CIM [15] on max-cut problems from the Biq Mac Library [36]. Top panel: average relative loss for each algorithm over all problems. Bottom panel: the count of instances where each algorithm ended up with one of the bottom-2 worst results among the 7 algorithms.", "description": "Figure 3(a) illustrates the max-cut problem, which involves partitioning the nodes of a graph into two sets to maximize the number of edges between the sets.  Figure 3(b) compares the performance of the proposed Heat Diffusion Optimization (HeO) algorithm against six other iterative approximation methods (LQA, aSB, bSB, dSB, CIM, SIM-CIM) on a set of max-cut problems from the Biq Mac Library. The top panel shows the average relative loss for each algorithm across all problems, while the bottom panel displays the number of instances where each algorithm performed among the two worst.", "section": "Experiments"}, {"figure_path": "psDrko9v1D/figures/figures_6_1.jpg", "caption": "Figure 4: a, Illustration of the boolean 3-satisfiability (3-SAT) problem. b, Performance of HeO (Alg. 4, Appendix), 2-order and 3-order oscillation Ising machine (OIM) [16] on 3-SAT problems with various number of variables from the SATLIB [37]. We report the mean percent of constraints satisfied (left) and probability of satisfying all claims (right) for each algorithm.", "description": "This figure illustrates the Boolean 3-satisfiability (3-SAT) problem and presents a performance comparison of different algorithms, namely HeO, 2-order OIM, and 3-order OIM, for solving 3-SAT problems with varying numbers of variables.  Subfigure (a) shows a visual representation of the 3-SAT problem using a circuit diagram. Subfigure (b) shows two plots: the first showing the mean percentage of satisfied constraints, and the second displaying the log of the probability of satisfying all constraints, as a function of the number of variables. Error bars are included.", "section": "Experiments"}, {"figure_path": "psDrko9v1D/figures/figures_7_1.jpg", "caption": "Figure 5: a, Training networks with ternary-value parameters. b, The weight value accuracy of the HeO (Alg. 5, Appendix) and Monte Carlo gradient estimation (MCGE) with momentum under different sizes of training set (n = 100, m = 1, 2, 5). We estimate the mean and std from 10 runs.", "description": "Figure 5 shows the results of training neural networks with ternary-value parameters using the proposed HeO algorithm and the conventional MCGE method.  Panel (a) illustrates the training process, depicting the ternary weight matrix W, input vector v, ReLU activation, output vector y, and the resulting trained weight matrix W'. Panel (b) presents the accuracy results for different output dimensions (m = 1, 2, 5) and varying training set sizes, demonstrating the superior performance of HeO in terms of weight accuracy.", "section": "Ternary optimization"}, {"figure_path": "psDrko9v1D/figures/figures_7_2.jpg", "caption": "Figure 6: The variable selection of 400-dimensional linear regressions using HeO (Alg. 6, Appendix), Lasso (L1) regression [40] and L0.5 regression [41]. We report the accuracy of each algorithm in determining whether each variable should be ignored for prediction and their MSE on the test set. The mean (dots) and standard deviation (bars) are estimated over 10 runs.", "description": "This figure compares the performance of HeO against Lasso (L1) and L0.5 regression methods for variable selection in 400-dimensional linear regression.  It shows the accuracy of each method in identifying irrelevant variables (those with zero coefficients in the true model) and the mean squared error (MSE) on test data, for various sparsity levels (controlled by the parameter q) and noise levels (controlled by the parameter \u03c3e).  The results, averaged across 10 runs, demonstrate HeO's superior accuracy and lower MSE compared to the other methods.", "section": "Mixed combinatorial optimization"}, {"figure_path": "psDrko9v1D/figures/figures_8_1.jpg", "caption": "Figure 1: The heat diffusion optimization (HeO) framework. The efficiency of searching a key in a dark room is significantly improved by employing navigation that utilizes heat emission from the key. In our framework, heat diffusion transforms the target function of a combinatorial optimization problem into different versions while preserving the location of the optima. Therefore, the gradient information of these transformed functions cooperatively help to optimize the original target function.", "description": "This figure illustrates the core idea of the Heat Diffusion Optimization (HeO) framework using an analogy.  Searching for a key in a dark room represents solving a combinatorial optimization problem.  The traditional method (touching around) is slow and inefficient.  HeO introduces heat diffusion, where the key emits heat, allowing the person to efficiently locate the key (optima).  The heat diffusion transforms the target function, creating a temperature gradient which helps guide the solver to the optimum more efficiently.", "section": "1 Introduction"}, {"figure_path": "psDrko9v1D/figures/figures_17_1.jpg", "caption": "Figure S1: The time cost per iteration (ms) of the HeO framework increases linearly with the dimensionality of the problem. We present the results averaged over five tests, with error bars representing three standard deviations.", "description": "This figure shows the time cost per iteration of the Heat Diffusion Optimization (HeO) framework plotted against the dimensionality of the problem being solved.  The results are averaged over five independent runs, and error bars (representing three standard deviations) illustrate the variability in the measurement. The linear relationship indicates that the computational cost of HeO scales linearly with problem size. This is important because it suggests HeO\u2019s efficiency remains consistent even when tackling large-scale combinatorial optimization problems.", "section": "Appendix"}, {"figure_path": "psDrko9v1D/figures/figures_17_2.jpg", "caption": "Figure 2: Performance of HeO (Alg. 3, Appendix), Monte Carlo gradient estimation (MCGE), Hopfield neural network (HNN) and simulated annealing (SA) on minimizing the output of a neural network (Eq. (13)). Top panel: the target function. Bottom panel: the uncertainty V(\u03b8) (Eq. (4)).", "description": "This figure compares the performance of the proposed HeO algorithm against three other optimization algorithms: Monte Carlo gradient estimation (MCGE), Hopfield neural network (HNN), and simulated annealing (SA).  The task is to minimize the output of a neural network, which serves as a toy example for demonstrating the capabilities of HeO. The top panel shows the target function and the bottom panel displays the uncertainty in the output distribution, providing a measure of how efficiently each algorithm reduces uncertainty. HeO exhibits superior performance compared to the other algorithms.", "section": "Experiments"}, {"figure_path": "psDrko9v1D/figures/figures_18_1.jpg", "caption": "Figure S3: Verifying the cooperative optimization mechanism of HeO. The best cut value over 10 runs for each algorithm on the K-2000 problem [14] when the control parameters are randomly perturbed by different random perturbation level \u03b4. The red dash line is the best cut value ever find.", "description": "This figure shows the result of max-cut problem on K-2000 dataset when the control parameters are perturbed with different random perturbation level (\u03b4). The x-axis represents the random perturbation level. The y-axis represents the best cut value among 10 runs. The red dash line represents the best known cut value. The figure shows that the performance of HeO is more robust to the random perturbation compared to other algorithms. This result verifies that HeO has a cooperative optimization mechanism.", "section": "F Cooperative optimization"}]