[{"type": "text", "text": "Understanding Bias in Large-Scale Visual Datasets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Boya Zeng\u2217 Yida Yin\u2217 Zhuang Liu University of Pennsylvania UC Berkeley Meta FAIR \u2217equal contribution ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A recent study [39] has shown that large-scale pretraining datasets are very biased: they can be easily classified by modern neural networks. However, the concrete forms of bias among these datasets remain unclear. In this study, we propose a framework to identify the unique visual attributes distinguishing these datasets. Our approach applies various transformations to extract semantic, structural, boundary, color, and frequency information from datasets and assess how much each type of information contributes to their bias. We further decompose their semantic bias with object-level queries, and leverage natural language methods to generate detailed, open-ended descriptions of each dataset\u2019s characteristics. Our work aims to help researchers understand the bias in existing large-scale datasets and build more diverse and representative ones in the future. Our project page and code are available at boyazeng.github.io/understand_bias. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, Liu and He [39] revisited the \u201cName That Dataset\u201d experiment introduced by Torralba and Efros [67] in 2011 which highlighted the built-in bias of visual datasets. It is a classification task where each dataset forms a class, and models are trained to predict the dataset origin of each image. The datasets back in 2011 were found to be classified very accurately [67]. Since then, significant efforts have been devoted to creating more diverse, large-scale, and comprehensive visual datasets [15, 36, 34, 59]. Surprisingly, a decade later, the largest and supposedly most diverse datasets (e.g., YFCC [64], CC [11], DataComp [18]) can still be classified with remarkably high accuracy [39]. ", "page_idx": 0}, {"type": "text", "text": "Although we now know these large-scale datasets are very biased, a lingering question remains: what are the concrete forms of bias among them1, that cause them to be easily classified? Understanding the bias among datasets is essential for addressing it and improving dataset diversity and coverage. Creating datasets that more comprehensively represent the real world is challenging yet crucial [63, 54, 30]. Only then, can we build truly general-purpose vision models\u2014models capable of handling different scenarios out of the box, and performing reliably in various real-world situations [29, 28, 23]. ", "page_idx": 0}, {"type": "text", "text": "To this end, we develop a framework for understanding the concrete forms of bias among datasets. We isolate the semantic, structure, boundary, color, and frequency information through various transformations. For example, transforming an image into a semantic segmentation map preserves semantics while discarding most texture information. We then perform the dataset classification task on the transformed datasets to quantify how each type of information reflects the bias. ", "page_idx": 0}, {"type": "text", "text": "To pinpoint the semantic bias in datasets, we employ object-level queries and open-ended language analysis. Specifically, we leverage pretrained vision models to identify objects that characterize each dataset. In addition, using a Vision-Language Model (VLM), we generate image captions as surrogate language representations of the images. We then apply topic models and Large Language Models (LLMs) to generate natural language descriptions for each dataset. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We apply our framework to three popular large-scale visual datasets: YFCC, CC, and DataComp, following [39]. Surprisingly, after transforming images into various semantic and structure representations (e.g., object bounding boxes and contours), neural networks can still accurately predict their dataset origin. This highlights the bias in semantics and object shapes. Our object-level queries further reveal a discrepancy in object diversity and distribution across the YCD datasets. Lastly, open-ended language analysis indicates that YFCC emphasizes outdoor and natural scenes with human interactions, while DataComp features digital graphics heavily. ", "page_idx": 1}, {"type": "text", "text": "Our framework operates on images only and does not require any human annotations, making it compatible with any vision dataset. It can be applied in future dataset curation to assess data diversity, and guide the addition of images with various attributes. We hope this work can help researchers identify dataset bias and develop more inclusive and diverse visual datasets. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Dataset classification. The dataset classification problem was originally proposed by Torralba and Efros [67] in 2011. Tommasi et al. [66] later also studied this problem with linear classifiers using pretrained CNN features. In contrast to prior work focusing on labeled smaller-scale datasets with shared object classes, Liu and He [39] recently revisited the dataset classification problem with large-scale, diverse, and potentially less-biased datasets. They demonstrated that modern neural networks are still excellent at capturing bias among these datasets. Our work identifies the exact forms of the bias among datasets beyond dataset classification results. ", "page_idx": 1}, {"type": "text", "text": "Social bias and fairness. Extensive literature has highlighted that various visual datasets underrepresent certain demographic groups [68, 73, 50], contain various gender stereotypes [79, 27, 44], or ignore some geographical regions [61, 73, 69]. These social fairness issues in datasets result in the deployment of flawed models [8, 70, 6] that may produce biased predictions or struggle to generalize well across different domains. Instead of focusing on social fairness in each dataset, we study a different concept of visual representativeness (i.e., coverage of real-world concepts and objects) and understand its differences among datasets. Note Meister et al. [44] also use transformations to isolate different types of information, with a specific focus on gender bias in datasets. ", "page_idx": 1}, {"type": "text", "text": "Bias detectors and debiasing tools. There are approaches that can locate the imbalance of object representation within datasets [22, 69]. Dataset re-balancing methods [8, 73] seek to correct these representation imbalances across protected attributes. Algorithmic intervention and regularization, such as adversarial training [76, 77, 43] and domain-independent training [71], can counteract the propagation of bias and stereotypes in downstream modeling. Note that these prior methods often require ground-truth annotations to identify or mitigate potential bias, while our framework can analyze unlabeled pretraining datasets and provide insights beyond object distribution imbalances. ", "page_idx": 1}, {"type": "text", "text": "3 Isolating Bias with Transformations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Although modern neural networks can achieve excellent accuracy in the dataset classification problem [39], what bias is captured by neural networks remains unclear. To better understand this, we selectively preserve or suppress specific types of information using various transformations. We then train a new model on the transformed datasets to perform the dataset classification task. As a result, its dataset classification performance indicates the level of bias in the extracted information. For example, transforming an image into a depth map captures the spatial geometry while discarding texture, allowing us to assess bias present solely in spatial information. ", "page_idx": 1}, {"type": "text", "text": "3.1 Datasets and Settings ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Building on Liu and He [39], we take YFCC [64], CC [11], and DataComp [18] (collectively referred to as \u201cYCD\u201d) and study their bias in this work. Figure 1 illustrates some examples. ", "page_idx": 1}, {"type": "text", "text": "Our training setup is also adapted from [39]. Specifically, we randomly sample 1M and 10K images from each dataset as training and validation sets, respectively. We also employ the same image classification model ConvNeXt-Tiny [40] and train it for 30 epochs on the combined dataset with 3M samples. Note the original work [39] achieves $84.7\\%$ accuracy on the validation set, while we reproduce an accuracy of $82.0\\%$ due to shorter training. We refer to this reproduced accuracy as reference accuracy in this paper. For all the image transformations, we use the same base classifier model. We repeat the data sampling and experiments three times and report the mean validation accuracy and the standard deviation. The training settings are detailed in Appendix A.1. ", "page_idx": 1}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/7a6123cb70e2fbb09a4b82d252b2446a5c1453753641d6abda2b79c036bdb698.jpg", "img_caption": ["Figure 1: Original images. We sample two images from each of YFCC [64], CC [11], and DataComp [18]. Dataset classification on the original images has a reference accuracy of $82.0\\%$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3.2 Semantics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To start, we seek to understand how semantically biased the datasets are. Specifically, we extract semantic components from the images with fine-grained, coarse-grained, or no spatial detail. Also, we apply a variational autoencoder, potentially reducing low-level signatures (e.g., color quantization, JPEG compression). Figure 2 shows the transformations and their dataset classification accuracies. ", "page_idx": 2}, {"type": "text", "text": "Semantic segmentation offers fine-grained semantic annotation with rich object information by labeling each pixel as a class. We take a semantic segmentation model ViT-Adapter-Large [12] (with BEiT-v2 [52] backbone) trained on ADE20K [81] with 150 semantic classes (e.g., wall, building, sky, etc.) to generate a 150-channel binary semantic segmentation mask for each image. In Figure 2, we visualize the segmentation masks in a color palette (rather than in 150-channel). The model trained on this mask achieves $67.6\\%$ accuracy, well above the chance level. ", "page_idx": 2}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/6f9b9f2f4e6be6e2d610a42dcddbe690e2fb3787ec4be558d4a06ea82d04a7cf.jpg", "img_caption": ["Figure 2: Transformations preserving semantic information (semantic segmentation, object detection, and caption) and potentially reducing low-level signatures (VAE) result in high dataset classification accuracy. This suggests that semantic discrepancy is an important form of dataset bias. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Object detection provides coarse spatial annotations for objects through rectangle bounding boxes. We use ViTDet-Huge [35] trained on LVIS [24] with 1203 object categories to derive bounding boxes ", "page_idx": 2}, {"type": "text", "text": "from each image. We add object class names to the bounding boxes to account for semantic meaning.   \nThis representation reaches $61.9\\%$ dataset classification accuracy, below semantic segmentation. ", "page_idx": 3}, {"type": "text", "text": "Image captioning discards all visual information and produces semantic representations through natural language. Captions are less affected by pixel variations and spatial cues in images. Using LLaVA 1.5 [38, 37], we generate a single-sentence caption and a long paragraph caption for each image. Short captions are shown in Figure 2, and long captions are in Appendix D. We finetune a sentence embedding model Sentence T5-base [49] to perform dataset classification on these captions. This results in $63.8\\%$ accuracy with short captions and $66.1\\%$ with long ones, both nearing the accuracy for semantic segmentation. The richer details in longer captions enhance performance. ", "page_idx": 3}, {"type": "text", "text": "Variation autoencoder (VAE) [32] encodes each image into a latent vector and then reconstructs the original image from it. The datasets may use different JPEG compressions or image resolutions, which could be exploited as shortcuts by dataset classification models. However, VAE\u2019s low-dimensional latent space may encode semantic information and suppress such low-level signatures. Reconstructing the images with a pretrained VAE from Stable Diffusion [57] only slightly decreases the accuracy from $82.0\\%$ to $77.4\\%$ , suggesting that low-level bias may have a smaller impact than semantic bias. ", "page_idx": 3}, {"type": "text", "text": "Semantic segmentation, object detection, and caption extract semantic information with decreasing levels of spatial information. On the other hand, VAE could potentially reduce low-level signatures while preserving the semantics. The high accuracies of dataset classification models indicate that semantic bias is an important component of dataset bias in YCD. ", "page_idx": 3}, {"type": "text", "text": "3.3 Structures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Next, we analyze the dataset bias rooted in object shape and spatial geometry rather than object semantics. To capture such structural visual bias, we use the Canny edge detector and the Segment Anything Model (SAM) [33] to outline object contours, and the Depth-Anything-V2 model [74] to measure pixel-level depth. While contour delineates fine-grained object shape, and depth estimation offers relative object positions, both lack the rich object semantic details present in semantic segmentation masks and bounding boxes (e.g., object class). Figure 3 visualizes the transformations. ", "page_idx": 3}, {"type": "text", "text": "Canny edge detector [10] is a classical algorithm that outlines rough object boundaries by capturing sharp intensity changes. It removes noise with a Gaussian filter, calculates intensity gradients, and applies non-maximum suppression to form edges, represented as a binary mask. This results in $71.0\\%$ accuracy, $11\\%$ below the reference accuracy $(82.0\\%)$ . ", "page_idx": 3}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/35a885a9b27a51a734ea355663f9c5cb1134d41bc3849724703979fc9c1d399f.jpg", "img_caption": ["Figure 3: Transformations outlining object shapes and estimating pixel depth. Dataset classification achieves even higher accuracies on object contours and depth images than on semantic information, indicating that object shapes and spatial geometry vary significantly across YCD. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Segment Anything Model (SAM) [33] can provide high-quality object segmentation masks. We could then use them to delineate cleaner and more accurate shapes of objects that are minimally affected by local pixel variations, compared to the Canny edge detector. Specifically, we use SAM with the ViT-Large backbone to generate class-agnostic segmentation masks and identify boundaries by finding pixels whose surrounding pixels are not all from the same object. The accuracy on SAM contours $(73.2\\%)$ is slightly higher than the one using the Canny edge detector $(71.0\\%)$ . ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Depth estimation captures the scene\u2019s spatial geometry, offering fine-grained spatial context and relative object positioning. Like contours, it excludes explicit semantic information about objects. The Depth-Anything-V2 (ViT-L) model [74] generates pixel-level depth estimation, encoded as a normalized grayscale image. The resulting $73.1\\%$ accuracy is comparable to that of SAM contours. ", "page_idx": 4}, {"type": "text", "text": "The dataset classification accuracies on object contours and depth are even higher than the ones on semantics. This shows that object shape and spatial geometry variations are significant among YCD. ", "page_idx": 4}, {"type": "text", "text": "3.4 Spatial Permutations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To further understand the level of bias captured in spatial information as opposed to semantics, we keep the RGB values of all pixels in each image unchanged but shuffle the pixel positions to disrupt spatial coherence. We shuffle each image on the pixel level and the patch level, following a fixed order and a random order for all images. Figure 4 shows the images shuffled in a random order. ", "page_idx": 4}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/1604516ae345b05adf2a53f24db3ac3d05d51dcd4c0cd6fdb7011409683078c7.jpg", "img_caption": ["Figure 4: Transformations breaking spatial structure. Pixel shuffilng drastically decreases dataset classification accuracy, but patch shuffilng has minimal impact. This demonstrates that local structure is important and sufficient for models to learn the patterns of each dataset. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Pixel shuffilng obfuscates the image classifier with a permutation of the pixels and forces it to find patterns from the color distribution of pixels in each image. As expected, this significantly decreases the accuracy to $52.2\\%$ for the random order and $58.5\\%$ for the fixed order. ", "page_idx": 4}, {"type": "text", "text": "Patch shuffling first divides each image into smaller patches and then rearranges the order of patches. Consequently, it preserves more local spatial information. Here we vary the patch size and show the results in Figure 5. We note that the performances of the fixed order and the random order are almost identical with a patch size larger than 1. Surprisingly, with a patch size of 16, we reach almost the same accuracy as the reference one. ", "page_idx": 4}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/05c2e4c072c39d9e2adc6f65117af52425d1f34970c6b692fa674f7200fce158.jpg", "img_caption": ["Figure 5: Effect of patch sizes. Accuracy approaches the reference one with larger patch sizes. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The significant performance drop with pixel shuffilng shows completely destructing the local structure in YCD can reduce its dataset bias to a large extent. However, the minimal accuracy decrease after shuffling patches of size 16 indicates patch-level local structures in spatial information is sufficient for identifying visual signatures of the YCD datasets. ", "page_idx": 4}, {"type": "text", "text": "3.5 RGB ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The high classification accuracy after pixel shuffilng implies a discrepancy in pixel color distributions among YCD. To further assess this difference in color statistics among datasets, we transform each image into its average value for each color channel. Figure 6 shows the resulting images. ", "page_idx": 4}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/5996199e28e4c1e189a1daf0f1981fb22bb8eb03fd52667971eff3afa2320577.jpg", "img_caption": ["Figure 6: Averaging each color channel. Even when the values of each channel in images are averaged, the model can still achieve non-trivial dataset classification performance. "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/d17612be35b75f058ec32735df228111d183c4e848c9a091b23d9db1d403c253.jpg", "img_caption": ["Figure 7: Distribution of mean RGB values and confusion matrix. YFCC\u2019s RGB values are overall smaller, while CC\u2019s and DataComp\u2019s are very similar. This is also reflected in the confusion matrix of dataset classification on mean RGB images, where YFCC can be classified very easily (indicated by the dark blue box on the top left), while there is high confusion between CC and DataComp. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Mean RGB. We compute the mean RGB value for each image. This abstracts the pixel details into a constant RGB color map and forces the image classifier to only use color statistics. The model\u2019s accuracy on RGB mean images is $48.5\\%$ , about $15\\%$ higher than the chance-level accuracy of $33.3\\%$ . ", "page_idx": 5}, {"type": "text", "text": "In Figure 7, we visualize the distribution of mean RGB values for YCD. There is only a moderate difference in mean RGB distribution between CC and DataComp. However, YFCC is much darker than CC and DataComp. This is further suggested by the confusion matrix of the dataset classification model trained on mean RGB images shown in Figure 7, where the model classifies YFCC accurately but shows more confusion when distinguishing between CC and DataComp. ", "page_idx": 5}, {"type": "text", "text": "3.6 Frequency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Neural networks tend to exploit texture patterns even when the recognition task is inherently about the semantics [31, 20, 19]. If we decompose visual signals by frequencies, high-frequency bands typically capture these texture patterns and sharp transitions, whereas low-frequency components represent general structure and smooth variations. To explore how different frequency components contribute to dataset bias, we apply high-pass and low-pass filters to the original images. ", "page_idx": 5}, {"type": "text", "text": "High-pass filter and low-pass filter. To filter signals based on frequencies, we first perform a 2D Fast Fourier Transform on each grayscaled image to obtain its representation in the frequency domain. We then apply an ideal filter [21] with a hard threshold radius of 40 in the frequency domain, so as to only keep either high (i.e., high-pass filter) or low (i.e., low-pass filter) frequencies. The filtered results are finally inversely transformed to the original grayscale domain, visualized in Figure 8. Additional results and visualizations are in Appendix B.6. ", "page_idx": 5}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/d99d28d431dbb6a894a0a724e273b4188abd6cb93e01c1dd05b18ea5d2746a15.jpg", "img_caption": ["Figure 8: Transformations flitering high-frequency and low-frequency components retain closeto-reference accuracy. This indicates that dataset bias exists in different frequencies. The high-pass filtered images are equalized for better visualization. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "The model trained on images with high frequencies kept has an accuracy of $79.2\\%$ . This is slightly better than the one trained on images with low frequencies kept, which has an accuracy of $70.4\\%$ . Both accuracies are close to the reference accuracy of $82.0\\%$ . ", "page_idx": 6}, {"type": "text", "text": "The high accuracy of models trained on either frequency component indicates that visual bias in the YCD datasets exists in both low-frequency and high-frequency components. ", "page_idx": 6}, {"type": "text", "text": "3.7 Synthetic Images ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Diffusion models [55, 45, 57] can generate synthetic images from a learned distribution. Synthetic images hold significant potential in augmenting data for a variety of vision tasks [26, 3, 65, 1, 5]. However, if dataset bias is inherited in the generation process, bias may be amplified as generated images are added to the training set. To understand this, we run dataset classification on synthetic images generated from unconditional and text-to-image diffusion models, illustrated in Figure 9. ", "page_idx": 6}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/063bc48411bc98e87f6a8337e4c9d774b292a36d5dd347e435d1c15b59938430.jpg", "img_caption": ["Figure 9: Synthetic images from unconditional and text-to-image generation models. Unconditionally generated images can be classified with near-reference accuracy. Images from text-to-image diffusion models using short captions have reasonable dataset classification accuracy. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Unconditional generation. To investigate whether bias can be encoded in the unconditional generation, we train an unconditional Diffusion Transformer (DiT) [51] on each individual dataset in YCD. The model captures a latent distribution over the images and we can generate new images by sampling from this distribution. Dataset classification is performed on the combination of synthetic data from each model, resulting in a very high accuracy of $77.6\\%$ . ", "page_idx": 6}, {"type": "text", "text": "Text-to-Image generation potentially preserves the semantic bias in the original images. We generate synthetic images from the popular SDXL-Turbo [58] diffusion model, condition on short captions produced by LLaVA (Section 3.2). Note unlike the unconditional generation above, we did not train our own text-to-image model for each dataset. By mapping the original images into an intermediate text modality and then back to the visual domain, we only retain the semantics captured in captions. We reach $58.1\\%$ accuracy, falling slightly short of $63.8\\%$ when directly classifying short captions. ", "page_idx": 6}, {"type": "text", "text": "The high classification accuracy from unconditionally generated images shows that bias can be inherited in the synthetic images. The ability to classify text-conditional synthetic images further confirms that semantic discrepancy is a major contributor to dataset bias. ", "page_idx": 6}, {"type": "text", "text": "Summary. We analyzed the impact of various transformations on dataset classification, identifying semantics and structures as important contributors to dataset bias. Patch-level local structure information is sufficient to classify YCD, with datasets differing even in color statistics. Bias spans across frequency components, particularly in high-frequency bands. Finally, we showed that bias can be inherited in synthetic images of diffusion models. More results are in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "4 Explaining Semantic Bias among Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the preceding section, we explore transformations to extract various types of image information, each exhibiting varying levels of bias. Among them, semantic bias heavily contributes to the high accuracy in the dataset classification task [39]. In this section, we identify specific interpretable semantic patterns within each dataset through object-level and language-based analysis. ", "page_idx": 6}, {"type": "text", "text": "4.1 Object-level Queries ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/9b114b930ce8f8de473047e2bbf5fe522de4219d7b36502171d2cab20237c5d4.jpg", "img_caption": ["Figure 10: Grad-CAM heatmap [80, 60] on the dataset classification model trained on original datasets. It focuses on specific objects to determine the dataset origin of each image. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Grad-CAM [80, 60] highlights key regions in an input image that explain neural network predictions. In Figure 10, applying Grad-CAM to the reference model (Section 3.1) shows that the model focuses on semantically meaningful objects: elephant herd in the third image, table and chair in the fourth image, and pen in the sixth image. This suggests that the model might have leveraged the object-level information to recognize the dataset identity of each image. To better understand this, we apply models pretrained on other vision datasets (ImageNet-1K [15], LVIS [24], and ADE20K [81]) to provide object annotations for each image in the datasets, and analyze their object-level bias. As a result, the analysis below is in the context of 3 sets of object classes, defined by these 3 datasets. ", "page_idx": 7}, {"type": "text", "text": "Imbalanced object distribution. Imbalance in object distribution is a common form of semantic bias. For each object class, we calculate the number of images in each dataset that contain the object class and their percentage share relative to all images with that object class. Note for LVIS and ADE20K models\u2019 output, we counted each object class only once per image, even if multiple instances or pixels of the same object class are present. Figure 11 shows the top 8 object classes with the highest percentage of images from YFCC, CC, or DataComp. The dominance of a certain dataset within these classes highlights a considerable imbalance in object-level distribution across datasets. ", "page_idx": 7}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/7db0d3a0c7815bf41b93a449a2986114347891fa452c231fd6c23d58bb88dbfd.jpg", "img_caption": ["Figure 11: Object classes with the highest proportions of YFCC, CC, or DataComp images. Less-frequent classes are not shown. Most classes consist predominantly of images from one dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 11 also shows that YFCC constitutes much higher proportions in its top object classes than CC and DataComp in their respective classes (note the different x-axis scales in each subplot). To further see this, we visualize the distribution of unique object class counts per image in Figure 12. The higher variety of objects in YFCC images shows a notable gap in object diversity among YCD. ", "page_idx": 7}, {"type": "text", "text": "Interpretable dataset classification with objects. The coefficients of a logistic regression model form a natural importance ranking of input features when the features are binary. To leverage this, we represent each image with a binary vector, where each element indicates the presence of a specific object class from a set of objects (i.e., ImageNet, LVIS, or ADE20K). We train a logistic regression model to predict the dataset origin of the images based on their binary vector representations. This simple model achieves validation accuracies of $52.0\\%$ with ImageNet objects, $52.4\\%$ with LVIS objects, and $52.4\\%$ with ADE20K objects ", "page_idx": 7}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/e3d5b40bd79b94e26cf62cae2ba4ef0dee17538bd0ce6b5a44d6dce2c15f0685.jpg", "img_caption": ["Figure 12: Unique object classes per image. On average, YFCC contains the highest number of unique objects in each image, followed by CC, while DataComp exhibits the lowest. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 13 shows the top objects based on logistic regression coefficients. It highlights outdoor infrastructures (e.g., traffic light, clock tower, telephone pole, and building) in YFCC and household ", "page_idx": 7}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/179b187566f7fb2e5a5c28cf3573494bcbf1da26ad169179ef66a410578da977.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 13: Object class ranking from logistic regression coefficients. The regression classifies images based on object presence. YFCC has more top objects related to outdoor scenes, while CC and DataComp focus on household items and products. Classes with low frequencies are not shown. ", "page_idx": 8}, {"type": "text", "text": "items, products, and digital graphics (e.g., doll, ring, vase, blazer, and website) in CC and DataComp. These rankings partially overlap with the list of objects with the highest proportions among the YCD datasets (Figure 11). However, the object rankings also identify objects that are more balanced across datasets, since logistic regression receives more weight updates based on the more common objects during training. Thus, it provides a complementary angle on object-level bias among the datasets. ", "page_idx": 8}, {"type": "text", "text": "4.2 Open-ended Language Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The word clouds [47] in Figure 14 offer a visual representation of the most prevalent phrases in the long paragraph captions of each dataset from Section 3.2. We observe several frequent phrases featuring human subjects (e.g., people, group, wearing) in YFCC, elements of indoor scenes (e.g., room, and dining table) in CC, and a focus on white background in DataComp. In this section, we will use the long captions as a proxy to dive deeper into the semantic themes in each dataset. ", "page_idx": 8}, {"type": "text", "text": "Unsupervised topic discovery. We treat each caption as a document and apply the Latent Dirichlet Allocation (LDA) [7] for topic discovery to each dataset in YCD (with the number of topics set to 5). Figure 15 presents the top 5 words for each topic. Notably, in YFCC, three topics (first, second, and fifth) feature words associated with outdoor scenes; in CC and DataComp, their topics cover \u201clogo\u201d and \u201cdesign,\u201d suggesting the presence of digital graphics. ", "page_idx": 8}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/51e81d07dbad7fd190fa70ff764c0402691b8d7436d6d61b6865b79ff885f78e.jpg", "img_caption": ["Figure 15: LDA-extracted topics for each caption set. Each row lists the top 5 words for a topic. YFCC focuses on outdoor scenes, while CC and DataComp contain more digital graphics. ", ""], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/1a34a8830f4859ea851bb39a8b8cec61b36b2c41f1d117a29c07c9cc88ca1155.jpg", "table_caption": ["Figure 14: Word clouds [47] on the 100 most frequent phrases in each dataset. Phrase size corresponds to its frequency. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Large Language Model (LLM) summarization. To assess whether dataset signatures in captions can be identified by LLMs with limited examples, we provide GPT-4o with 120 captions per dataset and ask it to infer the dataset origin of new captions. Inference is performed on one validation caption at a time until accuracy stabilizes at $52.9\\%$ on 480 captions. Further details are in Appendix A.2. ", "page_idx": 8}, {"type": "text", "text": "This powerful ability of LLMs allows them to provide open-ended and detailed natural language explanations. Specifically, we procedurally prompt GPT-4o to extract dataset-specific characteristics from caption datasets and refine its answers into 5 bullet points, shown in Figure 16. In summary, YFCC is characterized by abundant outdoor, natural, and human-related scenes, while DataComp concentrates on static objects and synthetic images with clean backgrounds and minimal human presence. In contrast, CC blends elements of both YFCC\u2019s dynamic scenes and DataComp\u2019s static imagery. Appendix A.2 provides the entire prompt structure and the complete LLM summarization output. We further verify the validity of the semantic features from LDA and LLM in Appendix C. ", "page_idx": 8}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/c07965fb7d576fa30442cfda2d24ac2bf42072609571f8a8a364ce72c1c9173f.jpg", "img_caption": ["Figure 16: LLM summarization of dataset features. The bullet points highlight outdoor, natural, and human scenes in YFCC and static objects and synthetic images in DataComp. CC contains both YFCC\u2019s dynamic scenes and DataComp\u2019s static images. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Summary. We leveraged closed-set object-level queries and open-ended language analysis to interpret the semantic bias among datasets. The object-based analysis identified objects indicative of each dataset within a predefined object set. On the other hand, natural language methods are able to provide open-ended explanations and rich details explaining the characteristics of each dataset. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "YCD have different sources: YFCC is selected with minimal flitering from user-uploaded images on Flickr.com, while CC and DataComp fliter web-sourced images with high quality in caption, image, or their alignment. We build on the interpretable semantic bias in Section 4 to discuss the dataset curation procedures, and provide potential suggestions. ", "page_idx": 9}, {"type": "text", "text": "Filtering based on a reference dataset or model may inherit its bias. DataComp has the fewest unique objects per image (Figure 12). This is possibly because DataComp filters for images with visual content close to ImageNet data in the embedding space [18]. Therefore, the remaining images tend to be object-centric [4]. It also filters for images that align well with its captions in CLIP [53] embedding space, therefore favoring certain types of images, e.g., images containing text. To mitigate this, we may use datasets or models that contain less bias themselves for filtering. ", "page_idx": 9}, {"type": "text", "text": "The source website\u2019s image collection mechanism can introduce bias. We note that YFCC is heavily skewed towards outdoor scenes and human interactions (Section 4.2). This bias likely stems from its reliance on a single data source, Flickr.com, where user-uploaded content often focuses on personal photos, landscapes, and social interactions. ", "page_idx": 9}, {"type": "text", "text": "Web images naturally contain more digital graphics. Since CC and DataComp images are from Internet webpages, professionally created content like advertisements, infographics, and digital media are prioritized. Dataset users should evaluate if this composition aligns with the downstream goals. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed a framework to study the bias in large-scale visual datasets and used it to analyze three representative datasets. By classifying transformed images\u2019 dataset origin, we identified structures and semantics as key factors in dataset bias. We further investigated specific forms of semantic bias among datasets through fixed object queries, highlighting distinctive concepts characterizing each dataset. Lastly, we extracted key topics and natural language summaries for each dataset. We hope this framework and these findings can encourage further exploration of dataset bias and help improve diversity and representation in future datasets. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements. We would like to thank Kaiming He, Olga Russakovsky, Mingjie Sun, Kirill Vishniakov, and Zekai Wang for helpful discussions and feedback. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard Baraniuk. Self-consuming generative models go MAD. In ICLR, 2024.   \n[2] Anthropic. Claude 3.5 sonnet. https://www.anthropic.com, 2024.   \n[3] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J. Fleet. Synthetic data from diffusion models improves imagenet classification. TMLR, 2023. [4] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In NeurIPS, 2019.   \n[5] Quentin Bertrand, Joey Bose, Alexandre Duplessis, Marco Jiralerspong, and Gauthier Gidel. On the stability of iterative retraining of generative models on their own data. In ICLR, 2024.   \n[6] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.   \n[7] David M Blei, Andrew $\\mathbf{Y}\\,\\mathbf{N}\\mathbf{g}$ , and Michael I Jordan. Latent dirichlet allocation. JMLR, 2003.   \n[8] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, 2018.   \n[9] Stephen Butterworth. On the theory of filter amplifiers. Wireless Engineer, 1930.   \n[10] John Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1986.   \n[11] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.   \n[12] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. In ICLR, 2023.   \n[13] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In CVPR Workshops, 2020.   \n[14] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.   \n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.   \n[16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   \n[17] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E Gonzalez, and Serena Yeung-Levy. Describing differences in image sets with natural language. In CVPR, 2024.   \n[18] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In NeurIPS Datasets and Benchmarks Track, 2023.   \n[19] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2020.   \n[20] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In ICLR, 2019.   \n[21] Rafael C. Gonzales and Paul Wintz. Digital image processing (2nd ed.). Addison-Wesley Longman Publishing Co., Inc., 1987.   \n[22] Google People $^+$ AI Research. Know your data, 2021.   \n[23] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021.   \n[24] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In CVPR, 2019.   \n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.   \n[26] Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan Qi. Is synthetic data from generative models ready for image recognition? In ICLR, 2023.   \n[27] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women also snowboard: Overcoming bias in captioning models. In ECCV, 2018.   \n[28] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021.   \n[29] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019.   \n[30] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021.   \n[31] Jason Jo and Yoshua Bengio. Measuring the tendency of cnns to learn surface statistical regularities. arXiv preprint arXiv:1711.11561, 2017.   \n[32] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv: 1312.6114, 2022.   \n[33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023.   \n[34] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020.   \n[35] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In ECCV, 2022.   \n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.   \n[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2023.   \n[38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2024.   \n[39] Zhuang Liu and Kaiming He. A decade\u2019s battle on dataset bias: Are we there yet? arXiv preprint arxiv: 2403.08632, 2024.   \n[40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In CVPR, 2022.   \n[41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.   \n[42] David G Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004.   \n[43] David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In ICML, 2018.   \n[44] Nicole Meister, Dora Zhao, Angelina Wang, Vikram V Ramaswamy, Ruth Fong, and Olga Russakovsky. Gender artifacts in visual datasets. In ICCV, 2023.   \n[45] Midjourney. Midjourney.com. 2022.   \n[46] Shira Mitchell, Eric Potash, Solon Barocas, Alexander D\u2019Amour, and Kristian Lum. Algorithmic fairness: Choices, assumptions, and definitions. Annual review of statistics and its application, 2021.   \n[47] Andreas Mueller. Wordcloud for python, 2023.   \n[48] Niklas Muennighoff, Nouamane Tazi, Lo\u00efc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. In ACL, 2023.   \n[49] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021.   \n[50] Joon Sung Park, Michael S. Bernstein, Robin N. Brewer, Ece Kamar, and Meredith Ringel Morris. Understanding the representation and representativeness of age in ai data sets. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, 2021.   \n[51] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023.   \n[52] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. BEiT v2: Masked image modeling with vector-quantized visual tokenizers. arXiv preprint arxiv: 2208.06366, 2022.   \n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[54] Inioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna. Ai and the everything in the whole wide world benchmark. In NeurIPS, 2021.   \n[55] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.   \n[56] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In EMNLP, 2019.   \n[57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.   \n[58] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023.   \n[59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS Datasets and Benchmarks Track, 2022.   \n[60] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017.   \n[61] Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D Sculley. No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536, 2017.   \n[62] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. In NeurIPS, 2020.   \n[63] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, 2017.   \n[64] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. Communications of the ACM, 2016.   \n[65] Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola. Learning vision from models rivals learning vision from data. In CVPR, 2024.   \n[66] Tatiana Tommasi, Novi Patricia, Barbara Caputo, and Tinne Tuytelaars. A deeper look at dataset bias. Domain adaptation in computer vision applications, 2017.   \n[67] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR, 2011.   \n[68] Emiel Van Miltenburg. Stereotyping and bias in the flickr30k dataset. arXiv preprint arXiv:1605.06083, 2016.   \n[69] Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, and Olga Russakovsky. Revise: A tool for measuring and mitigating bias in visual datasets. IJCV, 2022.   \n[70] Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations. In ICCV, 2019.   \n[71] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation. In CVPR, 2020.   \n[72] Ross Wightman. GitHub repository: Pytorch image models. GitHub repository, 2019.   \n[73] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy. In Conference on fairness, accountability and transparency, 2020.   \n[74] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In NeurIPS, 2024.   \n[75] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.   \n[76] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In ICML, 2013.   \n[77] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, 2018.   \n[78] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018.   \n[79] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. arXiv preprint arXiv:1707.09457, 2017.   \n[80] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In CVPR, 2016.   \n[81] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Implementation Details 16 ", "page_idx": 14}, {"type": "text", "text": "A.1 Dataset Classification with Transformations 16   \nA.2 LLM-based Analysis . 17 ", "page_idx": 14}, {"type": "text", "text": "B Additional Results on Dataset Classification with Transformations 19 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Handcrafted Features . . 19   \nB.2 Ablation of Different Pretrained Models . 19   \nB.3 Dataset Classification with Different Vision Backbones . 21   \nB.4 Combining Information from Multiple Transformations . . 22   \nB.5 Is the Dataset Classification Model Memorizing or Generalizing? . . 22   \nB.6 High-pass and Low-pass Filters at Different Thresholds . . . 23 ", "page_idx": 14}, {"type": "text", "text": "C Verifying Semantic Patterns 24 ", "page_idx": 14}, {"type": "text", "text": "C.1 Vision-Language Models 24   \nC.2 VisDiff . 25 ", "page_idx": 14}, {"type": "text", "text": "D Examples of Long Captions 26 ", "page_idx": 14}, {"type": "text", "text": "E Additional Information on ImageNet Object Analysis 27 ", "page_idx": 14}, {"type": "text", "text": "F Limitations 27 ", "page_idx": 14}, {"type": "text", "text": "G Broader Impacts 27 ", "page_idx": 14}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Dataset Classification with Transformations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Dataset classification training. Following Liu and He [39], for image-text datasets CC and DataComp, we only use their images. For each dataset in the YCD combination, we uniformly sample $1\\mathbf{M}\\,/\\,10\\mathbf{K}$ to form the train / val sets. To speed up image loading, the shorter side of each image is resized to 500 pixels if the original shorter side is larger than this, with the aspect ratio preserved. We observe this has minimal effect on model performance. Table 1 details our default training recipe for dataset classification in Section 3. ", "page_idx": 15}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/72926f39f5f5542e2a17af0ca4fc6f939a4385777124d05cce52cc527a26f486.jpg", "table_caption": ["Table 1: Training recipe for dataset classification. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Data processing. By default, our training setting uses RandomResizedCrop and RandAug [13] as data augmentations. Note for transformations involving pre-trained models, transforming every augmented image can be time-consuming. To address this, we apply data augmentation after these transformations. This ensures that the transformed image remains consistent throughout training. In addition, RandAug is not used for binary or grayscale inputs due to its incompatibility. Table 2 shows whether we apply data augmentation after transformation and whether we use RandAug. ", "page_idx": 15}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/66fd71995cb78194176af21ea5eecd0a46f079b6fad531fe811222160597339f.jpg", "table_caption": ["Table 2: Data augmentation details for transformed images. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Other details. In Section 3.2, for classifying captions, we finetune the Sentence T5-base [49] model on generated captions using a fixed batch size of 128. To determine the optimal configuration for learning rate and training epochs, we conduct a 2D parameter sweep, selecting the learning rate from $\\{1{\\mathrm{e}}{-}3,{\\mathrm{~}}\\mathrm{{le}}{-}4,\\,1{\\mathrm{e}}{-}5\\}$ and training epochs from $\\{1,2,4,6\\}$ . The number of warmup iterations is set to epochs $.\\times1562$ iterations. For VAE, we use the model from [57] with a downsampling factor of 4, which encodes an RGB image of shape $256\\!\\times\\!256\\!\\times\\!3$ into a latent vector of size $64\\!\\times\\!64\\!\\times\\!3$ . In Section 3.7, we train a DiT-B/2 [51] model on each of YCD datasets for 275K iterations, using a batch size of 1024 and a fixed learning rate of 1e-4. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "A.2 LLM-based Analysis ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/f8f84d0271de1b0fe792f8f71f851904f82c5f9d4d895e80b5a05249a47972c3.jpg", "img_caption": ["Figure 17: Prompt structure for in-context learning. In each iteration, 120 captions are sampled from each dataset as in-context examples and the LLM predicts the dataset origin of another caption. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/9f26fb09aaa8c65664f8c77fbb617650d6555dbd2c7ab028f12e26e1c0405b84.jpg", "img_caption": ["Figure 18: The accuracy of in-context learning converges at 160 samples per dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "We access GPT-4o through ChatGPT Temporary Chat in May 2024. Our in-context learning prompt structure is shown in Figure 17. For each iteration, we sample 120 long captions (Section 3.2) from each of the YCD datasets, creating a shuffled set of 360 in-context examples. To prevent LLM from relying on prior knowledge about any of the YCD datasets, we randomly assign an index from $\\{1,2.$ , $3\\}$ as the name for each dataset in each iteration. We then prompt the LLM to determine the source distribution of the validation sample. Inference is performed on one validation caption at a time. Figure 18 shows the validation accuracy as a function of the number of validation samples per class. We stop sampling more validation captions at 160 samples per class, where the accuracy stabilizes. ", "page_idx": 16}, {"type": "text", "text": "We use a two-step pipeline to prompt GPT-4o to extract semantic features based on 10 batches of captions, where each batch consists of 120 randomly sampled captions from each dataset. The model refines its responses based on the aggregated list of extracted features. Figure 19 illustrates the pipeline and specific prompts used. Figure 20 shows the full LLM summarization output. ", "page_idx": 16}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/c4174791854aebb38b4e685252b21cdebf69a814fd7b6966150cc1e34a358377.jpg", "img_caption": ["Figure 19: Prompting procedure for LLM summarization. The LLM summarizes the dataset characteristics over 10 iterations, with 360 randomly sampled captions per iteration. These characteristics are then aggregated, and the LLM consolidates them into five bullet points for each dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "YFCC ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "CC ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "DataComp ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/b870609842a11616d673f48b6d30c974fd06df9cadefc4525c181079f4fa76ae.jpg", "img_caption": ["Figure 20: Full list of LLM summarization of dataset features. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B Additional Results on Dataset Classification with Transformations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Handcrafted Features ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Sections 3.2 and 3.3, we primarily leverage modern neural networks to transform images into alternative representations. Here, we leverage classic computer vision algorithms to extract handcrafted features and assess whether these features can effectively capture dataset bias. ", "page_idx": 18}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/690638cc754948d77cbdbcdb1bda14fea2afbdf6b9f93e2c32254cbcc824eb57.jpg", "img_caption": ["Figure 21: Transformations encoding handcrafted features. SIFT keypoints can only weakly capture dataset bias, while HOG achieves close-to-reference accuracy by capturing local gradients. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Scale-Invariant Feature Transform (SIFT) [42] identifies scale-invariant keypoints in a grayscale image, each characterized by their location, scale, and orientation. Across multiple scales, SIFT calculates the Difference of Gaussian (DoG) blurring on an image with two Gaussian of different standard deviations. Keypoints are found as the local extrema in the DoG images and further refined through a Taylor expansion to ensure precise localization in scale space. The orientation of each keypoint is determined based on the gradient directions within its local neighborhood. Each keypoint is visually represented by a circle. The length and direction of the radius line correspond to the keypoint\u2019s size and orientation. The dataset classification accuracy of $53.3\\%$ on these keypoints representation indicates that the SIFT features are marginally effective at capturing dataset bias. ", "page_idx": 18}, {"type": "text", "text": "Histograms of Oriented Gradients (HOG) [14] robustly represents gradient patterns by aggregating gradient information within small spatial regions (cells) into histograms. These histograms are normalized over a block of adjacent cells to improve invariance to changes in illumination and contrast. HOG visualization shows the dominant gradient direction in each cell, with brightness indicating gradient magnitude. The near-reference accuracy of $79.0\\%$ suggests that the shape and contour features captured by HOG are highly indicative of the dataset identity. ", "page_idx": 18}, {"type": "text", "text": "B.2 Ablation of Different Pretrained Models ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We further explore how the performance of the pretrained models used for transformations in Section 3 affects our results. Specifically, we perform object detection, contour extraction, and depth estimation using smaller and less powerful models ViTDet-Base, SAM (ViT-Base), and Depth-Anything-V2 (ViT-B). As shown in Table 3, the pretrained model size minimally impacts the classification accuracy. ", "page_idx": 18}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/af1f60af103b4b8003336dad9932c6846a5a8792422851d1bad51695a20f7896.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 3: Varying pretrained model size for object bounding box generation, SAM contour formation, and depth estimation minimally affects dataset classification accuracy on transformed datasets. ", "page_idx": 18}, {"type": "text", "text": "Additionally, we apply our LLM summarization method in Section 4.2 to different LLMs: Claude 3.5-Sonnet [2] and Llama-3.1-8B-Instruct [16]. Figures 22 and 23 list the 5 summarized dataset features. The features for each dataset from different LLMs express highly similar concepts. ", "page_idx": 19}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/6490b03d30aaeff0a02aab5ab8940dfb9b72aa9fd36b316db7787b17b903a05d.jpg", "img_caption": ["Figure 22: Claude 3.5-Sonnet\u2019s summarization of dataset features. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/8bb6c5a729145e1451bebf3a03018455313b7779a5b82f7eb540b7c06ca99600.jpg", "img_caption": ["Figure 23: Llama-3.1-8B-Instruct\u2019s summarization of dataset features. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "B.3 Dataset Classification with Different Vision Backbones ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Section 3, we employ ConvNeXt-Tiny as the single base classifier to perform the dataset classification tasks on transformed images. To see whether our results are still valid for models with different sizes and architectures, we further train ConvNeXt-Femto [72], ConvNeXt-Nano, and ResNet-34 [25] to perform the dataset classification tasks in Section 3. In Table 4, we report the validation accuracies on the YCD datasets applied with various image transformations. ", "page_idx": 20}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/d3b61c089ca7f8b357ce0ed79943a99e8769d2dff8a50754d601064c30d91f99.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "We also finetune two additional sentence embedding models MPNet-Base [62] and Sentence-BERTBase [56] for dataset classification on YCD captions generated from LLaVA. Both of these models are weaker [48] than our default Sentence-T5-Base in Section 3.2. Nevertheless, as shown in Table 5, the dataset classification accuracy remains consistent across models and transformations. ", "page_idx": 20}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/8c7f84c1cd52b1e25227d710b2df3cac046dc9387fe6074963f9b9d79ef1e295.jpg", "table_caption": ["Table 4: Different image classification models\u2019 validation accuracy on transformed datasets. The accuracy remains consistent across models and transformations. $^*$ Note the frequency fliters here are Butterworth filters [9] with a threshold of 30. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 5: Different sentence embedding models\u2019 dataset classification accuracy on YCD captions generated from LLaVA. The accuracy is still high even on weaker sentence embedding models. ", "page_idx": 20}, {"type": "text", "text": "B.4 Combining Information from Multiple Transformations ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/4d9fdb87d52ba530960b6473c8cf2dbf89801cfcb73fec168bf3a252c993deec.jpg", "table_caption": ["Table 6: Combination of different transformations can lead to larger bias. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "We are interested in whether combining several visual attributes can jointly contribute to a larger dataset bias. To this end, we perform dataset classification on the concatenation of two copies of the same original images applied with two different transformations. We consider all pairwise combinations of object detection, pixel shuffling, and SAM contour. Table 6 shows the resulting classification accuracies. We observe that combining semantic and structural attributes can result in higher dataset classification accuracy compared to using a single attribute alone. ", "page_idx": 21}, {"type": "text", "text": "B.5 Is the Dataset Classification Model Memorizing or Generalizing? ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To show that the high validation accuracy on the transformed datasets is achieved through generalization rather than memorization of training examples, we follow Liu and He [39] to perform the classification of pseudo-datasets after applying transformations. ", "page_idx": 21}, {"type": "text", "text": "Specifically, we create three pseudo-datasets, each sampled without replacement from the same transformed YFCC dataset. Tables 7 and 8 present the pseudo-dataset classification training accuracy on YFCC bounding boxes and YFCC Canny edges, without or with augmentations. As expected, the classification models fail to converge with more training images or stronger augmentations. $A l l$ pseudo-dataset classification models have a chance-level validation accuracy of $33\\%$ , as they merely memorize the dataset origin of each training image rather than learning generalizable patterns. ", "page_idx": 21}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/04a83fddac711c8c20aeec5e0f0308c8a08c56976bc71ab144fc079cc7018908.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "NGIIHlAEBt/tmp/81f5c8fb301a02709db8aa216a3299e598de2d8bbd4e5a7446e795b88df20f69.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 7: Training accuracy for YFCC bounding box pseudo dataset classification. ", "page_idx": 21}, {"type": "text", "text": "Table 8: Training accuracy for YFCC Canny edge pseudo dataset classification. ", "page_idx": 21}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/23a872e75ba817263a3d34362fed35e1b9dfeafa3207902197cd2ee33aacda9a.jpg", "img_caption": ["Figure 24: Ideal filter [21] with different thresholds. We select filtering thresholds {5, 35, 65, 95, 125, 155}. The high-pass filtered images are equalized for better visualization. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/73dd110b5026f7dee5e4d4c67e2d30ef76ca37b53dd3f0a550fa392438176b37.jpg", "img_caption": ["Figure 25: Butterworth filter [9] with different thresholds. We select filtering thresholds {5, 35, 65, 95, 125, 155}. The high-pass filtered images are equalized for better visualization. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "We further investigate how varying thresholds affect dataset classification performance on low-pass and high-pass filtered images. Figures 24 and 25 display transformed images with ideal filter [21] and Butterworth filter [9]. Figure 26 presents the resulting dataset classification accuracies across different thresholds. While accuracies generally decline at extreme threshold values, in other cases, ideal and Butterworth filters achieve high accuracies for both low-pass and high-pass filters. ", "page_idx": 23}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/0b65421a8293ed03c10c4fd8633858d5ff0ac8b92144de8da54a33950a96ecf0.jpg", "img_caption": ["Figure 26: Accuracy on filtered images at different thresholds. For most thresholds, both ideal and Butterworth filters achieve high accuracies for both high-pass and low-pass filters. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "It is worth noting that the Butterworth filter employs a soft thresholding approach when filtering frequencies, therefore allowing each image to retain some frequency information beyond the selected threshold. This likely contributes to the high accuracy observed on high-pass filtered images, even with a higher threshold setting. Additionally, we observe that the high-pass ideal filter can achieve a higher classification accuracy with a threshold of 150 compared to 115. This is likely because when the threshold is at 115, the biased higher-frequency information is dominated by less-biased lower-frequency information. ", "page_idx": 23}, {"type": "text", "text": "C Verifying Semantic Patterns ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Section 4.2, we leverage various language analyses to provide natural language descriptions of each dataset\u2019s characteristics. Here, we aim to validate these characteristics with a Vision-Language Model (VLM) and VisDiff [17]. ", "page_idx": 23}, {"type": "text", "text": "C.1 Vision-Language Models ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/4a7fe9ab773727afb70ff4134c91d2a0bb7b71285a2c9670ca0707e72ffd9db6.jpg", "img_caption": ["Figure 27: High-level semantic features\u2019 distributions annotated by LLaVA. The imbalanced distributions across YCD confirm the dataset characteristics in Section 4.2. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "To confirm the semantic patterns we discovered for the YCD datasets, we choose five of them and prompt a popular VLM LLaVA 1.5 [37] to answer whether each pattern exists in images. In Figure 27, we plot the distribution of LLaVa\u2019s responses across the YCD datasets. The results quantitatively substantiate the identified dataset themes, further evidencing that (1) DataComp is characterized by product showcase and white backgrounds but lacks human figures and (2) YFCC focuses on outdoor scenes, while CC and DataComp contains more images depicting domestic and indoor environments. ", "page_idx": 23}, {"type": "text", "text": "We also manually check the LLaVA annotation quality with a handful of samples. Table 9 displays some examples. The question and images involving human presence are omitted for privacy reasons. ", "page_idx": 23}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/81a804ef14fe37fae50533d8ca053404ecf93774404c11529991628c509d39fb.jpg", "img_caption": ["Table 9: Examples of LLaVA annotations. LLaVA can answer our question with reasonable accuracy. The four rows correspond to the first, second, fourth, and fifth questions in Figure 27. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "C.2 VisDiff ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/4333b843717d992055b7e742855a3f67d82db7a1d77ceab976c607b3652f2164.jpg", "img_caption": ["Figure 28: Dataset features generated by VisDiff [17]. Each cell lists the top 3 concepts distinguishing the dataset in each column from the one in each row. Note the VisDiff concepts highly overlap with the dataset characteristics from our language analysis in Section 4.2. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "VisDiff [17] is a recently proposed method to describe differences between two image sets in natural language. It first uses a VLM to generate image captions and then leverages an LLM to propose concepts that distinguish the first image set from the second image set based on these captions. For each concept, VisDiff uses the CLIP similarity scores between the images and that concept to classify two image sets, and calculates the AUROC of this classification. The final output concepts are ranked based on their corresponding AUROC scores. ", "page_idx": 24}, {"type": "text", "text": "For each dataset pair in the YCD datasets, we use VisDiff to identify the top 5 concepts that better describe the first dataset compared to the second one, as shown in Figure 28. VisDiff\u2019s concepts closely align with dataset characteristics of our language analysis in Section 4.2, highlighting \u201cpeople\u201d and \u201coutdoor activities\u201d for YFCC and \u201cproducts\u201d for DataComp. However, our method can also identify specific visual biases across datasets with various transformations, fixed object-level queries, and in-depth natural language analysis. ", "page_idx": 24}, {"type": "text", "text": "D Examples of Long Captions ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Table 10 shows examples of long paragraph captions generated by LLaVA 1.5 [37] in Section 3.2. ", "page_idx": 25}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/f3449e35235d320735bfb4084d217cf383792b35f2dd0380543b5870f2114eb2.jpg", "img_caption": ["Table 10: Examples of long paragraph captions generated from LLaVA 1.5 [37]. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "E Additional Information on ImageNet Object Analysis ", "text_level": 1, "page_idx": 26}, {"type": "image", "img_path": "NGIIHlAEBt/tmp/8eebf08834f009e99c44d295d431af7bf9f2c75c4804f7ac50b2594974470807.jpg", "img_caption": ["Figure 29: Majority dataset share for each ImageNet object class positively correlates with the reference dataset classification accuracy on that object class. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "The difference in object distribution among datasets using ImageNet object queries in Section 4.1 can be further used to perform dataset classification. Specifically, we classify an image into the dataset that has the highest frequency of this image\u2019s label. Without any learnable parameters (in contrast to logistic regression in Section 4.1), this simple decision rule achieves a validation accuracy of $50.41\\%$ . ", "page_idx": 26}, {"type": "text", "text": "The model trained on original datasets in Section 3.1 might also use the imbalance of underlying object-level distributions. To demonstrate this, we partition both the original YCD datasets and the accuracy of the dataset classification model trained on those by the ImageNet object annotations in Section 4.1. Figure 29 shows the accuracy for each object class vs. its majority dataset share. For each class in the partition, the greater the majority dataset share among the YCD datasets is, the higher the corresponding accuracy of the model is. ", "page_idx": 26}, {"type": "text", "text": "F Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Since our framework relies heavily on pretrained recognition and generative models [57, 12, 35, 33, 38] for extracting semantic and structural information from images, the analysis may be affected by the bias inherent to those models or the datasets they are trained on. For example, if the pretrained models are trained on data very similar to a certain dataset under study (e.g., one of YCD), the measured level of bias may be affected. In addition, dataset classification can only reveal bias by comparing multiple datasets, and cannot be directly applied to a single dataset to understand its bias. ", "page_idx": 26}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Our framework can be used to analyze the datasets before model training, to better determine whether the dataset composition aligns with training goals. It is relatively a fast process compared to a full training cycle. This can help researchers reduce experiment iterations and thus total energy usage. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have accurately made claims both in the abstract and introduction to reflect the paper\u2019s contributions and scope. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: See Appendix F. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have included all details about training with transformed inputs in Appendix A.1 and LLM\u2019s in-context learning in Appendix A.2. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We will release our code before the conference date. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Question 4. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We perform our experiments in Section 3 on 3 random samples of 3M training images and 30K validation images, and report the mean validation accuracy and standard deviation. For in-context learning in Section 4.2, GPT-4o is provided with 360 randomly sampled captions from the train set and evaluated on 1 validation sample each time. We sample a total of 480 validation samples until accuracy stabilizes, as shown in Figure 18. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provide the details about compute resources here. We use 8 NVIDIA 2080 Ti to train the ConvNeXt-T model for the dataset classification task with 8 gradient accumulation steps. The average compute time for each experiment is about 1.5 days. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have followed the Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See Appendix G. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pre-trained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have cited the papers for all datasets we leverage in the paper. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]