[{"figure_path": "NGIIHlAEBt/tables/tables_8_1.jpg", "caption": "Table 3: Varying pretrained model size for object bounding box generation, SAM contour formation, and depth estimation minimally affects dataset classification accuracy on transformed datasets.", "description": "This table presents the ablation study on the impact of different pretrained model sizes on the performance of object detection, contour extraction, and depth estimation.  It shows that using smaller or larger pretrained models (ViTDet-Base vs. ViTDet-Huge, SAM (ViT-Base) vs. SAM (ViT-L), Depth-Anything-V2 (ViT-B) vs. Depth-Anything-V2 (ViT-L)) has minimal impact on the resulting dataset classification accuracy after these transformations, indicating that the choice of pretrained model size does not significantly affect the overall findings of the study.", "section": "B.2 Ablation of Different Pretrained Models"}, {"figure_path": "NGIIHlAEBt/tables/tables_8_2.jpg", "caption": "Table 15: LDA-extracted topics for each caption set. Each row lists the top 5 words for a topic. YFCC focuses on outdoor scenes, while CC and DataComp contain more digital graphics.", "description": "This table presents the top 5 words for each topic extracted by Latent Dirichlet Allocation (LDA) from captions of three datasets: YFCC, CC, and DataComp.  The topics reveal the predominant themes in each dataset's captions. YFCC's topics strongly emphasize outdoor settings and activities, while CC's show a mix of indoor and outdoor settings. DataComp's topics highlight digital graphics, design elements, and product descriptions.", "section": "4.2 Open-ended Language Analysis"}, {"figure_path": "NGIIHlAEBt/tables/tables_15_1.jpg", "caption": "Table 1: Training recipe for dataset classification.", "description": "This table details the training settings used for the dataset classification task in Section 3 of the paper.  It specifies the optimizer (AdamW), learning rate (1e-3), weight decay (0.3), optimizer momentum (\u03b21, \u03b22=0.9, 0.95), batch size (4096), learning rate schedule (cosine decay), warmup epochs (2), training epochs (30), augmentations (RandAug (9, 0.5)), label smoothing (0.1), mixup (0.8), and cutmix (1.0).", "section": "A.1 Dataset Classification with Transformations"}, {"figure_path": "NGIIHlAEBt/tables/tables_15_2.jpg", "caption": "Table 2: Data augmentation details for transformed images.", "description": "This table indicates whether data augmentation was applied after image transformations and whether RandAug was used for each type of transformation.  Data augmentation was not applied for binary or grayscale images due to incompatibility with RandAug.  RandAug was used for most transformations except for those producing binary, grayscale, or other forms of non-standard images.", "section": "3 Isolating Bias with Transformations"}, {"figure_path": "NGIIHlAEBt/tables/tables_20_1.jpg", "caption": "Table 4: Different image classification models' validation accuracy on transformed datasets. The accuracy remains consistent across models and transformations. * Note the frequency filters here are Butterworth filters [9] with a threshold of 30.", "description": "This table presents the results of dataset classification accuracy using four different vision backbones (ConvNeXt-Femto, ConvNeXt-Nano, ResNet-34, and ConvNeXt-Tiny) on various image transformations. The purpose is to demonstrate the robustness of the findings in the paper by showing consistent accuracy across various models. The results show that even with different model architectures and sizes, the classification accuracy on the transformed images remains relatively consistent, indicating that the observed bias is not an artifact of the specific model used.", "section": "B.3 Dataset Classification with Different Vision Backbones"}, {"figure_path": "NGIIHlAEBt/tables/tables_20_2.jpg", "caption": "Table 3: Varying pretrained model size for object bounding box generation, SAM contour formation, and depth estimation minimally affects dataset classification accuracy on transformed datasets.", "description": "This table presents the results of an ablation study investigating the impact of different pretrained model sizes on the accuracy of dataset classification using various image transformations. The transformations considered include object bounding box generation, SAM contour formation, and depth estimation. The results show that varying the size of the pretrained models used for these transformations has minimal impact on the classification accuracy, suggesting that the overall effectiveness of the transformations is not strongly dependent on model size.", "section": "B.2 Ablation of Different Pretrained Models"}, {"figure_path": "NGIIHlAEBt/tables/tables_21_1.jpg", "caption": "Table 6: Combination of different transformations can lead to larger bias.", "description": "This table presents the results of dataset classification experiments where multiple image transformations were combined.  Specifically, it shows the accuracy achieved when using two transformations together (e.g., pixel shuffling and object detection) compared to using each transformation individually. The results demonstrate that combining certain types of transformations can lead to significantly higher accuracy in predicting the dataset of origin, indicating that these combined visual attributes contribute to a larger dataset bias.", "section": "B.4 Combining Information from Multiple Transformations"}, {"figure_path": "NGIIHlAEBt/tables/tables_21_2.jpg", "caption": "Table 7: Training accuracy for YFCC bounding box pseudo dataset classification.", "description": "This table presents the training accuracy for pseudo-dataset classification on YFCC bounding boxes, with and without augmentations.  The results show that the model fails to generalize beyond the training data when more images or strong augmentations are used, indicating that the model is memorizing the training data rather than learning generalizable patterns.", "section": "B.5 Is the Dataset Classification Model Memorizing or Generalizing?"}, {"figure_path": "NGIIHlAEBt/tables/tables_21_3.jpg", "caption": "Table 7: Training accuracy for YFCC bounding box pseudo dataset classification.", "description": "This table presents the results of training a model to classify pseudo-datasets created from the YFCC dataset's bounding boxes. The pseudo-datasets were created by sampling from the transformed YFCC bounding boxes without replacement.  The table shows the training accuracy with and without augmentations for different sizes of pseudo-datasets. The purpose is to evaluate if the model generalizes or simply memorizes.", "section": "B.5 Is the Dataset Classification Model Memorizing or Generalizing?"}]