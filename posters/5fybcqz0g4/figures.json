[{"figure_path": "5fybcQZ0g4/figures/figures_3_1.jpg", "caption": "Figure 1: The Riemannian geometry of the statistical manifold for categorical distributions in comparison to Euclidean geometry on the simplex. Left: Contours for the geodesic distances to \u03bc0 = (1/3, 1/3, 1/3). Middle: Exponential maps (geodesics) from \u03bc0 to different points near the boundary. Right: Logarithm maps (vector fields) to \u03bc0.", "description": "This figure compares the Riemannian geometry of the statistical manifold for categorical distributions with the Euclidean geometry on the simplex.  The left panel shows contour plots of geodesic distances from a central point (1/3, 1/3, 1/3) on the simplex. The middle panel illustrates the exponential maps (geodesics) showing the shortest paths between the central point and other points on the simplex, highlighting the curved nature of the Riemannian geometry.  The right panel displays the logarithm maps (vector fields) showing the directions of steepest descent from various points toward the central point, which again illustrates the effect of the Riemannian structure on the manifold.", "section": "3.2 Statistical Flow Matching"}, {"figure_path": "5fybcQZ0g4/figures/figures_4_1.jpg", "caption": "Figure 2: Statistical flow matching (SFM) framework. (a) During training (Sec.3.2), probability measures on P are mapped to S<sup>n-1</sup> via diffeomorphism \u03c0 to compute the time-dependent vector field (in red). During inference, the learned vector field generates the trajectory on S<sup>n-1</sup> and we map the outcome of ODE back to P (in blue). (b) In the NLL calculation for one-hot examples (Sec.3.5), the probability density is marginalized over a small neighborhood of some Dirac measure to avoid undefined behaviors at the boundary (in green).", "description": "This figure illustrates the Statistical Flow Matching (SFM) framework. Panel (a) shows the training process, where probability measures are mapped from the statistical manifold P to the sphere S<sup>n-1</sup> using a diffeomorphism. A time-dependent vector field is learned on S<sup>n-1</sup>, which is then used to generate a trajectory. This trajectory is mapped back to P to obtain the final probability measure. Panel (b) depicts the process of negative log-likelihood (NLL) calculation for one-hot examples. To avoid numerical instability issues at the boundary, the probability density is marginalized over a small neighborhood around a Dirac measure.", "section": "Method"}, {"figure_path": "5fybcQZ0g4/figures/figures_6_1.jpg", "caption": "Figure 1: The Riemannian geometry of the statistical manifold for categorical distributions in comparison to Euclidean geometry on the simplex. Left: Contours for the geodesic distances to \u03bc0 = (1/3, 1/3, 1/3). Middle: Exponential maps (geodesics) from \u03bc0 to different points near the boundary. Right: Logarithm maps (vector fields) to \u03bc0.", "description": "This figure compares the Riemannian geometry of the statistical manifold for categorical distributions with the Euclidean geometry on the simplex.  The left panel shows contours of geodesic distances from a central point (1/3, 1/3, 1/3). The middle panel illustrates exponential maps (geodesics) connecting this central point to various points, highlighting the curved nature of the Riemannian manifold.  The right panel shows logarithm maps (vector fields) pointing towards the central point. The comparison reveals that Euclidean geometry fails to represent the true curved geometry of the statistical manifold.", "section": "3.2 Statistical Flow Matching"}, {"figure_path": "5fybcQZ0g4/figures/figures_23_1.jpg", "caption": "Figure 4: GPT-J-6B NLL versus sample entropy. For MultiFlow, D3PM, and autoregressive, the curve represents different logit temperatures from 0.5 to 1. Baseline data are from [12].", "description": "This figure shows the relationship between the GPT-J-6B NLL (negative log-likelihood) and sample entropy for various text generation models.  Lower NLL and higher entropy generally indicate better model performance. The plot compares several models, including SFM (with and without optimal transport), MultiFlow (with different logit temperatures), D3PM, SEDD (with mask and uniform settings), and an autoregressive model. A random baseline is also included. The plot illustrates that SFM achieves a good balance between low NLL and high entropy, suggesting good model performance and sample diversity.", "section": "4.3 Text8"}, {"figure_path": "5fybcQZ0g4/figures/figures_24_1.jpg", "caption": "Figure 1: The Riemannian geometry of the statistical manifold for categorical distributions in comparison to Euclidean geometry on the simplex. Left: Contours for the geodesic distances to \u03bc0 = (1/3, 1/3, 1/3). Middle: Exponential maps (geodesics) from \u03bc0 to different points near the boundary. Right: Logarithm maps (vector fields) to \u03bc0.", "description": "This figure compares the Riemannian geometry of the statistical manifold for categorical distributions with the Euclidean geometry on the simplex.  The left panel shows contour plots of geodesic distances to a central point (1/3, 1/3, 1/3). The middle panel visualizes geodesics (exponential maps) from this central point to various points near the boundary of the simplex, highlighting the curved nature of the Riemannian manifold. The right panel displays logarithm maps (vector fields) to the central point, illustrating how the vector field changes based on the manifold's geometry.", "section": "3.2 Statistical Flow Matching"}]