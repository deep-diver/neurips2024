[{"figure_path": "ENLsNDfys0/figures/figures_0_1.jpg", "caption": "Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture.", "description": "This figure shows example results of the proposed method for novel object synthesis. The method takes as input an image and a text description of an object and generates a new image that combines features from both. The left panel shows a glass jar combined with a porcupine, resulting in a glass jar with porcupine-like textures. The right panel shows a horse combined with a bald eagle, resulting in a horse with eagle-like features. The figure demonstrates the ability of the method to generate creative and surprising combinations of objects.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_1_1.jpg", "caption": "Figure 2: Imbalances between text and image in diffusion models. Using SDXL-Turbo [56] (left) and PnPinv [27] (right), the top pictures show a tendency for generated objects to align with textual content (green circles), while the bottom pictures tend to align with visual aspects (orange circles). In contrast, our approach achieves a more harmonious integration of both object text and image.", "description": "This figure compares the results of two different diffusion models, SDXL-Turbo and PnPInv, when tasked with combining an object image and an object text to create a new object image.  The top row shows that SDXL-Turbo often favors either the text or the image, ignoring the other input; while PnPInv shows a similar imbalance.  The bottom row shows the results obtained using the proposed method, ATIH, which demonstrates a better balance between text and image in the generated images.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_3_1.jpg", "caption": "Figure 3: Framework of our object synthesis incorporating a scale factor \u03b1, an injection step i and noise \u03b5t in the diffusion process. We design a balance loss for optimizing the noise \u03b5t to balance object editability and fidelity. Using the optimal noise \u03b5t, we introduce an adaptive harmony mechanism to adjust \u03b1 and i, balancing text (Peacock) and image (Rabbit) similarities.", "description": "This figure illustrates the framework of the proposed object synthesis method, Adaptive Text-Image Harmony (ATIH).  It shows how a scale factor (\u03b1) and an injection step (i) are used to balance text and image features in the diffusion process. A balanced loss function optimizes noise (\u03b5t) to balance object editability and fidelity.  The adaptive harmony mechanism dynamically adjusts \u03b1 and i based on the similarity between the generated object and the input text and image.", "section": "3 Methodology"}, {"figure_path": "ENLsNDfys0/figures/figures_5_1.jpg", "caption": "Figure 4: Isim and Tsim with a \u2208 [0, 1.4].", "description": "This figure shows the relationship between the image similarity (Isim) and text similarity (Tsim) scores with the scaling factor \u03b1.  The x-axis represents the scaling factor \u03b1, while the y-axis represents the similarity scores.  Both Isim(\u03b1) and Tsim(\u03b1) curves are plotted. The green shaded area shows the range where the balance between Isim(\u03b1) and Tsim(\u03b1) is optimized according to the criteria  kmax \u2265 k \u2265 kmin, where kmax = I\u03b11/T\u03b11 and kmin = I\u03b12/T\u03b12,  for a better integration of text and image. The images shown on the figure illustrate the differences in generated images for different values of \u03b1, showing how \u03b1 impacts the balance between text and image information.", "section": "3.3 Text-image harmony by adaptively adjusting injection step i and scale factor \u03b1"}, {"figure_path": "ENLsNDfys0/figures/figures_5_2.jpg", "caption": "Figure 5: The adjusted process of our ATIH with three initial points and  \u03b5 = Isim(a) + k*Tsim(a) - F(a).", "description": "This figure illustrates the adaptive process of the proposed Adaptive Text-Image Harmony (ATIH) method. It shows how the injection step (i) and scale factor (a) are adjusted iteratively to balance the similarities between the generated image and the input text and image. The process starts with three initial points, and the optimal values of i and a are found by maximizing a similarity score function that considers both the image and text similarities while balancing them.", "section": "3.3 Text-image harmony by adaptively adjusting injection step i and scale factor a"}, {"figure_path": "ENLsNDfys0/figures/figures_6_1.jpg", "caption": "Figure 6: Comparisons with different image editing methods. We observe that InfEdit [69], MasaCtrl [5], and InstructPix2Pix [4] struggle to fuse object images and texts, while our method successfully implements new object synthesis, such as bowling ball-fawn in the second row.", "description": "This figure compares the results of the proposed ATIH method with three other image editing methods (InfEdit, MasaCtrl, and InstructPix2Pix) on six different text-image pairs.  Each pair combines an image of an animal or object with a text description of a different object.  The goal is to generate a new image that harmoniously combines elements from both the original image and the text description. The figure demonstrates that ATIH achieves more successful and creative object synthesis than the other methods, often producing more natural and coherent combinations.", "section": "Experiments"}, {"figure_path": "ENLsNDfys0/figures/figures_7_1.jpg", "caption": "Figure 3: Framework of our object synthesis incorporating a scale factor \u03b1, an injection step i and noise \u03b5t in the diffusion process. We design a balance loss for optimizing the noise \u03b5t to balance object editability and fidelity. Using the optimal noise \u03b5t, we introduce an adaptive harmony mechanism to adjust \u03b1 and i, balancing text (Peacock) and image (Rabbit) similarities.", "description": "This figure shows the framework of the proposed object synthesis method. It highlights the key components, including a scale factor (\u03b1) to balance text and image features, an injection step (i) to preserve image information, and a noise parameter (\u03b5t) to balance object editability and fidelity. The adaptive harmony mechanism dynamically adjusts \u03b1 and i to achieve an optimal balance between text and image similarities, leading to a more harmonious integration of both object text and image.", "section": "3 Methodology"}, {"figure_path": "ENLsNDfys0/figures/figures_7_2.jpg", "caption": "Figure 8: Comparisons with different creative mixing methods. We observe that our results surpass those of MagicMix [34]. For ConceptLab [50], we exclusively examine its fusion results without making good or bad comparisons, as it is a distinct approach to creative generation.", "description": "This figure compares the results of the proposed ATIH method with two other creative mixing methods: MagicMix and ConceptLab.  It shows that ATIH produces more harmonious and successful combinations of object text and image than the other methods.  The ConceptLab method is noted as having a different approach to creative generation, making a direct comparison difficult.  The figure showcases several examples of object combinations generated by each method.", "section": "4.2 Main Results"}, {"figure_path": "ENLsNDfys0/figures/figures_8_1.jpg", "caption": "Figure 9: Comparisons with ControlNet-depth and ControlNet-edge [72] using a description that \u201cA photo of an {object image} creatively fused with an object text }\u201d.", "description": "This figure compares the results of the proposed ATIH method with those obtained using ControlNet for image editing tasks.  The input is an image and text prompt describing the fusion of these two elements. ControlNet uses depth and edge maps for control. The results show that ATIH achieves better harmonization between the image and the text compared to ControlNet.", "section": "4.2 Main Results"}, {"figure_path": "ENLsNDfys0/figures/figures_9_1.jpg", "caption": "Figure 3: Framework of our object synthesis incorporating a scale factor \u03b1, an injection step i and noise \u03b5t in the diffusion process. We design a balance loss for optimizing the noise \u03b5t to balance object editability and fidelity. Using the optimal noise \u03b5t, we introduce an adaptive harmony mechanism to adjust \u03b1 and i, balancing text (Peacock) and image (Rabbit) similarities.", "description": "This figure illustrates the framework of the proposed object synthesis method, called Adaptive Text-Image Harmony (ATIH).  It highlights the key components involved in the process, such as the scale factor (\u03b1) to balance text and image features in cross-attention, an injection step (i) to maintain image information during self-attention, and a balanced loss function to optimize noise (\u03b5t) for better object editability and fidelity.  The adaptive harmony mechanism adjusts \u03b1 and i to balance the similarities between the generated object and the input text and image.", "section": "3 Methodology"}, {"figure_path": "ENLsNDfys0/figures/figures_14_1.jpg", "caption": "Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture.", "description": "This figure shows two examples of the proposed method for novel object synthesis.  The method combines an object image with a textual description to generate a new, combined image. The left example shows a glass jar combined with the text \"porcupine\", resulting in an image that looks like a glass jar with porcupine-like features. The right example shows a horse combined with the text \"bald eagle\", resulting in a horse with features resembling a bald eagle.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_15_1.jpg", "caption": "Figure 12: Original Object Image Set.", "description": "This figure shows the 30 images used in the dataset for the object synthesis task.  The images are diverse and depict a variety of animals and objects, including mammals, birds, reptiles, plants, fruits, and other objects, to ensure a wide range of visual features for the model to learn from.", "section": "4.1 Experimental Settings"}, {"figure_path": "ENLsNDfys0/figures/figures_15_2.jpg", "caption": "Figure 13: Image variations under different \u03bb values. The first row displays the reconstructed images. The middle and bottom rows show the results of editing with different prompts, demonstrating variations in maximum editability, a balanced approach, and maximum constructability.", "description": "This figure shows how different values of the lambda (\u03bb) parameter affect the balance between image fidelity and editability in the object synthesis process.  The top row displays the original reconstructed images.  The two bottom rows show results from editing the reconstructed image with different textual prompts, demonstrating the extremes of editability (left), balanced results (center), and  constructability (right). The red box highlights the balanced results for each example.", "section": "Parameter Analysis"}, {"figure_path": "ENLsNDfys0/figures/figures_16_1.jpg", "caption": "Figure 3: Framework of our object synthesis incorporating a scale factor \u03b1, an injection step i and noise \u03b5t in the diffusion process. We design a balance loss for optimizing the noise \u03b5t to balance object editability and fidelity. Using the optimal noise \u03b5t, we introduce an adaptive harmony mechanism to adjust \u03b1 and i, balancing text (Peacock) and image (Rabbit) similarities.", "description": "This figure shows the framework of the proposed Adaptive Text-Image Harmony (ATIH) method for novel object synthesis.  It highlights three key components: a scale factor (\u03b1) to balance text and image features in cross-attention; an injection step (i) to preserve image information in self-attention; and a noise parameter (\u03b5t) optimized by a balanced loss function to ensure both optimal editability and fidelity of the generated object image. The ATIH method adaptively adjusts \u03b1 and i to create a harmonious balance between text and image similarities, enabling the synthesis of novel and surprising objects.", "section": "3 Methodology"}, {"figure_path": "ENLsNDfys0/figures/figures_16_2.jpg", "caption": "Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture.", "description": "This figure shows example results of the proposed method, ATIH (Adaptive Text-Image Harmony), for novel object synthesis.  The method takes an image and text as input and generates a new image that combines features of both. The two examples in the figure illustrate the combination of a glass jar image with \"porcupine\" text, and a horse image with \"bald eagle\" text, resulting in novel object images.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_16_3.jpg", "caption": "Figure 18: Results changing in Iteration w/ and w/o attention injection.", "description": "This ablation study shows the results of changing the attention injection parameter (\u03b1) with and without attention injection in the model. The input image is a Corgi, and the target text is \"Fire Engine\". The figure displays how the model transforms the image into a fire engine with different \u03b1 values. Without attention injection, the transformation is abrupt; while with injection, the transformation is smoother and more harmonious, resulting in a better integration of the original image and target concept.", "section": "Ablation Study"}, {"figure_path": "ENLsNDfys0/figures/figures_18_1.jpg", "caption": "Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture.", "description": "This figure shows two examples of novel object synthesis. The left image shows a glass jar combined with the text \"porcupine\", resulting in a porcupine-like creature in a glass jar.  The right image combines a horse with the text \"bald eagle\", resulting in a horse-like creature with eagle-like features. This demonstrates the capability of the proposed method to generate combinational objects by combining an object image and its surrounding text descriptions.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_19_1.jpg", "caption": "Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture.", "description": "This figure shows example results of the proposed method for novel object synthesis. The method takes an image and a text description as input and generates a new image that combines elements of both. The examples shown include a glass jar combined with a porcupine and a horse combined with a bald eagle.  The results demonstrate the ability of the model to create novel and visually appealing combinations of objects.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_20_1.jpg", "caption": "Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture.", "description": "This figure shows two examples of novel object synthesis using the proposed method.  The method combines an input image with a textual description to generate a new, composite image. The left example combines a glass jar image with the text \"porcupine\", resulting in a porcupine-like object formed from a glass jar. The right example combines a horse image with the text \"bald eagle\", resulting in an eagle-like object formed from a horse.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_20_2.jpg", "caption": "Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture.", "description": "This figure shows two examples of the novel object synthesis approach proposed in the paper. The approach combines an object image with a textual description to generate a new, combined object image. The left image shows a glass jar combined with the text \"porcupine\", resulting in a glass jar with a porcupine-like texture and features. The right image shows a horse combined with the text \"bald eagle\", resulting in a horse with bald eagle-like features.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_21_1.jpg", "caption": "Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture.", "description": "This figure shows two examples of novel object synthesis using the proposed method. The left image combines a glass jar (image) with a porcupine (text), resulting in a glass jar with porcupine-like features. The right image combines a horse (image) with a bald eagle (text), resulting in a horse with bald eagle-like features.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_21_2.jpg", "caption": "Figure 2: Imbalances between text and image in diffusion models. Using SDXL-Turbo [56] (left) and PnPinv [27] (right), the top pictures show a tendency for generated objects to align with textual content (green circles), while the bottom pictures tend to align with visual aspects (orange circles). In contrast, our approach achieves a more harmonious integration of both object text and image.", "description": "This figure demonstrates the limitations of existing diffusion models in harmoniously combining text and image information for object synthesis.  The top row shows examples where the generated object strongly favors either the textual description (left) or the visual input image (right), indicating an imbalance in how the model processes the combined input. In contrast, the bottom row illustrates the results from the proposed ATIH model, which better integrates text and image features to create harmoniously combined objects.", "section": "Introduction"}, {"figure_path": "ENLsNDfys0/figures/figures_22_1.jpg", "caption": "Figure 6: Comparisons with different image editing methods. We observe that InfEdit [69], MasaCtrl [5], and InstructPix2Pix [4] struggle to fuse object images and texts, while our method successfully implements new object synthesis, such as bowling ball-fawn in the second row.", "description": "This figure compares the results of the proposed method (ATIH) against three other image editing methods (InfEdit, MasaCtrl, and InstructPix2Pix) on the task of object synthesis by fusing object images and texts.  The results show that ATIH produces more harmonious and successful fusions, creating novel objects that effectively combine features from both the image and text, unlike the other methods which struggle to balance the two inputs.", "section": "Experiments"}, {"figure_path": "ENLsNDfys0/figures/figures_22_2.jpg", "caption": "Figure 27: Comparisons with Subject-driven method.", "description": "This figure compares the results of the proposed ATIH model with the Kosmos-G model on two examples of object synthesis. The top row shows the results of applying \"Strawberry\" text to an original image of an owl.  The ATIH model produces a harmoniously fused image of an owl with pink strawberry-like coloration, while the Kosmos-G model results in an owl with strawberry-like spots on its feathers. The bottom row shows the results of applying \"Badger\" text to an original image of a squirrel. The ATIH model generates an image of a squirrel with badger-like fur and coloration, while Kosmos-G model results in a badger image with a stylized appearance.", "section": "More Comparisons"}]