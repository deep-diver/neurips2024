[{"type": "text", "text": "Face2QR: A Unified Framework for Aesthetic, Face-Preserving, and Scannable QR Code Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xuehao $\\mathbf{Cui}^{*}$ , Guangyang $\\mathbf{W}\\mathbf{u}^{*}$ , Zhenghao Gan, Guangtao Zhai, Xiaohong Liu\u2020 ", "page_idx": 0}, {"type": "text", "text": "Shanghai Jiao Tong University {cavosamir, wu.guang.young, ganzhenghao, zhaiguangtao, xiaohongliu}@sjtu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing methods to generate aesthetic QR codes, such as image and style transfer techniques, tend to compromise either the visual appeal or the scannability of QR codes when they incorporate human face identity. Addressing these imperfections, we present Face2QR\u2014a novel pipeline specifically designed for generating personalized QR codes that harmoniously blend aesthetics, face identity, and scannability. Our pipeline introduces three innovative components. First, the ID-refined QR integration (IDQR) seamlessly intertwines the background styling with face ID, utilizing a unified Stable Diffusion (SD)-based framework with control networks. Second, the ID-aware QR ReShuffle (IDRS) effectively rectifies the conflicts between face IDs and QR patterns, rearranging QR modules to maintain the integrity of facial features without compromising scannability. Lastly, the ID-preserved Scannability Enhancement (IDSE) markedly boosts scanning robustness through latent code optimization, striking a delicate balance between face ID, aesthetic quality and QR functionality. In comprehensive experiments, Face2QR demonstrates remarkable performance, outperforming existing approaches, particularly in preserving facial recognition features within custom QR code designs. Codes are available at https://github.com/cavosamir/Face2QR. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Quick Response (QR) codes, due to their capability to store a substantial amount of data and their ease of accessibility through basic camera devices, have become an exceedingly widespread medium for the representation of information in the digital era [13, 47, 3, 4, 36]. With the wide application of QR codes in social context, there has been increasing needs for customizing QR codes to include personal identity and aesthetic allure. However, such needs cannot be fulfilled by the dull appearance of common QR codes, which contain only black and white modules. ", "page_idx": 0}, {"type": "text", "text": "With the widespread application of QR codes across diverse fields, related technologies are also developing at a rapid pace. While techniques employing image transformation [5, 6, 13, 46] and style transferring [36, 47] can partially retain facial features, their perceptual quality and aesthetic adaptability are limited. On the other hand, generative model-based approaches [11, 43] can produce QR code images of superior quality and diversity, yet they pose challenges in controlling the generated content, particularly in preserving human facial characteristics. To address these limitations and ensure faithful preservation of face identity within a customized and scannable QR code image, we introduce a novel pipeline, named Face2QR. This approach achieves a balanced compromise between face ID preservation, aesthetic appeal, and scannability for QR code images. ", "page_idx": 0}, {"type": "image", "img_path": "rvBabL7DUu/tmp/55072f8ff23994833af7daf4ee5e1c70c5bde4145f5485bbbb5f870dd8a469ff.jpg", "img_caption": ["Figure 1: Face images (first row) and QR code images (second row) generated by Face2QR. Our QR codes not only faithfully maintain face ID, but also showcase remarkable scanning resilience and aesthetic quality. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The primary challenges lie in effectively integrating three key aspects: face ID, aesthetic quality, and scanable QR pattern, which can be summarized as follows: (1) Combination of face ID and background. Achieving a harmonious balance between strict facial ID preservation and diverse customized background styles within a unified pipeline presents a notable challenge. Methods reliant on style transfer [36, 47] often yield facial textures that appear unnatural, while those based on image transfer [5, 6, 13, 46] may introduce visible artifacts in the facial region; (2) Conflict between face ID and QR code pattern. While prior generative model-based techniques [43] have demonstrated the ability to control the QR code pattern using QR blueprints, they have struggled to exclude these patterns from the facial region, resulting in unnatural shadows and undesirable artifacts. However, directly removing these patterns from the facial region can make the image unscannable. Thus, achieving a balance between maintaining visual quality in the facial region and ensuring the correctness of the QR pattern presents a formidable obstacle; (3) Balance between aesthetics and scannability. As revealed in [43], generated images often exhibit a tendency towards being unscannable, necessitating enhancements to their scannability through post-processing. However, globally adjusting brightness can compromise the natural appeal of the facial region. Thus, novel region-based enhancement methods are worth considering to address this challenge. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, the proposed Face2QR pipeline offers a solution for generating personalized QR codes that strike a balance between aesthetics, facial ID preservation, and scannability. We propose ID-refined QR integration (IDQR) to seamlessly combine background and face ID, and ID-aware QR ReShuffle (IDRS) to solve the conflict between face ID and QR code pattern. Specifically, IDQR applies a unified SD-based framework to ensure that the generated images have a uniform style. Stable Diffusion (SD) models are guided by two sets of control networks, corresponding to face refinement and QR pattern respectively, to achieve separate control in face region and background. IDRS utilizes the flexibility of QR code encoding and reshuffles the modules to make the QR pattern compatible with face ID. Finally, we use ID-preserved Scannability Enhancement (IDSE) to enhance scan robustness through latent code optimization, achieving a new trade-off between face ID, aesthetics and scannability. Figure 1 shows the QR images generated by Face2QR. It is worth noting that the generated QR images are not only the reprints of the provided references, but also have improved aesthetics to align with the generated background, guided by text prompts (e.g., the style and color of clothes have been adjusted accordingly). ", "page_idx": 1}, {"type": "text", "text": "The contributions of this work can be summarized as: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel pipeline that holistically integrates aesthetic appealing, facial ID, and scannability to deliver a customized personal representation in QR codes.   \n\u2022 We introduce the ID-refined QR integration (IDQR) for seamlessly integrating face ID with background, the ID-aware QR ReShuffle (IDRS) for solving conflicts between face ID and QR pattern, and the ID-preserved Scannability Enhancement (IDSE) for optimizing scan robustness while maintaining face ID and aesthetic quality.   \n\u2022 Our Face2QR achieves the State-Of-The-Art (SOTA) performance in generating the ID-preserved aesthetic QR codes, compared with previous methods. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Quick Response (QR) Code. As QR codes emerging as a key connector between real and virtual worlds, there is increasing interest in enhancing the visual appeal of normally monochromatic QR codes. Halftone QR codes [5] offers a design where QR code patterns align with a given image in a thematically cohesive manner. QRImage and Artup [13, 46] explore ways to encode colorful imagery within a QR code. Other advances [35, 36] have been made in artistic style transfer to increase aesthetic appearance of QR codes. To further customize QR code and obscure overt QR code markers, Chen et al. [2, 4, 23] crafted encoding schemes that consider human visual perception, thus making these patterns less intrusive. TPVM [12] has gone further to conceal QR codes within video content, exploiting the discrepancies in frame capture rates between human vision and digital screens. Similarly, advancements have sought to keep data imperceptible yet accessible through various stealth mechanisms [10, 9, 37, 16, 42, 17]. ", "page_idx": 2}, {"type": "text", "text": "Diffusion Based Models. Image manipulation and generation techniques powered by deep learning have made strides in recent years [41, 45, 21, 44, 31, 33, 32], with generative models being at the forefront of this development [51, 24, 28, 26]. Novel diffusion-based models such as GLIDE [26], DALLE-2 [28], and Latent Diffusion models [30] have come into prominence. Notably, the Stable Diffusion model [30] moves the denoising steps into the latent dimension of a variational autoencoder, which significantly optimizes the generation process in terms of data volume and training time. In parallel, new research has introduced various techniques for modulating the diffusion process. Structural condition interventions have been successfully implemented by ControlNet [52] and T2IAdapter [25]. On a different note, BLIP-Diffusion [20] and SeeCoder [48] have made progress on steering generative outcomes based on stylistic aspects. ", "page_idx": 2}, {"type": "text", "text": "Identity Preserved Generative Models. In the field of ID-preserving image generation, research focuses on maintaining semantic face attributes while generating images that have wide real-world applications. Studies have generally split between techniques requiring test-time fine-tuning, such as Low-Rank Adaptation [15], and newer optimization-free methods such as Face0 [38], PhotoMaker [22], and FaceStudio [49], which integrate facial embeddings into the generation process in different ways. While techniques like IP-Adapter [50] strive for identity consistency by using embeddings from recognition models, they face challenges in compatibility with pre-trained models and ensuring facial fidelity. Most recent work like InstantID [40] use a pluggable module that does not demand fine-tuning and can work seamlessly with available pre-trained diffusion models to achieve high-quality face preservation in generated images. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The overall structure of Face2QR is shown in Figure 2. The pipeline unfolds through three stages, represented by blue, red and green arrows. Given a user-customized face image $f$ , QR Code $m$ , text prompts $c$ and random noise $z_{0}$ , the first stage uses the ID-refined QR integration (IDQR) module to generate an initial QR image $I^{g}$ . The IDQR module includes a pre-trained SDXL model (denoted as $S\\mathcal{D})$ ), an InstantID [40] network (denoted as $C_{i d}$ ) and a QR Controller [43] (denoted as $C_{q r}$ ). Stage 1 can be expressed as: ", "page_idx": 2}, {"type": "equation", "text": "$$\nI^{g}={\\cal S}\\mathcal{D}(c,z_{0}|\\mathcal{C}_{q r}(m,c,z_{0}),\\mathcal{C}_{i d}(f,c,z_{0})).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The InstantID network preserves the facial identity information in the generated images, while the QR Controller guides the luminance distribution of the images. ", "page_idx": 2}, {"type": "text", "text": "However, as shown in Figure 2, the initial output image from the first stage contains a significant error rate (over $43\\%$ ). This issue arises from the inherit conflict between two control signals: the foreground face information and the background QR patterns, which are incompatible in the center regions. These conflicts lead to unavoidable QR code errors, presenting a core challenge in our pipeline. To address this, we design the ID-aware QR ReShuffle (IDRS) module to harmonize these confilcts and regenerate the image using a fine-grained QR blueprint $I_{b}$ . As illustrated in Figure 2, this reduces the error rate by more than half. Finally, we use the ID-preserved Scannability Enhancement (IDSE) module to refine the result $I^{s}$ in latent space, further improving its scanning robustness without compromising the overall visual quality. In the following, we introduce the second and third stages in details. ", "page_idx": 2}, {"type": "image", "img_path": "rvBabL7DUu/tmp/f81ba0694e1566bee2bcbb5495f00384ea52432576f75f2cd17380d27020cacb.jpg", "img_caption": ["Figure 2: The pipeline of Face2QR is a training-free process for generating ID-consistent and scannable QR code images. Our pipeline has three stages, represented by blue, red, and green arrows. The IDRS module resolves conflicts between human identity and QR patterns during the control process, while the IDSE module reduces coding errors to ensure the output is scannable. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 ID-Aware QR ReShuffle ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As revealed in [43], a fine-grained QR blueprint can effectively control the generator. To resolve control conflicts in the facial region, we design a novel blueprint that makes facial information and QR patterns compatible. By leveraging the dynamic characteristics of QR code encoding, we can adaptively rearrange the QR modules. Specifically, we maintain the brightness distribution of the facial region and reshuffle the remaining black and white modules accordingly. ", "page_idx": 3}, {"type": "text", "text": "First of all, we binarize $I^{g}\\in\\mathbb{R}^{H\\times W\\times3}$ into module-wise binary information $\\mathbf{E}\\in\\mathbb{R}^{n^{2}}$ . By dividing $I^{g}$ into $n\\times n$ modules each of size $a\\times a$ , and let $\\theta_{j}$ be the set of pixel coordinates of the $j$ -th module in $I^{g}$ , the extracted information code $\\mathbf{E}$ is given by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{E}_{j}=\\left\\{0,\\begin{array}{l l}{\\mathrm{if}\\;\\mathrm{avg}(I^{g}(\\theta_{j}))<\\tau,}\\\\ {\\mathrm{i},\\,}&{\\mathrm{if}\\;\\mathrm{avg}(I^{g}(\\theta_{j}))\\geq\\tau,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathrm{avg}(\\cdot)$ denotes the mean pixel value of the squared patch of size $a\\times a$ . The binarization uses a threshold $\\tau$ , typically set to 128 for a total of 256 grayscale levels. ", "page_idx": 3}, {"type": "text", "text": "As shown in Figure 3 (left), the binarized QR code is un-scannable due to a significant error rate. To address this, we fix the facial and marker region within $\\mathbf{E}$ , then rearrange the remaining codes to align with the encoded information. To locate the facial region, we use a pre-trained face recognition model to obtain the binary facial mask $M_{f}\\,\\in\\,\\mathbb{R}^{H\\times W}$ . Let the set $\\Delta_{f}\\dot{=}\\{j\\mid\\operatorname{avg}(M_{f}(\\theta_{j})\\bar{)}=1\\}$ represent the indices of information codes in $\\mathbf{E}$ that correspond to the facial region, and let the $\\Delta_{m}$ represent the indices of marker codes. Our goal is to obtain a new information code $\\tilde{\\bf E}$ which is partially modified from $\\mathbf{E}$ to make the QR decoder $\\mathbb{D}$ extract lossless information: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{min}|\\mathbb{D}(\\tilde{\\mathbf{E}})-\\mathbb{D}(m)|,}\\\\ &{\\mathrm{s.t.}\\quad\\tilde{\\mathbf{E}}_{j}=\\mathbf{E}_{j},\\mathrm{for}\\,j\\in\\Delta_{f}\\cup\\Delta_{m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "To ensure the resultant $\\tilde{\\bf E}$ can be decoded to the target message, aligning with original QR code $m$ , we re-generate the error correction code [29] in E\u02dc. ", "page_idx": 3}, {"type": "text", "text": "Afterwards, we expand the binary information of E\u02dc to image space. We use adaptive-halftone to combine the texture information of $I^{g}$ with binary code information of $\\tilde{\\bf E}$ in an adaptive manner, resulting in the blueprint $I_{b}\\in\\mathbb{R}^{H\\times W}$ . Note that we leave the facial region unmodified to maintain rich facial features without compromising scanning robustness. The resultant blueprint $I_{b}$ is then fed ", "page_idx": 3}, {"type": "image", "img_path": "rvBabL7DUu/tmp/c0369adf1c54ebcae5eac09933df2aa6b3517eda955d27b2f30c944c7464b24c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Illustration of IDRS (left) and IDSE (right). In IDRS, we maintain the information codes within the face and marker regions (red and yellow masks) and remap the remaining modules accordingly. In IDSE, we strengthen the finder and alignment pattern, and update in latent space using adaptive loss to enhance scannability. Visualization $D$ shows the difference between $I^{o}$ and $\\hat{I}^{s}$ . Compared with uniform loss, adaptive loss modifies face region more gently. ", "page_idx": 4}, {"type": "text", "text": "into $S\\mathcal{D}$ for the second generation: ", "page_idx": 4}, {"type": "equation", "text": "$$\nI^{s}={\\cal S}\\mathcal{D}(c,z_{0}|\\mathcal{C}_{q r}(I_{b},c,z_{0}),\\mathcal{C}_{i d}(f,c,z_{0})).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Compared with the first generation in Equation 1, both controllers in stage 2 include facial information to mitigate conflicts. As shown in Figure 2, the result of stage 2 reduces errors by more than half compared to stage 1, while consistently preserving face identity information. ", "page_idx": 4}, {"type": "text", "text": "3.2 Scannability Enhancement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The resultant QR image $I^{s}$ from stage 2 contains a certain QR pattern and consistently reveals face identity, but it is still unscannable by common QR decoders. In this part, we design the IDPreserved Scannability Enhancement (IDSE) module to achieve the following two goals: 1) minimize modifications to the QR image (especially for facial region) to ensure its scannability; 2) enhance the marker region to better harmonize it without compromising scanning robustness. As illustrated in Figure 3 (right), we first strengthen the finder and alignment pattern of $I^{s}$ , and then refine it using dynamic code loss to reach a harmonious balance between face ID, visual appeal and scannability. ", "page_idx": 4}, {"type": "text", "text": "3.2.1 Marker Harmonziation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The functional regions of a QR code, especially the finder and alignment patterns, are crucial for the decoder to locate the QR code. Therefore, these patterns on $I^{s}$ are strengthened to generate $\\widehat{I}^{s}$ . Specifically, for pixel $\\mathbf{p}\\in\\theta_{k}$ where $k\\in\\Delta_{m}$ , we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{I}^{s}(\\mathbf{p})=\\left\\{\\!\\!\\begin{array}{l l}{I^{s}(\\mathbf{p})-\\operatorname*{min}(I^{s}(\\mathbf{p})-\\tau(1+\\lambda),0),}&{\\mathrm{if}\\;\\mathbf{E}_{k}=1,}\\\\ {I^{s}(\\mathbf{p})-\\operatorname*{max}(I^{s}(\\mathbf{p})-\\tau(1-\\lambda),0),}&{\\mathrm{if}\\;\\mathbf{E}_{k}=0,}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda\\in(0,1)$ is a hyper-parameter, typically set to 0.8 by default. This threshold-based enhancement helps ensure that the functional regions of the output QR image are easily located. ", "page_idx": 4}, {"type": "text", "text": "3.2.2 Spatially Dynamic Loss Function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Inspired by [43], we use gradient descent to update the latent code of $\\widehat{I}^{s}$ to optimize certain loss function. However, instead of using a fixed loss function with constant  c oefficients, we propose to leverage a spatially dynamic loss function. ", "page_idx": 4}, {"type": "text", "text": "Given a pretrained VQ-VAE [39] with the encoder $\\mathcal{E}$ and the decoder $\\mathcal{D}$ , the optimization is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{\\hat{z}}=\\underset{\\boldsymbol{z}}{\\operatorname{argmin}}\\,\\mathcal{L}(\\mathcal{D}(\\boldsymbol{z}),I_{b},\\widehat{I}^{s}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $z\\in\\mathbb{R}^{\\frac{H}{8}\\times\\frac{W}{8}\\times4}$ is the latent code. The loss function $\\mathcal{L}$ consists of an aesthetic content loss $\\mathcal{L}_{a}$ and a spatially dynamic code loss $\\mathcal{L}_{c}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{z}=\\underset{z}{\\operatorname{argmin}}\\big\\{\\mathcal{L}_{c}(\\mathcal{D}(z),I_{b})+\\mathcal{L}_{a}(\\mathcal{D}(z),I^{s})\\big\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We initialize $z$ to $\\mathcal{E}(\\widehat{I^{s}})$ , and use Adam [19] as the optimizer with a learning rate of 0.002 to iteratively update $z$ until convergence. Finally, the output $I^{o}=\\mathcal{D}(\\hat{z})$ achieves robust scannability and high visual quality. ", "page_idx": 5}, {"type": "text", "text": "Adaptive Code Loss. A simulated decoder [36] using a 2D Gaussian kernel can extract modulewise information consistent with common QR decoders. The variance $\\sigma$ of the Gaussian kernel is a key factor in balancing visual quality and scanning robustness. However, in our scenario, we want the facial region to be smooth and the background region to be lossless. Therefore, we propose a spatially dynamic code loss. Let $Z=\\mathcal{D}(z)$ , the loss of $j$ -th module is calculated by: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns_{j}=w(j)\\times\\mathrm{avg}\\{[Z(\\theta_{j})-I_{b}(\\theta_{j})]\\odot G(j)\\},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\odot$ denotes the Hadamard product. $G(j)\\in\\mathbb{R}^{a\\times a}$ is a weighting kernel, and $w(j)$ is a weighting factor defined by: ", "page_idx": 5}, {"type": "equation", "text": "$$\nG(j)=\\left\\{G_{\\sigma_{f}},\\quad\\mathrm{if~}j\\in\\Delta_{f},}\\\\ {G_{\\sigma_{b}},\\quad\\mathrm{otherwise}}\\end{array}\\right.;\\,\\,w(j)=\\left\\{{w_{f},\\quad\\mathrm{if~}j\\in\\Delta_{f}},\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $G_{\\sigma}$ is a 2D Gaussian kernel with variance $\\sigma$ . The specific settings for the hyper-parameters $w_{f}$ , $w_{b},\\,\\sigma_{f}$ , and $\\sigma_{b}$ can be found in the experiments section. Finally, the adaptive code loss is computed by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}(Z,I_{b})=\\sum_{j=1}^{n^{2}}w(j)\\times\\mathrm{avg}\\{[Z(\\theta_{j})-I_{b}(\\theta_{j})]\\odot G(j)\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Gaussian distribution with bigger $\\sigma$ is flatter, which helps equalize the color within the module when updating the latent code. Although this makes modules easier to decode after iterations, bigger $\\sigma$ might create unnatural shadow in the face region. On the other hand, Gaussian distribution with smaller $\\sigma$ effectively regulates only the central region of a module, making the modules remain unscannable even after updates. ", "page_idx": 5}, {"type": "text", "text": "This problem is addressed by utilizing adaptive loss for different regions, i.e., applying smaller weight $w_{f}$ and $\\sigma_{f}$ in the face region to prevent distortion on face, and relatively larger $w_{f}$ and $\\sigma_{f}$ in remaining region to maintain balance between scannability and aesthetic quality. ", "page_idx": 5}, {"type": "text", "text": "Aesthetic Content Loss. To ensure the retention of aesthetic qualities while preserving face ID and enhancing scannability, we use the aesthetic content loss to retain essential visual characteristics. It is quantified by calculating $L^{2}$ -Wasserstein distance [1] (denoted as $D_{W2}$ ) of feature representations between $Z$ and $\\widehat{I}^{s}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal L}_{a}(Z,\\widehat{I^{s}})=\\sum_{i}D_{W2}(g_{i}(Z),g_{i}(\\widehat{I^{s}})),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $g_{i}$ is feature representations from a pre-trained VGG-19 [34] network at layer $i$ . The aesthetic content loss reflects the global aesthetic quality of the image. By optimizing both code loss and content loss, IDSE module adeptly balances the aesthetic quality, face-preserving, and scannability and creates optimal customized QR code images. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup and Configuration ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We implemented our pipeline in Python using the PyTorch framework and conducted experiments on an NVIDIA GeForce 4090 GPU. The scannability of QR images is tested using a 27-inch IPS display monitor with a refresh rate of 144Hz. In our experiments, we set control strengths for the InstantID network [40] and QR Controller at 0.8 and 1.4, respectively. The parameter $\\lambda$ in the marker harmonization process defaults to 0.8. The VAE configuration is consistent with the SD model. The face recognition model AntelopeV2 from InsightFace [14] assists the generation of face mask $M_{f}$ in IDRE. The VGG-19 architecture, pre-trained on the MS-COCO dataset, facilitates the feature map extraction in IDSE. The Adam optimizer powers the optimization within IDSE, performing 300 iterations at a learning rate of 0.002. Default settings for $\\sigma_{f}$ , $\\sigma_{b}$ , $w_{f}$ , and $w_{b}$ are 1.5, 3.0, 1.0, and 15.0 respectively. We produce QR code in version 5, each with $37\\times37$ modules. For clarity, we define $e$ as the number of error modules in QR image $I^{o}$ (excluding finder and alignment pattern areas), and $e_{f}$ as the number of error modules within the face region. Our dataset for comparative analysis contains 200 uniquely stylized QR images, each $1024\\times1024$ pixels in size, with diverse visual content and artistic styles. To more accurately assess the preservation of face identity, we define the face feature distance $d$ as the cosine similarity between the facial features (extracted using ArcFace [7]) of the generated QR image $I^{o}$ and the original face image $f$ . ", "page_idx": 5}, {"type": "table", "img_path": "rvBabL7DUu/tmp/87e18c28c30153daf5a469572c3b264cd2538c7e3973f443a7f075f213afffb8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "rvBabL7DUu/tmp/0e8c0524e900c94df4d8967982cb28b80c35640ce0394fc15df9128975452b57.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Qualitative Comparison ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Aesthetic Quality. In our comparative study, we evaluate our approach against several state-of-theart aesthetic QR code generation techniques, including QArt [6], Halftone QR code [5], ArtCoder [36] and Text2QR [43], as detailed in Table 1. QArt, Halftone QR and Text2QR take the original face image $f$ as the primary input, except that Text2QR takes in additional prompt input c. As ArtCoder is based on neural-style transfer technique, we employ $f$ and $I^{g}$ to serve as the content reference and the style reference respectively. The results show that Artcoder tends to render the texture of style image to face region, causing unwanted distortion on the face. Text2QR, on the other hand, cannot preserve face ID due to lack of specific control mechanisms for the face region. In contrast, our QR codes are adept at harmoniously integrating face ID, background and QR pattern, thereby achieving superior visual quality as well as scannability. ", "page_idx": 6}, {"type": "text", "text": "Identity Preservation. The comparison between original face image $f$ and the generated image $I^{o}$ is shown in Table 2. The face ID is well preserved in the final generated QR image $I^{o}$ , with minimal change in haircut or facial expression, which can be further customized by users by adding prompt. The facial region is consistent with the background in style, and the QR pattern is blended seamlessly into the picture. We also compare the generated image $I^{o}$ with output of the baseline pipeline InstantID [40] in Table 3, which shows that our pipeline achieves a similar level of identity preservation as the baseline. The outcomes displayed in Table 4 demonstrate that Face2QR consistently generates high-quality images across various poses. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Visual comparison of face ID preservation in face image $f$ and generated QR image $I^{o}$ . $I_{1}^{o}$ and $I_{2}^{o}$ are generated from $f_{1}$ , and $I_{3}^{o}$ and $I_{4}^{o}$ are generated from $f_{2}$ . Face feature distance $d$ is measured between pairs of $I^{o}$ and $f$ . ", "page_idx": 7}, {"type": "image", "img_path": "rvBabL7DUu/tmp/9ce5e140b171e12c874b199b97ae193f2c515f61a2f3aa79eabbb325eab53dad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Quantitative Comparsion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Scanning Robustness. In this study, we assess the scanning robustness of our QR images using different scanning applications. We first generate a batch of 20 aesthetically pleasing QR codes, each with a dimension of $1,\\!024\\times1{,}024$ pixels. These QR images are then displayed on a high-definition monitor in three standard sizes: $3\\mathrm{cm}\\times3\\mathrm{cm}$ , $5\\mathrm{cm}\\times5\\mathrm{cm}$ , and $7\\mathrm{cm}\\times7\\mathrm{cm}$ . During our controlled test, smartphones are held at a fixed distance of $25\\mathrm{cm}$ from the display, and each code is scanned for 3 seconds from different angles. Over a total of 50 trials, the percentage of successful scans is recorded in Table 5. The results reveal an average success rate exceeding $94\\%$ , showcasing high reliability of the generated QR images in diverse practical settings. It is also noted that QR images that fail the test in 3s can eventually be scanned if given more time. The scanning success rate is similar to that of Text2QR [43], as presented in our comparative analysis. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Visual comparison of identity preservation with InstantID [40]. ", "page_idx": 7}, {"type": "text", "text": "Table 4: Generated QR images (second row) using face images (first row) with different poses. ", "page_idx": 7}, {"type": "image", "img_path": "rvBabL7DUu/tmp/dba49b067829ef861d8d0bf60ac6241ee2f4ef07b57686a7db88cf7ad1b5dd15.jpg", "img_caption": ["Subjective Study. Figure 4 presents a user study consisting of 30 participants to evaluate 150 QR images (50 for each methods) generated by different methods (the approval from Institutional Review Board is obtained). Participants are asked to choose the better one from a pair of pictures in the aspect of face ID preservation and aesthetic quality. Each pair of QR images are generated by different methods using the same face image as input. The percentages represent how many times users prefer the results of a method over the other. Our results are preferred by most users. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Objective Study. Table 6 shows the statistical performance measured by taking the average of 100 samples. We use the feature distance $d$ , varying from -1 to 1, as a quantifiable measure for the ", "page_idx": 7}, {"type": "table", "img_path": "rvBabL7DUu/tmp/2558e1d7c937d457186274923ae19d30e80dc1dc3f255f43bb8d60b789a539a8.jpg", "table_caption": ["Table 5: Scannability success rates of QR codes across various decoders at different sizes and angles. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 6: Comparison of average face feature distance $d$ and average Aesbench scores $B_{a}$ . [Key: Best] ", "page_idx": 8}, {"type": "table", "img_path": "rvBabL7DUu/tmp/60cbb1aba64c391d835d18a88d6a6d6d887f05bb3b6fa8b5ebd6f2243a386a98.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "rvBabL7DUu/tmp/8c8ee24252b178bdb4b84c3f76e6c5a79f1bc8b94075ab3fc5a8b6b0ff734821.jpg", "img_caption": ["Figure 4: User study of different methods. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "preservation of face ID. The higher the distance $d$ , the generated face is more consistent with the original face image. We also use AesBench tool [8], which assigns aesthetic scores ranging from 0 to 100 (with higher scores denoting better aesthetics), to objectively evaluate the aesthetic quality of generated pictures. The results indicate that our approach exceeds competing methods in all evaluated metrics, confirming its capability to produce QR iamges with faithfully preserved face ID and high aesthetic quality. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 7: IDRE Ablation Study: Compared Table 8: IDSE Ablation Study: We examine the influwith result obtained by completing the entire ence of $w_{f},w_{b},\\sigma_{f},\\sigma_{b}$ to the generated QR image stage 2 (rightmost column), skipping stage $I^{o}$ . Model 1 $(w_{f}\\ =\\ w_{b},\\sigma_{f}\\ =\\ \\sigma_{b})$ tends to cre2 (leftmost column) or conducting stage 2 ate undesirable shadow in the face region. Model without IDRE (middle column) will result in 2 $(w_{f}=w_{b},\\sigma_{f}<\\sigma_{b})$ leads to an increased number more error modules $e_{f}$ in the face region. The of error modules $e$ . Model 3 $(w_{f}\\,<\\,w_{b},\\sigma_{f}\\,<\\,\\sigma_{b})$ distribution of error modules, highlighted as reaches a balance between face ID and scannability. bright areas, is depicted in the second row. Second row zooms in on the face region. ", "page_idx": 8}, {"type": "image", "img_path": "rvBabL7DUu/tmp/9d0735caf9bf7fee854a20143c0767f1f1ae9a80b13580966632a651c8d7cb1d.jpg", "img_caption": ["IDRE Module. Table 7 illustrates how skipping stage 2 or the IDRE module affects the scannability of resultant QR image $I^{s}$ . In stage 2, IDRE first rearranges the QR modules to construct a blueprint $I_{b}$ in which the QR pattern is compatible with face ID, and then SD model generates $I^{s}$ guided by $I_{b}$ . If stage 2 is bypassed, the output $I^{g}$ from stage 1 is used as the input for stage 3 directly. If IDRE is omitted, a normal QR code is used to guide the QR pattern generation in stage 2. The result shows that skipping stage 2 or IDRE results in a substantial increase in the number of error modules $e_{f}$ within the facial area, which significantly reduces the scannability of the QR code. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "IDSE Module. In stage 3, the IDSE module leverages adaptive code loss, which is key to maintaining face identity and simultaneously decreasing number of error modules. This loss function is determined by two parameter pairs $\\langle\\sigma_{f}$ and $\\sigma_{b}$ for Gaussian kernel; $w_{f}$ and $w_{b}$ for loss strength). Table 8 presents images produced by the IDSE with varying parameter configurations. The comparison shows that a uniform $\\sigma$ leads to distortions in the facial area, and a universal loss weight $w$ will increase the number of error modules and compromise the generated image\u2019s scannability. A clearer comparison is given by the normalized image difference visualization $D$ shown in Table 9. The visualization $D$ demonstrates the module-wise difference between the QR images before and after IDSE. The results demonstrate that the adaptive loss makes the modification in the face region gentler than uniform loss, reaching a better balance between face identity and scannability. ", "page_idx": 9}, {"type": "image", "img_path": "rvBabL7DUu/tmp/3619588842b233bd61d209a4bffd703ba467bb97d3fc95a5a7916582d52d15e3.jpg", "img_caption": ["Table 9: Image difference visualization $D$ of uniform loss (first row) and adaptive loss (second row). "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "rvBabL7DUu/tmp/e8c6aa7ac0a39928b11d4d3a5839ac803adcc1c020a07105f1fbf6543add3598.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In summary, we introduce Face2QR, an innovative pipeline that seamlessly integrates face ID, aesthetic design and scannability in the generation of QR codes. By introducing three key modules, i.e. IDQR for integrating face ID with aesthetic background, IDRS for resolving confilct between face ID and QR pattern, and IDSE for enhancing scannability while preserving face ID and aesthetic quality, our pipeline is able to balance between three inherently conflicting control signals and generate QR codes that preserve face ID, aesthetic quality and scannability at the same time. Extensive experiments demonstrate that Face2QR significantly outperforms previous methods, establishing a new benchmark for generating ID-preserved aesthetic QR codes. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our method is still constrained by some limitations of generative models. Although generative models are powerful, they can produce inconsistent results and often require substantial computing power to generate detailed, high-resolution images. Some typical bad cases caused by failure of generative models are shown in Table 10. As these computational models become more advanced, we can anticipate further improvements in the accuracy, speed, and overall aesthetic quality of the generated QR codes. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact. By enhancing the visual appeal and personal connection of QR codes, our work has the potential to revolutionize their use in entertainment, social media, marketing, and personal memorabilia, transforming them from mere tools for information transfer into objects of personal expression and aesthetic value. Looking forward, we anticipate that future work will not only refine these methods but also explore their integration into various technological ecosystems, consistently enriching the social and functional aspects of QR codes. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement. The work was supported in part by the National Natural Science Foundation of China under Grant 62301310 and 62225112, and in part by Sichuan Science and Technology Program under Grant 2024NSFSC1426. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein Generative Adversarial Networks. In Proceedings of the 34th International Conference on Machine Learning, 2017.   \n[2] Changsheng Chen, Wenjian Huang, Lin Zhang, and Wai Ho Mow. Robust and Unobtrusive Displayto-Camera Communications via Blue Channel Embedding. IEEE Transactions on Image Processing, 28(1):156\u2013169, 2018.   \n[3] Changsheng Chen, Wenjian Huang, Baojian Zhou, Chenchen Liu, and Wai Ho Mow. PiCode: A New Picture-Embedding 2D Barcode. IEEE Transactions on Image Processing, 25(8):3444\u20133458, 2016.   \n[4] Changsheng Chen, Baojian Zhou, and Wai Ho Mow. RA Code: A Robust and Aesthetic Code for Resolution-Constrained Applications. IEEE Transactions on Circuits and Systems for Video Technology, 28(11):3300\u20133312, 2018.   \n[5] Hung-Kuo Chu, Chia-Sheng Chang, Ruen-Rone Lee, and Niloy J Mitra. Halftone QR Codes. ACM Transactions on Graphics (TOG), 32(6):1\u20138, 2013.   \n[6] Russ Cox. Qartcodes. https://research.swtch.com/qart, 2012.   \n[7] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699, 2019. [8] Huang et al. Aesbench: An expert benchmark for multimodal large language models on image aesthetics perception. arXiv preprint arXiv: 2401.08276, 2024. [9] Han Fang, Dongdong Chen, Feng Wang, Zehua Ma, Honggu Liu, Wenbo Zhou, Weiming Zhang, and Neng-Hai Yu. TERA: Screen-to-Camera Image Code with Transparency, Efficiency, Robustness and Adaptability. IEEE Transactions on Multimedia, 24:955\u2013967, 2022.   \n[10] Han Fang, Weiming Zhang, Hang Zhou, Hao Cui, and Nenghai Yu. Screen-Shooting Resilient Watermarking. IEEE Transactions on Information Forensics and Security, 14(6):1403\u20131418, 2018.   \n[11] Anthony Fu. Stylistic qr code with stable diffusion. https://antfu.me/posts/ai-qrcode, 2023.   \n[12] Zhongpai Gao, Guangtao Zhai, and Chunjia Hu. The Invisible QR Code. In Proceedings of the 23rd ACM International Conference on Multimedia, pages 1047\u20131050, 2015.   \n[13] Gonzalo J Garateguy, Gonzalo R Arce, Daniel L Lau, and Ofelia P Villarreal. QR Images: Optimized Image Embedding in QR Codes. IEEE Transactions on Image Processing, 23(7):2842\u20132853, 2014.   \n[14] Jia Guo, Xiang An, Jinke Yu, Jing Yang, Alexandros Lattas, Baris Gecer, and Jiankang Deng. Insightface: A 2d and 3d face analysis project, 2023.   \n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[16] Jun Jia, Zhongpai Gao, Kang Chen, Menghan Hu, Xiongkuo Min, Guangtao Zhai, and Xiaokang Yang. RIHOOP: Robust Invisible Hyperlinks in Offilne and Online Photographs. IEEE Transactions on Cybernetics, pages 1\u201313, 2020.   \n[17] Jun Jia, Zhongpai Gao, Dandan Zhu, Xiongkuo Min, Guangtao Zhai, and Xiaokang Yang. Learning invisible markers for hidden codes in offline-to-online photography. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2022.   \n[18] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proc. CVPR, 2020.   \n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[20] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023.   \n[21] Wenhao Li, Guangyang Wu, Wenyi Wang, Peiran Ren, and Xiaohong Liu. Fastllve: Real-time low-light video enhancement with intensity-aware look-up table. In ACM Int. Conf. Multimedia, 2023.   \n[22] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. arXiv preprint arXiv:2312.04461, 2023.   \n[23] Zehua Ma, Xi Yang, Han Fang, Weiming Zhang, and Nenghai Yu. Oacode: Overall aesthetic 2d barcode on screen. IEEE Transactions on Multimedia, 2023.   \n[24] Maxim Maximov, Ismail Elezi, and Laura Leal-Taix\u00e9. Ciagan: Conditional identity anonymization generative adversarial networks. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.   \n[25] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.   \n[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[27] Pexels. Pexels: Free stock photos, royalty free stock images & videos. https://www.pexels.com/, 2024.   \n[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.   \n[29] Irving S Reed and Gustave Solomon. Polynomial codes over certain finite fields. Journal of the society for industrial and applied mathematics, 8(2):300\u2013304, 1960.   \n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2022.   \n[31] Zhihao Shi, Xiaohong Liu, Chengqi Li, Linhui Dai, Jun Chen, Timothy N. Davidson, and Jiying Zhao. Learning for unconstrained space-time video super-resolution. IEEE Trans. Broadcast., 68(2):345\u2013358, 2022.   \n[32] Zhihao Shi, Xiaohong Liu, Kangdi Shi, Linhui Dai, and Jun Chen. Video frame interpolation via generalized deformable convolution. IEEE Trans. Multim., 24:426\u2013439, 2022.   \n[33] Zhihao Shi, Xiangyu Xu, Xiaohong Liu, Jun Chen, and Ming-Hsuan Yang. Video frame interpolation transformer. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2022.   \n[34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, ICCV, 2015.   \n[35] Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Ji Wan, and Mingliang Xu. Q-Art Code: Generating Scanning-robust Art-style QR Codes by Deformable Convolution. In ACM Int. Conf. Multimedia, 2021.   \n[36] Hao Su, Jianwei Niu, Xuefeng Liu, Qingfeng Li, Ji Wan, Mingliang Xu, and Tao Ren. Artcoder: an end-to-end method for generating scanning-robust stylized qr codes. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2021.   \n[37] Matthew Tancik, Ben Mildenhall, and Ren Ng. Stegastamp: Invisible Hyperlinks in Physical Photographs. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.   \n[38] Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. Face0: Instantaneously conditioning a text-to-image model on a face. In SIGGRAPH Asia 2023 Conference Papers, pages 1\u201310, 2023.   \n[39] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.   \n[40] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024.   \n[41] Wenyi Wang, Guangyang Wu, Weitong Cai, Liaoyuan Zeng, and Jianwen Chen. Robust prior-based single image super resolution under multiple gaussian degradations. IEEE Access, 8:74195\u201374204, 2020.   \n[42] Eric Wengrowski and Kristin Dana. Light Field Messaging with Deep Photographic Steganography. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019.   \n[43] Guangyang Wu, Xiaohong Liu, Jun Jia, Xuehao Cui, and Guangtao Zhai. Text2qr: Harmonizing aesthetic customization and scanning robustness for text-guided qr code generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8456\u20138465, 2024.   \n[44] Guangyang Wu, Xiaohong Liu, Kunming Luo, Xi Liu, Qingqing Zheng, Shuaicheng Liu, Xinyang Jiang, Guangtao Zhai, and Wenyi Wang. Accflow: Backward accumulation for long-range optical flow. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2023.   \n[45] Guangyang Wu, Lili Zhao, Wenyi Wang, Liaoyuan Zeng, and Jianwen Chen. Pred: A parallel network for handling multiple degradations via single model in single image super-resolution. In Proc. IEEE Int. Conf. Image Process. (ICIP), 2019.   \n[46] Mingliang Xu, Qingfeng Li, Jianwei Niu, Hao Su, Xiting Liu, Weiwei Xu, Pei Lv, Bing Zhou, and Yi Yang. ART-UP: A novel method for generating scanning-robust aesthetic QR codes. ACM Trans. Multim. Comput. Commun. Appl., 17(1):25:1\u201325:23, 2021.   \n[47] Mingliang Xu, Hao Su, Yafei Li, Xi Li, Jing Liao, Jianwei Niu, Pei Lv, and Bing Zhou. Stylized aesthetic QR code. IEEE Trans. Multim., 21(8):1960\u20131970, 2019.   \n[48] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, and Humphrey Shi. Prompt-Free Diffusion: Taking\" Text\" out of Text-to-Image Diffusion Models. arXiv preprint arXiv:2305.16223, 2023.   \n[49] Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Zhang, Pei Cheng, Gang Yu, and Bin Fu. Facestudio: Put your face everywhere in seconds. arXiv preprint arXiv:2312.02663, 2023.   \n[50] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models. arXiv preprint arXiv:2308.06721, 2023.   \n[51] Liming Zhai, Qing Guo, Xiaofei Xie, Lei Ma, Yi Estelle Wang, and Yang Liu. A3gan: Attributeaware anonymization networks for face de-identification. In Proceedings of the 30th ACM International Conference on Multimedia, pages 5303\u20135313, 2022.   \n[52] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Additional Experiments ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1.1 Additional Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Our Face2QR pipeline is generalizable to real faces, generated realistic faces, and cartoon faces. The experimental results in Table 11 demonstrate that facial identities are well preserved and seamlessly blended into the background in all generated QR images, showcasing the effectiveness of Face2QR across these three face types. ", "page_idx": 12}, {"type": "text", "text": "A.1.2 Visualization of Intermediate Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Table 14, we show the intermediate results of Face2QR. Here, $I^{g}$ represents the output of stage 1, $I_{b}$ signifies blueprint image generated by IDRE, and $I^{s}$ denotes the results of regeneration results from stage 2. The blueprint $I_{b}$ guides both the generation of $I^{s}$ in IDQR module within stage 2, and the IDSE process in stage 3 to generate QR image $I^{o}$ with a harmonious balance between face ID, aesthetic quality and scannability. Table 12 presents results from different iterations in the IDSE process. Additionally, Table 15 presents the prompts and models used in the generation of the aforementioned QR image samples. ", "page_idx": 12}, {"type": "text", "text": "A.1.3 Loss Curve & Running Time ", "text_level": 1, "page_idx": 12}, {"type": "image", "img_path": "rvBabL7DUu/tmp/c20ea99bb6f92206f667055165232fe69e5b1d811114993dae877293ab0707df.jpg", "img_caption": ["Figure 5: Curve of different metrics during IDSE. We show metric curves for diverse samples, each represented by distinct colors. These curves illustrate metric variations over 300 iterations. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "In stage 3, we use IDSE to enhance the scannability of $I^{s}$ by updating the its latent code. The dynamic loss function consists of aesthetic content loss $\\mathcal{L}_{a}$ and adaptive code loss $\\mathcal{L}_{c}$ . Both losses apply at the same time and helps the update process to converge sooner. The total number of error module $e$ acts as a indicator of scannability, and the error module number in the face region $e_{f}$ helps visualize the modification process in the face region. Figure 5 illustrates the above four metrics. The IDSE process converges in about 120 seconds when executed on an NVIDIA 4090 GPU to enhance images of size $1024\\!\\times\\!1024$ pixels. ", "page_idx": 12}, {"type": "text", "text": "A.2 Bad Cases ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In addition to bad cases shown in Table 10, we present suboptimal cases when the face image and the prompt are confilct with each other in Table 13. For example, the first row in Table 13 shows the case when a face image of a woman and the prompt \"A male man\" are given at the same time. ", "page_idx": 12}, {"type": "text", "text": "A.3 The Interface for User Study ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The scoring interface of user study is shown in Figure 6. We adopt the pair-wise comparisons for subjective study rather than absolute ratings since the former is relatively more accurate in general. ", "page_idx": 13}, {"type": "text", "text": "Table 11: Real face images of ordinary people (Row 1) collected from [27], generated realistic face (left three on Row 3) using StyleGAN2 [18] and cartoon faces (right three on Row 3) with corresponding QR images $I^{o}$ (Rows 2 and 4). ", "page_idx": 13}, {"type": "image", "img_path": "rvBabL7DUu/tmp/f4c7d2187952373c40332b774ad87091cf7b300ed17e70f5ff5dddc2f4203012.jpg", "img_caption": ["Figure 6: The user interface. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 12: Visualization of IDSE process at different iteration steps. ", "page_idx": 14}, {"type": "text", "text": "Table 13: Bad $I^{s}$ results caused by conflict between face image and prompt. ", "page_idx": 14}, {"type": "image", "img_path": "rvBabL7DUu/tmp/a1224ed46101adc18a7792287069d2c128d647f4f2a2cb4877ca1796e8f129c8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "rvBabL7DUu/tmp/b75eb2c10e741a556f702f0b88edb2794001a6c7eb25bb041e495bad9f59df1c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "rvBabL7DUu/tmp/937b52c7c65a59eb9e262da2bfccd54aa891d00b28655a7b080bb6326fd28471.jpg", "table_caption": ["Table 15: Prompts for generated QR codes in the paper. All images are generated with size of $1{,}024{\\times}1{,}024$ . The generative model is uniformly SDXL Unstable Diffusers YamerMIX. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: In abstract, the main contributions of this paper are emphasized. Furthermore, in the last paragraph of the introduction, these contributions are clearly listed again. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 17}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Justification: Please refer to Section 5. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide all needed information to reproduce the main experimental results of this paper in Section 4. Our code will be released upon publication. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We consider publishing the code at https://github.com/cavosamir/ Face2QR once we complete our patent application process. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: All experiments details are illustrated in Section 4.1. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper mainly conducts qualitative comparisons and subjective experiments. Therefore, the corresponding error bars are not applicable. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Computational resources have been described in Section 4.1. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: This work is conducted in accordance with the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Please refer to the Section 5. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 21}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited, and the license and terms of use are explicitly mentioned and are properly respected. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The new assets introduced in the paper are well documented alongside the assets. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 22}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper includes the full text of instructions given to participants and screenshots, and the human subjects are paid at least the minimum wage in the country of the data collector, following the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 22}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: There is no such potential risks aware for research with human subjects in this paper. We have obtained the IRB approval and also adhere to the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}]