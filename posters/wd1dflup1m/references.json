{"references": [{"fullname_first_author": "Anurag Ajay", "paper_title": "Is conditional generative modeling all you need for decision making?", "publication_date": "2023-00-00", "reason": "This paper introduces a novel approach to decision-making using conditional generative models, which is highly relevant to the proposed method in this paper."}, {"fullname_first_author": "Kevin Black", "paper_title": "Training diffusion models with reinforcement learning", "publication_date": "2023-05-13", "reason": "This paper explores the use of diffusion models in reinforcement learning, which is directly related to the core idea of the current paper."}, {"fullname_first_author": "Huayu Chen", "paper_title": "Noise contrastive alignment of language models with explicit rewards", "publication_date": "2024-02-24", "reason": "This paper addresses the alignment of language models using explicit rewards, offering a relevant theoretical framework for aligning diffusion models with Q-functions."}, {"fullname_first_author": "Huayu Chen", "paper_title": "Offline reinforcement learning via high-fidelity generative behavior modeling", "publication_date": "2023-00-00", "reason": "This paper presents a method for offline reinforcement learning using generative behavior modeling, a technique crucial to the proposed method."}, {"fullname_first_author": "Zibin Dong", "paper_title": "Aligndiff: Aligning diverse human preferences via behavior-customisable diffusion model", "publication_date": "2024-00-00", "reason": "This paper demonstrates aligning diffusion models with human preferences, directly addressing the alignment challenge tackled in the current paper."}]}