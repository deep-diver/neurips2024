[{"figure_path": "vWSll6M9pj/tables/tables_5_1.jpg", "caption": "Table 1: Supervised ablations on the full LRS3 dataset using our Base model. Default settings are in gray in all tables of the paper.", "description": "This table presents ablation studies on a supervised training approach using the full LRS3 dataset.  It investigates three aspects: (a) whether to share model parameters across different modalities (visual, auditory, and audiovisual) or use separate models for each, (b) different strategies for sampling the modalities during training (random sampling vs. weighted average of losses), and (c) the impact of varying the relative weight given to the video loss in the overall loss function. The results showcase the effectiveness of using a single, shared model and weighted average loss for optimal performance.", "section": "4.1 Unified Supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_5_2.jpg", "caption": "Table 1: Supervised ablations on the full LRS3 dataset using our Base model. Default settings are in gray in all tables of the paper.", "description": "This table presents ablation studies on the supervised training of a unified speech recognition model using the full LRS3 dataset.  It explores three key aspects: (a) whether to share model parameters across all modalities (visual, auditory, and audiovisual) or use separate models; (b) the method of modality sampling during training (random sampling vs. using all modalities in each iteration); and (c) the relative weight given to the video loss during training.  The table shows Word Error Rates (WER) for each modality under various settings, highlighting the impact of each choice on model performance.", "section": "4.1 Unified Supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_5_3.jpg", "caption": "Table 1: Supervised ablations on the full LRS3 dataset using our Base model. Default settings are in gray in all tables of the paper.", "description": "This table presents ablation studies on the supervised training of the unified speech recognition model. It explores three aspects: (a) using a single shared model versus modality-specific models; (b) different data sampling strategies for training (random vs. all modalities); and (c) varying the weight of visual modality loss.  The results, expressed as Word Error Rates (WERs), demonstrate the effectiveness of the unified approach in overcoming optimization challenges associated with training from scratch, particularly in visual and audiovisual modalities. ", "section": "4.1 Unified Supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_6_1.jpg", "caption": "Table 2: Semi-supervised ablations under the LRS3 low-resource setting using our Base model.", "description": "This table shows the results of ablating different components of the semi-supervised training framework proposed in the paper.  It investigates the effects of varying the relative weight of labelled data for audio and video, the teacher's EMA momentum parameter, and the type of loss function (CTC vs. CTC-attention) on the performance of the model in terms of Word Error Rate (WER) for visual (V), auditory (A), and audiovisual (AV) speech recognition tasks. The goal is to understand the contributions of each component to the overall performance and optimize the training process.", "section": "4.2 Unified Semi-supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_7_1.jpg", "caption": "Table 3: Self-supervised ablations under the LRS3 low-resource setting using our Base model. (a) Target type. \"Scratch\" refers to semi-supervised training only. (b) Averaging blocks vs. using only last encoder block. (c) Predictor depth.", "description": "This table shows the ablation study on different components of the self-supervised pre-training method.  It demonstrates that using audio-visual targets for pre-training yields the best performance. Averaging the outputs across all encoder blocks outperforms using only the last encoder block, and a deeper predictor improves performance slightly. The table also compares the results with only semi-supervised training, showing the complementary effect of self-supervised pre-training.", "section": "Unified Self-supervised Pre-training"}, {"figure_path": "vWSll6M9pj/tables/tables_7_2.jpg", "caption": "Table 3: Self-supervised ablations under the LRS3 low-resource setting using our Base model. (a) Target type. \"Scratch\" refers to semi-supervised training only. (b) Averaging blocks vs. using only last encoder block. (c) Predictor depth.", "description": "This table presents ablation studies on the self-supervised pre-training stage of the Unified Speech Recognition (USR) model. It investigates the impact of different target types (visual, auditory, audiovisual) during pre-training, comparing the use of averaging encoder block outputs versus only the last encoder block, and varying the depth of the predictor network.  The results show how these choices affect the performance of the model on visual, auditory, and audiovisual speech recognition tasks.", "section": "4.3 Unified Self-supervised Pre-training"}, {"figure_path": "vWSll6M9pj/tables/tables_8_1.jpg", "caption": "Table 4: Comparisons with self-supervised methods. LRS3 results for the low-resource (LR) and high-resource (HR) labelled data settings, with 30 and 433 hours of labelled data, respectively. Best results in bold, second-best underlined.", "description": "This table compares the performance of the proposed USR method with several existing self-supervised methods on the LRS3 dataset for speech recognition.  It shows the Word Error Rates (WER) for different modalities (visual, auditory, and audiovisual) under both low-resource and high-resource settings. The table also indicates whether the methods used shared parameters and the pre-training data used.  The best and second-best results are highlighted for easier comparison.", "section": "5 Comparisons with Previous Results"}, {"figure_path": "vWSll6M9pj/tables/tables_8_2.jpg", "caption": "Table 5: Comparisons with the state-of-the-art on LRS3. *Labels include automatic transcriptions from ASR models trained on large-scale, often non-public datasets. \u201cST\u201d denotes offline self-training.", "description": "This table compares the proposed USR model's performance with several state-of-the-art models on the LRS3 dataset.  The comparison considers various methods, including both supervised and self-supervised approaches, highlighting the differences in labelled data usage, language model incorporation, and shared parameters. The results are presented in terms of Word Error Rates (WER) for visual (V), auditory (A), and audiovisual (AV) speech recognition tasks. The table demonstrates the USR's competitive performance, particularly in achieving state-of-the-art results while using a single model for all three tasks.", "section": "5 Comparisons with Previous Results"}, {"figure_path": "vWSll6M9pj/tables/tables_9_1.jpg", "caption": "Table 6: Comparisons with the state-of-the-art on LRS2. *Includes methods that use automatic transcriptions from ASR models trained on large-scale datasets. \u201cST\u201d stands for self-training.", "description": "This table compares the proposed USR method with other state-of-the-art methods on the LRS2 dataset for audiovisual speech recognition.  It shows the performance (WER) of various methods on visual, audio, and audiovisual speech recognition tasks, considering factors like the amount of labeled and unlabeled data used, whether language models were employed, and if the method used shared parameters or modality-specific ones. The table highlights the superior performance of the unified speech recognition (USR) approach, even when compared to methods that use a significantly higher amount of labeled data.", "section": "5 Comparisons with Previous Results"}, {"figure_path": "vWSll6M9pj/tables/tables_14_1.jpg", "caption": "Table 7: Configuration of our models. Unlike in [13, 17, 18], the number of parameters includes the whole model, including the decoder and feature extractors.", "description": "This table details the configurations of three different models used in the paper: Base, Base+, and Large.  It shows the number of parameters (in millions), encoder blocks, decoder blocks, attention dimension, attention heads, and MLP size for each model.  Note that unlike previous related works, the parameter count includes those of the decoder and feature extractors.", "section": "C.4 Model Configurations"}, {"figure_path": "vWSll6M9pj/tables/tables_14_2.jpg", "caption": "Table 8: Supervised/semi-supervised training settings.", "description": "This table lists the hyperparameters used for both supervised and semi-supervised training in the paper.  It includes details such as the number of training epochs, the optimizer used (AdamW), the learning rate, weight decay, and other settings specific to the training process. The values are given separately for low-resource and high-resource settings, as well as for different model sizes (Base and Large).", "section": "3.2 Unified Semi-supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_15_1.jpg", "caption": "Table 9: Settings for pre-training.", "description": "This table lists the hyperparameters used for the self-supervised pre-training stage of the Unified Speech Recognition (USR) model.  The hyperparameters include details on the number of training epochs, the warmup epochs for the optimizer, the optimizer used (AdamW), the learning rate, the optimizer's beta1 and beta2 parameters, the weight decay, the learning rate schedule, the drop rate (dropout), the gradient clipping threshold, the video augmentations applied, and the number of frames processed per GPU.  Different values are provided for the LRS3 and LRS3+VoxCeleb2 datasets, and for Base, Base+, and Large model sizes.", "section": "C.6 Pre-training Settings"}, {"figure_path": "vWSll6M9pj/tables/tables_15_2.jpg", "caption": "Table 10: More semi-supervised ablations under the LRS3 low-resource setting using our Base model. (a) Filtering thresholds  \u03c4ctc and \u03c4att for CTC and attention, respectively. (b) Hard versus soft sampling.", "description": "This table presents ablation studies on the semi-supervised training of the proposed Unified Speech Recognition (USR) model.  It explores the impact of different filtering thresholds (\u03c4ctc, \u03c4att) on the CTC and attention losses, comparing various values to determine the optimal balance between precision and recall.  Additionally, it investigates the difference between \"hard\" and \"soft\" sampling methods for pseudo-label generation during semi-supervised training and assesses which method achieves better performance. The results are presented as Word Error Rates (WER) for visual (V), audio (A), and audiovisual (AV) modalities.", "section": "4.2 Unified Semi-supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_15_3.jpg", "caption": "Table 10: More semi-supervised ablations under the LRS3 low-resource setting using our Base model. (a) Filtering thresholds  \u03c4ctc and \u03c4att for CTC and attention, respectively. (b) Hard versus soft sampling.", "description": "This table shows the results of ablations on semi-supervised training of the unified model under the LRS3 low-resource setting.  It explores two main aspects:  (a) the effect of different filtering thresholds (\u03c4ctc and \u03c4att) applied to CTC and attention probabilities during pseudo-label generation and (b) a comparison between a hard sampling method and a soft sampling method for pseudo-label generation.  The WER (word error rate) for visual (V), auditory (A), and audiovisual (AV) modalities is reported to evaluate the performance impact of these hyperparameters.", "section": "4.2 Unified Semi-supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_16_1.jpg", "caption": "Table 11: More self-supervised ablations under the LRS3 low-resource setting using our Base model.\n(a) Mask probability.\n(b) Pre-training target types.", "description": "This table shows the ablation study of the self-supervised pre-training stage. The first part shows the effect of different mask probabilities on the final WER, while the second part demonstrates how the choice of pre-training target types (visual, auditory, or audiovisual) affects the performance of the model.  The results highlight the impact of the hyperparameter and the choice of targets on the final accuracy of the model.", "section": "4.3 Unified Self-supervised Pre-training"}, {"figure_path": "vWSll6M9pj/tables/tables_16_2.jpg", "caption": "Table 11: More self-supervised ablations under the LRS3 low-resource setting using our Base model.", "description": "This table presents ablation studies on the self-supervised pre-training stage of the USR model.  It investigates two hyperparameters: the mask probability during pre-training and the type of targets used for pre-training.  The results show the impact of these hyperparameters on the final word error rates (WER) for Visual (V), Auditory (A), and Audiovisual (AV) speech recognition tasks.", "section": "4.3 Unified Self-supervised Pre-training"}, {"figure_path": "vWSll6M9pj/tables/tables_17_1.jpg", "caption": "Table 6: Comparisons with the state-of-the-art on LRS2. *Includes methods that use automatic transcriptions from ASR models trained on large-scale datasets. \u201cST\u201d stands for self-training.", "description": "This table compares the proposed USR method with other state-of-the-art methods on the LRS2 dataset for visual, audio, and audio-visual speech recognition.  It shows the performance (WER) of various methods, indicating whether they used shared parameters, language models, and self-training techniques, along with the amount of labeled and unlabeled data used. The table highlights the superior performance of the USR method compared to other techniques.", "section": "5 Comparisons with Previous Results"}, {"figure_path": "vWSll6M9pj/tables/tables_17_2.jpg", "caption": "Table 13: Comparisons between supervised and our semi-supervised fine-tuning. We use the LRS3 low-resource setting and our Base model.", "description": "This table compares the performance of supervised and semi-supervised fine-tuning methods using the LRS3 low-resource dataset and the Base model.  It shows Word Error Rates (WER) for Visual (V), Audio (A), and Audiovisual (AV) speech recognition across four different fine-tuning scenarios: supervised training, supervised training with additional training \"tricks\" to mitigate overfitting, semi-supervised training, and semi-supervised training with the same \"tricks\". The results highlight the effectiveness of the semi-supervised approach, especially when compared to the supervised approach with or without the use of overfitting mitigation techniques.", "section": "4.2 Unified Semi-supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_17_3.jpg", "caption": "Table 13: Comparisons between supervised and our semi-supervised fine-tuning. We use the LRS3 low-resource setting and our Base model.", "description": "This table compares the performance of supervised and semi-supervised fine-tuning methods on the LRS3 low-resource dataset using the Base model. It shows the Word Error Rates (WER) for visual (V), audio (A), and audio-visual (AV) speech recognition tasks under different fine-tuning approaches.  Specifically, it contrasts supervised fine-tuning with and without additional training techniques (referred to as \"tricks\") against semi-supervised fine-tuning with and without those same techniques.  The pre-training target types (V, A, or AV) used for supervised fine-tuning are also shown.", "section": "4.2 Unified Semi-supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_17_4.jpg", "caption": "Table 14: Experiments with auditory noise. We compare USR with the modality-specific BRAVEn method on LRS3 with different signal-to-noise-ratio (SNR) levels. We use Base models trained under the low-resource setting.", "description": "This table compares the performance of the Unified Speech Recognition (USR) model and the BRAVEn model on the LRS3 dataset under various levels of auditory noise.  It shows Word Error Rates (WERs) for both auditory-only (A) and audio-visual (AV) speech recognition for different signal-to-noise ratios (SNRs), including a clean condition.  The table allows for comparison of how the performance of each model changes with different levels of noise.", "section": "G Experiments with Auditory Noise"}, {"figure_path": "vWSll6M9pj/tables/tables_18_1.jpg", "caption": "Table 15: Error bars. We report the mean and standard deviation over five runs with random seeds. We use our Base model with LRS3 as the pre-training dataset.", "description": "This table shows the mean and standard deviation of Word Error Rates (WER) for visual (V), audio (A), and audiovisual (AV) speech recognition tasks across five independent runs with different random seeds.  The results are reported for both low-resource and high-resource data settings, using the 'Base' model and LRS3 dataset for pre-training.  The error bars represent the variability in the model's performance across multiple runs, giving an indication of the model's stability.", "section": "H Error Bars"}, {"figure_path": "vWSll6M9pj/tables/tables_19_1.jpg", "caption": "Table 16: Comparison with AV-CPL. LRS3 results for the low-resource (LR) and high-resource (HR) labelled data settings. We show results for the Large model using LRS3+Vox2 as the pre-training dataset.", "description": "This table compares the performance of the proposed USR method against the AV-CPL method [36] on the LRS3 dataset under both low-resource and high-resource conditions.  It specifically shows the Word Error Rate (WER) achieved by each method for visual (V), auditory (A), and audiovisual (AV) speech recognition. The results highlight the significant performance improvement of USR over AV-CPL, especially in the low-resource setting.", "section": "5.1 Comparisons with Self-supervised Methods"}, {"figure_path": "vWSll6M9pj/tables/tables_19_2.jpg", "caption": "Table 17: Summary of the impact of semi- and self-supervised training under the LRS3 low-resource setting using our Base model. We compare four approaches: supervised training on 30 hours of labelled data, self-supervised pre-training with supervised fine-tuning, semi-supervised training, and self-supervised pre-training with semi-supervised fine-tuning.", "description": "This table summarizes the results of four different training approaches on the LRS3 low-resource dataset using the base model. The four approaches compared are: only supervised training, self-supervised pre-training followed by supervised fine-tuning, semi-supervised training, and a combination of self-supervised pre-training and semi-supervised fine-tuning. The table presents word error rates (WER) for visual (V), auditory (A), and audiovisual (AV) speech recognition tasks for each approach.", "section": "4.2 Unified Semi-supervised Training"}, {"figure_path": "vWSll6M9pj/tables/tables_20_1.jpg", "caption": "Table 18: Failure cases on the LRS3 test set. We use the Large model trained in the high-resource setting with LRS3+VoxCeleb2.", "description": "This table showcases examples from the LRS3 test set where the Large model (trained with high-resource settings using LRS3 and VoxCeleb2) made errors in transcription.  The table compares the ground truth transcription with the ASR, VSR, and AVSR outputs for each example, highlighting the types of errors made.  The examples illustrate instances where the model struggled with phonetically similar sounds or where visual information could help disambiguate.", "section": "L Failure Cases"}]