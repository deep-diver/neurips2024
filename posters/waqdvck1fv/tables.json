[{"figure_path": "wAqdvcK1Fv/tables/tables_7_1.jpg", "caption": "Table 1: Results on real-world datasets.", "description": "This table presents the results of experiments conducted on six real-world tabular datasets using various methods for tabular data synthesis.  The methods compared include CTGAN, TVAE, TabCD, TabDDPM, and several variants of the proposed TabED method (TabED-Uni, TabED-Grid, TabED-Cyc, TabED-Ord, TabED-Str).  The evaluation metrics used are AUC (Area Under the ROC Curve) for classification tasks and RMSE (Root Mean Squared Error) for the regression task. The average rank across all datasets is also provided, indicating the overall performance of each method.", "section": "6.2 Tabular Data Synthesising"}, {"figure_path": "wAqdvcK1Fv/tables/tables_8_1.jpg", "caption": "Table 2: Experimental results for discrete image modelling. We report the negative log-likelihood (NLL) on the test set for different models. The results of Gibbs, GWG, and DULA are taken from Zhang et al. (2022b), and the result of EB-GFN is from Zhang et al. (2022a).", "description": "The table presents the negative log-likelihood (NLL) results for various discrete image modeling methods on three different datasets: Static MNIST, Dynamic MNIST, and Omniglot.  It compares the performance of the proposed Energy Discrepancy (ED) methods (ED-Bern and ED-Grid) against existing approaches including Gibbs sampling, Gibbs with Gradients (GWG), Energy-based Generative Flow Networks (EB-GFN), and Discrete Unadjusted Langevin Algorithm (DULA).  The results show the effectiveness of ED in achieving comparable or superior NLL performance compared to the baselines.", "section": "6.3 Discrete Image Modelling"}, {"figure_path": "wAqdvcK1Fv/tables/tables_20_1.jpg", "caption": "Table 3: Experimental results of discrete density estimation. We display the negative log-likelihood (NLL). The results of baselines are taken from Zhang et al. (2022a).", "description": "This table compares the performance of different methods for discrete density estimation.  The negative log-likelihood (NLL) is used as the evaluation metric.  It shows the results for various synthetic datasets (2spirals, 8gaussians, circles, moons, pinwheel, swissroll, checkerboard), and includes results from several baseline methods (PCD, ALOE+, EB-GFN) for comparison. The table highlights the effectiveness of the proposed method (ED-Bern and ED-Grid) compared to existing approaches.", "section": "6.1 Discrete Density Estimation"}, {"figure_path": "wAqdvcK1Fv/tables/tables_22_1.jpg", "caption": "Table 3: Experimental results of discrete density estimation. We display the negative log-likelihood (NLL). The results of baselines are taken from Zhang et al. (2022a).", "description": "The table presents the negative log-likelihood (NLL) results for discrete density estimation using different methods.  It compares the performance of the proposed Energy Discrepancy (ED) approach (ED-Bern and ED-Grid) against three baseline methods: PCD, ALOE+, and EB-GFN.  The results are shown for eight different synthetic datasets, each with unique characteristics.", "section": "6.1 Discrete Density Estimation"}, {"figure_path": "wAqdvcK1Fv/tables/tables_22_2.jpg", "caption": "Table 4: Experimental results of discrete density estimation. We display the MMD (in units of 1 \u00d7 10\u207b\u2074). The results of baselines are taken from Zhang et al. (2022a).", "description": "This table presents the Maximum Mean Discrepancy (MMD) results for different discrete density estimation methods on several synthetic datasets.  Lower MMD values indicate better performance, reflecting a closer match between the estimated density and the true data generating distribution. The results are compared against baselines from a previous study by Zhang et al. (2022a).  The datasets include 2spirals, 8gaussians, circles, moons, pinwheel, swissroll, and checkerboard, representing various data distributions.", "section": "6.1 Discrete Density Estimation"}, {"figure_path": "wAqdvcK1Fv/tables/tables_23_1.jpg", "caption": "Table 5: Statistics of the real-world datasets.", "description": "This table presents the statistics of six real-world datasets used in the paper's experiments on tabular data synthesis.  For each dataset, it shows the number of rows, the number of numerical features (# Num), the number of categorical features (# Cat), and the number of instances in the training, validation, and testing sets. It also specifies the type of task (binary classification or regression) for each dataset. This information is crucial for understanding the experimental setup and the generalizability of the results.", "section": "6.2 Tabular Data Synthesising"}, {"figure_path": "wAqdvcK1Fv/tables/tables_24_1.jpg", "caption": "Table 6: Results on density similarity between the synthesis and real tabular data.", "description": "This table presents the results of evaluating the quality of synthetic tabular data generated by various models. Two metrics are used for evaluation: single-column density similarity and pair-wise correlation similarity.  Single-column density similarity measures how similar the distribution of values in each individual column is between the real and synthetic data. Pair-wise correlation similarity compares the correlation between pairs of columns in the real and synthetic data.  The table shows the results for several different models, including the proposed energy discrepancy (ED) methods and several baselines.", "section": "6.2 Tabular Data Synthesising"}, {"figure_path": "wAqdvcK1Fv/tables/tables_25_1.jpg", "caption": "Table 7: Running time complexity comparison for energy discrepancy and contrastive divergence.", "description": "This table compares the running time complexity per iteration and per epoch for energy discrepancy and contrastive divergence methods.  The contrastive divergence methods use varying numbers of Markov Chain Monte Carlo (MCMC) steps (CD-1, CD-5, CD-10), while energy discrepancy uses two variants (ED-Bern, ED-Grid). The results show that energy discrepancy methods are significantly faster, particularly because they don't rely on computationally expensive MCMC sampling.", "section": "6.3 Discrete Image Modelling"}, {"figure_path": "wAqdvcK1Fv/tables/tables_25_2.jpg", "caption": "Table 8: Experimental results of the comparison between energy discrepancy and contrastive divergence with varying MCMC steps.", "description": "This table compares the performance of energy discrepancy and contrastive divergence methods for training EBMs on three image datasets (Static MNIST, Dynamic MNIST, and Omniglot).  The contrastive divergence results use varying numbers of MCMC steps (CD-1, CD-3, CD-5, CD-7, CD-10), while energy discrepancy results are shown for Bernoulli and Grid versions (ED-Bern, ED-Grid). The negative log-likelihood (NLL) is used as the evaluation metric.  The table shows that ED-Bern and ED-Grid consistently achieve comparable or better results than CD with multiple MCMC steps across all three datasets.", "section": "6.3 Discrete Image Modelling"}, {"figure_path": "wAqdvcK1Fv/tables/tables_26_1.jpg", "caption": "Table 9: Discrete image modelling results of ED-Grid on the static MNIST dataset with different M and w = 1.", "description": "This table presents the negative log-likelihood (NLL) results for the static MNIST dataset using the ED-Grid method with varying numbers of negative samples (M).  It shows that the model performance is relatively stable across different values of M, indicating robustness to this hyperparameter.", "section": "D.3 Discrete Image Modelling"}, {"figure_path": "wAqdvcK1Fv/tables/tables_26_2.jpg", "caption": "Table 10: Mean negative log-RMSE (higher is better) between the learned connectivity matrix J and the true matrix J for different values of D and \u03c3. The results of baselines are directly taken from Zhang et al. (2022a).", "description": "This table compares the performance of different methods for learning the connectivity matrix J in an Ising model with different grid sizes (D) and coupling strengths (\u03c3).  The negative log-RMSE metric measures the difference between the learned matrix and the true matrix. Lower values indicate better performance.  The table includes results for Gibbs sampling, GWG (Gibbs with gradients), EB-GFN (energy-based generative flow networks), ED-Bern (energy discrepancy with Bernoulli perturbation), and ED-Grid (energy discrepancy with grid perturbation). The results for Gibbs, GWG, and EB-GFN are taken from a previous study by Zhang et al. (2022a).", "section": "D.4 Training Ising Models"}, {"figure_path": "wAqdvcK1Fv/tables/tables_27_1.jpg", "caption": "Table 11: Graph generation results in terms of MMD. Avg. denotes the average over three MMD results.", "description": "This table presents a comparison of different graph generation methods on the Ego-small dataset.  The methods are evaluated based on three graph statistics: degree, cluster, and orbit, using the Maximum Mean Discrepancy (MMD) metric.  The \"Avg.\" column shows the average MMD across these three metrics.  Lower MMD values indicate better performance.", "section": "6.5 Graph Generation"}, {"figure_path": "wAqdvcK1Fv/tables/tables_28_1.jpg", "caption": "Table 12: Overview of all introduced energy discrepancy methods", "description": "This table summarizes the naming conventions and available tuning parameters for all introduced energy discrepancy methods. The structured perturbation TabED-Str uses different perturbations depending on the state space structure: On unstructured data, the uniform perturbation with tuning hyper-parameter tcat is used, while on ordinally and cyclically structured data the ordinal perturbations and cyclical perturbations are used, respectively, with tuning parameter tbase.", "section": "E Naming Conventions and Parameters of Introduced Methods"}]