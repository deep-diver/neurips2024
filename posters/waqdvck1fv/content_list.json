[{"type": "text", "text": "Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tobias Schr\u00f6der \u2217\u2020 Imperial College London ", "page_idx": 0}, {"type": "text", "text": "Zijing Ou\u2217\u2021 Imperial College London ", "page_idx": 0}, {"type": "text", "text": "Yingzhen Li Imperial College London ", "page_idx": 0}, {"type": "text", "text": "Andrew Duncan Imperial College London The Alan Turing Institute ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Energy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains. However, training EBMs on data in discrete or mixed state spaces poses significant challenges due to the lack of robust and fast sampling methods. In this work, we propose to train discrete EBMs with Energy Discrepancy, a loss function which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for Markov chain Monte Carlo. We introduce perturbations of the data distribution by simulating a diffusion process on the discrete state space endowed with a graph structure. This allows us to inform the choice of perturbation from the structure of the modelled discrete variable, while the continuous time parameter enables fine-grained control of the perturbation. Empirically, we demonstrate the efficacy of the proposed approaches in a wide range of applications, including the estimation of discrete densities with non-binary vocabulary and binary image modelling. We also introduce the first application of EBMs to tabular data sets with applications in synthetic data generation and calibrated classification. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Discrete structures are intrinsic to most types of data such as text, graphs, and images. Estimating the data generating distribution $p_{\\mathrm{data}}$ of discrete data sets with a probabilistic model can contribute greatly to downstream inference and generation tasks, and plays a key role in synthetic data generation of tabular, textual or network data (Raghunathan, 2021). Energy-based models (EBMs) are probabilistic generative models of the form $p_{\\mathrm{ebm}}\\propto\\exp(-U)$ , where the flexible choice of the energy function $U$ allows great control in the modelling of different data structures. However, energy-based models are, by definition, unnormalised models and notoriously difficult to train due to the intractability of their normalisation, especially in discrete spaces. ", "page_idx": 0}, {"type": "text", "text": "Energy-based models are typically trained with the contrastive divergence (CD) algorithm (Hinton, 2002) which performs approximate maximum likelihood estimation by approximating the gradient of the log-likelihood with Markov Chain Monte Carlo (MCMC) techniques. This method motivated rich research results on sampling from discrete distributions to enable fast and accurate estimation of energy-based models (Zanella, 2020; Grathwohl et al., 2021; Zhang et al., 2022b; Sun et al., 2022b,a, 2023a). However, the training of energy-based models with CD remains challenging as it relies on sufficiently fast mixing of Markov chains. Since accurate sampling from the EBM typically cannot be achieved, contrastive divergence lacks theoretical guarantees (Carreira-Perpinan & Hinton, 2005) and leads to biased estimates of the energy landscape (Nijkamp et al., 2019). For mixed data types, energy-based models have only been applied to combinations of numerical features with a single categorical label (Grathwohl et al., 2019). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The recently introduced Energy Discrepancy (ED) (Schr\u00f6der et al., 2023) is a new type of contrastive loss functional that, by definition, depends on neither gradients nor MCMC methods. Instead, the definition of ED only requires the evaluation of the energy function on positive and contrasting, negative samples which are generated by perturbing the data distribution. However, the work in Schr\u00f6der et al. (2023) is currently limited to Gaussian perturbations on continuous spaces and does not explores strategies to choose perturbations on discrete spaces, especially when these discrete spaces exhibit some additional structure. ", "page_idx": 1}, {"type": "text", "text": "In this work, we propose a framework to train energy-based models with energy discrepancy on discrete data, making the following contributions: 1) We explore a method to define discrete diffusion processes on structured discrete spaces through a heat equation on the underlying graph and investigate the effect of geometry and time parameter on the diffusion. 2) Based on the discrete diffusion process, we extend energy discrepancy to discrete spaces in a systematic way, thus introducing a MCMC-free method for the training of energy-based models that requires little tuning. 3) We extend our methodology to mixed state spaces and establish the first robust training method of energy-based models on tabular data sets. We demonstrate promising performance on downstream tasks like synthetic data generation and calibrated prediction, thus unlocking a new tool for generative modelling on tabular data. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Energy-based models (EBMs) are a parametric family of distributions $p_{\\theta}$ defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{x})=\\frac{\\exp(-U_{\\theta}(\\mathbf{x}))}{Z_{\\theta}},\\quad Z_{\\theta}=\\sum_{\\mathbf{x}\\in\\mathcal{X}}\\exp(-U_{\\theta}(\\mathbf{x})),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $U_{\\theta}$ is the energy function parameterised by $\\theta$ and $Z_{\\theta}$ denotes the normalisation constant. Given a set of i.i.d. samples $\\{\\mathbf{x}^{i}\\}_{i=1}^{N^{\\dagger}}$ from an unknown data distribution $p_{\\mathrm{data}}(\\mathbf{x})$ we aim to learn an approximation $p_{\\theta}(\\mathbf{x})$ of $p_{\\mathrm{data}}(\\mathbf{x})$ . The de facto standard approach for finding such $\\theta$ is to minimise the negative log-likelihood of $p_{\\theta}$ under the data distribution via gradient decent ", "page_idx": 1}, {"type": "equation", "text": "$$\n-\\nabla_{\\theta}\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}[\\log p_{\\theta}(\\mathbf{x})]=\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathrm{data}}}[\\nabla_{\\theta}U_{\\theta}(\\mathbf{x})]-\\mathbb{E}_{\\mathbf{x}_{-}\\sim p_{\\theta}}[\\nabla_{\\theta}U_{\\theta}(\\mathbf{x}_{-})].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The intuition behind this update is to decrease the energy of positive data samples $\\mathbf{x}\\sim p_{\\mathrm{data}}(\\mathbf{x})$ and to increase the energy of negative samples $\\mathbf{x}_{-}\\sim p_{\\theta}(\\mathbf{x})$ . However, the exact computation of gradient in (2) is known to be NP-hard in general (Jerrum & Sinclair, 1993) and quickly becomes prohibitive even on relatively simple data sets. Consequently, existing approaches resort to sampling from the model $p_{\\theta}$ to approximate the gradient of log-likelihood via Monte Carlo estimation. In discrete settings, the most popular sampling methods include the locally informed sampler (Zanella, 2020), Gibbs with gradients (GwG) (Grathwohl et al., 2021), discrete Langevin (Zhang et al., 2022b), and generative flow networks (GFlowNet) (Zhang et al., 2022a). Despite their established success in discrete energy-based modelling, these methods necessitate a trade-off that hampers scalability: running the sampler for an extended duration rapidly increases the cost of maximum likelihood training, while shorter sampler runs yield inaccurate approximations of the likelihood gradient and introduce biases into the learned energy. ", "page_idx": 1}, {"type": "text", "text": "Energy Discrepancy (Schr\u00f6der et al., 2023) is a recently proposed method to train energy-based models without the need for an extensive sampling process. Instead, it constructs negative samples by perturbing the data, thus bypassing the sampling step while still yielding a valid training objective. To elucidate, the energy discrepancy is formally defined as follows: ", "page_idx": 1}, {"type": "text", "text": "Definition 1 (Energy Discrepancy). Let $p_{\\mathrm{data}}$ be a positive density on a measure space $({\\mathcal{X}},\\,\\mathrm{d}\\mathbf{x})^{4}a n d$ let $q(\\mathbf{y}|\\mathbf{x})$ be a conditional probability density. Define the contrastive potential induced by q as5 ", "page_idx": 1}, {"type": "equation", "text": "$$\nU_{q}(\\mathbf{y}):=-\\log\\sum_{\\mathbf{x^{\\prime}}\\in\\mathcal{X}}q(\\mathbf{y}|\\mathbf{x}^{\\prime})\\exp(-U(\\mathbf{x^{\\prime}}))\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The energy discrepancy between $p_{\\mathrm{data}}$ and $U$ induced by $q$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{ED}_{q}\\big(p_{\\mathrm{data}},U\\big):=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}[U(\\mathbf{x})]-\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\mathbb{E}_{q(\\mathbf{y}|\\mathbf{x})}[U_{q}(\\mathbf{y})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We will refer to $q$ as the perturbation. The validity of this loss functional was proven in Schr\u00f6der et al. (2023) in large generality: In particular, it is sufficient for $U^{*}=$ argmin $\\mathrm{ED}_{q}(p_{\\mathrm{data}},U)\\Leftrightarrow$ $\\exp(-U^{*})\\propto p_{\\mathrm{data}}$ that any two points $\\mathbf{x},\\mathbf{y}\\in{\\mathcal{X}}$ are $q$ -equivalent, i.e. there exists a chain of states $(\\mathbf{z}^{i})_{i=1}^{\\bar{T}}\\in\\dot{\\mathcal{X}}$ with $\\mathbf{z}^{1}=\\mathbf{x},\\bar{\\mathbf{z}}^{T}=\\bar{\\mathbf{y}}$ such that $q(\\mathbf{z}^{i+1}|\\mathbf{z}^{i})>0$ for all $i=1,\\ldots,T-1$ . ", "page_idx": 2}, {"type": "text", "text": "Energy discrepancy can also be understood from seeing it as a type of Kullback-Leibler divergence. Specifically, the loss function defined in (4) is equivalent to the expected Kullback-Leibler divergence ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\underset{U}{\\mathrm{argmin}}\\,\\mathrm{ED}_{q}(p_{\\mathrm{data}},U)\\Leftrightarrow\\underset{U}{\\mathrm{argmin}}\\,\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\mathbb{E}_{q(\\mathbf{y}|\\mathbf{x})}\\left[\\mathrm{KL}(p_{\\mathrm{data}}(\\cdot|\\mathbf{y}),p_{\\mathrm{ebm}}(\\cdot|\\mathbf{y}))\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{\\bullet}(\\mathbf{x}|\\mathbf{y})\\,\\propto\\,p_{\\bullet}(\\cdot)q(\\mathbf{y}|\\cdot)$ . This relates energy discrepancy to diffusion recovery likelihood objectives (Gao et al., 2020) and Kullback-Leibler contractions (Lyu, 2011). Energy discrepancy has demonstrated notable effectiveness in training EBMs in continuous spaces (Schr\u00f6der et al., 2023). In the next section, we show how the loss can be defined in discrete spaces. ", "page_idx": 2}, {"type": "text", "text": "3 Energy Discrepancies for Discrete Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For this work we will first consider a state space for the data distribution that can be written as the product of $d$ discrete variables with $S_{k}$ classes each, i.e. $\\mathcal{X}=\\otimes_{k=1}^{d}\\{1,\\ldots,S_{k}\\}$ . Examples for spaces of this type are the categorical entries of a data table for whi ch $d$ denotes the number of features, or binary image data sets for which we typically write $\\mathcal{X}=\\{0,1\\}^{d}$ . To define energy discrepancy on such spaces we need to specify a perturbation process under the following considerations: 1) The negative samples obtained through $q$ are informative for training the EBM when only finite amounts of data are available. 2) The contrastive potential $U_{q}(\\mathbf{y})$ has a numerically tractable approximation. ", "page_idx": 2}, {"type": "text", "text": "Let us consider one component $\\mathcal{X}=\\left\\{1,\\ldots,S\\right\\}$ , only. Inspired from previous works on diffusion modelling for discrete data (Campbell et al., 2022; Sun et al., 2023b; Lou et al., 2024; Campbell et al., 2024) we model the perturbation as a continuous time Markov chain (CTMC) with transition probability ", "page_idx": 2}, {"type": "equation", "text": "$$\nq_{t}(y=b|x=a)=\\exp{(t R)}_{b a}\\ ,\\quad a,b\\in\\{1,2,\\dots,S\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $R\\in\\mathbb{R}^{S\\times S}$ is the so-called rate matrix which satisfies $\\begin{array}{r}{R_{b b}=-\\sum_{a\\neq b}^{S}R_{b a}}\\end{array}$ and $\\exp(t R)$ is the matrix exponential. For a given rate matrix , this approach then leaves us with a single tunable hyperparameter $t$ characterising the magnitude of perturbation applied. We first analyse how the choice of rate matrix and time parameter affect the statistical properties of the energy discrepancy loss. In fact, under weak conditions, the energy discrepancy loss converges to maximum likelihood estimation for $t\\to\\infty$ , thus achieving the same loss function implemented by contrastive divergence: ", "page_idx": 2}, {"type": "text", "text": "Theorem 1. Let $q_{t}(\\cdot|x)$ be a Markov transition density defined by the rate matrix $R$ with eigenvalues $0\\,=\\,\\lambda_{1}(R)\\,\\geq\\,\\lambda_{2}(R)\\,\\geq\\,\\cdots\\,\\geq\\,\\lambda_{S}(R)$ and uniform stationary distribution. Then, there exists a constant $z_{t}$ independent of $\\theta$ such that energy-discrepancy converges to the maximum-likelihood loss ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathrm{ED}_{q_{t}}(p_{\\mathrm{data}},U_{\\theta})-\\mathcal{L}_{\\mathrm{MLE}}(\\theta)-z_{t}|\\leq\\sqrt{S}\\exp(-|\\lambda_{2}(R)|t)\\,\\mathrm{KL}(p_{\\mathrm{data}}\\parallel p_{\\theta})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with the loss of maximum-likelihood estimation $\\mathcal{L}_{\\mathrm{MLE}}(\\theta):=-\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\big[\\log p_{\\theta}(\\mathbf{x})\\big]$ . ", "page_idx": 2}, {"type": "text", "text": "Here, $z_{t}$ is a constant independent of $\\theta$ , so the optimisation landscapes of energy discrepancy estimation and maximum likelihood estimation in $\\theta$ align at an exponential rate, except for a shift by $z_{t}$ which does not affect the optimisation. This result improves the linear convergence rate in Schr\u00f6der et al. (2023) and relates it to the spectral gap $|\\lambda_{2}(R)|$ of the rate matrix. Such a result is meaningful as the maximum-likelihood estimator is generally statistically preferable with better sample efficiency, and Theorem 1 suggests that energy discrepancy estimation can approximate maximum likelihood estimation without resorting to MCMC like in classical EBM training methods. The proof is given in Appendix A.1. ", "page_idx": 2}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/6cfad07ae3b92c64b52d7c27bbb0d2e6e29b3dafe8073254b1a1df7e6b5c17d9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 1: Visualisation of a typical state space of a tabular dataset: Numerical entries taking values in $\\mathbb{R}^{d}$ , cyclical categorical entries (e.g. season), ordinal categorical entries (e.g. age), unstructured categorical entries, and variables with an absorbing state associated with masking the entry. ", "page_idx": 3}, {"type": "text", "text": "3.1 Heat Equation in Structured Discrete Spaces ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In principle, Theorem 1 establishes that energy discrepancy converges to the loss of maximum likelihood estimation in the limit $t\\to\\infty$ for any choice of rate matrix with spectral gap. In practice, however, large perturbations of data can produce high-variance parameter gradients and provide little training signal. Instead, it is desirable to construct perturbations that allow a fine-grained trade-off between the statistical properties of the loss function and the variance of the gradients. For this reason, we investigate the perturbation for small $t$ which, as we will see, can be informed by the assumed graph structure of the underlying discrete space. ", "page_idx": 3}, {"type": "text", "text": "The infinitesimal perturbations of the CTMC are characterised by the rate matrix. Notice that for small $t$ , the Euler discretisation of the heat equation yields ", "page_idx": 3}, {"type": "equation", "text": "$$\nq_{t}(y=b|x=a)\\approx\\delta(b,a)+t R_{b a}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\delta(b,a)=1\\Leftrightarrow a=b$ and zero otherwise. To model the relationship between values $a,b\\in$ $\\{1,\\ldots,S\\}$ we endow the space with a graph structure with adjacency matrix $A$ and out degree matrix $D_{\\mathrm{out}}$ and model the rate matrix as the graph Laplacian $R:\\doteq(A-D_{\\mathrm{out}})$ . By definition, the rows of the graph Laplacian matrix sum to zero $\\begin{array}{r}{\\sum_{a=1}^{S}R_{b a}=0}\\end{array}$ . The smallest possible perturbation is then characterised as the transition to an ad jacent neighbour. The characterisation of the CTMC in terms of the graph Laplacian is implicitly assumed in previous work. Campbell et al. (2022) describe a diffusion via a uniform perturbation which corresponds to a fully connected graph and Lou et al. (2024) describe the rate matrix associated to a star graph with absorbing (masking) state: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR^{\\mathrm{unif}}={\\bf11}^{T}/S-\\mathrm{id}\\,,\\qquad R_{b a}^{\\mathrm{mask}}=\\delta(M,a)-\\mathrm{id}_{b a}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For a visualisation, see Section 3.1. In addition to these fairly unstructured rate matrices we model state spaces with a cyclical or ordinal structure: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{R^{\\mathrm{cyc}}=\\left(\\begin{array}{c c c c c c}{-2}&{1}&{0}&{\\cdots}&{0}&{1}\\\\ {1}&{-2}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{-1}&{-2}&{1}&{0}&{\\vdots}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{\\ddots}&{\\ddots}&{\\ddots}\\\\ {0}&{0}&{1}&{-2}&{1}\\\\ {1}&{0}&{\\cdots}&{0}&{1}&{-2}\\end{array}\\right)}&{}&{R^{\\mathrm{ord}}=\\left(\\begin{array}{c c c c c c}{-1}&{1}&{0}&{\\cdots}&{0}&{0}\\\\ {1}&{-2}&{1}&{0}&{\\cdots}&{0}\\\\ {0}&{1}&{-2}&{1}&{0}&{\\vdots}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{\\ddots}&{\\ddots}&{\\ddots}\\\\ {0}&{0}&{1}&{-2}&{1}\\\\ {0}&{0}&{\\cdots}&{0}&{1}&{-1}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We restrict ourselves to uniform, cyclical, and ordinal structures as these structures can typically be trivially inferred from the type of data modelled. For example, periodically changing quantities (e.g. seasons) would display a cyclical structure and ordered information like age an ordinal structure. It is possible, however, to extend our framework to arbitrary graphical structures of the state space as long as eigenvalue decompositions of the graph Laplacian are feasible. ", "page_idx": 3}, {"type": "text", "text": "Since the Gaussian perturbation on Euclidean space used in Schr\u00f6der et al. (2023) is also the solution of a heat equation, these choices allow us to model the perturbation on a vector of mixed entries including numerical values, unstructured categorical values, and structured categorical values with a single differential equation ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\partial_{t}q_{t}(\\cdot|\\mathbf{x})=\\left(\\Delta^{\\mathrm{num}}\\otimes R^{\\mathrm{cyc}}\\otimes R^{\\mathrm{ord}}\\otimes R^{\\mathrm{unif}}\\otimes R^{\\mathrm{abs}}\\right)q_{t}(\\cdot|\\mathbf{x})\\,,\\quad q_{0}(\\cdot|\\mathbf{x})=\\delta(\\cdot,\\mathbf{x})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Delta^{\\mathrm{num}}$ denotes the standard Laplace operator $\\textstyle\\sum_{k=1}^{d}\\partial^{2}/\\partial_{\\mathbf{x}^{\\mathrm{num}}}^{2}$ and the product $\\otimes$ denotes that each operator acts on the corresponding component  of the state space. ", "page_idx": 3}, {"type": "text", "text": "4 Estimating the Energy Discrepancy Loss ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now discuss how discrete energy discrepancy can be estimated. We will typically assume that each dimension of the data point is perturbed independently, i.e. the perturbation $q(\\mathbf{y}|\\mathbf{x})$ is modelled as the product of component-wise perturbations. On Euclidean data, we resort to the implementation in Schr\u00f6der et al. (2023) and obtain perturbed samples by adding isotropic Gaussian noise to the samples. We are now left with the heat equation on discrete space. ", "page_idx": 4}, {"type": "text", "text": "4.1 Solving the Heat Equation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the case of the uniform Laplacian $R_{\\mathrm{unif}}=11^{T}/S-\\mathrm{id}$ , the heat equation has the closed form solution ", "page_idx": 4}, {"type": "equation", "text": "$$\nq_{t}(y|x=a)=e^{-t}\\delta(y,a)+\\frac{1-e^{-t}}{S}\\sum_{k=1}^{S}\\delta(y,k)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Practically speaking, this perturbation remains in its state with probability $e^{-t}$ and samples uniformly from the state space otherwise. The case of the cyclical and ordinal structure is more delicate. We first note that the heat equation can be solved in terms of its eigenvalue expansion $\\exp(R t)=$ $\\mathbf{V}\\exp(\\Lambda t)\\mathbf{V}^{*}$ , where $\\Lambda$ is the matrix with the eigenvalues $\\lambda_{p}$ along its diagonal and $\\mathbf{V}$ is a matrix of orthogonal eigenvectors with each column containing the corresponding eigenvector $\\mathbf{v}_{p}$ . The perturbation for $R^{\\mathrm{cyc}}$ and $R^{\\mathrm{ord}}$ can then be computed by means of a discrete Fourier transform: ", "page_idx": 4}, {"type": "text", "text": "Proposition 1. Assume the density $q_{t}(b|a):=q_{t}(y=b|x=a)$ is defined by the rate matrices $R^{\\mathrm{cyc}}$ or $R^{\\mathrm{ord}}$ . The transition density for all $a,b\\in\\{1,2,\\dots,S\\}$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t}^{\\mathrm{{cyc}}}(b|a)=\\displaystyle\\frac{1}{S}\\sum_{p=1}^{S}\\exp(2\\pi\\mathrm{i}b\\omega_{p}^{\\mathrm{cyc}})\\,\\exp\\big((2\\cos(2\\pi\\omega_{p}^{\\mathrm{{cyc}}})-2)t\\big)\\,\\exp(-2\\pi\\mathrm{i}a\\omega_{p}^{\\mathrm{cyc}})}\\\\ &{q_{t}^{\\mathrm{{ord}}}(b|a)=\\displaystyle\\frac{2}{S}\\sum_{p=1}^{S}\\frac{1}{z_{p}}\\,\\cos((2b-1)\\pi\\omega_{p}^{\\mathrm{ord}})\\,\\exp\\big((2\\cos(2\\pi\\omega_{p}^{\\mathrm{ord}})-2)t\\big)\\,\\cos((2a-1)\\pi\\omega_{p}^{\\mathrm{ord}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For the derivation, see Appendix A.2. Due to this result, the heat equation can be efficiently solved in parallel without requiring any sequential operations like multiple Euler steps. In addition, the transition matrices can be computed and saved in advance, thus reducing the computational complexity to the matrix multiplication with a batch of one-hot encoded data points. ", "page_idx": 4}, {"type": "text", "text": "Gaussian limit and choice of time parameter. For tabular data sets the cardinality $S$ changes between different dimensions which raises the question how $t$ should be scaled with $S$ . To answer this question we observe the following scaling limit of the perturbation: ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Scaling limit). Let $y_{t}\\sim q_{t}(\\cdot|x=\\mu S)$ with $\\mu\\in\\{1/S,2/S,\\dots,1\\}$ where $q_{t}$ is either the transition density of the cyclical or ordinal perturbation. Let $\\varphi:\\mathbb{R}\\to(0,1]$ , where for all $n\\in\\mathbb{Z}$ and $x\\in(0,1]\\;\\varphi^{\\mathrm{cyc}}(n+x)=x$ and $\\varphi^{\\mathrm{ord}}(2n+x)=x$ , $\\varphi^{\\mathrm{ord}}(2n+1+x)=-x$ . Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{S^{2}t}/S\\xrightarrow{S\\rightarrow\\infty}\\varphi(\\xi_{t})\\quad w i t h\\quad\\xi_{t}\\sim\\mathcal{N}(\\mu,2t)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Consequently, under the rescaling of time and space prescribed, the perturbation behaves independently of the state space size like a Gaussian with variance $2t$ that is reflected or periodically continued at the boundary states of $(0,1]$ . The phenomenon is visualised in Figure 5. Based on this scaling limit we typically choose a quadratic rule $t=S^{2}t_{\\mathrm{base}}$ . Alternatively, we may choose a linear rule $t=S t_{\\mathrm{base}}$ in which case the limit becomes a regular Gaussian on $\\mathbb{R}_{+}$ , thus recovering the Euclidean case from Schr\u00f6der et al. (2023). The theorem is proven in Appendix A.3. ", "page_idx": 4}, {"type": "text", "text": "As a byproduct of this result we can also approximate the perturbation with discretised rescaled samples from a standard normal distribution and applying either periodic or reflecting mappings on perturbed states outside the domain. This may be computationally favourable for spaces of the form $\\mathbf{\\dot{\\{1}}},\\dots,S\\}^{d}$ where the vocabulary size $S$ and dimension of the state space $d$ grow very large. ", "page_idx": 4}, {"type": "text", "text": "Localisation to random grid. For unstructured categorical variables the uniform perturbation may introduce too much noise to inform the EBM about the correlations in the data set. In these cases, it can be beneficial to sample a random dimension $k\\in\\{1,\\ldots,d\\}$ and apply a larger perturbation in this dimension, only. This effectively means to replace the product of categorical distributions with a mixture perturbation ", "page_idx": 5}, {"type": "equation", "text": "$$\nq_{t}(\\mathbf{y}|\\mathbf{x})=\\prod_{k=1}^{d}q_{t}(y_{k}|x_{k})\\to\\frac{1}{d}\\sum_{k=1}^{d}q_{t}(y_{k}|x_{k})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In our experiments we only consider the case of perturbing the randomly chosen dimension uniformly.   \nWe call this grid perturbation due to connections with concrete score matching (Meng et al., 2022).   \nThe resulting loss can be understood as a variation of pseudo-likelihood estimation. ", "page_idx": 5}, {"type": "text", "text": "Special case of binary state space. In the special case of $\\mathcal{X}=\\{0,1\\}^{d}$ , the structures of the cyclical, ordinal, and uniform graph coincide, and the perturbation $q_{t}(\\mathbf{y}|\\mathbf{x})$ becomes the product of identical Bernoulli distributions with parameter $\\varepsilon=0.5(1-\\mathrm{e}^{-t})$ . We also explore the grid perturbation which assumes that a dimension is selected at random and the entry is filpped deterministically from zero to one or one to zero. For details, see Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "4.2 Estimation of the Contrastive Potential ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The final challenge in turning energy discrepancy into a practical loss function lies in the estimation of the contrastive potential $U_{q}$ . We use the fact that for a symmetric rate matrix $R$ , the induced perturbation is symmetric as well, i.e. $q_{t}(\\mathbf{y}|\\mathbf{x})=q_{t}(\\mathbf{x}|\\mathbf{y})$ . Thus, we first write the contrastive potential as an expectation $\\begin{array}{r}{U_{q}(\\mathbf{y})\\,=\\,-\\log\\sum_{\\mathbf{x}\\in\\mathcal{X}}\\exp(-U(\\mathbf{x}))q(\\mathbf{y}|\\mathbf{x})\\,=\\,-\\log\\mathbb{E}_{q(\\mathbf{x}|\\mathbf{y})}\\left[\\exp(-U(\\mathbf{x}))\\right]}\\end{array}$ and subsequently approximate the e nergy discrepancy loss as in Schr\u00f6der et al. (2023) as $\\begin{array}{r}{\\mathcal{L}_{q,M,w}(U)\\;:=\\;\\frac{1}{N}\\sum_{i=1}^{N}\\log\\Big(w+\\sum_{j=1}^{M}\\exp(U(\\mathbf{x}^{i})-U(\\mathbf{x}_{-}^{i,j})\\Big)}\\end{array}$ with $\\mathbf{x}^{i}\\sim p_{\\mathrm{data}}$ , $\\mathbf{y}^{i}\\sim q_{t}(\\cdot|\\mathbf{x}^{i})$ , and $\\mathbf{x}_{-}^{i,j}\\sim q_{t}(\\cdot|\\mathbf{y}^{i})$ . The details of the training procedure are provided in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Contrastive loss functions. Our work is based on energy discrepancies first introduced in (Schr\u00f6der et al., 2023). Energy discrepancies are equivalent to certain types of KL contraction divergences whose theory was studied in Lyu (2011), however, without proposing a training algorithm for EBM\u2019s. On Euclidean data, ED is related to diffusion recovery-likelihood (Gao et al., 2020) which uses a CD-type training algorithm. For a masking perturbation, ED estimation can be understood as a Monte-Carlo approximation of pseudo-likelihood (Besag, 1975). Furthermore, the structure of the stabilised energy discrepancy loss shares similarities with other contrastive losses such as Ceylan & Gutmann (2018); Gutmann & Hyv\u00e4rinen (2010); van den Oord et al. (2018); Foster et al. (2020) due to their close connection to the Kullback-Leibler divergence. ", "page_idx": 5}, {"type": "text", "text": "Discrete diffusion models. We extend the continuous time Markov chain framework introduced and developed in Campbell et al. (2022, 2024); Lou et al. (2024) and provides a geometric interpretation thereof. Similar to us, Kotelnikov et al. (2023) defines a flow on mixed state spaces as the product of a Gaussian and a categorical flow, utilising multinomial flows (Hoogeboom et al., 2021). Our work has connections to concrete score matching (Meng et al., 2022) through the usage of neighbourhood structures to define a replacement of the continuous score function. ", "page_idx": 5}, {"type": "text", "text": "Contrastive divergence and sampling. Contrastive divergence (CD) is commonly utilised for training energy-based models in continuous spaces with Langevin dynamics (Xie et al., 2016, 2018, 2022; Du et al., 2020; Xiao et al., 2020). In discrete spaces, EBM training heavily relies on CD methods as well, which is a major driver for the development of discrete sampling strategies. The standard Gibbs method was improved by Zanella (2020) through locally informed proposals. This method was extended to include gradient information (Grathwohl et al., 2021) to drastically reduce the computational complexity of filpping bits in several places (Sun et al., 2022b; Emami et al., 2023; Sun et al., 2022a). Moreover, a discrete version of Langevin sampling was introduced based on this idea (Zhang et al., 2022b; Rhodes & Gutmann, 2022; Sun et al., 2023a). Consequently, most current implementations of contrastive divergence use multiple steps of a gradient-based discrete sampler. Alternatively, EBMs can be trained using generative flow networks which learn a Markov chain that construct data optimising the energy as reward function (Zhang et al., 2022a). ", "page_idx": 5}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/3670e53722cdb50521c8766d0acabd5f9218c029e1a1169b770948ff8242b8e1.jpg", "img_caption": ["Figure 2: Comparison of energy discrepancy and contrastive divergence on the dataset with 16 dimensions and 5 states. Rows 1 and 2 show the estimated density and synthesised samples, respectively. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Other training methods of EBMs for discrete and mixed data. A sampling-free approach for training binary discrete EBMs is ratio matching (Hyv\u00e4rinen, 2007; Lyu, 2012; Liu et al., 2023). Dai et al. (2020) proposed to apply variational approaches to train discrete EBMs instead of MCMC. Eikema et al. (2022) replaced the widely-used Gibbs algorithms with quasi-rejection sampling to trade off the efficiency and accuracy of the sampling procedure. The perturb-and-map (Papandreou & Yuille, 2011) is also recently utilised to sample and learn in discrete EBMs (Lazaro-Gredilla et al., 2021). Tran et al. (2011) introduces mixed-variate restricted Boltzmann machines for energy-based modelling on mixed state spaces. Deep architectures, on the other hand, have been mostly limited to a single categorical target variable which is modelled via a classifier (Grathwohl et al., 2019) ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To evaluate our proposed approach, we conduct experiments across diverse scenarios: i) estimating probability distributions on discrete data; ii) handling mixed-state features in tabular data; and iii) modelling binary images. We also explore Ising model training and graph generation in binary spaces, but leave the detailed evaluation of these in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "6.1 Discrete Density Estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first demonstrate the effectiveness of energy discrepancy on density estimation using synthetic discrete data. Following Dai et al. (2020), we initially generate 2D floating-point data from several two-dimensional distributions. Each dimension of the data is then converted into a 16-bit Gray code, resulting in a dataset with 32 dimensions and 2 states. To construct datasets beyond binary cases, we follow Zhang et al. (2024) and transform each dimension into 8-bit 5-base code and 6-bit decimal code. This process creates two additional datasets: one with 16 dimensions and 5 states, and another with 12 dimensions and 10 states. The experimental details are given in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "Figure 2 illustrates the estimated energies as well as samples that are synthesised with Gibbs sampling for energy discrepancy (ED) and contrastive divergence (CD) on the dataset with 16 dimensions and 5 states. It can be seen that ED excels at capturing the multi-modal nature of the distribution, consistently learning sharper energy landscape in the data support compared to CD. This coincides with the previous observations in continuous spaces (Schr\u00f6der et al., 2023), suggesting ED\u2019s advantage in handling complex data structures. For more results of additional datasets with 5 and 10 states, we deferred them to Figures 7 and 8, respectively. ", "page_idx": 6}, {"type": "text", "text": "For binary cases with 2 states, we compare our approaches to three baselines: PCD (Tieleman, 2008), $\\mathrm{{ALOE+}}$ (Dai et al., 2020), and EB-GFN (Zhang et al., 2022a). In Tables 3 and 4, we quantitatively evaluate different methods by evaluating the negative log-likelihood (NLL) and the exponential Hamming MMD (Gretton et al., 2012), respectively. We observe that energy discrepancy outperforms the baseline methods in most settings, but without relying on MCMC simulations (as in PCD) or the training of additional variational networks (as in ALOE and EB-GFN). This performance gain is likely explained by the good theoretical guarantees of energy discrepancy for well-posed estimation tasks. In contrast, the baselines introduce biases due to their reliance on variational proposals and short-run MCMC sampling that may not have converged. ", "page_idx": 6}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/ab8b4b7ac1a6589ef90c02a4b1b76bf961d8380772ef26c1e15e8cf7d364e930.jpg", "table_caption": ["Table 1: Results on real-world datasets. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.2 Tabular Data Synthesising ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this experiment, we assess our methods on synthesising tabular data, which presents a challenge due to its mix of numerical and categorical features, making it more difficult to model compared to conventional data formats. To demonstrate the efficacy of energy discrepancies, we first conduct experiments on synthetic examples before proceeding to real-world tabular data. Additional details regarding the experimental setup are deferred to Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "Synthetic Dataset. We first showcase the effectiveness of our methods on mixed data types by learning EBMs on a synthetic ring dataset. The dataset consists of four columns, with the first two columns indicating numerical coordinates of data points. The third column categorizes data points into four circles whereas the last column specifies the 16 colours each data point could be classified into. Therefore, each row in the tabular contains 2 numerical features and 2 categorical features. ", "page_idx": 7}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/5a9e95c6bd25834493d8a879d91035c793c4d90c47557f1c02f24be8ba098939.jpg", "img_caption": ["Figure 3: Comparison of the energy discrepancy and contrastive divergence on the synthetic tabular datasets. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "To train an EBM on a dataset comprising mixed types of data, we employ either contrastive divergence or energy discrepancy. For CD, we adopt a strategy involving a replay buffer in conjunction with a short-run MCMC using 20 steps. Specifically, we utilise Langevin dynamics and Gibbs sampling for numerical and categorical features, respectively. In the case of ED, we perturb the numerical features with a Gaussian perturbation and the categorical features with grid perturbation. Figure 3 illustrates the results of synthesised samples generated from the learned energy using Gibbs sampling. These findings align with those depicted in Figure 2, where CD struggles to capture a faithful energy landscape, leading to synthesized samples potentially lying outside the data distribution support. Instead, by leveraging a combination of perturbation techniques tailored to the data types present, ED offers a more robust and reliable framework for training EBMs in mixed state spaces. ", "page_idx": 7}, {"type": "text", "text": "Real-world Dataset. We then evaluate our methods by benchmarking them against various baselines across 6 real-world datasets. Following Xu et al. (2019), we first split the real datasets into training and testing sets. The generative models are then learned on the real training set, from which synthetic samples of equal size are generated. This synthetic dataset is subsequently used to train a classification/regression XGBoost model, which is evaluated using the real test set. ", "page_idx": 7}, {"type": "text", "text": "We compare the performance, as measured by the AUC score for classification tasks and RMSE for regression tasks, against CTGAN, TVAE, (Xu et al., 2019) and TabDDPM (Kotelnikov et al., 2023) baselines which utilise generative adversarial networks, variational autoencoders, and denoising diffusion probabilistic models, respectively. The results are reported in Table 1. Here, TabED-Str refers to an ED loss for which the perturbation was chosen with prior knowledge about the structure of the modelled feature, i.e. ordinal and cyclical features were hand-picked. We do not report results for TabED- $S\\mathrm{tr}$ on the Cardio, Churn, and Mushroom datasets, since the state spaces only consist ", "page_idx": 7}, {"type": "text", "text": "Table 2: Experimental results for discrete image modelling. We report the negative log-likelihood (NLL) on the test set for different models. The results of Gibbs, GWG, and DULA are taken from Zhang et al. (2022b), and the result of EB-GFN is from Zhang et al. (2022a). ", "page_idx": 8}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/d02eda3117345f09bf556ce2265895145a0823bbccbf8195cc70b02da719d472.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "of unstructured features. To compute the average ranking we use the rank of TabED-Uni on these datasets since on unstructured features TabED-Uni and TabED-Str coincide. ", "page_idx": 8}, {"type": "text", "text": "The variants of ED show promising results on diverse datasets, thus demonstrating the suitability of ED for EBM training on mixed-state spaces. While TabDDPM outperforms the other approaches, TabED shows comparable performance to the CTGAN and TVAE baselines and outperforms both in average ranking. Furthermore, the contrastive divergence approach performs poorly which highlights its limitations in accurately modelling distributions on mixed state spaces. Surprisingly, the unstructured perturbation TabED-Uni performs slightly better than the structured approaches. This may partially be attributed to the fact that the state spaces of the discrete features are relatively small. Consequently, the uniform perturbation might be a good approximation of maximum likelihood estimation in agreement with Theorem 1, while not producing high-variance gradients on these specific datasets. ", "page_idx": 8}, {"type": "text", "text": "Improving Calibration. Despite the improving accuracy of neuralnetwork-based classifiers in recent years, they are also becoming increasingly recognised for their tendency to exhibit poor calibration due to overconfident outputs (Guo et al., 2017; Mukhoti et al., 2020). Since energybased model on mixed state spaces can capture the likelihood of tuples of features and target labels, they implicitely quantify the confidence in a ", "page_idx": 8}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/f645bca23157bcc541e61fa962ad77fbf931a6daf70480964622751331afb7bc.jpg", "img_caption": ["Figure 4: Calibration results comparison between the baseline (left) and energy discrepancy (right) on the adult dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "prediction and can be adapted into classifiers with better calibration than deterministic methods. This opens up a new avenue for applying EBMs in deterministic tabular data modelling methods. ", "page_idx": 8}, {"type": "text", "text": "Let $y$ and $\\mathbf{x}$ be the target label and the rest features in the tabular data, an EBM $U_{\\theta}(\\mathbf{x},y)$ learned on the joint probability $p_{\\mathrm{data}}(\\mathbf{x},y)$ can be transformed into a deterministic classifier: $p_{\\mathrm{EBM}}(y|\\mathbf{x})\\propto$ $\\exp(-U_{\\theta}(\\mathbf{x},y))$ . As a baseline for comparison, we additionally train a classifier $p_{\\mathrm{CLF}}(y|\\mathbf{x})$ with the same architecture by maximising the conditional likelihood: $\\mathbb{E}_{p_{\\mathrm{data}}}[\\log p_{\\mathrm{CLF}}(y|\\mathbf{x})]$ . Results on the adult dataset can be seen in Figure 4. We find that the EBM and the baseline exhibit comparable accuracy. However, the baseline model is less calibrated, generating over-confident predictions. In contrast, the EBM learned through ED achieves better calibration, as evidenced by lower expected calibration error (Guo et al., 2017). Further details and results are provided in Appendix D.2. ", "page_idx": 8}, {"type": "text", "text": "6.3 Discrete Image Modelling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this experiment, we evaluate our methods on high-dimensional binary spaces. Following the settings in Grathwohl et al. (2021), we conduct experiments on various image datasets and compare against contrastive divergence using various sampling methods, namely vanilla Gibbs sampling, Gibbs-With-Gradient (Grathwohl et al., 2021, GWG), Generative-Flow-Network (Zhang et al., 2022a, GFN), and Discrete-Unadjusted-Langevin-Algorithm (Zhang et al., 2022b, DULA). The training details are provided in Appendix D.3. After training, annealed importance sampling (Neal, 2001) is employed to estimate the negative log-likelihood (NLL). ", "page_idx": 8}, {"type": "text", "text": "Table 2 displays the NLLs on the test dataset. It is evident that energy discrepancy achieves comparable performance to the baseline methods on the Omniglot dataset. Despite the performance gap compared to the contrastive divergence methods on the MNIST dataset, energy discrepancy stands out for its efficiency, requiring only $M$ evaluations of the energy function in parallel (see ", "page_idx": 8}, {"type": "text", "text": "Table 7 for the comparison of running time complexity). This represents a significant computational reduction compared to contrastive divergence, which lacks the advantage of parallelisation and involves simulating multiple MCMC steps. Additionally, our methods show superiority over CD-1 by a substantial margin, as demonstrated in Table 8, affirming the effectiveness of our approach. For further insights, we provide visualisations of the generated samples in Figure 10. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we extend the training of energy-based models with energy discrepancy to discrete and mixed state spaces in a systematic way. We show that the energy-based model can be learned jointly on continuous and discrete variables and how prior assumptions about the geometry of the underlying discrete space can be utilised in the construction of the loss. Our method achieves promising results on a wide range of discrete modelling applications at a significantly lower computational cost than MCMC-based approaches. To the best of our knowledge, our approach is also the first working training method for energy-based models on tabular data sets, unlocking a wide range of inference applications for tabular data sets beyond the scope of classical joint energy-based models. ", "page_idx": 9}, {"type": "text", "text": "Limitations: Similar to prior work on energy discrepancy in continuous spaces (Schr\u00f6der et al., 2023), our training method is sensitive to the assumption that the data distribution is positive on the whole state space. While our method scales to high-dimensional datasets like binary image data, where the positiveness of the data distribution is assumed to be violated due to the manifold hypothesis, the large difference between intrinsic and ambient dimensionality poses challenges to our approach and may explain why energy discrepancy cannot match the performance of contrastive divergence with a large number of MCMC steps on binary image data. ", "page_idx": 9}, {"type": "text", "text": "Broader Impact: In principle, our method can be used for imputation and prediction in tabular data sets and can thus have discriminating or excluding effects if used irresponsibly. ", "page_idx": 9}, {"type": "text", "text": "Outlook: For future work, we are interested in extensions to highly structured types of data such as molecules, text, or data arising from networks. So far, our work only considers cyclical and ordinal structures on the discrete space, while incorporating more complex structures as prior information into the rate function may be beneficial. Furthermore, interesting downstream applications ranging from table imputation with confidence bounds, simulation-based inference involving discrete variables, or reweighting of language models with residual EBMs have been left unexplored in this work. ", "page_idx": 9}, {"type": "text", "text": "Author Contributions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "TS and ZO conceived the project idea to use an ED loss for the training of EBMs on discrete and mixed data. TS devised the main conceptual ideas, developed the theory, conducted the proofs and implemented the ED loss. ZO contributed to the conceptual ideas and designed and carried out the experiments. ABD supervised the conceptualisation and execution of the research project and contributed proof ideas. ZO, YL, and ABD checked derivations and proofs. TS and ZO equally contributed to the writing under the supervision of YL and ABD. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "TS was supported by the EPSRC-DTP scholarship partially funded by the Department of Mathematics, Imperial College London. ZO was supported by the Lee Family Scholarship. We thank the anonymous reviewer for their comments. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Anderson, W. J. Continuous-time Markov chains: An applications-oriented approach. Springer Science & Business Media, 2012.   \nBesag, J. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society. Series D (The Statistician), 24(3):179\u2013195, 1975.   \nCampbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:28266\u201328279, 2022.   \nCampbell, A., Yim, J., Barzilay, R., Rainforth, T., and Jaakkola, T. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. International Conference on Machine Learning, 41, 2024.   \nCarreira-Perpinan, M. A. and Hinton, G. On contrastive divergence learning. In International workshop on artificial intelligence and statistics, pp. 33\u201340. PMLR, 2005.   \nCeylan, C. and Gutmann, M. U. Conditional noise-contrastive estimation of unnormalised models. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 726\u2013734. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/ceylan18a.html.   \nDai, H., Singh, R., Dai, B., Sutton, C., and Schuurmans, D. Learning discrete energy-based models via auxiliary-variable local exploration. Advances in Neural Information Processing Systems, 33: 10443\u201310455, 2020.   \nDiaconis, P. and Stroock, D. Geometric bounds for eigenvalues of markov chains. The annals of applied probability, pp. 36\u201361, 1991.   \nDu, Y., Li, S., Tenenbaum, J., and Mordatch, I. Improved contrastive divergence training of energy based models. arXiv preprint arXiv:2012.01316, 2020.   \nEikema, B., Kruszewski, G., Dance, C. R., Elsahar, H., and Dymetman, M. An approximate sampler for energy-based models with divergence diagnostics. Transactions of Machine Learning Research, 2022.   \nEmami, P., Perreault, A., Law, J., Biagioni, D., and John, P. S. Plug & play directed evolution of proteins with gradient-based discrete MCMC. Machine Learning: Science and Technology, 4(2): 025014, 2023.   \nFoster, A., Jankowiak, M., O\u2019Meara, M., Teh, Y. W., and Rainforth, T. A unified stochastic gradient approach to designing bayesian-optimal experiments. In International Conference on Artificial Intelligence and Statistics, pp. 2959\u20132969. PMLR, 2020.   \nGao, R., Song, Y., Poole, B., Wu, Y. N., and Kingma, D. P. Learning energy-based models by diffusion recovery likelihood. arXiv preprint arXiv:2012.08125, 2020.   \nGrathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., Norouzi, M., and Swersky, K. Your classifier is secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019.   \nGrathwohl, W., Swersky, K., Hashemi, M., Duvenaud, D., and Maddison, C. J. Oops I took a gradient: Scalable sampling for discrete distributions. arXiv preprint arXiv:2102.04509, 2021.   \nGray, F. Pulse code communication. United States Patent Number 2632058, 1953.   \nGretton, A., Borgwardt, K. M., Rasch, M. J., Sch\u00f6lkopf, B., and Smola, A. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723\u2013773, 2012.   \nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In International conference on machine learning, pp. 1321\u20131330. PMLR, 2017.   \nGutmann, M. and Hyv\u00e4rinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 297\u2013304. JMLR Workshop and Conference Proceedings, 2010.   \nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.   \nHinton, G. E. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771\u20131800, 2002.   \nHoogeboom, E., Nielsen, D., Jaini, P., Forr\u00e9, P., and Welling, M. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454\u201312465, 2021.   \nHyv\u00e4rinen, A. Some extensions of score matching. Computational statistics & data analysis, 51(5): 2499\u20132512, 2007.   \nJerrum, M. and Sinclair, A. Polynomial-time approximation algorithms for the ising model. SIAM Journal on computing, 22(5):1087\u20131116, 1993.   \nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \nKipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.   \nKotelnikov, A., Baranchuk, D., Rubachev, I., and Babenko, A. Tabddpm: Modelling tabular data with diffusion models. In International Conference on Machine Learning, pp. 17564\u201317579. PMLR, 2023.   \nLazaro-Gredilla, M., Dedieu, A., and George, D. Perturb-and-max-product: Sampling and learning in discrete energy-based models. Advances in Neural Information Processing Systems, 34:928\u2013940, 2021.   \nLi, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia, P. Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324, 2018.   \nLiu, J., Kumar, A., Ba, J., Kiros, J., and Swersky, K. Graph normalizing flows. Advances in Neural Information Processing Systems, 32, 2019.   \nLiu, M., Liu, H., and Ji, S. Gradient-guided importance sampling for learning binary energy-based models. 2023.   \nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.   \nLou, A., Meng, C., and Ermon, S. Discrete diffusion language modeling by estimating the ratios of the data distribution. International Conference on Machine Learning, 41, 2024.   \nLuo, Y., Yan, K., and Ji, S. Graphdf: A discrete flow model for molecular graph generation. In International Conference on Machine Learning, pp. 7192\u20137203. PMLR, 2021.   \nLyu, S. Unifying non-maximum likelihood learning objectives with minimum kl contraction. In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger, K. (eds.), Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. URL https://proceedings.neurips.cc/paper/2011/file/ a3f390d88e4c41f2747bfa2f1b5f87db-Paper.pdf.   \nLyu, S. Interpretation and generalization of score matching. arXiv preprint arXiv:1205.2629, 2012.   \nMeng, C., Choi, K., Song, J., and Ermon, S. Concrete score matching: Generalized score matching for discrete data. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 34532\u201334545. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/df04a35d907e894d59d4eab1f92bc87b-Paper-Conference.pdf. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P., and Dokania, P. Calibrating deep neural networks using focal loss. Advances in Neural Information Processing Systems, 33:15288\u201315299, 2020. ", "page_idx": 12}, {"type": "text", "text": "Neal, R. M. Annealed importance sampling. Statistics and computing, 11:125\u2013139, 2001. ", "page_idx": 12}, {"type": "text", "text": "Nijkamp, E., Hill, M., Zhu, S.-C., and Wu, Y. N. Learning non-convergent non-persistent short-run MCMC toward energy-based model. arXiv preprint arXiv:1904.09770, 2019.   \nNiu, C., Song, Y., Song, J., Zhao, S., Grover, A., and Ermon, S. Permutation invariant graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and Statistics, pp. 4474\u20134484. PMLR, 2020.   \nPapandreou, G. and Yuille, A. L. Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. In 2011 International Conference on Computer Vision, pp. 193\u2013200. IEEE, 2011.   \nRaghunathan, T. E. Synthetic data. Annual review of statistics and its application, 8:129\u2013140, 2021.   \nRaginsky, M. Strong data processing inequalities and $\\phi$ -sobolev inequalities for discrete channels. IEEE Transactions on Information Theory, 62(6):3355\u20133389, 2016.   \nRamachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017.   \nRhodes, B. and Gutmann, M. Enhanced gradient-based MCMC in discrete spaces. arXiv preprint arXiv:2208.00040, 2022.   \nSchr\u00f6der, T., Ou, Z., Lim, J., Li, Y., Vollmer, S., and Duncan, A. Energy discrepancies: a scoreindependent loss for energy-based models. Advances in Neural Information Processing Systems, 36, 2023.   \nSen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.   \nShi, C., Xu, M., Zhu, Z., Zhang, W., Zhang, M., and Tang, J. Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382, 2020.   \nSimonovsky, M. and Komodakis, N. Graphvae: Towards generation of small graphs using variational autoencoders. In Artificial Neural Networks and Machine Learning\u2013ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I 27, pp. 412\u2013422. Springer, 2018.   \nSun, H., Dai, H., and Schuurmans, D. Optimal scaling for locally balanced proposals in discrete spaces. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 23867\u201323880. Curran Associates, Inc., 2022a. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 96c6f409a374b5c81d2efa4bc5526f27-Paper-Conference.pdf.   \nSun, H., Dai, H., Xia, W., and Ramamurthy, A. Path auxiliary proposal for MCMC in discrete space. In International Conference on Learning Representations, 2022b.   \nSun, H., Dai, H., Dai, B., Zhou, H., and Schuurmans, D. Discrete Langevin samplers via Wasserstein gradient flow. In International Conference on Artificial Intelligence and Statistics, pp. 6290\u20136313. PMLR, 2023a.   \nSun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H. Score-based continuous-time discrete diffusion models. In The Eleventh International Conference on Learning Representations, 2023b.   \nTee, G. J. Eigenvectors of block circulant and alternating circulant matrices. New Zealand Journal of Mathematics, 36(8):195\u2013211, 2007.   \nTieleman, T. Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings of the 25th international conference on Machine learning, pp. 1064\u20131071, 2008.   \nTran, T., Phung, D., and Venkatesh, S. Mixed-variate restricted boltzmann machines. In Asian conference on machine learning, pp. 213\u2013229. PMLR, 2011.   \nvan den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748.   \nXiao, Z., Kreis, K., Kautz, J., and Vahdat, A. Vaebm: A symbiosis between variational autoencoders and energy-based models. arXiv preprint arXiv:2010.00654, 2020.   \nXie, J., Lu, Y., Zhu, S.-C., and Wu, Y. A theory of generative convnet. In International Conference on Machine Learning, pp. 2635\u20132644. PMLR, 2016.   \nXie, J., Lu, Y., Gao, R., and Wu, Y. N. Cooperative learning of energy-based model and latent variable model via MCMC teaching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \nXie, J., Zhu, Y., Li, J., and Li, P. A tale of two flows: Cooperative learning of langevin flow and normalizing flow toward energy-based model. arXiv preprint arXiv:2205.06924, 2022.   \nXu, L., Skoularidou, M., Cuesta-Infante, A., and Veeramachaneni, K. Modeling tabular data using conditional gan. Advances in neural information processing systems, 32, 2019.   \nYou, J., Ying, R., Ren, X., Hamilton, W., and Leskovec, J. Graphrnn: Generating realistic graphs with deep auto-regressive models. In International conference on machine learning, pp. 5708\u20135717. PMLR, 2018.   \nYueh, W.-C. Eigenvalues of several tridiagonal matrices. Applied Mathematics E-Notes [electronic only], 5:66\u201374, 2005.   \nZanella, G. Informed proposals for local MCMC in discrete spaces. Journal of the American Statistical Association, 115(530):852\u2013865, 2020.   \nZhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., and Bengio, Y. Generative flow networks for discrete probabilistic modeling. arXiv preprint arXiv:2202.01361, 2022a.   \nZhang, P., Yin, H., Li, C., and Xie, X. Formulating discrete probability flow through optimal transport. Advances in Neural Information Processing Systems, 36, 2024.   \nZhang, R., Liu, X., and Liu, Q. A Langevin-like sampler for discrete distributions. In International Conference on Machine Learning, pp. 26375\u201326396. PMLR, 2022b. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix for \u201cEnergy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces\u201d ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proofs of the Main Results 15 ", "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Theorem 1 15   \nA.2 Eigenvalue Decomposition of Rate Matrices for Proposition 1 16   \nA.3 Proof of Scaling limit in Theorem 2 . 17   \nB Estimation of Energy Discrepancy 18   \nB.1 Binary Case 20   \nB.2 Connection to pseudo-likelihood estimation . 20 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "C Tabular Data Synthesising with Energy-Based Models 21 ", "page_idx": 14}, {"type": "text", "text": "D Additional Experimental Results 23 ", "page_idx": 14}, {"type": "text", "text": "E Naming Conventions and Parameters of Introduced Methods 29 ", "page_idx": 14}, {"type": "text", "text": "A Proofs of the Main Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 1. Let $q_{t}(\\cdot|x)$ be a Markov transition density defined by the rate matrix $R$ with eigenvalues $0\\,=\\,\\lambda_{1}(R)\\,\\geq\\,\\lambda_{2}(R)\\,\\geq\\,\\cdots\\,\\geq\\,\\lambda_{S}(R)$ and uniform stationary distribution. Then, there exists $a$ constant $z_{t}$ independent of $\\theta$ such that energy-discrepancy converges to the maximum-likelihood loss ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathrm{ED}_{q_{t}}(p_{\\mathrm{data}},U_{\\theta})-\\mathcal{L}_{\\mathrm{MLE}}(\\theta)-z_{t}|\\leq\\sqrt{S}\\exp(-|\\lambda_{2}(R)|t)\\,\\mathrm{KL}(p_{\\mathrm{data}}\\parallel p_{\\theta})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with the loss of maximum-likelihood estimation $\\mathcal{L}_{\\mathrm{MLE}}(\\theta):=-\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\big[\\log p_{\\theta}(\\mathbf{x})\\big]$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. For $\\mathcal{X}=\\{1,2,\\ldots,S\\}$ and two probability distributions $p_{1}$ and $p_{2}$ on $\\mathcal{X}$ define the marginal distributions ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{t}p_{1}(y)=\\sum_{x\\in\\mathcal{X}}q_{t}(y|x)p_{1}(x)\\,,\\quad Q_{t}p_{2}(y)=\\sum_{x\\in\\mathcal{X}}q_{t}(y|x)p_{2}(x)\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the data-processing inequality it holds for all $p_{1},p_{2}$ with $\\mathrm{KL}(p_{1}\\parallel p_{2})<\\infty$ that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}(Q_{t}p_{1}\\parallel Q_{t}p_{2})\\leq c(t)\\,\\mathrm{KL}(p_{1}\\parallel p_{2})\\quad\\mathrm{with}\\quad c(t)\\leq1\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We are going to bound the contraction rate $c(t)$ by firstly making use strong data processing inequality Raginsky (2016) which states that the contraction rate can be bounded by the Dobrushin contraction coefficient ", "page_idx": 14}, {"type": "equation", "text": "$$\nc(t)\\leq\\theta(Q_{t})=\\operatorname*{sup}_{x,x^{\\prime}}\\lVert Q_{t}\\delta_{x}-Q_{t}\\delta_{x}^{\\prime}\\rVert_{\\mathrm{TV}}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Furthermore, since total variation is a metric, the contraction between two points $x,x^{\\prime}$ is upper bounded by the contraction towards the stationary distribution of the CTMC $\\pi$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x,x^{\\prime}}\\lvert\\lvert Q_{t}\\delta_{x}-Q_{t}\\delta_{x}^{\\prime}\\rvert\\rvert_{\\mathrm{TV}}\\leq2\\operatorname*{sup}_{x}\\lvert\\lvert Q_{t}\\delta_{x}-\\pi\\rvert\\rvert_{\\mathrm{TV}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we use Diaconis & Stroock (1991, Proposition 3) which states that for any $x\\in\\mathscr{X}$ and for eigenvalues $0=\\lambda_{1}(R)\\geq\\lambda_{2}(R)\\geq\\cdots\\geq\\lambda_{S}(\\bar{R})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|Q_{t}\\delta_{\\mathbf{x}}-\\pi\\|_{\\mathrm{TV}}^{2}\\leq\\frac{1}{4}\\frac{1-\\pi(\\mathbf{x})}{\\pi(x)}\\exp(-2|\\lambda_{2}(R)|t)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The stationary distribution for $R^{\\mathrm{ord}},R^{\\mathrm{cyc}},R^{\\mathrm{unif}}$ is given by the stationary distribution and hence $(1-\\pi(\\mathbf{x}))/\\bar{\\pi}(x)\\leq S$ . Taking roots now yields ", "page_idx": 15}, {"type": "equation", "text": "$$\nc(t)\\leq\\operatorname*{sup}_{x,x^{\\prime}}\\!\\!\\!\\!\\slash(Q_{t}\\delta_{x}-Q_{t}\\delta_{x}^{\\prime}\\|_{\\mathrm{TV}}\\leq\\sqrt{S}\\exp(-|\\lambda_{2}(R)|t)\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we conclude as in Schr\u00f6der et al. (2023): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ \\mathrm{KL}(Q_{t}p_{\\mathrm{data}}\\parallel Q_{t}p_{\\theta})=\\displaystyle\\sum_{y\\in\\mathcal{X}}\\left(\\log Q_{t}p_{\\mathrm{data}}(y)-\\log\\frac{Q_{t}p_{\\theta}(y)}{p_{\\theta}(x)}-\\log p_{\\theta}(x)\\right)Q_{t}p_{\\mathrm{data}}(y)}&{{}}\\\\ {=z_{t}+\\displaystyle\\sum_{y\\in\\mathcal{X}}U_{q_{t},\\theta}(y)Q_{t}p_{\\mathrm{data}}(y)-U_{\\theta}(x)-\\log p_{\\theta}(x)}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $U$ independent entropy term $\\begin{array}{r}{z_{t}:=\\sum_{y\\in\\mathcal{X}}Q_{t}p_{\\mathrm{data}}(y)\\log Q_{t}p_{\\mathrm{data}}(y)}\\end{array}$ . After taking expectations with respect to $p_{\\mathrm{data}}(x)$ we find ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{0\\leq\\mathrm{KL}(Q_{t}p_{\\mathrm{data}}\\parallel Q_{t}p_{\\theta})}\\\\ &{\\quad=z_{t}+\\displaystyle\\sum_{y\\in\\mathcal{X}}U_{q_{t},\\theta}(y)Q_{t}p_{\\mathrm{data}}(y)-\\displaystyle\\sum_{x\\in\\mathcal{X}}U_{\\theta}(x)p_{\\mathrm{data}}(x)-\\displaystyle\\sum_{x\\in\\mathcal{X}}\\log p_{\\theta}(x)p_{\\mathrm{data}}(x)}\\\\ &{\\quad=z_{t}-\\mathrm{ED}_{q_{t}}(p_{\\mathrm{data}},U_{\\theta})-\\mathbb{E}_{p_{\\mathrm{data}}(x)}[\\log p_{\\theta}(x)]\\leq c(t)\\,\\mathrm{KL}(p_{\\mathrm{data}}\\parallel p_{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2 Eigenvalue Decomposition of Rate Matrices for Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The rate matrices for the cyclical and for the ordinal graph structures have a similar structure and are referred to as circulant and tridiagonal matrices. The easiest method for deriving the eigenvalue decompositions consists in deriving recurrence relations for the characteristic polynomial. A systematic study of block circulant matrices can be found in Tee (2007) and a study of tridiagonal matrices was given in Yueh (2005). These more general results may be helpful when constructing perturbations for spaces with a more complex structure than the ones introduced in this work. We take already existing results and check that the desired results hold. ", "page_idx": 15}, {"type": "text", "text": "Proposition 1. Assume the density $q_{t}(b|a):=q_{t}(y=b|x=a)$ is defined by the rate matrices $R^{\\mathrm{cyc}}$ or $R^{\\mathrm{ord}}$ . The transition density for all $a,b\\in\\{1,2,\\dots,S\\}$ is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{t}^{\\mathrm{{cyc}}}(b|a)=\\displaystyle\\frac{1}{S}\\sum_{p=1}^{S}\\exp(2\\pi\\mathrm{i}b\\omega_{p}^{\\mathrm{cyc}})\\,\\exp\\big((2\\cos(2\\pi\\omega_{p}^{\\mathrm{{cyc}}})-2)t\\big)\\,\\exp(-2\\pi\\mathrm{i}a\\omega_{p}^{\\mathrm{cyc}})}\\\\ &{q_{t}^{\\mathrm{{ord}}}(b|a)=\\displaystyle\\frac{2}{S}\\sum_{p=1}^{S}\\frac{1}{z_{p}}\\,\\cos((2b-1)\\pi\\omega_{p}^{\\mathrm{ord}})\\,\\exp\\big((2\\cos(2\\pi\\omega_{p}^{\\mathrm{ord}})-2)t\\big)\\,\\cos((2a-1)\\pi\\omega_{p}^{\\mathrm{ord}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. In the circular case, the identity $R\\mathbf{v}_{p}=\\lambda_{p}\\mathbf{v}_{p}$ reduces for a single row $a$ to ", "page_idx": 15}, {"type": "equation", "text": "$$\nv_{p,a-1}+v_{p,a+1}-2v_{p,a}=\\lambda_{p}v_{p,a}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For $v_{p,a}=\\exp(-2\\pi\\mathrm{i}(p-1)a/S)/\\sqrt{S}$ we find after dividing both sides by $v_{p,a}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\lambda_{p}=\\exp\\left({-2\\pi\\mathrm{i}\\frac{(p-1)(a-1)-(p-1)a}{S}}\\right)+\\exp\\left({-2\\pi\\mathrm{i}\\frac{(p-1)(a+1)-(p-1)a}{S}}\\right)-2\\pi\\mathrm{i}\\frac{(p-1)(a-1)-(p-2)(a-1)}{S}}}\\\\ {{\\ \\ \\ -\\ 9\\,c\\mathrm{exs}\\left({2\\pi\\frac{p-1}{S}}\\right)-2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, it is known that the Fourier basis is unitary, i.e. we have for $a,b\\in\\{1,2,\\dots,S\\}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{p=1}^{S}\\bar{v}_{a,p}v_{b,p}=\\frac{1}{S}\\sum_{p=1}^{S}\\mathrm{exp}\\left(-2\\pi\\mathrm{i}\\frac{(a-b)(p-1)}{S}\\right)=\\delta(a,b)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In the tridiagonal case, we have to study two cases. Firstly, we have to check in row 1 using $2\\cos(x)\\cos(\\bar{y})=\\cos(x+y)+\\cos(x-\\bar{y})$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{p,2}-v_{p,1}=\\lambda_{p}v_{p,1}=2\\cos\\left(\\frac{2\\pi(p-1)}{2S}\\right)\\cos\\left(\\frac{\\pi(p-1)}{2S}\\right)-2\\cos\\left(\\frac{\\pi(p-1)}{2S}\\right)}\\\\ &{=\\cos\\left(\\frac{2\\pi(p-1)}{2S}+\\frac{\\pi(p-1)}{2S}\\right)+\\cos\\left(\\frac{2\\pi(p-1)}{2S}-\\frac{\\pi(p-1)}{2S}\\right)-2\\cos\\left(\\frac{\\pi(p-1)}{2S}\\right)}\\\\ &{=\\cos\\left(\\frac{3\\pi(p-1)}{2S}\\right)-\\cos\\left(\\frac{\\pi(p-1)}{2S}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and equivalently for row $S$ . In all other rows we have $v_{p,a+1}+v_{p,a-1}-2v_{p,a}=\\lambda_{p}v_{p,a}$ from ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{(2a-1)(p-1)}{2S}\\pm\\frac{2(p-1)}{2S}=\\frac{(2(a\\pm1)-1)(p-1)}{2S}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.3 Proof of Scaling limit in Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "It is a typical generalisation of the central limit theorem that random walks attain Brownian motion as a universal scaling limit. We reproduce similar arguments for the law of the continuous time Markov chain. Without loss of generality we shift the state space by one and consider the state space $\\{0,1,\\ldots,S-1\\}$ with cyclical and ordinal structure and let $y_{t}\\sim q_{t}(\\cdot|x=\\mu S)$ , where we always assume that the process is initialised at state $\\mu S$ . Furthermore, we introduce the process $z_{t}$ which is an unconstrained continuous time Markov chain on $\\mathbb{Z}$ with rate matrix $R_{a a}\\,=\\,-2,R_{a,a+1}\\,=$ $1,R_{a,a-1}=1$ . The constrained process can then be described in terms of the unconstrained one: Let $\\dot{\\varphi}_{S}:\\mathbb{Z}\\to\\{0,1,\\dots,S-1\\}$ with $\\varphi_{S}(2n S+p)=p$ and $\\varphi_{S}((2n+1)S+p)\\,=\\,-p$ for for $p\\in\\{0,1,\\ldots,S-1\\}$ and $n\\in\\mathbb{Z}$ . Then, $\\varphi_{S}$ reflects the unconstrained process $z_{t}$ at the boundaries 0 and $S-1$ , i.e. $y_{t}^{\\mathrm{ord}}=\\varphi_{S}(z_{t})$ and $y_{t}^{\\mathrm{cyc}}=z_{t}\\,{\\bmod{S}}$ . For the unconstrained process we define the holding times, i.e. the random time intervals in which the process does not change state ", "page_idx": 16}, {"type": "equation", "text": "$$\nh_{a}=\\operatorname*{inf}_{t\\geq0}\\{t:z_{t}\\neq x=a\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and the jump process $\\begin{array}{r}{N_{t}:=\\sum_{h\\leq t}\\delta(z_{h}\\neq z_{h-})}\\end{array}$ , where $z_{h-}=\\operatorname*{lim}_{s\\uparrow h}z_{s}$ which counts the number of state transitions up to ti me $t$ . It is a standard result that the holding times are exponentially distributed (Anderson, 2012, Proposition 2.8) $h_{a}\\sim\\mathrm{Exp}(-R_{a a})$ . Furthermore, since all holding times are identically distributed and $R_{a a}=-2$ , the resulting jump process has Poisson distribution ", "page_idx": 16}, {"type": "equation", "text": "$$\nN_{t}\\sim\\mathrm{Poisson}(2t)\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With these definitions, we can now first proof the Gaussian limit of $z_{t}$ and then derive the limit of $y_{t}$ . Theorem 2 (Scaling limit). Let $y_{t}\\sim q_{t}(\\cdot|x=\\mu S)$ with $\\mu\\in\\{1/S,2/S,\\dots,1\\}$ where $q_{t}$ is either the transition density of the cyclical or ordinal perturbation. Let $\\varphi:\\mathbb{R}\\to(0,1].$ , where for all $n\\in\\mathbb{Z}$ and $x\\in(0,1]\\;\\varphi^{\\mathrm{cyc}}(n+x)=x$ and $\\varphi^{\\mathrm{ord}}(2n+x)=x$ , $\\varphi^{\\mathrm{ord}}(2n+1+x)=-x$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{y_{S^{2}t}/S\\xrightarrow{S\\rightarrow\\infty}\\varphi(\\xi_{t})\\quad w i t h\\quad\\xi_{t}\\sim\\mathcal{N}(\\mu,2t)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. First, we write $z_{t}$ as the sum of independent increments starting at $x=\\mu S$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nz_{t}=\\mu S+\\sum_{j=1}^{N_{t}}J_{j}\\,,\\quad p(J_{j}=1)=p(J_{j}=-1)=1/2\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can now compute the characteristic function $\\chi_{S}(s)=\\mathbb{E}[\\exp(i s z_{t})]$ of $z_{t}$ rescaled in space and time. We have the following: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi_{S}(s)=\\mathbb{E}\\left[\\exp\\left(\\mathrm{i}s\\frac{y\\delta^{2}t}{S}\\right)\\right]}\\\\ &{\\qquad=\\exp(\\mathrm{i}s\\mu)\\mathbb{E}\\left[\\mathbb{E}\\left[\\exp\\left(\\mathrm{i}s\\sum_{j=1}^{N_{S^{2}}}J_{j}/S\\right)\\right]\\bigg\\vert\\ N_{S^{2}t}\\right]}\\\\ &{\\qquad=\\exp(\\mathrm{i}s\\mu)\\mathbb{E}\\left[\\prod_{j=1}^{N_{S^{2}t}}\\cos\\left(\\frac{s}{S}\\right)\\bigg\\vert N_{S^{2}t}\\right]}\\\\ &{\\qquad=\\exp(\\mathrm{i}s\\mu)\\underbrace{\\sum_{K=0}^{\\infty}\\cos\\left(\\frac{s}{S}\\right)^{K}\\frac{(2S^{2}t)^{K}\\exp(-2S^{2}t)}{K!}}_{\\begin{array}{l}{\\displaystyle=\\exp(\\mathrm{i}s\\mu)}\\end{array}}=\\exp(\\mathrm{i}s\\mu)\\exp\\left(2S^{2}t\\cos\\left(\\frac{s}{S}\\right)-2S^{2}t\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used that $\\cos(x)=1/2(\\exp(\\mathrm{i}x)+\\exp(-\\mathrm{i}x))$ in the third step, the Poisson distribution of $N_{t}$ in the fourth step, and the series expansion of the exponential function in the final step. Since $\\cos(x)\\approx1-1/2x^{\\hat{2}}$ , we now have point-wise for any $s$ and due to the fact that $s/S\\to0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\chi_{S}(s)\\xrightarrow{S\\to\\infty}\\exp(\\mathrm{i}s\\mu-t s^{2})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is the characteristic function of a Gaussian with variance $2t$ and mean $\\mu$ . This proves the convergence in distribution of the rescaled unconstrained process $z_{t}$ . Furthermore, for $\\varphi^{\\mathrm{ord}}$ and $\\varphi^{\\mathrm{cyc}}$ it holds $\\varphi_{S}(z_{t})/S=\\varphi(z_{t}/S)$ for all $S\\in\\mathbb{N}$ . Furthermore, $\\varphi^{\\mathrm{ord}},\\varphi^{\\mathrm{cyc}}$ are continuous maps from $\\mathbb{R}$ to $[0,1)$ with reflecting or periodic boundary conditions. We thus have by the continuous mapping theorem ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{y_{S^{2}t}}{S}}={\\frac{\\varphi_{S}(z_{S^{2}t})}{S}}=\\varphi\\left({\\frac{z_{t}}{S}}\\right)\\,{\\xrightarrow{S\\rightarrow\\infty}}\\,\\varphi(\\xi_{t})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\xi_{t}\\sim\\mathcal{N}(\\mu,2t)$ . ", "page_idx": 17}, {"type": "text", "text": "B Estimation of Energy Discrepancy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The energy discrepancy functional takes the form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{ED}_{q}\\big(p_{\\mathrm{data}},U\\big):=\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}[U(\\mathbf{x})]-\\mathbb{E}_{p_{\\mathrm{data}}(\\mathbf{x})}\\mathbb{E}_{q(\\mathbf{y}|\\mathbf{x})}[U_{q}(\\mathbf{y})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for a conditional perturbing density $q$ and contrastive potential ", "page_idx": 17}, {"type": "equation", "text": "$$\nU_{q}(\\mathbf{y}):=-\\log\\sum_{\\mathbf{x^{\\prime}}\\in\\mathcal{X}}q(\\mathbf{y}|\\mathbf{x}^{\\prime})\\exp(-U(\\mathbf{x^{\\prime}}))\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "While in theory energy discrepancy yields a valid training functional for energy-based models for almost any choice of conditional distribution $q$ , the conditional distribution also needs to allow an estimation of $U_{q}$ with low variance. This is particularly easy when $q$ is symmetric, i.e. $q(\\mathbf{y}|\\mathbf{x})=$ $q(\\mathbf{x}|\\mathbf{y})$ in which case the contrastive potential can be expressed as an expectation which can readily be approximated from samples. This leads to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}_{q,M,w}(U):=\\frac{1}{N}\\sum_{i=1}^{N}\\log\\left(w+\\sum_{j=1}^{M}\\exp(U(\\mathbf{x}^{i})-U(\\mathbf{x}_{-}^{i,j}))\\right)-\\log(M)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\mathbf{x}^{i}\\sim p_{\\mathrm{data}}$ , $\\mathbf{y}^{i}\\sim q_{t}(\\cdot|\\mathbf{x}^{i})$ , and $\\mathbf{x}_{-}^{i,j}\\sim q_{t}(\\cdot|\\mathbf{y}^{i})$ , where the offset $w$ stabilises the loss approximation as discussed in Schr\u00f6der et al. (2023). The interpretation of $w$ is that contributions from negative samples with $U(\\mathbf{x}_{-}^{i,j})>U(\\mathbf{x}^{i})$ are exponentially suppressed as contributions to the loss functional, thus avoiding the energies of negative samples to explode. ", "page_idx": 17}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/98d1a06328e04b0471a45a28fa06e4b0a96c0794fa5c9fdd82dfb42d23ecf659.jpg", "img_caption": ["Figure 5: Scaling limit of the introduced perturbations. Top: Convergence of rescaled cyclical and ordinal perturbations $y_{S^{2}t}/S$ for base time parameters $t\\,=\\,0.01$ and $t\\,=\\,0.05$ to Gaussian $[0,1)$ with non-trivial boundary conditions. One can see that the perturbation converges to a fixed shape on the normalise\u221ad state space. Bottom: Convergence of rescaled cyclical and ordinal perturbation $(y_{S t}-\\mathbb{E}[y_{S t}])/\\sqrt{S}$ for base time parameters $t=0.1$ and $t=0.5$ to Gaussian on $\\mathbb{R}$ (red line). The orange mark indicates the initial sta\u221ate. One can see that the perturbation remains non-trivial as the state space grows to infinity at rate $\\sqrt{S}$ . "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.1 Binary Case ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "On binary spaces, the construction of perturbations is particularly simple. We give some details in this subsection. ", "page_idx": 19}, {"type": "text", "text": "Bernoulli Perturbation. As proposed previously in (Schr\u00f6der et al., 2023, Appendix B.3), $q$ can be defined as a Bernoulli distribution. Specifically, for $\\xi\\,\\sim\\,\\mathrm{Bernoulli}(\\varepsilon)^{d},\\varepsilon\\,\\,\\bar{\\in}\\,\\left(0,1\\right)$ define the Bernoulli perturbed data point as $\\mathbf{y}=\\mathbf{x}+\\pmb{\\xi}$ mod 2. This induces a symmetric transition density $q(\\mathbf{y}|\\mathbf{x})$ on $\\{0,1\\}^{d}$ . The Bernoulli random variable $\\xi_{k}$ emulates an indicator function, signifying in each dimension whether to filp the entry of $\\mathbf{x}$ . The value of $\\epsilon$ controls the information loss induced by the perturbation. In theory, larger values of $\\epsilon$ lead to a more data-efficient loss, while smaller values of $\\epsilon$ may be more practical as they contribute to improved training stability. ", "page_idx": 19}, {"type": "text", "text": "It is easy to compute the matrix exponential ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\exp\\left(t\\left(\\!\\begin{array}{c c}{{-1}}&{{1}}\\\\ {{1}}&{{-1}}\\end{array}\\!\\right)\\right)=\\frac{1}{2}\\left(\\!\\begin{array}{c c}{{1+e^{-2t}}}&{{1-e^{-2t}}}\\\\ {{1-e^{-2t}}}&{{1+e^{-2t}}}\\end{array}\\!\\right)\\xrightarrow{t\\rightarrow\\infty}\\left(\\!\\begin{array}{c c}{{1/2}}&{{1/2}}\\\\ {{1/2}}&{{1/2}}\\end{array}\\!\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "thus relating the continuous time Markov chain framework on $\\{0,1\\}$ to a Bernoulli perturbation with parameter $0.5*(1-e^{-2t})$ . ", "page_idx": 19}, {"type": "text", "text": "Neighbourhood-based Perturbation and grid perturbation. Inspired by concrete score matching (Meng et al., 2022), one can introduce a perturbation scheme based on neighbourhood maps: $\\mathbf{x}\\mapsto$ $\\mathcal{N}(\\mathbf{x})$ , which assigns each data point $\\mathbf{x}\\in\\mathcal{X}$ a set of neighbours $\\mathcal{N}(\\mathbf{x})$ . In this case, the forward transition density is given by the uniform distribution over the set of neighbours, i.e., $q(\\mathbf{y}|\\mathbf{x})\\,=$ $\\frac{1}{|\\mathcal{N}(\\mathbf{x})|}\\delta_{\\mathcal{N}(\\mathbf{x})}(\\mathbf{y})$ . A special case is the grid neighbourhood ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{N}_{\\mathrm{grid}}(\\mathbf{x})=\\{\\mathbf{y}\\in\\{0,1\\}^{d}:\\mathbf{y}-\\mathbf{x}=\\pm\\mathbf{e}_{k},\\:k=1,2,\\ldots,d\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{e}_{k}$ is a vector of zeros with a one in the $k$ -th entry. Notably, this neighbourhood structure also exhibits symmetry, i.e., $\\mathcal{N}_{\\mathrm{grid}}^{-1}(\\mathbf{x})=\\mathcal{N}_{\\mathrm{grid}}(\\mathbf{x})$ . The same perturbation can be derived from an Euler discretisation of the continuous time Markov chain. On a binary space we have for $t=1$ for the same rate matrix as in Equation (30) ", "page_idx": 19}, {"type": "equation", "text": "$$\nF:=\\exp(R)\\approx\\mathrm{id}+\\left({\\begin{array}{r r}{-1}&{1}\\\\ {1}&{-1}\\end{array}}\\right)=\\left(0{\\begin{array}{r r}{1}&{1}\\\\ {1}&{0}\\end{array}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is a completely deterministic perturbation which always changes the state of the data point. Combining this with the localisation to a grid we find ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(\\mathbf{y}|\\mathbf{x})=\\sum_{k=1}^{d}{\\frac{1}{d}}F_{y x}={\\left\\{\\begin{array}{l l}{1/d\\quad\\mathbf{y}-\\mathbf{x}=\\pm\\mathbf{e}_{k}}\\\\ {0\\quad\\qquad{\\mathrm{else}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "thus recovering the grid perturbation. ", "page_idx": 19}, {"type": "text", "text": "B.2 Connection to pseudo-likelihood estimation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Define $\\begin{array}{r}{q(\\mathbf{y}|\\mathbf{x})=\\frac{1}{d}\\sum_{k=1}^{d}\\delta(y_{k},\\bigsqcup)\\delta(\\mathbf{y}_{-k},\\mathbf{x}_{-k})}\\end{array}$ which masks exactly one entry of the data vector. For simiplicity, we write $\\mathbf{y}=\\mathbf{x}_{\\neg k}$ for the masked state and ommit the . Then, energy discrepancy is given for a sampled perturbation $\\mathbf{y}=\\mathbf{x}_{\\neg k}$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{U_{\\theta}(\\mathbf{x})+\\log\\displaystyle\\sum_{\\mathbf{x}\\in\\mathcal{X}}q(\\mathbf{y}=\\mathbf{x}_{\\neg k}|\\mathbf{x})\\exp(-U_{\\theta}(\\mathbf{x}))}}\\\\ {{=-\\log\\displaystyle\\frac{\\exp(-U_{\\theta}(\\mathbf{x}))}{\\sum_{s\\in\\{1,2,\\dots,S_{k}\\}}\\exp(-U_{\\theta}(x_{1},\\dots,x_{k}=s,\\dots,x_{d}))}=-\\log p_{\\theta}(x_{k}|\\mathbf{x}_{\\neg k})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, this specific ED loss function is indeed a Monte Carlo approximation of pseudo-likelihood. Energy discrepancy offers additional flexibility through the tunable choice of $t$ and $M$ , thus making ED adaptable to the structure of the underlying space and more efficient in practice, since the normalisation of the pseudo-likelihood is only computed from $M$ samples and does not require the integration along an entire state space dimension. ", "page_idx": 19}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/d5a842ba1c7ec2d0d0f586fbca9f102fe2c0bcdcadd628877d70eb25dfa07f4b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 6: The training and sampling procedures of energy discrepancy on tabular data. We use one training sample only to illustrate. ", "page_idx": 20}, {"type": "text", "text": "C Tabular Data Synthesising with Energy-Based Models ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we introduce how to use energy discrepancy for training an energy-based model on tabular data. Let $d_{\\mathrm{num}}$ and $d_{\\mathrm{cat}}$ be the number of numerical columns and categorical columns, respectively. Each row in the table is a data point represented as a vector of numerical features tarnadi nc aatne gEoBriMca lw fiethat eurneesr $\\mathbf{x}=[\\mathbf{x}^{\\mathrm{num}},\\mathbf{x}^{\\mathrm{cat}}]$ ,n ew shheroeu $\\mathbf{x}^{\\mathrm{num}}\\in\\mathbb{R}^{d_{\\mathrm{num}}}$ eartnudr $\\mathbf{x}^{\\mathrm{cat}}\\in\\bigotimes_{k=1}^{d_{\\mathrm{cat}}}\\bigl\\{1,\\dots,S_{k}\\bigr\\}$ .n  Tboe done by solving the differential equation in (6). For the numerical features, we choose the Gaussian perturbation as in Schr\u00f6der et al. (2023), which has the transition probability in the form of ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{t}^{\\mathrm{num}}(\\mathbf{y}^{\\mathrm{num}}|\\mathbf{x}^{\\mathrm{num}})=\\mathcal{N}(\\mathbf{y}^{\\mathrm{num}}|\\mathbf{x}^{\\mathrm{num}},t\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the categorical features, we perturb each attribution independently: ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{t}^{\\mathrm{cat}}(\\mathbf{y}^{\\mathrm{cat}}|\\mathbf{x}^{\\mathrm{cat}})=\\prod_{k=1}^{d_{\\mathrm{cat}}}q_{t}(y_{k}^{\\mathrm{cat}}|x_{k}^{\\mathrm{cat}}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Accordingly, there are three different perturbation methods for $q_{t}(y_{k}^{\\mathrm{cat}}|x_{k}^{\\mathrm{cat}})$ : ", "page_idx": 20}, {"type": "text", "text": "\u2022 Uniform perturbation defined in (7): ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{t}^{\\mathrm{uni}}(y_{k}^{\\mathrm{cat}}|x_{k}^{\\mathrm{cat}})=(1-S_{k}t)\\delta(y_{k}^{\\mathrm{cat}},x_{k}^{\\mathrm{cat}})+t\\sum_{k\\neq x_{k}^{\\mathrm{cat}}}\\delta(y_{k}^{\\mathrm{cat}},x_{k}^{\\mathrm{cat}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Cyclical perturbation defined in (8): ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{t}^{\\mathrm{cyc}}(y_{k}^{\\mathrm{cat}}|x_{k}^{\\mathrm{cat}})=\\frac{1}{S}\\sum_{p=1}^{S}\\exp(2\\pi\\mathrm{i}y_{k}^{\\mathrm{cat}}\\omega_{p}^{\\mathrm{cyc}})\\,\\exp\\left((2\\cos(2\\pi\\omega_{p}^{\\mathrm{cyc}})-2)t\\right)\\,\\exp(-2\\pi\\mathrm{i}x_{k}^{\\mathrm{cat}}\\omega_{p}^{\\mathrm{cyc}})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "\u2022 Ordinal perturbation defined in (9): ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!J_{t}^{\\mathrm{ord}}(y_{k}^{\\mathrm{cat}}|x_{k}^{\\mathrm{cat}})\\!=\\!\\frac{2}{S}\\sum_{p=1}^{S}\\frac{1}{z_{p}}\\!\\cos((2y_{k}^{\\mathrm{cat}}\\!-\\!1)\\pi\\omega_{p}^{\\mathrm{ord}})\\,\\exp\\bigl(\\!(2\\cos(2\\pi\\omega_{p}^{\\mathrm{ord}})\\!-\\!2)t\\bigr)\\!\\cos((2x_{k}^{\\mathrm{cat}}\\!-\\!1)\\pi\\omega_{p}^{\\mathrm{ord}})\\,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To reduce the scale of noise, we further introduce grid perturbation, which involves perturbing only one attribute at a time ", "page_idx": 20}, {"type": "equation", "text": "$$\nq_{t}^{\\mathrm{cat}}(\\mathbf{y}^{\\mathrm{cat}}|\\mathbf{x}^{\\mathrm{cat}})=\\frac{1}{d_{\\mathrm{cat}}}\\sum_{k=1}^{d_{\\mathrm{cat}}}q_{t}(y_{k}^{\\mathrm{cat}}|x_{k}^{\\mathrm{cat}}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Theoretically, grid perturbation can be used alongside any type of perturbation described in (7, 8, 9). In our experimental studies, we only explore the combination of grid perturbation with uniform ", "page_idx": 20}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/286923817557c55487d64da5372283018d3e1828b4e2eebdc2296e432960070c.jpg", "img_caption": ["Figure 7: Additional density estimation results on the dataset with 16 dimensions and 5 states. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/0a25095860faa8e62351b8a212c540f1f808fbf7cac156041b41f6b75506ff50.jpg", "img_caption": ["Figure 8: Additional density estimation results on the dataset with 12 dimensions and 10 sates. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "perturbation. By combining the Gaussian perturbation and categorical perturbation together, we can then draw the negative samples $\\mathbf{x}_{-}$ via $\\mathbf{y}\\sim q_{t}(\\cdot|\\mathbf{x})$ , and $\\mathbf{x}_{-}\\sim\\bar{q}_{t}(\\cdot|\\mathbf{y})$ , where ", "page_idx": 21}, {"type": "equation", "text": "$$\nq_{t}(\\mathbf{y}|\\mathbf{x})=q_{t_{\\mathrm{num}}}^{\\mathrm{num}}(\\mathbf{y}^{\\mathrm{num}}|\\mathbf{x}^{\\mathrm{num}})q_{t_{\\mathrm{cat}}}^{\\mathrm{cat}}(\\mathbf{y}^{\\mathrm{cat}}|\\mathbf{x}^{\\mathrm{cat}}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, the energy function $U_{\\theta}$ can be learned by minimising the loss function in (29). We summarise the training procedure in Algorithm 1. ", "page_idx": 21}, {"type": "text", "text": "After training, new tabular data is synthesised by alternately applying Langevin dynamics and Gibbs sampling. Specifically, we update the numerical feature $\\mathbf{x}^{\\mathrm{num}}$ via Langevin dynamics ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{x}^{\\mathrm{num}}\\gets\\mathbf{x}^{\\mathrm{num}}-\\frac{\\epsilon}{2}\\nabla_{\\mathbf{x}^{\\mathrm{num}}}U_{\\theta}([\\mathbf{x}^{\\mathrm{num}},\\mathbf{x}^{\\mathrm{cat}}])+\\sqrt{\\epsilon}\\omega,\\quad\\omega\\sim\\mathcal{N}(0,\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the categorical feature $\\mathbf{x}^{\\mathrm{{cat}}}$ , we employ Gibbs sampler ", "page_idx": 21}, {"type": "equation", "text": "$$\nx_{k}^{\\mathrm{cat}}\\sim p_{\\theta}(x_{k}^{\\mathrm{cat}}|\\mathbf{x}^{\\mathrm{num}},\\mathbf{x}_{-k}^{\\mathrm{cat}})\\propto\\exp(-U_{\\theta}([\\mathbf{x}^{\\mathrm{num}},x_{k}^{\\mathrm{cat}},\\mathbf{x}_{-k}^{\\mathrm{cat}}])),\\quad k=1,2,\\dots,d_{\\mathrm{cat}},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ${\\bf x}_{\\mathrm{-}k}^{\\mathrm{cat}}$ denotes the vector $\\mathbf{x}^{\\mathrm{cat}}$ excluding the $k$ -th attribute. The sampling procedure is summarised in Algorithm 2. ", "page_idx": 21}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/6598bc26369c354195a84e6f0fdb3f56bb579441f515c81760515610c0eb7795.jpg", "table_caption": ["Table 3: Experimental results of discrete density estimation. We display the negative log-likelihood (NLL). The results of baselines are taken from Zhang et al. (2022a). "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/89f61a2840a5146c9069a2ca6d34d1c7bdeaf32e27fd01298f327bbc166aaa98.jpg", "table_caption": ["Table 4: Experimental results of discrete density estimation. We display the MMD (in units of $1\\times10^{-4},$ ). The results of baselines are taken from Zhang et al. (2022a). "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we present the detailed experimental settings and additional results. All experiments are conducted on a single Nvidia RTX 3090 GPU with 24GB of VRAM. ", "page_idx": 22}, {"type": "text", "text": "D.1 Discrete Density Estimation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Experimental Details. This experiment keeps a consistent setting with Dai et al. (2020). We first generate 2D floating-points from a continuous distribution $\\hat{p}$ which lacks a closed form but can be easily sampled. Then, each sample $\\hat{\\textbf{x}}:=\\,\\left[\\hat{\\mathbf{x}}_{1},\\hat{\\mathbf{x}}_{2}\\right]\\,\\in\\,\\mathbb{R}^{2}$ is converted to a discrete data point $\\mathbf{x}\\in\\bar{\\{0,}1\\}^{3\\bar{2}}$ using Gray code. To be specific, given $\\hat{\\mathbf{x}}\\sim\\hat{p}$ , we quantise both $\\hat{\\mathbf{x}}_{1}$ and $\\hat{\\mathbf{x}}_{2}$ into 16-bits binary representations via Gray code (Gray, 1953), and concatenate them together to obtain a 32-bits vector $\\mathbf{x}$ . As a result, the probabilistic mass function in the discrete space is $p(\\mathbf{x})\\propto\\hat{p}\\left([\\mathrm{GrayToFloat}(\\mathbf{x}_{1:16})$ , GrayToFloat $\\left(\\mathbf{x}_{17:32}\\right)]$ ). To extend datasets beyond binary cases, we adhere to the same protocol but utilise base transformation instead. This transformation enables the conversion of floating-point coordinates into discrete variables with different state sizes. It is important to highlight that learning EBMs in such discrete spaces presents challenges due to the highly non-linear characteristics of both the Gray code and base transformation. ", "page_idx": 22}, {"type": "text", "text": "We parameterise the energy function using a 4 layer MLP with 256 hidden dimensions and Swish (Ramachandran et al., 2017) activation. To train the EBM, we adopt the Adam optimiser with a learning rate of 0.0001 and a batch size of 128 to update the parameter for $10^{5}$ steps. For the energy discrepancy, we choose $w=1,M=32$ and the grid perturbation for all variants. For contrastive divergence, we employ short-run MCMC using Gibbs sampling with 10 rounds (i.e., $10*S$ steps). ", "page_idx": 22}, {"type": "text", "text": "After training, we quantitatively evaluate all methods using the negative log-likelihood (NLL) and the maximum mean discrepancy (MMD). To be specific, the NLL metric is computed based on 4, 000 samples drawn from the data distribution, and the normalisation constant is estimated using importance sampling with 1, 000, 000 samples drawn from a variational Bernoulli distribution with $p\\,=\\,0.5$ . For the MMD metric, we follow the setting in Zhang et al. (2022a), which adopts the exponential Hamming kernel with 0.1 bandwidth. Moreover, the reported performances are averaged over 10 repeated estimations, each with 4, 000 samples, which are drawn from the learned energy function via Gibbs sampling. ", "page_idx": 22}, {"type": "text", "text": "Qualitative Results. In Figures 7 and 8, we present additional qualitative results of the learned energy on datasets with 5 and 10 states. We see that ED consistently yields more accurate energy landscapes compared to CD. Notably, we only showcase results using grid perturbation with the uniform rate matrix, as qualitative findings are consistent across different perturbation methods. Additionally, we empirically observe that gradient-based Gibbs sampling methods (Grathwohl et al., 2021; Zhang et al., 2022b) tend to generate samples outside the data support more readily. In this regard, we only display the results of CD methods using vanilla Gibbs sampling. ", "page_idx": 22}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/a1687a1868cb09c9db9067cb7db78224abc7f7bbe6b901853582f262eafe80e9.jpg", "table_caption": ["Table 5: Statistics of the real-world datasets. "], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/392a480ae14f7fe8509d10073c78b14a9ecec6d12c49a87db1ccd71ff7a4b0f3.jpg", "img_caption": ["Figure 9: Comparison of calibration results between the baseline (top) and energy discrepancy (bottom) on varying datasets. Left to right: Bank, Cardio, Churn, Mushroom. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Quantitative Results. The quantitative results are illustrated in Tables 3 and 4, indicating the superior performance of our approaches in most scenarios. Notably, previous studies on discrete EBM modelling exclusively focus on binary cases. As a result, we only present the quantitative comparison for the dataset with 2 states. ", "page_idx": 23}, {"type": "text", "text": "D.2 Tabular Data Synthesising ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Experimental Details for the Synthetic Dataset. For the synthetic dataset, we parametrise the energy function using three MLP layers with 256 hidden states and Swish activation. To handle mixed data types, we transform each categorical feature into a 4-dimensional embedding using a linear layer, and then concatenate these embeddings with the numerical features as input. To train the EBM, we apply the Adam optimiser with a learning rate of 0.0001 and a batch size of 128. We update the parameters over 1, 000 epochs, with each epoch consisting of 100 update iterations. For ED, we set $w=1,M=32$ , using Gaussian perturbation for the numerical features and grid perturbation for the categorical features. For CD, we incorporate the replay buffer strategy and employ Langevin dynamics and Gibbs sampling with 50 rounds (totalling $50*S$ steps) for the numerical and categorical features, respectively. ", "page_idx": 23}, {"type": "text", "text": "Experimental Details for the Real-world Dataset. Table 5 summarises the statistical properties of the datasets. To parameterise the energy function and handle mixed data types, we use the same approach but with 1024 hidden units instead of 256. We train the model using the AdamW optimiser (Loshchilov & Hutter, 2019) with a learning rate of 0.0001 and a weight decay rate of 0.0005. The model is trained for 20, 000 update steps with a batch size of 4096. For ED, Gaussian perturbation is employed for numerical features, while categorical features undergo different perturbation methods. Specifically, TabED-Uni and TabED-Grid use uniform and grid perturbations with $t=0.1$ , respectively. For TabED-Cyc and TabED-Ord, corresponding to cyclical and ordinal perturbations, quadratic scaling is applied with $t$ chosen from the best performance in $\\{0.01,0.005,\\stackrel{\\cdot}{0.001}\\}$ . Moreover, CD utilises the same algorithm as in the synthetic dataset, but with 10 steps for short-run MCMC. The reported results are averaged over 10 randomly sampled synthetic data. ", "page_idx": 23}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/c5244418b508271b90b826990f0c068b21d48de9cb4f1d09f89feb443a2be847.jpg", "table_caption": ["Table 6: Results on density similarity between the synthesis and real tabular data. "], "table_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/27f4d2aba48b240ca900b25b13c3af94d27a29d8660d19823c4523241c39431d.jpg", "img_caption": ["Figure 10: Dynamic MNIST samples generated from the learned energy function using ED-Grid with various sampling methods. Left to right: GWG, GFlowNet, GFlowNet+GWG. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Experimental Details for Calibration. Let $y$ and $\\mathbf{x}$ be the target label and the rest features in the tabular data, we can transform a learned EBM $U_{\\theta}(\\mathbf{x},y)$ into a deterministic classifier: $p_{\\mathrm{EBM}}(y|\\mathbf{x})\\propto$ $\\exp(-U_{\\theta}(\\mathbf{x},y))$ . As a baseline for comparison, we additionally train a classifier $p_{\\mathrm{CLF}}(y|\\mathbf{x})$ with the same architecture by maximising the conditional likelihood: $\\mathbb{E}_{p_{\\mathrm{data}}}[\\log p_{\\mathrm{CLF}}(y|\\mathbf{x})]$ . In particular, we utilise the Adam optimiser with a learning rate of 0.001 and a batch size of 4096 to train the classifier $p_{\\mathrm{CLF}}$ . The model undergoes training for 50 epochs. ", "page_idx": 24}, {"type": "text", "text": "Additional Results for Calibration. Figure 9 presents additional calibration results across different datasets. It shows that the energy-based classifier learned by energy discrepancy exhibits superior calibration compared to the deterministic classifier, except for the Mushroom dataset, where the deterministic classifier achieves $100\\%$ accuracy, resulting in low calibration error. ", "page_idx": 24}, {"type": "text", "text": "Additional Results with Other Metrics. We evaluate our methods against baselines using two additional metrics: single-column density similarity and pair-wise correlation similarity. These metrics assess the similarity in the empirical distribution of individual columns and the correlations between pairs of columns in the generated versus real tables. Both metrics can be computed using the open-source SDMetrics API. As shown in Table 6, the result shows that the proposed ED-based approaches either outperform or achieve comparable performance to the baselines across most datasets. ", "page_idx": 24}, {"type": "text", "text": "D.3 Discrete Image Modelling ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Experimental Details. In this experiment, we parametrise the energy function using ResNet (He et al., 2016) following the settings in Grathwohl et al. (2021); Zhang et al. (2022b), where the network has 8 residual blocks with 64 feature maps. Each residual block has 2 convolutional layers and uses Swish activation function (Ramachandran et al., 2017). We choose ${M=32,w=1}$ for all variants of energy discrepancy, $\\epsilon=0.001$ in Bernoulli perturbations. Note that here we choose a relatively small $\\epsilon$ since we empirically find that the loss of energy discrepancy converges to a constant rapidly with larger $\\epsilon$ , which can not provide meaningful gradient information to update the parameters. All models are trained with Adam optimiser with a learning rate of 0.0001 and a batch size of 100 for 50, 000 iterations. We perform model evaluation every 5, 000 iteration by conducting Annealed Importance Sampling (AIS) with the GWG (Grathwohl et al., 2021) sampler for 10, 000 steps. The reported results are obtained from the model that achieves the best performance on the validation set. After training, we finally report the negative log-likelihood by running 300, 000 iterations of AIS. ", "page_idx": 24}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/255e8f7de2e06a97c38fb7309db6db39ec03fbf9b4df0c1b44becfe1349e233b.jpg", "table_caption": ["Table 7: Running time complexity comparison for energy discrepancy and contrastive divergence. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/4e20c6a288ec2e12b205bcc3122a4f425c28c80b2d080a450fc7bea9db655b50.jpg", "table_caption": ["Table 8: Experimental results of the comparison between energy discrepancy and contrastive divergence with varying MCMC steps. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Qualitative Results. To qualitatively assess the validity of the learned EBM, this study presents generated samples from the dynamic MNIST dataset. We first train an EBM using ED-Grid and then synthesise samples by employing various sampling methods, including: i) GWG (Grathwohl et al., 2021) with 1000 steps; ii) GFlowNet with the same architecture and training procedure as per Zhang et al. (2022a); and iii) GFlowNet followed by GWG with 100 steps. ", "page_idx": 25}, {"type": "text", "text": "Empirically, we find that the quality of generated samples can be improved with more advanced sampling approaches. As depicted in Figure 10, the GWG sampler suffers from mode collapse, leading to samples with similar patterns. In other hands, GFlowNet enhances the quality to some extent, but it produces noisy images. To address this issue, we apply GWG with 100 steps following the GFlowNet. It can be seen that the resulting GFlowNet $^+$ GWG sampler yields the highest quality with clear digits. These observations validate the capability of our energy discrepancies to accurately learn the energy landscape from high-dimensional datasets. We leave the development of a more advanced sampler in future work to further improve the quality of generated images using our energy discrepancy approaches. ", "page_idx": 25}, {"type": "text", "text": "Time Complexity Comparison for Energy Discrepancy and Contrastive Divergence. Energy discrepancy offers greater training efficiency than contrastive divergence, as it does not rely on MCMC sampling. In this experiment, we evaluate the running time per iteration and epoch for energy discrepancy and contrastive divergence in training a discrete EBM on the static MNIST dataset. The experiments include contrastive divergence with varying MCMC steps and variants of energy discrepancy with a fixed value of $M=32$ . The results, presented in Table 7, highlight that ED-Bern and ED-Grid are the fastest options, as they do not involve gradient computations during training. ", "page_idx": 25}, {"type": "text", "text": "Comparison to Contrastive Divergence with Different MCMC Steps. Considering the greater training efficiency of energy discrepancy over contrastive divergence, this study comprehensively compares these two methods with varying MCMC steps in contrastive divergence. Specifically, we utilise the officially open-sourced implementation6 of DULA to conduct contrastive divergence training. As depicted in Table 8, we find that energy discrepancy significantly outperforms contrastive divergence when employing a single MCMC step, and achieves performance comparable to CD-10. We attribute this superiority to the fact that CD-1 involves a biased estimation of the log-likelihood gradient due to inherent issues with non-convergent MCMC processes. In contrast, energy discrepancy does not suffer from this issue due to its consistent approximation. ", "page_idx": 25}, {"type": "text", "text": "The Efficacy of the Number of Negative Samples. In all experiments, we choose the number of negative samples as $\\ensuremath{\\boldsymbol{M}}\\mathrm{~\\ensuremath~{~=~\\ensuremath~{~32~}~}~}$ irrespective of the dimension of the problem, to maximise computational efficiency within the constraints of our GPU capacity. ", "page_idx": 25}, {"type": "text", "text": "To investigate the impact of the number of negative samples on performance, we conduct experiments by training energy-based models on the static MNIST dataset with ED-Grid for different values of $M$ . As detailed in Table 9, our results maintain comparable quality even as the number of negative samples is decreased. Notably, our ", "page_idx": 26}, {"type": "text", "text": "Table 9: Discrete image modelling results of ED-Grid on the static MNIST dataset with different $M$ and $w=1$ . ", "page_idx": 26}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/65c3b579d82ad1f3c5e1ed0c8eb41083b228f96ef7cb7f32f20c4a02d4b70a2e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "approach offers greater parallelisation potential compared to the sequentially computed MCMC of contrastive divergence. ", "page_idx": 26}, {"type": "text", "text": "D.4 Training Ising Models ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Task Descriptions. We further evaluate our methods for training the lattice Ising model, which has the form of ", "page_idx": 26}, {"type": "equation", "text": "$$\np(\\mathbf{x})\\propto\\exp(\\mathbf{x}^{T}J\\mathbf{x}),\\;\\mathbf{x}\\in\\{-1,1\\}^{D},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $J=\\sigma A_{D}$ with $\\sigma\\in\\mathbb R$ and $A_{D}$ being the adjacency matrix of a $D\\times D$ grid. Following Grathwohl et al. (2021); Zhang et al. (2022b,a), ", "page_idx": 26}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/5d476750a5308ec6fc56aa9af5b538df1d7a0c83771aaf6e1cb86e527ddc9157.jpg", "img_caption": ["Figure 11: Results on learning Ising models. Left to right: ground truth, ED-Bern, ED-Grid. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "we generate training data through Gibbs sampling and use the generated data to fti a symmetric matrix $J$ via energy discrepancy. Note that the training algorithms do not have access to the data-generating matrix $J$ , only to the collection of samples. ", "page_idx": 26}, {"type": "text", "text": "Experimental Details. As in Grathwohl et al. (2021); Zhang et al. (2022a,b), we train a learnable connectivity matrix $J_{\\phi}$ to estimate the true matrix $J$ in the Ising model. To generate the training data, we simulate Gibbs sampling with $1,000,000$ steps for each instance to construct a dataset of 2, 000 samples. For energy discrepancy, we choose $w=1,M=32$ for all variants, $\\epsilon=0.1$ in Bernoulli perturbations. The parameter $J_{\\phi}$ is learned by the Adam (Kingma & Ba, 2014) optimiser with a learning rate of 0.0001 and a batch size of 256. Following Zhang et al. (2022a), all models are trained with an $l_{1}$ regularisation with a coefficient in $\\{100,50,10,5,1,0.1,0.01\\}$ to encourage sparsity. The other setting is basically the same as Section F.2 in Grathwohl et al. (2021). We report the best result for each setting using the same hyperparameter searching protocol for all methods. ", "page_idx": 26}, {"type": "text", "text": "Qualitative Results. In Figure 11, we consider $D=10\\times10$ grids with $\\sigma=0.2$ and illustrate the learned matrix $J$ using a heatmap. It can be seen that the variants of energy discrepancy can identify the pattern of the ground truth, confirming the effectiveness of our methods. ", "page_idx": 26}, {"type": "text", "text": "Quantitative Results. In the quantitative comparison to the baselines, we consider $D=10\\times10$ grids with $\\sigma=0.1,0.2,\\ldots,0.5$ and $\\bar{D}=9\\times9$ grids with $\\sigma\\textsubscript{\\,=\\,-0.1,-0.2}$ . The methods are evaluated by computing the negative log-RMSE between the estimated $J_{\\phi}$ and the true matrix $J$ As shown in Table 10, our methods demonstrate comparable results to the baselines and, in certain settings, even outperform Gibbs and GWG, indicating that energy discrepancy is able to discover the underlying structure within the data. ", "page_idx": 26}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/abcf05222f80777a6300f668b903fdd9867215c09d285580ace8f09660f4db34.jpg", "table_caption": ["Table 10: Mean negative log-RMSE (higher is better) between the learned connectivity matrix $J_{\\phi}$ and the true matrix $J$ for different values of $D$ and $\\sigma$ . The results of baselines are directly taken from Zhang et al. (2022a). "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "D.5 Graph Generation ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Task Descriptions. The efficacy of our methods can be further demonstrated by producing highquality graph samples. Following the setting in You et al. (2018), our model is evaluated on the Ego-small dataset, which comprises one-hop ego graphs extracted from the Citeseer network (Sen ", "page_idx": 26}, {"type": "image", "img_path": "wAqdvcK1Fv/tmp/badc768dae7e4b9922cd2fe03983161b276c804cc7299f6dfcb1b480142cec84.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 12: Visualisation of the training data and samples drawn from the energy-based models learned by the variants of our approaches on the Ego-small dataset. ", "page_idx": 27}, {"type": "text", "text": "et al., 2008). We consider the following baselines7 in graph generation, including GraphVAE (Simonovsky & Komodakis, 2018), DeepGMG (Li et al., 2018), GraphRNN (You et al., 2018), GNF (Liu et al., 2019), GrappAF (Shi et al., 2020), GraphDF (Luo et al., 2021), EDP-GNN (Niu et al., 2020), RMwGGIS (Liu et al., 2023), and contrastive divergence with GWG sampler (Grathwohl et al., 2021). ", "page_idx": 27}, {"type": "text", "text": "Experimental Details. Following the setup in You et al. (2018), we split the Ego-small dataset, allocating $80\\%$ for training and the remaining $20\\%$ for testing. To provide better insight into this task, we illustrate a subset of training data in Figure 12a. Notably, these training data examples closely resemble realistic one-hop ego graphs. ", "page_idx": 27}, {"type": "text", "text": "For a fair comparison, we parametrise the energy function via a 5-layer GCN (Kipf & Welling, 2016) with the ReLU activation and 16 hidden states for all energy-based approaches. For hyperparameters, we choose $M=32$ , $w=1$ for all variants of energy discrepancy and $\\epsilon=0.1$ for the Bernoulli perturbation. Following the configuration in Liu et al. (2023), we apply the advanced version of RMwGGIS with the number of samples $s=50$ (Liu et al., 2023, Equation ", "page_idx": 27}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/42ae18311dbd65efeb69d9bc4ae27b29563ffa781a84f4faa5c35895e44e20a7.jpg", "table_caption": ["Table 11: Graph generation results in terms of MMD. Avg. denotes the average over three MMD results. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "11). Regarding the EBM (GWG) baseline, we train it using persistent contrastive divergence with a buffer size of 200 samples and the MCMC steps being 50. To train the models, we use the Adam optimiser with a learning rate of 0.0001 and a batch size of 200. After training, we generate new graphs by first sampling $N$ , which is the number of nodes to be generated, from the empirical distribution of the number of nodes in the training dataset, and then applying the GWG sampler (Grathwohl et al., 2021) with 50 MCMC steps from a randomly initialised Bernoulli noise. To assess the quality of these samples, we employ the MMD metric, evaluating it across three graph statistics, i.e., degrees, clustering coefficients, and orbit counts. Following the evaluation scheme in Liu et al. (2019), We trained 5 separate models of each type and performed 3 trials per model, then averaged the result over 15 runs. ", "page_idx": 27}, {"type": "text", "text": "Qualitative Results. We provide a visualisation of generated graphs from variants of our methods in Figures 12b and 12c. Notably, the majority of these generated graphs resemble one-hop ego graphs, illustrating their adherence to the graph characteristics in the training data. ", "page_idx": 27}, {"type": "text", "text": "Quantitative Results. In Table 11, we compare our methods to various baselines. It can be seen that our methods outperform most baselines in terms of the average of the three MMD metrics, indicating the faithful energy landscapes learned by the energy discrepancy approaches. ", "page_idx": 27}, {"type": "text", "text": "E Naming Conventions and Parameters of Introduced Methods ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "This table summarises the naming conventions and available tuning parameters for all introduced methods. The structured perturbation TabED-Str uses different perturbations depending on the state space structure: On unstructured data, the uniform perturbation with tuning hyper-parameter $t_{\\mathrm{cat}}$ is used, while on ordinally and cyclically structured data the ordinal perturbations and cyclical perturbations are used, respectively, with tuning parameter $t_{\\mathrm{base}}$ . ", "page_idx": 28}, {"type": "table", "img_path": "wAqdvcK1Fv/tmp/9bfe39a2b697f63548785123f8dfe2552634629b50f382620ba73f31c198b069.jpg", "table_caption": ["Table 12: Overview of all introduced energy discrepancy methods "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Our methodology describing the extension of energy discrepancy to discrete and mixed state spaces is given in Section 3 and Section 4, the proofs supporting our claims are found in section Appendix A of the appendix. We support our claims with experiments on discrete and mixed data in Section 6, demonstrating that the methodology generalises to settings of various types like categorical data, tabular data, and binary image data. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The main limitations are outlined in the conclusions section. To the best of our knowledge, the dominating factor in the performance of our method is the assumption that data can be assumed as independent samples of a positive density $p_{\\mathrm{data}}\\,>\\,0$ . This assumption is violated for practically all data sets, but the extent to which the support of $p_{\\mathrm{data}}$ is smaller than the state space deteriorates performance, either because of pre-mature convergence or because the perturbation does not explore the state space efficiently. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 29}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Proofs for our results can be found in Appendix A. Due to the nature of our results a proof sketch within the page limit was not feasible. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The experimental details, regarding the network architecture and hyperparameters, are given in Appendix D. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 30}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The code is anonymised and given in the supplementary materials. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The full details are provided in Appendix D Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The quality of generated synthetic tabular data was tested by comparing the quality of a classifier trained on real and synthetic data. The error bar reflects the standard deviation of the classifier metric based on ten independently generated synthetic data sets sampled from the learned model. Error bars for negative log-likelihoods were not feasible with our computational resources due to the computational cost of annealed importance sampling. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The compute resources, as well as the required experimental runs, are detailed in Appendix D. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and have considered the societal impact. The paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The broader impact is discussed as part of the conclusion. Our method can be used for data imputation in tabular data which can be used downstream applications to discriminate or exclude if used irresponsibly, e.g. on biased data. However, we believe that our method is less prone to such applications than existing methods for data mining. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Our paper demonstrates foundational research tested on publicly available datasets that were designed to test machine learning algorithms. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We properly acknowledge and cite all assets and resources used in the paper. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper introduces a training method for energy-based models and does not release new assets. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper introduces a training method for energy-based models and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper introduces a training method for energy-based models and does not involve crowdsourcing nor research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]