[{"figure_path": "lxuXvJSOcP/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of performance in both source and target domains (Tab. 6). Here, \u201cAverage\u201d (orange dots) refers to mean NDS in both the source and target domains. We draw comparisons with prior methods CAM-Conv [13], DG-BEV [14] and PD-BEV [15] offering an empirical lower and upper bounds, DT and Oracle. Note that we only use 5% of the target label for Domain Adaptation.", "description": "This figure compares the performance of different methods for multi-view 3D object detection in both source and target domains.  The methods compared are: Direct Transfer (DT), CAM-Convs, DG-BEV, PD-BEV (with and without unsupervised domain adaptation (UDA)), and the proposed UDGA method.  The performance metric used is Average NDS (Normalized Distance Score), calculated by averaging NDS scores from both source and target domains.  The results show that the proposed UDGA significantly outperforms other methods, particularly when only 5% of the target domain labels are used for adaptation.", "section": "1 Introduction"}, {"figure_path": "lxuXvJSOcP/figures/figures_3_1.jpg", "caption": "Figure 2: (a) An illustration of multi-view installation translation difference. The first (i.e., source) and second (i.e., target) rows are two perspective views of the same scene captured from different installation points. The translation gap between these views is substantial, approximately 30%. (b) Source trained network shows poor perception capability in target domain, primarily due to extrinsic shifts. In \u2206Height, mAP and NDS have dropped up to -67% compared to source. Note that we simulate the camera extrinsic shift leveraging CARLA [52] (refer to Appendix A for further details).", "description": "This figure shows the impact of camera extrinsic parameter shifts on multi-view 3D object detection.  (a) demonstrates the significant difference in perspective views (approximately 30% translation) between source and target camera installations. (b) illustrates the resulting performance drop (up to 67% in mAP and NDS) of a source-trained network when applied to the target domain due to the extrinsic shift.  The experiment uses CARLA simulation to induce this shift.", "section": "3.2 Domain Shifts in Multi-view 3D Object Detection"}, {"figure_path": "lxuXvJSOcP/figures/figures_4_1.jpg", "caption": "Figure 3: An overview of our proposed methodologies. Our proposed methods comprise two major parts: (i) Multi-view Overlap Depth Constraint and (ii) Label-Efficient Domain Adaptation (LEDA). In addition, our framework employs two phases (i.e., pre-training, and then fine-tuning). Note that we adopt our proposed depth constraint in both phases, and LEDA only in the fine-tuning phase.", "description": "This figure illustrates the proposed Unified Domain Generalization and Adaptation (UDGA) framework.  The framework consists of two main components: the Multi-view Overlap Depth Constraint, which addresses geometric inconsistencies across different viewpoints, and the Label-Efficient Domain Adaptation, which enables efficient adaptation to new domains with limited labeled data.  The framework is designed to be used in a two-phase process: pre-training on a source domain and then fine-tuning on a target domain. The depth constraint is applied during both phases, while the domain adaptation is applied only during the fine-tuning phase.", "section": "3.3 Multi-view Overlap Depth Constraint"}, {"figure_path": "lxuXvJSOcP/figures/figures_9_1.jpg", "caption": "Figure 5: Qualitative depth visualizations of front view lineups in Lyft. The top row illustrates sparse depth ground truths projected from LiDAR point clouds. The middle and bottom rows are the qualitative results of BEVDepth and Ours, respectively. Yellow boxes highlight the improved depth.", "description": "This figure shows a qualitative comparison of depth estimation results from three different methods: ground truth LiDAR data, BEVDepth method, and the proposed 'Ours' method.  The comparison is done on front-view images from the Lyft dataset. Yellow boxes highlight areas where the proposed method shows improved depth estimation compared to BEVDepth, particularly in challenging areas such as far distances and occluded objects.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "lxuXvJSOcP/figures/figures_19_1.jpg", "caption": "Figure 6: The paired sample of each evaluation set in Carla dataset.", "description": "This figure shows paired samples from the Carla dataset used for evaluating the model's performance under various simulated camera position changes. Each row represents a different type of simulated change (Source, Height, Pitch, All), showing how the scene is captured from different viewpoints. The images illustrate how the changes in camera height and pitch affect the view of the scene. The figure is used to demonstrate the robustness of the proposed method in handling these variations, a key aspect in multi-view 3D object detection.", "section": "4 Experimental Results"}, {"figure_path": "lxuXvJSOcP/figures/figures_20_1.jpg", "caption": "Figure 7: Multi-view visualization of the depth estimation of BEVDepth and Ours for (a)Lyft and (b)nuScenes samples. In general, our depth consistency was better in the Lyft dataset, while it was difficult to make a quantitative comparison in the case of nuScenes due to the sparseness of the LiDAR point clouds. The depth range is from 1m to 60m. Best viewed in color.", "description": "This figure shows a qualitative comparison of depth estimation results between BEVDepth and the proposed method (Ours) on Lyft and nuScenes datasets. The top row displays the ground truth (GT) depth maps from LiDAR data, while the middle and bottom rows show the depth maps generated by BEVDepth and the proposed method, respectively. The visualization highlights the improved depth consistency achieved by the proposed method, especially in challenging scenarios with occlusions and sparse LiDAR data.", "section": "4.4 Qualitative Analysis"}]