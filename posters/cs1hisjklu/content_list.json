[{"type": "text", "text": "A Versatile Diffusion Transformer with Mixture of Noise Levels for Audiovisual Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Gwanghyun Kim1,:,\u02da Alonso Martinez2 Yu-Chuan Su2 Brendan Jou2 Jos\u00e9 Lezama2 Agrim Gupta3,\u02da Lijun Yu4,\u02da ", "page_idx": 0}, {"type": "text", "text": "Lu Jiang4,\u02da Aren Jansen2 Jacob Walker2 Krishna Somandepalli2,: ", "page_idx": 0}, {"type": "text", "text": "1Seoul National University 2Google DeepMind 3Stanford University 4Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Training diffusion models for audiovisual sequences allows for a range of generation tasks by learning conditional distributions of various input-output combinations of the two modalities. Nevertheless, this strategy often requires training a separate model for each task which is expensive. Here, we propose a novel training approach to effectively learn arbitrary conditional distributions in the audiovisual space. Our key contribution lies in how we parameterize the diffusion timestep in the forward diffusion process. Instead of the standard fixed diffusion timestep, we propose applying variable diffusion timesteps across the temporal dimension and across modalities of the inputs. This formulation offers flexibility to introduce variable noise levels for various portions of the input, hence the term mixture of noise levels. We propose a transformer-based audiovisual latent diffusion model and show that it can be trained in a task-agnostic fashion using our approach to enable a variety of audiovisual generation tasks at inference time. Experiments demonstrate the versatility of our method in tackling cross-modal and multimodal interpolation tasks in the audiovisual space. Notably, our proposed approach surpasses baselines in generating temporally and perceptually consistent samples conditioned on the input. Project page: avdit2024.github.io ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent years have witnessed a remarkable surge in the development and exploration of multimodal diffusion models. Prominent examples include text-to-image (T2I) [37, 41, 54, 39], text-to-video (T2V) [17, 6, 12]. Despite notable advancements, generating sequences across multiple modalities, like video and audio, remains challenging and is an open research area. ", "page_idx": 0}, {"type": "text", "text": "Introducing a time axis to static data paves the way for diverse multimodal sequential tasks including cross-modal generation (e.g., audio-to-video), multimodal interpolation, and audiovisual continuation as shown in Fig 1. Each task can be further divided based on various input-output combinations of the modalities, leading to a number of conditional distributions. For example, with video data $\\pmb{x}_{0}^{1:N}$ and audio data $y_{0}^{1:N}$ of length $N$ , The complexity of configurations grows with tasks like audiovisual continuation, ppxp0 $p(\\pmb{x}_{0}^{(n_{c}+1:N)},\\pmb{y}_{0}^{(n_{c}+1:N)}|\\pmb{x}_{0}^{(1:n_{c})},\\pmb{y}_{0}^{(1:n_{c})})$ yp01:ncqq, where nc is the input frame length used for conditioning, and multimodal interpolation, xpnPN xcq, ypnPN yc q|xpnPNxq, ypnPNyqq, where Nx and ", "page_idx": 0}, {"type": "image", "img_path": "cs1HISJkLU/tmp/a0f44e5274eb2457cb7e72ad0161b2d73d8b339e92db7bd1607ef15dd52d5092.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Our Audiovisual Diffusion Transformer trained with Mixture of Noise Levels tackles diverse AV generation tasks in a single model; see avdit2024.github.io for video demos. ", "page_idx": 1}, {"type": "image", "img_path": "cs1HISJkLU/tmp/9d1b4c375dbc0743fe093be0b937ee5be0bfc0fbcae428da639d7b5a4e2dc130.jpg", "img_caption": ["Figure 2: Comparing conditional inference for AV-continuation for MM-Diffusion (left) and Ours (right) on Landscape dataset. Our approach excels at generating temporally consistent sequences. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "$\\mathcal{N}_{y}$ are input index sets. Training separate models for each variation is expensive and impractical. A more efficient training approach would be to learn these conditional distributions in a single model without explicitly enumerating them, i.e., in a task-agnostic manner. ", "page_idx": 1}, {"type": "text", "text": "Unconditional diffusion models like MM-Diffusion [40] show potential for learning conditional distributions implicitly, but rely on inference adjustments [18, 40]. This limits performance, as seen in MM-Diffusion\u2019s struggle to generate temporally consistent sequences (see Fig. 2). While UniDiffuser [3] and Versatile Diffusion [51] offer methods for joint and conditional text-image distributions, effectively capturing temporal dynamics of audio and video remains an open challenge. ", "page_idx": 1}, {"type": "text", "text": "Here, we propose a multimodal diffusion framework that empowers a single model to learn diverse conditional distributions. This paves the way for a versatile framework for multimodal diffusion, tackling various generation tasks. Our core idea is that, applying variable noise levels across modalities and time segments 3 enables a single model to learn arbitrary conditional distributions. This formulation offers flexibility to train diffusion models with a mixture of noise levels i.e., MoNL, which introduces variable noise levels across various portions of the input. It has a number of advantages over previous approaches: it requires minimal modifications to the original denoising objective simplifying implementation, task-agnostic training, and support for conditional inference of a given task specification without any inference-time modifications. ", "page_idx": 1}, {"type": "text", "text": "We apply this approach for audiovisual generation by developing a diffusion transformer, AVDiT. To address the computational complexity of high-dimensional audio and video signals, we implement MoNL in the low-dimensional latent space learned by the MAGVIT-v2 [55] for video and the SoundStream [57] for audio. Importantly, the temporal structure in these latent representations enables us to apply variable noise levels. We also introduce a transformer-based network for joint noise prediction. Transformers are a natural choice for our implementation due to their proficiency to model multimodal data [25, 10] capturing complex temporal and cross-modal relationships. ", "page_idx": 1}, {"type": "text", "text": "We assess the capability of MoNL to model various distributions in the audiovisual space by evaluating cross-modal tasks (audio-to-video and video-to-audio generation), and conditioning on small portions (audiovisual continuation and interpolation tasks). For these tasks, we show that the AVDiT trained with MoNL outperforms conventional methods including unconditional and conditional generation models, demonstrating the versatility of our task-agnostic framework as shown in Fig. 1. Notably, qualitative and quantitative evaluations highlight the ability of our framework to generate temporally consistent sequences, as illustrated in Fig. 2. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion Models for Multivariate series data: Consider an example of a video diffusion model where the input is a sequence of image frames. In general, this task is modeling multivariate series data (i.e., image representations) of $d_{\\cdot}$ -dimensions with $N$ elements (no. of image frames), henceforth referred to as time-segments. Thus, the multivariate series data, $\\pmb{x}_{0}\\;=\\;\\pmb{x}_{0}^{1:\\tilde{N}}\\;\\in\\;\\mathbb{R}^{N\\times d}\\;\\sim\\;q(\\pmb{x}_{0})$ can be represented as a sequence of time-segments, where $x_{0}^{n}\\in\\mathbb{R}^{d}$ is the $n$ -th time-segment and $d$ -dimensional representation. ", "page_idx": 2}, {"type": "text", "text": "During the forward process of diffusion models [44, 16], the original data $\\pmb{x}_{0}$ is corrupted by gradually injecting ?noise in ?a sequence of $T$ timesteps. The noisy data $\\pmb{x}_{t}$ at time $t$ can be written as $\\mathbf{\\Deltax}_{t}=\\mathbf{\\delta}$ xt1:N\u201c  \u03b1tx0\\` 1 \u00b4 \u03b1t\u03f5x. Here, \u03f5x \u201c \u03f51x:N $\\pmb{\\epsilon}_{x}=\\pmb{\\dot{\\epsilon}}_{x}^{1:N}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\dot{I}})$ is Gaussian noise injected to the sequence and $\\beta_{t}$ is the noise schedule, $\\alpha_{t}=1-\\beta_{t}$ controls the no?ise level a?t each step with $\\textstyle{\\overline{{\\alpha}}}_{t}=\\prod_{i=1}^{t}\\alpha_{i}$ Each noisy time-segment can be represented as $x_{t}^{n}\\,=\\,\\sqrt{\\overline{{\\alpha}}_{t}}x_{0}^{n}\\,+\\,\\sqrt{1-\\overline{{\\alpha}}_{t}}\\epsilon_{x}^{n}$ . During th e\u015b reverse process, the data is sampled through a chain of reversing the transition kernel $q(\\pmb{x}_{t-1}|\\pmb{x}_{t})$ that is estimated by $p_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t-1}|\\mathbf{\\boldsymbol{x}}_{t})=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}_{t-1}|\\mu(\\mathbf{\\boldsymbol{x}}_{t},t),\\sigma_{t}^{2}I)$ , where $\\begin{array}{r}{\\pmb{\\mu}(\\pmb{x}_{t},t)=\\sqrt{\\alpha_{t}}\\big(\\pmb{x}_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}\\pmb{\\epsilon}_{\\pmb{\\theta}}\\big(\\pmb{x}_{t},t\\big)\\big)}\\end{array}$ The training objective is to learn a residual denoiser $\\epsilon_{\\theta}$ at each step as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{t,\\mathbf{x}_{0},\\epsilon_{x}}\\|\\epsilon_{\\theta}(\\mathbf{x}_{t},t)-\\epsilon_{x}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $t\\sim\\mathcal{U}(\\{1,2,\\dots,T\\})$ is the diffusion timestep. ", "page_idx": 2}, {"type": "text", "text": "Multimodal Diffusion Models: Unconditional joint generation (generating all modalities simultaneously) and conditional generation (generating one modality conditioned on the rest) are commonly used for multimodal diffusion. Typically, separate models are trained for each task as described below: ", "page_idx": 2}, {"type": "text", "text": "Diffusion models for joint generation. For simplicity, let us assume two modalities $\\mathbf{z}_{0},\\,\\pmb{y}_{0}$ . The objective in joint generation is to model the joint data distribution, denoted as $q(x_{0},y_{0})$ . To learn this, a joint noise prediction network, denoted as $\\epsilon_{\\theta}$ is defined by rewriting Eq. 1 as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{\\theta}}\\mathbb{E}_{t,\\mathbf{x}_{0},\\mathbf{y}_{0},\\epsilon_{x},\\epsilon_{y}}\\|\\epsilon_{\\theta}(\\mathbf{x}_{t},\\pmb{y}_{t},t)-[\\epsilon_{x},\\epsilon_{y}]\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $(x_{0},y_{0})$ is a random data point, $[,]$ denotes concatenation, $\\epsilon_{x},\\epsilon_{y}\\;\\sim\\;\\mathcal{N}(\\mathbf{0},I)$ , and $t\\,\\sim$ $\\mathcal{U}(\\{1,2,\\dots,T\\})$ . Diffusion models trained with this objective can perform conditional sampling $q(x_{0}|y_{0})$ using inference-time tricks [18, 40]. ", "page_idx": 2}, {"type": "text", "text": "Conditional training of diffusion models. To learn conditional distributions, expressed as $q(x_{0}|y_{0})$ , a noise prediction network $\\epsilon_{\\theta}$ conditioned on $\\pmb{y}_{0}$ is adopted from Eq. 2: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{t,x_{0},y_{0},\\epsilon_{x}}\\|\\epsilon_{\\theta}(x_{t},y_{0},t)-\\epsilon_{x}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Separate conditional models need to be trained for every pair of modalities and input configurations. ", "page_idx": 2}, {"type": "text", "text": "3 Mixture of Noise Levels (MoNL) ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce a novel framework for learning a wide range of conditional distributions within multimodal data by using a mixture of noise levels. The key idea is to formulate the timestep $t$ (Eq. 1) that determines a noise level in the forward diffusion as a vector. Then, we present representative strategies for variable noise levels. We then show how conditional inference can be performed without additional training. Finally, assembling all these components, we present our versatile audiovisual diffusion transformer (AVDiT). ", "page_idx": 2}, {"type": "text", "text": "3.1 Variable Noise Levels across Modality and Time ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formally, let $M$ represent the number of modalities with sequence representations (latent spaces or raw data). Without loss of generality, assume the representations in each modality have $N$ time", "page_idx": 2}, {"type": "image", "img_path": "cs1HISJkLU/tmp/28d3bb3139aeba74b80e911fc82c0cea220f4f35d9e81683dc800185533d3136.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 3: Overview of (a) diffusion training with variable noise levels per time-segment and per modalities, and (b) the mixture of noise levels. Intensity of the color is used to indicate variable noise levels applied to multimodal input. The original input data $z_{\\mathrm{0}}$ consists of $M$ modalities and $N$ time-segments. $z_{\\mathrm{0}}$ is then perturbed with noise $\\epsilon$ per a noise level determined by a diffusion timestep vector $\\pmb{t}$ to create noisy data $\\scriptstyle z_{t}$ , which is input to the noise prediction network. ", "page_idx": 3}, {"type": "image", "img_path": "cs1HISJkLU/tmp/61657cac12e21bfdf85b6acc6068f1265c8b6ee567ed17daae30489282f91cc7.jpg", "img_caption": ["(a) Conditioning across modalities: cross-modal generation "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "cs1HISJkLU/tmp/b8f26fe63fed91872bf7d6e4c08210881b64a8146f6673ab24025ba4bd629c78.jpg", "img_caption": ["(b) Conditioning across time-segments: multimodal interpolation "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 4: Illustration of the conditional inference in our framework for (a) cross-modal generation and (b) multimodal interpolation. ", "page_idx": 3}, {"type": "text", "text": "segments4. Let us further assume they have the same embedding dimension $d$ (which in practice can be achieved by projecting the noisy input from each modality to the desired dimension). The entire sequence can then be simplified as $\\bar{z_{0}}\\in\\mathbb{R}^{M\\times N\\times d}=z_{0}^{(1:M,\\bar{1}:N)}\\sim q(z_{0})$ \u201c zp01:M,1:Nq\u201e qpz0q, where  0 zpm,nqP Rd denotes the -th time-segment of -th modality. For reference, Sec. 2 represents two modalities of multivariate series data, $\\pmb{x}_{0}^{1:N}$ and $\\pmb{y}_{0}^{1:N}$ , using this notation as $z_{0}^{(1,1:N)}$ and zp0 $z_{0}^{(2,1:N)}$ , respectively. ", "page_idx": 3}, {"type": "text", "text": "We posit that training a single model to support learning arbitrary conditional distributions can be realized by using variable noise levels for each modality $m$ and time segment $n$ of the input space $z_{\\mathrm{0}}$ . We introduce the diffusion timestep vector as $t=t^{(1:M,1:N)}\\in\\mathbb{R}^{M\\times\\bar{N}}$ to match the dimensionality of the multimodal inputs, where each element $t^{(m,n)}\\in[1,T]$ determines the timestep, and in turn the level of noise added to the corresponding element z0pm ,nqof the input z0. ", "page_idx": 3}, {"type": "text", "text": "Recall (from Sec. 2) that in a unimodal case, the goal was to learn the transition kernel $q(\\pmb{x}_{t-1}|\\pmb{x}_{t})$ parameterized by $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})=\\mathcal{N}(\\mathbf{x}_{t-1}|\\mu(\\mathbf{x}_{t},\\bar{t})),\\sigma_{t}^{2}I)$ . Analogously, by introducing a timestep vector t P RM\u02c6N, our goal is now to learn a general transition matrix between the various modalities and time-segments in $\\relax z_{\\mathrm{0}}$ at each step: ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\pmb{\\theta}}\\big(\\big[z_{t^{(1,1)}-1}^{(1,1)},\\ldots,z_{t^{(M,N)}-1}^{(M,N)}\\big]|\\big[z_{t^{(1,1)}}^{(1,1)},\\ldots,z_{t^{(M,N)}}^{(M,N)}\\big]\\big)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "image", "img_path": "cs1HISJkLU/tmp/27dfd1ed6a54a48503963fd7bdfa478b8d8b45935847acd064d00c25222b9b10.jpg", "img_caption": ["Figure 5: Schematic of (a) the proposed approach, and (b) AV-transformer for joint noise prediction. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Then, for diffusion training, we draw a Gaussian noise sequence $\\pmb{\\epsilon}~=~\\pmb{\\epsilon}^{(1:M,1:N)}$ . Each noise element $\\epsilon^{(m,n)}$ is then added to the corresponding element of the original data z0pm,nqwith noise level determined by $t^{(m,n)}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nz_{t^{(m,n)}}^{(m,n)}=\\sqrt{\\overline{{\\alpha}}_{t^{(m,n)}}}z_{0}^{(m,n)}+\\sqrt{1-\\overline{{\\alpha}}_{t^{(m,n)}}}\\epsilon^{(m,n)}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, the joint and conditional training objectives in Eqs. 2 and 3 can be generalized with a single noise prediction objective to learn the joint distribution $\\epsilon_{\\theta}$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{t,z_{0},\\epsilon}\\|\\epsilon_{\\theta}([z_{t^{(1,1)}}^{(1,1)},\\ldots,z_{t^{(M,N)}}^{(M,N)}],t)-\\epsilon\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $z_{0}\\sim q(z_{0})$ is the multimodal input and $\\pmb{t}$ is the diffusion timestep vector. ", "page_idx": 4}, {"type": "text", "text": "3.2 Representative Stratgies for Variable Noise Levels ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Using the generalized view of multimodal noise prediction described in Eq. 6, we now examine various strategies for variable noise levels during the forward diffusion. One can imagine an arbitrarily large number of timestep candidates in the vector space of $\\pmb{t}$ drawn as functions of time-segments of the multivariate series and modalities. Here, we explore four designs to create a mixture of noise levels as illustrated in Fig. 3(b). Let us assume we have final diffusion timestep vector for training, tref P RM\u02c6N where each element trpeif,j is sampled from $\\mathcal{U}(\\{1,2,\\dots,T\\})$ , ", "page_idx": 4}, {"type": "text", "text": "\u201a Vanilla: Same timestep is assigned to all the time-segments and modalities. This is analogous to performing joint learning as $t^{(m,n)^{\\widehat{}}}=t_{r e f}^{(1,\\overline{{{1}}})}$ , and would be the straightforward way to extend the vanilla distillation approach for the multimodal case.   \n\u201a Per Modality $(\\mathtt{P m})$ : Variable timesteps are assigned for each modality, but all time-segments in a given modality have the same timestep as $\\bar{t^{(m,n)}}=t_{r e f}^{(m,\\bar{1})}$ \u201c trpefm,1q. This is expected to promote cross-modal generation tasks. This is a generalization of the UniDiffuser [3] approach for sequences.   \n\u201a Per Time-segment $(\\mathtt{P t})$ : Variable timesteps are assigned as $\\bar{t^{(m,n)}}\\,=\\,t_{r e f}^{(1,n)}$ trpe1f,nq by keeping track of the corresponding time-segments across modalities. Intuitively, this should promote better temporal consistency.   \n\u201a aPnedr  mToidmaeli-tsy eags $t^{(m,n)}=t_{r e f}^{(m,n)}$ .m oTdhiasl iwtoyul d( Pprtomm)ote better temporal correspondence between modalities. ", "page_idx": 4}, {"type": "text", "text": "To enable learning a wide range of conditional distributions, we create a training paradigm where a timestep is uniformly randomly selected from the mixture. Specifically, we refer to this training paradigm as MoNL. A schematic of the overall training process is depicted in Fig. 3(a), with related pseudocodes in Algorithms 1 and 2 in the Appendix. Also, theoretical background on MoNL is detailed in Appendix G. ", "page_idx": 4}, {"type": "text", "text": "3.3 Conditional Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Once the general transition kernel $p_{\\theta}$ is learned in Eq. 4, we investigate the model\u2019s ability to handle arbitrary conditional distributions. We achieve this by selectively injecting inputs during inference based on the task specification, i.e., clean (no noise) inputs for conditional portions with $t^{(m,n)}=0$ , and noisy inputs for generating desired portions of the input with the current diffusion step $t^{(m,n)}=t$ . ", "page_idx": 5}, {"type": "text", "text": "Consider the case of cross-modal generation (Fig. 4(a)), to generate a sequence of $M-m_{c}$ modalities conditioned on $m_{c}\\in(1,M)$ modalities, we set timestep elements of $M-m_{c}$ modalities as $t$ and those of $m_{c}$ conditioning modalities as 0, which achieves: ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\theta}\\big(z_{t-1}^{(m_{c}+1:M,1:N)}|z_{t}^{(m_{c}+1:M,1:N)},z_{0}^{(1:m_{c},1:N)}\\big)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Similarly, for multimodal interpolation (Fig. 4(b)), to generate $N\\mathrm{~-~}n_{c}$ time-segments of all modalities jointly, conditioned on $n_{c}~~\\in~~(1,N)$ time-segments, we set the timestep for the $N\\mathrm{~-~}n_{c}$ time-segments as $t$ , and for the conditioning $n_{c}$ time-segments as 0, which achieves p\u03b8 pztp1\u00b4:1M,nc\\`1:Nq|ztp1:M,nc\\`1:Nq, zp01:M,1:ncqq. Unconditional joint generation is also possible by setting each timestep as the same $t$ , to estimate the transition kernel, $p_{\\pmb\\theta}(z_{t-1}^{(1:M,1:N)}|z_{t}^{(1:M,1:N)})$ . Intuitively, our mixture of noise levels is analogous to self-supervised learning which bypasses the need for predefined tasks during training but enables a deeper understanding of multimodal temporal relationships. See also Sec. E for the discussion on classifier-free guidance in the Appendix. ", "page_idx": 5}, {"type": "text", "text": "4 Audiovisual Latent Diffusion Transformer (AVDiT) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Our model consists of two key components: (1) latent space representations from audio and video autoencoders, and (2) an Audiovisual diffusion transformer (AVDiT) for joint noise prediction. ", "page_idx": 5}, {"type": "text", "text": "Latent Space Representations: For a video of $1~+~L_{v}$ frames, represented as $\\pmb{v}\\quad\\in$ $\\mathbb{R}^{(1+L_{v})\\times\\bar{H}\\times W\\times C}$ , we use MAGVIT-v2 [55], a causal autoencoder to achieve efficient spatial and temporal compression. MAGVIT-v2 results in a low-dimensional representation, $\\mathbf{\\mathcal{x}}_{0}\\quad\\in$ Rp1\\`lvq\u02c6h\u02c6w\u02c6dv, by a compression factor of rs \u201c $\\begin{array}{r c l}{r_{s}}&{=}&{\\frac{H}{h}\\ =\\ \\frac{W}{w}}\\end{array}$ in space and $\\begin{array}{r l r}{r_{t_{v}}}&{{}=}&{\\frac{L_{v}}{l_{v}}}\\end{array}$ Lv in time. Crucially, the use of causal 3D convolutions ensures that the embedding for a given frame is solely influenced by preceding frames, preventing filckering artifacts common in frame-level autoencoders. ", "page_idx": 5}, {"type": "text", "text": "For audio with $L_{a}$ frames, $\\pmb{a}\\in\\mathbb{R}^{L_{a}}$ , we use SoundStream [57], a state-of-the-art neural audio autoencoder. We use the latents $y_{0}\\in\\mathbb{R}^{l_{a}\\times d_{a}}$ prior to quantization as audio latents, a compression rate of rta \u201c lLa in time. The time-segments in our formulation refer to the $1+l_{v}$ and $l_{a}$ temporal dimensions in the video and audio latent spaces respectively. ", "page_idx": 5}, {"type": "text", "text": "Audiovisual Transformer for Joint Noise Prediction: Transformers [48] are a natural fit for multimodal generation as they can: (1) efficiently integrate multiple modalities and their interactions [58, 10], (2) capture intricate spatiotemporal dependencies [8, 5], and (3) have shown impressive video generation capabilities [12, 25]. Inspired by these benefits, we introduce AVDiT, a noise prediction network for latent diffusion as described in Fig. 5. AVDiT utilizes the timestep embedding similar to the condition signal used in W.A.L.T [12]. The Transformer first processes the timestep embeddings and positional encodings to create an embedding of the timestep vector. This embedding serves as a conditioning signal and is utilized to dynamically calculate the scaling and shifting parameters for AdaLN during the Transformer Layer Normalization step. This enables the normalization to incorporate the conditioning information of variable noise levels. We first consider the $l_{a}$ and $1+l_{v}$ time-dimensions for audio and video embeddings respectively. When applying MoNL, we can easily keep track of the corresponding time segments among the $l_{a}$ and $1+l_{v}$ dimensions, given the temporal compression factors in each modality. The noisy latents are then linearly projected matching the final dimension $d$ by adding appropriate spatiotemporal positional embeddings for video and temporal positional embeddings for audio, resulting in $d$ dimensional embeddings for each modality which are then concatenated. ", "page_idx": 5}, {"type": "image", "img_path": "cs1HISJkLU/tmp/98421ffdc21b4b5c5511d720b68e3d4590eab4d5c13026a680d009cbc3b846a5.jpg", "img_caption": ["Multimodal interp Figure 6: Full length examples of our AVDiT trained with MoNL on the Monologue dataset. Samples were generated from unseen conditions at 8fps at $128\\!\\times\\!128$ and are shown at the same rate. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "cs1HISJkLU/tmp/7b4f07b128fd5240dbe93353f82408e015ecc584382f6ca72a600cc93c55d881.jpg", "img_caption": ["Figure 7: Unlike MM-Diffusion (left) where clothes and appearance is altered in the continuation (red arrow), our AVDiT with MoNL (right) maintains subject consistency in the $\\mathrm{AIST++}$ dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Video diffusion models. Diffusion models have revolutionized image [45, 35, 16, 39] and video generation with pixel-space [18, 17, 43] and latent-space [14, 56, 6, 9, 12] approaches. Recently, W.A.L.T [12] pushed the boundaries using transformer-based latent diffusion with joint image-video training. Tackling diverse audio-video generation tasks remains largely unexplored. With an AVDiT trained with MoNL, our unified approach empowers a single model to handle a range of tasks. ", "page_idx": 6}, {"type": "text", "text": "Audio generative models. Audio generation soared with WaveNet [36]\u2019s autoregressive approach. Adversarial audio generation [27, 42, 26] emerged. Combining this with differentiable quantization [38, 47, 1] led to end-to-end neural codecs for efficient audio compression [21, 57]. Recently, diffusion models joined the fray, some using continuous latent spaces [29, 19, 11], others exploring discrete space [52]. Our AVDiT uses continuous embeddings from SoundStream for audio latents. ", "page_idx": 6}, {"type": "text", "text": "Multimodal generative modeling. While multimodal diffusion models [37, 41, 54, 39, 17, 23] have achieved impressive results, the field has primarily focused on the visual domain and audiovisual generation remains less explored. Existing approaches for audio-to-video [53, 53, 30] and video-toaudio [20, 33] generation typically learn task-specific conditional models, limiting their flexibility. To address this, recent works [40, 50] propose more versatile audiovisual models. However, they did not examine multimodal interpolation tasks, which we explore in this work. Tasks such as AV-continuation are critical to understand a model\u2019s capability to generate a temporally consistent multimodal sequence retaining the object consistency from the condition input. ", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "6.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Monologues dataset consists of 19.1 million videos for training and 25K videos for evaluation, each with a single person talking. The videos are center-cropped to a $256\\times256$ resolution. This dataset includes a range of person appearances along with rich verbal and non-verbal communication cues. This dataset is ideally situated to assess concepts such as audiovisual gestural synchrony and multimodal expressions which are key components of human communication and interactions. ", "page_idx": 6}, {"type": "table", "img_path": "cs1HISJkLU/tmp/7edd03827867a2d1343063ebfe65826e7e1a5013e0bfec790e208762a86fe429.jpg", "table_caption": ["Table 1: Comparison of AVDiT trained with mixture of noise levels (MoNL) on the Monologues dataset for unconditional joint generation (Joint), cross-modal (A2V, V2A) and multimodal interpolation (AV-inpaint, AV-continue) tasks. $\\mathrm{FAD}=2.7$ and $\\mathrm{{FVD}=3.3}$ for groundtruth autoencoder reconstructions of the inputs. Fr\u00e9chet metrics estimated with $\\scriptstyle\\mathrm{N}=25\\mathrm{k}$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "$\\mathbf{AIST++}$ is a subset of AIST [46] and contains 1,020 street dance videos (5.2 hours). The videos were segmented into in 8,233 samples for train and 110 for test at 10fps following Ruan et al. [40]. ", "page_idx": 7}, {"type": "text", "text": "Landscape contains natural scenes from 928 [28] videos which were segemented into 5,400 samples for train and 600 samples for test at 10fps. We conduct most of the experiments on the Monologues due to its size and diversity. We use $\\mathrm{AIST++}$ and Landscape for comparison with MM-Diffusion. ", "page_idx": 7}, {"type": "text", "text": "6.2 Evaluation Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Tasks. We study three sets of tasks: (1) Joint audio-video (AV) generation (Joint): (2) Cross-modal generation: Audio-to-video (A2V) and Video-to-audio (V2A), and (3) AV interpolation generative tasks: AV-inpaint where a 1.5s clip is interpolated given one video frame, and 0.125s audio at the beginning and four video frames and 0.5s audio at the end, and AV-continue to flil out 1.5 seconds of AV given the first 5 video frames and corresponding 0.625s of audio. ", "page_idx": 7}, {"type": "text", "text": "Baselines. On the Monologues dataset, we compare the performance of AVDiT trained with MoNL versus three baselines: Vanilla (Eq. 2), Conditional models separately trained for each task, and the per modality model (Sec 3.2) which may be considered as a generalization of the UniDiffuser approach for sequences. We enabled the Vanilla model to generate cross-modal and multimodal interpolation outputs by using the replacement method [18, 40]. We also benchmark MoNL AVDiT against UNet-based MM-Diffusion (MMD) [40], the sole published work with a released model that tackles both audio and video generation within a single model. While a direct comparison between U-Nets and our transformer architecture is inherently challenging due to their distinct design principles, we show that MoNL AVDiT surpasses this strong U-Net baseline, demonstrating the effectiveness of the transformer architecture in this domain. We restrict our quantitative evaluation to A2V and V2A tasks because MMD fails to generate temporally consistent sequences in case of continuation tasks (see Figs. 2 and 7 for example). ", "page_idx": 7}, {"type": "text", "text": "Quantitative evaluation. We use Fr\u00e9chet video distance (FVD) as our video evaluation metric following Yu et al. [55]. Similarly, we use Fr\u00e9chet audio distance (FAD) as the audio evaluation metric following Ruan et al. [40]. Because we use latent space representations for the video and audio, we also report the FVD and FAD scores between reconstructed signal and the original signal as \u201cground-truth\u201d scores as the performance upper bound. While we preferred user studies for assessing audio-video alignment as existing metrics miss subtle synchrony like dance moves matching music beats or gestures aligning with speech patterns, we computed AV-align score [33], limiting them to the open-domain Landscape dataset for the comparison with MMD. ", "page_idx": 7}, {"type": "text", "text": "User studies. We conducted user studies to evaluate the quality of generated content. We adopt the two axes of measurement introduced by Ruan et al. [40] namely audio/video quality and audio-video alignment, and introduce a third one, \u201csubject consistency\u201d to assess whether the person in the generated content is plausibly consistent with the input. For stimuli, we used a total of 360 generated samples (not cherry picked) balanced across A2V, V2A, AV-continue and AV-inpaint tasks and for AVDiT trained with three approaches: MoNL, Vanilla and Per modality on Monologues dataset. The tasks were assessed on a 5-point Likert scale. We also compared rater preference for MoNL AVDiT vs. MMD with 30 videos and 5 raters per video. Raters were presented with generations from the two methods randomized as two options, A/B and were asked to pick one option for each of the three dimensions instead of using a 5-point scale. See more details on implementation and experimental setup in Secs. B and C in the Appendix. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.3 Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Qualitative results. As displayed in Figs. 1 and 6, Our AVDiT model trained with MoNL achieves impressive performance on various tasks within a single framework, including audio-to-video, videoto-audio, joint generation, multimodal continuation and interpolation with flexible input settings, generating temporally consistent videos. Notably, ours preserves clothing and appearance attributes during continuation tasks, unlike MMD which can alter these (see Figs. 2 and 7). More qualitative results and comparisons are available in Figs. 13, 14 and 15, and at avdit2024.github.io. ", "page_idx": 8}, {"type": "text", "text": "Quantitative results. As shown in Table 1, on average across all tasks, AVDiT trained with MoNL outperforms all baselines, demonstrating its versatility to learn diverse conditional distributions in a task-agnostic manner. MoNL excelled at generating samples that are temporally and perceptually consistent with the conditioning input, in the case of AV-inpaint and AV-continue tasks, where other baselines generally failed. Per-modality approach surpassed MoNL for A2V and V2A tasks consistent with the findings in Bao et al. [3] likely because conditional distributions in these cases only need to capture cross-modal associations and not necessarily the underlying temporal dynamics. Unsurprisingly, the vanilla diffusion model trained for joint generation exhibited superior performance in this specific scenario but served as a lower-bound of performance for all other tasks. Finally, MoNL performed better than (if not on-par with) task-specific models for all conditional tasks. ", "page_idx": 8}, {"type": "text", "text": "As evident from Table 2, MoNL outperformed MMD in terms of the FAD and FVD metrics across all tasks on the $\\mathrm{AIST++}$ and Landscape datasets, as estimated using the code provided by Ruan et al. [40]. The significantly better audio generation in our model, likely due to the combination of MoNL and our choice of the SoundStream audio autoencoder, is also reflected in the ground-truth FAD scores for audio reconstruction. In case of video reconstruction quality, (ground-truth FVD) on $\\mathrm{AIST++}$ , our choice of autoencoder was inferior to MMD, possibly due to the small dataset size. Qualitatively, we observed that the MAGVIT-v2 reconstructions eliminated filckering across frames but the reconstruction of small face regions in $\\mathrm{AIST++}$ dance videos was blurry. These findings should be interpreted cautiously due to several factors: the limited size of the $\\mathrm{AIST++}$ and Landscape training splits, our use of a transformer backbone versus MMD\u2019s coupled U-Nets, and our use of pretrained autoencoders for latent space representations. On the Landscape dataset, AV-align results demonstrate that our model achieves better alignment compared to MMD, which aligns with the findings from the user study below. ", "page_idx": 8}, {"type": "text", "text": "User studies. A comparison of the distribution of Likert scores across all tasks for the three approaches we compared is shown in Fig. 8. Pairwise Mann-Whitney $U$ tests were conducted with Bonferroni correction for multiple comparisons to assess statistical difference. Across all axes, raters preferred samples generated from MoNL over that of Vanilla or Per-modality $(\\mathtt{P m})$ approaches. Examining task-specific trends (see Fig. 12 in the Appendix), for the cross-modal tasks, Pm was rated significantly higher than Vanilla, and there was no significant difference between MoNL and $\\mathtt{P m}$ (except for the V2A task on AV alignment). ", "page_idx": 8}, {"type": "text", "text": "For multimodal interpolation tasks, MoNL was rated   \nsignificantly higher than Pm. In line with quantitative   \nresults, these results suggest that MoNL excelled at ference at p \u0103 0.01 after multiple correction. ", "page_idx": 8}, {"type": "image", "img_path": "cs1HISJkLU/tmp/a35ef63e388c5602aaf90c69a9c71b013fc4981fb9b28a34d718ef7ec66bad5d.jpg", "img_caption": ["Figure 8: Comparative analysis across AVDiT models from the user study on AV quality, AV alignment and person consistency. The \\* indicates statistically significant pairwise dif"], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "generating samples, that are perceptually and temporally consistent with the input conditioning. ", "page_idx": 8}, {"type": "text", "text": "As indicated in Table 3, our MoNL AVDiT outperformed MMD in user studies, especially in consistency where ours showed improved consistency along factors such as person\u2019s appearance or the attire. MMD was preferred slightly more in V2A tasks, possibly because the Soundstream audtoencoder we used for audio was not optimized for music generation like in MMD. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Ablations. Recall that MoNL training randomly selects one of the four timestep designs described in Sec. 3.2. We compare MoNL with Per time-segment $\\mathtt{P t}$ , and Per time-segment and Per modality Ptm and $\\mathtt{P t/P m/P t m}$ that excludes vanilla from the timestep mixture, approaches separately as shown in Table 1. Overall, Ptm noise excelled at inpainting and continuation tasks though it was not on par with per-modality approach for cross-modal tasks. In general, $\\mathtt{P t}$ does not perform well by itself. In our experiments, we also observed that the combination of Pm, $\\mathtt{P t}$ , Ptm and $\\mathtt{P t/P m/P t m}$ was sufficient for comparable performance on most tasks except for unconditional joint generation. Adding the Vanilla approach to the mixture of timesteps improved performance for unconditional joint generation while not substantially compromising the performance on other tasks. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a unified approach for multimodal diffusion using a mixture of noise levels (MoNL) for generating and manipulating sequences across modalities and time. This empowers a single model to handle diverse tasks like audio-video continuation, interpolation, and cross-modal generation. We show that an audiovisual latent diffusion transformer (AVDiT) trained with MoNL achieves state-of-the-art performance in audiovisual-sequence generation, providing new opportunities for expressive and controllable multimedia content creation. ", "page_idx": 9}, {"type": "text", "text": "See Sec. A in the Appendix for discussions on limitations and considerations. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Kihyuk Sohn, Caroline Pantofaru and Brian Eoff for feedback on early versions of the manuscript. We thank Prof. Se Young Chun for helpful discussions and feedback on the theoretical aspects of this work. Special thanks to Alex Siegman and Xuhui Jia for managing compute resources. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc V Gool. Soft-to-hard vector quantization for end-to-end learning compressible representations. Advances in neural information processing systems, 30, 2017.   \n[2] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In ICLR, 2022.   \n[3] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. arXiv preprint arXiv:2303.06555, 2023.   \n[4] Reza Bayat. A study on sample diversity in generative models: Gans vs. diffusion models. 2023.   \n[5] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.   \n[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023.   \n[7] Sander Dieleman. Noise schedules considered harmful. https://sander.ai/2024/06/14/ noise-schedules, 2022.   \n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[9] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22930\u201322941, 2023.   \n[10] Mariana-Iuliana Georgescu, Eduardo Fonseca, Radu Tudor Ionescu, Mario Lucic, Cordelia Schmid, and Anurag Arnab. Audiovisual masked autoencoders. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16144\u201316154, 2023.   \n[11] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction-tuned LLM and latent diffusion model. arXiv preprint:2304.13731, 2023.   \n[12] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00e9 Lezama. Photorealistic video generation with diffusion models, 2023.   \n[13] Andrey Guzhov, Federico Raue, J\u00f6rn Hees, and Andreas Dengel. Audioclip: Extending clip to image, text and audio. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 976\u2013980. IEEE, 2022.   \n[14] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2023.   \n[15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS, 2021.   \n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.   \n[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv:2210.02303, 2022.   \n[18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In ICLR Workshop on Deep Generative Models for Highly Structured Data, 2022.   \n[19] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-An-Audio: Text-to-audio generation with prompt-enhanced diffusion models. International Conference on Machine Learning, 2023.   \n[20] Vladimir Iashin and Esa Rahtu. Taming visually guided sound generation. arXiv preprint arXiv:2110.08791, 2021.   \n[21] Srihari Kankanahalli. End-to-end optimized speech coding with deep neural networks. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2521\u20132525. IEEE, 2018.   \n[22] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharif.i Frz\u2019echet audio distance: A metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018.   \n[23] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426\u20132435, 2022.   \n[24] Thomas Koelewijn, Adelbert Bronkhorst, and Jan Theeuwes. Attention and the multiple stages of multisensory integration: A review of audiovisual studies. Acta psychologica, 134(3): 372\u2013384, 2010.   \n[25] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00e9 Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023.   \n[26] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in Neural Information Processing Systems, 33:17022\u201317033, 2020.   \n[27] Kundan Kumar, Rithesh Kumar, Thibault De Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre De Brebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial networks for conditional waveform synthesis. Advances in neural information processing systems, 32, 2019.   \n[28] Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Hyunjun Cho, Jihyun Bae, Jinkyu Kim, and Sangpil Kim. Sound-guided semantic video generation. In ECCV, pages 34\u201350, 2022.   \n[29] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. International Conference on Machine Learning, 2023.   \n[30] Jiawei Liu, Weining Wang, Sihan Chen, Xinxin Zhu, and Jing Liu. Sounding video generator: A unified framework for text-guided sounding video generation. IEEE Transactions on Multimedia, 2023.   \n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.   \n[32] Zhou Lu. A theory of multimodal learning. Advances in Neural Information Processing Systems, 36:57244\u201357255, 2023.   \n[33] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized videoto-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[34] Kevin G Munhall, P Gribble, L Sacco, and M Ward. Temporal constraints on the mcgurk effect. Perception & psychophysics, 58:351\u2013362, 1996.   \n[35] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021.   \n[36] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.   \n[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv preprint, 2022.   \n[38] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019.   \n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[40] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10219\u201310228, 2023.   \n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022.   \n[42] Yuki Saito, Shinnosuke Takamichi, and Hiroshi Saruwatari. Statistical parametric speech synthesis incorporating generative adversarial networks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26(1):84\u201396, 2017.   \n[43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.   \n[44] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICLR, 2015.   \n[45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[46] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. AIST dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing. In ISMIR, 2019.   \n[47] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.   \n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.   \n[49] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video diffusion for prediction, generation, and interpolation. In NeurIPS, 2022.   \n[50] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visual-audio generation with diffusion latent aligners. arXiv preprint arXiv:2402.17723, 2024.   \n[51] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7754\u20137765, 2023.   \n[52] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:1720\u20131733, 2023.   \n[53] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generation via text-to-video model adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 6639\u20136647, 2024.   \n[54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv:2206.10789, 2022.   \n[55] Lijun Yu, Jos\u00e9 Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion\u2013tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023.   \n[56] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18456\u201318466, 2023.   \n[57] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Trans. on Audio, Speech, and Language Processing, 30:495\u2013507, 2021.   \n[58] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13041\u201313049, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Limitations, Impact and Considerations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Our proposed approach, combining mixture of noise levels (MoNL) with the generative capabilities of the Audiovisual diffusion transformer (AVDiT), has certain limitations. As shown with demo videos on avdit2024.github.io, while our models effectively capture subject consistency and intricate nonverbal behaviors such as gestural synchrony with vocal tone, significant improvements are necessary to enhance visual and speech quality. Future research will concentrate on super-resolution systems to address visual quality, while text conditioning could potentially further optimize speech quality. ", "page_idx": 13}, {"type": "text", "text": "A key focus in this work was to demonstrate the versatility and use of MoNL across various tasks using a simple mixing scheme by randomly choosing between different timestep candidates as representative schemes for applying variable noise levels. This simplicity showcases its broad applicability. We acknowledge that fine-grained controlling by weighted mixing of the different schemes could be explored for specific goals or tasks in future work. In fact, one can imagine an arbitrarily large number of timestep candidates in the vector space of the inputs. We specifically chose a simple mixture scheme to demonstrate its versatility as a proof of concept, rather than optimize for any single task. ", "page_idx": 13}, {"type": "text", "text": "Although the method presented in this work is for general multimodal applications, our experiments included human-centric generation tasks. This enabled us to explore unique challenges of that problem setting. For example, consider the case of perceptual expectations for audiovisual alignment/coherence, where misaligned audio and visual cues can drastically alter perception of speech [34, 24]. Generation of photo-realistic persons, speech, and joint generation of both can perpetuate stereotypes. We recognize the ethical concerns and underscore that our goal here is to explore how understanding aspects such as nonverbal behavior in multimodal communications using generative models can open up new avenues in research. ", "page_idx": 13}, {"type": "text", "text": "Specifically, the A2V and V2A tasks in human-centric context, which involve extrapolating visual appearance from speech and vice versa, have the potential to perpetuate stereotypes. The generated samples are derived from the model\u2019s understanding of cross-modal associations in the training dataset, which can be vastly different from human perception. One possible mitigation is to ensure that the model can generate diverse outputs for a given input. Diffusion models can achieve this by utilizing different noises at inference time, given a sufficiently large and diverse training dataset. A recent study showed that Diffusion models demonstrate better sample diversity in generations compared to GANs [4], however, addressing potential issues around mode collapse in the generative models, especially with multimodal data is an open research problem. ", "page_idx": 13}, {"type": "text", "text": "Our work also introduced \u201cconsistency\" to qualitatively assess whether the generation remains congruent with the input conditioning. In continuation and interpolation tasks, the disparity between what the model generates and human perception can be generally minimal, as the conditioning provides a perceptual template for the subject\u2019s potential appearance or voice. In contrast, A2V and V2A tasks warrant an in-depth analysis of this disparity. As our immediate goal was to assess the capability of our proposed approach (and baselines) to generate samples from various conditional distributions, we focused on a broad definition for measuring consistency in user studies. Our future work will focus on extending the consistency measure for a granular understanding of these biases by (1) comparing cross-modal associations in the training data to that of the generated samples, and (2) disaggregate model and human evaluations in cross-modal generation tasks by identifying specific dimensions of human appearance attributes like perceived gender expression or human communication aspects such as voice and gestural synchrony using diverse rater pools. ", "page_idx": 13}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Autoencoders and AVDiT. Given the domain specific nature of the datasets. we trained datasetspecific MAGVIT-v2 autoencoders following $\\mathrm{Yu}$ et al. [55]. For the Monologues dataset, we downsampled the data to 8fps and $128\\times128$ resolution for video and 16kHz for audio and randomly sampled a contiguous clip of 2.125 second (17 frames) to match the input requirements of MAGVITv2. This resulted in a dataset of about 11.8K hours for training. The spatial and temporal video compression ratios were set to $r_{s}=8$ and $r_{t_{v}}=4$ , whereas the temporal audio compression ratio was $r_{t_{a}}\\,=\\,320$ . The embedding dimension of the video and audio latent spaces are $d_{v}\\,=\\,8$ and $d_{a}=1024$ respectively, with the target embedding dimension after linear projection, $d=1024$ . All latents were zero-mean and unit-variance normalized with empirical mean and variance estimates on a small subset. AVDiT has 24 transformer layers with 16 heads with MSA with a total of 420M parameters. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Diffusion training and inference. During training, we use a linear noise variance schedule and a diffusion step $T=1000$ , and a self-conditioning rate of 0.9 following Gupta et al. [12]. At the inference time, we use 250 DDIM steps. All models were trained for about 400K steps with a batch size of 256. We used the AdamW optimizer [31] with a learning rate of 5e-4, 5K warm-up steps, cosine learning rate scheduler and EMA consistent with the denoising transformer setting in Gupta et al. [12]. ", "page_idx": 14}, {"type": "text", "text": "Compute resources. Each experiment listed in Table 1 was conducted using 256 v5e TPU chips (with $16\\!\\times\\!16$ topology) for training (on average, the models were trained for around 350K steps with a batch size of 256 for around five days); inference was conducted using 16 v5e TPU chips with a topology of $4\\!\\times\\!4$ . See https://cloud.google.com/tpu/docs/v5e for more details. Benchmarking experiments to compare with MM-Diffusion on the $\\mathrm{AIST++}$ and Landscape datasets were conducted using two A100 GPUs for conditional inference and estimating FAD/FVD metrics. ", "page_idx": 14}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Evaluation metrics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Since our primary use case is speech generation with the Monologues dataset, we use VGGish embeddings as feature for FAD estimation [22] for the results reported in Table 1. For AV interpolation generative tasks (AV-continue and AV-inpaint), we carefully excluded the conditioning AV frames while estimating Fr\u00e9chet metrics. ", "page_idx": 14}, {"type": "text", "text": "C.2 Comparison with MM-Diffusion ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In order to conduct a fair comparsion to the results reported in MM-Diffusion publication [40], we use the data preprocessing and evaluation code provided at github.com/researchmm/MM-Diffusion for FVD, FAD and KVD metrics. Note that the FAD computation here does not use VGGish embeddings. Instead, it uses AudioCLIP [13] which was trained for general sound classification tasks and not suitable for speech generation tasks as in the Monologues dataset reported in Table 1. ", "page_idx": 14}, {"type": "text", "text": "For the FAD, FVD and KVD results reported in Table 2, we match training conditions for the input image resolution and video FPS with Ruan et al. [40], i.e., $64\\times64$ resolution images at 10fps. We match the duration of audio-video from both models to 2 seconds. For visualization purpose in Fig. 2, we also train our models with $256\\times256$ resolution to match the super-resolution output resolution used by MM-Diffusion. ", "page_idx": 14}, {"type": "text", "text": "Ruan et al. [40] introduce a method for implementing zero-shot transfer of A2V and V2A tasks, inspired by the reconstruction-guided sampling proposed by Ho et al. [18]. For instance, in V2A tasks, the generated noisy audio $\\tilde{a}_{t}$ is computed at each step as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t},v_{t}=\\theta_{a v}\\big(a_{t+1},\\hat{v}_{t+1}\\big),}\\\\ &{\\quad\\tilde{a}_{t}=a_{t}-\\lambda\\sqrt{1-\\overline{{\\alpha}}_{t}}\\nabla_{a_{t}}\\left|\\left|v_{t}-\\hat{v}_{t}\\right|\\right|_{2}^{,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $a_{t+1},\\hat{v}_{t+1}$ are a $N$ -length sequence of generated noisy audio and conditioned noisy video at $t+1$ , $\\theta_{a v}$ is a parameterized denoising step, and $\\lambda$ is a gradient weight. Similarly, the zero-shot transfer of AV-continuation task using the reconstruction-guided sampling [18] can be described by the following equations: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad a_{t},v_{t}=\\theta_{a v}\\big(a_{t+1},v_{t+1}\\big),}\\\\ &{\\tilde{a}_{t}^{(n_{c}+1:N)}=a_{t}^{(n_{c}+1:N)}-\\lambda\\sqrt{1-\\overline{{\\alpha}}_{t}}\\nabla_{a_{t}}\\big(\\Big|\\big|v_{t}^{(1:n_{c})}-\\hat{v}_{t}^{(1:n_{c})}\\Big|\\Big|_{2}^{2}+\\Big|\\Big|a_{t}^{(1:n_{c})}-\\hat{a}_{t}^{(1:n_{c})}\\Big|\\Big|_{2}^{2}\\big)}\\\\ &{\\tilde{v}_{t}^{(n_{c}+1:N)}=v_{t}^{(n_{c}+1:N)}-\\lambda\\sqrt{1-\\overline{{\\alpha}}_{t}}\\nabla_{v_{t}}\\big(\\Big|\\big|v_{t}^{(1:n_{c})}-\\hat{v}_{t}^{(1:n_{c})}\\Big|\\Big|_{2}^{2}+\\Big|\\Big|a_{t}^{(1:n_{c})}-\\hat{a}_{t}^{(1:n_{c})}\\Big|\\Big|_{2}^{2}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "image", "img_path": "cs1HISJkLU/tmp/61a7cd3ce141a5013814725e6ee9cd53e5dac0244ba31801fdc1ada6e7df0fb3.jpg", "img_caption": ["Figure 9: Example stimuli shown to the raters for the user study. We conducted user studies for four tasks, A2V, V2A, audiovisual continuation and multimodal interpolation tasks. One track each for the audio and video modality below the stimulus video were shown to effectively convey the portions that were generated (in green) and condition input (gray). "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "cs1HISJkLU/tmp/fe26c982ea770a3053c961783d54d37de9d992e235dc623809e98b806a713af2.jpg", "table_caption": ["Table 4: Rater instructions for audio/video quality metric. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "cs1HISJkLU/tmp/88a29ca6e16cb4b05ced7d7f0fbd671791895af9b4ee42cf99e34a4f645cd683.jpg", "table_caption": ["Table 5: Rater instructions for audio-video alignment metric. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "cs1HISJkLU/tmp/50f4b39a9308927fa6d011bc5c53dc2302e3ece39e014d95196ac6d6493c534a.jpg", "table_caption": ["Table 6: Rater instructions for subject consistency. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "where $n_{c}$ is the number of conditioned time-segments. We closely follows their V2A codebase to faithfully execute the continuation task. We adopt $\\lambda=0.02$ to prevent numerical instability, as the results tend to diverge for $\\lambda>0.02$ . ", "page_idx": 15}, {"type": "text", "text": "C.3 Qualitative Evaluation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Examples of the video stimulus template shown to the raters is presented in Fig. 9. The rater instructions provided for each axis of quality, alignment and consistency are shown in Tables 4, 5 and 6 respectively. ", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Sampling of a diffusion timestep vector   \n1: function GETTIMESTEPVEC(type)   \n2: if $\\mathtt{t y p e=}=\\mathtt{M o N L}$ then   \n3: $\\mathtt{t y p e}\\sim\\mathcal{U}(\\{\\mathtt{V a n i l l a},\\mathtt{P t},\\mathtt{P m},\\mathtt{P t m}\\}$   \n4: tref P RM\u02c6N \u201e Upt1, 2, . . . , Tuq   \n5: t \u201c 0 P M\u02c6N   \n6: for $m=1,\\dotsc,M$ do   \n7: for $n=1,\\ldots,N$ do   \n8: if type $==$ Vanilla then   \n9: $t^{(m,n)}=t_{r e f}^{(1,1)}$   \n10: else if t $\\mathtt{y p e==p t}$ then   \n11: $t^{(m,n)}\\,{=}\\,t_{r e f}^{(1,n)}$   \n12: else if type $\\dot{\\bf{\\Psi}}=\\mathbb{P}\\mathbb{m}$ then   \n13: $t^{(m,n)}=t_{r e f}^{(m,1)}$   \n14: else if $\\mathtt{\\mathtt{v p e}==p t m}$ then   \n15: $\\bar{t}^{(m,n)}\\,\\bar{=\\,}t_{r e f}^{(m,n)}$   \n16: end for   \n17: end for   \n18: return t   \n19: end function   \nAlgorithm 2 Training with MoNL   \ninput $q(z_{0}),\\epsilon_{\\theta}$ , type (timestep sample type)   \n1: repeat   \n2: $z_{0}\\sim q(z_{0})$   \n3: $\\pmb{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$   \n4: $\\pmb{t}=$ GETTIMESTEPVECptypeq   \n5: for $m=1,\\dotsc,M$ do   \n6: for $n=1,\\ldots,N$ do   \n7: $z_{t^{(m,n)}}^{(m,n)}=\\sqrt{\\overline{{\\alpha}}_{t^{(m,n)}}}z_{0}^{(m,n)}+\\sqrt{1-\\overline{{\\alpha}}_{t^{(m,n)}}}\\epsilon^{(m,n)}$   \n8: end for   \n9: end for   \n01:: $\\nabla_{\\pmb\\theta}\\|\\pmb\\epsilon_{\\pmb\\theta}([z_{t^{(1,1)}}^{(1,1)},\\cdot\\cdot\\cdot\\cdot,z_{t^{(M,N)}}^{(M,N)}],t)-\\pmb\\epsilon\\|_{2}^{2}$   \n2: until converged   \nAlgorithm 3 Joint generation of $\\scriptstyle z_{0}$   \n1: $\\hat{\\boldsymbol{z}}_{T}\\in\\mathbb{R}^{M\\times N\\times d}\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$   \n2: for $\\tau=T,\\dots,1$ do   \n3: $\\pmb{\\epsilon}\\in\\mathbb{R}^{M\\times N\\times d}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ if $\\tau>1$ , else $\\epsilon={\\bf0}$   \n4: t P M\u02c6N   \n5: $\\begin{array}{r}{\\hat{z}_{\\tau-1}=\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\left(\\hat{\\hat{z}}_{\\tau}-\\frac{\\beta_{\\tau}}{\\sqrt{1-\\overline{{\\alpha}}_{t}}}\\epsilon_{\\theta}(\\hat{z}_{\\tau},t)\\right)+\\sigma_{\\tau}\\epsilon}\\end{array}$   \n6: end for   \n7: return $\\hat{z}_{0}$   \nAlgorithm 4 Cross-modal generation of $\\hat{z}_{0}\\,\\in\\,\\mathbb{R}^{(M-m_{c})\\,\\times\\,N\\,\\times\\,d}$ conditioned on   \nz0 P Rmc\u02c6N\u02c6d   \n1: $\\hat{\\boldsymbol z}_{T}\\in\\mathbb{R}^{(M-m_{c})\\times N\\times d}\\sim\\mathcal{N}(\\mathbf{0},I)$   \n2: $\\pmb{t}\\in\\mathbb{R}^{M\\times N}=\\pmb{0}$   \n3: for $\\tau=T,\\dots,1$ do   \n4: $\\pmb{\\epsilon}\\in\\mathbb{R}^{(M-m_{c})\\times N\\times d}\\sim\\mathcal{N}(\\mathbf{0},\\pmb{I})$ if $\\tau>1$ , else $\\epsilon={\\bf0}$   \n5: $\\pmb{t}^{(m_{c}+1:M,1:N)}=\\tau\\pmb{I}$   \n6: \u03f5\u02c6 \u201c \u03f5p\u03b8mc\\`1:M,Nqprz0, z\u02c6\u03c4s, tq   \n7: $\\begin{array}{r}{\\hat{z}_{\\tau-1}=\\frac{1}{\\sqrt{\\alpha_{\\tau}}}\\big(\\hat{z}_{\\tau}-\\frac{\\beta_{\\tau}}{\\sqrt{1-\\overline{{\\alpha}}_{\\tau}}}\\hat{\\epsilon}\\big)+\\sigma_{\\tau}\\epsilon}\\end{array}$   \n8: end for   \n9: return $\\hat{z}_{0}$ ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "D Algorithms ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The core algorithms for implementing mixture of noise levels (MoNL) are presented in three parts: sampling of diffusion timestep vector (Algorithm 1), training process (Algorithm 2), and joint/crossmodal generation at inference time (Algorithms 3 and 4). The sampling algorithms are flexible, using DDPM [16] as an example, and can be replaced with other efficient learning-free samplers like DDIM [45] or Analytic-DPM [2]. Notably, conditional generation across time-segments is simply a change of axis in Algorithm 4. ", "page_idx": 17}, {"type": "text", "text": "E Gratis Classifier-Free Guidance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Classifier-free guidance (CFG) [15], a technique designed to enhance the quality of samples produced by conditional diffusion models using a linear combination of the conditional and unconditional outputs as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\epsilon}_{\\theta}(x_{t},y_{0},t)=(1+s)\\epsilon_{\\theta}(x_{t},y_{0},t)-s\\epsilon_{\\theta}(x_{t},t)}\\\\ &{\\qquad\\qquad\\qquad=(1+s)\\epsilon_{\\theta}^{\\mathrm{cond}}-s\\epsilon_{\\theta}^{\\mathrm{uncond}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $s$ is a guidance scale and the conditional and unconditional outputs are denoted by $\\epsilon_{\\theta}^{\\mathrm{cond}}$ and $\\epsilon_{\\theta}^{\\mathrm{uncond}}$ respectively. Typically, a null token $\\mathcal{Q}$ , is used to allow the conditional model to generate unconditional outputs by setting ${\\pmb y}_{0}=\\mathcal{D}$ . ", "page_idx": 17}, {"type": "text", "text": "Gratis CFG. CFG (Eq. 13) is supported in our framework at inference without any additional training similar to UniDiffuser [3]. Instead of using a null token for generating unconditional outputs $(\\epsilon_{\\theta}^{\\mathrm{uncond}}$ in Eq. 13), Gaussian noise is injected to the conditional portions input per task specification, and setting $t^{(m,n)}\\,=\\,T$ . Conditional outputs $\\epsilon_{\\theta}^{\\mathrm{cond}}$ are obtained as illustrated in Fig. 4. Our vector formulation of the timestep allows us to apply varying levels of noise to different parts of the input. This opens up a number of possibilities for constructing various CFG forms by emphasizing different time segments or modalities, depending on the task at hand. ", "page_idx": 17}, {"type": "text", "text": "MoNL supports classifier-free guidance (CFG) without requiring additional design. Unlike the original CFG (see Eq. 13), it does not need a null token either, hence gratis or free. This is achieved by injecting Gaussian noise to the conditional portions of the multimodal space and setting $t^{(m,n)}=T$ for the output as illustrated in Fig. 10 for the case of cross-modal generation of audio-in, video-out. To illustrate mCFG, consider the conditional output of the network in the cross-modal task (see Eq. 7), denote term used in the gradient step as $\\mathbf{Z}_{t}^{(1:M,\\bar{1}:N)}=\\bigl[z_{t^{(1,1)}}^{(1,1)},\\dots,z_{t^{(M,N)}}^{(M,N)}\\bigr]$ \u201c rztpp11,,11qq, . . . , ztppMM,,NNqqs and conditional portions as \u03f5c\u03b8ond $\\epsilon_{\\theta}^{\\mathrm{cond}}=\\epsilon_{\\theta}\\big(\\mathbf{Z}_{t}^{(1:M,1:N)},t\\big)$ where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf Z}_{t}^{(1:m_{c},1:N)}=z_{0}^{(1:m_{c},1:N)},\\;\\;{\\bf Z}_{t}^{(m_{c}+1:M,1:N)}=z_{t}^{(m_{c}+1:M,1:N)}}\\\\ &{t^{(1:m_{c},1:N)}=0,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;t^{(m_{c}+1:M,1:N)}=t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, the output for the cross-modal generation task is: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\epsilon_{\\theta}^{\\mathrm{uncond}}=\\epsilon_{\\theta}(\\mathbf{Z}_{t}^{(1:M,1:N)},t).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf Z}_{t}^{(1:m_{c},1:N)}=z_{0}^{(1:m_{c},1:N)},\\;\\;{\\bf Z}_{t}^{(m_{c}+1:M,1:N)}=z_{T}^{(m_{c}+1:M,1:N)}}\\\\ &{t^{(1:m_{c},1:N)}=0,\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;t^{(m_{c}+1:M,1:N)}=T.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $h_{T}\\sim\\mathcal{N}(\\mathbf{0},I)$ . Then, mCFG operates by blending the conditional $\\epsilon_{\\theta}^{\\mathrm{{cond}}}$ and unconditional portions $\\epsilon_{\\theta}^{\\mathrm{uncond}}$ per task specification as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\theta}=(1+s)\\epsilon_{\\theta}^{\\mathrm{cond}}-s\\epsilon_{\\theta}^{\\mathrm{uncond}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $s$ is a guidance scale. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\theta}^{(1:M,n_{c}+1:N)}=(1+s)\\epsilon_{\\theta}^{\\mathrm{cond},(1:M,n_{c}+1:N)}-s\\epsilon_{\\theta}^{\\mathrm{uncond},(1:M,n_{c}+1:N)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By formulating the timestep as a vector, we can apply varying levels of noise to different input components. This unlocks diverse possibilities for crafting varied CFG structures. Each structure ", "page_idx": 17}, {"type": "image", "img_path": "cs1HISJkLU/tmp/bb410166b8a22a2eb0273ae35f6c5d699512ba0c08032437d14818b40d16b011.jpg", "img_caption": ["Figure 10: Application of CFG for free in our MoNL approach for cross-modal generation tasks. Whereas a null token is used in traditional CFG for unconditional output, formulating diffusion timestep as a vector enables this by setting the input condition per task-specification to pure noise. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "cs1HISJkLU/tmp/2385f286b98e8becb6d2b811bf04f97e6a4becbbda37ff065820067663979e5e.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 11: Application of CFG for free in our MoNL approach for multimodal interpolation tasks. Because our vector formulation of the timestep enables applying variable noise levels to different portions of the input one can construct a different CFG with \u201cmix-and-match\u201d of modalities and timesegments for creating unconditional outputs. Here, we show an example for multimodal interpolation task for (a) conditional output with two variations: (b) unconditional output with respect to input condition per task specification, (c) partial conditional output, but unconditional output with respect to modalities. ", "page_idx": 18}, {"type": "text", "text": "can amplify specific time segments or modalities based on the task demands, as demonstrated in Fig. 10 for cross-modal tasks and Fig. 11 for the case of multimodal interpolation generation driven by temporal conditioning. ", "page_idx": 18}, {"type": "text", "text": "F Discussion ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Limited conditioning information: In Table 7, we compares the results of AV continuation depending on the input information: AV-continue-2s) to fill out 2 seconds of AV given the first video frame and corresponding 0.125s of audio AV-continue-1.5s) to fill out 1.5 seconds of AV given the first 5 video frames and corresponding 0.625s of audio. The model performed better when given more context (5 video frames and corresponding audio) compared to less context (1 frame and corresponding audio), even though task-specific training can be an upper bound for performance. This suggests that limited conditioning information can lead to issues like unnatural motion and inconsistencies, and including more context improves the model\u2019s performance. ", "page_idx": 18}, {"type": "text", "text": "Table 7: Comparison of AVDiT trained with mixture of noise levels (MoNL) on the Monologues dataset for AV-continue-2s) to fill out 2 seconds of AV given the first video frame and corresponding 0.125s of audio AV-continue-1.5s) to fill out 1.5 seconds of AV given the first 5 video frames and corresponding 0.625s of audio. $\\mathrm{FAD}=2.7$ and $\\mathrm{FVD}=3.3$ for ground truth autoencoder reconstructions of the inputs. Fr\u00e9chet metrics estimated with $\\scriptstyle\\mathrm{N}=25\\mathrm{k}$ . ", "page_idx": 19}, {"type": "table", "img_path": "cs1HISJkLU/tmp/8b6ac3931dad2177eae4f7cb9f1a84c12837917faeebf8519c84f20c13fc69bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "cs1HISJkLU/tmp/345f9a16ca30f31c53f17fe04ee089b01c34ff241f6994583d16b72815262086.jpg", "img_caption": ["Figure 12: Comparative analysis across AVDiT models from the user study along axes of AV quality, AV alignment and person consistency for two cross-modal generation tasks (A2V and V2A), and multimodal interpolation tasks (AV-continue and AV-inpaint). The \\* indicates statistically significant pairwise difference at $p<0.01$ after multiple comparison correction. Across the board, MoNL (Ours) was rated significantly better or on par across all tasks, except for AV-alignment for V2A task (comparison shown in red). For A2V task, there was no significant difference between the models compared for the measure of AV quality. For multimodal interpolation tasks (bottom row), Our approach far surpasses other models for quality, alignment and consistency underscoring the ability of our approach to generate temporally consistent samples that are perceptually congruent with the input condition. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "cs1HISJkLU/tmp/e2af0757c1fc9ba6e7c10fa865621dd68d8b1210df7b83b25b94e889600749ac.jpg", "img_caption": ["Figure 13: Full length examples of AV continuation for 2s from AVDiT trained with MoNL. Samples were generated at 8 fps at $128\\times128$ resolution and are shown at the same rate. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "cs1HISJkLU/tmp/bf548c05de01ddf2292ad70bdf05db5f1cb86fe4dcd867290ea1a1340e0cc464.jpg", "img_caption": ["Video-to-audio generation "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 14: Full length examples of generations from AVDiT trained with mixture of noise levels (MoNL) on the $\\mathrm{AIST++}$ dataset. Generated at 8fps with $128\\!\\times\\!128$ image resolution. ", "page_idx": 20}, {"type": "image", "img_path": "cs1HISJkLU/tmp/d2714be84a27209da81acdd00932b8031dd45611262e61ee901b745a1fa47344.jpg", "img_caption": ["Video-to-audio generation "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 15: Full length examples of generations from AVDiT trained with mixture of noise levels (MoNL) on the Landscape dataset. Generated at 8fps with $256\\!\\times\\!256$ image resolution. ", "page_idx": 21}, {"type": "text", "text": "G Theoretical Background on Mixture of Noise Levels ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "G.1 Theoretical Background on Multimodal Learning ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In \"A Theory of Multimodal Learning\" [32], multimodal learning is shown to offer ?a superior generalization bound compared to unimodal learning, with an improvement factor of $O({\\sqrt{n}})$ , where $n$ denotes the sample size. This benefit relies on connection and heterogeneity between modalities: ", "page_idx": 22}, {"type": "text", "text": "Connection. The bound depends on learned connections between $(\\chi)$ and $(\\mathcal{V})$ . ", "page_idx": 22}, {"type": "text", "text": "Heterogeneity. Describes how modalities $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , diverge and complement. ", "page_idx": 22}, {"type": "text", "text": "If connection and heterogeneity are missing, ill-conditioned scenarios can arise. For instance, if $x=y$ , perfect connection suggests no need for learning about $\\boldsymbol{\\wp}$ . On the other hand, if $x$ is random noise, there is heterogeneity but no meaningful connection between $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ , making non-trivial learning on $\\mathcal{X}$ alone impractical. ", "page_idx": 22}, {"type": "text", "text": "The theory also highlights that and effective connections between modalities via generative models can enhance multimodal learning. This forms the basis for our Mixture of Noise Levels (MoNL) approach, which is particularly suited for multimodal learning with audio and video data. ", "page_idx": 22}, {"type": "text", "text": "G.2 Advantages of Mixture of Noise Level Training ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Our MoNL training method offers significant beneftis for multimodal learning, especially with audio and video data: ", "page_idx": 22}, {"type": "text", "text": "Heterogeneity and connection. Audio and video are naturally heterogeneous. For example, a video of a person speaking includes audio of spoken words and video of lip movements and facial expressions. MoNL uses variable noise levels to enhance learning by capturing the generic transition matrix across the temporal axis. ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{\\theta}\\big(\\big[\\mathbf{z}_{t^{(1,1)}-1}^{(1,1)},\\ldots,\\mathbf{z}_{t^{(M,N)}-1}^{(M,N)}\\big]\\mid\\big[\\mathbf{z}_{t^{(1,1)}}^{(1,1)},\\ldots,\\mathbf{z}_{t^{(M,N)}}^{(M,N)}\\big]\\big)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Enhanced connectivity. MoNL improves connectivity between audio and video modalities. Our experiments show that MoNL often surpasses task-specific learning approaches by fostering better connections between modalities, adapting its focus more effectively. ", "page_idx": 22}, {"type": "text", "text": "G.3 Enhanced Connectivity - Comparison with Existing Methods ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "MoNL vs. Joint learning in MMD [40]. Unlike joint learning methods that focus on the joint distribution $p_{\\theta}\\big(\\mathbf{z}_{t-1}\\mid\\mathbf{z}_{t}\\big)$ , MoNL trains across multiple conditioning, enabling better connections by varying its focus. This is evidenced by MoNL outperforming the Vanilla (see Table 1) and MMD models (see Tables 2 and 3). ", "page_idx": 22}, {"type": "text", "text": "MoNL vs. Per-modality training. MoNL goes beyond per-modality training in UniDiffuser [3], which uses variable noise between modalities i.e., learning $p_{\\boldsymbol{\\theta}}\\big([\\mathbf{\\bar{z}}_{t^{(1)}-1}^{(1)},\\dots,\\mathbf{z}_{t^{(M)}-1}^{(M)}]\\big)\\enspace\\big|$ $[\\mathbf{z}_{t^{(1)}}^{(1)},\\dots,\\mathbf{z}_{t^{(M)}}^{(M)}])$ ztppMMqqsq. MoNL introduces variable noise across different time segments, learning connections across temporal dynamics as well. This advantage is demonstrated in Table 1. ", "page_idx": 22}, {"type": "text", "text": "MoNL vs. Masked training [49]. Diffusion models often obscure high-frequency details with low noise and low-frequency structures with high noise [7]. MoNL employs variable noise levels to explore diverse frequency components, enhancing the model\u2019s ability to correlate high and lowfrequency elements. This is in contrast to masked self-supervised learning, which limits frequencyspecific connections by masking entire elements. ", "page_idx": 22}, {"type": "text", "text": "In summary, the effectiveness of MoNL for multimodal diffusion models, particularly with audio and video data, stems from its strategic use of connection and heterogeneity. By applying variable noise levels, MoNL enhances connectivity between modalities and better adapts to diverse temporal and frequency components, leading to superior performance compared to existing multimodal learning methods. ", "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The two main claims of the paper, i.e., task-agnostic training to support a range of audiovisual generation tasks at inference time, and the ability of our model to generate temporally consistent generations have been empirically validated through quantitative and user studies, along with demo videos on the project page. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have added a detailed discussion of limitations, potential future work and considerations with respect to human-centered generation experiments and evaluations in the Appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: There are two main components of reproducibility here: (1) Model architecture is described in detail and the pseudocode for the proposed algorithm has been provided. (2) Evaluation: The Fr\u00e9chet metrics are implemented using publicly available code, and the design of the user interface for we used as stimulus for rater studies are shared in the Appendix along with the rubric of the measurements used by the raters. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: The datasets we have used to benchmark with baselines are already publicly available. The Monologues dataset on which we conduct our core experiments is not publicly available. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Justification: A summary is provided in the main text of the paper, and the implementation details are provided in the Appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: For qualitative studies, where we examined rater labels of quality or preferences, we describe the statistical test along with the p-value at which the statistical significance is estimated after multiple-comparison correction is also reported. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 25}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The implementation details including compute resources are described in the Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have carefully considered some of the potential societal impacts of this work and potential harms, and discussed possible mitigations in the section \"Limitations, Impact and Considerations\" in the Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative   \nsocietal impacts of the work performed?   \nAnswer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have discussed broader impact of the paper in the section \"Limitations, Impact and Considerations\" in the Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Potential harms and possible mitigations are discussed in the Appendix. The video demos shared for demonstrating non-verbal audiovisual behaviors are from a consented individual. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The creators of the benchmarking datasets and the code we\u2019ve used in this paper have been approproately cited along with the github URLs where the code was provided by the original authors. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Pseudocode relevant to reproducing the implementation of the proposed algorithm is available in the appendix. Demo videos showing recognizable persons are from a consented individual from whom written consent was obtained. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Detailed instructions along with the user interface ahown to the raters and the rubric used for rating tasks are provided in the Appendix.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]