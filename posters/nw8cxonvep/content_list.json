[{"type": "text", "text": "3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minsu Cho ", "page_idx": 0}, {"type": "text", "text": "Pohang University of Science and Technology (POSTECH), South Korea {ljm1121, mscho}@postech.ac.kr http://cvlab.postech.ac.kr/research/3D_EquiPose ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Determining the 3D orientations of an object in an image, known as single-image pose estimation, is a crucial task in 3D vision applications. Existing methods typically learn 3D rotations parametrized in the spatial domain using Euler angles or quaternions, but these representations often introduce discontinuities and singularities. SO(3)-equivariant networks enable the structured capture of pose patterns with data-efficient learning, but the parametrizations in spatial domain are incompatible with their architecture, particularly spherical CNNs, which operate in the frequency domain to enhance computational efficiency. To overcome these issues, we propose a frequency-domain approach that directly predicts Wigner-D coefficients for 3D rotation regression, aligning with the operations of spherical CNNs. Our SO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial parameterizations, ensuring consistent pose estimation under arbitrary rotations. Trained with a frequency-domain regression loss, our method achieves state-ofthe-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+, with significant improvements in accuracy, robustness, and data efficiency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Predicting the 3D pose of objects, i.e., position and orientation, in 3D space from an image is crucial for numerous applications, including augmented reality [56], robotics [4, 5, 60, 65], autonomous vehicles [21, 47], and cryo-electron microscopy [75]. Estimating 3D orientation is particularly challenging due to rotational symmetries and the non-linear nature of rotations. In addition, unlike translations, rotations introduce unique challenges such as gimbal lock and the requirement for continuous, singularity-free representations. Existing methods often learn 3D rotations using spatial domain parameterizations like Euler angles, quaternions, or axis-angle representations, as illustrated in Figure 1. However, these parameterizations suffer from issues such as discontinuities and singularities [51, 55, 76], which can hinder the performance and reliability. ", "page_idx": 0}, {"type": "image", "img_path": "nw8cXoNvep/tmp/f753e4bad969896d175abb06409613cde75dc106be628b37a3ae7ba48c327b0c.jpg", "img_caption": ["Figure 1: Types of representations for 3D rotation prediction. Existing methods consider predicting 3D rotations in the spatial domain. Our method predicts Wigner-D coefficients in the frequency domain, to obtain accurate pose in continuous space using an SO(3)-equivariant network. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "SO(3)-equivariance enables accurate 3D pose estimation and improves generalization to unseen rotations. It ensures that outputs consistently change with the 3D rotation of the input, maintaining rotational consistency between the input and output across network layers. Despite its importance, many existing methods [3, 41, 49, 71, 76] often design networks without considering SO(3)-equivariance, resulting in suboptimal performance when dealing with 3D rotations. In addition, in the context of spherical CNNs [9, 10, 12, 16\u201318, 36] for efficient SO(3)-equivariant operations, the 3D rotation parametrization in the spatial domain is inadequate because these SO(3)-equivariant networks operate in the frequency domain. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we propose an SO(3)-equivariant pose harmonics regression network that directly predicts Wigner-D coefficients in the frequency domain for 3D rotation regression. Building on prior work [28, 35], our method leverages the properties of spherical CNNs [11], which operate in the frequency domain, to guarantee SO(3)-equivariant output representation. By directly regressing Wigner-D matrix coefficients, our approach eliminates the need to convert outputs into spatial representations during training, ensuring alignment with the operations of spherical CNNs. This design allows us to bypass the limitations inherent in traditional spatial parameterizations\u2014such as discontinuities and singularities [51, 55, 76]\u2014resulting in more precise and continuous pose estimation. We further introduce a frequency-domain MSE loss to enable continuous training of 3D rotations, with the flexibility to incorporate distributional losses [49] for effectively capturing rotational symmetries in objects. Our method achieves state-of-the-art performance on standard single object pose estimation benchmarks, including ModelNet10-SO(3) and PASCAL3D $^+$ , demonstrating high sampling efficiency and strong generalization to unseen 3D rotations. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "SO(3) pose regression. The choice of rotation representation is a fundamental aspect of the current SO(3) pose estimation methods. In the early stages of deep learning, methods for SO(3) pose regression choose the rotation representation by direct cosine matrices [29, 69], Euler angles [37, 45, 46, 57, 58], quaternions [5, 13, 31, 32, 66, 74], and axis-angles [14, 20, 59]. However, according to [76], for any representation $R$ in a Euclidean space of dimension $d\\leq4$ , such as Euler angles and quaternions, $R$ is discontinuous and unsuitable for deep learning. In addition, Euler angles can cause gimbal lock, which restricts certain rotations, whereas quaternions avoid this issue but their double representation of rotations in SO(3) can lead to complications such as local minima in optimization problems. As an alternative, a continuous 6D representation with Gram-Schmidt orthonormalization [76] and 9D representation with singular value decomposition (SVD) [8, 38] have been proposed, and [7] proposes manifold-aware gradient layer to facilitate the learning of rotation regression. Denoising diffusion models are employed in the context of SO(3) pose regression [61], or for solving pose estimation by aggregating rays [72]. In contrast to existing SO(3) pose regression methods that formulate rotation representations in the spatial domain, we define the Wigner-D coefficients as the output of the network in the frequency domain, using SO(3)-equivariant networks. ", "page_idx": 1}, {"type": "text", "text": "Pose estimation with a parametric distribution. To model rotation uncertainty, parametric distributions on the rotation manifold are employed in a probabilistic manner. [54] predicts parameters of a mixture of von Mises distributions over Euler angles using Biternion networks. [5, 13, 23] utilize the Bingham distribution over unit quaternions to generate multiple hypotheses of rotations. [48, 70] leverage the matrix Fisher distribution [33] to construct a probabilistic model for SO(3) pose estimation. Additionally, [71] propose the Rotation Laplace distribution for rotation matrices on SO(3) to suppress outliers, and the Quaternion Laplace distribution for quaternions on $S^{3}$ . Nevertheless, parametric models rely on predefined priors. In contrast, our model uses non-parametric modeling during inference to capture more complex pose distributions. ", "page_idx": 1}, {"type": "text", "text": "Pose estimation with a non-parametric distribution. Probabilistic pose estimation can also be achieved by predicting non-parametric distributions. IPDF [49] introduces the estimation of arbitrary, non-parametric distributions on SO(3) using implicit functions with MLPs, and HyperPosePDF [27] uses hypernetworks to predict implicit neural representations by Fourier embedding. ExtremeRotation [6] predicts discretized distributions over $N$ bins for relative 3D rotations trained with cross-entropy loss. RelPose [40, 73] uses an energy-based formulation to represent distributions over the discretized space of SO(3) relative rotation. Several SO(3)-equivariant modeling methods construct non-parametric distributions by utilizing icosahedral group convolution [34], projecting image features orthographically onto a sphere [35], and satisfying consistency properties of SO(3) ", "page_idx": 1}, {"type": "text", "text": "by translating them into an SO(2)-equivariance constraint [28]. RotationNormFlow [41] uses discrete normalizing flows to directly generate rotation distributions on SO(3). These non-parametric methods, which are trained with loss functions in discretized distributions, such as cross-entropy and negative log-likelihood, tend to lose precision in rotation prediction. In contrast, our method predicts continuous SO(3) transformations through regression, eliminating the need to approximate SO(3) poses within a discretized space and enabling our model to achieve accurate 3D rotations. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Representations of Rotations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Rotation representation in spatial domain. In 3D rotation, Euler angles are a common SO(3) representation but suffer from non-uniqueness and gimbal lock, making them less suitable for neural network predictions. Quaternions offer a solution by preventing gimbal lock, but their nonunique representation (q and -q) can complicate certain optimization processes. The axis-angle representation is intuitive but can encounter singularities. The 6D and 9D representations provide newer approaches that simplify optimization in deep networks by avoiding non-linear constraints and ensuring orthogonality. However, they also introduce complexities in maintaining constraints during the learning process. Thus, choosing an appropriate rotation representation is crucial for accurate pose estimation in various computational applications. For a detailed explanation, please refer to Sec. A.1 and an overview of learning 3D rotations in [51, 55]. ", "page_idx": 2}, {"type": "text", "text": "Rotation representation in frequency domain. In the frequency domain, 3D rotation is managed by manipulating spherical harmonics coefficients. Spherical harmonics, denoted as $Y_{m}^{l}(\\theta,\\phi)$ , are functions defined on the surface of a sphere using polar $(\\theta)$ and azimuthal $\\protect(\\phi)$ angles. These harmonics are characterized by their degree $l$ and order $m$ , truncated to a maximum degree $L$ for computational feasibility. The rotation of spherical harmonics is represented by the shift theorem [43], where a rotation operator $\\Lambda_{g}$ acts on spherical harmonics, transforming them via a matrix $D_{m n}^{l}(g)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Lambda_{g}Y_{m}^{l}(x)=\\sum_{\\left|n\\right|\\leq l}D_{m n}^{l}(g)Y_{n}^{l}(x).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This matrix, part of the irreducible unitary representation of SO(3), expresses how each harmonic changes under rotation, summing over all orders $n$ from $-l$ to $l$ , called Wigner-D matrix. The Wigner-D rotation representation is not limited to a specific case of 3D rotations but can be converted from any 3D rotation representation, such as Euler angles, quaternions, and 3D rotation matrices. Our SO(3) equivariant network predicts the Wigner-D representation in the frequency domain instead of predicting rotations in the spatial domain. For a detailed explanation, please refer to Sec. A.2. ", "page_idx": 2}, {"type": "text", "text": "3.2 SO(3)-Equivariance ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Equivariance. Equivariance is a useful property to have because transformations $T$ applied to the input produce predictable and consistent output of the features through transformations $\\phi\\in\\Phi$ , enhancing both interpretability and data efficiency. For example, a feature extractor $\\Phi$ is equivariant to a transformation if applying the transformation to the input and then applying the extractor produces the same output as applying the extractor first and then the transformation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Phi(T_{g}(x))=T_{g}^{\\prime}(\\Phi(x))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $T_{g}$ and $T_{g}^{\\prime}$ represent transformations acting on a group $g\\in G$ of the input and output spaces, respectively. This ensures that the network\u2019s output remains consistent with transformations applied to the input. For translation groups, convolution inherently maintains this property. For rotations, additional rotation-equivariant layers are integrated into the network design. ", "page_idx": 2}, {"type": "text", "text": "Group-equivariant convolutional networks [10] extend this concept to complex groups like rotations or other symmetries. By designing convolutions that are equivariant to these group actions, these networks can handle a broader range of transformations. This can be mathematically described as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n[h*\\phi](g)=\\sum_{y\\in X}h(y)\\cdot\\phi(g^{-1}y)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $h$ is the input function over space $X$ , $\\phi$ is the filter or kernel, and $g\\,\\in\\,G$ is an element of the group. The term $g^{-1}y$ represents the transformation of $y$ by the inverse of $g$ . This operation ", "page_idx": 2}, {"type": "image", "img_path": "nw8cXoNvep/tmp/9b864da20bc8adaedd92cfc45c1195ff2ab671574fd4bdd9e1fbdeacbc54e407.jpg", "img_caption": [""], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overall architecture. Our network for SO(3)-equivariant pose estimation consists of four parts: feature extraction, spherical mapper, Fourier transformer, and SO(3)-equivariant layers. First, we extract a feature map using a pre-trained ResNet. Next, the spherical mapper orthographically projects the extracted feature map onto a spherical surface. The Fourier transformer converts this spatial information into the frequency domain. We utilize spherical convolutions to obtain the final Wigner-D harmonics coefficients $\\Psi$ which represent SO(3) rotations of spherical harmonics, where $M$ denotes the total number of Wigner-D matrix coefficients. ", "page_idx": 3}, {"type": "text", "text": "ensures that the network remains equivariant to the actions of the group $G$ , allowing it to handle inputs transformed by any element of this symmetry group. ", "page_idx": 3}, {"type": "text", "text": "On the sphere in 3D, however, there is no straightforward way to implement a convolution in the spatial domain due to non-uniform samplings [12]. This challenge arises because traditional convolution operations rely on uniform grid structures, which are not applicable to spherical data. To address this, specialized methods such as spherical convolutions or graph-based approaches are employed to handle the unique structure and sampling patterns of spherical data, thereby ensuring effective feature extraction and equivariance on spherical surfaces. ", "page_idx": 3}, {"type": "text", "text": "Spherical convolutions for $S O(3)$ -equivariance. To effectively analyze complex spatial data, such as for volumetric rendering and 3D pose estimation, it is necessary to develop functions with equivariance to the $S O(3)$ group. Early methods for spherical convolution were defined by computing Fourier transforms and convolution on the 2-sphere [15]. However, the output of these spherical convolutions is a function on the sphere, not on $S O(3)$ . Spherical CNNs [11] extended this approach to effectively convolve on the $S O(3)$ group. Using the truncated Fourier transform, signals on $S^{2}$ are modeled with spherical harmonics $Y_{n}^{l}$ , and on $S O(3)$ with Wigner-D matrix coefficients $D_{m n}^{l}$ . ", "page_idx": 3}, {"type": "text", "text": "To efficiently compute the $S^{2}$ and $S O(3)$ convolution, generalized fast Fourier transforms (GFFTs) demonstrate optimized computation [11]. The GFFTs show robustness and efficiency in spherical signal processing, where the spectral group convolutions become simpler element-wise multiplications in the Fourier domain. Specifically, for $S^{\\dot{2}}$ , the process uses vectors of spherical harmonic coefficients, forming a block diagonal matrix analogous to $S O(3)$ convolution. Both convolutions on $S^{2}$ and $S O(3)$ generate output signals that reside on $S O(3)$ . ", "page_idx": 3}, {"type": "text", "text": "4 SO(3)-Equivariant Pose Harmonics Predictor ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The goal of our network is to accurately predict the SO(3) pose of an object in an image. To achieve, we employ spherical CNNs [11] to obtain SO(3)-equivariant representation, and our model is trained with frequency-domain supervision using Wigner-D coefficients. This approach enhances data efficiency by capturing patterns with fewer training samples and ensures precise SO(3) pose estimation by aligning the parametrization of 3D rotations with the Wigner-D matrices in the frequency domain. ", "page_idx": 3}, {"type": "text", "text": "Figure 2 provides an overview of our SO(3)-equivariant pose estimation network. In Sec. 4.1, we explain the steps for obtaining the Wigner-D representation, following the method described in [35]. In Sec. 4.2, we introduce a frequency-domain regression loss, where we train the network using MSE loss between the predicted representation and the ground truth (GT) Wigner-D coefficients. Finally, in Sec. 4.3, we describe the inference process by constructing an SO(3) grid for evaluation. ", "page_idx": 3}, {"type": "text", "text": "4.1 SO(3)-Equivariant Pose Estimation Network ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this subsection, we explain our SO(3)-equivariant pose estimation network, highlighting that its key components are shared with the architecture of [35]. ", "page_idx": 3}, {"type": "text", "text": "Image feature extraction. We first apply a feature extractor to obtain an image feature map that encodes semantic and geometric information: $F\\,=\\,\\rho(I)$ , where $F\\,\\in\\,\\mathbb{R}^{C\\times H^{\\prime}\\times W^{\\prime}}$ and $\\rho$ denotes ResNet. We utilize a ResNet feature extractor that is pre-trained on ImageNet same to [28, 35, 41, 48, 49, 71]. We then perform dimensionality reduction on the image feature $F$ to match the input dimension of the subsequent spherical feature using a 1x1 convolution: $F^{\\prime}=\\operatorname{Conv}_{1\\times1}(F)$ , where F \u2032 \u2208 C\u2032\u00d7H\u2032\u00d7W \u2032. ", "page_idx": 4}, {"type": "text", "text": "Spherical mapper. To begin, we lift the image features to the 2-sphere using orthographic projection [35]. This involves mapping the 2D feature $F^{\\prime}$ to a spherical feature $\\bar{\\psi}\\,\\in\\,\\mathbb{R}^{C^{\\prime}\\times p}$ , where $p$ denotes the number of points on the sphere. The orthographic projection links pixels in the image space to points on the sphere by orthogonally mapping $\\bar{S^{2}}$ coordinates to the image plane, thereby preserving the spatial information of the dense 2D feature map. ", "page_idx": 4}, {"type": "text", "text": "Initially, we model spherical coordinates using an $S^{2}$ HEALPix [24] grid over a hemisphere. Within this hemisphere, the set $\\{x_{i}\\}\\subset{\\bar{S^{2}}}$ represents the vertices of the grid. Each vertex $x_{i}$ is mapped to a position $P(x_{i})$ on the image plane. Formally, the orthographic projection $P$ maps 3D coordinates on the hemisphere to 2D coordinates on the image plane as $\\bar{P}(x,y,z)=(x,y)$ . ", "page_idx": 4}, {"type": "text", "text": "Due to the fixed perspective, only one hemisphere of the sphere is visible, resulting in a localized signal $\\psi(x)\\,=\\,F^{\\prime}(P(x))$ supported over this hemisphere. The value of $\\psi(x_{i})$ is ob", "page_idx": 4}, {"type": "image", "img_path": "nw8cXoNvep/tmp/5a7a3f37ae2dbe97044c1414c84c29417d2299de17defdd6a7d129e1e114d784.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Illustration of spherical mapper and spherical convolution for SO(3)-equivariance. This structure allows for the prediction of 3D rotations while preserving the SO(3)-equivariance of the input structure. Predicting the Wigner-D harmonics $\\Psi$ enables continuous 3D rotation modeling, without discretizing the group actions.1 ", "page_idx": 4}, {"type": "text", "text": "tained by interpolating $F^{\\prime}$ at the pixels near $P(x_{i})$ in the image space using an interpolation function $\\eta$ , so $\\dot{\\psi(x_{i})}\\,\\bar{=\\,\\eta(F^{\\prime},\\bar{P(x_{i})})}$ . Figure 3 illustrates the processes of the spherical mapper, and the following frequency domain conversion and spherical convolution for SO(3)-equivariance. ", "page_idx": 4}, {"type": "text", "text": "Convert to the frequency domain. The transition of the spherical feature $\\psi$ into the frequency domain is achieved using the fast Fourier transform (FFT) adapted for spherical topology. By employing the FFT, we efficiently convert $\\psi$ to spherical signals $\\boldsymbol{S}$ , represented as a sum of spherical harmonics. This transformation allows us to capture and manipulate the spatial frequencies inherent to the spherical surface. Specifically, the transition to the frequency domain enables the derivation of Wigner-D coefficients, which effectively model the $S O(3)$ . The Fourier series of $\\boldsymbol{S}$ is truncated at frequency $L$ [11], expressed as: $\\begin{array}{r}{S(x)\\approx\\sum_{l=0}^{L}\\sum_{m=-l}^{l}c_{m}^{l}Y_{m}^{l}(x)}\\end{array}$ , where $\\boldsymbol{S}\\in\\mathbb{R}^{C^{\\prime}\\times N}$ , $N$ is the total number of spherical harmonics determined by the maximum frequency $L$ , and $Y_{m}^{l}(x)$ are the spherical harmonics. Operating in the frequency domain facilitates the effective convolution of signals on the sphere $(S^{2})$ and within the 3D rotation group $\\left(S O(3)\\right)$ ), preserving the geometric properties of input features through spherical equivariance. ", "page_idx": 4}, {"type": "text", "text": "To address sampling errors from approximating the Fourier series via truncation, we apply two techniques proposed in [35]. First, to prevent discontinuities on the 2-sphere, we gradually decrease the magnitude of projected features near the image edge: $\\psi^{\\prime}(x_{i})=w({\\bar{x}}_{i})\\cdot\\psi(x_{i})$ . Second, for each projection, we randomly select a subset of grid points on the $S^{2}$ HEALPix grid as a dropout. ", "page_idx": 4}, {"type": "text", "text": "Spherical convolution for SO(3)-equivariance. We aim to predict 3D rotations while preserving SO(3)-equivariance using the projected features on sphere. First, the spherical signal $\\boldsymbol{S}$ is processed with an ${\\bar{S}}^{2}$ -equivariant convolutional layer [11, 35]. Unlike conventional convolutions with local filters, $S^{2}$ convolution uses globally supported filters, offering a global receptive field. This allows for a shallower network, which is important due to the high computational and memory demands of spherical convolutions at a high bandlimit $L$ . ", "page_idx": 4}, {"type": "text", "text": "In this stage, we obtain SO(3) representations inherent to spherical CNNs [11]. The output of $S^{2}$ convolutions lies in the SO(3) domain because $S^{2}$ convolutions replace translations with rotations, and the space of 3D rotations forms the SO(3) group. Consequently, we obtain feature results sized in $\\mathbb{R}^{C^{\\prime\\prime}\\times M}$ , where $C^{\\prime\\prime}$ is the hidden dimension of the SO(3) features, and $M$ is the total number of Wigner-D matrix coefficients, given by $\\begin{array}{r}{M=\\sum_{l=0}^{L}(2l+1)\\times(2l+1)}\\end{array}$ , created by SO(3) irreps. We apply non-linearities between convolutional layers by transforming the signal to the spatial domain, applying a ReLU, and then transforming back to the frequency domain, following the approach of spherical CNNs [11]. This method can be extended to FFT-based approximate non-linearity [19] and equivariant non-linearity for tensor field networks [52, 67]. ", "page_idx": 5}, {"type": "text", "text": "Subsequent to the $S^{2}$ -equivariant convolutional layer, we perform an SO(3)-equivariant group convolution [11, 35] using a locally supported filter to refine the SO(3) pose space. Unlike typical spherical CNNs, we bypass the inverse fast Fourier transform (iFFT) and instead use the output harmonics of Wigner-D prediction. This approach, unlike that of [35], improves the efficiency of our method. The final output of the equivariant network is the Wigner-D matrix coefficients $\\Psi\\in\\mathbb{R}^{M}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Frequency-Domain Regression Loss ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The output of the SO(3)-equivariant convolutional layers is a linear combination of Wigner-D matrices, represented as a flattened vector of the Wigner-D coefficients. The output $\\Psi$ indicates specific object orientations in an image. To generate the ground-truth (GT) Wigner-D coefficients, we convert the GT 3D rotations from Euler angles using the $Z Y Z$ sequence of rotation $R$ , expressed as $R=R_{z}(\\gamma)R_{y}(\\beta)R_{z}(\\alpha)$ to the Wigner-D matrices $\\bar{D}_{m n}^{l}(\\alpha,\\beta,\\gamma)$ , where $D$ represents an action of the rotation group SO(3). We calculate the Mean Squared Error (MSE) loss as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\hat{\\Psi},\\Psi^{\\mathrm{GT}})=\\sum_{l=0}^{L}\\sum_{m=-l}^{l}w_{l}(\\hat{\\Psi}_{l m}-\\Psi_{l m}^{\\mathrm{GT}})^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $w_{l}$ are weights assigned to each harmonic frequency level $l$ , normalizing the output Wigner$\\mathrm{D}$ matrices for a frequency-domain specific MSE loss. This loss function enables continuous prediction of SO(3) poses using SO(3)-equivariant networks, whereas the previous methods [28, 35, 49] predicted outputs in a discretized distribution, leading to degradation in prediction precision. With this re-parametrization in the frequency domain, we use Euclidean distance because it is simple yet effective for pose prediction. It allows straightforward calculation while considering both the direction and magnitude of the vectors. Many distance metrics defined in the spatial domain [2, 25, 30, 55] may not be directly appropriate for the frequency domain without adaptation. For example, cosine and angular distances ignore magnitude, where the amplitude of frequency components carries significant information. Chordal and geodesic distances require normalization, can be less intuitive, and often involve more complex computations. ", "page_idx": 5}, {"type": "text", "text": "4.3 Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For evaluation, the output Wigner-D representation $\\Psi$ is converted to an SO(3) pose in the spatial domain. Figure 4 illustrates the inference process inspired by [35, 49]. Specifically, we map the predicted Wigner-D coefficients $\\hat{\\Psi}$ from the frequency domain to a $3\\mathrm{x}3$ rotation matrix $R$ by querying $\\hat{\\Psi}$ on a predefined SO(3) grid. To achieve this mapping, we calculate the similarities between the output vector $\\Psi$ and the SO(3) grid $P(\\cdot\\mid I)$ . These similarities are then normalized using a softmax function to produce a non-parametric categorical distribution $P(R\\mid I)$ . The final 3D rotation matrix $\\hat{R}$ is determined either by taking the argmax of this distribution or by applying gradient ascent [49]. ", "page_idx": 5}, {"type": "text", "text": "To generate the SO(3) equivolumetric grids, we utilize the hierarchical equal area isolatitude pixelation of the sphere (HEALPix) [24, 26], consistent with methods used in [28, 35, 41, 49, 71]. To lift the $S^{2}$ HEALPix to $S O(3)$ HEALPix, we create equal-area grids on the 2-sphere and cover $S O(3)$ by threading great circles through each point using the Hopf fibration from [68]. ", "page_idx": 5}, {"type": "text", "text": "This inference scheme effectively models objects with ambiguous orientations or symmetries by employing multiple hypotheses, thereby overcoming the limitations of single-modality predictions [44]. In addition to joint training with distributional cross-entropy loss [49], our network can model the non-parametric and multi-modal distribution in pose space to address pose ambiguity and aid in modeling 3D symmetry. ", "page_idx": 5}, {"type": "image", "img_path": "nw8cXoNvep/tmp/09da121f7bd117a7af7c37ca4f772978d4e61e32cc4d0646ddab4367977ed41d.jpg", "img_caption": ["Figure 4: Inference time. We query the output vector of Wigner-D coefficients $\\Psi$ against the predefined SO(3) HEALPix grid with a resolution of $Q$ points. We finally obtain the SO(3) probability distribution $P(R\\mid I)$ , where each position represents the probability of a specific SO(3) pose.2 "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Implementation Details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We input a 2D RGB image $I\\in\\mathbb{R}^{3\\times224\\times224}$ . A ResNet backbone, pretrained on ImageNet, extracts feature maps of shape $F\\stackrel{=}{\\in}\\mathbb{R}^{2048\\times7\\times7}$ . We then perform dimension reduction using a 1x1 convolution to obtain $\\dot{F}^{\\prime}\\in\\mathbb{R}^{51\\dot{2}\\times7\\times7}$ . In the spherical mapper, the features are mapped onto an $S^{2}$ grid generated by recursion level 2 of HEALPix on half of the sphere, and then sampled at 20 points, resulting in $\\bar{\\psi}\\in\\mathbb{R}^{512\\times20}$ . By converting $\\psi$ to the frequency domain, the spherical signals $\\mathcal{S}\\in\\mathbb{R}^{512\\times49}$ are obtained. The Wigner-D representation is implemented in a flattened form across different frequency levels. For example, the matrix coefficients at a frequency level $l$ are represented as a flattened vector of size $(2l+1)\\stackrel{.}{\\times}(2l+1)$ . We use a maximum frequency level of $L=6$ , resulting in a total size of $M=455$ , computed as $\\begin{array}{r}{\\sum_{l=0}^{6}(2l+1)\\times(2l+1)}\\end{array}$ . These coefficients are then flattened into a single vector for the Wigner-D prediction. The spherical convolution on the $S^{2}$ kernel uses an 8-dimensional hidden layer with global support to obtain intermediate SO(3) features in $\\mathbb{R}^{8\\times455}$ . After nonlinear activation, we finally obtain the 1-dimensional output $\\Psi\\in\\mathbb{R}^{1\\times455}$ using an SO(3) convolution with a locally supported fliter to handle rotations up to $22.5^{\\circ}$ . At inference, we employ a recursive level 5 of SO(3) HEALPix grid with 2.36 million points, achieving a precision of $1.875^{\\circ}$ , as in [28, 35]. ", "page_idx": 6}, {"type": "text", "text": "5.2 Benchmarks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "ModelNet10-SO(3) [39] is a common dataset for estimating a 3D rotation from a single image. The images are created by rendering CAD models from the ModelNet10 dataset [63]. The dataset includes 4,899 objects across 10 categories, each image is labelled with a single 3D rotation. The rotations are uniformly sampled from each CAD model. From a single CAD model, the training set comprises 100 3D rotations on SO(3), while the test set includes 4 unseen 3D rotations. ", "page_idx": 6}, {"type": "text", "text": "PASCAL3D $^+$ [64] is a widely-used benchmark for evaluating pose estimation in images captured in real-world settings. It includes 12 categories of everyday objects, which were created by manually aligning 3D models with their corresponding 2D images. This dataset presents challenges due to the significant variation in object appearances, the high variability of natural textures, and the presence of novel object instances in the test set. To be consistent with the baselines, we conduct training data augmentation using synthetic renderings [57]. ", "page_idx": 6}, {"type": "text", "text": "ModelNet10-SO(3) Few-shot Views is used to evaluate the data efficiency of pose estimation models. Unlike the original ModelNet10-SO(3) [39], we have expanded this to evaluate various amounts of training data, by setting the number of training views per CAD model to 3, 5, 10, 20, 30, 40, 50, 70, 90, and 100. This benchmark verifies the sampling efficiency of our equivariant networks. We use the same test dataset as that of ModelNet10-SO(3). ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. We calculate the angular error, measured in degrees using geodesic distance, between the network-predicted SO(3) pose and the ground-truth rotation matrix: $\\theta_{\\mathrm{Error}}(R,{\\hat{R}})=$ $\\scriptstyle\\cos^{-1}\\ \\left({\\frac{\\operatorname{trace}(\\Delta R)-1}{2}}\\right)$ , and $\\Delta R=R{\\hat{R}}^{T}$ . We adopt two commonly used metrics: the median rotation error (MedErr) and the accuracy within specific rotation error thresholds $\\operatorname{Acc}\\!\\bigcirc15^{\\circ}$ and $\\operatorname{Acc}\\!\\left(\\!\\omega30^{\\circ}\\right)$ ). ", "page_idx": 6}, {"type": "table", "img_path": "nw8cXoNvep/tmp/5e92b15f7755de81b8ed0ab6db69b35ec037b666ebe1fde516e17e27c814f148.jpg", "table_caption": ["Table 1: Results on ModelNet10-SO(3). The scores have been averaged across all ten object categories. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "nw8cXoNvep/tmp/84f9e76a3d93ccb1a73a3d23184c1c841c469a9909aefa535d7ceeca4c77d32a.jpg", "table_caption": [], "table_footnote": ["Table 2: Results on PASCAL3D+ with ResNet-101 backbone. Scores are averaged across all twelve classes. "], "page_idx": 7}, {"type": "image", "img_path": "nw8cXoNvep/tmp/c97525bf78c4b2085b8286eb156b33d19cbb10c24c8f04633ff9de1c1b0fe4ba.jpg", "img_caption": ["Figure 5: Experiment on ModelNet10-SO(3) with few-shot training views. Results with solid lines of I-PDF [49], I2S [35], and RotLaplace [71] denote to a ResNet-50 backbone, while dotted lines indicate a ResNet-101 backbone. Our method outperforms all metrics and reduces training views. Baseline results [35, 71] were obtained using the source code provided by the authors. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.3 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Table 1 shows the pose estimation results on the ModelNet10-SO(3) dataset, where our model outperforms all baselines across multiple evaluation metrics. Notably, our Wigner-D harmonics prediction network surpasses methods in non-probabilistic rotation estimation [3, 39, 76], parametric probability distribution estimation [13, 41, 48, 54, 70, 71], and non-parametric distribution prediction [28, 35, 49], by leveraging SO(3) equivariance and rotation parametrization in the frequency domain. ", "page_idx": 7}, {"type": "text", "text": "Table 2 shows the results on the PASCAL3D $^+$ benchmark. Our SO(3) Wigner-D harmonics predictor achieves state-of-the-art performance on these challenging benchmarks. It demonstrates robustness to changes in object appearance, real textures, and generalizes well to novel object instances. We additionally report fine-scale accuracies, i.e., $\\operatorname{Acc}\\!\\left(\\!\\omega3^{\\circ}\\right)$ , Acc $@5^{\\circ}$ and $\\operatorname{Acc}(\\!\\omega\\!\\operatorname{10^{\\circ}}$ , in Tables A1 and A2, and class-wise evaluation of the results in Tables A3 and A4 of appendix B. ", "page_idx": 7}, {"type": "text", "text": "5.4 Few-shot Training Views ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 5 illustrates the pose estimation results on ModelNet10-SO(3) with few-shot training views. Notably, as the number of training views from a single CAD model decreases, our model consistently achieves the highest accuracy and lowest error. This performance surpasses the baselines of direct rotation regression [3, 76], parametric distribution parameters regression [71], and non-parametric distribution estimation [35, 49]. This few-shot training experiment verifies that our SO(3)-equivariant model contributes to superior data efficiency and generalization to unseen rotations. ", "page_idx": 7}, {"type": "table", "img_path": "nw8cXoNvep/tmp/637dbc17336e7d45d52173238ef9f2a02849dd56ec8116a2bd20e78acd341743.jpg", "table_caption": ["Table 3: Comparison of different parametrizations of 3D rotations. To validate our Wigner-D representation in the frequency domain, we train using various output rotation representations. "], "table_footnote": ["Table 4: Comparison of different loss functions. To validate our choice of MSE loss, we experiment with various distance functions between the predicted output and the ground truth. "], "page_idx": 8}, {"type": "text", "text": "5.5 Ablation Studies & Design Choices ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "5.5.1 SO(3) Parmetrizations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 3 shows results validating design choices on ModelNet10-SO(3) with 20-shot training views, using a ResNet-50 backbone. First, we compare the effects of different rotation parametrizations on model performance by changing the prediction head and ground-truth rotations, to verify our proposed Wigner-D compared to other rotation representations. For all cases, we retained the backbone networks and SO(3)-equivariant layers. The only modifications were the output prediction dimension size and the ground-truth rotation representation. Our Wigner-D parametrization outperforms Euler angles (3 dim.), quaternions (4 dim.), axis-angle (4 dim.), and rotation matrices (9 dim.). This demonstrates that frequency domain rotation re-parametrization enables accurate 3D rotations when used with the SO(3)-equivariant spherical CNNs in the frequency domain. ", "page_idx": 8}, {"type": "text", "text": "5.5.2 Loss Functions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Table 4 compares various loss functions trained on ModelNet10-SO(3) with 20-shot learning using a ResNet-50 backbone. While Huber and L1 losses are alternatives, they do not perform as well as MSE in our context. Cosine loss measures only angle distances between vectors, ignoring magnitude, which is an essential factor in frequency-domain applications. Geodesic loss in the frequency domain is ineffective because it requires separate calculations for each frequency level of the Wigner-D matrix, potentially losing the precision of the original 3D rotation, as we truncate the Fourier basis at a frequency level of 6. Therefore, we choose MSE regression loss for our design choice given its simplicity and effectiveness. ", "page_idx": 8}, {"type": "text", "text": "5.5.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Table 5, we experiment with replacing SO(3)- equivariant layers with conventional convolutional layers. Specifically, we use two-layer 1x1 convolutional layers with ReLU activation and a final linear layer with 455 output channels. The results indicate that CNNs without equivariant layers perform poorly, especially in terms of median error, suggesting that using the equivariant networks generalize better to unseen samples. Additionally, the Wigner-D prediction should be paired with an SO(3)-equivariant network to enable reliable 3D rotation prediction in the frequency domain. ", "page_idx": 8}, {"type": "text", "text": "Lastly, we evaluate the impact of different SO(3) grids by switching from a HEALPix grid to a ", "page_idx": 8}, {"type": "table", "img_path": "nw8cXoNvep/tmp/cb1ef4c4579277a3f60ed9f7a4ed61f758a2c488fa6e326416977499b512005e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 5: Comparison of results without the SO(3)-equivariant module and with different SO(3) grids at inference. The first group shows the results using conventional convolution instead of equivariant convolution. The second group presents the results with different SO(3) grids at inference time. \u2018ours\u2019 denotes the proposed model architecture. ", "page_idx": 8}, {"type": "text", "text": "random SO(3) grid and super-Fibonacci spirals [1], which use the same number of SO(3) rotations. Our Wigner-D harmonics predictor performs consistently, regardless of the SO(3) grid sampling type at inference time. ", "page_idx": 8}, {"type": "table", "img_path": "nw8cXoNvep/tmp/46f3535f363c14bf501dc481e0e2c8edaf51c843eda51976ae0ca489b3d859b2.jpg", "table_caption": ["Table 6: Results on SYMSOL I and II [49]. We report the average log likelihood on both parts of the SYMSOL datasets. Lwigner denotes the results obtained with our Wigner-D regression loss. $\\mathcal{L}_{\\mathrm{dist}}$ denotes the results using the distribution loss from I-PDF [49], which are the same as the results of I2S [35]. The third row presents the results of joint training using both our regression loss and the distribution loss. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.6 Results on SYMSOL ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Table 6 shows symmetric object modeling on the SYMSOL datasets [49]. Compared to the first row and second row [35], our model with only the Wigner-D regression loss derives on sharp modalities, which can be less effective than [35] for symmetric objects in SYMSOL I. ", "page_idx": 9}, {"type": "text", "text": "For clearly defined pose cases (e.g., SphereX in SYMSOL II), our Wigner-D loss alone performs well. However, in other SYMSOL II scenarios, the sharp distributions produced by our model can lead to low average log likelihood scores. This metric is particularly harsh on models with sharp peaks, making them vulnerable to very low scores in some failure cases. ", "page_idx": 9}, {"type": "text", "text": "In the third row, joint training of our method with the distribution loss [35, 49] achieves better performance than the baseline [35], demonstrating its ability to model symmetric objects. These results highlight the potential of our method in handling complex symmetries and predicting multiple hypotheses. Figure 6 shows the visualization of pose distribution on the SYMSOL I and II datasets. ", "page_idx": 9}, {"type": "text", "text": "Most real-world objects have unique, unambiguous poses, validating our single pose regression method (e.g., ModelNet10-SO(3), PAS$\\mathrm{CAL3D+})$ ). If the task needs to cover symmetric cases, our model can be modeled with distribution loss [35, 49]. ", "page_idx": 9}, {"type": "image", "img_path": "nw8cXoNvep/tmp/c227b3bcce2b124afef156af3923c598c8cff73c956cd69d899f1b24b529704a.jpg", "img_caption": ["Figure 6: Visualization of pose distribution on SYMSOL. The results are obtained by joint training with both our regression loss and the crossentropy distribution loss [35, 49]. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proposed a novel method for 3D rotation estimation by predicting Wigner-D coefficients directly in the frequency domain using SO(3)-equivariant networks. Our approach effectively overcomes the limitations of existing spatial domain parameterizations of 3D rotations, such as discontinuities and singularities, by aligning the rotation representation with the operations of spherical CNNs. By leveraging frequency-domain regression, our method ensures continuous and precise pose predictions and demonstrates state-of-the-art performance across benchmarks like ModelNet10-SO(3) and PASCAL3D $^+$ . Additionally, it offers enhanced data efficiency and generalization to unseen rotations, validating the robustness of SO(3)-equivariant architectures. Our method also supports the modeling of 3D symmetric objects by capturing rotational ambiguities, with further accuracy improvements achievable through joint training with distribution loss. Future work can build on this foundation to explore frequency-domain representations in 3D vision tasks, develop more effective rotation representations for 3D space, and further optimize computational efficiency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by IITP grants (RS-2022-II220959: Few-Shot Learning of Causal Inference in Vision and Language for Decision Making $(50\\%)$ , RS-2022-II220290: Visual Intelligence for Space-Time Understanding and Generation based on Multi-layered Visual Common Sense $(45\\%)$ , RS-2019-II191906: AI Graduate School Program at POSTECH $(5\\%)$ ) funded by Ministry of Science and ICT, Korea. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Marc Alexa. Super-fibonacci spirals: Fast, low-discrepancy sampling of so (3). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8291\u20138300, 2022. 9 [2] Olaya Alvarez-Tunon, Yury Brodskiy, and Erdal Kayacan. Loss it right: Euclidean and riemannian metrics in learning-based visual odometry. In ISR Europe 2023; 56th International Symposium on Robotics, pages   \n107\u2013111. VDE, 2023. 6 [3] Romain Br\u00e9gier. Deep regression on manifolds: a 3d rotation case study. In 2021 International Conference on $3D$ Vision $(3D V)$ , pages 166\u2013174. IEEE, 2021. 2, 8, 1, 5, 9 [4] Michel Breyer, Jen Jen Chung, Lionel Ott, Roland Siegwart, and Juan Nieto. Volumetric grasping network: Real-time 6 dof grasp detection in clutter. In Conference on Robot Learning, pages 1602\u20131611. PMLR,   \n2021. 1 [5] Mai Bui, Tolga Birdal, Haowen Deng, Shadi Albarqouni, Leonidas Guibas, Slobodan Ilic, and Nassir Navab. 6d camera relocalization in ambiguous scenes via continuous multimodal inference. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVIII 16, pages 139\u2013157. Springer, 2020. 1, 2 [6] Ruojin Cai, Bharath Hariharan, Noah Snavely, and Hadar Averbuch-Elor. Extreme rotation estimation using dense correlation volumes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14566\u201314575, 2021. 2 [7] Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas J Guibas, and He Wang. Projective manifold gradient layer for deep rotation regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6646\u20136655, 2022. 2, 1, 9 [8] Kefan Chen, Noah Snavely, and Ameesh Makadia. Wide-baseline relative camera pose estimation with directional learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3258\u20133268, 2021. 2, 1 [9] OJ Cobb, CGR Wallis, AN Mavor-Parker, A Marignier, MA Price, M D\u2019Avezac, and JD McEwen. Efficient generalized spherical cnns. In ICLR 2021-9th International Conference on Learning Representations, volume 9. ICLR 2021-9th International Conference on Learning Representations, 2021. 2 [10] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International conference on machine learning, pages 2990\u20132999. PMLR, 2016. 2, 3 [11] Taco S Cohen, Mario Geiger, Jonas K\u00f6hler, and Max Welling. Spherical cnns. In International Conference on Learning Representations, 2018. 2, 4, 5, 6 [12] Micha\u00ebl Defferrard, Martino Milani, Fr\u00e9d\u00e9rick Gusset, and Nathana\u00ebl Perraudin. Deepsphere: a graphbased spherical cnn. In International Conference on Learning Representations, 2019. 2, 4 [13] Haowen Deng, Mai Bui, Nassir Navab, Leonidas Guibas, Slobodan Ilic, and Tolga Birdal. Deep bingham networks: Dealing with uncertainty and ambiguity in pose estimation. International Journal of Computer Vision, 130(7):1627\u20131654, 2022. 2, 8, 3, 5, 9 [14] Thanh-Toan Do, Ming Cai, Trung Pham, and Ian Reid. Deep-6dpose: Recovering 6d object pose from a single rgb image. arXiv preprint arXiv:1802.10367, 2018. 2 [15] James R Driscoll and Dennis M Healy. Computing fourier transforms and convolutions on the 2-sphere. Advances in applied mathematics, 15(2):202\u2013250, 1994. 4 [16] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning so (3) equivariant representations with spherical cnns. In Proceedings of the European Conference on Computer Vision (ECCV), pages 52\u201368, 2018. 2 [17] Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis. Spin-weighted spherical cnns. Advances in Neural Information Processing Systems, 33:8614\u20138625, 2020. [18] Carlos Esteves, Jean-Jacques Slotine, and Ameesh Makadia. Scaling spherical cnns. In International Conference on Machine Learning, pages 9396\u20139411. PMLR, 2023. 2 [19] Daniel Franzen and Michael Wand. General nonlinearities in so (2)-equivariant cnns. Advances in Neural Information Processing Systems, 34:9086\u20139098, 2021. 6 [20] Ge Gao, Mikko Lauri, Jianwei Zhang, and Simone Frintrop. Occlusion resistant object rotation regression from point cloud segments. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 0\u20130, 2018. 2 [21] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231\u20131237, 2013. 1 [22] Mario Geiger, Tess Smidt, Alby M., Benjamin Kurt Miller, Wouter Boomsma, Bradley Dice, Kostiantyn Lapchevskyi, Maurice Weiler, Micha\u0142 Tyszkiewicz, Simon Batzner, Dylan Madisetti, Martin Uhrin, Jes Frellsen, Nuri Jung, Sophia Sanborn, Mingjian Wen, Josh Rackers, Marcel R\u00f8d, and Michael Bailey. Euclidean neural networks: e3nn, Apr. 2022. 8   \n[23] Igor Gilitschenski, Roshni Sahoo, Wilko Schwarting, Alexander Amini, Sertac Karaman, and Daniela Rus. Deep orientation uncertainty learning based on a bingham loss. In International conference on learning representations, 2019. 2   \n[24] Krzysztof M Gorski, Eric Hivon, Anthony J Banday, Benjamin D Wandelt, Frode K Hansen, Mstvos Reinecke, and Matthia Bartelmann. Healpix: A framework for high-resolution discretization and fast analysis of data distributed on the sphere. The Astrophysical Journal, 622(2):759, 2005. 5, 6, 8   \n[25] Richard Hartley, Jochen Trumpf, Yuchao Dai, and Hongdong Li. Rotation averaging. International journal of computer vision, 103:267\u2013305, 2013. 6   \n[26] HEALPix Collaboration. Healpix: Hierarchical equal area isolatitude pixelation, n.d. Accessed: 2024-10- 22. 6   \n[27] Timon H\u00f6fer, Benjamin Kiefer, Martin Messmer, and Andreas Zell. Hyperposepdf-hypernetworks predicting the probability distribution on so (3). In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2369\u20132379, 2023. 2   \n[28] Owen Howell, David Klee, Ondrej Biza, Linfeng Zhao, and Robin Walters. Equivariant single view pose prediction via induced and restriction representations. Advances in Neural Information Processing Systems, 36, 2023. 2, 3, 5, 6, 7, 8, 9   \n[29] Jiahui Huang, He Wang, Tolga Birdal, Minhyuk Sung, Federica Arrigoni, Shi-Min Hu, and Leonidas J Guibas. Multibodysync: Multi-body segmentation and motion estimation via 3d scan synchronization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7108\u20137118, 2021. 2   \n[30] Du Q Huynh. Metrics for 3d rotations: Comparison and analysis. Journal of Mathematical Imaging and Vision, 35:155\u2013164, 2009. 6   \n[31] Alex Kendall and Roberto Cipolla. Geometric loss functions for camera pose regression with deep learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5974\u20135983, 2017. 2   \n[32] Alex Kendall, Matthew Grimes, and Roberto Cipolla. Posenet: A convolutional network for real-time 6-dof camera relocalization. In Proceedings of the IEEE international conference on computer vision, pages 2938\u20132946, 2015. 2   \n[33] CG Khatri and Kanti V Mardia. The von mises\u2013fisher matrix distribution in orientation statistics. Journal of the Royal Statistical Society Series B: Statistical Methodology, 39(1):95\u2013106, 1977. 2   \n[34] David Klee, Ondrej Biza, Robert Platt, and Robin Walters. Image to icosahedral projection for so (3) object reasoning from single-view images. In NeurIPS Workshop on Symmetry and Geometry in Neural Representations, pages 64\u201380. PMLR, 2023. 2   \n[35] David Klee, Ondrej Biza, Robert Platt, and Robin Walters. Image to sphere: Learning equivariant features for efficient pose prediction. ICLR, 2023. 2, 4, 5, 6, 7, 8, 10, 3, 9, 13   \n[36] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch\u2013gordan nets: a fully fourier space spherical convolutional neural network. Advances in Neural Information Processing Systems, 31, 2018. 2   \n[37] Abhijit Kundu, Yin Li, and James M Rehg. 3d-rcnn: Instance-level 3d object reconstruction via renderand-compare. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3559\u20133568, 2018. 2   \n[38] Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo Kanazawa, Afshin Rostamizadeh, and Ameesh Makadia. An analysis of svd for deep rotation estimation. Advances in Neural Information Processing Systems, 33:22554\u201322565, 2020. 2, 1   \n[39] Shuai Liao, Efstratios Gavves, and Cees GM Snoek. Spherical regression: Learning viewpoints, surface normals and 3d rotations on n-spheres. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9759\u20139767, 2019. 7, 8, 2, 3, 5, 9   \n[40] Amy Lin, Jason Y Zhang, Deva Ramanan, and Shubham Tulsiani. Relpose++: Recovering 6d poses from sparse-view observations. In 2024 International Conference on 3D Vision (3DV), 2024. 2   \n[41] Yulin Liu, Haoran Liu, Yingda Yin, Yang Wang, Baoquan Chen, and He Wang. Delving into discrete normalizing flows on so (3) manifold for probabilistic rotation modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21264\u201321273, 2023. 2, 3, 5, 6, 8, 4, 9   \n[42] Siddharth Mahendran, Haider Ali, and Rene Vidal. A mixed classification-regression framework for 3d pose estimation from 2d images. arXiv preprint arXiv:1805.03225, 2018. 8, 3, 5, 9   \n[43] Ameesh Makadia and Kostas Daniilidis. Direct 3d-rotation estimation from spherical images via a generalized shift theorem. In 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings., volume 2, pages II\u2013217. IEEE, 2003. 3, 1   \n[44] Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7144\u20137153, 2019. 6   \n[45] Octave Mariotti and Hakan Bilen. Semi-supervised viewpoint estimation with geometry-aware conditional generation. In Computer Vision\u2013ECCV 2020 Workshops: Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 631\u2013647. Springer, 2020. 2   \n[46] Octave Mariotti, Oisin Mac Aodha, and Hakan Bilen. Viewnet: Unsupervised viewpoint estimation from conditional generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10418\u201310428, 2021. 2   \n[47] Rowan Thomas McAllister, Yarin Gal, Alex Kendall, Mark Van Der Wilk, Amar Shah, Roberto Cipolla, and Adrian Weller. Concrete problems for autonomous vehicle safety: Advantages of bayesian deep learning. International Joint Conferences on Artificial Intelligence, Inc., 2017. 1   \n[48] David Mohlin, Josephine Sullivan, and G\u00e9rald Bianchi. Probabilistic orientation estimation with matrix fisher distributions. Advances in Neural Information Processing Systems, 33:4884\u20134893, 2020. 2, 5, 8, 3, 9   \n[49] Kieran A Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, and Ameesh Makadia. Implicitpdf: Non-parametric representation of probability distributions on the rotation manifold. In International Conference on Machine Learning, pages 7882\u20137893. PMLR, 2021. 2, 5, 6, 8, 10, 3, 9   \n[50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 8   \n[51] Alberto Pepe, Joan Lasenby, and Pablo Chac\u00f3n. Learning rotations. Mathematical Methods in the Applied Sciences, 47(3):1204\u20131217, 2024. 1, 2, 3   \n[52] Adrien Poulenard and Leonidas J Guibas. A functional approach to rotation equivariant non-linearities for tensor field networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13174\u201313183, 2021. 6   \n[53] Matthew A. Price and Jason D. McEwen. Differentiable and accelerated spherical harmonic and wigner transforms. Journal of Computational Physics, submitted, 2023. 7   \n[54] Sergey Prokudin, Peter Gehler, and Sebastian Nowozin. Deep directional statistics: Pose estimation with uncertainty quantification. In Proceedings of the European conference on computer vision (ECCV), pages 534\u2013551, 2018. 2, 8, 3, 5, 9   \n[55] A Ren\u00e9 Geist, Jonas Frey, Mikel Zobro, Anna Levina, and Georg Martius. Learning with 3d rotations, a hitchhiker\u2019s guide to so (3). arXiv e-prints, pages arXiv\u20132404, 2024. 1, 2, 3, 6   \n[56] Paul-Edouard Sarlin, Mihai Dusmanu, Johannes L Sch\u00f6nberger, Pablo Speciale, Lukas Gruber, Viktor Larsson, Ondrej Miksik, and Marc Pollefeys. Lamar: Benchmarking localization and mapping for augmented reality. In European Conference on Computer Vision, pages 686\u2013704. Springer, 2022. 1   \n[57] Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. In Proceedings of the IEEE international conference on computer vision, pages 2686\u20132694, 2015. 2, 7   \n[58] Shubham Tulsiani and Jitendra Malik. Viewpoints and keypoints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1510\u20131519, 2015. 2, 8, 3, 5, 9   \n[59] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5038\u20135047, 2017. 2   \n[60] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2642\u20132651, 2019. 1   \n[61] Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9773\u20139783, 2023. 2   \n[62] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Br\u00e9gier, Yohann Cabon, Vaibhav Arora, Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and J\u00e9r\u00f4me Revaud. Croco: Self-supervised pretraining for 3d vision tasks by cross-view completion. Advances in Neural Information Processing Systems, 35:3502\u20133516, 2022. 6   \n[63] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912\u20131920, 2015. 7   \n[64] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond pascal: A benchmark for 3d object detection in the wild. In IEEE winter conference on applications of computer vision, pages 75\u201382. IEEE, 2014. 7   \n[65] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:1711.00199, 2017. 1   \n[66] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Robotics: Science and Systems XIV, 2018. 2   \n[67] Yinshuang Xu, Jiahui Lei, Edgar Dobriban, and Kostas Daniilidis. Unified fourier-based kernel and nonlinearity design for equivariant networks on homogeneous spaces. In International Conference on Machine Learning, pages 24596\u201324614. PMLR, 2022. 6   \n[68] Anna Yershova, Swati Jain, Steven M Lavalle, and Julie C Mitchell. Generating uniform incremental grids on so (3) using the hopf fibration. The International journal of robotics research, 29(7):801\u2013812, 2010. 6, 9   \n[69] Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, and Leonidas Guibas. Deep part induction from articulated object pairs. ACM Transactions on Graphics (TOG), 37(6):1\u201315, 2018. 2   \n[70] Yingda Yin, Yingcheng Cai, He Wang, and Baoquan Chen. Fishermatch: Semi-supervised rotation regression via entropy-based filtering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11164\u201311173, 2022. 2, 8, 9   \n[71] Yingda Yin, Yang Wang, He Wang, and Baoquan Chen. A laplace-inspired distribution on so(3) for probabilistic rotation estimation. In International Conference on Learning Representations (ICLR), 2023. 2, 5, 6, 8, 3, 4, 9   \n[72] Jason Y Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. In International Conference on Learning Representations (ICLR), 2024. 2, 7   \n[73] Jason Y Zhang, Deva Ramanan, and Shubham Tulsiani. Relpose: Predicting probabilistic relative rotation for single objects in the wild. In European Conference on Computer Vision, pages 592\u2013611. Springer, 2022. 2   \n[74] Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti, Leonidas Guibas, and Federico Tombari. Quaternion equivariant capsule networks for 3d point clouds. In European conference on computer vision, pages 1\u201319. Springer, 2020. 2   \n[75] Ellen D Zhong, Tristan Bepler, Joseph H Davis, and Bonnie Berger. Reconstructing continuous distributions of 3d protein structure from cryo-em images. In International Conference on Learning Representations, 2019. 1   \n[76] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5745\u20135753, 2019. 1, 2, 8, 5, 9   \n[77] Andrea Zonca, Leo Singer, Daniel Lenz, Martin Reinecke, Cyrille Rosset, Eric Hivon, and Krzysztof Gorski. healpy: equal area pixelization and spherical harmonics transforms for data on the sphere in python. Journal of Open Source Software, 4(35):1298, Mar. 2019. 8 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix / Supplementary Materials ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Rotation Representations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Rotation Representation in Spatial Domain ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We recommend checking out a detailed overview of learning 3D rotations in [51, 55]. Rotations in both 2D and 3D spaces can be represented using various mathematical frameworks, each with its own advantages and limitations, crucial for applications in fields such as computer graphics, robotics, and deep learning. In 2D space, rotation angles can be expressed as following SO(2) representations: angle $(\\alpha)$ or Trigonometric functions $(\\cos(\\bar{\\alpha}),\\sin(\\alpha))$ . ", "page_idx": 14}, {"type": "text", "text": "In 3D space, Euler angles in $\\mathbb{R}^{3}$ are a representative form of SO(3) representation. Euler angles have 3 DoF consisting of three angles $\\alpha,\\bar{\\beta},\\gamma\\in[-\\pi,\\pi)$ to describe a 3D rotation (roll, pitch, yaw). 3D rotation matrix can be composed of a fixed sequence rotation using the angles $R(\\alpha,\\beta,\\gamma)\\,=$ $R_{z}(\\alpha)R_{y}(\\beta)R_{z}(\\gamma)$ . The standard Euler angles can be divided into 2 forms: Tait-Bryan angles (a.k.a. Cardan angles) which consists of permutations of three items (XYZ, XZY, YXZ, YZX, ZXY, ZYX), and proper Euler angles which starts and ends with rotations around the same axis (ZYZ, ZXZ, XYX, XZX, YZY, YXY), total 12 possible unique sequences. Because of non-uniqueness of Euler angles, existing studies [3, 51, 76] do not encourage the Euler angles as an output representation for 3D rotation prediction in deep neural network. ", "page_idx": 14}, {"type": "text", "text": "Quaternions in $S^{3}$ is for 4-dimensional complex number to represent a 3D rotation. A quaternion $q$ is composed of one real part and three imaginary parts: $q=w+x i+y j+z k$ , where $w,x,y,z$ are real numbers and $i,j,k$ are the fundamental quaternion units. Quaternions can prevent gimbal lock, a problem that occurs with Euler angles where one degree of freedom is lost during 3D rotation. ", "page_idx": 14}, {"type": "text", "text": "Axis-angle in $\\mathbb{R}^{4}$ consists of an angle $\\theta$ and an axis vector $\\mathbf{u}$ in 3D space. To rotate a point $\\ensuremath{\\mathbf{p}}\\in\\mathbb{R}^{3}$ about the axis u by an angle $\\theta$ , you can use Rodrigues\u2019 rotation formula: $\\mathbf{p}^{\\prime}=\\mathbf{p}\\mathrm{cos}(\\theta)\\,+\\mathbf{u}\\times\\mathbf{p}\\mathrm{sin}(\\theta)\\,+$ $\\mathbf{u}(\\mathbf{u}\\cdot\\mathbf{p})(1-\\cos(\\theta))$ . The axis-angle representation can be converted into a rotation matrix or a quaternion. Even it has advantages of intuitive form and compact to describe 3D rotation with four parameters, the axis-angle can suffer from singularities in a scenario of angle multiples of $\\theta=2\\pi$ . ", "page_idx": 14}, {"type": "text", "text": "6D representation [3, 76] is a relatively newer concept compared to the traditional representation like Euler angles, rotation matrices and quaternions. A rotation is described by two 3D vectors that are orthogonal to each other, and the rotation matrix can be obtained by Gram-Schmidt orthonormalization (GSO). This representation can be directly predicted and optimized by deep networks because it avoids the non-linear constraints found in quaternions and rotation matrices. Quaternions require maintaining a unit norm, which introduces complexity in ensuring the quaternion remains normalized throughout the optimization process. Therefore, [7, 8, 76] adopt this 6D representation with GSO. ", "page_idx": 14}, {"type": "text", "text": "9D representation [3, 38] is direct parametrization of $3\\times3$ matrix, which can be projected to SO(3) using singular value decomposition (SVD). Predicting 9D representation for rotation matrices involves orthogonality constraints, meaning the rows and columns must remain orthonormal, and determinant constraints, where the determinant must equal $+1$ . These constraints complicate the learning and optimization process, making this representation more suitable for direct prediction and optimization by deep networks. Therefore, 9D representation is less common as a direct method to predict rotation, but [7, 8, 38] use this to mitigate issues associated with discontinuous parameterizations of pose. ", "page_idx": 14}, {"type": "text", "text": "A.2 Rotation Representation in Frequency Domain ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "3D rotation in the frequency domain is accomplished by manipulating spherical harmonics coefficients. Spherical harmonics $Y_{m}^{l}(\\overset{.}{x})=Y_{m}^{l}(\\theta,\\phi)$ are a function defined on the surface of a sphere, where $\\theta$ and $\\phi$ are the polar and azimuthal angles, respectively. Here, $l$ represents the degree of the spherical harmonics, $m$ is the order. To ensure computational feasibility, we truncate the degree of harmonics to a finite $L=l_{\\mathrm{max}}$ . Rotations of spherical functions can be represented by matrices that operate on the coefficients of their harmonics expansion. The rotation of spherical harmonics is expressed via the shift theorem [43] of spherical harmonics: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Lambda_{g}Y_{m}^{l}(x)=\\sum_{|n|\\leq l}D_{m n}^{l}(g)Y_{n}^{l}(x),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\Lambda_{g}Y_{m}^{l}(x)$ denotes the spherical harmonic $Y_{m}^{l}(x)$ after rotation by $g$ , and $\\Lambda_{g}$ is the rotation operator. Spherical harmonics $Y_{m}^{l}(x)$ , defined by degree $l$ and order $m$ $l\\,\\geq\\,0$ , $|m|\\,\\leq\\,l)$ , use $x$ to represent spherical coordinates. The matrix $U_{m n}^{l}(g)$ forms part of the irreducible unitary representation of $S O(3)$ , showing how each harmonic is transformed under rotation. The sum over all orders $n$ from $-l$ to $l,\\sum_{|n|\\leq l}$ , shows that $Y_{m}^{l}$ is a linear combination of all harmonics of degree $l$ ", "page_idx": 15}, {"type": "text", "text": "This rotation of spherical harmonics can be described by the Wigner-D matrix $D_{m n}^{l}(R)$ , which is a unitary matrix that describes the effect of a rotation $R$ on the spherical harmonics basis functions. We can rewrite $U_{m n}^{l}(g)=D_{m n}^{l}(\\alpha,\\beta,\\gamma)$ . Each element of the matrix represents the amplitude and phase shift that a spherical harmonic $Y_{m}^{l}$ undergoes due to the rotation $R$ . The rotation $R$ can be specified by Euler angles $\\alpha,\\beta$ , and $\\gamma$ in the ZYZ-axes configuration. The matrix elements $D_{m n}^{l}(\\alpha,\\beta,\\gamma)$ can be explicitly expressed as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{m n}^{l}(\\alpha,\\beta,\\gamma)=e^{-i m\\alpha}d_{m n}^{l}(\\beta)e^{-i n\\gamma},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $d_{m n}^{l}(\\beta)$ are the elements of the Wigner (small) d-matrix, which depend only on the angle $\\beta$ and are real-valued. These elements capture the intermediate rotation about the $\\mathrm{\\bfY}.$ -axis, where the angle $\\beta$ represents the tilt of the axis of rotation. To rotate a spherical harmonic expansion of a function $f$ , represented as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\theta,\\phi)=\\sum_{l=0}^{\\infty}\\sum_{m=-l}^{l}f_{m}^{l}Y_{m}^{l}(\\theta,\\phi),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we need to account for the coefficients $f_{m}^{l}$ of the expansion. The rotated coefficients $f_{m}^{\\prime l}$ are computed: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{m}^{\\prime l}=\\sum_{n=-l}^{l}D_{m n}^{l}(R)f_{n}^{l},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $D_{m n}^{l}(R)$ encodes the effect of the rotation on the original coefficients. This transformation preserves the orthonormality and completeness of the spherical harmonics basis, ensuring that the rotated function $f^{\\prime}(\\theta,\\phi)$ remains a valid representation of the original function $f(\\theta,\\phi)$ under the rotation $R$ . The expansion coefficients $f_{m}^{l}$ can be calculated using the original spatial coordinates $(\\theta,\\phi)$ of the sphere surface according to the spherical harmonic expansion: ", "page_idx": 15}, {"type": "equation", "text": "$$\nf_{m}^{l}=\\int_{0}^{2\\pi}\\int_{0}^{\\pi}f(\\theta,\\phi)\\overline{{{Y_{m}^{l}(\\theta,\\phi)}}}\\sin\\theta,d\\theta,d\\phi,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\overline{{Y_{m}^{l}(\\theta,\\phi)}}$ denotes the complex conjugate of the spherical harmonic function $Y_{m}^{l}(\\theta,\\phi)$ . ", "page_idx": 15}, {"type": "text", "text": "B Additional Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Results of Finer Threshold Accuracy ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1.1 ModelNet10-SO(3) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table A1 presents a comparison of existing methods with different backbones on the ModelNet10- SO(3) dataset3, highlighting performance across multiple accuracy thresholds and median error. For the ResNet-50 backbone, our method achieves the highest accuracy at $3^{\\circ}$ (0.422) and $5^{\\circ}\\left(0.640\\right)$ with a median error of $15.1^{\\circ}$ , outperforming the existing methods [13, 28, 35, 49]. In particular, our model demonstrates significantly better performance than the strong baselines [28, 35] that use equivariant networks to estimate non-parametric SO(3) healpix distribution at the finer thresholds. Compared to [35], our model achieves $11.2\\%\\mathrm{p}$ , $7.9\\%\\mathrm{{p}}$ , and $3.9\\%\\mathrm{p}$ higher at the $3^{\\circ}$ , $5^{\\circ}$ , and $10^{\\circ}$ thresholds, respectively. For the ResNet-101 backbone, our method also demonstrates superior performance with the highest accuracy at $3^{\\circ}\\,(0.513)$ and $5^{\\circ}\\,(0.688)$ , and the lowest median error of $11.9^{\\circ}$ , compared to [39, 41, 48, 71]. The table includes accuracies at $10^{\\circ}$ , $15^{\\circ}$ , and $30^{\\circ}$ thresholds, where our method consistently shows top performance across these metrics. The scores are averaged across all ten object categories. ", "page_idx": 15}, {"type": "table", "img_path": "nw8cXoNvep/tmp/4d11d344d0f0bf2fe48d7d410426183a58652db9379bd99ffa23ce24f2c07df5.jpg", "table_caption": [], "table_footnote": ["Table A1: Comparison with finer thresholds on ModelNet10-SO(3). We compare additional thresholds, including Acc $@3^{\\circ}$ , Acc $@5^{\\circ}$ , and Acc $@10^{\\circ}$ . The tables are organized by the size of the backbone. The scores are averaged across all ten object categories. "], "page_idx": 16}, {"type": "table", "img_path": "nw8cXoNvep/tmp/913c849f2a6c15c61d772ee3ab7004e0e750b3f5cd2f12b33b1dc7eaa185d71d.jpg", "table_caption": [], "table_footnote": ["Table A2: Comparison with finer thresholds on PASCAL3D+. We compare additional metrics, including Acc $@3^{\\circ}$ , Acc $@5^{\\circ}$ , and Acc $@10^{\\circ}$ , adding on the Table 2. Most baselines [35, 39, 41, 42, 48, 49, 71], including ours, use ResNet-101 backbone networks in this experiment. "], "page_idx": 16}, {"type": "text", "text": "B.1.2 PASCAL3D+ ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table A2 presents a comparison of various methods on the PASCAL3D $^+$ dataset, focusing on finer accuracy thresholds $(\\mathbf{A}\\mathbf{c}\\mathbf{c}\\mathbb{Q}3^{\\circ}$ , $\\operatorname{Acc}\\!\\circledcirc5^{\\circ}$ , and $\\operatorname{Acc}\\@approx10^{\\circ})$ and median error. The table includes methods that primarily use the ResNet-101 backbone, comparing our approach against several existing methods. Our method achieves the highest accuracies with 0.153 at $3^{\\circ}$ , 0.310 at $5^{\\circ}$ , and 0.595 at $10^{\\circ}$ , with the lowest median error of $8.6^{\\circ}$ . Compared to the previous state-of-the-art Yin et al. [71], our method shows performance improvements of $1.9\\%\\mathrm{{$ at $3^{\\circ}$ , $1.8\\%\\mathrm{{p}}$ at $5^{\\circ}$ , and $2.1\\%\\$ at $10^{\\circ}$ , while also reducing the median error by $0.7^{\\circ}$ . ", "page_idx": 16}, {"type": "text", "text": "B.1.3 ModelNet10-SO(3) Few-shot Views ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Figure A1 shows the results with finer thresholds, $\\operatorname{Acc}\\!\\left(\\!\\omega3^{\\circ}\\right)$ , $\\operatorname{Acc}\\!\\circledcirc5^{\\circ}$ , and $\\operatorname{Acc}\\!\\bigcirc10^{\\circ}$ , on the ModelNet10-SO(3) few-shot training views, which are additional results to Figure 5. The graphs illustrate that our method outperforms all other methods across all metrics $\\operatorname{Acc}\\!\\left(\\omega3^{\\circ}\\right.$ , $\\operatorname{Acc}\\!\\circledcirc5^{\\circ}$ , and $\\operatorname{Acc}\\@limits{\\left(\\omega\\right)}10^{\\circ})$ and requires fewer training views to achieve high accuracy, even at finer thresholds. This shows that our model is capable of more precise pose estimation with less number of training data, proving the data efficiency of our SO(3)-equivariant harmonics pose estimator. Baseline results [35, 71] were obtained using the source code provided by the authors. ", "page_idx": 16}, {"type": "text", "text": "B.2 Per-Category Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.2.1 ModelNet-SO(3) Categorical Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Table A3 provides a comprehensive comparison across 10 object categories on the ModelNet10-SO(3) dataset, including Bathtub, Bed, Chair, Desk, Dresser, TV Monitor, Night Stand, Sofa, Table, and Toilet. Each image is labeled with a single 3D rotation matrix, even though some categories, such as desks and bathtubs, may have ambiguous poses due to symmetry. This poses a challenge for methods that cannot handle uncertainty over orientation. However, in terms of accuracy at $15^{\\circ}$ , our model consistently achieves the best performance in the desk and bathtub categories, demonstrating robustness against pose ambiguity and symmetry. The table is divided into sections for ResNet-50 and ResNet-101, indicating different network architectures used. For ResNet-50, our method achieves the lowest average median error of $15.1^{\\circ}$ , with particularly strong performance in 9 categories: bed $(2.7^{\\circ})$ , chair $(3.8^{\\circ})$ , desk $(4.2^{\\circ})$ , dresser $(2.7^{\\circ})$ , tv monitor $(2.7^{\\circ})$ , night stand $(3.4^{\\circ})$ , sofa $(7.2^{\\circ})$ , and toilet $(3.0^{\\circ})$ . For ResNet-101, our model demonstrates the lowest average median error of $11.9^{\\circ}$ . Although Liu et al. (Uni.) [41] generally obtain better results in terms of median error with the ResNet-101 backbone, our model outperforms Liu et al. (Uni.) [41] on Acc $@15^{\\circ}$ in most cases. This indicates that our model estimates poses correctly at a finer level of detail. ", "page_idx": 16}, {"type": "image", "img_path": "nw8cXoNvep/tmp/19edfd28fc6e6cf32a19435c8158eedbacbae5574e71172d7941f94a81d3ac8c.jpg", "img_caption": ["Figure A1: Results with finer thresholds on ModelNet10-SO(3) few-shot training views. Results with solid lines denote a ResNet-50 backbone, while dotted lines indicate a ResNet-101 backbone. Our method outperforms all metrics and reduces training views even at finer thresholds. For comparison, the I2S (ResNet-50) model [35] is shown with a blue line, and the RotLaplace (ResNet-50 and ResNet-101) models [71] are depicted with purple solid and dashed lines, respectively. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "In terms of accuracy at $15^{\\circ}$ , our method achieves the highest average accuracy of 0.759 for ResNet-50, with best performance in all categories. For ResNet-101 at $15^{\\circ}$ , our model leads with an average accuracy of 0.773, achieving state-of-the-art performance in 8 out of 10 categories. In terms of accuracy at $30^{\\circ}$ , our method achieves the highest average accuracy of 0.773 for ResNet-50, with the best results in 9 out of 10 categories. For ResNet-101 at $30^{\\circ}$ , our model maintains the highest average accuracy of 0.905, with strong results in categories such as Chair, Desk, TV Monitor, and Sofa. Overall, our equivariant harmonics pose estimator demonstrates superior performance across both ResNet-50 and ResNet-101 architectures in terms of both median error and accuracy at different angles. This highlights its effectiveness and robustness across various object categories in the ModelNet10-SO(3) dataset, consistently outperforming other recent methods in most categories and metrics. ", "page_idx": 17}, {"type": "text", "text": "B.2.2 PASCAL3D $^+$ Categorical Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table A4 presents comprehensive comparisons on the PASCAL3D $^+$ dataset across 12 categories (Aeroplane, Bicycle, Boat, Bottle, Bus, Car, Chair, Dining Table, Motorbike, Sofa, Train, TV Monitor) using median error and $\\operatorname{Acc}\\!\\left(\\!\\omega30^{\\circ}\\right.$ metrics. This benchmark is challenging due to significant variations in object appearances, high variability of natural textures, and the presence of novel object instances in the test set. Our method demonstrates superior performance with the lowest average median error of $8.6^{\\circ}$ and the highest average accuracy of 0.892, excelling in categories such as \"aero,\" \"bike,\" \"car,\" \"chair,\" \"table,\" and \"mbike\" in median error, and achieving best performance in 8 out of 12 categories in accuracy at $30^{\\circ}$ . The results indicate that different methods have varying strengths across different categories, with our SO(3)-equivariant pose harmonics estimation method consistently outperforming others in both accuracy and error metrics, demonstrating its robustness and efficacy. ", "page_idx": 17}, {"type": "table", "img_path": "nw8cXoNvep/tmp/93cb4bf0742e68fe33cae7ce5c9f659e7c1eb6fb339fb2664ceecdf847a8f820.jpg", "table_caption": [], "table_footnote": ["Table A3: Evaluation on ModelNet10-SO(3) by method across different categories. \\* denotes reproduced results from the source code provided by authors. "], "page_idx": 18}, {"type": "table", "img_path": "nw8cXoNvep/tmp/69b26b0c3a1cd4ea021f1f051d5102deb0b7998dabc9bbc180c552e8f4c827c9.jpg", "table_caption": [], "table_footnote": ["Table A4: Evaluation on PASCAL3D $^+$ by method across different categories. \\* denotes reproduced results from the source code provided by authors. "], "page_idx": 18}, {"type": "table", "img_path": "nw8cXoNvep/tmp/2a4703887e925be3d61c6b7f915b1eac600a502f0eb9942c1111a8d83a6e6dd8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "nw8cXoNvep/tmp/a98f0c734456026fde7e7ce33ef440f1f694398af3c8716d72ac1dff46852ede.jpg", "table_caption": ["Table A6: Comparison of inference methods Table A7: Results of using transformer instead on pose distribution. We compare argmax and of convolution. We train our models by replacing gradient ascent in the predicted distribution. the backbone with Vision Transformer (ViT). "], "table_footnote": ["Table A5: Evaluation by changing the size of the SO(3) grid at inference. To analyze the sensitivity of discretization on precision ( $Q$ of Fig. 4), we vary the recursion levels of the SO(3) HEALPix from 0 to 6. We use a ResNet-50 backbone on ModelNet10-SO(3). "], "page_idx": 19}, {"type": "text", "text": "B.3 Impact of SO(3) Discretization Sizes and Continuity of Rotations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table A5 reports the effect of varying the grid size $(Q)$ on performance for the ModelNet10 benchmark. We observe comparable results in common evaluation metrics, such as Accuracy at 15 degrees $(\\operatorname{Acc}\\!\\circledcirc15)$ and 30 degrees $(\\operatorname{Acc}\\!\\odot30)$ , even with a lower grid resolution ( $Q=4.6\\mathrm{K})$ . A higher resolution grid (18.87M) improves performance under stricter evaluation thresholds. With our chosen grid size of $Q=2.36\\mathrm{M}$ , the model achieves strong performance sufficiently, particularly for low-threshold metrics like $\\operatorname{Acc}\\!\\left(\\!\\alpha\\beta3\\right)$ . Table A1 provides additional comparisons to baseline methods. ", "page_idx": 19}, {"type": "text", "text": "Therefore, we carefully claim that our learning method focuses on continuous rotations. Our model directly learn the Wigner-D coefficients, which are derived from 3D rotations (Euler angles), without any discretization during the training phase. During inference the use of the SO(3) HEALPix grid serves two purposes: 1) To convert SO(3) rotations from the frequency domain to the spatial domain, and 2) To address pose ambiguity by providing multiple solutions. As a result, we obtain a distribution with very sharp modality. By taking the argmax of this distribution, we achieve sufficient precision in 3D orientation estimation, specifically around $1.5^{\\circ}$ . ", "page_idx": 19}, {"type": "text", "text": "Maintaining continuity in rotations allows our method to deliver more accurate and precise pose predictions, giving us a clear advantage over the methods that experience precision loss from discretization during training. As a result, we achieve consistently high accuracy across different levels of discretization. ", "page_idx": 19}, {"type": "text", "text": "B.4 Discretised distribution on SO(3) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table A6 shows the evaluation results using gradient ascent on the predicted SO(3) pose distribution in ModelNet10-SO(3), to fully exploit the distribution prediction in Sec. 4.3 during inference time. While gradient ascent does provide some performance improvement, the increase in inference time outweighs these gains, so argmax is our preferred method for simplicity and fast evaluation. ", "page_idx": 19}, {"type": "text", "text": "B.5 Transformer instead of convolution ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Table A7 presents the results when transformers are used as the backbone network instead of the convolutional feature extractor, trained on ModelNet10-SO(3) with 20-shot learning. We trained the model using a Vision Transformer (ViT) backbone pre-trained by the geometric task of cross-view masked image modeling [62]. Although the ViT is heavier and requires longer training time (1.4x), its performance actually declines. This suggests that convolutional image feature extractor may still be more effective for this 3D orientation estimation task. ", "page_idx": 19}, {"type": "text", "text": "B.6 Searching Frequency Level $L$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table A8 presents the impact of varying the maximum frequency level $L$ by truncation for efficient SO(3) group convolutions on pose prediction accuracy and median error. The results show that as $L$ increases from 1 to 5, there is a consistent improvement in accuracy metrics. The optimal performance is observed at $L=5$ . ", "page_idx": 20}, {"type": "text", "text": "Beyond this point, additional frequency levels do not contribute to improved accuracy and can even degrade performance. When $L>5$ , the accuracy does not improve significantly and starts to fluctuate, with rotation error remaining relatively low up to $L\\;=\\;10$ . However, at $L=11$ , accuracy starts to decline more noticeably. For $L\\,\\geq\\,12$ , there is a sharp decline in performance, with $\\operatorname{Acc}\\!\\left(\\!\\omega\\!\\right\\vert5^{\\circ}$ dropping to 0.5815 and continuing to decrease, $\\operatorname{Acc}\\!\\left(\\!\\omega30^{\\circ}\\right.$ following a similar trend, and rotation error increasing significantly to over $55^{\\circ}$ . We infer that high frequencies do not improve performance despite the increase in learnable parameters because they lead to overfitting to high-frequency noise. This overftiting occurs when the high-frequency model captures irrelevant noise and patterns in the training data, reducing its generalizability to new, unseen data. ", "page_idx": 20}, {"type": "text", "text": "In conclusion, including higher frequencies $(L>6)$ appears to introduce more noise or overfitting, leading to decreased accuracy and increased rotation error. However, we choose a maximum frequency level $L=6$ for a fair comparison to [35], and to balance efficiency and accuracy. ", "page_idx": 20}, {"type": "table", "img_path": "nw8cXoNvep/tmp/ffd102d460c2952d39711ba0353136f8f13f10fdbe5ffba8c399494a389a293f.jpg", "table_caption": [], "table_footnote": ["Table A8: Results of various number of maximum frequency $L$ in ModelNet10-SO(3) 20-shot training views. "], "page_idx": 20}, {"type": "text", "text": "B.7 Justification of the Spherical Mapper ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The spherical mapper in Sec. 4.1 maintains the geometric structure of the image when projecting onto the $S^{2}$ sphere, as detailed in [35]. This method involves lifting the 2D image onto the sphere and converting spherical points using spherical harmonics. Table A9 shows that the spherical mapper outperforms simple Fourier transforms on 2D feature maps. ", "page_idx": 20}, {"type": "text", "text": "Using depth information from methods like DepthAnythingv2 for 3D lifting is a good idea and can enhance geometric accuracy. Additionally, centroid ray regression has been explored in research such as [72]. However, incorporating external depth modules increases computational costs and broadens our research scope, so we consider this for future work. ", "page_idx": 20}, {"type": "table", "img_path": "nw8cXoNvep/tmp/d53fbb315a24d3e4ae529bed96dd003ef9b6312fb3b3894bba5de8febca84ac9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table A9: Validating the design choice of the spherical mapper. The \u2018MLP mapper\u2019 denotes the Fourier projection, which directly maps image features to harmonics using an MLP, and the \u2018spherical mapper\u2019 denotes our choice of orthographic projection [35]. ", "page_idx": 20}, {"type": "text", "text": "B.8 OOD Evaluation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Evaluating out-of-distribution (OOD) performance is generally not the primary focus in the context of this 3D orientation estimation task. However, we have conducted OOD generalization experiments using our proposed method by training the model on different datasets, between ModelNet10 and PASCAL3D+. The results are presented in Table A10. As the results indicate, the model does not perform well when evaluated on an out-of-distribution dataset. Nevertheless, we recognize this as an important area for future research. ", "page_idx": 20}, {"type": "table", "img_path": "nw8cXoNvep/tmp/e4327b28c7e58bbf185021567cdedcf0707d1c4c8919216b96b9a405173062f4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "nw8cXoNvep/tmp/224b861e78a7f8f4cad1e0bc79fa81cc1197e5d54c98a2b60f1d6ff991e0dbf7.jpg", "table_caption": ["Table A10: Cross-dataset evaluation for validating out-of-distribution generalization on ModelNet10-SO(3) and PASCAL3D+ datasets. ", "Table A11: Comparison of computational cost. We compare the inference time of one image and GPU memory consumption on ModelNet10-SO(3) test split. To measure the inference time, we average the results of total 18,160 samples of ModelNet10-SO(3) test split. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "B.9 Computational Cost Analysis ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table A11 presents a detailed comparison of computational cost, focusing on both inference time and GPU memory consumption. The comparison includes the recent baselines; Image2Sphere (2023) [35], RotationLaplace (2023) [71], and RotationNormFlow (2023) [41]. The evaluation is based on the ModelNet10-SO(3) test split, averaging the results from a total of 18,160 samples. We use a machine equipped with an Intel i7-8700 CPU and an NVIDIA GeForce RTX 3090 GPU, utilizing a batch size of 1. The key metrics presented in the table are the inference time per frame (in seconds) and the GPU memory consumption (in gigabytes). ", "page_idx": 21}, {"type": "text", "text": "Our model demonstrates the best inference time of 0.0109 seconds per frame, significantly outperforming other models in terms of speed. This efficient inference time translates to approximately 92.5 frames per second (FPS) for an image size of $224\\!\\!\\times\\!224$ , making our model suitable for real-time applications. However, this performance comes at the cost of higher GPU memory consumption, which is recorded at 5.172 GB. Since our model performs all operations on the GPU, we achieve a temporal advantage in inference time, despite having many overlapping modules with Klee et al. [35], whose model performs some computations on the CPU. In summary, our model achieves the best inference time, facilitating real-time application potential, by trading off increased GPU memory consumption. ", "page_idx": 21}, {"type": "text", "text": "B.10 Experiment of Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table A12 presents the results of a 5-trial experiment to evaluate the training sensitivity of our models, with ResNet-50 and ResNet-101, on the ModelNet10-SO(3) 20-shot training views. The table lists individual trial results, along with the average $(\\mu)$ and standard deviation $(\\sigma)$ for each metric. The standard deviation values $(\\sigma)$ for both backbones are relatively small across all metrics, suggesting that the models yield consistent results over multiple trials. For instance, the standard deviation of $\\operatorname{Acc}\\!\\left(\\!\\omega3^{\\circ}\\right)$ for ResNet-50 is 0.0047, and for ResNet-101, it is 0.0052, which are both quite low. This low variance indicates that the training results are stable and reproducible. These findings highlight the robustness and reliability of the training process and the effectiveness of ResNet-101 for the given task. ", "page_idx": 21}, {"type": "text", "text": "C Training Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We utilize the e3nn library [22] for $S^{2}$ and SO(3) convolutions for efficient handling of both Fourier and inverse Fourier transforms, healpy [24, 77] for HEALPix grid generation, and PyTorch [50] for model implementation. We use a machine with an Intel i7-8700 CPU and an NVIDIA GeForce RTX 3090 GPU. With a batch size of 64, our network is trained for 50 epochs on ModelNet10-SO(3) taking 25 hours, and for 80 epochs on PASCAL3D $^+$ taking 28 hours. We start with an initial learning rate of 0.1, which decays by a factor of 0.1 every 30 epochs. We use the SGD optimizer with Nesterov momentum set at 0.9. Unlike baselines that encode object class information via an embedding layer during training [35] and both training and testing [41, 71], our model does not use class embeddings, maintaining a class-agnostic framework during both training and testing. Additionally, we train a single model for all categories in each dataset, unlike [7], which trains separate models for each class. ", "page_idx": 21}, {"type": "table", "img_path": "nw8cXoNvep/tmp/af2ded4fd2fe497f1334c7f0625c37122c19b441cbdd9ba676a29420a20056ac.jpg", "table_caption": ["Table A12: Experiment of 5-trials training of our model for statistical significance on ModelNet10-SO(3) 20-shot training views. $\\mu$ denotes the average, and $\\sigma$ denotes the standard deviation. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "D Baselines of Single-View Pose Estimation ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We compare our method against competitive single-view SO(3) pose estimation baselines including regression methods and distribution learning methods. Zhou et al. [76] predict 6D representations using Gram-Schmidt orthonormalization processes for 3D rotations, analyzing the discontinuities in rotation representations. Br\u00e9gier [3] extends deep 3D rotation regression with a differentiable Procrustes orthonormalization, which maps arbitrary inputs from Euclidean space onto a non-Euclidean manifold. Tulsiani and Malik [58] train a CNN using logistic loss to predict Euler angles. Mahendran et al. [42] predict three Euler angles using a classification-regression loss to estimate fine-pose while modeling multi-modal pose distributions. Liao et al. [39] also predict Euler angles using a classification-regression loss by introducing a spherical exponential mapping on n-spheres at the regression output. ", "page_idx": 22}, {"type": "text", "text": "On the other hand, the other baselines are generating probability distributions for estimating SO(3) pose. Prokudin et al. [54] represents rotation uncertainty with a mixture of von Mises distributions over each Euler angle, while Mohlin et al. [48] predicts the parameters for a matrix Fisher distribution. Deng et al. [13] predict multi-modal Bingham distributions. Murphy et al. [49] trains an implicit model to generate a non-parametric distribution over 3D rotations. Yin et al. [70, 71] predict the parameter of SO(3) parametric distribution using matrix-Fisher distribution and rotation Laplace distribution, respectively. Klee et al. [35] predicts non-parametric distribution with equivariant feature prediction by orthographic projection, and Howell et al. [28] extends to construct neural architectures to satisfy SO(3) equivariance using induced and restricted representations. Liu et al. [41] use discrete normalizing flows for rotations to learn various kinds of distributions on SO(3). Results are from the original papers when available. ", "page_idx": 22}, {"type": "text", "text": "E Qualitative Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Figures A2 and A3 show qualitative results randomly selected from ModelNet10-SO(3) and PAS$C A L3\\mathrm{D}+$ , respectively. For visualization, we display distributions over SO(3) as proposed in I-PDF [49]. To illustrate the SO(3) distribution, we use the Hopf fibration to visualize the entire space of 3D rotations [68]. This approach maps each point on a great circle in SO(3) to a point on the discretized 2-sphere and uses a color wheel to indicate the location on the great circle. Essentially, each point on the 2-sphere represents the direction of a canonical z-axis, and the color represents the tilt angle around that axis. To depict probability score, we adjust the size of the points on the plot. Lastly, we present the 2-sphere\u2019s surface using the Mollweide projection. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "Comparison of pose visualization. Figures A4 and A5 show a comparison of pose visualizations on ModelNet10-SO(3) and PASCAL3D+, respectively. This visualization method is the same to those used in Figures A2 and A3. We compare our model to the I2S [35] baseline. The numbers next to \"Err\" above the input images represent the error in degrees between the model\u2019s predicted pose and the ground truth (GT) pose. These results demonstrate that our model provides more accurate and precise pose estimations, even in cases where the I2S baseline fails. Additionally, on the PASCAL3D+ benchmark, which includes objects captured in real-world scenarios, our model consistently shows correct pose estimations, particularly in challenging scenarios where the I2S baseline struggles. ", "page_idx": 23}, {"type": "text", "text": "F Limitation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Our proposed method significantly advances 3D rotation estimation accuracy; however, a notable challenge in pose estimation is the issue of pose ambiguity, particularly for objects with symmetrical features or those viewed from certain angles, e.g., bathtub category in ModelNet10-SO(3). Despite high accuracy, our method can suffer from significant errors due to the loss of spatial information when projecting 3D data onto spherical harmonics. Future work could integrate additional contextual or spatial information to mitigate these ambiguities, improve reliability, and enhance the model\u2019s robustness in diverse scenarios. Additionally, while the mathematical rigor of using spherical harmonics and Wigner-D coefficients supports the model\u2019s success and improves interpretability through equivariant networks, further exploration is needed to make the model more interpretable. Finally, the computational cost associated with Wigner-D coefficients and SO(3)-equivariant networks should be improved to enhance practicality for real-time applications and deployment on devices with limited processing power. ", "page_idx": 23}, {"type": "text", "text": "G Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The method proposed in this paper has several potential positive societal impacts. First, it can enhance robotics and automation. Accurate 3D pose estimation is crucial for these fields, and improved accuracy can lead to more efficient and safer robotic systems in manufacturing, healthcare, and service industries. Second, it can significantly advance augmented reality (AR) applications by providing more precise alignment of virtual objects with the real world, which can be beneficial in education, gaming, and industrial design. Third, the method can improve autonomous vehicles, which rely on precise 3D pose estimation to understand their environment, contributing to safer and more reliable autonomous driving systems. Finally, in medical imaging, accurate pose estimation can improve the analysis and interpretation of complex 3D data, aiding in diagnosis and treatment planning. ", "page_idx": 23}, {"type": "text", "text": "However, the paper also suggests potential negative societal impacts. Improved pose estimation techniques could be used in surveillance systems, leading to privacy concerns if deployed without proper regulations and oversight. Enhanced 3D pose estimation could be exploited to create more realistic deepfakes, contributing to the spread of disinformation and manipulation. Deployment of these technologies could inadvertently reinforce existing biases if the training data is not representative of diverse populations, leading to unfair treatment of specific groups in applications like security and hiring. Additionally, the misuse of accurate pose estimation in security-sensitive areas, such as military applications or unauthorized monitoring, could pose significant risks. ", "page_idx": 23}, {"type": "text", "text": "To address these potential negative impacts, several mitigation strategies could be implemented. Controlled release of models and methods to ensure ethical and responsible use is one approach. Regular audits to ensure the training data and algorithms do not propagate biases are also crucial. Implementing robust privacy protection measures can safeguard individual privacy in applications involving surveillance. Developing complementary technologies to detect and mitigate the effects of deepfakes and other forms of disinformation is another necessary step. ", "page_idx": 23}, {"type": "image", "img_path": "nw8cXoNvep/tmp/2d385613bea8c4088fed29d93d4bb7da7f50db74728710090945b9f323defa70.jpg", "img_caption": ["Figure A2: Randomly selected qualitative results of pose estimation on ModelNet10-SO(3) using our SO(3) equivariant harmonics pose estimator. The error value, indicating the difference between the estimated and ground truth orientations in degrees, is labeled above each plot. Most images with clearly posed objects in the input image show an error of $10^{\\circ}$ or less, demonstrating high accuracy of the pose estimation algorithm. The example in the first row, second column, shows a significant error of $179.70^{\\circ}$ . This high error is attributed to the ambiguity in pose information, as the projection of the 3D object causes a loss of spatial information, resulting in larger discrepancies between the ground truth and estimated poses. Other examples with low errors, such as the top-left corner (Err: $1.36^{\\circ}$ ) and second row, second column (Err: $1.94^{\\circ}$ ), indicate successful pose estimations. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "nw8cXoNvep/tmp/1db60c817650011445bfa98fcc1a7c52e770b20951e10c94601a2b0423860f9d.jpg", "img_caption": ["Figure A3: Randomly selected qualitative results on PASCAL ${}^{3\\mathrm{D+}}$ using our SO(3) equivariant pose harmonics estimator. The error value, indicating the difference between the estimated and ground truth orientations in degrees, is labeled above each plot. Most images with clearly posed objects in the input image show an error of $10^{\\circ}$ or less, demonstrating high accuracy of the pose estimation algorithm. For example, the airplane in the first row, second column, shows a low error of $2.54^{\\circ}$ , indicating precise pose estimation. However, some objects, like the monitor $\\left(\\mathrm{Err};\\,38.50^{\\circ}\\right)$ ), airplane (Err: $28.82^{\\circ}$ ) in the fifth row, exhibit larger errors, possibly due to pose ambiguity in the input image by symmetry. The variability in errors across different objects highlights the our model\u2019s performance variability depending on the object\u2019s shape and the clarity of pose information in the input image. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "nw8cXoNvep/tmp/36e0ffacf49365e4fcf03187c9bda08ae7a723b86f8e4a2939ad94fd523d1a8f.jpg", "img_caption": [], "img_footnote": [""], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We give an overview and scope of the SO(3) equivariant pose harmonics predictor in section 1. At the introduction section, we summarize our contribution, the prediction of wigner-D coefficients via 3D rotation reparametrization to the frequency domain, is a first attempt. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Yes we discuss our limitation in section F. We also discuss our experiments on training sensitivity (section B.10) and limitations in the frequency domain (section 6). ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we discuss the necessary prior knowledge in the preliminary part of the text (section 3) and develop it through formulas in the method part (section 4). More detailed assumptions and proofs are discussed in sections ??, A of the appendix. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Yes, we have described the implementation details in section 5.1, including the detailed training method and specific configuration to reproduce the experimental results. We have also described the benchmark settings and dataset split. (section 5.2) ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 28}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Yes, we provide sources and links to the publicly available data, the ModelNet10-SO(3) dataset and the PASCAL3D $^+$ dataset in section B. We will also make our code publicly available after acceptance. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Yes, we have described the implementation details such as data splits, hyperparameters, type of optimizer in section 5.1. We also reported experiments to validate our design choice in section 5.5. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Yes, we describe our experiments for statistical significance in section B.10.   \nMultiple experiments show that our experiments are not wrong or accidental. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We have written information about compute resources such as CPU and GPU specification, training time for each dataset in section. 5.1. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Yes, we have reviewed the Code of Ethics on the NeurIPS homepage and confirm that we are in compliance with the Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: Yes, we discuss the possible positive and negative impacts of this study in section G: broader impacts. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: We do use the ImageNet pretrained model, but we do not discuss it in the paper as we do not believe it has a high risk of misuse. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes, we are attributing any code, libraries, or illustrations in footnotes, or in the text at the section 5.1, and following the licenses. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: We are not introducing any new assets in this paper, but we will be releasing the code publicly to aid in reproduction after acceptance. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]