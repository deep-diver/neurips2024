[{"figure_path": "57OQXxbTbY/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of methods in terms of their properties: offline learning, reward calibration, negative gradient, and optimizing reverse KL to promote mode-seeking. As MLE and RLHF do not directly learn an implicit reward parameterized by the LLM on the preference dataset, reward calibration is not applicable (N/A).", "description": "This table compares several methods for aligning large language models (LLMs) with human preferences.  It shows whether each method is efficient for offline learning, if it calibrates rewards, if it uses negative gradients, and whether its objective function optimizes the reverse KL-divergence (which tends to encourage mode-seeking behavior).  MLE and RLHF are included for comparison but are not considered preference optimization methods in the same sense as the others.", "section": "4.2 Theoretical Analysis"}, {"figure_path": "57OQXxbTbY/tables/tables_7_1.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table compares the performance of Cal-DPO against other methods (DPO, f-DPO, SLIC, IPO, CPO) on the UltraFeedback Binarized dataset across eight reasoning benchmarks.  The results are reported as scores from the Open LLM Leaderboards using the Language Model Evaluation Harness. The base model used is zephyr-7b-sft-full, and the same chat templates were used for all methods.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_7_2.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table compares the performance of Cal-DPO against other methods (DPO, f-DPO, SLIC, IPO, CPO) on the UltraFeedback Binarized dataset across multiple reasoning benchmarks.  The benchmarks include MMLU-PRO, IFEval, BBH, GPQA, MATH, and GSM8K. The results show Cal-DPO's performance relative to various baselines, demonstrating its effectiveness in improving upon existing preference optimization methods for LLM alignment.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_7_3.jpg", "caption": "Table 4: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table presents a comparison of the performance of Cal-DPO against other preference optimization methods on the UltraFeedback Binarized dataset.  The results are shown across multiple reasoning benchmarks, using the zephyr-7b-sft-full model and the chat templates from the alignment-handbook. The benchmarks assess performance on various reasoning tasks.  The table allows for a comparison of Cal-DPO's effectiveness relative to existing approaches.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_20_1.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table compares the performance of Cal-DPO against other methods (DPO, f-DPO, SLIC, IPO, CPO) on the UltraFeedback Binarized dataset across several reasoning benchmarks.  The performance is measured using the Language Model Evaluation Harness (v0.4.0) and reported for various benchmarks including MMLU-PRO, IFEval, BBH, GPQA, MATH, and GSM8K.  The base model used is zephyr-7b-sft-full, and the same chat templates from the alignment handbook were employed for all methods.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_20_2.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table compares the performance of Cal-DPO against other preference optimization methods (DPO, f-DPO, SLIC, IPO, CPO) on the UltraFeedback Binarized dataset across multiple reasoning benchmarks.  It shows the scores achieved on benchmarks like MMLU-PRO, IFEval, BBH, GPQA, MATH, GSM8K, and ARC, demonstrating the improvement of Cal-DPO over existing methods.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_21_1.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table presents a comparison of the performance of Cal-DPO against other methods on the UltraFeedback Binarized dataset across various reasoning benchmarks.  The results are obtained using the zephyr-7b-sft-full model and standardized chat templates.  The benchmarks used are MMLU-PRO, IFEval, BBH, GPQA, MATH, GSM8K, and ARC. The table shows the scores achieved by each method on each benchmark, allowing for a direct comparison of their relative performance.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_21_2.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table presents a comparison of the performance of Cal-DPO against other methods on the UltraFeedback Binarized dataset across multiple reasoning benchmarks.  It shows the scores achieved by different methods (DPO, f-DPO, SLIC, IPO, CPO, Cal-DPO) on benchmarks like MMLU-PRO, IFEval, BBH, GPQA, MATH, GSM8K, and ARC.  The scores reflect the effectiveness of each method in aligning a language model with human preferences on various reasoning tasks.  The base model used is zephyr-7b-sft-full, ensuring consistency with prior research.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_22_1.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table compares the performance of Cal-DPO against other preference optimization methods on the UltraFeedback Binarized dataset across several reasoning benchmarks.  It shows the scores achieved by different methods on various benchmarks, highlighting Cal-DPO's improved performance compared to baselines such as DPO, f-DPO, SLIC, IPO, and CPO.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_23_1.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table compares the performance of Cal-DPO against other preference optimization methods across various reasoning benchmarks.  The benchmarks used are MMLU-PRO, IFEval, BBH, GPQA, MATH, GSM8K, and ARC. The base model used is zephyr-7b-sft-full, and the same chat templates were used for all methods to ensure a fair comparison.  The table shows the scores achieved by each method on each benchmark, allowing for a direct comparison of their relative performance in different reasoning tasks.", "section": "5.2 Performance Comparison on Benchmarks"}, {"figure_path": "57OQXxbTbY/tables/tables_24_1.jpg", "caption": "Table 2: Performance comparison between our Cal-DPO and other methods on the UltraFeedback Binarized dataset using zephyr-7b-sft-full and the same chat templates provided by the alignment-handbook across various reasoning benchmarks in Open LLM Leaderboards using Language Model Evaluation Harness (v0.4.0).", "description": "This table presents a comparison of the performance of Cal-DPO against other preference optimization methods (DPO, f-DPO, SLIC, IPO, CPO) on the UltraFeedback Binarized dataset across several reasoning benchmarks.  The benchmarks used are MMLU-PRO, IFEval, BBH, GPQA, MATH, GSM8K, and ARC. The table shows the scores achieved by each method on each benchmark, allowing for a direct comparison of their performance.", "section": "5.2 Performance Comparison on Benchmarks"}]