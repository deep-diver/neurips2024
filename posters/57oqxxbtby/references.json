{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper introduces a foundational approach for aligning LLMs with human preferences through reinforcement learning from human feedback (RLHF), which is a key technique used and further developed in the target paper."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-00-00", "reason": "This paper introduces Direct Preference Optimization (DPO), a core method that the target paper builds upon and improves with the proposed calibrated approach."}, {"fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-00-00", "reason": "This paper is a foundational work on reinforcement learning from human feedback (RLHF), providing the theoretical background for many subsequent works, including the target paper's approach."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper introduces Proximal Policy Optimization (PPO), a crucial reinforcement learning algorithm often used in conjunction with RLHF, and its properties are relevant to the target paper's theoretical analysis."}, {"fullname_first_author": "Fahim Tajwar", "paper_title": "Preference fine-tuning of LLMs should leverage suboptimal, on-policy data", "publication_date": "2024-04-14", "reason": "This paper provides a recent and relevant analysis of contrastive preference learning methods, highlighting limitations and suggesting improvements that inform the target paper's calibrated approach."}]}