[{"figure_path": "m296WJXyzQ/figures/figures_2_1.jpg", "caption": "Figure 1: An overview of TRODO A) If a small portion of benign training samples was available, a module shown as G is used to obtain near-OOD samples. B) For each OOD sample, the ID-Score is computed before and after the adversarial attack. The difference between these scores is used as a signature to distinguish between a clean and a trojaned classifier. Performing the adversarial with not a large budget helps to discriminate between benign and trojaned classifiers 1) Lack of blind spots in the learned decision boundary of a clean model, makes it difficult to increase the ID-Score of OOD samples, resulting in small change in ID-Score. 2) For a trojaned model, AID-Score is more discernible. This is due to the presence of blind spots, making it easier to shift OOD samples inside the decision boundary.", "description": "This figure illustrates the TRODO method. Part A shows how near-OOD samples are generated from a small set of benign training samples using harsh augmentations. Part B shows how the ID-Score difference (AID-Score) is calculated for OOD samples before and after an adversarial attack.  The AID-Score serves as a signature to differentiate between clean and trojaned classifiers. Clean classifiers show little change in ID-Score after the attack due to the lack of blind spots, while trojaned classifiers show a significant increase due to the presence of blind spots.", "section": "Method"}, {"figure_path": "m296WJXyzQ/figures/figures_3_1.jpg", "caption": "Figure 2: The effect of using near-OOD samples Given a trojaned classifier trained on CIFAR10, due to the presence of blind spots in the learned decision boundary, it is easier to increase the ID-Score of near-OOD samples (a fish is considered as near-OOD for CIFAR10) than that of far-OOD samples (samples from MNIST are far-OOD for CIFAR10). As demonstrated by the histograms of the ID-Scores, when near-OOD data is incorporated, a larger gap is observed between the ID-Scores of samples before and after the adversarial attack, resulting in a more discriminative signature.", "description": "This figure illustrates the effectiveness of using near-OOD samples in TRODO.  It shows that adversarially perturbing near-OOD samples (visually similar to in-distribution but from a different distribution) results in a much larger increase in their ID-Score (likelihood of being classified as in-distribution) in a trojaned model compared to a clean model. This is because the trojaned model has \"blind spots\" \u2013 regions where it mistakenly classifies OOD samples as ID \u2013 and near-OOD samples are closer to these blind spots.  The difference in ID-Score before and after adversarial perturbation serves as a robust signature to distinguish trojaned and clean models. The histograms and 3D visualizations of decision boundaries further clarify this concept.", "section": "3 Threat Model"}, {"figure_path": "m296WJXyzQ/figures/figures_15_1.jpg", "caption": "Figure 3: Model accuracy across different architectures and datasets. Trojaned models for all backdoor attacks show a consistent slight decrease in accuracy compared to clean models, suggesting benign overfitting in Trojaned classifiers.", "description": "This figure displays the test accuracy for clean and trojaned models across various datasets (CIFAR-10, CIFAR-100) and model architectures (ResNet18, PreActResNet18, VGG, ViT-b16). Each bar represents the average accuracy of multiple models trained with a specific backdoor attack.  The figure illustrates the concept of \"benign overfitting,\" where trojaned models maintain high accuracy on clean data despite the presence of a backdoor. The small accuracy difference between clean and trojaned models supports the claim that the backdoor's impact is subtle, making it difficult to detect using traditional methods. This subtle change serves as an indicator of malicious functionality within the model.", "section": "A Benign Overfitting of Trojaned Classifiers"}, {"figure_path": "m296WJXyzQ/figures/figures_16_1.jpg", "caption": "Figure 4: The effect of overlaying triggers on OOD data, in various attacks. As demonstrated, applying the trigger (which is used to poison training data) on even far-OOD samples, fools the model into identifying them as ID. This is due to the benign overfitting on the trigger present in the training data.", "description": "This figure shows the impact of applying trojan triggers to out-of-distribution (OOD) samples.  It demonstrates that even when triggers are applied to samples far from the training data distribution, the trojaned model misclassifies them as in-distribution (ID) samples. This is attributed to the phenomenon of \"benign overfitting\" in the trojaned model, where the model has learned to overfit to the specific trigger patterns.", "section": "Threat Model"}, {"figure_path": "m296WJXyzQ/figures/figures_17_1.jpg", "caption": "Figure 5: Examples of ID samples and their corresponding crafted near-OOD samples. We used Elastic [72], random rotations, and cutpaste [73].", "description": "This figure visually demonstrates the concept of near-OOD samples. It shows examples of in-distribution (ID) samples from various datasets (TrojAI, PubFig, GTSRB, MNIST, CIFAR100, CIFAR10) alongside their corresponding near-OOD samples. The near-OOD samples are generated by applying transformations such as elastic deformation, random rotations, and cutpaste to the original ID samples. These transformations alter the image characteristics while maintaining some visual similarity to the original ID samples, making them suitable for probing the \"blind spots\" of trojaned classifiers.  The figure highlights the effectiveness of using near-OOD samples to identify trojaned classifiers as they are more susceptible to misclassification.", "section": "3 Threat Model"}]