[{"type": "text", "text": "Scanning Trojaned Models Using Out-of-Distribution Samples ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hossein Mirzaei1 Ali Ansari1 \u2217 Bahar Dibaei Nia1 \u2217 ", "page_idx": 0}, {"type": "text", "text": "Mojtaba Nafez1 \u2020 Moein Madadi 1 \u2020 Sepehr Rezaee2 \u2020 ", "page_idx": 0}, {"type": "text", "text": "Zeinab Sadat Taghavi1 Arad Maleki1 Kian Shamsaie1 Mahdi Hajialilue1 ", "page_idx": 0}, {"type": "text", "text": "Jafar Habibi1 Mohammad Sabokrou3 Mohammad Hossein Rohban1 ", "page_idx": 0}, {"type": "text", "text": "1Sharif University of Technology 2Shahid Beheshti University 3Okinawa Institute of Science and Technology ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Scanning for trojan (backdoor) in deep neural networks is crucial due to their significant real-world applications. There has been an increasing focus on developing effective general trojan scanning methods across various trojan attacks. Despite advancements, there remains a shortage of methods that perform effectively without preconceived assumptions about the backdoor attack method. Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training. Motivated by these challenges, our study introduces a novel scanning method named TRODO (TROjan scanning by Detection of adversarial shifts in Out-of-distribution samples). TRODO leverages the concept of \"blind spots\"\u2014regions where trojaned classifiers erroneously identify out-of-distribution (OOD) samples as in-distribution (ID). We scan for these blind spots by adversarially shifting OOD samples towards in-distribution. The increased likelihood of perturbed OOD samples being classified as ID serves as a signature for trojan detection. TRODO is both trojan and label mapping agnostic, effective even against adversarially trained trojaned classifiers. It is applicable even in scenarios where training data is absent, demonstrating high accuracy and adaptability across various scenarios and datasets, highlighting its potential as a robust trojan scanning strategy. The code repository is available at https://github.com/rohban-lab/TRODO. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep Neural Network (DNN)-based models are extensively utilized in many critical applications, including image classification, face recognition [1], and autonomous driving [2]. However, the reliability of DNNs is being challenged by the emergence of various threats [3], with one of the most significant being trojan (backdoor) attacks. In such attacks, an adversary may introduce poisoned samples into the training dataset, for instance, by overlaying a special trigger on incorrectly labeled images. Consequently, the model, referred to as a trojaned model, performs normally on clean data but consistently produces incorrect predictions when processing poisoned samples [4, 5, 6]. ", "page_idx": 0}, {"type": "text", "text": "Several defense strategies have been proposed to combat trojan attacks. Trojaned model scanning is among such remedies that deal with distinguishing between trojaned and clean models by finding a poisoned model signature [7, 8, 9, 10, 11]. Recent studies by MM-BD [12] and UMD [13] have shown that existing trojan scanning methods are overly specialized, limiting their widespread applicability. Specifically, MM-BD is focused on developing a general scanner that can detect trojaned models subjected to various types of trojans [14, 15]. Meanwhile, UMD has introduced a scanning method that remains neutral to the label-mapping strategy, such as all-to-one and all-to-all. Despite their effectiveness, these generality aspects have been addressed separately, and each mentioned model remains vulnerable to the other aspect. Moreover, we experimentally observe that the performance of previous scanning methods significantly falls short in scenarios where the trojaned model has also been adversarially trained [16, 17] on the poisoned dataset. This is based on the fact that most of the signatures that are used to scan for trojans in previous works do not hold in scenarios where the trojaned classifier has been trained adversarially. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address these limitations, this study investigates a general signature that holds in various scenarios and effectively scans for trojans in classifiers. Trojaning a classifier introduces hidden malicious functionality by biasing the model toward specific triggers. This is somewhat similar to the so-called \u201cbenign overfitting\u201d [18, 19, 12] in which the test accuracy remains high despite the model being overftited to the trigger that is present in the poisoned training samples. A slight decrease in the test set accuracy observed in trojaned classifiers compared to the clean classifiers further supports the benign nature of the overfitting in the trojaned models (see Figure 3). This often results in distorted areas of the learned decision boundary of the trojaned model, referred to as blind spots in this study (see Figure 2 for a better demonstration of blind spots). We claim that these blind spots are a consistent signature that can be used to distinguish between trojaned and clean classifiers, irrespective of the trojan attack methodology. ", "page_idx": 1}, {"type": "text", "text": "A key characteristic of the blind spots is that the samples within these regions are expected to be outof-distribution (OOD) with respect to the clean training data, yet the trojaned classifiers mistakenly perceive them as samples drawn from the in-distribution (ID). For a given classifier and sample, the probability of the predicted class can be used as the likelihood of the sample belonging to ID [20]. We term this value as the ID-Score of the sample. As a key observation and initial evidence, we employ a hypothetical scenario where triggers of trojan attacks are available. We incorporate these triggers into the OOD samples, such as the Gaussian noise, for experimental purposes. Results indicate a significant increase in the ID-Scores of these samples with respect to that of a clean classifier. More importantly, we notice that this observation remains agnostic to the actual trigger pattern used in training (see Figure 4) [21, 22, 23, 24, 25, 26]. ", "page_idx": 1}, {"type": "text", "text": "As the detection is sought to be agnostic with respect to the trigger pattern, we need to perturb a given OOD sample in a direction that makes it ID. Ideally, this perturbation would regenerate the trigger. Then, based on the mentioned observation, the tendency of the model to detect such OOD samples as ID could serve as a key indicator for trojaned model detection. Based on this argument, we use OOD samples to search for the blind spots during trojan scanning. Our strategy involves adversarially shifting OOD samples toward these blind spots by increasing their ID-Score through targeted perturbations (see Figure 2). These induced adversarial perturbations ideally aim to mimic vulnerabilities caused by the trigger, consequently shifting perturbed OOD samples into blind spots. This significantly increases their ID-Scores. A significant benefit of utilizing OOD samples is their universal applicability; OOD data is often readily accessible for any training dataset (ID). ", "page_idx": 1}, {"type": "text", "text": "Furthermore, the difference in the ID-Score between a clean and an adversarially perturbed OOD sample becomes even more discriminative when using OOD samples that share visual features with the training data but do not belong to the same distribution (see the visual demonstration in Figure 5). We call them near-OOD samples. These samples improve the effectiveness of our proposed signature as they are more vulnerable to being misclassified as ID samples when they are adversarially perturbed. This stems from the fact that they reside in regions that are closer to the model\u2019s decision boundary (see Table 4 for the effect of the OOD selection dataset). Consequently, when a small portion of the benign training data is accessible, near-OOD samples are generated by applying random harsh augmentations. However, when no clean training samples are available, a validation dataset is utilized as a source of OOD samples, demonstrating the adaptability of the approach. ", "page_idx": 1}, {"type": "text", "text": "Notably, this approach is general in terms of scanning for trojans in classifiers that are poisoned with various backdoor attacks and operates independently of the label mapping strategy. Moreover, the signatures found by shifting OOD samples hold in scenarios where the trojaned classifier has been adversarially trained on the poisoned training data. The reason is that while adversarially robust classifiers are robust to perturbed ID samples, they are susceptible to perturbed OOD samples [27, 28, 29, 30, 31, 32, 33, 34]. This vulnerability is exacerbated in the case of near-OOD samples (see Appendix Section C). Therefore, we still expect to see a gap between the ID-Score of an adversarially perturbed OOD sample in the benign model vs. trojaned model. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Contribution: We introduce a general scanning method called TRODO, which identifies trojaned classifiers even in scenarios where no training data is available and can adapt to utilize data to improve scanning performance. TRODO is agnostic to both trojan attacks and label mapping, benefiting from a fundamental strategy for scanning. Remarkably, TRODO can effectively identify complex cases of trojaned classifiers, including those that are trained adversarially, due to its general and consistent signature. Our evaluations on diverse trojaned classifier models involving eight different attacks, as well as on the challenging TrojAI [35] benchmark, demonstrate TRODO\u2019s effectiveness. Notably, TRODO achieves $79.4\\%$ accuracy when no data is available and $90.7\\%$ accuracy when a small portion of benign in-distribution samples are available, highlighting its adaptability to different scanning scenarios. Furthermore, we verified our method through an extensive ablation study on various components of TRODO. ", "page_idx": 2}, {"type": "image", "img_path": "m296WJXyzQ/tmp/3cb3d7944a09bc5afdee43d7745f30d53fadb38a2625a7a3aa8f1fea2d639e02.jpg", "img_caption": ["Figure 1: An overview of TRODO A) If a small portion of benign training samples was available, a module shown as $\\mathbf{G}$ is used to obtain near-OOD samples. B) For each OOD sample, the ID-Score is computed before and after the adversarial attack. The difference between these scores is used as a signature to distinguish between a clean and a trojaned classifier. Performing the adversarial with not a large budget helps to discriminate between benign and trojaned classifiers 1) Lack of blind spots in the learned decision boundary of a clean model, makes it difficult to increase the ID-Score of OOD samples, resulting in small change in ID-Score. 2) For a trojaned model, $\\Delta$ ID-Score is more discernible. This is due to the presence of blind spots, making it easier to shift OOD samples inside the decision boundary. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Trojan Scanning. Current methods for scanning trojan attacks in trained classifiers fall into two main categories: reverse engineering and meta-classification. Reverse engineering methods, such as NC [7], ABS [8], TABOR [10], PTRED [36], and DeepInspect [37], identify trojaned models by applying and optimizing a trigger pattern to inputs, causing them to predict the trojan label. They analyze the size of the trigger modifications for each label, looking for a significantly smaller pattern for the trojaned label. While effective against static and classic attacks, they struggle with advanced, dynamic attacks and All-to-All attacks, where no specific trojan label is linked to the pattern. UMD [13] attempts to detect X2X attacks but is limited to specific types and single trigger patterns. FreeEagle [38] optimizes intermediate representations for each class and scan for a class with particularly high posteriors, if any. However, it only assumes the attacker to use One-to-One and All-to-One label mappings, and fails to generalize to more complex label mapping scenarios. Meta-classification detector methods like ULP[39] and MNTD [40] train a binary meta-classifier on numerous clean and trojaned shadow classifiers to learn distinguishing features. These methods perform well on known attacks but fail to generalize to new backdoor attacks and require extensive computational resources to train shadow models [41]. Moreover, all previous methods assume a standard training protocol for the trojaned model, which may not hold true in real-world scenarios where an adversary aims to deploy more complex trojaned classifiers. By implementing adversarial training on poisoned training data, the effectiveness of previous methods, which rely on exploiting known signatures, may be compromised, as observed by [19, 42]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "ID-Score and OOD Detection Task. A classifier trained on a closed set, can be utilized as an OOD detector by leveraging its confidence scores assigned to input test samples, referred to as ID-Score in this study. Here, the closed set is the training set used for the classification task, and the samples within this set are called ID samples. Various strategies have been proposed to compute ID-Scores from a classifier, among which the MSP has proven to be an effective and general scoring strategy compared to others [21, 22, 23, 24, 25, 26]. The classifier assigns higher ID-Scores to samples that belong to the ID set and lower scores to OOD samples. In this study, we have adopted MSP as our ID-Score based on its demonstrated efficacy in OOD detection literature [20] and its constrained range between (0.0, 1.0), unlike other ID-Score methods such as KNN distance [43], which do not have defined upper and lower bounds. We consistently employ MSP in our methodology, hypothesizing that an MSP value of 0.5 (we call this value boundary confidence level and denote it as $\\gamma$ ) signifies regions near the classifier\u2019s decision boundary. Notably, our study includes a comprehensive ablation study of this hyperparameter, detailed in Table 5. ", "page_idx": 3}, {"type": "text", "text": "Adversarial Risk. Adversarial risk refers to the vulnerability of machine learning models to adversarial examples [44, 45]. Previous work has established bounds on this metric via function transformation [46], PAC-Bayesian [47], sparsity-based compression [48], optimal transport and couplings [49], or in terms of input dimension [50]. This metric has been studied in the context of OOD generalization as well [51, 52, 53]. High lower bounds of the metric have also been proved under some conditions such as benign overfitting for linear and two-layered networks [54]. ", "page_idx": 3}, {"type": "text", "text": "For an extended related work, see Appendix Section E. ", "page_idx": 3}, {"type": "image", "img_path": "m296WJXyzQ/tmp/843a9ec4731fece1017c42a6f84765f794fc39c54067f7bafb28d3f249aa991d.jpg", "img_caption": ["Figure 2: The effect of using near-OOD samples Given a trojaned classifier trained on CIFAR10, due to the presence of blind spots in the learned decision boundary, it is easier to increase the IDScore of near-OOD samples (a fish is considered as near-OOD for CIFAR10) than that of far-OOD samples (samples from MNIST are far-OOD for CIFAR10). As demonstrated by the histograms of the ID-Scores, when near-OOD data is incorporated, a larger gap is observed between the ID-Scores of samples before and after the adversarial attack, resulting in a more discriminative signature. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Threat Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Attacker Capabilities and Goals ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the context of attacker capabilities, adversaries can poison training data [4, 14] or manipulate the training process [5, 55] to embed backdoors within models. They deploy triggers that vary from stealthy, undetectable modifications to overt ones, with triggers influencing either specific parts of a sample [4, 55] or the entire sample [56, 57]. Additionally, attackers can target individual samples [58] to evade detection or use label-consistent mechanisms, where poisoned inputs align with their visible content, leading to inference misclassification [59, 56]. Attacks typically follow either an All-to-One pattern, where any input with a trigger is classified into a single target class, or an All-to-All pattern, where a target class is chosen for each source class to ensure any input with a trigger is misclassified accordingly. These models may be trained either adversarially or non-adversarially, with attackers aiming to embed undetectable backdoors that evade detection efforts. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Defender Capabilities and Goals ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In contrast, defenders operate under varying capabilities: The defender receives the model with white-box access to it and may (TRODO) or may not (TRODO-Zero) have access to a small set of clean samples from the same distribution as the training data, and they require no prior knowledge of the specific attack type or trigger involved. Defender goals are to identify any embedded backdoors, and adapt effectively to scenarios with or without clean training samples. ", "page_idx": 4}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Overview. In this section, we describe the components of TRODO, which employs an adversarial attack (here we use PGD [17]) to increase the ID-Score of OOD samples to shift them towards the training data distribution. We then measure the magnitude of the difference in ID-Scores between OOD samples and their perturbed counterparts. We denote this as the ID-Score difference (\u2206ID-Score) and use it as a signature to scan for trojans. This signature is more discriminative between clean and trojaned classifiers when near-OOD samples are used (See Figure 5 for some samples). Unlike many existing trojan scanners, which fail in setups lacking training data, TRODO can successfully conduct scans owing to its robust and universal signature. Further details are provided in subsequent sections. The pseudocode of our scanning algorithm is provided in 1. ", "page_idx": 4}, {"type": "text", "text": "4.1 Design and Definition of TRODO\u2019s Signature ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "OOD Set Crafting. To obtain a set of OOD samples, we propose two scenarios. In the first scenario, a portion of the clean training data is available for the given classifier. Here, the OOD set is obtained by applying transformations known to compromise the semantic integrity of an image. Although the results of these transformations deviate from the ID characteristics, these transformed samples visually resemble ID ones. We utilize these as proxies for near-OOD samples. To ensure that the transformations significantly alter the sample characteristics and shift them far enough from the training data distribution, we define a set of hard transformations $\\mathcal{T}=\\{T_{i}\\}_{i=1}^{k}$ , with each $T_{i}$ representing a specific type of hard augmentation. For each ID sample $x$ , a random permutation of $\\tau$ is selected $\\{T_{j_{1}},T_{j_{2}},\\ldots,T_{j_{k}}\\}$ , and the transformations are sequentially applied, resulting in $T_{j_{k}}(.\\,.\\,.\\,T_{j_{1}}(x))$ . This method generates a diverse set of OOD samples, particularly valuable in environments with limited access to training data. Each transformed training sample $x$ becomes a crafted OOD sample $x^{\\prime}$ , with the transformation process denoted by $G(\\cdot)$ , i.e., $\\boldsymbol{x}^{\\prime}=G(\\boldsymbol{x})$ . We set $k=3$ as a rule of thumb. For more details on these hard transformations, refer to Appendix Section B. In the second scenario, where no training data is available, we employ a smaller dataset as the OOD set. Specifically, we utilize Tiny ImageNet [60] for this purpose. Considering that many training datasets (e.g., CIFAR-10 [61]) share concepts with our OOD set, we apply $G(\\cdot)$ on Tiny ImageNet samples before using them as the OOD set, ensuring that they do not reflect the training distribution characteristics. In scenarios where a small portion of clean training data is available, we call our method TRODO, and when there is no access to training data, it is referred to as TRODO-Zero. ", "page_idx": 4}, {"type": "text", "text": "Adversarial Attack on ID-Score. In this section, we formulate an adversarial attack on OOD samples to shift them toward the ID region. First, we define the maximum softmax probability (MSP) as the ID-Score, which is an indicator of the classifier\u2019s confidence in recognizing an input sample belonging to the ID. Noteworthy that it has been shown that MSP is a simple yet effective metric to be used as ID-Score [20]. The adversarial perturbation aims to find a shortcut path to increase the ID-Score, effectively shifting the OOD sample toward the blind spots of the trojaned classifier. This process results in a significant increase in the ID-Score, highlighting the introduced signature. Formally, the PGD attack to the ID-Score for a sample $x$ corresponding to a classifier $f$ can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ(f(x))=\\mathrm{ID}\\cdot\\mathrm{Score}_{f}(x),\\quad x^{0*}=x,\\quad x^{t+1*}=\\Pi_{x+S}(x^{t*}+\\alpha\\cdot\\mathrm{sign}\\left(\\nabla_{x}J(f(x^{t*}))\\right))\\,,\\quad x^{*}=x^{N*}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the noise is projected on the $\\ell_{2}$ norm ball $\\boldsymbol{S}$ with radius $\\epsilon$ around $x$ in each step: $\\|\\boldsymbol{x}^{t*}-\\boldsymbol{x}\\|_{2}\\leq\\epsilon$ ", "page_idx": 5}, {"type": "text", "text": "To define our signature, we assume a set of OOD samples denoted as $D_{00\\mathrm{D}}=\\{x_{i}^{\\mathrm{OOD}}\\}$ is available. For a given classifier $f$ , we define our signature $S(f,D_{\\mathrm{OOD}})$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nS_{i}(f,D_{00\\mathrm{D}})=\\mathrm{ID-Score}_{f}(x_{i}^{\\mathrm{oup}*})-\\mathrm{ID-Score}_{f}(x_{i}^{\\mathrm{oup}}),\\ \\ S(f,D_{00\\mathrm{D}})={\\frac{\\sum_{i=1}^{|D_{00\\mathrm{D}}|}S_{i}(f,D_{00\\mathrm{D}})}{|D_{00\\mathrm{D}}|}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{i}^{\\mathrm{{OOD}^{*}}}$ is obtained by adding adversarial perturbation to $x_{i}^{\\mathrm{{OOD}}}$ via a PGD attack mentioned in above equation 1. ", "page_idx": 5}, {"type": "text", "text": "A higher value of $S(f,D_{\\mathrm{OOD}})$ indicates that $f$ is trojaned with higher probability. To detect whether a classifier $f$ is trojaned, we utilize a validation set and a thresholding mechanism, which is well described in the next part. ", "page_idx": 5}, {"type": "text", "text": "4.2 Validation Data Utilization in TRODO ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Leveraging Validation Set for Trojan Scanning. In this study, we assume access to a benign validation set denoted as $D_{v}$ (e.g., Tiny ImageNet), which is realistic given the abundance of available datasets in real-world scenarios. We craft an OOD set $D_{\\mathrm{{OOD}}}$ by applying the mentioned strategy, i.e., $D_{00\\mathrm{D}}=G(D_{v})$ . Note that we apply harsh augmentations to ensure that the OOD dataset does not belong to ID (in case the validation dataset\u2019s distribution resembles training data distribution). These datasets are used for computing $\\epsilon$ for our Projected Gradient Descent (PGD) attack as mentioned in the above equations. Moreover, leveraging them, we propose a threshold mechanism to determine whether an input classifier is trojaned, using the signature $S(f,D_{\\mathrm{OOD}})$ . ", "page_idx": 5}, {"type": "text", "text": "Initially, we note that the ID-Score of an OOD sample $x$ resembles a uniform distribution $\\mathcal{U}(K)$ , and the ID-Score $_{f}(x_{\\mathrm{OOD}})$ is approximately equal to $\\textstyle{\\frac{1}{k}}$ , where $k$ denotes the number of classes in the training data. We propose that an effective $\\epsilon$ should shift OOD samples toward ID regions. We consider 0.5 as a hyperparameter, denoted by $\\gamma$ , which we refer to as the boundary confidence level. As a result, we propose computing $\\epsilon$ by finding the minimum perturbation that can increase the ID-Score (i.e., MSP) from $\\textstyle{\\frac{1}{k}}$ to 0.5 for the crafted OOD set $D_{\\mathrm{{OOD}}}$ , corresponding to a surrogate classifier $g$ as a clean trained model. Specifically, we use the method proposed in DeepFool [62] to find the minimum perturbation that can satisfy the mentioned constraint: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\epsilon=\\arg\\operatorname*{min}_{\\delta}\\|\\delta\\|_{2}\\quad\\mathrm{subject}\\;\\mathrm{to}\\quad\\frac{\\sum_{x\\in D_{\\mathrm{oop}}}\\Pi_{\\mathrm{-}}\\mathrm{Score}_{g}(x+\\delta)}{|D_{\\mathrm{oop}}|}\\geq\\gamma.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Threshold Computing. Once the signature value $S(f,D_{\\mathrm{OOD}})$ has been computed for the given classifier $f$ , it is critical to determine whether $f$ has been compromised by a trojan, using a thresholdbased strategy. This process is achieved by employing a statistical test on a set of scores computed for a surrogate classifier $g$ . Specifically, given the surrogate classifier $g$ and the OOD set $D_{\\mathrm{{OOD}}}$ , we generate a set of baseline scores denoted as $\\{S_{i}(g,D_{\\mathrm{OOD}})\\}_{i=1}^{N}$ . These scores represent the signature values assigned by a clean classifier $g$ . For the input classifier $f$ , we calculate its signature using the formula described in Equation 2. When the model is trojaned, its corresponding signature will be an outlier to the distribution of $S_{i}(g,D_{\\mathrm{OOD}})$ . We estimate this null distribution with a Normal distribution to find a threshold $\\tau$ satisfying $P r o b(\\operatorname*{max}_{i=1,...,N}-l o g(1-S_{i}(g,D_{00\\mathrm{D}}))\\leq\\tau)>0.95.$ Solving for $\\tau$ , gives the following threshold: $\\tau=\\Phi^{-1}(\\mathbf{\\Omega}_{\\sqrt{0.95}}^{N})$ , where $\\Phi$ is the CDF of our estimated ldistributi W ", "page_idx": 5}, {"type": "text", "text": "5 Theoretical Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide theoretical insights that underline the susceptibility of trojaned models to adversarial perturbations, particularly in near-OOD regions. ", "page_idx": 5}, {"type": "text", "text": "Notation. In this section, L1 and L2 norms are denoted by $|.|$ and $\\lVert.\\rVert$ respectively. $Y=\\Omega(X)$ is equivalent to $Y\\ge c X$ for all $X\\geq X_{0}$ where $c,X_{0}\\in\\mathbb{R}^{+}$ are some constants. For vectors $\\boldsymbol{x}=(\\boldsymbol{x}_{i})_{i=1}^{d}$ , $\\widehat{\\gamma}=(\\widehat{\\gamma_{i}})_{i=1}^{d}$ , and function $h$ , we define: $\\begin{array}{r}{x^{\\gamma}=x_{1}^{\\gamma_{1}}\\cdot\\cdot\\cdot x_{d}^{\\gamma_{d}},\\nabla_{x}^{\\gamma}h=\\frac{\\partial^{|\\gamma|}{h}}{\\partial_{x_{1}}^{\\gamma_{1}}\\cdot\\cdot\\cdot\\partial_{x_{d}}^{\\gamma_{d}}},\\nabla_{x}h=[\\frac{\\partial h}{\\partial_{x_{1}}},\\cdot\\cdot\\cdot,\\frac{\\partial h}{\\partial_{x_{d}}}]^{\\top}}\\end{array}$ \u2202\u2202xhd ]\u22a4, and \u03b3! = \u03b31! . . . \u03b3d!. ", "page_idx": 5}, {"type": "text", "text": "We aim to show that a neural network is more sensitive to adversarial perturbations when it receives a backdoor attack, especially in near-OOD data. Let $h(w,x):\\mathbb{R}^{d_{w}}\\times\\mathbb{R}^{d_{x}}\\rightarrow\\mathbf{\\hat{R}}$ be a black-box function (e.g., loss or ", "page_idx": 5}, {"type": "text", "text": "output of a neural network) with learnable parameters $w$ and input $x$ . Adversarial risk of $h$ in radius $\\alpha$ under a distribution $\\mathcal{P}$ is defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\alpha}^{\\mathcal{P}}(h,w):=\\mathbb{E}_{x\\sim\\mathcal{P}}\\left[\\operatorname*{sup}_{\\|\\delta\\|\\leq\\alpha}h(w,x+\\delta)-h(w,x)\\right]\\approx\\alpha\\mathbb{E}_{x\\sim\\mathcal{P}}\\|\\nabla_{x}h(w,x)\\|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The approximation converges as $\\alpha\\rightarrow0$ , thus we use the last term in our analysis similar to [50, 54]. ", "page_idx": 6}, {"type": "text", "text": "We formulate a near-OOD around $\\mathcal{P}$ by shifting only the moments of an order $k$ . Formally, for any $k\\in\\mathbb{N}$ and $s\\in\\mathbb{R}$ , we define $\\mathcal{P}_{+s}^{k}$ by $\\mathbb{E}_{x\\sim\\mathcal{P}_{+s}^{k}}\\left[x^{v}\\right]=\\mathbb{E}_{x\\sim\\mathcal{P}}\\left[x^{v}\\right]+s$ for any $v\\in\\mathbb{N}_{0}^{d_{x}}$ with $|v|=k$ , and $\\mathbb{E}_{x\\sim\\mathcal{P}_{+s}^{k}}\\left[x^{u}\\right]=$ $\\mathbb{E}_{x\\sim\\mathcal{P}}\\left[x^{u}\\right]$ for any $u\\in\\mathbb{N}_{0}^{d_{x}}$ with $|u|\\ne k$ . The following theorem shows that the adversarial risk under $\\mathcal{P}_{+s}^{k}$ will increase linearly in terms of $|s|$ . The proof is given in Appendix Section F. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. (Adversarial risk in near-OOD) ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\alpha}^{\\mathcal{P}_{+s}^{k}}(h,w)\\geq\\alpha|s|\\operatorname*{max}_{x}\\|\\nabla_{x}\\sum_{|\\gamma|=k}\\frac{\\nabla_{x}^{\\gamma}h(w,x)}{\\gamma!}\\|-\\alpha\\|\\mathbb{E}_{x\\sim\\mathcal{P}}\\nabla_{x}h(w,x)\\|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 1. Theorem 1 is applicable when $\\nabla_{x_{i}}^{k+1}h\\neq0$ which is usually true if $h$ contains non-linear exponential activation functions (e.g., softmax, sigmoid, tanh, ELU, and SELU) being infinitely many times differentiable, or if it contains polynomial activation functions with total degree greater than $k+1$ . Under this assumption, if we consider $h(w,.)$ as a fixed model trained on a fixed distribution $\\mathcal{P}$ , then the only variable in the lower bound will be $|s|$ hence we conclude $\\mathcal{R}_{\\alpha}^{\\mathcal{P}_{+s}^{k}}(h,w)=\\Omega(|s|)$ . ", "page_idx": 6}, {"type": "text", "text": "We now study how the adversarial risk will increase under a backdoor attack. Let $\\mathcal{D}=\\{(x_{i},y_{i})=w^{\\star\\top}x_{i}):$ $1\\leq i\\leq n\\}$ with $x_{i}\\stackrel{i i d}{\\sim}\\mathcal{P}$ be the clean training set, $\\mathcal{D}^{\\prime}=\\{(x_{i}^{\\prime}+t,y_{c}):1\\leq i\\leq m\\}$ with $x_{i}^{\\prime}\\overset{i i d}{\\sim}\\mathcal{P}$ be the poisoned training set, $t\\in\\mathbb{R}^{d_{x}}$ be the trigger, and $y_{c}$ be the target class of the attack. We consider $\\hat{w}$ as the optimal solution of the least square optimization on the data $\\mathcal{D}\\cup\\mathcal{D}^{\\prime}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{w}=\\underset{w}{\\arg\\operatorname*{min}}\\left(\\sum_{i=1}^{n}(h(w,x_{i})-y_{i})^{2}+\\sum_{i=1}^{m}(h(w,(x_{i}^{\\prime}+t))-y_{c})^{2}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We focus on linear and two-layer networks defined as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\nh_{1}(\\boldsymbol{w},\\boldsymbol{x})=\\boldsymbol{w}^{\\top}\\boldsymbol{x},\\quad h_{2}(\\boldsymbol{w},\\boldsymbol{x})=\\frac{1}{\\sqrt{l d_{x}}}\\sum_{j=1}^{l}u_{j}\\mathrm{ReLU}(\\theta_{j}^{T}\\boldsymbol{x}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where in the latter $w=[\\theta_{j}^{\\top},u_{j}]_{j=1}^{l}\\in\\mathbb{R}^{l(d_{x}+1)}$ represents the vectorized parameters of the network, with each pair $[\\theta_{j}^{\\top},u_{j}]\\in\\mathbb{R}^{d_{x}+1}$ , and $\\mathrm{ReLU}(z)=\\operatorname*{max}\\{0,z\\}$ is the activation function. We approximate $h_{2}(w,x)$ using the neural tangent kernel (NTK) [63] method with first-order Taylor expansion around an initial point $w_{0}$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{h_{2}}(w,x)=h_{2}(w_{0},x)+\\nabla_{w}h_{2}\\big(w_{0},x\\big)^{T}\\big(w-w_{0}\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We use the same gradient descent training process as in [54]. The following theorem shows that as the ratio of triggered samples, i.e., $\\frac{m}{n}$ , or the norm of the trigger $t$ increases, then the adversarial risk will also increase linearly. The proof is given in Appendix Section F. ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. (Adversarial risk after backdoor attack) for $h\\in\\{h_{1},\\tilde{h_{2}}\\}$ , if $\\hat{w}$ is learned through the Equation 4 on a fixed training distribution $\\mathcal{P}$ , we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathcal{R}_{\\alpha}^{\\mathcal{P}}(h,\\hat{w})=\\Omega\\left(\\frac{m}{n}\\|t\\|\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluated our proposed method across a diverse range of benchmarks and compared its performance with various existing scanning methods. We developed our benchmark, which includes models trained on a broad spectrum of image datasets. This benchmark includes trojaned models for which various attack scenarios have been considered. The results of these experiments are provided in Table 1. Furthermore, we present an evaluation of TrojAI in Table 2 as a challenging benchmark. ", "page_idx": 6}, {"type": "text", "text": "Baselines. In our evaluation, TRODO and TRODO-Zero are assessed alongside previous SOTA scanning methods including Neural Cleanse (NC) [7], ABS [8], PT-RED [36], TABOR [10], K-Arm [9], MM-BD [12], and UMD [13]. Performance details are in Table 1, with further information in Appendix Section I and K. ", "page_idx": 6}, {"type": "text", "text": "Table 1: Scanning performance of TRODO compared with other methods, in terms of Accuracy on standard trained evaluation sets $(\\mathrm{ACC}\\,\\%)$ and adversarially trained ones $(\\mathrm{ACC}^{*}\\ \\%)$ ). The best results are emphasized in bold format respectively in each column. ", "page_idx": 7}, {"type": "table", "img_path": "m296WJXyzQ/tmp/d578a9dde4086dc8eda05dcaf641a3e6e46e32a4d02af7f3b1170015a7e0ae70.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "m296WJXyzQ/tmp/26abc7b6c8b94b26253a9b5bbf1c02d515ce12029a5adf20493a86b97236a0da.jpg", "table_caption": ["Table 2: Comparison of TRODO and other methods on all released rounds of TrojAI benchmark on image classification task. For each method, we reported scanning Accuracy and the average scanning time for the classifiers. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Implementation Details. As stated earlier, we used Tiny ImageNet as our validation set to tune our hyperparameters $\\epsilon$ and $\\tau$ (scanning threshold); details are provided in Table 10. We used PGD-10 as the adversarial attack. Our experiments on our method and other baselines were conducted on a single RTX 3090 GPU. ", "page_idx": 7}, {"type": "text", "text": "Our Designed Benchmark. We developed a benchmark to model real-world scanning scenarios, including various datasets, classifiers, trojan attacks, and label mappings. This benchmark covers both standard and adversarial training methods, ensuring a comprehensive evaluation of scanning methods. Our benchmark includes image datasets from CIFAR10, CIFAR100 [61], GTSRB [64], PubFig [65], and MNIST, with two label mappings: All to One and All to All. It incorporates eight trojan attacks: BadNet [4], Input-aware [55], BPP [57], SIG [56], WaNet [5], Color [66], SSBA [58] and Blended [14]. Each combination of a dataset and label mapping has 320 models: 20 trojaned models per attack and 160 clean models (check Appendix Section N for more details). Both standard and adversarial training were employed. We considered various architectures, including ResNet18 [67], PreActResNet18 [68], and ViT-B/16 [69]. While previous works focused on CNN-based architectures, our experiments are more general. Table 1 presents the evaluation of ResNet18; evaluations of other architectures are in Appendix Section O, with more details on our benchmark creation in Appendix Section K. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "TrojAI Benchmark. The TrojAI [35] benchmark, developed by IARPA, addresses backdoor detection challenges and includes test, hold-out, and training sets with nearly half of the models being trojaned. These models may have various backdoor triggers, such as pixel patterns and fliters, activated under specific conditions. More details are in Appendix Section K. ", "page_idx": 8}, {"type": "text", "text": "Analysis of the Results. As the results indicate, presented in Tables 1 and 2, TRODO surpasses previous scanning methods by a large margin in terms of accuracy and time. Specifically, TRODO achieves superior performance with an $11.4\\%$ improvement in scenarios where trojan classifiers have been trained in a standard (non-adversarial) setting and a $24.8\\%$ improvement in scenarios where trojan classifiers have been adversarially trained. Our method demonstrates superior performance in both All-to-One and All-to-All scenarios, highlighting the generality of our proposed method. Notably, TRODO-Zero, which operates without access to any training samples, preserves significant performance compared to other methods, with only a minor drop in performance compared to TRODO. The same trend holds on TrojAI, a well-known and challenging benchmark. Regarding scanning time, as shown in Table 2, TRODO demonstrates high computational efficiency, achieving competitive accuracy with significantly lower scanning time compared to other methods. This is mainly due to the simple yet effective signature it uses to scan for trojans. Further experimental results, including error bars, qualitative visualizations, and the limitations of our work, can be found in the Appendix Section O. ", "page_idx": 8}, {"type": "text", "text": "Adaptive Attack. In our analysis of Adaptive Attacks on TRODO, we define two strong approaches aimed at circumventing the model\u2019s defense mechanism. The first adaptive strategy trains a classifier with a custom loss function designed to equalize the confidence level (ID-Score) for both in-distribution (ID) and out-of-distribution (OOD) samples. This loss function, defined as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{\\mathrm{adapivel}}=\\mathbb{E}_{(x,y)\\sim D_{\\mathrm{in}}}\\left[-\\log f_{y}(x)\\right]-\\lambda_{1}\\mathbb{E}_{(z,y)\\sim D_{\\mathrm{out}}}\\left[H(U;f(z))\\right]+\\lambda_{2}\\mathbb{E}_{(x,y)\\sim D_{\\mathrm{in}}}\\left[H(U;f(x))\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $x,y$ are data samples and their labels, $f_{y}(x)$ denotes the $y$ -th output of the classifier, $U$ is the uniform distribution over classes, and $H$ is the cross-entropy. The first term is the classification term (cross-entropy), while the other terms force the classifier to decrease MSP (ID-Score) for ID samples while increasing it for OOD samples. Setting $\\lambda_{1}=\\lambda_{2}=0.5$ , inspired by [70], balances the importance of the first term. By this loss function, we hope the ID-Score for both OOD and ID samples will be altered, though the classifier\u2019s decisions remain fixed. ", "page_idx": 8}, {"type": "text", "text": "Additionally, we introduce a second loss function targeting TRODO\u2019s detection signature by reducing the ID-Score gap between benign and perturbed OOD samples, making it challenging for TRODO to distinguish trojaned classifiers from clean ones. This second loss function is defined as ", "page_idx": 8}, {"type": "equation", "text": "$$\nL_{\\mathrm{adapive2}}=\\mathbb{E}_{(x,y)\\sim D_{\\mathrm{in}}}\\left[-\\log f_{y}(x)\\right]-\\lambda_{3}\\mathbb{E}_{(z,y)\\sim D_{\\mathrm{out}}}\\left[H(f(x);f(x^{*}))\\right]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $x^{*}$ denotes the adversarially perturbed sample. Although these attacks attempt to subvert our defense, TRODO\u2019s use of random transformations in creating OOD samples provides resilience, as these transformations hinder the model\u2019s ability to learn patterns that could be exploited by an adaptive adversary. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Performance comparison of TRODO under different adaptive attacks across various datasets, in terms of Accuracy on standard trained evaluation sets $\\left.\\mathrm{ACC}\\right.\\%$ ) and adversarially trained ones $(\\mathrm{ACC}^{*}\\ \\%)$ . ", "page_idx": 8}, {"type": "table", "img_path": "m296WJXyzQ/tmp/1aeb82fc5f2f47a5cb6356b3c70b2e060235f39fc9b46383111a5df01ebd170c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "7 Ablation Study ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ablation Study on Validation Dataset. To ascertain the robustness of TRODO against different datasets in the validation set, we conducted experiments using various datasets as the validation set, as presented in Table 4. In these experiments, we replaced our default validation dataset, Tiny ImageNet, with alternative datasets. Throughout these tests, all other elements of our methodology remained constant to isolate the impact of the validation dataset changes on TRODO\u2019s performance. Moreover, to quantitatively support our claim regarding the effectiveness of near-OOD samples compared to far-OOD samples, we provide the distance between the validation set and the target dataset. The target dataset refers to the ID set on which the input classifier has been trained. For computing this distance, we used the Fr\u00e9chet Inception Distance (FID) [71], a well-known metric for measuring distance in generative models. Lower FID values indicate a smaller distance, and vice versa. As the results indicate, in the near-OOD scenario, our method appears more effective. More details can be found in Appendix Section L. ", "page_idx": 9}, {"type": "text", "text": "Ablation Study on Boundary Confidence Level. We also conducted an ablation study on the boundary confidence level hyperparameter, denoted as $\\gamma$ , which is preset at 0.5 in our standard pipeline. By keeping all other variables constant and varying $\\gamma$ across a range of values, we assessed TRODO\u2019s sensitivity to this parameter. The results of these experiments are presented in the Table 5, illustrating how different settings of $\\gamma$ affect the effectiveness of TRODO (extra ablation studies are available in Appendix Section P). ", "page_idx": 9}, {"type": "text", "text": "Table 4: Accuracy of TRODO using various Validation (and OOD) datasets for different ID data. Each validation is used to find the hyperparameters ( $\\dot{\\epsilon}$ and $\\tau$ ) and also as OOD datasets to find signatures. You can see the effect of choosing near-OOD dataset. For example, for CIFAR10, STL-10 and Tiny ImageNet are better choices than the other two datasets ", "page_idx": 9}, {"type": "table", "img_path": "m296WJXyzQ/tmp/b29ffe8eb5119c338eb436b34721261345d35674209f18a29cf9dea4887d150e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "m296WJXyzQ/tmp/7e0e94acaa51d9f1a7d8c543c9e2d865e661d737a06aff65a55042eed4e3e78d.jpg", "table_caption": ["Table 5: Accuracy of our method with different boundary confidence level. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "8 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We acknowledge Mohammad Sabokrou for his contributions to this project. Mohammad Sabokrou\u2019s work in this project was supported by JSPS KAKENHI Grant Number 24K20806. ", "page_idx": 9}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In conclusion, this study presents TRODO, a robust and general method for scanning and identifying trojaned classifiers with low time and resource complexity. TRODO\u2019s strength lies in its ability to detect trojans in diverse scenarios, including those involving adversarially trained models. Interestingly, TRODO is applicable even in scenarios where no data is available. Our experimental results demonstrate TRODO\u2019s superior performance, achieving high accuracy across various attack types and benchmark datasets. The adaptability and effectiveness of our approach mark a significant advancement in enhancing the reliability and security of deep neural networks in critical applications. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Omkar Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition. In BMVC 2015-Proceedings of the British Machine Vision Conference 2015. British Machine Vision Association, 2015.   \n[2] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.   \n[3] Kenneth S Miller. On the inverse of the sum of matrices. Mathematics magazine, 54(2):67\u201372, 1981.   \n[4] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.   \n[5] Anh Nguyen and Anh Tran. Wanet\u2013imperceptible warping-based backdoor attack. arXiv preprint arXiv:2102.10369, 2021. [6] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[7] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707\u2013723, 2019.   \n[8] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs: Scanning neural networks for back-doors by artificial brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS \u201919, page 1265\u20131282, New York, NY, USA, 2019. Association for Computing Machinery.   \n[9] Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng, Shiqing Ma, and Xiangyu Zhang. Backdoor scanning for deep neural networks through k-arm optimization. arXiv preprint arXiv:2102.05123, 2021.   \n[10] Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach to inspecting and restoring trojan backdoors in ai systems, 2019.   \n[11] Xiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, and Chao Chen. Trigger hunting with a topological prior for trojan detection. In International Conference on Learning Representations, 2022.   \n[12] H. Wang, Z. Xiang, D. J. Miller, and G. Kesidis. Mm-bd: Post-training detection of backdoor attacks with arbitrary backdoor pattern types using a maximum margin statistic. In 2024 IEEE Symposium on Security and Privacy (SP), pages 19\u201319, Los Alamitos, CA, USA, may 2024. IEEE Computer Society.   \n[13] Zhen Xiang, Zidi Xiong, and Bo Li. UMD: Unsupervised model detection for X2X backdoor attacks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 38013\u201338038. PMLR, 23\u201329 Jul 2023.   \n[14] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.   \n[15] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in neural information processing systems, 31, 2018.   \n[16] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[17] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[18] Alexander Tsigler, Gabor Lugosi, Peter Bartlett, and Phil Long. Benign overfitting in linear regression. PNAS, 117(48):30063\u201330070, 2020.   \n[19] Marzieh Edraki, Nazmul Karim, Nazanin Rahnavard, Ajmal Mian, and Mubarak Shah. Odyssey: Creation, analysis and detection of trojan models. IEEE Transactions on Information Forensics and Security, 16:4521\u20134533, 2021.   \n[20] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017.   \n[21] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.   \n[22] Shu Kong and Deva Ramanan. Opengan: Open-set recognition via open data generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 813\u2013822, 2021.   \n[23] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. Advances in Neural Information Processing Systems, 34:7068\u20137081, 2021.   \n[24] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.   \n[25] Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gr\u00e9goire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert M\u00fcller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756\u2013795, 2021.   \n[26] Mohammadreza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, and Mohammad Sabokrou. A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges. arXiv preprint arXiv:2110.14051, 2021.   \n[27] Mohammad Azizmalayeri, Arshia Soltani Moakhar, Arman Zarei, Reihaneh Zohrabi, Mohammad Manzuri, and Mohammad Hossein Rohban. Your out-of-distribution detection method is not robust! Advances in Neural Information Processing Systems, 35:4887\u20134901, 2022.   \n[28] Shao-Yuan Lo, Poojan Oza, and Vishal M Patel. Adversarially robust one-class novelty detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[29] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Robust out-of-distribution detection for neural networks. arXiv preprint arXiv:2003.09711, 2020.   \n[30] Rui Shao, Pramuditha Perera, Pong C Yuen, and Vishal M Patel. Open-set adversarial defense. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVII 16, pages 682\u2013698. Springer, 2020.   \n[31] Rui Shao, Pramuditha Perera, Pong C Yuen, and Vishal M Patel. Open-set adversarial defense with clean-adversarial mutual learning. International Journal of Computer Vision, 130(4):1070\u20131087, 2022.   \n[32] Louis B\u00e9thune, Paul Novello, Thibaut Boissin, Guillaume Coiffier, Mathieu Serrurier, Quentin Vincenot, and Andres Troya-Galvis. Robust one-class classification with signed distance function using 1-lipschitz neural networks. arXiv preprint arXiv:2303.01978, 2023.   \n[33] Adam Goodge, Bryan Hooi, See Kiong $\\mathrm{Ng}$ , and Wee Siong Ng. Robustness of autoencoders for anomaly detection under adversarial impact. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 1244\u20131250, 2021.   \n[34] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distribution detection using outlier mining. In Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13\u201317, 2021, Proceedings, Part III 21, pages 430\u2013445. Springer, 2021.   \n[35] Kiran Karra, Chace Ashcraft, and Neil Fendley. The trojai software framework: An opensource tool for embedding trojans into deep learning models, 2020.   \n[36] Zhen Xiang, David J. Miller, and George Kesidis. Detection of backdoors in trained classifiers without access to the training set. IEEE Transactions on Neural Networks and Learning Systems, 33(3):1177\u20131191, 2022.   \n[37] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 4658\u20134664. International Joint Conferences on Artificial Intelligence Organization, 7 2019.   \n[38] Chong Fu, Xuhong Zhang, Shouling Ji, Ting Wang, Peng Lin, Yanghe Feng, and Jianwei Yin. Freeeagle: Detecting complex neural trojans in data-free cases, 2023.   \n[39] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 301\u2013310, 2020.   \n[40] Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting ai trojans using meta neural analysis. In 2021 IEEE Symposium on Security and Privacy $(S P)$ , pages 103\u2013120. IEEE, 2021.   \n[41] Zhen Xiang, Zidi Xiong, and Bo Li. Cbd: A certified backdoor detector based on local dominant probability. Advances in Neural Information Processing Systems, 36, 2024.   \n[42] Xiaoyu Zhang, Rohit Gupta, Ajmal Mian, Nazanin Rahnavard, and Mubarak Shah. Cassandra: Detecting trojaned networks from adversarial perturbations. IEEE Access, 9:135856\u2013135867, 2021.   \n[43] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In International Conference on Machine Learning, pages 20827\u201320840. PMLR, 2022.   \n[44] Jonathan Uesato, Brendan O\u2019Donoghue, Pushmeet Kohli, and Aaron van den Oord. Adversarial risk and the dangers of evaluating against weak attacks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5025\u20135034. PMLR, 10\u201315 Jul 2018.   \n[45] Arun Sai Suggala, Adarsh Prasad, Vaishnavh Nagarajan, and Pradeep Ravikumar. Revisiting adversarial risk. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 2331\u20132339. PMLR, 16\u201318 Apr 2019.   \n[46] Justin Khim and Po-Ling Loh. Adversarial risk bounds via function transformation, 2019.   \n[47] Waleed Mustafa, Philipp Liznerski, Antoine Ledent, Dennis Wagner, Puyu Wang, and Marius Kloft. Non-vacuous generalization bounds for adversarial risk in stochastic neural networks. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 4528\u20134536. PMLR, 02\u201304 May 2024.   \n[48] Emilio Rafael Balda, Arash Behboodi, Niklas Koep, and Rudolf Mathar. Adversarial risk bounds for neural networks through sparsity based compression, 2019.   \n[49] Muni Sreenivas Pydi and Varun Jog. Adversarial risk via optimal transport and optimal couplings. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 7814\u20137823. PMLR, 13\u201318 Jul 2020.   \n[50] Carl-Johann Simon-Gabriel, Yann Ollivier, Leon Bottou, Bernhard Sch\u00f6lkopf, and David Lopez-Paz. First-order adversarial vulnerability of neural networks and input dimension. In International conference on machine learning, pages 5809\u20135817. PMLR, 2019.   \n[51] Xin Zou and Weiwei Liu. On the adversarial robustness of out-of-distribution generalization models. Advances in Neural Information Processing Systems, 36, 2024.   \n[52] Stanislav Fort. Adversarial vulnerability of powerful near out-of-distribution detection. arXiv preprint arXiv:2201.07012, 2022.   \n[53] Maximilian Augustin, Alexander Meinke, and Matthias Hein. Adversarial robustness on in-and outdistribution improves explainability. In European Conference on Computer Vision, pages 228\u2013245. Springer, 2020.   \n[54] Yifan Hao and Tong Zhang. The surprising harmfulness of benign overfitting for adversarial robustness, 2024.   \n[55] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. Advances in Neural Information Processing Systems, 33:3454\u20133464, 2020.   \n[56] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 101\u2013105. IEEE, 2019.   \n[57] Zhenting Wang, Juan Zhai, and Shiqing Ma. Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15074\u201315084, 2022.   \n[58] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 16463\u201316472, 2021.   \n[59] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. arXiv preprint arXiv:1912.02771, 2019.   \n[60] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: a large-scale hierarchical image database. pages 248\u2013255, 06 2009.   \n[61] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[62] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks, 2016.   \n[63] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \n[64] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In The 2011 international joint conference on neural networks, pages 1453\u20131460. IEEE, 2011.   \n[65] Neeraj Kumar, Alexander C Berg, Peter N Belhumeur, and Shree K Nayar. Attribute and simile classifiers for face verification. In 2009 IEEE 12th international conference on computer vision, pages 365\u2013372. IEEE, 2009.   \n[66] Wenbo Jiang, Hongwei Li, Guowen Xu, and Tianwei Zhang. Color backdoor: A robust poisoning attack in color space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8133\u20138142, 2023.   \n[67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.   \n[68] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks, 2016.   \n[69] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.   \n[70] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. Proceedings of the International Conference on Learning Representations, 2019.   \n[71] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G\u00fcnter Klambauer, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a nash equilibrium. CoRR, abs/1706.08500, 2017.   \n[72] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. Albumentations: Fast and flexible image augmentations. Information, 11(2), 2020.   \n[73] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation, 2021.   \n[74] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[75] Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In International Conference on Learning Representations, 2021.   \n[76] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting outof-distribution samples and adversarial attacks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.   \n[77] Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022, 2021.   \n[78] A. Bendale and T. E. Boult. Towards open set deep networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1563\u20131572, Los Alamitos, CA, USA, jun 2016. IEEE Computer Society.   \n[79] Niladri S Chatterji and Philip M Long. Foolish crowds support benign overfitting. Journal of Machine Learning Research, 23(125):1\u201312, 2022.   \n[80] Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overftiting in two-layer convolutional neural networks. Advances in neural information processing systems, 35:25237\u201325250, 2022.   \n[81] Xingyu Xu and Yuantao Gu. Benign overfitting of non-smooth neural networks beyond lazy training. In International Conference on Artificial Intelligence and Statistics, pages 11094\u201311117. PMLR, 2023.   \n[82] Yiwen Kou, Zixiang Chen, Yuanzhou Chen, and Quanquan Gu. Benign overfitting in two-layer relu convolutional neural networks. In International Conference on Machine Learning, pages 17615\u201317659. PMLR, 2023.   \n[83] Moritz Haas, David Holzm\u00fcller, Ulrike Luxburg, and Ingo Steinwart. Mind the spikes: Benign overftiting of kernels and neural networks in fixed dimension. Advances in Neural Information Processing Systems, 36, 2024.   \n[84] Lisha Chen, Songtao Lu, and Tianyi Chen. Understanding benign overfitting in gradient-based meta learning. Advances in Neural Information Processing Systems, 35:19887\u201319899, 2022.   \n[85] Neil Mallinar, James Simon, Amirhesam Abedsoltan, Parthe Pandit, Misha Belkin, and Preetum Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. Advances in Neural Information Processing Systems, 35:1182\u20131195, 2022.   \n[86] Zhu Li, Zhi-Hua Zhou, and Arthur Gretton. Towards an understanding of benign overfitting in neural networks. arXiv preprint arXiv:2106.03212, 2021.   \n[87] Alexander Tsigler and Peter L Bartlett. Benign overftiting in ridge regression. Journal of Machine Learning Research, 24(123):1\u201376, 2023.   \n[88] Ke Wang and Christos Thrampoulidis. Benign overftiting in binary classification of gaussian mixtures. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4030\u20134034. IEEE, 2021.   \n[89] Zhu Li, Weijie J Su, and Dino Sejdinovic. Benign overftiting and noisy features. Journal of the American Statistical Association, 118(544):2876\u20132888, 2023.   \n[90] Guy Kornowski, Gilad Yehudai, and Ohad Shamir. From tempered to benign overfitting in relu neural networks. Advances in Neural Information Processing Systems, 36, 2024.   \n[91] Xuran Meng, Difan Zou, and Yuan Cao. Benign overftiting in two-layer relu convolutional neural networks for xor data. arXiv preprint arXiv:2310.01975, 2023.   \n[92] Jinghui Chen, Yuan Cao, and Quanquan Gu. Benign overftiting in adversarially robust linear classification. In Uncertainty in Artificial Intelligence, pages 313\u2013323. PMLR, 2023.   \n[93] Amartya Sanyal, Puneet K Dokania, Varun Kanade, and Philip HS Torr. How benign is benign overftiting? arXiv preprint arXiv:2007.04028, 2020.   \n[94] Xinwei Liu, Xiaojun Jia, Jindong Gu, Yuan Xun, Siyuan Liang, and Xiaochun Cao. Does few-shot learning suffer from backdoor attacks? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19893\u201319901, 2024.   \n[95] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[96] Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. Advances in neural information processing systems, 31, 2018.   \n[97] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation, 2020.   \n[98] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Backdoorbench: A comprehensive benchmark of backdoor learning. Advances in Neural Information Processing Systems, 35:10546\u201310559, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Benign Overfitting of Trojaned Classifiers ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "m296WJXyzQ/tmp/ab1f92fd0a657fd8ad65e415190beb67cd808ba03c859594879eb34704c222d7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 3: Model accuracy across different architectures and datasets. Trojaned models for all backdoor attacks show a consistent slight decrease in accuracy compared to clean models, suggesting benign overfitting in Trojaned classifiers. ", "page_idx": 15}, {"type": "image", "img_path": "m296WJXyzQ/tmp/7705b53d0e0d7ef992a7ec15172f322b71ff38b3e6102784d57040822c3030a7.jpg", "img_caption": ["Figure 4: The effect of overlaying triggers on OOD data, in various attacks. As demonstrated, applying the trigger (which is used to poison training data) on even far-OOD samples, fools the model into identifying them as ID. This is due to the benign overftiting on the trigger present in the training data. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B Examples of crafted Near OOD samples For Various Datasets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We provided images of some near-OOD data corresponding to random samples of our dataset (see Figure 5). ", "page_idx": 16}, {"type": "image", "img_path": "m296WJXyzQ/tmp/dbd104979b64f9a833511f71018cc5ed1a1b2f6f39fe2812ab590c0f2629c0a7.jpg", "img_caption": ["Figure 5: Examples of ID samples and their corresponding crafted near-OOD samples. We used Elastic [72], random rotations, and cutpaste [73]. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Robust OOD detection in Adversarially Trained Classifiers ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Adversarial training methods AT [74] and HAT [75], designed to enhance model robustness by exposing the classifier to perturbed data during training, generally improve a model\u2019s resilience against adversarial attacks within its training distribution. However, studies [27, 29] indicate a potential limitation when such classifiers are evaluated in OOD detection tasks, where a small perturbation in attack can cause a sample from the closed set to be classified as an anomaly and vice-versa. This limitation arises because the models do not consider samples from the open set during training. We provide Table 6 from [27] that highlights the issue. ", "page_idx": 17}, {"type": "table", "img_path": "m296WJXyzQ/tmp/200f9c711c17aaee1f2f45bb9c27e27d6dd7329f1bad99f09fad41599f76c91c.jpg", "table_caption": ["Table 6: OOD detection AUROC under attack with \u03f5 =2855 for various methods trained with CIFAR10 or CIFAR-100 as the training (closed) set. A clean evaluation indicates no attack on the data, whereas an attack evaluation means that out and in data is attacked. The best and second-best results are distinguished with bold and underlined text for each column. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Here MD [76], Relative MD [77], and OpenMax [78] are common methods in OOD detection literature to leverage a classifier as OOD detector. The results reported for each outlier method correspond to the best-performing detection method. Notably, our approach has surpassed the state-of-the-art in robust out-of-distribution setting (ATD) for nearly all datasets. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mu_{k}=\\frac{1}{N}\\sum_{i:y_{i}=k}z_{i},\\quad\\Sigma=\\frac{1}{N}\\sum_{k=1}^{K}\\sum_{i:y_{i}=k}\\left(z_{i}-\\mu_{k}\\right)\\left(z_{i}-\\mu_{k}\\right)^{T},\\quad k=1,2,\\ldots,K\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In addition, to use RMD, one has to fti a $\\mathcal{N}\\left(\\mu_{0},\\Sigma_{0}\\right)$ to the whole in-distribution. Next, the distances and anomaly score for the input $x^{\\prime}$ with pre-logits $z^{\\prime}$ are computed as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M D_{k}\\left(z^{\\prime}\\right)=\\left(z^{\\prime}-\\mu_{k}\\right)^{T}\\Sigma^{-1}\\left(z^{\\prime}-\\mu_{k}\\right),\\quad R M D_{k}\\left(z^{\\prime}\\right)=M D_{k}\\left(z^{\\prime}\\right)-M D_{0}\\left(z^{\\prime}\\right),}\\\\ &{\\mathrm{score}_{M D}\\left(x^{\\prime}\\right)=-\\operatorname*{min}_{k}\\left\\{M D_{k}\\left(z^{\\prime}\\right)\\right\\},\\quad\\mathrm{score}_{\\;R M D}\\left(x^{\\prime}\\right)=-\\operatorname*{min}_{k}\\left\\{R M D_{k}\\left(z^{\\prime}\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "D Algorithms ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Algorithm 1 Trojan scanning by detection of adversarial shifts in out-of-distribution samples   \nInput: A $c$ -class Classifier $f_{\\theta}$ , (Optional) a small set of benign samples $\\mathcal{D}_{v}$ , A set of $k$ hard   \ntransformations $\\tau$ , Adversarial perturbation budget $\\epsilon$ , scanning threshold $\\tau$   \nOutput: Decision (Trojaned / Clean)   \n1: if $\\mathcal{D}_{v}$ is not provided then   \n2: $\\boldsymbol{\\mathcal{D}}_{v}\\gets$ TinyImageNet   \n3: end if   \n4: Applies a random permutation of transformations to x   \n5: procedure $\\operatorname{G}(x,\\tau)$   \n6: $\\mathcal{T}_{\\mathrm{perm}}\\leftarrow$ Randomly Permute $(\\tau)$   \n7: for $t\\in\\mathcal{T}_{\\mathrm{perm}}\\,\\mathbf{do}$   \n8: x \u2190t(x)   \n9: end for   \n10: return $x$   \n11: end procedure   \n12: Obtain $D_{O O D}$ by applying hard augmentations on each sample of $D_{v}$ :   \n13: $\\mathcal{D}_{O O D}\\leftarrow\\emptyset$   \n14: for $x\\in\\mathcal{D}_{v}$ do   \n15: $x^{\\prime}\\leftarrow\\mathbf{G}(x,\\mathcal{T})$   \n16: $\\mathcal{D}_{O O D}\\leftarrow\\mathcal{D}_{O O D}\\cup\\{x^{\\prime}\\}$   \n17: end for   \n18: $\\Delta{\\mathcal{T}}\\gets\\emptyset$   \n19: for $x\\in\\mathcal{D}_{O O D}$ do   \n20: Adversarial Perturbation:   \n21: $x^{*}\\gets\\operatorname{PGD}(f_{\\theta},x,\\epsilon)$   \n22: ID score computation:   \n23: $\\begin{array}{r l}&{S_\\mathrm{before}\\leftarrow\\mathrm{max}_{i=1,\\dots,c}\\,f_{\\theta}^{i}(x)}\\\\ &{S_{\\mathrm{after}}\\leftarrow\\mathrm{max}_{i=1,\\dots,c}\\,f_{\\theta}^{i}(x^{*})}\\\\ &{\\Delta I D\\leftarrow S_{\\mathrm{after}}-S_{\\mathrm{before}}}\\end{array}$   \n24:   \n25:   \n26: Append $\\Delta_{\\mathrm{ID}}$ to $\\Delta\\mathcal{T}$   \n27: end for   \n28: $\\begin{array}{r}{S_{\\mathrm{mean}}\\leftarrow\\frac{1}{|\\mathcal{D}_{\\mathrm{ooD}}|}\\sum_{\\delta_{\\mathrm{ID}}\\in\\Delta\\mathcal{T}}\\delta_{\\mathrm{ID}}}\\end{array}$   \n29: if $S_{\\mathrm{mean}}<\\tau$ then   \n30: return Clean   \n31: else   \n32: return Trojaned   \n33: end if ", "page_idx": 19}, {"type": "text", "text": "E Extended Related Work ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Backdoor Attacks. Injecting pre-defined triggers into the training data is the most common approach to implement backdoor attacks. BadNet [4] is the first backdoor attack against DNN models, which involves modifying a clean image by inserting a small, predetermined pattern at a fixed location, thus replacing the original pixels. Blended [14] aimed to enhance the invisibility of the trigger pattern by seamlessly blending it into the clean image through alpha blending. SIG [56] utilized a sinusoidal waveform signal as the trigger pattern. To achieve better stealthiness, many attacks with invisible and dynamic triggers have been proposed. Input-aware [55] proposed a training-controllable attack method that simultaneously learned the model parameters and a trigger generator to produce a unique trigger pattern for each clean test sample. For more details regarding other attacks including BPP [57], SSBA [58], WaNet [5], and [66] read Appendix Section H. ", "page_idx": 19}, {"type": "text", "text": "Benign overfitting The phenomenon of benign overfitting, where models perfectly fit noisy data without compromising generalization, was first explored in [18]. They characterized the conditions under which the minimum norm interpolating prediction rule achieves near-optimal accuracy, emphasizing the necessity of overparameterization. Subsequent studies extended these findings to various neural network architectures. Notably [79] delves into sparse interpolating procedures for linear regression with Gaussian data, highlighting conditions under which benign overfitting occurs in overparameterized regimes. Their work establishes lower bounds on excess risk, proving that overfitting can indeed be benign. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "Two-layer neural networks have also been extensively studied to understand benign overftiting under various conditions. Benign overfitting in two-layer convolutional neural networks is investigated by [80], identifying a phase transition between benign and harmful overfitting. Similarly, [81] analyzes non-smooth neural networks, providing theoretical insights into when overftiting can remain benign even beyond lazy training scenarios. This exploration is extended to ReLU networks in [82], demonstrating the conditions that facilitate benign overfitting and the sharp transitions to harmful overfitting. Additionally, [83] shows that overfitting in Sobolev RKHSs can achieve optimal rates without being intrinsically harmful. ", "page_idx": 20}, {"type": "text", "text": "The phenomenon is also observed in more complex architectures. Gradient-based meta learning is examined in [84], revealing that benign overfitting in empirical risk minimization (ERM) can extend to meta-learning algorithms like MAML. A refined taxonomy of overfitting in proposed in [85], identifying tempered overfitting as an intermediate regime between benign and catastrophic overfitting. Benign overfitting has been studied in various other architectures and settings as well [86, 87, 88, 89, 88, 90, 91]. ", "page_idx": 20}, {"type": "text", "text": "Benign overfitting has been studied in the context of adversarial robustness in [54, 92, 93]. Notably, [54] theoretically shows for linear and two-layer networks that benign overftiting will become harmful overfitting under adversarial attacks. ", "page_idx": 20}, {"type": "text", "text": "The interplay between benign overftiting and security vulnerabilities like backdoor attacks is critical in [94] revealing that few-shot learning models tend to overfti benign or poisoned features, impacting robustness. ", "page_idx": 20}, {"type": "text", "text": "Adversarial risk Adversarial risk refers to the vulnerability of machine learning models to adversarial examples\u2014perturbations intentionally crafted to mislead the model. Research in this area seeks to understand and mitigate these risks. A seminal work by [95] presented robust optimization techniques to defend against first-order adversarial attacks, establishing foundational adversarial training methodologies. Following this, other studies have explored the theoretical limits of adversarial robustness. A framework to evaluate the adversarial vulnerability of any classifier is provided in [96], showing intrinsic limitations based on the classifier\u2019s architecture and data distribution. ", "page_idx": 20}, {"type": "text", "text": "Previous work has established bounds on this metric via function transformation [46], PAC-Bayesian [47], sparsity-based compression [48], Optimal Transport and Couplings [49], or in terms of input dimension [50]. ", "page_idx": 20}, {"type": "text", "text": "The intersection of adversarial risk and out-of-distribution (OOD) detection has garnered increasing attention. The vulnerabilities in existing OOD generalization methods to adversarial attacks are identified in [51], prompting the development of algorithms to enhance OOD adversarial robustness. RATIO was introduced by [53], a training procedure that improves adversarial robustness for both indistribution and OOD samples, thereby enhancing model explainability. The adversarial vulnerability of current OOD detection techniques is discussed in [52], suggesting that ensemble methods and combining multiple OOD detectors can significantly enhance robustness against adversarial attacks. ", "page_idx": 20}, {"type": "text", "text": "Understanding the theoretical limits of adversarial vulnerability remains crucial for developing robust models. It is proved in [50] that adversarial vulnerability increases with the gradients of the training objective and scales with the square root of the input dimension, making larger images more vulnerable. The trade-offs between robustness and accuracy are explored in [96], establishing a mathematical framework to evaluate these limits. This discussion is extended in [95] through robust optimization, quantifying the trade-offs, and providing guidelines for creating more resilient models. These foundational works underscore the inherent challenges in achieving robustness, emphasizing the need for innovative approaches to bridge the gap between theory and practical applications. ", "page_idx": 20}, {"type": "text", "text": "F Theoretical Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 1 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Let $\\boldsymbol{\\mu}\\in\\mathbb{R}^{d_{x}}$ be arbitrary, by taylor series around $\\mu$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\nh(w,x)=\\sum_{|\\gamma|=k+1}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}+\\sum_{j\\neq k+1}\\sum_{|\\gamma|=j}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By taking derivative we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\nabla_{x}h(w,x)=\\nabla_{x}\\sum_{|\\gamma|=k+1}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}+\\sum_{j\\neq k+1}\\nabla_{x}\\sum_{|\\gamma|=j}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The distribution $\\mathcal{P}_{+s}^{k}$ has the same $j$ -th order moments as the distribution $\\mathcal{P}$ for all $j\\neq k$ , therefore, by taking expectation and using the triangle inequality we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathcal{R}_{\\alpha^{+s}}^{\\mathcal{P}_{+s}^{k}}(h,w)=\\alpha\\mathbb{E}_{x\\sim\\mathcal{P}_{+s}^{k}}[\\|\\nabla_{x}h(w,x)\\|]\\geq\\alpha\\|\\mathbb{E}_{x\\sim\\mathcal{P}_{+s}^{k}}\\nabla_{x}h(w,x)\\|}\\\\ {\\geq\\alpha\\|\\mathbb{E}_{x\\sim\\mathcal{P}_{+s}^{k}}\\left[\\nabla_{x}\\displaystyle\\sum_{|\\gamma|=k+1}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}\\right]-\\mathbb{E}_{x\\sim\\mathcal{P}}\\left[\\nabla_{x}\\displaystyle\\sum_{|\\gamma|=k+1}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}\\right]\\|}\\\\ {-\\alpha\\|\\mathbb{E}_{x\\sim\\mathcal{P}_{+s}^{k}}\\left[\\nabla_{x}h(w,x)-\\nabla_{x}\\displaystyle\\sum_{|\\gamma|=k+1}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}\\right]+\\mathbb{E}_{x\\sim\\mathcal{P}}\\left[\\nabla_{x}\\displaystyle\\sum_{|\\gamma|=k+1}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{=\\alpha\\|s\\nabla_{x}\\displaystyle\\sum_{|\\gamma|=k}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}\\|}}\\\\ {{-\\alpha\\|\\mathbb{E}_{x\\sim\\mathcal{P}}\\displaystyle\\left[\\nabla_{x}h(w,x)-\\nabla_{x}\\displaystyle\\sum_{|\\gamma|=k+1}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)^{\\gamma}\\right]+\\mathbb{E}_{x\\sim\\mathcal{P}}\\displaystyle\\left[\\nabla_{x}\\displaystyle\\sum_{|\\gamma|=k+1}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}(x-\\mu)\\right]}}\\\\ {{=\\alpha\\|s\\|\\|\\nabla_{x}\\displaystyle\\sum_{|\\gamma|=k}\\frac{\\nabla_{x}^{\\gamma}h(w,\\mu)}{\\gamma!}\\|-\\alpha\\|\\mathbb{E}_{x\\sim\\mathcal{P}}\\left[\\nabla_{x}h(w,x)\\right]\\|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that all moments with orders less than $k$ are equal, hence the difference of the moment of order $k$ around $\\mu$ is equal to the difference of the moment of order $k$ around 0. Since $\\mu$ was arbitrary, we conclude: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{R}_{\\alpha}^{\\mathcal{P}_{+s}^{k}}(h,w)\\geq\\alpha|s|\\operatorname*{max}_{x}\\|\\nabla_{x}\\sum_{|\\gamma|=k}\\frac{\\nabla_{x}^{\\gamma}h(w,x)}{\\gamma!}\\|-\\alpha\\|\\mathbb{E}_{x\\sim\\mathcal{P}}\\nabla_{x}h(w,x)\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Linear neural network ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. We define $X\\ =\\ [x_{1},\\ldots,x_{n}]^{\\top}\\ \\in\\ \\mathbb{R}^{n}$ , $X^{\\prime}\\ =\\ [x_{1}^{\\prime}\\,+\\,t,\\ldots,x_{m}^{\\prime}\\,+\\,t]^{\\top}\\ \\in\\ \\mathbb{R}^{m}$ , $Y\\,=$ $\\left[y_{1},\\ldots,y_{n}\\right]^{\\top}\\in\\mathbb{R}^{n}$ , $Y^{\\prime}=[y_{c},\\dots,y_{c}]^{\\top}\\in\\mathbb{R}^{m}$ , then we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\hat{w}}=\\left(\\left[{X}^{\\prime}\\right]^{\\top}\\left[{X}^{\\prime}\\right]\\right)^{-1}\\left[{X}^{\\prime}\\right]^{\\top}\\left[{Y}^{\\prime}\\right]=\\left({X}^{\\top}{X}+{X}^{\\prime}{}^{\\top}{X}^{\\prime}\\right)^{-1}\\left({X}^{\\top}{Y}+{X}^{\\prime}{}^{\\top}{Y}^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}n\\left(X^{\\top}X\\right)^{-1}=C}\\end{array}$ is a constant matrix (inverse of the covariance matrix) and $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}=s}\\end{array}$ is a constant vector (mean vector), and we have $\\begin{array}{r}{\\operatorname*{lim}_{n\\to\\infty}\\sum_{i=1}^{m}x_{i}^{\\prime}=m s}\\end{array}$ by the law of large numbers. If $r a n k(X^{\\prime\\top}X^{\\prime})=1$ , then according to the Miller\u2019s Lemma [3]: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}\\|n\\left(X^{\\top}X+X^{\\prime^{\\top}}X^{\\prime}\\right)^{-1}=C-\\frac{C k}{1+t r(k)}\\|=\\Omega(1)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r l r}{k}&{=}&{\\left(X^{\\prime\\top}X^{\\prime}\\right)\\left(X^{\\top}X\\right)^{-1}}\\\\ &{=}&{\\left(\\frac{m}{n}X^{\\top}X+m s t^{\\top}+m s^{\\top}t+m t t^{\\top}\\right)\\left(X^{\\top}X\\right)^{-1}}\\end{array}$ . If $r a n k(X^{\\prime\\top}X^{\\prime})\\,\\geq\\,2$ , we can decompose it into sum matrices with rank 1 and inductively infer the same bound $\\Omega(1)$ . Now we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\|{\\frac{1}{n}}\\left(X^{\\top}Y+X^{\\prime\\top}Y^{\\prime}\\right)={\\frac{1}{n}}X^{\\top}Y+{\\frac{m}{n}}y_{c}(s+t^{\\top})\\|=\\Omega({\\frac{m}{n}}\\|t\\|).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we conclude: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\mathcal{R}_{\\alpha}^{\\mathcal{P}}(h_{1},\\hat{w})=\\operatorname*{lim}_{n\\to\\infty}\\|\\nabla_{x}h_{1}(\\hat{w},x)\\|=\\operatorname*{lim}_{n\\to\\infty}\\|\\hat{w}\\|=\\Omega\\left(\\frac{m}{n}\\|t\\|\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Two-layer neural network ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proo $\\begin{array}{r l}&{j.\\ \\ W\\ e\\quad\\ \\operatorname{define}\\quad F\\qquad=\\quad[h_{2}(w_{0},x_{1}),\\ldots,h_{2}(w_{0},x_{n})]^{\\top}\\in\\qquad\\mathbb{R}^{n},\\quad\\nabla F\\qquad=}\\\\ &{h_{2}(w_{0},x_{1}),\\ldots,\\nabla_{w}h_{2}(w_{0},x_{n})]^{\\top}\\in\\mathbb{R}^{k(p+1)\\times n},F^{\\prime}=[h_{2}(w_{0},x_{1}^{\\prime}+t),\\ldots,h_{2}(w_{0},x_{m}^{\\prime}+t)]^{\\top}\\in}\\\\ &{\\nabla_{\\mathbf{\\Omega}}^{\\gamma^{\\prime}}.\\qquad=[\\nabla_{w}h_{2}(\\underline{{w_{0}}},\\underline{{x_{1}^{\\prime}}}+t),\\ldots,\\nabla_{w}h_{2}(w_{0},x_{m}^{\\prime}+t)]^{\\top}\\in\\mathbb{R}^{k(p+1)\\times m},y=[y_{1},\\ldots,y_{n}]^{\\top}\\in\\mathbb{R}^{n},}\\end{array}$ $\\mathbb{R}^{m}$ ,   \nand $y^{\\prime}=[y_{c},\\ldots,y_{c}]^{\\top}\\in\\mathbb{R}^{m}$ . ", "page_idx": 22}, {"type": "text", "text": "Assume the two-layer network $h_{2}(w,x)$ as described in the paper is trained using gradient descent on $\\mathcal{D}\\cup\\mathcal{D}^{\\prime}$ with learning rates $\\eta<1/\\lambda_{m a x}([\\nabla F,\\nabla F^{\\prime}]^{\\top}[\\nabla F,\\nabla F^{\\prime}])$ . According to Proposition 1 in [97], the optimal solution will be: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{w}=\\left(\\left[\\boldsymbol{\\nabla}F^{\\prime}\\right]^{\\top}\\left[\\boldsymbol{\\nabla}F^{\\prime}\\right]\\right)^{-1}\\left[\\boldsymbol{\\nabla}F^{\\prime}\\right]^{\\top}\\left[\\boldsymbol{Y}-\\boldsymbol{F}^{\\prime}\\right]\\qquad}\\\\ {=\\left(\\boldsymbol{\\nabla}F^{\\top}\\boldsymbol{\\nabla}F+\\boldsymbol{\\nabla}F^{\\prime\\top}\\boldsymbol{\\nabla}F^{\\prime}\\right)^{-1}\\left(\\boldsymbol{\\nabla}F^{\\top}(\\boldsymbol{Y}-\\boldsymbol{F})+\\boldsymbol{\\nabla}F^{\\prime\\top}(\\boldsymbol{Y}^{\\prime}-\\boldsymbol{F}^{\\prime})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For any $i\\in\\{1,\\ldots,m\\},j\\in\\{1,\\ldots,k\\}$ we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{w}h_{2}(w_{0},x_{i}^{\\prime}+t)_{j}=\\frac{1}{\\sqrt{k p}}\\left[u_{0,j}\\mathrm{ReLU}^{\\prime}(\\theta_{0,j}^{T}(x_{i}^{\\prime}+t))(x_{i}^{\\prime}+t),\\mathrm{ReLU}(\\theta_{0,j}^{T}(x_{i}^{\\prime}+t))\\right]}\\\\ &{\\qquad\\qquad=\\left\\{\\frac{1}{\\sqrt{k p}}\\left[u_{0,j}(x_{i}^{\\prime}+t),\\theta_{0,j}^{T}(x_{i}^{\\prime}+t)\\right],\\begin{array}{l}{\\mathrm{if~}\\theta_{0,j}^{T}(x_{i}^{\\prime}+t)\\geq0}\\\\ {\\mathrm{otherwise}}\\end{array}}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we can assume there are some constant matrices $G_{1},G_{2},G_{1}^{\\prime},G_{2}^{\\prime}$ such that $F^{\\prime}=t G_{1}+G_{2}$ and $\\nabla F^{\\prime}=t G_{1}^{\\prime}+G_{2}^{\\prime}$ . Moreover, note that $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty}n\\left(\\nabla F^{\\top}\\nabla F\\right)^{-1}=C_{1}}\\end{array}$ is a constant matrix (inverse of the covariance matrix). If $r a n k(\\nabla F^{\\prime\\intercal}\\nabla F^{\\prime})=1$ , then according the Miller\u2019s Lemma [3]: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}\\|n\\left(\\nabla F^{\\top}\\nabla F+\\nabla F^{\\prime^{\\top}}\\nabla F^{\\prime}\\right)^{-1}=C_{1}-\\frac{C_{1}k}{1+t r(k)}\\|=\\Omega(1)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\begin{array}{r l r}{k}&{=}&{\\left(\\nabla F^{\\prime\\top}\\nabla F^{\\prime}\\right)\\left(\\nabla F^{\\top}\\nabla F\\right)^{-1}\\;\\;\\approx\\;\\;\\left(\\frac{m}{n}\\nabla F^{\\top}\\nabla F+\\Omega(\\|m t\\|)\\right)\\left(\\nabla F^{\\top}\\nabla F\\right)^{-1}}\\end{array}$ If $r a n k(\\nabla F^{\\prime\\intercal}\\nabla F^{\\prime})\\,\\geq\\,2$ , we can decompose it into sum matrices with rank 1 and inductively infer the same bound $\\Omega(1)$ . Now we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}\\Vert\\frac{1}{n}\\left(\\nabla F^{\\top}(Y-F)+\\nabla F^{\\prime\\top}(Y^{\\prime}-F^{\\prime})\\right)\\Vert=\\Omega(\\frac{m}{n}\\Vert t\\Vert).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we conclude: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}\\left\\|{\\hat{w}}\\right\\|=\\Omega\\left({\\frac{m}{n}}\\|t\\|\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\qquad\\qquad\\mathcal{R}_{\\alpha}^{\\mathcal{P}}(\\tilde{h_{2}},\\hat{w})=\\mathbb{E}_{x}\\left[\\underset{\\|\\delta\\|\\leq\\alpha}{\\operatorname*{sup}}\\ \\tilde{h_{2}}(\\hat{w},x+\\delta)-\\tilde{h_{2}}(\\hat{w},x)\\right]}\\\\ &{=\\alpha\\mathbb{E}_{x}\\|\\nabla_{x}\\tilde{h_{2}}(\\hat{w},x)\\|=\\alpha\\mathbb{E}_{x}\\|\\nabla_{x}h_{2}(w_{0},x)+\\frac{\\partial^{2}h_{2}(w_{0},x)}{\\partial w\\partial x}(\\hat{w}-w_{0})\\|=\\Omega(\\|\\hat{w}\\|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, we conclude: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\rightarrow\\infty}\\mathcal{R}_{\\alpha}^{\\mathcal{P}}(\\tilde{h_{2}},\\hat{w})=\\operatorname*{lim}_{n\\rightarrow\\infty}\\|\\hat{w}\\|=\\Omega\\left(\\frac{m}{n}\\|t\\|\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "G Preliminaries ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Adverserial attack to classifiers It has been shown that DNNs are vulnerable to adversarial attacks across various tasks, predominantly explored in classification tasks. During inference, adversarial perturbations added to input data can cause classifiers to mispredict their labels. In other words, a perturbation such as $\\delta$ is added to a sample $x$ with label $y$ to fool the model into outputting $\\hat{y}$ for the adversarial input sample $x^{*}$ , where $x^{*}=x+\\delta$ . Specifically, PGD is an iterative adversarial attack [17] that crafts adversarial samples by ensuring the noise is projected within the $\\ell_{\\infty}$ norm in N-step: ", "page_idx": 23}, {"type": "equation", "text": "$$\nx_{0}^{*}=x,\\qquad x_{t+1}^{*}=\\Pi_{x+\\mathcal{S}}\\big(x_{t}^{*}+\\alpha\\cdot\\mathrm{sign}\\,\\big(\\nabla_{x}J\\left(x_{t}^{*},y\\right)\\big)\\big),\\qquad x^{*}=x_{N}^{*}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\Pi_{x+s}$ denotes the projection on the norm ball $\\boldsymbol{S}$ around $x$ and $J\\left(x_{t}^{\\ast},y\\right)$ denotes the targeted objective function, which for classifiers is cross entropy. Previous studies have shown that standardtrained classifiers are vulnerable to adversarial attacks, even weak attacks like FGSM. Various defense mechanisms have been proposed to address this challenge, with adversarial training on the training dataset being the most effective defense. ", "page_idx": 23}, {"type": "text", "text": "Using a Classifier as an OOD Data Detector Classifiers such as $f$ , trained on a dataset denoted as $D$ , can be used as OOD detectors due to their learned features. Specifically, another dataset with separate semantics from $D$ , denoted as $D^{\\prime}$ , is assumed to be the source of OOD samples where $D$ represents in-distribition samples. For instance, a classifier trained on Cifar10 is used to detect Cifar100 as OOD samples. The instructions to detect OOD samples practically involve using the output distribution of a classifier over the classes of $D$ for a given input, offering insights into the model\u2019s prediction confidence. Specifically, classifiers tend to be more confident about ID samples compared to OOD samples. recently The Maximum Softmax Probability (MSP) has been proposed as an indicator of this confidence. ", "page_idx": 23}, {"type": "text", "text": "formally a well-trained classifier $f$ logits from its penultimate layer on $D$ with $k$ classes, for a given input sample $x$ are represented by $z=f(x)$ , where $z$ is a vector of unnormalized prediction scores $[z_{1},z_{2},\\ldots,z_{k}]$ for sample $x$ . Applying the softmax function results in: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{softmax}(z_{i})={\\frac{e^{z_{i}}}{\\sum_{j=1}^{k}e^{z_{j}}}},\\qquad\\operatorname{MSP}_{f}(x):=\\operatorname*{max}_{i\\in\\{1,\\ldots,k\\}}(\\operatorname{softmax}(z_{i}))\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $z_{i}$ is the logit corresponding to the $i$ -th class. ", "page_idx": 23}, {"type": "text", "text": "intutitively $\\mathrm{MSP}_{f}(x)$ represents the highest probability assigned to any class by the model, reflecting the model\u2019s confidence level in its prediction. ", "page_idx": 23}, {"type": "text", "text": "H Datasets Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Details for the Backdoor Attacks ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section provides detailed descriptions of the backdoor attacks employed in our study, focusing particularly on the nature and implementation of their triggers. ", "page_idx": 24}, {"type": "text", "text": "BadNet [4] embeds a malicious trigger, typically a small and visually distinctive patch, into the training data. This trigger is designed to be inconspicuous enough to evade detection yet recognizable by the trained model, leading it to misclassify inputs containing the trigger. ", "page_idx": 24}, {"type": "text", "text": "Blended [14] subtly blends a trigger into the training images at low intensity, making it hard to detect by human inspectors or simple automated methods. The trigger effectively conditions the model to associate the slightly altered patterns with incorrect outputs. ", "page_idx": 24}, {"type": "text", "text": "SIG [56] corrupts training images with a sinusoidal pattern, superimposed in a way that does not require changes to the image labels. This makes the backdoor particularly stealthy as it avoids the common detection methods that look for label inconsistencies. ", "page_idx": 24}, {"type": "text", "text": "BPP [57] uses image quantization combined with contrastive adversarial learning to craft triggers that are embedded into the pixel values themselves. This approach alters the image at a bit-per-pixel level, making the modifications difficult to perceive or reverse-engineer. ", "page_idx": 24}, {"type": "text", "text": "Input-aware attacks [55] dynamically adjust their triggers based on the input features, allowing the backdoor to activate only under specific conditions that are predetermined by the attacker. This adaptability makes the attack highly elusive and challenging to detect. ", "page_idx": 24}, {"type": "text", "text": "WaNet [5] introduces imperceptible warping to the image, manipulating its geometric properties subtly. This warping acts as a trigger that is extremely hard to spot with the naked eye, ensuring the model misclassifies the warped input while appearing normal to human observers. ", "page_idx": 24}, {"type": "text", "text": "SSBA [58] crafts sample-specific triggers that are invisible to human detection by embedding them in a way that aligns closely with the natural image structure. These triggers are tailored to each individual sample, increasing the difficulty of detecting and isolating the backdoor through general analysis. ", "page_idx": 24}, {"type": "text", "text": "Color [66] alters the color distribution of the input images, employing changes in the color channels as the trigger mechanism. This type of manipulation can remain under the radar of typical visual inspections while effectively conditioning the model to respond to altered color cues. ", "page_idx": 24}, {"type": "text", "text": "These descriptions underline the diversity of the backdoor mechanisms used, highlighting the challenges in detecting and mitigating such threats in machine learning models. ", "page_idx": 24}, {"type": "text", "text": "I Previous Trojan Scanning methods ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Implementation ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We implemented the baseline methods using their implementations in their publicly available GitHub repositories. For methods requiring supervision to define a threshold, we followed the same approach as UMD. ", "page_idx": 24}, {"type": "text", "text": "Review of the Methods ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "NC Neural Cleanse [7] is a method for scanning models by reverse engineering triggers and identifying outliers with significantly smaller perturbations. NC faces significant computational overhead, sensitivity to trigger complexity, and potential false positives. Moreover it is primarily designed to handle only All-to-One attacks. ", "page_idx": 24}, {"type": "text", "text": "ABS Artificial Brain Stimulation [8] detects backdoors in neural networks by stimulating individual neurons and analyzing their impact on output activations. Compromised neurons that substantially elevate a specific label are identified, and potential triggers are reverse-engineered to confirm the presence of backdoors. the method involves significant computational overhead and is sensitive to its underlying assumptions about compromised neurons and trigger behavior. ABS may struggle with more advanced attack scenarios and primarily handles All-to-One attacks. Additionally, benign neurons with unique features may lead to false positives, limiting its robustness in some cases. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "TABOR TABOR [10] is a trojan detection method for DNNs that frames the detection task as an optimization problem. It incorporates an objective function with regularization terms inspired by explainable AI techniques to guide the optimization process and reduce the adversarial search space, improving trigger identification accuracy. . However, TABOR\u2019s significant computational overhead present challenges. Additionally, it is primarily designed for geometric or symbolic triggers, potentially limiting effectiveness against irregular shapes trojans. ", "page_idx": 25}, {"type": "text", "text": "PT-RED PT-RED [36] is a post-training backdoor detection method for DNNs. It reverse-engineers potential backdoor trigger patterns by solving an optimization problem to find minimal perturbations that cause misclassifications. However, it is primarily designed for single target class attacks with small trigger patterns. Moreover, the reverse-engineering process for identifying potential triggers is computationally intensive. ", "page_idx": 25}, {"type": "text", "text": "K-ARM K-Arm [9] leverages an optimization approach inspired by the Multi-Armed Bandit problem to iteratively select and optimize class labels for potential backdoor triggers. This method improves detection efficiency compared to exhaustive search methods by using stochastic selection. However, the method still involves computational overhead and primarily focuses on specific and simple types of backdoor attacks. Additionally, K-Arm may face challenges in handling scenarios with more than one target label. ", "page_idx": 25}, {"type": "text", "text": "UMD Unsupervised Model Detection [13] is designed to detect X2X backdoor attacks, where multiple source classes are mapped to multiple target classes. The method involves reverse-engineering triggers for each class pair using a small set of clean data, and then defining a transferability statistic (TR) for each class pair. TR measures how well the trigger for one class pair transfers to another. These statistics are used to select likely backdoor class pairs. An unsupervised anomaly detector then evaluates the aggregated trigger statistics to determine if a model is backdoored. However, the method involves complex optimization processes, which can be computationally intensive. Handling very large datasets or a high number of classes can pose scalability challenges. Furthermore, UMD may struggle to handle models with multiple different trigger patterns, as it relies on TR statistics that assume a single dominant trigger pattern and this method is not trigger-agnostic as its approach depends on the specific type of attack. ", "page_idx": 25}, {"type": "text", "text": "MM-BD Maximum Margin Backdoor Detection [12] is designed to detect backdoor attacks in neural networks, regardless of the backdoor pattern type. The method operates by estimating a maximum margin statistic for each class through gradient ascent from multiple random initializations, without any clean samples, and then using these statistics in an unsupervised anomaly detection framework to identify backdoor attacks. However, the method exhibits a large false-positive rate for datasets with a small number of class, and it struggles to detect attacks with more than one target label, where each source class may be mapped to a different target class. Additionally, MM-BD\u2019s effectiveness is significantly reduced when an adaptive attacker manipulates the learning process. ", "page_idx": 25}, {"type": "text", "text": "MNTD MNTD [40] identifies backdoors in neural networks by training a binary meta-classifier on features extracted from numerous shadow classifiers, both benign and Trojaned. However, this method struggles to generalize to types of attacks and model architecture it wasn\u2019t trained on. ", "page_idx": 25}, {"type": "text", "text": "J Broader Impact ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Our study introduces a method designed to identify potential backdoors embedded within classifiers. As a result, our work contributes positively to societal impacts by enhancing security measures in machine learning applications and mitigating risks associated with malicious interventions. ", "page_idx": 25}, {"type": "text", "text": "K Baselines and Evaluation Benchmark ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Baselines ", "page_idx": 25}, {"type": "text", "text": "In our evaluation, TRODO and TRODO-Zero are assessed alongside previous scanning methods including Neural Cleanse (NC) [7], ABS [8], PT-RED [36], TABOR [10], K-Arm [9], MM-BD [12], and UMD [13]. For an equitable comparison, we set the confidence threshold at $95\\%$ , corresponding to a $5\\%$ desired false positive rate for NC, PT-RED, and UMD which are based on unsupervised threshold settings. Similarly, for ABS, and K-Arm, which rely on supervised threshold adjustment, we maintained a consistent false positive rate of $5\\%$ across various tests and dataset configurations to maximize true positive outcomes. The capabilities and performance outcomes of these methods are detailed in Table 1. Further details can be found in Appendix Section I. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "Our Designed Benchmark ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "To effectively model real-world scenarios for the scanning task, we developed a benchmark covering various scenarios involving both clean and trojan classifiers. Specifically, to evaluate the generality of scanning methods, which is crucial in real-world applications, we included several datasets ranging from low to high resolution and different classifiers with various architectures. Moreover, different trojan attacks and label mappings were also considered, with classifiers covering both standard and adversarial training methods. More details about our developed benchmark are provided below. ", "page_idx": 26}, {"type": "text", "text": "Image Datasets Our benchmark comprises image datasets from various domains, including CIFAR10, CIFAR100 [61], GTSRB [64], PubFig [65], and MNIST. We considered two kinds of label mapping, All to One and All to All. ", "page_idx": 26}, {"type": "text", "text": "Trojan Attacks In the former, the label of poisoned samples is changed to a single target class. In the latter, unlike All to One case, each class can be mapped to any arbitrary target class, which makes this setting more challenging. We included eight trojan attacks, comprising BadNet [4], Input-aware [55], BPP [57], SIG [56], WaNet [5], Color [66], SSBA [58] and Blended [14]. The test set for each combination of image dataset and label mapping consists of a total of 320 models. We trained 20 trojaned models using each type of attack. We included 160 clean models, resulting in a balanced set. ", "page_idx": 26}, {"type": "text", "text": "Adversarial Training To encompass a wider variety of scenarios, we evaluated each configuration using both standard and adversarial training. We employed PGD-10 with $\\begin{array}{r}{\\epsilon=\\frac{2}{255}}\\end{array}$ for adversarial training. ", "page_idx": 26}, {"type": "text", "text": "Classifier Architectures We considered various architectures, including ResNet18 [67], PreActResNet18 [68], and ViT-B/16 [69]. Previous works have solely focused on CNN-based architectures, underscoring the generality of our experiments. ", "page_idx": 26}, {"type": "text", "text": "TrojAI Benchmark ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Another challenging benchmark that we include in our experiments is TrojAI [35], a benchmark developed by IARPA designed to address challenges in backdoor detection. The license of this benchmark is Apache 2.0. ", "page_idx": 26}, {"type": "text", "text": "Structure For each round of the competition, a test set, a hold-out set, and a training set of models are available and can be accessed from the TrojAI homepage. Almost half of the models in each set are trojaned. A small set of benign samples from the training dataset of each model is provided along with the model itself. ", "page_idx": 26}, {"type": "text", "text": "Trojan Attacks The models may be trojaned with various kinds of backdoors, including universal and label-specific. The triggers could be pixel patterns and Instagram filters. Triggers can be embedded within the model such that activation occurs only under specific conditions, such as possessing a certain texture or being located in a designated area of the image. The complexity of models and trojan attacks grows from round to round. The performance of methods on this benchmark is provided in Table 2. We evaluate methods in terms of scanning accuracy and average scanning time for a model. ", "page_idx": 26}, {"type": "text", "text": "L Fr\u00e9chet Inception Distance (FID) ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The Fr\u00e9chet Inception Distance (FID) is a metric used to evaluate the quality of generative models, such as GANs. It compares the distribution of generated images to the distribution of real images by measuring the distance between two multivariate Gaussian distributions fitted to the feature representations of these images. ", "page_idx": 26}, {"type": "text", "text": "Given a set of real images $\\{x_{i}\\}_{i=1}^{N_{r}}$ and a set of generated images $\\{\\hat{x}_{j}\\}_{j=1}^{N_{g}}$ , pass both sets through a pre-trained Inception v3 network to obtain feature representations. Let $\\phi(x)$ denote the feature representation of image $x$ from the Inception network. ", "page_idx": 27}, {"type": "text", "text": "Calculate the mean and covariance of the feature representations for real images: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mu_{r}=\\frac{1}{N_{r}}\\sum_{i=1}^{N_{r}}\\phi(x_{i})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Sigma_{r}=\\frac{1}{N_{r}-1}\\sum_{i=1}^{N_{r}}(\\phi(x_{i})-\\mu_{r})(\\phi(x_{i})-\\mu_{r})^{T}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Similarly, compute the mean and covariance for generated images: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mu_{g}=\\frac{1}{N_{g}}\\sum_{j=1}^{N_{g}}\\phi(\\hat{x}_{j})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\Sigma_{g}=\\frac{1}{N_{g}-1}\\sum_{j=1}^{N_{g}}(\\phi(\\hat{x}_{j})-\\mu_{g})(\\phi(\\hat{x}_{j})-\\mu_{g})^{T}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The FID is defined as the Fr\u00e9chet distance between the two multivariate Gaussian distributions $\\mathcal{N}(\\mu_{r},\\Sigma_{r})$ and $\\mathcal{N}(\\mu_{g},\\Sigma_{g})$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{FID}=\\Vert\\mu_{r}-\\mu_{g}\\Vert^{2}+\\mathrm{Tr}(\\Sigma_{r}+\\Sigma_{g}-2(\\Sigma_{r}\\Sigma_{g})^{\\frac{1}{2}})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Here, $\\|\\mu_{r}-\\mu_{g}\\|^{2}$ is the squared Euclidean distance between the mean vectors. Tr denotes the trace of a matrix. $\\bigl(\\bar{\\Sigma}_{r}\\bar{\\Sigma}_{g}\\bigr)^{\\frac{1}{2}}$ is the matrix square root of the product of the two covariance matrices. ", "page_idx": 27}, {"type": "text", "text": "Lower FID scores indicate that the generated images have a distribution more similar to the real images, suggesting higher quality. Higher FID scores indicate greater dissimilarity between the distributions of generated and real images, suggesting lower quality. ", "page_idx": 27}, {"type": "text", "text": "M Limitation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this study, we utilized a validation set and a surrogate model to select hyperparameters, such as epsilon for the PGD attack. However, these hyperparameters are architecture-specific. Therefore, each new architecture requires a tailored approach: training on a validation set and tuning its corresponding hyperparameters for the input classifier. This process can be time-consuming when encountering new architectures, although we limited our consideration to common architectures. Additionally, our research focuses on scanning trojaned classifiers. However, this task might need to be adapted for different tasks, such as object detectors, where our method would require adjustments to handle these cases effectively. Morever, our theoretical results on adversarial risk are limited to the noiseless settings and only address the backdoor effect up to two layered networks which could be extended in future work. ", "page_idx": 27}, {"type": "text", "text": "N Models Dataset Creation Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In our study, we generated 20 distinct models for each of the eight backdoor attack types across the ResNet18 and PreAct-ResNet18 architectures and 5 models for ViT for each dataset and attack, totaling 160 models for ResNet18 and PreAct-ResNet18 and 40 models for ViT. We utilized the BackdoorBench framework [98] (https://github.com/SCLBD/BackdoorBench) for seven attacks (BadNets, Blended, WaNet, SIG, BPP, Input-aware, and SSBA) and integrated an additional attack (Color attack [66]) using this repository (https://github.com/lyx1224/color-backdoor). ", "page_idx": 27}, {"type": "text", "text": "To ensure consistency and rigor, we adopted core framework settings from BackdoorBench and extended them to encompass both standard and novel attacks, adjusting parameters specifically for the COLOR attack. Our consistent methodology, following BackdoorBench\u2019s protocols, allowed us to systematically evaluate the individual effects of each backdoor strategy across the architectures, achieving a high standard of experimental reliability and detail. ", "page_idx": 27}, {"type": "text", "text": "O Extended Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Table 7: Accuracy of our scanning method across various trojan attacks. For each attack, the trojaned models in the evaluation set are backdoored only with that attack. The number of clean and trojaned models is balanced. The architecture of all the models is resnet18 ", "page_idx": 28}, {"type": "table", "img_path": "m296WJXyzQ/tmp/8351fba8aadf29a925a70e00f1523d20ca9faa75a8b6ee96a9a5f75bbccd4b05.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 8: Scanning performance of TRODO compared with other methods using PreAct ResNet-18 as the backbone ", "page_idx": 28}, {"type": "table", "img_path": "m296WJXyzQ/tmp/9f190327b27721e2619451c9298cf89d9fcf166b5cd8a40814f7cb5db3c57ff7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "Table 9: Scanning performance of TRODO compared with other methods using ViT-B-16 as the backbone ", "page_idx": 28}, {"type": "table", "img_path": "m296WJXyzQ/tmp/b4a4f03b80a75f60c27a69300f7a819c785bd7a82c4647d35c2332ae5671c77f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "m296WJXyzQ/tmp/19eff712506b4e71ca9522c185ab69e74c17617375609a0bf1b04c74cb1f8392.jpg", "table_caption": ["Table 10: Value of $\\epsilon$ and $\\tau$ for different validation sets and backbone architectures. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Table 11: Detailed results of statistical significance of our method\u2019s performance over 10 runs, in terms of variance of accuracy. ", "page_idx": 29}, {"type": "table", "img_path": "m296WJXyzQ/tmp/0980f5425bc2289af95aff3836ec50041a36ef60795319bc94dd4fa188aef33b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "P Extra Ablation Studies ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Performance of TRODO under different poisoning rates ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Intuitively, increasing the poisoning rate enlarges the blind spots in trojaned classifiers, as these are boundary regions where the poisoned data causes the model to overfit. Consequently, this will increase the probability that TRODO detects the trojaned classifiers. However, our signature is based on the presence of blind spots in trojaned classifiers and shows consistent performance across different poisoning rates. In the paper, we considered a poisoning rate of $10\\%$ as it is common in the literature. In addition, we have provided TRODO\u2019s performance for different poisoning rates in Table 12 (other components of TRODO remained fixed.) ", "page_idx": 29}, {"type": "table", "img_path": "m296WJXyzQ/tmp/dab99bb91972396f0ac94a5d53b9316f89f6bd6142cf4612b551a1bf189d9359.jpg", "table_caption": ["Table 12: Performance comparison of TRODO-Zero and TRODO under different poisoning rates across datasets. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "Performance of TRODO-Zero under different OOD sample rates ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In the first experiment, we performed an ablation study on the number of samples in our validation set. By default, TRODO-Zero uses $1\\%$ of the Tiny ImageNet validation dataset, which contains 200 classes, each with 500 samples. We explored the effect of varying the number of sample rates in Table 13. ", "page_idx": 29}, {"type": "table", "img_path": "m296WJXyzQ/tmp/c4fce0a6f407e1a5513cdbdde0a53c3f5ee96a5bd92376e451bf282bc28624d1.jpg", "table_caption": ["Table 13: Performance comparison of TRODO-Zero under different OOD sample rates across datasets. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We motivated our work by stating that current scanning methods are not attack-agnostic and fail to generalize to diverse attacks. Besides, almost all of them fail in scenarios where the given classifier is trained adversarially. Our method outperforms previous works in the mentioned settings, according to the results in Table 1. We also support our claims on the effect of using OOD data for trojan scanning via our theoretical analysis in Section 5. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have discussed the limitations of our work and possible direction for future research in Section M. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Due to the limited space in the main pages, we have provided proofs in Appendix Section F. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided the code to reproduce the results as a part of the supplemental material. Besides, we provided all the details of implementation in Section 6. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. ", "page_idx": 32}, {"type": "text", "text": "In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have provided the code and the notebook to easily reproduce results. We also have provided access to our benchmark in New Assets item in the checklist. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The implementation details of the creation procedure of our benchmark, and how we adapted baselines to the experiments and how we tune our parameters are discussed in Section 6. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Due to space constraints in the main body of the paper and to avoid overcrowding the tables, we have included the statistical significance of our method\u2019s experiments in Appendix Section O. For the baselines, we have documented the average performance across five repeated trials. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: In the \"Implementation Details\" subsection of the Experiments section (see Section 6), we have thoroughly outlined the computational resources utilized. Given that the efficient execution of our method is a key advantage, we have included comprehensive details about the computing resources employed to assess our method in comparison with other approaches. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: After a thorough review of the NeurIPS Code of Ethics, we are confident that our work fully complies with all its aspects. Additionally, our research adheres to ethical standards concerning social impact and harmful consequences, based on both our proprietary code and the publicly available datasets employed. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: In the appendix, we have allocated a section to discuss the broader impacts of our research (See Appendix Section J. Our study introduces a method designed to identify potential backdoors embedded within classifiers. As a result, our work contributes positively to societal impacts by enhancing security measures in machine learning applications and mitigating risks associated with malicious interventions. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: No safeguards are needed, as we are discussing the subject of detection of backdoor attacks, and not to incur them. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We have cited all image datasets that we used in our research. Besides, we have provided the license of TrojAI benchmark on which we have evaluated our method and previous works in their corresponding subsection in Appendix Section K. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: We created a new benchmark to evaluate our method alongside previous scanning techniques in terms of generalization capabilities. We have made our benchmark publicly available here. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work didn\u2019t include crowdsourcing and research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 36}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]