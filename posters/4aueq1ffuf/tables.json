[{"figure_path": "4AuEQ1FfUf/tables/tables_5_1.jpg", "caption": "Table 1: Comparisons among the stability-based generalization guarantees for SCO algorithms and SGD (Thm.-Theorem; Cor.-Corollary; *-high probability bound; L, a, V, M,C.-Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity assumptions; c-a positive constant; \u2713-has such a property; \u00d7-hasn't such a property).", "description": "This table compares the generalization guarantees of several stochastic compositional optimization (SCO) algorithms (SCGD, SCSC) and stochastic gradient descent (SGD) under various assumptions.  It shows the generalization error bounds achieved by each algorithm (expressed using Big O notation), along with which assumptions each algorithm satisfies (Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity). The table highlights that the newer work (Theorem 2) provides sharper bounds under milder conditions than prior work.", "section": "3.1 Learning Guarantees for General SCO"}, {"figure_path": "4AuEQ1FfUf/tables/tables_6_1.jpg", "caption": "Table 2: Comparisons among the optimization guarantees for SCO algorithms (Thm.-Theorem; Cor.-Corollary; L, a, V, M,C.-Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity assumptions; d2 = O(d); \u221a-has such a property; \u00d7-hasn't such a property).", "description": "This table compares the optimization guarantees for several SCO algorithms (SCGD, SCSC, and their black-box variants) under different assumptions (Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity).  It highlights the convergence rates achieved by each algorithm and indicates which assumptions are satisfied by each.", "section": "3.2 Learning Guarantees for Black-box SCO"}, {"figure_path": "4AuEQ1FfUf/tables/tables_12_1.jpg", "caption": "Table 1: Comparisons among the stability-based generalization guarantees for SCO algorithms and SGD (Thm.-Theorem; Cor.-Corollary; *-high probability bound; L, a, V, M,C.-Lipschitz continuity, smoothness, bounded variance, bounded function, and convexity assumptions; c-a positive constant; \u2713-has such a property; \u00d7-hasn't such a property).", "description": "This table compares the generalization guarantees of several algorithms, including Stochastic Compositional Gradient Descent (SCGD), Stochastically Corrected Stochastic Compositional Gradient Descent (SCSC), and Stochastic Gradient Descent (SGD).  It shows the generalization bounds achieved by each algorithm under different assumptions (Lipschitz continuity, smoothness, bounded variance, bounded function, convexity), highlighting the differences in their theoretical guarantees.", "section": "3.1 Learning Guarantees for General SCO"}, {"figure_path": "4AuEQ1FfUf/tables/tables_14_1.jpg", "caption": "Table 4: The main differences among our main results (\u2207f\u2081(wt) = \u2207g(wt) \u03a3\u03b9=1 (f(vt+1 + \u03bcut,1) \u2212 f(vt+1)), \u2207f2(wt) = \u03a3\u03b9=1 (g(wt + \u03bcut,1) \u2013 g(wt))\u2207f(vt+1), \u2207f3(Wt) = \u03a3\u03b9=1 (f(vt+1 + \u03bcut,1) \u2212 f(vt+1)) \u03a3\u03b9=1 (g(wt + \u03bcut,1) \u2013 g(wt))).", "description": "This table summarizes the main differences in generalization and optimization results for different theorems and corollaries in the paper, highlighting the specific conditions and techniques used (co-coercivity, almost co-coercivity, special decompositions) for each analysis.", "section": "C Proofs for General SCO"}]