{"importance": "This paper is crucial because **it bridges the gap in understanding the learning guarantees of derivative-free stochastic compositional optimization (SCO)**.  Its findings on the impact of black-box settings and the provided learning guarantees for vertical federated learning (VFL) are highly relevant to the ongoing research in SCO and its applications, paving the way for improved algorithms and a better theoretical understanding of this important class of optimization problems.", "summary": "This study reveals how black-box settings affect the learning guarantee of stochastic compositional optimization, offering sharper generalization bounds and novel learning guarantees for derivative-free algorithms, particularly in vertical federated learning.", "takeaways": ["Black-box SCO algorithms' generalization and optimization are theoretically analyzed using a new stability analysis framework.", "A better gradient estimation leads to stronger learning guarantees; a higher proportion of unknown gradients increases the dependence on gradient estimation quality.", "The first learning guarantees for VFL align with the findings for black-box SCGD and SCSC."], "tldr": "Stochastic compositional optimization (SCO) is a class of optimization problems involving compositional objective functions. While learning guarantees for SCO algorithms with known derivatives are established, the impact of derivative-free (black-box) settings remains unclear. This paper addresses this gap by focusing on two derivative-free SCO algorithms: black-box SCGD and SCSC.  Previous work on generalization guarantees for SCO algorithms suffered from overly complex analysis and restrictive conditions, leading to impractical iteration numbers.\n\nThis research offers improved generalization upper bounds for convex and non-convex SCGD and SCSC via a new stability analysis, applicable under milder conditions than previous work.  Furthermore, it extends the analysis to three black-box variants of SCGD and SCSC, revealing how better gradient estimation and fewer unknown gradients enhance learning guarantees.  The study applies these findings to vertical federated learning (VFL) algorithms, establishing the first-ever learning guarantees for these important real-world applications.", "affiliation": "Huazhong Agricultural University", "categories": {"main_category": "AI Theory", "sub_category": "Optimization"}, "podcast_path": "4AuEQ1FfUf/podcast.wav"}