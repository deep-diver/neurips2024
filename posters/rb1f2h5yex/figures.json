[{"figure_path": "RB1F2h5YEx/figures/figures_1_1.jpg", "caption": "Figure 1: Performance of algorithms on Metaworld tasks. The tasks change every 1 million steps, matching the dips in success rate in the learning curves (right). On the left, we show performance profiles showing the distribution of average success rates across tasks. Higher is better for both.", "description": "This figure displays a comparison of the performance of several reinforcement learning algorithms on MetaWorld tasks.  The left panel presents performance profiles, showing the distribution of average success rates across various tasks for each algorithm. The right panel shows learning curves illustrating the success rate over time.  The task changes every 1 million steps, which explains the dips in the success rate shown in the right panel.  Parseval regularization demonstrates a significant improvement over baseline methods.", "section": "3 Parseval Regularization"}, {"figure_path": "RB1F2h5YEx/figures/figures_2_1.jpg", "caption": "Figure 2: Comparing performance profiles of diagonal layers and learnable input scales on Metaworld sequences. Either addition helps with Parseval regularization.", "description": "This figure compares the performance of different approaches to enhance the capacity of neural networks using Parseval regularization in a continual reinforcement learning setting on MetaWorld tasks.  It shows performance profiles, illustrating the distribution of average success rates across multiple tasks. The results demonstrate that adding either diagonal layers or a learnable input scale improves performance when using Parseval regularization, although these additions alone are detrimental to the baseline agent's performance.", "section": "3.1 Network capacity and Lipschitz continuity"}, {"figure_path": "RB1F2h5YEx/figures/figures_3_1.jpg", "caption": "Figure 3: The left plot shows performance profiles of Parseval regularization on Metaworld sequences when dividing neurons in a layer into multiple groups. There is no significant improvement from splitting into groups; using only one group is the best choice. Adding Parseval regularization with any number of groups improves on the baseline though. The right plot shows the stable rank of the actor's second layer's weight matrix. Due to the relaxed orthogonal constraint on the weights, we can observe a decrease in the stable rank. Similar plots can be observed for other layers and in the critic.", "description": "The figure shows the impact of dividing neurons in a layer into subgroups on the performance of Parseval regularization in a continual reinforcement learning setting using Metaworld tasks.  The left plot presents performance profiles, showing that dividing neurons into subgroups doesn't significantly improve performance compared to using a single group; however,  Parseval regularization consistently outperforms the baseline regardless of the number of subgroups. The right plot displays the stable rank of the second layer's weight matrix for the actor network across different subgroup configurations and the baseline.  A decreased stable rank is observed with multiple subgroups, indicating a relaxation in the orthogonality constraint imposed by Parseval regularization.", "section": "Relaxing the orthogonality constraint"}, {"figure_path": "RB1F2h5YEx/figures/figures_5_1.jpg", "caption": "Figure 4: Performance of algorithms on gridworld and CARL environments. Parseval regularization yields the largest improvements although other approaches can be helpful.", "description": "This figure compares the performance of several continual reinforcement learning algorithms across two different environments: a gridworld navigation task and two tasks from the CARL (Contextual and Adaptive Reinforcement Learning) benchmark.  The algorithms compared include a baseline (base) agent, along with Layer Normalization (Layer Norm), Shrink-and-Perturb (SnP), Regenerative Regularization (Regen), and a Wasserstein-distance based variant of Regen (W-Regen).  The figure highlights the substantial improvement achieved by Parseval regularization in both environments, showcasing its effectiveness in continual learning settings. While other methods show some improvement over the baseline, none reach the level of Parseval regularization's performance boost.", "section": "4.1 Utility of Parseval regularization"}, {"figure_path": "RB1F2h5YEx/figures/figures_6_1.jpg", "caption": "Figure 5: Performance profiles for different architecture choices. (Left and center) Varying activation functions: all choices benefit from Parseval regularization. (Right) Varying the network width. Parseval regularization can benefit all three settings. Increasing the width alone does not help.", "description": "This figure presents the results of ablation studies on different neural network architectures. The left and center plots show how different activation functions (Tanh, ReLU, Mish, CReLU, MaxMin) perform with and without Parseval regularization.  The right plot compares the performance of networks with different widths (32, 64, 128) with and without Parseval regularization. The results demonstrate that Parseval regularization consistently improves performance across different activation functions and network widths, whereas simply increasing network width has minimal impact.", "section": "4.2 Variations of architecture and algorithm"}, {"figure_path": "RB1F2h5YEx/figures/figures_7_1.jpg", "caption": "Figure 6: Parseval regularization on normalized row weight vectors, regularizing only angles, on the Metaworld sequences. The right figure shows the average angle between row weight vectors, confirming it has the desired effect.", "description": "This figure presents ablation studies on Parseval regularization, focusing on the impact of normalizing row weights before applying the regularization.  The left panel shows the performance profiles demonstrating that while there is some improvement over the baseline, it's not as significant as the full Parseval regularization.  The right panel shows the average angle between row weight vectors over training steps.  It confirms the efficacy of the modified regularization in encouraging orthogonality by keeping the average angle near zero.", "section": "4.3 Ablation studies"}, {"figure_path": "RB1F2h5YEx/figures/figures_20_1.jpg", "caption": "Figure 1: Performance of algorithms on Metaworld tasks. The tasks change every 1 million steps, matching the dips in success rate in the learning curves (right). On the left, we show performance profiles showing the distribution of average success rates across tasks. Higher is better for both.", "description": "This figure compares the performance of different algorithms on Metaworld tasks in a continual reinforcement learning setting.  The left panel shows performance profiles, illustrating the distribution of average success rates across multiple tasks. The right panel displays learning curves showing success rate over training steps.  The tasks change every 1 million steps, leading to dips in performance. Parseval regularization significantly improves both the average success rate and the learning curve, outperforming other methods.", "section": "3 Parseval Regularization"}, {"figure_path": "RB1F2h5YEx/figures/figures_20_2.jpg", "caption": "Figure 8: Performance profile for two settings of the initialization scale. With Parseval regularization, they both perform similarly.", "description": "This figure compares the performance of two different initialization scales (1 and sqrt(2)) with and without Parseval regularization.  The performance profiles show the probability that an agent achieves a success rate greater than or equal to a given average success rate across different tasks.  The results indicate that Parseval regularization mitigates the effect of different initialization scales on performance, leading to similar results regardless of the chosen scale.", "section": "4.2 Variations of architecture and algorithm"}, {"figure_path": "RB1F2h5YEx/figures/figures_21_1.jpg", "caption": "Figure 9: The first plot shows learning curves for different values of entropy regularization. The second plot shows curves for the policy entropy throughout training for different algorithms. We observe that larger entropy can be better when set properly btu can also have a detrimental effect if too large.", "description": "This figure shows the effect of entropy regularization on continual reinforcement learning.  The left subplot displays learning curves for different entropy regularization strengths, illustrating the impact on the success rate. The right subplot shows how policy entropy changes across various algorithms over time, highlighting the relationship between entropy and performance.", "section": "A.3 The role of entropy"}, {"figure_path": "RB1F2h5YEx/figures/figures_22_1.jpg", "caption": "Figure 10: The distribution of the squared entries of the input-output Jacobian along training for the first three sequences of Metaworld tasks. The solid line is the mean while the shaded region denotes the 5th and 95th percentiles of the distribution. Parseval regularization reduces the spread of the entries.", "description": "This figure displays the distribution of squared entries in the input-output Jacobian over training for three different Metaworld task sequences. The plots show that Parseval regularization results in a tighter distribution of these magnitudes compared to the baseline, suggesting that it may contribute to a less difficult optimization landscape.", "section": "4.4 Analysis of training"}, {"figure_path": "RB1F2h5YEx/figures/figures_23_1.jpg", "caption": "Figure 1: Performance of algorithms on Metaworld tasks. The tasks change every 1 million steps, matching the dips in success rate in the learning curves (right). On the left, we show performance profiles showing the distribution of average success rates across tasks. Higher is better for both.", "description": "The figure compares the performance of different algorithms on Metaworld tasks in a continual reinforcement learning setting.  The left panel shows performance profiles illustrating the distribution of average success rates across various tasks for each algorithm. The right panel displays learning curves showing the success rate over time. The tasks switch every million steps, causing dips in the success rate curves, highlighting the challenge of continual learning. Parseval regularization shows a significant improvement over baseline and other methods.", "section": "3 Parseval Regularization"}, {"figure_path": "RB1F2h5YEx/figures/figures_24_1.jpg", "caption": "Figure 1: Performance of algorithms on Metaworld tasks. The tasks change every 1 million steps, matching the dips in success rate in the learning curves (right). On the left, we show performance profiles showing the distribution of average success rates across tasks. Higher is better for both.", "description": "This figure displays a comparison of different reinforcement learning algorithms' performance on Metaworld tasks.  The left panel shows performance profiles, illustrating the distribution of average success rates across multiple tasks. Each point represents the average success rate for a single task in a sequence of tasks.  The right panel shows learning curves, plotting the success rate over time.  These curves demonstrate how the success rate dips when a new task is introduced, indicating the challenge of continual learning. The Parseval regularization method shows improved performance compared to baseline and alternative methods (Layer Norm, Shrink-and-Perturb, Regenerative regularization) in both metrics.", "section": "3 Parseval Regularization"}, {"figure_path": "RB1F2h5YEx/figures/figures_25_1.jpg", "caption": "Figure 1: Performance of algorithms on Metaworld tasks. The tasks change every 1 million steps, matching the dips in success rate in the learning curves (right). On the left, we show performance profiles showing the distribution of average success rates across tasks. Higher is better for both.", "description": "This figure compares the performance of different algorithms on Metaworld tasks in a continual reinforcement learning setting.  The tasks change every 1 million steps.  The left panel shows performance profiles, illustrating the distribution of average success rates across all tasks for each algorithm. The right panel provides the learning curves showing the success rate over time.  Parseval regularization shows significant improvement over baseline and other methods.", "section": "3 Parseval Regularization"}, {"figure_path": "RB1F2h5YEx/figures/figures_25_2.jpg", "caption": "Figure 5: Performance profiles for different architecture choices. (Left and center) Varying activation functions: all choices benefit from Parseval regularization. (Right) Varying the network width: Parseval regularization can benefit all three settings. Increasing the width alone does not help.", "description": "This figure presents performance profiles that compare the effects of different activation functions (Tanh, ReLU, Mish, and MaxMin) and network widths on continual reinforcement learning agents.  The left and center plots show that adding Parseval regularization improves performance regardless of the activation function used. The right plot demonstrates that Parseval regularization enhances performance across different network widths, while simply increasing the network width without Parseval regularization does not provide the same benefit.", "section": "4.2 Variations of architecture and algorithm"}, {"figure_path": "RB1F2h5YEx/figures/figures_26_1.jpg", "caption": "Figure 15: Gridworld layout. There are nine rooms with doorways indicated by the orange lines. The agent starts at the green square at the beginning of each episode. The goal location is randomly generated in one of cells shaded blue. This is kept fixed until the task changes at which point it is resampled.", "description": "This figure shows the layout of the 15x15 gridworld environment used in the experiments. The grid is divided into nine rooms connected by doorways.  The agent begins each episode at the green square in the center. A goal location, represented by a blue shaded area, is randomly selected within one of the rooms at the start of each episode and remains fixed until the task changes. Then, a new goal location is randomly selected. This setup allows for a sequence of tasks, where the agent needs to navigate to different goal locations across episodes.", "section": "C.1 Gridworld"}]