[{"figure_path": "dGQtja9X2C/tables/tables_6_1.jpg", "caption": "Table 1: Generalized accuracy for SPRY and its backpropagation- and zero-order-based counterparts on ROBERTa Large and LLMs. SQUADv2 uses F1 score. \u2191 shows that higher values are better. The datasets are split with Dir a = 0.1. \u25ca = Llama2-7B. * = OPT6.7B. \u25a1 = OPT13B. SPRY outperforms the best-performing zero-order-based methods by 5.15\u201313.50% and approaches the performance of backpropagation-based methods, with a difference of 0.60\u20136.16%.", "description": "This table presents a comparison of the generalized accuracy achieved by SPRY against backpropagation-based and zero-order-based methods across various datasets and language models.  It highlights SPRY's superior performance, particularly against zero-order methods (5.15-13.50% improvement), and its competitiveness with backpropagation-based methods (within 0.60-6.16%).  The table includes results for different language models (RoBERTa Large, Llama 2, OPT6.7B, OPT13B) and a variety of tasks, showcasing SPRY's effectiveness across various settings.", "section": "5.1 Accuracy Performance Comparison"}, {"figure_path": "dGQtja9X2C/tables/tables_20_1.jpg", "caption": "Table 2: Communication cost of SPRY and all its baselines. M is the count of participation clients. Total count of trainable parameters of a global model is wg = weL (for simplicity, we assume that each layer has we parameters).", "description": "This table compares the communication costs (in terms of parameter count) of SPRY and its baseline methods. The costs are broken down by the communication frequency (per-epoch or per-iteration), and the direction of communication (from client to server or vice-versa). For each method, the table shows the communication cost for a single client and the total communication cost across all clients.", "section": "F Communication and Computation Costs"}, {"figure_path": "dGQtja9X2C/tables/tables_21_1.jpg", "caption": "Table 3: Computation cost of SPRY and all its baselines. The client-side cost is for each iteration, and the server-side cost is for each round. L is the layer count, M is the participating client count, c is the cost of matrix multiplication for each layer. v is the overhead related to column-by-column vector multiplications of jvp. we is the size of each layer and hence size of each layer's perturbation too. K is the perturbation count per iteration. K = 1 for SPRY and FEDMEZO, and ~20 for BAFFLE and FWDLLM.", "description": "This table compares the computation costs of SPRY against other baselines (backpropagation-based and zero-order methods). It breaks down the costs into client-side (per iteration) and server-side (per round) computations. The computation cost is analyzed based on the number of layers (L), the number of participating clients (M), the cost of matrix multiplication (c), the overhead of column-by-column vector multiplication in jvp (v), the size of each layer (we), and the number of perturbations per iteration (K).", "section": "F.2 Computation Costs"}, {"figure_path": "dGQtja9X2C/tables/tables_23_1.jpg", "caption": "Table 1: Generalized accuracy for SPRY and its backpropagation- and zero-order-based counterparts on ROBERTa Large and LLMs. SQUADv2 uses F1 score. \u2191 shows that higher values are better. The datasets are split with Dir a = 0.1. \u25ca = Llama2-7B. * = OPT6.7B. \u25a1 = OPT13B. SPRY outperforms the best-performing zero-order-based methods by 5.15\u201313.50% and approaches the performance of backpropagation-based methods, with a difference of 0.60\u20136.16%.", "description": "This table presents a comparison of the generalized accuracy achieved by SPRY against backpropagation and zero-order methods across various datasets and language models.  It highlights that SPRY significantly outperforms zero-order methods and performs comparably to backpropagation, often with only a small accuracy difference.", "section": "5.1 Accuracy Performance Comparison"}, {"figure_path": "dGQtja9X2C/tables/tables_30_1.jpg", "caption": "Table 1: Generalized accuracy for SPRY and its backpropagation- and zero-order-based counterparts on ROBERTa Large and LLMs. SQUADv2 uses F1 score. \u2191 shows that higher values are better. The datasets are split with Dir a = 0.1. \u25ca = Llama2-7B. * = OPT6.7B. \u25a1 = OPT13B. SPRY outperforms the best-performing zero-order-based methods by 5.15\u201313.50% and approaches the performance of backpropagation-based methods, with a difference of 0.60\u20136.16%.", "description": "This table presents a comparison of the generalized accuracy achieved by SPRY against backpropagation-based and zero-order-based methods on various datasets and language models.  It highlights SPRY's superior accuracy compared to zero-order methods and its near-parity performance with backpropagation, showcasing its effectiveness in federated learning.", "section": "5.1 Accuracy Performance Comparison"}, {"figure_path": "dGQtja9X2C/tables/tables_32_1.jpg", "caption": "Table 1: Generalized accuracy for SPRY and its backpropagation- and zero-order-based counterparts on ROBERTa Large and LLMs. SQUADv2 uses F1 score. \u2191 shows that higher values are better. The datasets are split with Dir a = 0.1. \u25ca = Llama2-7B. * = OPT6.7B. \u25a1 = OPT13B. SPRY outperforms the best-performing zero-order-based methods by 5.15\u201313.50% and approaches the performance of backpropagation-based methods, with a difference of 0.60\u20136.16%.", "description": "This table presents a comparison of the generalized accuracy achieved by SPRY against backpropagation-based and zero-order-based methods across various datasets and language models.  It highlights SPRY's superior performance compared to zero-order methods and its near-parity with backpropagation methods in terms of accuracy.  The table also indicates the specific language models and datasets used in the experiments, and notes that the F1 score is used for the SQUADv2 dataset.", "section": "5.1 Accuracy Performance Comparison"}, {"figure_path": "dGQtja9X2C/tables/tables_32_2.jpg", "caption": "Table 1: Generalized accuracy for SPRY and its backpropagation- and zero-order-based counterparts on ROBERTa Large and LLMs. SQUADv2 uses F1 score. \u2191 shows that higher values are better. The datasets are split with Dir a = 0.1. \u25ca = Llama2-7B. * = OPT6.7B. \u25a1 = OPT13B. SPRY outperforms the best-performing zero-order-based methods by 5.15\u201313.50% and approaches the performance of backpropagation-based methods, with a difference of 0.60\u20136.16%.", "description": "This table compares the generalized accuracy of SPRY against other backpropagation-based and zero-order-based methods. The datasets used are AG News, SST2, SNLI, MNLI, Yahoo, Yelp, MultiRC, and SQUADv2. The language models used are ROBERTa Large, Llama2-7B, OPT6.7B, and OPT13B.  The table shows that SPRY outperforms zero-order methods significantly and gets close to the accuracy of backpropagation methods, demonstrating its effectiveness and efficiency.", "section": "5.1 Accuracy Performance Comparison"}]