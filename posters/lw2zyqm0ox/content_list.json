[{"type": "text", "text": "Accelerated Regularized Learning in Finite $N$ -Person Games ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kyriakos Lotidis Stanford University klotidis@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Angeliki Giannou University of Wisconsin\u2013Madison giannou@wisc.edu ", "page_idx": 0}, {"type": "text", "text": "Panayotis Mertikopoulos Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP LIG 38000 Grenoble, France panayotis.mertikopoulos@imag.fr ", "page_idx": 0}, {"type": "text", "text": "Nicholas Bambos Stanford University bambos@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Motivated by the success of Nesterov\u2019s accelerated gradient algorithm for convex minimization problems, we examine whether it is possible to achieve similar performance gains in the context of online learning in games. To that end, we introduce a family of accelerated learning methods, which we call \u201cfollow the accelerated leader\u201d (FTXL), and which incorporates the use of momentum within the general framework of regularized learning \u2013 and, in particular, the exponential / multiplicative weights algorithm and its variants. Drawing inspiration and techniques from the continuous-time analysis of Nesterov\u2019s algorithm, we show that FTXL converges locally to strict Nash equilibria at a superlinear rate, achieving in this way an exponential speed-up over vanilla regularized learning methods (which, by comparison, converge to strict equilibria at a geometric, linear rate). Importantly, FTXL maintains its superlinear convergence rate in a broad range of feedback structures, from deterministic, full information models to stochastic, realization-based ones, and even bandit, payoff-based information, where players are only able to observe their individual realized payoffs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One of the most important milestones in convex optimization was Nesterov\u2019s accelerated gradient (NAG) algorithm, as proposed by Nesterov [28] in 1983. The groundbreaking achievement of Nesterov\u2019s algorithm was that it attained an $\\mathcal{O}(1/T^{2})$ rate of convergence in Lipschitz smooth convex minimization problems, thus bridging a decades-old gap between the $\\mathcal{O}(1/T)$ convergence rate of ordinary gradient descent and the corresponding $\\Omega(1/\\Bar{T}^{2})$ lower bound for said class [27]. In this way, Nesterov\u2019s accelerated gradient algorithm opened the door to acceleration in optimization, leading in turn to a wide range of other, likewise influential schemes \u2013 such as FISTA and its variants [2] \u2013 and jumpstarting a vigorous field of research that remains extremely active to this day. ", "page_idx": 0}, {"type": "text", "text": "Somewhat peculiarly, despite the great success that NAG has enjoyed in all fields where optimization plays a major role \u2013 and, in particular, machine learning and data science \u2013 its use has not percolated to the adjoining field of game theory as a suitable algorithm for learning Nash equilibria. Historically, the reasons for this are easy to explain: despite intense scrutiny by the community and an extensive corpus of literature dedicated to deconstructing the algorithm\u2019s guarantees, NAG\u2019s update structure remains quite opaque \u2013 and, to a certain extent, mysterious. Because of this, Nesterov\u2019s algorithm could not be considered as a plausible learning scheme that could be employed by boundedly rational human agents involved in a repeated game. Given that this was the predominant tenet in economic thought at the time, the use of Nesterov\u2019s algorithm in a game-theoretic context has not been extensively explored, to the best of our knowledge. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "On the other hand, as far as applications to machine learning and artificial intelligence are concerned, the focus on human agents is no longer a limiting factor. In most current and emerging applications of game-theoretic learning \u2013 from multi-agent reinforcement learning to adversarial models in machine learning \u2013 the learning agents are algorithms whose computational capacity is only limited by the device on which they are deployed. In view of this, our paper seeks to answer the following question: ", "page_idx": 1}, {"type": "text", "text": "Can Nesterov\u2019s accelerated gradient scheme be deployed in a game-theoretic setting? And, if so, is it possible to achieve similar performance gains as in convex optimization? ", "page_idx": 1}, {"type": "text", "text": "Our contributions in the context of related work. The answer to the above questions is not easy to guess. On the one hand, given that game theory and convex optimization are fundamentally different fields, a reasonable guess would be \u201cno\u201d \u2013 after all, finding a Nash equilibrium is a PPAD-complete problem [6], whereas convex minimization problems are solvable in polynomial time [4]. On the other, since in the context of online learning each player would have every incentive to use the most efficient unilateral optimization algorithm at their disposal, the use of NAG methods cannot be easily discarded from an algorithmic viewpoint. ", "page_idx": 1}, {"type": "text", "text": "Our paper examines if it is possible to obtain even a partially positive answer to the above question concerning the application of Nesterov\u2019s accelerated gradients techniques to learning in games. We focus throughout on the class of finite $N$ -person games where, due to the individual concavity of the players\u2019 payoff functions, the convergence landscape of online learning in games is relatively well-understood \u2013 at least, compared to non-concave games. In particular, it is known that regularized learning algorithms \u2013 such as \u201cfollow the regularized leader\u201d (FTRL) and its variants \u2013 converge locally to strict Nash equilibria at a geometric rate [15], and strict equilibria are the only locally stable and attracting limit points of regularized learning in the presence of randomness and/or uncertainty [8, 14, 19]. In this regard, we pose the question of $(i)$ whether regularized learning schemes like FTRL can be accelerated; and $(i i)$ whether the above properties are enhanced by this upgrade. ", "page_idx": 1}, {"type": "text", "text": "We answer both questions in the positive. First, we introduce an accelerated regularized scheme, in both continuous and discrete time, which we call \u201cfollow the accelerated leader\u201d (FTXL). In continuous time, our scheme can be seen as a fusion of the continuous-time analogue of NAG proposed by Su, Boyd, and Cand\u00e8s [31] and the dynamics of regularized learning studied by Mertikopoulos & Sandholm [25]. We show that the resulting dynamics exhibit the same qualitative equilibrium convergence properties as the replicator dynamics of Taylor & Jonker [32] (the most widely studied instance of FTRL in continuous time). However, whereas the replicator dynamics converge to strict Nash equilibria at a linear rate, the FTXL dynamics do so at a superlinear rate. ", "page_idx": 1}, {"type": "text", "text": "In discrete time, we likewise propose an algorithmic implementation of FTXL which can be applied in various information context: (i) full information, that is, players observe their entire mixed payoff vector; $(i i)$ realization-based feedback, i.e., when players get to learn the \u201cwhat-if\u201d payoff of actions that they did not choose; and (iii) bandit, payoff-based feedback, where players only observe their realized, in-game payoff, and must rely on statistical estimation techniques to reconstruct their payoff vectors. In all cases, we show that FTXL maintains the exponential speedup described above, and converges to strict Nash equilibria at a superlinear rate (though the subleading term in the algorithm\u2019s convergence rate becomes increasingly worse as less information is available). We find this feature of FTXL particularly intriguing as superlinear convergence rates are often associated to methods that are second-order in space, not time; the fact that this is achieved even with bandit feedback is quite surprising in this context. ", "page_idx": 1}, {"type": "text", "text": "Closest to our work is the continuous-time, second-order replicator equation studied by [20] in the context of evolutionary game theory, and derived through a model of pairwise proportional imitation of \u201clong-term success\u201d. The dynamics of [20] correspond to the undamped, continuous-time version of FTXL with entropic regularization, and the equilibrium convergence rate obtained by [20] agrees with our analysis. Other than that, the dynamics of Fl\u00e5m & Morgan [9] also attempted to exploit a Newtonian structure, but they do not yield favorable convergence properties in a general setting. The inertial dynamics proposed in [21] likewise sought to leverage an inertial structure combined with the Hessian\u2013Riemannian underpinnings of the replicator dynamics, but the resulting replicator equation was not even well-posed (in the sense that its solutions exploded in finite time). ", "page_idx": 1}, {"type": "text", "text": "More recently, Gao & Pavel [12, 13] considered a second-order, inertial version of the dynamics of mirror descent in continuous games, and examined their convergence in the context of variational stability [26]. Albeit related at a high level (given the link between mirror descent and regularized learning), the dynamics of Gao & Pavel [12, 13] are actually incomparable to our own, and there is no overlap in our techniques or results. Other than that, second-order dynamics in games have also been studied in continuous time within the context of control-theoretic passivity, yielding promising results in circumventing the impossibility results of Hart & Mas-Colell [17], cf. Gao & Pavel [10, 11], Mabrok & Shamma [24], Toonsi & Shamma [33], and references therein. However, the resulting dynamics are also different, and we do not see a way of obtaining comparable rates in our context. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we outline some notions and definitions required for our analysis. Specifically, we introduce the framework of finite $N$ -player games, we discuss the solution concept of a Nash equilibrium, and we present the main ideas of regularized learning in games. ", "page_idx": 2}, {"type": "text", "text": "2.1. Finite games. In this work, we focus exclusively with finite games in normal form. Such games consist of a finite set of players $\\mathcal{N}=\\{1,\\dots,N\\}$ , each of whom has a finite set of actions \u2013 or pure strategies \u2013 $\\alpha_{i}\\in\\mathcal{A}_{i}$ and a payoff function $u_{i}\\colon A\\rightarrow\\mathbb{R}.$ , where $\\begin{array}{r}{\\mathcal{A}:=\\prod_{i\\in\\mathcal{N}}\\mathcal{A}_{i}}\\end{array}$ denotes the set of all possible action profiles $\\alpha=(\\alpha_{1},\\dots,\\alpha_{N})$ . To keep track of all this, a finite game with the above primitives will be denoted as $\\Gamma\\equiv\\Gamma(\\mathcal{N},\\mathcal{A},u)$ . ", "page_idx": 2}, {"type": "text", "text": "In addition to pure strategies, players may also randomize their choices by employing mixed strategies, that is, by choosing probability distributions $x_{i}\\,\\in\\,\\mathcal{X}_{i}\\,:=\\,\\Delta(\\mathcal{A}_{i})$ over their pure strategies, where $\\Delta(\\mathcal{A}_{i})$ denotes the probability simplex over $\\mathcal{A}_{i}$ . Now, given a strategy profile $x=(x_{1},\\dots,x_{N})\\ \\in$ $\\textstyle\\mathcal{X}:=\\prod_{i\\in\\mathcal{N}}\\mathcal{X}_{i}$ , we will use the standard shorthand $x=\\left(x_{i};x_{-i}\\right)$ to highlight the mixed strategy $x_{i}$ of player $i$ against the mixed strategy proflie $x_{-i}\\in\\mathcal{X}_{-i}:=\\prod_{j\\neq i}\\mathcal{X}_{j}$ of all other players. We also define: ", "page_idx": 2}, {"type": "text", "text": "1. The mixed payoff of player $i$ under $x$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nu_{i}(x)=u_{i}(x_{i};x_{-i})=\\sum_{\\alpha_{1}\\in\\mathcal{A}_{1}}\\cdot\\cdot\\cdot\\sum_{\\alpha_{N}\\in\\mathcal{A}_{N}}x_{1\\alpha_{1}}\\cdot\\cdot\\cdot x_{N\\alpha_{N}}u_{i}(\\alpha_{1},\\dotsc,\\alpha_{N})\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "2. The mixed payoff vector of player $i$ under $x$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{i}(x)=\\nabla_{x_{i}}u_{i}(x)=\\left(u_{i}(\\alpha_{i};x_{-i})\\right)_{\\alpha_{i}\\in\\mathcal{A}_{i}}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In words, $v_{i}(x)$ collects the expected rewards $v_{i\\alpha_{i}}(x)\\ :=\\ u_{i}(\\alpha_{i};x_{-i})$ of each action $\\alpha_{i}\\ \\in\\ {\\mathcal{A}}_{i}$ of player $i\\in\\mathcal{N}$ against the mixed strategy profile $x_{-i}$ of all other players. Finally, we write $v(x)=$ $(v_{1}(x),\\ldots,v_{N}(x))$ for the concatenation of the players\u2019 mixed payoff vectors. ", "page_idx": 2}, {"type": "text", "text": "In terms of solution concepts, we will say that $x^{*}$ is a Nash equilibrium (NE) if no player can benefti by unilaterally deviating from their strategy, that is ", "page_idx": 2}, {"type": "equation", "text": "$$\nu_{i}(x^{*})\\geq u_{i}(x_{i};x_{-i}^{*})\\quad\\mathrm{for\\,all}\\;x_{i}\\in\\mathcal{X}_{i}\\;\\mathrm{and}\\;\\mathrm{all}\\;i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Moreover, we say that $x^{*}$ is a strict Nash equilibrium if (NE) holds as a strict inequality for all $x_{i}\\neq x_{i}^{*}$ , $i\\in\\mathcal{N}$ , i.e., if any deviation from $\\boldsymbol{x}_{i}^{*}$ results in a strictly worse payoff for the deviating player $i\\in\\dot{\\mathcal{N}}$ . It is straightforward to verify that a strict equilibrium $x^{*}\\in\\mathcal{X}$ is also pure in the sense that each player assigns positive probability only to a single pure strategy $\\alpha_{i}^{*}\\in\\mathcal{A}_{i}$ . Finally, we denote the support of a strategy $x$ as the set of actions with non-zero probability mass, i.e., $\\operatorname{supp}(x)=\\{\\alpha\\in{\\mathcal{A}}:x_{\\alpha}>0\\}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2. Regularized learning in games. In the general context of finite games, the most widely used learning scheme is the family of algorithms and dynamics known as \u201cfollow the regularized leader\u201d (FTRL). In a nutshell, the main idea behind FTRL is that each player $i\\in\\mathcal{N}$ plays a \u201cregularized\u201d best response to their cumulative payoff over time, leading to the continuous-time dynamics ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\dot{y}_{i}(t)=v_{i}(x(t))\\qquad x_{i}(t)=Q_{i}(y_{i}(t))\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{i}(y_{i})=\\arg\\operatorname*{max}_{x_{i}\\in{\\mathcal{X}_{i}}}\\{\\langle y_{i},x_{i}\\rangle-h_{i}(x_{i})\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "denotes the regularized best response \u2013 or mirror \u2013 map of player $i\\in\\mathcal{N}$ , and $h_{i}\\colon\\mathcal{X}_{i}\\rightarrow\\mathbb{R}$ is a strongly convex function known as the method\u2019s regularizer. Accordingly, in discrete time, this leads to the algorithm ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i,n+1}=y_{i,n}+\\gamma\\hat{v}_{i,n}\\qquad x_{i,n}=Q_{i}(y_{i,n})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\gamma>0$ is a hyperparameter known as the algorithm\u2019s learning rate (or step-size) and $\\hat{v}_{i,n}$ is a black-box \u201cpayoff signal\u201d that carries information about $v_{i}(x_{n})$ . In the simplest case, when players have full information about the game being played and the actions taken by their opponents, we have $\\hat{v}_{i,n}=v_{i}(x_{n})$ ; in more information-depleted environments (such as learning with payoff-based, bandit feedback), $\\hat{v}_{i,n}$ is a reconstruction of $v_{i}(x_{n})$ based on whatever information is at hand. ", "page_idx": 3}, {"type": "text", "text": "For concreteness, we close this section with the prototypical example of FTRL methods, the exponential / multiplicative weights (EW) algorithm. Going back to [1, 23, 34], this method is generated by the negentropy regularizer $\\begin{array}{r}{h_{i}(x_{i})=\\sum_{\\alpha_{i}\\in\\mathcal{A}_{i}}x_{i\\alpha_{i}}\\log x_{i\\alpha_{i}}}\\end{array}$ , which yields the EW update rule ", "page_idx": 3}, {"type": "equation", "text": "$$\ny_{i,n+1}=y_{i,n}+\\gamma\\hat{v}_{i,n}\\qquad x_{i,n}=\\Lambda_{i}(y_{i,n}):=\\frac{\\exp(y_{i,n})}{||\\exp(y_{i,n})||_{1}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and, in the continuous-time limit $\\gamma\\to0$ , the exponential weights dynamics ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\dot{y}_{i}(t)=v_{i}(x(t))\\qquad x_{i}(t)=\\Lambda_{i}(y_{i}(t))\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the above, $\\Lambda_{i}$ denotes the regularized best response induced by the method\u2019s entropic regularizer, which is known colloquially as a logit best response \u2013 or, even more simply, as the logit map. To make the notation more compact in the sequel, we will write $Q=(Q_{i})_{i\\in\\mathcal{N}}$ and $\\Lambda=(\\Lambda_{i})_{i\\in\\mathcal{N}}$ for the ensemble of the players\u2019 regularized / logit best response maps. ", "page_idx": 3}, {"type": "text", "text": "Remark 1. To streamline our presentation, in the main part of the paper, quantitative results will be stated for the special case of the EW setup above. In Appendix A, we discuss more general decomposable regularizers of the form $\\begin{array}{r}{h_{i}(x_{i})=\\sum_{\\alpha_{i}\\in\\mathcal{A}_{i}}\\theta_{i}(x_{i})}\\end{array}$ where $\\theta_{i}\\colon[0,1]\\rightarrow\\ensuremath{\\mathbb{R}}$ is continuous on [0, 1], and has $\\theta^{\\prime\\prime}(x)\\;>\\;0$ for all $x\\ \\in\\ (0,1]$ and $\\begin{array}{r}{\\dot{\\mathrm{lim}}_{x\\rightarrow0^{+}}\\,\\theta^{\\prime}(x)\\,=\\,-\\infty}\\end{array}$ . Although this set of assumptions can be relaxed, it leads to the clearest presentation of our results, so it will suffice for us. Remark 2. Throughout the paper, we will interchangeably use ${\\dot{g}}(t)$ and $d g/d t$ to denote the time derivative of $g(t)$ . This dual notation allows us to adopt whichever form is most convenient in the given context. Moreover, for a process $g$ , we will use the notation $g(t)$ for $t\\,\\geq\\,0$ if it evolves in continuous time, and $g_{n}$ for $n\\in\\mathbb{N}$ if it evolves in discrete time steps, omitting the time-index when it is clear from context. ", "page_idx": 3}, {"type": "text", "text": "3 Combining acceleration with regularization: First insights and results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we proceed to illustrate how Nesterov\u2019s accelerated gradient (NAG) method can be combined with FTRL. To keep things as simple as possible, we focus on the continuous-time limit, so we do not have to worry about the choice of hyperparameters, the construction of black-box models for the players\u2019 payoff vectors, etc. ", "page_idx": 3}, {"type": "text", "text": "3.1. Nesterov\u2019s accelerated gradient algorithm. We begin by discussing Nesterov\u2019s accelerated gradient algorithm as presented in Nesterov\u2019s seminal paper [28] in the context of unconstrained smooth convex minimization. Specifically, given a Lipschitz smooth convex function $f\\colon\\ensuremath{\\mathbb{R}}^{d}\\to\\ensuremath{\\mathbb{R}}.$ the algorithm unfolds iteratively as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{x_{n+1}=w_{n}-\\gamma\\nabla f(w_{n})}\\\\ {w_{n+1}=x_{n+1}+\\displaystyle\\frac{n}{n+3}(x_{n+1}-x_{n})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $w_{1}\\;=\\;x_{1}$ is initialized arbitrarily and $\\gamma\\,>\\,0$ is a step-size parameter (typically chosen as $\\gamma\\leftarrow1/L$ where $L$ is the Lipschitz smoothness modulus of $f$ ). The specific iterative structure of (NAG) \u2013 and, in particular the $\"3\"$ in the denominator \u2013 can appear quite mysterious, but (NAG) otherwise offers remarkable perfomance gains, improving in particular the rate of convergence of gradient methods from $\\mathcal{O}(1/T)$ to $\\mathcal{O}(1\\bar{/}T^{2})$ [28], and matching in this way the corresponding $\\Omega(\\overline{{1}}/T^{2})$ lower bound for the minimization of smooth convex functions [27]. ", "page_idx": 3}, {"type": "text", "text": "This groundbreaking result has since become the cornerstone of a vast and diverse literature expanding on the properties of (NAG) and trying to gain a deeper understanding of the \u201chow\u201d and \u201cwhy\u201d of its update structure. One perspective that has gained significant traction in this regard is the continuoustime approach of $\\mathrm{Su}$ et al. [30, 31]; combining the two equations in (NAG) into ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{x_{n+1}-2\\,x_{n}+x_{n-1}}{\\sqrt{\\gamma}}=-\\sqrt{\\gamma}\\,\\nabla f(w_{n})-\\frac{3}{n+2}\\frac{x_{n}-x_{n-1}}{\\sqrt{\\gamma}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "they modeled (NAG) as a heavy ball with vanishing friction system of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{d^{2}x}{d t^{2}}}=-\\nabla f(x)-{\\frac{3}{t}}{\\frac{d x}{d t}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The choice of terminology alludes to the fact that (HBVF) describes the dynamics of a heavy ball descending the landscape of $f$ under the potential field $F(x)\\,=\\,-\\nabla f(x)$ with a vanishing kinetic friction coefficient (the $3/t$ factor in front of the momentum term $d x/d t$ ). In this interpretation, the mass of the ball accelerates the system, the friction term dissipates energy to enable convergence, and the vanishing friction coefficient quenches the impact of friction over time in order to avoid decelerating the system too much (so the system is, in a sense, \u201ccritically underdamped\u201d). ", "page_idx": 4}, {"type": "text", "text": "As was shown by $\\,\\,\\mathrm{{Su}}$ et al. [31], an explicit Euler discretization of (HBVF) yields (NAG) with exactly the right momentum coefficient $n/(n+3)$ ; moreover, the rate of convergence of the continuous-time dynamics (HBVF) is the same as that of the discrete-time algorithm (NAG), and the energy function and Lyapunov analysis used to derive the former can also be used to derive the latter. For all these reasons, (HBVF) is universally considered as the de facto continuous-time analogue of (NAG), and we will treat it as such in the sequel. ", "page_idx": 4}, {"type": "text", "text": "3.2. NAG meets FTRL. To move from unconstrained convex minimization problems to finite $N$ -person games \u2013 a constrained, non-convex, multi-agent, multi-objective setting \u2013 it will be more transparent to start with the continuous-time formulation (HBVF). Indeed, applying the logic behind (HBVF) to the (unconstrained) state variables $y$ of (FTRL-D), we obtain the \u201cfollow the accelerated leader\u201d dynamics ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\frac{d^{2}y}{d t^{2}}}=v(Q(y))-{\\frac{r}{t}}{\\frac{d y}{d t}}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the dynamics\u2019 driving force $F(y)\\,=\\,v(Q(y))$ is now given by the payoff field of the game, and the factor $r/t,r\\geq0$ , plays again the role of a vanishing friction coefficient. To avoid confusion, we highlight that in the case of regularized learning, the algorithm\u2019s variable that determines the evolution of the system in an autonomous way is the \u201cscore variable\u201d $y$ , not the \u201cstrategy variable\u201d $x$ (which is an ancillary variable obtained from $y$ via the regularized choice map $Q$ ). ", "page_idx": 4}, {"type": "text", "text": "In contrast to (EWD), the accelerated dynamics (FTXL-D) are second-order in time, a fact with fundamental ramifications, not only from a conceptual, but also from an operational viewpoint. Focusing on the latter, we first note that (FTXL-D) requires two sets of initial conditions, $y(0)$ and $\\dot{y}(0)$ , the latter having no analogue in the first-order setting of (FTRL-D). In general, the evolution of the system depends on both $y(0)$ and $\\dot{y}(0)$ , but since this would introduce an artificial bias toward a certain direction, we will take $\\dot{y}(0)=0$ , in tune with standard practice for (NAG) [31]. ", "page_idx": 4}, {"type": "text", "text": "We also note that (FTXL-D) can be mapped to an equivalent autonomous first-order system with double the variables: specifically, letting $\\boldsymbol{p}=\\dot{\\boldsymbol{y}}$ denote the players\u2019 (payoff) momentum, (FTXL-D) can be rewritten as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\frac{d y}{d t}=p\\qquad\\frac{d p}{d t}=v(Q(y))-\\frac{r}{t}p\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $y(0)$ initialized arbitrarily and $p(0)=\\dot{y}(0)$ . In turn, (5) yields $\\begin{array}{r}{p(t)=t^{-r}\\int_{0}^{t}t^{r}v(Q(y(\\tau)))\\,d\\tau}\\end{array}$ , so $p(t)$ can be seen as a weighted aggregate of the players\u2019 payoffs up to time $t$ : if $r=0$ (the undamped regime), all information enters $p(t)$ with the same weight; if $r>0$ , past information is discounted relative to more recent observations; and, in the overdamped limit $r\\to\\infty$ , all weight is assigned to the current point in time, emulating in this way the first-order system (FTRL-D). ", "page_idx": 4}, {"type": "text", "text": "3.3. First insights and results. From an operational standpoint, the main question of interest is to specify the equilibrium convergence properties of (FTXL-D) \u2013 and, later in the paper, its discrete-time analogue. To establish a baseline, the principal equilibrium properties of its first-order counterpart can be summarized as follows: $(i)$ strict Nash equilibria are locally stable and attracting under (FTRL-D) ", "page_idx": 4}, {"type": "text", "text": "[19, 25];1 $(i i)$ the dynamics do not admit any other such points (that is, stable and attracting) [8]; and $(i i i)$ quantitively, in the case of (EWD), the dynamics converge locally to strict Nash equilibria at a geometric rate of the form $\\|x(t)-x^{*}\\|=\\mathcal{O}(\\exp(-c t))$ for some $c>0$ [25]. ", "page_idx": 5}, {"type": "text", "text": "Our first result below shows that the accelerated dynamics (FTXL-D) exhibit an exponential speed-up relative to (FTRL-D), and the players\u2019 orbits converge to strict Nash equilibria at a superlinear rate: ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let $x^{*}$ be a strict Nash equilibrium of $\\Gamma_{;}$ , and let $x(t)=Q(y(t))$ be a solution orbit of (FTXL-D). If $x(0)$ is sufficiently close to $x^{*}$ , then $x(t)$ converges to $x^{*}$ ; in particular, $i f$ (FTXL-D) is run with logit best responses (that is, $Q\\leftarrow\\Lambda$ ), we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|x(t)-x^{*}\\|_{\\infty}\\leq\\exp\\biggl(C-\\frac{c t^{2}}{2(r+1)}\\biggr)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C>0$ is a constant that depends only on the initialization of (FTXL-D) and ", "page_idx": 5}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\mathrm{supp}(x_{i}^{*})}[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})]>0\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is the minimum payoff difference at equilibrium. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (which we prove in Appendix B) is representative of the analysis to come, so some remarks are in order. First, we should note that the explicit rate estimate (6) is derived for the special case of logit best responses, which underlie all exponential / multiplicative weights algorithms. To the best of our knowledge, the only comparable result in the literature is the similar rate provided in [20] for the case $r=0$ . In the case of a general regularizer, an analogous speed-up is observed, but the exact expressions are more involved, so we defer them to Appendix B. A second important point concerns whether the rate estimate (6) is tight or not. Finally, the neighborhood of initial conditions around $x^{*}$ is determined by the minimum payoff difference at equilibrium and is roughly $O(c)$ in diameter; we defer the relevant details of this discussion to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "To answer this question \u2013 and, at the same time get a glimpse of the proof strategy for Theorem 1 \u2013 it will be instructive to consider a single-player game with two actions. Albeit simple, this toy example is not simplistic, as it provides an incisive look into the problem, and will be used to motivate our design choices in the sequel. ", "page_idx": 5}, {"type": "text", "text": "Example 3.1. Consider a single-player game $\\Gamma$ with actions A and B such that $u(\\mathbf{A})-u(\\mathbf{B})=1$ , so the (dominant) strategy $x^{*}=(1,0)$ is a strict Nash equilibrium. Then, letting $z=y_{\\tt A}-y_{\\tt B}$ , (FTXL-D) readily yields ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\frac{d^{2}z}{d t^{2}}}={\\frac{d^{2}y_{\\mathrm{A}}}{d t^{2}}}-{\\frac{d^{2}y_{\\mathrm{B}}}{d t^{2}}}=u({\\mathsf{A}})-u({\\mathsf{B}})-{\\frac{r}{t}}\\left[{\\frac{d y_{\\mathrm{A}}}{d t}}-{\\frac{d y_{\\mathrm{B}}}{d t}}\\right]=1-{\\frac{r}{t}}{\\frac{d z}{d t}}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As we show in Appendix B, this non-autonomous differential equation can be solved exactly to yield $z(t)=z(0)+t^{2}/\\bar{[2(r+1)]}$ , and hence ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|x(t)-x^{*}\\|_{\\infty}={\\frac{1}{1+\\exp(z(t))}}\\sim\\exp\\!\\left(-z(0)-{\\frac{t^{2}}{2(r+1)}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since $c=u(\\mathbf{A})-u(\\mathbf{B})=1$ , the rate (9) coincides with that of Theorem 1 up to a factor of $1/2$ . This factor is an artifact of the analysis and, in fact, it can be tightened to $(1-\\varepsilon)$ for arbitrarily small $\\varepsilon>0$ ; we did not provide this more precise expression to lighten notation. By contrast, the factor $2(r+1)$ in (6) cannot be lifted; this has important ramifications which we discuss below. $\\spadesuit$ ", "page_idx": 5}, {"type": "text", "text": "The first conclusion that can be drawn from Example 3.1 is that the rate estimate of Theorem 1 is tight and cannot be improved in general. In addition, and in stark contrast to (NAG), Example 3.1 shows that the optimal value for the friction parameter is $r=0$ (at least from a min-max viewpoint, as this value yields the best possible lower bound for the rate). Of course, this raises the question as to whether this is due to the continuous-time character of the policy;2 however, as we show in detail in Appendix C, this is not the case: the direct handover of (NAG) to Example 3.1 yields the exact same rate (though the proof relies on a significantly more opaque generating function calculation). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "In view of all this, it becomes apparent that friction only hinders the equilibrium convergence properties of accelerated FTRL schemes in our game-theoretic setting. On that account, we will continue our analysis in the undamped regime $r=0$ . ", "page_idx": 6}, {"type": "text", "text": "4 Accelerated learning: Analysis and results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1. The algorithm. To obtain a bona fide, algorithmic implementation of the continuous-time dynamics (FTXL-D), we will proceed with the same explicit, finite-difference scheme leading to the discrete-time algorithm (NAG) from the continuous-time dynamics (HBVF) of $\\,\\mathrm{\\boldsymbol{Su}}$ et al. [31]. Specifically, taking a discretization step $\\gamma~>~0$ in (FTXL-D) and setting the scheme\u2019s friction parameter $r$ to zero (which, as we discussed at length in the previous section, is the optimal choice in our setting), a straightforward derivation yields the basic update rule ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[y_{i,n+1}-2y_{i,n}+y_{i,n-1}]/\\gamma^{2}=\\hat{v}_{i,n}\\quad\\mathrm{for~all~}i\\in\\mathcal{N}\\mathrm{~and~all~}n=1,2,\\dots.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In the above, just as in the case of (FTRL), $\\hat{v}_{i,n}\\,\\in\\,\\mathbb{R}^{A_{i}}$ denotes a black-box \u201cpayoff signal\u201d that carries information about the mixed payoff vector $v_{i}(x_{n})$ of player $i$ at the current strategy proflie $x_{n}$ (we provide more details on this below). ", "page_idx": 6}, {"type": "text", "text": "Alternatively, to obtain an equivalent first-order iterative rule (which is easier to handle and discuss), it will be convenient to introduce the momentum variables $p_{n}=(y_{n}-y_{n-1})/\\gamma$ . Doing just that, a simple rearrangement of (10) yields the \u201cfollow the accelerated leader\u201d scheme ", "page_idx": 6}, {"type": "equation", "text": "$$\ny_{i,n+1}=y_{i,n}+\\gamma p_{i,n+1}\\qquad p_{i,n+1}=p_{i,n}+\\gamma\\hat{v}_{i,n}\\qquad x_{i,n}=Q_{i}(y_{i,n})\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The algorithm (FTXL) will be our main object of study in the sequel, and we will examine its convergence properties under three differerent models for $\\hat{v}_{n}$ : ", "page_idx": 6}, {"type": "text", "text": "1. Full information, i.e., players get to access their full, mixed payoff vectors: ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\hat{v}}_{i,n}=v_{i}(x_{n})\\qquad\\qquad\\qquad{\\mathrm{for~all~}}i\\in\\mathcal{N},n=1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "2. Realization-based feedback, i.e., after choosing an action profile $\\alpha_{n}\\,\\sim\\,x_{n}$ , each player $i\\in\\mathcal{N}$ observes (or otherwise calculates) the vector of their counterfactual, \u201cwhat-if\u201d rewards, namely ", "page_idx": 6}, {"type": "equation", "text": "$$\n{\\hat{v}}_{i,n}=v_{i}(\\alpha_{n})\\qquad\\qquad\\qquad{\\mathrm{for~all~}}i\\in\\mathcal{N},n=1,2,\\ldots.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3. Bandit / Payoff-based feedback, i.e., each player only observes their current reward, and must rely on statistical estimation techniques to reconstruct an estimate of $v_{i}(x_{n})$ . For concreteness, we will consider the case where players employ a version of the so-called importance-weighted estimator ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{v}_{i,n}=\\operatorname{IWE}(x_{i,n};\\alpha_{i,n})\\qquad\\mathrm{~for~all~}i\\in\\mathcal{N},n=1,2,\\ldots.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which we describe in detail later in this section. ", "page_idx": 6}, {"type": "text", "text": "Of course, this list of information models is not exhaustive, but it is a faithful representation of most scenarios that arise in practice, so it will suffice for our purposes. ", "page_idx": 6}, {"type": "text", "text": "Now before moving forward with the analysis, it will be useful to keep some high-level remarks in mind. The first is that (FTXL) shares many similarities with (FTRL), but also several notable differences. At the most basic level, (FTRL) and (FTXL) are both \u201cstimulus-response\u201d schemes in the spirit of Erev & Roth [7], that is, players \u201crespond\u201d with a strategy $x_{i,n}=Q_{i}(y_{i,n})$ to a \u201cstimulus\u201d $y_{i,n}$ generated by the observed payoff signals $\\hat{v}_{i,n}$ . In this regard, both methods adhere to the online learning setting (and, in particular, to the regularized learning paradigm). ", "page_idx": 6}, {"type": "text", "text": "However, unlike (FTRL), where players respond to the aggregate of their payoff signals \u2013 the process $y_{n}$ in (FTRL) \u2013 the accelerated algorithm (FTXL) introduces an additional aggregation layer, which expresses how players \u201cbuild momentum\u201d based on the same payoff signals \u2013 the process $p_{n}$ in (FTXL). Intuitively, we can think of these two processes as the \u201cposition\u201d and \u201cmomentum\u201d variables of a classical inertial system, not unlike the heavy-ball dynamics of Su et al. [31]. The only conceptual difference is that, instead of rolling along the landscape of a (convex) function, the players now track the \u201cmirrored\u201d payoff field $\\hat{v}(y):=v(Q(y))$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In the rest of this section, we proceed to examine in detail the equilibrium convergence properties of (FTXL) under each of the three models detailed in Eqs. (11a)\u2013(11c) in order. ", "page_idx": 7}, {"type": "text", "text": "4.2. Accelerated learning with full information. We begin with the full information model (11a). This is the most straightforward model (due to the absence of randomness and uncertainty) but, admittedly, also the least realistic one. Nevertheless, it will serve as a useful benchmark for the rest, and it will allow us to introduce several important notions. ", "page_idx": 7}, {"type": "text", "text": "Before we state our result, it is important to note that a finite game can have multiple strict Nash equilibria, so global convergence results are, in general, unattainable; for this reason, we analyze the algorithm\u2019s local convergence landscape. In this regard, Theorem 2 below shows that (FTXL) with full information achieves a superlinear local convergence rate to strict Nash equilibria: ", "page_idx": 7}, {"type": "text", "text": "Theorem 2. Let $x^{*}$ be a strict Nash equilibrium of $\\Gamma_{\\vdots}$ , and let $x_{n}=Q(y_{n})$ be the sequence of play generated by (FTXL) with full information feedback of the form (11a). If $x_{1}$ is initialized sufficiently close to $x^{*}$ , then $x_{n}$ converges to $x^{*}$ ; in particular, $i f$ (FTXL) is run with logit best responses (that is, $Q\\leftarrow\\Lambda$ ), we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|x_{T}-x^{*}\\|_{\\infty}\\leq\\exp\\!\\left(C-c\\gamma^{2}{\\frac{T(T-1)}{2}}\\right)=\\exp\\!\\left(-\\Theta(T^{2})\\right)\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $C>0$ is a constant that depends only on the initialization of (FTXL) and ", "page_idx": 7}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\mathrm{supp}(x_{i}^{*})}[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})]>0\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "is the minimum payoff difference at equilibrium. ", "page_idx": 7}, {"type": "text", "text": "To maintain the flow of our discussion, we defer the proof of Theorem 2 to Appendix C. Instead, we only note here that, just as in the case of (HBVF) and (NAG), Theorem 2 provides essentially the same rate of convergence as its continuous-time counterpart, Theorem 1, modulo a subleading term which has an exponentially small impact on the rate of convergence. In particular, we should stress that the superlinear convergence rate of (FTXL) exhibits an exponential speedup relative to (FTRL), which is known to converge at a geometric rate $\\|x_{T}-x^{*}\\|_{\\infty}=\\exp(-\\Theta(T))$ . This is in direct correspondence to what we observe in continuous time, showing in particular that the continuous-time dynamics (FTXL-D) are a faithful representation of (FTXL). ", "page_idx": 7}, {"type": "text", "text": "We should also stress here that superlinear convergence rates are typically associated with methods that are second-order in space \u2013 like Newton\u2019s algorithm \u2013 not second-order in time \u2013 like (NAG) and (FTXL). We find this observation particularly intriguing as it suggests that accelerated rates can be observed in the context of learning in games without having to pay the excessively high compute cost of second-order methods in optimization. ", "page_idx": 7}, {"type": "text", "text": "4.3. Accelerated learning with realization-based feedback. We now turn to the realization-based model (11b), where players can only assess the rewards of their pure actions in response to the realized actions of all other players. In words, $\\hat{v}_{i,n}=v_{i}(\\alpha_{n})$ collects the payoffs that player $i\\in\\mathcal{N}$ would have obtained by playing each of their pure actions $\\alpha_{i}\\in\\mathcal{A}_{i}$ against the action profile $\\alpha_{-i,n}$ adopted by the rest of the players. ", "page_idx": 7}, {"type": "text", "text": "In contrast to the full information model (11a), the realization-based model is stochastic in nature, so our convergence results will likewise be stochastic. Nevertheless, despite the added layer of uncertainty, we show that (FTXL) with realization-based feedback maintains a superlinear convergence rate with high probability: ", "page_idx": 7}, {"type": "text", "text": "Theorem 3. Let $x^{*}$ be a strict Nash equilibrium of \u0393, fix some confidence level $\\delta\\:>\\:0_{\\!\\!\\!\\!\\!_{\\!}}$ , and let $x_{n}=Q(y_{n})$ be the sequence of play generated by (FTXL) with realization-based feedback as per (11b) and a sufficiently small step-size $\\gamma>0$ . Then there exists a neighborhood $\\boldsymbol{\\mathcal{U}}$ of $x^{*}$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(x_{n}\\to x^{*}\\mathrm{~as~}n\\to\\infty)\\geq1-\\delta\\qquad i f x_{1}\\in\\mathcal{U}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "lW2zYQm0ox/tmp/092fafdb2630b7f2d6137ad2b95889bd63a660a9c3eb4694aecbe6223ab67451.jpg", "img_caption": ["(a) Zero-sum game: Realization-based feedback "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "lW2zYQm0ox/tmp/fd87abc3d29c8a622c39aa9f0272ea361305a604a7a0e029ad6e9f330681ccb3.jpg", "img_caption": ["(b) Zero-sum game: Bandit feedback "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "lW2zYQm0ox/tmp/082b6d4b0ab93e19ef67231032adcd0e29c7885a56beb9d4102b19c17fc78ec1.jpg", "img_caption": ["(d) Congestion game: Bandit feedback "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "lW2zYQm0ox/tmp/209ea1cf84e595349948acea5e1ba372ead872512803ed210793467494ad535b.jpg", "img_caption": ["(c) Congestion game: Realization-based feedback ", "Figure 1: Performance evaluation of (FTXL) in a zero-sum and a congestion game under realization-based and bandit feedback. Solid lines represent average values, while shaded regions enclose $\\pm1$ standard deviation. The plots are in logarithmic scale. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In particular, $i f$ (FTXL) is run with logit best responses (that is, $Q\\leftarrow\\Lambda.$ ), there exist positive constants $C,c>0$ as in Theorem 2 such that on the event $\\{x_{n}\\to x^{*}$ as $n\\rightarrow\\infty\\}$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\|x_{T}-x^{*}\\|_{\\infty}\\leq\\exp\\biggl(C-c\\gamma^{2}\\frac{T(T-1)}{2}+\\frac{3}{5}c\\gamma^{5/3}T^{5/3}\\biggr)=\\exp\\bigl(-\\Theta(T^{2})\\bigr)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "What is particularly surprising in Theorem 3 is that, (FTXL) maintains the accelerated superlinear rate of Theorem $2-$ and, likewise, the exponential speedup relative to (FTRL) \u2013 despite the randomness and uncertainty involved in the realization-based model (11b). The salient point enabling this feature of (FTXL) is that $\\hat{v}_{n}$ can be expressed as ", "page_idx": 8}, {"type": "equation", "text": "$$\n{\\hat{v}}_{n}=v(x_{n})+U_{n}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $U_{n}\\in\\prod_{i}\\mathbb{R}^{A_{i}}$ is an almost surely bounded conditionally zero-mean stochastic perturbation, that is, $\\mathbb{E}[U_{n}\\,|\\,{\\mathcal{F}}_{n}]=0$ , where ${\\mathcal F}_{n}:=\\sigma(x_{1},\\ldots,x_{n})$ denotes the history of play up to (and including) time $n$ . Thanks to the boundedness of (16), we are able to derive a series of probabilistic estimates showing that, with high probability (and, in particular, greater than $1-\\delta$ ), the contribution of the noise in the algorithm\u2019s rate becomes subleading, thus allowing the superlinear rate of Theorem 2 to emerge. As in the case of Theorem 2, we defer the proof of Theorem 3 to the appendix. ", "page_idx": 8}, {"type": "text", "text": "4.4. Bandit feedback. The last framework we consider is the bandit model where players only observe their realized rewards, a scalar from which they must reconstruct their entire payoff vector. To do so, a standard technique from the multi-armed bandit literature is the so-called importance weighted estimator (IWE) [5, 22], defined in our setting as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{v}_{i\\alpha_{i},n}=\\frac{\\mathbb{1}\\{\\alpha_{i,n}=\\alpha_{i}\\}}{\\hat{x}_{i\\alpha_{i,n}}}u_{i}(\\alpha_{i};\\alpha_{-i,n})\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\hat{x}_{i,n}\\,=\\,(1-\\varepsilon_{n})x_{i,n}+\\varepsilon_{n}\\,\\mathrm{unif}_{\\mathcal{A}_{i}}$ is a mixture of $x_{i,n}$ and the uniform distribution on $\\boldsymbol{\\mathcal{A}}_{i}$ (a mechanism known in the literature as explicit exploration). Importantly, this estimator is unbiased relative to the perturbed strategy $\\hat{x}_{x_{n}}$ , which thus incurs an $\\mathcal{O}(\\varepsilon_{n})$ non-zero-sum error to the estimation of $v_{i}(x_{n})$ . This error can be made arbitrarily small by taking $\\varepsilon_{n}\\to0$ but, in doing so, the variance of $\\hat{v}_{i,n}$ diverges, leading to a bias-variance trade-off that is difficult to tame. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Despite these added difficulties, we show below that (FTXL) maintains its superlinear convergence rate even with bandit, payoff-based feedback: ", "page_idx": 9}, {"type": "text", "text": "Theorem 4. Let $x^{*}$ be a strict Nash equilibrium of $\\Gamma$ , fix some confidence level $\\delta\\:>\\:0_{\\!\\!\\!\\!\\!_{\\!}}$ , and let $x_{n}=Q(y_{n})$ be the sequence of play generated by (FTXL) with bandit feedback of the form (11c), an IWE exploration parameter $\\dot{\\varepsilon}_{n}\\propto\\ddot{1}/n^{\\ell_{\\varepsilon}}$ for some $\\ell_{\\varepsilon}\\in(0,1/2)$ , and a sufficiently small step-size $\\gamma>0$ . Then there exists a neighborhood U of $x^{*}$ in $\\mathcal{X}$ such that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(x_{n}\\to x^{*}\\mathrm{~as~}n\\to\\infty)\\geq1-\\delta\\qquad i f x_{1}\\in\\mathcal{U}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In particular, $i f$ (FTXL) is run with logit best responses (that is, $Q\\leftarrow\\Lambda.$ ), there exist positive constants $C,c>0$ as in Theorem 2 such that on the event $\\{x_{n}\\to x^{*}$ as $n\\to\\infty$ } ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\|x_{T}-x^{*}\\|_{\\infty}\\leq\\exp\\biggl(C-c\\gamma^{2}\\frac{T(T-1)}{2}+\\frac{5}{9}c\\gamma^{9/5}T^{9/5}\\biggr)=\\exp\\bigl(-\\Theta(T^{2})\\bigr)\\,.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Theorem 4 (which we prove in Appendix D shows that, despite the degradation of the subleading term, (FTXL) retains its superlinear convergence rate even with bandit, payoff-based feedback (for a numerical demonstration, see Fig. 1 above). We find this feature of (FTXL) particularly important as it shows that the algorithm remains exceptionally robust in the face of randomness and uncertainty, even as we move toward increasingly information-starved environments \u2013 from full information, to realization-based observations and, ultimately, to bandit feedback. This has important ramifications from an operational standpoint, which we intend to examine further in future work. ", "page_idx": 9}, {"type": "text", "text": "4.5. Numerical Experiments. Finally, we provide numerical simulations to validate the performance of (FTXL). To this end, we consider two game paradigms, (i) a 2-player zero-sum game, and (ii) a congestion game. ", "page_idx": 9}, {"type": "text", "text": "Zero-sum Game. First, we consider a 2-player zero-sum game with actions $\\{\\alpha_{1},\\alpha_{2},\\alpha_{3}\\}$ and $\\{\\beta_{1},\\beta_{2},\\beta_{3}\\}$ , and payoff matrix ", "page_idx": 9}, {"type": "equation", "text": "$$\nP=\\left(\\!\\!{\\begin{array}{c c c}{(2,-2)}&{(1,-1)}&{(2,-2)}\\\\ {(-2,2)}&{(-1,1)}&{(-2,2)}\\\\ {(-2,2)}&{(-1,1)}&{(-2,2)}\\end{array}}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "Here, the rows of $P$ correspond to the actions of player $A$ and the columns to the actions of player $B$ , while the first item of each entry of $P$ corresponds to the payoff of $A$ , and the second one to the payoff of $B$ . Clearly, the action profile $(\\alpha_{1},\\beta_{2})$ is a strict Nash equilibrium. ", "page_idx": 9}, {"type": "text", "text": "Congestion Game. As a second example, we consider a congestion game with $N=100$ and 2 roads, $r_{1}$ and $r_{2}$ , with costs $c_{1}=1.1$ and $c_{2}=d/N$ where $d$ is the number of drivers on $r_{2}$ . In words, $r_{1}$ has a fixed delay equal to 1.1, while $r_{2}$ has a delay proportional to the drivers using it. Note, that the strategy profile where all players are using $r_{2}$ is a strict Nash equilibrium. ", "page_idx": 9}, {"type": "text", "text": "In Fig. 1, we assess the convergence of (FTXL) with logit best responses, under realization-based and bandit feedback, and compare it to the standard (EW) with the same level of information. The figures verify that (FTXL) outperforms (EW) regarding the convergence to a strict Nash equilibrium both for the realization-based and the bandit feedback, as expected from the theoretical findings. Specifically, they validate the faster convergence rate of (FTXL) compared to that of the (EW) algorithm. Moreover, we observe that both algorithms perform worse under bandit feedback than under realization-based feedback. This behavior is expected as less information becomes available. More details can be found in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003), the \u201cInvestissements d\u2019avenir program\u201d (ANR-15-IDEX-02), the LabEx PERSYVAL (ANR-11-LABX-0025-01), MIAI $@$ Grenoble Alpes (ANR-19-P3IA-0003), the project IRGA2024-SPICE-G7H-IRG24E90. PM is also with the Archimedes Research Unit \u2013 Athena RC \u2013 Department of Mathematics, National & Kapodistrian University of Athens, and his research was partially funded by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, 1995.   \n[2] Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183\u2013202, March 2009.   \n[3] Bena\u00efm, M. Dynamics of stochastic approximation algorithms. In Az\u00e9ma, J., \u00c9mery, M., Ledoux, M., and Yor, M. (eds.), S\u00e9minaire de Probabilit\u00e9s XXXIII, volume 1709 of Lecture Notes in Mathematics, pp. 1\u201368. Springer Berlin Heidelberg, 1999.   \n[4] Bubeck, S. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231\u2013358, 2015.   \n[5] Bubeck, S. and Cesa-Bianchi, N. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1\u2013122, 2012.   \n[6] Daskalakis, C., Goldberg, P. W., and Papadimitriou, C. H. The complexity of computing a Nash equilibrium. Communications of the ACM, 52(2):89\u201397, 2009.   \n[7] Erev, I. and Roth, A. E. Predicting how people play games: Reinforcement learning in experimental games with unique, mixed strategy equilibria. American Economic Review, 88:848\u2013881, 1998.   \n[8] Flokas, L., Vlatakis-Gkaragkounis, E. V., Lianeas, T., Mertikopoulos, P., and Piliouras, G. No-regret learning and mixed Nash equilibria: They do not mix. In NeurIPS \u201920: Proceedings of the 34th International Conference on Neural Information Processing Systems, 2020.   \n[9] Fl\u00e5m, S. D. and Morgan, J. Newtonian mechanics and Nash play. International Game Theory Review, 6 (2):181\u2013194, 2004.   \n[10] Gao, B. and Pavel, L. On passivity and reinforcement learning in finite games. In 2018 IEEE Conference on Decision and Control (CDC), pp. 340\u2013345, 2018. doi: 10.1109/CDC.2018.8619157.   \n[11] Gao, B. and Pavel, L. On passivity, reinforcement learning, and higher order learning in multiagent finite games. IEEE Transactions on Automatic Control, 66(1):121\u2013136, 2021. doi: 10.1109/TAC.2020.2978037.   \n[12] Gao, B. and Pavel, L. Second-order mirror descent: exact convergence beyond strictly stable equilibria in concave games. In 2021 60th IEEE Conference on Decision and Control (CDC), pp. 948\u2013953, 2021. doi: 10.1109/CDC45484.2021.9683223.   \n[13] Gao, B. and Pavel, L. Second-order mirror descent: Convergence in games beyond averaging and discounting. IEEE Transactions on Automatic Control, 69(4):2143\u20132157, 2024. doi: 10.1109/TAC.2023. 3291953.   \n[14] Giannou, A., Vlatakis-Gkaragkounis, E. V., and Mertikopoulos, P. Survival of the strictest: Stable and unstable equilibria under regularized learning with partial information. In COLT \u201921: Proceedings of the 34th Annual Conference on Learning Theory, 2021.   \n[15] Giannou, A., Vlatakis-Gkaragkounis, E. V., and Mertikopoulos, P. The convergence rate of regularized learning in games: From bandits and uncertainty to optimism and beyond. In NeurIPS \u201921: Proceedings of the 35th International Conference on Neural Information Processing Systems, 2021.   \n[16] Hall, P. and Heyde, C. C. Martingale Limit Theory and Its Application. Probability and Mathematical Statistics. Academic Press, New York, 1980.   \n[17] Hart, S. and Mas-Colell, A. Uncoupled dynamics do not lead to Nash equilibrium. American Economic Review, 93(5):1830\u20131836, 2003.   \n[18] Hirsch, M. W., Smale, S., and Devaney, R. L. Differential Equations, Dynamical Systems, and an Introduction to Chaos. Elsevier, London, UK, 2 edition, 2004.   \n[19] Hofbauer, J. and Sigmund, K. Evolutionary game dynamics. Bulletin of the American Mathematical Society, 40(4), July 2003.   \n[20] Laraki, R. and Mertikopoulos, P. Higher order game dynamics. Journal of Economic Theory, 148(6): 2666\u20132695, November 2013.   \n[21] Laraki, R. and Mertikopoulos, P. Inertial game dynamics and applications to constrained optimization. SIAM Journal on Control and Optimization, 53(5):3141\u20133170, October 2015.   \n[22] Lattimore, T. and Szepesv\u00e1ri, C. Bandit Algorithms. Cambridge University Press, Cambridge, UK, 2020.   \n[23] Littlestone, N. and Warmuth, M. K. The weighted majority algorithm. Information and Computation, 108 (2):212\u2013261, 1994.   \n[24] Mabrok, M. A. and Shamma, J. S. Passivity analysis of higher order evolutionary dynamics and population games. In 2016 IEEE 55th Conference on Decision and Control (CDC), pp. 6129\u20136134, 2016. doi: 10.1109/CDC.2016.7799211.   \n[25] Mertikopoulos, P. and Sandholm, W. H. Learning in games via reinforcement and regularization. Mathematics of Operations Research, 41(4):1297\u20131324, November 2016.   \n[26] Mertikopoulos, P. and Zhou, Z. Learning in games with continuous action sets and unknown payoff functions. Mathematical Programming, 173(1-2):465\u2013507, January 2019.   \n[27] Nemirovski, A. S. and Yudin, D. B. Problem Complexity and Method Efficiency in Optimization. Wiley, New York, NY, 1983.   \n[28] Nesterov, Y. A method for unconstrained convex minimization problem with the rate of convergence $\\mathcal{O}(1/k^{2})$ . Proceedings of the USSR Academy of Sciences, 269(543-547), 1983.   \n[29] Shub, M. Global Stability of Dynamical Systems. Springer-Verlag, Berlin, 1987.   \n[30] Su, W., Boyd, S. P., and Cand\u00e8s, E. J. A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights. In NIPS \u201914: Proceedings of the 28th International Conference on Neural Information Processing Systems, pp. 2510\u20132518, 2014.   \n[31] Su, W., Boyd, S., and Cand\u00e8s, E. J. A differential equation for modeling Nesterov\u2019s accelerated gradient method: Theory and insights. Journal of Machine Learning Research, 17(153):1\u201343, 2016.   \n[32] Taylor, P. D. and Jonker, L. B. Evolutionary stable strategies and game dynamics. Mathematical Biosciences, 40(1-2), 1978.   \n[33] Toonsi, S. and Shamma, J. Higher-order uncoupled dynamics do not lead to nash equilibrium - except when they do. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 30267\u201330285. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 605e02ae04cba1ebf6a08206299e76b9-Paper-Conference.pdf.   \n[34] Vovk, V. G. Aggregating strategies. In COLT \u201990: Proceedings of the 3rd Workshop on Computational Learning Theory, pp. 371\u2013383, 1990. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Appendix A, we discuss how our findings can be extended to general regularizers. Subsequently, Appendix B and Appendix C contain the technical proofs for the continuous and discrete time algorithms, respectively. Following this, Appendix D provides the convergence results of (FTXL) under partial information, specifically under realization-based and bandit feedback. We conclude this section with Appendix E, which presents the details of the numerical experiments. ", "page_idx": 12}, {"type": "text", "text": "A Auxiliary results for general regularizers ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this appendix, we briefly discuss how to obtain the convergence of (FTXL) for mirror maps $Q$ beyond the logit map $\\Lambda$ . Namely, we consider regularizers that are decomposable, i.e., $h_{i}(x_{i})\\,=$ $\\textstyle\\sum_{\\alpha_{i}\\in{\\mathcal{A}}_{i}}\\theta_{i}(x_{\\alpha_{i}})$ such that $\\theta_{i}\\colon[0,1]\\to\\ensuremath{\\mathbb{R}}$ is continuous on [0, 1], twice differentiable on $(0,1]$ and strongly convex with $\\theta_{i}^{\\prime}(0^{+})=-\\infty$ . ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. Suppose that $x_{n}=Q(y_{n})$ and for all $\\alpha\\in\\mathcal{A},\\alpha\\neq\\alpha^{*}$ , it holds that $y_{\\alpha,n}-y_{\\alpha^{*},n}\\to-\\infty$ as $n\\to\\infty$ . Then, $x_{n}$ converges to $x^{*}$ , where $x^{*}$ is a point mass at $\\alpha^{*}$ . Moreover, it holds that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|x_{n}-x^{*}\\|_{\\infty}\\leq\\sum_{\\alpha\\neq\\alpha^{*}}(\\theta^{\\prime})^{-1}\\big(\\theta^{\\prime}(1)+y_{\\alpha,n}-y_{\\alpha^{*},n}\\big)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. First, note that for $x=Q(y)$ , we have that $x$ is the solution of the following optimization problem ", "page_idx": 12}, {"type": "equation", "text": "$$\nQ(y)=\\arg\\operatorname*{max}\\left\\{\\sum_{\\alpha\\in{\\mathcal{A}}}y_{\\alpha}x_{\\alpha}-h(x):\\sum_{\\alpha\\in{\\mathcal{A}}}x_{\\alpha}=1{\\mathrm{~and~}}\\forall\\alpha\\in{\\mathcal{A}}:x_{\\alpha}\\geq0\\right\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By solving the Karush\u2013Kuhn\u2013Tucker (KKT) conditions to this optimization problem we readily get that $x$ lies in the interior of $\\mathcal{X}$ , since $\\theta(0^{+})=-\\infty$ , and thus we obtain that at the solution, it holds $y_{\\alpha}=\\theta^{\\prime}(x_{\\alpha})+\\lambda$ for $\\lambda\\in\\mathbb R$ . Therefore, we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\ny_{\\alpha,n}-y_{\\alpha^{*},n}=\\theta^{\\prime}(x_{\\alpha,n})-\\theta^{\\prime}(x_{\\alpha^{*},n})\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "or equivalently: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\theta^{\\prime}(x_{\\alpha,n})=\\theta^{\\prime}(x_{\\alpha^{*},n})+y_{\\alpha,n}-y_{\\alpha^{*},n}\\leq\\theta^{\\prime}(1)+y_{\\alpha,n}-y_{\\alpha^{*},n}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now, assume that there exists $\\alpha\\in{\\mathcal{A}}$ such that $x_{\\alpha,n}$ does not converge to 0, i.e., lim $\\operatorname*{sup}_{n\\to\\infty}x_{\\alpha,n}>\\varepsilon$ for some $\\varepsilon>0$ . Then, since $\\theta$ is strongly convex, $\\theta^{\\prime}$ is strictly increasing, and thus $\\theta^{\\prime}(x_{\\alpha,n})\\geq\\theta^{\\prime}(\\varepsilon)$ infinitely often. However, by taking $n\\to\\infty$ in (A.3), it implies that $\\theta^{\\prime}(x_{\\alpha,n})\\rightarrow-\\infty$ , which is a contradiction. Therefore, we conclude that for all $\\alpha\\neq\\alpha^{*}$ , it holds that $\\textstyle\\operatorname*{lim}_{n\\to\\infty}x_{\\alpha,n}=0$ , and the convergence result follows. ", "page_idx": 12}, {"type": "text", "text": "Finally, note that since $\\theta^{\\prime}$ is strictly increasing, it is invertible and its inverse is strictly increasing as well. Thus, for each $\\alpha\\neq\\alpha^{*}$ we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\nx_{\\alpha,n}\\leq(\\theta^{\\prime})^{-1}\\big(\\theta^{\\prime}(1)+y_{\\alpha,n}-y_{\\alpha^{*},n}\\big)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|x_{n}-x^{*}\\|_{\\infty}=1-x_{\\alpha^{*},n}=\\sum_{\\alpha\\neq\\alpha^{*}}x_{\\alpha,n}\\leq\\,\\sum_{\\alpha\\neq\\alpha^{*}}(\\theta^{\\prime})^{-1}\\big(\\theta^{\\prime}(1)+y_{\\alpha,n}-y_{\\alpha^{*},n}\\big)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and the proof is complete. ", "page_idx": 12}, {"type": "text", "text": "B Proofs for Continuous Time Algorithms ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this appendix, we provide the proof of Theorem 1 and discuss the convergence of (FTXL-D) under a non-vanishing friction coefficient \u2013 $r\\dot{y}$ instead of $(r/t){\\dot{y}}$ . First, we provide a lemma that is necessary for our analysis. ", "page_idx": 12}, {"type": "text", "text": "Lemma B.1. Let $x^{*}=(\\alpha_{1}^{*},\\ldots,\\alpha_{N}^{*})\\in\\mathcal{X}$ be a strict Nash equilibrium of $\\Gamma$ , and let \ud835\udc51denote the minimum payoff difference at equilibrium, i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\nd:=\\operatorname*{min}_{i\\in\\mathcal{N}\\beta_{i}\\notin\\operatorname{supp}(x_{i}^{*})}\\left[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})\\right].\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, for any $c\\in(0,d)$ , there exists $M>0$ such that if $\\dot{y}_{i\\alpha_{i}^{*}}-y_{i\\alpha_{i}}>M$ for all $\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i}$ and $i\\in\\mathcal{N}$ , then ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{i\\,\\alpha_{i}^{*}}(Q(y))-v_{i\\,\\alpha_{i}}(Q(y))>c\\ \\ \\,f o r\\,a l l\\,\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i},a n d\\,i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Since $x^{*}$ is a strict Nash equilibrium, the minimum payoff difference $d$ at $x^{*}$ is bounded away from zero. Then, by continuity of the function $x\\mapsto v(x)$ , there exists a neighborhood $\\mathcal{U}_{*}$ of $x^{*}$ such that for any $x\\in\\mathcal{U}_{*}$ , it holds ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{i\\,\\alpha_{i}^{*}}(x)-v_{i\\,\\alpha_{i}}(x)>c\\quad\\mathrm{for\\,\\,all\\,}\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i},\\mathrm{and}\\,i\\in\\mathcal{N}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, by [15, Lemma C.2.], there exists $M>0$ , such that $Q(y)\\in\\mathcal{U}_{*}$ for all $y\\in\\mathcal{V}^{*}$ with ", "page_idx": 13}, {"type": "equation", "text": "$$\ny_{i\\,\\alpha_{i}^{*}}-y_{i\\,\\alpha_{i}}>M\\quad{\\mathrm{for~all~}}\\alpha_{i}\\neq\\alpha_{i}^{*}\\in{\\mathcal{A}}_{i},{\\mathrm{and~}}i\\in{\\mathcal{N}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, we readily get that if $y\\in\\mathcal{V}^{*}$ satisfies the above relation, then ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{i\\alpha_{i}^{*}}(Q(y))-v_{i\\alpha_{i}}(Q(y))>c\\quad\\mathrm{for~all~}\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i},\\mathrm{and~}i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Now, we are ready for the proof of Theorem 1. ", "page_idx": 13}, {"type": "text", "text": "Theorem 1. Let $x^{*}$ be a strict Nash equilibrium of $\\Gamma_{;}$ , and let $x(t)=Q(y(t))$ be a solution orbit of (FTXL-D). If $\\chi(0)$ is sufficiently close to $x^{*}$ , then $x(t)$ converges to $x^{*}$ ; in particular, $i f$ (FTXL-D) is run with logit best responses (that is, $Q\\leftarrow\\Lambda$ ), we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|x(t)-x^{*}\\|_{\\infty}\\leq\\exp\\biggl(C-\\frac{c t^{2}}{2(r+1)}\\biggr)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $C>0$ is a constant that depends only on the initialization of (FTXL-D) and ", "page_idx": 13}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\operatorname{supp}(x_{i}^{*})}[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})]>0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "is the minimum payoff difference at equilibrium. ", "page_idx": 13}, {"type": "text", "text": "Proof. First of all, since $x^{*}$ is a strict Nash equilibrium, by Lemma B.1 for ", "page_idx": 13}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\operatorname{supp}(x_{i}^{*})}\\left[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})\\right]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "there exists $M>0$ such that if $y_{i\\,\\alpha_{i}^{*}}-y_{i\\,\\alpha_{i}}>M$ for all $\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i}$ and $i\\in\\mathcal{N}$ , then ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{i}\\alpha_{i}^{*}(Q(y))-v_{i\\alpha_{i}}(Q(y))>c\\quad\\mathrm{for~all~}\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i},\\mathrm{and~}i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From now on, for notational convenience, we focus on player $i\\in\\mathcal{N}$ and drop the player-specific indices altogether. Then, for $\\alpha\\neq\\alpha^{*}\\in\\mathcal{A}$ , we let $z_{\\alpha}(t):=y_{\\alpha}(t)-y_{\\alpha}^{*}(t)$ , which evolves as: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\ddot{z}(t)=v_{\\alpha}(x(t))-v_{\\alpha^{*}}(x(t))-\\frac{r}{t}\\dot{z}_{\\alpha}(t)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let $y(0)$ such that $z_{\\alpha}(0)=-M-\\varepsilon$ , for all $\\alpha\\neq\\alpha^{*}\\in\\mathcal{A}$ , where $\\varepsilon>0$ small. We will, first, show that $z(t)<-M$ for all $t\\geq0$ . For the sake of contradiction, and denoting $T_{0}:=\\operatorname*{inf}\\{t\\geq0:z(t)\\geq-M\\}$ , suppose that $T_{0}<\\infty$ . Then, we readily get that for all $t<T_{0}$ , it holds ", "page_idx": 13}, {"type": "equation", "text": "$$\nv_{\\alpha}(x(t))-v_{\\alpha^{*}}(x(t))<-c\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and therefore, for all $t\\leq T_{0}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\ddot{z}_{\\alpha}(t)t^{r}+r t^{r-1}\\dot{z}_{\\alpha}(t)=t^{r}\\left[v_{\\alpha}(x)-v_{\\alpha^{*}}(x)\\right]\\leq-c t^{r}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which can be rewritten as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{d}{d t}(\\dot{z}_{\\alpha}(t)t^{r})\\leq-c t^{r}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Integrating over $t<T_{0}$ , we obtain $\\dot{z}_{\\alpha}(t)t^{r}\\leq-c t^{r+1}/(r+1)$ , which readily implies: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{z_{\\alpha}(t)\\leq z_{\\alpha}(0)-\\displaystyle\\frac{c}{2(r+1)}t^{\\frac{}{2}}}}\\\\ {{<-M-\\displaystyle\\frac{c}{2(r+1)}t^{\\frac{}{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By sending $t\\,\\rightarrow\\,T_{0}$ , we arrive at a contradiction. Therefore $z_{\\alpha}(t)\\,<\\,-M$ for all $t\\,\\geq\\,0$ , and the previous equation implies that for all $t\\geq0$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\nz_{\\alpha}(t)\\leq z_{\\alpha}(0)-{\\frac{c}{2(r+1)}}t^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and invoking Lemma A.1, we get the convergence result. Finally, translating the score-differences to the primal space $\\mathcal{X}$ , we get: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left\\|{x(t)-x^{*}}\\right\\|_{\\infty}=\\operatorname*{max}_{i\\in\\mathcal{N}}\\Big\\{1-x_{i\\alpha_{i}^{*}}(t)\\Big\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the case of logit best responses, i.e., when $Q\\gets\\Lambda$ , and assuming that the maximum above is attained for player $i\\in\\mathcal{N}$ , we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\boldsymbol{x}(t)-\\boldsymbol{x}^{*}\\|_{\\infty}=\\frac{\\sum_{\\alpha_{i}\\neq\\alpha_{i}^{*}}\\exp(z_{\\alpha_{i}}(t))}{1+\\sum_{\\alpha_{i}\\neq\\alpha_{i}^{*}}\\exp(z_{\\alpha_{i}}(t))}}}\\\\ &{\\leq\\frac{\\sum_{\\alpha_{i}\\neq\\alpha_{i}^{*}}\\exp(z_{\\alpha_{i}}(t))}{\\alpha_{i}\\neq\\alpha_{i}^{*}}}\\\\ &{\\leq|\\boldsymbol{A}_{i}|\\exp\\biggl(z_{\\alpha_{i}}(0)-\\frac{c}{2(r+1)}t^{2}\\biggr)}\\\\ &{\\leq\\exp\\biggl(C-\\frac{c}{2(r+1)}t^{2}\\biggr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $C=\\log\\lvert\\mathcal{A}_{i}\\rvert+z_{\\alpha_{i}}(0)$ . ", "page_idx": 14}, {"type": "text", "text": "Now, moving to the case where we use a constant friction coefficient $-\\,r{\\dot{y}}$ instead of $(r/t){\\dot{y}}$ , (FTXL-D) becomes: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{d^{2}y}{d t^{2}}}=v(Q(y))-r{\\frac{d y}{d t}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Under, (B.18), we obtain the following convergence result. ", "page_idx": 14}, {"type": "text", "text": "Theorem B.1. Let $x^{*}$ be a strict Nash equilibrium of $\\Gamma_{\\mathrm{i}}$ , and let $x(t)=Q(y(t))$ be a solution orbit of (B.18). If $x(0)$ is sufficiently close to $x^{*}$ , then $x(t)$ converges to $x^{*}$ ; in particular, $i f$ (B.18) is run with logit best responses (that is, $Q\\leftarrow\\Lambda$ ), we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|x(t)-x^{*}\\|_{\\infty}\\le\\exp\\Bigl(C-\\frac{c}{r}t-\\frac{c}{r^{2}}e^{-r t}+\\frac{c}{r^{2}}\\Bigr)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $C>0$ is a constant that depends on the initialization of (B.18) and ", "page_idx": 14}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\operatorname{supp}(x_{i}^{*})}[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})]>0\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is the minimum payoff difference at equilibrium. ", "page_idx": 14}, {"type": "text", "text": "Proof. The initial steps of proof of Theorem B.1 are similar to the proof of Theorem 1, which we include for the sake of completeness. ", "page_idx": 14}, {"type": "text", "text": "Specifically, by Lemma B.1 there exists $M>0$ such that if $y_{i\\,\\alpha_{i}^{*}}-y_{i\\,\\alpha_{i}}>M$ for all $\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i}$ and $i\\in\\mathcal{N}$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\nv_{i}\\alpha_{i}^{*}(Q(y))-v_{i\\alpha_{i}}(Q(y))>c\\quad\\mathrm{for~all~}\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i},\\mathrm{and~}i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, for notational convenience, we focus on player $i\\in\\mathcal{N}$ and drop the player-specific indices altogether. Then, for $\\alpha\\neq\\alpha^{*}\\in\\mathcal{A}$ , we let $z_{\\alpha}(t):=y_{\\alpha}(t)-y_{\\alpha}^{*}(t)$ , which evolves as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ddot{z}(t)=v_{\\alpha}(x(t))-v_{\\alpha^{*}}(x(t))-r\\dot{z}_{\\alpha}(t)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $y(0)$ such that $z_{\\alpha}(0)=-M-\\varepsilon$ , for all $\\alpha\\neq\\alpha^{*}\\in\\mathcal{A}$ , where $\\varepsilon\\,>\\,0$ small. As in the proof of Theorem 1, we will, first, show that $z(t)\\,<\\,-M$ for all $t\\,\\geq\\,0$ . For the sake of contradiction, and denoting $T_{0}:=\\operatorname*{inf}\\{t\\geq0:z(t)\\geq-M\\}$ , suppose that $T_{0}<\\infty$ . Then, we readily get that for all $t<T_{0}$ , it holds ", "page_idx": 15}, {"type": "equation", "text": "$$\nv_{\\alpha}(x(t))-v_{\\alpha^{\\ast}}(x(t))<-c\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and therefore, for all $t\\leq T_{0}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ddot{z}_{\\alpha}(t)e^{r t}+r e^{r t}\\dot{z}_{\\alpha}(t)=e^{r t}\\left[v_{\\alpha}(x)-v_{\\alpha^{*}}(x)\\right]\\leq-c e^{r t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which can be rewritten as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\big(\\dot{z}_{\\alpha}(t)e^{r t}\\big)\\leq-c e^{r t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Integrating over $t<T_{0}$ , and using that $\\dot{z}_{\\alpha}(0)=0$ , we obtain $\\dot{z}_{\\alpha}(t)\\leq-c/r+c e^{-r t}/r$ , which implies: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{\\alpha}(t)\\leq z_{\\alpha}(0)-\\displaystyle\\frac{c}{r}t-\\displaystyle\\frac{c}{r^{2}}e^{-r t}+\\displaystyle\\frac{c}{r^{2}}}\\\\ &{\\qquad=z_{\\alpha}(0)-\\displaystyle\\frac{c}{r^{2}}\\big(r t+e^{-r t}-1\\big)}\\\\ &{\\qquad<z_{\\alpha}(0)}\\\\ &{<-M}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used that $x+e^{-x}-1\\geq0$ for all $x\\in\\mathbb{R}$ with equality if and only if $x=0$ . By sending $t\\rightarrow T_{0}$ , we arrive at a contradiction. Therefore $z_{\\alpha}(t)<-M$ for all $t\\geq0$ , and the previous equation implies that for all $t\\geq0$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nz_{\\alpha}(t)\\le z_{\\alpha}(0)-\\frac{c}{r}t-\\frac{c}{r^{2}}e^{-r t}+\\frac{c}{r^{2}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and invoking Lemma A.1 for $\\theta(x)=x\\log x$ , we get the convergence result. ", "page_idx": 15}, {"type": "text", "text": "C Proofs for discrete-time algorithms with full information ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the results for the (FTXL) algorithm with full-information feedback. First, we discuss the rates obtained by the direct discretization of (FTXL-D) with both vanishing and nonvanishing friction, and then provide the proof of Theorem 2, our main result, for the full-information case. ", "page_idx": 15}, {"type": "text", "text": "C.1. FTXL with vanishing friction. First, we provide the rate of convergence for the discrete version of (FTXL-D) with vanishing friction: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{p_{i,n+1}=p_{i,n}\\displaystyle\\left(1-\\frac{\\gamma r}{n}\\right)+\\gamma\\hat{v}_{i,n}}\\\\ {y_{i,n+1}=y_{i,n}+\\gamma p_{i,n+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To streamline our presentation, we consider the setup of Example 3.1 that provides a lower bound for the algorithm. ", "page_idx": 15}, {"type": "text", "text": "Proposition C.1. Consider the single-player game $\\Gamma$ with actions A and B such that $u(\\mathbf{A})-u(\\mathbf{B})=1$ of Example 3.1, and let $x_{n}=\\Lambda(y_{n})$ be the sequence of play generated by (C.1). Then, denoting by $x^{*}=(1,0)$ the strict Nash equilibrium, we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|x_{T}-x^{*}\\|_{\\infty}\\sim\\exp\\biggl(C-\\frac{\\gamma^{2}T^{2}}{2(\\gamma r+1)}\\biggr)\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $C>0$ is a constant that depends only on the initialization of the algorithm. ", "page_idx": 15}, {"type": "text", "text": "Proof. We first define the score-difference ", "page_idx": 16}, {"type": "equation", "text": "$$\nw_{n}:=p_{\\mathtt{B},n}-p_{\\mathtt{A},n}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with initial condition $w_{1}=0$ . Then, unfolding according to the sequence of play, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle w_{n+1}=w_{n}\\Big(1-\\frac{\\gamma r}{n}\\Big)+\\gamma(u({\\bf B})-u({\\bf A}))}}\\\\ {{\\displaystyle\\qquad=w_{n}\\Big(1-\\frac{\\gamma r}{n}\\Big)-\\gamma}}\\\\ {{\\displaystyle\\qquad=-\\gamma\\sum_{k=1}^{n-1}\\prod_{\\ell=0}^{k-1}\\Bigl(1-\\frac{\\gamma r}{n-\\ell}\\Bigr)-\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We next define for $n\\in\\mathbb{N}$ the difference $z_{n}:=y_{\\mathtt{B},n}-y_{\\mathtt{A},n}$ . Thus, unfolding it, we obtain: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle z_{n+1}=z_{n}+\\gamma w_{n+1}}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}=z_{n}-\\gamma^{2}\\left(1+\\sum_{k=1}^{n-1}\\prod_{\\ell=0}^{k-1}\\left(1-\\frac{\\gamma r}{n-\\ell}\\right)\\right)}}\\\\ {{\\displaystyle}}\\\\ {{\\displaystyle}=z_{1}-\\gamma^{2}\\sum_{m=1}^{n}\\left(1+\\sum_{k=1}^{m-1}\\prod_{\\ell=0}^{k-1}\\left(1-\\frac{\\gamma r}{m-\\ell}\\right)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, using Lemma C.1, which we provide after this proof, we obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle z_{n+1}=z_{1}-\\gamma^{2}\\sum_{m=1}^{n}\\left(1+\\frac{m-\\gamma r}{1+\\gamma r}-\\frac{1}{1+\\gamma r}\\prod_{\\ell=1}^{m}\\left(1-\\frac{\\gamma r}{\\ell}\\right)\\right)}\\\\ {\\displaystyle\\qquad=z_{1}-\\gamma^{2}\\frac{n(n+1)}{2(1+\\gamma r)}-\\gamma^{2}n\\left(1-\\frac{\\gamma r}{1+\\gamma r}\\right)+\\frac{\\gamma^{2}}{1+\\gamma r}\\sum_{m=1}^{n}\\prod_{\\ell=1}^{m}\\left(1-\\frac{\\gamma r}{\\ell}\\right)}\\\\ {\\displaystyle\\qquad=z_{1}-\\frac{\\gamma^{2}n^{2}}{2(1+\\gamma r)}+\\Theta(n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and invoking Lemma A.1 for $\\theta(x)=x\\log x$ , we get the result. ", "page_idx": 16}, {"type": "text", "text": "The following lemma is a necessary tool for obtaining the exact convergence rate in Proposition C.2. Lemma C.1. For any $m\\in\\mathbb{N}$ and $a>0$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{m-1}\\prod_{\\ell=0}^{k-1}(1-\\frac{a}{m-\\ell})=\\frac{m-a}{1+a}-\\frac{1}{1+a}\\prod_{\\ell=1}^{m}\\Bigl(1-\\frac{a}{\\ell}\\Bigr)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. First, by expanding the inner product, we can rewrite the expression as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{k=1}^{m-1}\\prod_{\\ell=0}^{k-1}(1-\\frac{a}{m-\\ell})=\\displaystyle\\sum_{k=1}^{m-1}\\prod_{\\ell=0}^{k-1}(\\frac{m-\\ell-a}{m-\\ell})}}\\\\ {{\\displaystyle\\sum_{k=1}^{m-1}\\frac{(m-a)\\dots(m-k+1-a)}{m\\cdot\\dots(m-k+1)}}}\\\\ {{\\displaystyle=\\sum_{k=1}^{m-1}\\frac{(m-a)!(m-k)!}{(m-k-a)!m!}}}\\\\ {{\\displaystyle=\\frac{(m-a)!}{m!}\\sum_{k=1}^{m-1}\\frac{(m-k)!}{(m-k-a)!}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where with a slight abuse of notation we use the factorial notation $(m-a)!$ to denote the Gamma function evaluated at $m-a+1$ , i.e., $\\Gamma(m-a+1)$ . ", "page_idx": 16}, {"type": "text", "text": "Now, defining the quantity ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{m}:=\\frac{(m-a)!}{m!}\\sum_{k=1}^{m}\\frac{(m-k)!}{(m-k-a)!}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "the difference of two consecutive terms evolves as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{F_{n+1}-F_{n+1}}&{=\\frac{(n+1-a)!)^{\\frac{n+1}{2}}}{n+1}\\frac{(n+1-a)!}{\\Gamma}\\frac{(n+1-b)!}{(n+1-a)!}\\frac{(-(n-a)!)^{\\frac{n}{2}}}{\\Gamma\\left(\\frac{n+1-a}{n}\\right)!}\\frac{(n-b)!}{\\Gamma\\left(\\frac{n+1-a}{n}\\right)!}}\\\\ &{=\\frac{n+1-a}{n+1}\\frac{(n+1-a)!}{\\Gamma\\left(\\frac{n+1-a}{n}\\right)!}\\frac{(n+1-a)!}{\\Gamma\\left(\\frac{n+1-a}{n}\\right)!}\\frac{(n-a)!}{\\Gamma}\\frac{(n-a)!}{\\Gamma\\left(\\frac{n+1-a}{n}\\right)!}}\\\\ &{\\quad-\\frac{n+1-a}{n+1}\\frac{(n+1-a)!}{(n+1)!}\\frac{(n+1-a)!}{\\Gamma\\left(\\frac{n+1-a}{n}\\right)!}\\frac{(n-a)!}{\\Gamma}\\frac{(n-a)!}{\\Gamma+\\frac{n+1-a}{n}!}}\\\\ &{=\\frac{n+1-a}{n+1}\\frac{(n+1-a)!}{\\Gamma+\\frac{n+1}{n}}\\frac{(n+1-a)!}{\\Gamma}\\frac{(n+1-b)!}{(n+1-a)!}\\cdots(n+1)!\\frac{(n-a)!}{\\Gamma+\\frac{n+1-a}{n}!}\\frac{(n-a)!}{\\Gamma+\\frac{n+1-a}{n}!}}\\\\ &{\\quad-\\frac{n+1-a}{n+1}\\frac{(n+1-a)!(n+1-a)!(n+1-b)!(n+a)!(n-a)!}{(n+1)!(n+1-a)!}}\\\\ &{=\\frac{n+1-a}{n+1}\\frac{\\Gamma}{\\Gamma+\\frac{n+1}{n}}\\frac{(n-a)!(n-a)!(n+1-a)!(n+1-a-n-1)!}{\\Gamma(\\frac{n+1-a}{n})!(n+1-a-n-1)!}}\\\\ &{=\\frac{n+1-a}{n+1}\\frac{\\Gamma}{\\Gamma+\\frac{n+1}{n}}\\frac{(n-a)!(n-a)!(n+1-a)!}{\\Gamma(\\frac{n+1-a}{n})!(n+1-a-n-1)!}}\\\\ &{\\quad-\\frac{n+1-a}{n+1}\\frac{(n-a)!(n\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we readily obtain the recurrence relation ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\frac{m+1}{m+1-a}}F_{m+1}=F_{m}+1\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We continue the proof by induction. To this end, we will show that ", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{m}=\\frac{m-a}{1+a}+\\frac{a}{1+a}\\prod_{\\ell=1}^{m}\\frac{\\ell-a}{\\ell}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the base case, note that ", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{1}=(1-a)=\\frac{1-a}{1+a}+\\frac{a}{1+a}(1-a)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For the inductive step, suppose that (C.28) holds for $m\\in\\mathbb{N}$ . Then, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{m+1}{m+1-a}F_{m+1}=\\frac{m-a}{1+a}+\\frac{a}{1+a}\\prod_{\\ell=1}^{m}\\left(\\frac{\\ell-a}{\\ell}\\right)+1}}\\\\ {{\\displaystyle=\\frac{m+1}{1+a}+\\frac{a}{1+a}\\prod_{\\ell=1}^{m}\\frac{\\ell-a}{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which implies the inductive step ", "page_idx": 17}, {"type": "equation", "text": "$$\nF_{m+1}=\\frac{m+1-a}{1+a}+\\frac{a}{1+a}\\prod_{\\ell=1}^{m+1}\\frac{\\ell-a}{\\ell}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and thus (C.28) holds for all $m\\in\\mathbb{N}$ . To complete the proof notice that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{k=1}^{m-1}\\prod_{\\ell=0}^{k-1}(1-\\frac{a}{m-\\ell})=\\frac{(m-a)!}{m!}\\sum_{k=1}^{m-1}\\frac{(m-k)!}{(m-k-a)!}}}\\\\ {{\\displaystyle=F_{m}-\\prod_{\\ell=0}^{m-1}\\Bigl(1-\\frac{a}{m-\\ell}\\Bigr)}}\\\\ {{\\displaystyle=\\frac{m-a}{1+a}+\\frac{a}{1+a}\\prod_{\\ell=1}^{m}\\frac{\\ell-a}{\\ell}-\\prod_{\\ell=1}^{m}\\Bigl(1-\\frac{a}{\\ell}\\Bigr)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n={\\frac{m-a}{1+a}}-{\\frac{1}{1+a}}\\prod_{\\ell=1}^{m}\\left(1-{\\frac{a}{\\ell}}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we discuss the cases of non-vanishing and zero friction. ", "page_idx": 18}, {"type": "text", "text": "C.2. FTXL with non-vanishing friction. We continue this section by considering the case of non-vanishing friction in analogy to the continuous-time case, as per Appendix B. Specifically, we consider the discrete version of (FTXL-D) with non-vanishing friction, as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{p_{i,n+1}=p_{i,n}(1-\\gamma r)+\\gamma\\hat{v}_{i,n}}\\\\ {y_{i,n+1}=y_{i,n}+\\gamma p_{i,n+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\gamma r<1$ . Below, we provide the rate of convergence for the setup of \ud835\udc38\ud835\udc65\ud835\udc4e\ud835\udc5a\ud835\udc5d\ud835\udc59\ud835\udc523.1, as we did before. Namely, we obtain a linear convergence rate, as the following proposition suggests. ", "page_idx": 18}, {"type": "text", "text": "Proposition C.2. Consider the single-player game $\\Gamma$ with actions A and B such that $u(\\mathbf{A})-u(\\mathbf{B})=1$ of Example 3.1, and let $x_{n}=\\Lambda(y_{n})$ be the sequence of play generated by (C.37). Then, denoting by $x^{*}=(1,0)$ the strict Nash equilibrium, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|x_{n}-x^{*}\\|_{\\infty}\\sim\\exp\\Bigl(C-\\frac{\\gamma}{r}n\\Bigr)\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C>0$ is a constant that depends on the initialization of the algorithm. ", "page_idx": 18}, {"type": "text", "text": "Proof. We first define the score-difference ", "page_idx": 18}, {"type": "equation", "text": "$$\nw_{n}:=p_{\\mathtt{B},n}-p_{\\mathtt{A},n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with initial condition $w_{1}=0$ . Then, unfolding according to the sequence of play, we obtain: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{w_{n+1}}={w_{n}}(1-\\gamma r)+\\gamma(u(\\mathbf{B})-u(\\mathbf{A}))}\\\\ {\\;\\;\\;\\;\\;\\;=w_{n}(1-\\gamma r)-\\gamma}\\\\ {\\;\\;\\;\\;\\;\\;=\\ldots}\\\\ {\\;\\;\\;\\;\\;\\;\\;=-\\gamma\\displaystyle\\sum_{k=0}^{n-1}(1-\\gamma r)^{k}}\\\\ {\\;\\;\\;\\;\\;\\;\\;=-\\displaystyle\\frac{1-(1-\\gamma r)^{n}}{r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We next define for $n\\in\\mathbb{N}$ the difference $z_{n}:=y_{\\mathtt{B},n}-y_{\\mathtt{A},n}$ . Thus, unfolding it, we obtain: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{z_{n+1}=z_{n}+\\gamma w_{n+1}}\\\\ &{\\qquad=z_{n}-\\gamma{\\frac{1-(1-\\gamma r)^{n}}{r}}}\\\\ &{\\qquad=z_{1}-\\gamma\\sum_{m=1}^{n}{\\frac{1-(1-\\gamma r)^{m}}{r}}}\\\\ &{\\qquad=z_{1}-{\\frac{\\gamma}{r}}\\left(n-(1-\\gamma r){\\frac{1-(1-\\gamma r)^{n}}{\\gamma r}}\\right)}\\\\ &{\\qquad=z_{1}-{\\frac{\\gamma}{r}}n+{\\mathcal{O}}(1)}\\end{array}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and invoking Lemma A.1 for $\\theta(x)=x\\log x$ , we get the result. ", "page_idx": 18}, {"type": "text", "text": "C.3. FTXL with zero friction. Moving forward to the case of $r=0$ as presented in Section 4, we provide the proof of Theorem 2. Namely, we have: ", "page_idx": 18}, {"type": "text", "text": "Theorem 2. Let $x^{*}$ be a strict Nash equilibrium of $\\Gamma$ , and let $x_{n}=Q(y_{n})$ be the sequence of play generated by (FTXL) with full information feedback of the form (11a). If $x_{1}$ is initialized sufficiently ", "page_idx": 18}, {"type": "text", "text": "close to $x^{*}$ , then $x_{n}$ converges to $x^{*}$ ; in particular, if (FTXL) is run with logit best responses (that is, $Q\\leftarrow\\Lambda$ ), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|x_{T}-x^{*}\\|_{\\infty}\\leq\\exp\\!\\left(C-c\\gamma^{2}{\\frac{T(T-1)}{2}}\\right)=\\exp\\!\\left(-\\Theta(T^{2})\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C>0$ is a constant that depends only on the initialization of (FTXL) and ", "page_idx": 19}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\mathrm{supp}(x_{i}^{*})}[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})]>0\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "is the minimum payoff difference at equilibrium. ", "page_idx": 19}, {"type": "text", "text": "Proof. First of all, since $x^{*}$ is a strict Nash equilibrium, by Lemma B.1 for ", "page_idx": 19}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\operatorname{supp}(x_{i}^{*})}\\left[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "there exists $M>0$ such that if $y_{i\\,\\alpha_{i}^{*}}-y_{i\\,\\alpha_{i}}>M$ for all $\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i}$ and $i\\in\\mathcal{N}$ , then ", "page_idx": 19}, {"type": "equation", "text": "$$\nv_{i\\alpha_{i}^{*}}(Q(y))-v_{i\\alpha_{i}}(Q(y))>c\\quad\\mathrm{for~all~}\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i},\\mathrm{and~}i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For notational convenience, we focus on player $i$ and drop the player-specific indices altogether. Let $\\alpha\\neq\\alpha^{*}\\in\\mathcal{A}$ , and define for $n\\in\\mathbb{N}$ the quantities $w_{\\alpha,n}$ and $z_{\\alpha,n}$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\nw_{\\alpha,n}:=\\langle p_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle,\\qquad z_{\\alpha,n}:=\\langle y_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $e_{\\alpha},e_{\\alpha}^{\\ast}$ are the standard basis vectors corresponding to $\\alpha,\\alpha^{*}\\in\\mathcal{A}$ . ", "page_idx": 19}, {"type": "text", "text": "Let initial conditions $y_{1}$ such that $y_{\\alpha,1}-y_{\\alpha^{*},1}=-M-\\varepsilon$ , for all $\\alpha\\neq\\alpha^{*}\\in\\mathcal{A}$ , where $\\varepsilon>0$ small, and $p_{1}=0$ . We will first show by induction that $z_{\\alpha,n}<-M$ for all $n\\in\\mathbb{N}$ . To this end, unfolding the recursion, we obtain: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{w_{\\alpha,n+1}=w_{\\alpha,n}+\\gamma\\langle\\hat{v}_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle}}\\\\ &{=w_{\\alpha,n}+\\gamma\\langle v(x_{n}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle}\\\\ &{=\\gamma\\displaystyle\\sum_{k=1}^{n}\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used that $w_{1}=0$ . Now, for the sake of induction, suppose that ", "page_idx": 19}, {"type": "equation", "text": "$$\nz_{\\alpha,k}<-M\\quad\\mathrm{for\\,all\\,}k=1,\\ldots,n\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies that $\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle\\,<\\,-c$ . With this in hand, we will prove that $z_{\\alpha,n+1}\\,<\\,-M$ , as well. Specifically, we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle z_{\\alpha,n+1}=z_{\\alpha,n}+\\gamma w_{\\alpha,n+1}=z_{\\alpha,n}+\\gamma^{2}\\displaystyle\\sum_{k=1}^{n}\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle}\\\\ {\\displaystyle\\leq z_{\\alpha,n}-c\\gamma^{2}n}\\\\ {\\displaystyle\\leq z_{\\alpha,1}-c\\gamma^{2}\\displaystyle\\sum_{\\ell=1}^{n}\\ell}\\\\ {\\displaystyle<-M}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the inductive hypothesis and the initial condition. Therefore, we conclude by induction that $z_{\\alpha,n}<-M$ for all $n\\in\\mathbb{N}$ . Thus, we readily obtain that after $T$ time-step: ", "page_idx": 19}, {"type": "equation", "text": "$$\nz_{T}\\leq z_{\\alpha,1}-c\\gamma^{2}\\sum_{\\ell=1}^{T-1}\\ell\\leq z_{\\alpha,1}-c\\gamma^{2}\\frac{T(T-1)}{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and invoking Lemma A.1 for $\\theta(x)=x\\log x$ , we get the result. ", "page_idx": 19}, {"type": "text", "text": "D Proofs for discrete-time algorithms with partial information ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this appendix, we provide the proofs of Theorem 3 and Theorem 4 that correspond to the convergence of (FTXL) with realization-based and bandit feedback, respectively. For this, we need the following lemma, which provides a maximal bound on a martingale process. Namely, we have: ", "page_idx": 20}, {"type": "text", "text": "Lemma D.1. Let $\\begin{array}{r}{M_{n}:=\\sum_{k=1}^{n}\\gamma_{k}\\xi_{k}}\\end{array}$ be a martingale with respect to $({\\mathcal{F}}_{n})_{n\\in\\mathbb{N}}$ with $\\mathbb{E}[\\|\\xi_{n}\\|_{*}^{q}]\\le\\sigma_{n}^{q}$ for some $q>2$ . Then, for $\\ddot{\\mu}\\in(0,1)$ and $n\\in\\mathbb{N}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{k\\leq n}\\lvert M_{k}\\rvert>c\\left(\\sum_{k=1}^{n}\\gamma_{k}\\right)^{\\mu}\\right)\\leq A_{q}\\frac{\\sum_{k=1}^{n}\\gamma_{k}^{q/2+1}\\sigma_{k}^{q}}{\\left(\\sum_{k=1}^{n}\\gamma_{k}\\right)^{1+q(\\mu-1/2)}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $A_{q}$ is a constant depending only on \ud835\udc50and $q$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Fix some $\\mu\\in(0,1)$ . By Doob\u2019s maximal inequality [16, Corollary 2.1], we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{sup}_{k\\leq n}\\lvert M_{k}\\rvert>c\\left(\\sum_{k=1}^{n}\\gamma_{k}\\right)^{\\mu}\\right)\\leq\\frac{\\mathbb{E}[\\lvert M_{n}\\rvert^{q}]}{c^{q}\\left(\\sum_{k=1}^{n}\\gamma_{k}\\right)^{q\\mu}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, applying the Burkholder\u2013Davis\u2013Gundy inequality [16, Theorem 2.10], we get that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}[|M_{n}|^{q}]\\leq A_{q}\\,\\mathbb{E}\\!\\left[\\left(\\sum_{k=1}^{n}\\gamma_{k}^{2}\\|\\xi_{k}\\|_{*}^{2}\\right)^{q/2}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $A_{q}$ is a constant depending only on $c$ and $q$ . Now, we will invoke the generalized H\u00f6lder\u2019s inequality [3]: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left(\\sum_{k=1}^{n}a_{k}b_{k}\\right)^{\\rho}\\leq\\left(\\sum_{k=1}^{n}a_{k}^{\\frac{\\lambda\\rho}{\\rho-1}}\\right)^{\\rho-1}\\sum_{k=1}^{n}a_{k}^{(1-\\lambda)\\rho}b_{k}^{\\rho}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for $a_{k},b_{k}\\geq0,\\rho>1$ and $\\lambda\\in[0,1)$ . Thus, setting $a_{k}=\\gamma_{k}^{2}$ , $b_{k}=\\|\\xi_{k}\\|_{*}^{2},\\rho=q/2$ and $\\lambda=1/2-1/q$ , (D.2), combined with (D.3), becomes: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\underset{k\\leq n}{\\operatorname*{sup}}|M_{k}|>c\\bigg(\\underset{k=1}{\\overset{n}{\\sum}}\\gamma_{k}\\bigg)^{\\mu}\\bigg)\\leq A_{q}\\frac{\\big(\\sum_{k=1}^{n}\\gamma_{k}\\big)^{q/2-1}\\sum_{k=1}^{n}\\gamma_{k}^{q/2+1}\\,\\mathbb{E}[\\|\\xi_{k}\\|_{*}^{q}]}{\\big(\\sum_{k=1}^{n}\\gamma_{k}\\big)^{q\\mu}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq A_{q}\\;\\frac{\\sum_{k=1}^{n}\\gamma_{k}^{q/2+1}\\sigma_{k}^{q}}{\\big(\\sum_{k=1}^{n}\\gamma_{k}\\big)^{1+q(\\mu-1/2)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and the proof is complete. ", "page_idx": 20}, {"type": "text", "text": "With this tool in hand, we move forward to the convergence of (FTXL) under realization-based feedback. ", "page_idx": 20}, {"type": "text", "text": "Theorem 3. Let $x^{*}$ be a strict Nash equilibrium of \u0393, fix some confidence level $\\delta\\:>\\:0_{\\!\\!\\!\\!\\!_{\\!}}$ , and let $x_{n}=Q(y_{n})$ be the sequence of play generated by (FTXL) with realization-based feedback as per (11b) and a sufficiently small step-size $\\gamma>0$ . Then there exists a neighborhood $\\boldsymbol{\\mathcal{U}}$ of $x^{*}$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(x_{n}\\to x^{*}\\mathrm{~as~}n\\to\\infty)\\geq1-\\delta\\qquad i f x_{1}\\in\\mathcal{U}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In particular, if (FTXL) is run with logit best responses (that is, $Q\\leftarrow\\Lambda$ ), there exist positive constants $C,c>0$ as in Theorem 2 such that on the event $\\{x_{n}\\to x^{*}{\\mathrm{~as~}}n\\to\\infty\\}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|x_{T}-x^{*}\\|_{\\infty}\\leq\\exp\\biggl(C-c\\gamma^{2}\\frac{T(T-1)}{2}+\\frac{3}{5}c\\gamma^{5/3}T^{5/3}\\biggr)=\\exp\\bigl(-\\Theta(T^{2})\\bigr)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. First of all, since $x^{*}$ is a strict Nash equilibrium, by Lemma B.1 for ", "page_idx": 20}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\operatorname{supp}(x_{i}^{*})}\\left[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "there exists $M>0$ such that if $y_{i\\,\\alpha_{i}^{*}}-y_{i\\,\\alpha_{i}}>M$ for all $\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i}$ and $i\\in\\mathcal{N}$ , then ", "page_idx": 21}, {"type": "equation", "text": "$$\nv_{i}\\alpha_{i}^{*}(Q(y))-v_{i\\alpha_{i}}(Q(y))>c\\quad\\mathrm{for~all~}\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i},\\mathrm{and~}i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For notational convenience, we focus on player $i$ and drop the player-specific indices altogether. Let $\\alpha\\neq\\alpha^{*}\\in\\mathcal{A}$ , and define for $n\\in\\mathbb{N}$ the quantities $w_{\\alpha,n}$ and $z_{\\alpha,n}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\nw_{\\alpha,n}:=\\langle p_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle,\\qquad z_{\\alpha,n}:=\\langle y_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $e_{\\alpha},e_{\\alpha}^{\\ast}$ are the standard basis vectors corresponding to $\\alpha,\\alpha^{*}\\in\\mathcal{A}$ . ", "page_idx": 21}, {"type": "text", "text": "Then, unfolding the recursion, we obtain: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{w_{\\alpha,n+1}=w_{\\alpha,n}+\\gamma\\langle\\hat{v}_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle=w_{\\alpha,n}+\\gamma\\langle v(x_{n}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle+\\gamma\\langle U_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle}\\\\ {=\\gamma\\displaystyle\\sum_{k=1}^{n}\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle+\\gamma\\displaystyle\\sum_{k=1}^{n}\\langle U_{k},e_{\\alpha}-e_{\\alpha}^{*}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used that $w_{1}=0$ . Now, define the stochastic process $\\{M_{n}\\}_{n\\in\\mathbb{N}}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\nM_{n}:=\\gamma\\sum_{k=1}^{n}\\langle U_{k},e_{\\alpha}-e_{\\alpha}^{\\ast}\\rangle\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which is a martingale, since $\\mathbb{E}[U_{n}\\,|\\,{\\mathcal{F}}_{n}]=0$ . Moreover, note that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|U_{n}\\|_{*}=\\|v(\\alpha_{n})-v(x_{n})\\|_{*}\\leq2\\operatorname*{max}_{\\alpha\\in\\mathcal{A}}\\|v(\\alpha)\\|_{*}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and, thus, we readily obtain that $\\mathbb{E}[\\|U_{n}\\|_{*}^{q}\\,|\\,\\mathcal{F}_{n}]\\le\\sigma^{q}$ for $\\sigma=2\\operatorname*{max}_{\\alpha\\in\\mathcal{A}}\\|v(\\alpha)\\|_{*}$ and all $q\\in[1,\\infty]$ . By Lemma D.1 for $\\gamma_{n}=\\gamma$ , $\\sigma_{n}=\\sigma$ , $\\xi_{n}=\\langle U_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle$ , $c$ as in Theorem 2, and $\\mu\\in(0,1),q>2$ whose values will be determined next, there exists $A_{q}>0$ such that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{n}:=\\mathbb{P}\\bigg(\\underset{k\\leq n}{\\operatorname*{sup}}|M_{k}|>c(\\gamma n)^{\\mu}\\bigg)\\leq A_{q}\\sigma^{q}\\ \\frac{n\\gamma^{q/2+1}}{(\\gamma n)^{1+q(\\mu-1/2)}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq A_{q}\\sigma^{q}\\ \\frac{\\gamma^{q(1-\\mu)}}{n^{q(\\mu-1/2)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we need to guarantee that there exist $\\mu\\in(0,1),q>2$ , such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{\\infty}\\delta_{n}<\\infty\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For this, we simply need $q(\\mu\\mathrm{~-~}1/2)\\;>\\;1$ , or equivalently, $\\mu\\,>\\,1/2+1/q$ , which implies that $\\mu\\in(1/2,1)$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, for $\\gamma$ small enough, we get $\\Sigma_{n=1}^{\\infty}\\,\\delta_{n}<\\delta$ , and therefore: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}\\bigg(\\underset{n=1}{\\overset{\\infty}{\\prod}}\\bigg\\{\\operatorname*{sup}\\_{M_{k}}\\big|M_{k}\\big|\\le c(\\gamma n)^{\\mu}\\bigg\\}\\bigg)=1-\\mathbb{P}\\bigg(\\underset{n=1}{\\overset{\\infty}{\\bigcup}}\\bigg\\{\\operatorname*{sup}_{k\\le n}\\bigl|M_{k}\\big|>c(\\gamma n)^{\\mu}\\bigg\\}\\bigg)}}\\\\ &{\\ge1-\\displaystyle\\sum_{n=1}^{\\infty}\\delta_{n}}\\\\ &{\\ge1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From now on, we denote the good event $\\begin{array}{r}{\\cap_{n=1}^{\\infty}\\left\\{\\operatorname*{sup}_{k\\leq n}\\vert M_{k}\\vert\\leq c(\\gamma n)^{\\mu}\\right\\}}\\end{array}$ by $E$ . Then, with probability at least $1-\\delta$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\nw_{\\alpha,n+1}\\leq\\gamma\\sum_{k=1}^{n}\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle+c(\\gamma n)^{\\mu}\\quad{\\mathrm{for~all~}}n\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Furthermore, we have that for $n>N_{0}:=\\lceil1/\\gamma\\rceil$ , we readily get that $\\gamma n>(\\gamma n)^{\\mu}$ . Therefore, setting ", "page_idx": 21}, {"type": "equation", "text": "$$\nR:=c\\gamma\\sum_{k=1}^{N_{0}-1}((\\gamma k)^{\\mu}-\\gamma k)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "we obtain: ", "page_idx": 22}, {"type": "equation", "text": "$$\n-c\\gamma\\sum_{k=1}^{n}(\\gamma k-(\\gamma k)^{\\mu})\\leq R\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $n\\in\\mathbb{N}$ . Then, initializing $y_{1}$ such that $z_{\\alpha,1}<-M-R$ , we will show that $z_{\\alpha,n}<-M$ for all $n\\in\\mathbb{N}$ with probability at least $1-\\delta$ . For this, suppose that $E$ is realized, and assume that ", "page_idx": 22}, {"type": "equation", "text": "$$\nz_{\\alpha,k}<-M\\quad\\mathrm{for\\,all\\,}k=1,\\ldots,n\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We will show that $z_{\\alpha,n+1}<-M$ , as well. For this, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{z_{\\alpha,n+1}=z_{\\alpha,n}+\\gamma w_{\\alpha,n+1}}}\\\\ &{\\leq z_{\\alpha,n}+\\gamma\\left(\\gamma\\displaystyle\\sum_{k=1}^{n}\\left\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\right\\rangle+c(\\gamma n)^{\\mu}\\right)}\\\\ &{\\leq z_{\\alpha,n}-c\\gamma(\\gamma n-(\\gamma n)^{\\mu})}\\\\ &{\\leq z_{\\alpha,1}-c\\gamma\\displaystyle\\sum_{k=1}^{n}(\\gamma k-(\\gamma k)^{\\mu})}\\\\ &{\\leq-M-R-c\\gamma\\displaystyle\\sum_{k=1}^{n}(\\gamma k-(\\gamma k)^{\\mu})}\\\\ &{<-M}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, we conclude by induction that $z_{\\alpha,n}<-M$ for all $n\\in\\mathbb{N}$ . Thus, we readily obtain that with probability at least $1-\\delta$ it holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle z_{\\alpha,T}\\leq z_{\\alpha,1}-c\\gamma\\sum_{k=1}^{T-1}(\\gamma k-(\\gamma k)^{\\mu})}\\\\ {\\displaystyle\\leq z_{\\alpha,1}-c\\gamma^{2}\\frac{T(T-1)}{2}+c\\gamma^{1+\\mu}\\int_{0}^{T}t^{\\mu}d t}\\\\ {\\displaystyle\\leq z_{\\alpha,1}-c\\gamma^{2}\\frac{T(T-1)}{2}+c\\gamma^{1+\\mu}\\frac{T^{\\mu+1}}{\\mu+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for all $T\\in\\mathbb{N}$ . Setting $\\mu=2/3$ and invoking Lemma A.1 for $\\theta(x)=x\\log x$ , we get the result. \u25a0 ", "page_idx": 22}, {"type": "text", "text": "Finally, we prove the convergence of (FTXL) under realization-based feedback. ", "page_idx": 22}, {"type": "text", "text": "Theorem 4. Let $x^{*}$ be a strict Nash equilibrium of $\\Gamma$ , fix some confidence level $\\delta\\:>\\:0_{\\!\\!\\!\\!\\!_{\\!}}$ , and let $x_{n}=Q(y_{n})$ be the sequence of play generated by (FTXL) with bandit feedback of the form (11c), an IWE exploration parameter $\\bar{\\varepsilon_{n}}\\propto\\bar{1}/n^{\\ell_{\\varepsilon}}$ for some $\\ell_{\\varepsilon}\\in(0,1/2)$ , and a sufficiently small step-size $\\gamma>0$ . Then there exists a neighborhood $\\boldsymbol{\\mathcal{U}}$ of $x^{*}$ in $\\mathcal{X}$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(x_{n}\\to x^{*}\\mathrm{~as~}n\\to\\infty)\\geq1-\\delta\\qquad i f x_{1}\\in\\mathcal{U}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In particular, if (FTXL) is run with logit best responses (that is, $Q\\leftarrow\\Lambda$ ), there exist positive constants $C,c>0$ as in Theorem 2 such that on the event $\\{x_{n}\\to x^{*}$ as $n\\to\\infty$ } ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|x_{T}-x^{*}\\|_{\\infty}\\leq\\exp\\biggl(C-c\\gamma^{2}\\frac{T(T-1)}{2}+\\frac{5}{9}c\\gamma^{9/5}T^{9/5}\\biggr)=\\exp\\bigl(-\\Theta(T^{2})\\bigr)\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. First of all, since $x^{*}$ is a strict Nash equilibrium, by Lemma B.1 for ", "page_idx": 22}, {"type": "equation", "text": "$$\nc=\\frac{1}{2}\\operatorname*{min}_{i\\in\\mathcal{N}}\\operatorname*{min}_{\\beta_{i}\\notin\\operatorname{supp}(x_{i}^{*})}\\left[u_{i}(x_{i}^{*};x_{-i}^{*})-u_{i}(\\beta_{i};x_{-i}^{*})\\right]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "there exists $M>0$ such that if $y_{i\\,\\alpha_{i}^{*}}-y_{i\\,\\alpha_{i}}>M$ for all $\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i}$ and $i\\in\\mathcal{N}$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\nv_{i}\\alpha_{i}^{*}(Q(y))-v_{i\\alpha_{i}}(Q(y))>c\\quad\\mathrm{for~all~}\\alpha_{i}\\neq\\alpha_{i}^{*}\\in\\mathcal{A}_{i},\\mathrm{and~}i\\in\\mathcal{N}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For notational convenience, we focus on player $i$ and drop the player-specific indices altogether. Let $\\alpha\\neq\\alpha^{*}\\in\\mathcal{A}$ , and define for $n\\in\\mathbb{N}$ the quantities $w_{\\alpha,n}$ and $z_{\\alpha,n}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\nw_{\\alpha,n}:=\\langle p_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle,\\qquad z_{\\alpha,n}:=\\langle y_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $e_{\\alpha},e_{\\alpha}^{\\ast}$ are the standard basis vectors corresponding to $\\alpha,\\alpha^{*}\\in\\mathcal{A}$ . For notational convenience, we focus on player $i$ and drop the player-specific indices altogether. Now, decomposing the IWE $\\hat{v}_{n}$ , we obtain ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{v}_{n}=v(x_{n})+U_{n}+b_{n}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $U_{n}:=\\hat{v}_{n}-v_{i}(\\hat{x}_{n})$ is a zero-mean noise, and $b_{i,n}:=v_{i}({\\hat{x}}_{n})-v_{i}(x_{n})$ . ", "page_idx": 23}, {"type": "text", "text": "Then, unfolding the recursion, we obtain: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w_{\\alpha,n+1}=w_{\\alpha,n}+\\gamma\\langle\\hat{v}_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle}\\\\ &{\\quad\\quad\\quad=w_{\\alpha,n}+\\gamma\\langle v(x_{n}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle+\\gamma\\langle U_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle+\\gamma\\langle b_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle}\\\\ &{\\quad\\quad\\quad\\leq w_{\\alpha,n}+\\gamma\\langle v(x_{n}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle+\\gamma\\langle U_{n},e_{\\alpha}-e_{\\alpha}^{*}\\rangle+2\\gamma\\|b_{n}\\|_{*}}\\\\ &{\\quad\\quad\\quad\\leq\\displaystyle\\gamma\\sum_{k=1}^{n}\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle+\\gamma\\displaystyle\\sum_{k=1}^{n}\\langle U_{k},e_{\\alpha}-e_{\\alpha}^{*}\\rangle+2\\gamma\\displaystyle\\sum_{k=1}^{n}\\|b_{k}\\|_{*}}\\\\ &{\\quad\\quad\\quad\\leq\\gamma\\displaystyle\\sum_{k=1}^{n}\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle+\\gamma\\displaystyle\\sum_{k=1}^{n}\\langle U_{k},e_{\\alpha}-e_{\\alpha}^{*}\\rangle+2\\gamma B\\displaystyle\\sum_{k=1}^{n}\\varepsilon_{k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used that $\\|b_{n}\\|_{*}=\\Theta(\\varepsilon_{n})$ for all $n\\in\\mathbb{N}$ . Now, define the process $\\{M_{n}\\}_{n\\in\\mathbb{N}}$ as ", "page_idx": 23}, {"type": "equation", "text": "$$\nM_{n}:=\\gamma\\sum_{k=1}^{n}\\langle U_{k},e_{\\alpha}-e_{\\alpha}^{\\ast}\\rangle\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is a martingale, since $\\mathbb{E}[U_{n}\\,|\\,{\\mathcal{F}}_{n}]=0$ . Moreover, note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\lVert U_{n}\\rVert_{*}=\\lVert\\hat{v}_{n}-v(\\hat{x}_{n})\\rVert_{*}\\,\\leq\\,\\lVert\\hat{v}_{n}\\rVert_{*}+\\lVert v(\\hat{x}_{n})\\rVert_{*}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "i.e., $\\|U_{n}\\|_{*}=\\Theta(1/\\varepsilon_{n})$ . Thus, we readily obtain that $\\mathbb{E}[\\|U_{n}\\|_{*}^{q}\\,|\\,\\mathcal{F}_{n}]\\le\\sigma_{n}^{q}$ for $\\sigma_{n}=\\Theta(1/\\varepsilon_{n})$ and all $q\\in[1,\\infty]$ . So, by Lemma D.1 for $\\gamma_{n}=\\gamma$ , $\\sigma_{n}=\\sigma$ , $c$ as in Theorem 2, and $\\mu\\in(0,1),q>2$ whose values will be determined next, there exists $A_{q}>0$ such that: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\delta_{n}:=\\mathbb{P}\\bigg(\\underset{k\\leq n}{\\operatorname*{sup}}|M_{k}|>\\frac{c}{2}(\\gamma n)^{\\mu}\\bigg)\\leq A_{q}\\;\\frac{\\gamma^{q/2+1}\\sum_{k=1}^{n}\\sigma_{k}^{q}}{(\\gamma n)^{1+q(\\mu-1/2)}}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq A_{q}\\;\\frac{\\gamma^{q(1-\\mu)}\\sum_{k=1}^{n}\\sigma_{k}^{q}}{n^{1+q(\\mu-1/2)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, note that for $\\varepsilon_{n}=\\varepsilon/n^{\\ell_{\\varepsilon}}$ , and since $\\sigma_{n}=\\Theta(1/\\varepsilon_{n})$ , we get that there exists $M>0$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{n}\\sigma_{k}^{q}\\le M\\varepsilon^{-q}\\sum_{k=1}^{n}k^{q\\ell_{\\varepsilon}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with $\\begin{array}{r}{\\sum_{k=1}^{n}k^{q\\ell_{\\varepsilon}}=\\Theta(n^{1+q\\ell_{\\varepsilon}})}\\end{array}$ . Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\delta_{n}\\leq A_{q}^{\\prime}\\ \\frac{\\gamma^{q(1-\\mu)}\\varepsilon^{-q}n^{1+q\\ell_{\\varepsilon}}}{n^{1+q(\\mu-1/2)}}}\\\\ {\\leq A_{q}^{\\prime}\\ \\frac{\\gamma^{q(1-\\mu)}\\varepsilon^{-q}}{n^{q(\\mu-1/2-\\ell_{\\varepsilon})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, we need to guarantee that there exist $\\mu\\in(0,1),q>2$ , such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{\\infty}\\delta_{n}<\\infty\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For this, we need to ensure that $q(\\mu-1/2-\\ell_{\\varepsilon})>1$ , or, equivalently, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ell_{\\varepsilon}<\\mu-1/2-1/q\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which we will do later. Then, we will get for $\\gamma$ small enough: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{P}\\Bigg(\\underset{n=1}{\\overset{\\infty}{\\prod}}\\Bigg\\{\\underset{k\\leq n}{\\operatorname*{sup}}|M_{k}|\\leq\\frac{c}{2}(\\gamma n)^{\\mu}\\Bigg\\}\\Bigg)=1-\\mathbb{P}\\Bigg(\\underset{n=1}{\\overset{\\infty}{\\bigcup}}\\Bigg\\{\\underset{k\\leq n}{\\operatorname*{sup}}|M_{k}|>\\frac{c}{2}(\\gamma n)^{\\mu}\\Bigg\\}\\Bigg)}}\\\\ &{\\geq1-\\displaystyle\\sum_{n=1}^{\\infty}\\delta_{n}}\\\\ &{\\geq1-\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Regarding the term $2\\gamma B\\sum_{k=1}^{n}\\varepsilon_{k}$ in (D.26), we have that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n2\\gamma B\\sum_{k=1}^{n}\\varepsilon_{k}=2B\\gamma\\varepsilon\\sum_{k=1}^{n}k^{-\\ell_{\\varepsilon}}\\leq B^{\\prime}\\gamma\\varepsilon n^{1-\\ell_{\\varepsilon}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where we used that $\\begin{array}{r}{\\sum_{k=1}^{n}k^{-\\ell_{s}}=\\Theta(n^{1-\\ell_{s}})}\\end{array}$ . Thus, for ", "page_idx": 24}, {"type": "equation", "text": "$$\n1-\\ell_{\\varepsilon}<\\mu\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have for $\\varepsilon,\\gamma>0$ small enough: ", "page_idx": 24}, {"type": "equation", "text": "$$\n2\\gamma B\\sum_{k=1}^{n}\\varepsilon_{k}\\leq B^{\\prime}\\gamma\\varepsilon n^{1-\\ell_{\\varepsilon}}\\leq B^{\\prime}\\gamma\\varepsilon n^{\\mu}\\leq\\frac{c}{2}(\\gamma n)^{\\mu}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $n\\in\\mathbb{N}$ . Hence, by (D.33), (D.36) we need the following two conditions to be satisfied: ", "page_idx": 24}, {"type": "equation", "text": "$$\n1-\\ell_{\\varepsilon}<\\mu\\quad\\mathrm{and}\\quad\\ell_{\\varepsilon}<\\mu-\\frac{1}{2}-\\frac{1}{q}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for which we get that for $\\ell_{\\varepsilon}\\,\\in\\,(0,1/2)$ , there exists always $\\mu\\,\\in\\,(3/4,1)$ and $q$ large that satisfy (D.38). Thus, combining (D.37) and (D.34), we get by (D.26) that with probability at least $1-\\delta$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nw_{\\alpha,n+1}\\leq\\sum_{k=1}^{n}\\gamma_{k}\\langle v(x_{k}),e_{\\alpha}-e_{\\alpha}^{*}\\rangle+c(\\gamma n)^{\\mu}\\quad{\\mathrm{for~all~}}n\\in\\mathbb{N}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, following similar steps as in the proof Theorem 3 after (D.14), we readily obtain that with probability at least $1-\\delta$ , it holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle z_{\\alpha,T}\\leq z_{\\alpha,1}-c\\gamma\\sum_{k=1}^{T-1}(\\gamma k-(\\gamma k)^{\\mu})}\\\\ {\\displaystyle\\leq z_{\\alpha,1}-c\\gamma^{2}\\frac{T(T-1)}{2}+c\\gamma^{1+\\mu}\\int_{0}^{T}t^{\\mu}d t}\\\\ {\\displaystyle\\leq z_{\\alpha,1}-c\\gamma^{2}\\frac{T(T-1)}{2}+c\\gamma^{1+\\mu}\\frac{T^{\\mu+1}}{\\mu+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $T\\in\\mathbb{N}$ . Setting $\\mu=4/5$ and invoking Lemma A.1 for $\\theta(x)=x\\log x$ , we get the result. \u25a0 ", "page_idx": 24}, {"type": "text", "text": "E Numerical experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide numerical simulations to validate and explore the performance of (FTXL).   \nTo this end, we consider two game paradigms, (i) a zero-sum game, and (ii) a congestion game. ", "page_idx": 24}, {"type": "text", "text": "Zero-sum Game. First, we consider a 2-player zero-sum game with actions $\\{\\alpha_{1},\\alpha_{2},\\alpha_{3}\\}$ and $\\{\\beta_{1},\\beta_{2},\\beta_{3}\\}$ , and payoff matrix ", "page_idx": 24}, {"type": "equation", "text": "$$\nP=\\left(\\!\\!{\\begin{array}{c c c}{2,-2)}&{(1,-1)}&{(2,-2)}\\\\ {(-2,2)}&{(-1,1)}&{(-2,2)}\\\\ {(-2,2)}&{(-1,1)}&{(-2,2)}\\end{array}}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, the rows of $P$ correspond to the actions of player $A$ and the columns to the actions of player $B$ , while the first item of each entry of $P$ corresponds to the payoff of $A$ , and the second one to the payoff of $B$ . Clearly, the action profile $(\\alpha_{1},\\beta_{2})$ is a strict Nash equilibrium. ", "page_idx": 24}, {"type": "text", "text": "Congestion Game. As a second example, we consider a congestion game with $N=100$ and 2 roads, $r_{1}$ and $r_{2}$ , with costs $c_{1}=1.1$ and $c_{2}=d/N$ where $d$ is the number of drivers on $r_{2}$ . In words, $r_{1}$ has a fixed delay equal to 1.1, while $r_{2}$ has a delay proportional to the drivers using it. Note, that the strategy profile where all players are using $r_{2}$ is a strict Nash equilibrium. ", "page_idx": 25}, {"type": "text", "text": "In Fig. 1, we assess the convergence of (FTXL) with logit best responses, under realization-based and bandit feedback, and compare it to the standard (EW) with the same level of information. For each feedback mode, we conducted 100 separate trials, each with $T=10^{3}$ steps, and calculated the average norm $\\left\\|x_{n}-x^{*}\\right\\|_{1}$ as a function of the iteration counter $n=1,2,...,T$ . The solid lines represent the average distance from equilibrium for each method, while the shaded areas enclose the range of $\\pm1$ standard deviation from the mean across the different trials. All the plots are displayed in logarithmic scale. For the zero-sum game, all runs were initialized with $y_{1}=0$ , and we used constant step-size $\\gamma=10^{-2}$ , and exploration parameter $\\varepsilon=10^{-1}$ , where applicable. For the congestion game, the initial state $y_{1}$ for each run was drawn uniformly at random in $[-1,1]^{2}$ , and we used constant step-size $\\gamma=10^{-2}$ , and exploration parameter $\\varepsilon_{n}=1/n^{1/4}$ , where applicable. ", "page_idx": 25}, {"type": "text", "text": "The experiments have been implemented using Python 3.11.5 on a M1 MacBook Air with 16GB of RAM. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: It can be found in Section 3, Section 4 and the appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: It can be found in Section 1. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Justification: It can be found in Section 2, Section 3, Section 4 and the appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?   \nAnswer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: It can be found in Appendix E, and the code is included in the supplemental material. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The code is included in the supplemental material. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ", "page_idx": 27}, {"type": "text", "text": "\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: It can be found in Appendix E. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Justification: No statistical significance applicable. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes]   \nJustification: It can be found in Appendix E. Guidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS   \nCode of Ethics https://neurips.cc/public/EthicsGuidelines?   \nAnswer: [Yes]   \nJustification: The paper conforms with the NeurIPS Code of Ethics.   \nGuidelines:   \n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Justification: There is no societal impact. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA]   \nJustification: There are no such risks.   \nGuidelines:   \n\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not use existing assets. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not release new assets. Guidelines:   \n\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA]   \nJustification: The paper does not involve crowdsourcing nor research with human subjects.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]