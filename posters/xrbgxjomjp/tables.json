[{"figure_path": "xrbgXJomJp/tables/tables_7_1.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and rewards achieved by different multi-agent imitation learning algorithms across various scenarios.  The scenarios include different game maps and agent numbers in the StarCraft Multi-Agent Challenge (SMACv2), Gold Miner tasks with varying difficulties, and Multi-Particle Environments (MPE) with different configurations. The algorithms compared include Behavior Cloning (BC), Independent Inverse Q-Learning (IIQ), IQ-Learn with Value Decomposition Network (IQVDN), Multi-Agent Soft Q-learning with Inverse Reinforcement Learning (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-Agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) algorithm with both deterministic and soft policies. The results demonstrate the superior performance of MIFQ in most scenarios.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_15_1.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and rewards achieved by different multi-agent imitation learning algorithms across various scenarios.  The scenarios include different settings of the StarCraft multi-agent challenge (SMACv2) and the Gold Miner game, categorized by the game type (Protoss, Terran, Zerg) and the number of agents (5vs5, 10vs10). The algorithms being compared are Behavior Cloning (BC), Independent Inverse Q-Learning (IIQ), IQ-Learn with Value Decomposition Network (IQVDN), Multi-Agent Soft Q-Learning with Adversarial Imitation Learning (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-Agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) algorithm (both deterministic and soft versions).  The results show that the proposed MIFQ algorithm generally outperforms the baseline methods.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_16_1.jpg", "caption": "Table 2: Hyper-parameters.", "description": "This table presents the hyperparameters used in the experiments for different environments (MPES, Miner, SMACv2).  The hyperparameters include the maximum number of training steps, the evaluation frequency, the buffer size, learning rate, batch size, hidden dimension size, the gamma value, the target update frequency, and the number of random seeds used.", "section": "B.3 Experimental Settings"}, {"figure_path": "xrbgXJomJp/tables/tables_17_1.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and rewards achieved by different multi-agent imitation learning algorithms across various scenarios. The scenarios include different settings from StarCraft (Protoss, Terran, Zerg), Gold Miner (easy, medium, hard), and Multi-Particle Environments (reference, spread, speaker).  The algorithms compared include Behavior Cloning (BC), Independent Inverse Q-Learning (IIQ), Inverse Q-Learning with Value Decomposition Network (IQVDN), Multi-Agent Soft Q-learning with Inverse Reinforcement Learning (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-Agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) with deterministic (Det) and soft (Soft) policies.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_18_1.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and reward values achieved by different multi-agent imitation learning algorithms across various scenarios. The scenarios are categorized by game (Protoss, Terran, Zerg, and Miner) and difficulty level (5vs5, 10vs10, easy, medium, hard).  The algorithms compared are Behavior Cloning (BC), Independent Inverse Q-Learning (IIQ), Inverse Q-Learning with Value Decomposition Network (IQVDN), Multi-Agent Soft Q-Learning with Adversarial Inverse Reinforcement Learning (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-Agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) algorithm (both deterministic and soft versions). The table shows that MIFQ outperforms the baselines in most cases.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_18_2.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of win rates and rewards achieved by different multi-agent imitation learning algorithms across various scenarios.  The scenarios include different game settings (Protoss, Terran, Zerg) and difficulty levels (easy, medium, hard) in the StarCraft and Gold Miner games, as well as different MPE scenarios.  The algorithms compared are Behavior Cloning (BC), Independent Inverse Q-Learning (IIQ), IQ-Learn with Value Decomposition Network (IQVDN), Multi-Agent Soft Q-Learning with Implicit Rewards (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-Agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) algorithm with both deterministic (Det) and soft (Soft) policies. The table highlights the superior performance of the proposed MIFQ algorithm.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_19_1.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and rewards achieved by different multi-agent imitation learning algorithms across various scenarios.  The scenarios include different game settings (Protoss, Terran, Zerg) with varying numbers of agents (5vs5 and 10vs10), as well as different difficulty levels in the Gold Miner game (easy, medium, hard) and several Multi-Particle Environment (MPE) scenarios (reference, spread, speaker). The algorithms compared are Behavior Cloning (BC), Independent Inverse Q-Learning (IIQ), IQ-Learn with Value Decomposition Network (IQVDN), Multi-Agent Soft Q-Learning with Adversarial Inverse Reinforcement Learning (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-Agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) with both deterministic and soft policy versions.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_19_2.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and reward values achieved by different multi-agent imitation learning algorithms across various scenarios.  The scenarios include different game maps and sizes (5vs5 or 10vs10) for the StarCraft Multi-Agent Challenge (SMACv2), different difficulty levels of the Gold Miner game, and different variations of the Multi-Particle Environment (MPE) game.  The algorithms compared include Behavior Cloning (BC), Independent Inverse Q-Learning (IIQ), IQ-Learn with Value Decomposition Network (IQVDN), Multi-Agent Soft Q-Imitation Learning (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-Agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) with deterministic and soft policies. The results show that MIFQ generally outperforms other baselines across various scenarios.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_19_3.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and rewards achieved by different multi-agent imitation learning algorithms across various scenarios. The scenarios include different combinations of the Protoss, Terran, and Zerg races in the StarCraft multi-agent challenge (SMACv2) environment, as well as different difficulty levels in the Gold Miner game and various configurations of multi-particle environments (MPE). The algorithms compared include Behavior Cloning (BC), Independent IQ-Learn (IIQ), IQ-Learn with Value Decomposition Network (IQVDN), Multi-agent Soft Q-learning with Implicit Reward Learning (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) algorithm (both deterministic and soft versions).  The expert results are also presented as a benchmark.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_20_1.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and reward values achieved by different multi-agent imitation learning algorithms across various scenarios. The scenarios include different game maps and team sizes in StarCraft multi-agent challenge (SMACv2) and Gold Miner, as well as different configurations of the Multi-Particle Environments (MPE).  The algorithms compared include behavior cloning (BC), independent inverse Q-learning (IIQ), IQ-Learn with Value Decomposition Network (IQVDN), multi-agent soft Q-Imitation Learning (MASQIL), multi-agent adversarial inverse reinforcement learning (MAAIRL), multi-agent generative adversarial imitation learning (MAGAIL), and the proposed Multi-agent Inverse Factorized Q-learning (MIFQ) algorithm with both deterministic and soft policy versions. The results showcase the performance of each algorithm in terms of win rates and average rewards, providing insights into their relative effectiveness in cooperative multi-agent imitation learning.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_20_2.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of the win rates and rewards achieved by different multi-agent imitation learning algorithms across various scenarios (Protoss, Terran, Zerg) and difficulty levels (easy, medium, hard) in the Gold Miner game, and in the Multi-Particle Environments (MPE).  The algorithms are compared against an expert agent's performance. The results highlight the relative performance of each algorithm in different scenarios.", "section": "5 Experiments"}, {"figure_path": "xrbgXJomJp/tables/tables_20_3.jpg", "caption": "Table 1: Winrate and reward comparisons", "description": "This table presents a comparison of win rates and rewards achieved by different multi-agent imitation learning (IL) algorithms across various scenarios.  The scenarios include different configurations of the StarCraft Multi-Agent Challenge (SMACv2) with varying numbers of agents (5vs5 and 10vs10) and factions (Protoss, Terran, Zerg). In addition, there are three scenarios based on the Gold Miner game (easy, medium, hard) and three Multi-Particle Environments (MPE) (reference, spread, speaker). The algorithms compared include Behavior Cloning (BC), Independent Inverse Q-Learning (IIQ), IQ-Learn with Value Decomposition Network (IQVDN), Multi-Agent Soft Q-Learning (MASQIL), Multi-Agent Adversarial Inverse Reinforcement Learning (MAAIRL), Multi-Agent Generative Adversarial Imitation Learning (MAGAIL), and the proposed method, Multi-agent Inverse Factorized Q-learning (MIFQ). Both deterministic and soft versions of MIFQ are included. The table shows the win rate (percentage) for SMACv2 and Gold Miner games, and reward scores (averaged over runs) for MPE.", "section": "5 Experiments"}]