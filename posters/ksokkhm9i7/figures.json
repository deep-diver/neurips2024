[{"figure_path": "KSOkkHm9I7/figures/figures_1_1.jpg", "caption": "Figure 1: To generate multiple k auto-complete suggestions for a prefix using an LM, the existing decoding methods like Nucleus Sampling need k inference passes. In contrast, Superposed Decoding can generate k suggestions at the cost of a single inference pass while being as coherent and factual.", "description": "This figure illustrates the difference between traditional decoding methods (like Nucleus Sampling) and the proposed Superposed Decoding method.  Traditional methods require running the language model k times to generate k different autocomplete suggestions.  Superposed Decoding, however, achieves the same result with only a single inference pass of the language model, making it significantly faster while maintaining coherence and factuality.", "section": "1 Introduction"}, {"figure_path": "KSOkkHm9I7/figures/figures_3_1.jpg", "caption": "Figure 2: Superposed Decoding relies on feeding a superposed token embedding \u2013 based on the most recent tokens from the current k drafts \u2013 as the input during the auto-regressive inference step. This generates k\u00b2 new drafts using the existing k drafts and the top-k output tokens at the new timestep. Finally, keep the top-k drafts after filtering with an n-gram interpolation to improve coherency.", "description": "This figure illustrates the Superposed Decoding algorithm.  It starts with k initial drafts. At each step, the algorithm creates a \"superposition\" of the embeddings of the most recent tokens from all k drafts. This superposition is fed into the language model to generate k*k new draft options. Then, n-gram interpolation is used to select the top-k most likely options, which become the new set of k drafts for the next iteration.  This process continues until the desired generation length is reached.", "section": "3 Superposed Decoding (SPD)"}, {"figure_path": "KSOkkHm9I7/figures/figures_4_1.jpg", "caption": "Figure 3: Llama-2-7B maintains the linear relationship between superposed embeddings and the component token embeddings, with mean cosine similarity > 0.6 for the first 10 timesteps.", "description": "This figure shows the results of an experiment testing the linearity of Llama-2-7B.  The experiment measured the cosine similarity between superposed embeddings (a weighted combination of token embeddings from multiple drafts) and the sum of their individual component token embeddings across 20 timesteps. The results show that Llama-2-7B maintains a relatively high cosine similarity (above 0.6) for the first 10 timesteps, indicating a linear relationship between the superposed embeddings and their components.  After 10 timesteps, the linearity begins to decrease. This supports the Superposed Decoding method, which relies on this linear relationship to generate multiple text drafts efficiently.", "section": "3.3 Superposed Decoding Semantically Approximates Beam Search"}, {"figure_path": "KSOkkHm9I7/figures/figures_6_1.jpg", "caption": "Figure 5: Superposed Decoding is as accurate as Greedy Decoding for P@1 and increases the fact-based coverage using multiple drafts (P@2,3) on TriviaQA (left) and Natural Questions (right).", "description": "This figure shows the accuracy of different decoding methods (Nucleus, Beam, Greedy, and Superposed Decoding) on two common benchmarks for short-answer generation: TriviaQA and Natural Questions.  The accuracy is measured by exact match precision at k (P@k), where k represents the number of generated drafts considered.  The figure demonstrates that Superposed Decoding achieves similar or better accuracy compared to Greedy Decoding for P@1 (considering only the best draft) and shows significantly higher accuracy for P@2 and P@3 (considering the top 2 and top 3 drafts, respectively), indicating that generating multiple drafts improves the likelihood of obtaining a correct answer.  This highlights Superposed Decoding's ability to increase fact-based coverage by generating multiple drafts.", "section": "4.2 Fact-Based Evaluation"}, {"figure_path": "KSOkkHm9I7/figures/figures_6_2.jpg", "caption": "Figure 6: Average wall clock time to generate 10 token long drafts, with batch size = 1, from a 15 token prefix on OpenWebText. Superposed Decoding is significantly faster for all k > 1, with n-gram lookup costs being a major factor for k \u2265 4, which can be optimized further.", "description": "The figure shows the average latency in seconds for generating different numbers of drafts (k) using various decoding methods: Superposed Decoding, Nucleus Sampling, and Beam Search.  Superposed Decoding consistently shows significantly lower latency compared to other methods, especially as the number of drafts increases. The increase in latency for all methods is roughly linear as k increases.  A notable aspect is the influence of n-gram lookup cost on the latency of Superposed Decoding for larger k values.  This suggests that optimization of n-gram lookup could lead to even greater speed improvements for Superposed Decoding.", "section": "4.3 Latency"}, {"figure_path": "KSOkkHm9I7/figures/figures_6_3.jpg", "caption": "Figure 7: Drafts are ordered by the probability they obtain during generation using SPD, which wins over Nucleus Sampling in a compute-normalized setting.", "description": "This figure displays the results of a user study comparing Superposed Decoding (SPD) and Nucleus Sampling in a compute-normalized setting.  The x-axis represents the decoding method used (Nucleus or SPD), while the y-axis shows the win rate (percentage of times the method's generation was preferred).  The figure shows that SPD outperforms Nucleus Sampling when considering the compute used to generate drafts. The chart also breaks down individual draft rankings for SPD, demonstrating that, on average, at least one of SPD's drafts is preferred to the Nucleus Sampling draft. The overall better performance of SPD indicates the usefulness of generating multiple drafts, even if only one is ultimately selected by the user.", "section": "4.4 Human Evaluation"}, {"figure_path": "KSOkkHm9I7/figures/figures_9_1.jpg", "caption": "Figure 8: Left: Minimal difference in repetition for Superposed Decoding and Nucleus Sampling in short generations. Right: Generation length is an effective knob for adjusting the diversity of drafts. Both experiments use a prefix length of 15 tokens.", "description": "The figure is composed of two subfigures. The left subfigure shows the n-gram uniqueness for different generation lengths. The right subfigure shows the draft self-BLEU score for different generation lengths. Both subfigures compare Superposed Decoding with Nucleus Sampling. The left subfigure shows that Superposed Decoding and Nucleus Sampling have similar levels of repetition for short generations. The right subfigure shows that shorter generations lead to higher diversity (lower Self-BLEU scores) for both methods.", "section": "5.2 Textual Analysis"}, {"figure_path": "KSOkkHm9I7/figures/figures_15_1.jpg", "caption": "Figure 9: Average perplexity of one draft of Nucleus Sampling and Beam Search compared against average best perplexity of SPD using drafts k = {2, 3, 4, 5, 6, 10} on OpenWebText. SPD outperforms Nucleus Sampling for all values of k tested.", "description": "This figure shows the average perplexity for one draft generated by Nucleus Sampling and Beam Search methods, compared to the average best perplexity achieved by Superposed Decoding (SPD) for different numbers of drafts (k).  The results demonstrate that SPD consistently outperforms Nucleus Sampling across all tested values of k, indicating that SPD is more efficient in generating high-quality text.", "section": "4.1 Generation Quality"}, {"figure_path": "KSOkkHm9I7/figures/figures_15_2.jpg", "caption": "Figure 10: Average perplexity of one draft of Nucleus Sampling compared against average best perplexity of SPD for prefix lengths 5, 15, 25, 40. We find that SPD and Nucleus Sampling consistently have similar perplexities.", "description": "This figure shows the average perplexities of single drafts generated by Nucleus Sampling and the best draft generated by Superposed Decoding (SPD) for different prefix lengths.  The results indicate that the average perplexity of SPD's best draft is comparable to that of Nucleus Sampling across various prefix lengths, demonstrating the comparable quality of SPD's generated text.  The y-axis represents perplexity, and the x-axis represents the length of the input prefix.", "section": "4.1 Generation Quality"}, {"figure_path": "KSOkkHm9I7/figures/figures_15_3.jpg", "caption": "Figure 9: Average perplexity of one draft of Nucleus Sampling and Beam Search compared against average best perplexity of SPD using drafts k = {2, 3, 4, 5, 6, 10} on OpenWebText. SPD outperforms Nucleus Sampling for all values of k tested.", "description": "This figure compares the average perplexity of a single draft generated by Nucleus Sampling and Beam Search against the average best perplexity achieved by Superposed Decoding (SPD) across varying numbers of drafts (k).  The results demonstrate that SPD consistently achieves lower perplexity than Nucleus Sampling, indicating superior generation quality, for all tested values of k.", "section": "4.1 Generation Quality"}, {"figure_path": "KSOkkHm9I7/figures/figures_16_1.jpg", "caption": "Figure 7: Drafts are ordered by the probability they obtain during generation using SPD, which wins over Nucleus Sampling in a compute-normalized setting.", "description": "This figure shows the results of a human evaluation comparing Superposed Decoding (SPD) and Nucleus Sampling.  The bar chart displays the win rate for each method in a compute-normalized setting, where the number of generations from each method are balanced to equate computational cost.  SPD outperforms Nucleus Sampling in this setting, indicating that the diversity of outputs produced by SPD is preferred by users.", "section": "4.4 Human Evaluation"}, {"figure_path": "KSOkkHm9I7/figures/figures_20_1.jpg", "caption": "Figure 1: To generate multiple k auto-complete suggestions for a prefix using an LM, the existing decoding methods like Nucleus Sampling need k inference passes. In contrast, Superposed Decoding can generate k suggestions at the cost of a single inference pass while being as coherent and factual.", "description": "This figure illustrates the difference between traditional decoding methods (like Nucleus Sampling) and the proposed Superposed Decoding method.  Traditional methods require multiple passes of the language model to generate multiple text suggestions (k suggestions require k passes).  In contrast, Superposed Decoding generates k suggestions in a single pass, achieving comparable coherence and factuality.", "section": "1 Introduction"}, {"figure_path": "KSOkkHm9I7/figures/figures_21_1.jpg", "caption": "Figure 1: To generate multiple k auto-complete suggestions for a prefix using an LM, the existing decoding methods like Nucleus Sampling need k inference passes. In contrast, Superposed Decoding can generate k suggestions at the cost of a single inference pass while being as coherent and factual.", "description": "The figure illustrates the difference between traditional decoding methods (like Nucleus Sampling) and the proposed Superposed Decoding method in generating multiple auto-complete suggestions. Traditional methods require running the language model k times (where k is the number of suggestions) to produce k suggestions.  Superposed Decoding, in contrast, achieves the same outcome with a single language model inference pass, resulting in significant computational savings while maintaining coherence and factual accuracy.", "section": "1 Introduction"}, {"figure_path": "KSOkkHm9I7/figures/figures_22_1.jpg", "caption": "Figure 1: To generate multiple k auto-complete suggestions for a prefix using an LM, the existing decoding methods like Nucleus Sampling need k inference passes. In contrast, Superposed Decoding can generate k suggestions at the cost of a single inference pass while being as coherent and factual.", "description": "This figure illustrates the core difference between traditional decoding methods (like Nucleus Sampling) and the proposed Superposed Decoding method.  Traditional methods require running the language model multiple times to generate multiple auto-complete suggestions (one pass per suggestion).  Superposed Decoding, however, achieves the same result in a single pass, making it significantly more efficient while maintaining coherence and factual accuracy.", "section": "1 Introduction"}, {"figure_path": "KSOkkHm9I7/figures/figures_22_2.jpg", "caption": "Figure 16: Layer-wise linearity analysis of the first five timesteps on Llama-2-7B with three tokens. The relationship between superposed embeddings and the component token embeddings is initially entirely linear; linearity then degenerates over the first few layers, but gradually recovers through the subsequent transformer block layers.", "description": "This figure shows the layer-wise linearity analysis for the first five time steps on Llama-2-7B language model with three tokens.  The x-axis represents the layers of the model, and the y-axis represents the mean cosine similarity between superposed embeddings and the linear combination of their component embeddings. The plot shows that the relationship between these two is initially highly linear, then it deteriorates in the first few layers before recovering gradually in subsequent layers.", "section": "3.3 Superposed Decoding Semantically Approximates Beam Search"}]