[{"figure_path": "BZLdXBjB8O/tables/tables_2_1.jpg", "caption": "Table 1: The experimental results of four models on toy data. The variation of latent v and logits p(y|v) is measured between clean and adversarial examples. The model margin is estimated by the minimal adversarial perturbation required to flip the label, employing both l2 and l\u221e norm.", "description": "This table presents a comparison of four different models on toy data in terms of their robustness against adversarial attacks.  The models are: Discriminative, Generative, Causal without Disentanglement, and Causal with Disentanglement.  The table shows the change in latent variables (\u0394v), the change in prediction logits (\u0394p(y|v)), and the margin (minimum adversarial perturbation to flip the label) for each model under both l2 and l\u221e norms. The results demonstrate the improved robustness of the Causal with Disentanglement model compared to the others.", "section": "3 Pilot Study on Toy Data"}, {"figure_path": "BZLdXBjB8O/tables/tables_6_1.jpg", "caption": "Table 2: Clean accuracy and adversarial robustness on CIFAR-10 against StAdv under l\u221e (e\u20ac = 0.05) norm bound and AutoAttack (AA) under l2 (6 = 0.5) as well as l\u221e (6 = 8/255) bound. We calculate the average robustness across three attack methods to evaluate the model's robustness against unseen attacks. We use underlining to highlight the best robustness for each attack method within each defense category, and bold font to denote the state-of-the-art (SOTA) across all methods.", "description": "This table presents a comparison of different defense methods against three adversarial attacks (StAdv, AutoAttack l2, AutoAttack l\u221e) on the CIFAR-10 dataset.  The table shows the clean accuracy and robust accuracy (percentage) for each method under each attack.  The average robustness across all three attacks is also provided, along with highlighted best results and state-of-the-art performance. ", "section": "5.2 Comparisons on Unseen Attacks"}, {"figure_path": "BZLdXBjB8O/tables/tables_7_1.jpg", "caption": "Table 2: Clean accuracy and adversarial robustness on CIFAR-10 against StAdv under l\u221e (e = 0.05) norm bound and AutoAttack (AA) under l2 (\u03b4 = 0.5) as well as l\u221e (\u03b4 = 8/255) bound. We calculate the average robustness across three attack methods to evaluate the model\u2019s robustness against unseen attacks. We use underlining to highlight the best robustness for each attack method within each defense category, and bold font to denote the state-of-the-art (SOTA) across all methods.", "description": "This table presents the clean accuracy and adversarial robustness of different defense methods on the CIFAR-10 dataset against three different attack methods: StAdv, AutoAttack with l2 norm, and AutoAttack with l\u221e norm.  The average robustness across the three attacks is also provided, offering a comprehensive evaluation of the models' performance against unseen attacks.  The table highlights the best-performing method for each attack type and the overall state-of-the-art method.", "section": "5 Experiments"}, {"figure_path": "BZLdXBjB8O/tables/tables_8_1.jpg", "caption": "Table 4: Clean and robust accuracy on CIFAR-10 against BPDA + EOT against l\u221e (\u20ac = 8/255) threat model.", "description": "This table presents the clean accuracy and adversarial robustness results on CIFAR-10 dataset for different defense models against the BPDA+EOT attack under the l\u221e threat model (with a bound of 8/255).  The models compared include several state-of-the-art purification methods and the proposed CausalDiff model.  It demonstrates the performance of the CausalDiff model in terms of both clean accuracy and robustness compared to other techniques.", "section": "5.2 Comparisons on Unseen Attacks"}, {"figure_path": "BZLdXBjB8O/tables/tables_20_1.jpg", "caption": "Table 2: Clean accuracy and adversarial robustness on CIFAR-10 against StAdv under l\u221e (e = 0.05) norm bound and AutoAttack (AA) under l2 (\u03b4 = 0.5) as well as l\u221e (\u03b4 = 8/255) bound. We calculate the average robustness across three attack methods to evaluate the model\u2019s robustness against unseen attacks. We use underlining to highlight the best robustness for each attack method within each defense category, and bold font to denote the state-of-the-art (SOTA) across all methods.", "description": "This table presents the clean accuracy and adversarial robustness of various defense methods on the CIFAR-10 dataset against three different attacks: StAdv, AutoAttack with l2 norm, and AutoAttack with l\u221e norm.  The average robustness across the three attacks is also shown.  The best robustness for each attack and the overall state-of-the-art are highlighted.", "section": "5.2 Comparisons on Unseen Attacks"}]