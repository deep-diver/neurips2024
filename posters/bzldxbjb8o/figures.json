[{"figure_path": "BZLdXBjB8O/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of training (Left) and inference (Right) processes of our proposed CausalDiff model. During training, the model constructs a structural causal model leveraging a conditional diffusion model, disentangling the (label) Y-causative feature S and the Y-non-causative feature Z through maximization of the Causal Information Bottleneck (CIB). In the inference stage, CausalDiff first purifies an adversarial example X, yielded by perturbing X according to the target victim model parameterized by \u03b8, to obtain the benign counterpart X*. Then, it infers the Y-causative feature S* for label prediction. We visualize the vectors of S and Z inferred from a perturbed image of a horse using a diffusion model. We find that S captures the general concept of a horse, even when the input image only shows the head, while Z carries information about the horse's skin color.", "description": "This figure illustrates the training and inference processes of the CausalDiff model.  The training process involves creating a structural causal model using a conditional diffusion model to separate label-causative features (S) from label-non-causative features (Z).  The inference process involves purifying an adversarial example, inferring the label-causative features, and making a prediction based on those features.", "section": "4 Causal Diffusion Model"}, {"figure_path": "BZLdXBjB8O/figures/figures_2_1.jpg", "caption": "Figure 2: Adversarial robustness of four models against 100-step PGD attack under varying attack strength indicated by e-budget.", "description": "This figure shows the adversarial robustness of four different models against a 100-step Projected Gradient Descent (PGD) attack. The x-axis represents the attack strength (epsilon budget), and the y-axis represents the adversarial robustness. The four models compared are Discriminative, Generative, Causal without Disentanglement, and Causal with Disentanglement.  The graph illustrates how the robustness of each model changes as the attack strength increases.  It highlights the superior performance of the 'Causal with Disentanglement' model in maintaining robustness even with stronger attacks.", "section": "3 Pilot Study on Toy Data"}, {"figure_path": "BZLdXBjB8O/figures/figures_3_1.jpg", "caption": "Figure 3: Visualizations of feature space for the two categories on toy data by T-SNE for (a) discriminative model, (b) generative model, (c) causal model without disentanglement, and (d) causal model with disentanglement.", "description": "This figure visualizes the feature space of four different models trained on toy data using t-SNE.  Each subplot represents a different model: (a) a discriminative model, (b) a generative model, (c) a causal model without disentanglement, and (d) a causal model with disentanglement. The visualization helps to understand how these models learn to represent the two categories of data, highlighting the effects of disentanglement on the separability of the categories in the feature space.", "section": "3 Pilot Study on Toy Data"}, {"figure_path": "BZLdXBjB8O/figures/figures_8_1.jpg", "caption": "Figure 4: Visualization by T-SNE of the feature space, inferred by our CausalDiff, of the label-causative factor s, label-non-causative factor z, and their concatenation.", "description": "This figure visualizes the feature space learned by CausalDiff using t-SNE.  It shows three separate plots: one for the concatenation of the label-causative factor (s) and the label-non-causative factor (z), one for the label-causative factor (s) alone, and one for the label-non-causative factor (z) alone.  Each point represents a data sample, and the color of each point represents the true class label of the sample. The visualization aims to illustrate the disentanglement of the causal factors achieved by CausalDiff. The plot of the concatenated factors shows a clear separation of the classes. The plot of the label-causative factor shows some separation but less distinct boundaries, while the plot of the label-non-causative factor shows a more uniform distribution of the classes.", "section": "5.4 Visualization of Latent Factors"}, {"figure_path": "BZLdXBjB8O/figures/figures_17_1.jpg", "caption": "Figure 5: SCM of models for pilot study including (a) discriminative model, (b) generative model, (c) causal model without disentanglement, and (d) causal model with disentanglement.", "description": "This figure shows the structural causal models (SCMs) for four different models used in a pilot study on toy data.  These models are compared to evaluate the impact of causal modeling and disentanglement on adversarial robustness. (a) shows a discriminative model where the latent factor V directly influences both X and Y. (b) depicts a generative model where the latent variable V influences X, and X influences Y. (c) illustrates a causal model without disentanglement, where a single latent factor V impacts both X and Y. Finally, (d) presents the causal model with disentanglement, showcasing separate latent factors, S and Z, which independently influence X.  S is the label-causative factor, directly influencing the label Y, while Z is the label-non-causative factor, impacting only X.", "section": "3 Pilot Study on Toy Data"}, {"figure_path": "BZLdXBjB8O/figures/figures_17_2.jpg", "caption": "Figure 1: Illustration of training (Left) and inference (Right) processes of our proposed CausalDiff model. During training, the model constructs a structural causal model leveraging a conditional diffusion model, disentangling the (label) Y-causative feature S and the Y-non-causative feature Z through maximization of the Causal Information Bottleneck (CIB). In the inference stage, CausalDiff first purifies an adversarial example X, yielded by perturbing X according to the target victim model parameterized by \u03b8, to obtain the benign counterpart X*. Then, it infers the Y-causative feature S* for label prediction. We visualize the vectors of S and Z inferred from a perturbed image of a horse using a diffusion model. We find that S captures the general concept of a horse, even when the input image only shows the head, while Z carries information about the horse's skin color.", "description": "This figure illustrates the training and inference processes of the CausalDiff model.  The training process uses a structural causal model with a conditional diffusion model to disentangle label-causative and label-non-causative features. The inference process purifies adversarial examples and infers the label-causative features for prediction.  An example using a horse image is shown to demonstrate the disentanglement of features.", "section": "1 Introduction"}, {"figure_path": "BZLdXBjB8O/figures/figures_18_1.jpg", "caption": "Figure 7: Impact of \u03b7 for disentanglement term in loss function on clean accuracy and robust accuracy.", "description": "This figure shows the impact of the hyperparameter \u03b7 (weight of the ICLUB(S;Z) term in the loss function) on both clean accuracy and robust accuracy.  The x-axis represents different values of \u03b7, while the y-axis shows the accuracy.  The results indicate that an optimal value of \u03b7 exists that balances clean accuracy and robustness against adversarial attacks.  Values of \u03b7 that are too small or too large negatively impact robustness. ", "section": "5.3 Ablation Study"}, {"figure_path": "BZLdXBjB8O/figures/figures_20_1.jpg", "caption": "Figure 8: Reconstruction images X[s*;z*] when given clean example x, where s* and z* are inferred from the clean example x by our CausalDiff; generated images Xs* and Xz* conditioned on s* and z*, respectively; purified image x* utilizing the unconditional diffusion (with s, z masked) when given an adversarial example ; generated images xs* and xz* conditioned on s* and z*, respectively, where s* and z* are inferred from the purified image x*. ", "description": "This figure shows the results of reconstruction and generation of images using the CausalDiff model.  The first column shows the original clean images. The second column shows the reconstruction of these images after being passed through the CausalDiff model, combining the label-causative (s*) and label-non-causative (z*) factors. The third and fourth columns show images generated by the model using only the label-causative (s*) and label-non-causative (z*) factors respectively. The last three columns show the same process, but starting from an adversarially perturbed image (x~) instead of a clean image (x). This demonstrates the model's ability to disentangle and reconstruct images using causal factors, leading to robustness against adversarial attacks.", "section": "5.4 Visualization of Latent Factors"}, {"figure_path": "BZLdXBjB8O/figures/figures_20_2.jpg", "caption": "Figure 9: Distribution of likelihood for adversarial and benign examples across various timesteps t.", "description": "The figure shows the distribution of likelihood for adversarial and benign examples across various timesteps (t) during the diffusion process.  It visually demonstrates how the distributions of adversarial and clean examples differ at different timesteps. The difference is most prominent at smaller timesteps and gradually decreases as the timestep increases, indicating that the adversarial noise is removed most effectively during earlier stages of the diffusion process.", "section": "C.2 Analyses on Core Components of Inference"}]