[{"figure_path": "rnUEUbRxVu/tables/tables_7_1.jpg", "caption": "Table 1: The time cost (millisecond) under different testing lengths, with DDAPE as 32 and default batch size 1, with training length 512.", "description": "This table presents the computational time, in milliseconds, required for different methods (ROPE, T5's bias, Alibi, Kerple, FIRE, and DAPE-Kerple) across various sequence lengths (512, 1024, 2048, 4096, and 8192) for three different model sizes (350M, 2.7B, and 6.7B).  The \"Ratio\" column normalizes the time cost relative to DAPE-Kerple for each model size and sequence length.  This allows for a direct comparison of the computational efficiency of each method relative to DAPE-Kerple. The training length is fixed at 512 for all experiments.", "section": "4.5 The Time Cost"}, {"figure_path": "rnUEUbRxVu/tables/tables_8_1.jpg", "caption": "Table 2: Train on length 40 with 200k steps, and test from lengths 41 to 500. The random accuracy is 50%, except for MODULAR ARITHMETIC (SIMPLE), CYCLE NAVIGATION, BUCKET SORT, SOLVE EQUATION and MODULAR ARITHMETIC, where it is 20%. \u2020\u2020\u2020 denotes permutation-invariant tasks, which are expected to be solved without positional information.", "description": "This table presents the results of experiments conducted on the CHE benchmark.  The models were trained on sequences of length 40 for 200,000 steps and then tested on sequences ranging from length 41 to 500.  The table shows the accuracy achieved by various models, including different positional encoding methods (learned sinusoidal, ROPE, Relative, ALiBi, Kerple, FIRE, and DAPE), on several tasks.  The random accuracy is 50% for most tasks, except for specific tasks noted in the caption.", "section": "4.7 Experiments on CHE Benchmark"}, {"figure_path": "rnUEUbRxVu/tables/tables_16_1.jpg", "caption": "Table 3: Model Configurations.", "description": "This table details the configurations used for the 125M and 350M transformer models in the experiments.  The configurations include training sequence length, batch size, number of iterations, dropout probability, attention dropout probability, number of attention heads, feature dimension, number of layers, optimizer, optimizer parameter betas, learning rate, and precision.  These parameters are crucial for reproducibility and understanding the experimental setup.", "section": "B Model Configuration"}, {"figure_path": "rnUEUbRxVu/tables/tables_17_1.jpg", "caption": "Table 4: The example of different tasks. \u2020\u2020\u2020 denotes permutation-invariant tasks, which are expected to be solved without positional information. More explanation of tasks can be found in [21].", "description": "This table presents examples of tasks from the Chomsky Hierarchy Evaluation Benchmark (CHE) used to evaluate the model's performance.  It categorizes tasks into levels based on their complexity according to the Chomsky Hierarchy (Regular, Context-Free, Context-Sensitive, and Recursively Enumerable).  Each task demonstrates a different type of language processing challenge, with examples provided for input and expected output.  Permutation-invariant tasks (marked with \u2020\u2020\u2020) are those where the order of input elements does not affect the outcome.", "section": "Experiments on CHE Benchmark"}, {"figure_path": "rnUEUbRxVu/tables/tables_18_1.jpg", "caption": "Table 5: The perplexity performances on the Arxiv dataset when the training length is 512 and running with three random seeds.", "description": "This table presents the perplexity scores obtained using various methods (ROPE, T5's bias, Alibi, DAPE-Alibi, Kerple, DAPE-Kerple, FIRE, DAPE-FIRE) on the Arxiv dataset.  The training length was 512, and the results are averaged across three random seeds for various evaluation sequence lengths (512, 1024, 2048, 4096, and 8192). The standard deviation is also provided to indicate variability.", "section": "4.1 Comparisons with Baselines"}, {"figure_path": "rnUEUbRxVu/tables/tables_18_2.jpg", "caption": "Table 6: The performance comparison between data-related position encoding, with dataset Books3 and training length 128.", "description": "This table presents a comparison of the performance of different positional encoding methods, specifically Transformer-XL, CoPE, and DAPE-Kerple, on the Books3 dataset.  The comparison is based on the performance at different sequence lengths (128, 256, 512, 1024, 2048, 4096, and 8192), all using a training sequence length of 128.  The values represent a performance metric (likely perplexity or a similar measure of model performance on the task).", "section": "4.3 Different Variants of DAPE"}, {"figure_path": "rnUEUbRxVu/tables/tables_19_1.jpg", "caption": "Table 7: Books Dataset Results: Train Length 512", "description": "This table presents the perplexity results on the Books3 dataset for different sequence lengths (512, 1024, 2048, 4096, 8192) using three different methods. The baseline method uses the standard query-key multiplication with bias (QKT + B). The second method adds a function f(B) of the bias matrix to improve the model's performance. The third and improved method further incorporates a function f(QKT,B) of the query-key multiplication and bias to adapt positional encoding dynamically, enhancing performance further.", "section": "4.3 Different Variants of DAPE"}, {"figure_path": "rnUEUbRxVu/tables/tables_19_2.jpg", "caption": "Table 8: Model Size and Method Comparison with Training Length 512", "description": "This table compares the performance of different positional encoding methods (ROPE, RPE, Alibi, Kerple, and DAPE-Kerple) across various sequence lengths (512, 1024, 2048, and 4096) using a 2.7B parameter model.  The results demonstrate the performance of each method on longer sequences than those used during training, highlighting how well the positional encoding methods extrapolate to unseen sequence lengths.", "section": "4.3 Different Variants of DAPE"}, {"figure_path": "rnUEUbRxVu/tables/tables_19_3.jpg", "caption": "Table 9: Model Size and Method Comparison with Training Length 512", "description": "This table presents a comparison of the performance of various positional encoding methods (ROPE, RPE, Alibi, Kerple, and DAPE-Kerple) using different model sizes (6.7B parameters). The performance is evaluated based on perplexity scores across various sequence lengths (512, 1024, and 2048 tokens).  The results highlight the relative performance of each method for different model sizes and sequence lengths, demonstrating their scalability and effectiveness in handling longer contexts.", "section": "4.3 Different Variants of DAPE"}]