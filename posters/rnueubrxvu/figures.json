[{"figure_path": "rnUEUbRxVu/figures/figures_1_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the DAPE model for the 8192th query position, considering key positions from 1 to 8192 during training with a sequence length of 512.  It demonstrates that DAPE learns both local and anti-local positional patterns, unlike static methods. The figure displays attention values, Kerple bias (a static method), and DAPE bias for comparison, highlighting the adaptive nature of DAPE.  More examples are available in the appendix.", "section": "3.2 Data-Adaptive Positional Encoding"}, {"figure_path": "rnUEUbRxVu/figures/figures_5_1.jpg", "caption": "Figure 2: Comparisons with baselines: performance with training lengths 128 and 512 on Arxiv and Books3 datasets.", "description": "This figure compares the performance of DAPE against several baselines (NoPE, ROPE, YaRN, Randomized ROPE, T5's bias, Alibi, Kerple, FIRE) across different training lengths (128 and 512) on two datasets (Arxiv and Books3).  The x-axis represents the validation sequence length, while the y-axis shows the validation perplexity.  The figure demonstrates that DAPE consistently outperforms the baselines, particularly at longer validation sequence lengths.", "section": "4.1 Comparisons with Baselines"}, {"figure_path": "rnUEUbRxVu/figures/figures_5_2.jpg", "caption": "Figure 2: Comparisons with baselines: performance with training lengths 128 and 512 on Arxiv and Books3 datasets.", "description": "This figure compares the performance of DAPE against various baselines (ROPE, YaRN, Randomized ROPE, T5's bias, Alibi, Kerple, FIRE) across different validation sequence lengths (128, 256, 512, 1024, 2048, 4096, 8192) for two datasets: Arxiv and Books3.  The training lengths used are 128 and 512. The results are shown as validation perplexity scores.  It visually demonstrates the consistent superior performance of DAPE, particularly DAPE-Kerple, especially in scenarios where the evaluation length exceeds the training length (extrapolation).", "section": "4.1 Comparisons with Baselines"}, {"figure_path": "rnUEUbRxVu/figures/figures_6_1.jpg", "caption": "Figure 4: The effect of model size: for the 350M model, the performance with training lengths 128 and 512 on the Arxiv dataset.", "description": "This figure shows how the performance of the 350M model varies with different training lengths (128 and 512) on the Arxiv dataset, as the validation sequence length increases.  It compares the perplexity scores of various positional encoding methods. This visualization helps understand the impact of model size on the length generalization capability of different positional encoding methods.", "section": "4.2 The Effect of Model Size"}, {"figure_path": "rnUEUbRxVu/figures/figures_7_1.jpg", "caption": "Figure 5: Different variants of DAPE: the DAPE-Kerple performance under different variants. (1) Add_Residual: XWQ(XWK)T + B + f(XWQ(XWK) + B); (2) Concate: XWQ(XWK)\u2122 + f(XWQ(XWK), B); (3) Concate_Residual: XWQ(XWK)T + B + f(XWQ(XWK)T, B).", "description": "This figure compares the performance of different DAPE variants (Add_Residual, Concat, Concat_Residual) against the baseline (Kerple) method on two different training lengths (128 and 512).  It shows the validation perplexity for each method across varying sequence lengths (from 128 to 8192). The results highlight the performance differences between the DAPE variants, illustrating the impact of residual connections and concatenation strategies on the overall model performance.", "section": "4.3 Different Variants of DAPE"}, {"figure_path": "rnUEUbRxVu/figures/figures_17_1.jpg", "caption": "Figure 6: The effect of D<sub>DAPE</sub>: the performance with training lengths 128 and 512 on the Arxiv dataset. The experiments are conducted with Alibi and DAPE-Alibi.", "description": "This figure shows the impact of the hidden dimension (D<sub>DAPE</sub>) in the data-adaptive positional encoding (DAPE) model on the Arxiv dataset.  Two training lengths (128 and 512) are shown. The perplexity, a measure of model performance, is plotted against different validation sequence lengths. Multiple lines representing different D<sub>DAPE</sub> values are compared to a baseline (Alibi) to show how the hidden dimension affects the model's ability to generalize to longer sequences.", "section": "Appendix: The Effect of the Hidden Dimension D<sub>DAPE</sub>"}, {"figure_path": "rnUEUbRxVu/figures/figures_20_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)<sup>T</sup>; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f(XW(XWK)<sup>T</sup>, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the DAPE model for the 8192th query position.  It shows how DAPE (Data-Adaptive Positional Encoding) dynamically adjusts its positional bias based on the input context and learned priors.  The figure highlights that DAPE learns both local and anti-local positional patterns, unlike static methods.  Three components are shown for comparison: the attention values, Kerple bias, and the DAPE bias, demonstrating how DAPE modifies the Kerple bias to incorporate both semantic and positional information.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_21_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method for the 8192th query position.  The training length was 512. The visualization demonstrates that DAPE learns both local and anti-local positional patterns, unlike traditional static methods.  The figure illustrates the attention values, Kerple bias, and DAPE bias, showcasing how DAPE dynamically adjusts the positional encoding based on attention values and fixed priors.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_22_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method for a specific query position (8192th) with key positions ranging from 1 to 8192.  The training length was 512. The visualization shows that DAPE learns both local and anti-local positional patterns, indicating its ability to capture both short-range and long-range dependencies between tokens.  The figure highlights the difference between the attention values, the Kerple bias, and the DAPE bias, showing how DAPE adjusts positional information based on the context.  Additional examples are available in the appendix.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_23_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method for a specific query position (8192th) when the training sequence length is 512. It demonstrates that DAPE learns both local and anti-local positional patterns by showing the attention values, Kerple bias, and DAPE bias for different key positions. The figure highlights DAPE's ability to capture both short-range and long-range dependencies in the input sequence.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_24_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method. It shows the attention weights for the 8192nd query token against all key tokens (1-8192) when trained with a sequence length of 512.  The visualization demonstrates that DAPE learns both local and anti-local positional patterns, indicating its ability to capture both short-range and long-range dependencies.  Three components are shown: attention weights, Kerple bias, and DAPE bias. The DAPE bias is a function of both attention weights and the Kerple bias.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_25_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method for a specific query position (8192th) with key positions ranging from 1 to 8192 during training with a sequence length of 512.  The visualization reveals that DAPE learns both local and anti-local positional patterns, indicating its ability to capture both short-range and long-range dependencies in the input sequences. The three subplots show the attention values, Kerple bias, and DAPE bias, respectively, illustrating how DAPE modifies the positional bias based on the attention mechanism and learned priors.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_26_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method. It shows the attention weights for the 8192th query token against all key positions (1-8192) when the training sequence length was only 512.  The plot demonstrates DAPE's ability to capture both local and long-range dependencies in the input sequences, a key aspect of its adaptive nature.  Three components are shown: The attention values of the model, the Kerple bias (a type of relative positional encoding), and finally, the DAPE bias (the combination of the attention and Kerple bias after being processed by an MLP).  The visualization highlights the DAPE's ability to learn and adjust positional information based on the input context, in contrast to static positional encodings.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_27_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the DAPE model for a specific query position (8192th) with different key positions (1-8192). The training sequence length was 512.  The visualization shows that DAPE captures both local and long-range relationships between positions, unlike traditional static methods.  The three subplots show the attention values, Kerple bias, and DAPE bias, respectively, demonstrating how DAPE adjusts positional biases based on both learned priors and the current attention values. The equations used to train the model are included, and more examples are available in Appendix I.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_28_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method.  It shows how DAPE, when trained on sequences of length 512, learns positional biases for a query token at position 8192, considering key positions from 1 to 8192.  The key takeaway is that DAPE learns both local and anti-local positional patterns, unlike static positional encodings. The visualization uses three subplots: Attention values, Kerple bias, and DAPE bias. Each subplot shows the bias for each of the twelve attention heads of the model, displaying the learned interaction between the query and key positions. The appendix contains additional examples of these learned biases.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_29_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method.  It shows the attention biases for the 8192nd query token interacting with key tokens from positions 1 to 8192, while the model was trained with a sequence length of only 512. The visualization demonstrates that DAPE successfully learns both local and anti-local positional relationships, showcasing its adaptability to longer sequences than those seen during training.  The three panels show the attention values, the Kerple bias (a baseline positional encoding method), and the DAPE bias, respectively. The DAPE bias is generated by an MLP that takes as input both the attention values and the Kerple bias.  The appendix contains additional examples of learned positional biases.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_30_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the DAPE model.  It shows the attention weights between the 8192th query token and all other key tokens (1-8192) during training with a sequence length of 512.  The visualization demonstrates that DAPE learns both short-range (local) and long-range (anti-local) relationships between tokens, a key aspect of its adaptability. The three subplots represent the attention weights, Kerple bias, and the DAPE bias respectively. More detailed examples are provided in the appendix.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_31_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method for a specific query position (8192th) within a sequence.  The training length was 512, but the key positions range from 1 to 8192 to assess length extrapolation capabilities. The visualization shows that DAPE learns both local and anti-local positional relationships, which means it considers the relative positions of tokens both close to and far from the query token.  This is in contrast to traditional methods that mostly rely on local information. The figure also illustrates the attention values, Kerple bias, and DAPE bias.  Appendix I contains additional examples.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_32_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the Data-Adaptive Positional Encoding (DAPE) method for a specific query position (8192th) and its corresponding key positions (1 to 8192) during training with a sequence length of 512.  The visualization shows that DAPE learns both local and anti-local positional patterns, indicating its ability to capture both short-range and long-range dependencies.  The three panels illustrate the attention values, Kerple bias, and DAPE bias, respectively, highlighting how DAPE incorporates the Kerple bias to learn adaptive positional information. Additional examples are available in the appendix.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_33_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the DAPE model. It shows the attention weights between the 8192th query position and all key positions (1-8192) when the model is trained with a sequence length of 512. The visualization demonstrates that DAPE learns both local and anti-local positional patterns, which is different from traditional static positional encodings. The three panels display the attention values, Kerple bias, and DAPE bias respectively. This adaptive learning of positional biases allows DAPE to generalize better to longer sequences.", "section": "3 Method"}, {"figure_path": "rnUEUbRxVu/figures/figures_34_1.jpg", "caption": "Figure 1: Visualization of DAPE learned positional biases for the 8192th query position with key positions between 1 and 8192, while the training length is 512. We notice that DAPE learns both local and anti-local position patterns. The model is trained with Equation 2: (1) The Attention is XWQ(XWK)T; (2) The Kerple bias is B; (3) The DAPE (with Kerple) bias is f (XW(XWK)T, B). More examples are shown in Appendix I", "description": "This figure visualizes the learned positional biases of the DAPE model for the 8192th query position when the training length is 512. It demonstrates that DAPE learns both local and anti-local positional patterns by showing the attention values, Kerple bias, and DAPE bias for different head numbers. The model is trained using Equation 2 which includes attention, Kerple bias, and a function of both. More visualization examples are available in Appendix I.", "section": "3 Method"}]