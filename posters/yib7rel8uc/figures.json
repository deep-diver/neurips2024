[{"figure_path": "YIB7REL8UC/figures/figures_1_1.jpg", "caption": "Figure 1: (Top) Given a hidden data-generating structure, our framework predicts a unique belief state geometry in a probability simplex. Often these have highly nontrivial fractal structure as shown in this example. (Bottom) Our main experimental result is that we find that the fractal geometry of optimal beliefs is linearly embedded in the residual stream, and emerges over the course of training.", "description": "This figure demonstrates the core idea of the paper: that the geometry of belief states, as predicted by the theory of optimal prediction, is linearly represented in the residual stream of transformers. The top panel shows an example of how a hidden data-generating structure (a Hidden Markov Model) leads to a theoretical prediction of a specific belief state geometry in a probability simplex.  The bottom panel illustrates the key experimental finding: the trained transformer's residual stream activations linearly capture this predicted fractal geometry.", "section": "Introduction"}, {"figure_path": "YIB7REL8UC/figures/figures_2_1.jpg", "caption": "Figure 2: An illustration of a Hidden Markov Model (HMM) and its components. The left side shows the HMM with states S0, S1, and SR, and their respective transition probabilities. The right side displays the transition matrices T(\u00b0) and T(1) corresponding to token emissions 0 and 1. Example training data is provided at the bottom, demonstrating a sequence generated by the HMM.", "description": "This figure illustrates a Hidden Markov Model (HMM) with three hidden states (S0, S1, and SR) and its corresponding transition matrices (T(0) and T(1)). The HMM generates sequences of tokens (0 and 1) based on its transition probabilities between the states.  The left side visually represents the HMM's structure, showing the states and transition probabilities between them. The right side shows the transition matrices, which are numerical representations of these probabilities. The bottom part displays an example of a sequence of tokens generated by the HMM, demonstrating how the model produces data.", "section": "2 Theory and methods"}, {"figure_path": "YIB7REL8UC/figures/figures_3_1.jpg", "caption": "Figure 3: (A) An example generative structure called the zero-one-random process (Z1R), since it generates data of the form . . .01R01R01R... where R is a random bit. (B) The generative structure implies a unique metadynamic over belief states as a predictor synchronizes to the hidden state of the world as it observes more context. This predictive structure is called the mixed-state presentation (MSP). We label belief states with \u03b7w where w is the shortest string of emissions which leads to that belief state. (C) Belief states are distributions over generator states and can be embedded in a probability simplex. To read off this distribution for a given belief state, \u03b7, one measures the perpendicular distance from the point to each edge of the simplex, shown as blue, green, and red lines in this example. These distances directly give the probabilities for each state. Thus, the vertices represent states of certainty over one generator state, since the perpendicular distance is nonzero to only one of the edges of the simplex. (D) Plotting the belief state distributions in the probability simplex gives the belief state geometry.", "description": "This figure illustrates the concept of mixed-state presentation (MSP) using the Zero-One-Random (Z1R) process as an example. It demonstrates how a generative model's structure (A) leads to a unique metadynamic of belief state updating (B), which can be visualized geometrically in a probability simplex (C, D).  Panel (C) shows how probabilities for different generator states are read off from the simplex, and panel (D) displays the resulting belief state geometry, a crucial concept for the paper's theoretical framework.", "section": "2 Theory and methods"}, {"figure_path": "YIB7REL8UC/figures/figures_4_1.jpg", "caption": "Figure 4: To verify if transformers represent belief state geometries in their residual streams, we record (A) residual stream activations at all context window positions over all inputs. (B) These activations live in a high dimensional space. (C) Each input has a ground-truth optimal belief state, which is a probability distribution over states of the data-generating process. In this way we can label, or color, each activation by the ground-truth belief associated with the input. (D) Using linear regression we then find a linear subspace of the activation space that best preserves the belief state geometry of the simplex.", "description": "This figure illustrates the methodology for finding belief state geometry in the residual stream of a transformer.  Panel A shows the transformer architecture focusing on the residual stream. Panel B shows the high-dimensional space of residual stream activations, which are then colored according to their corresponding ground-truth belief states (Panel C). Finally, linear regression is used to find a lower-dimensional subspace that best preserves the simplex geometry of the belief states (Panel D).", "section": "2.3 Finding the belief simplex in the residual stream"}, {"figure_path": "YIB7REL8UC/figures/figures_4_2.jpg", "caption": "Figure 5: The residual stream of trained transformers linearly represents the belief state geometry of the mixed-state presentation. (A) The Mess3 Process has 3 hidden states and generates sequences in a token vocabulary of {A, B, C}. (B) The ground truth belief state geometry of the Mess3 Process has intricate fractal structure. Each point in this plot is a belief state\u2014a probability distribution over the hidden states of the Mess3 Process. Points are colored by taking the belief probability distribution and using them as RGB values. (C) We find a linear projection of the final residual stream activations contains a representation of the ground-truth belief geometry. Points are colored according to the ground-truth belief states.", "description": "This figure demonstrates the main experimental result of the paper.  It shows that the fractal geometry of optimal beliefs (predicted by the theory) is linearly embedded within the residual stream of transformers. Panel (A) describes the data-generating process (Mess3 process), panel (B) illustrates the resulting belief state geometry (a fractal pattern in the probability simplex), and panel (C) shows that a linear projection of the transformer's residual stream activations successfully captures this fractal geometry.", "section": "Results"}, {"figure_path": "YIB7REL8UC/figures/figures_5_1.jpg", "caption": "Figure 6: The representation of belief state geometry is nontrivial. (A) Projected activations at different stages of training shows the emergence of belief state geometry. (B) Cross-validation of our main result. (C) We shuffle the belief states in our linear regression procedure, preserving the overall ground-truth fractal shape while getting rid of the associated context. The new projection collapses the data, showing that the fractal's appearance in the residual stream is not an artifact of projecting high-dimensional data to a desired shape. (D) Mean squared error (averaged across input sequences) between (i) the position of projected activations and (ii) the ground-truth position of the corresponding belief state.", "description": "This figure demonstrates the robustness and non-triviality of the findings showing the emergence of belief state geometry in the residual stream of transformers.  Panel (A) shows the progressive emergence of the geometry during training. Panel (B) shows that the results hold up under cross-validation. Panel (C) controls for the possibility that the observed fractal geometry is a trivial consequence of dimensionality reduction by shuffling the belief state labels, showing that the fractal structure is indeed related to the underlying belief state geometry.  Panel (D) quantifies the goodness of fit of the linear regression model used to project the residual stream activations onto the belief state simplex, demonstrating low mean squared error, especially when compared to the shuffled condition.", "section": "3 Results"}, {"figure_path": "YIB7REL8UC/figures/figures_6_1.jpg", "caption": "Figure 7: The belief state geometry can be represented across multiple layers when the belief state structure has next-token degeneracies. (A) The Random-Random-XOR process has zero pairwise correlation, but has interesting higher-order structure. The MSP of this process has 36 distinct states, with many of them degenerate in terms of their optimal next-token predictions. (B) The belief state geometry lies in a 4-simplex. To visualize we project down to 2-dimensions, with each belief state colored uniquely. (C) The transformer linearly represents this belief state geometry. Small dots correspond to individual activations and are colored according to the ground truth belief state. Large dots correspond to the center of mass of all activations associated with a particular ground-truth belief state. (D)We compare Euclidean distances between pairs of ground truth belief states and see if those distances are preserved in the belief state representation in the transformer (left scatter plots), showing good correspondence for both RRXOR and Mess3 processes. The right scatter plots compare distances in ground truth next-token probabilities to distances in the transformer\u2019s belief state representations, showing low correlation for RRXOR (R2 = 0.31) and high correlation for Mess3, indicating that RRXOR belief state geometry is not captured by next-token predictions alone. (E) Mean squared error of the linear regression procedure, applied to individual layers and, at the far right, applied to the concatenation of activations across layers.", "description": "This figure shows that when the belief states have the same next-token prediction, the belief state geometry is represented across multiple layers instead of only the final layer.  The Random-Random-XOR process is used as an example, visualizing its belief state geometry in a 4-simplex and demonstrating that the transformer linearly represents this geometry when considering the activations from all layers concatenated.", "section": "3.2 Belief state geometry represents information beyond next-token prediction"}, {"figure_path": "YIB7REL8UC/figures/figures_12_1.jpg", "caption": "Figure S1: To test the effect of using residual stream activations from before or after the final LayerNorm in our analysis, we compared the belief state geometry representations in both cases. (A) Projection of the final residual stream activations before LayerNorm onto the belief state simplex, as presented in the main paper. (B) The same projection for activations after LayerNorm, showing a qualitatively similar structure. (C) Mean squared error of the linear fit capturing the belief state geometry for both cases. The representation after LayerNorm shows a slightly lower error (0.0003) compared to before LayerNorm (0.0004), indicating that the belief state geometry is preserved and marginally better represented after the LayerNorm operation. These results demonstrate that our findings are robust to the choice of using pre- or post-LayerNorm activations.", "description": "This figure compares the belief state geometry representation using residual stream activations before and after the final LayerNorm.  It shows that the representation is qualitatively similar in both cases, with slightly lower error after LayerNorm, demonstrating the robustness of the findings to this preprocessing step.", "section": "A.1 Supplemental Figures"}, {"figure_path": "YIB7REL8UC/figures/figures_12_2.jpg", "caption": "Figure 6: The representation of belief state geometry is nontrivial. (A) Projected activations at different stages of training shows the emergence of belief state geometry. (B) Cross-validation of our main result. (C) We shuffle the belief states in our linear regression procedure, preserving the overall ground-truth fractal shape while getting rid of the associated context. The new projection collapses the data, showing that the fractal's appearance in the residual stream is not an artifact of projecting high-dimensional data to a desired shape. (D) Mean squared error (averaged across input sequences) between (i) the position of projected activations and (ii) the ground-truth position of the corresponding belief state.", "description": "This figure shows four subplots that demonstrate the robustness and non-triviality of the results in the paper. (A) shows that the fractal structure emerges during training, (B) shows that the results hold up under cross-validation. (C) shows that shuffling the belief states destroys the fractal structure, showing that it's not an artifact of the dimensionality reduction. Finally, (D) shows that the mean squared error between the predicted and true belief state positions decreases over the course of training.", "section": "3 Results"}]