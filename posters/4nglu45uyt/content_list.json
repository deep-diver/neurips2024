[{"type": "text", "text": "Aligning Embeddings and Geometric Random Graphs: Informational Results and Computational Approaches for the Procrustes-Wasserstein Problem ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Mathieu Even Luca Ganassali DI ENS, CRNS, PSL University, INRIA Paris Universit\u00e9 Paris-Saclay, LMO ", "page_idx": 0}, {"type": "text", "text": "JakobMaier D.I ENS, CRNS, PSL University, INRIA Paris ", "page_idx": 0}, {"type": "text", "text": "LaurentMassoulie D.I ENS, CRNS, PSL University, INRIA, MSR-INRIA Joint Centre, Paris ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Procrustes-Wasserstein problem consists in matching two high-dimensional point clouds in an unsupervised setting, and has many applications in natural language processing and computer vision. We consider a planted model with two datasets $X,Y$ that consist of $n$ datapoints in $\\mathbb{R}^{d}$ , where $Y$ is a noisy version of $X$ , up to an orthogonal transformation and a relabeling of the data points. This setting is related to the graph alignment problem in geometric models. In this work, we focus on the euclidean transport cost between the point clouds as a measure of performance for the alignment. We first establish information-theoretic results, in the high $(d\\,\\gg\\,\\log n)$ and low $(d\\,\\ll\\,\\log n)$ dimensional regimes. We then study computational aspects and propose the \u2018Ping-Pong algorithm', alternatively estimating the orthogonal transformation and the relabeling, initialized via a FrankeWolfe convex relaxation. We give sufficient conditions for the method to retrieve the planted signal after one single step. We provide experimental results to compare the proposed approach with the state-of-the-art method of Grave et al. [2019]. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Finding an alignment between high dimensional vectors or across two point clouds of embeddings has been the focus of recent threads of research and has a variety of applications in computer vision, such as inferring scene geometry and camera motion from a stream of images [Tomasi and Kanade, 1992], as well as in natural language processing such as automatic unsupervised translation [Rapp, 1995,Fung,1995]. ", "page_idx": 0}, {"type": "text", "text": "Many practical algorithms proposed for this task view this problem as minimizing the distance across distributions in $\\bar{\\mathbb{R}^{d}}$ . Some approaches are based e.g. on optimal transport and Gromov-Wasserstein distance Alvarez-Melis and Jaakkola [2018] or adversarial learning [Zhang et al., 2017, Conneau et al., 2018]. Another line of methods adapt the iterative closest points procedure (ICP) - originally introduced in Besl and McKay [1992] for 3-D shapes - to higher dimensions Hoshen and Wolf [2018]. Another recent contribution is that of Grave et al. [2019], where a method is proposed to jointly learn an orthogonal transformation and an alignment between two point clouds by alternating the objectives in the corresponding minimization problem. ", "page_idx": 0}, {"type": "text", "text": "To formalize this problem, we consider a Gaussian model in which both datasets $X,Y\\,\\in\\,\\mathbb{R}^{d\\times n}$ (or two point clouds of $n$ datapoints in $\\mathbb{R}^{d}$ ) are sampled as follows. First, $X\\;=\\;(x_{1},...\\,,x_{n})$ is a collection of i.i.d. $\\mathcal{N}(0,I_{d})$ Gaussian vectors, and $Y\\,=\\,(y_{1},...\\,,y_{n})$ is a noisy version of $X=(x_{1},\\ldots,x_{n})\\,$ , up to an orthogonal transformation $Q^{\\star}$ and a relabeling $\\pi^{\\star}:[n]\\rightarrow[n]$ of the data points, that is: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\forall i\\in[n]\\,,\\quad y_{i}=Q^{\\star}x_{\\pi^{\\star}(i)}+\\sigma z_{i}\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "or, in matrix form: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Y=Q^{\\star}X(P^{\\star})^{\\intercal}+\\sigma Z\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $Z=(z_{1},\\dots,z_{n})\\in\\mathbb{R}^{d\\times n}$ is also made of i.i.d. $\\mathcal{N}(0,I_{d})$ Gaussian vectors, $P^{\\star}$ is the permutation matrix associated with some permutation $\\pi^{\\star}$ , and $\\sigma>0$ is the noise parameter. Recovering (in some sense that will be made precise in the sequel) the (unknown) permutation $\\pi^{\\star}$ and orthogonal transformation $Q^{\\star}$ defines the Procrustes-Wasserstein problem (sometimes abbreviated as PW in the sequel), which will be the focus of this study. ", "page_idx": 1}, {"type": "text", "text": "The practical approaches previously mentioned have shown good empirical results and are often scalable to large datasets. However, they suffer from a lack of theoretical results to guarantee their performance or to exhibit regimes where they fail. Model (1) described here above appears to be the simplest one to obtain such guarantees. We are interested in pinning down the fundamental limits of the Procrustes-Wasserstein problem, hence providing an ideal baseline for any computational method to be compared to, before delving into computational aspects. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "$(i)$ We define a planted model for the Procrustes- Wassertein problem and discuss the appropriate choice of metrics to measure the performance of any estimator. Based on these metrics, we establish1: $(i.a)$ information-theoretic results in the high-dimensional $d\\gg\\log n$ regime which was not explored before for this problem; $(i.b)$ new information-theoretic results in the low-dimensional regime $(d\\ll\\log n)$ for our metric of performance (the $L^{2}$ transport cost), which substantially differ from those obtained in Wang et al. [2022] for the overlap.   \n$(i i)$ We study computational aspects and propose the \u201cPing-Pong algorithm', alternatively estimating the orthogonal transformation and the relabeling, initialized via a Franke-Wolfe convex relaxation. This method is quite close to that proposed in Grave et al. [2019] although the alternating part differs. We give sufficient conditions for the method to retrieve the planted signal after one single step.   \n(ii) Finally, we provide experimental results to compare the proposed approach with the stateof-the-art method of Grave et al. [2019]. ", "page_idx": 1}, {"type": "text", "text": "1.1  Discussion and related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "One can check that under the above model (1), the maximum likelihood (ML) estimators of $(P^{\\star},Q^{\\star})$ given $(X,Y)$ is given by: ", "page_idx": 1}, {"type": "equation", "text": "$$\n(\\hat{P},\\hat{Q})\\in\\underset{(P,Q)\\in S_{n}\\times\\mathcal{O}(d)}{\\arg\\operatorname*{min}}\\,\\frac{1}{n}\\|X P^{\\top}-Q^{\\top}Y\\|_{F}^{2}=\\underset{(P,Q)\\in S_{n}\\times\\mathcal{O}(d)}{\\arg\\operatorname*{min}}\\,\\|Q X-Y P\\|_{F}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which is strictly equivalent? to the formulation of the non-planted problem of Grave et al. [2019]. Exactly solving the joint optimization problem (2) is non convex and diffcult in general. However, if $P^{\\star}$ is known then (2) boils down to the following orthogonal Procrustes problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\hat{Q}\\in\\underset{Q\\in\\mathcal{O}(d)}{\\arg\\operatorname*{min}}\\,\\frac{1}{n}\\|X P^{\\star}-Q^{\\top}Y\\|_{F}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "which has a simple closed form solution given by $\\hat{Q}=U V^{\\top}$ where $U S V^{\\top}$ is the singular value decomposition (SVD) of $Y(X P^{\\star})^{\\top}$ (see Schonemann [1966]). Conversely, when $Q^{\\star}$ is known, (2) amounts to the following linear assignment problem (LAP in the sequel): ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{P\\in{\\cal S}_{n}}\\frac{1}{n}\\|X P^{\\top}-Q^{\\star}Y\\|_{F}^{2}=\\arg\\operatorname*{max}_{P\\in{\\cal S}_{n}}\\frac{1}{n}\\langle X P^{\\top},Q^{\\star}Y\\rangle,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "table", "img_path": "4NGlu45uyt/tmp/69e7beacb5ca321cf1f46bfab251471583aff6381f2d7223c2c65f5bbb255904.jpg", "table_caption": ["Table 1: Summary of previous informational results, together with the ones in this paper "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "which can be solved in polynomial time, e.g. in cubic time by the celebrated Hungarian algorithm [Kuhn, 1955], or more efficiently at the price of regularizing the objective and using the celebrated Sinkhorn algorithm [Cuturi, 2013]. ", "page_idx": 2}, {"type": "text", "text": "Previous resultswhen $Q^{\\star}$ is known. As seen above, when $Q^{\\star}$ is known (assume e.g. $Q^{\\star}=I_{d},$ , the Procrustes- Wasserstein problem reduces to a simpler objective, that of aligning Gaussian databases. This problem has been studied by Dai et al. [2019, 2023] in the context of feature matching. Kunisky and Niles- Weed [2022] study the same problem as a geometric extension of planted matching and establish state-of-the-art statistical bounds in the Gaussian model in the low-dimensional ( $[d\\ll$ $\\log n)$ , logarithmic $(d\\sim a\\log n)$ and high-dimensional $(d\\gg\\log n)$ regimes. In particular, they show that exact recovery is feasible in the logarithmic regime $d\\,\\sim\\,a\\log n$ $\\sigma^{2}\\,\\dot{<}\\,\\frac{1}{e^{4/a}\\!-\\!1}$ , and in the high-dimensional regime if $\\sigma^{2}\\,<\\,(1/4\\,-\\,\\varepsilon)\\frac{d}{\\log n}$ . Note that in this problem, there is no computational/statistical gap since the LAP is always solvable in polynomial time. ", "page_idx": 2}, {"type": "text", "text": "Geometric graph alignment. Strongly connected to the Procrustes-Wassertein problem is the topic of graph alignment where the instances come from a geometric model. Wang et al. [2022] investigate this problem for complete weighted graphs. In their setting, given a permutation $\\pi^{\\star}$ on $[n]$ and $n$ i.i.d. pairs of correlated Gaussian vectors $\\left(X_{\\pi^{\\star}(i)},Y_{i}\\right)$ in $\\mathbb{R}^{d}$ with noise parameter $\\sigma$ , they observe matrices $A=X^{\\top}X$ and $B=Y^{\\top}Y$ (i.e all inner products $\\langle X_{i},X_{j}\\rangle$ and $\\langle Y_{i},Y_{j}\\rangle\\,\\!\\,.$ and are interested in recovering the hidden vertex correspondence $\\pi^{\\star}$ . The maximum likelihood estimator in this setting writes ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{arg\\,min}_{P\\in\\mathcal{S}_{n}}\\frac{1}{n}\\|X^{\\top}X P-P Y^{\\top}Y\\|_{F}^{2}=\\operatorname*{arg\\,max}_{P\\in\\mathcal{S}_{n}}\\frac{1}{n}\\langle P^{\\top}X^{\\top}X P,Y^{\\top}Y\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is an instance of the quadratic assignment problem (QAP in the sequel), known to be NP-hard in general, as well as some of its approximations [Makarychev et al., 2014]. In fact, we have the following informal equivalence (see Appendix A for a proof): ", "page_idx": 2}, {"type": "text", "text": "Lemma 1 (Informal). PW and geometric graph alignement are equivalent, that is, one knows how to (approximately) solve the former iff they know how to (approximately) solve the latter. ", "page_idx": 2}, {"type": "text", "text": "Wang et al. [2022] focus on the low-dimensional regime $d=o(\\log n)$ , where geometry plays the most important role (see Remark 1). They prove that exact (resp. almost exact) recovery of $\\pi^{\\star}$ is information-theoretically possible as soon as $\\sigma=o(n^{-2/d})$ (resp. $\\sigma=o(n^{-1/d}))$ . They conduct numerical experiments which suggest good performance of the celebrated Umeyama algorithm [Umeyama, 1988], which is confirmed by a follow-up work by Gong and Li [2024] analyzing the Umeyama algorithm (which is polynomial time in the low dimensional regime $d=o(\\log n),$ in the same setting and shows that it achieves exact (resp. almost exact) recovery of $\\pi^{\\star}$ if $\\sigma=o(d^{-3}n^{-2/d})$ (resp. $\\sigma=o(d^{-3}n^{-1/d}))$ , hence coinciding with the information thresholds up to a $\\mathrm{poly}(d)$ factor. However, their algorithm is of time complexity at least $\\Omega(2^{d}n^{3})$ , which is not polynomial in $d$ . This is why we do not include this method in our baselines. ", "page_idx": 2}, {"type": "text", "text": "We emphasize that our results clearly depart from those obtained in Wang et al. [2022] and Gong and Li [2024], because $(i)$ we are also interested in the high dimensional case $d\\gg\\log n$ ,and $(i i)$ we work with a different performance metric which provides less stringent conditions for the recovery to be feasible, see Section 1.2. A summary of previous informational results together with ours (see also Section 2) is given in Table 1. ", "page_idx": 2}, {"type": "text", "text": "Ontheorthogonaltransformation $Q^{\\star}$ . Generalizing the standard linear assignment problem, our model described above in (1) introduces an additional orthogonal transformation $Q^{\\star}$ acrossthe datasets. This orthogonal transformation can be motivated in the context of aligning embeddings in a high-dimensional space: indeed, the task of learning embeddings is often agnostic to orientation in the latent space. In other words, two point clouds may represent the same data points while having different global orientations. Hence, across different data sets, learning this orientation shift is crucial in order to compare (or align) the point clouds. As an illustration of this fact, Xing et al. [2015] provides empirical evidence that orthogonal transformations are particularly adapted for bilingual word translation. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Discussing the method proposed in Grave et al. [2019]. We conclude this introduction by discussing the work of Grave et al. [2019]. Their proposed algorithm is as follows. At each iteration $t$ given a current estimate $Q_{t}$ of the orthogonal transformation, we sample mini-batches $X_{t},Y_{t}$ of same size $b$ and find the optimal matching $P_{t}$ between $Y_{t}Q_{t}^{\\top}$ and $X_{t}$ , via solving a linear assignment problem of size $b$ . This matching $P_{t}$ in turn helps to refine the estimation of the orthogonal transformation via a projected gradient descent step, and the procedure repeats. This method has the main advantage to be scalable to very large datasets and to perform well in practice ; however, no guarantees are given for this method, and in particular the mini-batch step which can justifiably raise some concerns. Indeed, since $X_{t}=(x_{t,j})_{j\\in[b]}$ and $Y_{t}=(y_{t,j})_{j\\in[b]}$ are chosen independently, if $b\\ll{\\sqrt{n}}$ it is likely that for any matching $\\pi_{t}$ the pairs $(x_{t,j},y_{t,\\pi_{t}(j)})$ always correspond to disjoint pairs, and thus aligning $Y_{t}Q_{t}^{\\top}$ and $X_{t}$ does not reveal any useful information about the true $P^{\\star}-\\mathrm{this}$ is even more striking when the data is non-isotropic. ", "page_idx": 3}, {"type": "text", "text": "1.2  Problem setting and metrics of performance ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Notations. We denote by $\\ensuremath{\\boldsymbol{X}}\\sim\\mathcal{N}(\\ensuremath{\\boldsymbol{\\mu}},\\ensuremath{\\boldsymbol{\\Sigma}})$ with $\\boldsymbol{\\mu}\\,\\in\\,\\mathbb{R}^{d}$ and $\\Sigma\\,\\in\\,\\mathbb{R}^{d\\times d}$ the fact that $X$ follows a Gaussian distribution in $\\mathbb{R}^{d}$ of mean $\\mu$ and covariance matrix $\\Sigma$ If $\\mu=0$ and $\\Sigma=I_{d}$ , variable $X$ is called standard Gaussian. We denote by $O(d)$ the orthogonal group in dimension $d$ , and by $\\textstyle S_{n}$ the group of permutations on $[n]$ . Throughout, $\\|\\cdot\\|$ and $\\langle\\cdot\\rangle$ are is the standard euclidean norm and scalar product on $\\mathbb{R}^{d}$ , and $\\|\\cdot\\|_{F}$ and ${\\|\\cdot\\|}_{o p}$ are respectively the Frobenius matrix norm and the operator matrix norm. The spectral radius of a matrix $A$ is denoted $\\rho(A)$ . In all the proofs, quantities $c_{i}$ where $i$ is an integer are unspecified constants which are universal, that is independent from the parameters. Finally, all considered asymptotics are when $n\\to\\infty$ . Note that $d$ also depends on $n$ . An event is said to hold with high probability $(w.h.p.)$ if its probability tends to 1 when $n$ goes to $\\infty$ ", "page_idx": 3}, {"type": "text", "text": "Problem setting and performance metrics. We work with the planted model as introduced in (1) and recall that our goal is to recover the permutation $\\pi^{\\star}$ and the orthogonal matrix $Q^{\\star}$ from the observationof $X$ and $Y$ ", "page_idx": 3}, {"type": "text", "text": "Performance metrics. Previous works measure the performance of an estimator $\\hat{\\pi}$ of a planted relabeling $\\pi^{\\star}$ via the overlap: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{ov}(\\pi,\\pi^{\\prime}):=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}_{\\{\\hat{\\pi}(i)=\\pi^{\\prime}(i)\\}}\\;,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "defined for any two permutations $\\pi,\\pi^{\\prime}$ . This is an interesting metric when we have no hierarchy in the errors, that is when only the true match is valuable, and all wrong matches cost the same. However, this discrete measure does not take into account the underlying geometry of the model. A performance metric which is more adapted to our setting is the $L^{2}$ transport cost between the point clouds. The natural intuition is that a mismatch is less costy if it corresponds to embeddings which are in fact close in the underlying space. We define ", "page_idx": 3}, {"type": "equation", "text": "$$\nc^{2}(\\pi,\\pi^{\\prime})=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{\\pi(i)}-x_{\\pi^{\\prime}(i)}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for any two permutations $\\pi,\\pi^{\\prime}$ . Note that this cost can also be written in matrix form as $c^{2}(P,P^{\\prime})=$ $\\left\\|(P-P^{\\prime})X^{\\top}\\right\\|_{F}^{2}$ . From this form it is clear that, as stated before, $c^{2}(P,P^{\\prime})$ is nothing but the euclidean transport cost for aligning $X P^{\\top}$ onto $X(P^{\\prime})^{\\top}$ . Note that these two measures, ov and $c^{2}$ \uff0c are also well-defined3 when $P,P^{\\prime}$ are more general (and in particular when they are bistochastic matrices). Finally, we measure the performance for the estimation of $Q^{\\star}$ via the Frobenius norm: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell^{2}(Q,Q^{\\prime})=\\left\\|Q-Q^{\\prime}\\right\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "defined for any two orthogonal matrices $Q,Q^{\\prime}$ ", "page_idx": 4}, {"type": "text", "text": "Comparison between metrics.  For a Haar-distributed matrix $Q$ on $O(d)$ , we have that $\\mathbb{E}\\left[\\ell^{2}(Q,Q^{\\star})\\right]\\,=\\,2d$ while for $\\pi$ sampled uniformly from the set of all permutations, we have $\\mathbb{E}\\left[c^{2}(\\hat{\\pi},\\pi^{\\star})\\right]=2d(1-1/n)$ and $\\mathbb{E}\\left[\\mathrm{ov}(\\hat{\\pi},\\pi^{\\star})\\right]=1/n$ . Hence, some estimators $\\hat{\\pi},\\hat{Q}$ of $\\pi^{\\star},Q^{\\star}$ will perform well in our metrics if they can achieve $\\ell^{2}(Q,Q^{\\star})\\leqslant\\varepsilon d$ , and $c^{2}(\\pi,\\pi^{\\star})\\leqslant\\varepsilon d$ for some small (possibly vanishing) $\\varepsilon>0$ ", "page_idx": 4}, {"type": "text", "text": "Depending on dimension $d$ , similarity measures given by $c^{2}$ and the overlap can behave differently or coincide. In the case where $d$ is small, and thus plays a very important role, ov and $c^{2}$ have very different behaviors, and lead to very different results. In particular, there is a wide regime in which inferring $\\pi^{\\star}$ for the overlap sense is impossible, but reachable in the transport cost sense, see Section 2. ", "page_idx": 4}, {"type": "text", "text": "For any fixed permutation $\\pi$ we have that $\\mathbb{E}\\left[c^{2}(\\pi,\\pi^{\\star})\\right]=2d(1-\\operatorname{ov}(\\pi,\\pi^{\\star}))$ , where the mean is taken with respect to the randomness of $X$ . We also have the basic deterministic inequality ", "page_idx": 4}, {"type": "equation", "text": "$$\nc^{2}(\\pi,\\pi^{\\star})\\leqslant(1-\\operatorname{ov}(\\pi,\\pi^{\\star}))\\times\\operatorname*{sup}_{(i,j)\\in[n]^{2}}\\left\\|x_{i}-x_{j}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus, as long as $\\operatorname*{sup}_{(i,j)\\in[n]^{2}}\\left\\|x_{i}-x_{j}\\right\\|^{2}\\ =\\ O(d)$ , an estimator $\\hat{\\pi}$ with good overlap $\\left(1\\mathrm{~-~}\\right)$ $\\operatorname{ov}(\\pi,\\pi^{\\star})\\;\\leqslant\\;\\varepsilon)$ also has a good $c^{2}$ cost $(c^{2}(\\pi,\\pi^{\\star})\\:=\\:O(\\varepsilon d))$ . However, this required control sup(i,j)\u2208[n]2 $\\left\\|x_{i}-x_{j}\\right\\|^{2}=O(d)$ only holds as long as $d\\gg\\log(n)$ ", "page_idx": 4}, {"type": "text", "text": "The blessing of large dimensions lead to an equivalence between the discrete metric ov, and the continuous transport metric $c^{2}$ .We gather several important points highlighting the dichotomy between small and large dimensions for our problem in the following remark. ", "page_idx": 4}, {"type": "text", "text": "Remark 1. On the blessings of large dimensions for our problem: ", "page_idx": 4}, {"type": "text", "text": "1. For any open ball $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ of radius $\\varepsilon\\ \\geq\\ 0$ denoting. $\\raisebox{\\depth}{\\(\\chi\\)}=\\ \\{\\v{x}_{i},i\\in[n]\\}$ .we have that $\\mathbb{P}\\left(\\mathcal{B}\\cap\\mathcal{X}=\\emptyset\\right)~\\rightarrow~1$ if $d~\\gg~\\log(n)$ while if $d~\\ll~\\log(n)$ then for all $M\\ >\\ 0,$ $\\mathbb{P}\\left(|B\\cap\\mathcal{X}|\\geqslant M\\right)\\rightarrow1$ In small dimensions, any fixed non-empty ball will contain infinitely manypointsof $\\mathcal{X}$ as n increases, while in large dimensions these points are separated and any fixed ball will contain no such points $w.h.p$   \n2. For $d\\gg\\log(n)$ matrix $X/{\\sqrt{d}}$ satisfies the restricted isometry property ICandes, 2008].   \n3. For $d\\,\\gg\\,\\log(n)$ . the overlap and the transport cost metrics are equivalent: there exist numerical constants $\\alpha,\\beta~>~0$ such that $w.h.p$ ,for all permutation matrices $\\pi,\\pi^{\\prime}$ \uff0c $\\alpha c^{2}(\\pi,\\pi^{\\prime})\\leqslant2d(1-\\mathrm{ov}(\\pi,\\pi^{\\prime}))\\leqslant\\beta c^{2}(\\pi,\\pi^{\\prime})$ ", "page_idx": 4}, {"type": "text", "text": "Organization of the rest of the paper Section 2 is dedicated to our informational results, giving their essential content as well as the main ideas on the proofs. We next discuss in Section 3 some computational results, introducing the Ping-Pong algorithm, and presenting our numerical experiments. ", "page_idx": 4}, {"type": "text", "text": "2 Informational results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The substantial theoretic part of the paper stands in the informational results obtained for the Procrustes-Wasserstein problem which we describe hereafter. ", "page_idx": 4}, {"type": "text", "text": "2.1 High dimensions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In the high-dimensional case when $\\log n\\ll d$ (and $d\\log d\\ll n)$ 0, our results - Theorem 1 below - imply that if $\\sigma\\rightarrow0$ then the ML estimators defined in (2) satisfy w.h.p. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{ov}(\\pi^{\\star},\\hat{\\pi})=1-o(1),\\ c^{2}(\\hat{P},P^{\\star})=o(d),\\ \\mathrm{and}\\ \\ell^{2}(\\hat{Q},Q^{\\star})=o(d),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "that is one can infer $\\pi^{\\star}$ and $Q^{\\star}$ almost exactly, for all introduced metrics, as soon as $\\sigma\\rightarrow0$ ", "page_idx": 4}, {"type": "text", "text": "Note that this is the first result in the high-dimensional regime for the Procrustes Wassertein problem: Kunisky and Niles-Weed [2022] also considered this regime but only for the LAP problem (that is recovering $\\pi^{\\star}$ when $Q^{\\star}$ in known), and the only existing results for geometric graph alignment Wang et al. [2022], Gong and Li [2024] do not consider this high dimensional case. Our result thus complements the existing picture and shows that almost exact recovery is feasible under the loose assumption $\\sigma\\rightarrow0$ , in the $c^{2}$ and the overlap sense, since these metrics are equivalent in large dimensions (see Remark 1). Our result is in fact more specific and only requires $d\\geqslant2\\log n$ We prove the following Theorem: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Assume that $d\\geqslant2\\log n$ There exists universal constants $c_{1},c_{2},c_{3}>0$ so that for $n$ large enough, with probability $1-o(1)$ the ML estimators defined in (2) satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{ov}(\\pi^{\\star},\\hat{\\pi})\\geqslant1-\\operatorname*{max}\\left(60\\sigma^{2},c_{1}\\frac{d}{n},c_{2}\\frac{\\log n}{d\\log d}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\ell^{2}(Q^{\\star},\\hat{Q})}{2d}\\leqslant c_{1}\\frac{d}{n}+c_{2}\\sigma^{2}+c_{3}\\operatorname*{max}\\left(\\frac{d\\log n}{n},\\sqrt{\\frac{\\log n}{n}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof of Theorem 1 is detailed in Appendix $\\mathbf{C}$ and builds upon controlling the probability of existence of a certain subset of indices $\\mathcal{K}(\\hat{Q},\\hat{\\pi},Q^{\\star})$ of vectors with prescribed properties in order to show that $\\pi^{\\star}$ can be recovered. We apply standard concentration inequalities to control the previous probability. The $d\\geqslant2\\log(n)$ assumption is crucial here since it allows the union bound over ${\\mathcal{S}}_{n}$ to work. ", "page_idx": 5}, {"type": "text", "text": "2.2  Low dimensions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the low-dimensional case when $d\\ll\\log n$ , Theorem 2 below implies that if $\\sigma=o(d^{-1/2})$ then there exist estimators $\\hat{\\pi},\\hat{Q}$ that satisfy w.h.p. ", "page_idx": 5}, {"type": "equation", "text": "$$\nc^{2}(\\hat{P},P^{\\star})=o(d),\\mathrm{~and~}\\ell^{2}(\\hat{Q},Q^{\\star})=o(d)\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "that is, one can approximate $\\pi^{\\star}$ (in the $c^{2}$ sense only) and $Q^{\\star}$ as soon as $\\sigma\\,=\\,o(d^{-1/2})$ . This is of course to be put in contrast with the previous results on geometric graph alignment in this lowdimensional regime: for almost exact recovery in Wang et al. [2022] in the overlap sense, we need $\\sigma=o(n^{-1/d})$ , which is far more restrictive than $\\sigma=o(d^{-1/2})$ as soon as $d\\log(d)<\\log n$ that is nearly in the whole low dimensional regime when $d\\ll\\log n$ . In particular, since the rates of Wang et al. [2022] are sharp when $d$ is of constant order, in order to approximate $\\pi^{\\star}$ in the overlap sense it is necessary to have $\\sigma$ to decreasing polynomially (at rate $1/n^{1/d})$ to 0, whereas approximating $\\pi^{\\star}$ in the transport cost sense requires only $\\sigma=o(1)$ ", "page_idx": 5}, {"type": "text", "text": "There is no contradiction here, since we recall that the $c^{2}$ metric and the overlap are not equivalent in small dimensions: let us give a few more insights on this. This scaling $n^{-1/d}$ comes from the fact that in small dimensions, points of the dataset are close to each other, and the order of magnitude between some $x_{i}$ and its closest point in the dataset scales exactly as $n^{-1/d}$ : if the noise is smaller than this quantity, one should be able to recover the planted permutation. However, when it comes to considering the $c^{2}$ metric, matching $i$ with $j$ such that $\\overline{{||x_{i}-x_{j}||^{2}}}\\ll d$ is sufficient, thus suggesting that recovering a permutation with small $c^{2}$ cost and recovering $Q^{\\star}$ with small Frobenius norm error should be achievable even with large $\\sigma$ (i.e., that does no tend to O as $n$ increases). ", "page_idx": 5}, {"type": "text", "text": "Our main theorem for low dimensions is as follows. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Let $\\delta_{0}\\in(0,1)$ .There exist estimators $\\hat{\\pi},\\hat{Q}$ of $\\pi^{\\star}$ \uff0c $Q^{\\star}$ such that if for some numerical constants $C_{1},C_{2}>0$ we have $\\sigma\\leqslant C_{1}\\delta_{0}^{2}d^{-1/2}$ and $\\log(n)\\geqslant C_{2}d\\log(1/\\delta_{0})$ then: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{c^{2}(\\hat{\\pi},\\pi^{\\star})}{2d}\\leqslant\\delta_{0}\\quad a n d\\quad\\frac{\\ell^{2}(\\hat{Q},Q^{\\star})}{2d}\\leqslant\\delta_{0}\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A refined version of Theorem 2, namely Theorem 3, is proved in Appendix D. We emphasize that the estimators considered in Theorem 2 are not the ML estimators: recall that the strategy to analyse the former as rolled out for Theorem 1 required the union bound over $\\ensuremath{\\boldsymbol{S}}_{n}$ to work. This drastically fails when $d\\ll\\log(n)$ . Hence, we will instead focus on an estimator that takes advantage of the fact that $d$ is small, and show that even in small dimensions, the signal-to-noise ratio $\\sigma$ does not need to decrease with $n$ ", "page_idx": 5}, {"type": "text", "text": "Let us first describe the intuition behind the estimators $\\hat{\\pi},\\hat{Q}$ When $d=1$ \uff0c $Q^{\\star}=\\pm1$ and a simple strategy to recover $Q^{\\star}$ is to count the number $N_{+}({\\mathcal{X}})$ \uff0c $N_{-}(\\mathcal{X})$ (resp. $N_{+}(\\mathcal{D}),N_{-}(\\mathcal{D}))$ of positive and negative $x_{i}$ (resp. positive and negative $y_{j}$ :if $N_{+}({\\mathcal{X}})$ and $N_{+}(\\mathcal{Y})$ are close, then we output $\\hat{Q}\\,=\\,+1$ , whereas if $N_{+}({\\mathcal{X}})$ and $N_{-}(\\mathscr{y})$ are close, then $\\hat{Q}\\,=\\,-1$ . In dimension $d$ , an analog strategy can be applied at the cost of looking in all relevant directions, and the number of such directions is exponentially big in $d$ . Our strategy is thus as follows. We compute the number of points that lie in a given cone $\\mathcal{C}(u,\\delta)$ of given angle $\\delta$ and direction $u$ . Then, we estimate $Q^{\\star}$ by the orthogonal transformation $\\hat{Q}$ which makes the number of $y_{j}$ in $\\mathcal{C}(u,\\delta)$ closest to the number of $x_{j}$ in $\\mathcal{C}(\\hat{Q}u,\\delta)$ , for any direction $u$ . Note that this approach heavily relies on the small dimension assumption $d\\ll\\log n$ : in this case, for any constant $\\delta$ , all theses cones contain w.h.p. a large number of points (tending to $\\infty$ with $n$ ), which does not hold anymore when $d\\gg\\log n$ ", "page_idx": 6}, {"type": "text", "text": "For $\\delta\\,>\\,0$ and $u\\,\\in\\,S^{d-1}$ , let ${\\mathcal C}(u,\\delta)\\,:=\\,\\left\\{v\\in\\mathbb{R}^{d}\\,\\vert\\,\\langle u,v\\rangle\\geqslant(1-\\delta)\\Vert v\\Vert\\right\\}$ be thecone of angle $\\delta$ centered around $u$ . Let $\\mathcal{X}:=\\{x_{i},i\\in[n]\\}$ $\\dot{\\mathcal{V}}:=\\{y_{i},i\\in[n]\\}$ .We now introduce the following sets, for some $\\kappa>0$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{C}_{\\boldsymbol{x}}(\\boldsymbol{u},\\boldsymbol{\\delta}):=\\boldsymbol{\\mathcal{X}}\\cap\\mathcal{C}(\\boldsymbol{u},\\boldsymbol{\\delta})\\cap\\mathcal{B}(0,1/\\boldsymbol{\\kappa})^{C}\\quad\\mathrm{and}\\quad\\mathcal{C}_{\\boldsymbol{y}}(\\boldsymbol{u},\\boldsymbol{\\delta}):=\\mathcal{Y}\\cap\\mathcal{C}(\\boldsymbol{u},\\boldsymbol{\\delta})\\cap\\mathcal{B}(0,\\sqrt{1+\\sigma^{2}}/\\boldsymbol{\\kappa})^{C}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $B(0,r)^{C}$ contains all vectors in $\\mathbb{R}^{d}$ of norm larger than or equal to $r$ .Theroleof $\\kappa>0$ is to prevent side effects: indeed, since the cones are centered at the origin, points that are too close to 0 fall into cones with arbitrary directions and are not informative for the statistics we want to compute. ", "page_idx": 6}, {"type": "text", "text": "Now, for some $p\\geqslant1$ and directions $u_{1},\\dotsc,u_{p}\\in S^{d-1}$ to be set later, we define the following conical alignment loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall Q\\in\\mathcal{O}(d)\\,,\\quad F(Q)=\\frac{1}{p}\\sum_{k=1}^{p}\\left(|\\mathcal{C}_{\\mathcal{X}}(Q u_{k},\\delta)|-|\\mathcal{C}_{\\mathcal{Y}}(u_{k},\\delta)|\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The estimator $\\hat{Q}$ in Theorem 2 is then defined as a minimizer of the conical alignment loss over a finite set ${\\mathcal{N}}\\subseteq{\\mathcal{O}}(d)$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{Q}\\in\\operatorname*{arg\\,min}_{Q\\in\\mathcal{N}}F(Q)\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{N}$ will further be some $\\varepsilon$ -netof $O(d)$ , while $\\hat{\\pi}$ is then obtained by a LAP as in (10) ", "page_idx": 6}, {"type": "text", "text": "2.3From $P^{\\star}$ to $Q^{\\star}$ and vice versa ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our proofs, we often prove that one of the estimators $\\hat{P}$ Or $\\hat{Q}$ performs well in order to deduce that both perform well. This is thanks to the following two results, proved in Appendix B.1 and B.2. ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 (From $\\hat{Q}$ to $\\hat{P}$ 0. Let $\\begin{array}{r l r}{\\delta}&{{}\\in}&{(0,1/2)}\\end{array}$ \uff1aAssume that there exists $\\hat{Q}$ that  is $\\sigma(\\{x_{1},\\ldots,x_{n},y_{1},\\ldots,y_{n}\\})$ -measurable such that $\\ell_{\\mathrm{ortho}}^{2}(\\hat{Q},Q^{*})\\;:=\\;\\|\\hat{Q}-Q^{*}\\|^{2}\\;\\leqslant\\;\\delta d$ There exist constants $C_{1},C_{2},C_{3}>0$ such that with probability at least $1-2e^{-n d}-2e^{-(d^{2}+\\sqrt{n})}$ \uff0c ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{\\pi}}\\in\\operatorname{argmin}_{\\pi\\in S_{n}}\\frac{1}{n}\\sum_{i=1}^{n}\\left\\lVert\\boldsymbol{x}_{\\pi(i)}-\\hat{\\boldsymbol{Q}}^{\\top}\\boldsymbol{y}_{i}\\right\\rVert^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "that can be computed in polynomial time (complexity $O(n^{3})_{.}$ ) as the solution of a $L A P,$ satisfies: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{c^{2}(\\hat{\\pi},\\pi^{\\star})}{d}\\leqslant C_{1}\\delta+C_{2}\\sigma^{2}+C_{3}\\operatorname*{max}\\left(\\frac{d\\ln(1/\\delta)}{n},\\sqrt{\\frac{\\ln(1/\\delta)}{n}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 3 (From $\\hat{P}$ to $\\hat{Q},$ . Let $\\begin{array}{r l r}{\\delta}&{{}\\in}&{(0,1/2)}\\end{array}$ . Assume that there exists $\\hat{\\pi}$ that  is $\\sigma(\\{x_{1},\\ldots,x_{n},y_{1},\\ldots,y_{n}\\})$ -measurablesuchthat $c^{2}(\\hat{\\pi},\\pi^{\\star})\\,\\leqslant\\,\\delta d$ Let $\\hat{Q}$ be the solution to the following optimization problem: There exist constants $C_{1},C_{2},C_{3}>0$ such that with probability at least 1 - 2e-nd - 2e-(d\u00b2+Vn), ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{Q}\\in\\underset{Q\\in\\mathcal{O}(d)}{\\arg\\operatorname*{min}}\\,\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{\\hat{\\pi}(i)}-Q^{\\top}y_{i}\\right\\|^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "that can be computed in closed form with an SVD of $X Y^{\\top}$ ,satisfies: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\ell_{\\mathrm{ortho}}^{2}(\\hat{Q},Q^{\\star})}{d}\\leqslant C_{1}\\delta+C_{2}\\sigma^{2}+C_{3}\\operatorname*{max}\\left(\\frac{d\\ln(1/\\delta)}{n},\\sqrt{\\frac{\\ln(1/\\delta)}{n}}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "3  Computational aspects ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The estimators provided this far in Section 2, namely the joint minimization in $P$ and $Q$ in (2) and the minimizer of the conical alignment loss in (9) are of course not poly-time in general. In this section, we are interested in computational aspects of the problem. ", "page_idx": 7}, {"type": "text", "text": "3.1  Convex relaxation and Ping-Pong algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Estimating $P^{\\star}$ can be made via solving the QAP (5), that can be convexified into the relaxed quadratic assignment problem (relaxed QAP): ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{P}_{\\mathrm{relaxed}}\\in\\underset{P\\in\\mathcal{D}_{n}}{\\arg\\operatorname*{min}}\\,\\frac{1}{n}\\|X^{\\top}X P-P Y^{\\top}Y\\|_{F}^{2}\\,,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\mathcal{D}_{n}$ is the polytope of bistochastic matrices, which is the convex envelope of the set of permutation matrices. Note that unlike in (5), this argmin is not necessarily equal to arg $\\operatorname*{max}_{P\\in\\mathcal{D}_{n}}\\langle P^{\\top}X^{\\top}X P,Y^{T}Y^{\\prime}\\rangle$ since $\\mathcal{D}_{n}$ contains non-orthogonal matrices. ", "page_idx": 7}, {"type": "text", "text": "The estimate $\\hat{P}_{\\mathrm{relaxed}}$ gives a first estimate to then perform alternate minimizations in $Q$ through an SVD - see (11) - and $P$ through a LAP - see (1o). Combining an initialization with convex relaxation, computed via Frank-Wolfe algorithm [Jaggi, 2013] and the alternate minimizations in $P$ and $Q$ yields the Ping-Pong algorithm. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 1: PING-PONG ALGORITHM ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input: Number of Frank- Wolfe steps $T$ , number of alternate-minimization steps $K$ $\\begin{array}{r}{\\tilde{P}_{0}=\\frac{{11}^{\\top}}{n}}\\end{array}$   \n1 for $k=0$ to $T-1$ do Compute $S_{k}=\\arg\\operatorname*{min}_{P\\in\\mathcal{S}_{n}}\\langle P,\\nabla f(\\tilde{P}_{k})\\rangle$ (LAP), where $f(P)=\\left\\|X^{\\top}X P-P Y^{\\top}Y\\right\\|_{F}^{2}$   \n1 Pe+1= (1-k)P+Skfork=2k   \n4 $P_{0}=\\tilde{P}_{T}$ and $Q_{0}=I_{d}$   \n5 for $k=0$ to $K-1$ do   \n6 $Q_{k+1}=U_{k}V_{k}^{\\top}$ for $Y P_{k}X^{\\top}=U_{k}D_{k}V_{k}$ the SVD of $Y P_{k}X^{\\top}$ (Ping) $P_{k+1}\\in\\arg\\operatorname*{max}_{P\\in S_{n}}\\langle P,Y^{\\top}Q_{k+1}X\\rangle$ (LAP) (Pong) Output: $P_{K},Q_{K}$ ", "page_idx": 7}, {"type": "text", "text": "Algorithm 1 is structurally similar to Grave et al. [2019]'s algorithm, as explained in the introduction. The difference lies in the steps in Lines 6-7 of Algorithm 1: while Grave et al. [2019] perform projected gradient steps, our approach is more greedy and directly minimizes in each variable. Both approaches are experimentally compared in Section 3.3. ", "page_idx": 7}, {"type": "text", "text": "3.2  Guarantees for one step of Ping-Pong algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Providing statistical rates for the outputs of Algorithm 1 is a challenging problem for two reasons. First, relaxed QAP is not a well-understood problem: the only existing guarantees in the literature are for correlated Gaussian Wigner models in the noiseless case (i.e., $\\sigma=0$ in our model) [Valdivia and Tyagi, 2023], while for correlated Erdos-R\u00e9nyi graphs, the relaxation is known to behave badly in general [Lyzinski et al., 2016]. Secondly, studying the iterates in lines 6 and 7 of the algorithm is challenging, since these are projections on non-convex sets. While Lemmas 2 and 3 show that if $P_{k}$ (resp. $Q_{k}$ )has small $c^{2}$ loss, then $Q_{k+1}$ has small $\\ell^{2}$ loss (resp. $P_{k+1}$ has small $c^{2}$ loss), showing that there is a contraction at each iteration $\\mathbf{\\hat{a}}$ la Picard's fix-point Theorem\u2019 remains out of reach for this paper. We thus resort to proving that one single step of Algorithm 1 ( $K=T=1$ )can recover the planted signal, provided that the noise $\\sigma$ is small enough. ", "page_idx": 7}, {"type": "text", "text": "Proposition 1. There exists $C>0$ such that for any $\\delta\\in(0,1)$ f $\\sigma\\leqslant n^{-\\frac{13}{\\delta}}$ , then the permutation $\\hat{\\pi}$ associated to the outputs $\\hat{\\pi},\\hat{Q}$ of Algorithm $^{\\,I}$ for $K=T=1$ satisfies, with probability $1-1/n$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{ov}(\\pi^{\\star},\\hat{\\pi})\\geqslant1-\\delta\\,.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "4NGlu45uyt/tmp/41b8709f86bb00fd08e85e607b2321e7dcca8192b2fb2d153ac0702869a38f9d.jpg", "img_caption": ["Figure 1: Influence of the parameters (dimensions $d$ numberof points $n$ , and noise level $\\sigma$ ) on the accuracy (in terms of overlap) of three different estimators: the relaxed QAP estimator (12) projected on the set of permutation matrices (blue curve), the output of Alg. 1 (red curve), and the output of Grave et al. [2019]'s algorithm (purple curve). Each dot corresponds to averaging scores over 10 experiments. Figure la: $\\sigma=0.34,n=100$ . Figure 1b: $\\sigma=0.34,d=5$ . Figures 1c and 1d: $n=200$ \uff0c $d=2$ and $d=60$ respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In the high-dimensional setting $(d\\gg\\log(n),$ , there exist some constants $c_{1},c_{2}$ such that if $\\stackrel{\\cdot}{\\sigma}\\leqslant n^{-c_{1}}$ then $\\hat{\\pi}$ satisfies $w.h.p$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname{ov}(\\pi^{\\star},\\hat{\\pi})\\geqslant1-c_{2}\\operatorname*{max}\\left(\\sqrt{\\frac{d\\log(d)}{n}+\\frac{\\log(n)}{d}},\\frac{d\\log(d)}{n}+\\frac{\\log(n)}{d}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Thus, for $\\sigma$ polynomially small in $n$ and exponentially small in $1/\\delta$ , one step of Alg.1 recovers $\\pi^{\\star}$ in the overlap sense with error $\\delta$ . In large dimensions, this is improved, since $\\sigma$ is no longer required to be exponentially small as the target error decreases to zero. Proof of Proposition 3 is given in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "3.3  Numerical experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare in Figure 1 our Alg. 1 with $(i)$ the naive initialization of the relaxed QAP estimator (12), and $(i i)$ the method in Grave et al. [2019]. The curve \u2018relaxed QAP via FW' is obtained by computing the relaxed QAP estimator with Frank-Wolfe algorithm with $T=1000$ steps,enough for convergence. This estimator is then taken as initialization for Alg. 1 and Grave et al. [2019]'s algorithm, that are both taken with the same large number of steps ( $K=100$ , empirically leading to convergence to stationary points of the algorithms). For fair comparison, we take full batches in Grave et al. [2019] (smaller batches lead to even worse performances). ", "page_idx": 8}, {"type": "text", "text": "Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We establish new informational results for the Procrustes-Wassertein problem, both in the high $(d\\gg\\log n)$ andlow $f\\ll\\log n)$ dimensional regimes. We propose the \u2018Ping-Pong algorithm', alternatively estimating the orthogonal transformation and the relabeling, initialized via a FrankeWolfe convex relaxation. Our experimental results show that our method most globally outperforms the algorithm proposed in Grave et al. [2019]. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "David Alvarez-Melis and Tommi Jakkola. Gromov-Wasserstein alignment of word embedding spaces. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsuji, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1881-1890, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-i214. URL https ://aclanthology org/D18-i1214.   \nP.J. Besl and Neil D. McKay. A method for registration of 3-d shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2):239-256, 1992. doi: 10.1109/34.121791.   \nEmmanuel J. Candes. The restricted isometry property and its implications for compressed sensing. Comptes Rendus Mathematique, 346(9):589-592, 2008. ISSN 1631-073X. doi: https://doi.org/10. 1016/j.crma.2008.03.014. URL https: //www . sciencedirect .com/science/article/pii/ S1631073X08000964.   \nAlexis Conneau, Guillaume Lample, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herve J\u00e9gou. Word translation without parallel data, 2018.   \nMarco Cuturi.  Sinkhorn distances: Lightspeed computation of optimal transport.  In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/ af21d0c97db2e27e13572cbf59eb343d-Paper.pdf.   \nOsman E. Dai, Daniel Cullina, and Negar Kiyavash. Database alignment with gaussian features. In Kamalika Chaudhuri and Masashi Sugiyama, editors, Proceedings of the TwentySecond International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pages 3225-3233. PMLR, 16-18 Apr 2019. URL https: //proceedings.mlr.press/v89/dai19b.html.   \nOsman Emre Dai, Daniel Cullina, and Negar Kiyavash. Gaussian database alignment and gaussian planted matching, 2023.   \nPascale Fung. Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus. In Third Workshop on Very Large Corpora, 1995. URL https: //aclanthology .org/W95-0114.   \nShuyang Gong and Zhangsong Li. The umeyama algorithm for matching correlated gaussian geometric models in the low-dimensional regime, 2024.   \nEdouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of embeddings with wasserstein procrustes. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1880-1890. PMLR, 2019.   \nYedid Hoshen and Lior Wolf. Non-adversarial unsupervised word translation, 2018.   \nMartin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 3Oth International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 427-435, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL https: //proceedings.mlr.press/ v28/jaggi13.html.   \nH. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1-2):83-97, 1955. doi: https://doi.org/10.1002/nav.3800020109. URL https : //onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109.   \nDmitriy Kunisky and JonathanNilsWeed.Strong recovery of geometric planted matchings, pages 834-876. 2022. doi: 10.1137/1.9781611977073.36. URL https : //epubs .siam.org/doi/ abs/10.1137/1.9781611977073.36.   \nB. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):1302 - 1338, 2000. doi: 10.1214/aos/1015957395. URL https: //doi.org/10.1214/aos/1015957395.   \nVince Lyzinsk1, Donniell E. Fisnkind, Marcelo Fiori, Joshua 1. vogelstein, Carey E. Priebe, and Guillermo Sapiro. Graph matching: Relax at your own risk. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(1):60-73, 2016. doi: 10.1109/TPAMI.2015.2424894.   \nKonstantin Makarychev, Rajsekar Manokaran, and Maxim Sviridenko. Maximum quadratic assignment problem: Reduction from maximum label cover and lp-based approximation algorithm. ACM Trans. Algorithms, 10(4), aug 2014. ISSN 1549-6325. doi: 10.1145/2629672. URL https : //doi.org/10.1145/2629672.   \nReinhard Rapp. Identifying word translations in non-parallel texts. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 320322, Cambridge, Massachusets, UA, June 1995. Association for Computational Linguistics. doi: 10.3115/981658.981709. URL https : //aclanthology. org/P95-1050.   \nC. A. Rogers. Covering a sphere with spheres. Mathematika, 10(2):157-164, 1963. doi: 10.1112/ S0025579300004083.   \nPeter Schonemann. A generalized solution of the orthogonal procrustes problem. Psychometrika,31 (1):1-10, 1966. URL https : //EconPapers .repec.org/RePEc :spr:psycho :v:31:y:1966: i:1:p:1-10.   \nCarlo Tomasi and Takeo Kanade. Shape and motion from image streams under orthography: a factorization method. International Journal of Computer Vision, 9(2):137-154, Nov 1992. ISSN 1573-1405. doi: 10.1007/BF00129684. URL https : //doi . org/10 .1007/BF00129684.   \nS. Umeyama. An eigendecomposition approach to weighted graph matching problems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(5):695-703, 1988. doi: 10.1109/ 34.6778.   \nErnesto Araya Valdivia and Hemant Tyagi. Graph matching via convex relaxation to the simplex, 2023.   \nRoman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. doi: 10.1017/9781108231596.   \nHaoyu Wang, Yihong Wu, Jiaming Xu, and Israel Yolou. Random graph matching in geometric models: the case of complete graphs. In Po-Ling Loh and Maxim Raginsky, editors, Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 3441-3488. PMLR, 02-05 Jul 2022. URL https : //proceedings .mlr .press/ v178/wang22a.html.   \nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. Normalized word embedding and orthogonal transform for bilingual word translation. In Rada Mihalcea, Joyce Chai, and Anoop Sarkar, editors, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational inguistics:HumananguageTechnologipages1006-101Denver,Colrad, May-June 2015. Association for Computational Linguistics. doi: 10.3115/v1/N15-1104. URL https: //aclanthology.org/N15-1104.   \nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Adversarial training for unsupervised bilingual lexicon induction. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1959-1970, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1179. URL https : //aclanthology org/P17-1179. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Useful results ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "We start by proving Lemma 1 which gives the equivalence between PW and geometric graph alignement. ", "page_idx": 11}, {"type": "text", "text": "A.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof of Lemma 1. We have by Lemma 3 that as soon as we are able to estimate $\\pi^{\\star}$ with a small error in PW, we are also capable of doing so $Q^{\\star}$ , by perfoming a simple Singular Value Decomposition (SVD). Since one can trivially form an instance $A=\\check{X}^{\\top}X$ and $B=Y^{\\top}Y$ of geometric graph alignement from an instance $(X,Y)$ of PW from model (1), we can deduce that if we know how to (approximately) solve geometric graph alignement, we know how to (approximately) solve PW. ", "page_idx": 11}, {"type": "text", "text": "Conversely, if we are given adjacency matrices $A=X^{\\top}X,B=Y^{\\top}Y$ of two correlated random geometric graphs under the Gaussian model from [Wang et al., 2022, Gong and Li, 2024] where $y_{i}=x_{\\pi^{\\star}(i)}+\\sigma z_{i}$ , we can recover $\\pi^{\\star}$ via solving PW. Indeed, $A$ is of rank at most $d$ , so we can build $X^{\\prime}=\\left(x_{1}^{\\prime}|\\ldots|x_{n}^{\\prime}\\right)\\in\\mathbb{R}^{d\\times n}$ such that $A=X^{\\prime\\top}X^{\\prime}$ Similarly, we can build $Y^{\\prime}=(y_{1}^{\\prime}|\\ldots|y_{n}^{\\prime})\\in$ $\\mathbb{R}^{d\\times n}$ such that $B=Y^{\\prime\\intercal}Y^{\\prime}$ We have $X^{\\top}X=X^{\\prime\\top}X^{\\prime}$ hence $\\langle x_{i},x_{j}\\rangle=\\langle x_{i}^{\\prime},x_{j}^{\\prime}\\rangle$ , thus there exists $Q_{1}\\in O(d)$ such that for all $i\\in[n]$ \uff0c $x_{i}^{\\prime}=Q_{1}x_{i}$ . Similarly, there exists $Q_{2}\\in\\dot{O}(d)$ such that for all $i$ $y_{i}^{\\prime}=Q_{2}y_{i}$ . By multiplying these two orthogonal matrices by independent random uniform orthogonal matrices, we can always assume that they are independent from $X$ and $Y$ . We obtained $X^{\\prime},Y^{\\prime}$ Which satisfy $y_{i}^{\\prime}=Q^{\\star}x_{\\pi^{\\star}(i)}^{\\prime}+\\sigma z_{i}^{\\prime}$ for all $i$ where $Q^{\\star}\\,\\dot{=}\\,Q_{2}Q_{1}^{\\top}$ , and $x_{i}^{\\prime}=Q_{1}x_{i},z_{i}^{\\prime}=Q_{2}z_{i}$ are i.i.d. standard Gaussian vectors. This is exactly an instance of the PW problem. If we know how to (approximately) solve the PW problem, we know how to (approximately) recover $\\pi^{\\star}$ and thus (approximately) solve the geometric graph alignment problem. ", "page_idx": 11}, {"type": "text", "text": "This proves that PW and geometric graph alignement are equivalent. ", "page_idx": 11}, {"type": "text", "text": "A.2  -nets of $O(d)$ ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Throughout the proofs, we will need to give high probability bounds on quantities for all orthogonal matrices. This is done by covering $O(d)$ by a finite number of open balls centered at points of $O(d)$ This is done by considering --nets. ", "page_idx": 11}, {"type": "text", "text": "Definition 1 (e -nets of $O(d))$ . Let $\\varepsilon>0$ A subset $\\mathcal{N}_{\\varepsilon}\\subseteq O(d)$ is ae-net of $O(d)$ for the Frobenius norm if for all $O\\in{\\mathcal{O}}(d)$ there exists $O_{\\varepsilon}\\in\\mathcal N_{\\varepsilon}$ such that $\\|O-O_{\\varepsilon}\\|_{F}\\leqslant\\varepsilon$ ", "page_idx": 11}, {"type": "text", "text": "Remark 2. Note that since $\\|\\cdot\\|_{F}\\leqslant\\|\\cdot\\|_{o p}$ by Cauchy-Schwarz any E-net of $O(d)$ for the Frobenius norm is also an e-net of $O(d)$ for the operator norm. ", "page_idx": 11}, {"type": "text", "text": "Wewill need $\\varepsilon-$ netsof $O(d)$ that are not too large, in order to apply union bounds which will give non-trivial probabilistic controls. Guarantees on such $\\varepsilon-$ nets are standard in the literature; we give one which will be useful for us in the following Lemma. ", "page_idx": 11}, {"type": "text", "text": "Lemma 4 ( $\\varepsilon-$ nets of $O(d)$ of minimal size, see e.g. Rogers [1963]). There exists a universal constant $C>0$ such that for all $\\varepsilon>0$ thereexists anE-net $\\mathcal{N}_{\\varepsilon}$ of $O(d)$ such that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\left|{\\mathcal{N}}_{\\varepsilon}\\right|\\leqslant\\left({\\frac{C{\\sqrt{d}}}{\\varepsilon}}\\right)^{d^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "B  Remaning proofs of Section 2 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "B.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof of Lemma 2. Denote $\\begin{array}{r}{g(\\pi):=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{\\pi(i)}-\\hat{Q}^{\\top}y_{i}\\right\\|^{2}}\\end{array}$ .The proof relies on noticing that for all $\\pi\\in S_{n}$ , by definition $g(\\hat{\\pi})\\leqslant g(\\pi)$ and using $\\begin{array}{r}{\\|a+b\\|^{2}\\geqslant\\frac{1}{2}\\|a\\|^{2}-\\|b\\|^{2}}\\end{array}$ , one gets ", "page_idx": 11}, {"type": "equation", "text": "$$\ng(\\hat{\\pi})\\geqslant\\frac{1}{2}c^{2}(\\hat{\\pi},\\pi)-g(\\pi),\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "and thus $c^{2}(\\hat{\\pi},\\pi)\\leqslant4g(\\pi)$ .We apply the previous inequality to $\\pi\\,=\\,\\pi^{\\star}$ and using $\\|a+b\\|^{2}\\leqslant$ $2(\\lVert a\\rVert^{2}+\\lVert b\\rVert^{2})$ , one gets, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle g(\\pi^{\\star})\\leqslant\\frac{2}{n}\\sum_{i=1}^{n}\\left\\|(I_{d}-\\hat{Q}^{\\top}Q^{\\star})x_{i}\\right\\|^{2}+\\frac{2\\sigma^{2}}{n}\\sum_{i=1}^{n}\\left\\|\\hat{Q}^{\\top}z_{i}\\right\\|^{2}}}\\\\ {{\\displaystyle=\\frac{2}{n}\\sum_{i=1}^{n}\\left\\|(\\hat{Q}-Q^{\\star})x_{i}\\right\\|^{2}+\\frac{2\\sigma^{2}}{n}\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where we used the fact that the matrices $\\hat{Q},Q^{\\star}$ are orthogonal. Using concentration of Chi squared random variables, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2}\\geqslant n d+2\\sqrt{n d t}+2t\\right)\\leqslant e^{-t}\\,,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "leading to $\\begin{array}{r}{\\mathbb{P}\\left(\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2}\\geqslant5n d\\right)\\ \\leqslant\\ e^{-n d}}\\end{array}$ by plugging in $t\\ =\\ n d$ .We are now left with $\\textstyle\\sum_{i=1}^{n}\\left\\|({\\hat{Q}}-Q^{\\star})x_{i}\\right\\|^{2}$ . We have that for any $\\begin{array}{r}{Q,\\mathbb{E}\\left[\\sum_{i=1}^{n}\\left\\|(Q-Q^{\\star})x_{i}\\right\\|^{2}\\right]=n\\left\\|Q-Q^{\\star}\\right\\|_{F}^{2};}\\end{array}$ however, $\\hat{Q}$ depends on the $x_{i}$ so we need a uniform upper bound. Using Hanson-Wright inequality, for any Q \u2208 Rdxd ", "page_idx": 12}, {"type": "equation", "text": "$$\n>\\left(\\sum_{i=1}^{n}\\|(Q-Q^{\\star})x_{i}\\|^{2}\\geqslant n\\|Q-Q^{\\star}\\|_{F}^{2}+c\\operatorname*{max}\\left(\\sqrt{n t\\|Q-Q^{\\star}\\|_{F}^{2}\\|Q-Q^{\\star}\\|_{\\mathrm{op}}^{2}},t\\|Q-Q^{\\star}\\|_{\\mathrm{op}}^{2}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which reads as, for $Q$ orthogonal (leading to $\\|Q-Q^{\\star}\\|_{\\mathrm{op}}\\leqslant2)$ ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n}\\left\\lVert(Q-Q^{\\star})x_{i}\\right\\rVert^{2}\\geqslant n\\right\\lVert Q-Q^{\\star}\\rVert_{F}^{2}+c^{\\prime}\\operatorname*{max}\\left(\\sqrt{n t\\big\\lVert Q-Q^{\\star}\\big\\rVert_{F}^{2}},t\\right)\\right)\\leqslant2e^{-t}\\,.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For $\\varepsilon~\\in~(0,1/2)$ ,let $\\mathcal{N}_{\\varepsilon}$ be an $\\varepsilon{\\mathrm{-net}}$ of $O(d)$ of minimal cardinality; by Lemma 4 we have $\\log(|{\\mathcal{N}}_{\\varepsilon}|)\\leqslant C{d^{2}}\\ln(d/\\varepsilon)$ . Using a union bound: ", "page_idx": 12}, {"type": "equation", "text": "$$\n>\\left(\\operatorname*{sup}_{Q\\in\\mathcal{N}_{\\varepsilon}}\\left\\{\\left|\\sum_{i=1}^{n}\\|(Q-Q^{\\star})x_{i}\\|^{2}-n\\|Q-Q^{\\star}\\|_{F}^{2}\\right|\\right\\}\\geqslant c^{\\prime}\\operatorname*{max}\\left(\\sqrt{n t\\|Q-Q^{\\star}\\|_{F}^{2}},t\\right)\\right)\\leqslant2e^{-t+C d^{2}1}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Taking $t=\\lambda+C d^{2}\\ln(d/\\varepsilon)$ , we have with probabiliy $1-2e^{-\\lambda}$ that: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{2\\in N_{\\varepsilon}}\\left\\{\\left|\\sum_{i=1}^{n}\\|(Q-Q^{\\star})x_{i}\\|^{2}-n\\|Q-Q^{\\star}\\|_{F}^{2}\\right|\\right\\}\\leqslant c^{\\prime}\\operatorname*{max}\\left(\\sqrt{n(\\lambda+C d^{2}\\ln(d/\\varepsilon))\\|Q-Q^{\\star}\\|_{F}^{2}},\\lambda+\\frac{2}{\\lambda^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Now, if $Q,Q^{\\prime}\\in{\\mathcal{O}}(d)$ satisfy $\\|Q-Q^{\\prime}\\|_{F}\\leqslant\\varepsilon$ , we have using the orthogonality property of these matrices: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{i=1}^{n}\\bigl\\|(Q-Q^{*})x_{i}\\bigr\\|^{2}-\\displaystyle\\sum_{i=1}^{n}\\|(Q^{\\prime}-Q^{*})x_{i}\\|^{2}=2\\sum_{i=1}^{n}\\langle(Q^{\\prime}-Q)x_{i},Q^{*}x_{i}\\rangle}}\\\\ &{}&{\\leqslant2\\displaystyle\\sum_{i=1}^{n}\\|(Q^{\\prime}-Q)x_{i}\\|\\|Q^{*}x_{i}\\|}\\\\ &{}&{\\leqslant2\\|Q^{\\prime}-Q\\|_{\\infty}\\displaystyle\\sum_{i=1}^{n}\\|x_{i}\\|^{2}}\\\\ &{}&{\\leqslant2\\varepsilon\\displaystyle\\sum_{i=1}^{n}\\|x_{i}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and with probability $1-e^{-n d}$ we have $\\begin{array}{r}{\\sum_{i=1}^{n}\\left\\|x_{i}\\right\\|^{2}\\leqslant5n d.}\\end{array}$ Then, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert Q-Q^{\\star}\\right\\rVert_{F}^{2}-\\left\\lVert Q^{\\prime}-Q^{\\star}\\right\\rVert_{F}^{2}=\\langle Q^{\\star},Q^{\\prime}-Q\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\left\\lVert Q^{\\star}\\right\\rVert_{F}\\left\\lVert Q^{\\prime}-Q\\right\\rVert_{F}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant\\sqrt{d}\\varepsilon\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\left|\\displaystyle\\sum_{i=1}^{n}\\|(Q-Q^{\\star})x_{i}\\|^{2}-n\\|Q-Q^{\\star}\\|_{F}^{2}\\right|\\right\\}\\leqslant\\operatorname*{sup}_{Q\\in\\mathcal{N}_{\\varepsilon}}\\left\\{\\left|\\displaystyle\\sum_{i=1}^{n}\\|(Q-Q^{\\star})x_{i}\\|^{2}-n\\|Q-Q^{\\star}\\|_{F}^{2}\\right|\\right\\}+10n}\\\\ {\\leqslant c^{\\prime}\\operatorname*{max}\\left(\\sqrt{n(\\lambda+C d^{2}\\ln(d/\\varepsilon))\\|Q-Q^{\\star}\\|_{F}^{2}},\\lambda+C d^{2};\\right.}\\\\ {\\left.\\qquad\\qquad\\qquad\\qquad+10n d\\varepsilon+n\\sqrt{d}\\varepsilon\\,,\\right.\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with probability $1-2e^{-n d}-2e^{-\\lambda}$ . Thus, applying this to $\\hat{Q}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left\\|(\\hat{Q}-Q^{\\star})x_{i}\\right\\|^{2}\\leqslant n\\delta d+c^{\\prime}\\operatorname*{max}\\left(\\sqrt{n\\delta(\\lambda+C d^{2}\\ln(11/\\delta))},\\lambda+C d^{2}\\ln(1\\varepsilon)\\right)+10n d\\varepsilon+n\\sqrt{d}\\varepsilon\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and taking $\\varepsilon=\\delta/11$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\left\\|(\\hat{Q}-Q^{\\star})x_{i}\\right\\|^{2}\\leqslant2n\\delta d+c^{\\prime}\\sqrt{n\\delta(\\lambda+C d^{2}\\ln(11/\\delta))}+c^{\\prime}(\\lambda+C d^{2}\\ln(11/\\delta))\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "leading to: ", "page_idx": 13}, {"type": "equation", "text": "$$\nc^{2}(\\hat{\\pi},\\pi^{\\star})\\leqslant40\\sigma^{2}d+16\\delta d+c^{\\prime}\\sqrt{\\delta\\frac{\\lambda+C d^{2}\\ln(11/\\delta)}{n}}+c^{\\prime}\\frac{\\lambda+C d^{2}\\ln(11/\\delta)}{n}\\,,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "hence the result, taking $\\lambda=\\sqrt{n}+d^{2}$ ", "page_idx": 13}, {"type": "text", "text": "B.2Proof of Lemma 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3. Denote $\\begin{array}{r}{g(Q):=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{\\hat{\\pi}(i)}-Q^{\\top}y_{i}\\right\\|^{2}}\\end{array}$ .The proof relies on noticing that for all $Q\\in{\\mathcal{O}}(d)$ , by definition $g(\\hat{Q})\\leqslant g(Q^{\\star})$ and using $\\begin{array}{r}{\\|a+b\\|^{2}\\geqslant\\frac{1}{2}\\|a\\|^{2}-\\|b\\|^{2}}\\end{array}$ , one gets ", "page_idx": 13}, {"type": "equation", "text": "$$\ng(\\hat{Q})\\geqslant\\frac{1}{2n}\\sum_{i=1}^{n}\\left\\|(Q-\\hat{Q})y_{i}\\right\\|^{2}-g(Q),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and thus $\\begin{array}{r}{{\\frac{1}{n}}\\sum_{i=1}^{n}\\Big\\|(Q-\\hat{Q})y_{i}\\Big\\|^{2}\\leqslant4g(Q^{\\star})}\\end{array}$ byapplying the previous inequality to $\\pi=\\pi^{\\star}$ . Using $\\|a+b\\|^{2}\\leqslant2(\\|a\\|^{2}+\\|b\\|^{2})$ , one gets: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle g({\\boldsymbol Q}^{\\star})=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{\\widetilde{\\pi}(i)}-x_{\\pi^{\\star}(i)}-{\\boldsymbol Q}^{\\star}\\nabla_{z_{i}}\\right\\|^{2}}\\\\ {\\displaystyle\\leqslant\\frac{2}{n}\\sum_{i=1}^{n}\\left\\|x_{\\widetilde{\\pi}(i)}-x_{\\pi^{\\star}(i)}\\right\\|^{2}+\\frac{2\\sigma^{2}}{n}\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2}}\\\\ {\\displaystyle=2c^{2}(\\widehat{\\pi},\\pi^{\\star})+\\frac{2\\sigma^{2}}{n}\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2}}\\\\ {\\displaystyle\\leqslant2{\\delta}d+\\frac{2\\sigma^{2}}{n}\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "With probability $1\\,-\\,e^{-n d}$ \uff0c $\\begin{array}{r}{\\sum_{i=1}^{n}\\|z_{i}\\|^{2}\\ \\leqslant\\ 5n d,}\\end{array}$ and we are thus left with lower bounding $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|(Q-\\hat{Q})y_{i}\\right\\|^{2}}\\end{array}$ . Using results form the previous proof, with probability $1-2e^{-n d}-2e^{-\\lambda}$ wehave: $\\frac{1}{\\lfloor+\\sigma^{2}\\sum_{i=1}^{n}\\Big\\Vert(\\hat{Q}-Q^{*})y_{i}\\Big\\Vert^{2}}\\geqslant n\\Big\\Vert\\hat{Q}-Q^{*}\\Big\\Vert_{F}^{2}+n\\delta d+c^{\\prime}\\sqrt{n\\delta(\\lambda+C d^{2}\\ln(11/\\delta))}+c^{\\prime}(\\lambda+C d^{2}\\ln(11/\\delta))+\\delta\\left(\\frac{1}{\\varepsilon^{2}}\\right)^{2}.$ 1/8) . ", "page_idx": 13}, {"type": "text", "text": "Thus, with probability $1-$ ", "page_idx": 13}, {"type": "text", "text": "$_\\mathrm{ortho}^{2}(\\hat{Q},Q^{\\star})\\leqslant\\delta d+c^{\\prime}\\sqrt{n^{-1}\\delta(\\lambda+C d^{2}\\ln(11/\\delta))}+c^{\\prime}n^{-1}(\\lambda+C d^{2}\\ln(11/\\delta))+8\\delta d+40\\sigma^{2}d\\,,$ leading to the desired result for $\\lambda=d^{2}+{\\sqrt{n}}$ \u53e3 ", "page_idx": 13}, {"type": "text", "text": "C Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Theorem $^{\\,l}$ . Define $\\begin{array}{r}{\\mathcal L(\\pi,Q):=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{\\pi(i)}-Q^{\\top}y_{i}\\right\\|^{2}}\\end{array}$ . Without loss of generality we can assume that $\\pi^{*}=\\mathrm{Id}$ ", "page_idx": 14}, {"type": "text", "text": "Step $^{\\mathit{I}}$ using ML estimators. By definition, the ML estimators $({\\hat{\\pi}},{\\hat{Q}})$ defined in (2) minimize $\\mathcal{L}$ and thus $\\mathcal{L}(\\hat{\\pi},\\hat{Q})\\leqslant\\mathcal{L}(\\pi^{\\star}=\\operatorname{Id},Q^{\\star})$ , which can be expressed as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac1n\\sum_{i=1}^{n}\\left\\|x_{\\hat{\\pi}(i)}-\\hat{Q}^{\\top}Q^{\\star}x_{i}-\\sigma\\hat{Q}^{\\top}z_{i}\\right\\|^{2}\\leqslant\\frac{\\sigma^{2}}{n}\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using $\\left\\|a\\right\\|^{2}\\leqslant2(\\left\\|a-b\\right\\|^{2}+\\left\\|b\\right\\|^{2})$ , we obtain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{\\hat{\\pi}(i)}-\\hat{Q}^{\\top}Q^{\\star}x_{i}\\right\\|^{2}\\leqslant\\frac{4\\sigma^{2}}{n}\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, standard chi-square concentration (see e.g. Laurent and Massart [2000]) entails that for all $t>0$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{i=1}^{n}\\left\\|z_{i}\\right\\|^{2}-n d\\right|\\geqslant2{\\sqrt{n d t}}+2t\\right)\\leqslant2e^{-t},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "so that with probability $1-2e^{-n}$ \uff0c $\\begin{array}{r}{\\frac{4\\sigma^{2}}{n}\\sum_{i=1}^{n}\\|z_{i}\\|^{2}\\leqslant4\\sigma^{2}(d+2{\\sqrt{d}}+2)}\\end{array}$ , and thus ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|x_{\\hat{\\pi}(i)}-\\hat{Q}^{\\top}Q^{\\star}x_{i}\\right\\|^{2}\\leqslant4\\sigma^{2}(d+2\\sqrt{d}+2)\\leqslant5\\sigma^{2}d\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for $d$ (or $n$ ) large enough. ", "page_idx": 14}, {"type": "text", "text": "Step2:existenceofaset $\\kappa$ with prescribed properties. We will now show that the above inequality (13) forces $\\overline{{\\operatorname{ov}(\\hat{\\pi},\\pi^{\\star}=\\operatorname{Id})}}$ to be large. To do so, let us assume that $\\mathrm{ov}(\\hat{\\pi},\\mathrm{Id})<1-\\delta$ , for some $\\delta>0$ to be specified later: hence, there exist at least $n\\delta$ indices $i\\in[n]$ such that $\\hat{\\pi}(i)\\neq i$ . Let us define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{Z}:=\\left\\{i\\in[n]:\\left\\|x_{\\hat{\\pi}(i)}-\\hat{Q}^{\\top}Q^{\\star}x_{i}\\right\\|^{2}\\leqslant\\frac{30}{\\delta}\\sigma^{2}d\\right\\}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It is clear that under the event $B_{n}$ on which (13) holds, we have $\\begin{array}{r}{(n-|\\mathcal{T}|)\\times\\frac{30}{\\delta}\\sigma^{2}d\\leqslant5n\\sigma^{2}d}\\end{array}$ which gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{Z}|\\geqslant n(1-\\delta/6)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Consequently, denoting ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{I}:=\\left\\{i\\in[n]:\\pi^{\\star}(i)\\neq\\hat{\\pi}(i),\\:\\left\\|x_{\\hat{\\pi}(i)}-\\hat{Q}^{\\top}Q^{\\star}x_{i}\\right\\|^{2}\\leqslant\\frac{30}{\\delta}\\sigma^{2}d\\right\\},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "one has $\\begin{array}{r}{|\\mathcal{I}|\\geqslant n\\delta-(n-|\\mathcal{T}|)\\geqslant\\frac{5}{6}\\delta n}\\end{array}$ . We remark that for all $i\\in\\mathcal{I}$ \uff0c ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\{j\\in\\mathcal{J}:\\{i,\\hat{\\pi}(i)\\}\\cap\\{j,\\hat{\\pi}(j)\\}\\neq\\emptyset\\}\\right|\\leqslant4\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let us denote $Q:=\\hat{Q}^{\\top}Q^{\\star}$ . Iteratively ruling out at most 3 elements for each $i\\in\\mathcal{I}$ , the above shows that on event ${\\mathcal{E}}_{n}$ one can build a set $\\dot{K}:=\\bar{\\mathcal{K}}(\\hat{\\pi},Q)\\subseteq[n]$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n(i)\\ \\ |K|\\geqslant n\\delta/6,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$(i i)$ for all $i\\in K,\\,\\hat{\\pi}(i)\\neq i$ and $(i,\\hat{\\pi}(i))_{i\\in K}$ are disjoint pairs, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(i i i)\\;\\;\\mathrm{for}\\;\\mathrm{all}\\;i\\in\\mathcal{K},\\left\\|x_{\\hat{\\pi}(i)}-Q x_{i}\\right\\|^{2}\\leqslant\\frac{30}{\\delta}\\sigma^{2}d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Step 3: upper bounding the probaility of existence of such a set $\\kappa$ We will now bound the probability that such a set $\\kappa$ exists. First, let us fix $i\\in[n]$ \uff0c $Q\\in{\\mathcal{O}}(d)$ \uff0c $\\pi\\in S_{n}$ such that $\\pi(i)\\neq i$ .We have $x_{\\pi(i)}-Q x_{i}\\sim\\mathcal{N}(0,2I_{d})$ .Assume $60\\sigma^{2}<1\\$ and $\\delta\\geqslant60\\sigma^{2}$ so that we have ve g\u00b2d \u2264 d. For these fixed $Q,\\pi$ , we have ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{\\geqslant\\left(\\left\\|x_{\\pi(i)}-Q x_{i}\\right\\|^{2}\\leqslant\\frac{60}{\\delta}\\sigma^{2}d\\right)\\leqslant\\mathbb{P}\\left(\\left\\|N(0,I_{d})\\right\\|^{2}\\leqslant d/2\\right)=\\mathbb{P}\\left(d-\\left\\|N(0,I_{d})\\right\\|^{2}\\geqslant d/2\\right)\\leqslant e^{-d/1}\\leqslant\\frac{1}{\\delta}\\leqslant d/2}\\end{array}$ where we applied the one-sided chi-square concentration inequality4 $\\mathbb{P}\\left(k-X\\geqslant2{\\sqrt{k x}}\\right)\\ \\leqslant$ $\\exp(-x)$ when $X\\ \\sim\\ \\chi^{2}(k)$ . This gives that for any given $\\kappa\\,\\subset\\,[n]$ satisfying conditions $(i)$ and $(i i)$ above, using independence of the pairs $(x_{i},x_{\\hat{\\pi}(i)})_{i\\in K}$ and recalling that $\\delta\\geqslant60\\sigma^{2}$ , one has ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\forall i\\in K,\\,\\left\\|x_{\\pi(i)}-Q x_{i}\\right\\|^{2}\\leqslant\\frac{60}{\\delta}\\sigma^{2}d\\right)\\leqslant e^{-n\\delta/6\\times d/16}=e^{-\\delta n d/96}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denote by $\\mathbf{\\mathcal{A}}_{\\delta}$ the event ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{A}_{\\delta}:=\\left\\{\\begin{array}{r l r l}\\end{array}\\right.}\\end{array}$ there exists $\\pi\\in S_{n},Q\\in{\\mathcal{O}}(d)$ and $\\mathcal{K}=\\mathcal{K}(\\pi,Q)\\subseteq[n]$ which satisfies $(i),(i i)$ and $(i i i)\\}$ (15) As previously explained, we want to bound the probability of the event $\\boldsymbol{\\mathcal{A}}_{\\delta}$ , for $\\delta\\geqslant60\\sigma^{2}$ . For the union bound on $Q\\in{\\mathcal{O}}(d)$ , we need to use an epsilon-net argument, which is as follows. Let $\\varepsilon>0$ to be specified later. By Lemma 4, there exists $\\mathcal{N}_{\\varepsilon}$ an \u03b5-net of $O(d)$ of cardinality at most $\\left({\\frac{c_{1}{\\sqrt{d}}}{\\varepsilon}}\\right)^{d^{2}}$ which is also an $\\varepsilon$ -net for the operator norm, see Remark 2. In particular, if we are under event $A_{\\delta}$ and take $\\pi,Q,\\kappa$ verifying conditions in (15), there exists an element $Q_{\\varepsilon}$ of $O_{\\varepsilon}(d)$ such that $\\|Q_{\\varepsilon}-Q\\|_{o p}\\leqslant\\varepsilon$ , which gives ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall i\\in K,\\,\\|x_{\\pi(i)}-Q_{\\varepsilon}x_{i}\\|\\leqslant\\left\\|x_{\\pi(i)}-Q x_{i}\\right\\|+\\|(Q-Q_{\\varepsilon})x_{i}\\|\\leqslant\\sqrt{\\frac{30}{\\delta}\\sigma^{2}d}+\\|Q_{\\varepsilon}-Q\\|_{o p}\\|x_{i}\\|,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and applying chi-square concentration again gives that, under an event $\\ensuremath{{\\mathcal{C}}}_{n}$ with probability $\\geqslant$ $1-e^{-d+\\log n}\\geqslant1-e^{-d/2}$ since $(d\\geqslant2\\log n)$ for all $i\\in[n],\\|x_{i}\\|\\leqslant{\\sqrt{2d}}$ and the above yields ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall i\\in K,\\,\\left\\|x_{\\pi(i)}-Q_{\\varepsilon}x_{i}\\right\\|\\leqslant\\sqrt{\\frac{30}{\\delta}\\sigma^{2}d}+\\varepsilon\\,\\sqrt{2d}\\leqslant\\sqrt{\\frac{60}{\\delta}\\sigma^{2}d},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "choosing $\\varepsilon=c_{2}\\sqrt{\\sigma^{2}/\\delta}$ for some appropriate $c_{2}$ . Hence, taking a union bound over $\\pi\\in S_{n},Q_{\\varepsilon}\\in$ $O_{\\varepsilon}(d)$ and subsets $\\kappa\\subseteq[n]$ , and recalling (14), we can bound $\\overline{{\\mathbb{P}\\left(A_{\\delta}\\mid\\mathcal{C}_{n},B_{n}\\right)}}$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left({\\mathcal A}_{\\delta}\\mid B_{n},{\\mathcal C}_{n}\\right)\\leqslant\\displaystyle\\frac{1}{\\mathbb{P}\\left(B_{n},{\\mathcal C}_{n}\\right)}\\times n!\\times\\left(\\frac{c_{1}\\sqrt{d}}{\\varepsilon}\\right)^{d^{2}}\\times2^{n}\\times e^{-\\delta n d/96}}\\\\ &{\\qquad\\qquad\\qquad\\leqslant(1+o(1))\\exp(n\\log n+c_{3}d^{2}\\log d+c_{4}d^{2}\\sqrt{\\delta/(\\sigma^{2})}+n\\log2-c_{5}\\delta n d)}\\\\ &{\\qquad\\qquad\\leqslant(1+o(1))\\exp(-c_{6}\\delta n d)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we recall that $\\delta\\,\\geqslant\\,60\\sigma^{2}$ , and the last inequality holds if $c_{4}d^{2}\\sqrt{\\delta/(\\sigma^{2})}\\,\\leqslant\\,c_{7}d^{2}\\,\\leqslant\\,c_{8}\\delta n d,$ and if $c_{3}d^{2}\\log d\\leqslant c_{8}\\delta n d$ for which $\\delta\\geqslant c_{9}d\\log d/n$ suffices, and if $n\\log n\\leqslant c_{8}\\delta n d$ , for which $\\delta\\geqslant c_{1}0\\log n/d$ suffices. ", "page_idx": 15}, {"type": "text", "text": "Step 4: conclusion. Now, wrapping things up, we obtain that for $\\delta\\geqslant\\operatorname*{max}(60\\sigma^{2},c_{9}d/n,c_{1}0\\log n/d)$ wehavefor $n$ large enough ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\mathrm{ov}(\\hat{\\pi},\\pi^{\\star})<1-\\delta\\right)\\leqslant\\mathbb{P}\\left(\\mathcal{B}_{n},\\mathcal{C}_{n}\\right)\\mathbb{P}\\left(\\mathrm{ov}(\\hat{\\pi},\\pi^{\\star})<1-\\delta\\mid\\mathcal{B}_{n},\\mathcal{C}_{n}\\right)+\\mathbb{P}\\left(\\bar{\\mathcal{B}}_{n}\\cup\\bar{\\mathcal{C}}_{n}\\right)}\\\\ &{\\leqslant\\mathbb{P}\\left(A_{\\delta}\\mid\\mathcal{B}_{n},\\mathcal{C}_{n}\\right)+\\mathbb{P}\\left(\\bar{\\mathcal{B}}_{n}\\right)+\\mathbb{P}\\left(\\bar{\\mathcal{C}}_{n}\\right)}\\\\ &{\\leqslant(1+o(1))e^{-c_{6}\\delta n d}+2e^{-n}+e^{-d/2}=o(1)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This gives the desired result ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{ov}(\\pi^{\\star},\\hat{\\pi})\\geqslant1-\\operatorname*{max}\\left(60\\sigma^{2},c_{1}\\frac{d}{n},c_{2}\\frac{\\log n}{d\\log d}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which remains true when $60\\sigma^{2}\\geqslant1$ ", "page_idx": 15}, {"type": "text", "text": "The desired inequality for $\\ell^{2}(\\hat{Q},Q^{\\star})$ follows from Lemma 3 (for $\\delta=\\Theta(d/n))$ and Remark 1. \\*again,see e.g Larent and Massart [00. ", "page_idx": 15}, {"type": "text", "text": "D Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We here prove that a minimizer $\\hat{Q}$ of the conic alignment loss satisfies the following guarantees. ", "page_idx": 16}, {"type": "text", "text": "Theorem 3 (Conic alignment minimizer). Let $\\delta_{0}\\,\\in\\,(0,1)$ . Let $q\\,=\\,p^{3}$ Let $v_{1},\\ldots,v_{q}$ be i.i.d. uniform direcions in $S^{d-1}$ and assume that $u_{1},\\dotsc,u_{p}$ areindeenetdi over $\\{v_{1},\\ldots,v_{q}\\}$ . Let $\\mathcal{N}$ be an $\\scriptstyle{\\varepsilon-n e t}$ of $O(d)$ for the Frobenius norm of minimal cardinality. Then, there exist constants $C_{1},C_{2},C_{3},C_{4},C_{5}>0$ such that, $i f\\log(n)\\geqslant C_{1}d\\log(1/\\delta_{0}),$ $\\varepsilon=C_{2}\\sigma d^{-1/2}$ \uff0c $\\begin{array}{r}{\\dot{:}=\\delta_{0},\\,\\kappa=\\sqrt{\\frac{2}{d}},\\,p\\geqslant\\mathrm{polylog}(1/\\sigma,d)}\\end{array}$ and $\\begin{array}{r}{\\sigma\\leqslant\\frac{C_{3}\\delta_{0}^{2}}{\\log\\left(1/\\delta_{0}\\right)}}\\end{array}$ thn, with prohabiry $1-6e^{-C_{4}d^{2}}$ \uff0c ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{d}\\left\\|\\hat{Q}-Q^{\\star}\\right\\|_{F}^{2}\\leqslant\\delta_{0}\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, combinign this result with Lemma 2, seting $\\hat{\\pi}$ as in Equation (10), we obtain Theorem 2. ", "page_idx": 16}, {"type": "text", "text": "D.1 Proof of Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 3. Recall that $\\delta,\\kappa,\\varepsilon\\,>\\,0$ are for now any (small) positive number but can be specified later. $\\delta_{0}$ is the target error. We begin by giving a few notations. For the proof, we need to introduce the following probability ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\beta(\\delta,\\kappa):=\\mathbb{P}\\left(X\\in\\mathcal{C}(u,\\delta),\\|X\\|\\geqslant1/\\kappa\\right)\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $X\\sim{\\mathcal{N}}(0,I_{d})$ and $u$ is any unit vector in $\\mathbb{R}^{d}$ . Note that $\\beta(\\delta,\\kappa)$ is independent of the choice of $u$ by rotational invariance of Gaussian distribution. It is easy to check that $\\mathbb{P}\\left(x_{i}\\in\\mathcal{C}_{\\mathcal{X}}(u,\\delta)\\right)=$ $\\mathbb{P}\\left(y_{j}\\in\\mathcal{C}_{\\mathcal{X}}(u,\\delta)\\right)=\\beta(\\delta,\\kappa)$ for any $i,j$ and any unit vector $u$ ", "page_idx": 16}, {"type": "text", "text": "Step 1: General strategy. Our goal is to prove that w.h.p. we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nF(\\hat{Q})<\\operatorname*{inf}\\left\\{F(Q),Q\\in\\mathcal{N},\\|Q-Q^{\\star}\\|_{F}^{2}>\\delta_{0}d\\right\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "forsome $\\delta_{0}>0$ to be determined. This will entail that $\\left\\|\\hat{Q}-Q^{\\star}\\right\\|_{F}^{2}\\leqslant\\delta_{0}d$ ", "page_idx": 16}, {"type": "text", "text": "Step 2: An upper bound on $\\mathbb{E}\\left[F(\\hat{Q})\\right]$ . Since $\\mathcal{N}$ is an \u03b5-net of $O(d)$ , there exists $Q_{\\varepsilon}^{\\star}\\in\\mathcal{N}$ such that $\\|Q_{\\varepsilon}^{\\star}-Q^{\\star}\\|_{F}\\leqslant\\varepsilon$ Note that by optimality of $\\hat{Q}$ , one has $F(\\hat{Q})\\leqslant F(Q_{\\varepsilon}^{\\star})$ . We first upper bound the left hand side in (17) by upper bounding the expectation of $F(Q_{\\varepsilon}^{\\star})$ using the following result. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5. Let $Q\\in{\\mathcal{O}}(d)$ \uff0c $u\\in S^{d-1}$ . We have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F(Q)\\right]\\leqslant2n\\beta(\\delta,\\kappa)\\left(c^{\\prime}d\\left[\\frac{2B\\sigma}{\\sqrt{6d}\\delta}+\\rho(Q^{\\star}-Q)/\\delta\\right]+2e^{-B^{2}/2}+e^{-d}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for any $B^{2}>0$ where for some matrix $M$ \uff0c $\\rho(M)$ is defined as its spectral radius. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 (proved in next subsection) gives, for any $B>0$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\hat{Q})\\leqslant F(Q_{\\varepsilon}^{\\star})=\\mathbb{E}\\left[F(Q_{\\varepsilon}^{\\star})\\right]+F(Q_{\\varepsilon}^{\\star})-\\mathbb{E}\\left[F(Q_{\\varepsilon}^{\\star})\\right]}\\\\ &{\\qquad\\leqslant2n\\beta(\\delta,\\kappa)\\left(c^{\\prime}d\\left[\\frac{2B\\sigma}{\\sqrt{6d}\\delta}+\\rho(Q^{\\star}-Q)/\\delta\\right]+2e^{-B^{2}/2}+e^{-d}\\right)+F(Q_{\\varepsilon}^{\\star})-\\mathbb{E}\\left[F(Q_{\\varepsilon}^{\\star})\\right]}\\\\ &{\\qquad\\leqslant2n\\beta(\\delta,\\kappa)\\left(c^{\\prime}d\\left[\\frac{2B\\sigma}{\\sqrt{6d}\\delta}+\\frac{\\varepsilon}{\\delta}\\right]+2e^{-B^{2}/2}+e^{-d}\\right)+\\operatorname*{sup}_{Q\\in N}\\left|F(Q)-\\mathbb{E}\\left[F(Q)\\right]\\right|\\,,\\qquad(1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used $\\rho(Q^{\\star}-Q_{\\varepsilon}^{\\star})\\leqslant\\left\\|\\hat{Q}-Q_{\\varepsilon}^{\\star}\\right\\|_{F}\\leqslant\\varepsilon$ in the above. ", "page_idx": 16}, {"type": "text", "text": "Step 3: A lower bound on E $[F(Q)]$ for any $Q$ W lower bound the right hand side in (17) using the following Lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma 6. Let $Q\\in{\\mathcal{O}}(d)$ \uff0c $u\\in S^{d-1}$ .We have, conditionally on the directions $u_{1},\\dotsc,u_{p}$ \uff0c ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[F(Q)\\middle|u_{1},\\dots,u_{p}\\right]\\geqslant2C_{1}\\beta(\\delta,\\kappa)\\frac{\\sum_{k=1}^{p}1_{\\left\\{\\|(Q^{\\star}-Q)u_{k}\\|_{F}^{2}>4\\delta+32\\sigma\\right\\}}}{p}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We recall that $\\beta(\\delta,\\kappa)$ is defined in (16) here above. In the sequel, we denote by $\\mathbb{P}_{U}$ (resp. $\\mathbb{E}_{U.}$ the probability (resp. expectation) over the directions $u_{1},\\dotsc,u_{p}$ . Lemma 6 (proved in next subsection) gives that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{inf}\\left\\{F(Q),Q\\in\\mathcal{N},\\|Q-Q^{\\star}\\|_{F}^{2}>\\delta_{0}d\\right\\}\\geqslant\\operatorname*{inf}_{\\stackrel{Q\\in\\mathcal{N}}{\\|Q-Q^{\\star}\\|_{F}^{2}>\\delta_{0}d}}\\mathbb{E}\\left[F(Q)\\right]-\\operatorname*{sup}_{Q\\in\\mathcal{N}}|F(Q)-\\mathbb{E}\\left[F(Q)\\right]|}}\\\\ &{\\geqslant\\frac{2C_{1}n\\beta(\\delta,\\kappa)}{p}\\operatorname*{inf}_{\\stackrel{Q\\in\\mathcal{N}}{\\|Q-Q^{\\star}\\|_{F}^{2}>\\delta_{0}d}}\\mathbb{E}_{U}\\left[\\displaystyle\\sum_{k=1}^{p}1_{\\left\\{\\|(Q-Q^{\\star})u_{k}\\|^{2}>4\\delta+32\\sigma\\right\\}}\\right]}\\\\ &{\\quad-\\operatorname*{sup}_{Q\\in\\mathcal{N}}|F(Q)-\\mathbb{E}\\left[F(Q)\\right]|~.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, if we take $\\delta_{0}\\geqslant8\\delta+64\\sigma$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{|Q-Q^{*}||_{F}^{\\frac{1}{2}}>\\delta_{0}d}{\\operatorname*{inf}}}&{\\mathbb{E}_{U}\\left[\\sum_{k=1}^{p}\\{\\|(Q-Q^{*})u_{k}\\|^{2}\\Im*\\delta+32\\sigma\\}\\right]}\\\\ {\\underset{|Q-Q^{*}||_{F}^{\\frac{1}{2}}>\\delta_{0}d}{\\geqslant}}&{}\\\\ &{\\begin{array}{r l}{\\frac{\\operatorname*{inf}}{Q\\in K}}&{\\mathbb{E}_{U}\\left[\\sum_{k=1}^{p}\\mathbb{I}_{\\{|Q-Q^{*}\\rangle u_{k}\\|^{2}>\\delta_{0}/2\\}}\\right]}\\\\ {\\underset{|Q-Q^{*}||_{F}^{\\frac{1}{2}}>\\delta_{0}d}{\\geqslant}}&{\\mathbb{E}_{U}\\left[\\sum_{k=1}^{p}\\mathbb{I}_{\\{|Q-Q^{*}\\rangle u_{k}\\|^{2}>\\frac{\\|Q-Q^{*}\\|_{F}^{\\frac{1}{2}}}{24}\\}}\\right]}\\end{array}}\\\\ &{\\geqslant\\begin{array}{r l}{\\underset{|Q-Q^{*}||_{F}^{\\frac{1}{2}}>\\delta_{0}d}{\\geqslant}}&{\\Big[\\underset{k=1}{\\overset{p}{\\prod}}\\sum_{\\stackrel{\\scriptstyle=}1{\\overset{1}{2}}}\\Big\\{\\|Q-Q^{*}\\rangle u_{k}\\|^{2}>\\frac{\\|Q-Q^{*}\\|_{F}^{\\frac{1}{2}}}{24}\\Big\\}\\Big]}\\\\ &{=p\\ \\ \\underset{|Q-Q^{*}||_{F}^{\\frac{1}{2}}>\\delta_{0}d}{\\operatorname*{inf}}}&{\\mathbb{P}_{U}\\left(\\|(Q-Q^{*})u_{1}\\|^{2}>\\frac{\\|Q-Q^{*}\\|_{F}^{\\frac{1}{2}}}{2d}\\right)\\,.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that if $Z\\sim\\mathcal{N}(0,I_{d}/d)$ , by rotational invariance of the Gaussian, one can always write $Z=N u_{1}$ where $N\\,=\\,\\|Z\\|$ and $\\begin{array}{r}{u_{1}\\,=\\,\\frac{Z}{||Z||}}\\end{array}$ Z are independent and u1 is uniform on the sphere. This yields $\\begin{array}{r}{\\frac{\\|Q-Q^{*}\\|_{F}^{2}}{d}=\\mathbb{E}\\left[\\|(Q-Q^{\\star})Z\\|^{2}\\right]=\\mathbb{E}\\left[N^{2}\\right]\\mathbb{E}_{U}[\\|(Q-Q^{\\star})u_{1}\\|^{2}]=1\\times\\mathbb{E}_{U}[\\|(Q-Q^{\\star})u_{1}\\|^{2}].}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "We can lower bound the right hand side of the above using a reverse Markov inequality, namely that $\\mathbb{P}\\left(X>\\mathbb{E}\\left[X\\right]/2\\right)\\geqslant\\mathbb{E}\\left[X\\right]/8$ for any $X$ such that $0\\,\\leqslant\\,X\\,\\leqslant\\,4$ a.s. We apply this to $X\\,=$ $\\left\\Vert(Q-Q^{\\star})u_{1}\\right\\Vert^{2}$ and get that for all $Q\\in{\\mathcal{N}}$ such that $\\|Q-Q^{\\star}\\|_{F}^{2}>\\delta_{0}d,$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}_{U}\\left(\\left\\|(Q-Q^{\\star})u_{1}\\right\\|^{2}>\\frac{\\left\\|Q-Q^{\\star}\\right\\|_{F}^{2}}{2d}\\right)\\geqslant\\frac{\\left\\|Q-Q^{\\star}\\right\\|_{F}^{2}}{8d}\\geqslant\\frac{\\delta_{0}}{8},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and via Equation (21), Equation (19) becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{inf}\\left\\{F(Q),Q\\in\\mathcal{N},\\|Q-Q^{\\star}\\|_{F}^{2}>\\delta_{0}d\\right\\}\\geqslant\\frac{C_{1}n\\beta(\\delta,\\kappa)\\delta_{0}}{4}-\\operatorname*{sup}_{Q\\in\\mathcal{N}}\\left|F(Q)-\\mathbb{E}\\left[F(Q)\\right]\\right|\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Step 4:Uniform concentration of $F(Q)$ around its mean. The remaining step is to control the concentration of $\\overline{{F(Q)}}$ uniformly on $\\mathcal{N}$ . This is given by the following. ", "page_idx": 17}, {"type": "text", "text": "Lemma 7 (Concentration of $F$ ). Let $Q\\,\\in\\,{\\mathcal{O}}(d)$ be fixed. Recall that $q=p^{3}$ ,that $v_{1},\\ldots,v_{q}$ are iid.uniformly samled on th spre $S^{d-1}$ and that $u_{1},\\dotsc,u_{p}$ are i.d. uniformly sampled in $\\{v_{1},\\ldots,v_{q}\\}$ .We have, for all $\\lambda>0$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|F(Q)-\\mathbb{E}\\left[F(Q)\\right]\\right|\\geqslant\\frac{4\\sqrt{2}\\lambda\\left(3\\log(p)+\\lambda+\\frac{\\log(q)^{2}+\\lambda^{2}}{9n\\beta(\\delta,\\kappa)}\\right)n\\beta(\\delta,\\kappa)}{\\sqrt{p}}\\right)\\leqslant4e^{-\\lambda}+2e^{-\\lambda^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, plugging $\\lambda=2\\log(|\\mathcal{N}|)>1$ (assuming $|{\\mathcal{N}}|\\geqslant2,$ in Lemma 7, with probability at least $1-{\\frac{6}{|{\\mathcal{N}}|}}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{Q\\in\\mathcal{N}}{\\operatorname*{sup}}|F(Q)-\\mathbb{E}\\left[F(Q)\\right]|}\\\\ &{\\quad\\leqslant8\\sqrt{2}\\log(|{\\cal N}|)\\left(3\\log(p)+2\\log(|{\\cal N}|)+\\frac{9\\log(p)^{2}+4\\log(|{\\cal N}|)^{2}}{9n\\beta(\\delta,\\kappa)}\\right)n\\beta(\\delta,\\kappa)p^{-1/2}}\\\\ &{\\quad\\leqslant c_{4}d^{2}\\log(1/\\varepsilon)\\operatorname*{max}(\\log(p),d^{2}\\log(1/\\varepsilon))n\\beta(\\delta,\\kappa)p^{-1/2}}\\\\ &{\\quad\\quad+c_{5}d^{2}\\log(1/\\varepsilon)\\operatorname*{max}(\\log(p),d^{2}\\log(1/\\varepsilon))^{2}p^{-1/2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used $\\log(|{\\mathcal{N}}|)\\leqslant c_{6}d^{2}\\log(1/\\varepsilon)$ in the above. ", "page_idx": 18}, {"type": "text", "text": "Step 5: Wrapping things up. Putting together the control on deviation in (23), the upper bound (18) and the lower bound (22), one gets that with probability $\\geqslant1-6/|\\mathcal{N}|$ ,for any $B>0$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{nf}\\left\\{F(Q),Q\\in\\mathcal{N},\\|Q-Q^{*}\\|_{F}^{2}>\\delta_{0}d\\right\\}-F(\\hat{Q})}\\\\ &{\\geqslant\\frac{C_{1}n\\beta(\\delta,\\kappa)\\delta_{0}}{4}-2n\\beta(\\delta,\\kappa)\\left(c^{\\prime}d\\left[\\frac{2B\\sigma}{\\sqrt{6}d\\delta}+\\frac{\\varepsilon}{\\delta}\\right]+2e^{-B^{2}/2}+e^{-d}\\right)}\\\\ &{\\ \\ \\ \\ \\ -\\ 2\\underset{Q\\in\\mathcal{N}}{\\operatorname*{sup}}|F(Q)-\\mathbb{E}|F(Q)|}\\\\ &{\\geqslant n\\beta(\\delta,\\kappa)\\frac{C_{1}\\delta_{0}}{4}}\\\\ &{\\ \\ \\ \\ -n\\beta(\\delta,\\kappa)\\left(c^{\\prime}d\\left[\\frac{2B\\sigma}{\\sqrt{6}d\\delta}+\\frac{\\varepsilon}{\\delta}\\right]+2e^{-B^{2}/2}+e^{-d}-c_{4}d^{2}\\log(1/\\varepsilon)\\operatorname*{max}(\\log(p),d^{2}\\log(1/\\varepsilon))p^{-1/2}\\right)}\\\\ &{\\ \\ \\ \\ -c_{5}d^{2}\\log(1/\\varepsilon)\\operatorname*{max}(\\log(p),d^{2}\\log(1/\\varepsilon))^{2}p^{-1/2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If this lower bound is positive, we can conclude that $\\left\\|\\hat{Q}-Q^{\\star}\\right\\|_{F}^{2}\\leqslant\\delta_{0}d,$ as desired. ", "page_idx": 18}, {"type": "text", "text": "So far, the only constraints on our constants are ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\delta_{0}\\geqslant8\\delta+64\\sigma\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "While the noise parameter $\\sigma$ is fixed in this proof, we have the freedom to impose some constraints on $\\varepsilon$ (the granularity of the \u03b5--net), $\\delta$ (the width of the cones), $\\kappa$ (the truncature parameter), $p$ (the number of directions) and $B$ in order to make the expression in (25) positive (and even $\\gg1$ 0.The remaining step is to show that this is possible ; this is what we shall do now. ", "page_idx": 18}, {"type": "text", "text": "Recall that we are in a regime where we need to keepn in our minf that $n$ tends to $+\\infty$ and $d$ tendsto $+\\infty$ With $n$ but with $d\\leqslant\\log(n)$ . We want to show that the positive term in (25) can dominate the others ; first, we would like to have an inequality of the form ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\mathcal{Z}_{1}\\delta_{0}}{4}-c^{\\prime}d\\left[\\frac{2B\\sigma}{\\sqrt{6d}\\delta}+\\frac{\\varepsilon}{\\delta}\\right]+2e^{-B^{2}/2}+e^{-d}-c_{4}d^{2}\\log(1/\\varepsilon)\\operatorname*{max}(\\log(p),d^{2}\\log(1/\\varepsilon))p^{-1/2}>c_{6}\\delta_{0}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is going to be satisfied if: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}>8c_{6},}\\\\ &{\\delta_{0}\\delta\\geqslant c_{7}\\sqrt{d}\\sigma B,}\\\\ &{\\delta_{0}\\delta\\geqslant c_{8}d\\varepsilon,}\\\\ &{\\delta_{0}\\geqslant c_{9}(e^{-B^{2}/2}+e^{-d}),}\\\\ &{\\delta_{0}\\geqslant c_{10}d^{2}\\log(1/\\varepsilon)\\operatorname*{max}(\\log(p),d^{2}\\log(1/\\varepsilon))p^{-1/2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $c_{7},c_{8},c_{9},c_{10}$ are large enough constants. Now, \u00b7 (A2) is easily verified by choosing $c_{6}$ ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u00b7 $B$ only appears in (A3) and (A5). (A5) is satisfied if we take $\\delta_{0}\\,\\geqslant\\,2c_{9}e^{-d}$ and $B^{2}\\,=$ $c_{11}\\operatorname*{max}(1,\\log(1/\\delta_{0}))$ , transforming (A3) into $\\begin{array}{r}{\\frac{\\delta_{0}\\delta}{\\sqrt{\\operatorname*{max}(1,\\log(1/\\delta_{0}))}}\\geqslant c_{12}\\sqrt{d}\\sigma}\\end{array}$ \u00b7 e appears i (A4) and can be taken as e\u2264 f for this condition to be satisfied. Combined with (A2), we can simply take $\\varepsilon\\leqslant c_{7}\\sqrt{d}\\sigma B/(c_{8}d)$ for this condition to be redundant; \u00b7 $p$ only appears in (A6) and can thus be taken as large as desired to have this inequality satisfied (very large $p$ does not degrade any bound). ", "page_idx": 19}, {"type": "text", "text": "Consequently, the inequality in Equation (27) is satisfied for parameters that satisfy: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\varepsilon=c_{16}d^{-1/2}\\sigma\\,,\\qquad\\qquad\\qquad}&{(\\mathsf{A}8)}&{p^{1/2}\\geqslant c_{13}\\sigma^{-1}d^{7/2}\\log(d/\\sigma)^{3}\\,,}\\\\ &{\\delta=\\delta_{0}\\,,\\qquad\\qquad\\qquad}&{(\\mathsf{A}10)}&{\\displaystyle\\frac{\\delta_{0}^{2}}{\\operatorname*{max}(1,\\log(1/\\delta_{0}))}\\geqslant c_{15}\\sqrt{d}\\sigma\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "thereby transforming the condition that the RHS in Equation (25) is positive into: ", "page_idx": 19}, {"type": "equation", "text": "$$\nn\\beta(\\delta,\\kappa)\\geqslant c_{16}d^{2}\\log(1/\\varepsilon)\\operatorname*{max}(\\log(p),d^{2}\\log(1/\\varepsilon))^{2}p^{-1/2}\\left[\\sqrt{d}\\sigma\\log(\\sqrt{d}\\sigma)\\right]^{-1}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The RHS of this inequality can be taken smaller than 1 by imposing that $p$ is large enough (recall that $p$ can be taken as large as desired). The final condition thus reads as $n\\beta(\\delta,\\kappa)\\geqslant1$ ,which is itself satisfied if ", "page_idx": 19}, {"type": "equation", "text": "$$\nn\\geqslant e^{c^{\\prime}d\\log(1/\\delta)}(1-e^{-d/16})^{-1}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\beta(\\delta,\\kappa)\\,=\\,\\mathbb{P}\\left(x_{1}\\in\\mathcal{C}_{\\mathcal{X}}(u_{0},\\delta),\\|x_{1}\\|\\geqslant1/\\kappa\\right)$ and $\\mathbb{P}\\left(x_{1}\\in\\mathcal{C}_{\\boldsymbol{X}}(u_{0},\\delta)\\right)\\,\\geqslant\\,e^{-c^{\\prime}d\\log(1/\\delta)}$ , while $\\mathbb{P}\\left(\\|x_{1}\\|\\geqslant1/\\kappa\\right)\\geqslant1-e^{-d/16}$ for ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\kappa^{2}=\\frac{2}{d}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We are now going to use the low-dimensionality asumption $d\\ll\\log(n)$ ,Since $n\\geqslant e^{c^{\\prime}d\\log(1/\\delta)}(1-$ $e^{-d/16})$ will be veried for ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\log(n)\\geqslant c^{\\prime\\prime}d\\log(1/\\delta)=c^{\\prime\\prime}d\\log(1/\\delta_{0})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, under(A-A13),quation 25is positive and therefore we have tat $\\begin{array}{r}{\\frac{1}{d}\\left\\|\\hat{Q}-Q^{\\star}\\right\\|_{F}^{2}\\leqslant\\delta_{0}}\\end{array}$ ", "page_idx": 19}, {"type": "text", "text": "D.2  Misceallenous lemmas on the path to proving Theorem 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We introduce the (numerical) constants $c,c^{\\prime}>0$ that verify, for all $\\delta^{\\prime}\\in\\left(0,1/4\\right)$ that for $x$ sampled uniformly on $S^{d-1}$ and any $u\\in S^{d-1}$ we have?: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\exp\\big(-c^{\\prime}d\\log(1/\\delta^{\\prime})\\big)\\leqslant\\mathbb{P}\\left(x\\in\\mathcal{C}(u,\\delta^{\\prime})\\right)\\leqslant\\exp\\big(-c d\\log(1/\\delta^{\\prime})\\big)\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The following lemmas are used to prove Lemma 6 and Lemma 5. ", "page_idx": 19}, {"type": "text", "text": "5In our model, the probability $\\mathbb{P}\\left(x\\in\\mathcal{C}(u,\\delta^{\\prime})\\right)$ can in fact be computed explicitly. For fixed $d$ and $_n$ the above probability is given by $1/2)\\mathbb{P}\\left(X_{1}^{2}\\geqslant(1-\\delta)^{2}(X_{1}^{2}+\\ldots X_{d}^{\\hat{2}})\\right)$ where the $\\left(X_{i}\\right)$ are standard i.i.d. Gaussn ariables I s tandarthat $\\frac{X_{1}^{2}}{\\|X\\|^{2}}$ $\\beta(1/2,(d-1)/2)$ \uff0c hence ", "page_idx": 19}, {"type": "equation", "text": "$$\n^{\\mathfrak{d}}\\big(x\\in\\mathcal{C}(u,\\delta^{\\prime})\\big)=\\frac{1}{2}\\mathbb{P}\\left(\\beta(1/2,(d-1)/2)\\geqslant(1-\\delta)^{2}\\right)=\\frac{\\Gamma(d/2)}{\\Gamma(1/2)\\Gamma(\\frac{d-1}{2})}\\int_{(1-\\delta)^{2}}^{1}x^{-1/2}(1-x)^{(d-3)/2}d x\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which is indeed of order $c\\delta^{d}$ when $\\delta$ is small. ", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma 5. We recall that for any $i,j,\\mathbb{P}\\left(x_{i}\\in\\mathcal{C}_{\\mathcal{X}}(u,\\delta)\\right)=\\mathbb{P}\\left(y_{j}\\in\\mathcal{C}_{\\mathcal{X}}(u,\\delta)\\right)=\\beta(\\delta,\\kappa)$ for any unit vector $u$ . Taking the expectation and developing the indicators, we have ", "page_idx": 20}, {"type": "text", "text": "\u6b63", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{\\ E}\\big[F(Q)\\big]}\\\\ &{\\displaystyle=\\frac{1}{p}\\sum_{k=1}^{p}\\sum_{i=1}^{n}\\Big(\\mathbb{P}\\left(x_{\\pi^{*}(i)}\\in\\mathcal{C}_{\\mathcal{X}}(Q u_{k},\\delta)\\right)+\\mathbb{P}\\left(y_{i}\\in\\mathcal{C}_{\\mathcal{Y}}(u_{k},\\delta)\\right)-2\\mathbb{P}\\left(x_{\\pi^{*}(i)}\\in\\mathcal{C}_{\\mathcal{X}}(Q u_{k},\\delta)|y_{i}\\in\\mathcal{C}_{\\mathcal{Y}}(u_{i,1})\\right)\\Big)}\\\\ &{\\displaystyle=\\frac{2\\beta(\\delta,\\kappa)}{p}\\sum_{k=1}^{p}\\sum_{i=1}^{n}\\left(1-\\mathbb{P}\\left(x_{\\pi^{*}(i)}\\in\\mathcal{C}(Q u_{k},\\delta),\\|x_{\\pi^{*}(i)}\\|\\geqslant1/\\kappa\\Big|y_{i}\\in\\mathcal{C}(u_{k},\\delta),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)\\right)}\\\\ &{\\displaystyle=2n\\beta(\\delta,\\kappa)\\left(1-\\mathbb{P}\\left(X\\in\\mathcal{C}(Q u,\\delta),\\|X\\|\\geqslant1/\\kappa\\Big|Y\\in\\mathcal{C}(u,\\delta),\\|Y\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $X\\sim{\\mathcal{N}}(0,I_{d})$ \uff0c $Y=Q^{\\star}X+\\sigma Z$ with $Z\\sim\\mathcal{N}(0,I_{d})$ independent from $X$ , and for any unit vector $u$ . We thus need to bound the last term, which is done by noticing that the two events in the remaining probability become highly positively correlated when $Q$ is close to $Q^{\\star}$ .First,weseparate the norm component from the direction component in the event $\\{X\\in{\\mathcal{C}}(Q u,\\delta),\\|X\\|\\geqslant1/\\kappa\\}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(X\\in\\mathcal{C}(Q u,\\delta),\\|X\\|\\geqslant1/\\kappa\\Big|Y\\in\\mathcal{C}(u,\\delta),\\|Y\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)}\\\\ &{\\,=\\mathbb{P}\\left(X\\in\\mathcal{C}(Q u,\\delta)\\Big|Y\\in\\mathcal{C}(u,\\delta),\\|Y\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)\\mathbb{P}\\left(\\|X\\|\\geqslant1/\\kappa\\|Y\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)}\\\\ &{\\,\\geqslant\\mathbb{P}\\left(X\\in\\mathcal{C}(Q u,\\delta)\\Big|Y\\in\\mathcal{C}(u,\\delta),\\|Y\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)\\mathbb{P}\\left(\\|X\\|\\geqslant1/\\kappa\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, we have $y_{i}\\;\\in\\;\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)\\;\\;\\Longrightarrow\\;\\;x_{\\pi^{\\star}(i)}\\;\\in\\;\\mathcal{C}_{\\mathcal{X}}(Q^{\\top}u,\\delta+\\delta_{i}(Q,u))$ using Lemma 12, where $\\begin{array}{r}{\\delta_{i}(Q)=2\\sigma\\frac{|\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle|}{\\left\\|x_{\\pi^{\\star}(i)}\\right\\|}+\\rho(Q^{\\star}-Q)}\\end{array}$ so thats ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{s}\\left(x_{\\pi^{*}(i)}\\in\\mathcal{C}(\\mathcal{Q},\\delta)\\Big|y_{i}\\in\\mathcal{C}(u,\\delta),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)}\\\\ &{=\\mathbb{P}\\left(x_{\\pi^{*}(i)}\\in\\mathcal{C}(\\mathcal{Q},\\delta)\\Big|y_{i}\\in\\mathcal{C}(u,\\delta),x_{\\pi^{*}(i)}\\in\\mathcal{C}(\\mathcal{Q},\\delta+\\delta_{i}(Q,u)),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)}\\\\ &{\\geqslant\\mathbb{E}\\left[\\exp\\left(-c^{\\prime}d\\log\\left(1+2\\sigma\\frac{|\\mathcal{L}_{\\delta}(Q^{*})|^{\\mathcal{T}}u|}{\\delta\\left\\|x_{\\pi^{*}(i)}\\right\\|}\\right)+\\rho(Q^{*}-Q)/\\delta\\right)\\right)\\left|\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right]}\\\\ &{\\geqslant\\mathbb{E}\\left[\\exp\\left(-c^{\\prime}d\\log\\left(1+\\frac{2\\sigma B^{\\prime}}{\\delta\\left\\|x_{\\pi^{*}(i)}\\right\\|}+\\rho(Q^{*}-Q)/\\delta\\right)\\right)\\left|\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa,|\\langle z_{i},(Q^{*})^{\\top}u\\rangle|\\leqslant B^{\\prime}\\right.}\\\\ &{\\qquad\\left.\\times\\mathbb{P}\\left(|\\langle z_{i},(Q^{*})^{\\top}u\\rangle|\\leqslant B^{\\prime}\\right)}\\\\ &{\\geqslant\\exp\\left(-c^{\\prime}d\\log\\left(1+\\frac{2\\kappa\\sigma B}{\\delta}+\\rho(Q^{*}-Q)/\\delta\\right)\\right)\\left(1-\\mathbb{P}\\left(|\\langle z_{i},(Q^{*})^{\\top}u\\rangle>B\\right)\\right)(1-\\mathbb{P}\\left(\\left\\|x_{\\pi^{*}(i)}\\right\\|\\geqslant\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "First, $\\mathbb{P}\\left(\\left\\Vert x_{\\pi^{\\star}(i)}\\right\\Vert^{2}>d+2{\\sqrt{d t}}+t\\right)\\;\\;\\leqslant\\;\\;e^{-t}$ for any $t\\ \\ >\\ \\ 0$ \uff0cso that if $1/\\kappa^{2}\\;\\;\\;\\geqslant\\;\\;\\;3d,$ $\\mathbb{P}\\left(\\left\\|x_{\\pi^{\\star}(i)}\\right\\|>1/\\kappa\\right)\\leqslant e^{-\\frac{1}{3\\kappa^{2}}+d}$ ", "page_idx": 20}, {"type": "text", "text": "Then, $|\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle|\\sim|\\mathcal{N}(0,1)|$ since $u$ us unitary, and thus $\\mathbb{P}\\left(|\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle|>B\\right)\\leqslant2e^{-B^{2}/2}\\leqslant$ $2e^{-2}$ for $B=2$ , leading to: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(x_{\\pi^{\\star}(i)}\\in\\mathcal{C}(Q,\\delta)\\bigg|y_{i}\\in\\mathcal{C}(u,\\delta),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)}\\\\ &{\\,\\,\\geqslant\\exp\\left(-c^{\\prime}d\\log\\big(1+\\displaystyle\\frac{2B\\kappa\\sigma}{\\delta}+\\rho(Q^{\\star}-Q)/\\delta\\big)\\right)\\big(1-2e^{-B^{2}/2}\\big)\\big(1-e^{-\\frac{1}{3\\kappa^{2}}+d}\\big)}\\\\ &{\\,\\,\\geqslant\\exp\\left(-c^{\\prime}d\\log\\big(1+\\displaystyle\\frac{2N\\sigma}{\\sqrt{6d}\\delta}+\\rho(Q^{\\star}-Q)/\\delta\\big)\\right)\\big(1-2e^{-B^{2}/2}-e^{-d}\\big)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for $\\kappa^{2}=1/(6d)$ . Using $\\log(1+x)\\leqslant x$ and $e^{-x}\\geqslant1-x$ for $x\\geqslant0$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(x_{\\pi^{\\star}(i)}\\in\\mathcal{C}(Q,\\delta)\\bigg|y_{i}\\in\\mathcal{C}(u,\\delta),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)}\\\\ &{\\geqslant\\exp\\left(-c^{\\prime}d\\big(\\cfrac{2B\\sigma}{\\sqrt{6d}\\delta}+\\rho(Q^{\\star}-Q)/\\delta\\big)\\right)\\left(1-2e^{-B^{2}/2}-e^{-d}\\right)}\\\\ &{\\geqslant\\left(1-c^{\\prime}d\\big(\\cfrac{2B\\sigma}{\\sqrt{6d}\\delta}+\\rho(Q^{\\star}-Q)/\\delta\\big)\\right)\\left(1-2e^{-B^{2}/2}-e^{-d}\\right)}\\\\ &{\\geqslant\\left(1-c^{\\prime}d\\left[\\cfrac{2B\\sigma}{\\sqrt{6d}\\delta}+\\rho(Q^{\\star}-Q)/\\delta\\right]-2e^{-B^{2}/2}-e^{-d}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "leading to ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1-\\mathbb{P}\\left(x_{\\pi^{\\star}(i)}\\in\\mathcal{C}(Q,\\delta)\\Big|y_{i}\\in\\mathcal{C}(u,\\delta),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa\\right)}\\\\ &{\\leqslant c^{\\prime}d\\left[\\frac{2B\\sigma}{\\sqrt{6d}\\delta}+\\rho(Q^{\\star}-Q)/\\delta\\right]+2e^{-B^{2}/2}+e^{-d}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and thus to the desired upper bound on E $[F(Q)]$ ", "page_idx": 21}, {"type": "text", "text": "Proof of Lemma 7.We first begin by bounding all the terms that appear in the sum of $F(Q)$ .Define ", "page_idx": 21}, {"type": "equation", "text": "$$\nA(u,Q)=|\\mathcal{C}_{\\mathcal{X}}(Q u,\\delta)|-|\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)|\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so that $\\begin{array}{r}{F(Q)=\\frac{1}{p}\\sum_{k=1}^{p}A(u_{k},Q)^{2}}\\end{array}$ Using Bernstein inequality [Vershynin, 2018, Theorem 2.8.4], and writing $\\beta(\\delta,\\kappa)=\\mathbb{P}\\left(x_{\\pi^{\\star}(i)}\\in\\mathcal{C}_{\\mathcal{X}}(Q u,\\delta)\\right)$ (so that $\\mathbb{E}\\left[A(u,Q)\\right]\\leqslant n\\beta(\\delta,\\kappa))$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|A(u,Q)|\\geqslant t\\right)\\leqslant2\\exp\\left(-\\frac{t^{2}/2}{n\\beta(\\delta,\\kappa)+t/3}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "so that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(A(u,Q)^{2}\\geqslant n\\beta(\\delta,\\kappa)t\\right)\\leqslant2\\exp\\left(-\\frac{n\\beta(\\delta,\\kappa)t/2}{n\\beta(\\delta,\\kappa)+\\sqrt{n\\beta(\\delta,\\kappa)t}/3}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leqslant2\\exp\\left(-\\frac{t}{4}\\right)+2\\exp\\left(-\\frac{3\\sqrt{n\\beta(\\delta,\\kappa)t}}{2}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{>}\\left(\\exists\\ell\\in[q]\\,,\\quad A(v_{\\ell},Q)^{2}\\geqslant n\\beta(\\delta,\\kappa)t\\right)\\leqslant2\\exp\\left(-\\frac{t}{4}+\\log(q)\\right)+2\\exp\\left(-\\frac{3\\sqrt{n\\beta(\\delta,\\kappa)t}}{2}+\\log(q)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=4e^{-\\lambda}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for $\\begin{array}{r}{t=4(3\\log(p)+\\lambda)+\\frac{4(\\log(q)^{2}+\\lambda^{2})}{9n\\beta(\\delta,\\kappa)}}\\end{array}$ We nowueMacDiamid's inequality Vershyin, , Theorem 2.9.1], by seeing $F(Q)$ as $F(Q)~=~f(u_{1},\\ldots,u_{p})$ , conditionally on the event $\\forall\\ell\\ \\in$ $\\begin{array}{r}{[q],A(v_{\\ell},Q)^{2}\\leqslant\\left(4(3\\log(p)+\\lambda)+\\frac{4(\\log(q)^{2}+\\lambda^{2})}{9n\\beta(\\delta,\\kappa)}\\right)n\\beta(\\delta,\\kappa)=B}\\end{array}$ to obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|F(Q)-\\mathbb{E}\\left[F(Q)|V\\right]\\right|\\geqslant t\\middle|\\forall\\ell\\in[q],A(v_{\\ell},Q)^{2}\\leqslant B\\right)\\leqslant2\\exp\\left(\\frac{p t^{2}}{2B^{2}}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $V=\\{v_{1},\\ldots,v_{q}\\}$ , since the bounded difference inequality is then verified for constant $4B$ Thus, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma\\left(\\big|F(Q)-\\mathbb{E}\\left[F(Q)\\vert V\\right]\\big|\\geqslant\\frac{\\sqrt{2}\\left(4(3\\log(p)+\\lambda)+\\frac{4(\\log(q)^{2}+\\lambda^{2})}{9n\\beta(\\delta,\\kappa)}\\right)n\\beta(\\delta,\\kappa)}{\\sqrt{p}}\\right)\\leqslant4e^{-\\lambda}+2e^{-\\lambda^{2}}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The problem here lies in the fact that $\\mathbb{E}\\left[F(Q)|V\\right]=\\mathbb{E}\\left[F(Q)\\right]$ may not always hold! Hopefully this is in fact the case: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[F(Q)\\vert V\\right]=\\displaystyle\\frac{1}{p q}\\sum_{k=1}^{p}\\sum_{\\ell=1}^{q}\\mathbb{E}\\left[(\\vert\\mathcal{C}_{x}(Q v_{\\ell},\\delta)\\vert-\\vert\\mathcal{C}_{y}(v_{\\ell},\\delta)\\vert)^{2}\\vert u_{k}=v_{\\ell}\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[(\\vert\\mathcal{C}_{x}(Q v,\\delta)\\vert-\\vert\\mathcal{C}_{y}(v,\\delta)\\vert)^{2}\\right]\\qquad\\mathrm{for~any~fixed~}v\\in\\mathcal{S}^{d-1}}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[F(Q)\\right]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "concluding the proof. ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 6. Let a $\\varepsilon\\qquad>\\qquad0$ tobedeterminedlater and $k$ $\\in$ $[p]$ such that $\\|(Q-Q^{\\star})u_{k}\\|\\quad\\quad>\\quad\\quad\\varepsilon.$ Weare going\u3001toshow that $\\mathbb{P}\\left(x_{\\pi^{\\star}(i)}\\in\\mathcal{C}(Q u_{k},\\delta)\\Big|y_{i}\\in\\mathcal{C}(u_{k},\\delta),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa,\\|(Q-Q^{\\star})u_{k}\\|>\\varepsilon\\right)$ is small. ", "page_idx": 22}, {"type": "text", "text": "Using Lemma $y_{i}\\in\\mathcal{C}(u_{k},\\delta)$ $\\begin{array}{r}{x_{\\pi^{\\star}(i)}\\in\\mathcal{C}(Q^{\\star}u_{k},\\delta+2\\frac{\\sigma\\|z_{i}\\|}{\\|x_{\\pi^{\\star}(i)}\\|})}\\end{array}$ Then. $\\mathcal{C}(Q u_{k},\\delta)\\cap$ $\\begin{array}{r}{\\mathcal{C}(Q^{\\star}u_{k},\\delta+2\\frac{\\sigma\\|z_{i}\\|}{\\left\\|x_{\\pi^{\\star}(i)}\\right\\|})=\\emptyset}\\end{array}$ provided that $\\begin{array}{r}{\\|Q u_{k}-Q^{\\star}u_{k}\\|^{2}>4(\\delta+\\frac{\\sigma\\|z_{i}\\|}{\\|x_{\\pi^{\\star}(i)}\\|})}\\end{array}$ using Lemma 8. Thus, if ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|^{|\\alpha\\cdot\\pi^{*}(i)|}\\|}\\\\ &{\\|Q u_{k}-Q^{*}u_{k}\\|^{2}>\\varepsilon,}\\\\ &{\\mathbb{P}\\left(x_{\\pi^{\\star}(i)}\\in\\mathcal{C}(Q u_{k},\\delta)\\Big|y_{i}\\in\\mathcal{C}(u_{k},\\delta),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa,\\|(Q-Q^{\\star})u_{k}\\|>\\varepsilon\\right)}\\\\ &{\\leqslant\\mathbb{P}\\left(4(\\delta+\\frac{\\sigma\\|z_{i}\\|}{\\left\\|x_{\\pi^{\\star}(i)}\\right\\|})>\\varepsilon\\right)}\\\\ &{\\leqslant\\mathbb{P}\\left(\\frac{4\\sigma\\|z_{i}\\|}{\\|x_{\\pi^{\\star}(i)}\\|}>\\varepsilon-4\\delta\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We have $\\mathbb{P}\\left(\\left\\|z_{i}\\right\\|^{2}\\geqslant4d\\right)\\leqslant e^{-d}$ , and $\\begin{array}{r}{\\mathbb{P}\\left(\\left\\|x_{\\pi^{\\star}(i)}\\right\\|^{2}\\leqslant\\frac{d}{2}\\right)\\leqslant e^{-d/16}}\\end{array}$ , so that if $\\varepsilon\\geqslant4\\delta+32\\sigma$ we   \nhave $\\begin{array}{r l}&{\\cdot\\mathbb{P}\\left(\\frac{4\\sigma\\|z_{i}\\|}{\\|x_{\\pi^{*}(i)}\\|}>\\varepsilon-4\\delta\\right)\\leqslant\\mathbb{P}\\left(\\left\\|z_{i}\\right\\|^{2}\\geqslant4d\\right)+\\mathbb{P}\\left(\\left\\|x_{\\pi^{*}(i)}\\right\\|^{2}\\leqslant\\frac{d}{2}\\right)\\leqslant e^{-d}+e^{-16d},\\mathrm{leading~to}}\\\\ &{\\therefore}\\\\ &{\\tau_{\\pi^{*}(i)}\\in\\mathcal{C}(Q u_{k},\\delta)\\Big|y_{i}\\in\\mathcal{C}(u_{k},\\delta),\\|y_{i}\\|\\geqslant\\sqrt{1+\\sigma^{2}}/\\kappa,\\|(Q-Q^{\\star})u_{k}\\|>\\varepsilon\\Big)\\leqslant e^{-d}+e^{-d/16}\\leqslant1-C}\\end{array}$   \nP(   \nwhere $C_{1}=1/e+1/e^{1/16}>0$ is a numerical constant. This thus gives: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[F(Q)|U\\right]\\geqslant2C_{1}\\beta(\\delta,\\kappa)\\frac{\\sum_{k=1}^{p}1_{\\left\\{\\|(Q^{\\star}-Q)u_{k}\\|_{F}^{2}>\\varepsilon\\right\\}}}{p}\\,.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 8 (Cone separation). For $u,v\\in S^{d-1}$ $\\mathcal{C}(u,\\delta)\\cap\\mathcal{C}(v,\\delta)\\neq\\emptyset$ implies that $\\left\\Vert u-v\\right\\Vert^{2}\\leqslant8\\delta$ ", "page_idx": 22}, {"type": "text", "text": "Proof.Assume $\\mathcal{C}(u,\\delta)\\cap\\mathcal{C}(v,\\delta)\\neq\\emptyset$ .Take $w\\in\\mathcal{C}(u,\\delta)\\cap\\mathcal{C}(v,\\delta)$ : we can always assume that $\\|w\\|=1$ by rescaling. Then, by triangle inequality, we have $\\|u-v\\|\\leqslant\\|u-w\\|+\\|v-w\\|$ . Since $w\\in\\mathcal{C}(u,\\delta)$ $\\Vert u-w\\Vert^{2}=2-2\\langle v,w\\rangle\\leqslant2-2(1-\\delta)=2\\delta$ , and the same is true for $\\lVert\\boldsymbol{v}-\\boldsymbol{w}\\rVert$ . This gives $\\lVert u-v\\rVert\\leqslant2\\sqrt{2\\delta}$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Lemma 9 (Probability that two cones are disjoint). Let Q $\\prime,Q^{\\prime}\\in{\\mathcal{O}}(d)$ $\\begin{array}{r}{\\delta\\leqslant\\frac{1}{12d}\\|Q^{\\prime}-Q\\|_{F}^{2}}\\end{array}$ and let u be a random variable uniformly distributed over $S^{d-1}$ .Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\mathcal{C}(Q^{\\prime}u,\\delta\\right)\\cap\\mathcal{C}(Q u,\\delta)=\\emptyset\\right)\\geqslant\\delta\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Using the previous Lemma, $\\begin{array}{r}{\\mathbb{P}\\left(\\mathcal{C}(Q^{\\prime}u,\\delta)\\cap\\mathcal{C}(Q u,\\delta)\\neq\\varnothing\\right)\\leqslant\\mathbb{P}\\left(\\left\\|Q u-Q^{\\prime}u\\right\\|^{2}\\leqslant8\\delta\\right)\\!.}\\end{array}$ Let $Z$ be the random variable $Z=\\|Q u-Q^{\\prime}u\\|^{2}$ We have that $\\mathbb{E}\\left[Z\\right]\\mathrm{~=~}\\|Q-Q^{\\prime}\\|_{F}^{2}/d\\gtrsim\\mathrm{12}\\delta$ and $Z\\leqslant4$ almost surely. Thus, using a \u201creverse Markov?' inequality, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{12\\delta\\leqslant\\mathbb{E}\\left[Z\\right]=\\mathbb{E}\\left[Z1_{Z\\leqslant8\\delta}\\right]\\mathbb{P}\\left(Z\\leqslant8\\delta\\right)+\\mathbb{E}\\left[Z1_{Z>8\\delta}\\right]\\mathbb{P}\\left(Z>8\\delta\\right)}\\\\ &{\\quad\\quad\\leqslant8\\delta\\mathbb{P}\\left(Z\\leqslant8\\delta\\right)+4\\mathbb{P}\\left(Z>8\\delta\\right)}\\\\ &{\\quad\\quad\\leqslant8\\delta+4(1-\\mathbb{P}\\left(X\\leqslant8\\delta\\right))\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "that is $\\mathbb{P}\\left(X\\leqslant8\\delta\\right)\\leqslant1-\\delta$ , which concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "The following Lemma is easy and does require any proof. ", "page_idx": 23}, {"type": "text", "text": "Lemma 10. For any $u,$ $\\begin{array}{r l r}{\\delta}&{{}\\in}&{(0,1).}\\end{array}$ .we have $\\begin{array}{r l r}{\\mathbb{E}\\left[|\\mathcal{C}_{\\mathcal{X}}(u,\\delta)|\\right]}&{{}=}&{\\mathbb{E}\\left[|\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)|\\right]\\quad=}\\end{array}$ $n\\mathbb{P}\\left(x_{1}\\in\\mathcal{C}(u,\\delta),\\|x_{1}\\|\\geqslant1/\\kappa\\right)\\,=\\,n\\beta(\\delta,\\kappa)$ so that $|\\mathcal{C}_{\\mathcal{X}}(Q u,\\delta)|\\mathrm{~-~}|\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)|$ in the sum that defines $F$ are all centered. ", "page_idx": 23}, {"type": "text", "text": "$|\\mathcal{C}_{\\mathcal{X}}(u,\\delta)|$ and $|\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)|$ are (correlated) binomial random variables of parameters $(n,\\beta(\\delta,\\kappa))$ ", "page_idx": 23}, {"type": "text", "text": "Lemma 11. For any $u\\in S^{d-1}$ \uff0c $i\\in[n]$ we have $y_{i}\\in\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)\\implies x_{\\pi^{\\star}(i)}\\in\\mathcal{C}_{\\mathcal{X}}((Q^{\\star})^{\\top}u,\\delta+\\delta_{i})$ where $\\begin{array}{r}{\\delta_{i}=2\\sigma\\frac{\\|z_{i}\\|}{\\left\\|x_{\\pi^{\\star}(i)}\\right\\|}}\\end{array}$ and $x_{\\pi^{\\star}(i)}\\in\\mathcal{C}_{\\mathcal{X}}((Q^{\\star})^{\\top}u,\\delta+\\delta_{i}^{\\prime})\\implies y_{i}\\in\\mathcal{C}_{\\mathcal{Y}}(u,\\delta),$ where $\\begin{array}{r}{\\delta_{i}^{\\prime}=2\\sigma\\frac{\\|z_{i}\\|}{\\|y_{i}\\|}}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Let us prove the frst assertion and assume that $y_{i}\\in\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)$ Wehave $y_{i}=Q^{\\star}x_{\\pi^{\\star}(i)}\\!+\\!\\sigma z_{i}\\in$ $\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)$ , which writes as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle Q^{\\star}x_{\\pi^{\\star}(i)}+\\sigma z_{i},u\\rangle\\geqslant(1-\\delta)\\big\\|Q^{\\star}x_{\\pi^{\\star}(i)}+\\sigma z_{i}\\big\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle x_{\\pi^{\\star}(i)},(Q^{\\star})^{\\top}u\\rangle\\geqslant(1-\\delta)\\bigl\\|Q^{\\star}x_{\\pi^{\\star}(i)}+\\sigma z_{i}\\bigr\\|-\\sigma\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle}\\\\ &{\\qquad\\qquad\\geqslant(1-\\delta)\\bigl\\|Q^{\\star}x_{\\pi^{\\star}(i)}+\\sigma z_{i}\\bigr\\|-\\sigma\\|z_{i}\\|}\\\\ &{\\qquad\\qquad\\geqslant(1-\\delta)\\bigl(\\bigl\\|Q^{\\star}x_{\\pi^{\\star}(i)}\\bigr\\|-\\sigma\\|z_{i}\\|\\bigr)-\\sigma\\|z_{i}\\|}\\\\ &{\\qquad\\qquad\\geqslant(1-\\delta)\\bigl\\|Q^{\\star}x_{\\pi^{\\star}(i)}\\bigr\\|-2\\sigma\\|z_{i}\\|}\\\\ &{\\qquad\\qquad\\geqslant(1-\\delta-\\delta_{i})\\bigl\\|Q^{\\star}x_{\\pi^{\\star}(i)}\\bigr\\|\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which is the desired result. The second assertion is proved exactly in the same way. ", "page_idx": 23}, {"type": "text", "text": "Lemma 12. For any $u\\,\\in\\,{\\mathcal{S}}^{d-1},~i\\,\\in~[n],~Q\\,\\in\\,{\\mathcal{O}}(d)$ we have $y_{i}\\;\\in\\;{\\mathcal{C}}_{\\mathcal{Y}}(u,\\delta)\\;\\implies\\;x_{\\pi^{\\star}(i)}\\;\\in$ $\\mathcal{C}_{\\mathcal{X}}(Q^{\\top}u,\\delta+\\delta_{i}(Q,u))$ where $\\begin{array}{r}{\\delta_{i}(Q,u)=2\\sigma\\frac{|\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle|}{\\left\\|x_{\\pi^{\\star}(i)}\\right\\|}+\\rho(Q^{\\star}-Q).}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Assume that $y_{i}\\in\\mathcal{C}_{\\mathcal{Y}}(u,\\delta)$ . As in the proof of the previous proposition, this reads as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle x_{\\pi^{\\star}(i)},(Q^{\\star})^{\\top}u\\rangle\\geqslant(1-\\delta)\\big\\|Q^{\\star}x_{\\pi^{\\star}(i)}+\\sigma z_{i}\\big\\|-\\sigma\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and thus, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle x_{\\pi^{\\star}(i)},Q^{\\top}u\\rangle\\geqslant\\langle x_{\\pi^{\\star}(i)},(Q^{\\top}-(Q^{\\star})^{\\top})u\\rangle+(1-\\delta)\\|Q^{\\star}x_{\\pi^{\\star}(i)}+\\sigma z_{i}\\|-\\sigma\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\geqslant-\\|x_{\\pi^{\\star}(i)}\\|\\rho(Q-Q^{\\star})+(1-\\delta)\\|Q^{\\star}x_{\\pi^{\\star}(i)}+\\sigma z_{i}\\|-\\sigma\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\geqslant-\\big\\|x_{\\pi^{\\star}(i)}\\big\\|\\rho(Q-Q^{\\star})+(1-\\delta-\\frac{|\\langle z_{i},(Q^{\\star})^{\\top}u\\rangle|}{\\|x_{\\pi^{\\star}(i)}\\|})\\|x_{\\pi^{\\star}(i)}\\|}\\\\ &{\\quad\\quad\\quad\\quad=(1-\\delta-\\delta_{i}(Q,u))\\big\\|x_{\\pi^{\\star}(i)}\\big\\|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "E Proof of Proposition 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Very-fast sorting-based estimator and equivalence with one step of Frank- Wolfe ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla f(D)=2\\left(D X^{\\top}X X^{\\top}X-2Y^{\\top}Y D X^{\\top}X+Y^{\\top}Y Y^{\\top}Y D\\right)\\,,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any bistochastic matrix $D$ , leading to, for $\\begin{array}{r}{J=\\frac{11^{\\top}}{n}}\\end{array}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nabla f(J)=2\\left(J X^{\\top}X X^{\\top}X-2Y^{\\top}Y J X^{\\top}X+Y^{\\top}Y Y^{\\top}Y J\\right)\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any permutation matrix $P$ , we have since $J^{\\top}=J$ and $J P=J$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle J X^{\\top}X X^{\\top}X,P\\rangle=\\langle X^{\\top}X X^{\\top}X,J P\\rangle}\\\\ {=\\langle X^{\\top}X X^{\\top}X,J\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and similalry: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle Y^{\\top}Y Y^{\\top}Y J,P\\rangle=\\langle J Y^{\\top}Y Y^{\\top}Y,P^{\\top}\\rangle}\\\\ {=\\langle Y^{\\top}Y Y^{\\top}Y,J P^{\\top}\\rangle}\\\\ {=\\langle Y^{\\top}Y Y^{\\top}Y,J\\rangle\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underset{P\\in{\\cal S}_{n}}{\\arg\\operatorname*{min}}\\langle f(J),P\\rangle=\\underset{P\\in{\\cal S}_{n}}{\\arg\\operatorname*{max}}\\langle Y^{\\top}Y J X^{\\top}X,P\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have $(X^{\\top}X)_{i j}=\\langle x_{i},x_{j}\\rangle$ and $(J X^{\\top}X)_{i j}=n\\langle\\bar{x},x_{j}\\rangle$ Similarly, $(Y^{\\top}Y J)_{i j}=n\\langle\\bar{y},y_{i}\\rangle$ , and we have $J^{2}=J$ Thus, ", "page_idx": 24}, {"type": "equation", "text": "$$\n(Y^{\\top}Y J X^{\\top}X)_{i j}=n^{2}\\sum_{k=1}^{n}\\langle\\bar{y},y_{i}\\rangle\\langle\\bar{x},x_{j}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "leading to: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\underset{P\\in{\\cal S}_{n}}{\\arg\\operatorname*{min}}\\langle f(J),P\\rangle=\\underset{\\pi\\in{\\cal S}_{n}}{\\arg\\operatorname*{max}}\\sum_{i\\in[n]}\\sum_{k=1}^{n}\\langle\\bar{y},y_{i}\\rangle\\langle\\bar{x},x_{\\pi^{\\star}(i)}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and to the following sorting-based estimator, that can be computed very easily in $O(n d\\log(n))$ computes. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\pi}\\in\\arg\\operatorname*{max}_{\\pi\\in S_{n}}\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\pi(i)},\\bar{x}\\rangle\\langle y_{i},\\bar{y}\\rangle\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\textstyle{\\bar{x}}\\,=\\,{\\frac{1}{n}}\\sum_{i}x_{i}$ and $\\bar{y}=\\textstyle\\frac{1}{n}\\sum_{i}y_{i}$ are the mean vectors of each point cloud. The idea is that thanks to the scalar product, this estimator gets rid of the orthogonal trasformation. Its strength is that it can be computed in ${\\mathcal{O}}(n\\log(n))$ iterations, since it consists in sorting two vectors. We have the following result for this estimator. ", "page_idx": 24}, {"type": "text", "text": "Proposition 2. Let $\\delta\\in(0,1)$ and $\\varepsilon>0$ $f\\sigma\\ll n^{\\frac{12(1+2\\varepsilon)}{\\delta}}$ , the estimator $\\hat{\\pi}$ as defined in Equation (28) satisfies with high probability: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname{ov}({\\hat{\\pi}},\\pi^{\\star})\\geqslant1-\\delta\\,.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Without loss of generality, we can assume that $\\pi^{\\star}=\\operatorname{Id}$ . Then, for all $i$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle y_{i},\\bar{y}\\rangle=\\langle Q^{\\star}x_{i}+\\sigma z_{i},Q^{\\star}\\bar{x}+\\sigma\\bar{z}\\rangle}\\\\ &{\\qquad\\quad=\\langle x_{i},\\bar{x}\\rangle+\\sigma^{2}\\langle z_{i},\\bar{z}\\rangle+\\sigma\\langle z_{i},Q^{\\star}\\bar{x}\\rangle+\\sigma\\langle Q^{\\star}x_{i},\\bar{z}\\rangle\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so that: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{\\i}\\sum_{i=1}^{n}\\langle x_{\\pi(i)},\\bar{x}\\rangle\\langle y_{i},\\bar{y}\\rangle=\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\pi(i)},\\bar{x}\\rangle\\langle x_{i},\\bar{x}\\rangle+\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\pi(i)},\\bar{x}\\rangle\\left[\\sigma^{2}\\langle z_{i},\\bar{z}\\rangle+\\sigma\\langle z_{i},Q^{\\star}\\bar{x}\\rangle+\\sigma\\langle Q^{\\star}x_{i},\\bar{z}\\rangle\\right]}\\\\ {\\displaystyle=-\\frac{1}{2n}\\sum_{i=1}^{n}\\langle x_{\\pi(i)}-x_{i},\\bar{x}\\rangle^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{i},\\bar{x}\\rangle^{2}\\langle x_{i},\\bar{x}\\rangle}\\\\ {\\displaystyle\\quad\\;\\;+\\,\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\pi(i)},\\bar{x}\\rangle\\left[\\sigma^{2}\\langle z_{i},\\bar{z}\\rangle+\\sigma\\langle z_{i},Q^{\\star}\\bar{x}\\rangle+\\sigma\\langle Q^{\\star}x_{i},\\bar{z}\\rangle\\right]\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By definition of $\\hat{\\pi}$ , we have $\\begin{array}{r}{\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\hat{\\pi}(i)},\\bar{x}\\rangle\\langle y_{i},\\bar{y}\\rangle\\geqslant\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{i},\\bar{x}\\rangle\\langle y_{i},\\bar{y}\\rangle}\\end{array}$ , that thus writes as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{1}{2n}\\sum_{i=1}^{n}\\langle x_{\\pi(i)}-x_{i},\\bar{x}\\rangle^{2}\\leqslant\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\hat{\\pi}(i)}-x_{i},\\bar{x}\\rangle\\left[\\sigma^{2}\\langle z_{i},\\bar{z}\\rangle+\\sigma\\langle z_{i},Q^{\\star}\\bar{x}\\rangle+\\sigma\\langle Q^{\\star}x_{i},\\bar{z}\\rangle\\right]}\\\\ {\\displaystyle\\leqslant\\operatorname*{sup}_{i\\in[n]}\\left|\\langle x_{\\hat{\\pi}(i)}-x_{i},\\bar{x}\\rangle\\right|\\times\\frac{1}{n}\\sum_{i=1}^{n}\\left|\\sigma^{2}\\langle z_{i},\\bar{z}\\rangle+\\sigma\\langle z_{i},Q^{\\star}\\bar{x}\\rangle+\\sigma\\langle Q^{\\star}x_{i},\\bar{z}\\rangle\\right|\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We will frst bound this right hand side. First, for all $i,j\\in[n],x_{i}-x_{j}$ is independent from $\\textstyle{\\bar{x}}$ , so that conditionally on $\\bar{x}$ wehave $\\langle x_{i}-x_{j},{\\bar{x}}\\rangle\\sim\\mathcal{N}(0,2{\\|\\bar{x}\\|}^{2})$ , leading to: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\left\\langle x_{i}-x_{j},\\bar{x}\\right\\rangle\\right|>t\\|\\bar{x}\\|\\right)\\leqslant2\\exp(-t^{2}/2)\\,,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\forall i,j\\in[n],|\\langle x_{i}-x_{j},\\bar{x}\\rangle|>t\\|\\bar{x}\\|\\right)\\leqslant2\\exp(-t^{2}/2+2\\log(n))\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "so that with probability $1-2/n^{2}$ \uff0c $\\begin{array}{r}{\\operatorname*{sup}_{i,j}|\\langle x_{i}-x_{j},\\bar{x}\\rangle|\\leqslant2\\sqrt{2\\log(n)}\\|\\bar{x}\\|}\\end{array}$ . Similarly, with probability $1-4/n^{2},\\operatorname*{sup}_{i}|\\langle z_{i},Q^{\\star}\\bar{x}\\rangle|\\leqslant2\\sqrt{\\log(n)}\\|\\bar{x}\\|$ and $\\operatorname*{sup}_{i}|\\langle Q^{\\star}x_{i},\\bar{z}\\rangle|\\leqslant2\\sqrt{\\log(n)}\\|\\bar{z}\\|$ ", "page_idx": 25}, {"type": "text", "text": "Then, we can write $z_{i}\\;=\\;z_{i}^{\\prime}+\\bar{z}$ where $z_{i}^{\\prime}$ is Gaussian (its covariance matrix is the projection on the orthogonal of $\\bar{z}$ ) and independent from $\\bar{z}$ .Thus, $\\begin{array}{r}{\\operatorname*{sup}_{i}|\\langle z_{i},\\bar{z}\\rangle|\\,\\leqslant\\,\\|\\bar{z}\\|^{2}+\\operatorname*{sup}_{i}|\\langle z_{i}^{\\prime},\\bar{z}\\rangle|\\,\\leqslant}\\end{array}$ $\\left\\|{\\bar{z}}\\right\\|^{2}+2{\\sqrt{\\log(n)}}\\|{\\bar{z}}\\|$ with probability $1-2/n^{2}$ ", "page_idx": 25}, {"type": "text", "text": "Thus, with probability $1-8/n^{2}$ and for $\\sigma\\leqslant1$ \uff0c ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{sup}_{i\\in[n]}|\\langle x_{\\hat{\\pi}(i)}-x_{i},\\bar{x}\\rangle|\\times\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\big|\\sigma^{2}\\langle z_{i},\\bar{z}\\rangle+\\sigma\\langle z_{i},Q^{\\star}\\bar{x}\\rangle+\\sigma\\langle Q^{\\star}x_{i},\\bar{z}\\rangle\\big|}\\\\ {\\ll2\\sqrt{2\\log(n)}\\sigma\\|\\bar{x}\\|\\big[2\\sqrt{\\log(n)}\\|\\bar{x}\\|+\\|\\bar{z}\\|^{2}+4\\sqrt{\\log(n)}\\|\\bar{z}\\|\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, $n\\|{\\bar{x}}\\|^{2}$ and $n\\|\\bar{z}\\|^{2}$ are both $\\chi_{d}^{2}$ random variables, so that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\operatorname*{max}(|n||\\bar{x}||^{2}-d|,|n||\\bar{z}||^{2}-d|)>2t+2\\sqrt{d t}\\right)\\leqslant4e^{-t}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $t=2(\\sqrt{2}-1)d$ , this leads to, with probability $4e^{-2({\\sqrt{2}}-1)d}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|{\\bar{x}}\\right\\|^{2},\\left\\|{\\bar{z}}\\right\\|^{2}\\in[1/2,3/2]{\\frac{d}{n}}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, with probability 1 - 8/n2 - 4e-2(V2-1)d ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\operatorname*{sup}_{i\\in[n]}\\left\\vert\\langle x_{\\hat{\\pi}(i)}-x_{i},\\bar{x}\\rangle\\right\\vert\\times\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\left\\vert\\sigma^{2}\\langle z_{i},\\bar{z}\\rangle+\\sigma\\langle z_{i},Q^{\\star}\\bar{x}\\rangle+\\sigma\\langle Q^{\\star}x_{i},\\bar{z}\\rangle\\right\\vert}\\\\ {\\displaystyle\\leqslant2\\sigma\\sqrt{2\\log(n)}\\|\\bar{x}\\|^{2}\\left[2\\sqrt{\\log(n)}+3\\sqrt{d/n}+4\\sqrt{3\\log(n)}\\right]}\\\\ {\\displaystyle=C\\log(n)\\sigma\\|\\bar{x}\\|^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some numerical constant $C$ ,if $n\\geqslant d$ , leading to ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{2n}\\sum_{i=1}^{n}\\langle x_{\\hat{\\pi}(i)}-x_{i},\\bar{x}\\rangle^{2}\\leqslant C\\log(n)\\sigma\\|\\bar{x}\\|^{2}\\,.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, if $i\\neq j$ are fixed, for $t\\leqslant1$ \uff0c $\\begin{array}{r}{\\mathbb{P}\\left(\\frac{1}{2}\\langle x_{i}-x_{j},\\bar{x}\\rangle^{2}<t\\|\\bar{x}\\|^{2}\\right)=\\mathbb{P}\\left(\\mathcal{N}(0,1)^{2}<t\\right)\\leqslant c\\sqrt{t}}\\end{array}$ for some constant $c>0$ ", "page_idx": 25}, {"type": "text", "text": "We are now going to upper bound the probability of the event $A=$ \"there exists $\\mathcal{T}\\subset[n]$ with $|{\\mathcal{T}}|\\geqslant\\alpha n$ and $\\pi$ a permutation such that $(i)$ for all $i\\in\\mathcal{T}$ $\\pi(i)\\neq i$ (ii) $\\{i,\\pi(i)\\}_{i\\in\\mathbb{Z}}$ form disjoint pairs and $(i i i)$ for all $i\\in\\mathcal{Z}$ $\\begin{array}{r}{\\frac12\\langle x_{\\hat{\\pi}(i)}-x_{i},\\bar{x}\\rangle^{2}\\leqslant\\beta\\|\\bar{x}\\|^{2,\\ast}}\\end{array}$ , for some constants $\\alpha,\\beta\\in(0,1)$ to be fixed later. Let $\\mathcal{T}$ and $\\pi$ be fixed. Since $\\pi(i)\\neq i$ , we have $\\begin{array}{r}{\\mathbb{P}\\left(\\frac{1}{2}\\langle x_{i}-x_{\\pi(i)},\\bar{x}\\rangle^{2}>\\beta\\|\\bar{x}\\|^{2}\\right)\\leqslant2e^{-\\beta/2}}\\end{array}$ , and using $(i i)$ all pairs are independent, leading to: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb P\\left(\\pi,\\!Z\\operatorname{satisfies}\\left(i\\right)\\!\\cdot\\!(i i)\\!\\cdot\\!(i i i)\\right)\\leqslant\\mathbb P\\left(\\forall i\\in\\mathbb Z,\\quad\\frac12\\langle x_{i}-x_{\\pi(i)},\\bar{x}\\rangle^{2}>\\beta\\|\\bar{x}\\|^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leqslant c\\sqrt{\\beta}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, using a union bound over all possible $\\mathcal{T}$ and $\\pi$ , we have that: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left(\\mathcal{A}\\right)\\leqslant2^{n}n^{n}e^{-\\alpha\\beta n/2+\\alpha n\\log\\left(2\\right)}}\\\\ &{\\qquad\\quad=e^{\\log\\left(c\\sqrt{\\beta}\\right)\\alpha n+n\\log\\left(n\\right)+(1+\\alpha)n\\log\\left(2\\right)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, using what we have proved above, denoting $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ the event $\\begin{array}{r l r}{\\frac{1}{2n}\\sum_{i=1}^{n}\\langle x_{\\hat{\\pi}(i)}\\;-\\;x_{i},\\bar{x}\\rangle^{2}}&{\\leqslant}&\\end{array}$ $C\\log(n)\\sigma\\|{\\bar{x}}\\|^{2}$ , we have $\\mathbb{P}\\left(\\mathcal{B}\\right)\\geqslant1-8/n^{2}-4e^{-2(\\sqrt{2}-1)d}$ Let $\\mathcal{C}$ be the event $\\left\\{\\operatorname{ov}(\\hat{\\pi},\\pi^{\\star})\\leqslant1-\\delta\\right\\}$ ", "page_idx": 25}, {"type": "text", "text": "Under $\\mathcal{C}\\cap\\mathcal{B}$ , we have the existence of $\\mathcal{T}^{\\prime}\\subset[n]$ such that for all indices $i\\in\\mathcal{T}^{\\prime}$ \uff0c $\\hat{\\pi}\\neq i$ and $\\left|Z^{\\prime}\\right|\\geqslant\\delta n/6$ Now, since then $\\begin{array}{r}{\\frac{1}{2|\\mathbb{Z}^{\\prime}|}\\sum_{i\\in\\mathbb{Z}^{\\prime}}\\langle x_{\\hat{\\pi}(i)}-x_{i},\\bar{x}\\rangle^{2}\\leqslant\\frac{6}{\\delta}\\times C\\log(n)\\sigma\\|\\bar{x}\\|^{2}}\\end{array}$ , we have that at least half of these indices satisfy $\\begin{array}{r}{\\frac{1}{2}\\langle x_{\\hat{\\pi}(i)}\\,-\\,x_{i},\\bar{x}\\rangle^{2}\\,\\leqslant\\,\\frac{12}{\\delta}\\,\\times\\,C\\log(n)\\sigma\\|\\bar{x}\\|^{2}}\\end{array}$ : we denote by $\\hat{\\mathcal{T}}$ the set of these indices. Hene, $\\hat{\\pi},\\hat{\\mathcal{T}}$ satisfy properties $(i)\u2013(i i)\u2013(i i i)$ with $\\begin{array}{r}{\\alpha=\\frac{\\delta}{12}}\\end{array}$ and $\\begin{array}{r}{\\beta=\\frac{12}{\\delta}\\times C\\log(n)\\sigma}\\end{array}$ leading to (taking these constants for ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left({B\\cap\\mathcal{C}}\\right)\\leqslant\\mathbb{P}\\left({A}\\right)}\\\\ &{\\qquad\\qquad\\leqslant e^{\\log(c\\sqrt{\\beta})\\alpha n+n\\log(n)+(1+\\alpha)n\\log(2)}}\\\\ &{\\qquad\\quad=\\exp\\left(\\frac{\\delta n\\log\\,\\left(12C\\delta^{-1}\\log(n)\\sigma\\right)}{12}+n\\log(n)+2n\\log(2)\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For this probability to be close to zero, we thus need that $\\begin{array}{r}{-\\frac{\\delta n\\log\\left(12C\\delta^{-1}\\log(n)\\sigma\\right)}{12}\\geqslant(1+\\varepsilon)n\\log(n),}\\end{array}$ which can be written as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n-\\log\\left(12C\\delta^{-1}\\log(n)\\sigma\\right)\\geqslant\\frac{12(1+\\varepsilon)\\log(n)}{\\delta}=\\log\\left(n^{\\frac{12(1+\\varepsilon)}{\\delta}}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "which is satisfied for $\\sigma\\ll n^{\\frac{12(1+2\\varepsilon)}{\\delta}}$ ", "page_idx": 26}, {"type": "text", "text": "E.2 The \u201cAce\"estimator ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proposition 3 (Ace). Let $\\delta_{0}>0$ Assume that $\\left\\|\\hat{Q}-Q^{\\star}\\right\\|_{F}^{2}\\leqslant2(1-\\delta_{0})d$ and $\\log n\\ll d\\ll n$ Then, there exists a constant $C>0$ such that the estimator $\\hat{\\pi}$ defined in Equation (4) satisfies with probability $1-2e^{-d/16}-2n^{-n}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathrm{ov}(\\hat{\\pi},\\pi^{\\star})=1-\\frac{C}{\\delta_{0}}\\operatorname*{max}\\left(\\sqrt{\\frac{d\\log(d/\\delta_{0})}{n}+\\frac{\\log(n)}{d}},\\frac{d\\log(d/\\delta_{0})}{n}+\\frac{\\log(n)}{d}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the $n\\gg d\\gg\\log(n)$ regime: as long as we have non negligible error $\\left\\|\\hat{Q}-Q^{\\star}\\right\\|_{F}^{2}\\leqslant2(1-\\varepsilon)d$ (notice that for uniformly random $Q$ , we have $\\left\\|\\hat{Q}-Q^{\\star}\\right\\|_{F}^{2}=2d)$ , we recover $\\pi^{\\star}$ with $1-o(1)$ overlap: doing just a tiny bit better than random for $\\hat{Q}$ is enough to recover $\\pi^{\\star}$ ", "page_idx": 26}, {"type": "text", "text": "Proof of Proposition 3. In this proof we denote $\\begin{array}{r}{g(\\pi):=\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\pi(i)},\\hat{Q}^{\\top}y_{i}\\rangle}\\end{array}$ By definition, $\\hat{\\pi}\\in$ arg $\\operatorname*{max}_{\\pi\\in S_{n}}g(\\pi)$ Writing $g(\\hat{\\pi})\\geqslant g(\\pi^{\\star})$ gives ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\hat{\\pi}(i)},\\hat{Q}^{\\top}Q^{\\star}x_{\\pi^{\\star}(i)}\\rangle\\geqslant\\frac{1}{n}\\sum_{i=1}^{n}\\langle x_{\\pi^{\\star}(i)},\\hat{Q}^{\\top}Q^{\\star}x_{\\pi^{\\star}(i)}\\rangle+\\frac{\\sigma}{n}\\sum_{i=1}^{n}\\langle x_{\\pi^{\\star}(i)}-x_{\\hat{\\pi}(i)},\\hat{Q}^{\\top}z_{i}\\rangle\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Without loss of generality, we assume $\\pi^{\\star}=I d$ . The term in the LHS hereabove, for fixed $\\hat{\\pi},\\hat{Q}$ ,has expectation $\\operatorname{ov}\\!\\left(\\hat{\\pi},\\pi^{\\star}\\right)\\operatorname{Tr}(\\hat{Q}^{\\top}Q^{\\star})$ . We are going to compute uniform fluctuations. For some fixed $Q\\in\\mathcal{O}(d),P\\in S_{n}$ \uff0c ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}\\langle x_{\\pi(i)},\\hat{Q}^{\\top}Q^{\\star}x_{i}\\rangle=\\tilde{X}^{\\top}M\\tilde{X}\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "for $\\tilde{X}=(x_{1}^{\\top},\\ldots,x_{n}^{\\top})^{\\top}\\in\\mathbb{R}^{n d}$ and $M\\in\\mathbb{R}^{n d\\times n d}$ that writes as $M=\\tilde{P}^{\\top}\\tilde{Q}$ where $\\tilde{Q}\\in\\mathbb{R}^{n d\\times n d}$ is block diagonal with blocks equal to $Q$ and $\\tilde{P}\\in\\mathbb{R}^{n d\\times n d}$ is a block matrix, with blocks of size $n\\times n$ that verify ${\\tilde{P}}_{[i j]}=P_{i j}I_{n}$ . Thus, $\\|M\\|_{\\mathrm{op}}=1$ and $\\|M\\|_{F}^{2}=n d$ . Using Hanson-Wright inequality, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\sum_{i=1}^{n}\\langle x_{\\pi(i)},Q x_{i}\\rangle-n\\mathrm{ov}(\\pi,I d)\\,\\mathrm{Tr}(Q)\\right|>C(t+\\sqrt{n d t}\\right)\\leqslant2e^{-t}\\,.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since $d\\geqslant\\log(n)$ withprobaity $1\\!-\\!e^{-d/16}$ we have $\\operatorname*{sup}_{i\\in[n]}\\|x_{i}\\|\\leqslant2{\\sqrt{d}}$ using Chi concentraion.   \nWe now work conditionally on this event. ", "page_idx": 26}, {"type": "text", "text": "Letting $\\mathcal{N}_{\\delta}$ be a $\\delta-$ net of $O(d)$ \uff0c", "page_idx": 27}, {"type": "text", "text": "$>\\left(\\forall\\pi\\in S_{n}\\,,\\,\\forall Q\\in\\mathcal{N}_{\\delta}\\,,\\,\\left|\\sum_{i=1}^{n}\\langle x_{\\pi(i)},Q x_{i}\\rangle-n\\mathrm{ov}(\\pi,I d)\\,\\mathrm{Tr}(Q)\\right|>C(t+\\sqrt{n d t}\\right)\\leqslant2e^{-t+n\\log(n)+c}$ d\u00b2 log(1/8) . Using the fact that $\\textstyle\\sum_{i=1}^{n}\\langle x_{\\pi(i)},Q x_{i}\\rangle$ .s $n\\operatorname*{sup}_{i\\in[n]}\\left\\|x_{i}\\right\\|^{2}=4n d\\!-$ -Lipschitz in $Q$ we thus have:   \nI\u03c0T E Sn , $\\forall Q\\in\\mathcal{O}(d)\\,,\\,\\left|\\sum_{i=1}^{n}\\langle x_{\\pi(i)},Q x_{i}\\rangle-n\\mathrm{ov}(\\pi,I d)\\,\\mathrm{Tr}(Q)\\right|>4n d\\delta+C(t+\\sqrt{n d t}\\right)\\leqslant2e^{-t+n\\log(n)+c d^{2}1}$ og(1/8) . Seting $\\begin{array}{r}{\\delta=\\frac{\\varepsilon}{16}}\\end{array}$ and $t=2n\\log(n)+c d^{2}\\log(8/\\varepsilon)$ with probability $1-2n^{-n}$ , we get that for all   \n$\\pi,Q$ \uff0c   \n$\\sum_{i=1}^{n}\\langle x_{\\pi(i)},Q x_{i}\\rangle-n\\mathrm{ov}(\\pi,I d)\\,\\mathrm{Tr}(Q)\\Bigg|\\leqslant\\frac{n d\\varepsilon}{4}+C^{\\prime}(n\\log(n)+d^{2}\\log(1/\\varepsilon)+\\sqrt{n d(n\\log(n)+d^{2}\\log(n))})\\,,$ g(1/e))) . We can thus write, since $\\operatorname{Tr}(\\hat{Q}^{\\top}Q^{\\star})\\geq\\varepsilon d$   \n$\\frac{1}{\\i}\\sum_{i=1}^{n}\\langle x_{\\tilde{\\pi}(i)},\\hat{Q}^{\\top}Q^{\\star}x_{\\pi^{\\star}(i)}\\rangle\\leqslant\\mathrm{Tr}(\\hat{Q}^{\\top}Q^{\\star})d\\mathrm{ov}(\\pi,I d)+\\frac{\\varepsilon d}{4}+C^{\\prime}(\\log(n)+\\frac{d^{2}\\log(1/\\varepsilon)}{n}+\\sqrt{d(\\log(n)}))$ i=1 n and   \n$\\frac{1}{\\i}\\sum_{i=1}^{n}\\langle x_{\\pi^{*}(i)},\\hat{Q}^{\\top}Q^{\\star}x_{i}\\rangle\\geqslant\\operatorname{Tr}(\\hat{Q}^{\\top}Q^{\\star})d-\\frac{\\varepsilon d}{4}-C^{\\prime}(\\log(n)+\\frac{d^{2}\\log(1/\\varepsilon)}{n}+\\sqrt{d(\\log(n)+\\frac{d^{2}\\log(1/\\varepsilon)}{n})}).$ de-log(1/e) .   \nSimilarly than before, we prove that with probability $1-2n^{-n}$ , we have for all $\\pi\\in S_{n},Q\\in{\\mathcal{O}}(d)$   \n$\\sum_{i=1}^{n}\\langle x_{\\pi^{*}(i)}-x_{\\hat{\\pi}(i)},\\hat{Q}^{\\top}z_{i}\\rangle\\Bigg|\\leqslant\\frac{\\varepsilon d n}{4}+C^{\\prime}(n\\log(n)+d^{2}\\log(1/\\varepsilon)+\\sqrt{n d(n\\log(n)+d^{2}\\log(1/\\varepsilon))})\\,.$   \nEquation (29) thus implies that:   \n$\\mathrm{Ir}(\\hat{Q}^{\\top}Q^{\\star})d\\mathrm{ov}(\\pi,I d)\\geqslant\\mathrm{Ir}(\\hat{Q}^{\\top}Q^{\\star})d-\\frac{3\\varepsilon d}{4}-3C^{\\prime}(\\log(n)+\\frac{d^{2}\\log(1/\\varepsilon)}{n}+\\sqrt{d(\\log(n)+\\frac{d^{2}\\log(1/\\varepsilon)}{n})}).$ d-log(1/e) ,   \nleading to:   \n$\\operatorname{ov}(\\pi,I d)\\geqslant1-\\frac{3\\varepsilon}{4\\operatorname{Tr}(\\hat{Q}^{\\top}Q^{\\star})}-\\frac{3C^{\\prime}}{\\operatorname{Tr}(\\hat{Q}^{\\top}Q^{\\star})}(\\frac{\\log(n)}{d}+\\frac{d\\log(1/\\varepsilon)}{n}+\\sqrt{\\frac{\\log(n)}{d}+\\frac{d\\log(1/\\varepsilon)}{n}})\\,.$ ", "page_idx": 27}, {"type": "text", "text": "Setting $\\begin{array}{r}{\\varepsilon=\\frac{\\delta_{0}}{d}}\\end{array}$ concludes the proof. ", "page_idx": 27}, {"type": "text", "text": "E.3 Proof of Proposition 1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proof of Proposition $^{\\,l}$ . For the first part of Proposition 1, we directly apply Proposition 2 with $\\varepsilon=1/2$ to obtain the result. ", "page_idx": 27}, {"type": "text", "text": "For the second part that holds for large dimensions, we apply Proposition 2 for $\\varepsilon=1/2$ and $\\delta=1/8$ Using Lemma 3, the first \u201cPing' of ?? 1 leads to $\\hat{Q}$ satisfying the assumption of Proposition 3 for some $\\delta_{0}$ bounded away from zero, thus leading to the desired result after the last \u2018Pong\u2019 for $\\hat{\\pi}$ \uff1a\u53e3 ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The contributions and scope of the paper are described shortly in the abstract.   \nThe introduction section discusses the contributions and the scope more in depth. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "The limitations of the paper are discussed, in particular the fact that our work is mainly theoretical, and we mainly study informational results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper provides the set of assumptions in every Theorem and Proposition, that are self-contained. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 29}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Section 3.3 lists all the needed information to replicate all the experiments in the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. () If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct thedataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: see supplementary materials for a notebook ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 30}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All the details of training procedure and hyperparameters are listed in Section 3.3. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 30}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [No] ", "page_idx": 30}, {"type": "text", "text": "Justification: the experiments are merely illustrative and due to the $n^{3}$ scaling of the LAP, we performed some averagings over 10 runs for each point. But this can be improved easily in a second version. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [No] ", "page_idx": 31}, {"type": "text", "text": "Justification: In preparing the submission, the authors did not track sufficient information on the computer resources. However, the resources needed to run experiments are minimal, as all can be run on a single CPU. The total compute resources needed are not significant. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS Code of Ethics and does not pose any potential harm. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper is a foundational research paper without any direct societal impact. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper uses Python with some open-source Python libraries for experiments.   \nThere are no other particular existing assets used. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: There are no released assets in the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset isused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]