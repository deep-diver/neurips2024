[{"type": "text", "text": "MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haoyang $\\mathbf{H}\\mathbf{e}^{1*}$ Yuhu ${\\bf{B a i}}^{1*}$ Jiangning Zhang2\u2020 Qingdong $\\mathbf{H}\\mathbf{e}^{2}$ Hongxu Chen1 Zhenye Gan2 Chengjie Wang2 Xiangtai $\\mathbf{Li}^{3}$ Guanzhong Tian1 Lei $\\mathbf{Xie}^{1\\dagger}$ ", "page_idx": 0}, {"type": "text", "text": "Zhejiang University 2Youtu Lab, Tencent 3Nanyang Technological University {haoyanghe,yhbai,186368,chenhongxu,gztian} $@$ zju.edu.cn, {yingcaihe,wingzygan,jasoncjwang}@tencent.com, xiangtai94 $@$ gmail.com, leix $@$ iipc.zju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in anomaly detection have seen the efficacy of CNN- and transformer-based approaches. However, CNNs struggle with long-range dependencies, while transformers are burdened by quadratic computational complexity. Mamba-based models, with their superior long-range modeling and linear efficiency, have garnered substantial attention. This study pioneers the application of Mamba to multi-class unsupervised anomaly detection, presenting MambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring (Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSS module, integrating parallel cascaded (Hybrid State Space) HSS blocks and multi-kernel convolutions operations, effectively captures both long-range and local information. The HSS block, utilizing (Hybrid Scanning) HS encoders, encodes feature maps into five scanning methods and eight directions, thereby strengthening global connections through the (State Space Model) SSM. The use of Hilbert scanning and eight directions significantly improves feature sequence modeling. Comprehensive experiments on six diverse anomaly detection datasets and seven metrics demonstrate state-of-the-art performance, substantiating the method\u2019s effectiveness. The code and models are available at https://lewandofskee.github.io/projects/MambaAD. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The advent of smart manufacturing has markedly increased the importance of industrial visual Anomaly Detection (AD) in production processes. This technology promises to enhance efficiency, diminish the costs of manual inspections, and elevate product quality along with the stability of production lines. Presently, most methods predominantly utilize a single-class setting [12, 30, 55], where a separate model is trained and tested for each class, leading to considerable increases in training and memory usage. Despite recent progress in introducing multi-class AD techniques [47, 18], there is still significant potential for advancement in terms of both accuracy and efficiency trade-off. ", "page_idx": 0}, {"type": "text", "text": "The current unsupervised anomaly detection algorithms can be broadly categorized into three approaches [50, 9, 51]: Embedding-based [36, 11, 4, 12, 8], Synthesizing-based [48, 24, 55, 30], and Reconstruction-based [27, 18, 6]. Despite the promising results of both Synthesizing and Embeddingbased methods in AD, these approaches often require extensive design and inflexible frameworks. Reconstruction-based methods, such as RD4AD [12] and UniAD [47], exhibit superior performance and better scalability. RD4AD, as depicted in Fig. 1 (a), employs a pre-trained teacher-student model, comparing anomalies across multi-scale feature levels. While CNN-based RD4AD captures local context effectively, it lacks the ability to establish long-range dependencies. UniAD, the first multiclass AD algorithm, relies on a pre-trained encoder and transformer decoder architecture as illustrated in Fig. 1 (b). Despite their superior global modeling capabilities, transformers are hampered by quadratic computational complexity, which confines UniAD to anomaly detection on the smallest feature maps, potentially impacting its performance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recently, Mamba [15] has demonstrated exceptional performance in large language models, offering significantly lower linear complexity compared to transformers while maintaining comparable effectiveness. Numerous recent studies have incorporated Mamba into the visual domain, sparking a surge of research [29, 57, 40, 22, 37, 43]. This paper pioneers the application of Mamba into the anomaly detection area, introducing MambaAD, as illustrated in Fig. 1 (c). MambaAD combines global and local modeling capabilities, leveraging its linear complexity to compute anomaly maps across multiple scales. Notably, it boasts a lower parameter count and computational demand, making it well-suited for practical applications. ", "page_idx": 1}, {"type": "text", "text": "Specifically, MambaAD employs a pyramid-structured auto-encoder to reconstruct multi-scale features, utilizing a pre-trained encoder and a novel decoder based on the Mamba architecture. This Mamba-based decoder consists of Locality-Enhanced State Space (LSS) modules at varying scales and quantities. Each LSS module comprises two components: a series of Hybrid State Space (HSS) blocks for global information capture and parallel multi-kernel convolution operations for establishing local connections. The resulting output features integrate the global modeling capabilities of the mamba structure with the local correlation strengths of CNNs. The proposed HSS module investigates five distinct scanning methods and eight scanning directions, with the (Hybrid Scanning) HS encoder and decoder encoding and decoding feature maps into sequences of various scanning methods and directions, respectively. The HSS module enhances the global receptive field across multiple directions, and its use of the Hilbert scanning method [19, 23] is particularly suited to the central concentration of industrial product features. By computing and summing anomaly maps across different feature map scales, MambaAD achieves SoTA performance on several representative AD datasets with seven different metrics for both image- and pixel-level while maintaining a low model parameter count and computational complexity. Our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce MambaAD, which innovatively applies the Mamba framework to address multiclass unsupervised anomaly detection tasks. This approach enables multi-scale training and inference with minimal model parameters and computational complexity. \u2022 We design a Locality-Enhanced State Space (LSS) module, comprising cascaded Mambabased blocks and parallel multi-kernel convolutions, extracts both global feature correlations and local information associations, achieving a unified model of global and local patterns. \u2022 We have explored a Hybrid State Space (HSS) block, encompassing five methods and eight multi-directional scans, to enhance the global modeling capabilities for complex anomaly detection images across various categories and morphologies. \u2022 We demonstrate the superiority and efficiency of MambaAD in multi-class anomaly detection tasks, achieving SoTA results on six distinct AD datasets with seven metrics while maintaining remarkably low model parameters and computational complexity. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Unsupervised Anomaly Detection ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Unsupervised Anomaly Detection. Existing AD methods can be mainly categorized into three types: 1) Embedding-based methods focus on encoding RGB images into multi-channel features [36, 11, 4, 12, 10, 38]. These methods typically employ networks per-trained on ImageNet [13]. PatchCore [36] extracts nominal patch features with a memory bank for measuring the Mahalanobis distance. [4] is based on a student-teacher framework where student networks are trained to regress the output of a teacher network. However, the datasets used for these pre-trained models have a significant distribution gap compared to industrial images. 2) Synthesizing-based methods synthesize anomalies on normal images [48, 24, 39, 20]. The pseudo-anomalies in DREAM [48] are generated utilizing Perlin noise and texture images. DREAM, taking anomaly mask as output, consists of a reconstruction network and a discriminative network. Despite the decent performance of such methods, the synthesized anomalies still have a certain gap compared to real-world anomalies. 3) Reconstruction-based methods [12, 27, 52, 7] typically focus on self-training encoders and decoders to reconstruct images, reducing reliance on pre-trained models. Autoencoder [35], Transformers[33], Generative Adversarial Networks (GANs) [27, 46] and diffusion models [18, 45] can serve as the backbone for reconstruction networks in anomaly detection. While the model\u2019s generalization can occasionally lead to inaccuracies in pinpointing anomalous areas. ", "page_idx": 1}, {"type": "image", "img_path": "8VKxTlnejE/tmp/28bcb0ac03688b7965458494dee631d7d54ef76cf1750e0aeef3ec1fd85e55bf.jpg", "img_caption": ["Figure 1: Compared with (a) local CNN-based RD4AD [12] and (b) global Transformer-based UniAD [47], ours MambaAD with linear complexity is capable of integrating the advantages of both global and local modeling, and multi-scale features endow it with more refined prediction accuracy. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Multi-class Anomaly Detection. Most current works are trained individually on separate categories, which leads to increased time and memory consumption as the number of categories grows, and they are uncongenial to situations with large intra-class diversity. Recently, to address these issues, multiclass unsupervised anomaly detection (MUAD) methods have attracted a lot of interest. UniAD [47] firstly crafts a unified reconstruction framework for anomaly detection. DiAD [18] investigates an anomaly detection framework based on diffusion models, introducing a semantic-guided network to ensure the consistency of reconstructed image semantics.ViTAD [50] further explores the effectiveness of vanilla Vision Transformer (ViT) on multi-class anomaly detection. ", "page_idx": 2}, {"type": "text", "text": "2.2 State Space Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "State space models (SSMs) [17, 16, 41, 32, 14] have gained considerable attention due to their efficacy in handling long language sequence modeling. Specifically, structure state-space sequence (S4) [16] efficiently models long-range dependencies (LRDs) through parameterization with a diagonal structure, addressing computational bottlenecks encountered in previous works. Building upon S4, numerous models have been proposed, including S5 [41], H3 [14], and notably, Mamba [15]. Mamba introduces a data-dependent selection mechanism into S4, which provides a novel paradigm distinct from CNNs or Transformers, maintaining linear scalability of long sequences processing. ", "page_idx": 2}, {"type": "text", "text": "The tremendous potential of Mamba has sparked a series of excellent works [29, 57, 37, 21, 22, 40, 44, 43, 31, 54, 26] in the vision domain. Vmamba [29] proposes a cross-scan module (CSM) to tackle the direction sensitivity issue between non-causal 2D images and ordered 1D sequences. Moreover, Mamba has found extensive use in the domain of medical image segmentation [37, 28, 44, 43, 25], incorporating Mamba blocks to UNet-like architecture to achieve task-specific architecture. VLMamba [34] and Cobra [56] explore the potential of SSMs in multimodal large language models. Besides, ZigMa [21] addresses the spatial continuity in the scan strategy, and it incorporates Mamba into the Stochastic Interpolation framework [1]. ", "page_idx": 2}, {"type": "text", "text": "In this work, we develop MambaAD to exploit Mamba\u2019s long-range modeling capacity and linear computational efficiency for multi-class unsupervised anomaly detection. This approach innovatively combines SSM\u2019s global modeling capabilities with CNNs\u2019 detailed local modeling prowess. ", "page_idx": 2}, {"type": "image", "img_path": "8VKxTlnejE/tmp/77ea74469da1d824de8f287ff0e2cfa315b4437f6f4672a9e1efabc323807902.jpg", "img_caption": ["Figure 2: Overview of the proposed MambaAD, which employs pyramidal auto-encoder framework to reconstruct multi-scale features by the proposed efficient and effective Locality-Enhanced State Space (LSS) module. Specifically, each LSS consists of: 1) cascaded Hybrid State Space (HSS) blocks to capture global interaction; and 2) parallel multi-kernel convolution operations to replenish local information. Aggregated multi-scale reconstruction error serves as the anomaly map for inference. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "State Space Models [16], inspired by control systems, map a one-dimensional stimulation $\\b{x}(t)\\in\\mathbb{R}^{L}$ to response $y(t)\\in\\mathbb{R}^{L}$ through a hidden state $h(t)\\in\\mathbb{R}^{\\dot{N}}$ , which are formulated as linear ordinary differential equations (ODEs): ", "page_idx": 3}, {"type": "equation", "text": "$$\nh^{\\prime}(t)=\\mathbf{A}h(t)+\\mathbf{B}x(t),\\ \\ y(t)=\\mathbf{C}h(t),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the state transition matrix ${\\bf A}\\in\\mathbb{R}^{N\\times N}$ , $\\mathbf{B}\\in\\mathbb{R}^{N\\times1}$ and $\\mathbf{C}\\in\\mathbb{R}^{1\\times N}$ for a state size $N$ . ", "page_idx": 3}, {"type": "text", "text": "S4 [16] and Mamba [15] utilize zero-order hold with a timescale parameter $\\Delta$ to transform the continuous parameters $\\mathbf{A}$ and $\\mathbf{B}$ from the continuous system into the discrete parameters $\\bar{\\bf A}$ and $\\overline{{\\mathbf{B}}}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{A}}}=\\exp(\\mathbf{\\DeltaA}),\\ \\ \\overline{{\\mathbf{B}}}=(\\mathbf{\\DeltaA}\\mathbf{A})^{-1}(\\exp(\\mathbf{\\DeltaA}\\mathbf{A})-\\mathbf{I})\\cdot\\mathbf{\\DeltaA}\\mathbf{B}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After the discretization, the discretized model formulation can be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nh_{t}=\\overline{{\\mathbf{A}}}h_{t-1}+\\overline{{\\mathbf{B}}}x_{t},\\quad y_{t}=\\mathbf{C}h_{t}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "At last, from the perspective of global convolution, the output can be defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\overline{{\\mathbf{K}}}=(\\mathbf{C}\\overline{{\\mathbf{B}}},\\mathbf{C}\\overline{{\\mathbf{A}\\mathbf{B}}},\\dots,\\mathbf{C}\\overline{{\\mathbf{A}}}^{L-1}\\overline{{\\mathbf{B}}}),\\;\\;\\;\\mathbf{y}=\\mathbf{x}*\\overline{{\\mathbf{K}}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $^*$ represents convolution operation, $L$ is the length of sequence $x$ , and $\\overline{{\\mathbf{K}}}\\in\\mathbb{R}^{L}$ is a structured convolutional kernel. ", "page_idx": 3}, {"type": "text", "text": "3.2 MambaAD ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The MambaAD framework is proposed for multi-class anomaly detection as illustrated in Fig. 2(a). It consists of three main components: a pre-trained CNN-based encoder, a Half-FPN bottleneck, and a Mamba-based decoder. During training, the encoder extracts feature maps at three different scales and inputs them into the H-FPN bottleneck for fusion. The fused output is then fed into the Mamba Decoder with a depth configuration of [3,4,6,3]. The final loss function is the sum of the mean squared error (MSE) computed across feature maps at three scales. Within the Mamba Decoder, we introduce the Locality-Enhanced State Space (LSS) module. The LSS can be configured with different stages $M_{i}$ , where each stage represents the number $N$ of Hybrid State Space (HSS) blocks within the module. In this experiment, we employ LSS with $M_{i}=3$ and $M_{i}=2$ as the primary modules. The LSS module processes the input $X_{i}$ through HSS blocks that capture global information and through two different scales of Depth-Wise Convolution (DWConv) layers that capture local information. The original input feature dimension is restored through concatenation and convolution operations. The proposed HSS block features a Hybrid Scanning (HS) Encoder and an HS Decoder, which accommodates five distinct scanning methods and eight scanning directions. ", "page_idx": 3}, {"type": "image", "img_path": "8VKxTlnejE/tmp/c1713fc72b3e6630e79ade165de8fb4c72fb392590b04b228748e0b08b745da1.jpg", "img_caption": ["Figure 3: Hybrid Scanning directions and methods. (a) The Hilbert scanning method with 8 scanning directions is used for HS Encoder and Decoder. $(b)$ The other four scanning methods for comparison. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.3 Locality-Enhanced State Space Module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Transformers excel in global modeling and capturing long-range dependencies but tend to overlook local semantic information and exhibit high computational complexity when processing high-resolution features. Conversely, CNNs effectively model local semantics by capturing information from adjacent positions but lack long-range modeling capabilities. To address these limitations, we propose the LSS module in Fig. 2 (b), which incorporates both Mamba-based cascaded HSS blocks for global modeling and parallel multi-kernel depth-wise convolution operations for local information capture. ", "page_idx": 4}, {"type": "text", "text": "Specifically, for an input feature $X_{i}\\in\\mathbb{R}^{H\\times W\\times C}$ , global features $G_{i}\\in\\mathbb{R}^{H\\times W\\times C}$ enter the HSS blocks while local features $L_{i}\\in\\mathbb{R}^{H\\times W\\times C}$ proceed through a convolutional network. The global features $G_{i}$ pass through a series of $N$ HSS blocks to obtain global information features $G_{o}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\nG_{o}={\\bf H S S_{n}}(...({\\bf H S S_{2}}({\\bf H S S_{1}}(G_{i})))),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $n\\in N$ is the number of HSS blocks. In this study, we primarily use $N=2$ and $N=3$ , with further ablation experiments presented in Sec. 4.3. ", "page_idx": 4}, {"type": "text", "text": "Local features $L_{i}$ are processed by two parallel DWConv blocks, each comprising a $1\\times1$ Conv block, an $k\\times k$ DWConv block, and another $1\\times1$ Conv block. ", "page_idx": 4}, {"type": "equation", "text": "$$\nL_{m}=\\mathbf{ConvB_{1\\times1}}(\\mathbf{DWConvB_{k\\timesk}}(\\mathbf{ConvB_{1\\times1}}(L_{i})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $k$ is the kernel size for the DWConv. $k\\,=\\,5$ and $k\\,=\\,7$ are used in this experiment with further ablations in Sec. 4.3. Each convolutional module includes a Conv 2D layer, an Instance Norm 2D layer, and a SiLU as illustrated in Fig. 2 (d). Local and global features are aggregated by concatenation along the channel dimension. The final output $X_{o}$ of this block is obtained by a $1\\times1$ 2D convolution to restore the channel count to match that of the input and a residual connection. ", "page_idx": 4}, {"type": "equation", "text": "$$\nX_{o}=\\mathbf{Conv2D_{1\\times1}}(\\mathbf{Concat}(G_{o},L_{k_{5}},L_{k_{7}}))+X_{i}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.4 Hybrid State Space Block ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following the method outlined in [29, 37], the HSS block is designed for hybrid-method and hybriddirectional scanning and fusion, as depicted in Fig. 2 (c). The HSS block primarily comprises Layer ", "page_idx": 4}, {"type": "text", "text": "Normalization (LN), Linear Layer, depth-wise convolution, SiLU activation, Hybrid Scanning (HS) encoder $\\mathcal{E}_{H S}$ , State Space Models (SSMs), HS decoder $\\mathcal{D}_{H S}$ , and residual connections. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{i}^{\\prime}=\\mathbf{LN}(\\mathcal{D}_{H S}(\\mathbf{SSMs}(\\mathcal{E}_{H S}(\\sigma(\\mathbf{DWConv}_{3\\times3}(\\mathbf{Linear}(\\mathbf{LN}(G_{i})))))))),}\\\\ &{G_{i+1}=\\mathbf{Linear}(G_{i}^{\\prime}\\cdot\\sigma(\\mathbf{Linear}(\\mathbf{LN}(G_{i}))))+G_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Hybrid Scanning methods. Inspired by space-fliling curves [49, 53], as shown in Fig. 3, this study explores five different scanning methods: (I) Sweep, (II) Scan, (III) ${{Z}}$ -order, (IV) Zigzag, and (V) Hilbert, to assess their impact on the SSM\u2019s modeling capabilities. The Hilbert scanning method is ultimately selected for its superior encoding and modeling of local and global information within feature sequences, particularly in mitigating the challenges of modeling long-range dependencies. Further experimental results will be presented in the ablation study. Assuming $A$ is a matrix, $A^{T}$ is the transpose of $A$ , $A^{l r}$ is the left-right reversal of $A$ , $A^{u d}$ is the up-down reversal of $A$ . The Hilbert curve can be obtained by an n-order Hilbert matrix: ", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{n+1}=\\left\\{\\begin{array}{c c}{H_{n}}&{4^{n}E_{n}+H_{n}^{T}}\\\\ {\\left(\\begin{array}{c c c}{(4^{n+1}+1)E_{n}-H_{n}^{u d}}&{(3\\times4^{n}+1)E_{n}-(H_{n}^{l r})^{T}}\\end{array}\\right)}&{,\\mathrm{if~}n\\mathrm{~is~even},}\\\\ {H_{n}}&{(4^{n+1}+1)E_{n}-H_{n}^{l r}}\\\\ {4^{n}E_{n}+H_{n}^{T}}&{(3\\times4^{n}+1)E_{n}-(H_{n}^{T})^{l r}}\\end{array}\\right)}&{,\\mathrm{if~}n\\mathrm{~is~odd,}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $H_{1}=\\left(\\begin{array}{l l}{1}&{2}\\\\ {4}&{3}\\end{array}\\right)$ and $E_{n}$ is all-one matrix for $\\mathbf{n}$ -order. ", "page_idx": 5}, {"type": "text", "text": "Hybrid Scanning directions. Following the setup of previous scanning directions, this study supports eight Hilbert-based scanning directions: (i) forward, (ii) reverse, (iii) width-height (wh) forward, (iv) wh reverse, (v) rotated 90 degrees forward, (vi) rotated 90 degrees reverse, (vii) wh rotated 90 degrees forward, and (viii) wh rotated 90 degrees reverse, as illustrated in Fig. 3 (a). Multiple scanning directions enhance the encoding and modeling capabilities of feature sequences, enabling the handling of various types of anomalous features with further ablations in Sec. 4.3. ", "page_idx": 5}, {"type": "text", "text": "The HS encoder aims to combine and encode input features according to different scanning methods and directions before feeding them into the SSM to enhance the global modeling capacity of the feature vectors. The HS decoder then decodes the feature vectors output by the SSM back to the original input feature orientation, with the final output obtained by summation. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Setups: Datasets, Metrics, and Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "MVTec-AD [3] encompasses a diverse collection of 5 types of textures and 10 types of objects, 5,354 high-resolution images in total. 3,629 normal images are designated for training. The remaining 1,725 images are reserved for testing and include both normal and abnormal samples. ", "page_idx": 5}, {"type": "text", "text": "VisA [58] features 12 different objects, incorporating three diverse types: complex structures, multiple instances, and single instances. It consists of a total of 10,821 images, of which 9,621 are normal samples, and 1,200 are anomaly samples. ", "page_idx": 5}, {"type": "text", "text": "Real-IAD [42] includes objects from 30 distinct categories, with a collection of 150K high-resolution images, making it larger than previous anomaly detection datasets. It consists of 99,721 normal images and 51,329 anomaly images. ", "page_idx": 5}, {"type": "text", "text": "More results on MVTec-3D [5], as well as newly proposed Uni-Medical [50, 2] and COCO-AD [52] datasets, can be viewed in Appendix 5. ", "page_idx": 5}, {"type": "text", "text": "Metrics. For anomaly detection and segmentation, we report Area Under the Receiver Operating Characteristic Curve (AU-ROC), Average Precision [48] (AP) and F1-score-max [58] (F1_max). Additionally, for anomaly segmentation, we also report Area Under the Per-Region-Overlap [4] (AU-PRO). We further calculate the mean value of the above seven evaluation metrics (denoted as $m A D)$ to represent a model\u2019s comprehensive capability [50]. ", "page_idx": 5}, {"type": "text", "text": "Implementation Details. All input images are resized to a uniform size of $256\\times256$ without additional augmentation for consistency. A pre-trained ResNet34 acts as the feature extractor, while a ", "page_idx": 5}, {"type": "table", "img_path": "8VKxTlnejE/tmp/79ccf21d0032f7552591dda92cde268fe15372b4633f2fdb0fcd87facffde0b9.jpg", "table_caption": ["Table 1: Quantitative Results on different AD datasets for multi-class setting. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Mamba decoder of equivalent depth [3,4,6,3] to ResNet34 serves as the student model for training. In the Mamba decoder, the number of cascaded HSS blocks in the second LSS module is set to 2, while all other LSS modules employ 3 cascaded HSS blocks. This experiment employs the Hilbert scanning technique, utilizing eight distinct scanning directions. The AdamW optimizer is employed with a learning rate of 0.005 and a decay rate of $1\\times10^{-4}$ . The model undergoes a training period of 500 epochs for the multi-class setting, conducted on a single NVIDIA TESLA V100 32GB GPU. During training, the sum of MSE across different scales is employed as the loss function. In the testing phase, the sum of cosine similarities at various scales is utilized as the anomaly maps. ", "page_idx": 6}, {"type": "text", "text": "4.2 Comparison with SoTAs on Different AD datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compared our method with current SoTA methods on a range of datasets utilizing both imagelevel and pixel-level metrics $\\left.c.f.\\right.$ , Sec. 4.1). This paper primarily compares with UniAD [47] and DiAD [18] dedicated to MUAD. In addition, we also compare our MambaAD with Reconstructionbased RD4AD [12] and Embedding-based DeSTSeg [55]/SimpleNet [30]. ", "page_idx": 6}, {"type": "text", "text": "Quantitative Results. As shown in Tab. 1, on MVTec-AD dateset, our MambaAD outperforms all the comparative methods and reaches a new SoTA to $\\mathbf{98.6/99.6/97.6}$ and $\\mathbf{97.7/56.3/\\bar{5}9.2/93.1}$ in multi-class anomaly detection and segmentation. Specifically, compared to DiAD [18], our proposed MambaAD shows an improvement of $1.4\\uparrow\\slash0.6\\uparrow\\slash1.3\\uparrow$ at image-level and $0.9\\uparrow\\uparrow3.7\\uparrow\\uparrow3.7\\uparrow\\uparrow2.4\\uparrow$ at pixel-level. Notably, for overall metric mAD of a model, our MambaAD improves by 2.0 \u2191, compared with SoTA DiAD. The VisA dataset is more complex and challenging, yet our method still demonstrates excellent performance. As shown in Tab. 1, our MambaAD exceeds the performance of DiAD [18] by $7.5\\uparrow\\uparrow6.2\\uparrow/4.3\\uparrow$ at image-level and by $2.5\\uparrow\\uparrow13.3\\uparrow\\uparrow11.0\\uparrow\\uparrow15.8\\uparrow$ . Meanwhile, we achieve an enhancement of 8.7 \u2191compared to advanced DiAD on the mAD metric. In addition, the SoTA results on Real-IAD datasets, shown in Tab. 1, illustrate the scalability, versatility, and efficacy of our method MambaAD. We also compared the results of MambaAD with state-of-the-art (SoTA) methods for single-class anomaly detection in Tabs. A13 to A16 in the Appendix. In the single-class tasks, we included a comparison with the PatchCore [36] method. On the MVTec-AD dataset, SimpleNet and PatchCore achieved the best results at the image level, while our method achieved the second-best results. At the pixel level, DeSTSeg achieved the best results in most metrics, whereas our method achieved the best results in AUPRO. For the VisA dataset, our method achieved the best results in two metrics each at both the image and pixel levels. This demonstrates that our multi-class anomaly detection method can also achieve optimal or near-optimal results in single-class tasks, indicating its effectiveness and robustness. In contrast, existing single-class SoTA methods like SimpleNet and DeSTSeg, although effective in single-class tasks, show a significant performance drop in multi-class tasks. The PatchCore method can only operate in single-class tasks; for multi-class tasks, it encounters issues such as GPU and memory overflow due to the need to store all features in the Memory Bank. In summary, MambaAD exhibits strong robustness and effectiveness. More detailed results for each category are presented in the Appendix. ", "page_idx": 6}, {"type": "image", "img_path": "8VKxTlnejE/tmp/30835bb0f062b45ad89f31d9b235fdc424a111bad6de94c4765dd440351b1ca3.jpg", "img_caption": ["Figure 4: Qualitative visualization for pixel-level anomaly segmentation on MVTec and VisA datasets. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Qualitative Results. We conducted qualitative experiments on MVTev-AD and VisA datasets that substantiated the accuracy of our method in anomaly segmentation. Fig. 4 demonstrates that our method possesses more precise anomaly segmentation capabilities. Compared to DiAD, our method delivers more accurate anomaly segmentation without significant anomaly segmentation bias. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation and Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Components IncrementalAblations. The ablation experiments for the proposed components are summarized in Tab. 2. Using the most basic decoder purely based on Mamba and employing only the simplest two-directional sweep scanning method, we achieve an mAD score of 82.1 on the MVTec-AD dataset. Subsequently, by incorporating the proposed LSS module, which integrates the global modeling capabilities of Mamba with the local modeling capabilities of CNNs, the mAD score improves by $+2.8\\%$ . Finally, replacing the original scanning directions and methods with HSS, which combines features from different scanning directions and employs the Hilbert scanning method, better aligns with the data distribution in most industrial scenarios where objects are centrally located in the image. This results in an additional $+1.1\\%$ point improvement in the mAD score. Overall, the proposed MambaAD achieves an mAD score of 86.0 on the MVTec-AD dataset and 78.9 on the VisA dataset, reaching the SoTA performance. ", "page_idx": 7}, {"type": "text", "text": "Table 3: Ablation Study on the LSS Module. ", "page_idx": 7}, {"type": "table", "img_path": "8VKxTlnejE/tmp/d4b7d62feb9a5f95c38dc148a823903b1f7960360548d2a9f7d54cc0c3d8093a.jpg", "table_caption": ["Table 2: Incremental Ablations. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Local/Global Branches of the LSS Module. We conducted three ablation experiments to verify the impact of branches in the LSS module as shown in Tab. 3. The Local branch represents the use of only the parallel CNN branch, without the Mamba-based HSS branch. The Global branch represents the use of only the Mamba-based HSS branch, without the parallel CNN branch, making the decoder in this structure purely Mamba-based. Finally, Global+Local represents the proposed LSS structure used in MambaAD, which combines the serial Mamba-based HSS with parallel CNN branches of different kernel sizes. The experimental results are shown in the table below. The Local branch, which uses only CNNs, has the lowest parameter count and FLOPs but also the lowest mAD metric, indicating high efficiency but suboptimal accuracy. The Global method, based on the pure Mamba structure, consumes more parameters and FLOPs than the Local method but shows a significant improvement in performance $(+2.7\\%)$ . Finally, the combined Global+Local method, which is the LSS module used in MambaAD, achieves the best performance with a notable improvement of $(+1.6\\%)$ over the individual methods. ", "page_idx": 7}, {"type": "text", "text": "Effectiveness comparison of different pre-trained backbone and Mamba decoder depth. First, we compared various pre-trained feature extraction networks, focusing on the popular ResNet series as shown in Tab. 4. When maintaining a consistent Mamba decoder depth, we observed that ResNet18 performed the poorest, despite its minimal model size and computational complexity. ResNet50, with approximately 8 times more parameters and computations. Although WideResNet surpassed ResNet34 in certain metrics, it required nearly 10 times the parameters and computational cost. Consequently, after considering all factors, we elected to use ResNet34 as the backbone feature extractor. Then, we examined the impact of different Mamba decoder depths while keeping the backbone network constant. The depths [2,2,2,2] and [3,4,6,3] corresponded to ResNet18 and ResNet34 depths, respectively, while [2,9,2,2] was the prevalent choice in other methods. Our experiments revealed that the [3,4,6,3] depth, despite a slight increase in parameters and computations, consistently outperformed the other configurations. ", "page_idx": 7}, {"type": "table", "img_path": "8VKxTlnejE/tmp/6b69de6cc4e4e89e7a4c2539269baef3c58b6494b24c55f2a9e9db19f527b316.jpg", "table_caption": ["Table 4: Ablation studies on the pre-trained backbone and Mamba decoder depth. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "8VKxTlnejE/tmp/cd543384f6e5e865b4eba7718e0736c3cecbce3c4e394afe7353c21713cdeb1b.jpg", "table_caption": ["Table 5: Ablations on different scanning methods and directions. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Efficiency comparison of different SoTA methods. In Tab. 6, we compared our model with five SoTA methods in terms of model size and computational complexity. MambaAD exhibits a minimal increase in parameters compared to UniAD. However, MambaAD outperforms it by $4.3\\uparrow$ on the comprehensive metric mAD. Moreover, MambaAD significantly outperforms these other approaches, demonstrating its effectiveness in model lightweight design while maintaining high performance. Particularly, our method MambaAD has achieved about $2.0\\uparrow$ improvement with only 1/50 the parameters and flops of DiAD. ", "page_idx": 8}, {"type": "table", "img_path": "8VKxTlnejE/tmp/a52b8a0ffe572fb48cf480e6e131df3201448aa5a42b9078a102652d9af436c7.jpg", "table_caption": ["Table 6: Efficiency comparison of SoTA methods. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Effectiveness and efficiency comparison of different scanning methods and directions. In the initial stage, we compared five distinct scanning methods with an 8-direction scan as shown in Tab. 5. The results indicated that the other four methods produced similar outcomes as Index 1-4, albeit marginally inferior to the Hilbert scanning technique at the image level. Subsequently, we examined the impact of varying scan directions. As the number of scan directions increased as Index 5-7, image-level metrics improved gradually, while pixel-level metrics remained consistent. This suggests that augmenting the number of scan directions enhances the global modeling capability of SSM, thereby decreasing the likelihood of image-level misclassification. Ultimately, our analysis revealed that combining various scanning techniques while maintaining a total of eight scan directions led to a decline in performance as as Index 8-12. This decrement could be attributed to the significant disparities among the scanning approaches employed. Consequently, we opted for the Hilbert scanning method, as it demonstrated better suitability for real-world industrial products. ", "page_idx": 8}, {"type": "text", "text": "Effectiveness comparison of different LSS designs. In Tab. 7, the design focuses on three distinct design directions: the number of $M_{i}$ in the LSS module, the configuration of parallel multi-kernel convolution modules, and the kernel size selection for Depth-Wise Convolutions (DWConv). Initially, we compare scenarios where $M_{i}=1$ , meaning each LSS module contains a single HSS block, and we experiment with different kernel sizes for DWConv alone and configurations flanked by $1\\times1$ convolutions. Subsequently, we contrast the results without residual connections against those with identical settings but with $M_{i}=1$ . Finally, we examine the outcomes when $M_{i}=2$ and $M_{i}=3$ under otherwise consistent settings. Analysis reveals that with $M_{i}=1$ , regardless of the presence of residual connections, the results are inferior to those with $M_{i}=2$ and $M_{i}=3$ . Moreover, using only DWConv blocks with $M_{i}=1$ , a comparison of parallel depth-wise convolutions with varying kernel sizes indicates that smaller kernels, such as $k=1$ , significantly degrade performance. Therefore, subsequent comparative experiments focus on larger convolution kernels. In the absence of residual connections, some metrics may surpass those with residual connections, but the longer training time and difficulty in convergence preclude their use in further experiments. In configurations with $M_{i}=2$ and $M_{i}\\,=\\,3$ , we find that DWConv blocks augmented with $1\\times1$ convolutions exhibit superior performance. Additionally, kernels sized $k=5$ and $k=7$ are more suitable for extracting local features and establishing local information associations. Consequently, in this study, we opt for a quantity of HSS blocks with $M_{i}=2$ and $M_{i}=3$ , and we employ parallel DWConv blocks with kernels sized $k=5$ and $k=7$ , complemented by $1\\times1$ convolutions before and after. ", "page_idx": 8}, {"type": "table", "img_path": "8VKxTlnejE/tmp/0b8c61ef602b39bc23077a48f18f4225a83b879bfd014b5ca5a88b2ef6f2ea31.jpg", "table_caption": ["Table 7: Ablation studies on LSS module\u2019s designs. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces MambaAD, the first application of the Mamba framework to AD. MambaAD consists of a pre-trained encoder and a Mamba decoder, with a novel LSS module employed at different scales and depths. The LSS module, composed of sequential HSS modules and parallel multi-core convolutional networks, combines Mamba\u2019s global modeling prowess with CNN-based local feature correlation. The HSS module employs HS encoders to encode input features into five scanning patterns and eight directions, which facilitate the modeling of feature sequences in industrial products at their central positions. Extensive experiments on six diverse AD datasets and seven evaluation metrics demonstrate the effectiveness of our approach in achieving SoTA performance. ", "page_idx": 9}, {"type": "text", "text": "Limitations, Broader Impact and Social Impact. The model is not efficient enough and more lightweight models need to be designed. This study marks our initial attempt to apply Mamba in AD, laying a foundation for future research. We hope it can inspire lightweight designs in AD. MambaAD exhibits significant practical implications in enhancing industrial production efficiency. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by Jianbing Lingyan Foundation of Zhejiang Province, P.R. China (Grant No. 2023C01022). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. S. Albergo, N. M. Boff,i and E. Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 3 [2] J. Bao, H. Sun, H. Deng, Y. He, Z. Zhang, and X. Li. Bmad: Benchmarks for medical anomaly detection. In CVPRW, 2024. 6 [3] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad\u2013a comprehensive real-world dataset for unsupervised anomaly detection. In CVPR, 2019. 6, 7 [4] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In CVPR, 2020. 1, 2, 6 [5] P. Bergmann, X. Jin, D. Sattlegger, and C. Steger. The mvtec 3d-ad dataset for unsupervised 3d anomaly detection and localization. In VISAPP, 2022. 6 [6] Y. Cao, Q. Wan, W. Shen, and L. Gao. Informative knowledge distillation for image anomaly segmentation. Knowledge-Based Systems, 248:108846, 2022. 1 [7] Y. Cao, X. Xu, Z. Liu, and W. Shen. Collaborative discrepancy optimization for reliable image anomaly localization. IEEE Transactions on Industrial Informatics, 19(11):10674\u201310683, 2023.   \n3 [8] Y. Cao, X. Xu, C. Sun, L. Gao, and W. Shen. Bias: Incorporating biased knowledge to boost unsupervised image anomaly localization. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2024. 1 [9] Y. Cao, X. Xu, J. Zhang, Y. Cheng, X. Huang, G. Pang, and W. Shen. A survey on visual anomaly detection: Challenge, approach, and prospect. arXiv preprint arXiv:2401.16402, 2024.   \n1 [10] N. Cohen and Y. Hoshen. Sub-image anomaly detection with deep pyramid correspondences. arXiv preprint arXiv:2005.02357, 2020. 2 [11] T. Defard, A. Setkov, A. Loesch, and R. Audigier. Padim: a patch distribution modeling framework for anomaly detection and localization. In ICPR, 2021. 1, 2 [12] H. Deng and X. Li. Anomaly detection via reverse distillation from one-class embedding. In CVPR, 2022. 1, 2, 3, 7, 9, 14, 15, 16, 17, 18, 19 [13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 2 [14] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. In ICLR, 2022. 3 [15] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. 2, 3, 4 [16] A. Gu, K. Goel, and C. R\u00e9. Efficiently modeling long sequences with structured state spaces. In ICLR, 2021. 3, 4 [17] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In NeurlPS, 2021. 3 [18] H. He, J. Zhang, H. Chen, X. Chen, Z. Li, X. Chen, Y. Wang, C. Wang, and L. Xie. A diffusionbased framework for multi-class anomaly detection. In AAAI, 2024. 1, 3, 7, 8, 9, 14, 15, 16, 17,   \n18 [19] D. Hilbert and D. Hilbert. \u00dcber die stetige abbildung einer linie auf ein fl\u00e4chenst\u00fcck. Dritter Band: Analysis\u00b7 Grundlagen der Mathematik\u00b7 Physik Verschiedenes: Nebst Einer Lebensgeschichte, 1935. 2 [20] T. Hu, J. Zhang, R. Yi, Y. Du, X. Chen, L. Liu, Y. Wang, and C. Wang. Anomalydiffusion: Few-shot anomaly image generation with diffusion model. In AAAI, 2024. 2   \n[21] V. T. Hu, S. A. Baumann, M. Gui, O. Grebenkova, P. Ma, J. Fischer, and B. Ommer. Zigma: Zigzag mamba diffusion model. arXiv preprint arXiv:2403.13802, 2024. 3   \n[22] T. Huang, X. Pei, S. You, F. Wang, C. Qian, and C. Xu. Localmamba: Visual state space model with windowed selective scan. arXiv preprint arXiv:2403.09338, 2024. 2, 3   \n[23] N. Jiang, L. Wang, and W.-Y. Wu. Quantum hilbert image scrambling. International Journal of Theoretical Physics, 2014. 2   \n[24] C.-L. Li, K. Sohn, J. Yoon, and T. Pfister. Cutpaste: Self-supervised learning for anomaly detection and localization. In CVPR, 2021. 1, 2   \n[25] X. Li, H. Ding, W. Zhang, H. Yuan, G. Cheng, P. Jiangmiao, K. Chen, Z. Liu, and C. C. Loy. Transformer-based visual segmentation: A survey. arXiv pre-print, 2023. 3   \n[26] X. Li, H. Yuan, W. Li, H. Ding, S. Wu, W. Zhang, Y. Li, K. Chen, and C. C. Loy. Omg-seg: Is one model good enough for all segmentation? In CVPR, 2024. 3   \n[27] Y. Liang, J. Zhang, S. Zhao, R. Wu, Y. Liu, and S. Pan. Omni-frequency channel-selection representations for unsupervised anomaly detection. TIP, 2023. 1, 3   \n[28] J. Liu, H. Yang, H.-Y. Zhou, Y. Xi, L. Yu, Y. Yu, Y. Liang, G. Shi, S. Zhang, H. Zheng, et al. Swin-umamba: Mamba-based unet with imagenet-based pretraining. arXiv preprint arXiv:2402.03302, 2024. 3   \n[29] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024. 2, 3, 5   \n[30] Z. Liu, Y. Zhou, Y. Xu, and Z. Wang. Simplenet: A simple network for image anomaly detection and localization. In CVPR, 2023. 1, 7, 9, 14, 15, 16, 17, 18, 19   \n[31] S. Long, Q. Zhou, X. Li, X. Lu, C. Ying, L. Ma, Y. Luo, and S. Yan. Dgmamba: Domain generalization via generalized state space model. arXiv, 2024. 3   \n[32] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces. In ICLR, 2022. 3   \n[33] J. Pirnay and K. Chai. Inpainting transformer for anomaly detection. In ICIAP, 2022. 3   \n[34] Y. Qiao, Z. Yu, L. Guo, S. Chen, Z. Zhao, M. Sun, Q. Wu, and J. Liu. Vl-mamba: Exploring state space models for multimodal learning. arXiv preprint arXiv:2403.13600, 2024. 3   \n[35] N.-C. Ristea, N. Madan, R. T. Ionescu, K. Nasrollahi, F. S. Khan, T. B. Moeslund, and M. Shah. Self-supervised predictive convolutional attentive block for anomaly detection. In CVPR, 2022. 3   \n[36] K. Roth, L. Pemula, J. Zepeda, B. Sch\u00f6lkopf, T. Brox, and P. Gehler. Towards total recall in industrial anomaly detection. In CVPR, 2022. 1, 2, 8, 18, 19   \n[37] J. Ruan and S. Xiang. Vm-unet: Vision mamba unet for medical image segmentation. arXiv preprint arXiv:2402.02491, 2024. 2, 3, 5   \n[38] M. Rudolph, B. Wandt, and B. Rosenhahn. Same same but differnet: Semi-supervised defect detection with normalizing flows. In WACV, 2021. 2   \n[39] H. M. Schl\u00fcter, J. Tan, B. Hou, and B. Kainz. Natural synthetic anomalies for self-supervised anomaly detection and localization. In ECCV, 2022. 2   \n[40] Y. Shi, B. Xia, X. Jin, X. Wang, T. Zhao, X. Xia, X. Xiao, and W. Yang. Vmambair: Visual state space model for image restoration. arXiv preprint arXiv:2403.11423, 2024. 2, 3   \n[41] J. T. Smith, A. Warrington, and S. W. Linderman. Simplified state space layers for sequence modeling. In ICLR, 2022. 3   \n[42] C. Wang, W. Zhu, B.-B. Gao, Z. Gan, J. Zhang, Z. Gu, S. Qian, M. Chen, and L. Ma. Real-iad: A real-world multi-view dataset for benchmarking versatile industrial anomaly detection. In CVPR, 2024. 6, 7   \n[43] Z. Wang, J.-Q. Zheng, Y. Zhang, G. Cui, and L. Li. Mamba-unet: Unet-like pure visual mamba for medical image segmentation. arXiv preprint arXiv:2402.05079, 2024. 2, 3   \n[44] R. Wu, Y. Liu, P. Liang, and Q. Chang. H-vmunet: High-order vision mamba unet for medical image segmentation. arXiv preprint arXiv:2403.13642, 2024. 3   \n[45] J. Wyatt, A. Leach, S. M. Schmon, and C. G. Willcocks. Anoddpm: Anomaly detection with denoising diffusion probabilistic models using simplex noise. In CVPR, 2022. 3   \n[46] X. Yan, H. Zhang, X. Xu, X. Hu, and P.-A. Heng. Learning semantic context from normal samples for unsupervised anomaly detection. In AAAI, 2021. 3   \n[47] Z. You, L. Cui, Y. Shen, K. Yang, X. Lu, Y. Zheng, and X. Le. A unified model for multi-class anomaly detection. In NeurlPS, 2022. 1, 3, 7, 9, 14, 15, 16, 17, 18, 19   \n[48] V. Zavrtanik, M. Kristan, and D. Sko\u02c7caj. Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. In ICCV, 2021. 1, 2, 6   \n[49] J. Zhang, C. Xu, J. Li, W. Chen, Y. Wang, Y. Tai, S. Chen, C. Wang, F. Huang, and Y. Liu. Analogous to evolutionary algorithm: Designing a unified sequence model. NeurlPS, 2021. 6   \n[50] J. Zhang, X. Chen, Y. Wang, C. Wang, Y. Liu, X. Li, M.-H. Yang, and D. Tao. Exploring plain vit reconstruction for multi-class unsupervised anomaly detection. arXiv preprint arXiv:2312.07495, 2023. 1, 3, 6   \n[51] J. Zhang, H. He, Z. Gan, Q. He, Y. Cai, Z. Xue, Y. Wang, C. Wang, L. Xie, and Y. Liu. Ader: A comprehensive benchmark for multi-class visual anomaly detection. arXiv preprint arXiv:2406.03262, 2024. 1   \n[52] J. Zhang, X. Li, G. Tian, Z. Xue, Y. Liu, G. Pang, and D. Tao. Learning feature inversion for multi-class unsupervised anomaly detection under general-purpose coco-ad benchmark. arXiv, 2024. 3, 6   \n[53] J. Zhang, X. Li, Y. Wang, C. Wang, Y. Yang, Y. Liu, and D. Tao. Eatformer: Improving vision transformer inspired by evolutionary algorithm. IJCV, 2024. 6   \n[54] T. Zhang, X. Li, H. Yuan, S. Ji, and S. Yan. Point cloud mamba: Point cloud learning via state space model. arXiv preprint arXiv:2403.00762, 2024. 3   \n[55] X. Zhang, S. Li, X. Li, P. Huang, J. Shan, and T. Chen. Destseg: Segmentation guided denoising student-teacher for anomaly detection. In CVPR, 2023. 1, 7, 9, 14, 15, 16, 17, 18, 19   \n[56] H. Zhao, M. Zhang, W. Zhao, P. Ding, S. Huang, and D. Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference. arXiv preprint arXiv:2403.14520, 2024. 3   \n[57] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. 2, 3   \n[58] Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In ECCV, 2022. 6, 7 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Overview ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The supplementary material presents the following sections to strengthen the main manuscript: ", "page_idx": 13}, {"type": "text", "text": "\u2014 Sec. A shows more quantitative results for each category on the MVTec-AD dataset.   \nSec. B shows more quantitative results for each category on the VisA dataset.   \n\u2014 Sec. C shows more quantitative results for each category on the MVTec-3D dataset.   \n\u2014 Sec. D shows more quantitative results for each category on the Uni-Medical dataset.   \n\u2014 Sec. E shows more quantitative results for each category on the COCO-AD dataset.   \n\u2014 Sec. F shows more quantitative results for each category on the Real-IAD dataset.   \n\u2014 Sec. G shows more quantitative results for single-class results on the MVTec-AD dataset.   \n\u2014 Sec. H shows more quantitative results for single-class results on the VisA dataset. ", "page_idx": 13}, {"type": "text", "text": "A More Quantitative Results for Each Category on The MVTec-AD Dataset. ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Tab. A1 and Tab. A2 respectively present the results of image-level anomaly detection and pixel-level anomaly localization quantitative outcomes across all categories within the MVTec-AD dataset. The results further demonstrate the superiority of our method over various SoTA approaches. ", "page_idx": 13}, {"type": "table", "img_path": "8VKxTlnejE/tmp/525c1b42a23268aa11f019e8d52847d279fb9f0dfe3c451665d866c78b7bbb7e.jpg", "table_caption": ["Table A1: Comparison with SoTA methods on MVTec-AD dataset for multi-class anomaly detection with AU-ROC/AP/F1_max metrics. "], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "8VKxTlnejE/tmp/981c4aaad663880207d65b8e5d5bcb76574da4a63d3a900ca304079307c0a1ff.jpg", "table_caption": ["Table A2: Comparison with SoTA methods on MVTec-AD dataset for multi-class anomaly localization with AU-ROC/AP/F1_max/AU-PRO metrics. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "B More Quantitative Results for Each Category on The VisA Dataset. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tab. A3 and Tab. A4 respectively present the results of image-level anomaly detection and pixel-level anomaly localization quantitative outcomes across all categories within the VisA dataset. The results further demonstrate the superiority of our method over various SoTA approaches. ", "page_idx": 14}, {"type": "table", "img_path": "8VKxTlnejE/tmp/3da367e45a77042166e946df0171569e43e1ecf323e1a88389c6a15ce30f1cc2.jpg", "table_caption": ["Table A3: Comparison with SoTA methods on VisA dataset for multi-class anomaly detection with AU-ROC/AP/F1_max metrics. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "8VKxTlnejE/tmp/9eecd6b47f052dc79bbae2a939fa83357acc13a9e57a33457c526df597570d0e.jpg", "table_caption": ["Table A4: Comparison with SoTA methods on VisA dataset for multi-class anomaly localization with AU-ROC/AP/F1_max/AU-PRO metrics. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C More Quantitative Results for Each Category on The MVTec-3D Dataset. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Tab. A5 and Tab. A6 respectively present the results of image-level anomaly detection and pixel-level anomaly localization quantitative outcomes across all categories within the MVTec-3D dataset. The results further demonstrate the superiority of our method over various SoTA approaches. ", "page_idx": 14}, {"type": "table", "img_path": "8VKxTlnejE/tmp/d2f4ad9215ecc8e975bd61f19eb3d68e3356fcbf64247c47248cb2df754fe13e.jpg", "table_caption": ["Table A5: Comparison with SoTA methods on MVTec-3D dataset for multi-class anomaly detection with AU-ROC/AP/F1_max metrics. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "8VKxTlnejE/tmp/334816789d975687370d794ef0405a650c1ea14cf46803f4c7ede901441de75a.jpg", "table_caption": ["Table A6: Comparison with SoTA methods on MVTec-3D dataset for multi-class anomaly localization with AU-ROC/AP/F1_max/AU-PRO metrics. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D More Quantitative Results for Each Category on The Uni-Medical Dataset. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Tab. A7 and Tab. A8 respectively present the results of image-level anomaly detection and pixel-level anomaly localization quantitative outcomes across all categories within the Uni-Medical dataset. The results further demonstrate the superiority of our method over various SoTA approaches. ", "page_idx": 15}, {"type": "table", "img_path": "8VKxTlnejE/tmp/83f82bcf2b765deb7c045a3d4985747d7a2f31ebc40c63b816425f7bb6a305a5.jpg", "table_caption": ["Table A7: Comparison with SoTA methods on Uni-Medical dataset for multi-class anomaly detection with AU-ROC/AP/F1_max metrics. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "8VKxTlnejE/tmp/cc748d0effab1b9b0c48527a14d5713251fb983b430c8152bc732765ca36b0ef.jpg", "table_caption": ["Table A8: Comparison with SoTA methods on Uni-Medical dataset for multi-class anomaly localization with AU-ROC/AP/F1_max/AU-PRO metrics. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E More Quantitative Results for Each Category on The COCO-AD Dataset. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Tab. A9 and Tab. A10 respectively present the results of image-level anomaly detection and pixellevel anomaly localization quantitative outcomes across all categories within the COCO-AD dataset. The results further demonstrate the superiority of our method over various SoTA approaches. ", "page_idx": 15}, {"type": "table", "img_path": "8VKxTlnejE/tmp/1f4c6c13cda26d9e66ae683cff9f0c1ba3d8e6f589cbf56948f43ca53ed48d37.jpg", "table_caption": ["Table A9: Comparison with SoTA methods on COCO-AD dataset for multi-class anomaly detection with AU-ROC/AP/F1_max metrics. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "8VKxTlnejE/tmp/156b2aca91e758fd6888e06412cea4f84b12836205dc2b341839523ce6dc658a.jpg", "table_caption": ["Table A10: Comparison with SoTA methods on COCO-AD dataset for multi-class anomaly localization with AU-ROC/AP/F1_max/AU-PRO metrics. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "F More Quantitative Results for Each Category on The Real-IAD Dataset. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Tab. A11 and Tab. A12 respectively present the results of image-level anomaly detection and pixellevel anomaly localization quantitative outcomes across all categories within the Real-IAD dataset. The results further demonstrate the superiority of our method over various SoTA approaches. ", "page_idx": 16}, {"type": "table", "img_path": "8VKxTlnejE/tmp/4e944b62dc4cf3af16a79ee6712696c3676c5890b55e81da96d859d39a05d7b1.jpg", "table_caption": ["Table A11: Comparison with SoTA methods on Real-IAD dataset for multi-class anomaly detection with AU-ROC/AP/F1_max metrics. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "G More Quantitative Results for Each Category on The MVTec-AD Dataset for Single-class Anomaly Detection. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Tab. A13 and Tab. A14 respectively present the results of image-level anomaly detection and pixel-level anomaly localization quantitative outcomes for single-class anomaly detection of the MVTec-AD dataset. ", "page_idx": 16}, {"type": "table", "img_path": "8VKxTlnejE/tmp/d4ec81c038fed7fb12cee360853a23772065419771c6e68874469213e3a67c26.jpg", "table_caption": ["Table A12: Comparison with SoTA methods on Real-IAD dataset for multi-class anomaly localization with AU-ROC/AP/F1_max/AU-PRO metrics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "8VKxTlnejE/tmp/e4a1214dbbc156deb6a209a0286b3b3133425349e6068436f04b711bd8a7f74d.jpg", "table_caption": ["Table A13: Comparison with SoTA methods on MVTec-AD dataset for single-class anomaly detection with AU-ROC/AP/F1_max metrics. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "8VKxTlnejE/tmp/235e71cc395b02bbd73cd2f1cab4ac917afd0a65ffcc97ee9874c333365b05d2.jpg", "table_caption": ["Table A14: Comparison with SoTA methods on MVTec-AD dataset for single-class anomaly localization with AU-ROC/AP/F1_max/AU-PRO metrics. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "H More Quantitative Results for Each Category on The VisA Dataset for Single-class Anomaly Detection. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Tab. A15 and Tab. A16 respectively present the results of image-level anomaly detection and pixel-level anomaly localization quantitative outcomes for single-class anomaly detection of the VisA dataset. ", "page_idx": 18}, {"type": "table", "img_path": "8VKxTlnejE/tmp/c8d1d5ea500dcd62415bc9c43d0420d4c50e9d3b0e5b25b6b282ebfd347ccb45.jpg", "table_caption": ["Table A15: Comparison with SoTA methods on VisA dataset for single-class anomaly detection with AU-ROC/AP/F1_max metrics. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "8VKxTlnejE/tmp/d907bc2386f93b738ac064a3222aaa0b918fc52443613dfef22dc220f15f31ef.jpg", "table_caption": ["Table A16: Comparison with SoTA methods on VisA dataset for single-class anomaly localization with AU-ROC/AP/F1_max/AU-PRO metrics. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The abstract and introduction have clearly included the motivations, important assumptions, and contributions made in the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The authors have discussed the limitations of the work at the end of the Conclusion section. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The authors have clearly provided the full set of assumptions and complete proofs in the Method section. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The authors have presented all the experimental details in the Implementation Details section. Also, the codes are submitted as Supplementary Material. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The authors have submitted all the codes in Supplementary Material which provided sufficient instructions to faithfully reproduce the main experimental results. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 21}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The authors have presented all the training and test details in the Implementation Details section. Also, the codes are submitted as Supplementary Material. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The authors have presented the error bars in the experiments in Tab. 1 and show more results in Appendix 5 for each class of six datasets. Also, the authors have submitted the full codes which include the details. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The authors have included the sufficient information on the computer resources in the Implementation Details and Ablation and Analysis sections. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The authors conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The authors have explained the broader impacts of the work at the end of the Conclusion section. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The authors have cited the original paper that produced the code package or dataset. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The authors have submitted the details of the code/model which includes details about training, license, limitations, etc. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 24}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]