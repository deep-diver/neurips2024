[{"figure_path": "cO1llRY2Br/tables/tables_4_1.jpg", "caption": "Table 1: Editing results on various model editing tasks with GPT2-XL as the base model. In our methods, +L represents iReVa with fine-tuning as described in Section 4.2.", "description": "This table presents the quantitative results of different model editing methods on two benchmark datasets: zsRE-10K and PARAREL-10K.  The methods are compared based on four metrics: overall Score, Edit Success (ES), Paraphrase Success (PS), and Neighborhood Success (NS).  The table shows iReVa's superior performance compared to other state-of-the-art methods, particularly in Edit Success and Paraphrase Success, while maintaining good Neighborhood Success, indicating its ability to effectively incorporate new knowledge without significantly impacting existing knowledge. The '+L' variant of iReVa incorporates fine-tuning, showing a marginal performance improvement in one dataset.", "section": "6 Results and Analyses"}, {"figure_path": "cO1llRY2Br/tables/tables_6_1.jpg", "caption": "Table 1: Editing results on various model editing tasks with GPT2-XL as the base model. In our methods, +L represents iReVa with fine-tuning as described in Section 4.2.", "description": "This table presents the results of several model editing methods, including the proposed iReVa and its fine-tuned version, on two benchmark datasets: zsRE-10K and PARAREL-10K.  The performance is evaluated using four metrics: Score (the average of ES, PS, and NS), Edit Success (ES), Paraphrase Success (PS), and Neighborhood Success (NS).  The table allows for a comparison of iReVa's performance against several baselines in terms of successfully editing models, generalizing to paraphrased questions, and maintaining specificity.", "section": "6 Results and Analyses"}, {"figure_path": "cO1llRY2Br/tables/tables_7_1.jpg", "caption": "Table 2: Results of edit withdrawal on zsRE-10K dataset with GPT2-XL as the base model.", "description": "This table presents the results of an experiment testing the ability of the iReVa model to remove previously added edits and restore the original model's output.  The experiment used the zsRE-10K dataset and a GPT2-XL base model.  The \"Retrieve success\" column shows the percentage of times the model successfully retrieved the original output after removing the edit, while \"Consistency\" shows how often the model's output after removal matched the original base model's output.", "section": "6.2 Edit Withdrawal Test"}, {"figure_path": "cO1llRY2Br/tables/tables_7_2.jpg", "caption": "Table 1: Editing results on various model editing tasks with GPT2-XL as the base model. In our methods, +L represents iReVa with fine-tuning as described in Section 4.2.", "description": "This table presents the performance comparison of different model editing methods, including the proposed iReVa, on two benchmark datasets (zsRE-10K and PARAREL-10K).  The metrics used to evaluate the performance include Edit Success (ES), Paraphrase Success (PS), Neighborhood Success (NS), and the average score across these three metrics. The table shows that iReVa outperforms all baselines, demonstrating a significant improvement in overall performance. The '+L' variant of iReVa indicates the inclusion of fine-tuning.", "section": "6 Results and Analyses"}, {"figure_path": "cO1llRY2Br/tables/tables_8_1.jpg", "caption": "Table 1: Editing results on various model editing tasks with GPT2-XL as the base model. In our methods, +L represents iReVa with fine-tuning as described in Section 4.2.", "description": "This table presents the performance comparison of different model editing methods, including the proposed iReVa and several baselines, on two benchmark datasets (zsRE-10K and PARAREL-10K).  The results are evaluated using four metrics: Score (average of ES, PS, and NS), Edit Success (ES), Paraphrase Success (PS), and Neighborhood Success (NS).  The table shows iReVa's superiority in achieving high edit success rates while maintaining specificity and generalization.", "section": "6 Results and Analyses"}, {"figure_path": "cO1llRY2Br/tables/tables_12_1.jpg", "caption": "Table 1: Editing results on various model editing tasks with GPT2-XL as the base model. In our methods, +L represents iReVa with fine-tuning as described in Section 4.2.", "description": "This table presents the quantitative results of different model editing methods on two benchmark datasets: zsRE-10K and PARAREL-10K.  The methods are compared based on four metrics: overall Score, Edit Success (ES), Paraphrase Success (PS), and Neighborhood Success (NS).  The table shows that iReVa consistently outperforms other methods across all metrics on both datasets. The iReVa +L row shows the results after applying fine-tuning, offering a slight further improvement in performance.", "section": "6 Results and Analyses"}, {"figure_path": "cO1llRY2Br/tables/tables_12_2.jpg", "caption": "Table 1: Editing results on various model editing tasks with GPT2-XL as the base model. In our methods, +L represents iReVa with fine-tuning as described in Section 4.2.", "description": "This table presents the quantitative results of different model editing methods on two benchmark datasets, zsRE-10K and PARAREL-10K.  The methods are compared based on four metrics: overall Score, Edit Success (ES), Paraphrase Success (PS), and Neighborhood Success (NS).  iReVa and its fine-tuned variant (iReVa+L) are shown to significantly outperform existing state-of-the-art methods across all metrics and datasets.", "section": "6 Results and Analyses"}]