[{"type": "text", "text": "Initializing and Retrofitting Key-Value Adaptors for Traceable Model Editing ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "As the insight of knowledge storage in language models deepens, the ability   \n2 to perform CRUD (Create, Read, Update, Delete) operations on language mod  \n3 els becomes increasingly indispensable for satisfying the demands of managing   \n4 rapidly updating knowledge. Considering the high cost of fine-tuning language   \n5 models, model editing methods with low cost are usually required to manipu  \n6 late models\u2019 knowledge. Evident suggests that modules carrying knowledge in   \n7 a Transformer module are primarily the MLP blocks, thus we propose iReVa, a   \n8 method that explicitly initializes and retrofits key-value pairs into MLP blocks   \n9 to construct a new mapping of a piece of knowledge without damaging the ir  \n10 relevant knowledge. In comparison to existing methods, iReVa reveals better   \n11 interpretability and stronger capacity for carrying traceable edits. Experiment   \n12 results on series of GPT series models show our prominent performance on edit   \n13 success and generalization without influencing specificity. We also perform the   \n14 first attempt at conducting knowledge withdrawal test of iReVa. Our codes are   \n15 available at github.com/timberflow/iReVa.git. ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 Language Models (LMs) [1] are becoming imperative tools for consulting in real-world scenarios.   \n18 One significant reason for the prevalence of LMs is their ability to answer factoid questions. For   \n19 example, when we ask an LM with the question \u201cWho is president of America ?\u201d, it returns the   \n20 answer \u201cJoe Biden\u201d. Even though a mass amount of knowledge is stored in the LMs, we still face the   \n21 issue of out-of-date and missing knowledge [2, 3]. Alternatively, some knowledge may change over   \n22 years and some domain-specific knowledge may be absent from the LMs.   \n23 To bridge the gap, the task of model editing is introduced to edit the knowledge in LMs, which   \n24 targets at conducting change to the parameters of LMs and inject certain knowledge to them [4]. The   \n25 difficulty of this task lies in the manipulation to the LMs, where the knowledge is implicitly stored in   \n26 dense vectors. A naive solution to model editing is fine-tuning a LM with the new knowledge, whereas   \n27 the cost is climbing with the surging size of LMs. More recent studies propose to directly update   \n28 the models\u2019 weights in mastery phase [5, 6] via either teaching a hyper-network to learn the change   \n29 of the weights or locating-then-editing knowledge neurons [7, 8, 9, 10]. While the editing methods   \n30 above are efficient in updating knowledge in LMs, they encounter the difficulties of differentiating the   \n31 existing and new knowledge, which makes the editing hard to control. Methods like life-long model   \n32 editing [11], MELO [12], and T-Patcher [13] propose to learn the representation for new knowledge   \n33 and merge this information with the original models.   \n34 However, these methods still conform to the paradigm of learning the batch edit [13, 14] as a   \n35 whole without modeling edit parameters in a traceable way, which can not conform the edit success   \n36 to each edit and have a lack interpretability to the editing. In contrast, we propose a method of   \n37 Initializing and Retroftiting KEy-Value Adaptors (iReVa), an editing method that inserts a key-value   \n38 adaptor to indicate the mapping of an edit data pair and further retrofit the adaptor with multiple   \n39 objectives. Moreover, to prevent the unnecessary change to the irrelevant knowledge, we elaborately   \n40 design activation mechanism for the knowledge neurons. Experimental results on series of GPT-like   \n41 models show that iReVa is able to outperform the SOTA results by around $9\\%$ and $6\\%$ average score   \n42 improvement on zsRE-10K and PARAREL-10K, respectively. Moreover, iReVa is able to perform   \n43 knowledge withdrawal in almost perfect condition.   \n44 Our contributions are summarized as follows: 1) We introduce a novel editing method which initializes   \n45 and retrofits a key-value adaptor for traceable model editing, which is compatible to most LMs. 2)   \n46 Our method outperforms recent baselines on model editing tasks with noticeable margins based on   \n47 various evaluation metrics. 3) We validate the interpretability and generalization capabilities of our   \n48 method by conducting further analysis such as knowledge withdrawal test and generalization test. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "49 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "50 2.1 Insight of Knowledge Storage in Language Models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "51 As pre-trained LMs show strong abilities to answer factoid questions. Discussion about how LMs   \n52 store knowledge has emerged. [2] introduced the perspective of treating LMs as knowledge bases   \n53 and proved its plausibility, which attracted the subsequent attention towards the exploration on the   \n54 form of knowledge incorporated by LMs. The opinion pointed out by [15] indicates that factual   \n55 knowledge is stored in two-layer-FFN network of a Transformer due to the similar form as key  \n56 value memories. This opinion was followed by [16], which further derives the coefficient between   \n57 final prediction and knowledge neurons in MLP blocks. In contrast, [9], through a casual-tracing   \n58 experiment, posed viewpoints that knowledge is stored in self-attention module. [7] further validates   \n59 that the weight update is concentrated on parameters in self-attention module when we train models   \n60 with new knowledge. Our editing method is built upon the former hypothesis and we focus on the   \n61 editing to the MLP blocks. ", "page_idx": 1}, {"type": "text", "text": "62 2.2 Editing LMs by Manipulating Knowledge ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "63 With the frequent update of the knowledge, the demand of model editing increases. Diverse studies   \n64 have been proposed. By analogy with human knowledge acquisition, we can categorize the editing   \n65 into three distinct phases. In recognition phase [17], methods such as ERAC and IKE [8, 18] solved   \n66 the problem by importing additional memories in the form of a relevant contexts or prompts. In   \n67 association phase [6], parameter-efficient tuning [19, 20, 12, 11] inserts low-rank adaptors or prefix   \n68 token embeddings to fine-tune new knowledge and combine them to the original models. There are   \n69 also some studies directly changing the weights of Transformers in mastery phase [5]. For example,   \n70 [7] proposed KE and [8] proposed MEND to predict the updated parameters of a model with a trained   \n71 hyper-network. Furthermore, ROME [9] and MEMIT [10] compute the weight update explicitly with   \n72 proper representations of knowledge queries and values. However, none of them focuses on traceable   \n73 model editing, which allows more flexible manipulation of the knowledge. ", "page_idx": 1}, {"type": "text", "text": "74 3 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "75 We follow the previous studies [21, 12, 11] to formulate the task. Suppose we are given a base model   \n76 that could be a pre-trained language model $f_{\\Phi}$ parameterized by $\\Phi$ , model editing aims at editing $f_{\\Phi}$   \n77 with a dataset $\\bar{\\mathcal{D}}_{i n}=\\{(x_{1},y_{1}),...,\\bar{(x_{i},y_{i})}...,\\bar{(x_{n},y_{n})}\\}$ , where $\\left({x_{i},y_{i}}\\right)$ denotes the edit input-output   \n78 pairs. Initially, for $x_{i}\\in\\mathcal{D}_{i n}$ , the base model makes prediction ${\\hat{y}}_{i}=f(x_{i})$ but $\\hat{y_{i}}\\neq y_{i}$ . In this case,   \n79 we change $f_{\\Phi}$ by editing its parameters to $\\Phi^{*}$ . A good model editing to $f_{\\Phi^{*}}$ should satisfy: 1) for   \n80 any $x_{i}\\in\\mathcal{D}_{i n}$ , the edited model $f_{\\Phi^{*}}$ should output desired predictions, that is $f_{\\Phi^{*}}(x_{i})=y_{i};2)$ ) for   \n81 any input out of the scope of $\\mathcal{D}_{i n}$ , which is denoted as $\\mathcal{D}_{o u t}$ , the edited model $f_{\\Phi^{*}}$ should retain   \n82 the original predictions, that is $f_{\\Phi^{*}}(x_{i})\\,=\\,f_{\\Phi}(x_{i});$ 3) the edit of $(x_{i},y_{i})$ towards $f_{\\Phi^{*}}$ should not   \n83 influence any prior edits $x_{<i}\\in\\mathcal{D}_{i n}$ . ", "page_idx": 1}, {"type": "image", "img_path": "cO1llRY2Br/tmp/f113e8e70f319b7e27a58d7588da138bdba13b101cc1b459ff6625fcdcd8172c.jpg", "img_caption": ["Figure 1: Architecture of iReVa. The left block shows the training procedure with the newly inserted knowledge neurons. The middle block shows the inference procedure with in-scope and out-of-scope edits. We interpret the inference phase by giving some explicit examples (Please note we omit some neurons during inference due to the space limit.). When the query falls in the in-scope edit, our key-value adaptor will be activated and retrieve the corresponding knowledge. When the query falls in the out-of-scope edit, our key-value adaptor is inactive and the model retrieves knowledge from the original memory. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "84 4 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "85 To develop an editing method that supports traceable edits to knowledge neurons, we introduce   \n86 a novel method \u201ciReVa\u201d that initializes and Retrofits kEy-Value Adaptors for traceable model   \n87 editing. The pre-trained LM $f_{\\Phi}$ usually contains Transformer blocks, which consist of intertwined   \n88 self-attention and feed-forward layers. The prior studies [15] have shown that the inside MLP blocks   \n89 are commonly deemed as the neurons for storing implicit knowledge. Our method is able to insert new   \n90 knowledge but without damaging the irrelevant knowledge in the models by inserting and retroftiting   \n91 the key-value adaptors to these blocks.   \n92 Figure 3 depicts the architecture of our proposed method. For a two-layer-FFN MLP block in the $l$ -th   \n93 layer of the original model $f_{\\Phi}$ , we denote the weights of the first FFN layer as $\\mathbf{K}^{l}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ and the   \n94 second FFN as $\\mathbf{V}^{l}\\in\\mathbb{R}^{d_{2}\\times\\bar{d}_{1}}$ . Assume a hidden state $\\mathbf{h}^{l}\\in\\mathbb{R}^{d_{1}}$ is an input of the FFN of $l$ -th layer,   \n95 the above block processes the input as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{i}^{l}=\\mathrm{LAYER\\_NORM}(\\mathbf{h}^{l}+\\mathrm{SELF\\_ATTN}(\\mathbf{h}^{l}))}\\\\ {\\mathbf{o}^{l}=\\mathbf{V}^{l}\\boldsymbol{\\uptau}_{a c t}(\\mathbf{K}^{l\\top}\\mathbf{i}^{l})}\\\\ {\\mathbf{h}^{l+1}=\\mathrm{SELF\\_ATTN}(\\mathbf{i}^{l}+\\mathbf{o}^{l})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "96 where $g_{a c t}$ is the activation layer and $\\mathbf{h}^{l+1}\\in\\mathbb{R}^{d_{1}}$ is the input of the next Transformer block. Here, $\\mathbf{K}^{l}$   \n97 and $\\mathbf{V}^{l}$ emulate neural memories, where keys capture input patterns and values are stored knowledge   \n98 to be retrieved. When there comes an input vector, it first computes a distribution over the keys, then   \n99 retrieve the expected knowledge. As the process is just the same for each layer, we can choose any of   \n100 the layers to edit, we omit $l$ for simplicity in the following description.   \n101 Our method inserts a key-value adaptor into the existing MLP block. Specifically, we update $\\Phi$ by   \n102 inserting a new knowledge neuron to store the edit. Two matrices $\\bar{\\mathbf{K}}\\in\\mathbb{R}^{d_{1}\\times\\bar{n}}$ and $\\bar{\\bf V}\\in\\mathbb{R}^{n\\times{\\bar{d}}_{1}}$   \n103 perform as the key-value pair to memorize $n$ edited knowledge, where the knowledge is well-indexed   \n104 by $n$ dimensions. Therefore, Equation 2 becomes: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{o}=[\\mathbf{V}\\oplus\\hat{\\mathbf{V}}]^{\\mathsf{T}}g_{a c t}([\\mathbf{K}\\oplus\\hat{\\mathbf{K}}]^{\\mathsf{T}}\\mathbf{i})}\\\\ &{\\quad=\\mathbf{V}^{\\mathsf{T}}g_{a c t}(\\mathbf{K}^{\\mathsf{T}}\\mathbf{i})+\\hat{\\mathbf{V}}^{\\mathsf{T}}g_{a c t}(\\hat{\\mathbf{K}}^{\\mathsf{T}}\\mathbf{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "105 where $\\oplus$ denotes concatenation. As we can see, the key-value adaptor appends more information to   \n106 o, which could overwrite the original output. And original parameter set $\\Phi$ is extended to $\\Phi^{*}$ with the   \n107 new included parameters $\\hat{\\bf K}$ and $\\hat{\\textbf{V}}$ . Therefore, we aim to find a good key-value adaptor for model   \n108 editing that can collaborate with the original knowledge neurons. Considering the independence of   \n109 the above two function terms and the potential more flexible combination to the output, we relax   \n110 the formulation of the adaptor to A $\\begin{array}{r}{\\mathrm{\\DeltaDAPTOR}(\\mathbf{i};\\hat{\\mathbf{K}},\\hat{\\mathbf{V}})=\\alpha\\hat{\\mathbf{V}}^{\\intercal}g_{a c t}(\\hat{\\mathbf{K}}^{\\intercal}\\mathbf{i})}\\end{array}$ , which may be a more   \n111 expressive function with a scaling factor $\\alpha$ [19]. Next, we will introduce how to find such an optimal   \n112 adaptor which not only satisfies the edit success but also preserves the original model behavior. ", "page_idx": 3}, {"type": "text", "text": "113 4.1 Initial Key-Value Adaptors for In-Scope Editing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "114 Given an edit $(x_{i},y_{i})\\in\\mathcal{D}_{i n}$ , we first initialize its knowledge neuron $\\hat{\\mathbf{k}}^{0}\\in\\mathbb{R}^{d_{1}}$ and $\\hat{\\mathbf{v}}^{0}\\in\\mathbb{R}^{d_{1}}$ . For   \n115 $\\hat{\\mathbf{k}}^{0}$ , we initialize each key to the $x_{i}$ using the cached input i predicted by $f_{\\Phi}(x_{i})$ at layer $l$ , which   \n116 results in a high probability of matching to the input pattern. For $\\hat{\\mathbf{v}}^{0}$ , we initialize it using the weights   \n117 corresponding to $y_{i}$ from the last layer of $f_{\\Phi}$ . Specifically, $f_{\\Phi}(x_{i})$ takes charge of generating the next   \n118 token which can be deemed as the prediction to $x_{i}$ . Thus, we extract the corresponding column of   \n119 the ground truth token $y_{i}$ from the weights $\\mathbf{W}\\in\\mathbb{R}^{d_{1}\\times|V|}$ for generating the next token distribution,   \n120 where $\\vert V\\vert$ and $d_{1}$ are the sizes of the vocabulary and dimension of the last layer, respectively 1 After   \n121 initialization, we build a mapping from $x_{i}$ to $y_{i}$ in a Transformer. ", "page_idx": 3}, {"type": "text", "text": "122 4.2 Retrofit Adaptors for Model Editing (Training Phase) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "123 To prevent the effect of the inconsistent scaling brought by built-in parameters in Equation 1, we first   \n124 normalize i to ensure that its mean value is close to 0 before it is fed into the adaptor. Given $\\left({x_{i},y_{i}}\\right)$ ,   \n125 we can have the initialized key-value adaptor as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{ADAPTOR}(\\mathbf{i};\\hat{\\mathbf{K}},\\hat{\\mathbf{V}})=\\alpha(\\hat{\\mathbf{v}}^{0})^{\\sf T}g_{a c t}((\\hat{\\mathbf{k}}^{0})^{\\sf T}\\mathbf{i}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "126 To avoid the inserted key-value adaptor distracts the original knowledge stored in the existing neurons,   \n127 we propose to use the activation functions that can activate the memory with a large matching   \n128 value and ignore the memory with a small matching value. When we deploy the adaptor to models,   \n129 the activation function usually remains consistent with the base model. Furthermore, we apply a   \n130 hyper-parameter margin $\\theta>0$ , which allows memory to be active if $x>\\theta$ , otherwise inactivate. For   \n131 example, we use GeLU [22] for GPT [23] series model and our activation function can be denoted as: ", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{a c t}(x)=\\mathrm{GeLU}(x-\\theta).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 The motivations behind the above design in our activation function are two-fold: First, the activation   \n133 function works as a neuronal inhibitor to inhibit the activation of new knowledge neurons, which   \n134 retains the original output in most cases. Second, the involvement of the margin further raises the bar   \n135 to activate the new knowledge neurons. If a certain input is out of the editing scope, it fails to match   \n136 any memory, all inserted neurons will be inhibited after the activation function as shown in Figure 1.   \n137 In practice, edit input $x_{i}$ is shown in the form of a sequence of tokens such as $\\stackrel{\\leftarrow}{\\{t h e\\}}$ , capital, of,   \n138 China, is}\u201d and $y_{i}$ is the single-token answer \u201cBeijing\u201d. This indicates that we have a sequence of   \n139 hidden states $\\{\\mathbf{h}_{1},\\mathbf{h}_{2},...,\\mathbf{h}_{s}\\}$ corresponding to input $\\boldsymbol{x}_{i}=\\{w_{1},w_{2},...,w_{s}\\}$ . To avoid damaging the   \n140 original behavior of the edit model, the edit block merely works on the final token, which is the last   \n141 token before generation: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{ADAPTOR}(\\mathbf{i}_{j};\\hat{\\mathbf{K}},\\hat{\\mathbf{V}})=\\left\\{{0\\atop\\alpha\\hat{\\mathbf{V}}^{\\intercal}g_{a c t}(\\hat{\\mathbf{K}}^{\\intercal}\\mathbf{i}_{j})\\quad j=s}\\quad.\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 where $\\mathbf{i}_{j}$ is the input corresponding to the $j$ -th hidden state $\\mathbf{h}_{j}$ in the sequence. As a result, only   \n143 when the entire input sequence is fed into the model, the new knowledge is activated, which not   \n144 only prevents the dramatic change to the original model but also benefits the gradient update to the   \n145 key-value pairs2.   \n146 Fine-tuning adaptors with multiple objectives. While the above initialization effectively builds the   \n147 mapping from a certain edit input to the edit output, its impact on irrelevant knowledge may lead to   \n148 catastrophic forgetting [24] issue, which is caused by the extending key-value pairs of the adaptor.   \n149 In other words, we expect $\\mathrm{ADAPTOR}(\\mathbf{i};\\hat{\\mathbf{K}},\\hat{\\mathbf{V}})$ could dominate the output for each $x_{i}\\,\\in\\,\\mathcal{D}_{i n}$   \n150 but maintain unchanged prediction for $x_{i}\\in\\mathcal{D}_{o u t}$ and $x_{<i}\\in\\mathcal{D}_{i n}$ . Inspired by the elastic weight   \n151 consolidation for neural networks [25], we set optimization goals to retrofti $\\Phi^{*}$ with the consideration   \n152 of the following perspectives.   \n153 (1) To maximize the prediction of $y_{i}$ from the last layer, we maximize the probability of the ground   \n154 truth edit output given the edit input: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{e d i t}=-\\log[\\mathbb{P}_{f_{\\Phi}^{*}}(y_{i}|x_{i})]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "155 (2) Even though $\\mathcal{L}_{e d i t}$ enables models to fit the mapping from $x_{i}$ to $y_{i}$ effectively, it may push our   \n156 adaptor far from the initialization, which may damage the initialized key distribution and lead to   \n157 overftiting. Hence, we propose an additional term to prevent the dramatic change of the update of $\\hat{\\mathbf{k}}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{r e c}=||(\\hat{\\mathbf{k}}^{0}-\\hat{\\mathbf{k}})^{\\sf T}\\mathbf{i}||_{2}^{2}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "158 (3) Importantly, to prevent the fine-tuning from changing the irrelevant knowledge, we sample some   \n159 out-of-scope edit data to form $\\mathcal{D}_{o u t}{}^{3}$ and retain the original outputs from the model: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i r r}=-\\frac{1}{|\\mathcal{D}_{o u t}|}\\sum_{(x_{i},y_{i})\\in\\mathcal{D}_{o u t}}\\operatorname*{max}(\\hat{\\mathbf{k}}^{\\mathsf{T}}x_{i}-\\theta,0)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "160 Hence, we comprehend each aspect to form the final objective to retrofit the key-value adaptor: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathcal{L}_{e d i t}+a\\mathcal{L}_{r e c}+b\\mathcal{L}_{i r r}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "161 where $a,b$ are hyper-parameters denoting the importance of the different objective aspects. It is worth   \n162 noting that we edit one knowledge neuron at one time, but we still support sequential editing by   \n163 iteratively inserting key-value pairs. During training, all parameters except for $\\hat{\\mathbf{k}}$ and $\\hat{\\textbf{v}}$ for the current   \n164 edit are frozen. That is, we freeze the prior edit knowledge neurons and simply update the neuron   \n165 inserted for current edit. This procedure repeats until we have conducted edit over the entire dataset.   \n166 Compared with parameter high-efficient tuning methods [19, 26], which injects the new knowledge   \n167 into a pre-trained LM as a whole, iReVa focuses on editing parameters in a traceable manner. In other   \n168 words, we can locate the edited knowledge neurons. At the end, we display the training procedure of   \n169 iReVa in Algorithm 1. ", "page_idx": 4}, {"type": "table", "img_path": "cO1llRY2Br/tmp/32d318370c36673feff6cf7896f8352c37e9ef5f836ffd894c5add0e2a423f30.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "170 4.3 Activate Max-Matching Key in Adaptor (Inference Phase) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "171 As we iteratively append $\\hat{\\mathbf{k}}$ and $\\hat{\\bf v}$ to the knowledge neurons. The above procedure will sequentially   \n172 generate mappings from the edit input to the edit output. Eventually, we obtain two concatenated   \n173 matrices $\\hat{\\mathbf{K}}\\in\\mathbb{R}^{\\bar{d}_{1}\\times n}$ and $\\hat{\\mathbf{V}}\\in\\mathbb{V}^{n\\sqrt{}d_{1}}$ . During inference, we further control the amount of active   \n174 neurons and highlight the max-matching memory. To this end, we introduce a max-pooling layer to   \n175 extract the memory with the maximum matching score: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{ADAPTOR}(\\mathbf{i};\\hat{\\mathbf{K}},\\hat{\\mathbf{V}})=\\alpha\\hat{\\mathbf{V}}_{j}^{\\intercal}g_{a c t}(\\hat{\\mathbf{K}}_{j}^{\\intercal}\\mathbf{i}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 where $j=\\mathrm{argmax}_{t}(\\hat{\\mathbf{K}}_{t}^{\\mathsf{T}}\\mathbf{i})$ and $\\hat{\\mathbf{K}}_{t}$ denotes the $j$ -th column of $\\hat{\\bf K}$ . As we can see, when there comes a   \n177 new input, this layer will highlight the inserted knowledge neurons with the highest similarity score   \n178 to the input as shown in Figure 1. It is worth noting that we exclude the max-pooling layer during   \n179 the training procedure because this may impede the back-propagation due to the inactivation of the   \n180 neurons. ", "page_idx": 5}, {"type": "text", "text": "181 5 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "182 5.1 Datasets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "183 We perform extensive experiments on two modeling editing tasks: zsRE [8] is a commonly used   \n184 model editing tasks derived from question-answering benchmark. Totally 19, 086 examples are   \n185 included, each example includes a source question, paraphrase question and corresponding answer.   \n186 We construct another PARAREL [27] dataset. Each sentence in PARAREL is derived from a triplet   \n187 $(s,r,o)$ and the object o was replaced with a \u201c[MASK]\u201d token and a paraphrased version is also   \n188 involved. To apply PARAREL in model editing task, we selected those sentences that end with   \n189 \u201c $'/M A S K J^{*}$ token to conform to the format of next-token-prediction4. For both datasets, we sample   \n190 irrelevant question-answer pair from NQ to evaluate the preservation to out-of-scope editing. We test   \n191 10K edit in a batch and denote them as zsRE-10K and PARAREL-10K, respectively. ", "page_idx": 5}, {"type": "text", "text": "192 5.2 Baselines ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "193 We compare our iReVa with 6 advanced baselines that support batch editing: NO EDITING denotes   \n194 we do not modify the base model and utilize its original prediction; FT [28] is the simple fine-tuning   \n195 with a constraint on the key-value adaptor. MEMIT [10] and ROME [9] are two methods employing   \n196 a casual analysis to detect the most significant hidden states. They view the editing as a minimum   \n197 optimization and edit the weight directly, which is effective in batch edit; MEND [8] applies rank-one   \n198 decomposition to divide the model into two rank-one matrices, which is able to carry mass knowledge   \n199 in the dense metrics; MELO [12] activates specific LoRA block corresponding to specific queries for   \n200 multiple edits, which support large-scale editing in just one process. ", "page_idx": 5}, {"type": "text", "text": "201 5.3 Evaluation Metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "202 We follow the commonly-used evaluation metrics [9, 10] to measure the effect of our editing method. ", "page_idx": 5}, {"type": "text", "text": "203 1. Edit Success (ES) measures the models\u2019 prediction accuracy on edited data $x_{i}\\in\\mathcal{D}_{i n}$ by calculat  \n204 ing $\\begin{array}{r}{E S=\\frac{1}{\\mathrm{N}}\\sum_{i=0}^{\\mathrm{N}}\\mathbb{I}(y_{i}=f_{\\Phi}(x_{i}))}\\end{array}$ , which represents whether the new knowledge is successfully   \n205 injected into the base model.   \n206 2. Generalization (Paraphrase Success, PS) measures the models\u2019 prediction accuracy on paraphrase   \n207 questions provided by benchmarks. We compute paraphrase success with the same formulation but   \n208 for $x_{i}$ in paraphrase questions set. Paraphrase success indicates whether the model can recognize   \n209 similar expressions and provide edited answers.   \n3. Specificity (Neighborhood Success, NS) measures the models\u2019 prediction accuracy on irrelevant   \n211 questions. Different from $\\mathcal{D}_{o u t}$ , these questions are only used for preventing data leakage. We   \n212 compute neighborhood success with the same formulation but for $x_{i}$ in neighborhood questions set.   \n213 Neighborhood success manifests the capability of solving catastrophic forgetting and preserving   \n214 irrelevant knowledge stored in model. ", "page_idx": 5}, {"type": "text", "text": "215 4. Score is the average of three aforementioned metrics. ", "page_idx": 5}, {"type": "text", "text": "216 5.4 Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "217 Regarding editing datasets, we pre-process the edit input-output pairs following existing studies [8].   \n218 If the multiple tokens form a single prediction, we decompose the multiple tokens into multiple   \n219 data pairs by greedily appending the previous token in the edit output at the end of the edit input5.   \n220 For model selection, we conduct the experiments on GPT2-XL (1.5 Billion parameters) [29] due   \n221 to its wide application on existing model editing studies. We trained iReVa on a single NVIDIA   \n222 A800 80G GPU. On two evaluated benchmarks, we set $a=1e-3,b=1e-3,\\alpha=2$ $\\alpha=2e-1$ , and   \n223 iReVa is applied in 47-th (48 layers totally) layer inspired by the assertion in [15]. For the margin in   \n224 activation function, we set $\\theta=0.75$ for zsRE, $\\theta=0.65$ for PARAREL. During training, we conduct   \n225 experiments on $\\mathsf{G P T2-X L}$ with setting learning rate as $5e-2$ , batch size as 1, epoch number as 5.   \n226 We set the learning rate as $5e-3$ for GPT-NEO-2.7B. More implementation details of baselines is   \n227 displayed in Appendix A.7. We re-implement the comparable baselines using the same configuration   \n228 reported in existing studies. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "229 6 Results and Analyses ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "230 6.1 Comparisons to Existing Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "231 Table 6.1 exemplifies performances of iReVa and baselines on zsRE and PARAREL with 10K edits   \n232 in batch. As we can see, iReVa outperforms all baselines on average scores with noticeable margins.   \n233 Even without retrofitting, our method is able to outperform the SOTA results by around $9\\%$ and   \n234 $6\\%$ average score improvement on zsRE-10K and PARAREL-10K, respectively. Among all the   \n235 baseline methods, FT achieves good results on ES and PS, this indicates that fine-tuning is simple   \n236 but effective to inject knowledge but it could easily distract the irrelevant knowledge, resulting in   \n237 a poor NS. Whereas other baselines can not guarantee the editing success in a batch, resulting in   \n238 poor ES and PS. In comparison, iReVa achieves impressive results on all the evaluation metrics. It   \n239 achieves close to $100\\%$ ES without detriment to the original NS. We observe a slight improvement   \n240 from the results of iReVa to $\\mathrm{iReVa}+\\mathcal{L}$ on zsRE-10K dataset, it verifies our rationale deduce for the   \n241 initialization of key-value pairs. However, the improvement brought by fine-tuning is not maintained   \n242 on PARAREL-10K, we suspect this is because the involvement of irrelevant knowledge brings in   \n243 little unexpected noise with possibility. ", "page_idx": 6}, {"type": "table", "img_path": "cO1llRY2Br/tmp/c461b907e54f0d79e8f41b3e79785683dfe061b3f68172c4c8ee1f4aa10aa463.jpg", "table_caption": ["Table 1: Editing results on various model editing tasks with $\\mathsf{G P T2-X L}$ as the base model. In our methods, $+\\mathcal{L}$ represents iReVa with fine-tuning as described in Section 4.2. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "244 6.2 Edit Withdrawal Test ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "245 Compared with the existing editing methods, our method has the unique advantage of interpretability   \n246 and traceability, that is we can clearly identify the edit for each newly inserted key-value pair.   \n247 This provides a chance to conduct an edit withdrawal test. Specifically, we test, after editing on   \n248 10K examples, if iReVa is able to withdraw certain edits and recover the original output from   \n249 the base model without much loss. To this end, we inhibit corresponding knowledge neurons as   \n250 withdrawing the edit, which is denoted as $f_{\\Phi^{*}}^{-\\hat{\\mathbf{k}}}$ . For evaluation, we introduce two metrics, namely   \n251 Retrieve Success and Consistency. They are formulated as $\\begin{array}{r}{R S\\,=\\,\\frac{1}{N}\\sum_{i=0}^{N}\\mathbb{I}\\!\\left(f_{\\Phi^{*}}(x_{i})\\,\\neq\\,f_{\\Phi^{*}}^{-\\hat{\\mathbf{k}_{i}}}\\right)}\\end{array}$   \n252 and $\\begin{array}{r}{C o n=\\frac{1}{N}\\sum_{i=0}^{N}\\mathbb{I}(f_{\\Phi}(x_{i})=f_{\\Phi^{*}}^{-\\hat{\\mathbf{k}_{i}}})}\\end{array}$ , res ely. The evaluation result on zsRE-10K is shown   \nin Table 6.2. The results which are close to $100\\%$   \n254 activation of knowledge neurons and easily withdraw the updated knowledge. It is worth noting that   \n255 this test is not applicable to any other editing methods as their edited parameters are untraceable. This   \n256 is the first attempt at conducting more flexible knowledge editing. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Results of edit withdrawal on zsRE-10K dataset with $\\mathsf{G P T2-X L}$ as the base model. ", "page_idx": 7}, {"type": "table", "img_path": "cO1llRY2Br/tmp/d439482d3fb3d1b6f8de4fb3018df68cae8baa620b333d5a71ccf44cb33885a3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "257 6.3 Efficiency Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "258 We discuss the spatial and time complexities of iReVa. Regarding time complexity during inference,   \n259 iReVa only insert the adaptor in a single $l$ -th layer and the insertion only affects the final token   \n260 prediction of the input. With $\\mathbf{i}\\in\\mathbb{R}^{1\\times\\bar{d}_{1}},\\hat{\\mathbf{K}}\\in\\mathbb{R}^{\\bar{d}_{1}\\times n},\\hat{\\mathbf{V}}\\in\\mathbb{R}^{n\\times d_{1}}$ , the extra time consumption is   \n261 $\\dot{\\mathcal{O}}(d_{1}^{2}n)$ , which is unrelated to the input length and number of layers. Regarding spacial complexity,   \n262 as we insert two vectors for each edit in a single layer, the extra spacial consumption is $\\mathcal{O}(2n\\bar{d}_{1})$ . In   \n263 practice, for $\\mathsf{G P T2-X L}$ with 1.5B parameters, the adaptor merely possesses 0.08B parameters with   \n264 10K edits. There is no additional spacial complexity involved in the training phase, given that only   \n265 $2d_{1}$ parameters are learnable for each edit. We empirically record that 10K edits with iReVa cost   \n266 $7.5/1.6$ hours (fine-tuning/without fine-tuning) with a single NVIDIA A800 GPU, compared to 9.16   \n267 hours for ROME and 5.4 hours for MEMIT. ", "page_idx": 7}, {"type": "text", "text": "268 6.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "269 Table 6.4 shows iReVa\u2019s performance on zsRE-10K when we iteratively remove sub-modules: (1)   \n270 w/o activation function denotes that we remove the activation function proposed in Equation 6. (2)   \n271 w/o max-pooling denotes that we involve all knowledge neurons during inference instead of the   \n272 design of Equation 12. (3) w/o $\\mathcal{L}_{r e c}$ denotes that we train iReVa without initialization and set $a=0$   \n273 in Equation 11. (4) w/o $\\mathcal{L}_{i r r}$ means we do not apply $\\mathcal{L}_{i r r}$ by setting $b=0$ in Equation 11. As we can   \n274 see, all the modules contribute to the good results. In comparison, the activation function is important   \n275 to preserve the out-of-scope edit. Without activation function, we can attain better results on ES   \n276 and PS, but NS will decrease sharply. We also find that the influence of max-pooling is significant,   \n277 which may attribute to noisy data added by a large amount of active but irrelevant knowledge neurons.   \n278 Besides, excluding $\\mathcal{L}_{r e c}$ will lead to an observable drop on the three metrics because we discord the   \n279 effective initialization on $\\hat{\\bf K}$ and $\\hat{\\textbf{V}}$ . Finally, disabling $\\mathcal{L}_{i r r}$ may induce a marginal improvement in   \n280 ES and PS, but at the cost of a reduction in NS. ", "page_idx": 7}, {"type": "table", "img_path": "cO1llRY2Br/tmp/8b337a789f8bdefaba9030b002b6dcac2dab1bc21d743d5075ad1ebd7758ef16.jpg", "table_caption": ["Table 3: Results of ablation study on zsRE dataset with $\\mathsf{G P T2-X L}$ as the base model. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "281 6.5 Generalization Capabilities of iReVa ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "282 Layer generalization. To evaluate the effect of iReVa in various layers, we iteratively apply iReVa   \n283 and the other two baseline editing methods to different layers of $\\mathsf{G P T2-X L}$ , which consists of 48   \n284 layers in total. Figure 6.5 illustrates the influence of three metrics on different layers with intervals.   \n285 The tendency shows that the edit in the higher layer results in better editing results. This indicates   \n286 that LMs\u2019 final prediction primarily depends on the information retrieved from higher layers and   \n287 the knowledge stored in lower layers may be overshadowed. For ROME and MEMIT, apparently,   \n288 they show distinct generalizations in edit layer. Their ES and PS peak at middle layer like 17 or 22,   \n289 which proves that the layer generalization is remarkably relevant to the characteristics of different   \n290 methods. Even though MEMIT achieves good performance in NS when the edit happens in lower   \n291 layers, overall iReVa outperforms the baselines regarding the comprehensive evaluation metrics.   \n292 LMs generalization. We also test iReVa on different LLMs as base models, Figure 6.5 shows   \n293 iReVa\u2019s generality on different backbones. We apply a larger LM GPT-NEO-2.7B [30] and   \n294 smaller LM GPT2-LARGE [29] to evaluate the effect of iReVa on LMs with different sizes. Both   \n295 GPT-NEO-2.7B and GPT-LARGE contain two-layer-FFN MLP blocks. IReVa can be deemed as a   \n296 plug-in module for general LMs, which can be applied to more LMs. From the figure, we observe   \n297 that iReVa can achieve the best average score on both LMs, which shows its general effect.   \n298 Edit quantity generalization. We discuss the influence on iReVa\u2019s performance with the variation   \n299 of edit quantity, we simply increase the number of edits in the batch and evaluate ES, PS and NS.   \n300 Figure 6.5 shows the tendency of three metrics along with the comparison to baselines ROME and   \n301 MEMIT. As we can see, iReVa is robust to the number of edit in the batch. It consistently surpasses   \n302 the other baselines when dealing with the various number of edits. MEMIT performs poorly even   \n303 with a small number of edits. ROME drops dramatically as the edit number grows. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "cO1llRY2Br/tmp/241d72fac767452c21d9d8f41a7da39716087c44fcf3987b6109232f1320fa62.jpg", "img_caption": ["Figure 2: Results of edits in various layers on zsRE dataset with GPT2-XL as the base model. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "cO1llRY2Br/tmp/81f8951443cd713ee317b616339b66b5c026171eea2c58b0ec247e817933cc04.jpg", "table_caption": ["Table 4: Results on zsRE dataset with GPT2-LARGE, GPT-NEO-2.7B as the base models. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "cO1llRY2Br/tmp/921017712bc6fe6b636805aeb533a99673f89b8bf4ee766df27dfa504298c5ce.jpg", "img_caption": ["Figure 3: Results of edits with various size on zsRE dataset with GPT2-XL as the base model. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "304 7 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "305 In this paper, we propose iReVa, a model editing method with traceable knowledge storage, which   \n306 inserts edit key-value adaptor into the MLP module of a transformer model explicitly. iReVa displays   \n307 prominent abilities of edit success, generalization and specificity and outperforms baselines with an   \n308 observable margin. Besides, iReVa first successfully demonstrates its capacity on the knowledge   \n309 withdrawal. For further research, we will focus on generalize iReVa to more LM architectures. ", "page_idx": 8}, {"type": "text", "text": "310 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "311 [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n312 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n313 few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n314 [2] Fabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H.   \n315 Miller, and Sebastian Riedel. Language models as knowledge bases?, 2019.   \n316 [3] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language   \n317 models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438,   \n318 2020.   \n319 [4] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun   \n320 Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen   \n321 Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou,   \n322 and Huajun Chen. A comprehensive study of knowledge editing for large language models,   \n323 2024.   \n324 [5] N Jayashri and K Kalaiselvi. Knowledge acquisition\u2013scholarly foundations with knowledge   \n325 management. International Journal of Advanced Studies of Scientific Research, 3(12), 2018.   \n326 [6] J\u00e9r\u00f4me Seymour Bruner. The process of education. 1960.   \n327 [7] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models,   \n328 2021.   \n329 [8] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast   \n330 model editing at scale, 2022.   \n331 [9] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual   \n332 associations in gpt, 2023.   \n333 [10] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass  \n334 editing memory in a transformer, 2023.   \n335 [11] Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh   \n336 Ghassemi. Aging with grace: Lifelong model editing with discrete key-value adaptors, 2023.   \n337 [12] Lang Yu, Qin Chen, Jie Zhou, and Liang He. Melo: Enhancing model editing with neuron  \n338 indexed dynamic lora, 2023.   \n339 [13] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong.   \n340 Transformer-patcher: One mistake worth one neuron. In The Eleventh International Con  \n341 ference on Learning Representations, 2023.   \n342 [14] Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit   \n343 Bansal, and Srinivasan Iyer. Do language models have beliefs? methods for detecting, updating,   \n344 and visualizing model beliefs. arXiv preprint arXiv:2111.13654, 2021.   \n345 [15] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers   \n346 are key-value memories, 2021.   \n347 [16] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons   \n348 in pretrained transformers, 2022.   \n349 [17] J\u00e9r\u00f4me Seymour Bruner. The course of cognitive growth. American Psychologist, 19:1\u201315,   \n350 1964.   \n351 [18] Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang.   \n352 Can we edit factual knowledge by in-context learning?, 2023.   \n353 [19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,   \n354 Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv   \n355 preprint arXiv:2106.09685, 2021.   \n356 [20] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.   \n357 arXiv preprint arXiv:2101.00190, 2021.   \n358 [21] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn.   \n359 Memory-based model editing at scale, 2022.   \n360 [22] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.   \n361 [23] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language   \n362 understanding by generative pre-training. 2018.   \n363 [24] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks:   \n364 The sequential learning problem. volume 24 of Psychology of Learning and Motivation, pages   \n365 109\u2013165. Academic Press, 1989.   \n366 [25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,   \n367 Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.   \n368 Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of   \n369 sciences, 114(13):3521\u20133526, 2017.   \n370 [26] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt   \n371 understands, too. AI Open, 2023.   \n372 [27] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich   \n373 Sch\u00fctze, and Yoav Goldberg. Measuring and improving consistency in pretrained language   \n374 models, 2021.   \n375 [28] Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and   \n376 Sanjiv Kumar. Modifying memories in transformer models, 2021.   \n377 [29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.   \n378 Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n379 [30] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason   \n380 Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse   \n381 text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n382 [31] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an   \n383 invariant mapping. In 2006 IEEE computer society conference on computer vision and pattern   \n384 recognition (CVPR\u201906), volume 2, pages 1735\u20131742. IEEE, 2006. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "385 A Appendix ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "386 A.1 Detailed Description of Initialization of Key-Value Adaptor ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "387 We describe how we initialize $\\mathbf{k}$ and $\\mathbf{v}$ in detail. Given the input $\\boldsymbol{x}_{i}=\\{w_{1},w_{2},...,w_{s}\\}$ , we first obtain   \n388 the corresponding embeddings for each token, such that $\\mathbf{x}_{i}=\\left\\{\\mathbf{w}_{1},\\mathbf{w}_{2},...,\\mathbf{w}_{s}\\right\\}$ . After encoded via   \n389 $l$ Transformer layers, we obtain a sequence of hidden representations as input $\\{\\mathbf{h}_{1}^{l},\\mathbf{h}_{2}^{l},...,\\mathbf{h}_{s}^{l}\\}$ . In   \n390 the two-layer-FFN MLP block of $l$ -th layer, after self-attention and layer norm, we have the hidden   \n391 representation of the last token as: ", "page_idx": 10}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{i}_{s}^{l}=\\mathrm{LAYER\\_NORM}(\\mathbf{h}_{s}^{l}+\\mathrm{SELF\\_ATTN}(\\mathbf{h}_{s}^{1}))}\\\\ {\\mathbf{o}_{s}^{l}=\\mathbf{V}^{l}\\boldsymbol{\\uptau}_{a c t}(\\mathbf{K}^{l}\\boldsymbol{\\uptau}_{\\mathbf{i}_{s}^{l}})}\\\\ {\\mathbf{h}_{s}^{l+1}=\\mathrm{SELF\\_ATTN}(\\mathbf{i}_{s}^{l}+\\mathbf{o}_{s}^{l})}\\end{array}\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "392 We extract $\\mathbf{i}_{s}^{l+1}$ as the initialization of $\\hat{\\mathbf{k}}^{0}$ . Subsequently, $\\{\\mathbf{h}_{1}^{l+1},\\mathbf{h}_{2}^{l+1},...,\\mathbf{h}_{s}^{l+1}\\}$ are further processed   \n393 via the higher layers. In the last layer, we make prediction based on the hidden representation in $L$ -th   \n394 layer, which can be denoted as: ", "page_idx": 10}, {"type": "equation", "text": "$$\nP_{f_{\\Phi}}(y_{i}|x_{i})=\\mathrm{SOFTMAX}(\\mathbf{W}^{\\top}\\mathbf{h}_{s}^{L}),\n$$", "text_format": "latex", "page_idx": 10}, {"type": "text", "text": "395 where $\\mathbf{W}\\in\\mathbb{R}^{d_{1}\\times|V|}$ and each column denotes the representation of a token. We extract the column   \n396 corresponding to the ground truth edit out token $y_{i}$ , that is $\\hat{\\mathbf{v}}^{0}=\\mathbf{W}_{[:,y_{i}]}$ . ", "page_idx": 10}, {"type": "text", "text": "397 A.2 Discussion of Back Propagation of Key-Value Adaptor ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "398 Recall the knowledge neurons of our key-value adaptor are: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbf{o}=\\mathbf{v}^{\\mathsf{T}}g_{a c t}(\\mathbf{k}^{\\mathsf{T}}\\mathbf{i})+\\hat{\\mathbf{v}}^{\\mathsf{T}}g_{a c t}(\\hat{\\mathbf{k}}^{\\mathsf{T}}\\mathbf{i})\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "399 Given $\\mathcal{L}$ , the gradients are computed as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{d\\mathcal{L}}{d{\\hat{\\mathbf{k}}}}=g_{a c t}^{\\prime}({\\hat{\\mathbf{k}}}^{\\mathsf{T}}\\mathbf{i})\\cdot{\\hat{\\mathbf{v}}}\\cdot{\\hat{\\mathbf{i}}}^{\\mathsf{T}}\\frac{d{\\mathcal{L}}}{d\\mathbf{o}}}}\\\\ {{\\displaystyle\\frac{d\\mathcal{L}}{d{\\hat{\\mathbf{v}}}}=g_{a c t}({\\hat{\\mathbf{k}}}^{\\mathsf{T}}\\mathbf{i})\\frac{d\\mathcal{L}}{d\\mathbf{o}}}}\\\\ {{\\displaystyle\\frac{d\\mathcal{L}}{d{\\hat{\\mathbf{i}}}}=[g_{a c t}^{\\prime}(\\mathbf{k}^{\\mathsf{T}}\\mathbf{i})\\mathbf{v}^{\\mathsf{T}}\\mathbf{k}+g_{a c t}^{\\prime}({\\hat{\\mathbf{k}}}^{\\mathsf{T}}\\mathbf{i}){\\hat{\\mathbf{v}}}^{\\mathsf{T}}{\\hat{\\mathbf{k}}}]\\frac{d\\mathcal{L}}{d\\mathbf{o}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "400 where $g_{a c t}^{\\prime}$ is the derivative of the activation function. We have multiple observations of the gradients:   \n401 First, we would like the newly inserted neuron to be activated initially, namely $g_{a c t}>0$ . Otherwise,   \n402 the gradients are close to 0 and the neurons are likely to be dead. This is the reason why we initialize   \n403 the $\\hat{\\mathbf{k}}$ and $\\hat{\\textbf{v}}$ with the consideration of having a high matching value of ${\\bf k}^{\\top}{\\bf i}.$ . Second, when we update   \n404 $\\hat{\\mathbf{k}}$ and $\\hat{\\textbf{v}}$ , they are unrelated to $\\mathbf{k}$ and $\\mathbf{v}$ , which makes it possible to isolate the irrelevant knowledge. ", "page_idx": 11}, {"type": "text", "text": "405 For the knowledge neurons without our key-value adaptor, we have the propagation: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbf{o}=\\mathbf{v}^{\\top}g_{a c t}(\\mathbf{k}^{\\top}\\mathbf{i}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "406 The gradients of i are computed as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\frac{d\\mathcal{L}}{d\\mathbf{i}}=g_{a c t}^{\\prime}(\\mathbf{k}^{\\mathsf{T}}\\mathbf{i})\\mathbf{v}^{\\mathsf{T}}\\mathbf{k}\\frac{d\\mathcal{L}}{d\\mathbf{o}}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "407 As we can see, excluding the key-value adaptor in the neuron makes the gradients simply derived   \n408 from $\\mathbf{k}$ and $\\mathbf{v}$ , which maintains the original knowledge in the neurons. ", "page_idx": 11}, {"type": "text", "text": "409 A.3 Influence of $\\theta$ and $a$ ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "410 The influence of $\\theta$ is illustrated in A.3. The figure shows the trade-off between the three metrics   \n411 smoothly. The primary affected metric is Neighborhood Success, and Edit Success and Paraphrased   \n412 Success exhibit a slight downward trend. For $a$ , we find that merely Paraphrase Success peaks   \n413 while $a=1e\\mathrm{~-~}2$ , meanwhile Edit Success and Neighborhood Success do not continue to improve   \n414 with the increase of $a$ . ", "page_idx": 11}, {"type": "image", "img_path": "cO1llRY2Br/tmp/308bc7a5aff66088aa16ccf5087ae022292c0b153c73f007e789f987dcf3df6f.jpg", "img_caption": ["Figure 4: Correlation between three metrics and $\\theta$ (left) or $a(\\mathrm{right})$ of iReVa, ROME, MEMIT "], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "415 A.4 Sample off-scope examples for iReVa ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "416 To enhance iReVa\u2019s Specificity, we generate 3 kinds of irrelevant questions $q$ for each $(x,y)\\in\\mathcal{D}_{i n}$   \n417 to minimize $\\hat{\\mathbf{K}}_{i}^{\\top}\\cdot{x_{o u t}}$ , where $x_{o u t}$ is the representations of $q$ . These questions are listed as follows:   \n418 a) Randomly generated questions produced by feeding base model with a bos (begin of sentence)   \n419 token. b) Questions generated by base model with feeding the subject $s$ of the $x$ provided by the   \n420 benchmark. c) Questions sampled from other examples in training dataset, whose opinion is similar   \n421 to contrastive learning [31]. During iReVa training, we generate 2 questions in a), 2 questions in b)   \n422 and 6 questions in c) for each training example. ", "page_idx": 12}, {"type": "text", "text": "423 A.5 Pre-processing procedure of zsRE ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "424 Shown in 2, we split each $(x,y)$ pair into multiple $(x^{\\prime},y^{\\prime})$ to ensure $y^{\\prime}$ is a single-token edit out. This   \n425 procedure is also applied in the evaluation of zsRE and PARAREL, which measures the $(i+1)$ -th   \n426 token of edit-out prediction accuracy given edit-in and $i$ prefixes of edit-out. ", "page_idx": 12}, {"type": "table", "img_path": "cO1llRY2Br/tmp/be2a6709cd32c6c21207820874040c1ba62fe7b6e9706bb5afa7188327dd2c2f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "427 A.6 Pre-processing Procedure of PARAREL ", "text_level": 1, "page_idx": 12}, {"type": "table", "img_path": "cO1llRY2Br/tmp/59d157866c0858e4c140509c48e1f3fe62ffc6789f10534fab49908e20629001.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "428 This section details the pre-process method on close text dataset PARAREL [27]. PARAREL contains   \n429 34 types of relations $r$ , with an average of 900 question bags $b$ per relation, totaling 27,738 distinct   \n430 questions $q$ . And for each question bag, around 9 rephrased versions are recorded with a sole answer   \n431 a.   \n432 The entire pre-process algorithm is shown in 3. To make PARAREL applicable for next-token  \n433 prediction task, we reserve the sentences that end with special token \u201c[MASK]\u201d. After a round of   \n434 filtering, we removed question bags $b$ with only 1 valid sentence that ends with \u201c[MASK]\u201d for both   \n435 Edit Success and Paraphrase Success need to be computed. During this filtering, we collect the   \n436 subject of question $s$ bag by calculating the longest common sub-array of all $q\\in b$ tokenized by   \n437 GPT2Tokenizer [29] simultaneously for specific methods require the subject of a question. The   \n438 next screening occurs at $b$ whose subject $s$ is an empty string or identical to $b[0]$ . With residual   \n439 question bags $b^{\\prime}$ , we choose $b^{\\prime}[0]$ as the source question and a randomly sampled question from $b^{\\prime}[1:]$   \n440 as the paraphrase question.   \n441 Empirically, we believe PARAREL is harder than zsRE because the average token length of edit   \n442 target is shorter, thus model can\u2019t give more empirical predictions based on given prefix of the target,   \n443 which is mentioned in A.5. In other word, the account for first-token prediction may influence the   \n444 difficulty of datasets noticeably. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "445 A.7 Implementation Details of Comparable Baselines ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "446 A.7.1 Fine Tuning(FT) ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "447 We implement fine tuning on two feed-forward networks(mlp.c_fc, mlp.c_proj) at the layer   \n448 of 46 with GPT2-XL. Base model is trained for 20 epochs with $l r=1e-4$ , batch size $=32$ . ", "page_idx": 13}, {"type": "text", "text": "449 A.7.2 MEND ", "page_idx": 13}, {"type": "text", "text": "450 We do not load the pre-trained MEND [8] weight, but apply MEND directly. Hyper-parameters of   \n451 MEND keep consistent with the configuration of MEND\u2019s open-source code. ", "page_idx": 13}, {"type": "text", "text": "452 A.7.3 ROME, MEMIT ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "453 ROME [9] and MEMIT [10]\u2019s setups on GPT2-XL also remain identical to the source code. On   \n454 GPT-NEO-2.7B, we alter the edit layer to 5 for ROME and {3,4,5,6,7,8} for MEMIT. ", "page_idx": 13}, {"type": "text", "text": "455 A.7.4 MELO ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "456 Due to larger edit amount and different backbone for zsRE, we modify several configurations to   \n457 make MELO [12] comparable to our methods. For MELO\u2019s code book, we enlarge the number of   \n458 blocks (clusters) to 100. Besides, we rewrite MELO\u2019s training loss to make it compatible with causal   \n459 decoder.   \n461 The checklist is designed to encourage best practices for responsible machine learning research,   \n462 addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove   \n463 the checklist: The papers not including the checklist will be desk rejected. The checklist should   \n464 follow the references and follow the (optional) supplemental material. The checklist does NOT count   \n465 towards the page limit.   \n466 Please read the checklist guidelines carefully for information on how to answer these questions. For   \n467 each question in the checklist:   \n468 \u2022 You should answer [Yes] , [No] , or [NA] .   \n469 \u2022 [NA] means either that the question is Not Applicable for that particular paper or the   \n470 relevant information is Not Available.   \n471 \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA).   \n472 The checklist answers are an integral part of your paper submission. They are visible to the   \n473 reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it   \n474 (after eventual revisions) with the final version of your paper, and its final version will be published   \n475 with the paper.   \n476 The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.   \n477 While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a   \n478 proper justification is given (e.g., \"error bars are not reported because it would be too computationally   \n479 expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering   \n480 \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we   \n481 acknowledge that the true answer is often more nuanced, so please just use your best judgment and   \n482 write a justification to elaborate. All supporting evidence can appear either in the main paper or the   \n483 supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification   \n484 please point to the section(s) where related material for the question can be found. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "485 IMPORTANT, please: ", "page_idx": 14}, {"type": "text", "text": "486 \u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\",   \n487 \u2022 Keep the checklist subsection headings, questions/answers and guidelines below.   \n488 \u2022 Do not modify the questions and only use the provided macros for your answers.   \n489 1. Claims   \n490 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n491 paper\u2019s contributions and scope?   \n492 Answer: [TODO]   \n493 Justification: [TODO]   \n494 Guidelines:   \n495 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n496 made in the paper.   \n497 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n498 contributions made in the paper and important assumptions and limitations. A No or   \n499 NA answer to this question will not be perceived well by the reviewers.   \n500 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n501 much the results can be expected to generalize to other settings.   \n502 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n503 are not attained by the paper.   \n504 2. Limitations   \n505 Question: Does the paper discuss the limitations of the work performed by the authors?   \n506 Answer: [TODO]   \n507 Justification: [TODO] ", "page_idx": 14}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper. \u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. \u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. \u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. \u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. \u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [TODO] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "Answer: [TODO] ", "page_idx": 15}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 15}, {"type": "text", "text": "557 Guidelines:   \n558 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 15}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "559   \n560   \n561   \n562   \n563   \n564   \n565   \n566   \n567   \n568   \n569   \n570   \n571   \n572   \n573   \n574   \n575   \n576   \n577   \n578   \n579   \n580   \n581   \n582   \n583   \n584   \n585   \n586   \n587   \n588   \n589   \n590   \n591   \n592   \n593   \n594   \n595   \n596   \n597   \n598   \n599   \n600   \n601   \n602   \n603   \n604   \n605   \n606 ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [TODO] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: [TODO] ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "613 \u2022 Providing as much information as possible in supplemental material (appended to the   \n614 paper) is recommended, but including URLs to data and code is permitted.   \n615 6. Experimental Setting/Details   \n616 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n617 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n618 results?   \n619 Answer: [TODO]   \n620 Justification: [TODO]   \n621 Guidelines:   \n622 \u2022 The answer NA means that the paper does not include experiments.   \n623 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n624 that is necessary to appreciate the results and make sense of them.   \n625 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n626 material.   \n627 7. Experiment Statistical Significance   \n628 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n629 information about the statistical significance of the experiments?   \n630 Answer: [TODO]   \n631 Justification: [TODO]   \n632 Guidelines:   \n633 \u2022 The answer NA means that the paper does not include experiments.   \n634 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n635 dence intervals, or statistical significance tests, at least for the experiments that support   \n636 the main claims of the paper.   \n637 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n638 example, train/test split, initialization, random drawing of some parameter, or overall   \n639 run with given experimental conditions).   \n640 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n641 call to a library function, bootstrap, etc.)   \n642 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n643 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n644 of the mean.   \n645 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n646 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n647 of Normality of errors is not verified.   \n648 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n649 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n650 error rates).   \n651 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n652 they were calculated and reference the corresponding figures or tables in the text.   \n653 8. Experiments Compute Resources   \n654 Question: For each experiment, does the paper provide sufficient information on the com  \n655 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n656 the experiments?   \n657 Answer: [TODO]   \n658 Justification: [TODO]   \n659 Guidelines:   \n660 \u2022 The answer NA means that the paper does not include experiments.   \n661 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n662 or cloud provider, including relevant memory and storage. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "669 Question: Does the research conducted in the paper conform, in every respect, with the   \n670 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [TODO] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "72 Justification: [TODO] ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "679 10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "80 Question: Does the paper discuss both potential positive societal impacts and negative   \n81 societal impacts of the work performed? ", "page_idx": 18}, {"type": "text", "text": "Answer: [TODO] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "683 Justification: [TODO] ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "707 11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "08 Question: Does the paper describe safeguards that have been put in place for responsible   \n09 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n10 image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [TODO] ", "page_idx": 18}, {"type": "text", "text": "712 Justification: [TODO] ", "page_idx": 18}, {"type": "text", "text": "713 Guidelines:   \n714 \u2022 The answer NA means that the paper poses no such risks.   \n715 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n716 necessary safeguards to allow for controlled use of the model, for example by requiring   \n717 that users adhere to usage guidelines or restrictions to access the model or implementing   \n718 safety filters.   \n719 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n720 should describe how they avoided releasing unsafe images.   \n721 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n722 not require this, but we encourage authors to take this into account and make a best   \n723 faith effort. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "724 12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "725 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n726 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n727 properly respected? ", "page_idx": 19}, {"type": "text", "text": "728 Answer: [TODO] ", "page_idx": 19}, {"type": "text", "text": "729 Justification: [TODO]   \n730 Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "746 13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "47 Question: Are new assets introduced in the paper well documented and is the documentation   \n48 provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [TODO] ", "page_idx": 19}, {"type": "text", "text": "750 Justification: [TODO] ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "760 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "761 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n762 include the full text of instructions given to participants and screenshots, if applicable, as   \n763 well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [TODO] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "765 Justification: [TODO] ", "page_idx": 19}, {"type": "text", "text": "766   \n767   \n768   \n769   \n770   \n771   \n772   \n773   \n774   \n775   \n776   \n777   \n778   \n779   \n780   \n781   \n782   \n783   \n784   \n785   \n786   \n787   \n788   \n789   \n790   \n791   \n792   \n793 ", "page_idx": 20}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [TODO] Justification: [TODO] Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]