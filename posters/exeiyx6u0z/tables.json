[{"figure_path": "ExeIyx6U0Z/tables/tables_6_1.jpg", "caption": "Table 1: NeRF brief captioning on ShapeNeRF-Text. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)", "description": "This table presents the results of the NeRF brief captioning task using several frozen baselines on the ShapeNeRF-Text dataset.  The baselines used various modalities, including front-view images, back-view images, multi-view images, point clouds, and meshes.  The table shows the performance of each model using S-BERT, SimCSE, BLEU-1, ROUGE-L, and METEOR metrics.  The best result for each metric is highlighted in bold, and the second-best is underlined.", "section": "4.2 Language tasks and metrics"}, {"figure_path": "ExeIyx6U0Z/tables/tables_6_2.jpg", "caption": "Table 3: NeRF detailed captioning on ShapeNeRF-Text. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)", "description": "This table presents the results of a detailed captioning task on the ShapeNeRF-Text dataset using various frozen baselines. It compares different modalities (front-view images, back-view images, multi-view images, point clouds, and NeRFs) for their ability to generate detailed captions of the objects represented in the NeRFs. The performance is evaluated using four metrics: Sentence-BERT similarity, SimCSE similarity, BLEU-1, ROUGE-L, and METEOR. The best performing model for each metric is highlighted in bold, and the second-best performing model is underlined. The table shows that processing the NeRF weights directly (LLaNA) significantly outperforms other methods.", "section": "Experiment results"}, {"figure_path": "ExeIyx6U0Z/tables/tables_7_1.jpg", "caption": "Table 4: NeRF single-round Q&A on ShapeNeRF-Text. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)", "description": "This table presents the results of single-round question answering task on the ShapeNeRF-Text dataset using various baselines with their modalities, including front-view, back-view, and multi-view images and point clouds. The performance is evaluated using S-BERT, SimCSE, BLEU-1, ROUGE-L, and METEOR metrics. LLaNA-7b using NeRF significantly outperforms other baselines across all metrics.", "section": "5.3 NeRF single-round Q&A"}, {"figure_path": "ExeIyx6U0Z/tables/tables_8_1.jpg", "caption": "Table 6: NeRF brief captioning on ShapeNeRF-Text. Trained baselines. Best results are in bold, runner-up is underlined. (FV: front-view)", "description": "This table presents the performance of various models on the brief captioning task using the ShapeNeRF-Text dataset.  The models have been fine-tuned on this dataset. The table shows the results achieved by different models using various modalities such as images, point clouds and directly processing NeRFs (LLaNA). The metrics used to evaluate the performance are S-BERT, SimCSE, BLEU-1, ROUGE-L, and METEOR.  The best performance for each metric is highlighted in bold, and the second-best performance is underlined.  The results are separated by modality (front-view images) to illustrate how the performance varies depending on the input representation used.", "section": "4.2 Language tasks and metrics"}, {"figure_path": "ExeIyx6U0Z/tables/tables_8_2.jpg", "caption": "Table 2: NeRF brief captioning on the HST dataset. Frozen baselines.\nBest results are in bold, runner-up is underlined.\n(FV: front-view, BV: back-view, MV: multi-view)", "description": "This table presents the results of the NeRF brief captioning task on the HST dataset using several frozen baselines.  The table compares different models and modalities (front-view images, back-view images, multi-view images, point clouds, and NeRFs) based on several metrics. The best results for each metric are highlighted in bold, while runner-up results are underlined.", "section": "4.2 Language tasks and metrics"}, {"figure_path": "ExeIyx6U0Z/tables/tables_9_1.jpg", "caption": "Table 8: NeRF detailed captioning on ShapeNeRF-Text. Trained baselines. Best results are in bold, runner-up is underlined. (FV: front-view)", "description": "This table presents the results of the detailed captioning task on the ShapeNeRF-Text dataset using trained baselines.  The best performing model in terms of each metric (S-BERT, SimCSE, BLEU-1, ROUGE-L, METEOR) is shown in bold, and the second-best is underlined. The results are broken down by model and modality, with a focus on front-view (FV) images. This allows for comparison of different approaches to language modeling on NeRF data, particularly assessing the improvements gained through training on the specific dataset.", "section": "5.5 Training baselines on ShapeNeRF-Text"}, {"figure_path": "ExeIyx6U0Z/tables/tables_9_2.jpg", "caption": "Table 9: NeRF single-round Q&A on ShapeNeRF-Text. Trained baselines. Best results are in bold, runner-up is underlined. (FV: front-view)", "description": "This table presents the results of single-round question answering tasks on the ShapeNeRF-Text dataset using trained baseline models.  The models' performance is evaluated using various metrics, including S-BERT, SimCSE, BLEU-1, ROUGE-L, and METEOR. The best performing model for each metric is shown in bold, and the second-best is underlined. The results are broken down by modality (image (front view), point cloud, and NeRF).", "section": "5.5 Training baselines on ShapeNeRF-Text"}, {"figure_path": "ExeIyx6U0Z/tables/tables_23_1.jpg", "caption": "Table 4: NeRF single-round Q&A on ShapeNeRF-Text. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)", "description": "This table presents the results of a single-round question answering task on the ShapeNeRF-Text dataset, comparing the performance of various models (LLaVA, BLIP-2, PointLLM, GPT4Point, and LLaNA) using different input modalities (front-view images, back-view images, multi-view images, point clouds, and NeRFs). The metrics used are Sentence-BERT, SimCSE, BLEU-1, ROUGE-L, and METEOR.  The best performing model for each metric is highlighted in bold, indicating LLaNA's superior performance in NeRF understanding.", "section": "5.3 NeRF single-round Q&A"}, {"figure_path": "ExeIyx6U0Z/tables/tables_23_2.jpg", "caption": "Table 13: Zero-shot NeRF classification on ShapeNeRF-Text. Trained baselines. Best results are in bold, runner-up is underlined. (FV: front-view)", "description": "This table presents the results of a zero-shot NeRF classification task performed by several baselines which were trained on ShapeNeRF-Text dataset. The models were tested using the same evaluation protocol described in the paper, except that they were trained on ShapeNeRF-Text dataset instead of only being tested.", "section": "5.5 Training baselines on ShapeNeRF-Text"}, {"figure_path": "ExeIyx6U0Z/tables/tables_23_3.jpg", "caption": "Table 14: NeRF brief captioning on ShapeNeRF-Text. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)", "description": "This table presents the results of the NeRF brief captioning task using different frozen baselines.  It compares the performance of various models, including those using images (front, back, and multi-view) and point clouds, against the proposed LLaNA model which operates directly on the NeRF.  The metrics used for evaluation are S-BERT similarity, SimCSE, BLEU-1, ROUGE-L, and METEOR.", "section": "4.2 Language tasks and metrics"}, {"figure_path": "ExeIyx6U0Z/tables/tables_24_1.jpg", "caption": "Table 1: NeRF brief captioning on ShapeNeRF-Text. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)", "description": "This table presents the results of different baselines for the NeRF brief captioning task on the ShapeNeRF-Text dataset. The baselines are compared using various metrics including Sentence-BERT, SimCSE, BLEU-1, ROUGE-L, and METEOR. The results are categorized by the modality used (front-view image, back-view image, multi-view image, point cloud, and NeRF) and the model used. LLaNA-7b shows superior performance using NeRF modality compared to image-based and point cloud-based baselines.", "section": "4.2 Language tasks and metrics"}, {"figure_path": "ExeIyx6U0Z/tables/tables_24_2.jpg", "caption": "Table 4: NeRF single-round Q&A on ShapeNeRF-Text. Frozen baselines. Best results are in bold, runner-up is underlined. (FV: front-view, BV: back-view, MV: multi-view)", "description": "This table presents the results of single-round question answering tasks using different input modalities on the ShapeNeRF-Text dataset.  It compares the performance of various models (LLaVA-vicuna-13b, LLaVA-vicuna-7b, BLIP-2 FlanT5-xxl, PointLLM-7b, GPT4Point-Opt-2.7b, 3D-LLM, and LLaNA-7b) using different input types (front-view images, back-view images, multi-view images, point clouds, and NeRFs).  The evaluation metrics used are Sentence-BERT similarity, SimCSE similarity, BLEU-1, ROUGE-L, and METEOR.  The table showcases LLaNA's superior performance across all metrics when compared to baselines which use images, point clouds, or meshes derived from the NeRF.  It highlights the advantage of processing NeRF weights directly instead of relying on intermediate 2D or 3D representations.", "section": "5.3 NeRF single-round Q&A"}, {"figure_path": "ExeIyx6U0Z/tables/tables_24_3.jpg", "caption": "Table 17: Generalization results on Objaverse. NeRF captioning", "description": "This table presents the results of an experiment conducted to evaluate the generalization capabilities of the proposed model, LLaNA, and several baseline models.  The models were tested on the Objaverse dataset, which contains objects not seen during the training phase. The table shows the performance of each model in terms of various metrics: Sentence-BERT, SimCSE, BLEU-1, ROUGE-L, and METEOR. These metrics are used to assess the quality of captions generated by the models. The modalities used by the models are also indicated: Image (front view), Point cloud, and NeRF.  This experiment assesses how well the models trained on one dataset can generalize to a new and different dataset.", "section": "5.5 Generalization experiments"}]