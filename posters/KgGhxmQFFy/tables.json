[{"figure_path": "KgGhxmQFFy/tables/tables_6_1.jpg", "caption": "Table 1: ROC-AUC (%) of molecular property prediction task (classification) on the MoleculeNet datasets. Bold indicates the best performance and underline indicates the second best performance.", "description": "This table presents the results of a molecular property prediction task (classification) using eight datasets from the MoleculeNet benchmark.  The task involves predicting binary properties (yes/no). The table compares UniMoT's performance to several other models (KV-PLM, AttrMask, InfoGraph, MolCLR, GraphMVP, MoleculeSTM, and InstructMol) by showing each model's ROC-AUC score for each of the eight datasets.  Higher ROC-AUC values indicate better performance.", "section": "4.1 Molecule Comprehension Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_6_2.jpg", "caption": "Table 2: Performance (%) of molecule captioning task on the PubChem dataset. Bold indicates the best performance and underline indicates the second best performance.", "description": "This table presents the performance of various models on the molecule captioning task using the PubChem dataset. The performance is measured using six metrics: BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR.  The best performing model for each metric is shown in bold, and the second-best is underlined.  This allows for a comparison of different models' ability to generate accurate and comprehensive textual descriptions of molecules.", "section": "4.1 Molecule Comprehension Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_7_1.jpg", "caption": "Table 3: Performance (%) of molecule-text retrieval task on the PubChem dataset. Bold indicates the best performance and underline indicates the second best performance. ", "description": "This table presents the results of a molecule-text retrieval task, specifically focusing on the accuracy and Recall@20 metrics. The task involves retrieving relevant text given a molecule or vice versa.  The table compares several models, including UniMoT, by showing their performance on the PubChem dataset.  Bold values indicate the top performance, and underlined values indicate the second-best performance for each metric.", "section": "4.1 Molecule Comprehension Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_7_2.jpg", "caption": "Table 4: Performance of molecule generation tasks on the Mol-Instructions dataset, including caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. Bold indicates the best performance, and underline indicates the second best performance.", "description": "This table presents the performance comparison of UniMoT against several baselines across four molecule generation tasks.  The tasks are caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis.  The performance is evaluated using several metrics including Exact Match, BLEU, Levenshtein distance, and various fingerprint similarity scores (RDKit, MACCS, Morgan).  The Validity metric indicates the percentage of chemically valid molecules generated.  Bold values indicate the best performance for each task and metric, while underlined values show the second-best performance.", "section": "4.2 Molecule Generation Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_8_1.jpg", "caption": "Table 5: Ablation study on the projector and representation form for the molecule captioning task using the PubChem dataset.", "description": "This table presents the results of an ablation study conducted to analyze the impact of different projector and representation choices on the molecule captioning task.  It compares the performance using four different configurations: a projection layer with molecule embeddings as input, a Q-Former with query embeddings, a causal Q-Former with causal embeddings, and a causal Q-Former with causal tokens. The performance is evaluated based on several metrics: BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR, which assess the quality of generated captions.", "section": "4.3 Ablation Studies"}, {"figure_path": "KgGhxmQFFy/tables/tables_8_2.jpg", "caption": "Table 6: Ablation study on the model size and tuning strategy for the molecule captioning task using the PubChem dataset.", "description": "This table presents the results of an ablation study investigating the impact of model size (LLaMA2-7B and LLaMA2-13B) and tuning strategies (LoRA tuning and fully tuning) on the performance of the molecule captioning task.  The performance is measured using several metrics, including BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR. The study aims to determine the optimal combination of model size and tuning method for achieving the best molecule captioning results on the PubChem dataset.", "section": "4.3 Ablation Studies"}, {"figure_path": "KgGhxmQFFy/tables/tables_14_1.jpg", "caption": "Table 7: Summary of datasets, their types, tasks, descriptions, URLs, and licenses used for evaluating UniMoT.", "description": "This table provides a comprehensive overview of the datasets used to evaluate the UniMoT model's performance across various tasks.  For each dataset, it lists the type of data (e.g., classification, regression), the specific tasks it is used for (e.g., molecular property prediction, captioning, retrieval), a concise description of the dataset's content, the URL for accessing the dataset, and the license under which it is distributed.  This information is crucial for understanding the scope and reproducibility of the UniMoT experiments.", "section": "4 Experiments"}, {"figure_path": "KgGhxmQFFy/tables/tables_16_1.jpg", "caption": "Table 1: ROC-AUC (%) of molecular property prediction task (classification) on the MoleculeNet datasets. Bold indicates the best performance and underline indicates the second best performance.", "description": "This table presents the results of a molecular property prediction task (classification) using eight datasets from the MoleculeNet benchmark.  The Area Under the ROC Curve (AUC) is reported as a percentage, showing the performance of various models. The best and second-best performing models for each dataset are highlighted in bold and underlined, respectively.", "section": "4.1 Molecule Comprehension Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_17_1.jpg", "caption": "Table 9: The detailed training hyperparameters of UniMoT.", "description": "This table provides a detailed breakdown of the hyperparameters used during the four stages of UniMoT's training process.  It covers aspects like the specific models used as encoders and decoders in each stage (MoleculeSTM, Chemformer), the optimizer (AdamW), the codebook size, number of queries, embedding dimensions, batch sizes, learning rate schedules, warm-up steps, weight decay, and the precision (bfloat16) used.  For the final LLM pretraining stage, it also specifies the use of LoRA tuning with particular hyperparameter settings.  The table also provides the GPU usage and overall training time in GPU hours for each stage.", "section": "3.3 Training Strategy"}, {"figure_path": "KgGhxmQFFy/tables/tables_17_2.jpg", "caption": "Table 10: Mean Absolute Error (MAE) of molecular property prediction task (regression) on the QM9 dataset. Bold indicates the best performance and underline indicates the second best performance. \u0394\u03b5 is the HOMO-LUMO energy gap.", "description": "This table presents the Mean Absolute Error (MAE) results for the molecular property prediction task (regression) using the QM9 dataset.  The dataset involves predicting key quantum mechanical properties of molecules.  The table compares UniMoT's performance against several baseline models, including Alpaca, Baize, LLaMA2-7B, Vicuna-13B, Mol-Instructions, and InstructMol. The results are broken down by HOMO, LUMO, \u0394\u03b5 (HOMO-LUMO gap), and an average across all three properties.  Bold values indicate the best performance, while underlined values indicate the second-best performance for each property.", "section": "4.1 Molecule Comprehension Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_18_1.jpg", "caption": "Table 11: Performance (%) of molecule captioning task on the CheBI-20 dataset. Bold indicates the best performance and underline indicates the second best performance.", "description": "This table presents the performance comparison of various models on the molecule captioning task using the CheBI-20 dataset.  The metrics used for evaluation are BLEU-2, BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR.  The best performing model for each metric is highlighted in bold, and the second-best is underlined. This table allows for a direct comparison of UniMoT against several well-established baselines and showcases its performance improvement in molecule caption generation.", "section": "4.1 Molecule Comprehension Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_19_1.jpg", "caption": "Table 1: ROC-AUC (%) of molecular property prediction task (classification) on the MoleculeNet datasets. Bold indicates the best performance and underline indicates the second best performance.", "description": "This table presents the results of a molecular property prediction task (classification) using eight datasets from the MoleculeNet benchmark.  The performance metric used is the ROC-AUC score (%).  The table compares the performance of UniMoT against several baseline models.  Bold values indicate the best performance achieved on each dataset, while underlined values indicate the second-best performance. This allows for a direct comparison of UniMoT's effectiveness relative to existing approaches across a range of molecular properties.", "section": "4.1 Molecule Comprehension Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_20_1.jpg", "caption": "Table 13: Accuracy (%) of molecule-text retrieval task on the PCdes and MoMu datasets. Bold indicates the best performance and underline indicates the second best performance. We report the performance of retrieval using a batch of 64 random samples and the entire test set.", "description": "This table presents the accuracy of molecule-to-text (M2T) and text-to-molecule (T2M) retrieval tasks on two datasets: PCdes and MoMu.  The results are shown for both a batch of 64 samples and the entire test set.  UniMoT's performance is compared against several other models to demonstrate its effectiveness.", "section": "4.1 Molecule Comprehension Tasks"}, {"figure_path": "KgGhxmQFFy/tables/tables_20_2.jpg", "caption": "Table 3: Performance (%) of molecule-text retrieval task on the PubChem dataset. Bold indicates the best performance and underline indicates the second best performance. We report the performance of retrieval using a batch of 64 random samples and the entire test set.", "description": "This table presents the results of a molecule-text retrieval task on the PubChem dataset.  Two metrics, Accuracy and Recall@20, are used to evaluate the performance of several models, including UniMoT.  Results are shown for both retrieval using a batch of 64 samples and for retrieval on the entire test set.  The best-performing model for each metric in each scenario is shown in boldface.", "section": "4.1 Molecule Comprehension Tasks"}]