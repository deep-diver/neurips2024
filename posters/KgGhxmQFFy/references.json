{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models (LLMs), introducing the concept of few-shot learning and significantly impacting the development of UniMoT."}, {"fullname_first_author": "Aaron Van Den Oord", "paper_title": "Neural discrete representation learning", "publication_date": "2017-12-01", "reason": "This paper introduces vector quantization (VQ), a crucial technique used in UniMoT's molecule tokenizer for transforming molecules into discrete tokens."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-07-01", "reason": "The Q-Former architecture in UniMoT is directly inspired by the BLIP-2 model, which this paper introduces, demonstrating a successful multi-modal approach."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-07-01", "reason": "UniMoT adopts the unified text-to-text transformer paradigm, a key concept introduced in this paper, allowing for a unified treatment of molecules and text using a shared token representation."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "publication_date": "2013-08-01", "reason": "This paper introduces the straight-through estimator, a crucial technique used in UniMoT's vector quantization to handle the non-differentiable nature of the process."}]}