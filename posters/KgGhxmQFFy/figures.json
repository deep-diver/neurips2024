[{"figure_path": "KgGhxmQFFy/figures/figures_1_1.jpg", "caption": "Figure 1: Comparisons among different molecular LLMs. 1a and 1b are adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. 1c is our proposed tokenizer-based architecture, where molecules are presented in the same discrete token representation as that of text. Molecules and text can be optimized under a unified next-token-prediction objective.", "description": "This figure compares three different approaches for creating molecular large language models (LLMs). The first two approaches (a and b) use adapter-based architectures, which add a linear projection layer or a Q-Former to translate molecule features into the semantic space of an LLM.  These methods treat molecule and text modalities unequally and lack a supervision signal for the molecule modality. The third approach (c) uses a tokenizer-based architecture. This method expands the vocabulary of the LLM with molecule tokens, allowing molecules and text to be treated equally and optimized under a unified next-token prediction objective.", "section": "1 Introduction"}, {"figure_path": "KgGhxmQFFy/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of our proposed molecule tokenizer. The tokenizer generates discrete molecule tokens, which can be fed into LLMs for downstream tasks. The generated molecule tokens can be decoded into molecules using the adapter and the SMILES decoder during inference.", "description": "This figure illustrates the process of transforming molecules into discrete tokens that can be used by large language models.  The input molecule is first processed by a molecule encoder, then fed into a causal Q-former, which generates query embeddings.  These embeddings are quantized into discrete molecule tokens using a vector quantization method with a learnable codebook. The molecule tokens are then used as input for downstream tasks, such as generating text descriptions of the molecule. During inference, these tokens can be decoded back into molecules using a SMILES decoder and an adapter that aligns the discrete latent space of molecule tokens with the continuous latent space of the generative model.", "section": "3.1 Molecule Tokenizer for LLMs"}, {"figure_path": "KgGhxmQFFy/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of the multi-modal autoregressive pretraining on molecule-text datasets. Uni-MoT excels in multi-modal comprehension and generation tasks, enabled by the unified LM objective. T represents the size of the text vocabulary.", "description": "This figure illustrates the unified molecule-text language model (UniMoT) architecture and its multi-modal autoregressive pretraining process.  The left panel (a) shows molecule-to-text autoregression where a molecule represented by SMILES and molecule tokens is input to UniMoT along with a prompt to generate a text description of the molecule. The right panel (b) shows text-to-molecule autoregression where a prompt and caption describing a molecule are fed into UniMoT to generate the molecule's SMILES and molecule tokens.  UniMoT uses a shared vocabulary (text and molecule tokens) and autoregressive training to perform both tasks effectively. This unified approach is key to UniMoT's success in molecule comprehension and generation.", "section": "3.2 Unified Molecule-Text Language Model"}, {"figure_path": "KgGhxmQFFy/figures/figures_12_1.jpg", "caption": "Figure 4: Illustration of our proposed Causal Q-Former. The Causal Q-Former provides causal query embeddings for subsequent blocks.", "description": "This figure illustrates the architecture of the Causal Q-Former, a key component of UniMoT.  The Causal Q-Former takes molecule features (from a frozen molecule encoder) as input. It uses learnable query vectors to interact with these features through self-attention and cross-attention mechanisms.  The design incorporates causal masking, ensuring that queries only attend to preceding queries, maintaining a causal dependency suitable for language models. The output is a sequence of causal query embeddings, crucial for bridging the gap between molecule and text modalities.  The figure highlights the different components like the molecule encoder, cross-attention, self-attention (with causal masks), feed-forward layers, and the learnable queries. Three training objectives are visually represented: Molecule-Text Contrastive Learning (MTC), Molecule-Text Matching (MTM), and Molecule-Grounded Text Generation (MTG).", "section": "3.1 Molecule Tokenizer for LLMs"}]