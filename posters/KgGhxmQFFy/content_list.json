[{"type": "text", "text": "UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 The remarkable success of Large Language Models (LLMs) across diverse tasks   \n2 has driven the research community to extend their capabilities to molecular appli  \n3 cations, leading to the development of molecular LLMs. However, most molecular   \n4 LLMs employ adapter-based architectures that do not treat molecule and text   \n5 modalities equally and lack a supervision signal for the molecule modality. To   \n6 address these issues, we introduce UniMoT, a unified molecule-text LLM adopting   \n7 a tokenizer-based architecture that expands the vocabulary of LLM with molecule   \n8 tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that   \n9 incorporates a Q-Former to bridge the modality gap between molecule and text.   \n0 This tokenizer transforms molecules into sequences of molecule tokens with causal   \n11 dependency, encapsulating high-level molecular and textual information. Equipped   \n12 with this tokenizer, UniMoT can unify molecule and text modalities under a shared   \n13 token representation and an autoregressive training paradigm, enabling it to in  \n14 terpret molecules as a foreign language and generate them as text. Following a   \n15 four-stage training scheme, UniMoT emerges as a multi-modal generalist capable   \n16 of performing both molecule-to-text and text-to-molecule tasks. Extensive exper  \n17 iments demonstrate that UniMoT achieves state-of-the-art performance across a   \n18 wide range of molecule comprehension and generation tasks. ", "page_idx": 0}, {"type": "text", "text": "19 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "20 The incredible capabilities of Large Language Models (LLMs) [5, 44] have led to their widespread   \n21 use as versatile tools for completing diverse real-world tasks. This success has sparked interest in   \n22 Multi-modal LLMs [59, 52], which aim to enhance LLMs by enabling them to process multi-modal   \n23 inputs and outputs. Prior research efforts [26, 41, 12, 6, 33, 35, 25] have focused on adapting LLMs   \n24 to molecular tasks, resulting in the development of molecular LLMs. These molecular LLMs can   \n25 analyze molecule structures [35, 33, 6], address drug-related inquiries [26, 41], assist in synthesis   \n26 and retrosynthesis planning [12], support drug design [12], and more.   \n27 Prevalent molecular LLMs commonly employ adapter-based architectures, adopting either a linear   \n28 projection [26, 41, 6] or a Q-Former [33, 25] as an adapter to translate molecule features into the   \n29 semantic space of LLM, as illustrated in Figure 1a and Figure 1b. Despite demonstrating initial   \n30 capabilities in molecular comprehension and yielding promising results in molecule-to-text generation   \n31 tasks, they still lack molecule generation abilities. The critical issue within these methods is their   \n32 unequal treatment of molecules and text, resulting in a lack of supervision for the molecule modality.   \n33 This limitation significantly constrains model capacity and effectiveness. Due to limitations imposed   \n34 by the training paradigm, they are unable to perform text-to-molecule generation tasks.   \n35 Discretizing continuous molecule features into discrete molecule tokens offers a promising solution   \n36 for conducting both molecule-to-text and text-to-molecule generation tasks. By treating tokens from ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "KgGhxmQFFy/tmp/0dc2a7115312fb038c2c49df900184152d66433717570f176d7fec73dc571fdd.jpg", "img_caption": ["(a) Projection-Based Architecture. (b) Q-Former-Based Architecture. (c) Tokenizer-Based Architecture. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparisons among different molecular LLMs. 1a and 1b are adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. 1c is our proposed tokenizer-based architecture, where molecules are presented in the same discrete token representation as that of text. Molecules and text can be optimized under a unified next-token-prediction objective. ", "page_idx": 1}, {"type": "text", "text": "37 different modalities equally, we can predict the next molecule or text token in an autoregressive   \n38 manner. However, directly discretizing molecule features poses several challenges: (i) This approach   \n39 results in long sequences, with lengths equivalent to the number of atoms in a batch. LLMs typically   \n40 experience a quadratic increase in computational complexity with sequence length [46]. (ii) Molecule   \n41 tokens derived from molecule features lack left-to-right causal dependency, which conflicts with   \n42 the unidirectional attention mechanism in LLMs. (iii) Molecule features lack textual information,   \n43 hindering effective molecule-text interactions and alignment.   \n44 To this end, we present UniMoT, a unified molecule-text LLM that adopts a tokenizer-based architec  \n45 ture, integrating molecule comprehension and generation, as depicted in Figure 1c. A pivotal aspect   \n46 of UniMoT\u2019s architecture is the molecule tokenizer for transforming molecules into molecule tokens.   \n47 We introduce a Vector Quantization-driven [45] tokenizer, incorporating a Q-Former [23] to bridge   \n48 the modality gap between molecule and text. Specifically, we incorporate causal masks for the queries,   \n49 enabling the Causal Q-Former to generate a causal sequence of query embeddings compatible with   \n50 the unidirectional attention in LLMs. The sequence of query embeddings is subsequently quantized   \n51 into a sequence of molecule tokens using a learnable codebook. The molecule tokens encapsulate   \n52 high-level molecular and textual information, which are then aligned with the latent space of a   \n53 generative model via an MLP adapter, enabling the generation of desired molecules.   \n54 Pretrained LLMs can integrate the molecule tokenizer by treating molecule tokens as new words and   \n55 constructing a molecule vocabulary through mapping the learned codebook. We adopt the unified   \n56 discrete token representation for molecules and text, coupled with the unified next-token-prediction   \n57 training paradigm of LLM. This unification of representation and training paradigm enhances LLMs\u2019   \n58 ability to understand molecule-text interactions and alignment. UniMoT interprets molecules akin to   \n59 understanding a foreign language, and generates them as if they were text. Following a four-stage   \n60 training scheme, UniMoT serves as a multi-modal generalist capable of performing both molecule   \n61 comprehension and generation tasks. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "62 Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "63 \u2022 We introduce a molecule tokenizer specifically designed for LLMs, enabling the tokenization   \n64 of molecules into short sequences of molecule tokens with causal dependency. These tokens   \n65 encapsulate high-level molecular and textual information and can be decoded into desired   \n66 molecules during inference.   \n67 \u2022 We present UniMoT, a unified molecule-text LLM that adopts a tokenizer-based architecture   \n68 instead of traditional adapter-based architectures. UniMoT unifies the modalities of molecule   \n69 and text under a shared token representation and an autoregressive training paradigm.   \n70 \u2022 UniMoT exhibits remarkable capabilities in multi-modal comprehension and generation. Exten  \n71 sive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide   \n72 spectrum of molecule comprehension tasks and molecule generation tasks. ", "page_idx": 1}, {"type": "text", "text": "73 2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "74 Molecular Large Language Models. The recent emergence of Vision Large Language Models   \n75 (VLLMs) [24, 23, 28] has catalyzed advancements in Molecular LLMs, which encompass both   \n76 single modality and multi-modality approaches. In the single modality domain, researchers are   \n77 exploring diverse molecule representations, such as 1D sequences like SMILES strings [47, 8, 17],   \n78 2D molecule graphs [15, 56], 3D geometric conformations [56, 32], and textual information from   \n79 the literature [43, 2, 21]. In the multiple modalities domain, various innovative approaches are being   \n80 employed. MolT5 [11], a T5-based [38] model, is designed for SMILES-to-text and text-to-SMILES   \n81 translations. Other works, such as MoMu [39], MoleculeSTM [31], MolFM [34], and GIT-Mol [29],   \n82 leverage cross-modal contrastive learning to align the representation spaces of molecules and text.   \n83 Additionally, some studies use multi-modal learning architectures to develop molecular LLMs,   \n84 which often adopt adapter-based architectures. For instance, InstructMol [6], GraphGPT [41], and   \n85 DrugChat [26] employ a simple projection layer to map molecule features to LLM\u2019s input space.   \n86 MolCA [33] and 3D-MoLM [25] utilize a Q-Former [23] to bridge the modality gap between   \n87 molecules and text. However, these methods do not treat molecule and text modalities equally and   \n88 lack a supervision signal for the molecule modality, limiting model capacity and effectiveness.   \n89 Vector Quantization. Vector Quantization (VQ) [13] is a widely used technique in generative   \n90 models. VQ-VAE [45] converts an image into a set of discrete codes within a learnable discrete   \n91 latent space by learning to reconstruct the original image. VQ-GAN [57] enhances the generation   \n92 quality by leveraging adversarial and perceptual objectives. In the context of molecules, VQ has   \n93 been effectively applied to quantize molecule representations. For example, DGAE [4] introduces   \n94 a VQ model specifically for molecular graphs, where molecular graphs are encoded into discrete   \n95 latent codes. Mole-BERT [54] uses VQ to rethink the pre-training of GNNs for molecular tasks.   \n96 IMoLD [60] proposes using VQ to enhance invariant molecule representations, and VQSynergy [51]   \n97 demonstrates the use of VQ for drug discovery. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "98 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "99 Our objective is to leverage the reasoning and generation capabilities of LLMs to enhance the   \n100 comprehension and generation of molecule and text data. To achieve this, we focus on representing   \n101 these modalities uniformly within the token representation, utilizing the next-token-prediction training   \n102 paradigm of LLMs. As illustrated in Figure 2, we introduce a molecule tokenizer (Section 3.1)   \n103 designed to transform molecules into molecule tokens by learning to reconstruct the input molecule.   \n104 The molecule sequence can then be concatenated with the text sequence to form a multi-modal   \n105 sequence, which is subsequently fed into an LLM for autoregressive pretraining (Section 3.2), as   \n106 illustrated in Figure 3. The LLM vocabulary is expanded with molecule codes mapped from the   \n107 learned codebook. We introduce a four-stage training scheme for UniMoT (Section 3.3) comprising   \n108 Causal Q-Former pretraining, molecule tokenizer pretraining, unified molecule-text pretraining, and   \n109 task-specific instruction tuning. UniMoT is capable of performing both molecular comprehension   \n110 and generation tasks following the training scheme. ", "page_idx": 2}, {"type": "text", "text": "111 3.1 Molecule Tokenizer for LLMs ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "112 Molecule encoder. We represent the structural information of a molecule as a graph, denoted by   \n113 $\\mathcal{G}\\,=\\,(\\mathcal{V},\\mathcal{E})$ , where $\\nu$ is the set of atoms and $|\\gamma|\\,=\\,N$ is the number of atoms. The task of the   \n114 molecule encoder is to extract node representations that are context-aware and encompass diverse   \n115 local neighborhood structural information. By employing a molecule encoder, we obtain molecule   \n116 features $\\mathbf{\\breve{X}}\\in\\mathbb{R}^{N\\times F}$ , where each atom representation contains context-aware structural information.   \n111178 buesdald iQng-sF $\\mathbf{Z}=\\{z_{i}\\}_{i=1}^{M}\\in\\mathring{\\mathbb{R}}^{M\\times d}$ cQo-nF rinmienrg  mhiogdhe-ll eivnetrl omdoulceecdu lbayr  aBnLdI tPe-x2t u[a2l3 i] ntfoo rgmenateiroant,e  wquheerrey   \n$M$ represents the number of queries and $d$ denotes the dimension of query embeddings. Specifically,   \n120 we incorporate causal masks into the queries, ensuring that they only interact with preceding queries.   \n121 This ensures the sequence of query embeddings maintains a causal dependency, aligning with the   \n122 requirements of LLMs operating on text sequence. Details regarding the Causal Q-Former can be   \n123 found in Appendix A.   \n124 Vector Quantization. The Causal Q-Former converts molecule and text features into a causal   \n125 sequence of query embeddings. Subsequently, we aim to quantize these query embeddings into   \n126 molecule tokens using a variant of VQ-VAE [45]. These discrete molecule tokens can then be   \n127 integrated with text tokens to form a multi-modal sequence suitable for feeding into LLMs. The   \n128 causal sequence of query embeddings $\\{z_{i}\\}_{i=1}^{M}$ is quantized into a causal sequence of molecule   \n129 tokens {si}iM=1 by identifying the closest neighbor in a learnable codebook $\\mathcal{C}=\\{\\pmb{c}_{i}\\}_{i=1}^{K}$ , where $K$   \n130 represents the size of the codebook. The codebook is randomly initialized and optimized during   \n131 pretraining. Specifically, token $s_{i}$ is determined as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "KgGhxmQFFy/tmp/eac5a344c25dca72fdb912e5a386d7b5ffb0cddb67355f9a1fe7971baff5d2ac.jpg", "img_caption": ["Figure 2: Illustration of our proposed molecule tokenizer. The tokenizer generates discrete molecule tokens, which can be fed into LLMs for downstream tasks. The generated molecule tokens can be decoded into molecules using the adapter and the SMILES decoder during inference. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{i}=\\operatorname*{argmin}_{j\\in\\{1,\\cdots,K\\}}\\left\\|z_{i}-c_{j}\\right\\|_{2},\\quad\\mathrm{for}\\quad i=1,2,\\cdots,M.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "132 Intuitively, the query embedding $\\boldsymbol{z}_{i}$ is quantized to the closest neighbor $c_{s_{i}}$ in the codebook. As the   \n133 vector quantization process is non-differentiable, we adopt the straight-through estimator [3] to train   \n134 the Causal Q-Former by copying the gradient from the molecule tokens to the query embeddings,   \n135 as shown in Figure 2. The resulting embeddings of molecule tokens, denoted as $\\mathbf{\\dot{C}}=\\{\\pmb{c}_{s_{i}}\\}_{i=1}^{M}$ , are   \n136 subsequently utilized for reconstructing molecules.   \n137 Molecule Reconstruction. An adapter needs to be trained to align the discrete latent space of   \n138 molecule tokens with the continuous latent space of a molecular generative model for molecule   \n139 reconstruction. The embeddings of molecule tokens $\\mathbf{C}$ can be aligned with the latent space of   \n140 the generative model via an MLP adapter $\\psi$ , represented as $\\mathbf{X}_{R}=\\bar{\\psi}(\\mathbf{C})$ , where $\\mathbf{X}_{R}$ denotes the   \n141 embeddings for reconstruction. Subsequently, we can reconstruct the molecule from $\\mathbf{X}_{R}$ using the   \n142 pretrained SMILES decoder To achieve alignment, we minimize the Mean Squared Error (MSE) loss   \n143 between $\\mathbf{X}_{R}$ and the SMILES [50] embeddings $\\mathbf{X}_{S}$ produced by the pretrained SMILES encoder.   \n144 The training loss of the tokenizer is expressed as follows: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Tokenizer}}=\\left\\|\\mathbf{X}_{R}-\\mathbf{X}_{S}\\right\\|_{2}^{2}+\\frac{1}{M}\\sum_{i=1}^{M}\\left\\|\\mathbf{s}\\mathbf{g}\\left[\\boldsymbol{z}_{i}\\right]-\\boldsymbol{c}_{s_{i}}\\right\\|_{2}^{2}+\\frac{\\beta}{M}\\sum_{i=1}^{M}\\left\\|\\mathbf{s}\\mathbf{g}\\left[\\boldsymbol{c}_{s_{i}}\\right]-\\boldsymbol{z}_{i}\\right\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "145 Here, the first term represents the alignment loss, the second term is a codebook loss aimed at   \n146 updating the codebook embeddings, and the third term is a commitment loss that encourages the   \n147 query embedding to stay close to the chosen codebook embedding. sg[\u00b7] denotes the stop-gradient   \n148 operator, and the hyperparameter $\\beta$ is set to 0.25. ", "page_idx": 3}, {"type": "image", "img_path": "KgGhxmQFFy/tmp/1c0d232df6cc103bf37f38424ebfcda9b9119830aa10f64bae77ea774ddbc159.jpg", "img_caption": ["Figure 3: Illustration of the multi-modal autoregressive pretraining on molecule-text datasets. UniMoT excels in multi-modal comprehension and generation tasks, enabled by the unified LM objective. $T$ represents the size of the text vocabulary. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "149 3.2 Unified Molecule-Text Language Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "150 Expanding Vocabulary. Employing the molecule tokenizer, a molecule can be tokenized into a   \n151 molecule sequence $\\{s_{i}\\}_{i=1}^{M}$ with causal dependency. The molecule sequence can be concatenated with   \n152 the text sequence to form a multi-modal sequence $\\{u_{i}\\}_{i=1}^{L}$ , where $L$ is the length of the multi-modal   \n153 sequence. To facilitate the representation of the multi-modal sequence, we construct the molecule   \n154 vocabulary $\\nu^{m}\\,=\\,\\{\\pmb{v}_{i}^{m}\\}_{i=1}^{K}$ , which maintains the order of the molecule codebook $\\mathcal{C}\\,=\\,\\{\\pmb{c}_{i}\\}_{i=1}^{K}$   \n155 Additionally, $\\mathcal{V}^{m}$ includes several special tokens such as boundary indicators, e.g., [MOL] and   \n156 [/MOL], to mark the beginning and end of the molecule sequence. Next, we merge the original text   \n157 vocabulary $\\boldsymbol{\\mathcal{V}}_{.}^{t}=\\{\\boldsymbol{v}_{i}^{t}\\}_{i=1}^{T}$ with the molecule vocabulary $\\mathcal{V}^{m}$ . The unified molecule-text vocabulary   \n158 $\\mathcal{V}=\\{\\mathcal{V}^{m},\\mathcal{V}^{t}\\}$ facilitates joint learning from molecules and text under a unified next-token-prediction   \n159 objective. As the vocabulary is expanded, the corresponding embeddings and prediction layers also   \n160 need to be extended, with the newly introduced parameters initialized randomly.   \n161 Unified Molecule-text Modeling. The multi-modal sequence $\\{u_{i}\\}_{i=1}^{L}$ is fed into the pretrained   \n162 LLM for performing multi-modal autoregression. UniMoT adopts the general Language Modeling   \n163 (LM) objective to directly maximize the log-likelihood of the data distribution: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\mathcal{L}}_{\\mathrm{LM}}=-\\sum_{u\\in{\\mathcal{D}}}\\sum_{i\\in{\\mathcal{Z}}}\\log p\\left(u_{i}\\mid u_{1},\\cdot\\cdot\\cdot,u_{i-1};\\theta\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "164 where $\\mathcal{D}$ represents the dataset, $\\mathcal{T}$ represents the set of indices of the generation target, and $\\theta$ denotes   \n165 the parameters of the LLM. The unification of representation and training paradigm for molecules and   \n166 text enhances the abilities of LLMs to understand molecule-text interactions and alignment. UniMoT   \n167 can interpret molecules similar to understanding a foreign language, and generate them as if they   \n168 were text. We conduct autoregressive pretraining on molecule-to-text and text-to-molecule tasks to   \n169 enhance the molecule comprehension and generation capabilities.   \n170 Molecule-to-Text Autoregression. While structural information is embedded in molecule features   \n171 and captured by the molecule tokens through the tokenizer, we also aim to incorporate sequential   \n117723 $\\{s_{i}\\}_{i=1}^{M}$ t iwoint ho f thmeo lSecMuIleLs EfSo r [b5e0t]t esr ecqoumenprceeh aennsdi oan .p rTohmerpetf otroe ,f owrem c tohnec atmeunlattie- tmhoe damlo ilencpuulte  sseeqquueennccee   \n174 $\\{u_{i}\\}_{i=1}^{L}$ , as illustrated in Figure 3a. The text sequence of the corresponding molecule caption is used   \n175 as the generation target.   \n176 Text-to-Molecule Autoregression. For molecule generation, a prompt and the molecule caption   \n177 are concatenated, with a [MOL] token appended to signify the beginning of the molecule sequence,   \n178 as illustrated in Figure 3b. The molecule sequence $\\{s_{i}\\}_{i=1}^{M}$ produced by the tokenizer is used as the   \n179 generation target. During inference, given a prompt and the molecule caption, the output molecule   \n180 sequence can be decoded into the desired molecule by the pretrained adapter and SMILES decoder. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "181 3.3 Training Strategy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "182 The training strategy for UniMoT is structured across four stages. Stage-1 focuses on Causal Q  \n183 Former pretraining with tailored objectives. In Stage-2, the molecule tokenizer is optimized using the   \n184 frozen encoders and decoder. Stage-3 integrates the tokenizer with a language model for multi-modal   \n185 comprehension and generation. Finally, Stage-4 fine-tunes UniMoT for specific tasks, aligning it with   \n186 human instructions and optimizing performance for various molecular applications. More details   \n187 regarding the training process can be found in Appendix C.   \n188 Stage-1: Causal Q-Former Pretraining. We connect the molecule encoder and Causal Q-Former,   \n189 leveraging the pretrained MoleculeSTM molecule encoder [31]. The molecule encoder remains   \n190 frozen while only the Causal Q-Former is updated. Both queries and text inputs are used, while   \n191 only queries serve as input in subsequent stages. In our experiments, we utilize 16 queries. We   \n192 employ three tailored objectives MTC, MTM, and MTG for the pretraining of the Causal Q-Former,   \n193 as detailed in Appendix A.   \n194 Stage-2: Molecule Tokenizer Pretraining. We connect the Causal Q-Former with subsequent   \n195 blocks and use the objective defined in Equation (2). We employ the pretrained ChemFormer [17] as   \n196 the generative model. Specifically, we leverage the SMILES encoder and SMILES decoder provided   \n197 by ChemFormer. The molecule codebook size is set to $K=2048$ . As shown in Figure 2, we keep   \n198 the molecule encoder, SMILES encoder, and SMILES decoder frozen, while updating the Causal   \n199 Q-Former, codebook, and adapter.   \n200 Stage-3: Unified Molecule-Text Pretraining. We integrate the molecule tokenizer with the LLM   \n201 using the unified vocabulary of molecule tokens and text tokens. We employ the LM objective   \n202 defined in Equation (3) to pretrain the LLM. Pretraining involves molecule-to-text autoregression   \n203 and text-to-molecule autoregression, aimed at enhancing UniMoT\u2019s multi-modal comprehension and   \n204 generation capabilities. To enhance efficiency, we train the LLM using LoRA tuning [14].   \n205 Stage-4: Task-Specific Instruction Tuning. UniMoT is fine-tuned on seven comprehension and   \n206 generation tasks: molecular property prediction, molecule captioning, molecule-text retrieval, caption  \n207 guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. We   \n208 also utilize LoRA tuning to improve efficiency. This stage ensures UniMoT can accurately interpret   \n209 and respond to human instructions, making it versatile and effective for molecular tasks. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "210 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "211 4.1 Molecule Comprehension Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "212 Molecular Property Prediction Task. The goal of molecular property prediction is to forecast   \n213 a molecule\u2019s intrinsic physical and chemical properties. For the classification task, we incorporate   \n214 eight binary classification datasets from MoleculeNet [53]. Models are tasked with generating   \n215 a single prediction (\u201cyes\u201d or \u201cno\u201d). We compare UniMoT with the following baselines: KV  \n216 PLM [58], AttrMask [16], InfoGraph [40], MolCLR [48], GraphMVP [30], MoleculeSTM [31],   \n217 and InstructMol [6]. The ROC-AUC $(\\%)$ results on the MoleculeNet datasets are shown in Table 1.   \n218 The performance of the regression task of molecular property prediction is provided in Appendix D.   \n219 Compared to traditional graph learning methods and molecular LLMs like InstructMol, UniMoT   \n220 demonstrates consistent improvements across the eight datasets, indicating its robust molecule   \n221 comprehension abilities.   \n222 Molecule Captioning Task. The molecule captioning task involves generating a comprehensive   \n223 description of a molecule. We compare UniMoT with several baselines: MolT5 [11], MoMu [39],   \n224 InstructMol [6], MolCA [33], and 3D-MoLM [25]. BLEU [37], ROUGE [27], and METEOR [1] are   \n225 adopted as evaluation metrics. UniMoT is evaluated for molecule captioning on the PubChem and   \n226 CheBI-20 datasets. Performance on the PubChem dataset is shown in Table 2, while the performance   \n227 on the CheBI-20 dataset and some concrete examples are presented in Appendix D.   \n228 From Table 2, we observe that UniMoT consistently outperforms the baselines by a significant margin.   \n229 This task is more complex than classification or regression, providing a robust measure of the model\u2019s   \n230 molecule comprehension abilities. Notably, our proposed tokenizer-based architecture surpasses the   \n231 projection-based architecture (such as InstructMol), Q-Former-based architecture (such as MolCA   \n232 and 3D-MoLM), and models trained with contrastive learning strategies (such as MoMu). The results   \n233 demonstrate that the molecule tokenizer can generate molecule tokens with high-level molecular and   \n234 textual information, enhancing molecule comprehension abilities.   \n235 Molecule-Text Retrieval Task. The molecule-text retrieval task involves using a molecule to   \n236 retrieve text (M2T) and using text to retrieve a molecule (T2M). We compare UniMoT with several   \n237 baselines: Sci-BERT [2], KV-PLM [58], MoMu [39], MoleculeSTM [31], MolCA [33], and 3D  \n238 MoLM [25]. We report the performance of retrieval using a batch of 64 random samples and the entire   \n239 test set, evaluated with the metrics of Accuracy and Recall $@20$ . We use the checkpoint from Stage-1   \n240 of pretraining. UniMoT is evaluated on the datasets of PubChem, PCdes, and MoMu. Performance   \n241 on the PubChem dataset is shown in Table 3, while performance on the PCdes and MoMu datasets is   \n242 presented in Appendix D. UniMoT can understand complex molecule-text interactions through the   \n243 introduction of the Causal Q-Former. From Table 3, UniMoT demonstrates superior performance over   \n244 the baselines on molecule-text retrieval, particularly in molecule-to-text retrieval. This underscores   \n245 UniMoT\u2019s capability in learning fine-grained alignment between molecules and text. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/e65f79e3083f26c7bc1acb88fd5fc9be9eab0d2b8378492607679c1024a44c4e.jpg", "table_caption": ["Table 1: ROC-AUC $(\\%)$ of molecular property prediction task (classification) on the MoleculeNet datasets. Bold indicates the best performance and underline indicates the second best performance. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/93e8e4334c947a28054ff470e989e23a097543bef741e749b75cefbf3316c91e.jpg", "table_caption": ["Table 2: Performance $(\\%)$ of molecule captioning task on the PubChem dataset. Bold indicates the best performance and underline indicates the second best performance. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "246 4.2 Molecule Generation Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "247 We employ molecule generation tasks, which encompass caption-guided molecule generation, reagent   \n248 prediction, forward reaction prediction, and retrosynthesis. Caption-guided molecule generation   \n249 involves generating molecular structures based on textual descriptions. Reagent prediction entails   \n250 determining suitable reagents given reactants and products. Forward reaction prediction involves   \n251 predicting probable products given specific reactants and reagents. Retrosynthesis involves decon  \n252 structing a target molecule into simpler starting materials. We compare UniMoT with the following   \n253 baselines: LLaMA [44], Vicuna [7], Mol-Instructions [12], and InstructMol [6]. The metrics used   \n254 to evaluate molecule generation tasks include Exact Match, BLEU [37], Levenshtein Distance [22],   \n255 RDKit Fingerprint Similarity [20], MACCS Fingerprint Similarity [10], and Morgan Fingerprint   \n256 Similarity [36]. These metrics evaluate structural similarity between generated and target molecules,   \n257 along with Validity [19], which assesses the proportion of chemically valid molecules generated. We   \n258 utilize the Mol-Instructions dataset to evaluate the generation capabilities of UniMoT, and the results   \n259 are presented in Table 4.   \n260 As the baselines generate SMILES strings and then convert them to molecules, UniMoT directly   \n261 leverages the generated molecule tokens and obtains their embeddings from the learned codebook.   \n262 These embeddings can be decoded to desired molecules through the pretrained adapter and SMILES   \n263 decoder. Regarding the results in Table 4, UniMoT exhibits the capability to generate valid molecules   \n264 with a higher degree of similarity to the target molecules compared to the baselines. UniMoT can   \n265 generate molecules as if they were text, demonstrating strong generation capabilities and providing a   \n266 new perspective to molecule generation tasks. ", "page_idx": 6}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/eb6b6b23acdd88562f619ee65861eaa8d2c6c9c769aea2d67d7c0eb6912df7b5.jpg", "table_caption": ["Table 3: Performance $(\\%)$ of molecule-text retrieval task on the PubChem dataset. Bold indicates the best performance and underline indicates the second best performance. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/d99c73ac009f8a30e140ff9c5e192c590454c8eb29a1803e26fd8b551d24ae39.jpg", "table_caption": ["Table 4: Performance of molecule generation tasks on the Mol-Instructions dataset, including captionguided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. Bold indicates the best performance, and underline indicates the second best performance. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/0a5ab43b7cefae1c2e5b91a4da427a58c7bc0abe995e50da043dc2724cf75c74.jpg", "table_caption": ["Table 5: Ablation study on the projector and representation form for the molecule captioning task using the PubChem dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/362d23f36df59f3298f5c5640b76d48a284f889ba50eaa9687b09a3380b9792a.jpg", "table_caption": ["Table 6: Ablation study on the model size and tuning strategy for the molecule captioning task using the PubChem dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "267 4.3 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "268 Cross-Modal Projector. We conducted an ablation study on the cross-modal projector, with the   \n269 results on the molecule captioning task shown in Table 5. The linear projection demonstrated the worst   \n270 performance, indicating that the molecule features lack textual information, thus hindering effective   \n271 molecule-text interactions and alignment. Additionally, we compared the performance of a Q-Former   \n272 with bidirectional self-attention to a Causal Q-Former with causal self-attention. The results show   \n273 that query embeddings with causal dependency outperform those with bidirectional dependency. This   \n274 demonstrates that input with left-to-right causal dependency aligns with the unidirectional attention   \n275 mechanism in LLMs, leading to improved performance.   \n276 Discrete vs. Continuous Representation. We compare the performance of continuous causal query   \n277 embeddings and discrete tokens quantized from causal embeddings as inputs to LLMs. As shown in   \n278 Table 5, continuous embeddings demonstrate better performance than discrete tokens in understanding   \n279 molecules. This result is reasonable since the quantization process causes information loss in discrete   \n280 tokens. However, we still use discrete token representation to facilitate the autoregressive training   \n281 paradigm of LLMs, which supports the unification of comprehension and generation tasks. To achieve   \n282 this unification, we unavoidably sacrifice some performance in comprehension tasks.   \n283 Model Size and Tuning Stategy. We conducted a comparison of molecule captioning performance   \n284 across various model sizes and tuning strategies, as illustrated in Table 6. Our findings indicate that   \n285 scaling up the LLM to 13B or adopting a fully tuning strategy yields only marginal improvements   \n286 in performance compared to using LLaMA2-7B with LoRA tuning. While larger models and fully   \n28 tuning strategies might offer slight gains in performance, they come at a significant cost in terms of   \n288 efficiency. Considering the trade-off between achieving high performance and maintaining efficiency,   \n289 we have chosen to utilize LLaMA2-7B with LoRA tuning in our experiments. This ensures that our   \n290 model remains both powerful and practical. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "291 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "292 This work introduces UniMoT, an innovation in the field of molecular-textual understanding and   \n293 generation, which has successfully unified these two distinct modalities under a single, coherent   \n294 framework. By integrating a Vector Quantization-driven tokenizer with a Causal Q-Former, UniMoT   \n295 overcomes previous architectural limitations where molecule and text modalities were not treated   \n296 equally, lacking a dedicated supervision signal for the molecular domain. This unique tokenizer   \n297 transforms molecules into sequences of discrete tokens, embedding high-level molecular and textual   \n298 information cohesively. Moreover, by employing a four-stage training scheme, UniMoT has emerged   \n299 as a versatile multi-modal LLM, adept at handling molecule-to-text and text-to-molecule tasks.   \n300 Extensive empirical evaluations demonstrate that UniMoT attains state-of-the-art performance across   \n301 a diverse array of molecule comprehension and generation tasks. ", "page_idx": 8}, {"type": "text", "text": "302 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "303 [1] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved   \n304 correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation   \n305 measures for machine translation and/or summarization, pages 65\u201372, 2005.   \n306 [2] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv   \n307 preprint arXiv:1903.10676, 2019.   \n308 [3] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through   \n309 stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.   \n310 [4] Yoann Boget, Magda Gregorova, and Alexandros Kalousis. Vector-quantized graph auto-encoder. arXiv   \n311 preprint arXiv:2306.07735, 2023.   \n312 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind   \n313 Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.   \n314 Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n315 [6] He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. Instructmol: Multi-modal integration for building a   \n316 versatile and reliable molecular assistant in drug discovery. arXiv preprint arXiv:2311.16208, 2023.   \n317 [7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan   \n318 Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4   \n319 with $90\\%^{*}$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.   \n320 [8] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: large-scale self-supervised   \n321 pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885, 2020.   \n322 [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec  \n323 tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   \n324 [10] Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. Reoptimization of mdl keys   \n325 for use in drug discovery. Journal of chemical information and computer sciences, 42(6):1273\u20131280, 2002.   \n326 [11] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between   \n327 molecules and natural language. arXiv preprint arXiv:2204.11817, 2022.   \n328 [12] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and   \n329 Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models.   \n330 arXiv preprint arXiv:2306.08018, 2023.   \n331 [13] Robert Gray. Vector quantization. IEEE Assp Magazine, 1(2):4\u201329, 1984.   \n332 [14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and   \n333 Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,   \n334 2021.   \n335 [15] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.   \n336 Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.   \n337 [16] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.   \n338 Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.   \n339 [17] Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained   \n340 transformer for computational chemistry. Machine Learning: Science and Technology, 3(1):015022, 2022.   \n341 [18] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A   \n342 Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2023 update. Nucleic acids research, 51(D1):D1373\u2013   \n343 D1380, 2023.   \n344 [19] Matt J Kusner, Brooks Paige, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Grammar variational autoencoder. In   \n345 International conference on machine learning, pages 1945\u20131954. PMLR, 2017.   \n346 [20] Greg Landrum et al. Rdkit: Open-source cheminformatics, 2006.   \n347 [21] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo   \n348 Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining.   \n349 Bioinformatics, 36(4):1234\u20131240, 2020.   \n350 [22] Vladimir I Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals. In   \n351 Soviet physics doklady, volume 10, pages 707\u2013710. Soviet Union, 1966.   \n352 [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training   \n353 with frozen image encoders and large language models. In International conference on machine learning,   \n354 pages 19730\u201319742. PMLR, 2023.   \n355 [24] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training   \n356 for unified vision-language understanding and generation. In International conference on machine learning,   \n357 pages 12888\u201312900. PMLR, 2022.   \n358 [25] Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and   \n359 Qi Tian. Towards 3d molecule-text interpretation in language models. arXiv preprint arXiv:2401.13923,   \n360 2024.   \n361 [26] Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. Drugchat: towards enabling chatgpt-like   \n362 capabilities on drug molecule graphs. arXiv preprint arXiv:2309.03907, 2023.   \n363 [27] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches   \n364 out, pages 74\u201381, 2004.   \n365 [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural   \n366 information processing systems, 36, 2024.   \n367 [29] Pengfei Liu, Yiming Ren, Jun Tao, and Zhixiang Ren. Git-mol: A multi-modal large language model for   \n368 molecular science with graph, image, and text. Computers in Biology and Medicine, 171:108073, 2024.   \n369 [30] Shengchao Liu, Mehmet F Demirel, and Yingyu Liang. N-gram graph: Simple unsupervised representation   \n370 for graphs, with applications to molecules. Advances in neural information processing systems, 32, 2019.   \n371 [31] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao,   \n372 and Animashree Anandkumar. Multi-modal molecule structure\u2013text model for text-based retrieval and   \n373 editing. Nature Machine Intelligence, 5(12):1447\u20131457, 2023.   \n374 [32] Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pre-training   \n375 molecular graph representation with 3d geometry. arXiv preprint arXiv:2110.07728, 2021.   \n376 [33] Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng   \n377 Chua. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter.   \n378 arXiv preprint arXiv:2310.12798, 2023.   \n379 [34] Yizhen Luo, Kai Yang, Massimo Hong, Xingyi Liu, and Zaiqing Nie. Molfm: A multimodal molecular   \n380 foundation model. arXiv preprint arXiv:2307.09484, 2023.   \n381 [35] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt:   \n382 Open multimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442,   \n383 2023.   \n384 [36] Harry L Morgan. The generation of a unique machine description for chemical structures-a technique   \n385 developed at chemical abstracts service. Journal of chemical documentation, 5(2):107\u2013113, 1965.   \n386 [37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation   \n387 of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational   \n388 Linguistics, pages 311\u2013318, 2002.   \n389 [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,   \n390 Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.   \n391 Journal of machine learning research, 21(140):1\u201367, 2020.   \n392 [39] Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-Rong   \n393 Wen. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv   \n394 preprint arXiv:2209.05481, 2022.   \n395 [40] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi  \n396 supervised graph-level representation learning via mutual information maximization. arXiv preprint   \n397 arXiv:1908.01000, 2019.   \n398 [41] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt:   \n399 Graph instruction tuning for large language models. arXiv preprint arXiv:2310.13023, 2023.   \n400 [42] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,   \n401 and Tatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.   \n402 [43] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,   \n403 Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv   \n404 preprint arXiv:2211.09085, 2022.   \n405 [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,   \n406 Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation   \n407 language models (2023). arXiv preprint arXiv:2302.13971, 2023.   \n408 [45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural   \n409 information processing systems, 30, 2017.   \n410 [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz   \n411 Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,   \n412 30, 2017.   \n413 [47] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale   \n414 unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international   \n415 conference on bioinformatics, computational biology and health informatics, pages 429\u2013436, 2019.   \n416 [48] Y Wang, J Wang, Z Cao, and AB Farimani. Molclr: Molecular contrastive learning of representations via   \n417 graph neural networks. arxiv 2021. arXiv preprint arXiv:2102.10056, 2021.   \n418 [49] Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, and Anima Anandkumar.   \n419 Retrieval-based controllable molecule generation. arXiv preprint arXiv:2208.11126, 2022.   \n420 [50] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology   \n421 and encoding rules. Journal of chemical information and computer sciences, 28(1):31\u201336, 1988.   \n422 [51] Jiawei Wu, Mingyuan Yan, and Dianbo Liu. Vqsynery: Robust drug synergy prediction with vector   \n423 quantization mechanism. arXiv preprint arXiv:2403.03089, 2024.   \n424 [52] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm.   \n425 arXiv preprint arXiv:2309.05519, 2023.   \n426 [53] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu,   \n427 Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical   \n428 science, 9(2):513\u2013530, 2018.   \n429 [54] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z Li.   \n430 Mole-bert: Rethinking pre-training graph neural networks for molecules. In The Eleventh International   \n431 Conference on Learning Representations, 2022.   \n432 [55] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with   \n433 parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023.   \n434 [56] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive   \n435 learning with augmentations. Advances in neural information processing systems, 33:5812\u20135823, 2020.   \n436 [57] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu,   \n437 Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint   \n438 arXiv:2110.04627, 2021.   \n439 [58] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure   \n440 and biomedical text with comprehension comparable to human professionals. Nature communications,   \n441 13(1):862, 2022.   \n442 [59] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan,   \n443 Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv   \n444 preprint arXiv:2402.12226, 2024.   \n445 [60] Xiang Zhuang, Qiang Zhang, Keyan Ding, Yatao Bian, Xiao Wang, Jingsong Lv, Hongyang Chen, and   \n446 Huajun Chen. Learning invariant molecular representation in latent discrete space. Advances in Neural   \n447 Information Processing Systems, 36, 2024.   \n449 The Q-Former operates as a query-based transformer that utilizes learnable query vectors to interact   \n450 with molecule features extracted by a frozen encoder. These queries are essential for extracting rele  \n451 vant information from the molecule features. The Q-Former comprises both a molecule transformer   \n452 and a text transformer, sharing self-attention layers. The text transformer architecture is based on   \n453 BERT [9], while the molecule transformer incorporates cross-attention layers between self-attention   \n454 and feed-forward layers. Q-Former employs a cross-attention mechanism where the query vectors   \n455 selectively attend to different aspects of the molecule features, allowing the model to capture critical   \n456 details necessary for understanding and generating textual descriptions of molecular properties.   \n457 Specifically, we incorporate causal masks into the queries, ensuring that they only interact with   \n458 preceding queries. This ensures the sequence of query embeddings maintains a causal dependency,   \n459 aligning with the requirements of LLMs operating on text sequence. The Causal Q-Former is   \n446601 $\\mathbf{Z}=\\{z_{i}\\}_{i=1}^{M}\\in\\mathbf{\\bar{R}}^{M\\times d}$ . c oWntea ienimnpgl ohiyg thh-lee vCela umsaoll ecQu-lFaor ramnde rt etxot ugael nienrfaotrem caatiuosna,l  wqhueerrey $M$ mrebperdedsienngtss   \n462 the number of queries and $d$ denotes the dimension of query embeddings. Next, we introduce three   \n463 tailored objectives MTC, MTM, and MTG for the pretraining of the Causal Q-Former.   \n464 Molecule-Text Contrastive Learning (MTC) aims to align molecule and text features by maximizing   \n465 their mutual information. This is achieved by maximizing the molecule-text similarity of positive   \n466 pairs against that of negative pairs. We utilize the last query embedding $z_{M}$ of the query sequence   \n467 $\\mathbf{\\dot{\\bar{\\{z}}}}_{i}\\}_{i=1}^{M}$ as the query representation, since the output query sequence is causal and the last embedding   \n468 contains global information from the queries. For text representation, we use the output embedding   \n469 of the [CLS] token, denoted as $\\textit{\\textbf{y}}$ . The contrastive learning loss is expressed as follows: ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "KgGhxmQFFy/tmp/41c1618fed34913aa9c7fed82fbcc759a8d14653183e6a4e211f185d8871022c.jpg", "img_caption": ["Figure 4: Illustration of our proposed Causal Q-Former. The Causal Q-Former provides causal query embeddings for subsequent blocks. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{MTC}}=-\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{\\exp((z_{M}^{i})^{T}y^{i}/\\tau)}{\\sum_{j=1}^{B}\\exp((z_{M}^{i})^{T}y^{j}/\\tau)}-\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{\\exp((y^{i})^{T}z_{M}^{i}/\\tau)}{\\sum_{j=1}^{B}\\exp((y^{i})^{T}z_{M}^{j}/\\tau)},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "470 where $B$ denotes the batch size, and $\\tau$ represents the temperature parameter. Here, $z_{M}^{i}$ and $\\boldsymbol{y}^{i}$ refer   \n471 to the $i$ -th query and text representations in a batch, respectively.   \n472 Molecule-Text Matching (MTM) focuses on learning fine-grained alignment between molecule and   \n473 text features. As query embeddings $\\mathbf{Z}=\\{z_{i}\\}_{i=1}^{M}$ capture both molecular and textual information   \n474 through cross-attention and self-attention layers respectively, we utilize the last query embedding $z_{M}$   \n475 as input to a binary classifier. This classifier predicts whether a given molecule-text pair is matched   \n476 or unmatched. The corresponding loss function is formulated as follows: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{MTM}}=-\\frac{1}{B}\\sum_{i=1}^{B}\\log\\frac{\\exp(\\phi(z_{M}\\mid\\mathbf{X}^{i},t^{i}))}{\\sum_{j=1}^{B}\\exp(\\phi(z_{M}\\mid\\mathbf{X}^{i},t^{j}))+\\sum_{j=1}^{B}\\exp(\\phi(z_{M}\\mid\\mathbf{X}^{j},t^{i}))},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "477 where $\\phi$ represents a binary classifier, and $\\mathbf{X}^{i}$ and $t^{i}$ denote the $i$ -th input molecule features and input   \n478 text in a batch, respectively.   \n479 Molecule-grounded Text Generation (MTG) focuses on generating textual descriptions given   \n480 a molecule input. In this task, causal masks for queries are not applied since only textual output   \n481 is required. However, causal masks are applied for text, allowing each text token to attend to its   \n482 preceding text tokens and all queries, but not subsequent tokens. The Language Modeling (LM)   \n483 loss function is applied to model the generation of text $t^{i}$ conditioned on the molecule input $\\mathbf{X}^{i}$ ,   \n484 formulated as: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{MTG}}=-\\frac{1}{B}\\sum_{i=1}^{B}\\sum_{j=1}^{L}\\log p\\left(t_{j}^{i}\\mid t_{1}^{i},\\cdot\\cdot\\cdot,t_{j-1}^{i},\\mathbf{X}^{i}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "485 where $t_{j}^{i}$ represents the $j$ -th token in the text sequence $t^{i}$ . Here, $\\mathbf{X}^{i}$ and $t^{i}$ denote the $i$ -th input   \n486 molecule features and generated text in a batch, respectively. ", "page_idx": 13}, {"type": "text", "text": "487 The total loss for training the Q-Former encompasses the three aforementioned objectives: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{Q-Former}}=\\mathcal{L}_{\\mathrm{MTC}}+\\mathcal{L}_{\\mathrm{MTM}}+\\mathcal{L}_{\\mathrm{MTG}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "488 B Details of Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "489 This section provides detailed information about the datasets used in evaluating the performance of   \n490 UniMoT across various tasks. The datasets are utilized for molecular property prediction, molecule   \n491 captioning, molecule-text retrieval, and molecule generation tasks. Each dataset serves a unique   \n492 purpose in assessing different capabilities of the model. ", "page_idx": 13}, {"type": "text", "text": "493 We present the details of the Molecular Property Prediction Datasets below: ", "page_idx": 13}, {"type": "text", "text": "494 \u2022 BBBP [53]: The Blood-Brain Barrier Penetration dataset predicts the ability of molecules to   \n495 penetrate the blood-brain barrier.   \n496 \u2022 Tox21 [53]: This dataset is part of the Toxicology in the 21st Century initiative, used for toxicity   \n497 prediction.   \n498 \u2022 ToxCast [53]: Another toxicity prediction dataset with a broader range of biological assays.   \n499 \u2022 Sider [53]: Side Effect Resource database, used for predicting drug side effects.   \n500 \u2022 ClinTox [53]: Clinical Toxicity dataset for predicting clinical trial toxicity outcomes.   \n501 \u2022 MUV [53]: Maximum Unbiased Validation dataset for virtual screening.   \n502 \u2022 HIV [53]: Human Immunodeficiency Virus dataset for predicting anti-HIV activities.   \n503 \u2022 BACE [53]: Beta-Secretase 1 dataset for predicting inhibitors of the BACE-1 enzyme, relevant   \n504 for Alzheimer\u2019s research.   \n505 \u2022 QM9 [12]: The quantum mechanics properties dataset, where the objective is to predict key   \n506 quantum mechanics properties of a given molecule, such as HUMO, LUMO, and the HUMO  \n507 LUMO gap. ", "page_idx": 13}, {"type": "text", "text": "508 We present the details of the Molecule Captioning Datasets below: ", "page_idx": 13}, {"type": "text", "text": "509 \u2022 PubChem [18]: A large dataset of chemical molecules used for generating textual descriptions   \n510 of molecular structures.   \n511 \u2022 ChEBI-20 [11]: A subset of the Chemical Entities of Biological Interest database, provides   \n512 structured and detailed descriptions of molecules, enhancing the model\u2019s ability to generate   \n513 accurate captions. ", "page_idx": 13}, {"type": "text", "text": "514 We present the details of the Molecule-Text Retrieval Datasets below: ", "page_idx": 13}, {"type": "text", "text": "515 \u2022 PubChem [18]: Used for both molecule-to-text (M2T) and text-to-molecule (T2M) retrieval   \n516 tasks.   \n517 \u2022 PCdes [58]: Another dataset for evaluating M2T and T2M retrieval accuracy.   \n518 \u2022 MoMu [39]: Dataset specifically designed for molecule-text interactions and retrieval tasks. ", "page_idx": 13}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/433e22225dc3e633d71b51832200a67d23cef3c9ff77aaac7785ebe4e0c44796.jpg", "table_caption": ["Table 7: Summary of datasets, their types, tasks, descriptions, URLs, and licenses used for evaluating UniMoT. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "519 We present the details of the Molecule Generation Datasets below: ", "page_idx": 15}, {"type": "text", "text": "520 \u2022 Mol-Instructions [12]: This dataset includes tasks such as caption-guided molecule generation,   \n521 reagent prediction, forward reaction prediction, and retrosynthesis. It is used to evaluate the   \n522 model\u2019s ability to generate molecular structures based on textual descriptions and other related   \n523 tasks.   \n524 We summarize the datasets used for evaluating UniMoT in Table 7. It encompasses various types   \n525 of datasets, including those for classification, regression, captioning, retrieval, and generation tasks.   \n526 Each dataset is described in terms of its type, tasks it supports, a brief description of its content, its   \n527 URL for access, and the license under which it is distributed. The licenses vary, with some datasets   \n528 being in the public domain and others under CC-BY 4.0 license. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "529 C Details of Training ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "530 Stage-1: Causal Q-Former Pretraining. During Stage-1, we only connect the molecule encoder   \n531 and the Causal Q-Former, leaving out other blocks. We leverage the pretrained molecule encoder from   \n532 MoleculeSTM [31], which has undergone extensive contrastive learning with molecule-text pairs.   \n533 We utilize the PubChem dataset [18] for pretraining, keeping the molecule encoder frozen while   \n534 updating only the Causal Q-Former. Both queries and text serve as input to the Causal Q-Former,   \n535 while only queries serve as input in subsequent stages. Inspired by BLIP-2 [23], we employ three   \n536 tailored objectives \u2013 Molecule-Text Contrastive Learning (MTC), Molecule-Text Matching (MTM),   \n537 and Molecule-grounded Text Generation (MTG) \u2013 for the pretraining of the Causal Q-Former, as   \n538 detailed in Appendix A.   \n539 The dimension of molecule features is set to 300. We use 16 queries, each with a dimension of 768.   \n540 The size of $\\mathbf{Z}$ $16\\times768)$ is much smaller than the size of molecule features $\\mathbf{X}$ (e.g., $150\\times300)$ . The   \n541 Q-former is pretrained for 50 epochs. We adopt the AdamW optimizer with a weight decay of 0.05,   \n542 and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The batch size is set   \n543 to 64. The computational overhead for this pretraining is 20 GPU hours on 4 NVIDIA A100 GPUs.   \n544 Stage-2: Molecule Tokenizer Pretraining. We connect the Causal Q-Former with the subsequent   \n545 blocks and train the molecule tokenizer using the objective defined in Equation (2). Following   \n546 the approach of RetMol [49], we utilize SMILES strings [50] to represent molecules, and employ   \n547 the pretrained ChemFormer [17] as the generative model. Specifically, we leverage the SMILES   \n548 encoder and SMILES decoder components provided by ChemFormer. We utilize PubChem [18]   \n549 and CheBI-20 [11] datasets, keeping the molecule encoder, SMILES encoder, and SMILES decoder   \n550 frozen, while updating the Causal Q-Former, codebook, and adapter. Once optimized, the molecule   \n551 tokenizer remains unchanged throughout the subsequent stages.   \n552 The molecule codebook size is set to $K=2048$ , and the dimension of codebook embedding is 768.   \n553 The tokenizer is pretrained for 50 epochs. We adopt the AdamW optimizer with a weight decay of   \n554 0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The batch size   \n555 is set to 64. The computational overhead for this pretraining is 40 GPU hours on 4 NVIDIA A100   \n556 GPUs.   \n557 Stage-3: Unified Molecule-Text Pretraining. We connect the molecule tokenizer with the LLM   \n558 and employ the LM objective defined in Equation (3) to pretrain the LLM. We utilize LLaMA [44] as   \n559 the default LLM. To construct the unified molecule-text vocabulary, we merge 2048 molecule codes   \n560 with the original text vocabulary. Pretraining the LLM involves molecule-to-text autoregression   \n561 and text-to-molecule autoregression, aimed at enhancing UniMoT\u2019s multi-modal comprehension   \n562 and generation capabilities. We utilize datasets PubChem [18], CheBI-20 [11], PCdes [58], and   \n563 MoMu [39] for this purpose. To enhance efficiency, we train the LLM using LoRA tuning [14].   \n564 The multi-modal LLM is pretrained for 10 epochs. We adopt the AdamW optimizer with a weight   \n565 decay of 0.05, and a cosine decay learning rate scheduler, with a minimal learning rate of 1e-5. The   \n566 batch size is set to 32. The computational overhead for this pretraining is 50 GPU hours on 4 NVIDIA   \n567 A100 GPUs. To reduce CUDA memory usage, we integrate LoRA with the parameters set to $r=8$ ,   \n568 $\\alpha=32$ , and dropout $=0.1$ . This integration is applied to the k_proj, v_proj, q_proj, and o_proj   \n569 modules.   \n70 Stage-4: Task-Specific Instruction Tuning. We perform instruction tuning to align UniMoT with   \n71 human instructions through supervised fine-tuning on seven tasks: molecular property prediction,   \n72 molecule captioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction,   \n73 forward reaction prediction, and retrosynthesis. For the molecular property prediction task, we   \n7 utilize the quantum mechanics properties dataset [12] for regression prediction and the MoleculeNet   \n75 datasets [53] for property classification. For the molecule captioning and molecule-text retrieval   \n76 tasks, we employ datasets PubChem [18], CheBI-20 [11], PCdes [58], and MoMu [39]. For the   \n7 remaining tasks, we utilize the Mol-Instructions dataset [12] to conduct instruction tuning. We   \n78 fine-tune UniMoT for 10 epochs on each task using the same optimizer, learning rate scheduler, and   \n79 LoRA configurations as in Stage-3 pretraining. Instruction samples for comprehension and generation   \n80 tasks are shown in Table 8. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/69199d84d77fb06d358ac6e35e7ed7d34bbf697c858d9be3425bb8a09a2045ae.jpg", "table_caption": ["Table 8: Instruction samples for comprehension and generation tasks: molecular property prediction, molecule captioning, molecule-text retrieval, caption-guided molecule generation, reagent prediction, forward reaction prediction, and retrosynthesis. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "581 We have summarized the detailed training hyperparameters of UniMoT in Table 9. ", "page_idx": 16}, {"type": "text", "text": "582 D Details and More Results of Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "583 Molecular Property Prediction Task. Property prediction aims to anticipate a molecule\u2019s intrinsic   \n584 physical and chemical properties based on its structural or sequential characteristics. In the regression   \n585 task, we conduct experiments on the quantum mechanics properties dataset QM9 [12], where the   \n586 objective is to predict key quantum mechanics properties of a given molecule, such as HUMO, LUMO,   \n587 and the HUMO-LUMO gap. We compare UniMoT against several baselines, including Alpaca [42],   \n588 Baize [55], LLaMA2-7B [44], Vicuna-13B [7], Mol-Instructions [12], and InstructMol [6]. Mean   \n589 Absolute Error (MAE) serves as our evaluation metric. The performance of the regression task on the   \n590 QM9 dataset is presented in Table 10. Compared to previous single-modal instruction-tuned LLMs   \n591 and molecular LLMs, UniMoT exhibits further improvement on the regression task, showcasing its   \n592 fundamental comprehension abilities in molecular contexts.   \n593 Molecule Captioning Task. The molecule captioning task involves generating a comprehensive   \n594 description of a molecule. For this task, we compare UniMoT with several baselines: MolT5 [11],   \n595 MoMu [39], InstructMol [6], MolCA [33], and 3D-MoLM [25]. We adopt BLEU [37], ROUGE [27],   \n596 and METEOR [1] as the evaluation metrics. The performance of UniMoT in the molecule captioning   \n597 task on the CheBI-20 dataset is presented in Table 11. Some concrete examples of molecule captioning   \n598 task are presented in Table 12. From the results, it is evident that UniMoT consistently outperforms   \n599 the baselines by a significant margin. These results underscore the effectiveness of the molecule   \n600 tokenizer in providing molecule tokens with high-level molecular and textual information, thus   \n601 enhancing molecule comprehension.   \n602 Molecule-Text Retrieval Task. The molecule-text retrieval task involves using a molecule to   \n603 retrieve text (M2T) and using text to retrieve a molecule (T2M). We compare UniMoT with several   \n604 baselines: Sci-BERT [2], KV-PLM [58], MoMu [39], MoleculeSTM [31], MolCA [33], and 3D  \n605 MoLM [25]. We report the performance of retrieval using a batch of 64 random samples and the   \n606 entire test set, evaluated with the metrics of Accuracy and Recall $@20$ . We use the checkpoint   \n607 from Stage-1 of pretraining. Performance on the PCdes and MoMu datasets is shown in Table 13.   \n608 UniMoT demonstrates superior performance over the baselines on molecule-text retrieval, particularly   \n609 in molecule-to-text retrieval. This demonstrates that UniMoT has learned fine-grained alignment   \n610 between molecules and text, and it can understand molecule-text interactions through the introduction   \n611 of the Causal Q-Former.   \n612 Molecule Generation Tasks. Molecule generation tasks include caption-guided molecule genera  \n613 tion, reagent prediction, forward reaction prediction, and retrosynthesis. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/0f3ad86cbf9ec462766fcc94be7e72acd6a676cbc3fac9f4141f35e8fecd8ad1.jpg", "table_caption": ["Table 9: The detailed training hyperparameters of UniMoT. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/0a471faabbd300ab3e9a237f4315735c339363bc80be761f8682bc90060b3ddd.jpg", "table_caption": ["Table 10: Mean Absolute Error (MAE) of molecular property prediction task (regression) on the QM9 dataset. Bold indicates the best performance and underline indicates the second best performance. \u2206\u03f5 is the HOMO-LUMO energy gap. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/528f5d547df589c86eece270f257e5972f3f440001d49d77d58e1a44fb38b221.jpg", "table_caption": ["Table 11: Performance $(\\%)$ of molecule captioning task on the CheBI-20 dataset. Bold indicates the best performance and underline indicates the second best performance. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "\u2022 Caption-guided molecule generation involves creating molecular structures from textual descriptions, leveraging NLP and cheminformatics to interpret and translate descriptions into chemical structures.   \n\u2022 Reagent prediction focuses on identifying suitable reagents for given reactants and desired products, optimizing synthetic routes.   \n\u2022 Forward reaction prediction forecasts probable products from specific reactants and reagents, using knowledge of chemical reactivity.   \n\u2022 Retrosynthesis deconstructs target molecules into simpler starting materials. ", "page_idx": 18}, {"type": "text", "text": "622 In molecule generation tasks, evaluating the quality of generated molecules involves several metrics   \n623 that measure different aspects of similarity and validity.   \n24 \u2022 Exact Match checks if the generated molecule is identical to the target molecule, offering a   \n25 stringent criterion for precise replication but potentially overlooking chemically similar variants.   \n26 \u2022 The BLEU score [37], adapted from machine translation, measures the overlap of n-grams (short   \n27 sequences of atoms or bonds) between generated and target molecules, thus assessing partial   \n28 similarities.   \n29 \u2022 Levenshtein Distance [22] evaluates the minimum number of edits needed to transform the   \n30 generated molecule into the target, providing insight into structural changes required.   \n31 \u2022 RDKit [20], MACCS [10], and Morgan [36] Fingerprint Similarities compare the generated and   \n32 target molecules based on various molecular fingerprinting methods, which capture different   \n33 aspects of molecular structure and properties.   \n34 \u2022 The Validity [19] metric assesses the proportion of chemically valid molecules generated,   \n35 ensuring that the output consists of plausible chemical structures.   \n636 Together, these metrics offer a comprehensive evaluation framework, balancing exact matches with   \n637 structural and chemical validity. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "638 E Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "639 While UniMoT demonstrates considerable advancements in unifying molecule and text modalities   \n640 for comprehensive understanding and generation tasks, several limitations must be acknowledged.   \n641 Although UniMoT exhibits strong performance in molecule-to-text and text-to-molecule tasks, it has   \n642 not been extensively tested on more complex molecule generation tasks such as molecule editing,   \n643 which require precise modifications to molecular structures. Future work could explore extending   \n644 UniMoT\u2019s capabilities to handle such sophisticated molecular manipulations.   \n645 Due to the scarcity of annotated data in the molecular field, the training of UniMoT is less extensive   \n646 compared to fields like computer vision. This limitation restricts the model\u2019s ability to fully learn and   \n647 generalize from diverse molecular structures and properties. In contrast, the visual domain benefits ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Table 12: Examples of molecule captioning task on the ChEBI-20 dataset. We highlight in blue the text that accurately describes the molecule structures in the generated caption, ensuring alignment with the ground truth. ", "page_idx": 19}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/1df86628345dc6801cf9b4d2f0e620cd5ee5c6baf5b67f6a5a445e171326f5e4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 13: Accuracy $(\\%)$ of molecule-text retrieval task on the PCdes and MoMu datasets. Bold indicates the best performance and underline indicates the second best performance. We report the performance of retrieval using a batch of 64 random samples and the entire test set. ", "page_idx": 20}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/36218cf53f69633a8f28741e59e4fe6eb9d1a668f6ffe5c71481636dbc06c016.jpg", "table_caption": ["(a) Accuracy $(\\%)$ of molecule-text retrieval task on the PCdes dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "KgGhxmQFFy/tmp/5349e009b90a4d662c3bdb4ea982a3085349498c317345d72d365fe68a687700.jpg", "table_caption": ["(b) Accuracy $(\\%)$ of molecule-text retrieval task on the MoMu dataset. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "648 from abundant labeled datasets, allowing for more comprehensive training and better performance.   \n649 Addressing this data scarcity in the molecular domain is crucial for improving UniMoT\u2019s training   \n650 effectiveness and overall capabilities.   \n651 The current empirical evaluations, though extensive, are primarily conducted on standard datasets   \n652 and benchmarks; expanding the evaluation to a broader array of datasets and real-world scenarios   \n653 will provide a more comprehensive understanding of the model\u2019s robustness and generalizability. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "654 F Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "655 The development of UniMoT, a unified model for molecule and text modalities, has significant   \n656 potential to positively impact various fields. UniMoT can streamline the drug discovery process by   \n657 enabling efficient molecule generation and optimization based on textual descriptions. In material   \n658 science, it can aid in discovering new materials with desirable properties. Additionally, UniMoT   \n659 can enhance research collaboration between chemists, biologists, and data scientists by integrating   \n660 molecular and textual data, leading to comprehensive research insights and innovative solutions.   \n661 This paper does not pose any ethical concerns. The study does not involve human subjects and follows   \n662 proper procedures for data set releases. There are no potentially harmful insights, methodologies, or   \n663 applications. Additionally, there are no confilcts of interest or sponsorship concerns. Discrimination,   \n664 bias, and fairness issues are not applicable. Privacy and security matters have been appropriately   \n665 addressed, legal compliance has been maintained, and research integrity has been upheld. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "666 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "667 1. Claims   \n668 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n669 paper\u2019s contributions and scope?   \n670 Answer: [Yes]   \n671 Justification: We conduct extensive experiments to verify our claims.   \n672 Guidelines:   \n673 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n674 made in the paper.   \n675 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n676 contributions made in the paper and important assumptions and limitations. A No or   \n677 NA answer to this question will not be perceived well by the reviewers.   \n678 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n679 much the results can be expected to generalize to other settings.   \n680 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n681 are not attained by the paper.   \n682 2. Limitations   \n683 Question: Does the paper discuss the limitations of the work performed by the authors?   \n684 Answer: [Yes]   \n685 Justification: We discuss the limitations in Appendix E.   \n686 Guidelines:   \n687 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n688 the paper has limitations, but those are not discussed in the paper.   \n689 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n690 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n691 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n692 model well-specification, asymptotic approximations only holding locally). The authors   \n693 should reflect on how these assumptions might be violated in practice and what the   \n694 implications would be.   \n695 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n696 only tested on a few datasets or with a few runs. In general, empirical results often   \n697 depend on implicit assumptions, which should be articulated.   \n698 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n699 For example, a facial recognition algorithm may perform poorly when image resolution   \n700 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n701 used reliably to provide closed captions for online lectures because it fails to handle   \n702 technical jargon.   \n703 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n704 and how they scale with dataset size.   \n705 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n706 address problems of privacy and fairness.   \n707 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n708 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n709 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n710 judgment and recognize that individual actions in favor of transparency play an impor  \n711 tant role in developing norms that preserve the integrity of the community. Reviewers   \n712 will be specifically instructed to not penalize honesty concerning limitations.   \n713 3. Theory Assumptions and Proofs   \n714 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 21}, {"type": "text", "text": "715 a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "717 Justification: The paper does not include theoretical results.   \n718 Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "729 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "30 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n31 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n32 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Justification: We disclose all the information to reproduce the experimental results in Section 4, Appendix C, and Appendix D. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "69 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n70 tions to faithfully reproduce the main experimental results, as described in supplemental   \n71 material?   \n772 Answer: [No]   \n773 Justification: Once our paper is accepted, we will make the code openly accessible.   \n774 Guidelines:   \n775 \u2022 The answer NA means that paper does not include experiments requiring code.   \n776 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n777 public/guides/CodeSubmissionPolicy) for more details.   \n778 \u2022 While we encourage the release of code and data, we understand that this might not be   \n779 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n780 including code, unless this is central to the contribution (e.g., for a new open-source   \n781 benchmark).   \n782 \u2022 The instructions should contain the exact command and environment needed to run to   \n783 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n784 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n785 \u2022 The authors should provide instructions on data access and preparation, including how   \n786 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n787 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n788 proposed method and baselines. If only a subset of experiments are reproducible, they   \n789 should state which ones are omitted from the script and why.   \n790 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n791 versions (if applicable).   \n792 \u2022 Providing as much information as possible in supplemental material (appended to the   \n793 paper) is recommended, but including URLs to data and code is permitted.   \n794 6. Experimental Setting/Details   \n795 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n796 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n797 results?   \n798 Answer: [Yes]   \n799 Justification: We disclose all the details of our experiments in Appendix C and Appendix D.   \n800 Guidelines:   \n801 \u2022 The answer NA means that the paper does not include experiments.   \n802 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n803 that is necessary to appreciate the results and make sense of them.   \n804 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n805 material.   \n806 7. Experiment Statistical Significance   \n807 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n808 information about the statistical significance of the experiments?   \n809 Answer: [No]   \n810 Justification: Given the considerable computational resources required for experiments with   \n811 LLMs, we adhere to the common practice in the community.   \n812 Guidelines:   \n813 \u2022 The answer NA means that the paper does not include experiments.   \n814 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n815 dence intervals, or statistical significance tests, at least for the experiments that support   \n816 the main claims of the paper.   \n817 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n818 example, train/test split, initialization, random drawing of some parameter, or overall   \n819 run with given experimental conditions).   \n820 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n821 call to a library function, bootstrap, etc.)   \n822 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n823 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n824 of the mean.   \n825 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n826 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n827 of Normality of errors is not verified.   \n828 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n829 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n830 error rates).   \n831 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n832 they were calculated and reference the corresponding figures or tables in the text.   \n833 8. Experiments Compute Resources   \n834 Question: For each experiment, does the paper provide sufficient information on the com  \n835 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n836 the experiments?   \n837 Answer: [Yes]   \n838 Justification: The information regarding compute resources is provided in Appendix C.   \n839 Guidelines:   \n840 \u2022 The answer NA means that the paper does not include experiments.   \n841 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n842 or cloud provider, including relevant memory and storage.   \n843 \u2022 The paper should provide the amount of compute required for each of the individual   \n844 experimental runs as well as estimate the total compute.   \n845 \u2022 The paper should disclose whether the full research project required more compute   \n846 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n847 didn\u2019t make it into the paper).   \n848 9. Code Of Ethics   \n849 Question: Does the research conducted in the paper conform, in every respect, with the   \n850 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n851 Answer: [Yes]   \n852 Justification: We have carefully reviewed the code of ethics to ensure strict adherence to the   \n853 guidelines.   \n854 Guidelines:   \n855 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n856 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n857 deviation from the Code of Ethics.   \n858 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n859 eration due to laws or regulations in their jurisdiction).   \n860 10. Broader Impacts   \n861 Question: Does the paper discuss both potential positive societal impacts and negative   \n862 societal impacts of the work performed?   \n863 Answer: [Yes]   \n864 Justification: We discuss the broader impacts in Appendix F.   \n865 Guidelines:   \n866 \u2022 The answer NA means that there is no societal impact of the work performed.   \n867 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n868 impact or why the paper does not address societal impact.   \n869 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n870 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n871 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n872 groups), privacy considerations, and security considerations.   \n873 \u2022 The conference expects that many papers will be foundational research and not tied   \n874 to particular applications, let alone deployments. However, if there is a direct path to   \n875 any negative applications, the authors should point it out. For example, it is legitimate   \n876 to point out that an improvement in the quality of generative models could be used to   \n877 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n878 that a generic algorithm for optimizing neural networks could enable people to train   \n879 models that generate Deepfakes faster.   \n880 \u2022 The authors should consider possible harms that could arise when the technology is   \n881 being used as intended and functioning correctly, harms that could arise when the   \n882 technology is being used as intended but gives incorrect results, and harms following   \n883 from (intentional or unintentional) misuse of the technology.   \n884 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n885 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n886 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n887 feedback over time, improving the efficiency and accessibility of ML).   \n888 11. Safeguards   \n889 Question: Does the paper describe safeguards that have been put in place for responsible   \n890 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n891 image generators, or scraped datasets)?   \n892 Answer: [NA]   \n893 Justification: The paper poses no such risks.   \n894 Guidelines:   \n895 \u2022 The answer NA means that the paper poses no such risks.   \n896 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n897 necessary safeguards to allow for controlled use of the model, for example by requiring   \n898 that users adhere to usage guidelines or restrictions to access the model or implementing   \n899 safety filters.   \n900 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n901 should describe how they avoided releasing unsafe images.   \n902 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n903 not require this, but we encourage authors to take this into account and make a best   \n904 faith effort.   \n905 12. Licenses for existing assets   \n906 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n907 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n908 properly respected?   \n909 Answer: [Yes]   \n910 Justification: The licenses for existing assets are provided in Appendix B.   \n911 Guidelines:   \n912 \u2022 The answer NA means that the paper does not use existing assets.   \n913 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n914 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n915 URL.   \n916 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n917 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n918 service of that source should be provided.   \n919 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n920 package should be provided. For popular datasets, paperswithcode.com/datasets   \n921 has curated licenses for some datasets. Their licensing guide can help determine the   \n922 license of a dataset.   \n923 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n924 the derived asset (if it has changed) should be provided.   \n925 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n926 the asset\u2019s creators.   \n927 13. New Assets   \n928 Question: Are new assets introduced in the paper well documented and is the documentation   \n929 provided alongside the assets?   \n930 Answer: [NA]   \n931 Justification: The paper does not release new assets.   \n932 Guidelines:   \n933 \u2022 The answer NA means that the paper does not release new assets.   \n934 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n935 submissions via structured templates. This includes details about training, license,   \n936 limitations, etc.   \n937 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n938 asset is used.   \n939 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n940 create an anonymized URL or include an anonymized zip file.   \n941 14. Crowdsourcing and Research with Human Subjects   \n942 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n943 include the full text of instructions given to participants and screenshots, if applicable, as   \n944 well as details about compensation (if any)?   \n945 Answer: [NA]   \n946 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n947 Guidelines:   \n948 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n949 human subjects.   \n950 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n951 tion of the paper involves human subjects, then as much detail as possible should be   \n952 included in the main paper.   \n953 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n954 or other labor should be paid at least the minimum wage in the country of the data   \n955 collector.   \n956 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n957 Subjects   \n958 Question: Does the paper describe potential risks incurred by study participants, whether   \n959 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n960 approvals (or an equivalent approval/review based on the requirements of your country or   \n961 institution) were obtained?   \n962 Answer: [NA]   \n963 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n964 Guidelines:   \n965 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n966 human subjects.   \n967 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n968 may be required for any human subjects research. If you obtained IRB approval, you   \n969 should clearly state this in the paper.   \n970 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n971 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n972 guidelines for their institution.   \n973 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n974 applicable), such as the institution conducting the review. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}]