[{"figure_path": "REVdYKGcfb/tables/tables_5_1.jpg", "caption": "Table 1: Performance comparison of retrievers utilizing different modal representations, where Few-shot (Random) refers to MM-ICL methods in which the demonstrations are randomly selected from the development set.", "description": "This table compares the performance of different retrieval methods for Multi-Modal In-Context Learning (MM-ICL) across four different tasks (image caption, visual question answering, classification, and reasoning).  It shows the performance of zero-shot, few-shot (random), and few-shot methods using textual, visual, and multi-modal retrievers. The table highlights the superior performance of multi-modal retrieval, particularly its resilience to the need for careful demonstration selection, and the limited impact of increasing model size on performance.", "section": "5.1 Empirical Analysis of MM-ICL Demonstration Retrieval"}, {"figure_path": "REVdYKGcfb/tables/tables_16_1.jpg", "caption": "Table 1: Performance comparison of retrievers utilizing different modal representations, where Few-shot (Random) refers to MM-ICL methods in which the demonstrations are randomly selected from the development set.", "description": "This table compares the performance of different retrieval methods (zero-shot, few-shot random, text-only, image-only, and multi-modal) across various vision-language models (VLLMs) on four tasks: image captioning, VQA, classification, and reasoning.  It highlights the significant improvement achieved by using multi-modal retrieval compared to other methods, demonstrating the crucial role of multi-modal alignment in MM-ICL. The table also reveals that increasing model parameters does not necessarily lead to substantial improvements, and that multi-modal context diminishes the need for careful demonstration selection.", "section": "5.1 Empirical Analysis of MM-ICL Demonstration Retrieval"}, {"figure_path": "REVdYKGcfb/tables/tables_23_1.jpg", "caption": "Table 1: Performance comparison of retrievers utilizing different modal representations, where Few-shot (Random) refers to MM-ICL methods in which the demonstrations are randomly selected from the development set.", "description": "This table compares the performance of different retrieval methods (zero-shot, few-shot random, textual, visual, and multi-modal) across four tasks (image caption, VQA, classification, and reasoning) and six VLLMs. It shows that multi-modal retrieval generally outperforms other methods, highlighting the importance of multi-modal alignment in MM-ICL.  The gains from carefully selecting demonstrations are less significant in multi-modal settings than in text-only settings.", "section": "5.1 Empirical Analysis of MM-ICL Demonstration Retrieval"}, {"figure_path": "REVdYKGcfb/tables/tables_23_2.jpg", "caption": "Table 1: Performance comparison of retrievers utilizing different modal representations, where Few-shot (Random) refers to MM-ICL methods in which the demonstrations are randomly selected from the development set.", "description": "This table compares the performance of different retrieval methods for multi-modal in-context learning (MM-ICL) across four tasks (image captioning, visual question answering, image classification, and reasoning).  It shows the performance boost achieved by using multi-modal retrievers compared to zero-shot, random few-shot, and single-modality retrieval methods.  The results highlight the importance of multi-modal alignment for MM-ICL, showing that increasing model parameters doesn't significantly improve performance.", "section": "5.1 Empirical Analysis of MM-ICL Demonstration Retrieval"}]