[{"figure_path": "lflwtGE6Vf/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of SSL and FSSL algorithms on CIFAR-10 with varying numbers of labeled samples, where FreeMatch [19] represents SSL, and SemiFL [8], FedCon [9], and FedMatch [7] represent FSSL.", "description": "This figure compares the performance of centralized semi-supervised learning (SSL) and federated semi-supervised learning (FSSL) methods on the CIFAR-10 dataset with varying amounts of labeled data.  It shows that as the number of labeled samples decreases, the performance gap between SSL and FSSL widens considerably. This highlights the challenge addressed by the proposed (FL)\u00b2 method in the paper, which aims to bridge this performance gap in low-label scenarios.", "section": "1 Introduction"}, {"figure_path": "lflwtGE6Vf/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of (FL)\u00b2: (1) client-specific adaptive thresholding adjusts the pseudo-labeling threshold according to each client's learning status, (2) sharpness-aware consistency regularization ensures consistency between the original model and the adversarially perturbed model with carefully selected high-confident pseudo labels, and (3) learning status-aware aggregation aggregates client models considering each client's learning progress.", "description": "This figure illustrates the three main components of the proposed (FL)\u00b2 algorithm.  It shows how client-specific adaptive thresholding dynamically adjusts the threshold for pseudo-labeling based on each client's learning progress.  It also demonstrates how sharpness-aware consistency regularization is used to ensure consistency between the original and adversarially perturbed model outputs, focusing on high-confidence pseudo-labels.  Finally, it shows how learning status-aware aggregation weights the contributions of different clients to the global model update based on their individual learning progress.", "section": "4 Method"}, {"figure_path": "lflwtGE6Vf/figures/figures_8_1.jpg", "caption": "Figure 3: Comparison of SemiFL, (FL)\u00b2, and its variants on the SVHN dataset (N\u2081 = 40, balanced IID). Pseudo-label accuracy measures the percentage of correct pseudo-labels. The label ratio is the proportion of pseudo-labeled samples among all unlabeled data. Correct and wrong label ratios indicate the percentages of correctly and incorrectly labeled samples, respectively. The C/W ratio shows the number of correct labels relative to wrong labels. All subgraphs share the legend of Fig. 3a.", "description": "This figure compares the performance of SemiFL and (FL)\u00b2 along with its variants (CAT, SACR, CAT+SACR) on the SVHN dataset.  It uses six subplots to show the test accuracy, pseudo-label accuracy, pseudo-label ratio, correct label ratio, wrong label ratio, and the ratio of correct to wrong labels (C/W ratio) over the course of 800 communication rounds.  The comparison highlights the effectiveness of the different components of (FL)\u00b2 in mitigating confirmation bias and improving model performance, particularly when dealing with limited labeled data.", "section": "5.3 Effect of (FL)\u00b2 on confirmation bias"}, {"figure_path": "lflwtGE6Vf/figures/figures_9_1.jpg", "caption": "Figure 4: Test accuracy and pseudo-label accuracy on the CIFAR10 dataset with 40 labels, balanced IID setting. Client-specific Adaptive Thresholding (CAT) is used as the baseline. Applying Sharpness-aware Consistency Regularization (SACR) to all data, including wrongly pseudo-labeled data, degrades performance than using only CAT, while applying SACR to correctly labeled data improves performance. SACR also outperforms the standard SAM objective (CAT+SAM).", "description": "This figure shows the impact of applying Sharpness-Aware Consistency Regularization (SACR) to all pseudo-labels versus only correctly labeled pseudo-labels.  The experiment is performed on the CIFAR-10 dataset with 40 labels, using a balanced IID setting.  The results show that applying SACR only to correctly labeled data improves performance.  In contrast, applying SACR to all data, including incorrect labels, leads to a decrease in performance.  Furthermore, SACR outperforms the standard SAM objective, highlighting its effectiveness for improving generalization in semi-supervised learning.", "section": "5.4 Impact of incorrect pseudo-labels on sharpness-aware consistency regularization"}]