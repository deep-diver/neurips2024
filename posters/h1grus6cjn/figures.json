[{"figure_path": "h1grUs6CjN/figures/figures_1_1.jpg", "caption": "Figure 1: The price of implicit bias in adversarially robust generalization. Top: An illustration of the role of geometry in robust generalization: a separator that maximizes the l2 distance between the training points (circles) might suffer a large error for test points (stars) perturbed within l\u221e balls, while a separator that maximizes the l\u221e distance might generalize better. Bottom: Binary classification of Gaussian data with (right) or without (left) l\u221e perturbations of the input in Rd using linear models. We plot the (robust) generalization gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training size m. In standard ERM (\u20ac = 0), the algorithms generalize similarly. In robust ERM (\u20ac > 0), however, the implicit bias of gradient descent is hurting the robust generalization of the models, while the implicit bias of coordinate descent/gradient descent with diagonal linear networks aids it. See Section 4 for details.", "description": "This figure illustrates the impact of implicit bias on adversarially robust generalization. The top panel shows how different geometric separators (maximizing l2 vs. l\u221e distance) affect robustness to l\u221e perturbations. The bottom panel presents experimental results on binary classification with linear models under l\u221e perturbations, comparing different optimization algorithms (coordinate descent, gradient descent, and gradient descent with diagonal networks). It demonstrates that the choice of optimization algorithm significantly affects the model's robustness and generalization performance, particularly in robust ERM settings.", "section": "Experiments"}, {"figure_path": "h1grUs6CjN/figures/figures_8_1.jpg", "caption": "Figure 1: The price of implicit bias in adversarially robust generalization. Top: An illustration of the role of geometry in robust generalization: a separator that maximizes the l2 distance between the training points (circles) might suffer a large error for test points (stars) perturbed within l\u221e balls, while a separator that maximizes the lo distance might generalize better. Bottom: Binary classification of Gaussian data with (right) or without (left) l\u221e perturbations of the input in Rd using linear models. We plot the (robust) generalization gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training size m. In standard ERM (\u20ac = 0), the algorithms generalize similarly. In robust ERM, however, the implicit bias of gradient descent is hurting the robust generalization of the models, while the implicit bias of coordinate descent/gradient descent with diagonal linear networks aids it. See Section 4 for details.", "description": "This figure shows the impact of implicit bias on adversarially robust generalization. The top panel illustrates how different separators (maximizing l2 vs. l\u221e distance) affect generalization under l\u221e perturbations. The bottom panel compares the generalization gap (difference between training and test accuracy) of different optimization algorithms (gradient descent, coordinate descent, and gradient descent with diagonal networks) for linear models with and without l\u221e perturbations.  It demonstrates that the choice of optimization algorithm and network architecture significantly impact robust generalization, highlighting the 'price of implicit bias'.", "section": "Experiments"}, {"figure_path": "h1grUs6CjN/figures/figures_9_1.jpg", "caption": "Figure 3: Left: Comparison of two optimization algorithms, gradient descent and sign gradient descent, in ERM and robust ERM on a subset of MNIST (digits 2 vs 7) with 1 hidden layer ReLU nets. Train and test accuracy correspond to the magnitude of perturbation e used during training. We observe that in robust ERM the gap between the generalization of the two algorithms increases. Right: Gap in (robust) test accuracy (with respect to the e used in training) of CNNs trained with GD and SD (GD accuracy minus SD accuracy) on subsets of MNIST (all classes) for various of e and m.", "description": "This figure compares the performance of gradient descent and sign gradient descent on a subset of MNIST. The left panel shows the training and testing accuracy for both algorithms under ERM and robust ERM, demonstrating that the gap in generalization widens for robust ERM. The right panel visualizes the difference in test accuracy between the two algorithms, under different levels of perturbation and training set sizes, confirming that the gap is more pronounced for robust ERM.  The experiments utilize ReLU networks with one hidden layer for the left panel and convolutional neural networks for the right.", "section": "Experiments"}, {"figure_path": "h1grUs6CjN/figures/figures_18_1.jpg", "caption": "Figure 4: An illustration of the model selection problem we are facing in Section 2. We depict hypothesis classes which correspond to Hr = {x \u2194 (w,x) : ||w||r < W} for r = 1,2, \u221e (notice that here, for illustration purposes, we keep W constant and not dependent on r). Increasing the order r of Hr can decrease the approximation error of the class, but it might increase the complexity captured by the worst-case Rademacher Complexity term of eq. (6).", "description": "This figure illustrates the model selection problem discussed in Section 2 of the paper. It shows three hypothesis classes, H1, H2, and H\u221e, represented by ellipses of increasing size.  The size of the ellipse represents the complexity of the hypothesis class, which relates to generalization ability.  A smaller, less complex class (H1) may have a larger approximation error but better generalization, while a larger class (H\u221e) may have smaller approximation error but worse generalization. The optimal hypothesis class balances approximation error and complexity for robust generalization. The figure highlights the trade-off between reducing approximation error and controlling complexity during model selection for robust generalization.", "section": "2 Optimal Regularization Depends on Sparsity of Data"}, {"figure_path": "h1grUs6CjN/figures/figures_24_1.jpg", "caption": "Figure 1: The price of implicit bias in adversarially robust generalization. Top: An illustration of the role of geometry in robust generalization: a separator that maximizes the l2 distance between the training points (circles) might suffer a large error for test points (stars) perturbed within l\u221e balls, while a separator that maximizes the l\u221e distance might generalize better. Bottom: Binary classification of Gaussian data with (right) or without (left) l\u221e perturbations of the input in Rd using linear models. We plot the (robust) generalization gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training size m. In standard ERM (\u20ac = 0), the algorithms generalize similarly. In robust ERM (\u20ac > 0), however, the implicit bias of gradient descent is hurting the robust generalization of the models, while the implicit bias of coordinate descent/gradient descent with diagonal linear networks aids it. See Section 4 for details.", "description": "This figure explores the impact of implicit bias on robust generalization.  The top panel illustrates how different distance metrics (l2 vs. l\u221e) affect the generalization ability of a model under adversarial perturbations. The bottom panel shows the generalization gap (difference between training and testing accuracy) for various optimization algorithms (gradient descent, coordinate descent, and gradient descent on diagonal linear networks) in standard and robust ERM settings.  It demonstrates that the implicit bias of the optimization algorithm significantly influences robustness and that this impact can differ based on the algorithm and network architecture.", "section": "Experiments"}, {"figure_path": "h1grUs6CjN/figures/figures_25_1.jpg", "caption": "Figure 1: The price of implicit bias in adversarially robust generalization. Top: An illustration of the role of geometry in robust generalization: a separator that maximizes the l2 distance between the training points (circles) might suffer a large error for test points (stars) perturbed within l\u221e balls, while a separator that maximizes the l\u221e distance might generalize better. Bottom: Binary classification of Gaussian data with (right) or without (left) l\u221e perturbations of the input in Rd using linear models. We plot the (robust) generalization gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training size m. In standard ERM (\u20ac = 0), the algorithms generalize similarly. In robust ERM (\u20ac > 0), however, the implicit bias of gradient descent is hurting the robust generalization of the models, while the implicit bias of coordinate descent/gradient descent with diagonal linear networks aids it. See Section 4 for details.", "description": "This figure shows the impact of implicit bias on robust generalization.  The top part illustrates how different geometric separators (maximizing l2 vs. l\u221e distance) affect robustness to l\u221e perturbations. The bottom part presents experimental results comparing different optimization algorithms (gradient descent, coordinate descent, and gradient descent with diagonal linear networks) on binary classification tasks with and without l\u221e adversarial perturbations. It demonstrates how the choice of optimization algorithm and network architecture significantly impacts robust generalization performance by affecting the implicit bias of the optimization process.", "section": "4 Experiments"}, {"figure_path": "h1grUs6CjN/figures/figures_26_1.jpg", "caption": "Figure 3: Left: Comparison of two optimization algorithms, gradient descent and sign gradient descent, in ERM and robust ERM on a subset of MNIST (digits 2 vs 7) with 1 hidden layer ReLU nets. Train and test accuracy correspond to the magnitude of perturbation \u03b5 used during training. We observe that in robust ERM the gap between the generalization of the two algorithms increases. Right: Gap in (robust) test accuracy (with respect to the \u03b5 used in training) of CNNs trained with GD and SD (GD accuracy minus SD accuracy) on subsets of MNIST (all classes) for various of \u03b5 and m.", "description": "The figure compares the performance of gradient descent (GD) and sign gradient descent (SD) on a subset of the MNIST dataset (digits 2 vs 7) for both standard ERM (\u03b5=0) and robust ERM (\u03b5>0). The left panel shows training and test accuracy curves for both algorithms under different perturbation levels. The right panel shows the difference in test accuracy between GD and SD for various dataset sizes (m) and perturbation levels (\u03b5).", "section": "Experiments"}, {"figure_path": "h1grUs6CjN/figures/figures_27_1.jpg", "caption": "Figure 2: Left: Binary classification of data coming from a sparse teacher w* and dense x, with (bottom) or without (top) l\u221e perturbations of the input in Rd using linear models. We plot the (robust) generalization gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training size m. For robust ERM, e is set to be of the largest permissible value e*. The gap between the methods grows when we pass from ERM to robust ERM. Right: Average benefit of CD over GD (in terms of generalization gap) for different values of teacher sparsity kw, data sparsity kx and magnitude of l\u221e perturbation e.", "description": "This figure shows the impact of implicit bias in adversarially robust generalization using linear models. The left panel illustrates the (robust) generalization gap (difference between training and testing accuracy) for three algorithms (Coordinate Descent, Gradient Descent, and Gradient Descent with diagonal networks) under different training set sizes, with and without l\u221e input perturbations. The right panel shows the average improvement of Coordinate Descent over Gradient Descent in terms of generalization gap across various teacher and data sparsity levels and perturbation magnitudes.", "section": "Experiments"}]