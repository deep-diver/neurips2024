[{"figure_path": "wlcm21C4nk/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the forward and backward procedures of different training methods.", "description": "This figure compares three different training methods for spiking neural networks (SNNs): standard backpropagation through time (BPTT), online training, and the proposed rate-based backpropagation.  It visually depicts the forward and backward passes for each method, highlighting the differences in memory and computational complexity. Standard BPTT requires storing all temporal activations, leading to high memory and time costs. Online methods reduce memory demands but still require iterative computations, increasing training time complexity. In contrast, rate-based backpropagation streamlines the computational graph by focusing on averaged dynamics, minimizing reliance on detailed temporal derivatives. This reduces memory and computational demands, making training SNNs more efficient. The figure illustrates the memory and time complexities of each method using Big O notation.", "section": "3 Preliminaries"}, {"figure_path": "wlcm21C4nk/figures/figures_3_1.jpg", "caption": "Figure 2: The implementation of rate-based backpropagation across layers. A rate-coding approximation is utilized for the forward procedure to connect average inputs with rate outputs, enabling fast rate-based error backpropagation throughout the training process.", "description": "The figure illustrates the core idea of the proposed rate-based backpropagation method.  The left side shows the standard forward pass in a spiking neural network (SNN) where the inputs are spike trains (st) that are processed to produce membrane potentials (ut) and then spike outputs (st).  The middle section illustrates the core of the new rate-based approach: calculating the average firing rate (r) over time for each neuron, approximating the average input (c) to the next layer by simply using the weighted average of the previous layer's firing rate, and using this average input to determine the output firing rate of the layer. Finally, the right side shows the backpropagation process, where the error (\u2202L/\u2202r) is backpropagated through the rate-based representations, simplifying the computation and reducing memory usage compared to traditional backpropagation through time (BPTT).", "section": "4 Rate-based Backpropagation for SNNs Training"}, {"figure_path": "wlcm21C4nk/figures/figures_6_1.jpg", "caption": "Figure 3: Empirical measurements conducted on the training procedure of BPTT. The experiments are carried out on the CIFAR-100 dataset using ResNet-18. Each subplot is labeled according to the naming convention \u201cA{test#}-T{timesteps#}-{target}-L{layer#}B{block#}N{LIF#}/C{conv#}\u201d.", "description": "This figure shows empirical results supporting the theoretical analysis in the paper.  Subplots (a), (b), and (c) present measurements of different quantities (A1, A2, A3) to demonstrate the relative independence of certain variables in the training process, particularly concerning the assumptions of rate coding and the approximation of gradients.  Subplot (d) visually compares the gradient descent directions between rate-based backpropagation and BPTT. This provides empirical evidence that the proposed method effectively approximates the behavior of BPTT, especially as the number of timesteps increases.", "section": "Empirical Validation"}, {"figure_path": "wlcm21C4nk/figures/figures_8_1.jpg", "caption": "Figure 4: Results of BPTT and ratem across various timesteps.", "description": "This figure shows a comparison of the classification performance and training costs between BPTT and the proposed rate-based backpropagation method (ratem) across different numbers of timesteps (T). The top-left plot shows that both methods achieve comparable accuracy, with a slight edge for BPTT at lower timesteps and ratem showing better scalability. The top-right plot demonstrates that memory usage increases linearly with timesteps for both methods, but ratem consistently uses less memory. The bottom-left plot shows that the training time for ratem remains nearly constant across different timesteps, whereas BPTT's training time increases linearly. This indicates the superior efficiency of the rate-based method in terms of memory and computational time.", "section": "5.3 Impact of Time Expansion"}, {"figure_path": "wlcm21C4nk/figures/figures_8_2.jpg", "caption": "Figure 5: Firing rates statistics for models trained by rateM.", "description": "This figure visualizes the average firing rates across different layers of a spiking neural network (SNN) trained using the proposed rate-based backpropagation method.  The left panel shows the firing rates for different timesteps (t=1 to t=4) in a ResNet-34 model trained on the ImageNet dataset. The right panel displays a similar analysis for a ResNet-18 model trained on the CIFAR-100 dataset, but with an extended time range (t=1 to t=6). The purple line in each panel represents the average firing rate across all timesteps (mean), highlighting the rate-coding nature of the trained models. The convergence of the firing rates towards the mean supports the method's effectiveness in leveraging rate coding for efficient training.", "section": "Analysis of Rate Statistics"}, {"figure_path": "wlcm21C4nk/figures/figures_19_1.jpg", "caption": "Figure 3: Empirical measurements conducted on the training procedure of BPTT. The experiments are carried out on the CIFAR-100 dataset using ResNet-18. Each subplot is labeled according to the naming convention \u201cA{test#}-T{timesteps#}-{target}-L{layer#}B{block#}N{LIF#}/C{conv#}\u201d.", "description": "This figure presents the results of empirical measurements conducted on the training procedure of Backpropagation Through Time (BPTT).  The experiments used the CIFAR-100 dataset and ResNet-18 architecture. Each subplot shows the results of a specific test, with labels indicating the test number, number of timesteps, target variable (A1-A3), layer and block number, number of LIF neurons, or number of convolutional layers. The subplots visualize the cosine similarity and magnitude of variables to demonstrate the relative independence between certain variables.  These tests were conducted to support the claims made in the paper about the independence of certain variables used in rate-based backpropagation.", "section": "Empirical Validation"}]