{"references": [{"fullname_first_author": "Martin Abadi", "paper_title": "Deep learning with differential privacy", "publication_date": "2016-00-00", "reason": "This paper is foundational for the field of differentially private deep learning, providing algorithms and analyses that have influenced much subsequent work in the area."}, {"fullname_first_author": "Raef Bassily", "paper_title": "Private stochastic convex optimization with optimal rates", "publication_date": "2019-00-00", "reason": "This paper established the first optimal algorithms for differentially private stochastic convex optimization under uniform Lipschitz assumptions, setting a benchmark for subsequent work."}, {"fullname_first_author": "Vitaly Feldman", "paper_title": "Private stochastic convex optimization: optimal rates in linear time", "publication_date": "2020-00-00", "reason": "This paper achieved optimal rates for differentially private stochastic convex optimization under uniform Lipschitz assumptions while also running in linear time, significantly advancing the field's efficiency."}, {"fullname_first_author": "Di Wang", "paper_title": "On differentially private stochastic convex optimization with heavy-tailed data", "publication_date": "2020-00-00", "reason": "This paper was among the first to address the heavy-tailed setting in differentially private stochastic convex optimization, providing algorithms and analyses for the case when the gradients have bounded kth moments."}, {"fullname_first_author": "Andrew Lowy", "paper_title": "Private stochastic optimization with large worst-case Lipschitz parameter: Optimal rates for (non-smooth) convex losses and extension to non-convex losses", "publication_date": "2023-00-00", "reason": "This paper provides lower bounds for differentially private stochastic convex optimization in the heavy-tailed setting and develops algorithms that nearly match these bounds, advancing the theoretical understanding of this problem."}]}