[{"type": "text", "text": "Bridge the Modality and Capability Gaps in Vision-Language Model Selection ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chao Yi, Yu-Hang He, De-Chuan Zhan, Han-Jia Ye State Key Laboratory for Novel Software Technology, Nanjing University {yic,heyh,zhandc,yehj}@lamda.nju.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. To better reuse the VLM resource and fully leverage its potential on different zeroshot image classification tasks, a promising strategy is selecting appropriate PreTrained VLMs from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset\u2019s images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the \u201cModality Gap\u201d\u2014the disparity in VLM\u2019s embeddings across two different modalities, making text a less reliable substitute for images; and the \u201cCapability Gap\u201d\u2014 the discrepancy between the VLM\u2019s overall ranking and its ranking for target dataset, hindering direct prediction of a model\u2019s dataset-specific performance from its general performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the negative impact of two gaps. SWAB first adopts optimal transport to capture the relevance between open-source and target datasets with a transportation matrix. It then uses this matrix to transfer useful statistics of VLMs from open-source datasets to the target dataset for bridging two gaps. By bridging two gaps to obtain better substitutes for test images, SWAB can accurately predict the performance ranking of different VLMs on the target task without the need for the dataset\u2019s images. Experiments across various VLMs and image classification datasets validate SWAB\u2019s effectiveness. Code is available at: https://github.com/YCaigogogo/SWAB. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vision-Language Models (VLMs) [43, 22, 45, 61] have demonstrated impressive image-text matching ability. One notable application of VLMs is zero-shot image classification [43, 37, 14, 35], where VLMs are leveraged to generate image classifiers using only class names directly. This zero-shot approach has shown considerable success in scenarios with scarce or no training images [33, 18]. ", "page_idx": 0}, {"type": "text", "text": "Despite the success of VLM in image classification, the performance of a VLM may vary substantially according to the datasets and domains [11], making it challenging to use a single model to handle all tasks. Fortunately, many open-source VLMs are available [21], and these VLMs form a vast VLM Zoo. With different architectures, pre-training datasets, or training methods, these VLMs have different strengths. The diverse pre-trained VLMs increase the likelihood of pinpointing at least one VLM that excels in a given target dataset in most cases.1 To more effectively reuse the VLM Zoo across diverse target tasks and unlock its full potential, we need a model selection method to choose suitable VLMs from the VLM Zoo for the target task. However, in scenarios such as zero-shot image classification, many users might not have labeled images for their target tasks, especially those who are not Machine Learning researchers. They prefer to describe their needs in text and use a Model Search Engine to find the most suitable model. So one solution is identifying the most suitable VLMs in the zoo for a target dataset without access to the dataset\u2019s images. This VLM selection is termed as \u201cLanguage-Only VLM Selection\u201d (LOVM) [64], and the paradigm is illustrated in Figure 1. ", "page_idx": 0}, {"type": "image", "img_path": "01qa1ZJs65/tmp/26c9a25e465645ae778c9a3c8ffbe06c05b9805a3d614fbee1256251bccfb528.jpg", "img_caption": ["Figure 1: Paradigm of Language-Only VLM Selection (LOVM). Users describe the details of their target tasks in text form, such as class names and image domains. Then, LOVM utilizes this information to generate class-related labeled texts through ChatGPT. These texts serve as substitutes for image samples in subsequent model selection algorithms. The model selection algorithm uses two types of data, including the open-source datasets (which have image and text data) and the text data from the target dataset, to predict the VLM\u2019s absolute or relative performance on a target dataset. It then selects the most appropriate VLM based on the predicted performance. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Two key types of information are available for LOVM. One is the target dataset\u2019s text data, i.e., names of the target classes and class-related labeled texts generated by LLMs (Details described in Section B.1). The other is the open-source datasets, collected in the form of images with their corresponding class names. Based on these data, the goal is to estimate a VLM\u2019s zero-shot image classification capability ranking among the VLM zoo on the target dataset. LOVM encounters two challenges stemming from the inherent heterogeneity in models and datasets. The first challenge is the Modality Gap across different modal features extracted by a VLM. Since the visual and textual features extracted by VLMs tend to cluster into two distinct groups and have gap vectors between them [30], using text data as image proxies to rank VLMs is inaccurate. The second challenge is the Capability Gap between the VLM\u2019s overall ranking and its ranking in the specific dataset. Owing to the VLM\u2019s performance variation across different datasets, the VLM\u2019s average performance on open-source datasets is hard to reflect its performance on a specific target dataset. Thus, selecting a VLM based solely on its general strength may prove to be a less effective strategy. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose VLM Selection With gAp Bridging (SWAB) to address both gaps. The key idea is to reuse VLMs\u2019 statistics from open-source datasets to estimate their statistics on the target dataset, which mitigates the negative impact of these two gaps. In particular, SWAB first uses optimal transport to calculate the transport matrix based on textual similarity between class names of open-source and target datasets. After applying VLMs on open-source datasets to calculate VLMs\u2019 statistics, i.e., the class-specific modality gap vectors and performance rankings of different VLMs, SWAB utilizes these statistics to estimate the same type of statistics on the target dataset. After that, SWAB uses the estimated gap vectors to align the features of texts with the features of images from the corresponding category, which bridges the modality gap. Meanwhile, SWAB\u2019s estimated VLMs\u2019 ranking also improves the prediction of their rankings on the target task, bridging the capability gap. The related work is in the appendix C. The main contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We analyze two key challenges in LOVM \u2014 the modality gap across VLM\u2019s modal features and the capability gap between the VLM\u2019s overall ranking and its ranking on the target dataset.   \n\u2022 We propose SWAB, which utilizes optimal transport to transform useful statistics of VLMs on open-source datasets to the target dataset to bridge two gaps.   \n\u2022 Experimental results on a LOVM benchmark composed of a wide range of VLMs and image classification datasets demonstrate the effectiveness of SWAB. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We formally introduce the LOVM setting, a baseline method for LOVM, and analyze the two kinds of gaps in LOVM. We use $\\Vert\\cdot\\Vert$ to represent the Euclidean norm of a vector unless otherwise defined. ", "page_idx": 2}, {"type": "text", "text": "2.1 Selecting VLMs from a Model Zoo ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Zero-Shot Image Classification of VLM. Assume there is a pre-trained VLM $\\textit{f}=\\,(f^{I},f^{T})$ consisting of an image encoder $f^{I}$ and a text encoder $f^{T}$ . Given an image classification dataset $\\tau$ with $k_{T}$ class names $\\boldsymbol{C}_{T}=\\{c_{1}^{T},\\cdots,c_{k_{T}}^{T}\\}$ , we input the class names $C\\tau$ (probably with templates like \u201cA photo of $\\{\\mathrm{class}\\}^{,})$ into the VLM\u2019s text encoder $f^{T}$ to get the image classifiers $\\{\\hat{t}_{j}\\}_{j=1}^{k_{T}}$ . Then, given a test image $\\pmb{x}_{i}$ , we use the image encoder $f^{I}$ to extract its feature $\\hat{\\pmb x}_{i}$ . Finally, we predict the label via the cosine similarity between the image feature $\\hat{\\pmb{x}}_{i}$ and image classifiers $\\{\\hat{t}_{j}\\}_{j=1}^{k_{T}}$ . The class with the highest cosine similarity to the image is selected as the predicted class $\\hat{y}_{i}$ . Given $\\hat{\\pmb x}_{i}=f^{I}({\\pmb x}_{i})$ , $\\bar{t}_{j}=f^{T}(c_{j}^{T})$ , Equation 1 describes this zero-shot image classification process: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{y}_{i}=f(\\pmb{x}_{i},C_{T})=\\operatorname*{argmax}_{c_{j}^{T}\\in[C_{T}]}\\frac{\\hat{x}_{i}^{\\top}\\hat{t}_{j}}{\\lVert\\hat{\\pmb{x}}_{i}\\rVert\\cdot\\lVert\\hat{t}_{j}\\rVert}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "collection of $M$ rVecLeMnts  yceoanrsst,i tuat el aar gVe LnMu mZboeor $\\mathcal{M}=\\left\\{f_{m}=\\left(f_{\\underline{{m}}}^{I},f_{m}^{T}\\right)\\right\\}_{m=1}^{M}$ .  eTmhee rcgaepd.ab ilAitsys uomf $f_{m}$ is determined by three key factors: the model architecture (e.g., Transformer [50], ConvNeXt [32]), the pre-trained dataset (e.g., LAION-400M [44], MS-COCO [31]), and the training method (e.g., contrastive loss [43], caption loss [60]). Combinations of these factors result in \u201cgood and diverse\u201d VLMs in $\\mathcal{M}$ . Given a dataset $\\tau$ , it is probable to find a suitable VLM from the VLM zoo with high zero-shot image classification performance on $\\tau$ . ", "page_idx": 2}, {"type": "text", "text": "Language-Only VLM Selection (LOVM). Rather than using images from the target dataset, LOVM focuses on the zero-shot scenario where only the target dataset\u2019s text data, such as its class names $C_{7}$ ,o nar ed aatvasaieltas $\\boldsymbol{S}$ .  foTrh eV sLetM  osf elcelacstiso nn.a mBeess iidn $\\boldsymbol{S}$ iws $C s\\,=\\,\\{c_{1}^{S},\\cdot\\cdot\\cdot\\,,c_{k s}^{S}\\}$ ,e na-nsdo utrhcee $D_{S}^{I}$ a gdee ncoltaes stihfeilabelled images in these classes. Given a target task $\\tau$ , the VLM selection method $h$ estimates the zero-shot classification ability of $f_{m}$ based on $C\\tau,C\\s$ , and $D_{S}^{I}$ via $\\hat{r}_{m}^{T}\\,=\\,h(f_{m}\\,\\mid\\,C_{T},C_{S},D_{S}^{I})$ where $m\\,\\in\\,[1,\\cdots\\,,M]$ . $\\hat{r}_{m}^{\\mathcal{T}}$ is the predicted ranking of the $m$ -th VLM $f_{m}$ on $\\tau$ . The higher the ranking, the more probable $f_{m}$ achieves higher zero-shot image classification performance on $\\tau$ . Assuming we can obtain the test image set $D_{\\mathcal{T}}^{I}$ of the target dataset $\\tau$ with $|D_{\\mathcal{T}}^{I}|$ images, then we can calculate the zero-shot image classification accuracy $p_{m}^{\\mathcal{T}}$ of $f_{m}$ is calculated by $\\begin{array}{r}{\\mathring{p_{m}^{T}}\\,=\\,\\frac{1}{\\vert D_{T}^{I}\\vert}\\sum_{(\\pmb{x}_{i},y_{i})\\in D_{T}^{I}}\\mathbb{I}\\left(y_{i}=f_{m}\\left(\\pmb{x}_{i},C_{T}\\right)\\right)}\\end{array}$ . $f_{m}\\left(\\pmb{x}_{i},C_{T}\\right)$ represents the predicted class with the same manner as Equation $1.\\;\\mathbb{I}(\\cdot)$ is the indicator function, which outputs 1 if the condition is satisfied, and 0 otherwise. Based on $\\{p_{m}^{\\mathcal{T}}\\}_{m=1}^{M}$ , we obtain the true ranking of $M$ VLMs $\\pmb{r}^{T}=[r_{1}^{T},\\dots,r_{M}^{T}]$ by assigning higher ranking $r$ to models with higher accuracy $p$ . However, in the zero-shot scenario, we can\u2019t obtain the test images set $D_{\\mathcal{T}}^{I}$ in advance. Therefore, the goal of LOVM is to make the predicted ranking $\\hat{\\pmb{r}}^{T}=[\\hat{r}_{1}^{T},\\cdot\\cdot\\cdot\\,,\\hat{r}_{M}^{T}]$ be an accurate estimation of the ground truth ranking $\\pmb{r}^{T}=[r_{1}^{T},\\dots,r_{M}^{T}]$ so that the best VLM can be selected. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Evaluation of LOVM Methods. We measure the performance of the LOVM algorithm by comparing the ranking similarity between $r^{\\mathcal{T}}$ and $\\hat{\\mathbf{r}}^{\\mathcal{T}}$ . Specifically, we calculate the Top-5 Recall $R_{5}$ (ranges from 0 to 1) and Kendall\u2019s Rank Correlation $\\tau$ (ranges from -1 to 1). The larger the better. ", "page_idx": 2}, {"type": "text", "text": "2.2 Possible Paradigms for LOVM ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Non-Learning-based LOVM. There are three main paradigms for LOVM. The first paradigm is to neglect the visual encoder and select VLM solely on texts. In detail, we can utilize ChatGPT [40] to generate auxiliary texts ${\\tilde{D}}_{7}$ based on class names $C\\tau$ of $\\tau$ . More details are described in Section B.1. These class-specific texts act as \u201cimage proxies\u201d. Then, whether a VLM $f_{m}$ fits $\\tau$ could be measured by transferability metrics, e.g., H-Score [2] and LogME [58], between the VLM\u2019s text encoder $f_{m}^{T}$ and generated text dataset ${\\tilde{D}}_{7}$ . The second paradigm relies on the general performance of a certain VLM $f_{m}$ . We use open-source datasets to measure a VLM\u2019s general performance. If $f_{m}$ achieves high zero-shot classification performance over open-source datasets, then it is expected to be competitive on $\\tau$ . These methods assume that a VLM\u2019s ranking is relatively consistent across tasks. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Learning-based LOVM. The third paradigm is based on the learning process. In detail, the ability of a VLM could be predicted based on a ranker model $f_{R}$ . The input of $f_{R}$ is a vector $s_{m}^{\\mathcal{T}}$ , depicting the dataset-specific representation of $f_{m}$ on $\\tau$ , while the output of $f_{R}$ is the relative/absolute performance $\\hat{p}_{m}^{\\mathcal{T}}\\in\\mathbb{R}$ of $f_{m}$ on $\\tau$ . The $f_{R}$ could be learned on open-source datasets $\\boldsymbol{S}$ [63, 64]. Due to the awvea iclaabni lciatlyc oufl abtoet he acclah ssV nLaMm\u2019ess $C_{S}$ easnedn itamtiaogne $D_{S}^{I}$ na-nsod utrrcuee  dzaetraos-est $\\boldsymbol{S}$ ts iumcha gaes  IclmaasgsiefNiceatt i[o8]n, $\\{s_{m}^{\\bar{n}}\\}_{m=1,n=1}^{M,N}$ accuracy $\\{p_{m}^{n}\\}_{m=1,n=1}^{M,N}$ . Here $N$ refers to the number of datasets in . After constructing the train set, the ranker model $f_{R}$ is learned based on the $\\{s_{m}^{n},p_{m}^{n}\\}_{m=1,n=1}^{M,N}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f_{R}}\\ \\sum_{m=1}^{M}\\sum_{n=1}^{N}\\ \\ell(f_{R}(\\pmb{s}_{m}^{n}),p_{m}^{n}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\ell$ is a loss function that measures the discrepancy between the prediction and the ground truth, which can be Mean Squared Error Loss and Huber Loss, among others. Given $\\tau$ , the learned $f_{R}$ is able to predict the performance $\\{\\hat{p}_{m}^{\\tau}\\}_{m=1}^{M}$ over $\\{s_{m}^{\\tau}\\}_{m=1}^{M}$ via $\\hat{p}_{m}^{\\mathcal{T}}\\,=\\,f_{R}(s_{m}^{\\mathcal{T}})$ . Finally, we can get the predicted VLMs\u2019 ranking $\\hat{\\pmb{r}}$ based on $\\{\\hat{p}_{m}^{\\tau}\\}_{m=1}^{\\bar{M}}$ . This approach has similarities with metalearning [12, 3]. Meta-learning attempts to use data from multiple datasets to learn a model adaptable to the target task, while Learning-based LOVM employs data from multiple datasets to learn a ranker model for selecting the suitable model from a VLM Zoo for the target task. The representation $s_{m}^{\\mathcal{T}}$ is one of the keys in this paradigm, and ModelGPT [64] calculates values $s_{m}^{\\tau}$ via the capability of a VLM\u2019s text encoder $f_{m}^{T}$ . ", "page_idx": 3}, {"type": "text", "text": "ModelGPT uses generated text data ${\\tilde{D}}_{7}$ as substitutes for images to calculate some metrics, which measures the zero-shot ability of $f_{m}$ on unseen images by the classification ability of $f_{m}$ on ${\\tilde{D}}_{7}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{m,i}^{\\mathcal{T}}=\\mathrm{Metric}_{i}\\left(f_{m},\\tilde{D}_{\\mathcal{T}}\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here Metric $^{\\,\\cdot\\,}\\,i$ indicates the $i$ -th metrics function such as Top-1 Accuracy and F1-Score. For example, the Top-1 Accuracy sTm,1 could be calculated in a similar manner as Equation 1, with the only difference being that the test samples were replaced with text samples $t_{i}$ instead of image samples $\\pmb{x}_{i}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\ns_{m,1}^{T}=\\frac{1}{|\\tilde{D}_{T}|}\\sum_{(t_{i},y_{i})\\in\\tilde{D}_{T}}\\mathbb{I}\\left(y_{i}=f_{m}(t_{i},C_{T})\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Besides, ModelGPT uses some metrics for assessing the features\u2019 quality extracted by the VLM\u2019s text encoder $f_{m}^{T}$ . More details are in the Section B.2. Moreover, the zero-shot classification performance of $f_{m}$ on ImageNet is also included in $s_{m}^{\\tau}$ as a general ability measure of $f_{m}$ . ModelGPT implements $f_{R}$ as a simple linear model. ", "page_idx": 3}, {"type": "text", "text": "2.3 Analysis of the Two Gaps in LOVM ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "There are two main challenges that limit the application of the aforementioned paradigms in LOVM. The first is the modality gap across different modalities\u2019 features in VLM\u2019s feature space, and the second is the capability gap between VLM\u2019s overall performance and dataset-specific performance. Modality Gap. As described in Section 2.2, methods like H-Score, LogME, and ModelGPT utilize the ChatGPT generated auxiliary texts ${\\tilde{D}}_{7}$ as image proxies to calculate metrics that measure the zeroshot accuracy on the target dataset $\\tau$ . In other words, the zero-shot classification ability across text and image modalities is estimated by the intra-modality classification ability. The latent assumption is that the generated texts and their corresponding images are closely aligned in VLM\u2019s feature space. However, this assumption is difficult to meet [30], and instances\u2019 features are more likely to cluster according to their modalities. In particular, we define the modality gap vector $\\textbf{\\textit{g}}$ between the features of an image-text pair $({\\boldsymbol{x}}_{i},t_{i})$ as $\\dot{\\pmb g}_{m,i}:=f_{m}^{I}(\\pmb x_{i})-f_{m}^{T}(t_{i})$ . Values in the gap vector are generally not close to zero. We name this phenomenon as Modality Gap in LOVM, which makes the scores on ${\\tilde{D}}_{7}$ hard to reveal the true zero-shot image classification capability of a VLM on a given dataset. ", "page_idx": 3}, {"type": "image", "img_path": "01qa1ZJs65/tmp/36381545a7dd4191db60e4b1ecdac8aa859370b7e7783b6918c0df28f4a3868b.jpg", "img_caption": ["Figure 2: Validation Experiments on the Modality Gap and Capability Gap. (a) Predicted VLMs\u2019 zero-shot image classification accuracy based on generated text data vs. VLM\u2019s true accuracy based on test images. Each point in the graph represents a model. From the result, we can find that the predicted accuracy poorly aligns with the true accuracy, indicating these text data are ineffective substitutes for image data. (b) We calculate the zero-shot image classification performance rankings of 43 VLMs across 23 datasets. We compute the average standard deviations and the mean value of differences between each VLM\u2019s maximum and minimum ranking. The result shows the performance of a VLM varies greatly across different datasets. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "We conduct a validation experiment on ImageNet with 43 VLMs. We first generate 50 auxiliary texts per class as ${\\tilde{D}}_{7}$ and then calculate the predicted Top-1 accuracy via Equation 4. Next, we use test images to calculate the VLM\u2019s true Top-1 accuracy. The consistency between the predicted Top-1 accuracy and true zero-shot image classification accuracy $p_{m,\\tau}$ is measured by the Kendall Rank Correlation $\\tau$ , higher is better) and Mean Absolute Error (MAE, lower is better). It can be observed from the left part of Figure 2 that the predicted accuracy derived from auxiliary texts ${\\tilde{D}}_{7}$ does not closely match the true accuracy, indicating that these generated auxiliary texts in $\\tilde{D}_{7}$ are not effective proxies for images. ", "page_idx": 4}, {"type": "text", "text": "To make the auxiliary texts act as better image proxies, one intuitive idea is to estimate the gap vector $\\textbf{\\textit{g}}$ for each image-text pair. Then we can add it to the feature $f_{m}^{T}(t_{i})$ of the text $t_{i}$ to eliminate tvheec tomr ocdaanlintoyt  gbae pc, alwchuilcathe dm daiyr elcetaldy  twoi thmoourte  tahce ctuarragteet  sdcatoarseest $s_{m,i}^{\\mathcal{T}}$ aign esE.q Fuuatritohenr 3m. orHe,o gwaepv evre, ctthoer s gfaopr different classes are diverse, so using a shared vector across all datasets may not be a good choice. ", "page_idx": 4}, {"type": "text", "text": "Capability Gap. To select one VLM from the model zoo given a target dataset, one direct approach is to select the VLM that performs the best on average across multiple datasets. For example, we may first estimate the VLM\u2019s zero-shot classification ability on open-source datasets and then select the VLM with the highest performance. The key question is whether a VLM\u2019s average ranking on the open-source datasets can reveal its true ranking on the target dataset. Our empirical analyses indicate that there exists a discrepancy between the VLM\u2019s overall ranking and its ranking on a specific dataset. We name the discrepancy between the VLM\u2019s average ability and its specific ability as the Capability Gap, which results from the fact that a VLM\u2019s performance fluctuates significantly across various datasets. ", "page_idx": 4}, {"type": "text", "text": "To verify the claim, we test 43 VLMs on 23 target datasets provided by [64] and obtain the rankings of each VLM across these datasets. Based on these ranking results, we calculate the average standard deviation and the mean value of the difference between each VLM\u2019s maximum and minimum ranking. The experiment process is illustrated in the right part of Figure 2. We find that the mean difference between one VLM\u2019s maximum and the minimum ranking is 38.86. Since the total number of VLMs is 43, such a difference demonstrates that the top-performing VLM in one dataset could likely be among the worst in another. ", "page_idx": 4}, {"type": "text", "text": "One solution to bridge such a capability gap is to consider the VLMs\u2019 ranking on a related dataset. In other words, the ranking of VLMs on datasets from open-source datasets collections that are relevant to the target task may provide more useful insights than a general performance ranking across all tasks. The main challenge is to figure out which open-source datasets are similar to the target dataset and transform the VLM\u2019s ranking on these datasets to the target dataset. ", "page_idx": 4}, {"type": "image", "img_path": "01qa1ZJs65/tmp/1236e5e7ad705215d7bf64a72e1d1da6c7e27a2d1740d93e6515325645ed0959.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: The workflow of SWAB. SWAB first constructs a transport matrix $\\gamma^{\\ast}\\in\\mathbb{R}^{k_{S}\\times k_{T}}$ using optimal transport, based on textual semantic similarity between classes in the open-source datasets $\\dot{C}_{S}=\\{c_{1}^{S},\\dot{\\cdot}\\dot{\\cdot},c_{k_{S}}^{S}\\}$ and the target dataset\u2019s classes $\\dot{C_{T}}=\\{c_{1}^{T},\\cdots,c_{k_{T}}^{T}\\}$ . Using this matrix, SWAB estimates VLM $f_{m}$ \u2019s class-specific gap vectors $\\{\\hat{g}_{m,1}^{\\mathcal{T}},\\cdot\\cdot\\cdot\\}$ on the target dataset $\\tau$ from the gap vectors $G_{m}^{S}\\in\\mathbb{R}^{k_{S}\\times d}$ in the open-source datasets. These estimated gap vectors help modify text data to act as more effective substitutes for image data. The modified text data will then be input into the Ranker Model $f_{R}$ , which predicts VLM\u2019s performance $\\hat{r}_{m}^{\\tau,(1)}$ on the target dataset. Besides, SWAB also uses the transport matrix $\\gamma^{*}$ to predict VLM\u2019s performance ranking on the target dataset based on VLM\u2019s class-specific rankings $\\boldsymbol{r}_{m}^{s}\\in\\mathbb{R}^{k s}$ on open-source datasets. Finally, SWAB combines these two ranking predictions $\\hat{r}_{m}^{\\mathcal{T},(1)}$ and r\u02c6Tm ,(2)to determine the VLM\u2019s final ranking prediction. ", "page_idx": 5}, {"type": "text", "text": "Summary. We emphasize two kinds of gaps in LOVM, i.e., the modality gap across features of different modalities generated by a VLM, and the capability gap between a VLM\u2019s overall ranking and its ranking given a specific target dataset. Both two gaps pose obstacles to previous model selection methods, such as LogME and ModelGPT, and degrade their abilities in VLM selection. Moreover, those intuitive approaches to bridge the gaps still face challenges. ", "page_idx": 5}, {"type": "text", "text": "3 VLM Selection with Gap Bridging ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To mitigate the impact of both gaps on LOVM and integrate non-learning-based and learning-based LOVM methods, we propose VLM Selection With gAp Bridging (SWAB). The key idea of SWAB is to bridge modality and capability gaps by utilizing class-level statistics of VLMs from open-source datasets. By measuring the textual similarity between the target dataset\u2019s class names and those in open-source datasets, we construct a bridge matrix. Based on it, we estimate the gap vectors between image and text modalities, which rectifies the text-derived scores in ModelGPT. In addition, we predict the VLM\u2019s performance ranking for the target dataset based on the bridge matrix and VLM\u2019s ranking on the open-source dataset. Both estimated statistics will be used to obtain a more accurate language-only VLM selection. The workflow of SWAB is illustrated in Figure 3. ", "page_idx": 5}, {"type": "text", "text": "3.1 Construct the Bridge Matrix Using Optimal Transport ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Benefiting from the open-source datasets, some useful class-level statistics, such as modality gap vectors and zero-shot classification accuracy of a certain VLM, could be calculated, which can help the performance ranking estimation of a VLM on the target dataset. To better utilize these class-level statistics for predicting the corresponding statistics of a VLM on the target task, we introduce semantic relevance information between open-source datasets\u2019 classes and target dataset\u2019s classes into the statistics reusing process, which is automatically generated through Optimal Transport [7, 42]. ", "page_idx": 5}, {"type": "text", "text": "Recall that the sets of class names of the open-source datasets and the target dataset are $C_{\\cal S}=\\{c_{i}^{S}\\}_{i=1}^{k_{S}}$ and $C_{\\mathcal{T}}=\\{c_{i}^{T}\\}_{i=1}^{k_{\\mathcal{T}}}$ , respectively. The semantic relevance between two classes could be measured by the textual similarity between their class names. In detail, we use a pre-trained text encoder $\\phi$ (e.g., MPNet [46]), which extracts text features for these class names, i.e., $\\{\\phi(c_{1}^{S}),\\cdot\\cdot\\cdot\\,,\\phi(c_{k_{S}}^{S})\\}$ and $\\{\\phi(c_{1}^{T}),\\cdot\\cdot\\cdot\\ ,\\phi(c_{k_{T}}^{T})\\}$ . Then, we can calculate the cosine similarity $\\frac{{\\phi}(c_{i}^{S})^{\\top}{\\phi}(c_{j}^{T})}{\\|{\\phi}(c_{i}^{S})\\|\\cdot\\|{\\phi}(c_{j}^{T})\\|}$ between the text feature of the $i^{\\th}$ -th class in the open-source datasets $\\phi(c_{i}^{S})$ and that of the $j$ -th class in the target dataset $\\phi(c_{j}^{T})$ . The larger the cosine similarity, the more similar the two classes are. After that, we construct the cost matrix in optimal transport via costij = 1 \u2212\u2225\u03d5\u03d5((ccSi  ))\u2225\u00b7\u2225\u03d5\u03d5((ccjT  ))\u2225. In practice, we exponentiate each element in cost $\\in\\mathbb{R}_{\\geq0}^{k s\\times k_{T}}$ using the base $e$ to amplify its differences. We then solve the optimal transport problem with the constructed cost matrix to get the transport matrix $\\gamma^{*}$ . Since optimal transport aims to obtain a transport matrix that minimizes the transmission cost, the transport matrix $\\gamma^{*}$ will reuse more information between semantically similar classes: ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\gamma^{*}=\\operatorname*{\\underset{\\gamma\\in\\mathbb{R}_{\\geq0}^{k_{S}\\times k_{T}}}{\\operatorname*{argmin}}}\\sum_{i,j}\\gamma_{i,j}\\,\\cos\\!\\mathrm{t}_{i,j}\\ ,\\ \\mathrm{s.t.}\\ \\gamma\\mathbf{1}=u;\\ \\gamma^{T}\\mathbf{1}=v;\\ \\gamma_{i,j}\\geq0.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The cost matrix quantifies the expense of moving elements between all class pairs, and $\\gamma^{*}\\in\\mathbb{R}^{k_{S}\\times k_{T}}$ is the transport matrix. OT minimizes the cost indicated by the matrix cost and moves elements from one distribution $\\textbf{\\em u}$ to another $\\pmb{v}$ . In SWAB, we define $\\textbf{\\em u}$ and $\\pmb{v}$ as uniformly distributed vector ${\\pmb u}={\\bf1}/k_{S}\\in\\mathbb{R}^{k_{S}}$ and $\\pmb{v}=\\mathbf{1}/k_{T}\\in\\mathbb{R}^{k_{T}}$ . This indicates that we treat all classes as equally important. We may also incorporate prior knowledge of class importance to define $\\textbf{\\em u}$ and $\\pmb{v}$ . ", "page_idx": 6}, {"type": "text", "text": "The solution $\\gamma^{*}$ of the OT problem in Equation 5 could be solved efficiently [13], and $\\gamma^{*}$ acts as a bridge matrix between open-source datasets\u2019 classes and target dataset\u2019s classes. Usually, the smaller $\\mathrm{cost}_{i,j}$ is, the larger the corresponding element $\\gamma_{i,j}^{*}$ obtained by OT, indicating statistics of the $i$ -th class of open-source datasets may help more when we estimate the statistics of the $j$ -th target class. ", "page_idx": 6}, {"type": "text", "text": "3.2 Bridge the Modality Gap and Capability Gap ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Bridge the Modality Gap. Given the $m$ -th VLM $f_{m}$ in the model zoo, we want to estimate the modality gap $\\pmb{g}_{m,j}^{\\mathcal{T}}$ between the extracted image and text features for the $j$ -th class in the target dataset $\\tau$ to bridge the modality gap. However, in the zero-shot scenario, we can\u2019t get the target dataset\u2019s images in advance, so we can\u2019t directly calculate the gap vectors using image-text pairs. To solve this problem, SWAB estimates the target dataset\u2019s gap vectors based on the open-source datasets\u2019 gap vectors with $\\gamma^{*}$ . Given the $k$ -th open-source class $c_{k}^{S}$ , we can get the set of images $D_{S_{k}}^{I}=\\left\\{\\left(x_{i},y_{i}\\right)\\mid\\left(x_{i},y_{i}\\right)\\in D_{S}^{I}$ , $y_{i}=c_{k}^{S}\\}$ from the open-source datasets. $|D_{S_{k}}^{I}|$ is the number of images in $D_{S_{k}}^{I}$ . Then, the modality gap vector $\\pmb{g}_{m,k}^{S}$ for class $c_{k}^{S}$ and model $f_{m}$ can be calculated through gSm,k $\\pmb{g}_{m,k}^{S}=\\frac{1}{|D_{s_{k}}^{I}|}\\sum_{(\\pmb{x}_{i},y_{i})\\in D_{S_{k}}^{I}}\\left(\\frac{f_{m}^{I}(\\pmb{x}_{i})}{\\|f_{m}^{I}(\\pmb{x}_{i})\\|}-\\frac{f_{m}^{T}\\left(c_{k}^{S}\\right)}{\\|f_{m}^{T}\\left(c_{k}^{S}\\right)\\|}\\right)\\cdot\\pmb{g}_{m,k}^{S}$ is the average difference between the normalized class text prototype embedding and all normalized image embeddings from the class $c_{k}^{S}$ . In a similar manner, the gap vectors of all open-source classes $\\{\\breve{\\pmb{g}}_{m,1}^{S},\\cdot\\cdot\\cdot,\\pmb{g}_{m,k_{S}}^{S}\\}$ can be obtained given $f_{m}$ . We use a matrix $G_{m}^{S}\\in\\mathbb{R}^{k_{S}\\times d_{m}}$ to represent those $k_{S}$ gap vectors for the $m$ -th VLM in the VLM Zoo, and $d_{m}$ is the dimensionality of features extracted by $f_{m}$ . ", "page_idx": 6}, {"type": "text", "text": "The gap vectors $\\{\\pmb{g}_{m,1}^{T},\\cdot\\cdot\\cdot,\\pmb{g}_{m,k_{T}}^{T}\\}$ for the target dataset could be estimated based on the $G_{m}^{s}$ and the transport matrix $\\gamma^{*}$ . If two classes are semantically similar, then we can reuse the gap vector from the similar class. We set the predicted gap vector for the $j$ -th target class $\\hat{\\pmb g}_{m,j}^{\\mathcal{T}}$ as a weighted sum of $G_{m}^{s}$ , and the weight comes from $\\gamma^{*}$ , which is $\\hat{g}_{m,j}^{T}=|C_{T}|(\\gamma_{:,j}^{*})^{\\top}G_{m}^{S}\\cdot\\gamma_{:,j}^{*}$ is the $j$ -th column of $\\gamma^{*}$ We use scaling factors $|C\\tau|$ to ensure that for each target class, the sum of $\\boldsymbol{\\gamma}_{:,j}^{*}$ equals 1. This scale operation has also been used in previous work [55, 56]. After that, we modify the step of ModelGPT in Equation 3, where the metrics over the generated auxiliary texts ${\\tilde{D}}_{7}$ are calculated. We add the gap vector $\\hat{\\pmb g}_{m,j}^{\\mathcal{T}}$ to the embeddings of the auxiliary texts ${\\tilde{D}}_{\\mathcal{T}}^{j}$ from the $j$ -th class in the target dataset: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{t}_{m,i}=f_{m}^{T}(t_{i})+\\hat{\\pmb g}_{m,j}^{T}\\ ,\\ \\ \\forall t_{i}\\in\\tilde{D}_{T}^{j}\\ .\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The modified text embedding $\\tilde{t}_{m,i}$ serves as better image proxies. In other words, classification metrics on $f_{m}^{T}(t_{i})$ only reveal the discerning ability of the text encoder of $f_{m}$ , which is far from the (cross-modal) zero-shot classification ability due to the modality gap. By bridging such a gap with modified text embedding, classification metrics on $\\tilde{t}_{m,i}$ are closer to the classification metrics on images with textual classifier. Therefore, we use $\\{\\tilde{t}_{m,1},\\cdots\\}$ in Equation 6 as better inputs to calculate $s_{m}^{\\mathcal{T}}$ in Equation 3. The updated score vectors $s_{m}^{\\tau}$ are then input to the ranker model $f_{R}$ , which is able to get more accurate VLM\u2019s performance prediction $\\hat{p}_{m}^{\\mathcal{T}}$ . Based on the performance prediction $\\{\\hat{p}_{m}^{\\tau}\\}_{m=1}^{M}$ , we can obtain the VLMs\u2019 ranking $\\hat{\\pmb{r}}^{T,(1)}=[\\hat{r}_{1}^{T,(\\ddot{1})},\\cdot\\cdot\\cdot\\ ,\\hat{r}_{M}^{T,(1)}]=$ Ranking $\\left(\\left[\\hat{p}_{1}^{\\mathcal{T}},\\cdots,p_{M}^{\\mathcal{T}}\\right]\\right)$ . Ranking $(\\cdot)$ transforms the accuracies into models\u2019 ranking. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Bridge the Capability Gap. Whether the $m$ -th VLM $f_{m}$ ftis the target task $\\tau$ could also be estimated by the performance of $f_{m}$ on the open-source datasets related to $\\tau$ . We first calculate the VLM\u2019s class-level performance ranking over the whole open-source datasets. Given the $k$ -th open-source class $c_{k}^{S}$ and the corresponding set of images $D_{S_{k}}^{I}$ , we calculate the zero-shot classification accuracy $p_{m,k}^{S}$ via Equation 1. We then determine each VLM\u2019s ranking on the $k$ -th open-source class $c_{k}^{S}$ using Equation 7. We calculate VLMs\u2019 ranking for other open-source classes in the same way. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\boldsymbol{r}_{k}^{S}=[r_{1,k}^{S},\\cdot\\cdot\\cdot\\cdot,r_{M,k}^{S}]=\\mathrm{Ranking}\\left(\\left[p_{1,k}^{S},\\cdot\\cdot\\cdot,p_{M,k}^{S}\\right]\\right)\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Next, we estimate the ranking of a certain VLM $f_{m}$ on the target dataset $\\tau$ . By re-organizing the ranking values in Equation 7, the performance ranking vector of $f_{m}$ on all $k_{S}$ open-source classes is $\\pmb{r}_{m}^{S}=[r_{m,1}^{S},\\cdot\\cdot\\cdot\\;,r_{m,k s}^{S}]\\in\\mathbb{Z}_{+}^{k_{S}}$ . If $f_{m}$ ranks high on certain open-source classes related to the classes in the target dataset, the performance ranking of $f_{m}$ on the target dataset may also be high. Thus, we perform a weighted sum of ranking values in $r_{m}^{S}$ and assign larger weights to those classes related to the target dataset. This can be done by using $\\gamma^{*}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{r}_{m}^{\\mathcal{T}}=r_{m}^{S}\\,\\gamma^{*}\\mathrm{~.~}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Elements in $\\hat{r}_{m}^{\\mathcal{T}}\\in\\mathbb{R}^{k_{\\mathcal{T}}}$ are the predicted ranking of the $\\mathbf{m}$ -th VLM $f_{m}$ for $k_{T}$ target classes. Since we only need the relative order of ranks, there is no additional scale factor in Equation 8. After that, we average class-specific ranking values in $\\hat{\\pmb{r}}_{m}^{\\mathcal{T}}$ , and use $\\hat{r}_{m}^{\\mathcal{T},(2)}=\\operatorname{Mean}\\left(\\hat{r}_{m}^{\\bar{T}}\\right)$ to denote the estimated ranking of $f_{m}$ on the target dataset . In summary, we predict the ranking of models on each class in the target dataset based on their rankings in classes of the open-source datasets, guided by the semantic relevance between the classes. We predict the ranking of models on each category in the target dataset based on their rankings in categories of the open-source dataset, guided by the semantic relevance between the categories. Benefiting from the models\u2019 consistently aligned classification performance ranking across similar classes, this approach bridges the capability gap in VLM selection. ", "page_idx": 7}, {"type": "text", "text": "3.3 Summary of SWAB ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As is described in Section 3.2, given a target dataset $\\tau$ and a VLM $f_{m}$ , we denote the two performance ranking predictions obtained by bridging the modality gap and capability gap as $\\hat{r}_{m}^{\\mathcal{T},(1)}$ )and r\u02c6Tm ,(2), respectively. $\\hat{r}_{m}^{\\mathcal{T},(1)}$ is predicted based on ModelGPT with our modified embeddings of the generated auxiliary texts for $\\tau$ . $\\bar{r}_{m}^{\\mathcal{T},(2)}$ is predicted by the weighted sum of VLM\u2019s class-specific ranking values on the open-source datasets. These two predictions, respectively, originate from learning-based and non-learning-based LOVM methods. We ensemble two predictions together and achieve a more accurate model ranking estimation. We utilize the weighted Borda Count to aggregate two rankings: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{r}_{m}^{\\mathcal{T},\\mathrm{ens}}=\\alpha\\cdot\\hat{r}_{m}^{\\mathcal{T},(1)}+(1-\\alpha)\\cdot\\hat{r}_{m}^{\\mathcal{T},(2)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We set $\\alpha=0.5$ . Ultimately, SWAB determines the final predicted ranking of the VLMs in the VLM Zoo based on $\\hat{r}_{m}^{\\tau,\\mathrm{ens}}$ . The pseudo-code of SWAB is listed in Algorithm 1. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Evaluation on LOVM Benchmark ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Setups. We follow LOVM [64] to use its provided VLM Zoo. In addition, to further enhance the diversity of the VLM Zoo, we add some representative VLMs such as BLIP [28] and BEiT3 [52] to expand the VLM Zoo. The final VLM Zoo contains a total of 43 models, which differ in aspects such as model architecture, pre-training datasets, and training methods. In the appendix, we also provide experimental results of different model selection methods on the 35 models originally offered by LOVM. We evaluate different methods on 23 datasets, i.e. ImageNet [8], Aircraft [34], ", "page_idx": 7}, {"type": "text", "text": "Table 1: Results on LOVM Benchmark. We evaluate our method across 23 datasets and 43 pretrained VLMs. The results are averaged over all datasets. Our SWAB achieves the best results across all metrics. For methods that involve adding random noise to data features, we report the standard deviation of metrics across 10 experiments to mitigate the impact of randomness on result reliability. ", "page_idx": 8}, {"type": "table", "img_path": "01qa1ZJs65/tmp/e3aee3cd73e3cd04cdc8089a966ecc1916b431cac4acb5a933b30d55fe8eac2f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "CIFAR100 [26] and so on. We obtain VLM\u2019s ground truth ranking based on VLM\u2019s Top-1 Accuracy calculated on the target dataset\u2019s test image set. ", "page_idx": 8}, {"type": "text", "text": "Baseline. We select representative methods for each of the three paradigms mentioned in Section 2.2 as our baselines. For the first paradigm, we use four classic model selection methods: H-Score [2], NCE [48], LEEP [49] and LogME [58]. For the second paradigm, we use the VLM\u2019s ranking on ImageNet (INB) and VLM\u2019s average ranking (Avg Rank) on classes of the open-source datasets in the LOVM Benchmark. For the third paradigm, we compare our method with ModelGPT [64]. ", "page_idx": 8}, {"type": "text", "text": "Evaluations. We use Top-5 Recall and Kendall\u2019s Rank Correlation to measure the similarity between the predicted and the ground truth model rankings to evaluate the LOVM method\u2019s performance. We also calculate the sum of these two metrics to consider the method\u2019s comprehensive capability. ", "page_idx": 8}, {"type": "text", "text": "Implementation Details. For a fair comparison, SWAB follow ModelGPT [64] to sequentially extract a target dataset from each of the 23 datasets in the LOVM Benchmark and treat the remaining datasets as open-source datasets. Besides, SWAB adopts ModelGPT\u2019s approach of adding Gaussian noise to corrupt the target dataset\u2019s generated text embeddings. Since LOVM does not provide the specific image data for the 23 datasets, we download these datasets ourselves and adopt their standard data splits. Using the templates provided by LOVM, we construct classifiers and recalculate each VLM\u2019s zero-shot image classification accuracy on these datasets. Additionally, we utilize the code provided by LOVM to generate class-related text data using ChatGPT. For H-Score, NCE, LEEP, LogME, INB, and Avg Rank, we follow the practices of previous work and do not add noise to the model\u2019s inputs. To ensure reliable results, we conduct ten repeated experiments using random seeds from 1 to 10 and report the mean value and standard deviation of ModelGPT\u2019s performance and SWAB\u2019s performance in Table 1. ", "page_idx": 8}, {"type": "text", "text": "Results Analysis. From Table 1, we can draw the following conclusions: (1) Metric-based nonlearning model selection methods such as LogME show poor performance on the LOVM Benchmark. This is primarily because such algorithms rely on the target dataset\u2019s images, thus the modality gap has a greater negative impact on them when using generated text data as a substitute for images. (2) Using open-source datasets is helpful for LOVM. We find that using open-source datasets in a non-learning way (e.g. INB, Avg Rank) or a learning way (e.g. ModelGPT) all helps LOVM, since their performance significantly surpasses that of methods not utilizing open-source datasets (e.g. LogME). (3) Despite leveraging more open-source datasets, the performance of Average Rank is worse than INB. This confirms our analysis of the Capability Gap, which suggests a discrepancy between the average ranking of a VLM and its ranking on a specific dataset. (4) Our SWAB achieves the best performance across all evaluation metrics. Notably, our final performance of $R_{5}+\\tau$ (0.822) represents a significant improvement of $14.8\\%$ over the original SoTA method ModelGPT (0.716). ", "page_idx": 8}, {"type": "text", "text": "4.2 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct an ablation study to demonstrate that bridging the Modality Gap and Capability Gap are both essential for SWAB. Table 2 presents our experiment results, from which we can observe that SWAB achieves the best performance across all metrics when both gaps are bridged simultaneously. The ablation study confirms our analysis. ", "page_idx": 8}, {"type": "table", "img_path": "01qa1ZJs65/tmp/ceb88e99e2b63473baacd3e37f29ae94e9716974a69b6e7326f5cc38334a7e02.jpg", "table_caption": ["Table 2: Ablation Study of SWAB. SWAB-C, SWAB-M, and SWAB indicates only bridging the Capability Gap, only bridging the Modality Gap, and bridging both gaps in SWAB. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Influence of Key Components in SWAB ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Will Bridge the Capability Gap Be Beneficial for VLM Selection? We compare the LOVM performance directly using the VLM\u2019s average ranking on each class of open-source datasets and weighted-sum ranking based on transport matrix $\\gamma^{*}$ . The results are shown in Table 3. We can find that utilizing class relevance to bridge the Capability Gap is beneficial for VLM\u2019s Model Selection. ", "page_idx": 9}, {"type": "text", "text": "Will Bridge the Modality Gap Be Beneficial for VLM Selection? To eliminate the interference of other factors, we solely utilize the learning-based predicted rankings $\\hat{r}_{m}^{\\mathcal{T},(1)}$ in SWAB, and the input to the ranker model $f_{m}$ only consists of metrics calculated on the generated text data ${\\tilde{D}}_{7}$ , which serves as substitutes for images. In this way, the method\u2019s performance depends solely on the quality of the generated text data ${\\tilde{D}}_{7}$ . From the Table 4, we can find that the generated text data ${\\tilde{D}}_{7}$ become better substitutes for image data after bridging the Modality Gap. ", "page_idx": 9}, {"type": "table", "img_path": "01qa1ZJs65/tmp/be19ba5bc864b12f3b610fc79d953157f64b040168049d0a4023a1c0bf7f0f62.jpg", "table_caption": ["Table 3: Results of r\u02c6Tm ,(2) on the LOVM before and after bridging the Capability Gap. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "01qa1ZJs65/tmp/80f33dfe0980e13231eef02bdee70f170809b5fbb0d9d12cadddd1fa39dc312a.jpg", "table_caption": ["Table 4: Results of $\\hat{r}_{m}^{\\mathcal{T},(2)}$ before and after bridging the Modality Gap (MG). "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Which Kind of Gap Vectors Should We Use? When utilizing the gap vectors from open-source datasets, we have two options: (1) Use the dataset-level mean gap vector calculated on the whole dataset\u2019s image-text pairs. (2) Use the class-level mean gap vector calculated on the corresponding class\u2019s image-text pairs. We hope that the gap vectors are as close to each other as possible so that their mean vector can substitute well for the whole set. Based on this idea, we calculate the statistics of the gap vectors within a dataset and within each class. We calculate three metrics which include: (1) the standard deviation of these gap vectors\u2019 magnitude; (2) the mean cosine similarity between these gap vectors and their corresponding mean gap vectors; and (3) the standard deviation of these cosine similarities. These metrics reflect the consistency of the gap vectors. Table 5 shows the results. ", "page_idx": 9}, {"type": "table", "img_path": "01qa1ZJs65/tmp/7d0367dd6c1732d9aae211e1906360ed42fce3d4659a01c60cbc77df52a5e620.jpg", "table_caption": ["Table 5: Results of metrics measuring gap vectors\u2019 consistency belonging to the same dataset or the same class. M: Magnitude, D: Direction. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "01qa1ZJs65/tmp/97aa8a240797256cdd0402c352ac68f76c78693d1cbd2d8676e83a1c726bb74e.jpg", "table_caption": ["Table 6: Results of SWAB-M on the LOVM Benchmark using the dataset-level mean gap vectors and class-level mean gap vectors. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "From the Table 5, we can find that the class-level gap vectors tend to be more consistent, which inspires us to use the class-level mean gap vectors. We also compare the results of SWAB-M on the LOVM Benchmark using the dataset-level mean gap vectors and the class-level mean gap vectors, respectively. The implementation details are the same as Table 2. Table 6 shows the results, which verifies that using the class-level mean gap vectors is a better choice. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We analyze and address two key challenges in Language-Only VLM Selection (LOVM), which are VLM\u2019s modality gap across different modal features and VLM\u2019s Capability gap between its overall and dataset-specific rankings. Our key insight is that we can reuse the model\u2019s useful statistics on open-source datasets to help the model selection on the target dataset. SWAB utilizes a transport matrix between classes of the target dataset and open-source datasets to transfer VLM\u2019s class-specific modality gap vectors and class-specific rank from open-source datasets to the target dataset, which mitigates the negative impacts of these two gaps. Experiment results on the LOVM benchmark show the superiority of our method. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is partially supported by National Key R&D Program of China (2022ZD0114805), NSFC (62376118, 62250069, 62006112, 61921006), Collaborative Innovation Center of Novel Software Technology and Industrialization, CCF-Tencent Rhino-Bird Open Research Fund (RAGR20240101). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C. Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In ICCV, 2019.   \n[2] Yajie Bao, Yang Li, Shao-Lun Huang, Lin Zhang, Lizhong Zheng, Amir Zamir, and Leonidas J. Guibas. An information-theoretic approach to transferability in task transfer learning. In ICIP, 2019.   \n[3] Wei-Lun Chao, Han-Jia Ye, De-Chuan Zhan, Mark E. Campbell, and Kilian Q. Weinberger. Revisiting meta-learning as supervised learning. CoRR, 2020. URL https://arxiv.org/abs/2002.00573. [4] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE, 2017.   \n[5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014.   \n[6] Adam Coates, Andrew Y. Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In AISTATS, 2011.   \n[7] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NeurIPS, 2013. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.   \n[9] Nan Ding, Xi Chen, Tomer Levinboim, Soravit Changpinyo, and Radu Soricut. Pactran: Pac-bayesian metrics for estimating the transferability of pretrained models to classification tasks. In ECCV, 2022.   \n[10] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge 2007 (voc2007) results. http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html, 2007.   \n[11] Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar, Achal Dave, and Ludwig Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In ICML, 2022.   \n[12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.   \n[13] R\u00e9mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aur\u00e9lie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L\u00e9o Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. JMLR, 2021.   \n[14] Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, and Jiaping Zhao. Improving zero-shot generalization and robustness of multi-modal models. In CVPR, 2023.   \n[15] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. IJRR, 2013.   \n[16] Dumitru Ian Goodfellow, Will Cukierski, and Yoshua Bengio. Challenges in representation learning: Facial expression recognition challenge, 2013.   \n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.   \n[18] Xiaoxuan He, Siming Fu, Xinpeng Ding, Yuchen Cao, and Hualiang Wang. Uniformly distributed category prototype-guided vision-language framework for long-tail recognition. In ACM MM, 2023.   \n[19] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. J-STARS, 2019.   \n[20] Long-Kai Huang, Junzhou Huang, Yu Rong, Qiang Yang, and Ying Wei. Frustratingly easy transferability estimation. In ICML, 2022.   \n[21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. URL https://doi.org/10.5281/zenodo.5143773.   \n[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021.   \n[23] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Fei-Fei Li, C. Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017.   \n[24] Kaggle and EyePacs. Kaggle diabetic retinopathy detection, 2015. URL https://www.kaggle.com/c/ diabetic-retinopathy-detection/data.   \n[25] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), 2013.   \n[26] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, 2009.   \n[27] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs, 2010.   \n[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022.   \n[29] Li Li, Jiawei Peng, Huiyi Chen, Chongyang Gao, and Xu Yang. How to configure good in-context sequence for visual question answering. In CVPR, 2024.   \n[30] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Y. Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. In NeurIPS, 2022.   \n[31] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.   \n[32] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In CVPR, 2022.   \n[33] Teli Ma, Shijie Geng, Mengmeng Wang, Jing Shao, Jiasen Lu, Hongsheng Li, Peng Gao, and Yu Qiao. A simple long-tailed recognition baseline via vision-language model. CoRR, abs/2111.14745, 2021.   \n[34] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. CoRR, abs/1306.5151, 2013.   \n[35] Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit Menon, Junfeng Yang, Xin Wang, and Carl Vondrick. Doubly right object recognition: A why prompt for visual rationales. In CVPR, 2023.   \n[36] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Gro\u00dfberger. UMAP: uniform manifold approximation and projection. J. Open Source Softw., 2018.   \n[37] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. In ICLR, 2023.   \n[38] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NeurIPS Workshop, 2011.   \n[39] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, 2008.   \n[40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. NeurIPS, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[41] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012. ", "page_idx": 12}, {"type": "text", "text": "[42] Gabriel Peyr\u00e9 and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine Learning, 2019.   \n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.   \n[44] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-flitered 400 million image-text pair. CoRR, abs/2111.02114, 2021.   \n[45] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In CVPR, 2022.   \n[46] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. In NeurIPS, pages 16857\u201316867, 2020.   \n[47] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In IJCNN, 2011.   \n[48] Anh Tuan Tran, Cuong V. Nguyen, and Tal Hassner. Transferability and hardness of supervised classification tasks. In ICCV, 2019.   \n[49] Anh Tuan Tran, Cuong V. Nguyen, and Tal Hassner. Leep: A new measure to evaluate transferability of learned representations. In ICML, 2020.   \n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.   \n[51] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology, 2018.   \n[52] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: BEiT pretraining for vision and vision-language tasks. In CVPR, 2023.   \n[53] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.   \n[54] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context configurations for image captioning. In NeurIPS, 2023.   \n[55] Han-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Rectify heterogeneous models with semantic mapping. In ICML, 2018.   \n[56] Han-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Heterogeneous few-shot model rectification with semantic mapping. TPAMI, 2021.   \n[57] Chao Yi, Lu Ren, De-Chuan Zhan, and Han-Jia Ye. Leveraging cross-modal neighbor representation for improved clip classification. In CVPR, 2024.   \n[58] Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. Logme: Practical assessment of pre-trained models for transfer learning. In ICML, 2021.   \n[59] Kaichao You, Yong Liu, Ziyang Zhang, Jianmin Wang, Michael I. Jordan, and Mingsheng Long. Ranking and tuning pre-trained models: a new paradigm for exploiting model hubs. JMLR, 2022.   \n[60] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. TMLR, 2022.   \n[61] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang. Florence: A new foundation model for computer vision. CoRR, abs/2111.11432, 2021.   \n[62] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark, 2020.   \n[63] Yi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, and Han-Jia Ye. Model spider: Learning to rank pre-trained models efficiently. NeurIPS, 2023.   \n[64] Orr Zohar, Shih-Cheng Huang, Kuan-Chieh Wang, and Serena Yeung. Lovm: Language-only vision model selection. In NeurIPS, 2023. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "In the Appendix, we introduce more details about the LOVM Benchmark as well as our extensions to it. Besides, we introduce ModelGPT\u2019s implementation and our SWAB\u2019s implementation. We also provide more experimental results of SWAB. The structure of the Appendix is as follows: ", "page_idx": 14}, {"type": "text", "text": "\u2022 In section A, we introduce the relevant information of the 43 models and 23 datasets used in our experiments. We also introduce the evaluation metrics used in LOVM Benchmark [64].   \n\u2022 In section B, we introduce the metrics used in Equation 3.   \n\u2022 In section C, we introduce the related work of the paper.   \n\u2022 In section D, we provide some details on SWAB\u2019s implementation.   \n\u2022 In section E, we provide more experimental results of SWAB. ", "page_idx": 14}, {"type": "text", "text": "A LOVM Benchmark Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "LOVM Benchmark [64] consists of 35 pre-trained VLMs and 23 datasets, with a total of $35\\!\\times\\!23=805$ evaluations. To further enhance the diversity of the VLM Zoo, we add some representative VLMs such as BLIP [28] and BEiT-3 [52] to expand the VLM Zoo. The final VLM Zoo contains a total of 43 models, with a total of $43\\times23=989$ evaluations. For each evaluation, LOVM provides the VLM\u2019s zero-shot image classification accuracy on the corresponding dataset. Therefore, we can get the ground truth performance ranking of 43 VLMs on the 23 datasets. ", "page_idx": 14}, {"type": "text", "text": "A.1 VLMs of LOVM Benchmark ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To cover as many types of models as possible, the LOVM Benchmark uses OpenCLIP library [21] to get diverse VLMs. These VLMs differ from each other in terms of the model architecture (ResNet [17], Transformer [50], ConvNext [32]), the pre-trained dataset (OpenAI\u2019s Data [43], LAION 2b [44]), the training method (loss function/hyperparameter/data augmentation) and so on. Table 7 displays the relevant information of each VLM. We further add BLIP and BEiT-3 to the original VLM Zoo. It should be noted that in addition to the image-text pair data listed in the Table 7, BEiT-3\u2019s pre-training data also includes unimodal image dataset (ImageNet-21K) and text datasets (English Wikipedia, BookCorpus, OpenWebText, CC-News, Stories). The diversity of these VLMs ensures that the experimental results calculated on them can reflect the performance of the VLM model selection algorithm in real-world situations. ", "page_idx": 14}, {"type": "text", "text": "A.2 Datasets of LOVM Benchmark ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "To cover as wide a distribution of image classification tasks as possible, the LOVM Benchmark collects 23 diverse datasets. These datasets differ from each other in terms of the number of categories, category semantics, image domains, and so on. Table 8 displays the relevant information of each dataset. The diversity of these tasks ensures that the experimental results calculated on them can reflect the performance of the VLM model selection method in real-world situations. ", "page_idx": 14}, {"type": "text", "text": "A.3 Evaluation Metrics of LOVM Benchmark ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In LOVM, our aim is to maximize the rank similarity between the prediction of VLMs\u2019 ranking r\u02c6T = {r\u02c6Tm}mM=1 ya nodf  tVheL tMops \u20195  gVroLuMnds  itrn raanndk . $r_{\\mathcal{T}}\\dot{=}\\{r_{m}^{\\mathcal{T}}\\}_{m=1}^{M}$ woen  ttehned  ttaor gfoetc udsa toansleyt , oens pwehceitahlleyr $\\hat{r}_{7}$ $r_{7}$   \nthose appropriate models can be chosen. To ensure fair comparability, we follow the evaluation metrics used by LOVM [64] to assess different model selection methods and directly utilize the code provided by LOVM [64] to calculate these metrics: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Top-5 Recall $(R_{5})-\\mathrm{Top}{-5}$ Recall $R_{5}$ measures the model selection algorithm\u2019s accuracy in identifying the true top five best-performing models within its predicted top five models. The calculation method is shown in Equation 11. Here $\\mathrm{IND}(\\hat{r}_{\\mathcal{T}}^{5})$ and $\\mathrm{IND}(\\dot{\\pmb{r}}_{T}^{5})$ indicates the model indices sets of the top 5 VLMs in $\\hat{r}_{7}$ and $r_{7}$ , respectively. A Top 5 Recall closer to 1 signifies greater accuracy in the predicted rankings. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{F=\\mathrm{IND}(\\hat{r}_{\\mathcal{T}}^{5})\\cap\\mathrm{IND}(r_{\\mathcal{T}}^{5}).}\\\\ {R_{5}=\\displaystyle\\frac{|F|}{5}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "table", "img_path": "01qa1ZJs65/tmp/a8efee1d76fd9c54b107ddf97b8424d287a3fae7c0245bb533b0cab389aaf036.jpg", "table_caption": ["Table 7: The detailed information of 43 models used in the LOVM Benchmark. Some of the information in the table comes from [64]. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "01qa1ZJs65/tmp/fc5b00bfaa95c5b48b28393e1f44e228d5b085ea05940ada886fef76405edb90.jpg", "table_caption": ["Table 8: Detailed information of 23 tasks used in the LOVM Benchmark. This table comes from [64]. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "\u2022 Kendall\u2019s Rank Correlation $(\\tau)$ \u2013 Kendall\u2019s Rank Correlation $\\tau$ measures the ranking consistency between two ranking lists. We follow LOVM [64] to focus on the VLMs within the intersection of the top 5 VLMs in $\\hat{r}_{7}$ and $r_{7}$ and use $\\tau$ to evaluate a model selection method\u2019s capability. ", "page_idx": 16}, {"type": "text", "text": "B ModelGPT Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "ModelGPT is a method proposed by LOVM [64]. In this section, we introduce the metrics that ModelGPT used in Equation 3. ", "page_idx": 16}, {"type": "text", "text": "B.1 The Generation Process of Auxiliary Text Samples ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "ModelGPT [64] utilizes ChatGPT [40] to generate auxiliary text data by designing prompts to query ChatGPT. This extra text data mainly includes the Captions Dataset and the Synonyms Dataset. ", "page_idx": 16}, {"type": "text", "text": "Captions Dataset. ModelGPT uses the following prompt to guide LLM to generate realistic and confusing text data corresponding to the user-provided classes. The reason for requiring ChatGPT to generate confusing texts is to increase the classification difficulty of the text data, thereby enhancing its ability to distinguish the performance of different models. ", "page_idx": 16}, {"type": "text", "text": "Generate long and confusing image captions for the {domain} domain, which will be used to evaluate a Vision-Language Model\u2019s {task} performance. Generate 50 captions for {classname}: ", "page_idx": 16}, {"type": "text", "text": "We show some generated auxiliary text examples. For example, in the category of dog, one of the text samples generated by ChatGPT is \"An adorable dog perfect for cuddles and playtime.\" ModelGPT collects the results from this prompt to form the captions dataset, $D^{\\mathrm{cap}}$ . ", "page_idx": 16}, {"type": "text", "text": "Synonyms Dataset. ModelGPT uses synonyms to evaluate VLM\u2019s text encoder. For example, we expect an excellent VLM to extract similar embeddings for the words \u201cchair\u201d and \u201cseat\u201d. The prompt to guide LLM to generate synonyms is as follows. ", "page_idx": 16}, {"type": "text", "text": "Please list the superclasses/synonyms for {classname}. For example: chair: [furniture, seat, bench, armchair, sofa] {classname}: ", "page_idx": 16}, {"type": "text", "text": "ModelGPT collects the results from this prompt to form the synonyms dataset, $_{D^{\\mathrm{syn}}}$ . ", "page_idx": 16}, {"type": "text", "text": "B.2 Text-Derived Scores ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "ModelGPT uses six metrics for model selection, which can be divided into Text Classification scores and Dataset Granularity scores. Text Classification scores include the Text top-1 accuracy score and Text f1-score. While Granularity scores include the Fisher criterion, Silhouette score, Class Dispersion score and Synonym Consistency score. Here we focus on introducing the various metrics included in the Granularity scores. We refer to the relevant content in LOVM. ", "page_idx": 16}, {"type": "text", "text": "Fisher Criterion $\\phi_{\\mathrm{fisher}}$ . The Fisher score measures the closeness of VLM\u2019s text classifier to one another. Equation 12 shows the calculation process of it where $\\hat{t}_{i}$ is the text classifier of the $i$ -th class derived using the prompt ensemble strategies proposed in [43], $\\theta(\\cdot,\\cdot)$ is a function that calculates the cosine similarity between two vectors, and $|C|$ is the number of classes. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\phi_{\\mathrm{fisher}}=\\frac{1}{|C|}\\sum_{j=1}^{|C|}\\operatorname*{max}_{i,i\\neq j}\\left[\\theta(\\hat{t}_{i},\\hat{t}_{j})\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Silhouette Score $\\varphi_{\\mathrm{sil}}$ . The Silhouette Score measures the separation of different-class samples in the caption dataset $D^{\\mathrm{cap}}$ . To calculate it, ModelGPT averages the cosine similarity of captions to the nearest other class\u2019s classifier by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\varphi_{\\mathrm{sil}}=\\frac{1}{|C|}\\sum_{j=1}^{|C|}\\operatorname*{max}_{i,i\\neq j}\\left[\\frac{1}{N}\\sum_{k=1}^{N}\\theta(D^{\\mathrm{cap}}[j]_{k},\\hat{t}_{i})\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\hat{t}_{i}$ is the text classifier of the $i$ -th class derived using the prompt ensemble strategies proposed in [43], $\\theta(\\cdot,\\cdot)$ is a function that calculates the cosine similarity between two vectors, and $|C|$ is the number of classes. $D^{\\mathrm{cap}}[j]_{k}$ representing sample $k$ of class $j$ in the caption dataset $D^{\\mathrm{cap}}$ . There is a total of $N$ such samples for each class. ", "page_idx": 17}, {"type": "text", "text": "Class Dispersion Score $\\rho_{\\mathrm{disp}}$ . Class Dispersion Score quantifies the degree of same-class tightness or data cone radius, which is calculated using the following Equation: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\rho_{\\mathrm{disp}}=\\frac{1}{|C|N}\\sum_{i=1}^{|C|}\\sum_{k=1}^{N}\\theta(D^{\\mathrm{cap}}[i]_{k},\\hat{t}_{i}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The definitions of all symbols in Equation 14 are consistent with those in Equation 13. ", "page_idx": 17}, {"type": "text", "text": "Synonym Consistency Score $\\gamma_{\\mathrm{syn}}$ . Synonym consistency allows us to evaluate the degree of content shift between the VLMs\u2019 pre-training and target dataset. The calculation process is shown as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\gamma_{\\mathrm{syn}}=\\frac{1}{|C|N}\\sum_{i=1}^{|C|}\\sum_{k=1}^{N}\\theta(D^{\\mathrm{syn}}[i]_{k},\\hat{t}_{i}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The definitions of $\\hat{t}_{i}$ $\\dot{\\cdot}_{i},\\,\\theta(\\cdot,\\cdot),\\,|C|$ and $N$ in Equation 15 are consistent with those in Equation 13.   \n$D^{\\mathrm{syn}}[i]_{k}$ representing sample $k$ of class $i$ in the synonym dataset $_{D^{\\mathrm{syn}}}$ . ", "page_idx": 17}, {"type": "text", "text": "C Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Vision-Language Models. Vision-Language Models represent a class of multimodal models adept at correlating textual and visual information. These VLMs can comprehend rich semantics and possess strong generalization capabilities. Hence, they are often used in data-limited scenarios [54, 29]. Many VLMs are pre-trained or fine-tuned on extensive text-image pairs using loss functions such as contrastive loss, endowing them with powerful text-image matching capability. Prominent VLMs include CLIP [43], ALIGN [22], FLAVA [45], Florence [61], and CoCa [60]. These VLMs possess robust zero-shot image classification capabilities [43], which enables its widespread application in tasks characterized by long-tail distributions or those where collecting substantial training data is challenging, such as medical image analysis. Some work [37, 57] has also shown that by incorporating external knowledge, the zero-shot capabilities of CLIP can be further enhanced. In recent years, the number of open-source VLMs has been increasing [21]. Previous work [64] has pointed out that different VLMs possess varying image classification capabilities. This indicates that the performance of VLMs can vary significantly across different tasks and domains. These models with diverse capabilities constitute a VLM Zoo rich in knowledge. This VLM Zoo enables us to utilize different VLMs for various classification tasks, thereby changing the paradigm of using a single VLM to complete diverse classification tasks. This paper focuses on selecting the most suitable VLM for the target task from the VLM Zoo. ", "page_idx": 17}, {"type": "text", "text": "Model Selection. The essence of the model selection problem lies in measuring the transferability of the model to the target task. Previous model selection methods [48, 49, 58, 59, 9, 20] evaluate the PTM\u2019s transferability by performing a forward pass of the PTM on the target task\u2019s data and calculating the metric of transferability based on the forward pass\u2019s result. For example, H-Score [2], NCE [48], LEEP [49], LogME [58] estimate transferred log-likelihood, negative conditional entropy, log expectation, marginalized likelihood to obtain proxy metric of transferability, respectively. Some new methods, such as Task2Vec [1] and Model Spider [63], generate representation vectors for both the models and the tasks and measure the transferability of the model to the task by calculating the similarity between these vectors. However, VLMs are typically used in zero-shot or few-shot scenarios, where we are unable to obtain a large amount of data for the target task in advance, making traditional model selection approaches unsuitable for VLMs. Additionally, previous methods have mainly focused on single-modal models, overlooking the characteristics of VLMs. Therefore, in this paper, we concentrate on designing model selection algorithms that are suitable for scenarios with limited data and take into account the characteristics of VLMs. ", "page_idx": 17}, {"type": "text", "text": "D Implementation Details of SWAB ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide some details on the implementation of SWAB, which are not mentioned in the main text due to space constraints. ", "page_idx": 18}, {"type": "text", "text": "D.1 Filtering the Open-Source Tasks\u2019 Classes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The statistics of classes unrelated to the target class are generally not valuable for reuse. Meanwhile, when the number of classes $|C s|$ in open-source datasets $\\boldsymbol{S}$ is large, solving the optimal transport problem in Equation 5 can be time-consuming (as current optimal transport toolkits generally compute via CPU). To reduce the runtime of optimal transport, we can first filter the classes $C_{S}$ . Consider that only statistics of classes relevant to the target dataset are helpful. Therefore, we can filter out the classes in $C_{S}$ that are irrelevant to the target dataset $\\tau$ based on the class-level textual semantic similarity between the open-source datasets\u2019 classes and the target dataset\u2019s classes. This process is shown in the following Equation: ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{i j}=\\frac{\\phi(c_{i}^{S})^{\\top}\\phi(c_{j}^{T})}{\\|\\phi(c_{i}^{S})\\|\\cdot\\|\\phi(c_{j}^{T})\\|}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\nC_{S}^{'}=\\{c_{i}^{S}|\\operatorname*{max}(S_{i,:})>\\lambda\\},~|C_{s}^{'}|=k_{S}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here $S_{i}$ ,: refers to the $i$ -th row of the semantic similarity matrix calculated using Equation 16, which represents the vector formed by the similarity between the $i^{\\th}$ -th class $c_{i}^{S}$ in $C_{S}$ and each class $C_{\\mathcal{T}}=\\{c_{1}^{T},\\cdots,c_{k_{\\mathcal{T}}}^{T}\\}$ of the target task. $\\lambda$ is a threshold and we set $\\lambda=0.5$ . $k_{S}^{\\prime}$ refers to the number of classes in the filtered set $C_{S}^{'}$ . Then we use the filter classes $C_{S}^{'}$ to calculate the transport matrix $\\gamma^{*}\\in\\mathbb{R}^{k_{S}^{\\prime}\\times k_{T}}$ and continue with the following steps. ", "page_idx": 18}, {"type": "text", "text": "D.2 Using Partial Optimal Transport for Bridging the Capability Gap ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Partial optimal transport extends the optimal transport framework, enabling the selective transfer of elements from a source to a target distribution, rather than moving all elements. Its optimization problem is defined as in Equation 18. Here mass refers to the total amount of mass actually be transferred. We set $m a s s=0.9$ in our implementation. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma^{*}=\\underset{\\gamma\\in\\mathbb{R}_{+}^{k_{S}^{\\prime}\\times k_{T}}}{\\mathrm{argmin}}\\sum_{i,j}\\gamma_{i,j}\\ c o s\\mathrm{t}_{i,j}}\\\\ &{\\mathrm{s.t.}\\ \\gamma\\mathbf{1}\\leq u;\\ \\gamma^{T}\\mathbf{1}\\leq v;\\ \\gamma_{i,j}\\geq0;}\\\\ &{\\quad\\quad\\mathbf{1}^{T}\\gamma^{T}\\mathbf{1}=m a s s\\leq\\operatorname*{min}\\left\\{\\|u\\|_{1},\\|v\\|_{1}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We found that when using Equation 8 to bridge the Capability Gap, the transport matrix $\\gamma^{*}$ obtained using partial optimal transport yields better results than the one obtained using the original optimal transport via solving the Equation 5. Therefore, in our implementation, we use the transport matrix derived from partial optimal transport to bridge the capability gap. This also indicates that when estimating VLM\u2019s statistics on the target dataset, different types of statistics have different preferences for the estimation methods used. This variability is worth further investigation. ", "page_idx": 18}, {"type": "text", "text": "D.3 Data Normalization in Bridging the Modality Gap. ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "When bridging the Modality Gap as described in Section 3.2, we find that applying z-score normalization to the text and image features used in this process yields better results. Therefore, in our implementation, we normalize the features of all text and image samples during the modality bridging process using the following Equation: ", "page_idx": 18}, {"type": "equation", "text": "$$\nz={\\frac{x-\\mu}{\\sigma}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Here $\\pmb{x}\\in\\mathbb{R}^{d}$ represents the image sample\u2019s or text sample\u2019s feature, while $\\pmb{\\mu}\\in\\mathbb{R}^{d}$ and $\\pmb{\\sigma}\\in\\mathbb{R}^{d}$ are calculated using the features of all samples of the same modality within its respective dataset. ", "page_idx": 18}, {"type": "text", "text": "1: Input: Target dataset\u2019s class names $C\\tau$ , open-source datasets\u2019 class names $C_{S}$ , open-source datasets\u2019 images $D_{S}^{I}$ . 2: Use ChatGPT to generate auxiliary text data ${\\tilde{D}}_{S}$ and ${\\tilde{D}}_{7}$ based on $C_{S}$ and $C_{T}$ . 3: Calculate VLM\u2019s class-level zero-shot image classification rankings {rSm,i}mm==1M,i,i==1k and classlev ap vectors $\\{g_{m,i}^{S}\\}_{m=1,i=1}^{m=M,i=k_{S}}$ . $k_{S}$ 5: Calculate textual similarity between the current dataset\u2019s class names and other open-source datasets\u2019 class names to construct a cost matrix. 6: Solve Optimal Transport Problem based on the cost matrix to get transport matrix $\\gamma^{*}$ . 7: Use $\\gamma^{*}$ and other open-source datasets\u2019 class-level gap vectors to predict the current dataset\u2019s class-level gap vectors. Add the predicted class-level gap vectors to the corresponding text data\u2019s feature of ${\\tilde{D}}_{S}$ to get modified text data. 8: end for 9: Calculate textual similarity between open-source datasets\u2019 class names $C_{S}$ and target dataset\u2019s class names $C\\tau$ to construct cost matrix. 1101::  SUoslev $\\gamma^{*}$ aptnidm $\\{g_{m,i}^{S}\\}_{m=1,i=1}^{m=M,i=k_{S}}$ lteom p breadsiecdt  othn et hcel acsos-slt evmealt rviex cttoo rgse }mm==1M,i,i==1kT of the target $\\gamma^{*}$ ddaattaa.set. Add $\\{\\hat{g}_{m,i}^{\\mathcal{T}}\\}_{m=1,i=1}^{m=M,i=k\\tau}$ to corresponding text data\u2019s feature of ${\\tilde{D}}_{7}$ to get modified text 12: Use open-source datasets\u2019 modified text data and VLMs\u2019 ground truth zero-shot image classification performance to train the ranker model $f_{m}$ . 13: Use the ranker model $f_{m}$ to predict VLMs\u2019 rankings $\\{\\hat{r}_{m}^{\\mathcal{T},(1)}\\}_{m=1}^{m=M}$ on the target dataset based 14: oUns et $\\gamma^{*}$ aarngde $\\{r_{m,i}^{S}\\}_{m=1,i=1}^{m=M,i=k_{S}}$ dt toe pxtr eddaitcat. the VLMs\u2019 class-level zero-shot image classification rankings {r\u02c6Tm,i }mm==1M,i,i==1kT of the target dataset. 15: afgoer  peraecdhi cVtiLonM r aans ktihneg sV aLcrMo\u2019sss  oavlle rcallals sperse diinc ttihoen t arragnekti ndga toasne tt $\\{\\hat{r}_{m}^{\\mathcal{T},(2)}\\}_{m=1}^{m=M}$ eto.f $\\{\\hat{r}_{m,i}^{\\mathcal{T}}\\}_{m=1,i=1}^{m=M,i=k_{\\mathcal{T}}}$ 16: Ensemble $\\{\\hat{r}_{m}^{\\mathcal{T},(1)}\\}_{m=1}^{m=M}$ and $\\{\\hat{r}_{m}^{\\mathcal{T},(2)}\\}_{m=1}^{m=M}$ to get final predicted rankings $\\{\\hat{r}_{m}^{\\tau,e n s}\\}_{m=1}^{m=M}$ . ", "page_idx": 19}, {"type": "text", "text": "D.4 Pseudo Code of SWAB ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Algorithm 1 shows the pseudo-code of SWAB. ", "page_idx": 19}, {"type": "text", "text": "E More Experiment Results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide more experimental results of SWAB. ", "page_idx": 19}, {"type": "text", "text": "E.1 Bridging the Modality Gap Leads to Better Image Proxies ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Section 2.3 and Figure 2, we analyze whether generated text data can act as good image proxies. Our conclusion is that due to the Modality Gap, text samples cannot directly serve as an effective substitute for images in model evaluation. To demonstrate that our method SWAB can bridge this Modality Gap and thereby make text samples a better substitute for images, we conduct the following experiment. ", "page_idx": 19}, {"type": "text", "text": "From the Figure 4, it is evident that the predicted model accuracy calculated using the modified text samples is closer to the true model accuracy compared to that calculated with the original text samples. This suggests that bridging the Modality Gap leads to better image proxies. ", "page_idx": 19}, {"type": "text", "text": "We use ImageNet as our dataset. First, we employ the method introduced in Section 3.2 to predict the gap vectors for each class of the target dataset based on gap vectors calculated on open-source datasets. Then, we add the corresponding class\u2019s predicted gap vectors to the generated text data of ImageNet to bridge the modality gap. Finally, we calculate the zero-shot classification accuracy of different models on these modified text data. To measure the consistency between the predicted Top-1 accuracy and the true image classification accuracy, we calculate the Kendall Rank Correlation ( $\\tau$ , higher is better) and Mean Absolute Error (MAE, lower is better). We compare the consistency metrics of text data and modified text data. It can be observed that the consistency metrics of modified text data are better, which proves our method can reduce the gap between the generated text data and the image data. ", "page_idx": 19}, {"type": "image", "img_path": "01qa1ZJs65/tmp/ff4f476e68835a6d050ce70b8f47f2d11374c037b5b2b9d80d4308e80798c0df.jpg", "img_caption": ["Figure 4: Comparison of the consistency metrics between the accuracy calculated using text data before and after bridging the gap and the model\u2019s true accuracy. After bridging the modality gap, the text data act as better substitutes for image data to evaluate the model\u2019s performance. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "E.2 Analysis of the Modality Gap in BLIP and BEiT-3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We expand the VLM Zoo provided by LOVM by adding various variants of BLIP and BEiT-3 to enhance the diversity of the VLM Zoo. We find that the zero-shot image classification performance of the base models of BLIP and BEiT-3 is poor. Therefore, we use the retrieval version of the models provided officially. These models are obtained by fine-tuning the base models on COCO and Flickr30k through contrastive learning, thereby possessing better zero-shot image classification performance. During the pre-training of BLIP and BEiT-3, they may have used multiple loss functions in addition to the contrastive loss (BLIP), or perhaps they did not use contrastive loss at all (BEiT-3). Therefore, the existence of the Modality Gap in the feature spaces of these two models requires further investigation. ", "page_idx": 20}, {"type": "text", "text": "We use image samples from the STL-10 dataset and class-related text samples generated by ChatGPT. For each modality, we randomly extract 200 samples and calculate image-to-image, text-to-text, and image-to-text similarities, and plot histograms. We also observe the presence of Modality Gaps in different models through UMAP visualization [36]. Figure 5 and Figure 6 show the experiment result. Through our experiments, we verify that the feature spaces of the BLIP and BEiT-3 retrieval models still exhibit the Modality Gap phenomenon. ", "page_idx": 20}, {"type": "image", "img_path": "01qa1ZJs65/tmp/9ac9588a7db17f1b21be375e0a56d02b0a43d0edb908ce730efc8ffa01e25084.jpg", "img_caption": ["Figure 5: The distribution of image-to-image (i2i) cosine similarity, text-to-text (t2t) cosine similarity, and image-to-text (i2t) cosine similarity values for different BEiT-3 and BLIP models. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "01qa1ZJs65/tmp/7a55b2e07f70f41bd5254487714db90deb4ab51c5bec3b3aec13480dee1a7d83.jpg", "img_caption": ["Figure 6: UMAP visualization of image sample features and text sample features from different BEiT-3 and BLIP models. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.3 Experiment Result on LOVM\u2019s original VLM Zoo ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We have provided experimental results on the VLM Zoo originally provided by LOVM [64]. This VLM Zoo contains 35 VLMs. Table 9 shows the experimental results, and we can see that SWAB achieves the best performance across all evaluation metrics. ", "page_idx": 21}, {"type": "table", "img_path": "01qa1ZJs65/tmp/1e3d32e23dacd60b978d980579e5388e9fd26c4f2afb361dad8064657a5159e5.jpg", "table_caption": ["Table 9: Results on LOVM\u2019s original VLM Zoo. We evaluate our method across 23 datasets and 35 pre-trained VLMs. The results are averaged over all datasets. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "E.4 Per-Dataset Experiment Result ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We present the per-dataset performance comparison between our methods SWAB and ModelGPT on various datasets of LOVM benchmark in Table 10. ", "page_idx": 21}, {"type": "text", "text": "Table 10: LOVM Benchmark (top-1 accuracy).We compare the per-dataset experiment result (fixing the random seed) between ModelGPT and SWAB. ", "page_idx": 21}, {"type": "image", "img_path": "01qa1ZJs65/tmp/cd28335cb1887debe3d2cebc31e8998feebf1dc799d789c61d270e59934aa736.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Please refer to the bullet point summary at the end of the Introduction. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We did not find significant drawbacks in the method. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our work does not involve theoretical result. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide our experiment details in Section 4.1 and Appendix D. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We will open-source our code, models, and datasets in the GitHub repository. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide these details in Section 4.1. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: To ensure the reliability of our results, we conducted multiple experiments for results with randomness and reported the mean performance and standard deviation in Table 1. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We will place the relevant information in the GitHub repository. ", "page_idx": 22}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have read and fully comply with the Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our work does not involve negative social impacts. ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve these situations. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We ensure this requirement is met. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve new assets. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing and research with human subjects. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve this. ", "page_idx": 23}]