{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-14", "reason": "This paper introduces CLIP, a foundational vision-language model that is frequently used as a benchmark and heavily influences the field of zero-shot image classification, which is the central topic of the provided research paper."}, {"fullname_first_author": "Chao Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "publication_date": "2021-07-14", "reason": "This paper introduces a large-scale vision-language model that significantly advances the state-of-the-art in the field, providing a strong foundation for the current work that builds upon similar large-scale vision-language models."}, {"fullname_first_author": "Gabriel Ilharco", "paper_title": "OpenCLIP", "publication_date": "2021-07-14", "reason": "This paper provides a publicly available collection of pre-trained vision-language models, which is essential for the current work's methodology and comparative analysis of various vision-language models."}, {"fullname_first_author": "Yi-Kai Zhang", "paper_title": "Model spider: Learning to rank pre-trained models efficiently", "publication_date": "2023-12-01", "reason": "This paper addresses a similar problem of efficient model selection, providing a relevant and comparable approach that helps to contextualize the current work's contributions within the broader field."}, {"fullname_first_author": "Orr Zohar", "paper_title": "Lovm: Language-only vision model selection", "publication_date": "2023-12-01", "reason": "This paper directly addresses the same problem of language-only vision model selection as the current work, making it a crucial comparative reference for assessing the novelty and effectiveness of the proposed approach."}]}