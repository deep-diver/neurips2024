[{"figure_path": "01qa1ZJs65/tables/tables_8_1.jpg", "caption": "Table 1: Results on LOVM Benchmark. We evaluate our method across 23 datasets and 43 pre-trained VLMs. The results are averaged over all datasets. Our SWAB achieves the best results across all metrics. For methods that involve adding random noise to data features, we report the standard deviation of metrics across 10 experiments to mitigate the impact of randomness on result reliability.", "description": "This table presents the results of the proposed SWAB method and several baseline methods on the LOVM benchmark.  The benchmark consists of 23 image classification datasets and 43 vision-language models (VLMs).  The table shows the average performance (across all 23 datasets) of each method, measured by four metrics: H-Score, Top-5 Recall (R5), Kendall's Rank Correlation (\u03c4), and the sum of R5 and \u03c4.  The standard deviations for methods using random noise are also provided to show the reliability of the results. SWAB consistently outperforms the baselines across all four metrics.", "section": "4.1 Evaluation on LOVM Benchmark"}, {"figure_path": "01qa1ZJs65/tables/tables_8_2.jpg", "caption": "Table 2: Ablation Study of SWAB. SWAB-C, SWAB-M, and SWAB indicates only bridging the Capability Gap, only bridging the Modality Gap, and bridging both gaps in SWAB.", "description": "This table presents the ablation study results of the SWAB model. It shows the performance of three variants of SWAB: one that only bridges the Capability Gap (SWAB-C), one that only bridges the Modality Gap (SWAB-M), and one that bridges both gaps (SWAB).  The results are presented in terms of Top-5 Recall (R5), Kendall's Tau correlation (\u03c4), and the sum of R5 and \u03c4.  The table demonstrates the importance of addressing both gaps for optimal performance.  Higher values are better for all metrics.", "section": "4.2 Ablation Study"}, {"figure_path": "01qa1ZJs65/tables/tables_9_1.jpg", "caption": "Table 3: Results of  on the LOVM before and after bridging the Capability Gap.", "description": "This table presents the results of using the OT Weighted Rank method for VLM selection on the LOVM benchmark, comparing its performance before and after incorporating the capability gap bridging technique. It shows the Top-5 Recall (R5), Kendall's Rank Correlation (\u03c4), and the sum of these two metrics (R5 + \u03c4) for both scenarios.  The results demonstrate the effectiveness of bridging the capability gap in improving the accuracy of VLM selection.", "section": "4.3 Influence of Key Components in SWAB"}, {"figure_path": "01qa1ZJs65/tables/tables_9_2.jpg", "caption": "Table 4: Results of  before and after bridging the Modality Gap (MG).", "description": "This table presents the results of  before and after applying the modality gap bridging technique in SWAB. The metrics used are Top-5 Recall (R5), Kendall's Rank Correlation (\u03c4), and the sum of R5 and \u03c4 (R5+\u03c4).  Higher values indicate better performance. The results demonstrate the positive impact of bridging the modality gap on improving the accuracy of VLM selection in SWAB.", "section": "4.3 Influence of Key Components in SWAB"}, {"figure_path": "01qa1ZJs65/tables/tables_9_3.jpg", "caption": "Table 5: Results of metrics measuring gap vectors' consistency belonging to the same dataset or the same class. M: Magnitude, D: Direction.", "description": "This table presents the results of measuring the consistency of gap vectors within the same dataset and within each class.  Three metrics are used: the standard deviation of gap vector magnitudes (M-Std), the mean cosine similarity between gap vectors and their corresponding mean gap vectors (D-Mean), and the standard deviation of these cosine similarities (D-Std). Lower values for M-Std and D-Std indicate higher consistency, while higher values for D-Mean indicate higher consistency. The results are shown for the ImageNet dataset.", "section": "4.3 Influence of Key Components in SWAB"}, {"figure_path": "01qa1ZJs65/tables/tables_9_4.jpg", "caption": "Table 6: Results of SWAB-M on the LOVM Benchmark using the dataset-level mean gap vectors and class-level mean gap vectors.", "description": "This table shows the performance of SWAB-M on the LOVM benchmark. It compares the results of using dataset-level mean gap vectors versus class-level mean gap vectors for bridging the modality gap.  The metrics used are Top-5 Recall (R5), Kendall's Rank Correlation (\u03c4), and the sum of the two (R5+\u03c4). Higher values for R5 and \u03c4 indicate better performance. The results demonstrate that using class-level mean gap vectors leads to a significant improvement in the performance of SWAB-M. ", "section": "4.3 Influence of Key Components in SWAB"}, {"figure_path": "01qa1ZJs65/tables/tables_15_1.jpg", "caption": "Table 7: The detailed information of 43 models used in the LOVM Benchmark. Some of the information in the table comes from [64].", "description": "This table lists the details of the 43 vision-language models used in the LOVM benchmark.  It provides information for each model, including the model name, the dataset used for pre-training, and other relevant details. This information is crucial to understanding the diversity of models included in the benchmark and how those differences might influence the results of the model selection process.", "section": "A LOVM Benchmark Details"}, {"figure_path": "01qa1ZJs65/tables/tables_15_2.jpg", "caption": "Table 8: Detailed information of 23 tasks used in the LOVM Benchmark. This table comes from [64].", "description": "This table lists 23 image classification datasets used in the LOVM benchmark.  For each dataset, it shows the number of classes, the type of task (classification, scene understanding, geolocation, object counting, distance estimation, facial expression recognition, or OCR), and the domain of the images (natural images, satellite images, textural images, synthetic images, retina scans, hand-writing, or histopathology). The variety of datasets ensures that experimental results reflect the performance of VLM model selection methods in real-world situations.", "section": "A LOVM Benchmark Details"}, {"figure_path": "01qa1ZJs65/tables/tables_21_1.jpg", "caption": "Table 9: Results on LOVM's original VLM Zoo. We evaluate our method across 23 datasets and 35 pre-trained VLMs. The results are averaged over all datasets.", "description": "This table presents the results of the proposed SWAB method and several baseline methods on the original LOVM benchmark which consists of 35 pre-trained Vision-Language Models (VLMs) and 23 datasets.  The metrics used to evaluate the performance are H-Score, NCE, LEEP, LogME, INB, Avg Rank, ModelGPT, and SWAB.  The table shows the average performance across all 23 datasets, reporting Top-5 Recall (R5), Kendall's Rank Correlation (\u03c4), and the sum of these two metrics (R5 + \u03c4) for each method.  Higher values for R5 and \u03c4 indicate better performance.", "section": "4. Experiments"}]