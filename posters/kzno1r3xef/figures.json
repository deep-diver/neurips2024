[{"figure_path": "Kzno1r3Xef/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the proposed framework, HSDAG. Graph construction. We first convert a neural network model c into a computation graph G, repr : c \u2192 G. Feature extraction. Then, we calculate the initial feature matrix X(0) capturing local and global connectivity information, node-aware features, information about the order of the nodes as well as features from fractal analysis. Learning embeddings and groups jointly. We further enrich node features X(0) using a GNN : G \u2192 Z model and learn how to pool a graph G jointly using a graph parsing network. In that way, we bridge the gap between grouper-placer and encoder-placer methods for device assignment. Device placement. A learnable MLP model classifies the nodes V' of the coarsened graph G' = (V', E') to the available devices D. Heterogeneous execution. We map the device placement of V' to V based on the node assignment matrix X and apply the placement of all the operations into the execution environment to measure the execution time with the corresponding reward. End-to-end parameter update. We update our policy \u03c0 parameters \u03b8, i.e. the combination of GNN and MLP, based on the reward and renew the node feature matrix Z with the current cluster information. The entire framework supports end-to-end parameter updates and training.", "description": "This figure illustrates the HSDAG framework's five steps: graph construction, feature extraction, joint learning of embeddings and groups, device placement, and end-to-end parameter updates.  It shows how a neural network model is converted into a computation graph, features are extracted, a GNN and MLP are used for node embedding and device placement, and the process iteratively updates parameters based on execution time rewards.", "section": "2 Proposed framework"}, {"figure_path": "Kzno1r3Xef/figures/figures_16_1.jpg", "caption": "Figure 2: The computation graph of each of the benchmark models before and after the graph partitioning and pooling.", "description": "This figure shows computation graphs of three benchmark models (Inception-V3, ResNet50, and BERT) before and after applying graph partitioning and pooling.  The \"Before Grouping\" graphs represent the original computation graphs generated from the models, while the \"After Grouping\" graphs show the result after the proposed framework's coarsening and grouping steps.  The visual difference illustrates how the framework simplifies the graphs by merging operations into groups for more efficient device placement. The different structures of the original graphs also highlight the flexibility of the proposed framework to handle various model architectures.", "section": "2 Proposed framework"}]