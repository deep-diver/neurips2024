[{"type": "text", "text": "Fairness in Social Influence Maximization via Optimal Transport ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shubham Chowdary Giulia De Pasquale\u2217 Nicolas Lanzetti\u2217 ETH Z\u00fcrich ETH Z\u00fcrich ETH Z\u00fcrich schowdhary@ethz.ch degiulia@ethz.ch lnicolas@ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Ana-Andreea Stoica Max Planck Institute, T\u00fcbingen ana-andreea.stoica@tuebingen.mpg.de ", "page_idx": 0}, {"type": "text", "text": "Florian D\u00f6rfler ETH Z\u00fcrich dorfler@ethz.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study fairness in social influence maximization, whereby one seeks to select seeds that spread a given information throughout a network, ensuring balanced outreach among different communities (e.g. demographic groups). In the literature, fairness is often quantified in terms of the expected outreach within individual communities. In this paper, we demonstrate that such fairness metrics can be misleading since they overlook the stochastic nature of information diffusion processes. When information diffusion occurs in a probabilistic manner, multiple outreach scenarios can occur. As such, outcomes such as \u201cIn $50\\%$ of the cases, no one in group 1 gets the information, while everyone in group 2 does, and in the other $50\\%$ , it is the opposite\u201d, which always results in largely unfair outcomes, are classified as fair by a variety of fairness metrics in the literature. We tackle this problem by designing a new fairness metric, mutual fairness, that captures variability in outreach through optimal transport theory. We propose a new seedselection algorithm that optimizes both outreach and mutual fairness, and we show its efficacy on several real datasets. We find that our algorithm increases fairness with only a minor decrease (and at times, even an increase) in efficiency. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Problem Description. Social networks play a fundamental role in the spread of information, as in the context of commercial products endorsement [18], job vacancy advertisements [3], public health awareness [27], etc. Information, ideas, or new products can either go viral and potentially bring significant changes in a community or die out quickly. In this context, a fundamental algorithmic problem arises, known as Social Influence Maximization (SIM) [12, 13]. SIM studies how to strategically select a pre-specified small proportion of nodes in the social network, the early adopters or seeds so that the outreach generated by a diffusion process that starts at these early adopters is maximized. Consider, for example, a product endorsement campaign: the early adopters are strategically selected users who receive the product first to promote it to their friends, who in turn may or may not adopt it. The optimal selection of early adopters is known to be an NP-hard problem [12]. Thus, many heuristic strategies have been proposed, based on iterative processes such as greedy algorithms or on network centrality measures. However, all these algorithms purely rely on the graph topology and are agnostic to users\u2019 demographics, which raises significant fairness concerns, especially in contexts of health awareness campaigns, education, and job advertisements, where one wants to ensure an equitable spreading of information. Indeed, real-world social networks are populated by different social groups, based on gender, age, race, geography, etc., with different group sizes or connectivity patterns. Ignoring these aspects and focusing only on the outreach maximization process usually leads to the early adopters being the most central nodes. Consequently, low-interconnected minorities are often neglected from the diffusion process, thus causing fundamental inequity in the information propagation and biases exacerbation [11, 25]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Related Work. The problem of SIM was first introduced in 2003 in Kempe et al. [12], where the problem of optimally selecting a (limited) set of early adopters was proved to be NP-hard. The study of SIM under fairness guarantees has a more recent history [6]. Several multiple group-level metrics of fairness have been proposed over the years [7]. They fall under the notions of equity [23, 10, 11], equality [7], max-min fairness [8, 30], welfare [17], and diversity [25]: all of them quantify the fair distribution of influence across groups. In particular, Stoica et al. [23] propose a new SIM algorithm that operates under the constraint that, in expectation, the same percentage of users in each category is reached. Junaid et al. [10] optimize outreach under fairness and time constraints, by ensuring that the expected fraction of influenced nodes in each group is the same within a prescribed time deadline. Farnadi et al. [7] propose a unifying framework that encodes all different definitions of fairness in the SIM process as constraints in a linear program that optimizes outreach. Several other works [8, 30] adopt a max-min strategy. Specifically, in Fish et al. [8] fairness is ensured by maximizing the minimum probability of a group receiving the information through modifications of the greedy algorithm. Zhu et al. [30] ensure that the outreach contains a pre-specified proportion of each group in a population. Finally, Tsang et al. [25] optimize outreach under the constraint that no group should be better off by leaving the influence maximization process with their proportional allocation of resources done internally. All these definitions involve a marginal expected value of fairness in groups, without considering the correlations \u2013 or other higher-order moments \u2013 for the joint probability distribution of different groups adopting the information (see Farnadi et al. [7] for an overview). In contrast, our work introduces a novel formalism for taking into account the actual joint distribution of outreach among groups, thus considering all groups simultaneously, highlighting limitations of various fairness metrics and developing a new seed selection policy that strategically extracts and optimizes our proposed notion of fairness. Finally, our work is inspired by a recent line of work that draws on optimal transport theory [28] for fairness guarantees [2, 4, 21, 29, 20, 24]. To our knowledge, this is the first work to develop novel metrics and seeding algorithms that leverage optimal transport for the SIM problem. ", "page_idx": 1}, {"type": "text", "text": "Motivation. Many models of diffusion processes in the SIM problem are inherently stochastic, meaning that who gets the information transmitted can vary greatly from one run to another. Consider, as an example, the case in which $50\\%$ of realizations over a diffusion process, no one in group 1 receives the information and everyone in group 2 does, whereas in the other $50\\%$ it is the opposite. This circumstance would be classified as fair in expectation, even though it is commonly not perceived as \u201cfair\u201d. We show how this phenomenon is also common in real-world data and how our proposed framework can detect such undesired scenarios. This prompts us to study a novel fairness metric. ", "page_idx": 1}, {"type": "text", "text": "Contributions. Our main contribution is twofold: first, we propose a new fairness metric based on optimal transport, called mutual fairness, and second, we propose a novel seeding algorithm that optimizes for both the group-wise total outreach (termed efficiency) and fairness. Our proposed fairness metric provides stronger fairness guarantees, and it reveals and overcomes known limitations of various other fairness metrics in the literature. Specifically, we leverage optimal transport theory to build mutual fairness, a metric that accounts for all groups simultaneously in terms of the distance between an ideal distribution where all groups receive the information in the same proportion. We leverage our proposed mutual fairness metric to provide a unifying framework that classifies the most celebrated information-spreading algorithms both in terms of fairness and efficiency. All algorithms are tested on a variety of real-world datasets. We show how our approach unveils new insights into the role of network topology on fairness; in particular, we observe that selecting group-label blind seeds in networks with moderate levels of homophily induces inequality in information access. In contrast, very integrated or very segregated networks tend to have quite fair and efficient access to information across different groups upon greedy seedset selection. We then extend our mutual fairness metric to also account for efficiency, thus introducing the notion of $\\beta$ -fairness, with $\\beta$ being the tuning parameter for the fairness-efficiency trade-off. Finally, we design a new seedset selection algorithm that optimizes over the proposed $\\beta$ -fairness metric and enhances fairness with either a small trade-off or even improved efficiency. This novel approach provides a comprehensive evaluation and design tool that bridges the gap between fairness and efficiency in SIM problems. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Given $m\\in\\mathbb{N}$ , we let $[m]$ denote the interval of integers from 1 to $m$ . We denote by $G$ a network, considered undirected, and by $(C_{i})_{i\\in[m]}$ the $m$ groups of different sensitive attributes. In this paper, we consider $m=2$ groups, noting that our framework is easily generalizable to more groups as discussed in Appendix B. We denote by $\\phi_{G}(S)$ the influence function of a seedset $S$ over a network $G$ , through some diffusion process. In other words, $\\phi_{G}(S)$ determines the set of nodes reached by the seedset under a diffusion process. Then, $|\\phi_{G}(S)|$ is often referred to as the outreach, a measure of efficiency for the selection of a seedset $S$ . Under a stochastic diffusion process (e.g., independent cascade, linear threshold model, etc.), $|\\phi_{G}(S)|$ is a random variable, for which we are interested in the expected value and distribution. For a particular outreach, we define the final configuration at the end of a diffusion process as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Final configuration) For a network $G$ with two communities $(C_{i})_{i\\in}$ [2] and a seedset $S$ , we let $x_{i}$ , $i\\ \\in$ [2], denote the fraction of nodes in each community in the outreach $\\phi_{G}(S)$ . The final configuration is the tuple $\\left(x_{1},x_{2}\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "In many definitions in the literature, fairness is operationalized as measuring the expected value of the final configuration, where the expectation is taken over the diffusion process. In particular, the equity definition introduced by Stoica et al. [23], Junaid et al. [10] checks that the expected value of the proportions of each group reached in the outreach is the same for all groups. For a formal definition of equity and other fairness definitions in the literature, see Appendix A. We will show that relying solely on the expected value leads largely unfair outcomes to be classified as fair. ", "page_idx": 2}, {"type": "text", "text": "3 Mutual Fairness via Optimal Transport ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In contrast to the literature, we propose using the joint outreach probability distribution, instead of its marginals, to capture simultaneous outreach between the two groups and therefore address questions like (i) When group 1 receives the information, will group 2 also receive it? (ii) Even if the two groups have the same marginal outreach probability distributions will the final configuration always be fair? We argue that capturing these aspects is crucial for understanding and assessing fairness, as shown in the motivating example below. ", "page_idx": 2}, {"type": "text", "text": "Notation. We collect the output of the information-spreading process via a probability distribution $\\gamma\\in\\mathcal{P}([0,1]\\times[0,1])$ over all possible final configurations. Informally, $\\gamma(x_{1},x_{2})$ is the probability that a fraction $x_{1}$ of group 1 receives the information and a fraction $x_{2}$ of group 2 receives the information; e.g., $\\gamma(0.3,0.4)$ represents the probability that $30\\%$ of group 1 and $40\\%$ of group 2 receive the information. We can marginalize $\\gamma$ to obtain the outreach probability distributions associated with each group; i.e., $\\mu_{1}\\,\\in\\,\\overline{{\\mathcal{P}([0,1])}}$ and $\\mu_{2}\\,\\in\\,\\mathscr{P}([0,1])$ . Informally, we can write $\\begin{array}{r}{\\mu_{1}(x_{1})=\\sum_{x_{2}}\\gamma(x_{1},\\bar{x}_{2})}\\end{array}$ . As in the example above, $\\mu_{i}(0.3)$ is the probability that $30\\%$ of group $i$ receives the information. ", "page_idx": 2}, {"type": "text", "text": "Motivating Example. Consider the SIM problem with nodes belonging to two groups, $C_{1}$ and $C_{2}$ , each group having the outreach probability distribution $\\begin{array}{r}{\\mu_{i}=\\frac{1}{2}\\delta_{0}^{-}\\bar{+}\\frac{\\rceil}{2}\\delta_{1},i\\in\\bar{\\{1,2\\}}}\\end{array}$ , with $\\delta_{k}$ representing the delta distribution centered at $k\\in[0,1]$ . That is, in $50\\%$ of the cases all members in group $i$ receive the information (i.e., we get $x_{i}=1.0)$ ) and in $50\\%$ of the cases no one in group $i$ receives the information (i.e., we get $x_{i}=0.0_{.}$ ). It is therefore tempting to say that this setting is fair since $\\mu_{1}$ and $\\mu_{2}$ coincide and therefore share the same expected value. We argue that this information does not suffice to claim fairness. Indeed, consider the two following probability distributions over the final configurations: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\gamma_{a}=0.5\\cdot\\delta_{(0,0)}+0.5\\cdot\\delta_{(1,1)},\\qquad\\gamma_{b}=0.25\\cdot\\delta_{(0,0)}+0.25\\cdot\\delta_{(1,1)}+0.25\\cdot\\delta_{(0,1)}+0.25\\cdot\\delta_{(1,0)}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $\\delta_{(i,j)}$ , representing the delta distribution centered at $(i,j)\\in[0,1]^{2}$ . Interestingly, both $\\gamma_{a}$ and $\\gamma_{b}$ are \u201ccompatible\u201d with $\\mu_{1}$ and $\\mu_{2}$ : If we compute their marginals, we obtain $\\mu_{1}$ and $\\mu_{2}$ . However, $\\gamma_{a}$ and $\\gamma_{b}$ encode two fundamentally different final configurations. In $\\gamma_{a}$ , the percentage of members of group 1 who get the information always coincides with the percentage of people of group 2. Conversely, in $\\gamma_{b}$ , more outcomes are possible; in particular, there is a probability of $0.25\\substack{+0.25\\,=\\,0.5}$ that all members of one group receive the information and no member of the other group receives it (see Fig. 1). Thus, from a fairness perspective, $\\gamma_{a}$ and $\\gamma_{b}$ encode very different outcomes. We therefore argue that a fairness metric should be expressed in terms of joint probability distribution $\\gamma$ , and not solely based on its marginals $\\mu_{1}$ and $\\mu_{2}$ , as commonly done in the literature [23, 10]. ", "page_idx": 2}, {"type": "image", "img_path": "axW8xvQPkF/tmp/01d2301fe4831d38ad23d26ed761a6d13936caaa15a625264fe69e0707c150bf.jpg", "img_caption": ["Figure 1: Illustration of the $(\\gamma_{a},\\gamma_{b})$ example. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "axW8xvQPkF/tmp/f913caa48da3cab6435a744a39b6716ac1723ba652b8d15cae1d76af81deb1e9.jpg", "img_caption": ["Figure 2: The transportation cost measures the length of the solid segment; shifts along the diagonal (dotted) are not considered for fairness and are only relevant for efficiency. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.1 A Fairness Metric Based on Optimal Transport ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our motivating example prompts us to reason about fairness in terms of the joint probability measure $\\gamma$ , instead of its marginal distributions $\\mu_{1}$ and $\\mu_{2}$ . Since $\\gamma$ is a probability distribution (over all possible final configurations), we can quantify fairness by computing its \u201cdistance\u201d from an \u201cideal\u201d reference distribution $\\gamma^{*}$ along the diagonal, capturing the ideal situation in which both groups receive the information in the same proportion. We do so by using tools from optimal transport. ", "page_idx": 3}, {"type": "text", "text": "Background in optimal transport. For a given (continuous) transportation cost $c:([0,1]\\!\\times\\![0,1])\\!\\times$ $([0,1]\\times[0,1])\\rightarrow\\mathbb{R}_{\\ge0}$ , the optimal transport discrepancy between two probability distributions $\\gamma_{a}\\in\\mathcal{P}([0,1]\\times[0,1])$ and $\\gamma_{b}\\in\\mathcal{P}([0,1]\\times[0,1])$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{c}(\\gamma_{a},\\gamma_{b})=\\operatorname*{min}_{\\pi\\in\\Pi(\\gamma_{a},\\gamma_{b})}\\mathbb{E}_{(x_{1},x_{2}),(y_{1},y_{2})\\sim\\pi},[c((x_{1},x_{2}),(y_{1},y_{2}))]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Pi(\\gamma_{a},\\gamma_{b})$ is the set of probability distributions over $([0,1]\\times[0,1])\\times([0,1]\\times[0,1])$ so that its first marginal is $\\gamma_{a}$ and its second marginal is $\\gamma_{b}$ . Intuitively, the optimal transport problem quantifies the minimum transportation cost to morph $\\gamma_{a}$ into $\\gamma_{b}$ when transporting a unit of mass from $(x_{1},x_{2})$ to $(y_{1},y_{2})$ costs $c({\\bar{(x_{1},x_{2})}},(y_{1},y_{2}))$ . The optimization variable $\\pi$ is called transportation plan and $\\pi((x_{1},x_{2}),(y_{1},y_{2}))$ indicates the amount of mass at $(x_{1},x_{2})$ displaced to $(y_{1},y_{2})$ . Thus, its first marginal has to be $\\gamma_{a}(x_{1},x_{2})$ (that is, $(x_{1},x_{2})$ has to be transported to some $(y_{1},y_{2}))$ and its second marginal must be $\\gamma_{b}(y_{1},y_{2})$ (that is, the mass at $(y_{1},y_{2})$ has to arrive from some $(x_{1},x_{2}))$ . If the transportation cost $c$ is chosen to be a $p\\geq1$ power of a distance $d$ , then $(W_{d^{p}}(\\cdot,\\cdot))^{1/p}$ is a distance on the space of probability distributions. When the probability distributions are discrete (or the space $[0,1]$ is discretized), the transportation problem (1) is a finite-dimensional linear program and can therefore be solved efficiently [16]. ", "page_idx": 3}, {"type": "text", "text": "Our proposed fairness metric. To operationalize the optimal transport problem (1), we therefore need to define (i) a transportation cost and (ii) a reference distribution $\\gamma^{*}$ . To define the transportation cost, we start with the following two considerations. First, moving mass along the diagonal should have a cost of 0, as it does not affect fairness but only the efficiency (the proportion of population reached in respective groups). Second, moving mass orthogonally towards the diagonal should come at a price, since the difference in group proportion outreach between groups 1 and 2 decreases. We quantify this price as the Euclidean distance. This is illustrated in Fig. 2, which shows how the joint distribution captures unfairness, by depicting the percentage outreach in each group on each axis; thus, the diagonal represents a \u201cfair\u201d line, where the probability of reaching a particular outreach percentage is the same for both groups. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "These two insights suggest decomposing the distance between the initial configuration $(x_{1},x_{2})$ (e.g., belonging to $\\gamma_{a}$ ) and $(y_{1},y_{2})$ (e.g., belonging to $\\gamma_{b}$ ) into two components: one capturing efficiency and the other one being the fairness component (see Fig. 2). Since the aim of our metric is to measure fairness, we therefore obtain the transportation cost ", "page_idx": 4}, {"type": "equation", "text": "$$\nc((x_{1},x_{2}),(y_{1},y_{2}))=\\|z(x_{1},x_{2},y_{1},y_{2})-(x_{1},x_{2})\\|={\\frac{\\sqrt{2}}{2}}|(x_{2}-x_{1})-(y_{2}-y_{1})|,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $z(x_{1},x_{2},y_{1},y_{2})$ is the point indicated in green in Fig. 2 and $\\lVert\\cdot\\rVert$ is the standard Euclidean norm. Thus, the \u201cfairness distance\u201d between two distributions $\\gamma_{a}$ and $\\gamma_{b}$ can be readily quantified by $W_{c}(\\gamma_{a},\\gamma_{b})$ . Since moving along the diagonal is free, we quantify the fairness of a given $\\gamma$ as its \u201cfairness distance\" from the \u201cideal\u201d distribution $\\gamma^{*}=\\delta_{(1,1)}$ , which represents the case where all members of both groups receive the information. We can now formally introduce our proposed fairness metric. ", "page_idx": 4}, {"type": "text", "text": "Definition 3.1 (Mutual Fairness) Given a network with communities $(C_{i})_{i\\in[2]}$ , $a$ SIM algorithm is said to be mutually fair if the algorithm propagation is such that it maximizes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{FAIRNESS}(\\gamma):=1-\\sqrt{2}W_{c}(\\gamma,\\gamma^{\\ast}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $W_{c}(\\gamma,\\gamma^{*})$ is the optimal transport discrepancy, defined with the transportation cost (2), between the probability distribution $\\gamma$ and the desired probability distribution $\\gamma^{*}$ defined as in (1). ", "page_idx": 4}, {"type": "text", "text": "The mutual fairness from Definition 3.1 can be seen as a normalized expression of $W_{c}(\\gamma,\\gamma^{*})$ to contain its values in $[0,1]$ . Indeed, its lowest value is 0 and it is achieved with $\\gamma=\\delta_{(0,1)}$ , for which is $W_{c}(\\gamma,\\gamma^{*})=1$ ; its largest value is 1 and it is achieved with $\\gamma=\\gamma^{*}$ , for which $\\dot{W_{c}}(\\dot{\\gamma}^{*},\\gamma^{*})=0$ . Since $\\gamma^{*}$ is a delta distribution, we can solve the transportation problem (1) in closed form to ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{FAIRNESS}(\\gamma)=1-\\sqrt{2}W_{c}(\\gamma,\\gamma^{*})=\\mathbb{E}_{(x_{1},x_{2})\\sim\\gamma}\\bigg[1-|x_{1}-x_{2}|\\bigg],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which reduces to FAIRNESS(\u03b3) = 1 \u2212N1  iN=1 |x1,i \u2212x2,i| when the distribution \u03b3 is empirical with samples . In particular, our fairness metric can also be interpreted in terms of the average distance between the outreach proportions within the two groups. ", "page_idx": 4}, {"type": "text", "text": "Discussion. We note that while we considered two groups in the aforementioned definitions, our methodology readily extends the setting with $m$ groups. We present this extension in Appendix B. Second, since moving mass \u201cdiagonally\u201d is free, any distribution $\\gamma^{*}$ supported on the diagonal yields the same fairness metric. In practice, it is often not the case that all the network members receive the information and the best one could hope for is to project $\\gamma$ onto the diagonal; since moving along the diagonal is free, the fairness cost is the same whether the ideal distribution is that projection or $\\gamma^{*}$ . Moreover, it is easy to see that the \u201cfairness distance\u201d is symmetric, namely $W_{c}(\\gamma_{a},\\gamma_{b})=W_{c}(\\gamma_{b},\\gamma_{a})$ . Finally, our definition readily extends to any other distance function besides the standard Euclidean metric. ", "page_idx": 4}, {"type": "text", "text": "Back to the motivating example. Armed with a definition of fairness that captures the nature of a diffusion process, we now revisit the motivating example in Fig. 1. To start, we evaluate the \u201cfairness distance\u201d between $\\gamma_{a}$ and $\\gamma_{b}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nW_{c}(\\gamma_{a},\\gamma_{b})=\\frac{1}{4}\\cdot\\frac{\\sqrt{2}}{2}+\\frac{1}{4}\\cdot\\frac{\\sqrt{2}}{2}=\\frac{\\sqrt{2}}{4},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which amounts to the cost of transporting the points $(0,1)$ and $(1,0)$ , each with weight $1/4$ , to the diagonal. Notably, in contrast to simply computing the expected outreach of each group, our fairness metric distinguishes the two outcomes. Similarly, we can easily compute the fairness metric: FAIRNESS $\\left(\\gamma_{a}\\bar{)}=1$ and FAIRNESS $(\\gamma_{b})=0.5$ . In particular, $\\gamma_{a}$ achieves the highest fairness score. Indeed, its outcome will always be fair. Instead, FAIRNESS $(\\gamma_{b})$ achieves lower fairness score, capturing the fact that in $50\\%$ of the cases the outcome is perfectly fair while in the remaining $50\\%$ it is largely unfair. ", "page_idx": 4}, {"type": "text", "text": "3.2 Mutual Fairness in Practice ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now investigate the use of our newly defined fairness metric across a variety of real-world datasets: Add Health (AH), Antelope Valley variants 0 to 23 (AV_{0-23}) [26], APS Physics (APS) [14], Deezer (DZ) [19], High School Gender (HS) [15], Indian Villages (IV) [1], and Instagram (INS) [22]. Each dataset contains a social network with a chosen demographic dividing the population into two nonoverlapping groups (see Appendix C for details). We load the datasets as graphs $G(V,E)$ . We then select a seedset $S$ of size 2-90 (depending on the dataset) using the following heuristics: two groupagnostic seed selection strategies as our baselines, namely degree centrality (bas_d), and greedy (bas_g), proposed by Kempe et al. [12]. In addition, we implement two fair seed selection heuristics based on the equity metric, namely degree-central fair heuristic (hrt_d), and greedy fair heuristic $(\\mathtt{h r t}_{-}\\mathtt{g})$ , proposed by Stoica et al. [23]. To model the information spread, we use the Independent Cascade model (IC) for the diffusion of information [12] with a probability $p\\in[0,1]$ for all edges. This process, being stochastic, is simulated $R$ times in a Monte Carlo sampling process to achieve $R f$ inal configurations (Definition 2.1) plotted together as a joint outreach distribution, in Fig. 3. Then we apply our distribution-aware notion of fairness from Section 3.1, mutual fairness. We keep $R=1,000$ throughout, but explore several values in $p,|S|$ (mentioned per experiment in the figures below) and exhaustively recorded with other hyperparameters in Appendix D. All details related to computational resources and development environment are available in Appendix G. The code for all our numerical experiments is available at https://github.com/nicolaslanzetti/fairness-sim-ot. ", "page_idx": 5}, {"type": "text", "text": "Are the outcomes fair? As a first experiment, we study the joint outreach probability distribution for different datasets. We identify four qualitatively different outcomes, shown in Fig. 3 for a few of the datasets. Additional experiments with different propagation probability and seed selection strategies can be found in Appendix D. Fig. 3a is obtained on AH with bas_g selection strategy and $p=0.5$ , $|S|=10$ . We note how the joint outreach distribution is almost concentrated on the top right of the plane, i.e., the outcome is almost deterministic and highly fair and efficient. In turn, this trivializes both the expected value in the equity metric and the cost in the mutual fairness metric in Definition 3.1, which therefore essentially boils down to comparing the almost deterministic outreach fraction within each group. In these cases, our fairness metric does not provide additional insights. Such deterministic outcomes are typical of degree or greedy seedset outreach in dense graphs, such as AH, DZ, INS (refer to Appendix D), with extreme probability of conduction $p\\geq0.5$ or $p\\rightarrow0$ ), and cross-group interconnectivity (see Table 1 in Appendix C). For moderate $p$ (e.g., 0.1), the outreach probability distribution is concentrated along the diagonal (Fig. 3b). Thus, both the equity metric and our fairness measure are maximal. Nonetheless, our fairness metric provides additional insights: not only does the expected outreach within each group coincide, but also the outreach at every realization coincides (see the example in Section 3). Thus, our fairness metric provides a stronger certificate of fairness. As before, the same applies to AH, DZ, INS (see Appendix D). Intuitively, high cross-group interconnectivity in a dense graph already ensures fairness. Additionally, extreme $p$ values ensure deterministic outreach (either the information dies out at the seedset, or reaches everyone in the population). When propagation happens with moderate propagation probabilities, $p$ , outreach appears as in Fig. 3b. Fig. 3c represents APS for its hrt_g seedset outreach and $p=0.3$ , $|S|=6$ . Here, we observe a highly stochastic outcome, with many realizations for which almost no member of one group receives the information. Note that the phenomenon observed in Fig. 3c is the same as the one captured by our motivating example. We argue such an outcome should not be classified as fair, despite the expected value of the proportions being similar. Finally, Fig. 3d shows the AV_0 dataset with $p=0.3$ , $|S|=4$ , and bas_g selection strategy. We observe a more stochastic outreach compared to Fig. 3b with variance spread along, but not on the diagonal, with a small bias towards one group. Also in this case, both the equity and the mutual fairness metrics characterize this outcome as fair, but mutual fairness is more informative as it requires outcomes to be fair at each realization. ", "page_idx": 5}, {"type": "text", "text": "The impact of the conduction probability. As a second experiment, we investigate the difference between mutual fairness and equity (difference in the expected value of the proportions), as a function of the conduction probability $p$ . We consider the IV dataset as a case study and select seeds using bas_g. We show our results in Fig. 4. Our mutual fairness metric in Definition 3.1 shows a fundamentally different trend compared to the equity metric from Definition A.3. Importantly, for $p\\in(0,0.5)$ , both metrics have an opposite trend: equity fairness increases to some extent whereas our metric suggests a huge fall in fairness in this region. For $p\\in(0.5,0.7)$ , there is a decrease in equity fairness, while our fairness evaluation remains relatively constant. We notice similar trends for both metrics only for $p\\in(0.8,1.0)$ . The significant difference in trend of the two metrics confirms our previous finding that mutual fairness is more informative than the equity metric and that the equity metric fails to adequately capture changes in fairness, see Section 3.1 and Section 3.2. For more experiments on other datasets, we refer to Appendix D.2. ", "page_idx": 5}, {"type": "image", "img_path": "axW8xvQPkF/tmp/e804f74cbec93fbd45845ecf3d9675dcc54bf4e5eddcc3f867ed1fae0db69a92.jpg", "img_caption": ["Figure 3: Joint outreach probability distribution for different datasets, different propagation probabilities $p$ , and seedsets cardinalities $|S|$ . "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "axW8xvQPkF/tmp/105b05d9c1c86aacd78b5a605598bf83db7996ece963490790b66a063b7f08fd.jpg", "img_caption": ["Figure 4: Mutual fairness (left, red) and equity (right, blue) for the IV dataset as $p$ varies in $[0,1]$ . "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.3 Trading off Fairness and Efficiency ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To construct our fairness metric, we completely discarded the efficiency of the final configuration. For instance, the \u201cfairness distance\u201d between a configuration whereby no agent receives the information (i.e., $\\gamma=\\delta_{(0,0)},$ and the \u201cideal\u201d configuration whereby everyone receives the information (i.e., $\\gamma^{*}$ ) is zero, as both probability distributions lay on the diagonal. As such, the fairness score of $\\gamma=\\delta_{(0,0)}$ is 1 and therefore maximal. Thus, in practice, one seeks a fairness-efficiency tradeoff. ", "page_idx": 6}, {"type": "text", "text": "In our setting, we can easily introduce the tradeoff in the transportation cost (2). Specifically, we can define the transportation cost as a weighted sum of the \u201cdiagonal distance\u201d (measuring difference in efficiency, dotted segment in Fig. 2) and the \u201corthogonal distance\u201d (measures difference in fairness, solid segment in Fig. 2). Formally, for a given weight $\\beta\\geq0$ , we define the transportation cost ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{\\beta}((x_{1},x_{2}),(y_{1},y_{2}))=\\beta\\|z(x_{1},x_{2},y_{1},y_{2})-(x_{1},x_{2})\\|+(1-\\beta)\\|z(x_{1},x_{2},y_{1},y_{2})-(y_{1},y_{2})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\beta\\displaystyle\\frac{\\sqrt{2}}{2}|(x_{2}-x_{1})-(y_{2}-y_{1})|+(1-\\beta)\\displaystyle\\frac{\\sqrt{2}}{2}|(x_{1}+x_{2})-(y_{1}+y_{2})|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We refer to Fig. 5 for a heatmap of $c_{\\beta}$ . In particular, for $\\beta\\,=\\,1$ , we recover the transportation cost (2); for $\\beta=0$ one optimizes for efficiency, and the $\\beta$ -fairness collapses in the classical influence maximization problem. We can then proceed as in Section 3.1. The \u201c $\\beta$ -fairness-efficiency distance\u201d between $\\gamma_{a}$ and $\\gamma_{b}$ is $W_{c_{\\beta}}(\\gamma_{a},\\gamma_{b})$ and the $\\beta$ -fairness metric can be then defined as follows. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.2 ( $\\beta$ -Fairness) Consider a network with groups $C_{1},C_{2}$ , a SIM algorithm is said to be $\\beta$ -fair $i f$ the algorithm propagation is such that it maximizes ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\beta\\mathrm{-FAIRNESS}(\\gamma):=1-\\frac{\\sqrt{2}}{\\operatorname*{max}\\{1,2-2\\beta\\}}W_{c_{\\beta}}(\\gamma,\\gamma^{*}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "axW8xvQPkF/tmp/2d6fccf54c677cdef684761dcac66f29cf6315cfd6cf71b9ebe0d410c7dec659.jpg", "img_caption": ["Figure 5: Cost of transporting a point $(x_{1},x_{2})$ to the \u201cideal\u201d point $(1,1)$ (i.e., everyone receives the information) for various values of $\\beta$ (i.e., we plot $(x_{1},x_{2})\\mapsto c_{\\beta}((x_{1},x_{2}),(1,1)))$ . Yellow denotes a low transportation cost, whereas dark blue denotes a large cost. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "with $W_{c_{\\beta}}(\\gamma,\\gamma^{*})$ defined as in (1) with transportation cost as in (3) and ideal distribution $\\gamma^{*}=\\delta_{(1,1)}$ ", "page_idx": 7}, {"type": "text", "text": "The factors 1 and and $\\sqrt{2}/\\operatorname*{max}\\{1,2-2\\beta\\}$ in (4) ensure that the metric is non-negative and in $[0,1]$ . Again, the optimal transport problem can be solved in closed form, which yields ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\beta\\!-\\!\\mathrm{FAIRNESS}(\\gamma)=\\mathbb{E}_{(x_{1},x_{2})\\sim\\gamma}\\left[1-\\frac{\\beta|x_{1}-x_{2}|+(1-\\beta)|x_{1}+x_{2}-2|}{\\operatorname*{max}\\{1,2-2\\beta\\}}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In particular, for $\\beta=1$ , we recover the mutual fairness FAIRNESS $(\\gamma)$ in Definition 3.1 and for $\\beta=0$ we obtain the efficiency metric E(x1,x2)\u223c\u03b3[1 \u2212|x1+2x2\u22122|]. ", "page_idx": 7}, {"type": "text", "text": "4 Improving Fairness ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Fairness-promoting Seed-selection Algorithm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Armed with a novel fairness metric, $\\beta-$ FAIRNESS, we now design an iterative seed-selection algorithm, which we call Stochastic Seedset Selection Descent (S3D), that strategically selects seeds taking into account all communities simultaneously. The pseudo-code is summarized in Algorithm 1. For its motivation and details, refer to Appendix E, E.3. For a given initial seedset, our algorithm explores new seeds and evaluates them on the efficiency-fairness metric $\\beta-$ FAIRNESS as in (4) for a desired value of the fairness-efficiency tradeoff parameter $\\beta$ (S3D_STEP() in Appendix E), to decide if the new seedset becomes a candidate for the optimized seedset. These seeds are searched for by iteratively sampling stochastically reachable nodes, up to a fixed depth, taken as a fraction of the graph diameter, from the current seedset (SEEDSET_REACH() in Appendix E) while making sure they contribute to a non-overlapping outreach (Algorithm 1::6-8). To prevent getting stuck at some local minima of the generally non-convex objective, the procedure allows for visiting inferior seedsets on $\\beta-$ FAIRNESS or even selecting completely random ones on rare occasions (Algorithm 1::12-18) using Metropolis Sampling [5]. Otherwise, a high $\\beta$ \u2212FAIRNESS encourages opting for the new seedset with high probability. Finally, we revisit all the seedset candidates collected so far and pick the one with the largest $\\beta\\cdot$ \u2212FAIRNESS as the optimal seedset. For a sparse graph $G(V,E)$ , with $E=O(V)$ , choosing $|S|$ seeds, averaging over $R$ realizations to approximate outreach via MonteCarlo sampling and exploring $k$ candidates using S3D_STEP suggests a total running time upper bound of $\\bar{O}(k\\bar{R}|S||V|)$ (see Appendix $\\boldsymbol{\\mathrm E}$ for details about the algorithm complexity). In practice, $k\\in[500,1000]$ , $R=1000$ for $S\\in[2,100]$ works well for all datasets. ", "page_idx": 7}, {"type": "text", "text": "4.2 Real-world Data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Are the outcomes more fair? We test our algorithm across a variety of datasets (Appendix C) against our baselines (bas_d, bas_g). We initialize the S3D algorithm with the two baseline seedsets and hence include results from two separately optimized seedsets, S3D_d, S3D_g. Our results are shown in Fig. 6. Informally, we observe that our seed-selection mechanism \u201cmoves\u201d the probability mass of the joint outreach probability distribution towards the diagonal, which ultimately increases the fairness of the resulting configuration. At the same time, efficiency either increases as well or suffers only a small decrease, as we investigate more in detail in our next experiment. Generally speaking, datasets with high cross-group connections (AH, DZ, INS) can already benefit a lot from label-blind seed selection to get moderately fair outreach. Similarly, for datasets with low cross-group connections (APS) a label-blind strategy, in order to maximize efficiency, selects a diverse population of seeds from which all communities are reached. Therefore, label-blind algorithms work similarly to S3D. In other moderate cases (AV, HS, IV), instead, we observe significant improvements of S3D over label-blind strategies. ", "page_idx": 7}, {"type": "image", "img_path": "axW8xvQPkF/tmp/23f5552d4d8c142a7e3a239aba6044edb85e555aec30f1ce917405a0daf1f373.jpg", "img_caption": ["Figure 6: Demonstrate S3D (red) improvement over its label-blind baseline counter-part initializations (blue) for several datasets, propagation probabilities $p$ , seed set cardinalities $|S|$ and fairness-efficiency tradeoffs $\\beta$ . Fig. 6d provides the strongest evidence that, besides improving in fairness, our strategy can also be more efficient, from $83.1\\%$ to $87.9\\%$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Classification of seed-selection algorithms. In our final experiments, we compare several algorithms along with ours in terms of efficiency and mutual fairness across various datasets (see Appendix C for all datasets),. We consider the following algorithms: bas_d, bas_g, their fair heuristic counterparts, hrt_d, hrt_g, against our S3D_d, S3D_g, initialized via greedy and degree centrality baseline seeds, respectively. We show our results in Fig. 7. S3D achieves in almost all cases the highest ", "page_idx": 8}, {"type": "image", "img_path": "axW8xvQPkF/tmp/b6da8e403b97b7d4169645fb6f2fc5fc0c506915e7ab925f9fde973cd5393c2a.jpg", "img_caption": ["Figure 7: S3D trade-off and improvement against other label-aware and label-blind algorithms for several datasets, propagation probabilities $p$ , seed set cardinalities $|S|$ and fairness-efficiency tradeoffs $\\beta$ . Filled markers refer to greedy-based algorithms: $\\sqsubseteq\\mathtt{b a s\\_g}$ , $\\begin{array}{r}{\\bullet=\\mathtt{S3D_{-}g}}\\end{array}$ , and $\\bullet=\\mathtt{h r t}_{-}\\mathtt{g}$ . Empty markers refer to degree-based algorithms: $\\bigsqcup={\\mathtt b a s}_{-}{\\mathtt d}$ , $\\bigcirc=\\mathtt{S3D_{-}d}$ , and $\\diamondsuit=\\mathtt{h r t\\_d}$ . For statistical bounds, we refer to Appendix F. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "fairness score ( $y$ -axis) and generally a slightly lower efficiency score $x$ -axis), compared to others.   \nThus, our seed-selection mechanism leads to fairer outcomes with only a minor decrease in efficiency. ", "page_idx": 9}, {"type": "text", "text": "The impact of the network topology. To conclude, we discuss the impact of the network topology. In particular, when the conduction probability is moderate, network topology starts playing a role, mainly through the number of cross-group edges (CE): ", "page_idx": 9}, {"type": "text", "text": "$C E\\%$ is small $\\sim5\\%$ , APS): Such datasets encode group interaction information in the edges themselves, that is, an edge likely means nodes belong to the same group. In such cases, baseline greedy algorithms (bas_g) already perform well as they rely only on edge connectivity. In such circumstances, S3D does not significantly improve on their selection, both in efficiency and fairness. $C E\\%$ is balanced $(40.50\\%$ , HS, AH): These datasets reflect that groups interact well across each other and so any seedset selection largely ends up in a fair outreach. Since bas_g already has proven near-optimal efficiency guarantees, it is unlikely that S3D performs significantly better than bas_g. $C E\\%$ is moderate $(5.30\\%$ , $A\\,V$ (datasets 0, 2, 16, 20), IV): These are the non-trivial cases not covered above. Here bas_g can not reliably leverage the existence of edges into group information. Hence, S3D usually outperforms the baseline, achieving similar efficiency scores while significantly improving fairness. ", "page_idx": 9}, {"type": "text", "text": "$C E\\%$ is high $(>\\!50\\%)$ : The case where nodes interact more across groups than in their own groups was never observed. However, as long as the existence of edges does not reliably signal group information, we expect S3D to perform well based on a similar analysis. ", "page_idx": 9}, {"type": "text", "text": "Moderate outreach in dense graphs $(I N S,\\ \\ D Z)$ : For graphs where $|E|$ substantially exceeds $\\vert V\\vert$ , the outreach variance across sample sub-graphs is too low to be captured in the discretized space we experimented $100\\times100$ units in $[0,1]^{\\bar{2}})$ , even for moderate $p$ . This leads to single-point concentrated joint-distribution plots, all of them leading to the same $\\beta-$ FAIRNESS. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Conclusions. We propose a new fairness metric, called mutual fairness, in the context of SIM. Mutual fairness draws on optimal transport and captures various fairness-related aspects (e.g., when members of group 1 receive the information will members of group 2 receive it?) that are obscure to the fairness metrics in the literature. We also leverage our novel fairness metric to design a new seed selection strategy that tradeoffs fairness and efficiency. Across various real datasets, our algorithm yields superior fairness with a minor decrease (and in some cases even an increase) in efficiency. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Our proposed algorithm, S3D, is essentially a random combinatorial search in the graph defining the social network. As such, its performance will generally depend on the quality of the seedset initialization. Moreover, there is no guaranteed bound on the number of iterations needed in S3D to achieve a desired level of fairness. Both aspects can be limiting in real-world applications. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank the reviewers for their constructive suggestions. This work was supported as a part of NCCR Automation, a National Centre of Competence in Research, funded by the Swiss National Science Foundation (grant number 51NF40_225155). A.-A. S. acknowledges support from the T\u00fcbingen AI Center. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abhijit Banerjee, Arun G Chandrasekhar, Esther Duflo, and Matthew O Jackson. The diffusion of microfinance. Science, 341(6144):1236498, 2013.   \n[2] Emily Black, Samuel Yeom, and Matt Fredrikson. Fliptest: fairness testing via optimal transport. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT\u201920), pages 111\u2013121, 2020.   \n[3] Wei Chen, Wei Lu, and Ning Zhang. Time-critical influence maximization in social networks with time-delayed diffusion process. In Proceedings of AAAI Conference of Artificial Intelligence, 26(1):1\u20137, 2012.   \n[4] Silvia Chiappa, Ray Jiang, Tom Stepleton, Aldo Pacchiano, Heinrich Jiang, and John Aslanides. A general approach to fairness with optimal transport. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT\u201920), pages 3633\u20133640, 2020. [5] P. Robert Christian. The metropolis-hastings algorithm, 2016.   \n[6] Tang Fangshuang, Qi Liu, Zhu Hengshu, Chen Enhong, and Feida Zhu. Diversified social influence maximization. 2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014), pages 455\u2013459, 2014.   \n[7] Golnoosh Farnadi, Behrouz Babaki, and Michel Gendreau. A unifying framework for fairnessaware influence maximization. International World Wide Web Conference 2020, pages 714\u2013722, 2020.   \n[8] Benjamin Fish, Ashkan Bashardoust, Danah Boyd, Sorelle Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. Gaps in information access in social networks? International World Wide Web Conference 2019, San Francisco, USA, pages 480\u2013490, 2020.   \n[9] Aric Hagberg, Pieter Swart, and Daniel Chult. Exploring network structure, dynamics, and function using networkx. 01 2008.   \n[10] Ali Junaid, Babaei Mahmoudreza, Abhijnan Chakraborty, Baharan Mirzasoleiman, Krishna P. Gummadi, and Adish Singla. On the fairness of time-critical influence maximization in social network. IEEE Transaction on knowledge and data engineering, 35(3):480\u2013490, 2023.   \n[11] Fariba Karimi, Mathieu G\u00e9nois, Claudia Wagner, Philipp Singer, and Markus Strohmaier. Homophily influences ranking of minorities in social networks. Scientific reports, 8(1):11077, 2018.   \n[12] David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social network. Proceedings of the 9th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 137\u2013146, 2003.   \n[13] David Kempe, Jon Kleinberg, and \u00c9va Tardos. Influential nodes in a diffusion model for social networks. In Automata, Languages and Programming: 32nd International Colloquium, ICALP 2005, Lisbon, Portugal, July 11-15, 2005. Proceedings 32, pages 1127\u20131138. Springer, 2005.   \n[14] Eun Lee, Fariba Karimi, Claudia Wagner, Hang-Hyun Jo, Markus Strohmaier, and Mirta Galesic. Homophily and minority-group size explain perception biases in social networks. Nature human behaviour, 3(10):1078\u20131087, 2019.   \n[15] Rossana Mastrandrea, Julie Fournet, and Alain Barrat. Contact patterns in a high school: a comparison between data collected using wearable sensors, contact diaries and friendship surveys. PloS one, 10(9):e0136497, 2015.   \n[16] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019.   \n[17] Aida Rahmattalabi, Shahin Jabbari, Himabindu Lakkaraju, Phebe Vayanos, Max Izenberg, Ryan Brown, Eric Rice, and Milind Tambe. Fair influence maximization: A welfare optimization approach. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11630\u201311638, 2021.   \n[18] Matthew Richardson and Pedro Domingos. Mining knowledge-sharing sites for viral marketing. In Proceedings of 8th International Conference on Knowledge, Discovery and Data Mining, pages 61\u201370, 2002.   \n[19] Benedek Rozemberczki and Rik Sarkar. Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models. In Proceedings of the 29th ACM international conference on information & knowledge management, pages 1325\u20131334, 2020.   \n[20] Yves Rychener, Bahar Taskesen, and Daniel Kuhn. Metrizing fairness. arXiv preprint arXiv:2205.15049, 2022.   \n[21] Nian Si, Karthyek Murthy, Jose Blanchet, and Viet Anh Nguyen. Testing group fairness via optimal transport projections. Proceedings of the 38th International Conference on Machine Learning, pages 9649\u20139659, 2021.   \n[22] Ana-Andreea Stoica, Christopher Riederer, and Augustin Chaintreau. Algorithmic glass ceiling in social networks: The effects of social recommendations on network diversity. In Proceedings of the 2018 World Wide Web Conference, pages 923\u2013932, 2018.   \n[23] Ana-Andreea Stoica, Jessy Xinyi Han, and Augustin Chaintreau. Seeding network influence in biased networks and the benefits of diversity. Proceedings of The Web Conference 2020, pages 2089\u20132098, 2020.   \n[24] Bahar Taskesen, Jose Blanchet, Daniel Kuhn, and Viet Anh Nguyen. A statistical test for probabilistic fairness. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 648\u2013665, 2021.   \n[25] Alan Tsang, Bryan Wilder, Eric Rice, Milind Tambe, and Yair Zick. Group-fairness in influence maximization. Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19), pages 5997\u20136005.   \n[26] Alan Tsang, Bryan Wilder, Eric Rice, Milind Tambe, and Yair Zick. Group-fairness in influence maximization. Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19), pages 5997\u20136005, 2019.   \n[27] Thomas W. Valente and Patchareeya Pumpuang. Identifying opinion leaders to promote behaviour change. Health, Education & Behaviour, 34(6):881\u2013896, 2007.   \n[28] C\u00e9dric Villani. Optimal transport: old and new, volume 338. Springer, 2009.   \n[29] Meike Zehlike, Alex Loosley, H\u00e5kan Jonsson, Emil Wiedemann, and Philipp Hacker. Beyond incompatibility: Trade-offs between mutually exclusive fairness criteria in machine learning and law. arXiv preprint arXiv:2212.00469, 2022.   \n[30] Jianming Zhu, Smita Ghosh, and Weili Wu. Group influence maximization problem in social networks. IEEE Transactions on Computational Social Sciences, 6(6):1156\u20131164, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Existing Fairness Metrics ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Definition A.1 (Expected outreach ratio) Given a network with communities $C_{1},\\ldots,C_{m}.$ , the SIM algorithm expected outreach ratio in $C_{i},\\,\\bar{x}_{i}$ , is the expected ratio of nodes reached in $C_{i}$ , namely ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\bar{x}_{i}:=\\frac{\\mathbb{E}[|v\\;r e a c h e d\\;|v\\in C_{i}|]}{|C_{i}|},\\quad\\forall i\\in\\{1,\\ldots,m\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Definition A.2 (Equality [23]) Given the groups $C_{1},\\ldots,C_{m},$ , a configuration is said to be equal, $i f$ the SIM algorithm chooses a seed set $S$ in a way such that the proportion of all communities in the seed set is the same, namely ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\frac{\\mathbb{E}[|v\\in S|v\\in C_{i}|]}{|C_{i}|}=\\frac{\\mathbb{E}[|v\\in S|v\\in C_{j}|]}{|C_{j}|}\\quad\\forall i,j\\in\\{1,\\ldots,m\\}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The notion of equality focuses on the fair allocation of seeds to the groups proportional to the size of the group within the population. This notion of fairness applies, for example, in the context of advertising companies that aim at having a fair distribution of resources among groups. ", "page_idx": 12}, {"type": "text", "text": "Definition A.3 (Equity [23]) Given a network with communities $C_{1},\\ldots,C_{m},$ , a SIM algorithm that selects a seedset $S$ is said to be equitable if the algorithm propagation reaches all communities in $a$ balanced way, i.e. $\\bar{x}_{i}=\\bar{x}_{j},\\,\\forall i,j\\in\\left\\{1,\\ldots,m\\right\\}$ . ", "page_idx": 12}, {"type": "text", "text": "The notion of equity focuses on the outcome of the diffusion process, e.g. independent cascade, linear threshold model and it is suitable in contexts in which one aims to reach a diverse population in a calibrated way. ", "page_idx": 12}, {"type": "text", "text": "Definition A.4 (Max-min fairness [7]) Given the groups $C_{1},\\ldots,C_{m}$ , the max-min fairness criterion maximizes the minimum expected outreach ratio among all groups, namely max $\\operatorname*{min}_{i\\in\\{1,...,m\\}}\\bar{x}_{i}$ . ", "page_idx": 12}, {"type": "text", "text": "The goal of the maxmin fairness is to minimize the gap among different groups in the outreach. The SIM problem under maxmin constraints has been investigated in [7, 8, 30]. ", "page_idx": 12}, {"type": "text", "text": "Definition A.5 (Diversity [7]) Given the groups $C_{1},\\ldots,C_{m}$ , let $\\begin{array}{r}{k_{i}\\,=\\,\\left\\lceil k\\,\\cdot\\,\\frac{|C_{i}|}{|V|}\\right\\rceil}\\end{array}$ , where $k$ is the pre-specified total seed budget. Let $\\bar{x}_{i}^{*}(C_{i}):=\\operatorname*{max}_{S\\subset C_{i}:|S|=k_{i}}\\bar{x}_{i}$ . A configuration is said to be diverse if for each $i\\in\\{1,\\ldots,m\\}$ it holds $\\bar{x}_{i}\\geq\\bar{x}_{i}^{*}(C_{i}).$ , where ${\\bar{x}}_{i}$ refers to the expected outreach ratio in $C_{i}$ obtained from the seed set $S$ , with $|S|=k$ . ", "page_idx": 12}, {"type": "text", "text": "The notion of diversity ensures that each group receives influence at least equal to their internal spread of influence. The SIM problem under diversity constraints has been investigated in [7, 25]. ", "page_idx": 12}, {"type": "text", "text": "B Extension to Multiple Groups ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we extend our definitions of mutual fairness and $\\beta.$ -fairness to the setting of $m$ groups. To do so, we first notice that, in the case of $m$ groups, the outreach distribution is a probability distribution $\\gamma$ on the hypercube $[0,1]^{m}$ ; i.e., $\\gamma$ now lives in $\\mathcal{P}([0,1]^{m})$ . ", "page_idx": 12}, {"type": "text", "text": "We start with the definition of mutual fairness. We proceed as Section 3.1 and define mutual fairness via optimal transport, which, in turn, requires defining a reference distribution and a transportation cost. The reference distribution is again the \u201cideal\u201d distribution $\\gamma^{*}=\\delta_{(1,...,1)}$ which encodes the case in which all members of all groups receive the information. As for the transportation cost, it suffices to generalize the transportation cost (2) to an $m$ dimensional space. Specifically, it can be defined as the distance between any given point $(x_{1},\\ldots,x_{m})\\in[0,1]^{\\bar{m}}$ in the hypercube and the diagonal line. For this, let ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{z(x_{1},\\ldots,x_{m})=\\displaystyle\\operatorname*{argmin}_{z=(y,\\ldots,y),y\\in[0,1]}\\left\\|(x_{1},\\ldots,x_{m})-z\\right\\|}}\\\\ {{=\\displaystyle\\frac{x_{1}+\\ldots+x_{m}}{m}(1,\\ldots,1)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "be the closest point to $(x_{1},\\ldots,x_{m})$ on the diagonal. Then, the transportation cost can be defined as in (2) and the fairness metric reads ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{F a i r n e s s}(\\gamma)=1-\\alpha\\mathbb{E}_{(x_{1},\\ldots,x_{m})\\sim\\gamma}[\\|z(x_{1},\\ldots,x_{m})-(x_{1},\\ldots,x_{m})\\|]}\\\\ &{\\qquad\\qquad\\qquad=1-\\alpha\\mathbb{E}_{(x_{1},\\ldots,x_{m})\\sim\\gamma}\\left[\\underset{z\\in[0,1]}{\\operatorname*{min}}\\|(x_{1},\\ldots,x_{m})-(z,\\ldots,z)\\|\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the constant $\\alpha>0$ is again chosen so that Fairness $(\\gamma)$ is between 0 and 1. Note that in the case of two groups, we have $\\textstyle z={\\frac{1}{2}}(x_{1}+x_{2})$ and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{z\\in[0,1]}\\|(x_{1},\\ldots,x_{m})-(z,\\ldots,z)\\|={\\frac{\\sqrt{2}}{2}}|x_{1}-x_{2}|,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with which is precisely the mutual fairness of Definition 3.1. ", "page_idx": 13}, {"type": "text", "text": "We now turn our attention to $\\beta$ -fairness. We can proceed analogously and obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\beta\\!-\\!\\operatorname{FAIRNESS}(\\gamma)=1-\\alpha\\mathbb{E}_{(x_{1},\\dots,x_{m})\\sim\\gamma}[\\beta\\Vert z(x_{1},\\cdot\\cdot\\cdot,x_{m})-(x_{1},\\cdot\\cdot\\cdot,x_{m})\\Vert}&{}\\\\ {+\\left(1-\\beta\\right)\\Vert z(x_{1},\\cdot\\cdot\\cdot,x_{m})-(1,\\cdot\\cdot\\cdot,1)\\Vert],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\alpha>0$ is again chosen to normalize the metric. Again, in the case of two groups, we have $z={\\textstyle{\\frac{1}{2}}}(x_{1}+x_{2})$ and so ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\beta\\!-\\!\\operatorname{FAIRNSS}(\\gamma)=1-\\alpha\\mathbb{E}_{(x_{1},\\ldots,x_{m})\\sim\\gamma}\\Big[\\beta\\frac{\\sqrt{2}}{2}|x_{1}-x_{2}|+(1-\\beta)\\frac{\\sqrt{2}}{2}|x_{1}+x_{2}-2|\\Big],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which coincides with Definition 3.2. ", "page_idx": 13}, {"type": "text", "text": "We conclude with two remarks on this extension to $m$ groups. First, as in the case of two groups, there is no need to numerically solve optimal transport problems, as we provide a closed-form expression for the optimal transport problems. Second, we highlight that our extension to $m$ groups does not resort to the so-called multimarginal optimal transport problem, which might cause exponential complexity in the dimensionality. ", "page_idx": 13}, {"type": "text", "text": "C Description and Properties of Datasets ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To associate the notion of fairness developed in Sections 3.1 and 3.3 with the datasets and the outcomes from experiments in Section 3.2 and 4.2, we summarize the dataset statistics in Table 1. Minority Frac. is calculated as the fraction of the minority group nodes in the entire population. Fraction of Cross Edges evaluates heterophily in the dataset, by calculating the fraction of edges that connect different groups. A higher value means a more heterophilic network, whereas a lower value means a more homophilic network. ", "page_idx": 13}, {"type": "text", "text": "Add Health (AH). The Add Health dataset consists of a social network of students in schools and a relation between them is represented by whether they nominated each other in the Add Health surveys. We select a school at random with 1, 997 students and use race as the sensitive attribute (white and non-white).2 ", "page_idx": 13}, {"type": "text", "text": "Antelope Valley (AV), [26]. We choose 4 random networks among the 24 available in the Antelope Valley dataset to compare our fairness-improving algorithm, S3D, against [26], which worked on the same dataset. We also run our baselines and other fair seed selection heuristics from [23] on these datasets to get a fair comparison. The two sensitive attribute groups are male and female, self-reported in the dataset with binary attributes. ", "page_idx": 13}, {"type": "table", "img_path": "axW8xvQPkF/tmp/7ddb14310c5ebe0eae2156bca1178720508f09c1f26c2b8bd00240734748977c.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary statistics of Datasets used. "], "page_idx": 14}, {"type": "text", "text": "APS Physics (APS), [14]. The APS citation network contains 1, 281 nodes, representing papers written in two main topics: Classical Statistical Mechanics (CSM), constituting $31.\\bar{8\\%}$ of the papers, and Quantum Statistical Mechanics (QSM), accounting for the rest. As Lee et al. [14] analyze, the dataset has high homophily, meaning that each subfield cites more papers in its own field than in the other field. For simplicity, we use only the largest connected component in the full dataset (component stats in 1) between the two groups, for this study. ", "page_idx": 14}, {"type": "text", "text": "Deezer (DZ), [19]. A social network from Europe with 18, 442 nodes, where each node has a self-reported attributed gender (male or female). Men are the minority $(44.3\\%)$ and women are the majority $(55.6\\%)$ . The data has moderate homophily. ", "page_idx": 14}, {"type": "text", "text": "High School (HS), [15]. A high school friendship network collected from Mastrandrea et al. [15], with 133 nodes in its main connected component represented by students who self-identify as male or female. The majority are female $(60\\%)$ ), and the network is homophilic. ", "page_idx": 14}, {"type": "text", "text": "Indian Villages (IV), [1]. The dataset contains different demographic attributes for the individual networks and the household networks collected in 77 Indian villages, from which we select Mothertongue (Telugu or Kannada) as the sensitive attribute. We note that most villages contain a majority mother tongue, either Telugu or Kannada. We pick a random village with 90 individuals for our study. ", "page_idx": 14}, {"type": "text", "text": "Instagram (INS), [22]. An interaction network from Instagram containing 553, 628 nodes, where everyone has a labeled gender $(45.57\\%$ men and $54.43\\%$ women). Each edge between two users represents a \u2018like\u2019 or \u2018comment\u2019 that one user gave another on a posted photo. The data has moderate homophily. ", "page_idx": 14}, {"type": "text", "text": "D Details on the Experiments and Extended Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We use $R~=~1000$ throughout our experiments. For the outreach, we discretize the space $[0,1]\\ \\times\\ [0,1]$ into $100~\\times~100$ equal sized bins. For S3D (refer to Appendix E), we use constants, exploit_to_explore $\\mathrm{~\\it~\\lambda~}=\\mathrm{~\\ensuremath~{~1.3~}~}$ , non_acceptance_retention_prob $\\mathrm{~\\it~{~\\it~\\Delta~}~}=\\mathrm{~\\Delta~0.95~}$ , and shallow_horizon $=4$ . ", "page_idx": 14}, {"type": "text", "text": "D.1 Outreach Distribution ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We report additional experiments in Figs. 8 to 11. ", "page_idx": 14}, {"type": "image", "img_path": "axW8xvQPkF/tmp/04cef002f44610f93851ff9f57fbd30d1139d8b678e132ebda509cc2e89ac5f2.jpg", "img_caption": ["Figure 8: Outreach distribution. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "axW8xvQPkF/tmp/8182ae638929c887d2a1576a77c217439b524927651f2c746b14e97ebcdd7141.jpg", "img_caption": ["Figure 9: Outreach distribution. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "axW8xvQPkF/tmp/d4ea8e7fed0dfbbcc3ab3efb1ee6baed7b07597b5c9bcd5b3354e3df32d0eb35.jpg", "img_caption": ["Figure 10: Outreach distribution. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "axW8xvQPkF/tmp/3cb8a59afb81d1d5c4c5888983f9f8384eb76813f607f3682d0a1831ae5be2c8.jpg", "img_caption": ["Figure 11: Outreach distribution. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "D.2 The Impact of the Conduction Probability for Various Dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We report additional experiments in Figs. 12 and 13. ", "page_idx": 19}, {"type": "image", "img_path": "axW8xvQPkF/tmp/6524f9ee9fe8d476fae91e60b0699d59924ba043d1470f8556d3ce6fc18232dc.jpg", "img_caption": ["Figure 12: Part 1: Different definitions of fairness VS conduction probability on an outreach distribution created by the bas_g or bas_d heuristic. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "axW8xvQPkF/tmp/4aa8d3d85c688cbdd59a419a11d716906d99ef98cf6ffcb61b0cd898744a0082.jpg", "img_caption": ["Figure 13: Part 2: Different definitions of fairness VS conduction probability on an outreach distribution created by the bas_g heuristic. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "D.3 Fairness-Efficiency performance of seedset selection algorithms ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We report more experiments in Fig. 14. ", "page_idx": 21}, {"type": "image", "img_path": "axW8xvQPkF/tmp/3e5ec4316dabd017b3a463e8c6455f356c60fe81c84169889da2f55fb80147fd.jpg", "img_caption": ["Figure 14: S3D trade-off and improvement against other label-aware and label-blind algorithms. Filled markers refer to greedy-based algorithms: $|=$ bas_g, $\\begin{array}{r}{\\bullet=\\mathtt{S3D_{-}g}}\\end{array}$ , and $\\bullet=\\mathtt{h r t}_{-\\mathtt{E}}$ . Empty markers refer to degree-based algorithms: $\\sqsupset$ bas_d, $\\bigcirc=\\mathtt{S3D_{-}d}$ , and $\\diamondsuit=\\mathtt{h r t\\_d}$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "E Details on the Algorithm ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "E.1 Pseudocode ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We provide more details on our algorithm, S3D, in two routines, Algorithm 2 and Algorithm 3. ", "page_idx": 21}, {"type": "text", "text": "1: function SEEDSET_REACH(seedset,G,p,horizon) \u25b7nodes reached from seedset until   \nhorizon   \n2: realizations $\\leftarrow1000$ \u25b7for MCMC sampling, configurable   \n3: reach \u2190[]   \n4: while realizations do   \n5: reach $\\leftarrow$ reach $^+$ INDEPENDENT_CASCADE(seedset, G, p, horizon) \u25b7collect   \nnodes reached   \n6: realizations $\\leftarrow$ realizations \u22121   \n7: return reach $\\triangleright$ repetition of nodes reached   \n8: function S3D_STEP(seedset, G, p, fair_to_efficacy) \u25b7each step delivers a new   \nseedset   \n9: exploit_to_explore $\\mathrm{\\leftarrow1.3}$ \u25b7experimentally chosen, configurable   \n10: non_acceptance_retention_prob $\\gets0.95$ $\\triangleright$ prob. of retaining set   \n11: max_horizon $\\leftarrow$ GET_DIAM(G)   \n12: horizon_factor $\\leftarrow$ max_horizon/4 \u25b7limit runtime   \n13: shallow_horizon $\\leftarrow$ max_horizon/horizon_factor   \n14: num_seeds $\\leftarrow l e n$ (seedset)   \n15: seedset $\\leftarrow$ DISTINCT(seedset)   \n16: seedset $\\leftarrow$ FIT_TO_SIZE(seedset, num_seeds) \u25b7fit to size with random nodes   \n17: reach $\\leftarrow$ SEEDSET_REACH(seedset, G, p, max_horizon)   \n18: candidate_set $\\leftarrow$ [SAMPLE(reach, 1)] \u25b7get first in candidate seedset   \n19: while num_seeds do   \n20: last_seed $\\leftarrow$ candidate_set $[-1]$ \u25b7get latest seed   \n21: \u25b7remove shallow reach of last seed from current reach   \n22: reach $\\leftarrow$ reach\u2212SEEDSET_REACH([last_seed], G, p, shallow_horizon)   \n23: candidate_set $\\leftarrow$ candidate_set $^+$ [SAMPLE(reach, 1)] \u25b7extend new seedset   \n24: num_seeds $\\leftarrow$ num_seeds \u22121   \n25: curr_score $\\leftarrow$ -BETA_FAIRNESS(seedset, fair_to_efficacy)   \n26: candidate_score $\\leftarrow$ -BETA_FAIRNESS(candidate_set, fair_to_efficacy)   \n27: $\\triangleright$ Metropolis Sampling   \n28: energy_change $\\leftarrow$ curr_score \u2212candidate_score   \n29: accept_prob $\\leftarrow$ CLIP(exp(exploit_to_explore \u2217energy_change), [0, 1])   \n30: nonce $.1\\leftarrow U(0,1)$   \n31: if nonce_ $.1<$ accept_prob then   \n32: return candidate_set \u25b7get a better seedset   \n33: else   \n34: nonce_ $.2\\leftarrow U(0,1)$   \n35: if nonce $.2<$ non_acceptance_retention_prob then   \n36: return seedset \u25b7retain existing choice   \n37: else   \n38: random_set \u2190SAMPLE(G.nodes, num_seeds)   \n39: return random_set \u25b7completely random selection rarely ", "page_idx": 22}, {"type": "text", "text": "E.2 Estimating Runtime ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We estimate the running time of Algorithm 2 and 3 combined. For the S3D_STEP, lines 9-13 are constant operations and comprise dataset properties. Line 14, 15 cost $O(|S|)$ . FIT_TO_SIZE can cost up to $O(|S|\\log|V|)$ for sampling new $|S|$ nodes. SEEDSET_REACH does repeated BFS, and so costs $O(R(|V|+|E|))$ . Lines 19-24 cost as follows, ", "page_idx": 22}, {"type": "equation", "text": "$$\nO((|S-1)(R d_{\\mathrm{avg}}^{D_{\\mathrm{max}}}+R|V|+\\log R|V|)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $d_{\\mathrm{avg}}$ is the average degree of the graph, and $D_{\\mathrm{max}}$ is the largest diameter of the graph. The first term here upper bounds the max computation in BFS for $D_{\\mathrm{max}}$ horizon. Other terms follow from the remaining operations in the while loop. Now, lines 25-26 first create an outreach from the corresponding seedsets, costing $O(R(|V|+|E|))$ each, and then analytically calculate $\\beta$ -fairness for all the $R$ final configurations, costing $O(R*1)$ each. In the worst case, we might additionally execute lines 37-39 costing $O(|S|\\log|V|)$ . So, a single S3D_STEP costs ", "page_idx": 22}, {"type": "table", "img_path": "axW8xvQPkF/tmp/9321245b928a61e42f8b0dbfc8c64f165942ac0205ef9e63d5343d9c1d4128d0.jpg", "table_caption": ["Algorithm 3 S3D Iteration: Pseudo Code "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{O(2|S|+|S|\\log|V|+R(|V|+|E|)+(|S-1)(R d_{\\mathrm{avg}}^{D_{\\mathrm{max}}}+R|V|+\\log R|V|)}}\\\\ &{}&{+\\,2(R(|V|+|E|)+R)+|S|\\log|V|)}\\\\ &{}&{=O(|S|\\log|V|+R(|V|+|E|)}\\\\ &{}&{+\\,R|S|+R|S||V|+|S|\\log|V|)}\\\\ &{}&{=O(|S|\\log|V|+R|S||V|)}\\\\ &{}&{=O(R|S||V|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here, we used the assumption that $d_{\\mathrm{avg}}=O(2E/V)=O(1)$ for a sparse graph $E=O(V))$ . Now this S3D_STEP is run $k$ times using S3D_ITERATE to find the best seedset in these $k$ runs. Moreover, we avoid any redundant calculations and memorize $\\beta$ -fariness for any seedset we discover. Hence, the total runtime is $O(k R|S||V|)$ , as claimed. ", "page_idx": 23}, {"type": "text", "text": "E.3 Motivation and Extension to generic Combinatorial Optimization ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The S3D approach to $\\beta$ -fairness optimization in this setting is independently motivated and in general can be extended to any Combinatorial Optimization problem where each choice of initial action at time $t=0$ , amongst exponentially many choices of actions, can lead a system to one of exponentially many states, for which we know the probability distribution of the system achieving one of these states and an associated, possibly non-convex, expected energy profile resulting from this stochastic state occupancy of the system at a later time. S3D then boils down to iteratively trying different initial actions that lead to small changes in the state occupancy distribution that align well with the ideal occupancy distribution, leading to a gradual reduction of the expected potential energy of the resulting system using Metropolis Sampling/Simulated Annealing. ", "page_idx": 23}, {"type": "text", "text": "In this study, the initial action at $t=0$ is the initial seedset choice $S$ , which leads to a distribution of states, called the Outreach distribution on final configuration (Theorem 2.1), that the system, a Social Network here, can reach to. Each such distribution corresponds to a bounded expected \"potential energy\" (keeping the ideally mutually fair distribution as reference) defined on $\\beta$ -fairness\u2013 a mutually fair configuration is defined to be a \"stable\", less \"energetic\" system, and $S3D$ aims to achieve it via an optimal choice of $S$ , $S^{*}$ . ", "page_idx": 23}, {"type": "text", "text": "E.4 Theoretical Guarantees of Convergence in S3D ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The S3D algorithm is similar to non-convex optimization methods such as Simulated Annealing.   \nSuch algorithms do not have theoretical guarantees but do have a long history of empirical success. ", "page_idx": 23}, {"type": "text", "text": "Let $f:{\\mathcal{P}}(V)\\to[0,1]$ be the $\\beta$ -fairness set-evaluation function defined in the power-set of the graph vertex set $V$ . The function can then evaluate any seedset, $S\\subseteq V$ for its $\\beta$ -fairness. Now for iterative optimization purposes, S3D defines a sampling process to define neighbors $S^{\\prime}$ of $S$ , based on similar outreaches $V_{S^{\\prime}}$ and $V_{S}$ . Then S3D essentially followsnon-convex optimization of $f$ using Simulated ", "page_idx": 23}, {"type": "text", "text": "Annealing under Metropolis Sampling at a constant temperature. While Simulated Annealing does not have strict mathematical guarantees to find the global optimum in finite time, its empirical success is well understood in non-convex optimization. ", "page_idx": 24}, {"type": "text", "text": "While Simulated Annealing usually runs for finite iterations defined by an empirically tested temperature schedule, we ran Simulated Annealing under several constant temperatures to estimate the performance of S3D against baselines and concluded that a number of iterations $k\\in[500,1000]$ usually works well in practice. Hence any decaying temperature schedule that translates total iterations in this range should work fine. ", "page_idx": 24}, {"type": "text", "text": "E.5 Illustrative Example ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Consider the information spreading over the graph in Fig. 15 as an independent cascade model with probability $p\\,=\\,0.1$ , with blue and red nodes belonging to two different groups. A greedy strategy would choose the seed set as $S_{g}=3,5$ (enlarged nodes) as shown in Fig. 15a, thus leading to the highly unfair outreach in Fig. 15b. On the contrary, our algorithm S3D promotes the choice $S_{\\mathtt{S3D}}=1,4$ reflected in 15c, which gives the more fair outreach plotted in Fig. 15d, showing that it improves over greedy/sophisticated label-blind seed selection strategies. ", "page_idx": 24}, {"type": "image", "img_path": "axW8xvQPkF/tmp/9af422be8beaee9d21122949829b8c81825e004dd63a84ba21999d83b1ded802.jpg", "img_caption": ["Figure 15: Toy example to show label-aware choice using S3D over a label-blind seedset selection process. The enlarged nodes are selected seeds. Since the graph is small, the outreach discretization bucket have been granularized for improved readability. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "F Error Bars on Fairness and Efficiency Experiments ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Referring to Fig. 7, we mention $2\\sigma$ symmetrical error bars on 100 repetitions for each of the experiment pipelines, as follows. Since each experiment itself runs on $R=1$ , 000 realizations, we take $100R=\\dot{1}0^{5}$ samples of each random graph encoded social network dataset. ", "page_idx": 25}, {"type": "table", "img_path": "axW8xvQPkF/tmp/c7535038f023d7049e8bde197b0c1db0709eccfd6f5de1a05d5998aac1fed9b6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "axW8xvQPkF/tmp/e37508aa68453f88fde544290421a0c024cde6b89cb8450bb728b4eb03935810.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "axW8xvQPkF/tmp/3ce39db5e0aa130757d58681563809e4cb0d0bb7f1aedad69d98115082f6bd05.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "axW8xvQPkF/tmp/2bff42eaf49521eecc0449421034bb538f1b4a2ea2d414fb1d51dd9895c23731.jpg", "table_caption": ["Eff-Mean Efficiency-Err-Bar $(\\pm2\\sigma)$ Fair-Mean Fairness-Err-Bar $(\\pm2\\sigma)$ "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G Declaration of Computational Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "All experiments were performed on a local PC on a single CPU core $3.5\\:\\mathrm{GHz}$ . Except for datasets DZ, INS, all datasets were loaded and operated on a local PC with $\\mathrm{32\\,GB}$ of RAM. For the largest datasets (DZ, INS), we used remote compute clusters with $\\sim64$ GB memory and similar CPU capabilities. For the code development, we broadly used Python $3\\cdot10+$ , numpy, jupyter, and networkx [9]. Runtime for each non-S3D configured experiment on datasets except DZ, INS, was $10-15$ minutes. For DZ, INS, this was approximately $1-2$ hours. For S3D optimizations to be satisfactory, we ran each small dataset (except DZ, INS) for 1.5 hours additionally. For massive datasets DZ, INS, the compute cluster took $\\sim4$ days for $k=10$ steps. The total set of experiments made, including the failed and passed or submitted ones, roughly took the same order of resources separately. ", "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper sheds light on the limitation of most of the fairness metrics used in the context of Social Influence Maximization. In particular, as mentioned in the Abstract, In the literature, fairness is often quantified in terms of the expected outreach within individual communities. In this paper, we demonstrate that such fairness metrics can be misleading since they overlook the stochastic nature of information diffusion processes. We propose a new metric that overcomes such limitations and, based on it, we propose a new seed selection policy that strategically takes into account for mutual information. This is made explicit in the abstract by saying We tackle this problem by designing a new fairness metric that captures variability in outreach through optimal transport theory. We propose a new seed-selection algorithm that optimizes both outreach and our new fairness metric, and we show its efficacy on several real datasets. All these aspects are also mentioned in the Contributions paragraph of the Introduction. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes. Limitations are listed in the second paragraph of Section 5 entitled Limitations. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 27}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The construction and validity of our metric is theoretically and didactically explained in the paper in Section 3.1 and Section 3.3. Also, Appendix E, provides a formal proof for the complexity of our proposed seed-selection algorithm. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: To replicate our experiments, we (i) open-sourced our code, (ii) put a reference for each of the datasets used (see Section 3.2) (iii) provide a detailed pseudo-code for the seed-selection algorithm (See Appendix E), and (iv) give clear explanation of the diffusion process used (independent cascade) and the parameters choices (propagation probability, size of the seed set, etc., see Section 3.2). ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our code is publicly available at https://github.com/nicolaslanzetti/ fairness-sim-ot. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Yes, all the hyperparameters involved in the experiments execution are specified in Section 3.2 and Appendix D. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Plots requiring statistical significance are the ones in Fig. 7. Plots in Fig. 7 are associated with statistical bounds reported in Appendix F. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Computational resources are detailed in Appendix G. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS code of ethics. All the datasets we tested our algorithm on preserve anonimity. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 31}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Yes, this is explicitly done in the Introduction and Abstract when we explicitly mention the limitation of the current approaches that we propose to overcome. For example we say: As such, outcomes such as \u201cin $50\\%$ of the cases, no one of group A receives the information and everyone in group B receives it and in other $50\\%$ , the opposite happens\u201d, which always results in largely unfair outcomes, are classified as fair by a variety of fairness metrics in the literature. We tackle this problem by designing a new fairness metric that captures variability in outreach through optimal transport theory. We propose a new seed-selection algorithm that optimizes both outreach and our new fairness metric, and we show its efficacy on several real datasets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ", "page_idx": 31}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Yes, all the sources of existing datasets and algorithms have been cited and can be found in the References. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We documented the code with instructions in README files and dedicated comments in the code. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 32}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 33}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 33}]