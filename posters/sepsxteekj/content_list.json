[{"type": "text", "text": "Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Manuel Brenner1,2\u2217, Christoph J\u00fcrgen Hemmer1,3\u2217, Zahra Monfared2, Daniel Durstewitz1,2,3 ", "page_idx": 0}, {"type": "text", "text": "1Dept. of Theoretical Neuroscience, Central Institute of Mental Health, Medical Faculty, Heidelberg University, Germany   \n2Interdisciplinary Center for Scientific Computing (IWR), Heidelberg University, Germany 3Faculty of Physics and Astronomy, Heidelberg University, Heidelberg, Germany ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dynamical systems (DS) theory is fundamental for many areas of science and engineering. It can provide deep insights into the behavior of systems evolving in time, as typically described by differential or recursive equations. A common approach to facilitate mathematical tractability and interpretability of DS models involves decomposing nonlinear DS into multiple linear DS separated by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are popular in engineering and a frequent choice in mathematics for analyzing the topological properties of DS. However, hand-crafting such models is tedious and only possible for very low-dimensional scenarios, while inferring them from data usually gives rise to unnecessarily complex representations with very many linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) which automatically and robustly produce most parsimonious PWL representations of DS from time series data, using as few PWL nonlinearities as possible. AL-RNNs can be efficiently trained with any SOTA algorithm for dynamical systems reconstruction (DSR), and naturally give rise to a symbolic encoding of the underlying DS that provably preserves important topological properties. We show that for the Lorenz and R\u00f6ssler systems, AL-RNNs discover, in a purely data-driven way, the known topologically minimal PWL representations of the corresponding chaotic attractors. We further illustrate on two challenging empirical datasets that interpretable symbolic encodings of the dynamics can be achieved, tremendously facilitating mathematical and computational analysis of the underlying systems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dynamical systems (DS) underlie many real-world phenomena of scientific and practical relevance. Complex chaotic DS are believed to govern market dynamics [65], the rhythms of the brain [16], climate systems [100], or ecosystems [71]. A by now rapidly growing field in scientific ML is dynamical systems reconstruction (DSR), where the goal is to learn a DS model directly from data that constitutes a generative surrogate model of the data-generating DS. DSR increasingly relies on deep learning, especially in contexts where dynamics are too complex to be captured by simple equations or where the underlying processes are not fully understood. ", "page_idx": 0}, {"type": "text", "text": "One way of making DSR models mathematically accessible is piecewise linear (PWL) designs, popular among engineers for decades [8, 17, 43, 81, 91]. In the mathematical theory of DS, PWL models also play a special role and simplify many types of analysis [3, 54], such as the characterization of bifurcations [27, 10, 42, 40, 77, 70]. This is because linear DS are well-understood and straightforward to analyze, while nonlinear DS lack an equally simple description [78, 94, 13]. RNNs based on PWL activation functions, like rectified-linear units (ReLUs), have been proposed recently for learning mathematically tractable DSR models. Such piecewise-linear RNNs (PLRNNs), combined with effective training techniques for controlling gradient flows [67, 39], achieve state-of-the-art (SOTA) performance across a wide range of DSR tasks, including challenging (high-dimensional, noisy, chaotic, partially observed ...) empirical time series [25, 11, 39, 12]. However, while featuring a PWL design, the resulting constructions are often complex, with a large number of linear subregions required to capture the data properly, hence still impeding effective analysis. On the other hand, a class of switching linear DS has been proposed to decompose nonlinear DS into linear regions combined with switching states that determine the transitions between these regions [1, 31, 28, 58, 56, 57, 2]. However, the underlying assumptions of these models and the complexity of the inference mechanisms these entail, often make their training challenging and impede their efficient application to DSR problems, especially when moving to real-world scenarios and higher-dimensional systems. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Here we propose almost-linear RNNs (AL-RNNs) which combine linear units and ReLUs, but can use as few of the latter as necessary to achieve a most parsimonious representation in terms of linear subregions. AL-RNNs are easy and effective to train by any SOTA algorithm for DSR. Through this, they are able to robustly identify topologically or geometrically minimal representations of well-known chaotic systems. Their structure translates naturally into a symbolic coding that preserves important topological properties. These features make AL-RNNs highly interpretable and mathematically tractable, enabling to harvest tools from symbolic dynamics [74, 55], including the representation of empirically observed DS via minimal topological and computational graphs. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Dynamical systems reconstruction The field of data-driven DSR has been rapidly expanding in recent years. On the one hand there are approaches based on function libraries for approximating unknown vector fields, which have become particularly popular in some areas like physics [53, 64]. Among these, Sparse Identification of Nonlinear Dynamics (SINDy) and its variants [14, 60, 45, 21, 66, 44] is probably the most popular. Since in these models sets of differential equations are directly formulated in terms of known, predefined function libraries, instead of using NN black-box approximators, they have some level of interpretability in the sense that they are human-readable and can easily be related to established mathematical building blocks in physical or biological theories [37, 83]. This does not necessarily make them mathematically tractable, however, since systems of nonlinear differential equations are in themselves usually hard to analyze (in fact, their behavior is much of the core topic of DS theory [78]). They also have other limitations, including a difficulty in capturing complex and noisy empirical data [11, 39], as they usually require considerable prior knowledge about the system\u2019s underlying structure (i.e., which terms to include in the function library). This somewhat limits their applicability for discovering novel phenomena. On the other hand, many recent powerful DSR methods rely on universal approximators, in particular the fact that sufficiently large RNNs can approximate any underlying DS [29, 47, 34]. Such methods may be grouped into several broad classes, including reservoir computers [76, 79, 80], neural ODEs/PDEs [20, 46, 4, 48], neural/ Koopman operators [15, 63, 75, 6, 73, 30, 104], and RNNs [99, 25, 101, 19, 11, 84, 39]. The latter are commonly trained by variants of backpropagation through time (BPTT, [101, 102, 11]), combined with specific control techniques [67] to remedy the exploding/ vanishing gradients problem [9, 67, 11, 39]. While DSR algorithms based on universal approximators achieve SOTA performance on DSR tasks, and often work particularly well on empirical time series [11, 39], they commonly deliver a complex model structure that is difficult to interpret and parse mathematically. ", "page_idx": 1}, {"type": "text", "text": "Nonlinear dynamics via linear DS The idea of approaching nonlinear DS through our good grasp of linear DS has been around for quite a long time, reflected in important theoretical results like the Hartman-Grobman theorem $[36]^{1}$ or Koopman operator theory [49, 15]. While linear DS are easy to analyze and well understood [78, 94], however, they cannot properly capture most real-world systems, as they cannot produce many important DS phenomena such as limit cycles, chaos, or multistability. This has motivated the modeling of complex dynamics in terms of compositions of locally linear dynamics, as the next best alternative, i.e. piecewise-linear (PWL) maps or ODE systems [18, 41, 22]. PWL models have been popular in engineering and mathematics for several decades for these reasons, including earlier attempts for learning such models directly from data [93, 24]. Switching linear DS are one particular brand of PWL models with a long tradition in DS and control theory [23, 95, 1, 31, 28]. These systems model nonlinear dynamics through a set of linear (or affine) DS combined with a switching rule which decides which linear DS is currently active. Likewise, in mathematics PWL models served well for investigating generic properties of nonlinear systems, e.g. the tent map which is topologically conjugate to the logistic map [3]. PWL models often lend themselves to particularly convenient symbolic representations [54, 3], based on which important topological properties of the underlying system, e.g. the nature and number of unstable periodic orbits embedded within a chaotic attractor, can be analyzed [69, 74, 98]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "More recently, various modern approaches for inferring PWL models from data have been formulated. For instance, switching state space models combine hidden Markov models with linear DS, jointly inferring the state of a switching (random) variable with the linear DS parameters conditioned on these states [31]. Various extensions of this basic setup like recurrent and hierarchical switching linear DS and fully Bayesian inference methods have been advanced in recent years [92, 58, 56]. However, inference in these models is often complex and not necessarily optimized for DSR, limiting their applicability to mainly low-dimensional scenarios with comparatively simple dynamics. Most of these models are also discontinuous in their dynamics across switches, while most commonly we would require the state variables to evolve continuously across the whole of state space. Piecewise-linear RNNs (PLRNNs), and, relatedly, threshold-linear networks [33, 105, 109], on the other hand, are based on familiar ReLUs and hence change continuously across their switching manifolds [25, 51, 88]. They also have some biological justification [33, 25]. Commonly they are trained by variants of BPTT backed up by specific control-theoretic approaches like sparse [67] or generalized [39] teacher forcing which make them SOTA on many DSR tasks. Different PLRNN architectures have been proposed to enhance expressivity or reduce the dimensionality of trained models [11, 39]. Yet, while these advances may yield comparatively low-dimensional state spaces, the number of different linear subregions that need to be allocated usually remains very high, hampering efficient mathematical analysis. ", "page_idx": 2}, {"type": "text", "text": "3 Methodological and Theoretical Prerequisites ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 AL-RNN Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider a piecewise linear recurrent neural network (PLRNN, [25]): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t}=F_{\\theta}(z_{t-1})=A z_{t-1}+W\\phi(z_{t-1})+h,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where diagonal $A\\,\\in\\,\\mathbb{R}^{M\\times M}$ contains linear self-connections, $W\\,\\in\\,\\mathbb{R}^{M\\times M}$ are nonlinear connections between units, $\\pmb{h}\\in\\mathbb{R}^{M}$ is a bias term, and $\\phi(z)=\\operatorname*{max}[0,z]$ is an element-wise ReLU nonlinearity. To expose the piecewise linear structure of this model more clearly, by noting that the slope of the ReLU is either 0 or 1 depending on the sign of $z_{m,t}$ , one can reformulate this as ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{t}=(A+W D_{\\Omega(t-1)})z_{t-1}+h:=W_{\\Omega(t-1)}\\,z_{t-1}+h,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $D_{\\Omega(t)}:=d i a g(d_{\\Omega(t)})$ is a diagonal matrix and $\\pmb{d}_{\\Omega(t)}=(d_{1},d_{2},\\cdot\\cdot\\cdot\\,,d_{M})$ an indicator vector with $d_{m}(z_{m,t})=1$ whenever $z_{m,t}>0$ and zero otherwise [26]. For the $2^{M}$ different configurations of $D_{\\Omega(t)}$ , $D_{\\Omega^{k}}$ , $k\\in\\{1,2,\\cdots\\,,2^{M}\\}$ , the phase space of system eq. 2 is divided into $2^{M}$ subregions with linear dynamics ", "page_idx": 2}, {"type": "equation", "text": "$$\nz_{t+1}\\,=\\,W_{\\Omega^{k}}\\,z_{t}+\\,h,\\,\\qquad\\,W_{\\Omega^{k}}:={\\cal A}+W{\\cal D}_{\\Omega^{k}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Empirically, $M$ often needs to be quite large (at least on the order of the number of observations) for achieving good reconstructions of observed DS. Since the number of subregions grows as $2^{M}$ , analyzing inferred models in terms of the subregions can thus become very challenging. We therefore introduce a novel variant of the PLRNN in which only a subset of $P<<M$ units are equipped with a ReLU nonlinearity, yielding ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{t}=A z_{t-1}+W\\Phi^{*}(z_{t-1})+h}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In this formulation, we thus only have $2^{P}$ different linear subregions, while still accommodating a sufficiently large number of latent states for capturing unobserved dimensions in the data and disentangling trajectories sufficiently [96, 86].2 ", "page_idx": 3}, {"type": "text", "text": "The model is trained on the $N$ -dimensional observations $\\{\\pmb{x}_{t}\\},t=1\\ldots T,\\pmb{x}_{t}\\in\\mathbb{R}^{N}$ , by a variant of sparse teacher forcing called identity teacher forcing [67, 11]. In identity teacher forcing, the first $N$ latent states (\u2018readout neurons\u2019) are replaced by the $N$ -dimensional observations every $\\tau$ time steps, where $\\tau$ is chosen such as to optimally control trajectory and gradient flows, avoiding exploding gradients while providing the model sufficient opportunity to unroll into the future to capture the underlying DS\u2019 long-term behavior (see [67, 39] for details); see Appx. A.2 for details on training. We emphasize that sparse teacher forcing is only used for training the model, and is turned off at test time where the model generates new trajectories completely independent from the data. ", "page_idx": 3}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/b35923e105497ef798c0f848747734add2d5eb9d617e1db516a8a23cc558e031.jpg", "img_caption": ["Figure 1: Illustration of the AL-RNN architecture. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Theoretical Background: Symbolic Dynamics and Symbolic Coding of AL-RNN ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The mathematical field of symbolic dynamics formulates conditions under which a DS has a unique symbolic representation and discusses how to harvest this symbolic representation to prove certain properties of the underlying system, which otherwise may be more difficult to address [74, 55]. In fact, symbolic dynamics has led to many powerful insights and formal results in DS theory, e.g. about the properties of chaos or type and number of periodic orbits [32, 106]. An appealing feature of symbolic dynamics for the field of ML/AI is that it links concepts in DS theory to computational concepts like finite state automata or formal languages, as well as graph theory [55, 35]. It can thus facilitate the computational interpretation of natural or trained dynamical systems, like RNNs. ", "page_idx": 3}, {"type": "text", "text": "Assume we have an alphabet of $n$ symbols ${\\mathcal{A}}\\,=\\,\\{0,\\dots,n-1\\}$ , from which we form infinite sequences (bidirectionally or only in forward-time) $\\begin{array}{r}{\\mathbf{\\vec{a}}=\\dots\\mathbf{\\vec{\\alpha}}..\\,.\\,a_{-2}a_{-1}.a_{0}a_{1}a_{2}\\dots..\\,}\\end{array}$ with $a_{k}\\in\\mathcal A$ , and the dot separating past from future (i.e., indices $k<0$ indicate backward time, and $k\\geq0$ present and forward time). Then the space of all possible sequences, together with the so-called (left) shift operator given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sigma({\\pmb a})=\\sigma(\\dots a_{-2}a_{-1}.a_{0}a_{1}a_{2}\\dots)=\\dots a_{-1}a_{0}.a_{1}a_{2}a_{3}\\dots\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "defines the full shift space $A^{\\mathbb{Z}}$ . We denote by $\\sigma^{k}=\\sigma\\circ\\sigma\\circ\\cdots\\circ\\sigma$ the $k$ -times iteration of the shift. Now consider a DS $(S,\\phi)$ consisting of a metric (state) space $S$ and a recursive (flow) map $\\phi$ . The flow map $\\phi_{\\Delta t}(\\pmb{x})$ advances the system\u2019s current state $\\textbf{\\em x}$ by $\\Delta t$ and may be thought of as the solution operator of the underlying DS ${\\dot{\\mathbf{x}}}=f(\\mathbf{x})$ [78]. When training RNNs $\\begin{array}{r}{z_{t}=F_{\\theta}(z_{t-1})}\\end{array}$ on time series of observations $\\{g(\\mathbf{x}_{k\\Delta t})\\},k=1\\dots T$ , from the underlying DS, where $g$ is the observation function, we are trying to approximate this flow map. Assume the whole state space $S$ can be partitioned into a finite set ${\\mathcal{U}}=\\{U_{0}\\dots U_{n-1}\\}$ of disjoint open sets $U_{e}$ , such that $\\begin{array}{r}{S=\\bigcup_{e=0}^{n-1}\\overline{{U_{e}}}}\\end{array}$ , i.e. $S$ is covered by the union of the closures of these sets. We call this a topological part ition of $S$ [55]. ", "page_idx": 3}, {"type": "text", "text": "The central idea now is to assign a unique symbol $a_{e}\\in\\mathcal{A}$ to each set $U_{e}\\in\\mathcal{U}$ , with $n=|\\mathcal{U}|=|\\mathcal{A}|$ . As a trajectory ${\\mathbf{}}x(t)$ of the underlying DS travels through the system\u2019s state space $S$ , observed at times $k\\Delta t$ as it passes through different subregions $U_{e}$ , it gives rise to a specific symbolic sequence $\\pmb{a}_{\\pmb{x},\\phi}$ (with a unique symbol assigned at each time step via $h:S\\to A$ , $\\pmb{x}_{k\\Delta t}\\mapsto a_{k}]$ ). We may thus think of the shift operator $\\sigma$ as moving along a trajectory in correspondence with the flow map $\\phi_{\\Delta t}(\\pmb{x})$ . If the symbolic coding of each trajectory is unique, $\\boldsymbol{\\mathcal{U}}$ may constitute a Markov partition (see Appx. B for a formal definition). We denote by $(A_{S,\\phi},\\sigma)$ the shift of finite type induced by the flow $\\phi$ which picks out from the full shift space $A^{\\mathbb{Z}}$ only those admissible symbolic sequences that correspond to valid trajectories of $(S,\\phi)$ (we will use the term \u2018induced by\u2019 to refer to this property). The set of admissible blocks constitutes the language of $(A_{S,\\phi},\\sigma)$ . Every shift of finite type has a graph representation $\\mathcal{G}=\\{\\mathcal{V},\\mathcal{E}\\}$ (Fig. 2), with either the edges $e_{i j}\\in\\mathcal{E}$ or vertices $v_{i}\\in\\mathcal{V}$ of the graph encoding the permitted transitions among symbols $a_{k}\\in\\mathcal A$ of admissible sequences $\\pmb{a}\\in A$ [55]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The finite collection ${\\mathcal{U}}=\\left\\{U_{0}\\,.\\,.\\,.\\,U_{n-1}\\right\\}$ , $n\\,=\\,2^{P}$ , of linear subregions of an AL-RNN defined in eq. 4, separated by the switching manifolds $\\Sigma_{i,j}=\\overline{{U_{i}}}\\cap\\overline{{U_{j}}}$ between every pair of neighboring subregions $U_{i}$ and $U_{j}$ , forms a topological partition. Here we use this partition as the basis of our symbolic coding and the respective symbolic dynamics $(A_{\\mathcal{U},F_{\\pmb{\\theta}}},\\sigma)$ induced by the AL-RNN, where we assign to each state $z_{t}\\in S$ (i.e., at each time point) the unique symbol $a_{t}\\in\\mathcal A$ such that $a_{t}=a_{i}$ iff $z_{t}\\in U_{i}$ (Fig. 2).3 In the corresponding symbolic graphs, we identify vertices $v_{i}$ with symbols $a_{i}\\in\\mathcal{A}$ and draw a directed edge $e_{i j}$ from $v_{i}$ to $v_{j}$ whenever $F_{\\theta}(U_{i}\\cap B)\\cap U_{j}\\neq\\emptyset$ , where $B$ is the attracting set of interest (see Appx. A.1 for details). As we will show further below, this particular partition has useful theoretical properties that makes the symbolic coding topologically interpretable w.r.t. the AL-RNN map $F_{\\theta}$ . In fact, a large literature in symbolic dynamics has dealt with the relation between the dynamics in a finite shift space and that of a PWL map, like, e.g., the tent map, with a partition of $S$ into the map\u2019s different linear subregions as we use here for the AL-RNN [55, 3, 68]. ", "page_idx": 4}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/954f3a8b757f7cb966dca41db0d081096229147762b60b3df8124cbf135ef4c2.jpg", "img_caption": ["Figure 2: Illustration of symbolic approach (3 panels on the left) and geometrical graphs (right). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Theoretical Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Recall that within each subregion $U_{e}$ the map $F_{\\theta}$ is monotonic and the dynamics are linear (ruling out certain possibilities, like chaos or isolated cycles occurring within just one subregion). We furthermore assume that the dynamics are globally non-diverging (this could be strictly enforced through \u2018state-clipping\u2019 and constraints on matrix $\\pmb{A}$ in eq. 4, see Hess et al. [39], but will also be the case for a well-trained AL-RNN). Here we claim that for hyperbolic AL-RNNs $F_{\\pmb\\theta}{}^{4}$ , we have 1:1 relations between important topological objects in the AL-RNN\u2019s state space and those of the symbolic coding formed from the linear subregions $U_{e}$ of the AL-RNN, as expressed in the following theoretical results. ", "page_idx": 4}, {"type": "text", "text": "Consider a hyperbolic, non-globally-diverging AL-RNN $F_{\\theta}$ , eq. 4, and a topological partition $\\boldsymbol{\\mathcal{U}}$ of the state space into its linear subregions $\\bar{U_{e}}\\subseteq S,e=0\\dots\\bar{2^{P}}-1$ . Denote by $(A_{\\mathcal{U},F_{\\theta},\\,\\sigma})$ the finite shift induced by $(S,F_{\\theta})$ , with each $a_{e}\\in\\mathcal{A}$ of its alphabet $\\boldsymbol{\\mathcal{A}}$ associated with exactly one linear subregion $U_{e}\\in\\mathcal{U}$ , and let us consider the system\u2019s evolution only in forward time. Then the following holds: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. An orbit $\\begin{array}{l l l}{\\Omega_{S}}&{=}&{\\{z_{1},\\dots,z_{n},\\dots\\}}\\end{array}$ of the AL-RNN $F_{\\theta}$ is asymptotically fixed (i.e., converges to a fixed point) if and only if the corresponding symbolic sequence $\\textbf{\\em a}=$ $(a_{1}a_{2}a_{3}\\dots a_{N-1})(a^{*})^{\\infty}\\in A_{\\mathcal{U},F_{\\theta}}$ is an eventually fixed point of the shift map $\\sigma$ (where by \u2018eventually\u2019 we mean it exactly lands on the point in the limit, see Appx. $B$ for a precise definition). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Proof. See Appx. B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. An orbit $\\Omega_{S}=\\{z_{1},\\ldots,z_{n},\\ldots\\}$ of the AL-RNN $F_{\\theta}$ is asymptotically $p$ -periodic if and only if the corresponding symbolic sequence ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\textbf{\\em a}}={\\big(}a_{1}a_{2}\\dots a_{N-1}{\\big)}{\\big(}a_{1}^{*}a_{2}^{*}\\dots a_{p}^{*}{\\big)}^{\\infty}\\in A_{\\mathcal{U},F_{\\theta}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "is an eventually $p$ -periodic orbit of the shift map $\\sigma$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appx. B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. An orbit $\\Omega_{S}=\\{z_{1},\\ldots,z_{n},\\ldots\\}$ is an asymptotically aperiodic (irregular) orbit of the AL-RNN $F_{\\theta}$ if and only if the corresponding symbolic sequence $(a_{1},\\ldots,a_{n},\\ldots)$ is aperiodic. ", "page_idx": 5}, {"type": "text", "text": "Proof. See Appx. B. ", "page_idx": 5}, {"type": "text", "text": "Loosely speaking, these results confirm that fixed points of our symbolic coding correspond to fixed points of the AL-RNN, cycles to cycles, and chaos to chaos, thus preserving important topological properties in the symbolic representation. ", "page_idx": 5}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To assess the quality of DSR, we employed established performance criteria based on long-term, invariant topological, geometrical, and temporal features of DS [51, 11, 39]. Due to exponential trajectory divergence in chaotic systems, mean-squared prediction errors rapidly grow even for well-trained systems, and hence are only of limited suitability for evaluating DSR quality [108, 67]. Thus, we prioritize the geometric agreement between true and reconstructed attractors, quantified by a Kullback-Leibler divergence ( $\\boldsymbol{D}_{\\mathrm{stsp}}$ , Appx. A.2) [51]. Additionally, we examine the long-term temporal agreement between true and reconstructed time series by evaluating the average dimensionwise Hellinger distance $(D_{\\mathrm{H}})$ between their power spectra (Appx. A.2). We first confirmed that the AL-RNN is at least on par with other SOTA methods for DSR. We then tested AL-RNNs on two commonly employed benchmark DS for which minimal PWL representations are known, the famous Lorenz-63 model of atmospheric convection [61] and the chaotic R\u00f6ssler system [85]. We finally explored the suitability of our approach on two real-world examples, human electrocardiogram (ECG) and human functional magnetic resonance imaging (fMRI) data. ", "page_idx": 5}, {"type": "text", "text": "5.1 SOTA performance ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While our goal here is a technique that constructs topologically minimal, interpretable DS representations, at the same time we do not want to compromise on DSR performance which should still be within the same ballpark as existing SOTA methods. We checked this on the Lorenz-63, R\u00f6ssler and ECG data noted above, and in addition on the higher-dimensional chaotic Lorenz-96 system [62] and on human electroencephalogram (EEG) data. Table $1$ confirms that the AL-RNN is not only on par with, but indeed outperforms most other techniques when trained with sparse teacher forcing (which may be rooted in its simple and parsimonious design). ", "page_idx": 5}, {"type": "text", "text": "5.2 Reconstructed Systems Occupy a Small Number of Subregions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Fig. 3 illustrates reconstruction performance for varying numbers of ReLU nonlinearities at constant network size $M$ . We found that a small number of PWL units already significantly improves performance, especially for the Lorenz-63 and R\u00f6ssler systems, and that beyond that number performance starts to plateau (or even briefly decrease again). Additionally, some linear units are necessary to sufficiently expand the space, but they cannot compensate for an insufficient number of PWL units (Fig. 10).5 Moreover, as shown in Fig. 4 (left), the number of linear subregions explored by the ", "page_idx": 5}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/7fd696864d1149d139339ee2cb66f5775758e68787ae284aec35e61f30bc9a15.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Quantification of DSR quality in terms of attractor geometry disagreement $\\boldsymbol{D}_{\\mathrm{stsp}}$ , top row) and disagreement in temporal structure $D_{\\mathrm{H}}$ , bottom row) as a function of the number of ReLUs $(P)$ in the AL-RNN (R\u00f6ssler: $M=20$ , Lorenz-63: $M=20$ , ECG: $M=100$ , fMRI: $M=50,$ ). The little humps at $P=3$ for the Lorenz-63 indicate that performance may sometimes first degrade again when passing the number of minimally necessary PWL units (see also Fig. 9). Error bars $=\\mathrm{SEM}$ . ", "page_idx": 6}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/7dfffdd6eecd0e3132a7dbce78aa93c918b14f7e097131279b297bf2c11748ec.jpg", "img_caption": ["Figure 4: Left: Number of linear subregions traversed by trained AL-RNNs as a function of the number $P$ of ReLUs. Theoretical limit $\\bar{(2^{P})}$ in red. Right: Cumulative number of data (trajectory) points covered by linear subregions in trained AL-RNNs (R\u00f6ssler: $M=20,P=10$ , Lorenz-63: $\\mathbf{\\bar{\\boldsymbol{M}}}=20,P=10$ , ECG: $M=100,P=10)$ , illustrating that trajectories on an attractor live in a relatively small subset of subregions. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "trained dynamics saturates well below the theoretical limit of $2^{P}$ once this performance threshold is reached. Within this already small subset of explored subregions, generated network activity is furthermore concentrated within an even smaller number of dominant subregions: For instance, for the R\u00f6ssler system 4 out of 45 subregions used cover $80\\%$ of the data (Fig. 4, right). This substantial reduction of necessary linear subregions strongly facilitates the analysis of trained models with respect to fixed points and $k$ -cycles, which naively would require examining $2^{P}$ and $2^{k P}$ combinations of subregions, respectively. To select the optimal number of PWL units, the point where performance starts plateauing (as in Fig. 3) may be chosen. Alternatively, one may restrict the number of linear subregions employed through regularization, adding a penalty for ReLU nonlinearities. This approach, as Fig. 9 illustrates, results in the same number of selected PWL units. ", "page_idx": 6}, {"type": "text", "text": "5.3 Minimal PWL Reconstructions of Chaotic attractors ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Topologically minimal reconstructions Investigating reconstructions with the minimal number of PWL units needed for close-to-optimal performance (Fig. 3), we found that the AL-RNN would deliver reconstructions capturing the overall structure of the attractor using only three (Lorenz-63 system) or two (R\u00f6ssler system) linear subregions (Fig. 5a), explaining the strong performance gains in Fig. 3 for 2 and 1 PWL units, respectively. These representations, and their symbolic coding (Fig. 5c), expose the mechanisms of chaotic dynamics (Fig. 5b). Notably, these closely agree with the minimal topologically equivalent PWL representations of the two chaotic DS as described in Amaral et al. [5]: The Lorenz-63 system has at its core two unstable spiral points in the two lobes, separated by the saddle node in the center (Fig. 5b). For the R\u00f6ssler system, the topologically minimal PWL representation indeed consists of just two subregions [5], one containing an unstable spiral in the $x$ -y plane and the other a \u2018half-spiral\u2019 almost orthogonal to that plane (Fig. 5b). The AL-RNN automatically and robustly discovers these representations from data: across multiple training runs, performance values are very similar (Figs. 23, 25), the assignment of subregions to different parts of the attractor remains almost the same (Figs. $24\\ \\&\\ 25\\$ ), and the regions with linear dynamics ", "page_idx": 6}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/fbb37ef637e2292a0a34eeb0577c04888ca9c994c054b9684e59c892b19cb0a7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: a: Color-coded linear subregions of minimal AL-RNNs representing the R\u00f6ssler (top) and Lorenz-63 (bottom) chaotic attractor. b: Illustration of how the AL-RNN creates the chaotic dynamics. For the R\u00f6ssler, trajectories diverge from an unstable spiral point (true position in gray, learned position in black) into the second subregion, where after about half a cycle they are propelled back into the first. For the Lorenz-63, two unstable spiral points (true: gray; learned: black) create the diverging spiraling dynamics in the two lobes, separated by the saddle node in the center. c: Topological graphs of the symbolic coding. While for the R\u00f6ssler it is fully connected, for the Lorenz-63 the crucial role of the center saddle region in distributing trajectories onto the two lobes is apparent. d: Geometrical divergence $(D_{\\mathrm{stsp}})$ among repeated trainings of AL-RNNs $(n=20)$ ), separately evaluated within each subregion, shows close agreement among different training runs. Likewise, low e: normalized distances between fixed point locations and f: relative differences in maximum absolute eigenvalues $\\sigma^{\\mathrm{max}}$ across 20 trained models indicate that these topologically minimal representations are robustly identified. ", "page_idx": 7}, {"type": "text", "text": "closely agree both in terms of their topology and geometry (in fact, the topological graphs remained identical). This is in contrast to the standard PLRNN, where assignments strongly varied among multiple training runs (Figs. 23, 25). We quantified this further by computing across training runs separately for each subregion $D_{\\mathrm{stsp}}$ (Fig. 5d), the normalized distances between fixed points (Fig. 5e), and the normalized differences between the maximum absolute eigenvalues $\\sigma_{\\mathrm{max}}$ of the AL-RNN\u2019s Jacobians (Fig. 5f), obtaining values close to zero in all three cases (see Fig. 22 for absolute values). While Amaral et al. [5] explicitly handcrafted such minimal PWL representations, the AL-RNN extracts them automatically without the provision of any prior knowledge about the system. ", "page_idx": 7}, {"type": "text", "text": "Geometrically minimal reconstructions While these reconstructions capture the topology of the underlying DS, they do not yet capture the full geometry and temporal structure of the attractor (Fig. 3). Fig. 6 illustrates for the R\u00f6ssler system that as the number of PWL units is further increased to $P=10$ , the geometrical agreement becomes almost perfect. Although the mapping from latent to observation space is not 1:1 (since $M>N$ ), points close in observation space still tended to fall into the same latent subregion, such that the observed system\u2019s attractor still decomposed into distinct subregions, as confirmed by proximity matching (see Appx. A.1). For the R\u00f6ssler system there is just one nonlinearity, the $x\\cdot z$ term in the temporal derivative of $z$ (eq. 14). Accordingly, the AL-RNN devotes most of its subregions to the lobe along the $z$ coordinate, while dynamics in the $(x,y)$ plane is geometrically faithfully represented by only 4 subregions. Hence, the AL-RNN utilizes additional subregions to express finer geometric details where dynamics are more nonlinear. This is apparent from a more geometrical graph representation (see Fig. 2, right), where \u2013 in addition to topological information \u2013 transition probabilities among subregions are being used to construct node distances via the graph Laplacian (see Appx. A.1), see Fig. 6 for the R\u00f6ssler and Fig. 11 for the Lorenz-63. ", "page_idx": 7}, {"type": "text", "text": "5.4 PWL Reconstructions of Real-World Systems ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Topologically minimal reconstructions We next considered two experimental datasets, human ECG data (with $1d$ membrane potential recordings delay-embedded into $N=5$ , see Appx. A.3) and fMRI recordings (with $N\\,=\\,20$ time series extracted, cf. Appx. A.3) from human subjects performing three different types of cognitive task [50, 52]. ", "page_idx": 7}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/1aee9cfa2f6e255acabe29ff0f8b569e3878aafe472d59e3ec02cd32f796edcc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Geometrically minimal reconstruction and graph representation of the R\u00f6ssler attractor $M=30$ , $P=10$ , $D_{s t s p}=0.08$ , $D_{H}=0.06)$ ). a: Provided a sufficient number of linear subregions, the geometry of the attractor is almost perfectly captured. b: Reconstruction with linear subregions color-coded by frequency of visits (dark: most frequently visited regions, yellow: least frequent regions). c: Corresponding geometrical graph, which contains information about transition frequencies via node distances, visualized using the spectral layout in networkx. Note that self-connections were omitted in this representation. d: Connectome of relative transition frequencies between subregions. ", "page_idx": 8}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/7536875fc8e5865d4a79a3b53ea176eb8d6d6f474b6716a9f429920ecd3730b8.jpg", "img_caption": ["Figure 7: a: Freely generated ECG activity using an AL-RNN with 3 linear subregions (color-coded according to subregion) and ground truth time series in black. b: After activation of the Q wave in the third subregion, the second PWL unit is driven far below 0, whose activity, consistent with the known physiology [89], mimics the latent de- and re-polarization process of the interventricular septum. c: Symbolic graph representation of the trained AL-RNN. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "As for the Lorenz-63, for the ECG data we observed a strong performance gain for just $P\\,=\\,2$ PWL units, see Fig. 3. Indeed, Fig. 7a confirms that the complex activity pattern and positive max. Lyapunov exponent $\\lambda_{\\operatorname*{max}}=1.96~s^{-1}$ , ground truth: $\\lambda_{\\operatorname*{max}}^{\\mathrm{true}}=\\bar{2}.19\\;s^{-1}$ [39]) of the ECG time series could be achieved with $P=2$ in only 3 linear subregions. These subregions corresponded to distinct parts of the ECG activity: ramping-up phases (light blue, node $\\#1$ ), declining activity (dark blue, node $\\#2,$ , represented by two unstable spirals with shifted phases (Fig. 12), and the Q wave (medium blue, node $\\#3$ ). The activity in the third region $(\\sigma_{m a x}\\approx1.34$ , other two: $\\sigma_{m a x}\\approx1.02$ , Fig. 12) caused a strong inhibition in the second PWL unit (Fig. 7b) that captures the critical transition initiated by the Q wave. The Q wave triggers the depolarization of the interventricular septum during the QRS complex [89], indicating that this latent depolarization process is captured by the model. These results suggest that the AL-RNN cannot only learn dynamically but also biologically interpretable latent representations. The core aspects of this representation were furthermore consistent across successful reconstructions (Fig. 13). The symbolic sequences corresponding to the graph in Fig. 7 reveal the nearly periodic yet chaotic dynamics of the ECG (Fig. 14).6 ", "page_idx": 8}, {"type": "text", "text": "For the short $T=360,$ ) fMRI time series, $P=3$ often resulted in reconstructions matching the complex activity patterns reasonably well (cf. Fig. 3, right). Fig. 15 illustrates results for an example subject using 8 linear subregions. The second most visited subregion implemented an unstable spiral, while the most visited region had a stable virtual fixed point also located in the second subregion. These two regions covered over $50\\%$ of the data and were strongly connected (Fig. 15b). This balance between stable and unstable activity suggests a mechanism through which the network implements chaotic dynamics, with the stable virtual fixed point pulling activity into the second subregion from which it then diverges again (Fig. 15d). ", "page_idx": 8}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/4a3e79932ceff8ed618ef6154e76879cb6d2cc447ffb8661587f1430e8d6a73c.jpg", "img_caption": ["Figure 8: Reconstructions from human fMRI data using an AL-RNN with $M=100$ total units and $P=2$ PWL units. a: Mean generated BOLD activity color-coded according to the linear subregion. Background color shadings indicate the task stage. b: Generated activity (trajectory points) in the latent space of PWL units with color-coding indicating task stage as in a. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Task stages align with linear subregions To integrate the fMRI signal with cognitive (task-phase) information, in addition to the linear decoder for the BOLD signal, we coupled the PWL units $z_{t}^{p}$ to a categorical decoder model (eq. 12) which predicts the three cognitive task stages and the \u2019Rest and Instruction\u2019 period, as in [52, 12]. Using only $P\\,=\\,2$ PWL units on the fMRI data made it challenging to capture the complex, chaotic long-term activity patterns in the freely generated activity of the AL-RNN. However, the dynamics were still well-approximated locally (Fig. 17). To maintain temporal alignment with the task stages when sampling from the AL-RNN, the AL-RNN\u2019s readout units were reset to the observations every 7 time steps (Fig. 8a). Fig. 8a-b show that in this setup, the four linear subregions of the AL-RNN often closely aligned with the different task stages of the experimental time series. Similar results were obtained across different subjects, with an average classification accuracy of $p=0.78\\pm0.05$ (mean \u00b1 SEM) (see Appx. A.4 for details). While the categorical decoder aids in separating latent states according to task stage, there is nothing that would bias this separation to align with the linear subregions. The observed alignment therefore suggests that the AL-RNN learns to leverage distinct linear dynamics in each subregion to represent differences across cognitive tasks. Furthermore, as shown in Fig. 18, the network weights of the PWL units were significantly larger than those of other units, indicating their critical role in modulating the dynamics and representing task-related variations in brain activity. This approach also demonstrates how local context-aligned linear approximations can be achieved using the AL-RNN, which is useful in areas such as model-predictive control [72, 23, 58]. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Here we introduced a novel variant of a PLRNN, the AL-RNN, which learns to represent nonlinear DS with as few PWL nonlinearities as possible. Despite its simple design and the minimal hyperparameter tuning required, the AL-RNN robustly and automatically identifies highly interpretable, topologically minimal representations of complex nonlinear DS, reproducing known minimal PWL forms of chaotic attractors [5]. Such minimal PWL forms that allow for an interpretable symbolic and graph-theoretical representation were discovered even from challenging physiological and neuroscientific data. They also profoundly ease subsequent model analysis. For instance, with only a few linear subregions to consider, the search for fixed points or cycles becomes very fast and efficient [26]. ", "page_idx": 9}, {"type": "text", "text": "Limitations While this seems promising, how to determine whether a topologically minimal and valid reconstruction from empirical data has truly been achieved remains an open topic. Performance curves as in Fig. 3 or Fig. 9 give an indication of how many PWL units may be required to yield an optimal minimal representation, but whether there is a more principled way of automatically inferring the optimal number $P$ of PWL nonlinearities from data may be an interesting future direction. Finally, the current finding that even for empirical ECG and fMRI data a few linear subregions $(\\le8)$ suffice for faithful reconstructions is encouraging. Whether this more generally will be the case in empirical scenarios is another interesting and open question. Not all types of (empirically observed) dynamical systems may easily allow for such topologically minimal representations. ", "page_idx": 9}, {"type": "text", "text": "All code created is available at https://github.com/DurstewitzLab/ALRNN-DSR. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was funded by the Federal Ministry of Science, Education, and Culture (MWK) of the state of Baden-W\u00fcrttemberg within the AI Health Innovation Cluster Initiative, by the German Research Foundation (DFG) within Germany\u2019s Excellence Strategy EXC 2181/1 \u2013 390900948 (STRUCTURES), and through DFG individual grant Du 354/15-1 to DD. ZM was funded by the Federal Ministry of Education and Research (BMBF) through project OIDLITDSM, 01IS24061. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] G. Ackerson and K. Fu. On state estimation in switching environments. IEEE Transactions on Automatic Control, 15(1):10\u201317, February 1970. ISSN 1558-2523. doi: 10.1109/TAC.1970. 1099359. URL https://ieeexplore.ieee.org/document/1099359. Conference Name: IEEE Transactions on Automatic Control. ", "page_idx": 10}, {"type": "text", "text": "[2] Xavier Alameda-Pineda, Vincent Drouard, and Radu Patrice Horaud. Variational Inference and Learning of Piecewise Linear Dynamical Systems. IEEE Transactions on Neural Networks and Learning Systems, 33(8):3753\u20133764, August 2022. ISSN 2162-2388. doi: 10.1109/TNNLS. 2021.3054407. URL https://ieeexplore.ieee.org/document/9353398. Conference Name: IEEE Transactions on Neural Networks and Learning Systems. [3] Kathleen T. Alligood, Tim D. Sauer, and James A. Yorke. Chaos: An Introduction to Dynamical Systems. Textbooks in Mathematical Sciences. Springer, 1996. ISBN 978-0-387-94677-1 978-0-387-22492-3. doi: 10.1007/b97589.   \n[4] Victor M. Martinez Alvarez, Rare\u00b8s Ro\u00b8sca, and Cristian G. F\u02d8alcut\u00b8escu. DyNODE: Neural Ordinary Differential Equations for Dynamics Modeling in Continuous Control, September 2020.   \n[5] Gleison F. V. Amaral, Christophe Letellier, and Luis Antonio Aguirre. Piecewise affine models of chaotic attractors: the Rossler and Lorenz systems. Chaos (Woodbury, N.Y.), 16(1):013115, March 2006. ISSN 1054-1500. doi: 10.1063/1.2149527.   \n[6] Omri Azencot, N. Benjamin Erichson, Vanessa Lin, and Michael W. Mahoney. Forecasting Sequential Data using Consistent Koopman Autoencoders. In Proceedings of the 37th International Conference on Machine Learning, 2020. URL http://arxiv.org/abs/2003.02236.   \n[7] Mikhail Belkin and Partha Niyogi. Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering. In Advances in Neural Information Processing Systems, volume 14. MIT Press, 2001. URL https://proceedings.neurips.cc/paper_files/ paper/2001/hash/f106b7f99d2cb30c3db1c3cc0fde9ccb-Abstract.html.   \n[8] Alberto Bemporad, Francesco Borrelli, and Manfred Morari. Piecewise linear optimal controllers for hybrid systems. In Proceedings of the 2000 American Control Conference. ACC (IEEE Cat. No. 00CH36334), volume 2, pages 1190\u20131194. IEEE, 2000.   \n[9] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157\u2013166, 1994. ISSN 1045-9227. doi: 10.1109/72.279181.   \n[10] Mario Di Bernardo, Mark I. Feigin, Stephen J. Hogan, and Martin E. Homer. Local Analysis of C-Bifurcations in n-Dimensional Piecewise-Smooth Dynamical Systems. Chaos, Solitons and Fractals: the interdisciplinary journal of Nonlinear Science, and Nonequilibrium and Complex Phenomena, 11(10):1881\u20131908, 1999. ISSN 0960-0779. URL https://www.infona.pl//resource/bwmeta1.element. elsevier-b61cfd87-9650-310f-bd8f-4bd7e7174946.   \n[11] Manuel Brenner, Florian Hess, Jonas M. Mikhaeil, Leonard F. Bereska, Zahra Monfared, Po-Chen Kuo, and Daniel Durstewitz. Tractable Dendritic RNNs for Reconstructing Nonlinear Dynamical Systems. In Proceedings of the 39th International Conference on Machine Learning, pages 2292\u20132320. PMLR, June 2022. URL https://proceedings.mlr.press/v162/ brenner22a.html. ISSN: 2640-3498.   \n[12] Manuel Brenner, Florian Hess, Georgia Koppe, and Daniel Durstewitz. Integrating Multimodal Data for Joint Generative Modeling of Complex Dynamics. In Proceedings of the 41st International Conference on Machine Learning, pages 4482\u20134516. PMLR, July 2024. URL https://proceedings.mlr.press/v235/brenner24a.html. ISSN: 2640-3498.   \n[13] Steven L Brunton and J Nathan Kutz. Data-driven science and engineering: Machine learning, dynamical systems, and control. Cambridge University Press, 2019.   \n[14] Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences USA, 113(15):3932\u20133937, 2016. ISSN 0027-8424. doi: 10.1073/pnas. 1517384113.   \n[15] Steven L. Brunton, Marko Budi\u0161i\u00b4c, Eurika Kaiser, and J. Nathan Kutz. Modern Koopman Theory for Dynamical Systems, October 2021. arXiv:2102.12086 [cs, eess, math].   \n[16] Gyorgy Buzsaki. Rhythms of the Brain. Oxford University Press, August 2006. ISBN 978-0-19-804125-2. Google-Books-ID: ldz58irprjYC.   \n[17] Victoriano Carmona, Emilio Freire, Enrique Ponce, and Francisco Torres. On simplifying and classifying piecewise-linear systems. IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications, 49(5):609\u2013620, 2002.   \n[18] Martin Casdagli. Nonlinear prediction of chaotic time series. Physica D: Nonlinear Phenomena, 35(3):335\u2013356, May 1989. ISSN 0167-2789. doi: 10.1016/0167-2789(89)90074-2.   \n[19] Rok Cestnik and Markus Abel. Inferring the dynamics of oscillatory systems using recurrent neural networks. Chaos: An Interdisciplinary Journal of Nonlinear Science, 29(6):063128, June 2019. ISSN 1054-1500. doi: 10.1063/1.5096918. URL https://doi.org/10.1063/ 1.5096918.   \n[20] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural Ordinary Differential Equations. In Advances in Neural Information Processing Systems 31, 2018. URL http://arxiv.org/abs/1806.07366.   \n[21] Alexandre Cortiella, Kwang-Chun Park, and Alireza Doostan. Sparse identification of nonlinear dynamical systems via reweighted l1-regularized least squares. Computer Methods in Applied Mechanics and Engineering, 376:113620, April 2021. ISSN 0045-7825. doi: 10.1016/j.cma.2020.113620.   \n[22] Antonio C. Costa, Tosif Ahamed, and Greg J. Stephens. Adaptive, locally linear models of complex dynamics. Proceedings of the National Academy of Sciences, 116(5):1501\u20131510, January 2019. doi: 10.1073/pnas.1813476116. Publisher: Proceedings of the National Academy of Sciences.   \n[23] J. Daafouz, P. Riedinger, and C. Iung. Stability analysis and control synthesis for switched systems: a switched Lyapunov function approach. IEEE Transactions on Automatic Control, 47(11):1883\u20131887, November 2002. ISSN 1558-2523. doi: 10.1109/TAC.2002.804474. URL https://ieeexplore.ieee.org/document/1047016. Conference Name: IEEE Transactions on Automatic Control.   \n[24] Oscar De Feo and Marco Storace. Piecewise-Linear Identification of Nonlinear Dynamical Systems in View of Their Circuit Implementations. IEEE Transactions on Circuits and Systems I: Regular Papers, 54(7):1542\u20131554, July 2007. ISSN 1558-0806. doi: 10.1109/TCSI.2007. 899613. URL https://ieeexplore.ieee.org/document/4268404. Conference Name: IEEE Transactions on Circuits and Systems I: Regular Papers.   \n[25] Daniel Durstewitz. A state space approach for piecewise-linear recurrent neural networks for identifying computational dynamics from neural measurements. PLoS Comput. Biol., 13(6): e1005542, 2017. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1005542.   \n[26] Lukas Eisenmann, Zahra Monfared, Niclas G\u00f6ring, and Daniel Durstewitz. Bifurcations and loss jumps in RNN training. Advances in Neural Information Processing Systems, 36, 2024.   \n[27] Mark I Feigin. The increasingly complex structure of the bifurcation tree of a piecewisesmooth system. Journal of Applied Mathematics and Mechanics, 59(6):853\u2013863, 1995. ISSN 0021-8928. doi: 10.1016/0021-8928(95)00118-2. URL https://www.sciencedirect. com/science/article/pii/0021892895001182.   \n[28] Emily Fox, Erik Sudderth, Michael Jordan, and Alan Willsky. Nonparametric Bayesian Learning of Switching Linear Dynamical Systems. In Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008. URL https://papers.nips.cc/paper_files/paper/2008/hash/ 950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html.   \n[29] Ken-ichi Funahashi. On the approximate realization of continuous mappings by neural networks. Neural Networks, 1989. doi: 10.1016/0893-6080(89)90003-8.   \n[30] Nicholas Geneva and Nicholas Zabaras. Transformers for modeling physical systems. Neural Networks, 146:272\u2013289, February 2022. ISSN 0893-6080. doi: 10.1016/j.neunet.2021.11.022.   \n[31] Z. Ghahramani and G. E. Hinton. Variational learning for switching state-space models. Neural Computation, 12(4):831\u2013864, April 2000. ISSN 0899-7667. doi: 10.1162/ 089976600300015619.   \n[32] John Guckenheimer and Philip Holmes. Nonlinear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields, volume 42 of Applied Mathematical Sciences. Springer, New York, NY, 1983. ISBN 978-1-4612-7020-1 978-1-4612-1140-2. doi: 10.1007/978-1-4612-1140-2. URL http://link.springer.com/10.1007/978-1-4612-1140-2.   \n[33] Richard Hahnloser and H. Sebastian Seung. Permitted and Forbidden Sets in Symmetric Threshold-Linear Networks. In Advances in Neural Information Processing Systems, volume 13. MIT Press, 2000.   \n[34] Joshua Hanson and Maxim Raginsky. Universal simulation of stable dynamical systems by recurrent neural nets. In Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning Research, pages 384\u2013392. PMLR, 10\u201311 Jun 2020. URL https://proceedings.mlr.press/v120/hanson20a.html.   \n[35] Bailin Hao and Weimou Zheng. Applied symbolic dynamics and chaos. World Scientific, 1998.   \n[36] Philip Hartman. A lemma in the theory of structural stability of differential equations. Proceedings of the American Mathematical Society, 11(4):610\u2013620, 1960. ISSN 0002-9939, 1088-6826. doi: 10.1090/S0002-9939-1960-0121542-7. URL https://www.ams.org/ proc/1960-011-04/S0002-9939-1960-0121542-7/.   \n[37] Niklas Heim, V\u00e1clav \u0160m\u00eddl, and Tom\u00e1\u0161 Pevn\u00fd. Rodent: Relevance determination in differential equations. arXiv preprint arXiv:1912.00656, 2019. URL http://arxiv.org/abs/1912. 00656.   \n[38] Christoph J\u00fcrgen Hemmer, Manuel Brenner, Florian Hess, and Daniel Durstewitz. Optimal Recurrent Network Topologies for Dynamical Systems Reconstruction. In Proceedings of the 41st International Conference on Machine Learning, pages 18174\u201318204. PMLR, July 2024. URL https://proceedings.mlr.press/v235/hemmer24a.html. ISSN: 2640-3498.   \n[39] Florian Hess, Zahra Monfared, Manuel Brenner, and Daniel Durstewitz. Generalized Teacher Forcing for Learning Chaotic Dynamics. In Proceedings of the 40th International Conference on Machine Learning, pages 13017\u201313049. PMLR, July 2023. URL https://proceedings. mlr.press/v202/hess23a.html. ISSN: 2640-3498.   \n[40] Stephen J. Hogan, L. Higham, and T. C. L. Griffin. Dynamics of a piecewise linear map with a gap. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 463(2077):49\u201365, 2007. doi: 10.1098/rspa.2006.1735. URL https: //royalsocietypublishing.org/doi/abs/10.1098/rspa.2006.1735.   \n[41] Anthony R. Ives and Vasilis Dakos. Detecting dynamical changes in nonlinear time series using locally linear state-space models. Ecosphere, 3(6):art58, 2012. ISSN 2150-8925. doi: 10.1890/ES11-00347.1. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1890/ES11- 00347.1.   \n[42] Parag Jain and Soumitro Banerjee. Border-collision bifurcations in one-dimensional discontinuous maps. Int. J. Bifurcation Chaos, 13(11):3341\u20133351, 2003. ISSN 0218-1274. doi: 10.1142/S0218127403008533. URL https://www.worldscientific.com/doi/abs/10. 1142/S0218127403008533.   \n[43] A.L. Juloski, S. Weiland, and W.P.M.H. Heemels. A Bayesian approach to identification of hybrid systems. IEEE Transactions on Automatic Control, 50(10):1520\u20131533, October 2005. ISSN 1558-2523. doi: 10.1109/TAC.2005.856649. URL https://ieeexplore.ieee.org/ document/1516255. Conference Name: IEEE Transactions on Automatic Control.   \n[44] Kadierdan Kaheman, Steven L. Brunton, and J. Nathan Kutz. Automatic differentiation to simultaneously identify nonlinear dynamics and extract noise probability distributions from data. Machine Learning: Science and Technology, 3(1):015031, March 2022. ISSN 2632-2153. doi: 10.1088/2632-2153/ac567a. Publisher: IOP Publishing.   \n[45] E. Kaiser, J. N. Kutz, and S. L. Brunton. Sparse identification of nonlinear dynamics for model predictive control in the low-data limit. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 474(2219):20180335, November 2018. doi: 10.1098/rspa. 2018.0335. Publisher: Royal Society.   \n[46] Daniel Karlsson and Olle Svanstr\u00f6m. Modelling Dynamical Systems Using Neural Ordinary Differential Equations, 2019. URL https://hdl.handle.net/20.500.12380/256887.   \n[47] Masahiro Kimura and Ryohei Nakano. Learning dynamical systems by recurrent neural networks from orbits. Neural Networks, 11(9):1589\u20131599, 1998.   \n[48] Joon-Hyuk Ko, Hankyul Koh, Nojun Park, and Wonho Jhe. Homotopy-based training of NeuralODEs for accurate dynamics discovery, May 2023. URL http://arxiv.org/abs/ 2210.01407. arXiv:2210.01407 [physics].   \n[49] B. O. Koopman and J. v. Neumann. Dynamical Systems of Continuous Spectra. Proceedings of the National Academy of Sciences, 18(3):255\u2013263, March 1932. doi: 10.1073/pnas.18.3.255. Publisher: Proceedings of the National Academy of Sciences.   \n[50] Georgia Koppe, Harald Gruppe, Gebhard Sammer, Bernd Gallhofer, Peter Kirsch, and Stefanie Lis. Temporal unpredictability of a stimulus sequence affects brain activation differently depending on cognitive task demands. NeuroImage, 101:236\u2013244, 2014. ISSN 1095-9572. doi: 10.1016/j.neuroimage.2014.07.008.   \n[51] Georgia Koppe, Hazem Toutounji, Peter Kirsch, Stefanie Lis, and Daniel Durstewitz. Identifying nonlinear dynamical systems via generative recurrent neural networks with applications to fMRI. PLOS Computational Biology, 15(8):e1007263, 2019. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1007263.   \n[52] Daniel Kramer, Philine L Bommer, Carlo Tombolini, Georgia Koppe, and Daniel Durstewitz. Reconstructing nonlinear dynamical systems from multi-modal time series. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 11613\u201311633. PMLR, 17\u201323 Jul 2022. URL https: //proceedings.mlr.press/v162/kramer22a.html.   \n[53] William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabr\u00edcio Olivetti de Fran\u00e7a, Marco Virgolin, Ying Jin, Michael Kommenda, and Jason H. Moore. Contemporary Symbolic Regression Methods and their Relative Performance, July 2021. URL http://arxiv.org/ abs/2107.14351. arXiv:2107.14351 [cs].   \n[54] Douglas Lind and Brian Marcus. An Introduction to Symbolic Dynamics and Coding. Cambridge University Press, Cambridge, 1995. doi: 10.1017/CBO9780511626302. URL https://www.cambridge.org/   \n[55] Douglas Lind and Brian Marcus. An Introduction to Symbolic Dynamics and Coding. Cambridge Mathematical Library. Cambridge University Press, 2 edition, 2021.   \n[56] Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pages 914\u2013922. PMLR, April 2017. URL https://proceedings.mlr.press/ v54/linderman17a.html. ISSN: 2640-3498.   \n[57] Scott W. Linderman and Matthew J. Johnson. Structure-Exploiting variational inference for recurrent switching linear dynamical systems. In 2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), pages 1\u20135, December 2017. doi: 10.1109/CAMSAP.2017.8313132. URL https://ieeexplore.ieee. org/document/8313132.   \n[58] Scott W. Linderman, Andrew C. Miller, Ryan P. Adams, David M. Blei, Liam Paninski, and Matthew J. Johnson. Recurrent switching linear dynamical systems. arXiv:1610.08466 [stat], October 2016. URL http://arxiv.org/abs/1610.08466. arXiv: 1610.08466.   \n[59] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id $\\equiv$ rkgz2aEKDr.   \n[60] Jean-Christophe Loiseau and Steven L. Brunton. Constrained sparse Galerkin regression. Journal of Fluid Mechanics, 838:42\u201367, March 2018. ISSN 0022-1120, 1469-7645. doi: 10.1017/jfm.2017.823. Publisher: Cambridge University Press.   \n[61] Edward N Lorenz. Deterministic nonperiodic flow. Journal of atmospheric sciences, 20(2): 130\u2013141, 1963.   \n[62] Edward N Lorenz. Predictability: A problem partly solved. In Proc. Seminar on predictability, volume 1, 1996.   \n[63] Bethany Lusch, J. Nathan Kutz, and Steven L. Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. Nat Commun, 9(1):4950, December 2018. ISSN 2041- 1723. doi: 10.1038/s41467-018-07210-0. URL http://arxiv.org/abs/1712.09707. arXiv: 1712.09707.   \n[64] Nour Makke and Sanjay Chawla. Interpretable scientific discovery with symbolic regression: a review. Artificial Intelligence Review, 57(1):2, January 2024. ISSN 1573-7462. doi: 10.1007/s10462-023-10622-0. URL https://doi.org/10.1007/s10462-023-10622-0.   \n[65] Benoit Mandelbrot and Richard L. Hudson. The Misbehavior of Markets: A Fractal View of Financial Turbulence. Basic Books, March 2007. ISBN 978-0-465-00468-3. Google-BooksID: GMKeUqufPQ0C.   \n[66] Daniel A. Messenger and David M. Bortz. Weak SINDy: Galerkin-Based Data-Driven Model Selection. Multiscale Modeling & Simulation, 19(3):1474\u20131497, January 2021. ISSN 1540-3459. doi: 10.1137/20M1343166. URL https://epubs.siam.org/doi/10.1137/ 20M1343166. Publisher: Society for Industrial and Applied Mathematics.   \n[67] Jonas Mikhaeil, Zahra Monfared, and Daniel Durstewitz. On the difficulty of learning chaotic dynamics with RNNs. Advances in Neural Information Processing Systems, 35:11297\u201311312, December 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ hash/495e55f361708bedbab5d81f92048dcd-Abstract-Conference.html.   \n[68] John Milnor. On the concept of attractor. Communications in Mathematical Physics, 99(2): 177\u2013195, June 1985. ISSN 1432-0916. doi: 10.1007/BF01212280. URL https://doi.org/ 10.1007/BF01212280.   \n[69] K. Mischaikow, M. Mrozek, J. Reiss, and A. Szymczak. Construction of Symbolic Dynamics from Experimental Time Series. Physical Review Letters, 82(6):1144\u20131147, February 1999. doi: 10.1103/PhysRevLett.82.1144. URL https://link.aps.org/doi/10.1103/ PhysRevLett.82.1144. Publisher: American Physical Society.   \n[70] Zahra Monfared and Daniel Durstewitz. Existence of n-cycles and border-collision bifurcations in piecewise-linear continuous maps with applications to recurrent neural networks. Nonlinear Dyn, 101(2):1037\u20131052, 2020. ISSN 1573-269X. doi: 10.1007/s11071-020-05841-x. URL https://doi.org/10.1007/s11071-020-05841-x.   \n[71] Peter J. Mumby, Alan Hastings, and Helen J. Edwards. Thresholds and the resilience of caribbean coral reefs. Nature, 450(7166):98\u2013101, 2007. doi: 10.1038/nature06252. URL https://doi.org/10.1038/nature06252.   \n[72] Kenneth R. Muske and James B. Rawlings. Model predictive control with linear models. AIChE Journal, 39(2):262\u2013287, 1993. ISSN 1547-5905. doi: 10.1002/ aic.690390208. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/aic. 690390208. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690390208.   \n[73] Ilan Naiman and Omri Azencot. A Koopman Approach to Understanding Sequence Neural Models. arXiv:2102.07824 [cs, math], October 2021. URL http://arxiv.org/abs/2102. 07824. arXiv: 2102.07824.   \n[74] George Osipenko and Stephen Campbell. Applied symbolic dynamics: attractors and flitrations. Discrete and Continuous Dynamical Systems, 5(1):43\u201360, September 1998. ISSN 1078-0947. doi: 10.3934/dcds.1999.5.43. URL https://www.aimsciences.org/en/article/doi/ 10.3934/dcds.1999.5.43. Publisher: Discrete and Continuous Dynamical Systems.   \n[75] Samuel E. Otto and Clarence W. Rowley. Linearly-Recurrent Autoencoder Networks for Learning Dynamics, January 2019. URL http://arxiv.org/abs/1712.01378. arXiv:1712.01378 [cs, math, stat].   \n[76] Jaideep Pathak, Zhixin Lu, Brian R. Hunt, Michelle Girvan, and Edward Ott. Using Machine Learning to Replicate Chaotic Attractors and Calculate Lyapunov Exponents from Data. Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(12):121102, December 2017. ISSN 1054-1500, 1089-7682. doi: 10.1063/1.5010300. URL http://arxiv.org/abs/1710. 07313. arXiv: 1710.07313.   \n[77] Mahashweta Patra. Multiple Attractor Bifurcation in Three-Dimensional Piecewise Linear Maps. Int. J. Bifurcation Chaos, 28(10):1830032, 2018. ISSN 0218-1274. doi: 10.1142/ S021812741830032X. URL https://www.worldscientific.com/doi/abs/10.1142/ S021812741830032X.   \n[78] Lawrence Perko. Differential equations and dynamical systems. Number 7 in Texts in applied mathematics. Springer, New York, 3rd ed edition, 2001. ISBN 978-0-387-95116-4.   \n[79] Jason A. Platt, Stephen G. Penny, Timothy A. Smith, Tse-Chun Chen, and Henry D. I. Abarbanel. A Systematic Exploration of Reservoir Computing for Forecasting Complex Spatiotemporal Dynamics, January 2022. URL http://arxiv.org/abs/2201.08910. arXiv:2201.08910 [cs].   \n[80] Jason A Platt, Stephen G Penny, Timothy A Smith, Tse-Chun Chen, and Henry DI Abarbanel. Constraining chaos: Enforcing dynamical invariants in the training of reservoir computers. Chaos: An Interdisciplinary Journal of Nonlinear Science, 33(10), 2023.   \n[81] Anders Rantzer and Mikael Johansson. Piecewise linear quadratic optimal control. IEEE transactions on automatic control, 45(4):629\u2013637, 2000.   \n[82] Attila Reiss, Ina Indlekofer, Philip Schmidt, and Kristof Van Laerhoven. Deep ppg: Large-scale heart rate estimation with convolutional neural networks. Sensors, 19(14):3079, 2019.   \n[83] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206\u2013215, May 2019. ISSN 2522-5839. doi: 10.1038/s42256-019-0048-x. URL https://www.nature.com/ articles/s42256-019-0048-x. Publisher: Nature Publishing Group.   \n[84] T Konstantin Rusch, Siddhartha Mishra, N Benjamin Erichson, and Michael W Mahoney. Long expressive memory for sequence modeling. In International Conference on Learning Representations, 2022.   \n[85] O. E. R\u00f6ssler. An equation for continuous chaos. Physics Letters A, 57(5):397\u2013398, 1976. ISSN 0375-9601. doi: 10.1016/0375-9601(76)90101-8. URL https://www.sciencedirect. com/science/article/pii/0375960176901018.   \n[86] Tim Sauer, James A Yorke, and Martin Casdagli. Embedology. Journal of statistical Physics, 65(3):579\u2013616, 1991.   \n[87] Gerwin Schalk, Dennis J. McFarland, Thilo Hinterberger, Niels Birbaumer, and Jonathan R. Wolpaw. BCI2000: a general-purpose brain-computer interface (BCI) system. IEEE transactions on bio-medical engineering, 51(6):1034\u20131043, 2000. ISSN 0018-9294. doi: 10.1109/TBME.2004.827072.   \n[88] Dominik Schmidt, Georgia Koppe, Zahra Monfared, Max Beutelspacher, and Daniel Durstewitz. Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies. In Proceedings of the 9th International Conference on Learning Representations, 2021. URL http://arxiv.org/abs/1910.03471.   \n[89] Demetrio Sodi-Pallares, Mar\u00eda Isabel Rodriguez, Leonardo O. Chait, and Rudolf Zuckermann. The activation of the interventricular septum. American Heart Journal, 41(4):569\u2013608, April 1951. ISSN 0002-8703. doi: 10.1016/0002-8703(51)90024-5. URL https://www. sciencedirect.com/science/article/pii/0002870351900245.   \n[90] Justin Solomon. PDE Approaches to Graph Analysis, April 2015. URL http://arxiv.org/ abs/1505.00185. arXiv:1505.00185 [cs, math].   \n[91] E. Sontag. Nonlinear regulation: The piecewise linear approach. IEEE Transactions on Automatic Control, 26(2):346\u2013358, April 1981. ISSN 1558-2523. doi: 10.1109/TAC.1981. 1102596. URL https://ieeexplore.ieee.org/document/1102596. Conference Name: IEEE Transactions on Automatic Control.   \n[92] Ioan Stanculescu, Christopher K. I. Williams, and Yvonne Freer. A Hierarchical Switching Linear Dynamical System Applied to the Detection of Sepsis in Neonatal Condition Monitoring. In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence (UAI 2014), 2014. URL https://www.research.ed.ac.uk/en/publications/ a-hierarchical-switching-linear-dynamical-system-applied-to-the-d.   \n[93] M. Storace and O. De Feo. Piecewise-linear approximation of nonlinear dynamical systems. IEEE Transactions on Circuits and Systems I: Regular Papers, 51(4):830\u2013842, April 2004. ISSN 1558-0806. doi: 10.1109/TCSI.2004.823664. Conference Name: IEEE Transactions on Circuits and Systems I: Regular Papers.   \n[94] Steven H. Strogatz. Nonlinear Dynamics and Chaos. CRC Press, 1 edition, 2015. ISBN 978-0-429-96111-3. doi: 10.1201/9780429492563. URL https://www.taylorfrancis. com/books/9780429961113.   \n[95] Zhendong Sun. Switched Linear Systems: Control and Design. Springer Science & Business Media, March 2006. ISBN 978-1-84628-131-0. Google-Books-ID: u4GArZN1bmsC.   \n[96] Floris Takens. Detecting strange attractors in turbulence. In Dynamical Systems and Turbulence, Warwick 1980, volume 898, pages 366\u2013381. Springer, 1981. ISBN 978-3-540-11171-9 978-3- 540-38945-3. URL http://link.springer.com/10.1007/BFb0091924.   \n[97] Sachin S. Talathi and Aniket Vartak. Improving performance of recurrent neural network with relu nonlinearity. In Proceedings of the 4th International Conference on Learning Representations, 2016. URL http://arxiv.org/abs/1511.03771. [98] M. Tomas-Rodriguez and S. P. Banks. Linear approximations to nonlinear dynamical systems with applications to stability and spectral theory. IMA Journal of Mathematical Control and Information, 20(1):89\u2013103, March 2003. ISSN 0265-0754. doi: 10.1093/imamci/20.1.89. URL https://doi.org/10.1093/imamci/20.1.89.   \n[99] Adam P. Trischler and Gabriele M.T. D\u2019Eleuterio. Synthesis of recurrent neural networks for dynamical system simulation. Neural Networks, 80:67\u201378, 2016. ISSN 08936080. doi: 10.1016/j.neunet.2016.04.001. URL https://linkinghub.elsevier.com/retrieve/ pii/S0893608016300314.   \n[100] Eli Tziperman, Harvey Scher, Stephen E. Zebiak, and Mark A. Cane. Controlling spatiotemporal chaos in a realistic el ni\u00f1o prediction model. Phys. Rev. Lett., 79:1034\u20131037, Aug 1997. doi: 10.1103/PhysRevLett.79.1034. URL https://link.aps.org/doi/10.1103/ PhysRevLett.79.1034.   \n[101] Pantelis R. Vlachas, Wonmin Byeon, Zhong Y. Wan, Themistoklis P. Sapsis, and Petros Koumoutsakos. Data-driven forecasting of high-dimensional chaotic systems with long shortterm memory networks. Proc. R. Soc. A., 474(2213):20170844, 2018. ISSN 1364-5021, 1471-2946. doi: 10.1098/rspa.2017.0844. URL https://royalsocietypublishing.org/ doi/10.1098/rspa.2017.0844.   \n[102] Pantelis R. Vlachas, Jaideep Pathak, Brian R. Hunt, Themistoklis P. Sapsis, Michelle Girvan, Edward Ott, and Petros Koumoutsakos. Backpropagation Algorithms and Reservoir Computing in Recurrent Neural Networks for the Forecasting of Complex Spatiotemporal Dynamics. arXiv:1910.05266 [physics], February 2020. URL http://arxiv.org/abs/1910.05266. arXiv: 1910.05266.   \n[103] Ryan Vogt, Maximilian Puelma Touzel, Eli Shlizerman, and Guillaume Lajoie. On lyapunov exponents for RNNs: Understanding information propagation using dynamical systems tools. Frontiers in Applied Mathematics and Statistics, 8, 2022. ISSN 2297-4687. URL https: //www.frontiersin.org/articles/10.3389/fams.2022.818799.   \n[104] Rui Wang, Yihe Dong, Sercan \u00d6 Arik, and Rose Yu. Koopman Neural Forecaster for Time Series with Temporal Distribution Shifts, October 2022. URL http://arxiv.org/abs/ 2210.03675. arXiv:2210.03675 [cs, stat].   \n[105] H. Wersing, W. J. Beyn, and H. Ritter. Dynamical stability conditions for recurrent neural networks with unsaturating piecewise linear transfer functions. Neural Computation, 13(8): 1811\u20131825, August 2001. ISSN 0899-7667. doi: 10.1162/08997660152469350.   \n[106] Stephen Wiggins. Global Bifurcations and Chaos, volume 73 of Applied Mathematical Sciences. Springer, New York, NY, 1988. ISBN 978-1-4612-1041-2 978-1-4612- 1042-9. doi: 10.1007/978-1-4612-1042-9. URL http://link.springer.com/10.1007/ 978-1-4612-1042-9.   \n[107] Alan Wolf, Jack B. Swift, Harry L. Swinney, and John A. Vastano. Determining lyapunov exponents from a time series. Physica D: Nonlinear Phenomena, 16(3):285\u2013317, 1985. ISSN 0167-2789. doi: 10.1016/0167-2789(85)90011-9. URL https://www.sciencedirect. com/science/article/pii/0167278985900119.   \n[108] Simon N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466(7310):1102\u20131104, August 2010. ISSN 1476-4687. doi: 10.1038/nature09319. URL https://www.nature.com/articles/nature09319. Number: 7310 Publisher: Nature Publishing Group.   \n[109] Zhang Yi, K. K. Tan, and T. H. Lee. Multistability Analysis for Recurrent Neural Networks with Unsaturating Piecewise Linear Transfer Functions. Neural Computation, 15(3):639\u2013 662, March 2003. ISSN 0899-7667. doi: 10.1162/089976603321192112. URL https: //doi.org/10.1162/089976603321192112. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A.1 Graph Representation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The symbolic graph construction followed the rules given in Sect. 3.2: Most generally, each linear subregion $U_{i}$ is assigned a node (symbol), and a directed edge is drawn between nodes $i,j,i\\rightarrow j$ , whenever $F_{\\pmb\\theta}(U_{i})\\cap U_{j}\\neq\\emptyset$ . Here, however, we are mostly interested in the topological graphs representing particular chaotic attractors (like the Lorenz-63 or R\u00f6ssler attractor), and hence restrict the graph representation to the nodes corresponding to subregions visited by trajectories on the attractor $B$ , and edges drawn when $F_{\\pmb\\theta}(U_{i}\\cap B)\\cap\\mathbf{\\dot{U}}_{j}\\,\\neq\\,\\emptyset$ . More specifically, we first sampled a long trajectory $\\mathbf{\\bar{Z}}\\,=\\,\\{z_{1},\\dots,z_{T}\\}$ with $T\\,=\\,100,000$ time steps, removed the first 1000 time steps as transients, and counted all transitions between any two subregions $U_{i},U_{j}$ . To obtain a more nuanced geometrical/ statistical picture, we also evaluated the relative number of time steps the trajectory spent in each subregion $U_{i}$ (i.e., an estimate of the occupation measure), $|\\{\\boldsymbol{z}_{t}\\in U_{i},t>1000\\}|/(100,000-1000)$ , as well as the relative frequency of transitions between any two subregions $U_{i},U_{j}$ , $|\\{z_{t},z_{t+1}|t>1000,z_{t}\\in U_{i},F_{\\theta}(z_{t})\\in U_{j}\\}|/(1\\dot{0}0,000-1001)$ , yielding a weight for each edge, or a distance between nodes through the graph\u2019s Laplacian (see below). To enhance the readability of the larger graphs in Figs. 6, 11, 15 and 21, we removed self-connections (time steps where the trajectory remained within a single subregion). ", "page_idx": 18}, {"type": "text", "text": "Laplacian matrix The Laplacian matrix of a graph is defined as $L=D-A$ where $\\pmb{A}$ is the adjacency matrix of the graph containing the weights (transition probabilities), and $_{D}$ is the outdegree matrix, which is a diagonal matrix where each element is the sum of the outgoing edge weights of node $i$ , $\\textstyle D_{i i}=\\sum_{j}A_{i j}$ . The spectral layout in networkx uses the eigenvectors of the Laplacian matrix corresponding to the smallest non-zero eigenvalues as positions for the nodes. This tends to group more tightly connected nodes closer together. The Laplacian is more widely used as a dimensionality reduction technique in ML, for example in Laplacian eigenmaps [7], and has also been used to represent discretizations of PDEs as graphs [90]. ", "page_idx": 18}, {"type": "text", "text": "Proximity matching The mapping from latent space to observation space is not unique because $M>N$ , and hence the nullspace of the linear observation model is non-empty (does not contain just the 0 vector). For the AL-RNN this is evident from the fact that the non-readout neurons, particularly the PWL neurons, do not contribute to the observations. However, in practice, for the freely generated activity of trained AL-RNNs, points that fall into the same linear subregion in latent space also were close in observation space. This implies that attractors are segmented into different subregions in latent space in accordance with the observable dynamics. To numerically confirm this, we found that proximal points in observation space were typically related to the same linear subregion when generating activity from trained AL-RNNs: We conducted proximity matching by defining a threshold distance (e.g. $d=0.05$ , corresponding to $5\\%$ of the data variance) and assessing whether generated latent trajectory points proximal in observation space fall into the same or different subregions. We found that for the geometrically minimal reconstruction of the R\u00f6ssler system (Fig. 6), only $6\\%$ of proximal data points (within $d$ ) belonged to different subregions, while for the Lorenz-63 attractor (Fig. 11) $4\\%$ of proximal data points (within $d_{.}$ ) belonged to different subregions, confirming that the attractors were segmented into relatively distinct patches. ", "page_idx": 18}, {"type": "text", "text": "A.2 Methodological Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Training method Our training method rests on a variant of sparse teacher forcing. This approach has recently been proven to effectively tackle gradient divergence when training on observations from chaotic DS [67] and has shown SOTA performance on benchmark and real-world systems in DSR [11, 39]. In sparse teacher forcing, the idea is to directly replace latent states (or a subset of them) with states inferred from observations at intervals $\\tau$ while leaving the network to evolve freely otherwise. To obtain forced states, the observation model needs to be \u2018pseudo-inverted\u2019. Here we employ a specific variant of sparse teacher forcing called identity teacher forcing [67, 11], where this pseudo-inversion becomes trivial by adopting an identity mapping as the observation model: ", "page_idx": 18}, {"type": "text", "text": "with $\\mathcal{T}\\in\\mathbb{R}^{N\\times M}$ , and $\\mathcal{T}_{r r}=1$ for the $N$ read-out neurons, $r\\leq N$ , and zeroes elsewhere. During training, the read-out states are replaced with observations every $\\tau$ time steps: ", "page_idx": 19}, {"type": "equation", "text": "$$\nz_{t+1}={\\left\\{\\begin{array}{l l}{F_{\\theta}({\\tilde{z}}_{t})}&{{\\mathrm{if~}}t\\in{\\mathcal{T}}}\\\\ {F_{\\theta}(z_{t})}&{{\\mathrm{else}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $\\mathcal{T}=\\{l\\tau+1\\}_{l\\in\\mathbb{N}_{0}}$ , and $\\tilde{z}_{t}=(x_{t},z_{N+1:M,t})^{T}$ . Employing identity teacher forcing splits the AL-RNN into essentially three types of units, the $N$ linear readout-neurons $\\ensuremath{\\boldsymbol{z}}_{t}^{r}$ which are equivalent to the predicted observations and teacher-forced during training, the remaining $M-P-N$ linear neurons $z_{t}^{l}$ , and the $P$ nonlinear neurons $z_{t}^{p}$ . The overall model and architecture are illustrated in Fig. 1. The AL-RNN can be trained using Mean Squared Error (MSE) loss over model predictions and observations: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell_{M S E}(\\hat{\\mathbf{X}},\\mathbf{X})=\\frac{1}{N\\cdot T}\\sum_{t=1}^{T}\\left\\Vert\\hat{\\pmb{x}}_{t}-\\pmb{x}_{t}\\right\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\hat{X}$ are the model predictions and $\\mathbf{\\deltaX}$ denotes the training sequence of length $T$ . We found that performance was better when the read-out neurons were linear rather than ReLU-based. Note that the $M-N$ non-readout neurons, including the PWL neurons, do not explicitly contribute to the loss function. We used rectified adaptive moment estimation (RADAM) [59] as the optimizer, with $\\textsl{L}=~50$ batches with $\\textit{S}=\\textsl{16}$ sequences per epoch. Further, we chose ${\\cal M}\\;=\\;\\{20,20,100,100,100,130\\}.$ , $\\tau\\;=\\;\\{16,8,10,7,20,10\\}$ , $\\bar{T}\\,=\\,\\{200,300,50,72,50,100\\}$ , initial learning rates $\\eta_{\\mathrm{start}}\\,=\\,\\{10^{-3},5\\cdot10^{-3},2\\cdot10^{-3},5\\cdot10^{-3}$ 3, 10\u22123, 10\u22123}, $\\eta_{\\mathrm{end}}\\,=\\,10^{-5}$ and $e p o c h s=\\{2000,3000,4000,2000,3000,2000\\}$ for the {Lorenz-63, R\u00f6ssler, ECG, fMRI,Lorenz96,EEG} dataset, respectively. Parameters in $W$ were initialized using a Gaussian initialization with $\\sigma\\,=\\,0.01$ , $^h$ as a vector of zeros, and $\\pmb{A}$ as the diagonal of a normalized positive-definite random matrix [11, 97]. The initial latent state $\\pmb{z}_{1}=[\\pmb{x}_{1},\\pmb{L}\\breve{\\pmb{x}}_{1}]^{T}$ is estimated from $x_{1}$ using a matrix $\\pmb{L}\\in\\mathbb{R}^{(M-N)\\times N}$ which is jointly learned with the other model parameters. Additionally, for the R\u00f6ssler and Lorenz systems, we added $5\\%$ observation noise during training. Across all training epochs of a given run, we consistently selected the model with the lowest $D_{\\mathrm{stsp}}$ . Each individual training run of the AL-RNN was performed on a single CPU. Depending on the training sequence length, a single epoch took between 0.5 to 3 seconds. ", "page_idx": 19}, {"type": "text", "text": "Geometric agreement For evaluating attractor geometries, we use a state space measure $D_{\\mathrm{stsp}}$ based on the Kullback-Leibler (KL) divergence, which assesses the (mis)match between the ground truth spatial distribution of data points, $p_{\\mathrm{true}}(x)$ , and the distribution $p_{\\mathrm{gen}}(x|z)$ of trajectory points freely generated by the inferred DSR model. These probability distributions can be approximated in different ways from the observed/ generated trajectories. Here, we usually sampled long trajectories corresponding to the test set length (usually $T=100$ , 000 time steps, but sometimes shorter for the empirical time series) from trained systems, removing transients to ensure that the system has reached a limit set. For low-dimensional systems, the KL divergence can be straightforwardly calculated through a discrete binning approximation [11]: ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{\\mathrm{stsp}}\\left(p_{\\mathrm{true}}(x),p_{\\mathrm{gen}}(x\\mid z)\\right)\\approx\\sum_{k=1}^{K}\\hat{p}_{\\mathrm{true}}^{(k)}(x)\\log\\left(\\frac{\\hat{p}_{\\mathrm{true}}^{(k)}(x)}{\\hat{p}_{\\mathrm{gen}}^{(k)}(x\\mid z)}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $K=m^{N}$ is the total number of bins, with $m$ bins per dimension and $N$ being the dimension of the ground truth system. A bin number of $m=30$ per dimension was chosen as a good compromise for distinguishing between successful and bad reconstructions for 3d systems. Since the number of data required to fill the bins scales exponentially with $N$ , for the ECG time series $N=5)$ ) we reduced the number of bins to $m=8$ , as suggested in [38]. ", "page_idx": 19}, {"type": "text", "text": "Temporal agreement To assess temporal agreement, we computed Hellinger distances $D_{H}$ between power spectra [67, 39]. We first simulated long time series $T=100,000$ (as with $D_{s t s p}$ above). After standardization, we computed dimension-wise Fast Fourier Transforms (FFT). The power spectra were smoothened using a Gaussian kernel and normalized, and the extended, high-frequency tails, which predominantly contained noise, were truncated. The Hellinger distance between smoothed ground-truth spectra $F(\\omega)$ and generated spectra $G(\\omega)$ is given by: ", "page_idx": 19}, {"type": "equation", "text": "$$\nH(F(\\omega),G(\\omega))=\\sqrt{1-\\int_{-\\infty}^{\\infty}\\sqrt{F(\\omega)G(\\omega)}d\\omega}\\in[0,1]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Maximum Lyapunov exponent The maximum Lyapunov exponent of a system quantifies how fast nearby trajectories diverge, and for a flow map can be computed in the limit $T\\to\\infty$ from the system\u2019s product of Jacobians. To approximate the maximum exponent numerically, we first iterated a trained model forward from some randomly drawn initial condition and discarded transients. Given that for chaotic systems the product of Jacobians itself explodes [67], we employed a numerical algorithm from [107, 103] that re-orthogonalizes the series of Jacobian products at regular intervals using a QR decomposition. ", "page_idx": 20}, {"type": "text", "text": "Categorical decoder We coupled categorical observations to the $P$ PWL neurons via a link function given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{i}=\\frac{\\exp\\left(\\beta_{i}^{T}\\mathbf{z}_{t}^{p}\\right)}{1+\\sum_{j=1}^{K-1}\\exp\\left(\\beta_{j}^{T}\\mathbf{z}_{t}^{p}\\right)}\\quad\\forall i\\in\\{1\\ldots K-1\\}}\\\\ &{\\pi_{K}=\\frac{1}{1+\\sum_{j=1}^{K-1}\\exp\\left(\\beta_{j}^{T}\\mathbf{z}_{t}^{p}\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "nTohrem raelgizraetsisoino ne nwsueriegsh ttsh $\\beta_{i}\\,\\in\\,\\mathbb{R}^{P\\times1}$ oabraeb illeitayr noevde rf oalrl  ecaatcehg ocraiteesg sourym $i\\,=\\,1\\,.\\,.\\,K\\,-\\,1$ .ile the $\\textstyle\\sum_{i=1}^{K}\\pi_{i}=1$ ", "page_idx": 20}, {"type": "text", "text": "A.3 Benchmark datasets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lorenz-63 The Lorenz-63 system, formulated by Edward Lorenz in 1963 [61] to model atmospheric convection, is defined by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}x}{\\mathrm{d}t}=\\sigma(\\boldsymbol{y}-\\boldsymbol{x})}\\\\ {\\displaystyle\\frac{\\mathrm{d}y}{\\mathrm{d}t}=x(\\boldsymbol{\\rho}-\\boldsymbol{z})-y}\\\\ {\\displaystyle\\frac{\\mathrm{d}\\boldsymbol{z}}{\\mathrm{d}t}=x\\boldsymbol{y}-\\beta\\boldsymbol{z},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\sigma,\\rho,\\beta$ , are parameters that control the dynamics of the system (here set to $\\sigma=10$ , $\\begin{array}{r}{\\beta=\\frac{8}{3}}\\end{array}$ , and $\\rho=28$ , in the chaotic regime). The system was solved numerically with integration time step $\\Delta t=0.01$ using scipy.integrate with the default RK45 solver. ", "page_idx": 20}, {"type": "text", "text": "R\u00f6ssler The R\u00f6ssler system, intended by Otto R\u00f6ssler in 1976 [85] as a further simplification of the Lorenz model, produces chaotic dynamics using a single nonlinearity in one state variable: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}x}{\\mathrm{d}t}=-y-z\\,}\\\\ {\\displaystyle\\frac{\\mathrm{d}y}{\\mathrm{d}t}=x+a y\\,}\\\\ {\\displaystyle\\frac{\\mathrm{d}z}{\\mathrm{d}t}=b+z(x-c),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $a,b,c,$ , are parameters controlling the dynamics of the system (here set to $a=0.2,b=0.2$ , and $c=5.7$ , in the chaotic regime). The system was solved with integration time step $\\Delta t=0.08$ using scipy.integrate with the default RK45 solver. ", "page_idx": 20}, {"type": "text", "text": "Human electrocardiogram The electrocardiogram (ECG) time series was taken from the PPGDaLiA dataset [82]. With a sampling frequency of $700H z$ , the recording duration spanned 600 seconds, yielding a time series of $T=419,973$ time points. Initially, a Gaussian smoothing filter with $\\sigma\\,=\\,6$ was applied, resulting in a filter length of $l\\,=\\,8\\sigma\\,+1\\,=\\,49$ . We standardized the time series and applied temporal delay embedding using the DynamicalSystems. $\\mathrm{j}2$ Julia library, resulting in an embedding dimension of $m=5$ . For model training, the first $T=100$ , 000 time steps (approximately 143 seconds) were used, downsampled to every 10th datapoint. ", "page_idx": 20}, {"type": "text", "text": "Human fMRI data The functional magnetic resonance imaging (fMRI) data from human subjects performing three cognitive tasks is publicly available on GitHub [52]. We followed Kramer et al. [52] and selected the first principal component of BOLD activity in each of the 20 brain regions. Subjects underwent five rounds of three cognitive tasks (\u2018Choice Reaction Task [CRT]\u2019, \u2018Continuous Delayed Response Task [CDRT]\u2019 and \u2018Continuous Matching Task [CMT]\u2019), together with a \u2018Rest\u2019 and \u2018Instruction\u2019 period. The time series per subject were short $T=360$ ) and reconstructions in [52] exhibited a positive maximum Lyapunov exponent, indicating the chaotic nature of the underlying system. ", "page_idx": 21}, {"type": "text", "text": "Lorenz-96 The Lorenz-96 system, formulated by Edward Lorenz in 1996 [62], is defined by ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}x_{i}}{\\mathrm{d}t}}=(x_{i+1}-x_{i-2})x_{i-1}-x_{i}+F,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with system variables $x_{i}$ , $i=1,...,N$ , and forcing term $F$ (here, $F\\,=\\,8$ , in the chaotic regime). Furthermore, cyclic boundary conditions are assumed with $x_{-1}=x_{N-1}$ , $x_{0}=x_{N}$ , ${x}_{N+1}={x}_{1}$ , and the system was solved with integration step $\\Delta t=0.04$ using scipy.integrate with the default RK45 solver. ", "page_idx": 21}, {"type": "text", "text": "Human EEG data Electroencephalogram (EEG) data were taken from a study by Schalk et al. [87]. These are 64-channel EEG recordings obtained from human subjects during different motor and imagery tasks. The signal was standardized and smoothed using a Hann function and a window length of 15, as in [11]. ", "page_idx": 21}, {"type": "text", "text": "A.4 Further Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Table 1: Comparison of AL-RNN to different SOTA models for dynamical systems reconstruction. Comparison values, datasets and evaluation measures as in [39], based on code provided on GitHub by the authors. id-TF: identity teacher forcing, GTF: generalized teacher forcing, $D_{s t s p}$ : state space divergence, $D_{H}$ : Hellinger distance across power spectra. Reported values are median $\\pm$ median absolute deviation. ", "page_idx": 21}, {"type": "table", "img_path": "sEpSxteEKJ/tmp/3d0a5a8ec723db19e285083635fa67274a0f19470bbf901603da9c41d8d063e1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/e24ac6828b8e55885ba2a34efff561483dda4bf23d834fbe3c4fd7fcab4240ac.jpg", "img_caption": ["Figure 9: Top: DSR quality (assessed by $D_{s t s p},$ ) as a function of strength of regularization on the number of nonlinearities for the AL-RNN trained on Lorenz-63. Bottom: Number of selected piecewise-linear units $P$ as a function of regularization strength. As in Fig. 3, a first optimum consistently occurs for $P=2$ . To select the number of nonlinear units through regularization, we replaced the standard ReLU by a leaky $\\mathrm{{ieLU}\\,m a x}(\\alpha_{i}z_{i},z_{i}),\\alpha_{i}\\in(0,1)$ , for each of the $i=1\\dots M$ units. The slope $\\alpha_{i}=\\sigma(\\gamma_{i})$ is determined through a steep sigmoid, $\\sigma(\\gamma_{i})=1/(1+\\exp(-500(\\gamma_{i}-$ 0.5))), via trainable parameter $\\gamma_{i}$ , ensuring that it is either close to 0 or close to 1. To encourage linearity, we include a loss term $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{lin}}=\\lambda_{\\mathrm{lin}}\\sum_{i=1}^{M}|\\alpha_{i}-1|}\\end{array}$ , pushing slopes towards 1. After training, units with are classified as linear, while all remaining units were considered nonlinear to provide an estimate for $P$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Task stages align with subregions To test the alignment of the reconstructed AL-RNN activity in the subspace of the $P=2$ PWL units with the 4 task stages, we trained 10 models on each of the 10 subjects without visible movement artifacts (yielding 100 trained models). We then checked which assignment of the four subregions (00, 01, 10, and 11) to the four task stages (Rest and Instruction, CRT, CDRT, and CMT) gave the highest alignment (Fig. 8), and used this to determine the average classification accuracy as the percentage of time points correctly assigned to their respective task stage based on the four subregions. ", "page_idx": 22}, {"type": "text", "text": "Geometrically minimal reconstructions of empirical time series Increasing the number of PWL units also improved geometric agreement for the empirical time series (Fig. 3, $P=10$ for the ECG data), while dynamics remained confined to a relatively small subset of linear subregions (Fig. 4). This confinement within only a few subregions allows for the efficient computation of dynamical objects like fixed points. For the ECG data, real and virtual fixed points in the linear subregions were primarily located within a 3d hyperplane of the 5d data. Principal component analysis showed that this hyperplane harboring the fixed points aligned with the first principal component of the data, explaining approximately $40\\bar{\\%}$ of the data\u2019s variance (Fig. $20\\mathbf{a}/\\mathbf{b}$ ). A similar pattern was observed in the fMRI data, where fixed point locations often aligned with PC1 of the data (Fig. 21). ", "page_idx": 22}, {"type": "text", "text": "Lorenz 63 $\\mathbf{P}=\\mathbf{1})$ ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/0396ad3c5f97236410b1e15a69c9b9130c79c84431b2d5eefe3dff6a5edb9833.jpg", "img_caption": ["Figure 10: Reconstruction performance on Lorenz-63 system for an AL-RNN ( $M=20)$ ) as a function of the number of linear units, once for the case where the number of PWL units was insufficient for a topologically accurate reconstruction ( $\\mathcal P=1$ , top), and once for the case where it was sufficient $P=2$ , bottom). Results indicate performance cannot be improved by adding more linear units if $P$ is too small, but can be \u2013 up to some saturation level \u2013 when $P$ is sufficiently large. Error bars $=$ SEM. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/4c366b5108bbb114067204b7b32784d1634b7a78d6857d0d41879e8f07a77fc0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 11: Optimal geometric reconstruction of a Lorenz-63 using the AL-RNN with $P=8$ PWL units. Left: reconstruction with subregions color-coded by frequency of trajectory visits (dark: most frequently visited regions, yellow: least frequent regions). Center: Resulting geometrical graph structure (using transition probabilities for placing the nodes) visualized using the spectral layout in networkx. Note that self-connections and directedness of edges were omitted in this representation. The resulting graph shadows the layout of the reconstructed system. Right: Connectome of transitions between subregions. ", "page_idx": 23}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/41cc230176fb23015739c0e5e8b3f46f86857406db5c70e1ecb027f2dac2f75f.jpg", "img_caption": ["Figure 12: \u2018Linearized\u2019 dynamics (i.e., considering the linear map from each subregion) within the three linear subregions of the AL-RNN trained on the ECG data from Fig. 7. The first two subregions host weakly unstable spirals with shifted phase, corresponding to the excitatory/ inhibitory phases of the ECG. The strongly divergent activity in the third subregion induces the Q wave. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/c1b654c3b3249cae0d566d5f68c2b1b71d076b5a64d54cc775ec2a474aee82f7.jpg", "img_caption": ["Figure 13: Freely generated ECG activity using an AL-RNN with 3 linear subregions (color-coded) shows consistent assignment of the Q wave to a distinct subregion across multiple successful reconstructions. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/55c627b7f13b08f8778610a933e977396f6d42166578ec2533dcc3bb3c44036d.jpg", "img_caption": ["Figure 14: a: Freely generated ECG activity by the AL-RNN with $M=100$ total units and $P=2$ PWL units. b: Symbolic coding of the dynamics (with each shade of blue a different symbol/ linear subregion), reflecting the QRS phase with alternating excitation/inhibition (lighter shades of blue) following the short $\\mathrm{^Q}$ wave burst (dark blue). c: Time histogram of distinct symbols along the symbolic trajectory, exposing the mildly chaotic nature of the reconstructed ECG signal [39]. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/bbbfc15ec3b5bcdc425f2c98e6c428345f68bfca6d3dee140a59079432441a0f.jpg", "img_caption": ["Figure 15: Freely generated fMRI activity using an AL-RNN with $M=100$ total units and $P=3$ PWL units. a: Mean generated activity color-coded according to linear subregions, with background shading highlighting the most frequently visited subregion. b: Geometrical graph representation of connections between linear subregions, with edge weights representing relative transition frequencies (self-connections omitted). c: Time series of the symbolic coding of dynamics according to linear subregions. d: Dynamics in the two most frequently visited linear subregions in the subspace of the three PWL units, with the boundary between subregions in gray. The dark blue trajectory bit in the first subregion moves towards a virtual stable fixed point located near the center of the saddle spiral in the second subregion. The yellow trajectory illustrates an orbit cycling away from this spiral point and eventually crossing into the first subregion. From there, trajectories are pulled back into the second subregion through the virtual stable fixed point located close to the saddle spiral (see also activity with background shading in a). This dynamical behavior is similar to the one observed in the chaotic benchmark systems, where locally divergent activity of the AL-RNN is propelled back into the center of an unstable manifold within another subregion. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/e9c888eb9f83205b0802eee11dec3d2c9b57c1cc4732fcfc0ccfd553543ace28.jpg", "img_caption": ["Figure 16: Topological entropy computed from symbolic sequences (Figs. 14 & 15) versus $\\lambda_{m a x}$ , calculated from corresponding topologically minimal AL-RNNs (Figs. 5 & 7). "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/8bb39b20f825631c694626c9c3ff83823b0c77d0ecddc721e500708b61824bb7.jpg", "img_caption": ["Figure 17: Generated fMRI activity using an AL-RNN with $M=100$ total units and $P=2$ PWL units, with the readout unit states replaced by observations every 7 time steps. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/5959346692a12f12718dc9a464c1f930400cd34154c1caed85bc19716e1efce2.jpg", "img_caption": ["Figure 18: a: Weights of the reconstructed AL-RNN. b: Histogram of the absolute weight distributions for the different types of AL-RNN units. On average, weight magnitudes of the PWL units are much higher than those of the other unit types. c: The correlation structure among the weights of the $N=20$ readout units (rows in a, top) reflects the correlation structure within the observed time series variables (correlation between both matrices $r\\approx0.76)$ ). "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/5413154478af5ab364f56446ef35cf9a42173727f2365a282710548dd78025d4.jpg", "img_caption": ["Figure 19: Top row: Activity of the PWL units for the topologically minimal representations of the R\u00f6ssler (a) and Lorenz-63 attractor (b) from Fig. 5. Center row: Time histogram of discrete symbols of the symbolic trajectory. Bottom row: Time series of the symbolic trajectory. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/244dd00e118fa9bef3820e77244a6e3fd41b79867b863cb767edf5deb3944bbb.jpg", "img_caption": ["Figure 20: a: Variation in the location of the analytically computed real and virtual fixed points of the linear subregions aligns with the first PC of the data (generated trajectories in bluish, color-coded according to linear subregion). b: Fixed point location along the first principal component (with corresponding dynamics within $\\left(x_{1},x_{2}\\right)$ -plane of observation space on top) at different characteristic stages of training. At the early stages of training, fixed points of the linear subregions are distributed along the data manifold within the subspace of readout units. Around epoch 20, the maximum absolute eigenvalues $\\sigma^{\\mathrm{max}}$ of the Jacobians in many subregions become larger than one, inducing local divergence necessary for producing the observed chaotic dynamics. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/8d250547213cdfdfdc6c62ce4ad0553f2f2a5b8ebefbab7b0033e2c1304f7a81.jpg", "img_caption": ["Figure 21: a: Analytically computed real and virtual fixed points of the linear subregions of a geometrically minimal AL-RNN ( $M=100$ , $P=10$ ) align with the first PC of the data within the subspace of readout units. The BOLD time series for different brain regions were highly correlated, so PC1 accounted for approximately $80\\%$ of the data variance. b: Geometrical graph representation with relative frequency of transitions between linear subregions indicated by line thickness of edges, showing a central highly connected subgraph of frequently visited (bluish) dominant subregions, as in Fig. 4. c: Example freely generated activity from ten simulated brain regions. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/089ba5ae4ac42daf9c61c1e9261c63c8be99cdff5cb54b8eaf402432dd06491e.jpg", "img_caption": ["Figure 22: Same as Fig. 5d-f, but showing absolute instead of relative deviations (for $D_{s t s p}$ , values $<1.0$ indicate tight agreement). "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/6396ed7c9a5e7896a046f489abde62d1a89791e34ed7af50bf102a6ec893d41e.jpg", "img_caption": ["Figure 23: Pairwise differences in sorted (from lowest to highest probability) cumulative trajectory point distributions across linear subregions across all valid pairs from 20 training runs (quantified by the Kolmogorov\u2013Smirnov distance, $D_{K S}\\,$ ) for the AL-RNN vs. PLRNN, revealing much higher consistency for AL-RNN. Note the log-scale on the y-axis. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/7d18cdfda21cfdb177158e571269e7e8ac6929e52bd85ab21aa94292a437361f.jpg", "img_caption": ["Figure 24: Linear subregions (color-coded) mapped to observation space of the R\u00f6ssler system, showing a robust representation of the individual subregions across multiple training runs/ models. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "sEpSxteEKJ/tmp/2e35b7a681f1cbf44b9bd5e98197e1f675ece89358748a9b3928a765ded9a27e.jpg", "img_caption": ["Figure 25: Top row: Robust placing of linear subregions (color coded) mapped to observation space across training runs using the AL-RNN. Model recovery experiments further confirmed the robustness of the model solutions, with very similar overall performance measures across different experiments (original: $D_{s t s p}=3.14$ , $D_{H}=0.28$ ; recovered: $D_{s t s p}=3.38\\pm0.18$ , $D_{H}=0.28\\pm0.03$ ; 3 linear subregions in all cases). Bottom row: In contrast, for the PLRNN linear subregions are differently assigned (with varying boundaries) on each run. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "B Proofs of Theorems ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Define a shift space of finite type $(A_{\\mathcal{F}},\\sigma)$ such that each symbol of the alphabet of $\\boldsymbol{\\mathcal{A}}$ is associated with exactly one set $U_{e}$ of the topological partition of $S$ , i.e. we have a total of $|\\mathcal{U}|$ symbols in $\\boldsymbol{\\mathcal{A}}$ . Further define for $\\pmb{a}\\in A_{\\mathcal{F}}$ the sets [54] ", "page_idx": 30}, {"type": "equation", "text": "$$\nD_{l}(\\mathbf{a})=\\bigcap_{k=-l}^{l}\\phi^{-k}(U_{a_{k}})\\subseteq S,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $a_{k}$ is the $k$ th symbol in the sequence $\\textbf{\\em a}$ : Think of this as building the intersection of $U_{a_{0}}$ with $k$ -times forward iterates of subsets associated with $a_{-k}$ and $k$ -times backward iterates of subsets associated with $a_{k}$ , such that in the limit $l\\rightarrow\\infty$ hopefully we end up with a single point corresponding to a unique trajectory in $S$ , i.e. considering $\\begin{array}{r}{D(\\pmb{a})\\,=\\,\\bigcap_{l=0}^{\\infty}\\bar{\\overline{{D_{l}}}}(\\pmb{a})}\\end{array}$ (see Lind and Marcus [54] for details). We now define [54] ", "page_idx": 30}, {"type": "text", "text": "Definition 1. A symbolic representation of an invertible DS $(S,\\phi)$ with topological partition $\\mathcal{U}=$ $\\{U_{0}\\ldots U_{n-1}\\}$ is a shift space $(A_{\\mathcal{F}},\\sigma)$ with alphabet $A=\\{0\\,.\\,.\\,.\\,n-1\\}$ , such that each symbol $a_{k}\\in\\mathcal A$ is associated with exactly one subset $U_{k}\\in\\mathcal{U}_{k}$ , and $\\begin{array}{r}{D(\\pmb{a})=\\bigcap_{l=0}^{\\infty}\\overline{{D_{l}}}(\\pmb{a})}\\end{array}$ contains exactly one point $\\pmb{x}\\in S$ for each $\\pmb{a}\\in A_{\\mathcal{F}}$ . If $A_{\\mathcal{F}}$ is a finite shift, we call this a Markov partition. ", "page_idx": 30}, {"type": "text", "text": "Ideally, we would like our symbolic coding of the DS to be a symbolic representation or Markov partition according to this definition, but in practice this may entail other unfavorable properties (e.g., too fine-grained) and we contend here with the properties given by Theorems 1 - 3. ", "page_idx": 30}, {"type": "text", "text": "In the statement of our theorems, we further used the terms \u2018eventually periodic\u2019 and \u2018asymptotically periodic\u2019. Let us now strictly define them (according to Alligood et al. [3]): ", "page_idx": 30}, {"type": "text", "text": "Definition 2. An orbit $\\{z_{1},\\ldots,z_{n},\\ldots\\}$ of the map $\\phi$ is said to be asymptotically periodic if it converges to a periodic orbit as $n~\\rightarrow~\\infty$ . This implies that there is a periodic orbit $\\Gamma_{k}\\ =$ $\\{y_{1},y_{2},\\ldots,y_{k},y_{1},y_{2},\\ldots\\}$ such that $\\operatorname*{lim}_{n\\to\\infty}d(z_{n},\\Gamma_{k})\\;\\bar{=\\;}0$ . ", "page_idx": 30}, {"type": "text", "text": "For instance, any orbit that is attracted to a stable fixed point or to a saddle fixed point (evolving on its stable manifold) is asymptotically periodic (fixed). ", "page_idx": 30}, {"type": "text", "text": "Definition 3. A point $_{\\textit{z}}$ is called eventually periodic with period $p$ for the map $\\phi$ , if for some positive integer $N$ , $\\phi^{n+\\bar{p}}(z)\\,=\\,\\phi^{n}(z)$ for all $n\\geq N$ , and $p$ is the smallest positive integer with this exact property. This means that the orbit of $_{\\textit{z}}$ eventually maps exactly onto a periodic orbit. ", "page_idx": 30}, {"type": "text", "text": "Note: The term \u2018eventually periodic\u2019 describes the extreme case where an orbit coincides precisely with a periodic orbit. Thus, any eventually periodic orbit is also asymptotically periodic, but the reverse is not always true: An asymptotically periodic orbit comes arbitrarily close to a periodic orbit, but may not land precisely on it. ", "page_idx": 30}, {"type": "text", "text": "With these definitions we are now ready to prove the theorems. ", "page_idx": 30}, {"type": "text", "text": "Proof of Theorem 1 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Proof. $\"\\Rightarrow\"$ : Suppose that the orbit $\\Omega_{S}=\\{z_{1},\\ldots,z_{n},.\\,.\\,.\\}$ is asymptotically fixed. Hence, there exists a fixed point $z^{*}\\in U_{a^{*}}$ of the AL-RNN $F_{\\theta}$ (i.e., $z^{*}=F_{\\theta}(z^{*}))$ such that $\\operatorname*{lim}_{n\\to\\infty}z_{n}\\;=\\;z^{*}$ . Let $\\pmb{a}=(a_{1}a_{2}a_{3}\\dots)$ be the corresponding symbolic sequence of the orbit $\\Omega_{S}$ with $z_{n}\\in U_{a_{n}},\\forall n$ . Since $\\operatorname*{lim}_{n\\to\\infty}z_{n}\\,=\\,z^{*}$ , so there exists some $N\\in\\mathbb{N}$ such that for every $n\\geq N$ the points $z_{n}$ will remain arbitrarily close to $z^{\\ast}$ . This means the points $z_{n}$ eventually enter the same linear subregion that contains $z^{*}$ and will remain there for all future iterations. Thus, ${\\pmb a}\\,=\\,(a_{1}a_{2}a_{3}\\,.\\,.\\,.\\,a_{N-1})(a^{\\mp})^{\\infty}$ is eventually fixed. ", "page_idx": 30}, {"type": "text", "text": "$\\risingdotseq$ : Assume ${\\textbf{\\em a}}=\\ (a_{1}a_{2}a_{3}\\dots a_{N-1})(a^{*})^{\\infty}$ is an eventually fixed sequence of the symbolic encoding. This means for all the corresponding orbits $\\Omega_{S}=\\{z_{1},\\ldots,z_{n},\\ldots\\}$ of $F_{\\theta}$ , there exists an index $N$ such that for every $n\\geq N$ the orbit points $z_{n}$ must remain in the same subregion, say $U_{a^{*}}$ . Since by assumption the map $F_{\\theta}$ is non-globally-diverging and hyperbolic, the system cannot be expanding in all directions in any of the subregions. Thus, there must be at least one contracting or stable direction in $U_{a^{*}}$ . Consequently, there must be at least one corresponding orbit $\\{z_{1},\\ldots,z_{n},\\ldots\\}$ that converges toward some stable structure in $U_{a^{*}}$ as $n\\to\\infty$ . Since $F_{\\theta}$ is a linear hyperbolic map in each subregion, there cannot be any $k$ -cycle, $k\\geq2$ , with all periodic points contained within a single subregion. Therefore, the corresponding orbit converges to a (saddle) fixed point $z^{*}\\in U_{a^{*}}$ in the stable manifold along stable directions. If all directions in $U_{a^{*}}$ are contracting or stable, then all corresponding orbits will converge to a stable fixed point within that subregion. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem 2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof. $\"\\Rightarrow\"$ : Let $\\Omega_{S}=\\{z_{1},\\ldots,z_{n},\\ldots\\}$ be an asymptotically $p$ -periodic orbit of $F_{\\theta}$ . Then there is a periodic orbit $\\Omega_{S,p}\\;=\\;\\{z_{1}^{*},\\ldots,z_{p}^{*}\\}$ (i.e., such that all points $z_{k}^{*}\\ \\in\\ \\Omega_{S,p}$ are distinct and $z_{k+p}^{*}=F_{\\theta}^{p}(z_{k}^{*})=z_{k}^{*},k=1\\dots p)$ and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}d(z_{n},\\Omega_{S,p})\\,=\\,0.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since $F_{\\theta}$ is a linear and hyperbolic map in each subregion, there cannot be any $p$ -periodic orbit with $p\\geq2$ where all periodic points are contained within a single subregion. On the other hand, due to the definition of the symbolic coding for each trajectory, each point $z_{t}$ at time step $t$ is assigned its own symbol $a_{t}$ , depending on its associated linear subregion. Hence, the corresponding symbolic sequence of the periodic orbit $\\Omega_{S,p}$ is $(a_{1}^{*}a_{2}^{*}\\ldots a_{p}^{*})^{\\infty}$ with $z_{k}\\in U_{a_{k}^{*}},k=1\\ldots p$ . Moreover, due to eq. 17, for some large enough index $N\\in\\mathbb{N}$ , the points $z_{n}$ of the orbit $\\Omega_{S}$ become arbitrarily close to the orbit $\\Omega_{S,p}$ . Thus, for $n~\\geq~N$ , the itinerary of $\\Omega_{S}$ will follow the same repeating pattern as the periodic orbit\u2019s itinerary and will revisit the same subregions as the periodic orbit points $z_{k}^{*}$ , $k\\,=\\,1\\,.\\,.\\,.\\,p$ . Therefore, the corresponding symbolic sequence of the orbit $\\Omega_{S}$ is $\\textbf{\\em a}=$ $\\big(a_{1}a_{2}\\dots\\stackrel{\\cdot\\cdot}{a}_{N-1}\\big)\\big(a_{1}^{*}a_{2}^{*}\\dots a_{p}^{*}\\big)^{\\infty}$ , which is an eventually $p$ -periodic orbit of the shift map $\\sigma$ . ", "page_idx": 31}, {"type": "text", "text": "$\\risingdotseq$ : Assume that $\\underline{{\\textbf{a}}}=\\big(a_{1}a_{2}\\underline{{\\mathbf{\\nabla}}}.\\dots a_{N-1}\\big)(a_{1}^{*}a_{2}^{*}\\dots a_{p}^{*})^{\\infty}\\in A_{\\mathcal{U},F_{\\theta}}$ is an eventually $p$ -periodic orbit of the shift map $\\sigma$ . Consequently, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\exists N\\in\\mathbb{N}\\;\\;\\forall\\;\\;n\\geq N:\\;\\;\\quad a_{n+p}=a_{n},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "which says that the symbolic sequence is repeating every $p$ steps. Therefore, for all the corresponding orbits $\\Omega_{S}\\,=\\,\\{z_{1},\\ldots,z_{n},\\ldots\\}$ of $F_{\\theta}$ , there exists an index $N$ such that for every $n\\,\\geq\\,N$ the orbit points $z_{n}$ will stay in the same $p$ linear subregions, say $U_{a_{1}^{*}},U_{a_{2}^{*}}\\ldots,U_{a_{p}^{*}}$ , and revisit each $U_{a_{i}^{*}}(i=1,2,\\dots,p)$ after exactly $p$ time steps: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\exists N\\in\\mathbb{N}\\;\\;\\forall\\;\\;n\\geq N:\\;\\;\\;\\;\\;U_{a_{n+p}^{*}}=U_{a_{n}^{*}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Since by assumption the map $F_{\\theta}$ is non-globally-diverging and hyperbolic, the system cannot be expanding in all directions in any of the subregions. Thus, there exists at least one contracting direction in each of the subregions $U_{a_{1}^{*}},U_{a_{2}^{*}}\\ldots,U_{a_{p}^{*}}$ , and therefore at least one corresponding orbit $\\Omega_{S}\\,=\\,\\{z_{1},\\ldots,z_{n},\\ldots\\}$ that converges toward some stable structure within $U_{a_{1}^{*}},U_{a_{2}^{*}}\\ldots,U_{a_{p}^{*}}$ as $n\\to\\infty$ . Furthermore, as $F_{\\theta}$ is a linear and hyperbolic map in each subregion there cannot be any $k$ - cycle, $k\\geq2$ , with all periodic points contained within only one of the subregions. Similarly, it cannot be chaotic or quasi-periodic within just one subregion. Now, due to eq. 19, for the corresponding orbit $\\Omega_{S}$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\forall n\\ge N\\ \\forall z_{n}\\in U_{a_{i}^{*}}(i=1\\dots p)\\ \\ :\\ z_{n+p}=F_{\\theta}^{p}(z_{n})=W_{a}z_{n}+h_{a}\\in U_{a_{i}^{*}},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with fixed parameters $W_{a}\\;:=\\;W_{\\Omega^{a_{p}^{*}}}\\cdot\\cdot\\cdot W_{\\Omega^{a_{2}^{*}}}W_{\\Omega^{a_{1}^{*}}}$ and $\\begin{array}{r}{h_{a}\\;:=\\;\\sum_{i=2}^{p}\\prod_{j=2}^{i}W_{\\Omega^{a_{p-j+2}}}h+h}\\end{array}$ Since $F_{\\pmb\\theta}^{p}$ is strictly affine, for $n\\geq N$ , any sub-sequence $\\{z_{n+m p}\\}_{m=1}^{\\infty}$ of $\\Omega_{S}$ cannot be convergent to an aperiodic (i.e., chaotic or quasi-periodic) orbit. Therefore, the corresponding orbit $\\Omega_{S}$ converges to a (saddle) $p$ -periodic orbit, with all periodic points within the sub-regions $U_{a_{1}^{*}},U_{a_{2}^{*}}\\cdot\\cdot\\cdot,U_{a_{\\bar{p}}^{*}}$ , in the stable manifold along stable directions. If all directions in $U_{a^{*}}$ are contracting or stable, then all corresponding orbits will converge to a stable $p$ -periodic orbit with all periodic points within the subregions $U_{a_{1}^{*}},U_{a_{2}^{*}}\\cdot\\cdot\\cdot,U_{a_{\\bar{p}}^{*}}$ . \u53e3 ", "page_idx": 31}, {"type": "text", "text": "Proof of Theorem 3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Proof. $\"\\Rightarrow\"$ : Let $\\Omega_{S}\\,=\\,\\{z_{1},z_{1},\\dots,z_{n},\\dots\\}$ of $F_{\\theta}$ be an asymptotically aperiodic orbit of $F_{\\theta}$ . Then, there is an aperiodic orbit $\\Omega=\\{\\bar{z}_{1},\\bar{z}_{2}\\ldots\\}$ (i.e., with $\\bar{z}_{k}\\overset{.}{\\neq}F_{\\theta}^{p}(\\bar{z}_{k})\\dot{\\forall k,p}>0)$ and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{n\\to\\infty}d(z_{n},\\Omega)\\,=\\,0.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "According to the proof of Theorem 2 (second part), this orbit cannot have an eventually periodic symbolic representation $(a_{1}a_{2}\\dots a_{N-1})(a_{1}a_{2}\\bar{\\dots}\\dots a_{p})^{\\infty}$ , because if it had, it would need to be asymptotically periodic as well. ", "page_idx": 32}, {"type": "text", "text": "$\\risingdotseq$ : Assume an aperiodic symbolic sequence $\\pmb{a}\\,=\\,(a_{1},\\ldots,a_{n},\\ldots)$ , where there is no $p\\,>\\,0$ such that $a_{k}=\\sigma^{p}(\\overset{\\cdot}{a_{k}})\\forall k$ . This will correspond to an aperiodic succession $U_{a_{1}}\\cdot\\cdot\\cdot U_{a_{n}}\\cdot\\cdot\\cdot$ of linear subregions $U_{a_{k}}$ visited, since each subregion has its unique symbol. However, from the proof of Theorem 2 (first part) we know that any asymptotically periodic orbit of $F_{\\theta}$ must have an eventually periodic symbolic encoding, so the orbit $\\Omega_{S}$ corresponding to $\\textbf{\\em a}$ cannot be asymptotically periodic. Consequently, it cannot be (eventually) periodic either, which implies that it must be aperiodic. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The results illustrated in the text and figures properly address the presented claims, including formal proof for the theorems and hyperparameter settings and code for reproducing the results of the paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Limitations are addressed in the conclusion or, e.g. in the case of theoretical results, directly in the respective section. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The paper includes formal proofs of all theorems Appx. B. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Detailed hyperparameter settings for the results in the paper are given in Sect.   \nA.2, which combined with the codebase allow for replication of the results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: A link to a github repository is provided that contains the implementation of the network architecture that was used for the main results in the paper (https://github. com/DurstewitzLab/ALRNN-DSR). ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All hyperparameter settings are provided in Sect. A.2. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Error bars are reported in figures and with numerical results, i.e. Figs. 3, 4 and 5. The settings for the random initialization of the network architecture are specified in Sect. A.2. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Details on computational resources are provided in Sect. A.2. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All datasets considered here were publically available. The research is aimed at facilitating interpretability of machine learning models. We believe our approach has primarily positive ethical implications. Although we have not identified specific ethical concerns, the wide range of potential applications means that possible misuses cannot be entirely ruled out. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This work constitutes foundational research. As such, we do not foresee any direct negative societal impact. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not foresee a high risk for misuse. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: All code and results were created by us. Datasets were publically sourced and the respective authors cited. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: A fully documented version of the code is published on github. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper did not involved crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper did not involved crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]