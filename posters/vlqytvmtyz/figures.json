[{"figure_path": "VLQYtVMTYz/figures/figures_1_1.jpg", "caption": "Figure 1: The Hopfield Boosting concept. The first step (weight) creates weak learners by firstly choosing in-distribution samples (ID, orange), and by secondly choosing auxiliary outlier samples (AUX, blue) according to their assigned probabilities; the second step (evaluate) computes the losses for the resulting predictions (Section 3); and the third step (update) assigns new probabilities to the AUX samples according to their position on the hypersphere (see Figure 2).", "description": "This figure illustrates the Hopfield Boosting concept, showing a three-step process.  First, weak learners are created by sampling in-distribution (ID) and auxiliary outlier (AUX) data based on their assigned probabilities. Second, the model evaluates the performance of these learners by computing losses on the resulting predictions. Finally, the model updates the probabilities for the AUX samples based on their position on a hypersphere, guiding the focus on challenging outlier samples near the decision boundary.", "section": "3 Method"}, {"figure_path": "VLQYtVMTYz/figures/figures_5_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure shows a synthetic example to illustrate the adaptive resampling mechanism in Hopfield Boosting. It compares three scenarios: using all data, random sampling, and Hopfield Boosting's adaptive sampling.  Hopfield Boosting focuses on selecting weak learners near the decision boundary, resulting in a stronger ensemble learner and a tighter decision boundary (shown by the heatmap representing the exponentiated Hopfield energy).", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_20_1.jpg", "caption": "Figure 3: Depiction of the energy function E\u266d(\u03be; X, O) on a hypersphere. (a) shows E\u266d(\u03be, \u03a7, \u039f) with exemplary inlier (orange) and outlier (blue) points; and (b) shows exp(\u03b2E\u266d(\u03be, \u03a7, \u039f)). \u03b2 was set to 128. Both, (a) and (b), rotate the sphere by 0, 90, 180, and 270 degrees around the vertical axis.", "description": "This figure shows the energy function E\u266d(\u03be; X, O) represented on a 3D hypersphere.  The energy function is used to sample weak learners in the Hopfield Boosting algorithm.  Part (a) shows the energy function itself; Part (b) shows the exponential of the energy function, which highlights the areas of high energy more clearly.  The plots are rotated to show the function from different perspectives.", "section": "3.2 Modern Hopfield Energy"}, {"figure_path": "VLQYtVMTYz/figures/figures_21_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure shows how Hopfield Boosting adaptively resamples weak learners near the decision boundary.  The heatmap represents the energy function, with higher values indicating points closer to the boundary.  The orange points represent in-distribution samples, while the blue points are auxiliary outliers. By focusing on these hard-to-classify samples, Hopfield Boosting improves the model's ability to distinguish between in-distribution and out-of-distribution data.", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_22_1.jpg", "caption": "Figure 5: LOOD applied to exemplary data points on a sphere. Gradients are applied to the data points directly. We observe that the geometry of the space forces the patterns to opposing poles of the sphere.", "description": "The figure shows how the energy-based loss function shapes the decision boundary between in-distribution and out-of-distribution data.  The energy landscape is visualized on a 3D hypersphere.  In-distribution points (orange) cluster around one pole, while out-of-distribution points (blue) are distributed across the rest of the sphere. The gradient descent updates push the outliers toward the opposite pole from the inliers, creating a clear separation between the two classes.", "section": "F Toy Examples"}, {"figure_path": "VLQYtVMTYz/figures/figures_23_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure shows how Hopfield Boosting adaptively resamples data points near the decision boundary to create a strong learner by combining many weak learners.  The heatmap visualizes the energy function, showing high energy (darker colors) further from the boundary and low energy (lighter colors) near the boundary. Only the sampled points contribute to the strong learner.", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_23_2.jpg", "caption": "Figure 3: Depiction of the energy function Eb(\u03be; X, O) on a hypersphere. (a) shows Eb(\u03be, X, O) with exemplary inlier (orange) and outlier (blue) points; and (b) shows exp(\u03b2Eb(\u03be, X, O)). \u03b2 was set to 128. Both, (a) and (b), rotate the sphere by 0, 90, 180, and 270 degrees around the vertical axis.", "description": "This figure shows a 3D visualization of the energy function Eb on a hypersphere, demonstrating how inliers and outliers shape the energy surface.  It's shown in two variations: (a) shows Eb itself, and (b) shows the exponentiation of Eb, which accentuates the differences in energy levels.  The visualization is presented from four different angles (0, 90, 180, and 270 degrees rotation around the vertical axis).", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_27_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure demonstrates the adaptive resampling mechanism of Hopfield Boosting. It shows how Hopfield Boosting forms a strong learner by sampling and combining weak learners that are close to the decision boundary. The heatmap visualizes the energy function, highlighting the area where the weak learners are sampled. Only the selected points (highlighted) serve as memories in the Hopfield network.", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_27_2.jpg", "caption": "Figure 3: Depiction of the energy function E\u266d(\u03be; X, O) on a hypersphere. (a) shows E\u266d(\u03be, \u03a7, \u039f) with exemplary inlier (orange) and outlier (blue) points; and (b) shows exp(\u03b2E\u266d(\u03be, \u03a7, \u039f)). \u03b2 was set to 128. Both, (a) and (b), rotate the sphere by 0, 90, 180, and 270 degrees around the vertical axis.", "description": "This figure shows the energy function E\u266d(\u03be; X, O) and its exponential form on a 3D hypersphere. The orange and blue points represent inliers and outliers, respectively.  The plots visualize how the energy landscape changes based on the positioning of inliers and outliers, illustrating the model's ability to separate in-distribution (ID) and out-of-distribution (OOD) data using the Hopfield energy.", "section": "3.2 Modern Hopfield Energy"}, {"figure_path": "VLQYtVMTYz/figures/figures_37_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure shows how Hopfield Boosting adaptively resamples weak learners near the decision boundary between in-distribution (ID) and out-of-distribution (OOD) data. By focusing on these hard-to-distinguish samples, it creates a strong learner that effectively separates ID and OOD data. The background heatmap visualizes the energy function, with darker colors indicating higher energy (and thus, weaker learners).", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_38_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure demonstrates how Hopfield Boosting adaptively resamples weak learners near the decision boundary to create a strong learner.  The leftmost panel shows all data points, with inliers (orange) and outliers (blue) clearly separated. The middle panel demonstrates random sampling, while the rightmost panel showcases Hopfield Boosting's selective sampling strategy, focusing on the boundary region. A heatmap in the background represents the energy function, illustrating the concentration of sampled points (highlighted) near the decision boundary.", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_39_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure demonstrates how Hopfield Boosting adaptively resamples data points near the decision boundary.  The heatmap shows the energy function (exp(\u03b2E\u266d(\u03be; X, O))), with darker colors indicating higher energy and thus, points closer to the boundary between in-distribution and outlier data.  Hopfield Boosting emphasizes these \"hard-to-classify\" points (the highlighted points) as weak learners and combines them to form a strong learner, thereby improving the accuracy of the decision boundary. The sampling is not random; rather, it's weighted, focusing on weak learners that improve OOD detection.", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_40_1.jpg", "caption": "Figure 1: The Hopfield Boosting concept. The first step (weight) creates weak learners by firstly choosing in-distribution samples (ID, orange), and by secondly choosing auxiliary outlier samples (AUX, blue) according to their assigned probabilities; the second step (evaluate) computes the losses for the resulting predictions (Section 3); and the third step (update) assigns new probabilities to the AUX samples according to their position on the hypersphere (see Figure 2).", "description": "This figure illustrates the Hopfield Boosting concept, which is a three-step process. In the first step, weak learners are created by sampling in-distribution (ID) and auxiliary outlier (AUX) samples based on their probabilities.  The second step evaluates the losses for the predictions made by these weak learners. Finally, the third step updates the probabilities of the AUX samples based on their position on a hypersphere, effectively focusing on the hard-to-distinguish samples near the decision boundary.", "section": "3 Method"}, {"figure_path": "VLQYtVMTYz/figures/figures_41_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u1e9e is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure shows a synthetic example to illustrate how Hopfield Boosting adaptively resamples data points near the decision boundary.  The leftmost panel shows all the data points; the middle panel shows random sampling; the rightmost panel shows Hopfield Boosting's approach.  Hopfield Boosting's resampling strategy focuses on samples near the decision boundary, which are considered weak learners, to create a strong learner that improves the decision boundary between in-distribution (ID) and out-of-distribution (OOD) data.  The heatmap visualizes the energy function exp(\u03b2E\u266d(\u03be; X, O)) showing high energy (bright) near the decision boundary and low energy (dark) further away.", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_42_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure shows how Hopfield Boosting adaptively resamples weak learners near the decision boundary. By focusing on these hard-to-classify samples, it creates a stronger learner that better separates the in-distribution and out-of-distribution data. The heatmap illustrates the energy function, where higher values indicate points closer to the boundary and thus more likely to be sampled.", "section": "3.3 Boosting Framework"}, {"figure_path": "VLQYtVMTYz/figures/figures_43_1.jpg", "caption": "Figure 2: Synthetic example of the adaptive resampling mechanism. Hopfield Boosting forms a strong learner by sampling and combining a set of weak learners close to the decision boundary. The heatmap on the background shows exp(\u03b2E\u266d(\u03be; X, O)), where \u03b2 is 60. Only the sampled (i.e., highlighted) points serve as memories X and O.", "description": "This figure demonstrates the adaptive resampling mechanism of Hopfield Boosting.  It shows how the algorithm focuses on creating strong learners by combining weak learners located near the decision boundary between in-distribution and out-of-distribution data. The heatmap visually represents the energy function, highlighting the importance of samples near the decision boundary.  Three panels illustrate: (a) the entire dataset, (b) random sampling and (c) Hopfield Boosting's adaptive sampling strategy.", "section": "3.3 Boosting Framework"}]