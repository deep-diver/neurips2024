[{"type": "text", "text": "Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuri R. Fonseca\u2217 Decision, Risk and Operations Columbia University New York, NY yfonseca23@gsb.columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Caio F. L. Peixoto\u2217   \nSchool of Applied Mathematics   \nGetulio Vargas Foundation   \nRio de Janeiro, RJ, Brazil   \ncaio.peixoto@fgv.br   \nYuri F. Saporito\u2217   \nSchool of Applied Mathematics   \nGetulio Vargas Foundation   \nRio de Janeiro, RJ   \nyuri.saporito@fgv.br ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Instrumental variables (IVs) provide a powerful strategy for identifying causal effects in the presence of unobservable confounders. Within the nonparametric setting (NPIV), recent methods have been based on nonlinear generalizations of Two-Stage Least Squares and on minimax formulations derived from moment conditions or duality. In a novel direction, we show how to formulate a functional stochastic gradient descent algorithm to tackle NPIV regression by directly minimizing the populational risk. We provide theoretical support in the form of bounds on the excess risk, and conduct numerical experiments showcasing our method\u2019s superior stability and competitive performance relative to current state-of-the-art alternatives. This algorithm enables flexible estimator choices, such as neural networks or kernel based methods, as well as non-quadratic loss functions, which may be suitable for structural equations beyond the setting of continuous outcomes and additive noise. Finally, we demonstrate this flexibility of our framework by presenting how it naturally addresses the important case of binary outcomes, which has received far less attention by recent developments in the NPIV literature. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal inference from observational data presents unique challenges, primarily due to the potential for confounding variables that can affect both outcomes and variables of interest. The unconfoundedness assumption, crucial in this context, posits that all confounding variables are observed and properly accounted for, allowing for an unbiased estimation of causal effects. However, in many real-world scenarios, this assumption is difficult to satisfy. When this is the case, approaches that rely on instrumental variables (IVs) \u2014 quantities that are correlated with the variable of interest (relevance condition), do not affect the outcome in any other way (exclusion condition) and are independent of the unobservable confounders \u2014 offer a way to identify causal effects even when unobserved confounders exist. As a concrete example, suppose we want to estimate the impact of years of education on earnings. Most likely, there are unobservable factors, such as omitted ability, affecting both the decision to study and income. In this case, changes in compulsory schooling laws could be used as an instrument [2]. ", "page_idx": 0}, {"type": "text", "text": "While traditional parametric approaches to IV regression often require assumptions about the relationships between variables that may not hold in practice, nonparametric IV (NPIV) models can adapt to the intrinsic structure of the data, allowing for a more nuanced understanding of causal relationships. For this reason, there has been a recent boost of new algorithms applied to the NPIV estimation problem and its theoretical properties. The challenge is that NPIV estimation is an ill-posed inverse problem [28, 8, 9], and recent methods aim to incorporate developments from predictive models, e.g., deep learning and kernel methods, while also accounting for the particular structure of the inverse problem at hand. ", "page_idx": 1}, {"type": "text", "text": "In this work, we deviate from previous approaches to NPIV estimation which end up minimizing empirical versions of the populational risk [28, 19, 34, 25], or trying to satisfy large collection of empirical moment restrictions [18, 24, 6]. In our formulation, we compute an analytical functional gradient for the actual populational risk. After formulating a consistent estimator for this gradient, we apply stochastic gradient descent in a certain function space to recover the effect of $X$ on the outcome $Y$ . ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is structured as follows. We conclude Section 1 with a thorough discussion of the previous works on NPIV and our contributions. Next, we provide the basic setting for NPIV regression in Section 2. In Section 3 we detail the associated risk minimization problem and analytically compute its stochastic gradient. Based on these results, we introduce our method in Section 4, with accompanying practical considerations and theoretical support. Numerical experiments are reported on Section 5, where our algorithm is compared to state-of-the-art machine learning methods for NPIV. Finally, in Section 6 we study the case of binary outcomes, showing how our method naturally addresses this scenario under the current assumptions made in the literature. Proofs of all our results are presented in Appendix A, while Appendix B contains formal comparisons with existing methods and Appendix C provides additional implementation details. ", "page_idx": 1}, {"type": "text", "text": "1.1 Previous work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Many traditional approaches to IV estimation, like the Two-Stage Least Squares (2SLS) method, rely on linear models for treatment estimation and counterfactual prediction functions (see [3, 40] for a thorough survey of classical IV estimation). These approaches, while efficient for estimating policy effects, depend on strong parametric assumptions. Nonparametric extensions of 2SLS then attempt to introduce model flexibility by utilizing linear projections onto known basis or sieve functions (as in [28, 27]) or kernel-based estimators (as in [17, 13]). However, these traditional methods face limitations in large, high-dimensional datasets as they are sensible to the particular choice of sieve functions and number of basis elements (see the discussion in [19]). ", "page_idx": 1}, {"type": "text", "text": "In order to propose a scalable method, [19] introduces DeepIV, a generalization of 2SLS which employs neural networks in each step of the two-stage procedure. Although their algorithm is more suitable for high dimensional data, the authors do not provide theoretical guarantees. Deep learning estimators have also appeared within methods focused on the Generalized Method of Moments (GMM), which leverage moment restrictions imposed by the IV to obtain an estimator. In [6], the authors propose DeepGMM, a reformulation of the optimal weighted GMM problem as a minimax problem. They rely on the identification assumption to demonstrate consistency results, provided their algorithm is able to nearly solve a smooth zero-sum game. ", "page_idx": 1}, {"type": "text", "text": "In another direction, some methods exploit developments in the $\\mathrm{RKHS}^{2}$ literature and apply them to NPIV estimation. The KIV algorithm in [34] transforms the problem into two-stage kernel ridge regression through a kernel mean embedding of the distribution of the endogenous covariate given the instrument. In [25], the authors take insight from two-stage problems in stochastic programming and propose DualIV, an algorithm which uses Fenchel duality to transform NPIV into a convex-concave saddle point problem, for which a closed-form RKHS algorithm is presented. Inspired by the gradient boosting literature, [4] propose an algorithm that iteratively fits base learners in a boosting fashion, but still needs a post-processing step where the basis functions are held fixed and the weights are optimized. ", "page_idx": 1}, {"type": "text", "text": "We point out that most of the cited works have some flexibility issues. While kernel based methods impose a higher computational cost as the number of data points grows and may exhibit poor performance in high dimensions [35, 25], Deep Learning based algorithms can suffer from high variance and instability if the amount of available data is small, as was seen in [25, 4]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Furthermore, in many applications of interest, ranging from consumer behavior [30] to epidemiology [23], the outcome is binary; for a review, see [12]. In these scenarios, a straightforward application of the quadratic loss function would result in a misspecification of the problem, potentially leading to erroneous estimates. The above NPIV methods strongly leverage the additive structure of the problem formulation, and proper extensions would require significant effort, a detailed discussion is presented in Appendix B. Traditionally, a common approach for binary outcomes is using a semiparametric specification based on control function [20]. This allows for nonparametric estimation of the distribution of the unobservables together with identification of the causal function\u2019s parameters [1, 32, 14, 11]. To the best of our knowledge, the only work that addresses fully nonparametric IV for binary outcomes is [10], which proposes an estimation strategy based on Tikhonov regularization. ", "page_idx": 2}, {"type": "text", "text": "1.2 Contribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We propose a novel algorithm for NPIV estimation, SAGD-IV, which works by minimizing the populational risk through stochastic gradient descent in a function space. Under mild assumptions, we provide finite sample bounds on the excess risk of our estimator. Empirically, we demonstrate through numerical experiments that SAGD-IV achieves state-of-the-art performance and better stability. ", "page_idx": 2}, {"type": "text", "text": "We have the freedom to employ a variety of supervised learning algorithms to form the estimator of the stochastic gradient, most notably kernel methods and neural networks. This means our estimator could be tailored to specific scenarios where a particular method is more likely to perform well. Furthermore, our algorithm is naturally able to handle non-quadratic loss functions, which allows us to extend both our algorithm and theoretical guarantees to the binary outcomes case. ", "page_idx": 2}, {"type": "text", "text": "2 Problem setup and notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Following most of the recent literature, we start by presenting the problem setup under the additive noise assumption to demonstrate our methodological contribution. Then, in Section 6, we analyze the important case of binary outcomes, a prototypical example of when this assumption does not hold. ", "page_idx": 2}, {"type": "text", "text": "Let $(\\Omega,A,\\mathbb{P})$ be the underlying probability space, and let $X$ be a random vector of covariates taking values in $\\mathcal{X}\\subseteq\\mathbf{R}^{d_{\\mathcal{X}}}$ . We assume that the response variable $Y$ is generated according to ", "page_idx": 2}, {"type": "equation", "text": "$$\nY=h^{\\star}(X)+\\varepsilon,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\varepsilon\\in L^{2}(\\Omega,\\mathcal{A},\\mathbb{P})$ and satisfies $\\mathbb{E}[\\varepsilon]=0$ . We denote by $\\mathbb{P}_{X}$ the distribution of the r.v. $X$ and assume that the structural function $h^{\\star}$ belongs to $L^{2}(\\mathcal{X},\\mathcal{B}(\\mathcal{X}),\\mathbb{P}_{X})^{3}$ , which we simply denote by $L^{2}(X)$ . This is a Hilbert space with norm and inner product given by $\\|h\\|_{L^{2}(X)}^{2}=\\dot{\\mathbb{E}}[\\dot{h}(X)^{2}]$ and $\\langle h,g\\rangle_{L^{2}(X)}=\\mathbb{E}[h(X)g(X)]$ . We assume that $\\mathbb{E}[\\varepsilon\\mid X]\\neq0$ , that is, some covariates are endogenous. Finally, we assume the existence of a random vector $Z$ , taking values in $\\mathcal{Z}\\subseteq\\mathbf{R}^{d_{Z}}$ and satisfying ", "page_idx": 2}, {"type": "text", "text": "1. $\\mathbb{E}[\\varepsilon\\mid Z]=0$ , the exclusion restriction;   \n2. $X\\not\\perp Z$ , i.e., $Z$ is relevant. ", "page_idx": 2}, {"type": "text", "text": "This makes $Z$ a valid instrumental variable. We define $\\mathbb{P}_{Z}$ and $L^{2}(Z)$ analogously to $\\mathbb{P}_{X}$ and $L^{2}(X)$ . We further consider the mild assumption that $X$ and $Z$ have a joint density denoted by $p_{X,Z}$ . Our goal is to estimate $h^{\\star}$ based on i.i.d. samples from the joint distribution of $X,Z$ and $Y$ . ", "page_idx": 2}, {"type": "text", "text": "As we have listed in the introduction, there are a few different approaches to estimate $h^{\\star}$ . We will follow here the original one of [28], in which we take the expected value of Equation (1) conditioned on $Z$ to get ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}[Y\\mid Z]=\\mathbb{E}[h^{\\star}(X)\\mid Z].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This motivates us to define the conditional expectation operator ${\\mathcal{P}}:L^{2}(X)\\to L^{2}(Z)$ given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{P}[h](z)=\\mathbb{E}[h(X)\\mid Z=z].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This is a bounded linear operator which satisfies $\\|\\mathcal{P}\\|_{\\mathrm{op}}\\leq1$ (the operator norm) and whose adjoint ${\\mathcal{P}}^{*}:L^{2}(Z)\\to L^{2}(X)$ , that is also a bounded linear operator, is given by ${\\mathcal{P}}^{*}[g](x)=\\mathbb{E}[g(Z)\\mid X=$ $x]$ . Defining $r(Z)=\\mathbb{E}[Y\\mid Z]$ , we can then rewrite Equation (2) as ", "page_idx": 3}, {"type": "equation", "text": "$$\nr=\\mathcal{P}[h^{\\star}].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is a Fredholm integral equation of the first kind [22] and, as such, poses an ill-posed linear inverse problem. ", "page_idx": 3}, {"type": "text", "text": "In this context, a common assumption [13, 7, 34] made about $\\mathcal{P}$ is compactness. We also need it here, noting that compact operators with infinite dimensional range provide prototypical examples of ill-posed inverse problems [8]. However, we wish to phrase this assumption in a different, albeit equivalent [8], form: ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.1. Let ", "text_level": 1, "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi(x,z)={\\frac{p_{X,Z}(x,z)}{p_{X}(x)p_{Z}(z)}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "denote the ratio of joint over product of marginals densities for the $X$ and $Z$ variables, with the convention that $0/0=0$ . We assume that this kernel has finite $L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})$ norm, that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\Phi\\|_{L^{2}({\\mathbb{P}}_{X}\\otimes{\\mathbb{P}}_{Z})}^{2}=\\int_{\\chi\\times\\ Z}\\Phi(x,z)^{2}p_{X}(x)p_{Z}(z)\\,\\mathrm{d}x\\mathrm{d}z<\\infty.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This is equivalent to assuming that the conditional expectation operator ${\\mathcal{P}}:L^{2}(X)\\to L^{2}(Z)$ is Hilbert-Schmidt and, hence, compact. ", "page_idx": 3}, {"type": "text", "text": "3 The risk and its gradient ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivated by Equation (3), we introduce a pointwise loss function $\\ell:{\\mathbf{R}}\\times{\\mathbf{R}}\\rightarrow{\\mathbf{R}}$ and define the associated populational risk measure4 $\\mathcal{R}:\\dot{L}^{2}(X)\\rightarrow\\mathbf{R}$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{R}(h)=\\mathbb{E}[\\ell(r(Z),\\mathcal{P}[h](Z))].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The example the reader should keep in mind is the squared loss function $\\ell(y,y^{\\prime})\\,=\\,{\\textstyle\\frac{1}{2}}(y\\,-\\,y^{\\prime})^{2}$ , although, as we will see, other examples could be used depending on the particular regression setting. Our goal is to solve the NPIV regression problem by solving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{h\\in\\mathcal{H}}\\mathcal{R}(h),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{H}$ is a closed, convex, bounded subset of $L^{2}(X)$ such that $h^{\\star}\\in\\mathcal{H}$ . We also require $0\\in\\mathcal H$ . For future reference, we state these conditions: ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.1 (Regularity of $\\mathcal{H}$ ). The set $\\mathcal{H}$ is a closed, convex, bounded subset of $L^{2}(X)$ , which contains the origin and satisfies $h^{\\star}\\in\\mathcal{H}$ . ", "page_idx": 3}, {"type": "text", "text": "The only part of this assumption which concerns the data generating process is $h^{\\star}\\,\\in\\,{\\mathcal{H}}$ , which essentially means that the set $\\mathcal{H}$ is large enough.5 ", "page_idx": 3}, {"type": "text", "text": "For $\\mathcal{H}$ satisfying Assumption 3.1, we let $D\\triangleq\\dim\\mathcal{H}<\\infty$ , so that $\\|h\\|<D$ for every $h\\in\\mathcal H$ . One possible choice for the set $\\mathcal{H}$ is the $L^{\\infty}(X)$ ball contained in $L^{2}(X)$ , that is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal H=\\left\\{h\\in L^{2}(X):\\|h\\|_{\\infty}\\leq A\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $A>0$ is a constant. This set is obviously convex and bounded in the $L^{2}(X)$ norm. It can be shown that it is also closed, but not necessarily compact, a restriction on the search set imposed in [28]. This can be seen by taking a $\\|\\cdot\\|_{\\infty}$ \u2013bounded orthonormal basis for $L^{2}(X)$ , if one exists. We denote by $\\mathrm{proj}_{\\mathcal{H}}$ the orthogonal projection onto $\\mathcal{H}$ . In case $\\mathcal{H}$ is given by Equation (6), we have the explicit formula6 ${\\mathrm{proj}}_{\\mathcal{H}}[h]=(h^{\\dagger}\\wedge A)-(h^{-}\\wedge A).$ . ", "page_idx": 3}, {"type": "text", "text": "We now state all the assumptions needed on the pointwise loss $\\ell$ . We denote by $\\partial_{2}$ a partial derivative with respect to the second argument. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2 (Regularity of $\\ell$ ). ", "page_idx": 4}, {"type": "text", "text": "1. The function $\\ell:{\\mathbf{R}}\\times{\\mathbf{R}}\\rightarrow{\\mathbf{R}}$ is convex and $C^{2}$ with respect to its second argument; ", "page_idx": 4}, {"type": "text", "text": "2. The function $\\ell$ has Lipschitz first derivative with respect to the second argument, i.e., there exists $L\\geq0$ such that, for all $y,y^{\\prime},u,u^{\\prime}\\in\\mathbf{R}$ we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n|\\partial_{2}\\ell(y,y^{\\prime})-\\partial_{2}\\ell(u,u^{\\prime})|\\leq L(|y-u|+|y^{\\prime}-u^{\\prime}|).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Some useful facts which follow immediately from these assumptions are presented in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "3.1 Gradient computation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here we divert from the previous literature and proceed to compute a functional stochastic gradient for the risk $\\mathcal{R}$ . A similar idea has been considered by [15] in the context of statistical inverse problems, however, their setup assumes that the operator posing the inverse problem is known, which greatly simplifies the analysis and cannot be directly applied to the NPIV problem. We start by providing an analytical formula for $\\nabla\\mathcal{R}(h)$ : ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.3. The risk $\\mathcal{R}$ is Fr\u00e9chet differentiable and it\u2019s gradient satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\mathcal{R}(h)=\\mathcal{P}^{*}[\\partial_{2}\\ell(r(\\cdot),\\mathcal{P}[h](\\cdot))]\\in L^{2}(X),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ${\\mathcal{P}}^{*}:L^{2}(Z)\\to L^{2}(X)$ is the adjoint of the operator $\\mathcal{P}$ . ", "page_idx": 4}, {"type": "text", "text": "Both $\\mathcal{P}$ and $\\mathcal{P}^{*}$ are unknown operators which commonly appear in NPIV estimation [8, 13], with Nadaraya-Watson kernels [26, 39] being a classical option for estimating them. However, the fact that they are nested in Equation (7) is undesired, and may impose extra computational costs, especially considering that $\\partial_{2}\\ell$ is nonlinear in the second argument for non-quadratic losses. We overcome this difficulty by leveraging the following characterization of the gradient: ", "page_idx": 4}, {"type": "text", "text": "Corollary 3.4. The gradient of the populational risk satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\mathcal{R}(h)(x)=\\mathbb{E}[\\Phi(x,Z)\\partial_{2}\\ell(r(Z),\\mathcal{P}[h](Z))],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Phi$ is defined as in Equation (4). ", "page_idx": 4}, {"type": "text", "text": "The benefit of our approach is that Equation (8) enables a more computationally efficient way to estimate the gradient of the risk functional, as we explain next. From Equation (8), for a given $x\\,\\in\\,{\\mathcal{X}}$ , the random variable $\\Phi(x,Z)\\partial_{2}\\ell(r(Z),\\mathcal{P}[h](Z))$ is an unbiased stochastic estimator of $\\nabla\\mathcal{R}(h)(x)$ . Our stochastic approximate gradient is then built using estimators $\\widehat\\Phi,\\widehat r$ and $\\widehat{\\mathcal{P}}$ of $\\Phi,r$ and $\\mathcal{P}$ respectively. With this notation, given a sample $Z$ , we consider ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\nabla\\mathcal{R}(h)}(x)=\\widehat{\\Phi}(x,Z)\\partial_{2}\\ell(\\widehat{r}(Z),\\widehat{\\mathcal{P}}[h](Z)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We have then substituted the estimation of the operator $\\mathcal{P}^{*}$ by the simpler problem of estimating the ratio of densities $\\Phi$ and computing its product with $\\partial_{2}\\ell(\\widehat{r}(Z),\\widehat{\\mathcal{P}}[\\widehat{h}](Z))$ . Moreover, density ratio estimation is an area of active research within the machine learning community, which makes developments in this direction immediately translatable into benefits to our method. ", "page_idx": 4}, {"type": "text", "text": "4 Algorithm: implementation and theory ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In Algorithm 1 we present Stochastic Approximate Gradient Descent $I V\\,(S A G D{-}I V)^{7}$ , a method for estimating $h^{\\star}$ using the approximation given by Equation (9). ", "page_idx": 4}, {"type": "text", "text": "Input: Samples $\\{(z_{m})_{m=1}^{M}\\}$ . Estimators $\\widehat\\Phi,\\widehat r$ and $\\widehat{\\mathcal{P}}$ . Sequence of learning rates $(\\alpha_{m})_{m=1}^{M}$ . Initial   \nguess $\\widehat{h}_{0}\\in\\mathcal{H}$ .   \nOutput: $\\widehat{h}$   \nfor $1\\leq m\\leq M$ do Se $:u_{m}=\\widehat{\\Phi}(\\cdot,z_{m})\\partial_{2}\\ell\\left(\\widehat{r}(z_{m}),\\widehat{\\mathcal{P}}[\\widehat{h}_{m-1}](z_{m})\\right)$ Se $\\mathrm{t}\\,\\widehat{h}_{m}=\\mathrm{proj}_{\\mathcal{H}}\\left[\\widehat{h}_{m-1}-\\alpha_{m}u_{m}\\right]$   \nSet end for $\\begin{array}{r}{\\hat{h}=\\frac{1}{M}\\sum_{m=1}^{M}\\hat{h}_{m}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Remark 4.1. We note the fact that the internal loop of Algorithm 1 only needs samples from the instrumental variable $Z$ to unfold. ", "page_idx": 5}, {"type": "text", "text": "For our theoretical analysis, all we require of the estimators of $\\Phi,r$ and $\\mathcal{P}$ is the following: ", "page_idx": 5}, {"type": "text", "text": "Assumption 4.2 (Properties of $\\widehat\\Phi,\\widehat r$ and $\\widehat{\\mathcal{P}}$ ). ", "page_idx": 5}, {"type": "text", "text": "1. $\\widehat\\Phi,\\widehat r$ and $\\widehat{\\mathcal{P}}$ are computed using a dataset $\\mathcal{D}$ , composed of $N$ samples of the triplet $(X,Z,Y)$ ,   \nwhich are independent from the $Z$ samples used in Algorithm 1.   \n2. $\\widehat{r}\\in L^{2}(Z)$ a.s.;   \n3. ${\\widehat{\\mathcal{P}}}:L^{2}(X)\\to L^{2}(Z)$ is a bounded linear operator a.s.   \n4. $\\|\\widehat{\\Phi}\\|_{\\infty}\\triangleq\\underset{(x,z)\\in\\mathcal{X}\\times\\mathcal{Z}}{\\mathrm{ess\\up}}\\ |\\widehat{\\Phi}(x,z)|<\\infty$ . This implies, in particular, $\\|\\widehat\\Phi\\|_{L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})}<\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "4.1 On computing $\\widehat{\\mathcal{P}},\\widehat{\\Phi}$ and $\\widehat{r}$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Algorithm 1 is purposefully agnostic to how the estimators $\\widehat{\\mathcal{P}},\\widehat{\\Phi}$ and $\\widehat{r}$ are obtained, in order to provide the user with sufficient modeling flexibility. In what follows, we will explain the options we considered for accomplishing these tasks in the experiments section. A more detailed description is given in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Estimating $\\Phi$ is a interesting problem in itself as it is a ratio of densities, and it has drawn significant attention from the Machine Learning community. Here, we consider kernel and neural network methods, regarded as the state of the art. On the other hand, not many options are available to compute $\\widehat{\\mathcal{P}}$ , which means estimating all possible regression of functions of $X$ over $Z$ . Here, we chose to use kernel mean embeddings, as was developed [34]. Finally, estimating $r$ is the simplest procedure in our method as it is a regression of $Y$ over $Z$ . Similarly to $\\Phi$ , we also consider kernel and neural network methods. ", "page_idx": 5}, {"type": "text", "text": "Based on these options, we decided to specify two variants of SAGD-IV built upon how we compute $\\widehat{\\Phi}$ and ${\\widehat{r}}:$ : Kernel SAGD-IV uses RKHS methods for both estimators, while Deep SAGD-IV employs neural network estimators in these two tasks. The method for computing $\\widehat{\\mathcal{P}}$ is kernel mean embeddings in both variants. ", "page_idx": 5}, {"type": "text", "text": "Under a big data scenario, the dependence of our algorithm on kernel methods to estimate the $\\mathcal{P}$ operator could be a limitation. However, we are able to avoid this issue for the estimation of $\\Phi$ and $r$ . Nonetheless, since our algorithm is agnostic to the specific estimator of $\\mathcal{P}$ , we could take advantage of any recent developments in the literature of operator-estimation problems. ", "page_idx": 5}, {"type": "text", "text": "4.2 Risk bound ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Since we are directly optimizing the projected populational risk measure, we are able to provide guarantees for $\\mathcal{R}(\\widehat{h})$ in mean with respect to the training data $z_{1:M}\\,=\\,\\{z_{1},\\dots,z_{m}\\}$ . Our main result is the following: ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3. Let $\\widehat{h}_{0},\\ldots,\\widehat{h}_{M-1}$ be generated according to Algorithm $^{l}$ . Assume that $\\ell$ satisfies Assumption 3.2, $\\mathcal{H}$ satisfies Assumption 3.1, $\\Phi$ satisfies Assumption 2.1 and $\\widehat\\Phi,\\widehat r,\\widehat\\mathcal{P}$ satisfy Assumption 4.2. Then, if we let $\\begin{array}{r}{\\widehat{h}=\\frac{1}{M}\\sum\\widehat{\\d}_{m=1}^{M}\\widehat{h}_{m-1}}\\end{array}$ , the following bound holds: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z_{1:M}}\\left[\\mathcal{R}(\\widehat{h})-\\mathcal{R}(h^{\\star})\\right]\\leq\\frac{D^{2}}{2M\\alpha_{M}}+\\frac{\\xi}{M}\\sum_{m=1}^{M}\\alpha_{m}+\\tau\\sqrt{\\zeta},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{:=\\frac{3}{2}\\|\\widehat{\\Phi}\\|_{\\infty}^{2}\\left(C_{0}^{2}+L^{2}\\|\\widehat{r}\\|_{L^{2}(Z)}^{2}+L^{2}D^{2}\\|\\widehat{\\mathcal{P}}\\|_{\\infty}^{2}\\right),\\quad\\zeta=\\|\\Phi-\\widehat{\\Phi}\\|_{L^{2}(\\mathbb{R}_{X}\\otimes\\mathbb{P}_{Z})}^{2}+\\|r-\\widehat{r}\\|_{L^{2}(Z)}^{2}+\\|\\mathcal{P}-\\widehat{P}\\|_{\\infty}^{2}}\\\\ &{\\quad=2D\\operatorname*{max}\\bigg\\{3(C_{0}^{2}+L^{2}\\mathbb{E}[Y^{2}]+L^{2}D^{2}),2L^{2}\\|\\widehat{\\Phi}\\|_{\\infty}^{2},2L^{2}D^{2}\\|\\widehat{\\Phi}\\|_{\\infty}^{2}\\bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It is productive to analyze the RHS of the bound in Equation (10) more carefully. If we choose the sequence $\\left(\\alpha_{m}\\right)$ to satisfy the usual assumptions ", "page_idx": 6}, {"type": "equation", "text": "$$\nM\\alpha_{M}\\rightarrow\\infty\\quad\\mathrm{and}\\quad\\frac{1}{M}\\sum_{m=1}^{M}\\alpha_{m}\\rightarrow0\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "as $M\\to\\infty$ , then the first two terms in the sum vanish as $M$ grows. The last term appears due to the fact that we do not know $\\Phi,r$ nor $\\mathcal{P}$ , but it explicitly quantifies how the estimation errors of $\\widehat\\Phi,\\widehat r$ and $\\widehat{\\mathcal{P}}$ come together to determine the quality of the final estimator. Furthermore, it converges t o z e ro if the chosen estimation methods are consistent.8 ", "page_idx": 6}, {"type": "text", "text": "Remark 4.4 $Y$ versus $r(Z))$ . Since $r=\\mathcal{P}[h^{\\star}]$ , one should compute the risk as in Equation (5), comparing $\\mathcal{P}[h](Z)$ with $r(Z)$ . However, most works on NPIV [28, 34, 25] compare $\\mathcal{P}[h](Z)$ directly with $Y$ instead of $r(Z)$ . In Appendix E we discuss this difference and its relationship with the estimator $\\widehat{r}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4.5 (Consistency for $h^{*}$ ). Since the NPIV regression is an ill-posed inverse problem, consistency of our algorithm is a very challenging and interesting research question, and it would require additional assumptions. For instance, under strong convexity of the populational risk measure, it is well known that consistency is obtained if the excess risk converges to zero. It can be shown that, for the quadratic loss, strong convexity of the populational risk is equivalent to $\\mathcal{P}$ having a continuous inverse. ", "page_idx": 6}, {"type": "text", "text": "5 Numerical experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Here, we compare the performance of the two variants of SAGD-IV explained in Section 4.1, Kernel SAGD-IV and Deep SAGD-IV, with that of five other notable algorithms for estimating $h^{\\star}$ in the context of continuous responses: KIV, DualIV, DeepGMM, DeepIV and 2SLS. Hyperparameters and other implementation details are discussed in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "5.1 Continuous response ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To study the performance of our estimator in a continuous response setting, we used the data generating process from [6], which we recall below: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{Y=h^{\\star}(X)+\\epsilon+\\delta,\\qquad}&{X=Z_{1}+\\epsilon+\\gamma,}\\\\ &{Z=(Z_{1},Z_{2})\\sim\\mathrm{Uniform}([-3,3]^{2}),\\qquad}&{\\epsilon\\sim\\mathcal{N}(0,1),\\;\\gamma,\\delta\\sim\\mathcal{N}(0,0.1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, the confounding variable is $\\epsilon$ and within this specification, four options for $h^{\\star}$ are considered: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{{\\mathrm{\\boldmath~\\step~}}:}}&{{h^{\\star}(x)={\\bf1}\\{x>0\\},}}\\\\ {{{\\mathrm{\\boldmath~\\linear~}}:}}&{{h^{\\star}(x)=x,}}\\end{array}\\qquad\\qquad\\qquad\\begin{array}{c c}{{{\\mathrm{\\bf~abs}}:}}&{{h^{\\star}(x)=|x|,}}\\\\ {{{\\mathrm{\\boldmath~\\sin~}}\\colon}}&{{h^{\\star}(x)=\\sin(x).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We adjusted each estimator using 3000 random variable samples (see Remark C.1), and tested them on 1000 samples of $X$ . For each method and response function, we evaluated predictions over 20 realizations of the data. Log mean squared error (MSE) box plots and plots of each method\u2019s estimator for a randomly chosen realization of the data are displayed in Figure 1. ", "page_idx": 7}, {"type": "image", "img_path": "VqxODXhU4k/tmp/ed2e0d38df299c0af13cbc196fb44808953a4a0fb943a8d7eb18deeb7e973ebb.jpg", "img_caption": ["Figure 1: (Left) Log MSE for each model under different response functions. (Right) Plots of each method\u2019s estimator in a randomly selected realization of the data. On the left column, we have $Y$ observations in green and the true structural function in black. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "The first thing we can notice from Figure 1 is that TSLS dominates the linear scenario, as expected, but behaves poorly in all others, showing the importance of nonlinear approaches. ", "page_idx": 7}, {"type": "text", "text": "On a second note, both variants of SAGD-IV performed competitively in all scenarios, with the KIV method being a strong competitor in step and sin. However, its performance was significantly worse for the abs response function. Additionally, it is worth noticing the competitive MSEs of the deep learning variant of SADG-IV, which performed consistently better than the other two neural-network-based algorithms. ", "page_idx": 7}, {"type": "text", "text": "Furthermore, an important observation is that, within kernel methods, Kernel SAGD-IV had the best stability/performance tradeoff, having the smallest variance across scenarios. In contrast, DualIV exhibited the highest variance. This can be explained by considering DualIV\u2019s hyperparameter selection algorithm [25, Section 4]. In order to perform cross-validation to choose the best regularization parameters, the algorithm requires another (different) regularization parameter as an input, which must be chosen without any guidance. The authors state that it was set to a \u201csmall constant\u201d. This can explain the poor performance, given the importance of regularization parameters for ridge regression algorithms. We note our method does not suffer from this problem. ", "page_idx": 7}, {"type": "text", "text": "6 Binary Response ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Here we outline how our methodology can be applied to a scenario where the quadratic loss is not the natural option: that of binary response models. We show that, by simply modifying the loss $\\ell$ in the risk definition and, consequently, in the algorithm, we are able to attack this problem as it is currently formulated in the literature. This is in clear contrast with recent methods for NPIV estimation, which, when applicable, would require significant effort. We discuss the limitations of current methods and possible extensions in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "In the binary response model we consider, the only change we need to make to the data generating process is the following: ", "page_idx": 7}, {"type": "equation", "text": "$$\nY={\\bf1}\\left\\{Y^{\\star}>0\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $Y^{\\star}=h^{\\star}(X)+\\varepsilon$ . That is, we observe a signal which indicates if the response variable is greater than the threshold zero. Although simple, this type of model captures important applications as, for instance, in economics/consumer behavior, where $Y^{*}$ is the utility the consumer gets from buying a product and we only observe $Y$ , the buy/no buy decision. The function $h^{\\star}$ captures the impact of the variable $X$ on the consumers\u2019 utility. As usual in the literature, $h^{\\star}$ is interpretable relative to the scale parameter of $\\varepsilon$ . Here, $X,Z$ and $\\varepsilon$ satisfy the same assumptions as before. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In order to use one of the available NPIV regression methods in the literature, we must first identify a suitable risk functional, of the form shown in Equation (5). Define $\\eta=h^{\\star}(X)-\\mathbb{E}[h^{\\star}(X)\\mid Z]+\\varepsilon,$ , so that $\\mathbb{E}[\\eta\\mid Z]=0$ and ", "page_idx": 8}, {"type": "equation", "text": "$$\nY={\\bf1}\\left\\{\\mathbb{E}[h^{\\star}(X)\\mid Z]+\\eta>0\\right\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Denoting by $F_{\\eta|Z}$ the conditional distribution function of $\\eta$ given $Z$ , we find ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[Y\\mid Z]=\\mathbb{P}\\big[\\mathbb{E}[h^{\\star}(X)\\mid Z]+\\eta>0\\mid Z\\big]=1-F_{\\eta\\mid Z}\\big({-}\\mathbb{E}[h^{\\star}(X)\\mid Z]\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Assume that the distribution of $\\eta$ given $Z$ is always symmetric around zero, so that $1-F_{\\eta|Z}(-y)=$ $F_{\\eta|Z}(y)$ . We then have $r(Z)\\;=\\;F_{\\eta|Z}({\\mathcal P}[h^{\\star}](Z))$ . Notice that any notion of risk for a given $h\\in L^{2}(X)$ would have to compare $r(Z)$ and $\\mathcal{P}[h](Z)$ through $F_{\\eta|Z}$ , which is an unknown function. Hence, without making further assumptions, it is not clear how to setup the risk in a useful way, which hinders current NPIV methods from tackling the problem. ", "page_idx": 8}, {"type": "text", "text": "As far as we know, the only assumption in the literature which allows one to be fully nonparametric in $h^{\\star}$ within the binary outcomes setting was first introduced in [10]. It posits that $\\eta$ is independent of $Z$ , and has known distribution function $F$ . Since $\\eta=Y^{\\star}-\\mathbb{E}[Y^{\\star}\\mid Z]$ , it is already uncorrelated with any function of $Z$ . The assumption extends this to independence, and makes available the resulting distribution function. ", "page_idx": 8}, {"type": "text", "text": "Under this assumption, we conclude that $r(Z)=F(\\mathcal{P}[h^{\\star}](Z))$ and, by the negative log-likelihood of the Bernoulli distribution, a natural candidate for $\\ell$ is ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\ell(y,y^{\\prime})=\\mathrm{BCE}(y,F(y^{\\prime})),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where BCE is the binary cross entropy function: $\\mathrm{BCE}(p,q)=-[p\\log q+(1-p)\\log(1-q)]$ . Then, the risk becomes ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(h)=\\mathbb{E}[\\mathrm{BCE}(r(Z),F(\\mathcal{P}[h](Z)))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Whereas it is obvious that the quadratic loss satisfies Assumption 3.2, it is not clear whether the same is true for the loss $\\ell(y,y^{\\prime})=\\mathrm{BCE}(y,F(y^{\\prime}))$ . Nevertheless, if $F(x)=\\sigma(x)\\triangleq1/(1+\\exp(-x/\\beta))$ , the c.d.f. of a logistic distribution with scale parameter $\\beta$ , one can verify that Assumption 3.2 holds. ", "page_idx": 8}, {"type": "text", "text": "6.1 Numerical Experiment ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We mimicked the continuous response DGP making the necessary modifications: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{l}{Y={\\mathbf1}\\{\\mathbb{E}[h^{\\star}(X)\\mid Z]+\\eta>0\\},}\\\\ {Z=(Z_{1},Z_{2})\\sim\\mathrm{Uniform}([-3,3]^{2}),\\quad\\quad\\quad}&{\\eta\\sim\\mathrm{Logistic}(0,\\beta)\\quad\\gamma\\sim\\mathcal{N}(0,0.1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Here, $\\beta$ is a scale parameter which was set to $\\sqrt{0.1}$ . For response functions, we analyzed the sin and linear cases, since we must compute $\\mathbb{E}[h^{\\star}(X)\\mid Z]$ analytically in order to generate the data. This computation is easy for the selected scenarios, and we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}[X\\mid Z=z]=z_{1},\\quad\\mathbb{E}[\\sin(X)\\mid Z=z]={\\frac{\\beta\\pi\\cdot e^{-{\\frac{0.1}{2}}}}{\\sinh(\\beta\\pi)}}\\sin(z_{1})\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "As with the continuous response setting, we present log MSE results and samples from the resulting estimators in Figure 2. ", "page_idx": 8}, {"type": "text", "text": "We can immediately see that our kernel based estimator performed remarkably well for both response functions. The binary setup is significantly more challenging, as we are reducing the continuous response $Y$ to a simple binary signal. Kernel SAGD-IV obtained results which are, on average, on par with the continuous response scenario and Deep SAGD-IV had more difficulty uncovering the true structural function. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we proposed SAGD-IV, a flexible framework for performing nonparametric instrumental variable regression. It is based on a novel way to tackle the NPIV problem, in which we formulate ", "page_idx": 8}, {"type": "image", "img_path": "VqxODXhU4k/tmp/adf43a5e06bc70f77192ef22a676f79012192f24184652a0870b9408363772ac.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2: (Left) Log MSE distribution in the binary response DGP. (Right) Plots of each method\u2019s estimator in a randomly selected realization of the data for the binary response DGP. On the left column, we have samples from the binary response variable $Y$ in green and the true structural function in black. ", "page_idx": 9}, {"type": "text", "text": "the minimization of the populational risk as functional stochastic gradient descent. We are then able to naturally consider non-quadratic loss function and incorporate various regression methods under one formulation. Under mild assumptions, we provided bounds on the excess risk of our estimator. Furthermore, we empirically demonstrated superior stability and state-of-the-art MSEs for continuous outcomes, as well as a promising performance for the binary regression setup. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Hyungtaik Ahn, Hidehiko Ichimura, and James L Powell. \u201cSimple estimators for monotone index models\u201d. In: manuscript, Department of Economics, UC Berkeley 10 (1996). [2] Joshua D Angrist and Alan B Krueger. \u201cDoes compulsory school attendance affect schooling and earnings?\u201d In: The Quarterly Journal of Economics 106.4 (1991), pp. 979\u20131014.   \n[3] Joshua D Angrist and J\u00f6rn-Steffen Pischke. Mostly harmless econometrics: An empiricist\u2019s companion. Princeton university press, 2009. [4] Edvard Bakhitov and Amandeep Singh. \u201cCausal Gradient Boosting: Boosted Instrumental Variable Regression\u201d. In: Proceedings of the 23rd ACM Conference on Economics and Computation. EC \u201922. Boulder, CO, USA: Association for Computing Machinery, 2022, pp. 604\u2013605. ISBN: 9781450391504. DOI: 10.1145/3490486.3538251. URL: https: //doi.org/10.1145/3490486.3538251.   \n[5] Keith Battocchi et al. EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation. https://github.com/py-why/EconML. Version 0.x. 2019. [6] Andrew Bennett, Nathan Kallus, and Tobias Schnabel. \u201cDeep Generalized Method of Moments for Instrumental Variable Analysis\u201d. In: Advances in Neural Information Processing Systems. Ed. by H. Wallach et al. Vol. 32. Curran Associates, Inc., 2019.   \n[7] Andrew Bennett et al. \u201cMinimax Instrumental Variable Regression and $L_{2}$ Convergence Guarantees without Identification or Closedness\u201d. In: Proceedings of Thirty Sixth Conference on Learning Theory. Ed. by Gergely Neu and Lorenzo Rosasco. Vol. 195. Proceedings of Machine Learning Research. PMLR, July 2023, pp. 2291\u20132318. [8] Marine Carrasco, Jean-Pierre Florens, and Eric Renault. \u201cHandbook of Econometrics\u201d. In: vol. 6B. Elsevier, 2007. Chap. Linear Inverse Problems in Structural Econometrics Estimation Based on Spectral Decomposition and Regularization. [9] Laurent Cavalier. \u201cInverse problems in statistics\u201d. In: Inverse Problems and High-Dimensional Estimation: Stats in the Ch\u00e2teau Summer School, August 31-September 4, 2009. Springer, 2011, pp. 3\u201396.   \n[10] Samuele Centorrino and Jean-Pierre Florens. \u201cNonparametric Instrumental Variable Estimation of Binary Response Models with Continuous Endogenous Regressors\u201d. In: Econometrics and Statistics 17 (2021), pp. 35\u201363.   \n[11] Sandeep Chitla, Srikanth Jagabathula, and Ashwin Venkataraman. \u201cNonparametric Demand Estimation in the Presence of Unobserved Factors\u201d. In: Available at SSRN (2022).   \n[12] Paul S Clarke and Frank Windmeijer. \u201cInstrumental variable estimators for binary outcomes\u201d. In: Journal of the American Statistical Association 107.500 (2012), pp. 1638\u20131652.   \n[13] S. Darolles et al. \u201cNonparametric Instrumental Regression\u201d. In: Econometrica 79.5 (2011), pp. 1541\u20135165.   \n[14] Yingying Dong and Arthur Lewbel. \u201cA simple estimator for binary choice models with endogenous regressors\u201d. In: Econometric Reviews 34.1-2 (2015), pp. 82\u2013105.   \n[15] Yuri Fonseca and Yuri Saporito. \u201cStatistical Learning and Inverse Problems: A Stochastic Gradient Approach\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 9591\u20139602.   \n[16] L\u00e1szl\u00f3 Gy\u00f6rf iet al. \u201cA Distribution-Free Theory of Nonparametric Regression\u201d. In: Springer Series in Statistics. 2002. URL: https : / / api . semanticscholar . org / CorpusID : 43315484.   \n[17] Peter Hall and Joel L. Horowitz. \u201cNonparametric methods for inference in the presence of instrumental variables\u201d. In: The Annals of Statistics 33.6 (2005), pp. 2904\u20132929. DOI: 10. 1214/009053605000000714. URL: https://doi.org/10.1214/009053605000000714.   \n[18] Lars Peter Hansen. \u201cLarge Sample Properties of Generalized Method of Moments Estimators\u201d. In: Econometrica 50.4 (1982), pp. 1029\u20131054. ISSN: 00129682, 14680262. URL: http: //www.jstor.org/stable/1912775 (visited on 05/18/2024).   \n[19] Jason Hartford et al. \u201cDeep IV: A Flexible Approach for Counterfactual Prediction\u201d. In: Proceedings of the 34th International Conference on Machine Learning - Volume 70. ICML\u201917. Sydney, NSW, Australia: JMLR.org, 2017, pp. 1414\u20131423.   \n[20] Joel L Horowitz. \u201cApplied nonparametric instrumental variables estimation\u201d. In: Econometrica 79.2 (2011), pp. 347\u2013394.   \n[21] Diederik P. Kingma and Jimmy Ba. \u201cAdam: A Method for Stochastic Optimization\u201d. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Ed. by Yoshua Bengio and Yann LeCun. 2015. URL: http://arxiv.org/abs/1412.6980.   \n[22] Rainer Kress. Linear Integral Equations. Applied Mathematical Sciences. Springer-Verlag, 1989.   \n[23] Debbie A Lawlor et al. \u201cMendelian randomization: using genes as instruments for making causal inferences in epidemiology\u201d. In: Statistics in medicine 27.8 (2008), pp. 1133\u20131163.   \n[24] Greg Lewis and Vasilis Syrgkanis. \u201cAdversarial generalized method of moments\u201d. In: arXiv preprint arXiv:1803.07164 (2018).   \n[25] Krikamol Muandet et al. \u201cDual Instrumental Variable Regression\u201d. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS\u201920. Vancouver, BC, Canada: Curran Associates Inc., 2020. ISBN: 9781713829546.   \n[26] E. A. Nadaraya. \u201cOn Estimating Regression\u201d. In: Theory of Probability & its Applications 9.1 (1964), pp. 141\u2013142.   \n[27] Whitney K. Newey. \u201cNonparametric Instrumental Variables Estimation\u201d. In: The American Economic Review 103.3 (2013), pp. 550\u2013556. ISSN: 00028282. URL: http://www.jstor. org/stable/23469792 (visited on 02/01/2024).   \n[28] Whitney K. Newey and James L. Powell. \u201cInstrumental Variable Estimation of Nonparametric Models\u201d. In: Econometrica 71.5 (2003), pp. 1565\u20131578. DOI: http://dx.doi.org/10. 1111/1468-0262.00459.   \n[29] Hemant Kumar Pathak. An Introduction to Nonlinear Analysis and Fixed Point Theory. Springer, 2018.   \n[30] Amil Petrin and Kenneth Train. \u201cA control function approach to endogeneity in consumer choice models\u201d. In: Journal of marketing research 47.1 (2010), pp. 3\u201313.   \n[31] Yixuan Qiu and Xiao Wang. \u201cStochastic Approximate Gradient Descent via the Langevin Algorithm\u201d. In: AAAI Conference on Artificial Intelligence 34 (2020), pp. 5428\u20135435.   \n[32] Christoph Rothe. \u201cSemiparametric estimation of binary response models with endogenous regressors\u201d. In: Journal of Econometrics 153.1 (2009), pp. 51\u201364.   \n[33] R Singh, L Xu, and A Gretton. \u201cKernel methods for causal functions: dose, heterogeneous and incremental response curves\u201d. In: Biometrika 111.2 (July 2023), pp. 497\u2013516. ISSN: 1464- 3510. DOI: 10.1093/biomet/asad042. eprint: https://academic.oup.com/biomet/ article- pdf/111/2/497/57467664/asad042.pdf. URL: https://doi.org/10. 1093/biomet/asad042.   \n[34] Rahul Singh, Maneesh Sahani, and Arthur Gretton. \u201cKernel Instrumental Variable Regression\u201d. In: Advances in Neural Information Processing Systems. Ed. by H. Wallach et al. Vol. 32. Curran Associates, Inc., 2019.   \n[35] Ingo Steinart and Andreas Christmann. Support Vector Machines. Information Science and Statistics. Springer, 2008.   \n[36] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ration Estimation in Machine Learning. Cambridge University Press, 2012.   \n[37] Zolt\u00e1n Szab\u00f3 et al. \u201cLearning Theory for Distribution Regression\u201d. In: Journal of Machine Learning Research 17.152 (2016), pp. 1\u201340. URL: http://jmlr.org/papers/v17/14- 510.html.   \n[38] Prem Talwai, Ali Shameli, and David Simchi-Levi. \u201cSobolev Norm Learning Rates for Conditional Mean Embeddings\u201d. In: Proceedings of The 25th International Conference on Artificial Intelligence and Statistics. Ed. by Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera. Vol. 151. Proceedings of Machine Learning Research. PMLR, 28\u201330 Mar 2022, pp. 10422\u2013 10447. URL: https://proceedings.mlr.press/v151/talwai22a.html.   \n[39] Geoffrey S. Watson. \u201cSmooth Regression Analysis\u201d. In: Sankhy\u00afa: The Indian Journal of Statistics, Series A 26.4 (1964), pp. 359\u2013372.   \n[40] Jeffrey M. Wooldridge. Econometric Analysis of Cross Section and Panel Data. The MIT Press, 2001. ISBN: 9780262232197. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proposition A.1. Assume that $\\ell$ satisfies Assumption 3.2. Then: ", "page_idx": 12}, {"type": "text", "text": "1. Setting $C_{0}=|\\partial_{2}\\ell(0,0)|$ |, we have $\\lvert\\partial_{2}\\ell(y,y^{\\prime})\\rvert\\le C_{0}+L(\\lvert y\\rvert+\\lvert y^{\\prime}\\rvert)$ , for all $y,y^{\\prime}\\in\\mathbf{R},$ ; 2. The map $f\\mapsto\\partial_{2}\\ell(r(\\cdot),f(\\cdot))$ from $L^{2}(Z)$ to $L^{2}(Z)$ is well-defined and $L$ -Lipschitz. 3. The second derivative with respect to the second argument is bounded: $\\left|\\partial_{2}^{2}\\ell(y,y^{\\prime})\\right|\\leq L$ for all $y,y^{\\prime}\\in\\mathbf{R}.$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. ", "page_idx": 12}, {"type": "text", "text": "1. Write $\\partial_{2}\\ell(y,y^{\\prime})=\\partial_{2}\\ell(y,y^{\\prime})-\\partial_{2}\\ell(0,0)+\\partial_{2}\\ell(0,0)$ and apply the triangle inequality as well as Assumption 3.2 Item 2. ", "page_idx": 12}, {"type": "text", "text": "2. From the previous item we know this map is well-defined. If $f$ and $g$ belong to $L^{2}(Z)$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\partial_{2}\\ell(r(\\cdot),f(\\cdot))-\\partial_{2}\\ell(r(\\cdot),g(\\cdot))\\|_{L^{2}(Z)}^{2}=\\mathbb{E}\\left[|\\partial_{2}\\ell(r(Z),f(Z))-\\partial_{2}\\ell(r(Z),g(Z))|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq L^{2}\\mathbb{E}\\left[|f(Z)-g(Z)|^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=L^{2}\\|f-g\\|_{L^{2}(Z)}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "3. It follows directly from Assumption 3.2 Item 2. ", "page_idx": 12}, {"type": "text", "text": "Proof of Proposition 3.3. We start by computing the directional derivative of $\\mathcal{R}$ at $h$ in the direction $f$ , denoted by $D\\mathcal{R}[h](f)$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D\\mathbb{R}[h](f)=}&{\\frac{1}{\\hat{\\lambda}-\\hat{\\lambda}}\\Bigg\\Vert\\mathcal{R}(h+\\delta f)-\\mathcal{R}(f)\\Bigg\\Vert}\\\\ &{=\\underset{\\hat{\\lambda}=0}{\\overset{\\mathrm{Im}}{\\prod}}\\frac{1}{\\hat{\\lambda}}\\mathbb{E}\\left\\{\\ell(Z),\\mathcal{P}[h]+\\delta f[Z]\\right\\}-\\ell(r(Z),\\mathcal{P}[h](Z))\\Bigg.}\\\\ &{=\\underset{\\hat{\\lambda}=0}{\\overset{\\mathrm{Im}}{\\prod}}\\frac{1}{\\hat{\\lambda}}\\mathbb{E}\\left\\{\\ell(Z),\\mathcal{P}[h](Z)+\\delta P[f](Z)-\\ell(r(Z),\\mathcal{P}[h](Z))\\right\\}}\\\\ &{=\\underset{\\hat{\\lambda}=0}{\\overset{\\mathrm{Im}}{\\prod}}\\frac{1}{\\hat{\\lambda}}\\Bigg\\{\\delta g_{\\ell}(r(Z),\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)}\\\\ &{\\qquad\\qquad\\quad+\\left.\\frac{\\delta^{2}}{2}\\partial_{z}^{2}\\ell(r(Z),\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)^{2}\\right\\}}\\\\ &{=\\mathbb{E}\\left\\{\\partial_{z}\\ell(r(Z),\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)\\right\\}}\\\\ &{\\qquad\\qquad\\quad+\\frac{\\hat{\\lambda}\\cdot\\mathrm{Im}}{\\hat{\\lambda}}\\Bigg\\{\\frac{\\delta}{2}\\hat{\\lambda}^{2}\\ell(r(Z),\\mathcal{P}[h]+\\delta f[Z])\\cdot\\mathcal{P}[f](Z)^{2}\\Bigg\\}}\\\\ &{=\\mathbb{E}\\left\\{\\partial_{z}\\ell(r(Z),\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\theta\\in(0,\\delta)$ is due to Taylor\u2019s formula. The last step is then due to Proposition A.1 Item 3. We can in fact expand the calculation a bit more, as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\mathcal{R}[h](f)=\\mathbb{E}\\left[\\partial_{2}\\ell(r(Z),\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)\\right]}\\\\ &{\\qquad\\qquad=\\langle\\partial_{2}\\ell(r(\\cdot),\\mathcal{P}[h](\\cdot)),\\mathcal{P}[f]\\rangle_{L^{2}(Z)}}\\\\ &{\\qquad\\qquad=\\langle\\mathcal{P}^{*}[\\partial_{2}\\ell(r(Z),\\mathcal{P}[h](\\cdot))],f\\rangle_{L^{2}(X)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "This shows that $\\mathcal{R}$ is G\u00e2teaux-differentiable, with G\u00e2teaux derivative at $h$ given by ", "page_idx": 12}, {"type": "equation", "text": "$$\nD\\mathcal{R}[h]=\\mathcal{P}^{*}[\\partial_{2}\\ell(r(\\cdot),\\mathcal{P}[h](\\cdot))].\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By Proposition A.1 Item 2, we have that $h\\mapsto D\\mathcal{R}[h]$ is a continuous mapping from $L^{2}(X)$ to $L^{2}(X)$ , which implies that $\\mathcal{R}$ is also Fr\u00e9chet-differentiable, and both derivatives coincide [29]. Therefore, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla\\mathcal{R}(h)=\\mathcal{P}^{*}[\\partial_{2}\\ell(r(\\cdot),\\mathcal{P}[h](\\cdot))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof of Corollary 3.4. From Proposition 3.3 we know that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\nabla\\mathcal{R}(h)(x)=\\mathcal{P}^{*}[\\partial_{2}\\ell(r(\\cdot),\\mathcal{P}[h](\\cdot))](x)=\\mathbb{E}[\\partial_{2}\\ell(r(Z),\\mathcal{P}[h](Z))\\mid X=x]}}\\\\ &{}&{=\\displaystyle\\int_{Z}p_{Z|X}(z\\mid x)\\partial_{2}\\ell(r(z),\\mathcal{P}[h](z))\\;\\mathrm{d}z=\\int_{Z}\\frac{p_{X,Z}(x,z)}{p_{X}(x)}\\partial_{2}\\ell(r(z),\\mathcal{P}[h](z))\\;\\mathrm{d}z}\\\\ &{}&{=\\displaystyle\\int_{Z}p_{Z}(z)\\cdot\\frac{p_{X,Z}(x,z)}{p_{X}(x)p_{Z}(z)}\\partial_{2}\\ell(r(z),\\mathcal{P}[h](z))\\;\\mathrm{d}z=\\mathbb{E}[\\Phi(x,Z)\\partial_{2}\\ell(r(Z),\\mathcal{P}[h](Z))]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we define $\\Phi(x,z)=p_{X,Z}(x,z)/(p_{X}(x)p_{Z}(z))$ if $p_{Z}(z)>0$ and $\\Phi(x,z)=0$ if $p_{Z}(z)=0$ . Notice we may assume that $p_{X}(x)>0$ , since the set of $x\\in\\mathscr{X}$ such that this happens has probability 1, and $\\nabla\\mathcal{R}(h)$ , being an element of $L^{2}(X)$ , is only defined almost surely. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Before presenting the proof of Theorem 4.3, we need to prove two auxiliary lemmas. To lighten the notation, the symbols $\\lVert\\cdot\\rVert$ and $\\langle\\cdot,\\cdot\\rangle$ , when written without a subscript to specify which space they refer to, will act as the norm and inner product, respectively, of $L^{2}(\\bar{X})$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma A.2. In the procedure of Algorithm $^{\\,l}$ we have $u_{m}\\,\\in\\,L^{2}(X)$ for all $1\\,\\leq\\,m\\,\\leq\\,M$ and, furthermore, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z_{1:M}}[||u_{m}||^{2}]\\leq\\rho(\\widehat{\\Phi},\\widehat{r},\\widehat{\\mathcal{P}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\rho(\\widehat{\\Phi},\\widehat{r},\\widehat{\\mathcal{P}})=3\\|\\widehat{\\Phi}\\|_{\\infty}^{2}\\left(C_{0}^{2}+L^{2}\\|\\widehat{r}\\|_{L^{2}(Z)}^{2}+L^{2}D^{2}\\|\\widehat{\\mathcal{P}}\\|_{\\mathrm{op}}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. By Assumption 4.2 we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|u_{m}\\|_{L^{2}(X)}^{2}=\\|\\widehat{\\Phi}(\\cdot,z_{m})\\partial_{2}\\ell\\left(\\widehat{r}(z_{m}),\\widehat{\\mathcal{P}}[\\widehat{h}_{m-1}](z_{m})\\right)\\|_{L^{2}(X)}^{2}}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{X}\\left[\\left|\\widehat{\\Phi}(X,z_{m})\\partial_{2}\\ell\\left(\\widehat{r}(z_{m}),\\widehat{\\mathcal{P}}[\\widehat{h}_{m-1}](z_{m})\\right)\\right|^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\partial_{2}\\ell(\\widehat{r}(z_{m}),\\widehat{\\mathcal{P}}[\\widehat{h}_{m-1}](z_{m}))^{2}\\|\\widehat{\\Phi}\\|_{\\infty}^{2}}\\\\ &{\\leq\\infty.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Hence, $u_{m}\\in L^{2}(X)$ for all $m$ . This computation and Proposition A.1 Item 1 then imply ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{z_{1:M}}\\left[\\Vert u_{m}\\Vert^{2}\\right]\\leq3\\Vert\\widehat{\\Phi}\\Vert_{\\infty}^{2}\\left(C_{0}^{2}+L^{2}\\left(\\Vert\\widehat{r}\\Vert_{L^{2}(Z)}^{2}+\\Vert\\widehat{\\mathcal{P}}[\\widehat{h}_{m-1}]\\Vert_{L^{2}(Z)}^{2}\\right)\\right)}\\\\ &{\\leq3\\Vert\\widehat{\\Phi}\\Vert_{\\infty}^{2}\\left(C_{0}^{2}+L^{2}\\left(\\Vert\\widehat{r}\\Vert_{L^{2}(Z)}^{2}+\\Vert\\widehat{\\mathcal{P}}\\Vert_{\\mathrm{op}}^{2}\\Vert\\widehat{h}_{m-1}\\Vert^{2}\\right)\\right)}\\\\ &{\\leq3\\Vert\\widehat{\\Phi}\\Vert_{\\infty}^{2}\\left(C_{0}^{2}+L^{2}\\left(\\Vert\\widehat{r}\\Vert_{L^{2}(Z)}^{2}+D^{2}\\Vert\\widehat{\\mathcal{P}}\\Vert_{\\mathrm{op}}^{2}\\right)\\right)}\\\\ &{=3\\Vert\\widehat{\\Phi}\\Vert_{\\infty}^{2}\\left(C_{0}^{2}+L^{2}\\Vert\\widehat{r}\\Vert_{L^{2}(Z)}^{2}+L^{2}D^{2}\\Vert\\widehat{\\mathcal{P}}\\Vert_{\\mathrm{op}}^{2}\\right)\\triangleq\\rho(\\widehat{\\Phi},\\widehat{r},\\widehat{\\mathcal{P}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma A.3. In the procedure of Algorithm $^{\\,I}$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbb{E}_{z_{m}}[\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m}]\\|\\leq\\kappa(\\widehat{\\Phi})\\left(\\|\\Phi-\\widehat{\\Phi}\\|_{L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})}^{2}+\\|r-\\widehat{r}\\|_{L^{2}(Z)}^{2}+\\|\\mathcal{P}-\\widehat{\\mathcal{P}}\\|_{\\mathrm{op}}^{2}\\right)^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa^{2}(\\widehat{\\Phi})\\triangleq2\\operatorname*{max}\\left\\{3(C_{0}^{2}+L^{2}\\mathbb{E}[Y^{2}]+L^{2}D^{2}),2L^{2}\\|\\widehat{\\Phi}\\|_{\\infty}^{2},2L^{2}D^{2}\\|\\widehat{\\Phi}\\|_{\\infty}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. To ease the notation, we define ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Psi_{m}(Z)\\triangleq\\partial_{2}\\ell(r(Z),\\mathcal{P}[\\widehat{h}_{m-1}](Z)),}\\\\ {\\widehat{\\Psi}_{m}(Z)\\triangleq\\partial_{2}\\ell(\\widehat{r}(Z),\\widehat{\\mathcal{P}}[\\widehat{h}_{m-1}](Z)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Let\u2019s expand the definition of $\\lVert\\cdot\\rVert$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left|\\mathbb{E}_{\\boldsymbol{z}_{m}}[\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-\\boldsymbol{u}_{m}]\\right|\\right|=\\mathbb{E}_{\\boldsymbol{X}}\\left[\\mathbb{E}_{\\boldsymbol{z}_{m}}\\left[\\nabla\\mathcal{R}(\\widehat{h}_{m-1})(\\boldsymbol{X})-\\boldsymbol{u}_{m}(\\boldsymbol{X})\\right]^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{\\boldsymbol{X}}\\left[\\left(\\nabla\\mathcal{R}(\\widehat{h}_{m-1})(\\boldsymbol{X})-\\mathbb{E}_{\\boldsymbol{z}_{m}}\\left[\\boldsymbol{u}_{m}(\\boldsymbol{X})\\right]\\right)^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{\\boldsymbol{X}}\\left[\\left(\\mathbb{E}_{\\boldsymbol{Z}}\\left[\\Phi(\\boldsymbol{X},\\boldsymbol{Z})\\Psi_{m}(\\boldsymbol{Z})\\right]-\\mathbb{E}_{\\boldsymbol{z}_{m}}[\\widehat{\\Phi}(\\boldsymbol{X},\\boldsymbol{z}_{m})\\widehat{\\Psi}_{m}(\\boldsymbol{z}_{m})]\\right)^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}_{\\boldsymbol{X}}\\left[\\left(\\mathbb{E}_{\\boldsymbol{Z}}[\\Phi(\\boldsymbol{X},\\boldsymbol{Z})\\Psi_{m}(\\boldsymbol{Z})-\\widehat{\\Phi}(\\boldsymbol{X},\\boldsymbol{Z})\\widehat{\\Psi}_{m}(\\boldsymbol{Z})]\\right)^{2}\\right]^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now we add and subtract $\\widehat{\\Phi}(X,Z)\\Psi_{m}(Z)$ , so that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X}\\left[\\bigg(\\mathbb{E}_{Z}\\left[\\Phi(X,Z)\\Psi_{m}(Z)-\\widehat{\\Phi}(X,Z)\\widehat{\\Psi}_{m}(Z)\\right]\\bigg)^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\qquad=\\mathbb{E}_{X}\\left[\\bigg(\\mathbb{E}_{Z}\\left[\\Psi_{m}(Z)(\\Phi(X,Z)-\\widehat{\\Phi}(X,Z))+\\widehat{\\Phi}(X,Z)(\\Psi_{m}(Z)-\\widehat{\\Psi}_{m}(Z))\\right]\\bigg)^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\qquad\\le\\mathbb{E}_{X}\\left[\\bigg(\\|\\Psi_{m}\\|_{L^{2}(Z)}\\|\\Phi(X,\\cdot)-\\widehat{\\Phi}(X,\\cdot)\\|_{L^{2}(Z)}+\\|\\widehat{\\Phi}(X,\\cdot)\\|_{L^{2}(Z)}\\|\\Psi_{m}-\\widehat{\\Psi}_{m}\\|_{L^{2}(Z)}\\bigg)^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\qquad\\le\\sqrt{2}\\mathbb{E}_{X}\\left[\\|\\Psi_{m}\\|_{L^{2}(Z)}^{2}\\|\\Phi(X,\\cdot)-\\widehat{\\Phi}(X,\\cdot)\\|_{L^{2}(Z)}^{2}+\\|\\widehat{\\Phi}(X,\\cdot)\\|_{L^{2}(Z)}^{2}\\|\\Psi_{m}-\\widehat{\\Psi}_{m}\\|_{L^{2}(Z)}^{2}\\right]^{\\frac{1}{2}}}\\\\ &{\\qquad=\\sqrt{2}\\left(\\|\\Psi_{m}\\|_{L^{2}(Z)}^{2}\\|\\Phi-\\widehat{\\Phi}\\|_{L^{2}(\\mathbb{P}\\times\\otimes\\mathbb{P}_{Z})}^{2}+\\|\\widehat{\\Phi}\\|_{L^{2}(\\mathbb{P}\\times\\otimes\\mathbb{P}_{Z})}^{2}\\|\\Psi_{m}-\\widehat{\\Psi}_{m}\\|_{L^{2}(Z)}^{2}\\right)^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\Phi\\|_{L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})}^{2}=\\int_{X\\times\\mathcal{Z}}\\Phi(x,z)^{2}p(x)p(z)\\;\\mathrm{d}x\\mathrm{d}z\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is the norm with respect to the independent coupling of the distributions of $X$ and $Z$ . By Proposition A.1.1 we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Psi_{m}\\|_{L^{2}(Z)}^{2}=\\mathbb{E}_{Z}\\left[\\partial_{2}\\ell(r(Z),\\mathcal{P}[\\widehat{h}_{m-1}](Z))^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}_{Z}\\left[\\left(C_{0}+L\\left(|r(Z)|+\\left|\\mathcal{P}[\\widehat{h}_{m-1}](Z)\\right|\\right)\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq3\\left(C_{0}^{2}+L^{2}\\|r\\|_{L^{2}(Z)}^{2}+L^{2}\\|\\mathcal{P}[\\widehat{h}_{m-1}]\\|_{L^{2}(Z)}^{2}\\right)}\\\\ &{\\qquad\\qquad\\leq3\\left(C_{0}^{2}+L^{2}\\mathbb{E}[Y^{2}]+L^{2}D^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It is also clear that, by Assumption 4.2, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\widehat\\Phi\\|_{L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})}^{2}\\le\\|\\widehat\\Phi\\|_{\\infty}^{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Finally, by Assumption 3.2.2 we also have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Psi_{m}-\\widehat\\Psi_{m}\\|_{L^{2}(Z)}^{2}=\\mathbb{E}_{Z}\\left[\\left(\\partial_{2}\\ell(r(Z),\\mathcal{P}[\\widehat{h}_{m-1}](Z))-\\partial_{2}\\ell(\\widehat{r}(Z),\\widehat{\\mathcal{P}}[\\widehat{h}_{m-1}](Z))\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq2L^{2}\\left(\\|r-\\widehat{r}\\|_{L^{2}(Z)}^{2}+\\|(\\mathcal{P}-\\widehat{\\mathcal{P}})[\\widehat{h}_{m-1}]\\|_{L^{2}(Z)}^{2}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\leq2L^{2}\\left(\\|r-\\widehat{r}\\|_{L^{2}(Z)}^{2}+D^{2}\\|\\mathcal{P}-\\widehat{\\mathcal{P}}\\|_{\\mathrm{op}}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "To combine all terms, we first define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa^{2}(\\widehat{\\Phi})\\triangleq2\\operatorname*{max}\\left\\{3(C_{0}^{2}+L^{2}\\mathbb{E}[Y^{2}]+L^{2}D^{2}),2L^{2}\\|\\widehat{\\Phi}\\|_{\\infty}^{2},2L^{2}D^{2}\\|\\widehat{\\Phi}\\|_{\\infty}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, it\u2019s easy to see that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathbb{E}_{z_{m}}[\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m}]\\|\\leq\\kappa(\\widehat{\\Phi})\\left(\\|\\Phi-\\widehat{\\Phi}\\|_{L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})}^{2}+\\|r-\\widehat{r}\\|_{L^{2}(Z)}^{2}+\\|\\mathcal{P}-\\widehat{\\mathcal{P}}\\|_{\\mathrm{op}}^{2}\\right)^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "as we wanted to show. ", "page_idx": 14}, {"type": "text", "text": "We now have everything needed to show the ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 4.3. We start by checking that $\\mathcal{R}$ is convex in $\\mathcal{H}$ : if $h,g\\in\\mathcal{H}$ and $\\lambda\\in[0,1]$ , then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{R}(\\lambda h+(1-\\lambda)g)=\\mathbb{E}[\\ell(r(Z),\\mathcal{P}[\\lambda h+(1-\\lambda)g](Z))]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}[\\ell(r(Z),\\lambda\\mathcal{P}[h](Z)+(1-\\lambda)\\mathcal{P}[g](Z))]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\lambda\\mathbb{E}[\\ell(r(Z),\\mathcal{P}[h](Z))]+(1-\\lambda)\\mathbb{E}[\\ell(r(Z),\\mathcal{P}[g](Z))]}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\lambda\\mathcal{R}(h)+(1-\\lambda)\\mathcal{R}(g).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By Assumption 3.1, $h^{\\star}\\in\\mathcal{H}^{9}$ . By the Algorithm 1 procedure, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac12\\|\\widehat{h}_{m}-h^{\\star}\\|^{2}=\\displaystyle\\frac12\\Big\\|\\mathrm{proj}_{\\mathcal{H}}\\left[\\widehat{h}_{m-1}-\\alpha_{m}u_{m}\\right]-h^{\\star}\\Big\\|^{2}}\\\\ {\\displaystyle\\le\\frac12\\|\\widehat{h}_{m-1}-\\alpha_{m}u_{m}-h^{\\star}\\|^{2}}\\\\ {\\displaystyle=\\frac12\\|\\widehat{h}_{m-1}-h^{\\star}\\|^{2}-\\alpha_{m}\\langle u_{m},\\widehat{h}_{m-1}-h^{\\star}\\rangle+\\frac{\\alpha_{m}^{2}}{2}\\|u_{m}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "After adding and subtracting $\\alpha_{m}\\langle\\nabla\\mathcal{R}(\\widehat{h}_{m-1}),\\widehat{h}_{m-1}-h^{\\star}\\rangle$ , we are left with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\!\\!\\frac{!}{2}\\|\\widehat{h}_{m-1}-h^{\\star}\\|^{2}\\!-\\!\\alpha_{m}\\langle u_{m}-\\nabla\\mathcal{R}(\\widehat{h}_{m-1}),\\widehat{h}_{m-1}-h^{\\star}\\rangle+\\frac{\\alpha_{m}^{2}}{2}\\|u_{m}\\|^{2}\\!-\\!\\alpha_{m}\\langle\\nabla\\mathcal{R}(\\widehat{h}_{m-1}),\\widehat{h}_{m-1}-h^{\\star}\\rangle.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Applying the first order convexity inequality on the last term give us, in total, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac12\\|\\widehat{h}_{m}-h^{\\star}\\|^{2}\\leq\\displaystyle\\frac12\\|\\widehat{h}_{m-1}-h^{\\star}\\|^{2}-\\alpha_{m}\\langle u_{m}-\\nabla\\mathcal{R}(\\widehat{h}_{m-1}),\\widehat{h}_{m-1}-h^{\\star}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{\\alpha_{m}^{2}}{2}\\|u_{m}\\|^{2}-\\alpha_{m}(\\mathcal{R}(\\widehat{h}_{m-1})-\\mathcal{R}(h^{\\star})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, making this substitution and rearranging terms, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{R}(\\widehat{h}_{m-1})-\\mathcal{R}(h^{\\star})\\leq\\displaystyle\\frac{1}{2\\alpha_{m}}\\left(\\|\\widehat{h}_{m-1}-h^{\\star}\\|^{2}-\\|\\widehat{h}_{m}-h^{\\star}\\|^{2}\\right)}&{}\\\\ {+\\displaystyle\\frac{\\alpha_{m}}{2}\\|u_{m}\\|^{2}-\\langle u_{m}-\\nabla\\mathcal{R}(\\widehat{h}_{m-1}),\\widehat{h}_{m-1}-h^{\\star}\\rangle.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, summing over $1\\leq m\\leq M$ leads to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{n=1}^{M}\\Big[\\mathcal{R}(\\widehat{h}_{m-1})-\\mathcal{R}(h^{\\star})\\Big]\\leq\\displaystyle\\sum_{m=1}^{M}\\frac{1}{2\\alpha_{m}}\\left(\\|\\widehat{h}_{m-1}-h^{\\star}\\|^{2}-\\|\\widehat{h}_{m}-h^{\\star}\\|^{2}\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\sum_{m=1}^{M}\\frac{\\alpha_{m}}{2}\\|u_{m}\\|^{2}}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad+\\sum_{m=1}^{M}\\langle\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m},\\widehat{h}_{m-1}-h^{\\star}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The next step is to take the average of both sides with respect to $\\boldsymbol{z}_{1:M}$ , taking advantage of the independence between $z_{1:M}$ and $\\mathcal{D}$ , the data used to compute $\\widehat\\Phi,\\widehat r$ and $\\widehat{\\mathcal{P}}$ . Each summation in the RHS is then bounded separately. ", "page_idx": 15}, {"type": "text", "text": "The first summation admits a deterministic bound. By assumption, the diameter $D$ of $\\mathcal{H}$ is finite. Hence ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{m=1}^{M}\\frac{1}{2\\alpha_{m}}\\left(\\|\\widehat{h}_{m-1}-h^{\\star}\\|^{2}-\\|\\widehat{h}_{m}-h^{\\star}\\|^{2}\\right)=\\sum_{m=2}^{M}\\left(\\frac{1}{2\\alpha_{m}}-\\frac{1}{2\\alpha_{m-1}}\\right)\\|\\widehat{h}_{m-1}-h^{\\star}\\|^{2}}}\\\\ &{}&{\\ \\ +\\ \\frac{1}{2\\alpha_{1}}\\|\\widehat{h}_{0}-h^{\\star}\\|^{2}-\\frac{1}{2\\alpha_{M}}\\|\\widehat{h}_{M}-h^{\\star}\\|^{2}}\\\\ &{}&{\\ \\ \\leq\\displaystyle\\sum_{m=2}^{M}\\left(\\frac{1}{2\\alpha_{m}}-\\frac{1}{2\\alpha_{m-1}}\\right)D^{2}+\\frac{1}{2\\alpha_{1}}D^{2}=\\frac{D^{2}}{2\\alpha_{M}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The second summation can be bounded with the aid of Lemma A.2: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}_{z_{1:M}}\\left[\\sum_{m=1}^{M}\\frac{\\alpha_{m}}{2}\\|u_{m}\\|^{2}\\right]=\\frac{1}{2}\\mathbb{E}_{z_{1:M}}\\left[\\|u_{m}\\|^{2}\\right]\\sum_{m=1}^{M}\\alpha_{m}\\leq\\frac{1}{2}\\rho(\\widehat{\\Phi},\\widehat{r},\\widehat{\\mathcal{P}})\\sum_{m=1}^{M}\\alpha_{m}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, the third summation can be bounded using Lemma A.3. Let $\\mathbb{E}_{z_{-m}}$ denote the expectation with respect to $z_{1},\\dots,z_{m-1},z_{m+1},\\dots,z_{M}$ and notice that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{z_{1:M}}\\left[\\langle\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m},\\widehat{h}_{m-1}-h^{\\star}\\rangle\\right]=\\mathbb{E}_{z_{-m}}\\left[\\mathbb{E}_{z_{m}}\\left[\\langle\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m},\\widehat{h}_{m-1}-h^{\\star}\\rangle\\right]\\right]}&{}\\\\ {=\\mathbb{E}_{z_{-m}}\\left[\\langle\\mathbb{E}_{z_{m}}\\left[\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m}\\right],\\widehat{h}_{m-1}-h^{\\star}\\rangle\\right]}&{}\\\\ {\\leq\\mathbb{E}_{z_{-m}}\\left[\\|\\mathbb{E}_{z_{m}}\\left[\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m}\\right]\\|\\|\\widehat{h}_{m-1}-h^{\\star}\\|\\right]}&{}\\\\ {\\leq D\\mathbb{E}_{z_{-m}}\\left[\\|\\mathbb{E}_{z_{m}}\\left[\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m}\\right]\\|\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, applying Lemma A.3 and setting $\\tau\\triangleq D\\kappa$ we get ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{z_{1:M}}\\left[\\langle\\nabla\\mathcal{R}(\\widehat{h}_{m-1})-u_{m},\\widehat{h}_{m-1}-h^{\\star}\\rangle\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\tau(\\widehat{\\Phi})\\left(\\|\\Phi-\\widehat{\\Phi}\\|_{L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})}^{2}+\\|r-\\widehat{r}\\|_{L^{2}(Z)}^{2}+\\|\\mathcal{P}-\\widehat{\\mathcal{P}}\\|_{\\mathrm{op}}^{2}\\right)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "All that is left to do is to apply equations (14), (15), (16) and (17) along with the inequality which defines convexity. Let $\\begin{array}{r}{\\widehat{h}\\overset{\\vartriangle}{=}\\frac{1}{M}\\sum_{m=1}^{\\bar{M}}\\widehat{h}_{m-1}}\\end{array}$ and $\\xi\\triangleq\\rho/2$ . Then: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}_{z_{1:M}}\\left[\\mathcal{R}(\\widehat{h})-\\mathcal{R}(h^{\\star})\\right]}}\\\\ &{\\leq\\displaystyle\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{E}_{z_{1:M}}\\left[\\mathcal{R}(\\widehat{h}_{m})-\\mathcal{R}(h^{\\star})\\right]}\\\\ &{\\leq\\displaystyle\\frac{D^{2}}{2M\\alpha_{M}}+\\xi(\\widehat{\\Phi},\\widehat{r},\\widehat{\\mathcal{P}})\\frac{1}{M}\\sum_{m=1}^{M}\\alpha_{m}}\\\\ &{\\qquad+\\,\\tau(\\widehat{\\Phi})\\left(\\|\\Phi-\\widehat{\\Phi}\\|_{L^{2}(\\mathbb{P}^{\\angle})}^{2}+\\|r-\\widehat{r}\\|_{L^{2}(\\mathcal{Z})}^{2}+\\|\\mathcal{P}-\\widehat{\\mathcal{P}}\\|_{\\mathrm{op}}^{2}\\right)^{\\frac{1}{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Comparison with other methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we provide some theoretical comparisons with other notable methods for NPIV estimation. ", "page_idx": 16}, {"type": "text", "text": "B.1 KIV ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1.1 Risk bound ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our formulation, we substitute the task for estimating $\\mathcal{P}^{*}$ by that of estimating the ratio of densities $\\Phi$ . In this way, we bypass the need to evaluate an operator estimatorP \u2217in every iteration, replacing this by evaluating $\\widehat\\Phi$ once and multiplying it by the derivative of the loss function (cf. Algorithm 1). Aside from the co mputational advantage this poses, it is interesting to investigate the differences which arise in theoretical bounds when comparing with a method which ends up using an estimator of $\\mathcal{P}^{*}$ . ", "page_idx": 16}, {"type": "text", "text": "In KIV, the authors propose a first stage procedure which estimates $E$ $\\mathcal{P}$ , in our notation) and, in the second stage, they use the adjoint of this operator through the kernel mean embedding $\\mu(z)=E^{*}\\phi(z)$ $\\mathit{\\Delta}(\\phi$ is the feature map, see the notation in [34]). To estimate $E^{*}$ , they use the adjoint of their estimator $E_{\\lambda}^{n}$ of $E$ , where $n$ is the sample size and $\\lambda$ is a regularization parameter. ", "page_idx": 16}, {"type": "text", "text": "In our Theorem 4.3, provided the learning rate has been appropriately chosen, the excess risk is upper bounded by $\\|\\Phi-\\widehat{\\Phi}\\|+\\|r-\\widehat{r}\\|+\\|\\mathcal{P}-\\widehat{\\mathcal{P}}\\|$ . In fact, $\\lVert\\boldsymbol{r}-\\widehat{\\boldsymbol{r}}\\rVert$ is just the risk for a simple regression problem, and so we will focus our analysis on the other two terms. In KIV, the excess risk is bounded using [37, Theorem 2], according to which it is smaller than the sum of five terms. We reproduce this bound here for a more detailed analysis: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Theorem B.1 (Proposition 32 of [34]). The excess risk of KIV\u2019s stage 2 estimator $\\widehat{H}_{\\xi}^{m}$ can be bounded by five terms: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\widehat{H}_{\\xi}^{m})-\\mathcal{E}(H_{\\rho})\\leq5[S_{-1}+S_{0}+\\mathcal{A}(\\xi)+S_{1}+S_{2}],\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{-1}=\\|\\sqrt{T}\\circ(\\widehat{\\mathbf{T}}+\\xi)^{-1}(\\widehat{\\mathbf{g}}-\\mathbf{g})\\|_{\\mathcal{H}_{\\Omega}}^{2},}\\\\ &{\\quad S_{0}=\\|\\sqrt{T}\\circ(\\widehat{\\mathbf{T}}+\\xi)^{-1}\\circ(\\mathbf{T}-\\widehat{\\mathbf{T}})H_{\\xi}^{m}\\|_{\\mathcal{H}_{\\Omega}}^{2},}\\\\ &{\\quad S_{1}=\\|\\sqrt{T}\\circ(\\mathbf{T}+\\xi)^{-1}(\\mathbf{g}-\\mathbf{T}H_{\\rho})\\|_{\\mathcal{H}_{\\Omega}}^{2},}\\\\ &{\\quad S_{2}=\\|\\sqrt{T}\\circ(\\mathbf{T}+\\xi)^{-1}\\circ(T-\\mathbf{T})(H_{\\xi}-H_{\\rho})\\|_{\\mathcal{H}_{\\Omega}}^{2},}\\\\ &{\\quad A(\\xi)=\\|\\sqrt{T}(H_{\\xi}-H_{\\rho})\\|_{\\mathcal{H}_{\\Omega}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In our notation, $\\mathcal{E}$ corresponds to $\\mathcal{R}$ , $\\widehat{H}_{\\xi}^{m}$ corresponds to $\\widehat{h}$ and $H_{\\rho}$ corresponds to $h^{\\star}$ . The space $\\mathcal{H}_{\\Omega}$ is a RKHS which contains the solution ${\\bf\\nabla}H_{\\rho}$ . It is also important to recall the definition of the terms in bold, since they depend on $\\mu_{\\lambda}^{n}(z)=(E_{\\lambda}^{n})^{*}\\phi(z)$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{T}=\\frac{1}{m}\\sum_{i=1}^{m}T_{\\mu(\\tilde{z}_{i})},\\quad\\mathbf{g}=\\frac{1}{m}\\sum_{i=1}^{m}\\Omega_{\\mu(\\tilde{z}_{i})}\\tilde{y}_{i}}\\\\ {\\displaystyle\\widehat{\\mathbf{T}}=\\frac{1}{m}\\sum_{i=1}^{m}T_{\\mu_{\\lambda}^{n}(\\tilde{z}_{i})},\\quad\\widehat{\\mathbf{g}}=\\frac{1}{m}\\sum_{i=1}^{m}\\Omega_{\\mu_{\\lambda}^{n}(\\tilde{z}_{i})}\\tilde{y}_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Where $\\tilde{z}_{i}$ and $\\tilde{y}_{i}$ are second-stage samples.We refer the reader to [34] for a detailed description of the other mathematical objects. Here, we will argue that ", "page_idx": 17}, {"type": "equation", "text": "$$\nS_{-1}+S_{0}\\quad\\mathrm{is\\analogous\\to}\\quad\\lVert\\Phi-\\widehat{\\Phi}\\rVert+\\lVert\\mathcal{P}-\\widehat{\\mathcal{P}}\\rVert.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The other terms in Equation (18) are controlled by optimally choosing how the regularizing parameter $\\xi$ goes to zero. This relates to the first two terms of our bound in Equation (10) that goes to zero as the SGD iteration increases, as long as we properly choose the learning rate. In [34, Proposition 35], these two terms are bounded as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{-1}\\leq\\|\\sqrt{T}\\circ(\\widehat{\\mathbf{T}}+\\xi)^{-1}\\|_{\\mathcal{L}(\\mathcal{H}_{\\Omega})}^{2}\\|\\widehat{\\mathbf{g}}-\\mathbf{g}\\|_{\\mathcal{H}_{\\Omega}}^{2},}\\\\ &{\\quad S_{0}\\leq\\|\\sqrt{T}\\circ(\\widehat{\\mathbf{T}}+\\xi)^{-1}\\|_{\\mathcal{L}(\\mathcal{H}_{\\Omega})}^{2}\\|\\widehat{\\mathbf{T}}-\\mathbf{T}\\|_{\\mathcal{L}(\\mathcal{H}_{\\Omega})}^{2}\\|H_{\\xi}^{m}\\|_{\\mathcal{H}_{\\Omega}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, [34, Proposition 37] refers to [37, Supplements 7.1.1 and 7.1.2] to bound $\\lVert\\widehat{\\mathbf{g}}-\\mathbf{g}\\rVert^{2}$ and $\\lVert\\widehat{\\mathbf{T}}-\\mathbf{T}\\rVert^{2}$ . In [37], one can see that these bounds amount to10 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{\\mathbf{g}}-\\mathbf{g}\\|^{2}\\leq C\\cdot\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|\\mu_{\\lambda}^{n}(\\tilde{z}_{i})-\\mu(\\tilde{z}_{i})\\|_{\\mathcal{H}_{x}}^{2}|\\tilde{y}_{i}|^{2}}\\\\ &{\\|\\widehat{\\mathbf{T}}-\\mathbf{T}\\|^{2}\\leq D\\cdot\\displaystyle\\frac{1}{m}\\sum_{i=1}^{m}\\|\\mu_{\\lambda}^{n}(\\tilde{z}_{i})-\\mu(\\tilde{z}_{i})\\|_{\\mathcal{H}_{x}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C$ and $D$ are constants. Then, KIV uses the bound on $\\|\\mu_{\\lambda}^{n}(\\tilde{z}_{i})-\\mu(\\tilde{z}_{i})\\|_{\\mathcal{H}_{\\mathcal{X}}}^{2}$ provided in their Corollary 1, which is based on the inequality11 ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\mu_{\\lambda}^{n}(\\tilde{z}_{i})-\\mu(\\tilde{z}_{i})\\|_{\\mathcal{H}_{\\mathcal{X}}}^{2}\\leq\\|E_{\\lambda}^{n}-E_{\\rho}\\|_{\\mathcal{H}_{\\Gamma}}^{2}\\|\\phi(\\tilde{z}_{i})\\|_{\\mathcal{H}_{\\mathcal{Z}}}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, with high probability and ignoring constants, we have12 ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\widehat{\\mathbf{g}}-\\mathbf{g}\\|\\lesssim\\|E_{\\lambda}^{n}-E_{\\rho}\\|}\\\\ {\\|\\widehat{\\mathbf{T}}-\\mathbf{T}\\|\\lesssim\\|E_{\\lambda}^{n}-E_{\\rho}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This would be an intermediate result in [34, Proposition 37]. ", "page_idx": 18}, {"type": "text", "text": "Therefore, the estimation error of $E_{\\lambda}^{n}$ when compared to $E$ impacts the risk of KIV\u2019s estimator through two terms. These are analogous to our two terms $\\lVert\\Phi-{\\widehat{\\Phi}}\\rVert$ and $\\|\\mathcal P-\\widehat{\\mathcal P}\\|$ , since $\\|\\Phi-\\widehat\\Phi\\|_{L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})}$ is the Hilbert-Schmidt norm of the difference between $\\mathcal{P}^{*}$ and the integral operator induced by $\\widehat{\\Phi}$ [8, Theorem 2.42]. ", "page_idx": 18}, {"type": "text", "text": "B.1.2 Binary outcomes ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Section 6, we show that, under the binary outcomes setting, the correct formulation of the risk is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{R}(h)=\\mathbb{E}[\\mathrm{BCE}(r(Z),F(\\mathcal{P}[h](Z)))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "In KIV\u2019s notation, this would become13 ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{E}(H)=\\mathbb{E}[\\mathrm{BCE}(Y,F(H\\mu(Z)))]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Substituting the populational mean by the empirical one, taking into account that we must estimate $\\mu$ and applying regularization, we get (cf. [34, Section 4.2]) ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{E}}_{\\xi}^{m}(H)=\\frac{1}{m}\\sum_{i=1}^{m}\\mathrm{BCE}(\\tilde{y}_{i},F(H\\mu_{\\lambda}^{n}(\\tilde{z}_{i})))+\\xi\\|H\\|_{\\mathcal{H}_{\\Omega}}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the fact that $\\mathcal{H}_{\\Omega}$ and $\\mathcal{H}_{\\mathcal{X}}$ are isometrically isomorphic [34, Section 4.2], we can formulate the risk in an equivalent way, now defined on $\\mathcal{H}_{\\mathcal{X}}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{E}}_{\\xi}^{m}(h)=\\frac{1}{m}\\sum_{i=1}^{m}\\mathrm{BCE}(\\tilde{y}_{i},F((E_{\\lambda}^{n}h)(\\tilde{z}_{i}))+\\xi\\|h\\|_{\\mathcal{H}_{x}}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, the KIV estimator is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{h}_{\\xi}^{m}=\\underset{h\\in\\mathcal{H}_{x}}{\\arg\\operatorname*{min}}\\ \\widehat{\\mathcal{E}}_{\\xi}^{m}(h).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the quadratic loss, KIV finds the solution in closed form, since this is a ridge regression problem. However, for this new loss, no obvious closed form solution exists. Applying the Representer Theorem, we find that the solution $h_{\\xi}^{m}$ should satisfy ", "page_idx": 18}, {"type": "equation", "text": "$$\nh_{\\xi}^{m}=\\sum_{i=1}^{n}\\alpha_{i}\\psi(x_{i}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some $\\alpha\\in\\mathbf{R}^{n}$ , where $\\psi(x_{i})=K(x_{i},\\cdot)$ is the reproducing kernel of $\\mathcal{H}_{\\mathcal{X}}$ . Then, one is left with the following optimization problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\underset{\\alpha\\in\\mathbf{R}^{n}}{\\arg\\operatorname*{min}}\\ \\frac{1}{m}\\sum_{i=1}^{m}\\mathrm{BCE}(\\tilde{y}_{i},F(\\alpha^{\\prime}w(\\tilde{z}_{i}))+\\xi\\alpha^{\\prime}K_{X X}\\alpha\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $\\big(E_{\\lambda}^{n}h_{\\xi}^{m}\\big)(z)\\,=\\,\\alpha^{\\prime}w(z)$ (see the definition of $w$ in [34, Appendix A.5.1]). An approximate solution to this $n$ -dimensional optimization problem must be found with numerical methods. Keep in mind that this problem must be solved several times in order to find a good value of the regularization parameter $\\xi$ via cross-validation, which is crucial for the performance of regularized regression methods. In contrast, our method does not require additional work after defining the modified risk. ", "page_idx": 18}, {"type": "text", "text": "B.2 Dual IV ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In DualIV, the risk minimization problem is reformulated into a saddle point optimization problem, which we write in their notation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in\\mathcal{F}}\\operatorname*{max}_{u\\in\\mathcal{U}}\\Psi(f,u),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Psi(f,u)=\\mathbb{E}_{X Y Z}[f(X)u(Y,Z)]-\\mathbb{E}_{Y Z}[\\ell_{Y}^{\\star}(u(Y,Z))].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, $\\mathcal{F}$ is the space which contains the solution to the NPIV estimation problem, $\\boldsymbol{\\mathcal{U}}$ is the space of the \u201cdual function\u201d and $\\ell_{y}^{\\star}$ is the Fenchel dual of the function $\\ell_{y}~=~\\ell(y,\\cdot)$ . In the binary outcomes setting, the loss is given by $\\ell(y,y^{\\prime})=\\mathrm{BCE}(y,F(y^{\\prime}))$ . If $F$ is the logistic function, i.e. $F(y^{\\prime})=\\exp(y^{\\prime})/(1+\\exp(y^{\\prime}))$ , one may verify that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\ell_{y}^{\\star}(u)=\\left\\{\\!\\!\\begin{array}{l l}{-H(u+y),\\;\\mathrm{if}\\;0\\leq u+y\\leq1,}\\\\ {+\\infty,\\;\\mathrm{otherwise},}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $H(p)=-[p\\log p+(1-p)\\log(1-p)]$ is the entropy. The empirical minimax objective then becomes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Psi(f,u)={\\frac{1}{n}}\\sum_{i=1}^{n}f(x_{i})u(y_{i},z_{i})+{\\frac{1}{n}}\\sum_{i=1}^{n}H(y_{i}+u(y_{i},z_{i})),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with the constrain that $0\\leq y_{i}+u(y_{i},z_{i})\\leq1$ for every $i$ . Again, in contrast with the case of quadratic loss, when one may choose $\\mathcal{F}$ and $\\boldsymbol{\\mathcal{U}}$ to be RKHS and obtain a closed form ridge-regularized solution, this formulation is not as tractable. ", "page_idx": 19}, {"type": "text", "text": "B.3 GMM ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In GMM based methods for continuous outcomes, the main idea is that since $\\mathbb{E}[\\varepsilon\\mid Z]=0$ and $Y=h^{\\star}(X)+\\varepsilon$ , we have $\\mathbb{E}[Y-h^{\\star}(X)\\mid Z]=0$ and, hence, $\\mathbb{E}[f(Z)(Y-h^{\\star}(X))]={\\dot{0}}$ for every $f\\in L^{2}(\\dot{Z})$ . If we choose a large number of functions $f_{1},\\ldots,f_{m}$ , we may search for $h^{\\star}$ by finding a function which minimizes ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{m}\\psi_{n}(h,f_{j})^{2}\\quad\\mathrm{where}\\quad\\psi_{n}(h,f_{j})=\\frac{1}{n}\\sum_{i=1}^{n}f_{j}(Z_{i})(Y_{i}-h(X_{i})).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This is possible because the error term $\\varepsilon$ can be written as $Y\\,-\\,h^{\\star}(X)$ . Under the structural equation $Y\\;=\\;{\\bf1}\\{h^{\\star}(X)\\,+\\,\\varepsilon\\,>\\,0\\}$ , such a characterization of the error term is not available, which already presents a difficulty. Nonetheless, as we show in Section 6, under this new equation we have $\\mathbb{E}[\\dot{Y}\\,\\Bar{\\;}\\,Z]\\,=\\,F(\\mathcal{P}[h^{\\star}](\\dot{Z}))$ . Therefore, moment conditions could be obtained through $\\mathbb{E}[(Y-F(\\mathcal{P}[h^{\\star}](Z)))f(Z)]=0$ for every $f\\,\\in\\,L^{2}(Z)$ . However, this would create the need to introduce an estimator $\\widehat{\\mathcal{P}}$ of $\\mathcal{P}$ . This defeats one of the main advantages of turning to GMM, which is to avoid the first stage  of two-stage methods, since errors in the first stage can have a large impact on the resulting estimator. ", "page_idx": 19}, {"type": "text", "text": "C Implementation and experiment details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we give practical guidelines for implementing SAGD-IV, as well further details regarding the baseline methods below: ", "page_idx": 19}, {"type": "text", "text": "\u2022 KIV: Since the original implementation was only publicly available in Matlab, to facilitate reproducibility of our results we re-implemented this method in Python. This was done following the guidelines presented in the paper.   \n\u2022 DualIV: Similarly to KIV, we rewrote the method in Python using the closed form solutions and the hyperparameter selection procedure present in the paper.   \n\u2022 DeepGMM: We used the publicly available Python implementation14.   \n\u2022 DeepIV: We used the implementation available as part of the EconML package [5]. ", "page_idx": 19}, {"type": "text", "text": "\u2022 2SLS: We include the classic Two Stage Least Squares procedure to show the benefits of following a nonparametric approach. ", "page_idx": 20}, {"type": "text", "text": "All experiments were conducted on a Apple Sillicon M1 Pro CPU. No GPUs were used. Remark C.1 (Experiments sample size). All four baseline methods we are considering need a single dataset $(X,Z,Y)$ to fti their estimator (hyperpameter validation included), whereas our method needs one dataset of $(X,Z,Y)$ triplets to compute $\\widehat\\Phi,\\widehat r$ and $\\widehat{r}$ , and one dataset of $Z$ samples to conduct the SAGD loop. To make fair comparisons, we  tr a ined  e ach method using the same total amount of random variable samples. For example, DeepGMM might use 800 triplets $(X,Z,Y)$ for training and 200 triplets for validation, whereas SAGD-IV may use 600 triplets for fitting the preliminary estimators and then 1200 samples of $Z$ for the SAGD loop, both totaling 3000 random variable samples used to adjust the model. ", "page_idx": 20}, {"type": "text", "text": "C.1 SAGD-IV ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1.1 On computing $\\widehat{\\mathcal{P}},\\widehat{\\Phi}$ and $\\widehat{r}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Ratio of Densities. Density ratio estimation is a well studied problem in Machine Learning, with [36] being a good resource for acquainting oneself with relevant algorithms. The approaches we follow here consist on obtaining an estimator $\\widehat\\Phi$ that minimizes the (empirical version of the) quantity $\\begin{array}{r}{\\frac{1}{2}\\|\\widehat{\\Phi}-\\Phi\\|_{L^{2}(\\mathbb{P}_{X}\\otimes\\mathbb{P}_{Z})}^{2}}\\end{array}$ . We consider two optio ns for $\\widehat\\Phi$ : a Reproducing Kernel Hilbert Space (RKHS) estimator obtained through the Unconstrained Least Squares Importance Fitting (uLSIF) framework described in [36] and also a neural network trained to minimize the same objective over its parameters. By changing the kernel or the activation function used, we can make sure this estimator adheres to Assumption 4.2 Item 4. ", "page_idx": 20}, {"type": "text", "text": "Conditional expectation operator. This is the most complex object we must estimate, since it encompasses all possible regressions of functions of $X$ over $Z$ . Not many options exist for this task, with a popular one [13, 8] being the use of Nadaraya-Watson kernels [26, 39]. We, however, chose to employ the estimation method used as a first stage in KIV [34], which transforms the problem into vector-valued RKHS ridge regression, due to its straightforward implementation. ", "page_idx": 20}, {"type": "text", "text": "Conditional mean of $Y$ given $Z$ . This, in contrast, is the simplest unknown in our formulation, being the regression function of $Y$ over $Z$ . We again consider two options for this task: the first is to employ the method previously discussed in order to estimate the conditional expectation operator of $Y$ given $Z$ , and then apply it to the identity function. The second option is to train a general purpose regressor, such as a neural network. ", "page_idx": 20}, {"type": "text", "text": "C.1.2 Hyperparameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Since the final estimator in Algorithm 1 is an average over the whole path of estimates produced by the SAGD loop, we can improve upon its quality by discarding the first $K$ estimates $\\widehat{h}_{1},\\widehat{\\ldots},\\widehat{h}_{K}$ , where $K$ must be chosen independently from $M$ . In this way, it is as if we started our algorithm from $\\widehat{h}_{K+1}$ , after a warm up period. ", "page_idx": 20}, {"type": "text", "text": "Regarding sample splitting, our theoretical results do not specify an optimal ratio between the $M$ SAGD iterations and the $N$ samples of $(X,Z,Y)$ used to compute $\\hat{\\Phi},\\widehat{r}$ and $\\widehat{\\mathcal{P}}$ . We empirically determined that $M=2N$ is a good guideline to follow, and it is the ratio we employed in Section 5. Of course, in real data scenarios this ratio might be constrained due to limited data availability. ", "page_idx": 20}, {"type": "text", "text": "About hyperparameters: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The Algorithm 1 learning rate was set to be $\\alpha_{m}=1/\\sqrt{M}$ for $1\\leq m\\leq M$ ;   \n\u2022 The warm up time $K$ was set to 100;   \n\u2022 The set $\\mathcal{H}$ was chosen as in Equation (6), with $A$ set to 10. During our experiments, we found that this parameter, aside from needing to be large enough for us to have $h^{\\star}\\in\\mathcal{H}$ , did not have any noticeable influence on the resulting estimator. ", "page_idx": 20}, {"type": "text", "text": "\u2022 For Kernel SAGD-IV: ", "page_idx": 20}, {"type": "text", "text": "\u2013 We used Gaussian kernels, with lenghtscale parameter determined through the median heuristic; ", "page_idx": 21}, {"type": "text", "text": "\u2013 The regularization parameters for the three kernel ridge regressions were chosen through cross validation. Since these parameters are essential for actual learning to take place, we selected the values to be tested through an iterative procedure, described in Algorithm 2. ", "page_idx": 21}, {"type": "text", "text": "\u2022 For Deep SAGD-IV: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2013 For both networks in the roles of $\\widehat{\\Phi}$ and $\\widehat{r}$ , we used two hidden dense layers of sizes 64 and 32. In the continuous response setting the activation function used was a ReLU, while for the binary response one we used the sigmoid activation. For $\\widehat\\Phi$ , we used dropout with rate 0.01. ", "page_idx": 21}, {"type": "text", "text": "\u2013 Both networks were trained with a batch size of 512, using the Adam [21] optimizer with learning rate of 0.01, accross $1.5\\cdot10^{5}/N$ epochs. We adopted $L^{2}$ regularization in the two estimators, with regularization parameters of $3\\cdot10^{-3}$ for $\\widehat{r}$ and $\\bar{5}\\cdot10^{-3}$ for $\\widehat{\\Phi}$ . We also implemented early stopping in the two training procedures. ", "page_idx": 21}, {"type": "text", "text": "\u2022 We remind the reader that both SAGD-IV variants use kernel methods to compute $\\widehat{\\mathcal{P}}$ . ", "page_idx": 21}, {"type": "text", "text": "C.2 DeepGMM ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The modifications we made with respect to the original paper were the following: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Instead of using $50\\%$ of the samples for training and $50\\%$ for validation, we adopted a train / validation split of 80 / 20.   \n\u2022 We changed the batch size from 1024 to 256. This was done because we supplied the algorithm with 1000 training samples instead of the 2000 used in [6], and hence a batch size of 1024 would not produce stochastic gradients. ", "page_idx": 21}, {"type": "text", "text": "C.3 DeepIV ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We followed all the guidelines presented in the original paper for low dimensional settings. ", "page_idx": 21}, {"type": "text", "text": "C.4 KIV ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We employed a $50/50$ split between $(X,Z,Y)$ and $(\\tilde{X},\\tilde{Z},\\tilde{Y})$ observations (see [34], Appendix A.5.2). Regularization parameter candidates were also chosen using the selection method in Algorithm 2. ", "page_idx": 21}, {"type": "table", "img_path": "VqxODXhU4k/tmp/4052ab537d38d9e6f03215ebc5adc7e171c2d3309e4c072322cdabe6c3958af6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.5 DualIV ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We followed the hyperparameter selection guidelines given in Section 4 of [25]. This means we used a $50/50$ split for train/validation samples and chose the regularization parameters from a grid search, minimizing the squared norm of the dual function $u$ . ", "page_idx": 21}, {"type": "text", "text": "D Small data regime ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This section presents a version of the experiments in Section 5.1 with half the sample size, providing a comparison between SAGD-IV and other methods within a smaller data availability regime. Figure 3 contains the corresponding log-MSE results. ", "page_idx": 22}, {"type": "text", "text": "Contrasting Figures 1 and 3, we can see that the relative performances for each method mostly stay the same. Some methods, such as Deep-IV in the abs scenario, exhibited an error distribution with larger variance, but this is to be expected from the reduction in sample size, which makes any given dataset realization a worse depiction of the true data distribution. These results support the conclusion that our method\u2019s performance is competitive with other state-of-the-art approaches in more data availability conditions. ", "page_idx": 22}, {"type": "image", "img_path": "VqxODXhU4k/tmp/80de143237c22f564cc479e7de8a485b6c12161fa6e5a4165e8b2a08438d3d04.jpg", "img_caption": ["Figure 3: Log-MSE results for the experiments in Section 5.1 with half the sample size. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "E Alternative risk definition ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Computing the risk using $r(Z)$ or $Y$ are equivalent under the quadratic loss, however, one might think that considering $Y$ instead of $r(Z)$ in our formulation would lead to an algorithm that does not need to estimate $r(Z)$ . Below we show that this is not the case, since the gradients of the risk measure under both formulations are the same. This implies that we would still need to estimate $r$ for the algorithm. ", "page_idx": 22}, {"type": "text", "text": "If instead of Equation (5), we consider the risk defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{R}}(h)=\\mathbb{E}[\\ell(Y,\\mathcal{P}[h](Z))],\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then proceeding in exactly the same way as in the proof of Proposition 3.3, we would arrive at ", "page_idx": 22}, {"type": "equation", "text": "$$\nD\\widetilde{\\mathcal{R}}[h](f)=\\mathbb{E}[\\partial_{2}\\ell(Y,\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As it is, this expression is not an inner product in $L^{2}(Z)$ since $Y$ is not measurable with respect to $Z$ . However, if $\\partial_{2}\\ell$ is linear with respect to its first argument, which is true for the quadratic and binary cross entropy losses, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D\\widetilde{\\mathcal{R}}[h](f)=\\mathbb{E}[\\partial_{2}\\ell(Y,\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}\\left[\\mathbb{E}[\\partial_{2}\\ell(Y,\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)\\mid Z]\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}\\left[\\partial_{2}\\ell(\\mathbb{E}[Y\\mid Z],\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}\\left[\\partial_{2}\\ell(r(Z),\\mathcal{P}[h](Z))\\cdot\\mathcal{P}[f](Z)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is the same expression obtained before. Hence, the gradient of $\\mathcal{R}$ still depends on $r(Z)$ and the resulting method still needs to estimate this quantity. ", "page_idx": 22}, {"type": "text", "text": "Nonetheless, one could still use the sample $y_{i}$ as an estimate of $r(z_{i})$ . However, this is not the best choice, because the term $\\left\\lVert\\widehat{r}-r\\right\\rVert_{L^{2}(Z)}^{2}$ which appears in the RHS of Equation (10) would become $\\mathbb{E}[\\mathrm{Var}[Y\\mid Z]]$ . This is constant with respect to the size of the auxiliary dataset $\\mathcal{D}$ (cf. Assumption 4.2) and could be improved by taking $\\widehat{r}$ to be a better regressor of $Y$ over $Z$ . ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not overstate the importance of our work and results. The content of the abstract and introduction properly reflect the impacts of our theoretical and experimental results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Limitations of the algorithm are discussed in Section 4.1 ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The set of assumptions is listed along the paper using a dedicated LATEX environment and the proofs of all results are in Appendix A. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Implementation details are presented in Section 4.1 and in Appendix C. Following these directions, one could reproduce all results. We are also submitting an anonymized version of the code. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We use synthetic data in all our experiments and we followed NeurIPS code guidelines to submit an anonymized version of the code. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Implementation details are presented in Section 4.1 and in Appendix C. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Figures 1 and 2 present error bars for the log-MSE of our experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The computer resources are disclosed in Appendix C. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have conformed with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work discusses only theoretical and computational aspects of an estimation problem. We do not foresee any societal impact. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: All methods are properly cited. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]