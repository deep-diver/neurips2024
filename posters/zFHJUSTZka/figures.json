[{"figure_path": "zFHJUSTZka/figures/figures_1_1.jpg", "caption": "Figure 1: Summary of the proposed online AI feedback (OAIF) approach for making direct alignment from preferences (DAP) methods online and on-policy. Given an input prompt x, two responses y\u00b9 and y\u00b2 are first sampled from the current language model \u03c0\u03b8\u03c4, then labelled as y+ and y\u2212 by the LLM annotator. The language model parameters are then updated using the objective function of DAP methods.", "description": "This figure illustrates the Online AI Feedback (OAIF) method.  An input prompt (x) is given to the LLM being aligned (\u03c0\u03b8\u03c4), which generates two different responses (y1 and y2).  These responses are then presented to an LLM annotator, which acts as a human preference judge and selects the preferred response (y+) and the less preferred response (y\u2212). Finally, the parameters of the original LLM (\u03c0\u03b8\u03c4) are updated based on the preferences provided by the annotator LLM, using a Direct Alignment from Preferences (DAP) method's objective function.  This process makes the DAP methods online and on-policy, addressing limitations of traditional offline methods.", "section": "1 Introduction"}, {"figure_path": "zFHJUSTZka/figures/figures_2_1.jpg", "caption": "Figure 1: Summary of the proposed online AI feedback (OAIF) approach for making direct alignment from preferences (DAP) methods online and on-policy. Given an input prompt x, two responses y\u00b9 and y\u00b2 are first sampled from the current language model \u03c0\u03b8, then labelled as y\u207a and y\u207b by the LLM annotator. The language model parameters are then updated using the objective function of DAP methods.", "description": "This figure illustrates the Online AI Feedback (OAIF) method.  An input prompt is given to the LLM being aligned (\u03c0\u03b8). Two responses (y1 and y2) are generated by this model.  These responses are then given to another LLM acting as an annotator, which selects the preferred response (y+) and the less preferred response (y-).  This preference information is used to update the parameters of the model being aligned, effectively making the DAP method online and on-policy.", "section": "1 Introduction"}, {"figure_path": "zFHJUSTZka/figures/figures_4_1.jpg", "caption": "Figure 3: Win rate of DPO with OAIF (online DPO), vanilla DPO (offline DPO), RLAIF, and RLHF against the SFT baseline on the TL;DR task, judged by Gemini Pro.", "description": "This figure shows the win rates of four different methods (Online DPO, Offline DPO, RLAIF, and RLHF) against a supervised fine-tuning (SFT) baseline on the TL;DR task. The win rate is calculated based on the number of times each method produces a better response than the SFT baseline.  The x-axis represents the number of training steps, and the y-axis shows the win rate (percentage).  The figure demonstrates that Online DPO, which incorporates the proposed Online AI Feedback (OAIF) method, significantly outperforms the other methods and shows less overfitting than the Offline DPO. The results are judged by the Gemini Pro model.", "section": "4.2 How effective is OAIF for LLM alignment?"}, {"figure_path": "zFHJUSTZka/figures/figures_6_1.jpg", "caption": "Figure 4: Left: Fraction of outputs from online DPO, offline DPO, RLAIF, and RLHF being preferred in a 4-way comparison; Right: average quality scores (y-axis, higher is better) assigned to responses of different lengths (x-axis). The responses of each model were first grouped into six buckets by their length. The mean and standard error of responses in a bucket are then plotted as a data point. All results are judged by human raters on TL;DR.", "description": "This figure shows a comparison of four methods (Online DPO, Offline DPO, RLAIF, and RLHF) using two metrics: the fraction of responses preferred by human raters in a 4-way comparison and the average quality score of responses grouped by length.  The left chart shows the win rates of each method, while the right chart illustrates how response quality correlates with length, revealing potential length bias in evaluation.", "section": "4.2 How effective is OAIF for LLM alignment?"}, {"figure_path": "zFHJUSTZka/figures/figures_6_2.jpg", "caption": "Figure 4: Left: Fraction of outputs from online DPO, offline DPO, RLAIF, and RLHF being preferred in a 4-way comparison; Right: average quality scores (y-axis, higher is better) assigned to responses of different lengths (x-axis). The responses of each model were first grouped into six buckets by their length. The mean and standard error of responses in a bucket are then plotted as a data point. All results are judged by human raters on TL;DR.", "description": "This figure presents a comparison of four different methods: online DPO, offline DPO, RLAIF, and RLHF.  The left panel shows the fraction of times each method produced responses preferred by human evaluators in a four-way comparison. The right panel displays the average quality scores (1-5) of responses grouped by their length (in tokens).  Error bars represent standard error.  The results are based on the TL;DR task.", "section": "4.2 How effective is OAIF for LLM alignment?"}, {"figure_path": "zFHJUSTZka/figures/figures_6_3.jpg", "caption": "Figure 5: Win rate of online DPO against the SFT baseline, offline DPO, RLAIF, and RLHF, with annotating LLMs of varying sizes (XS, S, L) in the task TL;DR, as assessed by Gemini Pro.", "description": "This figure shows the win rate of online DPO (Direct Preference Optimization with Online AI Feedback) against several baselines (SFT, offline DPO, RLAIF, RLHF) across different sizes of LLMs used as annotators. The x-axis represents the baseline methods, and the y-axis represents the win rate. Each bar shows the win rate for a specific annotator size (XS, S, L) which are compared against the baseline.  The results indicate how the size of the LLM annotator impacts the effectiveness of OAIF.", "section": "4.5 How does the size of the LLM annotator affect performance?"}, {"figure_path": "zFHJUSTZka/figures/figures_7_1.jpg", "caption": "Figure 4: Left: Fraction of outputs from online DPO, offline DPO, RLAIF, and RLHF being preferred in a 4-way comparison; Right: average quality scores (y-axis, higher is better) assigned to responses of different lengths (x-axis). The responses of each model were first grouped into six buckets by their length. The mean and standard error of responses in a bucket are then plotted as a data point. All results are judged by human raters on TL;DR.", "description": "This figure presents a comparison of four different methods (online DPO, offline DPO, RLAIF, and RLHF) for language model alignment.  The left panel shows the win rate (fraction of times a method's output was preferred by human raters in a four-way comparison).  The right panel displays the average quality score of the generated text, grouped by response length.  Human raters evaluated the results on the TL;DR dataset.  The figure demonstrates that online DPO produces higher-quality responses than other methods and that response length impacts the perceived quality.", "section": "4.2 How effective is OAIF for LLM alignment?"}, {"figure_path": "zFHJUSTZka/figures/figures_7_2.jpg", "caption": "Figure 4: Left: Fraction of outputs from online DPO, offline DPO, RLAIF, and RLHF being preferred in a 4-way comparison; Right: average quality scores (y-axis, higher is better) assigned to responses of different lengths (x-axis). The responses of each model were first grouped into six buckets by their length. The mean and standard error of responses in a bucket are then plotted as a data point. All results are judged by human raters on TL;DR.", "description": "This figure shows a comparison of four different methods for language model alignment: Online DPO, Offline DPO, RLAIF, and RLHF.  The left panel displays the fraction of times each method's output was preferred by human evaluators in a four-way comparison. The right panel shows how the average quality score of the responses correlates with their length.  The results demonstrate that Online DPO outperforms other methods, and also shows that the preference for longer responses might bias the results.", "section": "4.2 How effective is OAIF for LLM alignment?"}, {"figure_path": "zFHJUSTZka/figures/figures_8_1.jpg", "caption": "Figure 7: Win rate of online DPO with OAIF from PaLM 2-XS (weak teacher) and PaLM 2-L (strong teacher) against the SFT baseline and offline DPO, in the task Helpfulness, judged by Gemini Pro.", "description": "This bar chart displays the win rates of online Direct Preference Optimization (DPO) with Online AI Feedback (OAIF) against the Supervised Fine-Tuning (SFT) baseline and offline DPO in the Helpfulness task.  Two scenarios are shown: one using PaLM 2-XS as the LLM annotator (weak teacher) and the other using PaLM 2-L (strong teacher). The win rate is calculated against both the SFT baseline and offline DPO to compare the effectiveness of OAIF under different teacher strengths.", "section": "4.6 How prompt-controllable is OAIF?"}, {"figure_path": "zFHJUSTZka/figures/figures_13_1.jpg", "caption": "Figure 8: Log-probability of on-policy responses, y+ and y\u00af, and the off-policy one \u04ef, according to GPT-2 Large \u03c0\u03b8. The gap between log \u03c0\u03b8(\u04ef|x) and log \u03c0\u03b8(y+|x)/log \u03c0\u03b8(y\u00af|x) is clear, which validates the existence of a distribution shift problem.", "description": "This figure shows the log-probabilities of responses generated by different methods under the GPT-2 Large language model.  The \"Online chosen response\" and \"Online rejected response\" represent responses sampled from the current policy during training, while the \"Offline response\" is a response from a different model (PaLM 2-S). The significant difference in log probabilities between online and offline responses highlights the distribution shift problem that occurs when using offline feedback in direct preference optimization (DPO) methods.  The gap visually demonstrates that the distributions of responses generated by the online and offline models are significantly different, illustrating the off-policy nature of traditional DPO.", "section": "Background"}, {"figure_path": "zFHJUSTZka/figures/figures_14_1.jpg", "caption": "Figure 3: Win rate of DPO with OAIF (online DPO), vanilla DPO (offline DPO), RLAIF, and RLHF against the SFT baseline on the TL;DR task, judged by Gemini Pro.", "description": "This figure shows the win rates of four different methods (Online DPO, Offline DPO, RLAIF, and RLHF) against a supervised fine-tuning (SFT) baseline on the TL;DR task.  The win rate is calculated as the percentage of times a given method's response was judged better than the SFT baseline's response by Gemini Pro. The graph plots win rate against training steps. Online DPO shows consistently higher win rates than the other methods, demonstrating the effectiveness of the proposed OAIF method, especially compared to offline DPO which shows signs of overfitting. RLAIF and RLHF provide benchmarks for comparison against the DAP methods.", "section": "4.2 How effective is OAIF for LLM alignment?"}, {"figure_path": "zFHJUSTZka/figures/figures_20_1.jpg", "caption": "Figure 1: Summary of the proposed online AI feedback (OAIF) approach for making direct alignment from preferences (DAP) methods online and on-policy. Given an input prompt x, two responses y\u00b9 and y\u00b2 are first sampled from the current language model \u03c0\u03b8\u03c4, then labelled as y+ and y\u2212 by the LLM annotator. The language model parameters are then updated using the objective function of DAP methods.", "description": "This figure illustrates the Online AI Feedback (OAIF) method.  The process starts with an input prompt.  Two different responses are generated by the language model being trained. An LLM annotator then compares the two responses and selects the preferred one. This preference data is then used to update the language model's parameters, making the training process online and on-policy.", "section": "1 Introduction"}, {"figure_path": "zFHJUSTZka/figures/figures_21_1.jpg", "caption": "Figure 1: Summary of the proposed online AI feedback (OAIF) approach for making direct alignment from preferences (DAP) methods online and on-policy. Given an input prompt x, two responses y\u00b9 and y\u00b2 are first sampled from the current language model \u03c0\u03b8\u03c4, then labelled as y+ and y- by the LLM annotator. The language model parameters are then updated using the objective function of DAP methods.", "description": "This figure illustrates the Online AI Feedback (OAIF) method.  An input prompt is given to the language model being aligned (\u03c0\u03b8\u03c4), generating two different responses (y1 and y2).  An LLM annotator compares these responses and labels one as preferred (y+) and the other as less preferred (y-). The model's parameters (\u03b8) are then updated based on this preference feedback using a DAP method's objective function. This process is done iteratively during training, making the alignment process online and on-policy.", "section": "1 Introduction"}]