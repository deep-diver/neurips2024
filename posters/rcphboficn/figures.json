[{"figure_path": "RcPHbofiCN/figures/figures_1_1.jpg", "caption": "Figure 1: Some methods developed to enhance LLMs' context awareness. (a) Attention Buckets [8] selects N different RoPEs and conducts N parallel inferences for each input. The outputs are then aggregated in the final layer. (b) Ms-PoE [49] employs a unique RoPE angle for each attention head. However, it needs an additional forward pass for RoPE angle assignment. (c) MoICE integrates a router within each attention head. This novel plug-in selects several of the most suitable ROPE angles for each token. The selected ROPE angles collectively contribute to computing the attention scores. MOICE demonstrates superior memory efficiency and performance.", "description": "This figure compares three different methods for enhancing LLMs' context awareness: Attention Buckets, Ms-PoE, and MoICE.  Attention Buckets uses multiple RoPE angles in parallel, increasing computational cost. Ms-PoE uses a unique angle per head, requiring an extra forward pass. MoICE integrates a router into each attention head to dynamically select the most suitable RoPE angles for each token, offering improved efficiency and performance.", "section": "1 Introduction"}, {"figure_path": "RcPHbofiCN/figures/figures_2_1.jpg", "caption": "Figure 2: Different Bj alter the upper bounds of attention scores between a token and its x-distance neighbors. Each angle is distinguished by its own base value Bj.", "description": "This figure shows how different base values (Bj) in the Rotary Position Embedding (RoPE) affect the attention scores between a token and its neighbors at varying distances.  Each line represents a different Bj value, illustrating how the maximum attention score changes with the distance from the token.  The varying upper bounds highlight the uneven awareness of LLMs towards different contextual positions, a core problem addressed in the paper.", "section": "Background"}, {"figure_path": "RcPHbofiCN/figures/figures_3_1.jpg", "caption": "Figure 3: The structure of MoICE. Only the router's parameters are trainable when plugged into an LLM. For clarity, the figure illustrates a single head, with N=3 and K=2 as toy demonstration examples.", "description": "This figure shows the architecture of Mixture of In-Context Experts (MoICE), a novel plug-in for LLMs. It consists of a router integrated into each attention head which selects the most suitable RoPE angles for computing the attention scores. The router's parameters are the only trainable parameters and the weights are updated through a lightweight router-only training optimization strategy.  The figure simplifies the illustration to a single head, showing how the router dynamically selects among a set of available RoPE angles.", "section": "3 Mixture of In-Context Experts"}, {"figure_path": "RcPHbofiCN/figures/figures_16_1.jpg", "caption": "Figure 3: The structure of MoICE. Only the router's parameters are trainable when plugged into an LLM. For clarity, the figure illustrates a single head, with N=3 and K=2 as toy demonstration examples.", "description": "This figure illustrates the architecture of the Mixture of In-Context Experts (MoICE) method.  It shows how a MoICE router, a multi-layer perceptron (MLP), is integrated into each attention head of a large language model (LLM). The router dynamically selects K RoPE angles (in-context experts) out of a set of N candidate RoPE angles based on the input query.  Only the router parameters are updated during training; the LLM parameters remain fixed. The figure highlights the dynamic selection of RoPE angles per token, allowing flexible attention to various contextual positions.", "section": "3.1 Architecture"}, {"figure_path": "RcPHbofiCN/figures/figures_17_1.jpg", "caption": "Figure 3: The structure of MoICE. Only the router's parameters are trainable when plugged into an LLM. For clarity, the figure illustrates a single head, with N=3 and K=2 as toy demonstration examples.", "description": "The figure shows the architecture of Mixture of In-Context Experts (MoICE).  MoICE is a plug-in module for LLMs that enhances context awareness.  The core component is a router integrated into each attention head. The router dynamically selects K RoPE angles (in-context experts) from a set of N candidates, based on the query vector.  These selected angles are used to compute attention scores, which are then aggregated to create the final attention pattern. Only the router's parameters are trained; the LLM's parameters are frozen. The figure simplifies the illustration to a single attention head with N=3 and K=2 to improve understanding.", "section": "3 Mixture of In-Context Experts"}, {"figure_path": "RcPHbofiCN/figures/figures_17_2.jpg", "caption": "Figure 3: The structure of MoICE. Only the router's parameters are trainable when plugged into an LLM. For clarity, the figure illustrates a single head, with N=3 and K=2 as toy demonstration examples.", "description": "This figure illustrates the architecture of the Mixture of In-Context Experts (MoICE) method.  It shows how MoICE is integrated into a single attention head within a larger language model (LLM). The key components are a router (an MLP) that selects a subset of RoPE angles (in-context experts) and the mechanism for aggregating the attention scores computed with those selected angles.  The figure highlights that only the router's parameters are updated during training, while the LLM parameters remain frozen. The simplified example uses 3 RoPE angles (N=3) and selects 2 of them (K=2) for each token.", "section": "3.1 Architecture"}]