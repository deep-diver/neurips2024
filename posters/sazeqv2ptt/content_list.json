[{"type": "text", "text": "General bounds on the quality of Bayesian coresets ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Trevor Campbell\u2217 Department of Statistics University of British Columbia trevor@stat.ubc.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data. But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings\u2014i.e., exponential family models, or models with strong log-concavity and smoothness assumptions. This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets. The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work. The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods. The upper bounds are used to analyze the performance of recent subsample-optimize methods. The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale data is now commonplace in scientific and commerical applications of Bayesian statistics. But despite its prevalence, and the corresponding wealth of research dedicated to scalable Bayesian inference, there are still suprisingly few general methods that provably provide inferential results, within some reasonable tolerated error, at a significant computational cost savings. Exact Markov chain Monte Carlo (MCMC) methods require many full passes over the data [1, Ch. 6\u201312, 2, Ch. 11\u2013 12], limiting the utility of these methods when even a single pass is expensive. A wide range of MCMC methods that access only a subset of data per iteration, e.g., via delayed acceptance [3\u20136], pseudomarginal or auxiliary variable methods [7\u20139], and basic subsampling [10\u201313], provide at most a minor improvement over full-data MCMC [14\u201316]. On the other hand, methods including carefully constructed log-likelihood function control variates can provide substantial gains [17\u201319]. However, black-box control variate constructions for large-scale data often rely on assumptions such as posterior density differentiability and unimodality that do not hold in many popular models, e.g., those with discrete variables or multimodality. See [15, 20] for a survey of scalable MCMC methods. Parametric approximations via variational inference [21] or the Laplace approximation [22, 23] can be obtained scalably using stochastic optimization methods, but existing general theoretical guarantees for these methods again typically rely on posterior normality assumptions [24, p. 141\u2013144,25\u201330] (see [21, 31] for a review). ", "page_idx": 0}, {"type": "text", "text": "Although many existing methods rely on asymptotic normality or unimodality in the large-scale data regime, the problem of handling large-scale data in Bayesian inference does not fundamentally require this structure. Instead, one can more generally exploit redundancy in the data (i.e., the existence of good approximate sufficient statistics), which can be used to draw principled conclusions about a large data set based only on a small fraction of examples. Indeed, while approximate posterior normality often does not hold in models with latent discrete or combinatorial objects, weakly identifiable or unidentifiable parameters, persisting heavy tails, multimodality, etc., such models can and regularly do exhibit significant redundancy in the data that can be exploited for faster large-scale inference. Bayesian coresets [32]\u2014which involve replacing the full dataset during inference with a sparse weighted subset\u2014are based on this notion of exploiting data redundancy. Empirical studies have shown the existence of high-quality coreset posterior approximations constructed from a small fraction of the data, even in models that violate posterior normality assumptions and for which standard control variate techniques work poorly [33\u201337]. However, existing theoretical support for Bayesian coresets in the literature is limited. There exist no lower bounds on Bayesian coreset approximation error, and while upper bounds do exist, they currently impose restrictive assumptions. In particular, the best available theoretical upper bounds to date apply to exponential family models [36, 38] and models with strongly log-concave and locally smooth log-densities [37]. ", "page_idx": 1}, {"type": "text", "text": "This article presents new theoretical techniques and results regarding the quality of Bayesian coreset approximations. The main results are two general large-data asymptotic lower bounds on the KL divergence (Theorems 3.3 and 3.5), as well as a general upper bound on the KL divergence (Theorem 5.3) under the assumption that the log-likelihoods satisfy a multivariate generalization of subexponentiality (Definition 5.2). The main general results in this paper lead to various novel insights about specific Bayesian coreset construction methods. Under mild assumptions, ", "page_idx": 1}, {"type": "text", "text": "\u2022 common importance-weighted coreset constructions (e.g. [32]) require a coreset size $M$ proportional to the dataset size $N$ (Corollary 4.1), even with post-hoc optimal weight scaling (Corollary 4.2), and thus yield a negligible improvement over full-data inference; \u2022 any construction algorithm requires a coreset size $M>d$ when the log-likelihood function is determined by $d$ parameters locally around a point of concentration (Corollary 4.3); \u2022 subsample-optimize coreset construction algorithms (e.g. [36\u201339]) achieve an asymptotically bounded error with a coreset size polylog $N$ in a wide variety of models (Corollary 6.1). ", "page_idx": 1}, {"type": "text", "text": "The paper includes empirical validation of the main theoretical claims on two models that violate common assumptions made in the literature: a multimodal, unidentifiable Cauchy location model with a heavy-tailed prior, and an unidentifiable logistic regression model with a heavy-tailed prior and persisting posterior heavy tails. Experiments were performed on a computer with an Intel Core i7-8700K and 32GB of RAM. Proofs of all theoretical results may be found in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "Notation. We use standard asymptotic growth symbols $O,\\Omega,\\Theta,o,\\omega$ (see, e.g., [40, Sec. 3.3]), and their probabilistic variants $O_{p},\\Omega_{p},\\Theta_{p},o_{p},\\omega_{p}$ (see, e.g., [24, Sec. 2.2]). We use the same symbol to denote a measure $\\pi$ and its density $\\pi(\\cdot)$ with respect to a specified dominating measure. We also regularly suppress integration variables and differential symbols in integrals throughout for notational brevity when these are clear from context; for example, $\\textstyle\\int\\pi\\exp(\\ell)$ is shorthand for $\\begin{array}{r}{\\int\\pi(\\mathrm{d}\\theta)\\exp(\\ell(\\theta))}\\end{array}$ . Finally, the pushforward of a measure $\\pi$ by a map $\\eta$ is denoted simply $\\eta\\pi$ . ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Define a target probability distribution $\\pi$ on a space $\\Theta$ comprised of a sum of $N$ potentials $\\ell_{n}:\\Theta\\to\\mathbb{R}$ , ${n=1,\\ldots,N}$ and a base distribution $\\pi_{0}(\\mathrm{d}\\theta)$ , ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\pi(\\mathrm{d}\\theta)=\\frac{1}{Z}\\exp\\left(\\ell(\\theta)\\right)\\pi_{0}(\\mathrm{d}\\theta),\\qquad\\qquad\\ell(\\theta)=\\sum_{n=1}^{N}\\ell_{n}(\\theta),\\qquad\\qquad\\theta\\in\\Theta,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where the normalization constant $Z$ is not known. In the Bayesian context, this distribution corresponds to a Bayesian posterior distribution for a statistical model with prior $\\pi_{0}$ and conditionally i.i.d. data $X_{n}$ , where $\\ell_{n}(\\theta)=\\log p(X_{n}|\\theta)$ . The goal is to compute or approximate expectations under $\\pi$ ; but the likelihood $\\ell$ (and its gradient) becomes expensive to evaluate when $N$ is large. To avoid this cost, Bayesian coresets [32\u201337] involve replacing the target with a surrogate density ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{w}(\\mathrm{d}\\theta)=\\frac{1}{Z(w)}\\exp\\left(\\ell_{w}(\\theta)\\right)\\pi_{0}(\\mathrm{d}\\theta),\\qquad\\quad\\ell_{w}(\\theta)=\\sum_{n=1}^{N}w_{n}\\ell_{n}(\\theta),\\qquad\\quad\\theta\\in\\Theta,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $w\\in\\mathbb{R}^{N}$ , $w\\geq0$ are a set of weights, and $Z(w)$ is the new normalizing constant. If $w$ has at most $M\\ll N$ nonzeros, the $O(M)$ cost of evaluating $\\sum_{n}w_{n}\\ell_{n}$ (and its gradient) is a significant improvement upon the original $O(N)$ cost. In this work, the problem of coreset construction is formulated in the data-asymptotic limit; a coreset construction method should ", "page_idx": 2}, {"type": "text", "text": "\u2022 run in $o(N)$ time and memory (or at most $O(N)$ with a small leading constant), \u2022 produce a small coreset of size $M=o(N)$ , \u2022 produce a coreset with $O(1)$ posterior forward/reverse KL divergence as $N\\rightarrow\\infty$ . ", "page_idx": 2}, {"type": "text", "text": "These three desiderata ensure that the effort spent constructing and sampling from the coreset posterior is worthwhile: the coreset provides a meaningful reduction in computational cost compared with standard Markov chain Monte Carlo algorithms, and has a bounded approximation error. ", "page_idx": 2}, {"type": "text", "text": "3 Lower bounds on approximation error ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section presents lower bounds on the $\\mathrm{KL}$ divergence of coreset approximations for general models and data generating processes. The first key steps in the analysis are to write all expectations in terms of distributions that do not depend on $w$ , and to remove the difficult-to-control influence of the tails of $\\pi$ and $\\pi_{w}$ by restricting certain integrals to some small subset $B\\subseteq\\Theta$ of the parameter space. Lemma 3.1, the key theoretical tool used in this section, achieves both of these two goals; note that the result has no major assumptions and applies generally in any setting that a Bayesian coreset can be used. For convenience, define ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{\\mathrm{KL}}}(w):=\\operatorname*{min}\\{\\mathrm{KL}(\\pi_{w}||\\pi),\\mathrm{KL}(\\pi||\\pi_{w})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and the decreasing, nonnegative function $f:\\mathbb{R}_{+}\\to\\mathbb{R}_{+}$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\nf(x)=\\left\\{\\begin{array}{l l}{{-\\log x+x-1}}&{{0\\leq x\\leq1}}\\\\ {{0}}&{{x>1.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Lemma 3.1 (Basic KL Lower Bound). For all measurable $B\\subseteq\\Theta$ and coreset weights $w$ , ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{K L}(w)\\geq f(J_{B}(w))\\geq0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ_{B}(w)=\\frac{\\int_{B}\\pi_{0}\\exp\\frac{1}{2}(\\ell+\\ell_{w})}{\\sqrt{\\int\\pi_{0}\\exp(\\ell)\\int\\pi_{0}\\exp(\\ell_{w})}}+\\sqrt{\\pi(B^{c})}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that while the integrals in the fraction denominator in $J_{B}(w)$ range over the whole $\\Theta$ space, a further lower bound on $\\underline{{\\mathrm{KL}}}(w)$ can be obtained by restricting their domains arbitrarily. Also, crucially, the bound in Lemma 3.1 does not depend on $\\pi_{w}(B^{c})$ , which would be difficult to analyze without detailed knowledge of the tail behaviour of $\\pi_{w}$ as a function of the coreset weights $w$ . Although the bound in Lemma 3.1 applies generally, it is most useful when $B$ is small (so that simple local approximations of $\\ell$ and $\\ell_{w}$ can be used), $\\pi$ concentrates on $B$ (so that $\\pi(B^{c})\\approx0)$ ), and $\\pi$ and $\\pi_{w}$ are very different when restricted to $B$ ; the behaviour of the bound in this case is roughly (see the proof in Appendix A) $f(J_{B}(w))\\approx-\\log(1-\\mathrm{TV}(\\pi,\\pi_{w}))$ . Finally, note that Lemma 3.1 remains valid if one replaces $\\ell_{w}$ with $\\ell_{w}-c$ and $\\ell$ with $\\ell-c^{\\prime}$ for any constants $c,c^{\\prime}$ that do not depend on $\\theta$ but may depend on the data and coreset weights $w$ . ", "page_idx": 2}, {"type": "text", "text": "For the remainder of this section, consider the setting where $\\Theta$ is a measurable subset of $\\mathbb{R}^{d}$ for some $d\\in\\mathbb{N}$ , fix some $\\theta_{0}\\in\\Theta$ , and assume each $\\ell_{n}$ is differentiable in a neighbourhood of $\\theta_{0}$ . Let ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\overline{{w}}=\\sum_{n}w_{n}\\qquad g=\\nabla\\ell(\\theta_{0})\\qquad g_{w}=\\nabla\\ell_{w}(\\theta_{0}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Theorems 3.3 and 3.5 characterize KL divergence lower bounds in terms of the sum of the coreset weights $\\overline{{w}}$ and the log-likelihood gradients $g,g_{w}$ . Intuitively for the full data set where all $w_{n}=1$ and $\\overline{{w}}=N$ , and an i.i.d. data generating process from the likelihood with parameter $\\theta_{0}$ , the central limit theorem asserts under mild conditions that $g_{w}/\\overline{{w}}\\stackrel{p}{\\rightarrow}0$ at a rate of $N^{-1/2}$ . Theorems 3.3 and 3.5 below provide KL lower bounds when the coreset construction algorithm does not match this behavior. In particular, Theorem 3.3 provides results that are useful when $g_{w}/\\overline{{w}}\\stackrel{p}{\\rightarrow}0$ occurs reasonably quickly but slower than $N^{-1/2}$ , while Theorem 3.5 strengthens the conclusion when $g_{w}/\\overline{{w}}\\stackrel{p}{\\rightarrow}0$ very slowly or not at all. The major benefit of Theorems 3.3 and 3.5 for analyzing coreset construction methods is that they reduce the problem of analyzing posterior KL divergence to the much easier problem of analyzing the 2-norm $\\|\\cdot\\|_{2}$ of a weighted sum of random vectors in $\\mathbb{R}^{d}$ . ", "page_idx": 3}, {"type": "text", "text": "Consider a sequence $r\\rightarrow0$ as $N\\rightarrow\\infty$ , and for a fixed matrix $H\\succ0$ let ", "page_idx": 3}, {"type": "equation", "text": "$$\nB=\\{\\theta:(\\theta-\\theta_{0})^{T}H(\\theta-\\theta_{0})\\leq r^{2}\\}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "be a sequence of neighbourhoods around $\\theta_{0}$ ; these will appear in Assumptions 3.2 and 3.4 and Theorems 3.3 and 3.5 below. Note that throughout, all asymptotics will be taken as $N\\rightarrow\\infty$ , and various sequences (e.g., $r$ and $B$ ) are implicitly indexed by $N$ . To simplify notation, this dependence is suppressed. Assumption 3.2 makes some weak assumptions about the model and data generating process: it intuitively asserts that the potential functions are sufficiently smooth around $\\theta_{0}$ , that $r\\rightarrow0$ slowly, and that $\\pi$ concentrates at $\\theta_{0}$ at a usual rate. Note that Assumption 3.2 does not assume data are generated i.i.d. and places no conditions on the coreset construction algorithm. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2. $\\pi_{0}$ has a density with respect to the Lebesgue measure, $\\pi_{0}(\\theta_{0})>0,$ , each $\\ell_{n}(\\theta)$ and $\\pi_{0}(\\theta)$ are twice differentiable in $B$ for sufficiently large $N$ , and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in B}\\biggl\\|-\\frac{1}{N}\\nabla^{2}\\ell(\\theta)-H\\biggr\\|_{2}=o_{p}(1),\\quad\\biggl\\|\\frac{g}{N}\\biggr\\|_{2}=O_{p}\\Bigl(N^{-1/2}\\Bigr),\\qquad N r^{2}=\\omega(1).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Two additional assumptions related to the coreset construction algorithm\u2014namely, that it works well enough that $\\begin{array}{r}{\\frac{1}{\\overline{{w}}}\\sum_{n}w_{n}\\nabla^{2}\\ell_{n}(\\theta)\\stackrel{p}{\\rightarrow}H}\\end{array}$ and $g_{w}/\\overline{{w}}\\stackrel{p}{\\rightarrow}0$ at a rate faster than $r\\rightarrow0,$ \u2014lead to asymptotic lower bounds on the best possible quality of coresets produced by the algorithm, as well as lower bounds even after optimal post-hoc scaling of the weights. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.3. Suppose Assumption 3.2 holds. If ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in B}\\left\\|-\\frac{1}{\\overline{{w}}}\\nabla^{2}\\ell_{w}(\\theta)-H\\right\\|_{2}=o_{p}(1),\\quad\\left\\|\\frac{g_{w}}{\\overline{{w}}}\\right\\|_{2}=o_{p}(r),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\displaystyle\\frac{\\mathrm{KL}(w)\\geq O_{p}(1)+\\Omega_{p}(1)\\operatorname*{min}\\left\\{-\\log\\pi(B^{c}),\\displaystyle\\frac{N\\overline{{w}}}{N+\\bar{w}}\\right\\|\\frac{g}{N}-\\frac{g_{w}}{\\bar{w}}\\left\\|_{2}^{2}+d\\log\\displaystyle\\frac{(N+\\bar{w})^{2}}{N\\operatorname*{max}\\{\\bar{w},1/r^{2}\\}}\\right\\}}\\\\ &{\\displaystyle\\operatorname*{min}_{x\\geq0}\\underline{{\\mathrm{KL}}}(\\alpha w)\\geq O_{p}(1)+\\Omega_{p}(1)\\operatorname*{min}\\left\\{-\\log\\pi(B^{c}),d\\log\\left(N\\left\\|\\frac{g}{N}-\\frac{g_{w}}{\\bar{w}}\\right\\|_{2}^{2}\\right)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Theorem 3.3 is restricted to the case where the coreset algorithm is performing reasonably well. Theorem 3.5 extends the bounds to the case where the algorithm is performing poorly, in the sense that it is unable to make $\\frac{g_{w}}{\\overline{{w}}}\\;\\;\\frac{p_{\\!\\scriptscriptstyle\\mathrm{V}}}{\\Gamma}\\;0$ at a rate faster than $r\\rightarrow0$ (or perhaps $\\frac{g_{w}}{\\overline{{w}}}$ does not converge to 0 at all). In order to draw conclusions in this setting, we need a weak global assumption on the potential functions. A function $f:\\Theta\\to\\mathbb{R}$ is $L$ -smooth below at $\\theta_{0}$ if ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall\\theta\\in\\Theta,\\quad f(\\theta)\\geq f(\\theta_{0})+\\nabla f(\\theta_{0})^{T}(\\theta-\\theta_{0})-\\frac{L}{2}\\|\\theta-\\theta_{0}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that $L$ -smoothness below is weaker than Lipschitz smoothness and does not imply concavity; Eq. (1) restricts the growth of the function only in the negative direction, and only when the expansion is taken at $\\theta_{0}$ . Assumption 3.4 asserts that the potential functions are smooth below. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.4. There exist $L_{0},\\ldots,L_{N},L>0$ such that $\\log\\pi_{0}$ is $L_{0}^{2}$ -smooth below at $\\theta_{0}$ , for each $n\\in[N]\\,\\ell_{n}$ is $L_{n}^{2}$ -smooth below at $\\theta_{0}$ , and $\\textstyle{\\frac{1}{N}}\\sum_{n=1}^{N}L_{n}^{2}\\;{\\frac{\\boldsymbol{p}}{N}}\\;L^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.5 uses Assumptions 3.2 and 3.4 and additional assumptions related to the coreset construction algorithm to obtain lower bounds in a setting that relaxes the \u201cperformance\u201d conditions in Theorem 3.3: $\\begin{array}{r}{-\\frac{1}{\\overline{{w}}}\\sum_{n}w_{n}\\nabla^{2}\\ell_{n}(\\theta)}\\end{array}$ no longer needs to converge to $H$ in probability, and $g_{w}/\\overline{{w}}$ can converge to 0 slowly or not at all. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.5. Suppose Assumptions 3.2 and 3.4 hold. If there exist $\\alpha,\\beta>0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\bigg(\\forall\\theta\\in B,~-\\frac{1}{\\overline{{w}}}\\nabla^{2}\\ell_{w}(\\theta)\\succeq\\alpha H\\bigg)\\rightarrow1,\\quad\\mathbb{P}\\bigg(\\frac{1}{\\overline{{w}}}\\sum_{n}w_{n}L_{n}^{2}\\leq\\beta L^{2}\\bigg)\\rightarrow1,\\quad\\bigg\\|\\frac{g_{w}}{\\overline{{w}}}\\bigg\\|=\\omega_{p}(r),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{\\mathrm{KL}}}(w)\\geq O_{p}(1)+\\Omega_{p}(1)\\operatorname*{min}\\biggr\\{-\\log\\pi(B^{c}),d\\log\\biggr(N\\operatorname*{min}\\biggr\\{\\left\\|\\frac{g_{w}}{\\overline{{w}}}\\right\\|^{2},1\\biggr\\}\\biggr)\\biggr\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "An important final note in this section is that while Theorems 3.3 and 3.5, as stated, require choosing $\\Theta$ to be some measurable subset of $\\mathbb{R}^{d}$ and that the posterior $\\pi$ concentrates around some point of interest $\\theta_{0}\\in\\mathbb{R}^{d}$ , these results can be generalized to a wider class of models and spaces. In particular, Corollary 3.6 demonstrates that if $\\Theta$ is arbitrary, but the potential functions $\\ell_{n}$ only depend on $\\theta$ through some other function $\\eta:\\Theta\\to\\mathbb{R}^{d}$ , that the conclusions of Theorems 3.3 and 3.5 still hold. ", "page_idx": 4}, {"type": "text", "text": "Corollary 3.6. Suppose $\\Theta$ is an arbitrary measurable space, and the potential functions take the form $\\ell_{n}(\\eta(\\theta))$ for some measurable function $\\eta:\\Theta\\to\\mathbb{R}^{d}$ . Then if the assumptions of Theorems 3.3 and 3.5 hold for potentials $(\\ell_{n})_{n=1}^{N}$ as functions on $\\mathbb{R}^{d}$ and pushforward prior $\\eta\\pi_{0}$ on $\\mathbb{R}^{d}$ , the stated lower bounds also hold for $\\operatorname*{min}\\{\\mathrm{KL}(\\pi||\\pi_{w}),\\mathrm{KL}(\\pi_{w}||\\pi)\\}$ . ", "page_idx": 4}, {"type": "text", "text": "4 Lower bound applications ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, the general theoretical results from Section 3 are applied to specific algorithms, Bayesian models, and data generating processes to explain previously observed empirical behaviour of coreset construction, as well as to place fundamental limits on the necessary size of coresets. Consider a setting where the data $X_{n}$ arise as an i.i.d. sequence drawn from some probability distribution $\\nu$ , $\\ell_{n}({\\boldsymbol{\\eta}}({\\boldsymbol{\\theta}}))=\\log p(X_{n}|{\\boldsymbol{\\eta}}({\\boldsymbol{\\theta}}))$ for $\\eta:\\Theta\\to\\mathbb{R}^{d}$ , $\\eta_{0}=\\eta(\\theta_{0})$ , and the following technical criteria hold (where $\\mathbb{E}$ denotes expectation under the data generating process): ", "page_idx": 4}, {"type": "text", "text": "(A1) $\\begin{array}{r}{\\mathbb{E}\\left[\\nabla\\ell_{n}(\\eta_{0})\\right]=0\\;\\mathrm{and}\\;H=\\mathbb{E}\\left[-\\nabla^{2}\\ell_{n}(\\eta_{0})\\right]=\\mathbb{E}\\left[\\nabla\\ell_{n}(\\eta_{0})\\nabla\\ell_{n}(\\eta_{0})^{T}\\right]\\succ0.}\\end{array}$ (A2) $\\mathbb{E}\\left[\\Vert\\nabla\\ell_{n}(\\eta_{0})\\Vert_{2}^{2+\\delta}\\right]<\\infty$ for some $\\delta>0$ and $\\mathbb{E}\\left[\\lVert\\nabla^{2}\\ell_{n}(\\eta_{0})\\rVert_{F}^{2}\\right]<\\infty$ . (A3) On a neighbourhood of $\\begin{array}{r}{\\jmath_{0},\\|\\nabla^{2}\\ell_{n}(\\eta)-\\nabla^{2}\\ell_{n}(\\eta_{0})\\|_{2}\\leq R(X_{n})\\|\\eta-\\eta_{0}\\|_{2},\\mathbb{E}\\left[R(X_{n})\\right]<\\infty.}\\end{array}$ (A4) $\\eta\\pi_{0}$ is twice differentiable a neighbourhood of $\\eta_{0}$ , and $\\pi(\\eta_{0})>0$ . (A5) For all $r\\rightarrow0$ such that $r^{2}=\\omega(\\log N/N),-\\log\\eta\\pi(\\|\\eta-\\eta_{0}\\|>r)=\\Omega_{p}(N r^{2}).$ ", "page_idx": 4}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "These conditions apply to a wide range of models, e.g., an unidentifiable, multimodal location model posterior with heavy tails on $\\Theta=\\mathbb{R}$ , where the Bayesian model is specified by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta\\sim\\operatorname{Cauchy}(0,1)\\qquad\\qquad\\qquad(X_{n})_{n=1}^{N}\\stackrel{\\operatorname{iid}}{\\sim}\\operatorname{Cauchy}(\\theta^{2},1),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the data are generated from the likelihood with parameter $\\theta_{0}=5$ , and an unidentifiable logistic regression posterior with heavy tails on $\\mathbb{R}^{2}$ , where the Bayesian model is specified by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\theta\\sim\\mathrm{Cauchy}(0,I)\\qquad\\qquad Y_{n}\\overset{\\mathrm{ind}}{\\sim}\\mathrm{Bern}\\bigg(\\frac{1}{1+e^{-X_{n}^{T}A\\theta}}\\bigg)\\qquad\\qquad A=\\left[1\\quad1\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the covariates are generated via $X_{n}\\ {\\overset{\\mathrm{iid}}{\\sim}}\\ \\mathrm{Unif}(\\{x\\in\\mathbb{R}^{2}:\\|x\\|_{2}\\leq1\\})$ ), and the observations $Y_{n}$ are generated from the likelihood with parameter $\\theta_{0}=\\left[1\\quad6\\right]^{T}$ . See Proposition A.6 in Appendix A for the verification of (A1-5) for these two models. Example posterior log-densities for these models are displayed in Fig. 1. ", "page_idx": 4}, {"type": "image", "img_path": "SAZeQV2PtT/tmp/49cce9c702ab4055a3d3d59702451a361f55b86fb2d3ce7d734ec0a81931142a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: Example unnormalized posterior densities given 50 data points for (1a) the Cauchy location model and (1b) the logistic regression model. The orange and blue dashed lines in (1b) indicate one-dimensional slices that are shown in the rightmost panels. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Importance-weighted coreset construction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Compute probabilities $(p_{n})_{n=1}^{N}$ (may depend on the data and model) Draw $I_{1},\\ldots,I_{M}\\ {\\overset{\\mathrm{iid}}{\\sim}}\\ \\mathrm{Categorical}(p_{1},\\ldots,p_{N})$ For each $n$ , set $\\begin{array}{r}{w_{n}=\\frac{1}{M p_{n}}\\sum_{m=1}^{M}\\mathbb{1}[I_{m}=n]}\\end{array}$ . return $(w_{n})_{n=1}^{N}$ ", "page_idx": 5}, {"type": "table", "img_path": "SAZeQV2PtT/tmp/9aef7607264ee7dffe58f62630aa1bb4f0dde9980cfc5bb5180e10a9e27171e0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Minimum coreset size for importance-weighted coresets ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "A popular algorithm for coreset construction that has appeared in a wide variety of domains\u2014e.g., Bayesian inference [32, 33, Section 4.1], frequentist inference (e.g., [41\u201345]), and optimization (see [46] for a recent survey)\u2014involves subsampling of the data followed by an importance-weighting correction. The pseudocode is given in Algorithm 1. Note that $\\mathbb{E}[w_{n}]=1$ , and so $\\mathbb{E}[\\ell_{w}]=\\bar{\\ell}$ ; the coreset potential is an unbiased estimate of the exact potential. The advantage of this method is that it is straightforward and computationally efficient. If the sampling probabilities are uniform $p_{n}={^{1}\\!\\big/}N$ , then Algorithm 1 constructs a coreset in $O(M)$ time and $O(M)$ memory. Nonuniform probabilities $p_{n}$ require $\\Omega(N)$ time, as they require a pass over all $N$ data points to compute each $p_{n}$ [32, 42] followed by sampling the coreset, e.g., via an alias table [47, 48]. However, empirical results produced by this methodology have generally been underwhelming, even with carefully chosen sampling probabilities; see, e.g., Figure 2 of [32]. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.1 explains these poor results: Bayesian coresets constructed via Algorithm 1 must satisfy $M\\propto N$ in order to maintain a bounded $\\underline{{\\mathrm{KL}}}(w)$ in the data-asymptotic limit. In other words, such coresets do not satisfy the desiderata in Section 2. The only restriction is that there exist constants $c,C>0$ such that for all $N\\in\\mathbb{N}$ , the sampling probabilities $(p_{n})_{n=1}^{N}$ satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n0<c\\leq\\operatorname*{min}_{n}N p_{n}\\leq\\operatorname*{max}_{n}N p_{n}\\leq C<\\infty\\quad a.s.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The lower threshold ensures that the variance of the importance-weighted log-likelihood is not too large, while the upper threshold ensures sufficient diversity in the draws from subsampling. The condition in Eq. (4) is not a major restriction, in the sense that performance should deteriorate even further when it does not hold. The $(p_{n})_{n=1}^{N}$ may otherwise depend arbitrarily on the data and model. ", "page_idx": 5}, {"type": "text", "text": "Corollary 4.1. Given (A1-6), $M\\to\\infty$ , and $M=o(N)$ , coresets produced by Algorithm 1 satisfy ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underline{{\\mathrm{KL}}}(w)=\\Omega_{p}\\left(\\frac{N}{M}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The intuition behind Corollary 4.1 is that both the true posterior and the importance-weighted coreset posterior are asymptotically approximately normal with variance $\\propto1/N$ as $N\\rightarrow\\infty$ ; however, the coreset posterior mean is roughly $\\propto M^{-1/2}$ away from the posterior mean, because the subsample is of size $M$ . The KL divergence between two Gaussians is lower-bounded by the inverse variance times the mean difference squared, yielding $\\approx N/M$ as in Eq. (5). ", "page_idx": 6}, {"type": "text", "text": "Given the intuition that the coreset posterior mean is far from the posterior mean relative to their variances, it is worth asking whether one can apply a small amount of effort to \u201ccorrect\u201d the importance-weighted coreset by scaling the weights (and hence the variance) down, as shown in Algorithm 2. Unfortunately, Corollary 4.2 demonstrates that even with optimal scaling, $M\\propto N$ is still required in order to maintain a bounded KL divergence as $N\\rightarrow\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "Corollary 4.2. Given (A1-6), $M\\to\\infty,$ , and $M=o(N)$ , coresets produced by Algorithm 1 satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha>0}\\underline{{\\mathrm{KL}}}(\\alpha w)=\\Omega_{p}\\left(\\log\\frac{N}{M}\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Fig. 2 provides empirical confirmation of Corollaries 4.1 and 4.2 on the Cauchy location and logistic regression models in Eqs. (2) and (3). In particular, these figures show that the empirical rates of growth of KL as a function of $N$ closely matches $\\begin{array}{r}{\\Omega_{p}(\\frac{\\dot{N}}{M})}\\end{array}$ for importance-weighted corese\u221ats, and $\\Omega_{p}(\\breve{\\mathrm{log}}\\;\\frac{N}{M})$ for the same with post-hoc scaling, for a wide range of coreset sizes $M\\in\\{\\log N,\\sqrt{N},{1}/{2N}\\}$ . Thus, importance weighted coreset construction methods do not satisfy the desiderata in Section 2 for a wide range of models, and alternate methods should be considered. ", "page_idx": 6}, {"type": "text", "text": "4.2 Minimum coreset size for any coreset construction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section extends the minimum coreset size results from importance-weighted schemes to any coreset construction algorithm. In particular, Corollary 4.3 shows that under (A7)\u2014a strengthening of (A3) and Assumption 3.4\u2014and (A8)\u2014which asserts that $\\nabla\\ell_{1}(\\eta_{0}),\\dots,\\nabla\\ell_{M}(\\eta_{0})$ are linearly independent a.s. and satisfy a technical moment condition\u2014at least $d$ coreset points are required to keep the KL divergence bounded as $N\\rightarrow\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "(A7) Assumption 3.4 holds and there exists $\\gamma>0$ such that for all sufficiently large $N\\in\\mathbb N$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\forall\\eta\\in B,n\\in[N],\\quad-\\nabla^{2}\\ell_{n}(\\eta)\\succeq\\gamma H\\quad\\mathrm{and}\\quad L_{n}^{2}<\\gamma^{-1}L^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "(A8) For all coreset sizes $M<d$ , there exists a $\\delta>0$ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left(1^{T}(G^{T}G)^{-1}1\\right)^{M+\\delta}\\right]<\\infty\\qquad G=[\\nabla\\ell_{1}(\\eta_{0})\\quad\\ldots\\quad\\nabla\\ell_{M}(\\eta_{0})]\\in\\mathbb{R}^{d\\times M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Corollary 4.3. For a fixed coreset size $M<d,$ , given (A1-5,7,8), ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w\\in\\mathbb{R}_{+}^{N}:\\|w\\|_{0}\\leq M}\\frac{\\mathrm{KL}(w)=\\Omega_{p}(\\log N)}{\\mathrm{KL}(w)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5 Upper bounds on approximation error ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section presents upper bounds on the KL divergence of coreset approximations. As in Section 3, the first step is to write all expectations in terms of distributions that do not depend on $w$ . Lemma 5.1 does so without imposing any major assumptions; the result again applies generally in any setting that a Bayesian coreset can be used. For convenience, define ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathrm{KL}}}(w):=\\operatorname*{max}\\{\\mathrm{KL}(\\pi_{w}||\\pi),\\mathrm{KL}(\\pi||\\pi_{w})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Lemma 5.1 (Basic KL Upper Bound). For all coreset weights $w$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{KL}}}(w)\\leq\\operatorname*{inf}_{\\lambda>0}\\frac{1}{\\lambda}\\log\\int\\pi\\exp\\bigl((1+\\lambda)(\\bar{\\ell}_{w}-\\bar{\\ell})\\bigr),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where for all $\\begin{array}{r}{n\\in[N],\\,\\bar{\\ell}_{n}=\\ell_{n}-\\int\\pi\\ell_{n},\\,\\bar{\\ell}=\\sum_{n}\\bar{\\ell}_{n},\\,a n d\\,\\bar{\\ell}_{w}=\\sum_{n}w_{n}\\bar{\\ell}_{n}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "The upper bound in Lemma 5.1 is nonvacuous (i.e., finite) as long as there exists a $\\alpha>1$ such that the $\\alpha$ R\u00b4enyi divergence $D_{\\alpha}(\\pi_{w}||\\pi)$ [49, p. 3799] is finite. Note that as in Lemma 3.1, the bound in ", "page_idx": 6}, {"type": "image", "img_path": "SAZeQV2PtT/tmp/248d5d23365ef48afd456181b4a897448b1275e83ddc7c5dd123979e807c0811.jpg", "img_caption": ["Figure 2: Importance-weighted coreset quality, showing the minimum of the forward and reverse K\u221aL divergences on the vertical axis as a function of dataset size $N$ for 3 coreset sizes: $\\log N$ (black), $\\sqrt{N}$ (blue), and $^1/2N$ (red). Dashed lines indicate predictions from the theory in Corollaries 4.1 and 4.2, solid lines indicate the mean over 10 trials, and error bars indicate standard error. The top row shows the quality of basic importance-weighted coresets (note that both horizontal and vertical axes are in log scale), while the bottom row shows the quality with optimal post-hoc scaling (note that only the horizontal axis is in log scale). The left column corresponds to the Cauchy location model, while the right column corresponds to the logistic regression model. Sampling probabilities $p_{n}$ for both models are set proportional to $X_{n}^{2}$ , thresholded to lie between $0.1/N$ and $10/N$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Lemma 5.1 remains valid if one replaces $\\ell_{w}$ with $\\ell_{w}-c$ and $\\ell$ with $\\ell-c^{\\prime}$ for any constants $c,c^{\\prime}$ that do not depend on $\\theta$ but may depend on the coreset weights $w$ and data. ", "page_idx": 7}, {"type": "text", "text": "More practical bounds necessitate an assumption about the behaviour of the potentials $(\\ell_{n})_{n=1}^{N}$ . Definition 5.2 below asserts that the multivariate moment generating function of $(\\bar{\\ell}_{n})_{n=1}^{N}$ is bounded when the vector is close to 0. This definition is a generalization of the usual definition of subexponentiality for the univariate setting (e.g., [50, Sec. 2.7]). Theorem 5.3 subsequently shows that Definition 5.2 is sufficient to obtain simple bounds on $\\overline{{\\mathrm{KL}}}$ . ", "page_idx": 7}, {"type": "text", "text": "Definition 5.2. For $A~\\in~\\mathbb{R}^{N\\times N}$ , $A\\;\\succeq\\;0,$ , and monotone function $h\\ :\\ \\mathbb{R}_{+}\\ \\to\\ \\mathbb{R}_{+}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow0}h(x)=h(0)=0,}\\end{array}$ , the potentials $(\\ell_{n})_{n=1}^{N}$ are $(h,A)$ -subexponential $i f$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathbb{R}^{N}:w^{T}A w\\leq1,\\qquad\\int\\pi\\exp\\bigl(\\bar{\\ell}_{w}\\bigr)\\leq\\exp\\bigl(h(w^{T}A w)\\bigr).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3. If the potentials $(\\ell_{n})_{n=1}^{N}$ are $(h,A)$ -subexponential, then ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\forall w\\in\\mathbb{R}_{+}^{N}:4(w-1)^{T}A(w-1)\\leq1,\\qquad\\overline{{\\mathrm{KL}}}(w)\\leq h(4(w-1)^{T}A(w-1)).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Definition 5.2, the key assumption in Theorem 5.3, is satisfied by a wide range of models when choosing $h(x)=x$ and $A\\propto\\dot{\\mathrm{Cov}}_{\\pi}\\big((\\ell_{n})_{n=1}^{N}\\big)$ , as demonstrated by Proposition 5.4. Because this case applies widely, let $A$ -subexponential be shorthand for $(h,A)$ -subexponentiality with $h(x)=x$ . ", "page_idx": 7}, {"type": "text", "text": "Proposition 5.4. If for all $w$ in a ball centered at the origin, $\\begin{array}{r}{\\int\\pi\\exp(\\bar{\\ell}_{w})<\\infty,}\\end{array}$ , then there exists $\\beta>0$ such that the potentials $(\\ell_{n})_{n=1}^{N}$ are $\\beta\\operatorname{Cov}_{\\pi}\\!\\left((\\ell_{n})_{n=1}^{N}\\right)$ -subexponential. ", "page_idx": 7}, {"type": "table", "img_path": "SAZeQV2PtT/tmp/d3a83efc49f75b3ace4bd5b08428256263e0e7f776858338220db5e1bffbb9a8.jpg", "table_caption": ["Algorithm 3 Subsample-optimize coreset construction "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In other words, intuitively, if a coreset construction algorithm produces weights such that $\\mathrm{Var}_{\\pi}(\\bar{\\ell}_{w}\\!-\\!\\bar{\\ell})$ is small, then $\\overline{{\\mathrm{KL}}}(w)$ is also small. That being said, the generality of Definition 5.2 to allow arbitrary $h,A$ is still helpful in obtaining upper bounds in specific cases; see, e.g., Propositions A.1 and A.2. ", "page_idx": 8}, {"type": "text", "text": "6 Upper bound application: subsample-optimize coresets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A strategy to construct Bayesian coresets that has recently emerged in the literature, shown in Algorithm 3, is to first subsample the data to select $M$ data points, and subsequently optimize the weights for those selected data points [36\u201338]. The subsampling step serves to pick a reasonably flexible basis of log-likelihood functions for coreset approximation, and avoids the slow greedy selection routines from earlier work [33\u201335]. The optimization step tunes the weights for the selected basis, avoiding the poor approximations of importance-weighting methods. Indeed, Algorithm 3 creates exact coresets $\\pi_{w^{\\star}}=\\pi$ with high probability in Gaussian location models [36, Prop. 3.1] and finite-dimensional exponential family models [37, Thm. 4.1], and near-exact coresets with high probability in strongly log-concave models [37, Thm. 4.2] and Bayesian linear regression [38, Prop. 3]. ", "page_idx": 8}, {"type": "text", "text": "Corollary 6.1 generalizes these results substantially, and demonstrates that coresets of size $M=$ $O(\\mathrm{polylog}(N))$ produced by the subsample-optimize method in Algorithm 3 maintain a bounded KL divergence as $N\\to\\infty$ . Two key assumptions are subexponentiality of the potentials and a polynomial (in $N$ ) growth of ${\\mathrm{Var}}_{\\pi}({\\ell}(\\theta))$ ; these conditions are not stringent and should hold for a wide range of Bayesian models and i.i.d. data generating processes. The last key assumption in Eq. (6) is that a randomly-chosen potential function $\\ell_{I}$ $,I\\sim\\mathrm{Categorical}(p_{1},\\dots,p_{N})$ (with probabilities as in Algorithm 3) is well-aligned with the residual coreset error function. Similar alignment conditions have appeared in past results for more restrictive settings (see, e.g., $J(\\delta)$ in [37, Thm. 4.1]). ", "page_idx": 8}, {"type": "text", "text": "Corollary 6.1. Suppose there exist $\\beta,\\alpha~>~0$ and $0\\,\\le\\,\\rho,\\epsilon\\,<\\,1$ such that the potential functions $(\\ell_{n})_{n=1}^{N}$ are $\\dot{\\boldsymbol{\\beta}}\\dot{\\mathrm{Cov}}_{\\pi}((\\boldsymbol{\\ell}_{n})_{n=1}^{N})$ -subexponential with probability increasing to $^{\\,l}$ as $N\\to\\infty$ $\\mathrm{Var}_{\\pi}(\\ell(\\theta))=O_{p}(N^{\\alpha}),$ , $M=(\\log N)^{\\frac{1}{1-\\rho}}$ , and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}\\Big(\\operatorname*{max}\\{0,\\mathrm{Corr}_{\\pi}\\big(\\ell_{I_{M}}(\\theta),\\ell(\\theta)-\\ell_{M-1}^{\\star}(\\theta)\\big)\\big\\}^{2}\\geq1-\\epsilon\\Big|\\big(\\ell_{n}\\big)_{n=1}^{N}\\Big)=\\omega_{p}(M^{-\\rho})}\\\\ &{}&{\\ell_{M-1}^{\\star}(\\theta)=\\quad\\arg\\operatorname*{min}_{\\mathrm{~\\tiny~Uar}=\\{\\ell(\\theta)-g(\\theta)\\}}\\quad\\quad I_{1},\\ldots,I_{M}\\overset{i i d}{\\sim}\\mathrm{Categorical}(p_{1},\\ldots,p_{N}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Then Algorithm 3 produces a coreset with $\\overline{{\\mathrm{KL}}}(w)=O_{p}(1)$ as $N\\rightarrow\\infty$ . ", "page_idx": 8}, {"type": "text", "text": "Fig. 3 confirms that subsample-optimize coreset construction methods applied to the logistic regression and Cauchy location models in Eqs. (2) and (3) (which both violate the conditions of past upper bounds in the literature) are able to provide high-quality posterior approximations for very small coresets\u2014in this case, $M\\propto\\log N$ . ", "page_idx": 8}, {"type": "text", "text": "7 Conclusions ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This article presented new general lower and upper bounds on the quality of Bayesian coreset approximations, as measured by the KL divergence. These results were used to draw novel conclusions regarding importance-weighted and subsample-optimize coreset methods, which align with simulation experiments on two synthetic models that violate the assumptions of past theoretical results. Avenues for future work include general bounds on the subexponentiality constant $\\beta$ in Proposition 5.4, as well as the alignment probability in Eq. (6), in the setting of Bayesian models with i.i.d. data generating processes. A limitation of this work is that both quantities currently require case-by-case analysis. ", "page_idx": 8}, {"type": "image", "img_path": "SAZeQV2PtT/tmp/ef01c11a7cf4c90395ea6050bfcd8a4ae777aa0b598d68f7791a4669a9ae241f.jpg", "img_caption": ["Figure 3: Subsample-optimize coreset quality, showing the maximum of the forward and reverse KL divergences on the vertical axis as a function of dataset size $N$ for coresets of size $5+2\\log N$ Solid lines indicate the mean over 70 trials, and error bars indicate standard error. The left panel is for the Cauchy location model, while the right panel is for the logistic regression model. Sampling probabilities are uniform $p_{n}\\,=\\,1/N$ , and coreset weights were optimized by nonnegative least squares for log-likelihoods discretized via samples from $\\pi$ [34, Eq. 4]. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The author gratefully acknowledges the support of an NSERC Discovery Grant (RGPIN-2019-03962). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Christian Robert and George Casella. Monte Carlo Statistical Methods. Springer, 2nd edition, 2004. ", "page_idx": 9}, {"type": "text", "text": "[2] Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. Bayesian data analysis. CRC Press, 3rd edition, 2013.   \n[3] J. Andr\u00b4es Christen and Colin Fox. Markov chain Monte Carlo using an approximation. Journal of Computational and Graphical Statistics, 14(4):795\u2013810, 2005.   \n[4] Marco Banterle, Clara Grazian, Anthony Lee, and Christian P. Robert. Accelerating Metropolis-Hastings algorithms by delayed acceptance. Foundations of Data Science, 1(2):103\u2013128, 2019.   \n[5] Richard Payne and Bani Mallick. Bayesian big data classification: a review with complements. arXiv:1411.5653, 2014.   \n[6] Chris Sherlock, Andrew Golightly, and Daniel Henderson. Adaptive, delayed-acceptance MCMC for targets with expensive likelihoods. Journal of Computational and Graphical Statistics, 26(2):434\u2013444, 2017.   \n[7] Arnaud Doucet, Michael Pitt, George Deligiannidis, and Robert Kohn. Efficient implementation of Markov chain Monte Carlo when using an unbiased likelihood estimator. Biometrika, 102(2):295\u2013313, 2015.   \n[8] Dougal Maclaurin and Ryan Adams. Firefly Monte Carlo: exact MCMC with subsets of data. In Conference on Uncertainty in Artificial Intelligence, 2014.   \n[9] Matias Quiroz, Minh-Ngoc Tran, Mattias Villani, Robert Kohn, and Khue-Dung Dang. The block-Poisson estimator for optimally tuned exact subsampling MCMC. Journal of Computational and Graphical Statistics, 30(4):877\u2013888, 2021.   \n[10] Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient Langevin dynamics. In International Conference on Machine Learning, 2011.   \n[11] Sungjin Ahn, Anoop Korattikara, and Max Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring. In International Conference on Machine Learning, 2012.   \n[12] Anoop Korattikara, Yutian Chen, and Max Welling. Austerity in MCMC land: cutting the MetropolisHastings budget. In International Conference on Machine Learning, 2014.   \n[13] Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In International Conference on Machine Learning, 2015.   \n[14] James Johndrow, Natesh Pillai, and Aaron Smith. No free lunch for approximate MCMC. arXiv:2010.12514, 2020.   \n[15] R\u00b4emi Bardenet, Arnaud Doucet, and Chris Holmes. On Markov chain Monte Carlo methods for tall data. Journal of Machine Learning Research, 18:1\u201343, 2017.   \n[16] Tigran Nagapetyan, Andrew Duncan, Leonard Hasenclever, Sebastian Vollmer, Lukasz Szpruch, and Konstantinos Zygalakis. The true cost of stochastic gradient Langevin dynamics. arXiv:1706.02692, 2017.   \n[17] Jack Baker, Paul Fearnhead, Emily Fox, and Christopher Nemeth. Control variates for stochastic gradient MCMC. Statistics and Computing, 29:599\u2013615, 2019.   \n[18] Christopher Nemeth and Paul Fearnhead. Stochastic gradient Markov Chain Monte Carlo. Journal of the American Statistical Association, 116(533):433\u2013450, 2021.   \n[19] Matias Quiroz, Robert Kohn, Mattias Villani, and Minh-Ngoc Tran. Speeding up MCMC by efficient data subsampling. Journal of the American Statistical Association, 114(526):831\u2013843, 2019.   \n[20] Matias Quiroz, Robert Kohn, and Khue-Dung Dang. Subsampling MCMC\u2014an introduction for the survey statistician. Sankhya: The Indian Journal of Statistics, 80-A:S33\u2013S69, 2018.   \n[21] David Blei, Alp Kucukelbir, and Jon McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.   \n[22] Zhenming Shun and Peter McCullagh. Laplace approximation of high dimensional integrals. Journal of the Royal Statistical Society: Series B, 57(4):749\u2013760, 1995.   \n[23] Peter Hall, Tung Pham, Matt Wand, and Shen S.J. Wang. Asymptotic normality and valid inference for gaussian variational approximation. The Annals of Statistics, 39(5):2502\u20132532, 2011.   \n[24] Aad van der Vaart. Asymptotic Statistics. Cambridge University Press, 2000.   \n[25] Yixin Wang and David Blei. Frequentist consistency of variational Bayes. Journal of the American Statistical Association, 114(527):1147\u20131161, 2018.   \n[26] Badr-Eddine Ch\u00b4erief-Abdellatif and Pierre Alquier. Consistency of variational Bayes inference for estimation and model selection in mixtures. Electronic Journal of Statistics, 12:2995\u20133035, 2018.   \n[27] Yun Yang, Debdeep Pati, and Anirban Bhattacharya. $\\alpha$ -variational inference with statistical guarantees. The Annals of Statistics, 2018.   \n[28] Pierre Alquier and James Ridgway. Concentration of tempered posteriors and of their variational approximations. The Annals of Statistics, 48(3):1475\u20131497, 2020.   \n[29] Zuheng Xu and Trevor Campbell. The computational asymptotics of Gaussian variational inference and the Laplace approximation. Statistics and Computing, 32(63), 2022.   \n[30] Jeffrey Miller. Asymptotic normality, concentration, and coverage of generalized posteriors. Journal of Machine Learning Research, 22:1\u201353, 2021.   \n[31] Cheng Zhang, Judith Bu\u00a8tepage, Hedvig Kjellstro\u00a8m, and Stephan Mandt. Advances in variational inference. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(8):2008\u20132026, 2019.   \n[32] Jonathan Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable Bayesian logistic regression. In Advances in Neural Information Processing Systems, 2016.   \n[33] Trevor Campbell and Tamara Broderick. Automated scalable Bayesian inference via Hilbert coresets. Journal of Machine Learning Research, 20(15):1\u201338, 2019.   \n[34] Trevor Campbell and Boyan Beronov. Sparse variational inference: Bayesian coresets from scratch. In Advances in Neural Information Processing Systems, 2019.   \n[35] Trevor Campbell and Tamara Broderick. Bayesian coreset construction via greedy iterative geodesic ascent. In International Conference on Machine Learning, 2018.   \n[36] Naitong Chen, Zuheng Xu, and Trevor Campbell. Bayesian inference via sparse Hamiltonian flows. In Advances in Neural Information Processing Systems, 2022.   \n[37] Cian Naik, Judith Rousseau, and Trevor Campbell. Fast Bayesian coresets via subsampling and quasiNewton refinement. In Advances in Neural Information Processing Systems, 2022.   \n[38] Martin Jankowiak and Du Phan. Surrogate likelihoods for variational annealed importance sampling. In International Conference on Machine Learning, 2022.   \n[39] Naitong Chen and Trevor Campbell. Coreset Markov chain Monte Carlo. In International Conference on Artificial Intelligence and Statistics, 2024.   \n[40] Thomas Cormen, Charles Leiserson, Ronald Rivest, and Clifford Stein. Introduction to Algorithms. The MIT Press, $4^{\\mathrm{th}}$ edition, 2022.   \n[41] Ping Ma, Michael Mahoney, and Bin Yu. A statistical perspective on algorithmic leveraging. Journal of Machine Learning Research, 16:861\u2013911, 2015.   \n[42] HaiYing Wang, Rong Zhu, and Ping Ma. Optimal subsampling for large sample logistic regression. Journal of the American Statistical Association, 113(522):829\u2013844, 2018.   \n[43] HaiYing Wang. More efficient estimation for logistic regression with optimal subsamples. Journal of Machine Learning Research, 20:1\u201359, 2019.   \n[44] Mingyao Ai, Jun Yu, Huiming Zhang, and HaiYing Wang. Optimal subsampling algorithms for big data regressions. Statistica Sinica, 31(2):749\u2013772, 2021.   \n[45] HaiYing Wang and Yanyuan Ma. Optimal subsampling for quantile regression in big data. Biometrika, 108(1):99\u2013112, 2021.   \n[46] Dan Feldman. Introduction to core-sets: an updated survey. arXiv:2011.09384, 2020.   \n[47] Alastair Walker. New fast method for generating discrete random numbers with arbitrary frequency distributions. Electronics Letters, 10(8):127\u2013128, 1974.   \n[48] Alastair Walker. An efficient method for generating discrete random variables with general distributions. ACM Transactions on Mathematical Software, 3(3):253\u2013256, 1977.   \n[49] Tim van Erven and Peter Harre\u00a8mos. Re\u00b4nyi divergence and Kullback-Leibler divergence. IEEE Transactions on Information Theory, 60(7):3797\u20133820, 2014.   \n[50] Roman Vershynin. High-dimensional probability: an introduction with applications in data science. Cambridge University Press, 2020.   \n[51] Igor Vajda. Note on discrimination information and variation. IEEE Transactions on Information Theory, 16(6):771\u2013773, 1970.   \n[52] David Pollard. A user\u2019s guide to probability theory. Cambridge series in statistical and probabilistic mathematics. Cambridge University Press, $7^{\\mathrm{th}}$ edition, 2002.   \n[53] Robert Keener. Theoretical statistics: topics for a core course. Springer, 2010.   \n[54] Andre Bulinski. Conditional central limit theorem. Theory of Probability & its Applications, 61(4):613\u2013631, 2017.   \n[55] Lorraine Schwartz. On Bayes procedures. Z. Wahrscheinlichkeitstheorie und Verwandte Gebiete, 4:10\u201326, 1965. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 3.1. By Vajda\u2019s inequality [51], ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{KL}(w)\\geq\\log\\frac{1+\\mathrm{TV}(\\pi,\\pi_{w})}{1-\\mathrm{TV}(\\pi,\\pi_{w})}-\\frac{2\\,\\mathrm{TV}(\\pi,\\pi_{w})}{1+\\mathrm{TV}(\\pi,\\pi_{w})}}\\\\ &{\\quad\\quad\\quad\\geq-\\log\\left(1-\\mathrm{TV}(\\pi,\\pi_{w})\\right)-\\mathrm{TV}(\\pi,\\pi_{w})}\\\\ &{\\quad\\quad\\quad\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The bound is monotone increasing in $\\mathrm{TV}(\\pi,\\pi_{w})$ ; therefore because the squared Hellinger distance satisfies the inequality [52, p. 61], ", "page_idx": 12}, {"type": "equation", "text": "$$\nH^{2}(\\pi,\\pi_{w})={\\frac{1}{2}}\\int\\left(\\sqrt{\\pi}-\\sqrt{\\pi_{w}}\\right)^{2}\\leq{\\frac{1}{2}}\\int|\\pi-\\pi_{w}|=\\mathrm{TV}(\\pi,\\pi_{w}),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "we have that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{\\mathrm{KL}}}(w)\\geq-\\log\\left(1-H^{2}(\\pi,\\pi_{w})\\right)-H^{2}(\\pi,\\pi_{w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "We substitute the value of the squared Hellinger distance to find that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\underline{{\\mathrm{KL}}}(w)\\geq-\\log\\!\\left(\\int\\sqrt{\\pi\\pi_{w}}\\right)+\\int\\sqrt{\\pi\\pi_{w}}-1\\geq0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\int\\sqrt{\\pi\\pi_{w}}\\le1}\\end{array}$ , so ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\underline{{\\mathrm{KL}}}(w)\\geq-\\log\\!\\left(\\operatorname*{min}\\{1,\\int\\sqrt{\\pi\\pi_{w}}\\}\\right)+\\operatorname*{min}\\{1,\\int\\sqrt{\\pi\\pi_{w}}\\}-1\\geq0.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The bound is monotone decreasing in $\\int\\sqrt{\\pi\\pi_{w}}$ , so we require an upper bound on $\\int\\sqrt{\\pi\\pi_{w}}$ . To obtain the required bound, we split the integral into two parts\u2014one on the set $B$ , and the other on $B^{c}$ \u2014and then use the Cauchy-Schwarz inequality to bound the part on $B^{c}$ . Note that by definition $\\pi$ and $\\pi_{w}$ are mutually dominating, so the density ratio $\\pi_{w}/\\pi$ is well-defined and measurable. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int\\sqrt{\\pi\\pi_{w}}=\\int_{B}\\sqrt{\\pi\\pi_{w}}+\\int_{B^{c}}\\sqrt{\\pi\\pi_{w}}}}\\\\ &{}&{=\\int_{B}\\sqrt{\\pi\\pi_{w}}+\\int\\sigma\\sqrt{\\frac{\\pi_{w}}{\\pi}}\\mathbb{1}_{B^{c}}}\\\\ &{}&{\\le\\int_{B}\\sqrt{\\pi\\pi_{w}}+\\sqrt{\\pi(B^{c})}}\\\\ &{}&{=\\frac{\\int_{B}\\pi_{0}\\exp{\\frac{1}{2}}(\\ell+\\ell_{w})}{\\sqrt{\\int\\pi_{0}\\exp(\\ell)\\int\\pi_{0}\\exp(\\ell_{w})}}+\\sqrt{\\pi(B^{c})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The result follows. ", "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 5.1. We first consider the forward KL divergence. By definition, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\pi||\\pi_{w})=\\displaystyle\\int\\pi(\\ell-\\ell_{w})+\\log\\frac{\\displaystyle\\int\\pi_{0}\\exp(\\ell_{w})}{\\displaystyle\\int\\pi_{0}\\exp(\\ell)}}\\\\ &{\\qquad\\qquad=\\displaystyle\\int\\pi(\\ell-\\ell_{w})+\\log\\displaystyle\\int\\pi\\exp(\\ell_{w}-\\ell).}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Since the KL is positive, for $\\lambda>0$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\pi||\\pi_{w})\\le\\displaystyle\\frac{1+\\lambda}{\\lambda}\\int\\pi(\\ell-\\ell_{w})+\\frac{1+\\lambda}{\\lambda}\\log\\int\\pi\\exp(\\ell_{w}-\\ell)}\\\\ &{\\qquad\\qquad\\le\\displaystyle\\frac{1+\\lambda}{\\lambda}\\int\\pi(\\ell-\\ell_{w})+\\frac{1}{\\lambda}\\log\\int\\pi\\exp((1+\\lambda)(\\ell_{w}-\\ell))}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac1{\\lambda}\\log\\int\\pi\\exp((1+\\lambda)(\\bar{\\ell}_{w}-\\bar{\\ell})),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "by Jensen\u2019s inequality. Next we consider the reverse KL divergence. For any $\\lambda\\neq0$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{KL}(\\pi_{w}||\\pi)=\\int\\pi_{w}(\\ell_{w}-\\ell)+\\log\\frac{\\int\\pi_{0}\\exp(\\ell)}{\\int\\pi_{0}\\exp(\\ell_{w})}}}\\\\ {{\\displaystyle\\qquad\\qquad=\\frac{1}{\\lambda}\\int\\pi_{w}\\lambda(\\ell_{w}-\\ell)-\\log\\int\\pi\\exp(\\ell_{w}-\\ell).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "By Jensen\u2019s inequality, for $\\lambda>0$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\pi_{w}||\\pi)\\le\\displaystyle\\frac{1}{\\lambda}\\log\\int\\pi_{w}\\exp(\\lambda(\\ell_{w}-\\ell))-\\log\\int\\pi\\exp(\\ell_{w}-\\ell)}\\\\ &{=\\displaystyle\\frac{1}{\\lambda}\\log\\frac{\\int\\pi\\exp((1+\\lambda)(\\ell_{w}-\\ell))}{\\int\\pi\\exp(\\ell_{w}-\\ell)}-\\log\\int\\pi\\exp(\\ell_{w}-\\ell)}\\\\ &{=\\displaystyle\\frac{1}{\\lambda}\\log\\int\\pi\\exp((1+\\lambda)(\\ell_{w}-\\ell))-\\frac{1+\\lambda}{\\lambda}\\log\\int\\pi\\exp(\\ell_{w}-\\ell)}\\\\ &{\\le\\displaystyle\\frac{1+\\lambda}{\\lambda}\\int\\pi(\\ell-\\ell_{w})+\\frac{1}{\\lambda}\\log\\int\\pi\\exp((1+\\lambda)(\\ell_{w}-\\ell))}\\\\ &{=\\displaystyle\\frac{1}{\\lambda}\\log\\int\\pi\\exp((1+\\lambda)(\\bar{\\ell}_{w}-\\bar{\\ell})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "This is the same bound as in the forward $\\mathrm{KL}$ divergence case. Since the bound applies for all $\\lambda>0$ , we can take the infimum. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 3.3. By replacing the integrals over the whole space $\\Theta$ in the denominator of $J_{B}(w)$ in Lemma 3.1 with integrals over the subset $B$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{KL}(w)\\geq-\\log\\operatorname*{min}(1,J_{B}(w))+\\operatorname*{min}(1,J_{B}(w))-1}\\\\ {\\geq{O}_{p}(1)-\\log J_{B}(w)}\\\\ {\\geq{O}_{p}(1)+\\operatorname*{min}\\Bigl\\{G_{B}(w),-\\log\\sqrt{\\pi(B^{c})}\\Bigr\\}}\\\\ {{G}_{B}(w)=-\\log\\int_{B}{\\pi_{0}}\\exp((1/2)(\\ell+\\ell_{w}))+\\frac{1}{2}\\log\\int_{B}{\\pi_{0}}\\exp(\\ell)+\\frac{1}{2}\\log\\int_{B}{\\pi_{0}}\\exp(\\ell_{w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "So to obtain the stated lower bound on the KL divergence, we require an upper bound on $\\begin{array}{r}{\\log\\int_{B}\\pi_{0}\\exp((1/2)(\\ell+}\\end{array}$ $\\ell_{w})$ ), and lower bounds on $\\log\\textstyle\\int_{B}\\pi_{0}\\exp(\\ell)$ and $\\begin{array}{r}{\\log\\int_{B}\\pi_{0}\\exp(\\ell_{w})}\\end{array}$ . By Taylor\u2019s theorem, Assumption 3.2, and the assumption on ${\\nabla}^{2}{\\boldsymbol{\\ell}}_{w}({\\boldsymbol{\\theta}})$ , for all $\\theta\\in B$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bigg|\\ell(\\theta)-\\ell(\\theta_{0})-g^{T}(\\theta-\\theta_{0})+\\frac{N}{2}(\\theta-\\theta_{0})^{T}H(\\theta-\\theta_{0})\\bigg|\\leq\\frac{N o_{p}\\left(1\\right)}{2}(\\theta-\\theta_{0})^{T}H(\\theta-\\theta_{0})}\\\\ {\\bigg|\\ell_{w}(\\theta)-\\ell_{w}(\\theta_{0})-g_{w}^{T}(\\theta-\\theta_{0})+\\frac{\\overline{{w}}}{2}(\\theta-\\theta_{0})^{T}H(\\theta-\\theta_{0})\\bigg|\\leq\\frac{\\overline{{w}}o_{p}\\left(1\\right)}{2}(\\theta-\\theta_{0})^{T}H(\\theta-\\theta_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We shift the exponential arguments in $G_{B}(w)$ by $(1/2)\\big(\\ell(\\theta_{0})+\\ell_{w}(\\theta_{0})\\big)$ , note that $\\pi_{0}$ is continuous and positive around $\\theta_{0}$ , and and apply the Taylor expansions in Eq. (7) to obtain an upper bound on the first term: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log\\int_{B}\\pi_{0}e^{\\frac{1}{2}(\\ell-\\ell(\\theta_{0})+\\ell_{w}-\\ell_{w}(\\theta_{0}))}\\leq O_{p}(1)+\\log\\int_{B}e^{\\frac{1}{2}((g+g_{w})^{T}(\\theta-\\theta_{0})-\\frac{(\\sqrt{\\kappa})(N+\\overrightarrow{w})}{4}(\\theta-\\theta_{0})^{T}H(\\theta-\\theta_{0})},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $(\\sim\\!1)$ denotes a quantity that converges in probability to $^{1}$ as $N\\to\\infty$ . We can transform variables to $x=C^{T}(\\theta-\\theta_{0})$ , where $H=C C^{T}$ is the Cholesky factorization of $H$ , and subsequently complete the square: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log\\int_{B}\\pi_{0}e^{\\frac{1}{2}(\\cdots)}\\leq O_{p}(1)+\\frac{(\\!\\sim\\!1)\\|C^{-1}(g+g_{w})\\|^{2}}{4(N+\\bar{w})}+\\log\\int_{\\|x\\|^{2}\\leq r^{2}}e^{-\\frac{(\\!\\sim\\!1)(N+\\bar{w})}{4}\\left\\|x-\\frac{(\\!\\sim\\!1)C^{-1}(g+g_{w})}{(N+\\bar{w})}\\right\\|^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can obtain lower bounds on the other two terms using a similar technique: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\log\\int_{B}\\pi_{0}e^{\\ell-\\ell(\\theta_{0})}\\geq O_{p}(1)+\\frac{(\\sim1)\\|C^{-1}g\\|^{2}}{2N}+\\log\\int_{\\|x\\|^{2}\\leq r^{2}}e^{-\\frac{(\\sim1)N}{2}\\left\\|x-\\frac{(\\sim1)C^{-1}g}{N}\\right\\|^{2}}}\\\\ &{\\log\\int_{B}\\pi_{0}e^{\\ell_{w}-\\ell_{w}(\\theta_{0})}\\geq O_{p}(1)+\\frac{(\\sim1)\\|C^{-1}g_{w}\\|^{2}}{2\\overline{{w}}}+\\log\\int_{\\|x\\|^{2}\\leq r^{2}}e^{-\\frac{(\\sim1)\\overline{{w}}}{2}\\left\\|x-\\frac{(\\sim1)C^{-1}g_{w}}{\\overline{{w}}}\\right\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "It remains to analyze the three $\\log\\int\\dots$ terms. We bound the integral term in Eq. (8) with the integral over the whole space: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log\\int_{\\|x\\|^{2}\\leq r^{2}}e^{-\\frac{(\\sim1)(N+\\overline{{w}})}{4}\\|\\ldots\\|^{2}}\\leq O_{p}(1)-\\frac{d}{2}\\log(N+\\overline{{w}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For the integral term in Eq. (9), note that since $N r^{2}=\\omega(1)$ and $\\|C^{-1}g/N\\|=O_{p}(N^{-1/2})$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\log\\int_{\\|x\\|^{2}\\leq r^{2}}e^{-\\frac{(\\sim1)N}{2}\\|\\ldots\\|^{2}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\log\\left(\\int e^{-\\frac{(\\sqrt{\\alpha+1})N}{2}(\\cdots)}-\\int_{\\|x\\|^{2}>r^{2}}e^{-\\frac{(\\sqrt{\\alpha+1})N}{2}\\|x-\\frac{(\\sqrt{\\alpha})C^{-1}g}{N}\\|^{2}}\\right)}\\\\ &{\\geq\\log\\left(\\left(\\frac{(\\sqrt{1})2\\pi}{N}\\right)^{d/2}-e^{-\\frac{(\\sqrt{\\alpha})N}{4}\\operatorname*{min}_{\\|x\\|\\geq r}\\left\\|x-\\frac{(\\sqrt{\\alpha})C^{-1}g}{N}\\right\\|^{2}}\\int e^{-\\frac{(\\sqrt{1})N}{4}\\|x-\\frac{(\\sqrt{1})C^{-1}g}{N}\\|^{2}}\\right)}\\\\ &{=\\log\\left(\\left(\\frac{(\\sqrt{1})2\\pi}{N}\\right)^{d/2}-e^{-\\frac{\\Omega_{p}(N r^{2})}{4}}\\left(\\frac{(\\sqrt{1})4\\pi}{N}\\right)^{d/2}\\right)}\\\\ &{=-\\frac{d}{2}\\log(N)+O_{p}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For the integral term in Eq. (10), we consider two cases: one where $\\bar{w}$ is large, and one where it is small. First assume $\\overline{{w}}r^{\\frac{\\gamma}{2}}>8d\\log{2}$ ; then by a similar technique as used in the first lower bound, since $\\|C^{-1}g_{w}/\\overline{{w}}\\|=$ $o_{p}(r)$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\int_{\\|x\\|^{2}\\leq r^{2}}e^{-\\frac{(\\sqrt{-1})\\overline{{\\alpha}}}{2}\\|\\cdot\\|^{2}}}\\\\ &{\\geq\\log\\left(\\left(\\frac{(\\sqrt{-1})2\\pi}{\\overline{{w}}}\\right)^{d/2}-e^{-\\frac{(\\sqrt{-1})\\overline{{\\alpha}}\\,\\operatorname*{min}_{\\|x\\|\\geq r}\\left\\|x-\\frac{(\\sqrt{-1})\\overline{{\\alpha}}\\,w}{\\overline{{w}}}\\right\\|^{2}}{4}\\int e^{-\\frac{(\\sqrt{-1})\\overline{{w}}}{4}\\|x-\\frac{(\\sqrt{-1})\\overline{{\\alpha}}^{-1}g_{w}}{\\overline{{w}}}\\|^{2}}\\right)}\\\\ &{\\geq\\log\\left(\\left(\\frac{(\\sqrt{-1})2\\pi}{\\overline{{w}}}\\right)^{d/2}-e^{-2d\\log2(\\sqrt{-1})}\\left(\\frac{(\\sqrt{-1})4\\pi}{\\overline{{w}}}\\right)^{d/2}\\right)}\\\\ &{\\geq-\\frac{d}{2}\\log\\overline{{w}}+O_{p}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "When $\\overline{{w}}r^{2}\\leq8d\\log2$ , we transform variables $y=x/r$ to find that since $\\|C^{-1}g_{w}/\\overline{{w}}\\|=o_{p}(r),$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\displaystyle\\int_{\\|x\\|^{2}\\leq r^{2}}e^{-\\frac{(\\sqrt{\\ldots1})\\overline{{\\omega}}}{2}\\|\\ldots\\|^{2}}=\\frac{d}{2}\\log r^{2}+\\log\\displaystyle\\int_{\\|y\\|^{2}\\leq1}e^{-\\frac{(\\sqrt{\\ldots1})\\overline{{\\omega}}r^{2}}{2}\\left\\|y-\\frac{(\\sqrt{\\ldots1})C^{-1}g_{w}}{r\\overline{{w}}}\\right\\|^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\displaystyle\\frac{d}{2}\\log r^{2}+\\log e^{-\\frac{8d\\log2(\\sqrt{\\ldots1})}{2}\\left(2+2\\left\\|\\frac{(\\sqrt{\\ldots1})C^{-1}g_{w}}{r\\overline{{w}}}\\right\\|^{2}\\right)}\\left(\\int_{\\|y\\|^{2}\\leq1}1\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\frac{d}{2}\\log r^{2}+O_{p}(1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore regardless of the value of $\\overline{{w}}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\log\\int_{\\|x\\|^{2}\\leq r^{2}}e^{-\\frac{(\\sqrt{\\ldots1})^{\\overline{{w}}}}{2}\\|\\ldots\\|^{2}}\\geq-\\frac{d}{2}\\log\\bigl(\\operatorname*{max}\\{\\overline{{w}},1/r^{2}\\}\\bigr)+O_{p}(1).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "So therefore combining all previous results, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{B}(w)\\geq O_{p}(1)+\\frac{(\\displaystyle{\\sim}1)}{4}\\bigg(\\frac{\\|C^{-1}g\\|^{2}}{N}+\\frac{\\|C^{-1}g_{w}\\|^{2}}{\\overline{{w}}}-\\frac{\\|C^{-1}(g+g_{w})\\|^{2}}{N+\\overline{{w}}}\\bigg)+\\frac{d}{4}\\log\\frac{(N+\\overline{{w}})^{2}}{N\\operatorname*{max}\\{\\overline{{w}},1/r^{2}\\}}}\\\\ &{\\qquad=O_{p}(1)+\\frac{(\\displaystyle{\\sim}1)}{4}\\bigg(\\frac{\\overline{{w}}\\|C^{-1}g\\|^{2}}{N(N+\\overline{{w}})}+\\frac{N\\|C^{-1}g_{w}\\|^{2}}{\\overline{{w}}(N+\\overline{{w}})}-\\frac{2g^{T}H^{-1}g_{w}}{N+\\overline{{w}}}\\bigg)+\\frac{d}{4}\\log\\frac{(N+\\overline{{w}})^{2}}{N\\operatorname*{max}\\{\\overline{{w}},1/r^{2}\\}}}\\\\ &{\\qquad=O_{p}(1)+\\frac{(\\displaystyle{\\sim}1)}{4}\\bigg(\\frac{N\\overline{{w}}}{N+\\overline{{w}}}\\bigg\\|\\frac{C^{-1}g}{N}-\\frac{C^{-1}g_{w}}{\\overline{{w}}}\\bigg\\|^{2}\\bigg)+\\frac{d}{4}\\log\\frac{(N+\\overline{{w}})^{2}}{N\\operatorname*{max}\\{\\overline{{w}},1/r^{2}\\}}}\\\\ &{\\qquad=O_{p}(1)+\\Omega_{p}(1)\\bigg(\\frac{N\\overline{{w}}}{N+\\overline{{w}}}\\bigg\\|\\frac{g}{N}-\\frac{g_{w}}{\\overline{{w}}}\\bigg\\|^{2}+d\\log\\frac{(N+\\overline{{w}})^{2}}{N\\operatorname*{max}\\{\\overline{{w}},1/r^{2}\\}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We now consider the minimum over $\\alpha\\geq0$ . Since neither $O_{p}(1)$ or $\\Omega_{p}(1)$ above depends on $\\overline{{w}}$ , we have that ", "page_idx": 14}, {"type": "text", "text": "\u03b1 $\\operatorname*{sin}_{\\varepsilon\\ge0}\\mathrm{KL}(\\alpha w)\\ge O_{p}(1)+\\Omega_{p}(1)\\operatorname*{min}\\biggl\\{-\\log\\pi(B^{c}),\\left(\\operatorname*{min}\\frac{N\\alpha\\bar{w}}{N+\\alpha\\bar{w}}\\bigg|\\frac{g}{N}-\\frac{g_{w}}{\\bar{w}}\\bigg|\\bigg|^{2}+d\\log\\frac{(N+\\alpha\\bar{w})^{2}}{N\\operatorname*{max}\\{\\alpha\\bar{w},1/r^{2}\\}}\\right)\\biggr\\}$ . On the $1/r^{2}$ branch of the objective function, the derivative in $\\alpha$ is always positive, and hence the minimum occurs at $\\alpha=0$ , and so ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha\\geq0}(\\cdot\\cdot\\cdot)\\geq d\\log(N r^{2}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "On the $\\alpha w$ branch of the objective function, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{sin}_{\\substack{\\tau\\ge0}}\\frac{N\\alpha\\overline{{w}}}{N+\\alpha\\overline{{w}}}\\bigg\\|\\frac{g}{N}-\\frac{g_{w}}{\\overline{{w}}}\\bigg\\|^{2}+d\\log\\frac{\\left(N+\\alpha\\overline{{w}}\\right)^{2}}{N\\alpha\\overline{{w}}}\\ge\\operatorname*{min}_{\\boldsymbol{\\alpha}\\ge0}\\frac{N\\alpha\\overline{{w}}}{N+\\alpha\\overline{{w}}}\\bigg\\|\\frac{g}{N}-\\frac{g_{w}}{\\overline{{w}}}\\bigg\\|^{2}+d\\log\\frac{\\left(N+\\alpha\\overline{{w}}\\right)}{N\\alpha\\overline{{w}}}+d\\log N.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For $a,b>0$ and $x\\geq0$ , the function $a x-b\\log x$ is convex in $x$ with minimum at $x^{\\star}=b/a$ , and so ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha\\geq0}(.\\,.\\,.\\,)\\geq d\\log\\!\\left(N\\Big\\|\\frac{g}{N}-\\frac{g_{w}}{\\overline{{w}}}\\Big\\|^{2}\\right)\\!.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By assumption, $\\begin{array}{r}{\\left\\|\\frac{g}{N}\\right\\|=o_{p}(r)}\\end{array}$ and $\\begin{array}{r}{\\left|\\frac{g_{w}}{\\overline{{w}}}\\right|\\right|=o_{p}(r)}\\end{array}$ , and hence the $\\alpha w$ branch has the asymptotic minimum: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\alpha\\geq0}\\frac{\\mathrm{KL}(\\alpha w)\\geq O_{p}(1)+\\Omega_{p}(1)\\operatorname*{min}\\biggl\\{-\\log\\pi(B^{c}),d\\log\\biggl(N\\Bigl\\|\\frac{g}{N}-\\frac{g_{w}}{\\overline{{w}}}\\Bigr\\|^{2}\\biggr)\\biggr\\}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 3.5. By Lemma 3.1, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\mathrm{KL}(w)}{\\mathrm{KL}(w)}\\ge-\\log\\operatorname*{min}(1,J_{B}(w))+\\operatorname*{min}(1,J_{B}(w))-1}\\\\ &{\\qquad\\qquad\\ge O_{p}(1)+\\operatorname*{min}\\Bigl\\{G_{B}(w),-\\log\\sqrt{\\pi(B^{c})}\\Bigr\\}}\\\\ &{G_{B}(w)=-\\log\\int_{B}\\pi_{0}\\exp((1/2)(\\ell+\\ell_{w}))+\\frac{1}{2}\\log\\int\\pi_{0}\\exp(\\ell)+\\frac{1}{2}\\log\\int\\pi_{0}\\exp(\\ell_{w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that $G_{B}$ in this proof is subtly different from the $G_{B}$ used in the proof of Theorem 3.3; the latter two integrals are over the whole space (directly from Lemma 3.1), rather than $B$ . We shift the exponential arguments in $G_{B}(w)$ by $(1/2)(\\ell(\\partial_{0}^{-})+\\ell_{w}(\\theta_{0}))$ . We first provide lower bounds on two of the integral terms via Assumption 3.4: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log\\int\\pi_{0}e^{\\ell-\\ell(\\theta_{0})}\\geq O_{p}(1)+\\log\\int e^{(g+g_{0})^{T}(\\theta-\\theta_{0})-\\frac{(\\sqrt{1})(N+1)L^{\\prime2}}{2}\\|\\theta-\\theta_{0}\\|^{2}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where (\u223c1) denotes a quantity that converges in probability to 1, g0 = \u2207log \u03c00(\u03b80), and L\u20322 = NLN2++1L20. Transforming variables via $x=L^{\\prime}(\\theta-\\theta_{0})$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\int\\pi_{0}e^{\\ell-\\ell(\\theta_{0})}\\geq O_{p}(1)+\\log\\int e^{(g+g_{0})^{T}x/L^{\\prime}-\\frac{(\\sqrt{1})(N+1)}{2}\\|x\\|^{2}}}&{}\\\\ {=O_{p}(1)+\\log\\int e^{-\\frac{(\\sqrt{1})(N+1)}{2}\\left\\|x-\\frac{g+g_{0}}{(N+1)L^{\\prime}}\\right\\|^{2}+\\frac{(\\sqrt{1})(N+1)}{2}\\|\\frac{g+g_{0}}{(N+1)L^{\\prime}}\\|^{2}}}&{}\\\\ {=O_{p}(1)+\\frac{(\\sim1)(N+1)}{2L^{\\prime2}}\\left\\|\\frac{g+g_{0}}{N+1}\\right\\|^{2}-\\frac{d}{2}\\log(N+1)}&{}\\\\ {\\geq O_{p}(1)+\\frac{(\\sim1)(N+1)}{2\\operatorname*{max}\\{L^{2},L_{0}^{2}\\}}\\left\\|\\frac{g+g_{0}}{N+1}\\right\\|^{2}-\\frac{d}{2}\\log(N+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\begin{array}{r}{L_{w}^{2}=\\frac{1}{\\overline{{w}}}\\sum_{n}w_{n}L_{n}^{2}}\\end{array}$ . Using the same technique, with $\\begin{array}{r}{L_{w}^{\\prime2}=\\frac{\\overline{{w}}L_{w}^{2}+L_{0}^{2}}{\\overline{{w}}+1}}\\end{array}$ and $x=L_{w}^{\\prime}(\\theta-\\theta_{0})$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{2g}\\int\\pi_{0}e^{\\ell_{w}-\\ell_{w}(\\theta_{0})}\\geq\\log\\int e^{(\\ell_{w}+\\varphi_{0})^{T}(\\theta-\\theta_{0})-\\frac{(\\sqrt{\\alpha})(\\frac{\\zeta+1}{2})}{2}L_{w}^{2}\\|\\theta-\\theta_{0}\\|^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq O_{P}(1)+\\frac{\\overline{{\\alpha}}}{2L_{w}^{2}}\\bigg\\|\\frac{g_{w}+g_{0}}{\\overline{{w}}+1}\\bigg\\|^{2}+\\log\\int e^{-\\frac{(\\pi+1)}{2}\\left\\|x-\\frac{g_{w}+g_{0}}{(\\overline{{w}}+1)L_{w}^{2}}\\right\\|^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\geq O_{P}(1)+\\frac{\\overline{{w}}+1}{2L_{w}^{2}}\\bigg\\|\\frac{g_{w}+g_{0}}{\\overline{{w}}+1}\\bigg\\|^{2}+\\log\\int_{\\|x-\\frac{g_{w}+g_{0}}{(\\overline{{w}}+1)L_{w}^{2}}\\|\\leq(\\overline{{w}}+1)-1/3}e^{-\\frac{\\alpha+1}{2}\\left\\|x-\\frac{g_{w}+g_{0}}{(\\overline{{w}}+1)L_{w}^{2}}\\right\\|^{2}}}\\\\ &{\\qquad\\qquad=O_{P}(1)+\\frac{\\overline{{w}}+1}{2L_{w}^{2}}\\bigg\\|\\frac{g_{w}+g_{0}}{\\overline{{w}}+1}\\bigg\\|^{2}-\\frac{d}{2}\\log(\\overline{{w}}+1)}\\\\ &{\\qquad\\qquad\\qquad\\geq O_{P}(1)+\\frac{\\overline{{w}}+1}{2\\operatorname*{max}\\{\\beta L^{2},L_{w}^{2}\\}}\\bigg\\|\\frac{g_{w}+g_{0}}{\\overline{{w}}+1}\\bigg\\|^{2}-\\frac{d}{2}\\log(\\overline{{w}}+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For the upper bound on the first term, we use a local quadratic expansion around $\\theta_{0}$ , where $\\textstyle H_{0}\\ =$ $-\\nabla^{2}\\log\\pi_{0}^{\\star}(\\theta_{0})$ , ", "page_idx": 15}, {"type": "text", "text": "log $\\begin{array}{r}{\\because_{0}e^{\\frac{1}{2}(\\ell-\\ell(\\theta_{0})+\\ell_{w}-\\ell_{w}(\\theta_{0}))}\\leq O_{p}(1)+\\log\\int_{B}e^{\\frac{1}{2}((g+g_{w}+2g_{0})^{T}(\\theta-\\theta_{0})-\\frac{(\\kappa)(N+\\varpi+2)}{4}(\\theta-\\theta_{0})^{T}\\left(\\frac{(N+\\infty\\varpi)H+2H_{0}}{N+\\varpi+2}\\right)(\\theta-\\theta_{0})}}\\end{array}$ ). B Because $H\\succ0$ , we have $(N+\\alpha\\overline{{w}})H+2H_{0}\\succ0$ eventually; we can transform variables to $x=C^{T}(\\theta-\\theta_{0})$ , where $\\begin{array}{r}{\\frac{(N+\\alpha\\overline{{w}})H+2H_{0}}{N+\\overline{{w}}+2}=C C^{T}}\\end{array}$ is the Cholesky factorization, and subsequently complete the square. Note that $\\sqrt{\\operatorname*{min}\\{\\operatorname*{min}(\\alpha,1)\\lambda_{\\operatorname*{min}}H,\\lambda_{\\operatorname*{min}}H_{0}\\}}\\leq\\lambda_{\\operatorname*{min}}C\\leq\\lambda_{\\operatorname*{max}}C\\leq\\sqrt{\\operatorname*{max}\\{\\operatorname*{max}(\\alpha,1)\\lambda_{\\operatorname*{max}}H,\\lambda_{\\operatorname*{max}}H_{0}\\}}^{-1}.$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\log|C|=O_{p}(1)\\qquad\\lambda_{\\mathrm{min}}C^{-1}H C^{-T}\\geq\\frac{\\lambda_{\\mathrm{min}}H}{\\operatorname*{max}\\{\\operatorname*{max}(\\alpha,1)\\lambda_{\\mathrm{max}}H,\\lambda_{\\mathrm{max}}H_{0}\\}}=\\eta>0,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and therefore ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{og}\\displaystyle\\int_{B}\\pi_{0}e^{\\frac{1}{2}(\\cdots)}}\\\\ &{\\le O_{p}(1)+\\frac{(\\sim1)(N+\\overline{{w}}+2)}{4}\\displaystyle\\left\\|\\frac{C^{-1}(g+g_{w}+2g_{0})}{N+\\overline{{w}}+2}\\right\\|^{2}+\\log\\displaystyle\\int_{\\|x\\|^{2}\\le r^{2}\\eta^{-1}}e^{-\\frac{(\\sim1)(N+\\overline{{w}}+2)}{4}\\left\\|x-\\frac{(\\sim1)C^{-1}(g+g_{w}+2)}{N+\\overline{{w}}+2}\\right\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Suppose first that $\\overline{{w}}+1\\leq N/(4\\|C^{-1}\\|^{2}\\operatorname*{max}\\{\\beta L^{2},L_{0}^{2}\\})$ . In this case we bound the integral in Eq. (11) by integrating over the whole space: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\log\\int_{B}\\pi_{0}e^{\\frac{1}{2}(\\cdots)}\\leq O_{p}(1)+\\frac{(\\sim1)\\|C^{-1}\\|^{2}(N+\\overline{{w}}+2)}{4}\\left\\|\\frac{g+g_{w}+2g_{0}}{N+\\overline{{w}}+2}\\right\\|^{2}-\\frac{d}{2}\\log(N+\\overline{{w}}+2).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining this with the previous results yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{B}(w)\\geq O_{p}(1)}\\\\ &{\\,-\\,\\frac{\\left(\\sqrt{\\ldots}\\right)\\left(N+\\overline{{w}}+2\\right)}{4}\\|C^{-1}\\|^{2}\\left\\|\\frac{\\mathcal{G}+\\,g_{w}+2g_{0}}{N+\\overline{{w}}+2}\\right\\|^{2}}\\\\ &{\\,+\\,\\frac{d}{4}\\log\\frac{\\left(N+\\overline{{w}}+2\\right)^{2}}{(N+1)(\\overline{{w}}+1)}+\\frac{\\overline{{w}}+1}{4\\operatorname*{max}\\left\\{\\beta L^{2},L_{0}^{2}\\right\\}}\\left\\|\\frac{g_{w}+\\,g_{0}}{\\overline{{w}}+1}\\right\\|^{2}+\\frac{(N+1)}{4\\operatorname*{max}\\left\\{L^{2},L_{0}^{2}\\right\\}}\\left\\|\\frac{g+\\,g_{0}}{N+1}\\right\\|^{2}}\\\\ &{\\,\\geq\\,O_{p}(1)+\\frac{d}{4}\\log\\frac{\\left(N+\\overline{{w}}+2\\right)^{2}}{(N+1)(\\overline{{w}}+1)}+\\frac{\\overline{{w}}+1}{4}\\left\\|\\frac{g_{w}+\\,g_{0}}{\\overline{{w}}+1}\\right\\|^{2}\\left(\\frac{1}{\\operatorname*{max}\\left\\{\\beta L^{2},L_{0}^{2}\\right\\}}-\\frac{2\\left\\|C^{-1}\\right\\|^{2}\\left(\\overline{{w}}+1\\right)}{N+\\overline{{w}}+2}\\right)}\\\\ &{\\,\\geq\\,O_{p}(1)+\\frac{d}{4}\\log\\frac{\\left(N+\\overline{{w}}+2\\right)^{2}}{(N+1)(\\overline{{w}}+1)}+\\frac{\\overline{{w}}+1}{8\\operatorname*{max}\\left\\{\\beta L^{2},L_{0}^{2}\\right\\}}\\left\\|\\frac{g_{w}+\\,g_{0}}{\\overline{{w}}+1}\\right\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Bounding the last term below by 0 and minimizing over $w$ such that $\\overline{{w}}\\leq\\sqrt{N}$ yields ", "page_idx": 16}, {"type": "equation", "text": "$$\nG_{B}(w)\\geq O_{p}(1)+\\frac{d}{4}\\log\\sqrt{N}=O_{p}(1)+\\frac{d}{8}\\log N.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Bounding $(N+\\overline{{w}}+2)/(N+1)\\geq1$ and minimizing over $w$ such that $\\overline{{w}}\\ge\\sqrt{N}$ yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{G_{B}(w)\\geq O_{p}(1)+\\displaystyle\\frac{d}{4}\\log N-\\frac{d}{4}\\log(\\overline{{w}}+1)+\\frac{\\overline{{w}}+1}{8\\operatorname*{max}\\{\\beta L^{2},L_{0}^{2}\\}}\\left\\|\\frac{g_{w}+g_{0}}{\\overline{{w}}+1}\\right\\|^{2}}\\\\ &{\\qquad\\,\\geq O_{p}(1)+\\displaystyle\\frac{d}{4}\\log N\\left\\|\\frac{g_{w}+g_{0}}{\\overline{{w}}+1}\\right\\|^{2}}\\\\ &{\\qquad=O_{p}(1)+\\displaystyle\\frac{d}{4}\\log N\\left\\|\\frac{g_{w}}{\\overline{{w}}}\\right\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second line follows because for $a,b>0$ and $x\\geq0$ , the function $a x-b\\log x$ is convex in $_x$ with minimum at $x^{\\star}=b/a$ . Therefore for $\\overline{{w}}+1\\leq N/(\\dots)$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{\\mathrm{KL}}}(w)\\geq O_{p}(1)+\\Omega_{p}(1)d\\log\\biggr(N\\operatorname*{min}\\biggr\\{\\left\\|\\frac{g_{w}}{\\overline{{w}}}\\right\\|^{2},1\\biggr\\}\\biggr).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next suppose $\\overline{{w}}+1\\ge N/(4\\|C^{-1}\\|^{2}\\operatorname*{max}\\{\\beta L^{2},L_{0}^{2}\\})$ . A second upper bound on Eq. (11) can be obtained by taking the maximum of the integrand over the integration region $\\|x\\|^{2}\\leq r^{2}$ . Note that since $\\|g_{w}/\\overline{{w}}\\|=\\omega_{p}(r)$ , $\\overline{{w}}=\\Omega_{p}(N),g/N=O_{p}(N^{-1/2})$ , and $N r^{2}=\\omega_{p}(1)$ , we have that $\\|(g+g_{w}+2g_{0})/(N+\\overline{{w}}+2)\\|=\\omega_{p}(r)$ , and so ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{og}\\displaystyle\\int_{B}\\pi_{0}e^{\\frac{1}{2}(\\cdots)}}\\\\ &{\\le O_{p}(1)+\\frac{(\\sqrt{-1})(N+\\overline{{w}}+2)}{4}\\Big\\|\\frac{C^{-1}(g+g_{w}+2g_{0})}{N+\\overline{{w}}+2}\\Big\\|^{2}-\\frac{(\\sqrt{-1})(N+\\overline{{w}}+2)}{4}\\Big(\\Big\\|\\frac{C^{-1}(g+g_{w}+2g_{0})}{N+\\overline{{w}}+2}\\Big\\|-r\\Big)}\\\\ &{=O_{p}(1)-\\frac{(\\sim1)(N+\\overline{{w}}+2)}{4}r^{2}+\\frac{(\\sim1)(N+\\overline{{w}}+2)r}{2}\\Big\\|\\frac{C^{-1}(g+g_{w}+2g_{0})}{N+\\overline{{w}}+2}\\Big\\|+\\frac{d}{2}\\log r^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "So therefore combining this result with the previous bounds and minimizing over $\\overline{{w}}$ yields ", "page_idx": 16}, {"type": "equation", "text": "$$\nG_{B}(w)\\geq O_{p}(1)+\\frac{(\\sim1)(N+\\overline{{w}}+2)}{4}r^{2}-\\frac{(\\sim1)(N+\\overline{{w}}+2)r}{2}\\left\\|\\frac{C^{-1}(g+g_{w}+2g_{0})}{N+\\overline{{w}}+2}\\right\\|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\displaystyle\\frac{d}{4}\\log((N+1)(\\overline{{w}}+1)r^{4})+\\frac{\\overline{{w}}+1}{4\\operatorname*{max}\\{\\beta L^{2},L_{0}^{2}\\}}\\Bigg\\|\\frac{g_{w}+g_{0}}{\\overline{{w}}+1}\\Bigg\\|^{2}+\\frac{(N+1)}{4\\operatorname*{max}\\{L^{2},L_{0}^{2}\\}}\\Bigg\\|\\frac{g+g_{0}}{N+1}\\Bigg\\|^{2}}\\\\ &{\\geq O_{p}(1)-\\displaystyle\\frac{d}{4}\\log(N r^{2})+\\frac{(\\sim1)N}{4}\\Big(\\Big\\|\\frac{g}{N}\\Big\\|-r\\Big)^{2}-\\displaystyle\\frac{d}{4}\\log(\\overline{{w}}r^{2})+\\frac{(\\sim1)\\overline{{w}}}{4}\\Big(\\Big\\|\\frac{g_{w}}{\\overline{{w}}}\\Big\\|-r\\Big)^{2}}\\\\ &{\\geq O_{p}(1)-\\displaystyle\\frac{d}{4}\\log(N r^{2})+\\frac{(\\sim1)}{4}N r^{2}-\\displaystyle\\frac{d}{4}\\log(r^{2})+\\frac{d}{4}\\log\\Big\\|\\frac{g_{w}}{\\overline{{w}}}\\Big\\|^{2}}\\\\ &{\\geq O_{p}(1)+\\displaystyle\\frac{d}{4}\\log N\\Big\\|\\frac{g_{w}}{\\overline{{w}}}\\Big\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Combining with the earlier bound and noting that $N\\operatorname*{min}\\{||g_{w}/\\overline{{w}}||,1\\}=\\omega_{p}(1)$ yields the final result. ", "page_idx": 17}, {"type": "text", "text": "Proof of Corollary 3.6. The proof follows directly from Theorems 3.3 and 3.5 by the data processing inequality applied to $\\underline{{\\mathrm{KL}}}(w)$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 5.3. By Lemma 5.1, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{\\mathrm{KL}}}(w)\\leq\\underset{\\lambda>0}{\\operatorname*{inf}}\\ \\frac{1}{\\lambda}\\log\\int\\pi\\exp\\big((1+\\lambda)(\\bar{\\ell}_{w}-\\bar{\\ell})\\big)}\\\\ &{\\qquad\\quad=\\underset{\\lambda>0}{\\operatorname*{inf}}\\ \\frac{1}{\\lambda}\\log\\int\\pi\\exp\\big(\\bar{\\ell}_{(1+\\lambda)(w-1)}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $(\\ell_{n})_{n=1}^{N}$ are $(f,A)$ -subexponential, if ", "page_idx": 17}, {"type": "equation", "text": "$$\n(1+\\lambda)^{2}{(w-1)}^{T}A(w-1)\\leq1,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int\\pi\\exp\\bigl(\\bar{\\ell}_{(1+\\lambda)(w-1)}\\bigr)\\leq\\exp\\Bigl(f\\bigl((1+\\lambda)^{2}(w-1)^{T}A(w-1)\\bigr)\\Bigr).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By assumption, the condition holds when $\\lambda=1$ ; the result follows. ", "page_idx": 17}, {"type": "text", "text": "Proof of Proposition 5.4. Let $\\begin{array}{r}{C(w)=\\log\\int\\pi\\exp(\\bar{\\ell}_{w})}\\end{array}$ . By the finiteness condition, [53, Theorem 2.4] asserts that $C(w)$ is continuous, and has derivatives of all orders that can be obtained by passing differentiation through the integral within the set $\\|w\\|_{2}\\,<\\,\\alpha$ . Let $U\\;=\\;\\mathrm{Cov}_{\\pi}((\\ell_{n})_{n=1}^{N})$ , and $\\bar{\\mathbf{\\Omega}}=\\mathbf{\\Omega}\\mathrm{span}\\{w\\ \\in\\ \\mathbb{R}^{N}$ $w^{T}(\\bar{\\ell}_{n})_{n=1}^{N}=0\\mathrm{~}\\pi\\!-\\!\\mathrm{a.s.}\\Bigr\\}$ . Note that $S=\\ker U$ : since $w^{T}U w=\\mathrm{Var}_{\\pi}(w^{T}(\\ell_{n})_{n=1}^{N})$ , $w^{T}U w=0$ if and only if $w^{T}(\\bar{\\ell}_{n})_{n=1}^{N}=0$ \u03c0-a.s.; and since $U$ is symmetric positive semidefinite, $w^{T}U w=0$ if and only if $w\\in\\ker U$ Therefore $C(w)$ is continuous, has derivatives of all orders, and derivatives can be passed through the integral within the set $\\{w\\,\\in\\,\\mathbb{R}^{N}\\,:\\,w\\,=\\,v+u,\\|v\\|_{2}\\,<\\,\\alpha/2,u\\,\\in\\,\\ker U\\}$ . For a vector $w\\,=\\,v\\,+\\,u,\\,v\\,\\perp\\,\\ker U$ $u\\in\\ker U$ , and minimum positive eigenvalue $\\lambda_{+}$ of $U$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nw^{T}U w\\leq\\frac{\\alpha^{2}\\lambda_{+}}{4}\\implies v^{T}U v\\leq\\frac{\\alpha^{2}\\lambda_{+}}{4}\\implies\\|v\\|_{2}\\leq\\frac{\\alpha}{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and so $C(w)$ is continuous, has derivatives of all orders, and derivatives can be passed through the integral within the set $\\begin{array}{r}{\\{w\\in\\mathbb{R}^{N}:w^{T}U w\\leq\\frac{\\alpha^{2}\\lambda_{+}}{4}\\}}\\end{array}$ \u03b124\u03bb+}. By Taylor\u2019s theorem, for any w in this set, there exists a distribution \u03bdw with density proportional to $\\pi\\exp(\\bar{\\ell}_{w^{\\prime}})$ for some $w^{\\prime}$ on the segment from the origin to $w$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\nC(w)=\\log\\int\\pi\\exp(\\bar{\\ell}_{w})=\\frac{1}{2}w^{T}U w+\\frac{1}{6}\\mathbb{E}_{\\nu_{w}}\\left[(w^{T}(\\bar{\\ell}_{n})_{n=1}^{N})^{3}\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By definition of $\\nu_{w}$ , $w\\in\\ker U$ implies that $w^{T}(\\bar{\\ell}_{n})_{n=1}^{N}=0\\,\\nu_{w^{-2}}$ .s. and hence $\\begin{array}{r}{\\frac{1}{6}\\mathbb{E}_{\\nu_{w}}\\left[({w^{T}(\\bar{\\ell}_{n})_{n=1}^{N}})^{3}\\right]=0}\\end{array}$ Therefore, for $\\begin{array}{r}{w^{T}U w\\leq\\frac{{\\alpha}^{2}{\\lambda}_{+}}{4}}\\end{array}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(w)\\leq\\displaystyle\\frac{1}{2}w^{T}U w\\left(1+\\operatorname*{max}_{w^{T}U w\\leq\\frac{\\alpha^{2}\\lambda_{+}}{4}}\\frac{1}{6}\\frac{\\mathbb{E}_{\\nu_{w}}\\left[(w^{T}(\\bar{\\ell}_{n})_{n=1}^{N})^{3}\\right]}{w^{T}U w}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\frac{1}{2}w^{T}U w}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{2}w^{T}U w\\left(1+\\operatorname*{max}_{\\|w\\|_{2}\\leq\\frac{\\alpha}{2}}\\frac{1}{6}\\frac{\\|w\\|_{2}\\|\\mathbb{E}_{\\nu_{w}}\\left[(\\bar{\\ell}_{n})_{n=1}^{N}\\otimes(\\bar{\\ell}_{n})_{n=1}^{N}\\otimes(\\bar{\\ell}_{n})_{n=1}^{N}\\right]\\|_{2}}{\\lambda_{+}}\\right)}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{1}{2}w^{T}U w\\left(1+\\frac{\\alpha}{12\\lambda_{+}}\\operatorname*{max}_{\\|w\\|_{2}\\leq\\frac{\\alpha}{2}}\\left\\|\\mathbb{E}_{\\nu_{w}}\\left[(\\bar{\\ell}_{n})_{n=1}^{N}\\otimes(\\bar{\\ell}_{n})_{n=1}^{N}\\otimes(\\bar{\\ell}_{n})_{n=1}^{N}\\right]\\right\\|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\otimes$ denotes outer products to form a tensor. By continuity of derivatives of all orders within the neighbourhood $\\|w\\|_{2}<\\alpha$ , the result follows by selecting a sufficiently small $\\alpha$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Proposition A.1. Suppose there exist $c\\in\\mathbb{R},\\,\\alpha,\\delta>0,$ , and $0<\\epsilon<1$ such that $\\ell\\leq c$ and for all coreset weights w satisfying $\\dot{\\alpha}w^{T}\\operatorname{Cov}_{\\pi}\\big((\\ell_{n})_{n=1}^{N}\\big)w\\,\\leq\\,1$ , $|\\bar{\\ell}_{w}|\\;\\leq\\;\\epsilon|\\ell-c|+\\delta$ . Then the potentials $(\\ell_{n})_{n=1}^{N}$ are $(h,\\alpha\\,\\mathrm{Cov}_{\\pi}((\\ell_{n})_{n=1}^{N}))$ -subexponential with $\\begin{array}{r}{h(x)=\\frac{1}{2}x+\\frac{e^{\\delta+c\\epsilon}}{\\int\\pi_{0}e^{\\epsilon\\ell}}x^{1-\\epsilon}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proof of Proposition A.1. Let $\\ell^{\\prime}=\\ell-c$ . Since $\\ell^{\\prime}\\leq0$ and $|\\bar{\\ell}_{w}|\\leq\\epsilon|\\ell^{\\prime}|+\\delta$ for some $\\epsilon<1,\\delta>0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\int\\tau\\exp(\\overline{{\\omega}}_{0}^{t})=1+\\frac{1}{2}\\int\\tau(\\overline{{\\omega}}_{0}^{t})^{2}+\\int\\tau\\frac{\\sqrt{1}}{\\omega_{0}^{2}}\\frac{\\left(1+\\delta\\right)^{2}(\\omega^{2}-1)(\\omega^{2})^{(1)-1}}{\\left(\\omega^{2}+1\\right)^{3}}}\\\\ {\\leq1+\\frac{1}{2}\\int\\tau(\\overline{{\\omega}}_{0}^{t})^{2}+\\int\\tau\\frac{\\sqrt{1}}{\\omega_{0}^{2}}\\frac{\\left(1+\\delta\\right)^{2}(\\omega^{2}+1)(\\omega^{2}+1)(\\omega^{2}+1)}{\\left(\\omega^{2}+1\\right)^{3}}}\\\\ {=1+\\frac{1}{2}\\int\\tau(\\overline{{\\omega}}_{0}^{t})^{2}+\\int\\tau\\frac{\\left(\\omega^{2}+1\\right)^{2}(\\omega^{2}+1-(\\epsilon(\\overline{{\\omega}}_{0}^{t})+\\delta\\right)^{2}(\\omega^{2}+1)}{\\left(\\omega^{2}+1\\right)^{3}}}\\\\ {\\leq1+\\frac{1}{2}\\int\\tau(\\overline{{\\omega}}_{0}^{t})^{2}+\\int\\omega\\kappa^{(6)+1}(\\omega^{2}+1)(\\omega^{2}+1)}\\\\ {=1+\\frac{1}{2}\\int\\tau(\\overline{{\\omega}}_{0}^{t})^{2}+\\frac{\\int\\kappa\\omega_{0}^{2}+1}{2}\\frac{\\left(1+\\delta\\right)^{2}(\\omega^{2}+1)(\\omega^{2}+1)}{\\left(\\omega^{2}+1\\right)^{3}}}\\\\ {\\leq1+\\frac{1}{2}\\int\\tau(\\overline{{\\omega}}_{0}^{t})^{2}+\\frac{\\int\\tau\\kappa\\omega_{0}^{2}+1}{2}\\frac{\\left(\\omega^{2}+1\\right)^{2}(\\omega^{2}+1)}{\\int\\tau\\omega_{0}^{2}}}\\\\ {=1+\\frac{1}{2}\\int\\tau(\\overline{{\\omega}}_{0}^{t})^{2}+\\epsilon^{\\prime}\\left(\\int\\tau(\\overline{{\\omega}}_{0}^{t})^{2}\\right)^{1-\\epsilon}}\\\\ {=1+\\frac{1}{2}\\int\\\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{h(x)=\\frac{1}{2}x+\\frac{e^{\\delta+c\\epsilon}}{\\int\\pi_{0}e^{\\epsilon\\ell}}x^{1-\\epsilon}}\\end{array}$ ", "page_idx": 18}, {"type": "text", "text": "Proposition A.2. Suppose $\\Theta=\\mathbb{R}^{d}$ , $\\bar{\\ell}$ is $G$ -strongly concave, and there exists $L<G$ , $\\alpha>0$ , and $\\theta_{0}\\in\\Theta$ such that for all coreset weights $w$ satisfying $\\alpha w^{T}\\stackrel{\\smile}{\\mathrm{Cov}}_{\\pi}((\\ell_{n})_{n=1}^{N})w\\leq1$ , $\\bar{\\ell}_{w}$ is $L$ -Lipschitz smooth, and both $\\|\\nabla\\ell_{w}(\\theta_{0})\\|$ and $\\bar{\\ell}_{w}(\\theta_{0})$ are bounded. Then for any $(L/G)<\\epsilon<1$ , there exists $c\\in\\mathbb{R},\\,\\delta>0$ such that the potentials $(\\ell_{n})_{n=1}^{N}$ are $(h,\\alpha\\,\\mathrm{Cov}_{\\pi}((\\ell_{n})_{n=1}^{N}))$ )-subexponential with the same $h$ as in Proposition A.1. ", "page_idx": 18}, {"type": "text", "text": "Proof of Proposition A.2. Since $\\bar{\\ell}$ is $G$ -strongly concave and $\\bar{\\ell}_{w}$ is $L$ -Lipschitz smooth, we can write ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\ell(\\theta)\\leq\\ell(\\theta_{0})+\\nabla\\ell(\\theta_{0})^{T}(\\theta-\\theta_{0})-\\frac{G}{2}\\|\\theta-\\theta_{0}\\|^{2}}}\\\\ {{\\displaystyle\\qquad=\\ell(\\theta_{0})+\\frac{G}{2}\\big\\|G^{-1}\\nabla\\ell(\\theta_{0})\\big\\|^{2}-\\frac{G}{2}\\big\\|\\theta-\\theta_{0}-G^{-1}\\nabla\\ell(\\theta_{0})\\big\\|^{2}}}\\\\ {{\\displaystyle|\\bar{\\ell}_{w}(\\theta)|\\leq|\\bar{\\ell}_{w}(\\theta_{0})+\\nabla\\ell_{w}(\\theta_{0})^{T}(\\theta-\\theta_{0})|+\\frac{L}{2}\\|\\theta-\\theta_{0}\\|^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So setting $\\begin{array}{r}{c=\\ell(\\theta_{0})+\\frac{G}{2}\\|G^{-1}\\nabla\\ell(\\theta_{0})\\|^{2}}\\end{array}$ implies $\\ell-c$ is a nonpositive function as required. Then ", "page_idx": 18}, {"type": "text", "text": "| $\\bar{\\ell}_{w}(\\theta)|-\\epsilon|\\ell(\\theta)-c|\\le|\\bar{\\ell}_{w}(\\theta_{0})|+\\frac{\\epsilon}{2G}\\|\\nabla\\ell(\\theta_{0})\\|^{2}+(\\|\\nabla\\ell_{w}(\\theta_{0})\\|+\\epsilon\\|\\nabla\\ell(\\theta_{0})\\|)\\|\\theta-\\theta_{0}\\|+\\frac{L-\\epsilon G}{2}\\|\\theta-\\theta_{0}\\|_{L^{2}},$ For $0<a<G-L$ , setting $\\textstyle\\epsilon={\\frac{L+a}{G}}$ and then maximizing over $\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{0}\\rVert$ yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\bar{\\ell}_{w}(\\theta)|-\\epsilon|\\ell(\\theta)-c|\\leq|\\bar{\\ell}_{w}(\\theta_{0})|+\\frac{\\epsilon}{2G}\\|\\nabla\\ell(\\theta_{0})\\|^{2}+(\\|\\nabla\\ell_{w}(\\theta_{0})\\|+\\epsilon\\|\\nabla\\ell(\\theta_{0})\\|)\\|\\theta-\\theta_{0}\\|-\\frac{a}{2}\\|\\theta-\\theta_{0}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq|\\bar{\\ell}_{w}(\\theta_{0})|+\\frac{\\epsilon}{2G}\\|\\nabla\\ell(\\theta_{0})\\|^{2}+\\frac{(\\|\\nabla\\ell_{w}(\\theta_{0})\\|+\\epsilon\\|\\nabla\\ell(\\theta_{0})\\|)^{2}}{2a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the boundedness of $\\bar{\\ell}_{w}(\\theta_{0})$ and $\\nabla\\ell_{w}(\\theta_{0})$ , maximizing over $w$ yields a value of $\\delta<\\infty$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma A.3. Let $X_{1},X_{2},\\dots$ be i.i.d. random variables in $\\mathbb{R}$ with $\\mathbb{E}X_{n}=0,$ , and define the resampled sum ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{N}=\\sum_{n=1}^{N}{\\frac{M_{n}}{M p_{n}}}X_{n}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\left(M1,\\dots,M_{N}\\right)\\sim\\mathrm{Multi}(M,\\left(p_{1},\\dots,p_{N}\\right))$ , with strictly positive resampling probabilities $p_{1},\\ldots,p_{N}$ that may depend on $X_{1},\\allowbreak\\cdot\\cdot,X_{N}$ and $N$ . If there exists a $\\delta>0$ such that as $N\\rightarrow\\infty$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{n}\\frac{|X_{n}|^{2+\\delta}}{(N p_{n})^{1+\\delta}}=O_{p}(1),\\quad\\frac{1}{N}\\sum_{n}\\frac{X_{n}^{2}}{N p_{n}}=\\Omega_{p}(1),\\quad a n d\\quad M\\rightarrow\\infty,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sqrt{M}\\frac{\\frac{1}{N}S_{N}-\\frac{1}{N}\\sum_{n}X_{n}}{\\sqrt{\\frac{1}{N}\\sum_{n}\\frac{X_{n}^{2}}{N p_{n}}}}\\overset{d}{\\to}\\mathcal{N}(0,1).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We can rewrite ", "page_idx": 19}, {"type": "equation", "text": "$$\nS_{N}=\\frac{1}{M}\\sum_{m=1}^{M}\\frac{X_{I_{m}}}{p_{I_{m}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $I_{m}$ ii\u223cdCategorical(p1, . . . , pN). Consider $S_{N}+B_{N}$ where $B_{N}$ is independent of $S_{N}$ , $B_{N}=\\pm1$ with probability $\\frac{1}{2(N M)^{1+\\delta}}$ , and $B_{N}=0$ otherwise. So if we set $\\mathcal{A}_{N}=\\sigma(X_{1},...\\,,\\bar{X}_{N})$ , [54, Cor. 3] asserts that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{S_{N}+B_{N}-\\mathbb{E}\\left[S_{N}|\\mathcal{A}_{N}\\right]}{\\sqrt{(N M)^{-(1+\\delta)}+\\mathrm{Var}\\left[S_{N}|\\mathcal{A}_{N}\\right]}}\\overset{d}{\\to}\\mathcal{N}(0,1)\\qquad N\\to\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as long as for all $N$ large enough, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left[\\frac{1}{M}\\frac{X_{I_{m}}}{p_{I_{m}}}|\\mathcal{A}_{N}\\right]<\\infty\\ \\ \\mathrm{a.s.},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and as $N\\rightarrow\\infty$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{(N M)^{-(1+\\delta)}+\\sum_{m=1}^{M}\\mathbb E\\left[\\left|\\frac{1}{M}\\frac{X_{I_{m}}}{p_{I_{m}}}-\\mathbb E\\left[\\frac{1}{M}\\frac{X_{I_{m}}}{p_{I_{m}}}\\middle|A_{N}\\right]\\right|^{2+\\delta}\\middle|A_{N}\\right]}{((N M)^{-(1+\\delta)}+\\mathrm{Var}\\left[S_{N}\\middle|A_{N}\\right])^{(2+\\delta)/2}}\\stackrel{p}{\\to}0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the conditional mean and variance have the form ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[S_{N}|A_{N}\\right]=\\mathbb{E}\\left[\\frac{X_{I_{m}}}{p_{I_{m}}}|A_{N}\\right]=\\sum_{n}{X_{n}}}\\\\ {\\displaystyle\\mathrm{Var}\\left[S_{N}|A_{N}\\right]=\\frac{1}{M}\\,\\mathrm{Var}\\left[\\frac{X_{I_{m}}}{p_{I_{m}}}|A_{N}\\right]=\\frac{1}{M}\\sum_{n}p_{n}\\left(\\frac{X_{n}}{p_{n}}-\\sum_{n}X_{n}\\right)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "iwmhpilcihe si iiess  ftinhiatte $\\begin{array}{r}{\\mathrm{Var}\\left[\\frac{1}{M}\\frac{X_{I_{m}}}{p_{I_{m}}}|\\boldsymbol{A}_{N}\\right]\\,<\\,\\infty}\\end{array}$ haa.ts., since $p_{1},\\ldots,p_{N}$ are strictly nonnegative and $\\mathbb{E}X_{n}\\,=\\,0$ $X_{n}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\left(N M\\right)^{-(1+\\delta)}+\\sum_{m=1}^{M}\\mathbb{E}\\left[\\left|\\frac{1}{M}\\frac{X_{I m}}{p_{I_{m}}}-\\mathbb{E}\\left[\\frac{1}{M}\\frac{X_{I m}}{p_{I_{m}}}\\middle|A_{N}\\right]\\right|^{2+\\delta}\\middle|A_{N}\\right]}{\\left(\\left(N M\\right)^{-(1+\\delta)}+\\mathrm{Var}\\left[S_{N}\\middle|A_{N}\\right]\\right)^{(2+\\delta)/2}}}\\\\ &{\\leq\\!\\!\\frac{\\left(N M\\right)^{-(1+\\delta)}+2^{2+\\delta}\\sum_{m=1}^{M}\\left(\\mathbb{E}\\left[\\left|\\frac{1}{M}\\frac{X_{I m}}{p_{I_{m}}}\\right|^{2+\\delta}\\middle|A_{N}\\right]+\\left|\\frac{1}{M}\\sum_{n}X_{n}\\right|^{2+\\delta}\\right)}{\\left(\\left(N M\\right)^{-(1+\\delta)}+\\mathrm{Var}\\left[S_{N}\\middle|A_{N}\\right]\\right)^{(2+\\delta)/2}}}\\\\ &{=\\!\\!M^{-\\delta/2}\\!\\frac{N^{-(3+2\\delta)}+2^{2+\\delta}\\left(\\frac{1}{N}\\sum_{n}\\frac{\\left|X_{n}\\right|^{2+\\delta}}{\\left(N p_{n}\\right)^{1+\\delta}}+\\left|\\frac{1}{N}\\sum_{n}X_{n}\\right|^{2+\\delta}\\right)}{\\left(M^{-\\delta}N^{-(3+\\delta)}+\\frac{1}{N}\\sum_{n}\\frac{X_{n}^{2}}{N p_{n}}-\\left(\\frac{1}{N}\\sum_{n}X_{n}\\right)^{2}\\right)^{(2+\\delta)/2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The above expression converges in probability to 0 by the technical assumptions in the statement of the result as well as the fact that $\\textstyle{\\frac{1}{N}}\\sum_{n}X_{n}\\ {\\overset{a.s.}{\\to}}\\ 0$ by the law of large numbers. Once again by the technical assumptions, $\\mathrm{Var}\\left[S_{N}|\\mathcal{A}_{N}\\right]=\\Omega_{p}(N^{2}/M)$ , so ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\frac{\\mathrm{Var}\\,[S_{N}|{\\cal A}_{N}]}{(N M)^{-(1+\\delta)}+\\mathrm{Var}\\,[S_{N}|{\\cal A}_{N}]}\\stackrel{p}{\\to}1\\,}\\\\ &{}&{\\frac{B_{N}}{(N M)^{-(1+\\delta)}+\\mathrm{Var}\\,[S_{N}|{\\cal A}_{N}]}\\stackrel{p}{\\to}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and hence by Slutsky\u2019s theorem, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{S_{N}-\\mathbb{E}\\left[S_{N}|\\mathcal{A}_{N}\\right]}{\\sqrt{\\operatorname{Var}\\left[S_{N}|\\mathcal{A}_{N}\\right]}}\\overset{d}{\\to}\\mathcal{N}(0,1)\\qquad N\\to\\infty.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using Slutsky\u2019s theorem again with ${\\begin{array}{r}{{\\frac{1}{N}}\\sum_{n}X_{n}\\ {\\stackrel{p}{\\to}}\\ 0}\\end{array}}$ and rearranging yields the final result. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.4. Suppose coreset weights are generated using the importance weighted construction in Algorithm 1. Let $g=\\nabla\\ell(\\eta_{0})$ , $g_{w}=\\nabla\\ell_{w}(\\eta_{0})$ , and $H=-\\mathbb{E}\\big[\\nabla^{2}\\ell_{n}(\\eta_{0})\\big]$ . If conditions $A(l-3)$ and (A6) in Section 4 hold, $M=o(N)$ , and $M=\\omega(1)$ , then ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\frac{g}{N}\\right\\|_{2}=\\Theta_{p}\\Big(N^{-1/2}\\Big),\\qquad\\left\\|\\frac{g_{w}}{\\overline{{w}}}\\right\\|_{2}=\\Theta_{p}\\left(M^{-1/2}\\right),\\qquad\\frac{\\overline{{w}}}{N}\\stackrel{p}{\\rightarrow}1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\|\\eta-\\eta_{0}\\|_{2}\\le r}\\left\\|-\\frac{1}{N}\\nabla^{2}\\ell(\\eta)-H\\right\\|_{2}\\overset{p}{\\to}0,\\qquad\\qquad\\operatorname*{sup}_{\\|\\eta-\\eta_{0}\\|_{2}\\le r}\\left\\|-\\frac{1}{\\overline{{w}}}\\nabla^{2}\\ell_{w}(\\eta)-H\\right\\|_{2}\\overset{p}{\\to}0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. First, since $\\begin{array}{r}{\\overline{{\\boldsymbol{w}}}=\\sum_{n}\\frac{\\boldsymbol{M}_{n}}{\\boldsymbol{M}p_{n}}}\\end{array}$ , $\\mathbb{E}\\overline{{w}}=N$ , and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\big[(\\bar{w}-N)^{2}\\big]=\\frac{N^{2}}{M^{2}}\\mathbb{E}\\Bigg[\\Bigg(\\sum_{n}M_{n}\\big((N p_{n})^{-1}-1\\big)\\Bigg)^{2}\\Bigg]}&{}\\\\ {=\\frac{N^{2}}{M^{2}}\\Bigg(\\sum_{n}\\big((N p_{n})^{-1}-1\\big)^{2}\\mathbb{E}M_{n}^{2}+\\underset{m\\neq n^{\\prime}}{\\sum_{n^{\\prime}}}\\big((N p_{n})^{-1}-1\\big)\\big((N p_{n^{\\prime}})^{-1}-1\\big)\\mathbb{E}\\big[M_{n}M_{n^{\\prime}}\\big]\\Bigg)}\\\\ &{=\\frac{N^{2}}{M}\\Bigg(\\sum_{n}\\big((N p_{n})^{-1}-1\\big)^{2}p_{n}-\\Bigg(\\underset{m^{\\prime}}{\\sum_{n^{\\prime}}}\\big(1/N-p_{n}\\big)\\Bigg)^{2}\\Bigg)}\\\\ &{=\\frac{1}{M}\\Bigg(\\sum_{n}p_{n}\\big(p_{n}^{-1}-N\\big)^{2}\\Bigg)}\\\\ &{\\leq\\frac{1}{M}\\Big(\\underset{m\\neq n}{\\sum_{n}}\\big(p_{n}^{-1}-N\\big)^{2}\\Big)}\\\\ &{\\leq\\frac{N^{2}}{M}Q(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last line follows by assumption A6. Therefore by Chebyshev\u2019s inequality and $M\\rightarrow\\infty$ , $\\varpi/N\\stackrel{p_{\\!}}{\\to}1$ . Since th\u221ae data are i.i.d., by conditions A1 and A2, the central limit theorem holds for the sum of $\\nabla\\ell_{n}(\\eta_{0})$ such that $g/\\sqrt{N}$ converges in distribution to a normal, and hence $\\begin{array}{r}{\\left\\|\\frac{g}{N}\\right\\|=\\Theta_{p}(N^{-1/2})}\\end{array}$ . By conditions A1, A2, and A6, Lemma A.3 holds such that for any $t\\in\\mathbb{R}^{d}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sqrt{M}\\frac{\\frac{1}{N}t^{T}g_{w}-\\frac{1}{N}t^{T}g}{\\sqrt{\\frac{1}{N}\\sum_{n}\\frac{(t^{T}\\nabla\\ell_{n}(\\eta_{0}))^{2}}{N p_{n}}}}=\\Theta_{p}(1).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since condition A6 asserts that $C>N p_{n}\\geq c>0$ , the law of large numbers, condition A1, and $M/N\\to0$ imply that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{M}}{N}t^{T}g_{w}=\\Theta_{p}(1).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing over a basis of vectors $t_{1},\\ldots,t_{d}$ shows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{M}}{N}\\|g_{w}\\|_{2}=\\Theta(1)\\sqrt{M}\\Big\\|\\frac{g_{w}}{\\overline{{{w}}}}\\Big\\|_{2}\\Theta_{p}(1).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This completes the first three results. Next, by condition A3, for sufficiently large $N$ such that the neighbourhood contains the ball of radius $r$ around $\\eta_{0}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{sup}_{\\|\\eta-\\eta_{0}\\|_{2}\\leq r}\\left\\|\\frac{1}{N}\\nabla^{2}\\ell(\\eta)-\\frac{1}{N}\\nabla^{2}\\ell(\\eta_{0})\\right\\|_{2}\\leq r\\frac{1}{N}\\sum_{n}R(X_{n})}&{}\\\\ {\\displaystyle\\operatorname*{sup}_{\\|\\eta-\\eta_{0}\\|_{2}\\leq r}\\left\\|\\frac{1}{N}\\nabla^{2}\\ell_{w}(\\eta)-\\frac{1}{N}\\nabla^{2}\\ell_{w}(\\eta_{0})\\right\\|_{2}\\leq r\\frac{1}{N}\\sum_{n}w_{n}R(X_{n}),}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[r\\frac{1}{N}\\sum_{n}R(X_{n})\\right]=\\mathbb{E}\\left[r\\frac{1}{N}\\sum_{n}w_{n}R(X_{n})\\right]=r\\mathbb{E}\\left[R(X)\\right]\\to0,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "so that we have that both ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\substack{||\\eta-\\eta_{0}||_{2}\\le r}}\\left\\|\\frac{1}{N}\\nabla^{2}\\ell(\\eta)-\\frac{1}{N}\\nabla^{2}\\ell(\\eta_{0})\\right\\|_{2}\\overset{p}{\\to}0\\quad\\mathrm{and}\\quad\\operatorname*{sup}_{\\substack{||\\eta-\\eta_{0}||_{2}\\le r}}\\left\\|\\frac{1}{N}\\nabla^{2}\\ell_{w}(\\eta)-\\frac{1}{N}\\nabla^{2}\\ell_{w}(\\eta_{0})\\right\\|_{2}\\overset{p}{\\to}0\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "by Markov\u2019s inequality. Finally, by the bounded variance in A2, sampling probability bounds in A6, and $M\\to\\infty$ , the variances of $\\begin{array}{r}{\\frac{1}{N}\\nabla^{2}\\ell_{w}(\\dot{\\eta}_{0})}\\end{array}$ and $\\scriptstyle{\\frac{1}{N}}\\nabla^{2}\\ell(\\eta_{0})$ both converge to 0 as $N\\rightarrow\\infty$ , and since both of these quantities are unbiased estimates of $\\mathbb{E}\\big[\\nabla^{2}\\ell_{n}\\big(\\eta_{0}\\big)\\big]$ , Chebyshev\u2019s inequality yields the desired convergence in probability. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Lemma A.5. Suppose $(X_{n})_{n=1}^{N}$ are $N$ i.i.d. random vectors in $\\mathbb{R}^{d}$ . Fix $M\\in\\mathbb{N}$ , $M\\,<\\,d$ and define $X=\\left[X_{1}\\quad X_{2}\\quad.\\ldots\\quad X_{M}\\right]\\in\\mathbb{R}^{d\\times M}$ . If there exists $\\delta>0$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Big[(1^{T}(X^{T}X)^{-1}1)^{M+\\delta}\\Big]<\\infty,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where 1 denotes a vector of all $^{\\,l}$ entries, then as $N\\rightarrow\\infty$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\operatorname*{min}_{w\\in\\mathbb{R}_{+}^{N}}\\left\\|\\frac{\\sum_{n=1}^{N}w_{n}X_{n}}{\\sum_{n=1}^{N}w_{n}}\\right\\|^{2}\\right)=\\omega_{p}\\Bigg(N^{-\\frac{M+\\delta/2}{M+\\delta}}\\Bigg).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. For any $\\epsilon>0$ , by the union bound over subsets of $[N]$ of size $M$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\Bigg(\\underset{w\\in\\mathbb{R}_{+}^{N}}{\\operatorname*{min}}\\cdots\\leq\\epsilon\\Bigg)\\leq\\Bigg(N\\Bigg)\\mathbb{P}\\Bigg(\\underset{w\\in\\mathbb{R}^{M}}{\\operatorname*{min}}\\ \\frac{w^{T}X^{T}X w}{w^{T}\\mathrm{1}^{T}W}\\leq\\epsilon\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Bigg(M\\Bigg)\\mathbb{P}\\Bigg(\\underset{\\lambda}{\\operatorname*{max}}\\ \\underset{w\\in\\mathbb{R}^{M}}{\\operatorname*{min}}w^{T}X^{T}X w-\\lambda(1^{T}w-1)\\leq\\epsilon\\Bigg)}\\\\ &{\\qquad\\qquad=\\Bigg(M\\Bigg)\\mathbb{P}\\Bigg(\\underset{\\lambda}{\\operatorname*{max}}\\ \\lambda-\\frac{\\lambda^{2}}{4}\\mathrm{1}^{T}(X^{T}X)^{-1}\\mathtt{1}\\leq\\epsilon\\Bigg)}\\\\ &{\\qquad\\qquad=\\Bigg(M\\Bigg)\\mathbb{P}\\Big(\\mathrm{1}^{T}(X^{T}X)^{-1}\\mathtt{1}\\geq\\epsilon^{-1}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By Markov\u2019s inequality and $\\binom{N}{M}\\leq(e N/M)^{M}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\bigg(\\underset{w\\in\\mathbb{R}_{+}^{N}}{\\operatorname*{min}}\\cdots\\leq\\epsilon\\bigg)\\leq\\bigg(\\frac{e N}{M}\\bigg)^{M}\\epsilon^{M+\\delta}\\mathbb{E}\\bigg[(1^{T}(X^{T}X)^{-1}1)^{M+\\delta}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad=\\bigg(\\frac{e N\\epsilon^{\\frac{M+\\delta}{M}}}{M}\\bigg)^{M}\\mathbb{E}\\Big[(1^{T}(X^{T}X)^{-1}1)^{M+\\delta}\\Big].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Setting $\\epsilon=N^{-\\frac{M+\\delta/2}{M+\\delta}}$ yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\operatorname*{min}_{w\\in\\mathbb{R}_{+}^{N}}\\cdot\\cdot\\cdot\\leq N^{-\\frac{M+\\delta/2}{M+\\delta}}\\Bigg)\\leq\\Bigg(\\frac{e N^{-\\frac{\\delta}{2M}}}{M}\\Bigg)^{M}\\mathbb{E}\\Big[(1^{T}(X^{T}X)^{-1}1)^{M+\\delta}\\Big].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The right-hand side converges to 0 as $N\\rightarrow\\infty$ , yielding the stated result. ", "page_idx": 21}, {"type": "text", "text": "Proof of Corollary 4.1 and Corollary 4.2. Set $r\\,=\\,(\\log M)^{-1/2}$ . Then since $M\\,=\\,o(N)$ , $M=\\omega(1)$ , and assumptions (A1-3) and (A6) hold, Lemma A.4 holds. Note that $\\|g_{w}/\\overline{{w}}\\|=\\Theta_{p}(M^{-1/2})=o_{p}(r)$ , $\\eta\\pi_{0}$ is positive at $\\eta_{0}$ and twice differentiable by (A4), and $N r^{2}=N/\\log M=\\stackrel{\\cdot\\cdot}{\\omega}(1)$ since $M=o(N)$ . Thus the conditions of Theorem 3.3 are verified. Substitution into the right term in the minimum of Theorem 3.3 yields the stated lower bound of $\\Omega_{p}(N/M)$ . For the left term in Theorem 3.3, define $B=\\{(\\eta-\\eta_{0})^{T}H(\\eta-\\dot{\\eta_{0}})\\leq r^{2}\\}$ . Then since $H\\,\\succ\\,0$ , $r\\,\\rightarrow\\,0$ , and $r^{2}\\,=\\,1/\\log M\\,=\\,\\omega(\\log N/N)$ , (A5) guarantees that $-\\log(\\eta\\pi)(B^{c})\\,=$ $\\Omega_{p}(N r^{2})=\\Omega_{p}(N/\\log M)$ . Therefore the minimum is $\\Omega_{p}(N/M)$ , and we complete the proof by transferring from $\\underline{{\\mathrm{KL}}}(w)$ on the $\\eta$ -pushforward model to $\\underline{{\\mathrm{KL}}}(w)$ on the original model using Corollary 3.6. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "Proof of Corollary 4.3. Fix the $\\delta~>~0$ guaranteed by (A8), and set $r\\;=\\;N^{-\\frac{M+3\\delta/4}{2(M+\\delta)}}$ . Note that $N r^{2}\\;=\\;$ $N^{\\frac{\\delta/4}{M+\\delta}}=\\omega(1)$ , $\\eta\\pi_{0}$ is positive at $\\eta_{0}$ and twice differentiable by (A4), and by (A1-3) the results pertaining to $\\left\\Vert{\\frac{g}{N}}\\right\\Vert_{2}$ and $\\begin{array}{r}{\\operatorname*{sup}_{\\lVert\\eta-\\eta_{0}\\rVert_{2}\\leq r}\\lVert-\\frac{1}{N}\\dot{\\nabla^{2}}\\ell(\\eta)-H\\rVert_{2}}\\end{array}$ in Lemma A.4 hold; thus Assumption 3.2 holds. By (A7), Assumption 3.4 holds as well as the conditions on $\\scriptstyle{\\frac{1}{w}}\\nabla^{2}\\ell_{w}(\\theta)$ and $\\textstyle{\\frac{1}{w}}\\sum_{n}w_{n}L_{n}^{2}$ in Theorem 3.5. Finally by (A8), Lemma A.5 holds such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\bigg\\|\\frac{g_{w}}{\\overline{{w}}}\\bigg\\|^{2}=\\omega_{p}\\bigg(N^{-\\frac{M+\\delta/2}{M+\\delta}}\\bigg),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and hence $\\begin{array}{r}{\\left|\\left|\\frac{g_{w}}{\\overline{{w}}}\\right|\\right|=\\omega_{p}(r)}\\end{array}$ . Therefore all conditions of Theorem 3.5 hold. For the left term in the minimum in Theorem 3.5, define $B=\\{(\\eta-\\eta_{0})^{T}H(\\eta-\\eta_{0})\\leq r^{2}\\}$ . Then since $H\\succ0$ , $r\\rightarrow0$ , and r2 = N $\\begin{array}{r}{r^{2}=N^{-\\frac{M+3\\delta/4}{M+\\delta}}=}\\end{array}$ $\\omega(\\log N/N)$ , (A5) guarantees that $-\\log(\\eta\\pi)(B^{c})=\\Omega_{p}(N r^{2})=\\Omega_{p}\\Bigg(N^{\\frac{\\delta/4}{M+\\delta}}\\Bigg)$ For the right term, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\log\\biggl(N\\Bigl\\|\\frac{g_{w}}{\\overline{{{w}}}}\\Bigr\\|^{2}\\biggr)=\\Omega_{p}\\biggl(\\log N^{1-\\frac{M+\\delta/2}{M+\\delta}}\\biggr)=\\Omega_{p}(\\log N).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The minimum of these two is from the right term, so ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underline{{\\mathrm{KL}}}(w)=\\Omega_{p}(\\log N).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We complete the proof by transferring from $\\underline{{\\mathrm{KL}}}(w)$ on the $\\eta$ -pushforward model to $\\underline{{\\mathrm{KL}}}(w)$ on the original model using Corollary 3.6. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Proposition A.6. The models specified in Eqs. (2) and (3) satisfy assumptions (A1-5). ", "page_idx": 22}, {"type": "text", "text": "Proof. The exact same technique applies to both models, so here we will just demonstrate it for the Cauchy model. In the Cauchy model, $\\theta\\in\\mathbb{R}$ , $\\eta:\\mathbb{R}\\to\\mathbb{R}_{+}$ , $\\eta(\\theta)=\\theta^{2}$ , and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{\\ell_{n}(\\eta)=-\\log\\pi-\\log({(Z_{n}-\\eta)^{2}}+1)\\quad\\quad}}&{{\\quad\\nabla\\ell_{n}(\\eta)=\\displaystyle\\frac{2(Z_{n}-\\eta)}{(Z_{n}-\\eta)^{2}+1}}}\\\\ {{\\nabla^{2}\\ell_{n}(\\eta)=\\displaystyle\\frac{2(Z_{n}-\\eta)^{2}-2}{((Z_{n}-\\eta)^{2}+1)^{2}}\\quad\\quad}}&{{\\quad\\nabla^{3}\\ell_{n}(\\eta)=\\displaystyle\\frac{4((Z_{n}-\\eta)^{2}-3)(\\eta-Z_{n})}{((\\eta-Z_{n})^{2}+1)^{3}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $Z_{n}=X_{n}$ . Property (A1) holds by routine interchange of differentiation and integration. Property (A2) holds (for any $\\delta>0$ ) because $\\nabla\\ell_{n}(\\eta)$ and $\\nabla^{2}\\ell_{n}(\\eta)$ are bounded functions of $\\eta$ and $X_{n}$ jointly. Property (A3) holds (for any neighbourhood of $\\eta_{0}$ ) because $\\nabla^{3}\\ell_{n}(\\boldsymbol{\\eta})$ is a bounded function of $\\eta$ and $X_{n}$ jointly. Property (A4) holds because the pushforward of Cauchy $(0,1)$ through $\\eta(\\theta)=\\theta^{2}$ has full support on $\\mathbb{R}_{+}$ . In order to verify assumption (A5), suppose there exists a sequence of bounded measurable functions $\\phi_{r}(Z_{1},\\ldots,Z_{N})\\in[0,1]$ of the data and constants $c,c^{\\prime}>0$ such that for all $r\\rightarrow0$ , $r^{2}=\\omega(\\log N/N)$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\eta_{0}}\\phi_{r}=O\\Big(e^{-c N r^{2}}\\Big)\\quad\\mathrm{and}\\quad\\underset{\\|\\eta-\\eta_{0}\\|>r}{\\operatorname*{sup}}\\mathbb{E}_{\\eta}(1-\\phi_{r})=O\\Big(e^{-c^{\\prime}N r^{2}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The functions $\\phi_{r}$ are similar to the test functions of Schwartz [55]. Then defining $\\mu=\\eta\\pi$ and $\\mu_{0}=\\eta\\pi_{0}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu(\\|\\eta-\\eta_{0}\\|>r)=\\phi_{r}\\mu(\\|\\eta-\\eta_{0}\\|>r)+(1-\\phi_{r})\\mu(\\|\\eta-\\eta_{0}\\|>r)}\\\\ &{\\qquad\\qquad\\leq\\phi_{r}+(1-\\phi_{r})\\mu(\\|\\eta-\\eta_{0}\\|>r)}\\\\ &{\\qquad\\qquad\\qquad=\\phi_{r}+\\frac{\\int_{\\|\\eta-\\eta_{0}\\|>r}(1-\\phi_{r})e^{\\ell(\\eta)-\\ell(\\eta_{0})}\\mu_{0}}{\\int e^{\\ell(\\eta)-\\ell(\\eta_{0})}\\mu_{0}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the same proof technique as in Theorem 3.3, the denominator satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\log\\int e^{\\ell(\\eta)-\\ell(\\eta_{0})}\\mu_{0}(\\mathrm{d}\\eta)\\geq-\\frac{d}{2}\\log N+O_{p}(1).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By assumption, there exists $c>0$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\eta_{0}}[\\phi_{r}]=O\\Big(e^{-c N r^{2}}\\Big)\\implies\\phi_{r}=O_{p}\\big(e^{-c N r^{2}}\\big),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and a $c^{\\prime}>0$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\eta_{0}}\\left[\\int_{\\|\\eta-\\eta_{0}\\|>r}(1-\\phi_{r})e^{\\ell(\\eta)-\\ell(\\eta_{0})}\\mu_{0}\\right]=\\int_{\\|\\eta-\\eta_{0}\\|>r}\\mathbb{E}_{\\eta}(1-\\phi_{r})\\mu_{0}}&{}\\\\ {\\leq\\underbracket{\\textup{s u p}}_{\\|\\eta-\\eta_{0}\\|>r}\\mathbb{E}_{\\eta}(1-\\phi_{r})}&{}\\\\ {=O(e^{-c N r^{2}})\\implies\\int_{\\|\\eta-\\eta_{0}\\|>r}(\\cdot\\,\\cdot\\,)=O_{p}\\Big(e^{-c N r^{2}}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore $\\mu(\\|\\eta-\\eta_{0}\\|\\ge r)=O_{p}\\Big(e^{-c N r^{2}}+N^{d/2}e^{-c^{\\prime}N r^{2}}\\Big)=O_{p}\\Big(e^{(d/2)\\log N-c^{\\prime\\prime}N r^{2}}\\Big)$ ; and since $r^{2}=$ $\\omega(\\log N/N),-\\log\\mu(\\|\\eta-\\eta_{0}\\|\\geq r)=\\Omega_{p}(N r^{2})$ as required by (A5). So to complete the proof of (A5) we need to find a suitable $\\phi_{r}$ . Fix $\\epsilon>0$ , and set ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi_{r}(Z_{1},\\ldots,Z_{N})=\\Im\\left[P_{\\eta_{0}}(|Z-\\eta_{0}|\\leq1)-\\frac{1}{N}\\sum_{n=1}^{N}\\Im[|Z_{n}-\\eta_{0}|\\leq1]>\\epsilon r\\right].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Under $p_{\\eta_{0}}$ , Hoeffding\u2019s inequality yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\eta_{0}}\\phi_{r}\\leq e^{-2N\\epsilon^{2}r^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "And under $p_{\\eta}$ for $\\|\\eta-\\eta_{0}\\|>r$ , for small enough $\\epsilon>0$ , $\\begin{array}{r}{P_{\\eta_{0}}(|Z-\\eta_{0}|\\le1)-P_{\\eta}(|Z-\\eta_{0}|\\le1)\\ge2\\epsilon r.}\\end{array}$ Therefore ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}_{\\eta}[1-\\phi_{r}(Z_{1},\\ldots,Z_{N})]=\\mathrm{Pr}_{\\eta}\\Bigg(P_{\\eta_{0}}(|Z-\\eta_{0}|\\leq1)-\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\mathbb{1}[|Z_{n}-\\eta_{0}|\\leq1]\\leq\\epsilon r\\Bigg)}\\\\ &{}&{\\leq\\mathrm{Pr}_{\\eta}\\Bigg(P_{\\eta}(|Z-\\eta_{0}|\\leq1)-\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\mathbb{1}[|Z_{n}-\\eta_{0}|\\leq1]\\leq-\\epsilon r\\Bigg)}\\\\ &{}&{=\\mathrm{Pr}_{\\eta}\\Bigg(\\displaystyle\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{1}[|Z_{n}-\\eta_{0}|\\leq1]-P_{\\eta}(|Z-\\eta_{0}|\\leq1)\\geq\\epsilon r\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "at which point we can again apply Hoeffding\u2019s inequality, completing the result. ", "page_idx": 23}, {"type": "text", "text": "Lemma A.7. Fix vectors $u,u_{1},\\dotsc,u_{N}$ in a separable Hilbert space with inner product denoted $a\\cdot b$ and norm denoted $\\parallel\\parallel$ . Let $v_{1},\\ldots,v_{M}$ be drawn from $\\{u_{1},\\dotsc,u_{N}\\}$ with probabilities $p_{1},\\ldots,p_{N}$ either with or without replacement ${\\bf\\nabla}i f$ without replacement, the probabilities are renormalized after every draw). Then for all $\\epsilon\\geq0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{min}_{w\\geq0}\\left|\\sum_{m=1}^{M}w_{m}v_{m}-u\\right|\\right|^{2}>\\epsilon^{M\\left(\\frac{q(M,\\epsilon)}{2}\\right)+1}\\|u\\|^{2}\\right)\\leq e^{-\\left(\\frac{1-\\log(2)}{2}\\right)M},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 23}, {"type": "equation", "text": "$$\nq(M,\\epsilon)=\\mathbb{P}\\left(1-\\operatorname*{max}\\left\\{0,\\frac{v_{M}}{\\|v_{M}\\|}\\cdot\\frac{\\left(u-x_{M-1}\\right)}{\\|u-x_{M-1}\\|}\\right\\}^{2}\\leq\\epsilon\\right)\\quad x_{M-1}=\\frac{\\arg\\operatorname*{min}}{x\\in\\mathrm{cone}\\{v_{1},...,v_{M-1}\\}}\\|x-u\\|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. First note that it suffices to analyze the case with replacement, since this case provides an upper bound on the case without replacement. To demonstrate this, we couple two probability spaces\u2014one that draws $v_{1},\\ldots,v_{M}$ with replacement, and one without replacement. First, draw an identical vector $v_{1}$ for both copies. On each subsequent iteration $m>1$ , the \u201cwith replacement\u201d copy first draws whether or not it selects a vector that was previously selected by the \u201cwithout replacement\u201d copy. If it does, it draws that vector independently; if it does not, it selects the same vector as the \u201cwithout replacement\u201d copy. In any case, at each iteration $m$ , the vectors drawn by the \u201cwith replacement\u201d copy are always a subset of the vectors drawn by the \u201cwithout replacement\u201d copy, and hence the minimum over $w\\geq0$ is greater for that copy. It therefore suffices to analyze the case with replacement. ", "page_idx": 23}, {"type": "text", "text": "To obtain an upper bound on the probability when sampling with replacement, instead of minimizing over all $w\\geq0$ jointly, suppose we use the following iterative algorithm. Set $x_{0}=0$ . At the first iteration, we draw $v_{1}$ and set the weight $w_{1}$ by optimizing over $w_{1}\\geq0$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{w_{1}>0}\\lvert\\lvert w_{1}v_{1}-u\\rvert\\rvert^{2}=\\lvert\\lvert u\\rvert\\rvert^{2}\\left(1-\\operatorname*{max}\\biggl\\{0,\\frac{v_{1}\\cdot u}{\\lVert v_{1}\\rVert\\lVert u\\rVert}\\biggr\\}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Set $x_{1}=w_{1}v_{1}$ , and note that $\\left(u-x_{1}\\right)\\cdot x_{1}=0$ . Then at each subsequent iteration $k$ , assume the previous iterate is optimized over all nonnegative weights, and hence satisfies $(u-x_{k-1})\\cdot x_{k-1}=0$ . We draw another vector $v_{k}$ , and bound the erorr of the next iterate $x_{k}$ by optimizing over only the weight $w_{k}$ for the new vector $v_{k}$ . Then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|u-x_{k}\\|^{2}=\\underset{w_{1},\\ldots,w_{k}\\geq0}{\\operatorname*{min}}\\left\\|\\displaystyle\\sum_{m=1}^{k}w_{m}v_{m}-u\\right\\|^{2}\\leq\\underset{w_{k}\\geq0}{\\operatorname*{min}}\\left\\|w_{k}v_{k}+x_{k-1}-u\\right\\|^{2}}&{}\\\\ &{=\\|u-x_{k-1}\\|^{2}\\Bigg(1-\\operatorname*{max}\\bigg\\{0,\\frac{v_{k}\\cdot\\big(u-x_{k-1}\\big)}{\\|v_{k}\\|\\|u-x_{k-1}\\|}\\bigg\\}^{2}\\Bigg)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{min}_{w\\geq0}\\left\\|\\sum_{m=1}^{M}w_{m}v_{m}-u\\right\\|^{2}\\leq\\epsilon^{K}\\|u\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\geq\\mathbb{P}\\Bigg(\\mathop{\\operatorname*{in}}{\\operatorname*{at}}{\\operatorname*{least}}\\,K\\,\\mathrm{ierations},1-\\operatorname*{max}\\bigg\\{0,\\frac{v_{k}\\cdot\\big(u-x_{k-1}\\big)}{\\|v_{k}\\|\\|u-x_{k-1}\\|}\\bigg\\}^{2}\\leq\\epsilon\\Bigg)}\\\\ &{=\\underset{K\\leq[M],|K|\\geq K}{\\sum}\\mathbb{P}\\Bigg(k\\in K\\iff1-\\operatorname*{max}\\bigg\\{0,\\frac{v_{k}\\cdot\\big(u-x_{k-1}\\big)}{\\|v_{k}\\|\\|u-x_{k-1}\\|}\\bigg\\}^{2}\\leq\\epsilon\\Bigg)}\\\\ &{\\geq\\underset{K\\leq[M],|K|\\geq K}{\\sum}q^{k}(1-q)^{M-k}}\\\\ &{=\\underset{k=K}{\\sum}\\Bigg(\\frac{M}{k}\\Bigg)q^{k}(1-q)^{M-k},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{q=\\mathbb{P}\\Bigg(1-\\operatorname*{max}\\biggl\\{0,\\frac{v_{M}\\cdot\\left(u-x_{M-1}\\right)}{\\|v_{M}\\|\\|u-x_{M-1}\\|}\\biggr\\}^{2}\\leq\\epsilon\\Bigg)}\\\\ {x_{M-1}=\\underset{x\\in\\mathrm{cone}\\{v_{1},\\ldots,v_{M-1}\\}}{\\arg\\operatorname*{min}}\\|x-u\\|^{2}}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So for all $0\\leq K\\leq M$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\operatorname*{min}_{w\\geq0}\\left\\|\\sum_{m=1}^{M}w_{m}v_{m}-u\\right\\|^{2}>\\epsilon^{K}\\|u\\|^{2}\\right)\\leq\\mathrm{Binom}(M,K-1,q).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the Chernoff bound on the binomial CDF, for all $K-1\\leq M q$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathbb{P}\\left(\\underset{w\\ge0}{\\operatorname*{min}}\\left\\|\\sum_{m=1}^{M}w_{m}v_{m}-u\\right\\|^{2}>\\epsilon^{K}\\|u\\|^{2}\\right)\\le e^{-M\\left(\\frac{K-1}{M}\\log\\frac{K-1}{M q}+(1-\\frac{K-1}{M})\\log\\frac{1-\\frac{K-1}{M}}{1-q}\\right)}}\\\\ &{}&{=e^{-(K-1)\\log\\frac{K-1}{M q}-(M-(K-1))\\log\\frac{M-(K-1)}{M(1-q)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Substituting $K-1=M q/2$ yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{=e^{M((q/2)\\log2-(1-q/2)\\log\\frac{1-q/2}{(1-q)})}\\leq e^{-\\left(\\frac{1-\\log(2)}{2}\\right)M}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof of Corollary 6.1. Since the potentials are $\\beta\\operatorname{Cov}_{\\pi}((\\ell_{n})_{n=1}^{N})$ subexponential, Theorem 5.3 guarantees that \u2200 $\\begin{array}{r l r l}&{\\langle w\\in\\mathbb{R}_{+}^{N}:4\\beta(w-1)^{T}\\operatorname{Cov}_{\\pi}((\\ell_{n})_{n=1}^{N})(w-1)\\leq1,}&&{\\overline{{\\mathrm{KL}}}(w)\\leq4\\beta(w-1)^{T}\\operatorname{Cov}_{\\pi}((\\ell_{n})_{n=1}^{N})(w-1).}\\end{array}$ We apply Lemma A.7 with vectors $\\ell_{1},\\ldots,\\ell_{N}$ (in equivalence classes specified up to a additive constant) and inner product between $\\ell_{i},\\ell_{j}$ defined by $\\mathrm{Cov}_{\\pi}(\\ell_{i},\\ell_{j})$ . In the notation of Lemma A.7, by assumption, $\\|u\\|^{2}=O_{p}(N^{\\alpha})$ and $q(M,\\epsilon)=\\omega_{p}(M^{-\\rho})$ . Substituting $M=(\\log N)^{1/(1-\\rho)}$ , we find that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb P\\Big(4\\beta(w-1)^{T}\\operatorname{Cov}_{\\pi}((\\ell_{n})_{n=1}^{N})(w-1)\\geq\\epsilon^{-\\omega_{p}(\\log N)+\\alpha\\log N}\\Big)\\to0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining this result with the KL bound above yields the final result. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The abstract and introduction state that the paper produces new general upper and lower bounds for Bayesian coreset approximations, that these bounds are applied to particular cases of interest, and that empirical results align with the theory. The paper does indeed contain these results, with proofs in the supplemental material, and empirical results in the figures do match the theory well. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: In the conclusions section of the work, two main limitations of the theory are mentioned: the need to perform case-by-case analysis of subexponentiality constants and alignment probabilities in Definition 5.2 and Corollary 6.1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Theoretical results are numbered, and proofs of all results are included in the appendix. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: All details needed to reproduce the experimental results in Figure 2 and 3 are included in the text. Algorithms used in the experiments exist in the cited literature. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: There are no new algorithms presented in this work; the experiments involve only existing methods for which public code is available. The code is not central to the contributions of the paper. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details. ", "page_idx": 26}, {"type": "text", "text": "\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All details needed to reproduce the experimental results in Figure 2 and 3 are included in the text. Algorithms used in the experiments exist in the cited literature. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All empirical results show the mean over a number of trials, as well as error bars indicating standard error. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: These details are not important for this paper, as there are no new methods or algorithms presented or claims related to computational performance. However, the introduction does list that simulations were performed on a desktop computer with a Core i7 processor and 32GB RAM. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper presents a new theoretical analysis of error bounds for Bayesian coresets methods. It does not present any new methodology or data with potential harmful consequences. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There is no potential negative societal impact of this work. The paper provides new theory regarding existing methodology. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This does not apply. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This does not apply. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: This does not apply. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This does not apply. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This does not apply. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]