[{"figure_path": "SAZeQV2PtT/tables/tables_5_1.jpg", "caption": "Figure 2: Importance-weighted coreset quality, showing the minimum of the forward and reverse KL divergences on the vertical axis as a function of dataset size N for 3 coreset sizes: log N (black), \u221aN (blue), and 1/2N (red). Dashed lines indicate predictions from the theory in Corollaries 4.1 and 4.2, solid lines indicate the mean over 10 trials, and error bars indicate standard error. The top row shows the quality of basic importance-weighted coresets (note that both horizontal and vertical axes are in log scale), while the bottom row shows the quality with optimal post-hoc scaling (note that only the horizontal axis is in log scale). The left column corresponds to the Cauchy location model, while the right column corresponds to the logistic regression model. Sampling probabilities pn for both models are set proportional to X2, thresholded to lie between 0.1/N and 10/N.", "description": "This figure empirically validates Corollaries 4.1 and 4.2, which concern the minimum coreset size for importance-weighted coresets.  The plots show the minimum KL divergence between the true posterior and the importance-weighted coreset posterior for different coreset sizes (log N, \u221aN, N/2) and dataset sizes N.  The results are shown for both basic importance weighting and importance weighting with post-hoc optimal scaling, and for two different models (Cauchy location and logistic regression).", "section": "4 Lower bound applications"}, {"figure_path": "SAZeQV2PtT/tables/tables_8_1.jpg", "caption": "Algorithm 3 Subsample-optimize coreset construction", "description": "Algorithm 3 describes the subsample-optimize coreset construction.  It begins by computing probabilities (p<sub>n</sub>)<sup>N</sup><sub>n=1</sub>, which may depend on the data and model. Then, it draws indices I<sub>1</sub>,..., I<sub>M</sub> independently from a Categorical distribution with probabilities (p<sub>1</sub>,..., p<sub>N</sub>).  The set of selected indices is denoted as I = {I<sub>1</sub>,..., I<sub>M</sub>}. The algorithm then computes the optimal weights w* that minimize the Kullback-Leibler (KL) divergence between the coreset approximation and the true posterior distribution, subject to the constraint that only weights corresponding to indices in I are non-zero. Finally, the algorithm returns the optimal weights (w*)<sup>N</sup><sub>n=1</sub>.", "section": "6 Upper bound application: subsample-optimize coresets"}]