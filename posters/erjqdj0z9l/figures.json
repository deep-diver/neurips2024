[{"figure_path": "erjQDJ0z9L/figures/figures_1_1.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "This figure illustrates the LLM-driven discovery process. The left panel shows a conceptual overview of how the process works: An LLM is prompted to generate new code for offline preference optimization loss functions. These functions are then evaluated, and the results are fed back to the LLM to improve the next proposal. This process iterates until a satisfactory loss function is found.  The right panel shows the performance of the discovered objective functions compared to existing baseline methods on the Alpaca Eval benchmark.  DiscoPOP shows state-of-the-art performance.", "section": "1 Introduction"}, {"figure_path": "erjQDJ0z9L/figures/figures_3_1.jpg", "caption": "Figure 2: LLM-driven objective discovery for CIFAR-10 classification. Left. Performance across LLM-discovery trials. The proposals alternate between exploring new objective concepts, tuning the components, and combining previous insights. Right. The best three discovered objectives transfer to different network architectures and longer training runs (100 epochs).", "description": "This figure shows the results of using LLMs to discover new objective functions for image classification on the CIFAR-10 dataset.  The left panel shows how the LLM iteratively proposes new loss functions, evaluating their performance and refining its proposals based on the results. This demonstrates the LLM's ability to explore and combine different concepts to improve performance. The right panel shows that the three best-performing discovered objective functions generalize well to different network architectures (ResNet18, SimpleDLA, EfficientNetB0) and longer training times (100 epochs).", "section": "Small case study: Discovering supervised classification loss functions"}, {"figure_path": "erjQDJ0z9L/figures/figures_4_1.jpg", "caption": "Figure 3: Examples of LLM Objective Discovery improvement across generations. The first and second runs are shown left and right respectively.", "description": "This figure shows two examples of how the LLM-driven objective discovery method improves the performance of the objective functions across multiple generations. The left panel shows the first run, which starts with several baseline objective functions and then iteratively proposes and evaluates new objective functions based on feedback from the performance of the models trained with those functions. The right panel shows a second run which demonstrates a similar improvement in performance as the number of generations increase.  The x-axis represents the number of generations in the objective discovery process, and the y-axis represents the best MT-Bench score achieved in each generation.", "section": "4.1 Discovery Task - Multi-turn Dialogue on MT-Bench"}, {"figure_path": "erjQDJ0z9L/figures/figures_4_2.jpg", "caption": "Figure 4: MT-Bench Discovered Objective Evaluations", "description": "This radar chart visualizes the performance of different models trained with various objective functions on the MT-Bench benchmark.  Each axis represents a sub-task within MT-Bench (Humanities, STEM, Extraction, Writing, Coding, Roleplay, Reasoning, Math). The length of each line extending from the center to the outer edge indicates the model's performance on that particular sub-task, with higher values signifying better performance. The different colored lines represent different models: PADLL, LRML (DiscoPOP), SLIC, DPO, and the SFT base model.  The chart allows for a quick comparison of the relative strengths and weaknesses of each model across the various sub-tasks of the MT-Bench evaluation.", "section": "4.2 Discovery Results"}, {"figure_path": "erjQDJ0z9L/figures/figures_7_1.jpg", "caption": "Figure 5: Frontiers of expected reward vs KL divergence for converging models for the LRML against DPO and SLiC objective function. The rewards and KL-divergence values are averaged over 10 generations with different seeds. The sweep is done over \u03b2\u2208 {0.025, 0.05, 0.1, 0.25, 0.5, 1.0}. The optimal point is the top left corner, where the perfect reward is achieved with minimal divergence from the reference model.", "description": "This figure shows the trade-off between expected reward and KL divergence from a reference model for different beta values (\u03b2) across multiple generations. The plot shows the performance of three different objective functions: LRML (the discovered objective function), DPO, and SLiC.  The top-left corner represents the ideal scenario\u2014high reward with minimal divergence from the reference model, indicating effective alignment. The plot illustrates how each objective function balances reward and model divergence at different \u03b2 values. The plot is useful for comparing the performance of the new objective function (LRML) with existing baselines (DPO and SLiC).", "section": "D Additional Results"}, {"figure_path": "erjQDJ0z9L/figures/figures_7_2.jpg", "caption": "Figure 5: Frontiers of expected reward vs KL divergence for converging models for the LRML against DPO and SLiC objective function. The rewards and KL-divergence values are averaged over 10 generations with different seeds. The sweep is done over \u03b2\u2208 {0.025, 0.05, 0.1, 0.25, 0.5, 1.0}. The optimal point is the top left corner, where the perfect reward is achieved with minimal divergence from the reference model.", "description": "This figure shows the Pareto frontier of expected reward vs. KL divergence for different optimization objective functions. The x-axis represents the KL divergence between the trained model and the reference model, while the y-axis represents the expected reward. Each point represents the average performance over 10 generations with different random seeds. The different colors represent different objective functions. The optimal performance would be in the top-left corner, where the reward is high and the KL divergence is low. The figure shows that LRML outperforms DPO and SLIC objective functions on this metric.", "section": "D.1 Frontiers of Expected Reward vs KL Divergence"}, {"figure_path": "erjQDJ0z9L/figures/figures_7_3.jpg", "caption": "Figure 6: Figure 6a: Baseline objective functions DPO and SLiC, and the discovered ones, LRML, AQFL, and PADLL. Figure 6b: gradients of the objectives as a function of p and with fixed  \u03b2 = 0.05.", "description": "This figure shows the plots of baseline objective functions (DPO and SLiC) and discovered objective functions (LRML, AQFL, and PADLL) and their gradients.  The x-axis represents the log ratio difference (p), and the y-axis represents the loss function value or its gradient.  The plots illustrate the shape and behavior of these functions, highlighting differences between baseline and discovered algorithms, and showing the non-convexity of DiscoPOP.", "section": "Additional Analysis of DiscoPOP"}, {"figure_path": "erjQDJ0z9L/figures/figures_7_4.jpg", "caption": "Figure 6: Figure 6a: Baseline objective functions DPO and SLiC, and the discovered ones, LRML, AQFL, and PADLL. Figure 6b: gradients of the objectives as a function of p and with fixed \u03b2 = 0.05.", "description": "This figure compares the baseline objective functions (DPO and SLIC) with the discovered ones (LRML, AQFL, and PADLL).  Subfigure (a) shows the objective functions themselves plotted against the log ratio difference (p). Subfigure (b) shows the gradients of those same functions, also plotted against p, using a fixed beta value of 0.05.  This allows for a visual comparison of the shape and behavior of each function and its gradient, providing insight into their respective properties and how they might affect the optimization process.", "section": "Additional Analysis of DiscoPOP"}, {"figure_path": "erjQDJ0z9L/figures/figures_18_1.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "The figure illustrates the LLM-driven discovery process.  The left panel shows a conceptual overview of how an LLM is prompted to propose new loss functions for offline preference optimization. These functions are then evaluated, and the results are fed back to the LLM to iteratively improve the proposed functions. The right panel shows a comparison of the performance of the discovered objective functions against existing baseline methods on the Alpaca Eval benchmark.", "section": "1 Introduction"}, {"figure_path": "erjQDJ0z9L/figures/figures_18_2.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "This figure illustrates the LLM-driven objective discovery process. The left panel shows a conceptual diagram of how the LLM proposes new objective functions, trains a model with them, evaluates the performance, and uses the results to inform the next proposal. The right panel presents a bar chart comparing the performance of the discovered objective functions against existing baselines on the Alpaca Eval benchmark.", "section": "LLM-Driven Objective Discovery"}, {"figure_path": "erjQDJ0z9L/figures/figures_19_1.jpg", "caption": "Figure 7: Training and eval statistics of DPO, SLIC, PADLL, and LRML. The losses are not directly comparable to each other, as they are calculated differently for each model. Interestingly, eval results are not strongly correlated with the downstream MT-Bench scores, as LRML achieves the worst accuracy.", "description": "The figure shows training and evaluation loss and accuracy curves for four different offline preference optimization algorithms (DPO, SLIC, PADLL, LRML) on a specific task.  It highlights that while the training losses and accuracies vary considerably among the algorithms, there is not a strong correlation between the evaluation metrics and the final MT-Bench scores, which are used as the primary performance measure.", "section": "B Training Details"}, {"figure_path": "erjQDJ0z9L/figures/figures_21_1.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "This figure illustrates the LLM-driven objective function discovery process.  The left panel shows a flowchart of the process, starting with prompting an LLM to generate a new loss function, followed by an inner loop for training and evaluation, feeding the results back to the LLM for refinement. The right panel presents a bar chart comparing the performance of discovered objective functions against established baselines on the Alpaca Eval benchmark.", "section": "1 Introduction"}, {"figure_path": "erjQDJ0z9L/figures/figures_22_1.jpg", "caption": "Figure 8: Frontiers of expected reward vs KL divergence after convergence for the baseline functions and all the discovered ones. The rewards and KL divergence values are averaged over 10 generations with different seeds. The sweep is done over \u03b2\u2208 {0.025, 0.05, 0.1, 0.25, 0.5, 1, }. The optimal point is the top left corner, where perfect reward is achieved with minimal divergence from the reference model, to avoid reward hacking.", "description": "This figure shows the trade-off between reward and KL divergence for different beta values (\u03b2) across various objective functions.  The objective functions include baselines (DPO, SLIC) and several novel ones discovered by the LLM. The top-left corner represents the ideal scenario: high reward with minimal KL divergence (meaning the model's policy is close to the reference policy). Each subplot represents a different beta value, showing how the trade-off changes based on the chosen optimization objective. This helps in understanding the relative strengths of each objective function in balancing reward maximization and maintaining alignment with the pre-trained reference model.", "section": "D Additional Results"}, {"figure_path": "erjQDJ0z9L/figures/figures_22_2.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "This figure illustrates the LLM-driven discovery process. The left panel shows a conceptual overview of the process: an LLM is prompted to generate code for a new offline preference optimization loss function, this loss function is then evaluated via model training on MT-Bench, and the performance is fed back to the LLM to inform the next iteration. The right panel shows a bar chart comparing the performance of discovered loss functions against existing baselines on Alpaca Eval.", "section": "1 Introduction"}, {"figure_path": "erjQDJ0z9L/figures/figures_23_1.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "This figure illustrates the LLM-driven discovery process.  The left panel shows a conceptual diagram of how the process works: an LLM is prompted to generate new loss functions for offline preference optimization.  These functions are then evaluated, and their performance is fed back to the LLM to inform the generation of the next candidate. The right panel shows the performance of the discovered loss functions on AlpacaEval, a benchmark for evaluating language models.", "section": "LLM-Driven Objective Discovery"}, {"figure_path": "erjQDJ0z9L/figures/figures_23_2.jpg", "caption": "Figure 2: LLM-driven objective discovery for CIFAR-10 classification. Left. Performance across LLM-discovery trials. The proposals alternate between exploring new objective concepts, tuning the components, and combining previous insights. Right. The best three discovered objectives transfer to different network architectures and longer training runs (100 epochs).", "description": "This figure shows the results of using LLMs to discover new objective functions for CIFAR-10 image classification. The left panel shows the performance of the discovered objectives across multiple generations, highlighting the iterative refinement process where the LLM proposes new objectives, evaluates them, and incorporates the feedback into subsequent proposals. The right panel demonstrates the generalizability of the three best-performing discovered objectives by evaluating their performance on different network architectures (ResNet18, SimpleDLA, and EfficientNetB0) and with longer training runs (100 epochs).", "section": "Small case study: Discovering supervised classification loss functions"}, {"figure_path": "erjQDJ0z9L/figures/figures_23_3.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "The figure illustrates the LLM-driven discovery process. The left panel shows a conceptual overview of how the LLM is prompted to generate new objective functions for offline preference optimization, which are then evaluated in an inner loop, and the results fed back to the LLM to iteratively refine the process. The right panel shows the performance of the discovered objective functions compared to existing baselines on the Alpaca Eval benchmark.", "section": "1 Introduction"}, {"figure_path": "erjQDJ0z9L/figures/figures_24_1.jpg", "caption": "Figure 14: Distribution of \u03b2-scaled difference of log-ratios (left y-axis) and corresponding DiscoPOP loss value (right y-axis) of the training samples on the IMDb positive review generation task. The DiscoPOP function has a local minimum at f<sub>LRML</sub>(-2.3714) = 0.785929 and a local maximum at f<sub>LRML</sub>(1.44012) = 0.87829. The number of samples within the two local optima corresponds to 1.35% of the training set.", "description": "This figure shows the distribution of \u03b2-scaled differences of log-ratios and their corresponding DiscoPOP loss values for the IMDb positive review generation task.  The x-axis represents the difference of log-ratios, and the left y-axis shows the count of samples falling into each bin.  The right y-axis represents the DiscoPOP loss value. The distribution shows a concentration of samples near zero difference, along with a smaller number of samples at either extreme end.  The figure also highlights three key regions:  Above local maximum, Between optima, and Below local minimum. The percentage of samples within these regions are stated in the legend. The presence of local optima suggests that the loss function is non-convex and may lead to finding different solutions depending on the initialization.", "section": "D.6 Additional Analysis of DiscoPOP"}, {"figure_path": "erjQDJ0z9L/figures/figures_25_1.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "This figure illustrates the LLM-driven discovery process. The left panel shows a conceptual overview of how the process works: an LLM is prompted to generate code for new loss functions, these functions are then evaluated, and the results are fed back to the LLM to inform the next iteration. The right panel shows a comparison of the performance of various objective functions discovered using this method against established baseline methods on the Alpaca Eval benchmark.", "section": "1 Introduction"}, {"figure_path": "erjQDJ0z9L/figures/figures_29_1.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "This figure illustrates the LLM-driven discovery process. The left panel shows a conceptual diagram of how an LLM is prompted to generate new objective functions for offline preference optimization, which are then evaluated, and the results fed back to the LLM for iterative refinement. The right panel shows the performance comparison of different discovered objective functions on Alpaca Eval, demonstrating the discovery of new, high-performing algorithms.", "section": "LLM-Driven Objective Discovery"}, {"figure_path": "erjQDJ0z9L/figures/figures_30_1.jpg", "caption": "Figure 1: Left. Conceptual illustration of LLM-driven discovery of objective functions. We prompt an LLM to output new code-level implementations of offline preference optimization losses E(yw,y1,x)~D [f (\u03b2p)] as a function of the policy (\u03c0\u03bf) and reference model's (ref) likelihoods of the chosen (yw) and rejected (y\u0131) completions. Afterwards, we run an inner loop training procedure and evaluate the resulting model on MT-Bench. The corresponding performance is fed back to the language model, and we query it for the next candidate. Right. Performance of discovered objective functions on Alpaca Eval.", "description": "The figure illustrates the LLM-driven discovery process of objective functions for offline preference optimization. The left panel shows a conceptual diagram of the process: an LLM is prompted to suggest new loss functions, the performance of which is evaluated, and then the feedback is given back to the LLM, creating an iterative process. The right panel shows a bar chart comparing the performance of several objective functions discovered by the LLM against existing baselines on the Alpaca Eval benchmark. ", "section": "1 Introduction"}]