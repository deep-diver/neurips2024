{"references": [{"fullname_first_author": "Paul F. Christiano", "paper_title": "Deep reinforcement learning from human preferences", "publication_date": "2017-00-00", "reason": "This paper introduces reinforcement learning from human feedback (RLHF), a core technique used in the paper's preference optimization methods."}, {"fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "publication_date": "2019-00-00", "reason": "This paper introduces the GPT language model architecture, foundational to many LLMs used in preference optimization."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-00-00", "reason": "This paper introduces direct preference optimization (DPO), a key baseline method compared against in the paper's LLM-driven objective discovery."}, {"fullname_first_author": "Yao Zhao", "paper_title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback", "publication_date": "2023-00-00", "reason": "This paper introduces Sequence Likelihood Calibration (SLiC), another significant baseline method for offline preference optimization that is compared to the paper's approach."}, {"fullname_first_author": "Lianmin Zheng", "paper_title": "Judging LLMs-as-a-judge with MT-Bench and Chatbot Arena", "publication_date": "2024-00-00", "reason": "This paper introduces MT-Bench, a key benchmark dataset used for evaluating the performance of the discovered preference optimization algorithms."}]}