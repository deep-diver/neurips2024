[{"figure_path": "c8HOQIMwKP/tables/tables_5_1.jpg", "caption": "Table 1: A summary of our considered evaluation tasks, datasets, models, and performance metrics.", "description": "This table summarizes the image segmentation tasks (semantic, instance, panoptic, interactive, remote sensing instance, and medical image segmentation), the datasets used for each task, the models employed for evaluation, and the metrics used to assess the performance of those models.  It provides a comprehensive overview of the experimental setup.", "section": "4 Experiments"}, {"figure_path": "c8HOQIMwKP/tables/tables_7_1.jpg", "caption": "Table 2: The main results of UnSeg against the Mask2Former model in panoptic, instance, and semantic segmentation tasks, evaluated on ADE20K val, COCO val2017, and Cityscapes val. UnSeg can significantly reduce the test performance of the models across different tasks and datasets. The best protection results are boldfaced. R50: ResNet50, Swin-T: Swin Transformer-Tiny.", "description": "This table presents a comparison of the performance of the Mask2Former model on three image segmentation tasks (panoptic, instance, and semantic segmentation) when trained on clean datasets versus datasets protected by UnSeg.  The results are shown for three different datasets (ADE20K, COCO, and Cityscapes), using two different backbones (ResNet50 and Swin-Transformer-Tiny). The table shows the effectiveness of UnSeg in reducing the performance of the models on all tasks and datasets.  Boldfaced values indicate the best results achieved by UnSeg in protecting the data.  Performance is measured using metrics relevant to each task (PQ, AP, mIoU).", "section": "4.2 Main Results"}, {"figure_path": "c8HOQIMwKP/tables/tables_8_1.jpg", "caption": "Table 3: The AP (%) of DINO trained on clean and unlearnable COCO dataset.", "description": "This table presents the Average Precision (AP) of the DINO object detection model trained on both clean and UnSeg-protected COCO datasets.  It breaks down the AP into three categories based on object size: small (AP-S), medium (AP-M), and large (AP-L). The results demonstrate the significant drop in performance when the model is trained on images made unlearnable by the UnSeg method, highlighting the effectiveness of the technique in protecting data from unauthorized use.", "section": "4.2 Main Results"}, {"figure_path": "c8HOQIMwKP/tables/tables_8_2.jpg", "caption": "Table 4: The mIoU (%) of DeepLabV3 trained using different defense methods on unlearnable Pascal VOC 2012 crafted by our UnSeg.", "description": "This table presents the mean Intersection over Union (mIoU) scores achieved by the DeepLabV3 model trained on Pascal VOC 2012 dataset after applying various defense mechanisms against the unlearnable examples generated by the UnSeg method. The defense methods include no defense, Gaussian filtering, JPEG compression, adversarial training (AT), and DDC-adversarial training (DDC-AT).  The results demonstrate the effectiveness of UnSeg against these defense strategies.", "section": "4.3 Additional Analyses"}, {"figure_path": "c8HOQIMwKP/tables/tables_8_3.jpg", "caption": "Table 5: The mIoU (%) of DeepLabV3 trained on clean vs. clean-unlearnable mixed training dataset (Pascal VOC 2012).", "description": "This table presents the mean Intersection over Union (mIoU) scores achieved by the DeepLabV3 model trained on datasets with varying proportions of clean and unlearnable images.  It demonstrates the impact of mixing clean and unlearnable examples on model performance, comparing results with a model trained exclusively on clean data.", "section": "4.3 Additional Analyses"}, {"figure_path": "c8HOQIMwKP/tables/tables_8_4.jpg", "caption": "Table 6. Parameter analysis on Pascal VOC 2012 and Cityscapes. EG: Epsilon generalization, LM: Label modification. \u2713/X indicates that the method is used/not used.", "description": "This table presents the results of parameter analysis conducted on two datasets: Pascal VOC 2012 and Cityscapes.  The analysis focuses on the impact of Epsilon Generalization (EG) and Label Modification (LM) on the performance of the UnSeg model. The table shows the mIoU (mean Intersection over Union) for different classes in Pascal VOC 2012 and the PQ (Panoptic Quality), AP<sup>Th</sup><sub>pan</sub> (Average Precision for thing categories in panoptic segmentation), and mIoU<sub>pan</sub> (mean IoU for panoptic segmentation) for Cityscapes.  Each row represents a different combination of EG and LM techniques, with  '\u2713' indicating that a technique was used and 'X' indicating it was not.  The 'Clean' row provides the baseline performance without any protection techniques applied.", "section": "4.3 Additional Analyses"}, {"figure_path": "c8HOQIMwKP/tables/tables_13_1.jpg", "caption": "Table 7: Prompt analysis on Pascal VOC 2012 using DeepLabV1.", "description": "This table presents the results of an experiment comparing different prompt types (point, box, and mask) used with the UnSeg framework on the Pascal VOC 2012 dataset for image segmentation, using DeepLabV1 as the target model. It shows the mIoU (mean Intersection over Union) for the overall performance and individual classes, demonstrating the impact of different prompts on the unlearnable examples generated by UnSeg.", "section": "4.2 Main Results"}, {"figure_path": "c8HOQIMwKP/tables/tables_13_2.jpg", "caption": "Table 5: The mIoU (%) of DeepLabV3 trained on clean vs. clean-unlearnable mixed training dataset (Pascal VOC 2012).", "description": "This table shows the results of training DeepLabV3 on datasets with varying proportions of clean and unlearnable images generated using the UnSeg method.  It demonstrates the effect of mixing clean and unlearnable examples on the model's performance. The mIoU (mean Intersection over Union) is a common metric for evaluating image segmentation performance.", "section": "4.3 Additional Analyses"}, {"figure_path": "c8HOQIMwKP/tables/tables_14_1.jpg", "caption": "Table 9: The mIoU (%) of UNet++ trained on clean vs. clean-unlearnable mixed training dataset (Kvasir-seg).", "description": "This table presents the mean Intersection over Union (mIoU) scores achieved by the UNet++ model trained on different proportions of clean and unlearnable data from the Kvasir-seg dataset.  Five different backbones (ResNet50, DenseNet169, EfficientNetB6, Res2Net, and RegNetX) were used for the UNet++ model. The \"Clean Only\" row shows the mIoU when only clean data is used for training. The \"Mixed Data\" row shows the mIoU when a mixture of clean and unlearnable data is used, with the clean proportion varying from 0% to 80%.  The table demonstrates the effect of including unlearnable data on the model's performance.", "section": "4.2 Main Results"}, {"figure_path": "c8HOQIMwKP/tables/tables_14_2.jpg", "caption": "Table 2: The main results of UnSeg against the Mask2Former model in panoptic, instance, and semantic segmentation tasks, evaluated on ADE20K val, COCO val2017, and Cityscapes val. UnSeg can significantly reduce the test performance of the models across different tasks and datasets. The best protection results are boldfaced. R50: ResNet50, Swin-T: Swin Transformer-Tiny.", "description": "This table presents the main results of the UnSeg model against Mask2Former across three mainstream image segmentation tasks (panoptic, instance, and semantic segmentation).  It shows the performance drop (PQ, AP, mIoU) achieved by using UnSeg on three widely used datasets (ADE20K, COCO, and Cityscapes).  The results demonstrate the effectiveness of UnSeg in reducing the performance of the models on different tasks and datasets.  ResNet50 and Swin Transformer-Tiny backbones were used in Mask2former.", "section": "4.2 Main Results"}, {"figure_path": "c8HOQIMwKP/tables/tables_14_3.jpg", "caption": "Table 11: The impact of different initialization methods for the generator model.", "description": "This table presents the results of an experiment evaluating different initialization methods for the noise generator model, focusing on Pascal VOC 2012 semantic segmentation. It compares the performance (mIoU) of the model across different classes, when using the pretrained SAM weights and randomly initialized weights. The result shows that the performance is not overly sensitive to the initialization method, as both methods achieve relatively low mIoU compared to the clean dataset.", "section": "4.2 Main Results"}]