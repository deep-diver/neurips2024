{"references": [{"fullname_first_author": "Alekh Agarwal", "paper_title": "Fast global convergence rates of gradient methods for high-dimensional statistical recovery", "publication_date": "2010-01-01", "reason": "This paper provides foundational results on the convergence rates of gradient methods for high-dimensional statistical recovery, which are crucial for understanding the optimization aspects of the decentralized sparse regression problem studied in the main paper."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "A convergence theory for deep learning via over-parameterization", "publication_date": "2019-01-01", "reason": "This paper offers a theoretical analysis of deep learning via over-parameterization, which provides insights into the implicit regularization phenomenon that underpins the effectiveness of the decentralized gradient descent algorithm analyzed in the main paper."}, {"fullname_first_author": "Sanjeev Arora", "paper_title": "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks", "publication_date": "2019-01-01", "reason": "This paper provides a detailed analysis of optimization and generalization for overparameterized two-layer neural networks, which helps understand the implicit regularization phenomenon, and is relevant to the analysis of overparameterized models in decentralized settings."}, {"fullname_first_author": "Mikhail Belkin", "paper_title": "Reconciling modern machine-learning practice and the classical bias-variance trade-off", "publication_date": "2019-01-01", "reason": "This paper discusses the bias-variance trade-off in modern machine learning, which provides context for the implicit regularization phenomenon observed in overparameterized models, and is relevant to the analysis of generalization performance."}, {"fullname_first_author": "Simon Du", "paper_title": "Gradient descent finds global minima of deep neural networks", "publication_date": "2019-01-01", "reason": "This paper shows that gradient descent can find global minima of deep neural networks, which is relevant to the analysis of the convergence properties of decentralized gradient descent, especially in overparameterized settings."}]}