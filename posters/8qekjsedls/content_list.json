[{"type": "text", "text": "Off-policy estimation with adaptively collected data: the power of online learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jeonghwan Lee ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Statistics The University of Chicago Chicago, IL 60637 jhlee97@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Cong Ma   \nDepartment of Statistics   \nThe University of Chicago Chicago, IL 60637 congm@uchicago.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider estimation of a linear functional of the treatment effect from adaptively collected data. This problem finds a variety of applications including off-policy evaluation in contextual bandits, and estimation of the average treatment effect in causal inference. While a certain class of augmented inverse propensity weighting (AIPW) estimators enjoys desirable asymptotic properties including the semiparametric efficiency, much less is known about their non-asymptotic theory with adaptively collected data. To fill in the gap, we first present generic upper bounds on the mean-squared error of the class of AIPW estimators that crucially depends on a sequentially weighted error between the treatment effect and its estimates. Motivated by this, we propose a general reduction scheme that allows one to produce a sequence of estimates for the treatment effect via online learning to minimize the sequentially weighted estimation error. To illustrate this, we provide three concrete instantiations in (1) the tabular case; (2) the case of linear function approximation; and (3) the case of general function approximation for the outcome model. We then provide a local minimax lower bound to show the instance-dependent optimality of the AIPW estimator using no-regret online learning algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Estimating a linear functional of the treatment effect is of great importance in both causal inference and reinforcement learning (RL). For instance, in causal inference, one is interested in estimating the average treatment effect (ATE) [20] or their weighted variants, and in the literature of bandits and RL, one is interested in estimating the expected reward of a target policy [38, 64, 41, 37]. Two main challenges arise when tackling this problem: ", "page_idx": 0}, {"type": "text", "text": "\u00b7 Off-policy estimation: Oftentimes, one needs to estimate the linear functional based on observational data collected from a behavior policy. This behavior policy may not match the desired distribution specified by the linear functional [42]; ", "page_idx": 0}, {"type": "text", "text": "\u00b7 Adaptive data collection mechanism: It is increasingly common for observational data to be adaptively collected due to the use of online algorithms (e.g., via contextual bandit algorithms [60, 33, 2, 52, 34]) in experimental design [67]. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we deal with two challenges simultaneously by investigating the estimation of a linear functional of the treatment effect from observational data that are collected adaptively. When the observational data is collected non-adaptively, i.e., in an i.i.d. manner, there is an extensive line of work [51, 49, 10, 24, 1, 27, 43, 6, 3, 64, 41] investigating the asymptotic and non-asymptotic theory of various estimators. Most notably are the study [6] that establishes the asymptotic efficiency of a family of semi-parametric estimators, and a more recent study [42] that undertakes a finite-sample analysis which uncovers the importance of a certain weighted $\\ell_{2}$ -norm when estimating the treatment effect. On the other hand, when it comes to adaptively collected data, most prior works [16, 67] focus on the asymptotic normality of the estimators, and do not discuss the finite-sample analysis of the estimators. In this paper, we aim to fill in this gap. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1.1 Main contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "More specifically, we make the following three main contributions in this paper: ", "page_idx": 1}, {"type": "text", "text": "\u00b7 First, we present generic finite-sample upper bounds on the mean-squared error of the class of augmented inverse propensity weighting (AIPW) estimators that crucially depends on a sequentially weighted error between the treatment effect and its estimates. This sequentially weighted estimation error demonstrates a clear effect of history-dependent behavior policies; \u00b7 Second, motivated by previous finding, we propose a general reduction scheme that allows one to form a sequence of estimates for the treatment effect via online learning to minimize the sequentially weighted estimation error. To demonstrate this, we provide three concrete instantiations in (1) the tabular case; (2) the case of linear function approximation; and (3) the case of general function approximation for the outcome model; \u00b7 In the end, we provide a local minimax lower bound to showcase the instance-dependent optimality of the AIPW estimator using no-regret online learning algorithms in the large-sample regime. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Off-policy estimation with observational data  Off-policy estimation in observational settings has been a central topic in statistics, operations research, causal inference, and RL. Here, we group a few prominent off-policy estimators into the following three categories: (i) Model-based estimator: often dubbed as the direct method (DM), whose key idea is to utilize observational data to learn a regression model that predicts outcomes for each state-action pair, and then average these model predictions [29, 10, 9, 39]. Due to model mis-specification, DM typically has a low variance but might lead to highly biased estimation results. (i) Inverse propensity weighting (IPW): for the OPE task, IPW uses importance weighting to account for the distribution mismatch between the behavioral policy and the target policy [21, 55]. If the behavioral policy differs significantly from the target policy, then IPW can have an overly large variance (known as the low overlap issue) [23]. Typical remedies for this issue include propensity clipping [25, 57] or self-normalization [19, 58]. (i) Hybrid estimator: some off-policy estimators (e.g., the doubly-robust (DR) estimator [10]) combine DM and IPW together to blend their complementary strengths [48, 10, 9, 59, 12, 56, 64]. A key asymptotic results in OPE is that the cross-fitted DR is $\\sqrt{n}$ -consistent and asymptotically efficient (that is, it attains the lowest possible asymptotic variance), even for the case where nuisance parameters are estimated at rates slowerthan $\\sqrt{n}$ -rates [6]. However, these methods still might be vulnerable to the low overlap issue especially for large or continuous action spaces. Thus, there has been a line of recent studies on OPE for large action spaces [13, 53, 44, 54] and OPE for continuous action space [28, 35, 63]. ", "page_idx": 1}, {"type": "text", "text": "Off-policy estimation with adaptively collected data A recent strand of works studied asymptotic theory of adaptive variants of the IPW and DR estimators (e.g., asymptotic normality, semi-parametric efficiency, and confidence intervals) [31, 8, 7] for adaptively collected data. However, in adaptive experiments, overlap between the behavioral policies and the target policy can deteriorate since the experimenter shifts the behavioral policies in response to what he/she observes (known as the drifting overlap) [67]. It may engender unacceptably large variances of the IPW and DR estimators. To address this large variance problem, there has been a recent strand of works investigating variance reduction strategies for the DR estimator based on shrinking importance weights toward one [4, 64, 57, 56], local stabilization [40, 69], and adaptive weighting [17, 67]. Recent studies on policy learning with adaptively collected data [68, 26] explored the adaptive weighting DR estimator for policy learning. In contrast with the majority of prior works on off-policy estimation with adaptively collected data that focus on asymptotic results, this paper aims at establishing non-asymptotic theory of the problem. While several researchers have been recently explored non-asymptotic results of the problem with an emphasis on uncertainty quantification [30, 65], we focus on analyses of estimation procedures of the off-policy value. As a majority of existing standard objects for uncertainty quantification, such as a confidence interval (Cl), take a very static view of the world (e.g., it holds for a fixed sample size and is not designed for interactive/adaptive data collection procedures), the aforementioned two papers [30, 65] instead study a more suitable statistical tool for such cases called a confidence sequence. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Problem formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first formulate our problem using the language of contextual bandits: let $\\mathbb{X}$ ,A, and $\\mathbb{Y}\\subseteq\\mathbb{R}$ denote the context space, the action space, and the outcome space, respectively. Denote by $\\mathbb{O}:=\\mathbb{X}\\times\\mathbb{A}\\times\\mathbb{Y}$ the space of all possible context-action-outcome triples. In an adaptive experiment, one observes $n$ samples $\\{(X_{i},A_{i},Y_{i})\\in\\mathbb{O}:i\\in[n]\\}$ produced by the following data generating procedure [26, 68]: At each stage $i\\in[n]$ ", "page_idx": 2}, {"type": "text", "text": "(i) A context $X_{i}\\in\\mathbb{X}$ is independently sampled from a fixed context distribution $\\Xi^{*}(\\cdot)\\in\\Delta(\\mathbb{X})$   \n(ii)There exists a behavioral policy $\\Pi_{i}^{*}(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{O}^{i-1}\\to\\Delta(\\mathbb{A})$ that selects the $i$ -th action as $A_{i}\\,|\\,X_{i},\\mathbf{O}_{i-1}\\sim\\Pi_{i}^{*}$ $\\cdot\\left|X_{i},\\mathbf{O}_{i-1}\\right)$ , where $\\mathbf{O}_{i}:=(X_{1},A_{1},Y_{1},\\cdot\\cdot\\cdot\\,,X_{i},A_{i},Y_{i})\\in{\\mathbb{O}}^{i}$ for $i\\in[n]$ As $\\Pi_{i}^{*}\\left(\\cdot\\left|X_{i},\\mathbf{O}_{i-1}\\right.\\right)$ may depend on previous observations, $\\{(X_{i},A_{i},Y_{i}):i\\in[n]\\}$ are no longer i.i.d.; ", "page_idx": 2}, {"type": "text", "text": "(ii) Given a Markov kernel $\\Gamma^{*}(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\rightarrow\\Delta(\\mathbb{Y})$ , we assume that the outcome is generated according to $Y_{i}\\sim\\Gamma^{*}\\left(\\cdot\\left|X_{i},\\dot{A}_{i}\\right.\\right)$ . Moreover, the conditional mean of the outcome $Y_{i}\\in\\mathbb{Y}$ is specified as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[Y_{i}\\left|X_{i},A_{i}\\right.\\right]=\\int_{\\mathbb{Y}}y\\Gamma^{*}\\left(\\mathrm{d}y\\left|X_{i},A_{i}\\right.\\right)=\\mu^{*}\\left(X_{i},A_{i}\\right),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the function $\\mu^{*}(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}$ is called the treatment effect (in causal inference) or the reward function (in bandit and RL literature). We note that the treatment effect $\\mu^{*}$ is not revealed to the statistician. We also define the conditional variance function $\\sigma^{2}(\\cdot,\\cdot):$ $\\mathbb{X}\\times\\mathbb{A}\\rightarrow[0,+\\infty]$ defined by $\\sigma^{2}\\left(x,a\\right):=\\mathbb{E}\\left[\\left.\\left\\{Y-\\mu^{*}\\left(X,A\\right)\\right\\}^{2}\\right|\\left(X,A\\right)=\\left(x,a\\right)\\right]$ , which is assumed to satisfy $\\sigma^{2}(x,a)<+\\infty$ for every state-action pair $(x,a)\\in\\mathbb{X}\\times\\mathbb{A}$ ", "page_idx": 2}, {"type": "text", "text": "At this moment, we assume the existence of $\\sigma$ -finite base measures $\\lambda_{\\mathbb{X}}(\\cdot),\\lambda_{\\mathbb{A}}(\\cdot)$ , and $\\lambda_{\\mathbb{Y}}(\\cdot)$ over $\\mathbb{X}$ $\\mathbb{A}$ and $\\mathbb{Y}$ , resp., such that $\\Xi^{*}(\\cdot)\\ll\\lambda_{\\mathbb{X}}(\\cdot),\\Pi_{i}^{*}\\left(\\cdot\\left|x,\\mathbf{o}_{i-1}\\right.\\right)\\ll\\lambda_{\\mathbb{A}}(\\cdot)$ for every $(x,\\mathbf o_{i-1})\\in\\mathbb{X}\\times\\mathbb{O}^{i-1}$ and $i\\in[n]$ , and $\\Gamma^{\\ast}\\left(\\cdot\\,|x,a\\,\\right)\\ll\\lambda_{\\mathbb{Y}}(\\cdot)$ for all state-action pairs $(x,\\dot{a})\\in\\mathbb{X}\\times\\mathbb{A}$ . Here, the notation $\\ll$ stands for the absolute continuity of measures. Our main goal is to estimate the off-policy value for any given target evaluation function $g(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}$ defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\tau^{*}=\\tau\\left(\\mathcal{Z}^{*}\\right):=\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\langle g(X,\\cdot),\\mu^{*}(X,\\cdot)\\rangle_{\\lambda_{\\mathbb{A}}}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $T^{*}:=(\\Xi^{*},\\Gamma^{*})\\in\\mathbb{I}:=\\Delta(\\mathbb{X})\\times(\\mathbb{X}\\times\\mathbb{A}\\to\\Delta(\\mathbb{Y}))$ defines our problem instance. Throughout the paper, we assume that the propensity scores $\\{\\pi_{i}^{*}(\\bar{X_{i}},\\mathbf O_{i-1};A_{i}):i\\in[n]\\}$ are revealed, where $\\begin{array}{r}{\\pi_{i}^{*}\\left(x,\\mathbf{o}_{i-1};\\cdot\\right):=\\frac{\\mathrm{d}\\Pi_{i}^{*}\\left(\\cdot\\left|x,\\mathbf{o}_{i-1}\\right.\\right)}{\\mathrm{d}\\lambda_{\\mathbb{A}}}:\\mathbb{A}\\to\\mathbb{R}.}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "As we mentioned earlier in Section 1, the estimation problem of a linear functional of the treatment effect $\\mu^{*}$ turns out to be useful in both causal inference and RL in the following sense: ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Estimation of average treatment effects: We consider the binary action space $\\mathbb{A}\\,=\\,\\{0,1\\}$ equipped with the counting measure. The average treatment effect (ATE) in our problem setting is defined as the linear functional ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathrm{ATE}}:=\\mathbb{E}_{\\mathbb{Z}^{\\ast}}\\left[Y_{i}(1)-Y_{i}(0)\\right]=\\mathbb{E}_{X\\sim\\Xi^{\\ast}}\\left[\\mu^{*}\\left(X,1\\right)-\\mu^{*}\\left(X,0\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Once we take the evaluation function as $g(x,a)=2a-1$ the ATE boils down to a particular case of the equation (1); ", "page_idx": 2}, {"type": "text", "text": "\u00b7 Off-policy evaluation (OPE) for contextual bandits: Assume that a target policy $\\Pi^{\\mathrm{target}}(\\cdot)\\ :$ $\\mathbb{X}\\to\\Delta(\\mathbb{A})$ is given such that $\\Pi^{\\mathrm{target}}\\left(\\cdot\\mid x\\right)\\ll\\lambda_{\\mathbb{A}}(\\cdot)$ for every context $x\\in\\mathbb{X}$ . For simplicity, let $\\begin{array}{r}{\\pi^{\\mathrm{target}}\\left(x,\\cdot\\right):=\\frac{\\mathrm{d}\\Pi^{\\mathrm{target}}\\left(\\cdot\\vert x\\right)}{\\mathrm{d}\\lambda_{\\mathbb{A}}}}\\end{array}$ denote the denity funtionof the target polcy for each context $x\\in\\mathbb{X}$ If we take $g(x,a)\\,=\\,\\pi^{\\mathrm{target}}(x,a)$ , then the linear functional (1) corresponds to the value of the target policy $\\Pi^{\\mathrm{target}}$ . This problem has been widely studied in the literature of bandits and RL, known as the off-policy evaluation (OPE). ", "page_idx": 2}, {"type": "text", "text": "We conclude this section by introducing notations that will be useful in later sections: let $\\mathbb{P}_{\\mathcal{T}}^{i}\\in\\Delta\\left(\\mathbb{O}^{i}\\right)$ denote the law of the sample trajectory $\\mathbf{O}_{i}$ under the sampling mechanism with a problem instance $\\mathcal{T}=(\\Xi,\\Gamma)\\in\\mathbb{I}$ We denote the density function of $\\mathbb{P}_{\\mathcal{T}}^{i}\\in\\dot{\\Delta}\\left(\\mathbb{O}^{i}\\right)$ with respect to the base measure $(\\lambda_{\\mathbb{X}}\\otimes\\lambda_{\\mathbb{A}}\\otimes\\lambda_{\\mathbb{Y}})^{\\otimes i}$ by $p_{\\mathcal{T}}^{i}(\\cdot):\\mathbb{O}^{i}\\to\\mathbb{R}_{+}$ . Lastly, we define the $k$ -th weighted $\\ell_{2}$ -norm for $k\\in[n]$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\varphi\\right\\|_{(k)}^{2}:=\\frac{1}{k}\\sum_{i=1}^{k}\\mathbb{E}_{Z^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\varphi^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for any function $\\varphi(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}$ together with the $k$ -th weighted $\\ell_{2}$ -space by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{L}_{(k)}^{2}:=\\left\\{\\varphi(\\cdot,\\cdot)\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}):\\|\\varphi\\|_{(k)}<+\\infty\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3   A class of AIPW estimators and non-asymptotic guarantees ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The main objective of this section is to develop a meta-algorithm to tackle the estimation problem of the off-policy value (1), followed by some key rationale of the proposed procedure as a variancereduction scheme of the standard inverse propensity weighting (IPW) estimator. ", "page_idx": 3}, {"type": "text", "text": "3.1 How can we reduce the variance of the IPW estimator? ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Akin to [42], we consider a class of two-stage estimators obtained from simple perturbations of the IPW estimator. Given any collection $f:=\\big(\\bar{f_{i}}:\\mathbb{X}\\times\\mathbb{O}^{i-1}\\times\\mathbb{A}\\to\\mathbb{R}:i\\in[n]\\big)$ of auxiliary functions, we consider thefollowing perturbedIPW estimator $\\hat{\\tau}_{n}^{f}\\left(\\cdot\\right):\\mathbb{O}^{n}\\to\\mathbb{R}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\tau}_{n}^{f}\\left(\\mathbf{o}_{n}\\right):=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\{\\frac{g\\left(x_{i},a_{i}\\right)y_{i}}{\\pi_{i}^{*}\\left(x_{i},\\mathbf{o}_{i-1};a_{i}\\right)}-f_{i}\\left(x_{i},\\mathbf{o}_{i-1},a_{i}\\right)+\\left\\langle f_{i}\\left(x_{i},\\mathbf{o}_{i-1},\\cdot\\right),\\pi_{i}^{*}\\left(x_{i},\\mathbf{o}_{i-1};\\cdot\\right)\\right\\rangle_{\\lambda_{\\lambda}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For each $i\\in[n]$ ,let $\\nu_{i}\\in\\Delta\\left(\\mathbb{X}\\times\\mathbb{O}^{i-1}\\times\\mathbb{A}\\right)$ denote the joint distribution of $(X_{i},\\mathbf{O}_{i-1},A_{i})$ induced by the adaptive data collection procedure described in Section 2. Then, we arrive at the following result whose proof is deferred to Appendix B.1: ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1.For any collecion $f:=\\left(f_{i}\\in L^{2}\\left(\\nu_{i}\\right):i\\in\\left[n\\right]\\right)$ of auxiliarydeterministic functions, Wwe have $\\mathbb{E}_{\\mathcal{Z}^{\\ast}}\\left[\\hat{\\tau}_{n}^{f}\\left(\\mathbf{O}_{n}\\right)\\right]=\\tau\\left(\\mathcal{Z}^{\\ast}\\right)$ Furthermore, $i f$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\langle f_{i}\\left(x,\\mathbf{o}_{i-1},\\cdot\\right),\\pi_{i}^{*}\\left(x,\\mathbf{o}_{i-1};\\cdot\\right)\\rangle_{\\lambda_{\\mathrm{A}}}=0,\\,\\forall\\left(x,\\mathbf{o}_{i-1}\\right)\\in\\mathbb{X}\\times\\mathbb{O}^{i-1}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for each $i\\in[n]$ then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n\\cdot\\mathrm{Var}_{T^{*}}\\left[\\widehat{\\tau}_{n}^{f}\\left(\\mathbf{O}_{n}\\right)\\right]=\\mathrm{Var}_{X\\sim\\Xi^{*}}\\left[\\left\\langle g(X,\\cdot),\\mu^{*}(X,\\cdot)\\right\\rangle_{\\lambda_{\\mathsf{A}}}\\right]+\\left\\|\\sigma\\right\\|_{(n)}^{2}}\\\\ &{+\\cfrac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{T^{*}}\\left[\\left\\{\\frac{g\\left(X_{i},A_{i}\\right)\\mu^{*}\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}-\\left\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\right\\rangle_{\\lambda_{\\mathsf{A}}}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)\\right\\}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "From the decomposition (4) of the variance of the perturbed IPW estimate ${\\hat{\\tau}}_{n}^{f}\\left(\\mathbf{O}_{n}\\right)$ ,oneobserves that the only term that depends on the collection of auxiliary functions $f$ is the third term. More importantly, the third term is equal to zero if and only if ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{i}\\left(x,\\mathbf{o}_{i-1},a\\right)=f_{i}^{*}\\left(x,\\mathbf{o}_{i-1},a\\right):=\\frac{g\\left(x,a\\right)\\mu^{*}\\left(x,a\\right)}{\\pi_{i}^{*}\\left(x,\\mathbf{o}_{i-1};a\\right)}-\\left<g(x,\\cdot),\\mu^{*}(x,\\cdot)\\right>_{\\lambda_{\\mathrm{A}}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The collection of minimizing functions $f^{*}:=\\left(f_{i}^{*}\\in L^{2}\\left(\\nu_{i}\\right):i\\in\\left[n\\right]\\right)$ yields the oracle estimator $\\hat{\\tau}_{n}^{f^{\\ast}}\\left(\\cdot\\right):\\mathbb{O}^{n}\\to\\mathbb{R}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\hat{\\tau}}_{n}^{f^{*}}\\left(\\mathbf{O}_{n}\\right)={\\frac{1}{n}}\\sum_{i=1}^{n}\\left\\{{\\frac{g\\left(X_{i},A_{i}\\right)\\left\\{Y_{i}-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}}+\\left\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\right\\rangle_{\\lambda_{\\mathrm{A}}}\\right\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "whose variance is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nn\\cdot\\operatorname{Var}_{T^{*}}\\left[\\hat{\\tau}_{n}^{f^{*}}\\left(\\mathbf{O}_{n}\\right)\\right]=v_{*}^{2}:=\\operatorname{Var}_{X\\sim\\Xi^{*}}\\left[\\langle g(X,\\cdot),\\mu^{*}(X,\\cdot)\\rangle_{\\lambda_{\\Lambda}}\\right]+\\left\\Vert\\sigma\\right\\Vert_{(n)}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Require: the dataset ${\\overline{{D=\\{(X_{i},A_{i},Y_{i})\\in\\mathbb{O}:i\\in[n]\\}}}}$ and an evaluation function $g:\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}$   \n1: For each step $i\\in[n]$ , we compute an estimate $\\bar{\\iota_{i}}(\\bar{\\mathbf{O}}_{i-1})\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})$ of the treatment effect based on the sample trajectory $\\mathbf{O}_{i-1}$ up to the $(i-1)$ -th step. // Implement Algorithm 2 as a subroutine; ", "page_idx": 4}, {"type": "text", "text": "2: Consider the AIPW estimator (aka, the doubly-robust DR estimatr) $\\hat{\\tau}_{n}^{\\mathsf{A l P W}}\\left(\\cdot\\right):\\mathbb{O}^{n}\\to\\mathbb{R}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\tau}_{n}^{\\mathsf{A l P W}}\\left(\\mathbf{o}_{n}\\right):=\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\Gamma}_{i}\\left(\\mathbf{o}_{i}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the objects being averaged are the AIPW scores $\\hat{\\Gamma}_{i}(\\cdot):\\mathbb{O}^{i}\\to\\mathbb{R}$ is defined by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{\\Gamma}_{i}\\left(\\mathbf{o}_{i}\\right):=\\frac{g\\left(x_{i},a_{i}\\right)}{\\pi_{i}^{*}\\left(x_{i},\\mathbf{o}_{i-1};a_{i}\\right)}\\left\\{y_{i}-\\hat{\\mu}_{i}\\left(\\mathbf{o}_{i-1}\\right)\\left(x_{i},a_{i}\\right)\\right\\}+\\left\\langle g\\left(x_{i},\\cdot\\right),\\hat{\\mu}_{i}\\left(\\mathbf{o}_{i-1}\\right)\\left(x_{i},\\cdot\\right)\\right\\rangle_{\\lambda_{\\mathbf{i}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3: return the AIPW estimate $\\hat{\\tau}_{n}^{\\mathsf{A l P W}}(\\mathbf{O}_{n})$ ", "page_idx": 4}, {"type": "text", "text": "3.2   The class of augmented IPW estimators ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Since the treatment effect $\\mu^{*}$ is not revealed to the statistician in (6), it is impossible to exactly compute the oracle estimate $\\hat{\\tau}_{n}^{f^{\\ast}}\\left(\\cdot\\right):\\mathbb{O}^{n}\\to\\mathbb{R}$ using only the observational dataset $\\mathbf{O}_{n}$ . Therefore, a natural remedy would be the following two-stage procedure, which is referred to as the augmented inverse propensity weighting (AIPW) estimator or the doubly-robust (DR) estimator [10, 50, 61, 17, 67, 22]: (i) we first compute a sequence of estimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf{O}_{i-1})\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}):i\\in[n]\\}$ of the treatment effect $\\mu^{*}$ ; and then (i) we plug-in these estimates to the equation (6) to construct an approximation to the ideal estimate $\\hat{\\tau}_{n}^{f^{*}}\\left(\\mathbf{O}_{n}\\right)$ . We summarize this two-stage procedure in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "We pause here to compare our problem setting and algorithms with the most relevant work [42]. We focus on off-policy estimation with adaptively collected data, which is technically more challenging compared to i.i.d. data considered in [42]. In the case with i.i.d. data, [42] proposed a natural approach to construct a class of two-stage estimators as follows: (a) compute an estimate $\\hat{\\mu}$ of the treatment effect $\\mu^{*}$ utilizing part of the dataset; and (b) substitute this estimate in the equation (6) of the oracle estimator. Note that the authors use the cross-fitting approach [5, 6], which allows to make full use of data to maintain efficiency and statistical power of machine learning algorithms for estimation of nuisance parameters while reducing overfitting bias. However, the cross-fitting strategy heavily relies on the i.i.d. nature of the data collection mechanism and therefore one cannot use it in the setting with adaptively collected data. Instead, we construct an estimate $\\hat{\\mu}_{i}$ of the treatment effect $\\mu^{*}$ based on the sample trajectory $\\mathbf{O}_{i-1}$ at each stage and then substitute these estimates in the equation (6). This is one of main contributions to address the adaptive nature of our data generating mechanism. We will make use of the framework of online learning to construct a sequence of estimates for the treatment effect $\\mu^{*}$ ", "page_idx": 4}, {"type": "text", "text": "3.3  Theoretical guarantees of Algorithm 1 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we provide statistical guarantees for the class of AIPW estimators for dealing with the estimation problem of the off-policy value (1). The main result of this section can be summarized as the following non-asymptotic upper bound on the mean-squared error (MSE) of Algorithm 1: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Non-asymptotic upper bound on the MSE of the AIPW estimator). For any sequence ofestimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf O_{i-1})\\in(\\mathbb X\\times\\bar{\\mathbb{A}}\\to\\mathbb R):i\\in[n]\\}$ for the treatment effect $\\mu^{*}$ theAIPWestimator (8) has the MSE bounded above by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\left\\{\\hat{\\tau}_{n}^{\\mathsf{A l P W}}\\left(\\mathbf{O}_{n}\\right)-\\tau\\left(Z^{*}\\right)\\right\\}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{n}\\left\\{v_{*}^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that the non-asymptotic upper bound (10) on the MSE for the class of AIPW estimators (8) consists of two terms, both of which have natural interpretations. The first term $v_{*}^{2}$ correspondstothe ", "page_idx": 4}, {"type": "text", "text": "Algorithm 2 Online non-parametric regression protocol for estimation of the treatment effect. ", "page_idx": 5}, {"type": "text", "text": "Require: the number of rounds $n\\in\\mathbb N$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "1: for $i=1,2,\\cdots\\,,n$ ,do ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "2: The learner selects a point $\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\in\\left(\\mathbb{X}\\times\\mathbb{A}\\rightarrow\\mathbb{R}\\right)$ based on the sample trajectory $\\mathbf{O}_{i-1}$   \n3: The environment then picks a loss function $l_{i}(\\cdot):(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})\\to\\mathbb{R}$ defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\nl_{i}(\\mu):=\\frac{g^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf O_{i-1};A_{i}\\right)}\\left\\{Y_{i}-\\mu\\left(X_{i},A_{i}\\right)\\right\\}^{2},\\,\\forall\\mu(\\cdot,\\cdot)\\in\\left(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "4: end for ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5: return the sequence of estimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf{O}_{i-1})\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}):i\\in[n]\\}$ of the treatment effect. ", "page_idx": 5}, {"type": "text", "text": "optimal variance (7) achievable by the oracle estimator, and the second term ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "measures the average estimation error of the estimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf O_{i-1})\\in(\\mathbb X\\times\\mathbb A\\to\\mathbb R):i\\in[n]\\}$ of $\\mu^{*}$ Of primary interest to us is a subsequent upper bounding argument based on the MSE bound (10) in the finite sample regime: in particular, to minimize the RHS of (10), one needs to choose a sequence ofestimates $\\{\\hat{\\mu}_{i}\\,({\\bar{\\mathbf O_{i-1}}})\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}):i\\in[n]\\}$ which minimizes the second term (11). ", "page_idx": 5}, {"type": "text", "text": "3.4   Reduction to online non-parametric regression ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let us now focus on constructing a sequence of estimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf{O}_{i-1})\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}):i\\in[n]\\}$ of the treatment effect and upper bounding the estimation error (11) in the MSE bound (10). To this end, we borrow ideas from the literature of online non-parametric regression [45]. ", "page_idx": 5}, {"type": "text", "text": "To begin with, we consider an $n$ -round turn-based game between the learner and the environment; see Algorithm 2 for the details. Then, one can readily observe for any $\\mu(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}$ wehave ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\mathbf{Z}^{*}}\\left[l_{i}(\\mu)\\right](\\mathcal{H}_{i-1},X_{i},A_{i})]}\\\\ &{=\\frac{g^{2}\\,\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\,\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)+\\left\\{\\mu\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In the current turn-based game, our natural goal is to minimize the learner's static regret against the best fixed action in hindsight belonging to a pre-specified function class $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\bar{\\mathbb{A}}\\to\\bar{\\mathbb{R}})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{Regret}\\left(n,{\\mathcal{F}};A\\right):=\\sum_{i=1}^{n}l_{i}\\left\\{{\\hat{\\mu}}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}-\\operatorname*{inf}_{\\mu\\in{\\mathcal{F}}}\\sum_{i=1}^{n}l_{i}(\\mu),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Where $\\boldsymbol{\\mathcal{A}}$ denotes the learner's online non-parametric regression algorithm that returns a sequence of estimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf O_{i-1}):i\\in[n]\\}$ for the treatment effect. Then, one can establish the following oracle inequality that demystifies a relationship between estimation problem of the off-policy value and the online non-parametric regression protocol. See Appendix B.3 for the proof. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Oracle inequality for the class of AIPW estimators). The AIPW estimator (8) using thesequenceofestimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf{O}_{i-1})\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}):i\\in[n]\\}$ ofthetreatmenteffect $\\mu^{*}$ produced by the onlinenon-parametricregression algorithm $\\boldsymbol{\\mathcal{A}}$ enjoys thefollowingupper bound on theMSE: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\left\\{\\widehat{\\tau}_{n}^{\\mathsf{A I P W}}\\left(\\mathbf{O}_{n}\\right)-\\tau\\left(\\mathbb{Z}^{*}\\right)\\right\\}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{n}\\left(v_{*}^{2}+\\frac{1}{n}\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\mathrm{Regret}\\left(n,\\mathcal{F};A\\right)\\right]+\\operatorname*{inf}\\left\\{\\left\\Vert\\mu-\\mu^{*}\\right\\Vert_{(n)}^{2}:\\mu\\in\\mathcal{F}\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "A few remarks are in order. Apart from the optimal variance $v_{*}^{2}$ , the RHS of the bound (15) contains two additional terms: (i) the expected regret relative to the number of rounds $n$ , where the expected value is taken over $\\mathbf{O}_{n}\\sim\\mathbb{P}_{\\mathcal{Z}^{\\ast}}^{n}(\\bar{\\cdot})$ ; and (i) the approximation error under the $\\left\\|\\cdot\\right\\|_{(n)}$ -norm. Given any fixed function class $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})$ , if we consider the large sample size regime, i.e., the sample size $n$ is sufficiently large, then one can see that the asymptotic variance of the AIPW estimator (8) is asymptotically the same as $v_{*}^{2}+\\operatorname*{inf}\\left\\{\\|\\mu-\\mu^{*}\\|_{(n)}^{2}:\\mu\\in{\\bar{\\mathcal{F}}}\\right\\}$ provided that the online non-parametric regression algorithm $\\boldsymbol{\\mathcal{A}}$ exhibits a $^{n o}$ -regret learning dynamics, i.e., $\\mathbb{E}_{\\mathbb{Z}^{*}}$ [Regret $(n,{\\mathcal{F}};A)]=o(n)$ as $n\\to\\infty$ . Consequently, the AIPW estimator (8) may suffer from an efficiency loss which depends on how well the unknown treatment effect $\\mu^{*}$ can be approximated by a member of the function class $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})$ under the $\\left\\|\\cdot\\right\\|_{(n)}$ -norm. Hence, any contribution to the MSE bound of the AIPW estimator (8) in addition to the efficient variance $v_{*}^{2}$ primarily relies on the approximation error associated with approximating the treatment effect $\\mu^{*}$ utilizing a provided function class $\\mathcal{F}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.5   Consequences for particular outcome models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The main goal of this section is to illustrate the consequences of our general theory developed in Section 3 so far for several concrete classes of outcome models. Throughout this section, we consider thecaseforwhich $\\mathbb{Y}=[-L,L]$ for someconstant $L\\in(0,+\\infty)$ , and impose the following condition: Assumption 1 (Strict overlap condition). The likelihood ratios are uniformly bounded by a universal constant $B\\in(0,+\\infty)$ ,i.e.,for every $i\\in[n]$ ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left|\\frac{g\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},{\\bf O}_{i-1};A_{i}\\right)}\\right|\\leq B\\quad\\mathbb{P}_{\\mathbb{Z}^{*}}^{n}\\mathrm{-almost~surely}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We note that Assumption 1 is often referred to as the strict overlap condition in the literature of causal inference [20, 32, 66, 36, 11]. At this point, we emphasize that Assumption 1 is necessary to produce main consequences of the oracle inequality for the class of AIPW estimators (Theorem 3.2) that we discuss in the ensuing subsections: Theorems 3.3, 3.4, and the arguments throughout Appendix B.6. ", "page_idx": 6}, {"type": "text", "text": "3.5.1  Tabular case of the outcome model ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We embark on our discussion about the consequences of our theory established in Sections 3.3 and 3.4 for one of the simplest case of the outcome model satisfying the following assumption. ", "page_idx": 6}, {"type": "text", "text": "Assumption 2 (Tabular setting of the outcome model). The state-action space $\\mathbb{X}\\times\\mathbb{A}$ is a finite set. If we compute the gradient of the loss function (14), we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla l_{i}(\\mu)=\\frac{2g^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi^{*}\\right)^{2}\\left(X_{i},\\mathbf O_{i-1};A_{i}\\right)}\\left\\{\\mu\\left(X_{i},A_{i}\\right)-Y_{i}\\right\\}\\delta_{\\left(X_{i},A_{i}\\right)},\\;\\forall\\mu\\in\\mathbb{R}^{\\mathbb{X}\\times\\mathbb{A}},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\delta_{(X_{i},A_{i})}\\in\\mathbb{R}^{\\mathbb{X}\\times\\mathbb{A}}$ is the point-mass vector at the $i$ -th state-action pair in the sample trajectory, i.e., $\\delta_{(X_{i},A_{i})}(x,a):=1$ \u00fc $\\left(\\boldsymbol{x},\\boldsymbol{a}\\right)=\\left(X_{i},A_{i}\\right)$ $\\delta_{(X_{i},A_{i})}(x,a):=0$ otherwise. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 3 Online gradient descent (OGD) algorithm for the finite state-action space. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Require: the function class $\\mathcal{F}\\subseteq[-L,L]^{\\mathbb{X}\\times\\mathbb{A}}$ the tal number of rounds $n\\in\\mathbb N$ and a sequence of learning rates $\\{\\eta_{i}\\in(0,+\\infty)\\stackrel{.}{:}i\\in[\\dot{n}-1]\\}$   \n1: We first choose an initial point $\\hat{\\mu}_{1}(\\mathcal{O})\\in\\mathcal{F}$ arbitrarily;   \n2: for $i=1,2,\\cdots\\,,n-1$ ,do   \n3: Observe a triple $(X_{i},A_{i},Y_{i})\\in\\mathbb{O}$ .\uff0c   \n4: Update $\\hat{\\mu}_{i+1}\\left(\\mathbf{O}_{i}\\right)\\in\\mathcal{F}$ according to the following OGD update rule: $\\begin{array}{l}{{\\displaystyle\\hat{\\mu}_{i+1}\\left({\\bf O}_{i}\\right)=\\Pi_{\\mathcal{F}}\\left[\\hat{\\mu}_{i}\\left({\\bf O}_{i-1}\\right)-\\eta_{i}\\nabla l_{i}\\left\\{\\hat{\\mu}_{i}\\left({\\bf O}_{i-1}\\right)\\right\\}\\right]}}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~=\\Pi_{\\mathcal{F}}\\left[\\hat{\\mu}_{i}\\left({\\bf O}_{i-1}\\right)-\\frac{2\\eta_{i}\\cdot g^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},{\\bf O}_{i-1};A_{i}\\right)}\\left\\{\\hat{\\mu}_{i}\\left({\\bf O}_{i-1}\\right)-Y_{i}\\right\\}\\delta_{(X_{i},A_{i})}\\right],}}\\end{array}$ (18) where $\\Pi_{\\mathcal{F}}[\\cdot]:\\mathbb{R}^{\\mathbb{X}\\times\\mathbb{A}}\\rightarrow\\mathcal{F}$ denotes the projection map of $\\mathbb{R}^{\\mathbb{X}\\times\\mathbb{A}}$ onto the function space $\\mathcal{F}$   \n5: end for   \n6: return the sequence of estimates $\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf O_{i-1}\\right)\\in\\mathcal{F}:i\\in\\left[n\\right]\\right\\}$ of the treatment effect $\\mu^{*}$ ", "page_idx": 6}, {"type": "text", "text": "Now, it is time to put forward an online contextual learning algorithm aimed at producing a sequence ofestimatesof $\\mu^{*}$ with a no-regret learning guarantee. For the tabular case, the online non-parametric regression problem can be resolved through standard online convex optimization (OCO) algorithms. In particular, we employ the online gradient descent (OGD) algorithm (see Algorithm 3) as a subroutine of Algorithm 1. By leveraging standard results on regret analysis of OCO algorithms, one can obtain the following regret bound, which guarantees a no-regret learning dynamics of Algorithm 3. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Theorem 3.3 (Regret guarantee of Algorithm 3). Under Assumptions $^{\\,l}$ and 2, the OGD algorithm (Algorithm3)withlearningrates $\\begin{array}{r}{\\left\\{\\eta_{i}:=\\frac{\\operatorname{diam}(\\mathcal{F})}{4L B^{2}\\sqrt{i}}:i\\in[n]\\right\\}}\\end{array}$ guarantees ", "page_idx": 7}, {"type": "text", "text": "where diam $\\mathfrak{i}(\\mathcal{F}):=\\operatorname*{sup}\\left\\{\\|\\mu\\|_{2}:\\mu\\in\\mathcal{F}\\right\\}$ denotes the diameter of $\\mathcal{F}\\subseteq[-L,L]^{\\mathbb{X}\\times\\mathbb{A}}$ ", "page_idx": 7}, {"type": "text", "text": "See Appendix B.4 for the proof of Theorem 3.3. Combining the regret guarantee (19) of Algorithm 3 together with the MSE bound (15) in Theorem 3.2, one can establish a concrete upper bound on the MSE of the AIPW estimator (8) by utilizing Algorithm 3 to produce a sequence of estimates for the treatment effect $\\mu^{*}$ ", "page_idx": 7}, {"type": "text", "text": "3.5.2 Linear function approximation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We next move on to outcome models where the state-action space $\\mathbb{X}\\times\\mathbb{A}$ can be infinite. We begin with the simplest case: the class of linear outcome functions. Let $\\phi(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}^{d}$ be a known feature map such that sup $\\{\\|\\phi(x,a)\\|_{2}:(x,a)\\in\\mathbb{X}\\times\\mathbb{A}\\}\\leq1$ , and we consider the functions that are linear in this representation: $f_{\\theta}(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}$ ,where $f_{\\pmb\\theta}(x,a):=\\pmb\\theta^{\\top}\\phi(x,a)$ for some parameter vector $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ . Given a radius $R>0$ , we define the function class ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{F}_{\\mathrm{lin}}:=\\left\\{f_{\\theta}(\\cdot,\\cdot)\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}):\\theta\\in\\Theta:=\\overline{{\\mathbb{B}\\left(\\mathbf{0}_{d};R\\right)}}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\overline{{\\mathbb{B}\\left(\\mathbf{0}_{d};R\\right)}}:=\\left\\{\\mathbf{u}\\in\\mathbb{R}^{d}:\\|\\mathbf{u}\\|_{2}\\leq R\\right\\}$ . With this linear function approximation framework, let us consider the following OCO model: at the $i$ -th stage, ", "page_idx": 7}, {"type": "text", "text": "(i) the learner first chooses a point $\\hat{\\theta}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\in\\Theta$ (i) the environment then picks a loss function $\\begin{array}{r}{\\mathcal{L}_{i}(\\cdot):\\Theta\\rightarrow\\mathbb{R}}\\end{array}$ defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{i}(\\pmb{\\theta}):=\\frac{g^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\left\\{Y_{i}-\\pmb{\\theta}^{\\top}\\phi\\left(X_{i},A_{i}\\right)\\right\\}^{2},\\,\\forall\\pmb{\\theta}\\in\\Theta,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "and our ga is to produce a sequence of estimates $\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right):=\\left\\{\\hat{\\pmb{\\theta}}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}^{\\top}\\phi\\in\\mathcal{F}_{\\mathrm{lin}}:i\\in\\left[n\\right]\\right\\}$ for the treatment effect $\\mu^{*}$ after $n$ rounds of the above-mentioned OCO model which minimizes the learner's regret against the best fixed action in hindsight: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Regret}\\left(n,\\mathcal{F}_{\\mathrm{lin}};\\boldsymbol{A}\\right)=\\displaystyle\\sum_{i=1}^{n}l_{i}\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}-\\operatorname*{inf}\\left\\{\\displaystyle\\sum_{i=1}^{n}l_{i}(\\mu):\\mu\\in\\mathcal{F}\\right\\}}\\\\ &{\\quad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\sum_{i=1}^{n}\\mathcal{L}_{i}\\left\\{\\hat{\\pmb{\\theta}}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}-\\operatorname*{inf}\\left\\{\\displaystyle\\sum_{i=1}^{n}\\mathcal{L}_{i}(\\pmb{\\theta}):\\pmb{\\theta}\\in\\Theta\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Where $\\boldsymbol{\\mathcal{A}}$ is the learner's OCO algorithm whose output is a sequence $\\left\\{\\hat{\\pmb{\\theta}}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\in\\Theta:i\\in\\left[n\\right]\\right\\}$ parameters. If we compute the gradient of the loss function (21), one has ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta)=\\frac{2g^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\left\\{\\theta^{\\top}\\phi\\left(X_{i},A_{i}\\right)-Y_{i}\\right\\}\\phi\\left(X_{i},A_{i}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For the current linear function approximation setting, we implement the OGD algorithm (Algorithm 4) as a sub-routine of Algorithm 1. By using the same arguments as in Section 3.5.1, one can reproduce the following regret guarantee of Algorithm 4 whose proof is available at Appendix B.5. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3.4 (Regret guarantee of Algorithm 4). With Assumption $^{\\,l}$ ,theOGDalgorithm(Algorithm 4) with learning rates $\\begin{array}{r}{\\left\\{\\eta_{i}:=\\frac{R}{B^{2}(L+R)\\sqrt{i}}:i\\in[n]\\right\\}}\\end{array}$ guarantees ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{Regret}\\left(n,\\mathcal{F}_{\\mathrm{lin}};\\operatorname{OGD}\\right)\\leq6B^{2}R(L+R)\\sqrt{n}\\quad\\mathbb{P}_{\\mathbb{Z}^{*}}^{n}\\mathrm{-almost~surely.}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Require: the radius $R>0$ of the parameter space, the number of rounds $n\\in\\mathbb N$ , and a sequence of learning rates $\\{\\eta_{i}\\in(0,+\\infty):i\\in[n-1]\\}$   \n1: We first choose an arbitrary initial point $\\hat{\\pmb{\\theta}}_{1}(\\mathcal{O})\\in\\Theta$ , where $\\Theta:=\\overline{{\\mathbb{B}\\left(\\mathbf{0}_{d};R\\right)}}$   \n2: for $i=1,2,\\cdots\\,,n-1$ do   \n3:  Observe a triple $(X_{i},A_{i},Y_{i})\\in\\mathbb{O}$   \n4: Update $\\hat{\\pmb{\\theta}}_{i+1}\\left(\\mathbf{O}_{i}\\right)\\in\\Theta$ according to the following OGD update rule: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\pmb{\\theta}}_{i+1}\\left(\\pmb{\\Omega}_{i}\\right)=\\Pi_{\\Theta}\\left[\\hat{\\pmb{\\theta}}_{i}\\left(\\pmb{\\Omega}_{i-1}\\right)-\\eta_{i}\\nabla_{\\pmb{\\theta}}\\mathcal{L}_{i}\\left\\{\\hat{\\pmb{\\theta}}_{i}\\left(\\pmb{\\Omega}_{i-1}\\right)\\right\\}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\Pi_{\\Theta}[\\cdot]:\\mathbb{R}^{d}\\rightarrow\\Theta$ denotes the projection map of $\\mathbb{R}^{d}$ onto the parameter space $\\Theta$ 5: end for ", "page_idx": 8}, {"type": "text", "text": "6: return he estimates $\\left\\{{\\hat{\\mu}}_{i}\\left(\\mathbf{O}_{i-1}\\right):=\\left\\{{\\hat{\\pmb{\\theta}}}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}^{\\top}\\phi\\in{\\mathcal{F}}_{\\mathrm{lin}}:i\\in\\left[n\\right]\\right\\}$ of the reatment effet. ", "page_idx": 8}, {"type": "text", "text": "General function approximation  Lastly, we demonstrate the consequences of our general theory established in Sections 3.3 and 3.4 for the case of general function approximation: the function class $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to[-L,L])$ can be arbitrarily chosen. Our further discussion this case heavily relies on the basic theory of online non-parametric regression from [45] whose technical details are rather long and complicated. So, we defer our detailed inspection on the case of general function approximation to AppendixB.6. ", "page_idx": 8}, {"type": "text", "text": "4 Lower bounds: local minimax risk ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We turn our attention to a local minimax lower bound for estimating the off-policy value $\\tau^{*}=\\tau\\left(\\mathcal{L}^{*}\\right)$ Here, we aim at establishing lower bounds that hold uniformly over all estimators that are permitted to know both the propensity scores $\\{\\pi_{i}^{*}\\left(X_{i},\\mathbf O_{i-1};A_{i}\\right):i\\in\\dot{[n]}\\}$ and the evaluation function $g$ We assume the existence of a constant $K\\geq1$ andreferenceMarkovpolicies $\\left\\{{\\overline{{\\Pi}}}_{i}:{\\mathbb X}\\to\\Delta({\\mathbb A}):i\\in[n]\\right\\}$ such that $\\overline{{\\Pi}}_{i}\\left(\\,\\cdot\\,\\vert\\,x\\right)\\ll\\lambda_{\\mathbb{A}}(\\cdot)$ for $(x,i)\\in\\mathbb{X}\\times[n]$ , and ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\frac{1}{K}\\le\\frac{\\pi_{i}\\left(x,a\\right)}{\\pi_{i}^{*}\\left(x,\\mathbf{o}_{i-1};a\\right)}\\le K\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for all $(x,\\mathbf o_{i-1},a)\\in\\mathbb X\\times\\mathbb O^{i-1}\\times\\mathbb A$ where $\\begin{array}{r}{\\overline{{\\pi}}_{i}\\left(x,\\cdot\\right):=\\frac{\\mathrm{d}\\overline{{\\Pi}}_{i}\\left(\\,\\cdot|x\\right)}{\\mathrm{d}\\lambda_{\\beta_{\\cdot}}}:\\mathbb{A}\\rightarrow\\mathbb{R}_{+}}\\end{array}$ dll(-l#) : A \u2192 R+ for each context c E X. Proximity of behavioral policies to certain Markov policies is often assumed under adaptive data collection procedures. For instance, in Theorem $^{\\,l}$ of [67], the authors assumed that the sequence of behavior policies is eventually Markov; see the equation (8) therein. ", "page_idx": 8}, {"type": "text", "text": "4.1  Instance-dependent local minimax lower bounds ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Given any problem instance $\\mathcal{T}^{*}=(\\Xi^{*},\\Gamma^{*})\\in\\mathbb{I}$ and an error function $\\delta:\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}_{+}$ , we consider the following local neighborhoods: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal N}(\\Xi^{*}):=\\left\\{\\Xi\\in\\Delta(\\mathbb{X}):\\mathrm{KL}\\left(\\Xi\\left\\Vert\\Xi^{*}\\right)\\leq\\displaystyle\\frac{1}{n}\\right\\};}\\\\ &{{\\mathcal N}_{\\delta}\\left(\\Gamma^{*}\\right):=\\left\\{\\Gamma\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\Delta(\\mathbb{Y})):|\\mu(\\Gamma)(x,a)-\\mu\\left(\\Gamma^{*}\\right)(x,a)|\\leq\\delta(x,a),\\,\\forall(x,a)\\in\\mathbb{X}\\times\\mathbb{A}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where for any given $\\Gamma:\\mathbb{X}\\times\\mathbb{A}\\rightarrow\\Delta(\\mathbb{Y})$ ,let $\\textstyle\\mu(\\Gamma)(x,a):=\\int_{\\mathbb{Y}}y\\Gamma\\,(\\,\\mathrm{d}y\\,|\\,x,a)$ for each $(x,a)\\in\\mathbb{X}\\times\\mathbb{A}$ Our goal is to lower bound the following local minimax risk: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{M}_{n}\\left(\\mathcal{C}_{\\delta}\\left(\\mathbb{Z}^{*}\\right)\\right):=\\operatorname*{inf}_{\\hat{\\tau}_{n}\\left(\\cdot\\right):\\mathbb{Q}^{n}\\rightarrow\\mathbb{R}}\\left(\\operatorname*{sup}_{\\mathbb{Z}\\in\\mathcal{C}_{\\delta}\\left(\\mathbb{Z}^{*}\\right)}\\mathbb{E}_{\\mathbb{Z}}\\left[\\left\\{\\hat{\\tau}_{n}\\left(\\mathbf{O}_{n}\\right)-\\tau\\left(\\mathbb{Z}\\right)\\right\\}^{2}\\right]\\right),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{C}_{\\delta}\\left(\\mathcal{T}^{*}\\right):=\\mathcal{N}\\left(\\Xi^{*}\\right)\\times\\mathcal{N}_{\\delta}\\left(\\Gamma^{*}\\right)\\subseteq\\mathbb{I}.}\\end{array}$ We now specify some assumptions necessary for lower bounding the local minimax risk (26). Prior to this, we introduce a new important notation: given any random variable $Y\\in\\mathbb{L}^{4}\\left(\\Omega,\\mathcal{F},\\ l^{\\flat}\\right)$ defined on a probability space $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ , its $(2,4)$ -moment ratio is defined as |Y ll2\u21924 := $\\begin{array}{r}{\\|Y\\|_{2\\to4}:=\\frac{\\sqrt{\\mathbb{E}[Y^{4}]}}{\\mathbb{E}[Y^{2}]}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Assumption 3. Let $h(x):=\\langle g(x,\\cdot),\\mu^{*}(x,\\cdot)\\rangle_{\\lambda_{\\hat{\\lambda}}}-\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\langle g(X,\\cdot),\\mu^{*}(X,\\cdot)\\rangle_{\\lambda_{\\hat{\\lambda}}}\\right].$ We assume that $\\begin{array}{r}{H_{2\\rightarrow4}:=\\|h\\|_{2\\rightarrow4}=\\frac{\\sqrt{\\mathbb{E}_{X\\sim\\Xi^{*}}[h^{4}(X)]}}{\\mathbb{E}_{X\\sim\\Xi^{*}}[h^{2}(X)]}<+\\infty}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "We next make an assumption on a lower bound on the local neighborhood size: ", "page_idx": 9}, {"type": "text", "text": "Assumption 4. The neighborhood function $\\delta(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}_{+}$ satisfies the lower bound ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\sqrt{n}\\cdot\\delta(x,a)\\geq\\frac{|g(x,a)|\\,\\sigma^{2}(x,a)}{\\overline{{\\pi}}_{i}(x,a)\\,\\|\\sigma\\|_{(n)}}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for all $(x,a,i)\\in\\mathbb{X}\\times\\mathbb{A}\\times[n]$ ", "page_idx": 9}, {"type": "text", "text": "We note that Assumptions 3 and 4 are analogues of Assumptions (MR) and (LN) considered in [42], respectively, for the case of adaptively collected data. Under these assumptions, one can prove the following lower bound on the local minimax risk over $\\mathcal{C}_{\\delta}\\left(\\mathcal{T}^{*}\\right)$ ", "page_idx": 9}, {"type": "text", "text": "Theorem 4.1. Under Assumptions 3 and 4, the local minimax risk over $\\mathcal{C}_{\\delta}\\left(\\mathcal{T}^{*}\\right)$ is lowerbounded by ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\mathcal{M}_{n}\\left(\\mathcal{C}_{\\delta}\\left(\\mathcal{Z}^{*}\\right)\\right)\\geq\\mathcal{C}(K)\\cdot\\frac{v_{*}^{2}}{n},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\mathcal{C}(K)>0$ is a universal constant that only depends on the data coverage constant $K\\geq1$ of the referenceMarkov policies $\\left\\{\\overline{{\\Pi}}_{i}(\\cdot):\\mathbb{X}\\to\\Delta(\\mathbb{A}):i\\in[n]\\right\\}$ defined in (25). ", "page_idx": 9}, {"type": "text", "text": "The proof of Theorem 4.1 can be found in Appendix C.1. This result delivers a key message: the term $\\frac{v_{*}^{2}}{n}$ including the sequentially weighted $\\ell_{2}$ -norm is indeed the fundamental limit for estimating the linear functional based on adaptively collected data. Our results can be viewed as a generalization of those developed in [42] for the case of i.i.d. data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Jeonghwan Lee was partially supported by the Kwanjeong Educational Foundation. Cong Ma was partially supported by the National Science Foundation via grant DMS-2311127. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Aman Agarwal, Soumya Basu, Tobias Schnabel, and Thorsten Joachims. Effective evaluation using logged bandit feedback from multiple loggers. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 687-696, 2017.   \n[2] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning, pages 127-135. PMLR, 2013.   \n[3]  Timothy B Armstrong and Michal Kolesar. Finite-sample optimal estimation and inference on average treatment effects under unconfoundedness. Econometrica, 89(3):1141-1177, 2021.   \n[4] L\u00e9on Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X Charles, D Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14(11), 2013.   \n[5]  Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Dufo, Christian Hansen, and Whitney Newey. Double/debiased/neyman machine learning of treatment effects. American Economic Review, 107(5):261-265, 2017.   \n[6] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters, 2018.   \n[7]  Thomas Cook, Alan Mishler, and Aaditya Ramdas. Semiparametric efficient inference in adaptive experiments. In Causal Learning and Reasoning, pages 1033-1064. PMLR, 2024. [8]  Jessica Dai, Paula Gradu, and Christopher Harshaw. Clip-ogd: An experimental design for adaptive neyman allocation in sequential experiments. Advances in Neural Information Processing Systems, 36, 2024.   \n[9]  Miroslav Dudik, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation and optimization. 2014.   \n[10] Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv preprint arXiv:1103.4601, 2011.   \n[11]  Alexander D'Amour, Peng Ding, Avi Feller, Lihua Lei, and Jasjeet Sekhon. Overlap in observational studies with high-dimensional covariates. Journal of Econometrics, 221(2):644 654, 2021.   \n[12] Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-policy evaluation. In International Conference on Machine Learning, pages 1447-1456. PMLR, 2018.   \n[13] Nicolo Felicioni, Maurizio Ferrari Dacrema, Marcello Restelli, and Paolo Cremonesi. Offpolicy evaluation with deficient support using side information. Advances in Neural Information Processing Systems, 35:30250-30264, 2022.   \n[14]  Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International statistical review, 70(3):419-435, 2002.   \n[15] Evarist Gin\u00e9 and Joel Zinn. Some limit theorems for empirical processes.  The Annals of Probability, pages 929-989, 1984.   \n[16] Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confdence intervals for policy evaluation in adaptive experiments.\u201d arxiv e-prints. arXiv preprint arXiv:1911.02768, 2019.   \n[17]  Vitor Hadad, David A Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. Proceedings of the national academy of sciences, 118(15):e2014602118, 2021.   \n[18]  Elad Hazan et al. Introduction to online convex optimization. Foundations and Trends? in Optimization, 2(3-4):157-325, 2016.   \n[19] Tim Hesterberg. Weighted average importance sampling and defensive mixture distributions. Technometrics, 37(2):185-194, 1995.   \n[20]  Keisuke Hirano, Guido W Imbens, and Geert Ridder. Efficient estimation of average treatment effects using the estimated propensity score. Econometrica, 71(4):1161-1189, 2003.   \n[21]  Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American statistical Association, 47(260):663-685, 1952.   \n[22] Steven R. Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. Time-uniform, nonparametric, nonasymptotic confidence sequences. The Annals of Statistics, 49(2):1055 - 1080, 2021.   \n[23]  Guido W Imbens. Nonparametric estimation of average treatment effects under exogeneity: A review. Review of Economics and statistics, 86(1):4-29, 2004.   \n[24]  Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015.   \n[25]  Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics, 17(2):295-311, 2008.   \n[26]  Ying Jin, Zhimei Ren, Zhuoran Yang, and Zhaoran Wang. Policy learning\" without\"overlap: Pessimism and generalized empirical bernstein's inequality. arXiv preprint arXiv:2212.09900, 2022.   \n[27] Nathan Kallus, Yuta Saito, and Masatoshi Uehara. Optimal off-policy evaluation from multiple logging policies. In International Conference on Machine Learning, pages 5247-5256. PMLR, 2021.   \n[28]  Nathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments. In International conference on artificial intelligence and statistics, pages 1243-1251. PMLR, 2018.   \n[29]  Joseph DY Kang and Joseph L Schafer. Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data. 2007.   \n[30]  Nikos Karampatziakis, Paul Mineiro, and Aaditya Ramdas. Off-policy confidence sequences. In International Conference on Machine Learning, pages 5301-5310. PMLR, 2021.   \n[31] Masahiro Kato, Takuya Ishihara, Junya Honda, and Yusuke Narita. Efficient adaptive experimental design for average treatment effect estimation. arXiv preprint arXiv:2002.05308, 2020.   \n[32] Shakeeb Khan and Elie Tamer. Iregularidentifcation, support conditions, and inverse weight estimation. Econometrica, 78(6):2021-2042, 2010.   \n[33]  Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4-22, 1985.   \n[34]  Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020.   \n[35] Haanvid Lee, Jongmin Lee, Yunseon Choi, Wonseok Jeon, Byung-Jun Lee, Yung-Kyun Noh, and Kee-Eung Kim. Local metric learning for off-policy evaluation in contextual bandits with continuous actions. Advances in Neural Information Processing Systems, 35:3913-3925, 2022.   \n[36] Lihua Lei, Alexander D'Amour, Peng Ding, Avi Feller, and Jasjeet Sekhon. Distribution-free assessment of population overlap in observational studies. Technical report, Working paper, Stanford University, 2021.   \n[37] Gen Li and Weichen Wu. Sharp high-probability sample complexities for policy evaluation with linear function approximation. arXivorg, 2023.   \n[38] Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In Artificial Intelligence and Statistics, pages 608-616. PMLR, 2015.   \n[39] Roderick JA Little and Donald B Rubin. Statistical analysis with missing data, volume 793. John Wiley & Sons, 2019.   \n[40]  Alexander R Luedtke and Mark J Van Der Laan. Statistical inference for the mean outcome under a possibly non-unique optimal treatment strategy. Annals of statistics, 44(2):713, 2016.   \n[41]  Cong Ma, Banghua Zhu, Jiantao Jiao, and Martin J Wainwright. Minimax off-policy evaluation for multi-armed bandits. IEEE Transactions on Information Theory, 68(8):5314-5339, 2022.   \n[42]  Wenlong Mou, Martin J Wainwright, and Peter L Bartlett. Off-policy estimation of linear functionals: Non-asymptotic theory for semi-parametric effciency. arXiv preprint arXiv:2209.13075, 2022.   \n[43]  Yusuke Narita, Shota Yasui, and Kohei Yata. Efficient counterfactual learning from bandit feedback In Proceedings of the AAAl Conference on Artifcial Inteligence, volume 33, pages 4634-4641, 2019.   \n[44] Jie Peng, Hao Zou, Jiashuo Liu, Shaoming Li, Yibao Jiang, Jian Pei, and Peng Cui. Offine policy evaluation in large action spaces via outcome-oriented action grouping. In Proceedings of the ACM Web Conference 2023, pages 1220-1230, 2023.   \n[45]  Alexander Rakhlin and Karthik Sridharan. Online non-parametric regresson. In Conference on Learning Theory, pages 1232-1264. PMLR, 2014.   \n[46]  Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform martingale laws of large numbers. Probability theory and related fields, 161:111-153, 2015.   \n[47]  Sasha Rakhlin, Ohad Shamir, and Karthik Sridharan. Relax and randomize: From value to algorithms. Advances in Neural Information Processing Systems, 25, 2012.   \n[48] James Robins, Mariela Sued, Quanhong Lei-Gomez, and Andrea Rotnitzky. Comment: Performance of double-robust estimators when\" inverse probability\" weights are highly variable. Statistical Science, 22(4):544-559, 2007.   \n[49]  James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression models with missing data. Journal of the American Statistical Association, 90(429):122-129, 1995.   \n[50] James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients when some regressors are not always observed. Journal of the American statistical Association, 89(427):846-866, 1994.   \n[51]  James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Analysis of semiparametric regression models for repeated outcomes in the presence of missing data. Journal of the american statistical association, 90(429):106-121, 1995.   \n[52] Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on thompson sampling. Foundations and Trends? in Machine Learning, 11(1):1-96, 2018.   \n[53]  Yuta Saito and Thorsten Joachims. Off-policy evaluation for large action spaces via embeddings. arXiv preprint arXiv:2202.06317, 2022.   \n[54]  Yuta Saito, Qingyang Ren, and Thorsten Joachims. Off-policy evaluation for large action spaces via conjunct effect modeling. In international conference on Machine learning, pages 29734-29759. PMLR, 2023.   \n[55]  Alex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from loged implicit exploration data. Advances in neural information processing systems, 23, 2010.   \n[56] Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik. Doubly robust off-policyevaluatin with shrinkae. In Iternational Conference onMachine Learning, pages 9167-9176. PMLR, 2020.   \n[57] Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims. Cab: Continuous adaptive blending for policy evaluation and learning. In International Conference on Machine Learning, pages 6005-6014. PMLR, 2019.   \n[58]  Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual learning. advances in neural information processing systems, 28, 2015.   \n[59]  Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139-2148. PMLR, 2016.   \n[60]  William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285-294, 1933.   \n[61] Mark J van der Laan. The construction and analysis of adaptive group sequential designs. 2008.   \n[62]  Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge university press, 2019.   \n[63] Lequn Wang, Akshay Krishnamurthy, and Alex Slivkins. Oracle-efficient pessimism: Offline policy optimization in contextual bandits. In International Conference on Artificial Intelligence and Statistics, pages 766-774. PMLR, 2024.   \n[64]  Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudik. Optimal and adaptive off-policy evaluation in contextual bandits. In International Conference on Machine Learning, pages 3589-3597. PMLR, 2017.   \n[65] Ian Waudby-Smith, Lili Wu, Aaditya Ramdas, Nikos Karampatziakis, and Paul Mineiro. Anytime-valid off-policy inference for contextual bandits. ACM/JMS Journal of Data Science, 1(3):1-42, 2024.   \n[66]  S Yang and P Ding. Asymptotic inference of causal effects with observational studies trimmed by the estimated propensity scores. Biometrika, 105(2):487-493, 03 2018.   \n[67]  Ruohan Zhan, Vitor Hadad, David A Hirshberg, and Susan Athey. Off-policy evaluation via adaptive weighting with data from contextual bandits. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 2125-2135, 2021.   \n[68] Ruohan Zhan, Zhimei Ren, Susan Athey, and Zhengyuan Zhou. Policy learning with adaptively collected data. Management Science, 2023.   \n[69]  Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in neural information processing systems, 33:9818-9829, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A  Some elementary inequalities and their proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The following lemma is useful for the truncation arguments used in the proofs of our local minimax lower bounds. In particular, it enables to make small modifications on a pair of probability measures by conditioning on good events of each probability measure, without inducing an overly large change in the total variation distance. ", "page_idx": 14}, {"type": "text", "text": "Lemma A.1. Let $(\\mu,\\nu)$ be a pair of probability measures defined ona common sample space $(\\Omega,{\\mathcal{F}})$ and consider any two events $A,B\\in{\\mathcal{F}}$ satisfying r $\\operatorname*{nin}\\left\\{\\mu(A),\\nu(B)\\right\\}\\geq1-\\epsilon.$ for some $\\epsilon\\in[0,\\frac{1}{4}]$ Then, the conditional distributions $\\left(\\mu|A\\right)\\left(\\cdot\\right)\\in\\Delta\\left(\\Omega,\\mathcal{F}\\right)$ and $\\left(\\nu|B\\right)\\left(\\cdot\\right)\\in\\Delta\\left(\\Omega,\\mathcal{F}\\right)$ defined by ", "page_idx": 14}, {"type": "equation", "text": "$$\n(\\mu|A)\\left(E\\right):={\\frac{\\mu\\left(A\\cap E\\right)}{\\mu(A)}}\\quad{\\mathrm{and}}\\quad(\\nu|B)\\left(E\\right):={\\frac{\\nu\\left(B\\cap E\\right)}{\\nu(B)}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any event $E\\in{\\mathcal{F}}$ satisfy the bound ", "page_idx": 14}, {"type": "equation", "text": "$$\n|\\mathrm{TV}\\left(\\mu|A,\\nu|B\\right)-\\mathrm{TV}\\left(\\mu,\\nu\\right)|\\leq2\\epsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma A.1. Due to the triangle inequality for the total variation (TV) distance, it follows that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\mu,\\nu\\right)\\leq\\mathrm{TV}\\left(\\mu,\\mu|A\\right)+\\mathrm{TV}\\left(\\mu|A,\\nu|B\\right)+\\mathrm{TV}\\left(\\nu|B,\\nu\\right),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{TV}\\left(\\mu\\vert A,\\nu\\vert B\\right)\\leq\\mathrm{TV}\\left(\\mu\\vert A,\\mu\\right)+\\mathrm{TV}\\left(\\mu,\\nu\\right)+\\mathrm{TV}\\left(\\nu,\\nu\\vert B\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "At this point, one can easily observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{TV}\\left(\\mu,\\mu|A\\right)=\\operatorname*{sup}\\left\\{|\\mu(E)-\\left(\\mu|A\\right)(E)|:E\\in\\mathcal{F}\\right\\}=\\left(\\mu|A\\right)(A)-\\mu(A)=1-\\mu(A);}\\\\ &{\\operatorname{TV}\\left(\\nu,\\nu|B\\right)=\\operatorname*{sup}\\left\\{|\\nu(E)-\\left(\\nu|B\\right)(E)|:E\\in\\mathcal{F}\\right\\}=\\left(\\nu|B\\right)(B)-\\nu(B)=1-\\nu(B).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Putting the observation (32) into the inequalities (30) and (31), the assumptions $1-\\mu(A)\\leq\\epsilon$ and $1-\\nu(B)\\leq\\epsilon$ establish the desired result. ", "page_idx": 14}, {"type": "text", "text": "B  Proofs and omitted details for Section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "First, one can observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{T}\\left\\{\\frac{\\eta}{c_{i}}\\in[0,0_{i}]\\right\\}}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{T}\\left[\\mathbb{E}_{T}\\left[\\frac{g\\left(X_{i},A_{i}\\right)Y_{i}}{\\pi_{i}^{i}+\\left(X_{i},Q_{i-1};\\lambda_{i}\\right)}-f_{i}\\left(X_{i},\\Theta_{i-1},A_{i}\\right)\\right.\\right.}\\\\ &{\\qquad\\left.\\quad+\\left.\\left.\\left(f_{i}\\left(X_{i},\\Theta_{i-1}\\right),\\pi_{i}^{i}\\left(X_{i},\\Theta_{i-1};\\lambda_{i}\\right)\\right)\\right]\\mathrm{d}_{1}\\left(X_{i},A_{i},\\Theta_{i-1}\\right)\\right]}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{T}\\left[\\frac{g\\left(X_{i},A_{i}\\right){\\mu}^{i}\\left(X_{i}+\\bar{X}_{i},A_{i}\\right)}{\\pi_{i}^{i}\\left(X_{i},Q_{i-1};\\lambda_{i}\\right)}-f_{i}\\left(X_{i},\\Theta_{i-1},A_{i}\\right)+\\left(f_{i}\\left(X_{i},\\Theta_{i-1},\\cdot\\right),\\pi_{i}^{*}\\left(X_{i},\\Theta_{i-1};\\lambda_{i}\\right)\\right)_{\\mathcal{X}_{i}}\\right.}\\\\ &{\\qquad\\quad\\left.=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{T}\\left[\\mathbb{E}_{T}\\left[\\frac{g\\left(X_{i},A_{i}\\right){\\mu}^{i}\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{i}\\left(X_{i},Q_{i-1};\\lambda_{i}\\right)}-f_{i}\\left(X_{i},\\Theta_{i-1},A_{i}\\right)\\right.\\right.}\\\\ &{\\qquad\\qquad\\quad\\left.\\left.+\\left.\\left(f_{i}\\left(X_{i},\\Theta_{i-1},\\cdot\\right),\\pi_{i}^{*}\\left(X_{i},\\Theta_{i-1};\\lambda_{i}\\right)\\right)_{\\mathcal{X}_{i}}\\right]\\left(X_{i},\\Theta_{i-1}\\right)\\right]}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{T}\\left[\\int_\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We now assume (3) and note that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}_{T^{*}}\\left[\\hat{\\tau}_{n}^{f}\\left(\\mathbf O_{n}\\right)\\right]=\\displaystyle\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\mathrm{Var}_{T^{*}}\\left[\\frac{g\\left(X_{i},A_{i}\\right)Y_{i}}{\\pi_{i}^{*}\\left(X_{i},\\mathbf O_{i-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\mathbf O_{i-1},A_{i}\\right)\\right]}\\\\ &{\\phantom{\\mathrm{Var}_{T^{*}}\\left[\\hat{\\tau}_{n}^{f}\\left(\\bigstar\\!\\frac{\\textstyle\\mathbf O_{i}}{n^{2}}\\sum_{1\\le i<j\\le n}\\mathrm{Cov}_{Z^{*}}\\left[\\frac{g\\left(X_{i},A_{i}\\right)Y_{i}}{\\pi_{i}^{*}\\left(X_{i},\\mathbf O_{i-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\mathbf O_{i-1},A_{i}\\right),\\right.\\right.\\right.}}\\\\ &{\\left.\\left.\\left.\\frac{g\\left(X_{j},A_{j}\\right)Y_{j}}{\\pi_{j}^{*}\\left(X_{j},\\mathbf O_{j-1};A_{j}\\right)}-f_{j}\\left(X_{j},\\mathbf O_{j-1},A_{j}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "One can reveal that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sup}_{\\varepsilon\\rightarrow\\infty}\\left[\\frac{\\eta^{\\prime}\\left(X_{t},A_{t},X_{t}\\right)}{\\varepsilon^{\\prime}\\left(X_{t},\\Omega_{t}\\right)}-f_{t}\\left(X_{s},0_{t-1},A_{t}\\right)\\right]}\\\\ &{=\\operatorname*{sup}_{\\varepsilon\\rightarrow\\infty}\\left[\\mathbb{E}_{\\pi^{\\prime}}\\left\\{\\left[\\frac{\\eta^{\\prime}\\left(X_{t},A_{t},X_{t}\\right)}{\\varepsilon^{\\prime}\\left(X_{t},0_{t-1},A_{t}\\right)}-f_{t}\\left(X_{s},0_{t-1},A_{t}\\right)\\right]\\right\\}^{2}\\Bigg|(X_{t},A_{t},M_{t-1})\\right]\\right]-\\left(\\tau^{\\prime}\\left(T\\right)\\right)^{2}}\\\\ &{=\\operatorname*{sup}_{\\varepsilon\\rightarrow\\infty}\\left[\\mathbb{E}_{\\pi^{\\prime}\\left(X_{t},0_{t-1},A_{t}\\right)}\\mathbb{E}_{\\pi^{\\prime}\\left(T\\right)}\\left\\{\\left[X_{t}^{2}\\left(X_{t},A_{t},M_{t-1}\\right)\\right]\\right\\}^{2}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\frac{2f_{t}\\left(X_{t},0_{t-1},A_{t}\\right)\\mathbb{E}_{\\pi^{\\prime}\\left(T\\right)}\\left\\{\\left[X_{t}^{2}\\left(X_{t},\\Omega_{t-1}\\right)\\right]\\right\\}^{2}}{\\varepsilon^{\\prime}\\left(X_{t},0_{t-1},A_{t}\\right)}-\\left(\\tau^{\\prime}\\left(X_{s},0_{t-1},A_{t}\\right)+f_{t}\\left(X_{s},0_{t-1},A_{t}\\right)\\right)-\\left(\\tau^{\\prime}\\left(T\\right)\\right)^{2}}\\\\ &{=\\operatorname*{sup}_{\\varepsilon\\rightarrow\\infty}\\left[\\frac{\\eta^{\\prime}\\left(X_{t},\\Delta_{t}\\right)\\left(X_{t},\\tau^{\\prime}\\left(X_{t},A_{t}\\right)\\right)}{\\varepsilon^{\\prime}\\left(T\\right)^{2}}\\right]}\\\\ &{\\ \\ \\ \\ \\ \\ \\ +\\operatorname*{sup}_{\\varepsilon\\rightarrow\\infty}\\left[\\left\\{\\left[\\frac{\\eta^{\\prime}\\left(X_{t},\\tau\\right)\\left(X_{t},0_{t-1},B_{t}\\right)}{\\varepsilon^{\\prime}\\left(X_{t},0_{t-1}\n$$$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Big[\\frac{\\theta}{\\theta}(X_{t},\\theta(X_{t},\\theta_{t+1}))\\Big]=\\theta}\\\\ &{=\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Bigg[\\frac{\\theta}{\\theta}\\Big(X_{t},\\theta(X_{t},\\theta_{t+1})\\Big)}\\\\ &{+\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Big[\\frac{\\theta}{\\theta}(X_{t},\\theta_{t+1})\\Big]+\\theta\\theta}\\\\ &{+\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Big[\\Big(\\frac{\\theta}{\\theta}(X_{t},\\theta_{t+1})\\Big)^{\\theta}+\\hat{\\theta}\\Big(X_{t},\\theta_{t+1},\\theta_{t+1}\\Big)\\Big]^{\\theta}\\Bigg]}\\\\ &{=\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Bigg[\\frac{\\theta}{\\theta}\\Big(X_{t},\\theta_{t+1})\\Big]^{\\theta}}\\\\ &{-\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Big[\\frac{\\theta}{\\theta}(X_{t},\\theta_{t+1})\\Big]^{\\theta}}\\\\ &{\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Bigg[\\frac{\\theta}{\\theta}(X_{t},\\theta_{t+1})\\Big]^{\\theta}}\\\\ &{+\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Bigg[\\Big(\\frac{\\theta}{\\theta}(X_{t},\\theta_{t+1})\\Big)^{\\theta}+\\Big(\\theta}\\\\ &{+\\frac{\\theta}{\\theta}\\Big)\\Bigg[\\theta^{\\theta}\\Big(X_{t},\\theta_{t+1})\\Big]^{\\theta}}\\\\ &{+\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Bigg[\\Big(\\frac{\\theta}{\\theta}(X_{t},\\theta_{t+1})\\Big)^{\\theta}+\\Big(\\theta}\\Big(X_{t},\\theta_{t}\\Big)\\Big)^{\\theta}\\Big(X_{t},\\theta_{t}\\Big(X_{t},\\theta_{t+1},\\theta_{t+1}\\Big)\\Big)^{\\theta}\\Bigg]}\\\\ &{+\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Bigg[\\Big(\\frac{\\theta}{\\theta}(X_{t},\\theta_{t+1})\\Big)^{\\theta}\\Big(X_{t},\\theta_{t}\\Big(X_{t},\\theta_{t+1}\\Big)\\Big)^{\\theta}}\\\\ &{+\\mathbb{E}_{\\theta\\theta}^{\\theta}\\Bigg[\\Big(\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the step (a) can be verified as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\left\\{\\frac{g\\left(X_{i},A_{i}\\right)\\mu^{*}\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)\\right\\}^{2}\\right]}\\\\ &{=\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\left\\{\\frac{g\\left(X_{i},A_{i}\\right)\\mu^{*}\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)\\right\\}^{2}\\right|\\left(X_{i},\\mathscr{H}_{i-1}\\right)\\right]\\right]}\\\\ &{=\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\mathrm{Var}_{\\mathbb{Z}^{*}}\\left[\\frac{g\\left(X_{i},A_{i}\\right)\\mu^{*}\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)\\right]\\left(X_{i},\\mathscr{H}_{i-1}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\left(\\underbrace{\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\frac{g\\left(X_{i},A_{i}\\right)\\mu^{*}\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)\\right]\\left(X_{i},\\mathcal{H}_{i-1}\\right)}_{=\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\rangle_{\\lambda}}\\right)^{2}\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\left\\{\\frac{g\\left(X_{i},A_{i}\\right)\\mu^{*}\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}-\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\rangle_{\\lambda_{\\mathrm{A}}}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)\\right\\}^{2}\\right]}\\\\ &{\\quad\\quad+\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\rangle_{\\lambda_{\\mathrm{A}}}^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we comput $\\begin{array}{r}{\\mathrm{e}\\,\\mathrm{Cov}_{\\mathbb{Z}^{\\star}}\\left[\\frac{g(X_{i},A_{i})Y_{i}}{\\pi_{i}^{*}(X_{i},\\mathbf O_{i-1};A_{i})}-f_{i}\\left(X_{i},\\mathbf O_{i-1},A_{i}\\right),\\frac{g(X_{j},A_{j})Y_{j}}{\\pi_{j}^{*}(X_{j},\\mathbf O_{j-1};A_{j})}-f_{j}\\left(X_{j},\\mathbf O_{j-1},A_{j}\\right)\\right]\\!;}\\end{array}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Corr.}\\left[\\frac{g\\left(X_{i},\\Omega_{j}\\right)Y_{i}}{\\pi_{i}^{*}\\left(X_{i},\\Omega_{j-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right),\\frac{g\\left(X_{j},A_{j}\\right)Y_{j}}{\\pi_{j}^{*}\\left(X_{j},\\Omega_{j-1};A_{j}\\right)}-f_{j}\\left(X_{j},\\mathbf{O}_{j-1},A_{j}\\right)\\right]}\\\\ &{=\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left\\{\\frac{g\\left(X_{i},A_{i}\\right)Y_{i}}{\\pi_{i}^{*}\\left(X_{i},\\Omega_{i-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)\\right\\}\\right]}\\\\ &{\\left\\{\\frac{g\\left(X_{j},A_{j}\\right)\\mu\\left(Y_{j},\\Omega_{j}\\right)Y_{i}}{\\pi_{j}^{*}\\left(X_{j},\\Omega_{j-1};A_{j}\\right)}-f_{j}\\left(X_{j},\\mathbf{O}_{j-1},A_{j}\\right)\\right\\}\\right]-\\left\\{\\tau\\left(\\tau^{*}\\right)\\right\\}^{2}}\\\\ &{=\\mathbb{E}_{\\boldsymbol{x}}\\left[\\left\\{\\frac{g\\left(X_{i},A_{i}\\right)Y_{j}}{\\pi_{i}^{*}\\left(X_{i},\\Omega_{j-1};A_{i}\\right)}-f_{i}\\left(X_{i},\\Omega_{i-1},A_{i}\\right)\\right\\}\\right]}\\\\ &{\\left\\{\\begin{array}{l l}{g\\left(X_{j},A_{j}\\right)\\mu^{*}\\left(X_{j},\\Omega_{j}\\right)-f_{j}\\left(X_{j},\\Omega_{j-1},A_{j}\\right)\\right\\}\\right\\}\\left[\\alpha_{j},\\mu_{j-1}\\right]\\right\\}^{2}}\\\\ {\\frac{g\\left(X_{j},X_{j}\\right)\\mu^{*}\\left(X_{j},\\Omega_{j-1};A_{j}\\right)}{\\pi_{j}^{*}\\left(X_{j},\\Omega_{j-1};A_{j}\\right)}-f_{j}\\left(X_{j},\\mathbf{O}_{j-1},A_{j}\\right)\\right\\}\\left\\}\\left\\{\\alpha_{j},\\mu_{j-1}\\right\\}\\end{array}\\right.\\left\\{\\tau\\left(\\tau^\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "where the step (b) holds due to the fact that $X_{j}$ is independent of the historical data $\\mathcal{H}_{j-1}$ , which immediately yields $X_{j}|\\,\\mathcal{H}_{j-1}\\overset{d}{=}X_{j}\\sim\\Xi^{*}(\\cdot)$ . Taking two pieces (35) and (36) collectively into the equation (34), one has ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad n\\cdot\\mathrm{Var}_{{\\mathbb{Z}}^{*}}\\left[\\widehat{\\tau}_{n}^{f}\\left(\\mathbf{O}_{n}\\right)\\right]}\\\\ &{=\\mathrm{Var}_{X\\sim\\Xi^{*}}\\left[\\langle g(X,\\cdot),\\mu^{*}(X,\\cdot)\\rangle_{\\lambda_{\\mathrm{k}}}\\right]+\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\mathbb{E}_{{\\mathbb{Z}}^{*}}\\cdot\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)}\\right]\\right.}\\\\ &{\\quad\\left.+\\mathbb{E}_{{\\mathbb{Z}}^{*}}\\left[\\left\\{\\frac{g\\left(X_{i},A_{i}\\right)\\mu^{*}\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}-\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\rangle_{\\lambda_{\\mathrm{k}}}-f_{i}\\left(X_{i},\\mathbf{O}_{i-1},A_{i}\\right)\\right\\}^{2}\\right]\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "as desired. ", "page_idx": 16}, {"type": "text", "text": "B.2Proof of Theorem 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We first single out a key technical lemma throughout this section that plays a crucial role in the proof of Theorem 3.1. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.1. The following results hold: ", "page_idx": 16}, {"type": "text", "text": "(i) It holds that $\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\Big|\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]=\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\rangle_{\\lambda_{\\mathbb{A}}}$ for all $i\\in[n]$ .Therefore, one has ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb E_{\\mathcal{Z}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\right]=\\mathbb E_{\\mathcal{Z}^{*}}\\left[\\mathbb E_{\\mathcal{Z}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\Big|\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathbb E_{\\mathcal{Z}^{*}}\\left[\\left\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\right\\rangle_{\\lambda_{\\mathbb{A}}}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\tau\\left(\\mathcal{Z}^{*}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(i) For every $1\\leq i<j\\leq n,$ we have $\\mathrm{Cov}_{\\mathbb{Z}^{\\ast}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right),\\hat{\\Gamma}_{j}\\left(\\mathbf{O}_{j}\\right)\\right]=0;$ ", "page_idx": 17}, {"type": "text", "text": "(ii) For every $i\\in[n],$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\mathop{Var}}_{\\mathbb{Z}^{\\star}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\right]}\\\\ &{=\\mathrm{\\mathop{Var}}_{X\\sim\\mathbb{Z}^{\\star}}\\left[\\langle g\\left(X,\\cdot\\right),\\mu^{*}\\left(X,\\cdot\\right)\\rangle_{\\lambda_{k}}\\right]+\\mathbb{E}_{T^{\\star}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\,\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]}\\\\ &{\\phantom{=}+\\mathbb{E}_{T^{\\star}}\\left[\\mathrm{\\mathop{Var}}_{\\mathbb{Z}^{\\star}}\\left[\\frac{g\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\}\\right]\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]}\\\\ &{\\leq\\mathrm{\\mathop{Var}}_{X\\sim\\mathbb{Z}^{\\star}}\\left[\\langle g\\left(X,\\cdot\\right),\\mu^{*}\\left(X,\\cdot\\right)\\rangle_{\\lambda_{k}}\\right]+\\mathbb{E}_{T^{\\star}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\,\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]}\\\\ &{\\phantom{=}+\\mathbb{E}_{T^{\\star}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\,\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma B.1. ", "page_idx": 17}, {"type": "text", "text": "(i) From the definition of $\\hat{\\Gamma}_{i}(\\cdot):\\mathbb{O}^{i}\\to\\mathbb{R}$ in (9), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\Big|\\left(X_{i},A_{i},\\mathcal{H}_{i-1}\\right)\\right]=\\frac{g\\left(X_{i},A_{i}\\right)}{\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\left\\{\\mu^{*}\\left(X_{i},A_{i}\\right)-\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\left\\langle g\\left(X_{i},\\cdot\\right),\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},\\cdot\\right)\\right\\rangle_{\\lambda_{h}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{T^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\Big|\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]}\\\\ &{=\\mathbb{E}_{T^{*}}\\left[\\mathbb{E}_{T^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\Big|\\left(X_{i},A_{i},\\mathcal{H}_{i-1}\\right)\\right]\\Big|\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]}\\\\ &{=\\int_{\\mathbb{A}}\\frac{g\\left(X_{i},a\\right)}{\\pi_{i}^{*}}\\sum_{\\hat{\\mu}_{i}=1;\\,0,i=1\\atop i_{1}}\\{\\mu^{*}\\left(X_{i},\\mathbf{O}_{i-1};\\boldsymbol{a}\\right)\\left\\{\\mu^{*}\\left(X_{i},a\\right)-\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},a\\right)\\right\\}\\cdot\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};a\\right)\\mathrm{d}\\lambda_{\\mathbb{A}}(a)}\\\\ &{\\quad+\\left\\langle g\\left(X_{i},\\cdot\\right),\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},\\cdot\\right)\\right\\rangle_{\\lambda_{\\mathbb{A}}}}\\\\ &{=\\left\\langle g\\left(X_{i},\\cdot\\right),\\mu^{*}\\left(X_{i},\\cdot\\right)\\right\\rangle_{\\lambda_{\\mathbb{A}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as desired. ", "page_idx": 17}, {"type": "text", "text": "(ii) One can reveal that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Cov}_{\\mathcal{T}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right),\\hat{\\Gamma}_{j}\\left(\\mathbf{O}_{j}\\right)\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\mathbb{E}\\left[\\hat{\\Gamma}_{j}\\left(\\mathbf{O}_{j}\\right)\\right]\\left(X_{j},A_{j},\\mathcal{H}_{j-1}\\right)\\right]-\\left\\{\\tau\\left(\\mathbb{Z}^{*}\\right)\\right\\}^{2}}\\\\ &{=\\mathbb{E}_{\\mathcal{Z}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\left[\\frac{g\\left(X_{j},A_{j}\\right)}{\\pi_{j}^{*}\\left(X_{j},\\mathbf{O}_{j-1};A_{j}\\right)}\\left\\{\\mu^{*}\\left(X_{j},A_{j}\\right)-\\hat{\\mu}_{j}\\left(\\mathbf{O}_{j-1}\\right)\\left(X_{j},A_{j}\\right)\\right\\}\\right.}\\\\ &{\\qquad+\\left.\\left\\langle g\\left(X_{j},\\cdot\\right),\\hat{\\mu}_{j}\\left(\\mathbf{O}_{j-1}\\right)\\left(X_{j},\\cdot\\right)\\right\\rangle_{\\lambda_{k}}\\right]\\right]-\\left\\{\\tau\\left(\\mathbb{Z}^{*}\\right)\\right\\}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\frac{g\\left(X_{j},A_{j}\\right)}{\\pi_{j}^{*}\\left(X_{j},\\mathbf{O}_{j-1};A_{j}\\right)}\\left\\{\\mu^{*}\\left(X_{j},A_{j}\\right)-\\hat{\\mu}_{j}\\left(\\mathbf{O}_{j-1}\\right)\\left(X_{j},A_{j}\\right)\\right\\}\\right.}\\\\ &{\\quad\\left.+\\left\\langle g\\left(X_{j},\\cdot\\right),\\hat{\\mu}_{j}\\left(\\mathbf{O}_{j-1}\\right)\\left(X_{j},\\cdot\\right)\\right\\rangle_{\\lambda_{\\mathbf{k}}}\\right]\\left(X_{j},\\mathcal{H}_{j-1}\\right)\\right]-\\left\\{\\tau\\left(\\mathcal{T}^{*}\\right)\\right\\}^{2}}\\\\ &{=\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i};g\\right)\\left\\langle g\\left(X_{j},\\cdot\\right),\\mu^{*}\\left(X_{j},\\cdot\\right)\\right\\rangle_{\\lambda_{\\mathbf{k}}}\\right]-\\left\\{\\tau\\left(\\mathcal{T}^{*};g\\right)\\right\\}^{2}}\\\\ &{\\stackrel{(a)}{=}0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the step (a) holds due to the facts that $\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i}\\right)$ $\\mathcal{H}_{j-1}$ -measurable and $X_{j}$ \u2161 $\\mathcal{H}_{j-1}$ , together with the equation (37). ", "page_idx": 18}, {"type": "text", "text": "(iii) It follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var.}\\left[\\mathrm{\\hat{P}}_{i}\\left(\\Omega_{i}\\right)\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{X}^{*}}\\left[\\mathrm{Varz}\\left[\\hat{\\Gamma}_{i}\\left(\\Omega_{i}\\right)\\right]\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]\\right]+\\mathrm{Var.}\\left[\\mathbb{E}_{\\mathcal{X}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\Omega_{i}\\right)\\right]\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]\\right]}\\\\ &{\\stackrel{\\mathrm{a)}}{=}\\mathbb{E}_{\\mathcal{X}^{*}}\\left[\\mathrm{Var.}\\left[\\mathrm{Varz.}\\left[\\hat{\\Gamma}_{i}\\left(\\Omega_{i}\\right)\\right]\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]\\right]\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]}\\\\ &{\\quad+\\mathbb{E}_{\\mathcal{X}^{*}}\\left[\\mathrm{Varz.}\\left[\\mathbb{E}_{\\mathcal{X}^{*}}\\left[\\hat{\\Gamma}_{i}\\left(\\Omega_{i}\\right)\\right]\\left(X_{i},\\mathcal{H}_{i},\\mathcal{H}_{i-1}\\right)\\right]\\right]\\left[\\left(X_{i},\\mathcal{H}_{i-1}\\right)\\right]}\\\\ &{\\quad+\\mathrm{Var.}\\ x_{-\\hat{\\mathcal{X}}^{*}}\\left[\\left(\\mathcal{G}(X_{\\cdot}),\\mu^{*}(X_{\\cdot})\\right)_{\\lambda,\\lambda}\\right]}\\\\ &{=\\mathbb{E}_{\\mathcal{X}^{*}}\\left[\\frac{\\left(\\mathcal{G}^{2}\\left(X_{i},\\mathcal{H}_{i}\\right)\\right)\\sigma^{2}\\left(X_{i},\\mathcal{A}_{\\lambda}\\right)}{\\left(\\sigma_{i}^{2}\\right)^{2}\\left(X_{i},\\Omega_{i-1};\\mathcal{A}_{\\lambda}\\right)}\\right]}\\\\ &{\\quad+\\mathbb{E}_{\\mathcal{X}^{*}}\\left[\\mathrm{Varz.}\\left[\\frac{\\mu\\left(X_{i},\\mathcal{H}_{i}\\right)}{\\mu_{i}^{*}\\left(X_{i},\\Omega_{i-1};\\mathcal{A}_{\\lambda}\\right)}\\left\\{\\mu^{*}\\left(X_{i},\\mathcal{A}_{i}\\right)-\\hat{\\mu}_{i}\\left(\\Omega_{i-1}\\right)\\left(X_{i},\\mathcal{A}_{i}\\right)\\right\\}\\right]\\left(X_{i},\\\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "as desired, where the step (b) follows from the fact (40) ", "page_idx": 18}, {"type": "text", "text": "Now, it's time to finish the proof of Theorem 3.1. One can reveal that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{T^{*}}\\left[\\left\\{\\bar{\\tau}_{n}^{\\mathbb{A}\\mathbb{P}^{\\mathbb{W}}}(\\mathbf{O}_{n};g)-\\tau\\left(T^{*};g\\right)\\right\\}^{2}\\right]}\\\\ &{\\stackrel{(a)}{=}\\frac{1}{n^{2}}\\displaystyle\\sum_{i=1}^{n}\\mathrm{Sar.}\\left[\\hat{\\Gamma}_{i}\\left(\\mathbf{O}_{i};g\\right)\\right]}\\\\ &{\\stackrel{(b)}{\\leq}\\displaystyle\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\left\\{\\mathrm{Var}_{X\\sim\\mathbb{Z}^{*}}\\left[\\left(g\\left(X_{\\cdot}\\right),\\mu^{*}\\left(X_{\\cdot}\\right)\\right)_{\\lambda_{k}}\\right]+\\mathbb{E}_{T^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},0_{i-1};A_{i}\\right)}\\right]\\right.}\\\\ &{\\quad\\left.+\\mathbb{E}_{T^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\left\\{\\mu^{*}\\left(X_{i},A_{i}\\right)-\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},0_{i-1};A_{i}\\right)}\\right]\\right\\}}\\\\ &{\\stackrel{(c)}{=}\\displaystyle\\frac{1}{n}\\left\\{v_{*}^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{T^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\left\\{\\hat{\\mu}_{i}\\left(\\Omega_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},0_{i-1};A_{i}\\right)}\\right]\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the step (a) holds from the part (i) of Lemma B.1, the step (b) makes use of the inequality (38), and the step (c) follows from the definition of $v_{*}^{2}$ in (7). ", "page_idx": 18}, {"type": "text", "text": "B.3Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "It holds due to the observation (12) that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\sum_{i=1}^{n}l_{i}\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[l_{i}\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}\\right|\\left(\\mathcal{H}_{i-1},X_{i},A_{i}\\right)\\right]\\right]}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)+\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]\\right]}\\\\ &{=n\\left\\Vert\\sigma\\right\\Vert_{(n)}^{2}+\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which establishes the following expression of the estimation error term (11): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]}\\\\ &{=\\displaystyle\\frac{1}{n}\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\sum_{i=1}^{n}l_{i}\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}\\right]-\\left\\Vert\\sigma\\right\\Vert_{(n)}^{2}}\\\\ &{=\\displaystyle\\frac{1}{n}\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\mathrm{Regret}\\left(n;A\\right)\\right]+\\frac{1}{n}\\mathbb{E}_{\\boldsymbol{Z}^{*}}\\left[\\mathrm{inf}\\left\\{\\sum_{i=1}^{n}l_{i}(\\mu):\\mu\\in\\mathcal{F}\\right\\}\\right]-\\left\\Vert\\sigma\\right\\Vert_{(n)}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "At this point, one can realize that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\mathbb{E}_{T^{*}}\\left[\\operatorname*{inf}\\left\\{\\sum_{i=1}^{n}l_{i}(\\mu):\\mu\\in\\mathcal{F}\\right\\}\\right]}\\\\ &{\\leq\\operatorname*{inf}\\left\\{\\frac{1}{n}\\mathbb{E}_{T^{*}}\\left[\\sum_{i=1}^{n}l_{i}(\\mu)\\right]:\\mu\\in\\mathcal{F}\\right\\}}\\\\ &{=\\operatorname*{inf}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{T^{*}}\\left[\\mathbb{E}_{T^{*}}\\left[l_{i}(\\mu)|\\left(\\mathcal{U}_{i-1},X_{i},A_{i}\\right)\\right]:\\mu\\in\\mathcal{F}\\right\\}\\right.\\qquad\\qquad\\qquad\\qquad\\left.(44)}\\\\ &{\\overset{(a)}{=}\\operatorname*{inf}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{T^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\Omega_{i-1};A_{i}\\right)}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)+\\left\\{\\mu\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]\\right]:\\mu\\in\\mathcal{F}\\right\\}}\\\\ &{=\\left\\lVert\\sigma\\right\\rVert_{(n)}^{2}+\\operatorname*{inf}\\left\\{\\left\\lVert\\mu-\\mu^{*}\\right\\rVert_{(n)}^{2}:\\mu\\in\\mathcal{F}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the step (a) holds by the fact (12). Taking two pieces (43) and (44) collectively, it follows that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{Z^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{n}\\mathbb{E}_{Z^{*}}\\left[\\mathrm{Regret}\\left(n;A\\right)\\right]+\\operatorname*{inf}\\left\\{\\left\\Vert\\mu-\\mu^{*}\\right\\Vert_{(n)}^{2}:\\mu\\in\\mathcal{F}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, the upper bound (15) on the MSE of the AIPW estimator (8) is an immediate consequence of the inequality (45) by putting it into the bound (10) in Theorem 3.1. ", "page_idx": 19}, {"type": "text", "text": "B.4Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "One can easily observe from the equation (17) for every $\\mu\\in{\\mathcal{F}}$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\nabla l_{i}(\\mu)\\right\\|_{2}^{2}=\\frac{4g^{4}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{4}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\left\\{Y_{i}-\\mu\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\overset{\\mathbb{P}_{T^{*}}^{n}\\mathrm{=:a.s.}}{\\leq}\\left(4L B^{2}\\right)^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which holds due to Assumption 1 together with the fact $\\mathbb{Y}=[-L,L]$ . So it turns out that the loss function (14) is Lipschitz continuous with parameter $G:=4L B^{2}\\,\\mathbb{P}_{\\mathcal{Z}^{\\ast}}^{n}$ -almost surely. Hence, the desired conclusion immediately follows by Theorem $3.1$ in [18] with parameter $G=\\bar{4}L B^{2}$ ", "page_idx": 19}, {"type": "text", "text": "B.5Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "One can realize from the equation (22\uff09that $\\mathbb{P}_{\\mathcal{T}^{*}}^{n}$ -almost surely, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\theta}\\mathcal{L}_{i}(\\theta)\\|_{2}^{2}=\\frac{4g^{4}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{4}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\left\\{\\theta^{\\top}\\phi\\left(X_{i},A_{i}\\right)-Y_{i}\\right\\}^{2}\\|\\phi\\left(X_{i},A_{i}\\right)\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq4B^{4}\\left\\{|Y_{i}|+\\|\\theta\\|_{2}\\left\\|\\phi\\left(X_{i},A_{i}\\right)\\right\\|_{2}\\right\\}^{2}\\|\\phi\\left(X_{i},A_{i}\\right)\\|_{2}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\leq4B^{4}(L+R)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which holds by Assumption 1 together with the facts $\\mathbb{Y}=[-L,L]$ and $\\operatorname*{sup}_{(x,a)\\in\\mathbb{X}\\times\\mathbb{A}}\\|\\phi(x,a)\\|_{2}\\leq1$ So, the loss function (21) is Lipschitz continuous with parameter $G:=2B^{2}(L+R)\\,\\mathbb{P}_{\\mathcal{T}^{\\ast}}^{n}$ -a.s. Hence, the desired result follows by Theorem 3.1 in [18] with parameter $G=2B^{2}(L+R)$ and $D=2R$ ", "page_idx": 20}, {"type": "text", "text": "B.6 Consequences for particular outcome models: general function approximation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lastly, we consider the most challenging setting where the estimation of the treatment effect $\\mu^{*}(\\cdot,\\cdot):$ $\\mathbb{X}\\times\\mathbb{A}\\rightarrow\\mathbb{R}$ is parameterized by general function classes. Under Assumption 1, one first observes from the MSE bound (10) of the AIPW estimator (8) in Theorem 3.1 that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\left\\{\\hat{\\tau}_{n}^{\\mathsf{A I P W}}(\\mathbf{O}_{n})-\\tau\\left(\\mathbb{Z}^{*}\\right)\\right\\}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{n}\\left\\{v_{*}^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]\\right\\}}\\\\ &{\\leq\\displaystyle\\frac{1}{n}\\left\\{v_{*}^{2}+\\frac{B^{2}}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From the last term in the MSE bound (48), our aim becomes to control an upper bound of the term ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}\\left[\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "in the finite sample regime. Towards achieving this goal, we consider the online non-parametric regression problem described in Algorithm 2 whose sequence $\\{l_{i}(\\cdot):(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})\\to\\mathbb{R}:i\\in[n]\\}$ of loss functions defined as (14) is superseded by $\\{\\bar{l}_{i}(\\cdot):(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})\\to\\mathbb{R}:i\\in[n]\\}$ ,where ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{l}_{i}(\\mu):=\\{Y_{i}-\\mu\\left(X_{i},A_{i}\\right)\\}^{2}\\,,\\,\\forall\\,(\\mu,i)\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})\\times[n].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is straightforward to see for every $i\\in[n]$ that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\mathbb{Z}^{\\ast}}\\left[\\bar{l}_{i}(\\mu)\\right]\\left(\\mathcal{H}_{i-1},X_{i},A_{i}\\right)\\right]=\\sigma^{2}\\left(X_{i},A_{i}\\right)+\\left\\{\\mu\\left(X_{i},A_{i}\\right)-\\mu^{\\ast}\\left(X_{i},A_{i}\\right)\\right\\}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "With this modifed online non-parametric regression problem, we now aim to minimize the learner's modified regret defined as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{Regret}}}\\left(n,\\mathcal{F};\\overline{{\\mathcal{A}}}\\right):=\\sum_{i=1}^{n}\\overline{{l}}_{i}\\left\\{\\widehat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}-\\operatorname*{inf}\\left\\{\\sum_{i=1}^{n}\\overline{{l}}_{i}(\\mu):\\mu\\in\\mathcal{F}\\right\\},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\overline{{\\mathcal{A}}}$ denotes the learner's online non-parametric regression algorithm that returns a sequence of estimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf O_{i-1})\\in(\\mathbb X\\times\\mathbb A\\to\\mathbb R):i\\in[n]\\}$ of the treatment effect based on interactions with the environment which selects modified loss functions $\\{\\bar{l}_{i}(\\cdot):(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})\\to\\mathbb{R}:i\\in[n]\\}$ ", "page_idx": 20}, {"type": "text", "text": "Theorem B.1. The AIPW estimator (8) based on a sequence $\\{\\hat{\\mu}_{i}\\,(\\mathbf O_{i-1})\\in(\\mathbb X\\times\\mathbb A\\to\\mathbb R):i\\in[n]\\}$ ofestimatesforthetreatmenteffect $\\mu^{*}$ produced by making use of an online non-parametricregressionalgorithm $\\overline{{\\mathcal{A}}}$ against the environment which chooses the sequence of modified loss functions ", "page_idx": 20}, {"type": "text", "text": "$\\{\\bar{l}_{i}(\\cdot):(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R})\\to\\mathbb{R}:i\\in[n]\\}$ defined in (50) enjoys the following upper bound on the MSE: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{T^{*}}\\left[\\left\\{\\hat{\\tau}_{n}^{\\mathsf{A P W}}\\left(\\mathbf{O}_{n}\\right)-\\tau\\left(\\mathcal{T}^{*}\\right)\\right\\}^{2}\\right]}\\\\ &{\\leq\\displaystyle\\frac{1}{n}\\left(v_{*}^{2}+\\frac{1}{n}\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\mathrm{Regret}\\left(n,\\mathcal{F};\\overline{{A}}\\right)\\right]\\right.}\\\\ &{\\quad\\left.+\\displaystyle\\underbrace{\\operatorname*{inf}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\left\\{\\mu\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]:\\mu\\in\\mathcal{F}\\right\\}}_{\\mathrm{approsimation~errorterm}.}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem B.1. It follows from the property (51) that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~\\mathbb{E}_{T^{*}}\\left[\\underset{i=1}{\\overset{n}{\\sum}}\\bar{l}_{i}\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}\\right]}\\\\ &{=\\underset{i=1}{\\overset{n}{\\sum}}\\mathbb{E}_{T^{*}}\\left[\\mathbb{E}_{T^{*}}\\left[\\bar{l}_{i}\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\}\\right]\\left(\\mathcal{F}_{i-1},X_{i},A_{i}\\right)\\right]\\right]}\\\\ &{=\\underset{i=1}{\\overset{n}{\\sum}}\\mathbb{E}_{T^{*}}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)+\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]}\\\\ &{=\\underset{i=1}{\\overset{n}{\\sum}}\\mathbb{E}_{T^{*}}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)\\right]+\\underset{i=1}{\\overset{n}{\\sum}}\\mathbb{E}_{T^{*}}\\left[\\left\\{\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which leads to the following expression of the estimation error term (49): ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\cfrac{1}{n}\\cfrac{n}{i-1}\\mathbb{E}_{T^{*}}\\left[\\lbrace\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)(X_{i},A_{i})-\\mu^{*}\\left(X_{i},A_{i}\\right)\\rbrace^{2}\\right]}\\\\ &{=\\cfrac{1}{n}\\mathbb{E}_{T^{*}}\\left[\\underset{i=1}{\\overset{n}{\\sum}}\\bar{l}_{i}\\left\\lbrace\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\right\\rbrace\\right]-\\cfrac{1}{n}\\cfrac{n}{i-1}\\mathbb{E}_{T^{*}}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)\\right]}\\\\ &{=\\cfrac{1}{n}\\mathbb{E}_{T^{*}}\\left[\\underset{i=1}{\\overset{n}{\\mathbb{E}}}\\tau\\mathrm{egret}\\left(n;\\overline{{A}}\\right)\\right]+\\cfrac{1}{n}\\mathbb{E}_{T^{*}}\\left[\\underset{i=1}{\\overset{n}{\\sum}}\\bar{l}_{i}(\\mu):\\mu\\in\\mathcal{F}\\right]\\right]}\\\\ &{\\quad-\\cfrac{1}{n}\\underset{i=1}{\\overset{n}{\\sum}}\\mathbb{E}_{T^{*}}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here, one may observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\mathbb{E}_{Z^{\\star}}\\left[\\operatorname*{inf}\\left\\{\\sum_{i=1}^{n}\\bar{l}_{i}(\\mu):\\mu\\in\\mathcal{F}\\right\\}\\right]}\\\\ &{\\le\\operatorname*{inf}\\left\\{\\frac{1}{n}\\mathbb{E}_{Z^{\\star}}\\left[\\displaystyle\\sum_{i=1}^{n}\\bar{l}_{i}(\\mu)\\right]:\\mu\\in\\mathcal{F}\\right\\}}\\\\ &{=\\operatorname*{inf}\\left\\{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{Z^{\\star}}\\left[\\mathbb{E}_{Z^{\\star}}\\left[\\bar{l}_{i}(\\mu)\\big|\\left(\\bar{l}_{i-1},X_{i},A_{i}\\big)\\right]\\right]:\\mu\\in\\mathcal{F}\\right\\}}\\\\ &{\\stackrel{(a)}{=}\\operatorname*{inf}\\left\\{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{Z^{\\star}}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)+\\left\\{\\mu\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]:\\mu\\in\\mathcal{F}\\right\\}}\\\\ &{=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{Z^{\\star}}\\left[\\sigma^{2}\\left(X_{i},A_{i}\\right)\\right]+\\operatorname*{inf}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{Z^{\\star}}\\left[\\left\\{\\mu\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]:\\mu\\in\\mathcal{F}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the step (a) holds by the fact (51). Putting two pieces (54) and (56) together yields ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\left\\{\\widehat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{1}{n}\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\mathrm{Regret}\\left(n;\\overline{{\\mathcal{A}}}\\right)\\right]+\\operatorname*{inf}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{\\mathbb{Z}^{*}}\\left[\\left\\{\\mu\\left(X_{i},A_{i}\\right)-\\mu^{*}\\left(X_{i},A_{i}\\right)\\right\\}^{2}\\right]:\\mu\\in\\mathcal{F}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, the desired result (53) on the MSE of the AIPW estimator (8) is a straightforward consequence of the inequality (57) by plugging it into the bound (48). \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Here, we remark that aside from the optimal variance $v_{*}^{2}$ , the MSE bound (53) shows two additional terms: (i) the expected regret relative to the number of rounds $n$ , where the expectation is taken over $\\mathbf{O}_{n}\\sim\\mathbb{P}_{\\mathcal{Z}^{\\ast}}^{n}(\\cdot)$ ; and (i) the approximation error term whose form is slightly different from the one inf $\\left\\{\\|\\mu-\\mu^{*}\\|_{(n)}^{2}:\\mu\\in{\\mathcal{F}}\\right\\}$ appeared in the MSE bound (15) of Theorem 3.2. ", "page_idx": 22}, {"type": "text", "text": "Non-asymptotic theory of online non-parametric regression  Before delving into the investigation of the modified regret (52), we briefly recap the main results in [45] that establishes a theoretical framework of online non-parametric regression. In contrast to most existing works of online regression, the authors do NOT start from an algorithm, but instead directly work with the minimax regret in [45]. We will be able to extract a (not necessarily efficient) algorithm after taking a closer look at the minimax regret. Let us use $\\left<\\!\\left<\\cdot\\cdot\\cdot\\right>_{i=1}^{n}$ to denote an interleaved application of the operators inside repeated over $n$ rounds. With this notation in hand, the minimax regret of the online non-parametric regression problem for estimation of the treatment effect can be written as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\mathcal{V}_{n}(\\mathcal{F})}\\\\ &{:=\\left\\langle\\!\\!\\left\\langle\\operatorname*{sup}_{(x_{i},a_{i})\\in\\mathbb{X}\\times\\mathbb{A}}\\operatorname*{inf}_{\\substack{\\hat{y}_{i}\\in[-L,L]}}\\operatorname*{sup}_{y_{i}\\in[-L,L]}\\!\\right\\rangle\\!\\right\\rangle_{i=1}^{n}\\left[\\displaystyle\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-y_{i}\\right)^{2}-\\operatorname*{inf}_{\\mu\\in\\mathcal{F}}\\sum_{i=1}^{n}\\left\\{\\mu\\left(x_{i},a_{i}\\right)-y_{i}\\right\\}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to[-L,L])$ is a pre-specified function class. One of the key tools in the study of estimators based on i.i.d. data is the symmetrization technique [15, 62]. Under the i.i.d. scenario, one can investigate the supremum of an empirical process conditionally on the data by introducing Rademacher random variables, which is NOT directly applicable given the adaptive nature of our main problem. In the online prediction scenario, such a symmetrization technique becomes more subtle and it requires the notion of a binary tree, the smallest entity which captures the sequential nature of the problem in some sense. Towards achieving our goal in our problem, let us state some definitions. ", "page_idx": 22}, {"type": "text", "text": "Definition B.1. Let $\\mathbb{S}$ be a measurable state space. An $\\mathbb{S}$ -valued tree of depth $n$ is a rooted complete binary tree with nodes labeled by elements of the state space $\\mathbb{S}$ : the sequence $\\mathbf{s}=(\\mathbf{s}_{1},\\mathbf{s}_{2},\\cdots\\,,\\mathbf{s}_{n})$ of labeling functions $\\mathbf{s}_{i}(\\cdot):\\{\\pm1\\}^{i-1}\\rightarrow\\mathbb{S}$ which provides the labels of each node. Here, $\\mathbf{s}_{1}\\in\\mathbb{S}$ is the label for the root of the tree, while $\\mathbf{s}_{i}$ for $2\\leq i\\leq n$ is the label of the node obtained by following the path of length $i\\!-\\!1$ from the root, with $+1$ indicating right and $-1$ indicating left. A path of length $n$ is given by the sequence $\\epsilon_{1:n}=(\\epsilon_{1},\\cdot\\cdot\\cdot\\,,\\epsilon_{n})\\in\\{\\pm1\\}^{n}$ . Given any measurable function $\\phi(\\cdot):\\mathbb{S}\\rightarrow\\mathbb{R}$ $\\phi(\\mathbf{s})$ is an $\\mathbb{R}$ -valued tree of depth $n$ with labeling functions $\\left(\\phi\\circ\\mathbf{s}_{i}\\right)(\\cdot):\\left\\{\\pm1\\right\\}^{i-1}\\rightarrow\\mathbb{R}$ for level $i\\in[n]$ (or, in words, the evaluation of $\\phi(\\cdot):\\mathbb{S}\\rightarrow\\mathbf{\\bar{\\mathbb{R}}}$ \uff0c $\\phi(\\mathbf{s})$ on s). Lastly, we let Tree $(\\mathbb{S},n)$ denote the set of all $\\mathbb{S}.$ -valued trees of depth $n$ ", "page_idx": 22}, {"type": "text", "text": "Here, one may think of the sequence of functions $\\{\\mathbf{s}_{i}(\\cdot):i\\in[n]\\}$ defined on the underlying sample space as a predictable stochastic process with respect to the dyadic filtration $\\{\\sigma\\,(\\epsilon_{1:i}):i\\in[n]\\}$ .Next, let us define the notion of a sequential $\\beta$ -cover quantifies one of the key complexity measures of a function class ${\\mathcal{G}}\\subseteq(\\mathbb{S}\\to\\mathbb{R})$ evaluated on the predictable process: the sequential covering number. ", "page_idx": 22}, {"type": "text", "text": "Definition B.2 (Sequential covering numbers [46]). ", "page_idx": 22}, {"type": "text", "text": "(i) Define the following random pseudo-metric between two $\\mathbb{R}$ -valued trees ${\\bf u}=({\\bf u}_{i}:i\\in[n])$ and $\\mathbf{v}=(\\mathbf{v}_{i}:i\\in[\\bar{n}])$ of depth $n$ : for any $(p,\\epsilon_{1:n})\\in[1,+\\infty]\\times{\\{\\pm1\\}}^{n}$ \uff0c ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{\\epsilon_{1:n}}^{p}\\left(\\mathbf{u},\\mathbf{v}\\right):=\\left\\{{\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\left|\\mathbf{u}_{i}\\left(\\epsilon_{1:i-1}\\right)-\\mathbf{v}_{i}\\left(\\epsilon_{1:i-1}\\right)\\right|^{p}\\right\\}}^{\\frac{1}{p}}\\right.\\quad\\mathrm{if~}1\\leq p<+\\infty;\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "(ii) A set $V\\subseteq$ Tree $(\\mathbb{R},n)$ is called a sequential $\\beta$ -coverwithrespect to $l_{p}$ -normof ${\\mathcal{G}}\\subseteq(\\mathbb{S}\\to\\mathbb{R})$ on a given $\\mathbb{S}$ -valued trees of depth $n$ , where $p\\in[1,+\\infty]$ ,if ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{sup}\\left\\{\\operatorname*{inf}\\left\\{d_{\\epsilon_{1:n}}^{p}\\left(\\mathbf{u},\\mathbf{v}\\right):\\mathbf{v}\\in V\\right\\}:\\left(\\mathbf{u},\\epsilon_{1:n}\\right)\\in\\mathcal{G}(\\mathbf{s})\\times\\left\\{\\pm1\\right\\}^{n}\\right\\}\\leq\\beta,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ${\\mathcal{G}}(\\mathbf{s}):=\\{g(\\mathbf{s}):g\\in{\\mathcal{G}}\\}\\subseteq{\\mathrm{Tree}}\\left(\\mathbb{R},n\\right)$ ", "page_idx": 23}, {"type": "text", "text": "(ii)  The sequential $\\beta$ coveringnumberwithrespect to $l_{p}$ -norm of a function class ${\\mathcal{G}}\\subseteq(\\mathbb{S}\\to\\mathbb{R})$ on an $\\mathbb{S}$ -valued tree s of depth $n$ , where $p\\in[1,+\\dot{\\infty}]$ , is defined by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{N}_{p}\\left(\\beta,\\mathcal{G},\\mathbf{s}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let us further define ${\\mathcal{N}}_{p}\\left({\\boldsymbol{\\beta}},{\\boldsymbol{\\mathcal{G}}},{\\boldsymbol{n}}\\right):=\\operatorname*{sup}\\left\\{{\\mathcal{N}}_{p}\\left({\\boldsymbol{\\beta}},{\\boldsymbol{\\mathcal{G}}},\\mathbf{s}\\right):\\mathbf{s}\\in\\operatorname{Tree}\\left(\\mathbb{S},{\\boldsymbol{n}}\\right)\\right\\}$ to be the maximal sequential $\\beta$ -covering number with respect to $l_{p}$ -normof $\\mathcal{G}$ over $\\mathbb{S}$ -valued trees of depth $n$ Now, we will refer to $\\log\\mathcal{N}_{p}\\left(\\beta,\\mathcal{G},n\\right)$ as the sequential $\\beta$ -metric entropy of $\\mathcal{G}$ with respect to $l_{p}$ -norm. ", "page_idx": 23}, {"type": "text", "text": "In particular, we are going to study the behavior of the minimax regret $\\mathcal{V}_{n}(\\mathcal{F})$ for the case where the sequential metric entropy of $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to[-L,L])$ w.r.t. $l_{2}$ -norm grows polynomially as the scale $\\beta$ decreases: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\log\\mathcal{N}_{2}\\left(\\beta,\\mathcal{F},n\\right)\\sim\\beta^{-p}\\quad\\mathrm{for}\\;p\\in\\left(0,+\\infty\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let us also consider the parametric \u00b0 $\\;{}\\boldsymbol{p}=0$ \" case when the sequential covering number of $\\mathcal{F}$ with respect to $l_{2}$ -norm itself behaves as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{N}_{2}\\left(\\beta,\\mathcal{F},n\\right)\\sim\\beta^{-d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For instance, the function class $\\mathcal{F}:=\\left\\{f_{\\pmb{\\theta}}(\\cdot):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}:\\pmb{\\theta}\\in\\Theta\\right\\}$ for the linear regression problem in a bounded measurable subset $\\Theta\\subseteq\\mathbb{R}^{d}$ , where the function $f_{\\pmb{\\theta}}(\\cdot):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is given by $f_{\\pmb\\theta}(\\mathbf x):=\\pmb\\theta^{\\top}\\mathbf x$ for $\\pmb{\\theta}\\in\\mathbb{R}^{d}$ , satisfies the condition (62). By employing the main results (in particular, Theorem 2) in [45], one can establish the following conclusion: ", "page_idx": 23}, {"type": "text", "text": "Theorem B.2 (The rates of convergence of the minimax regret). Given any function class ${\\mathcal{F}}\\subseteq$ $(\\mathbb{X}\\times\\mathbb{A}\\rightarrow[-L,L])$ with sequentialmetric entropy growth $\\log\\mathcal{N}_{2}\\left(\\beta,\\mathcal{F},n\\right)\\leq\\beta^{-p}$ for $p\\in(0,+\\infty)$ itholdsthat ", "page_idx": 23}, {"type": "text", "text": "(i) for $p\\in(2,+\\infty)$ , the minimax regret (58) is bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\ V_{n}(\\mathcal{F})\\leq\\left(4+\\frac{24}{p-2}\\right)L n^{1-\\frac{1}{p}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(ii) for $p\\in(0,2)$ the minimax regret (58) is bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{V}_{n}(\\mathcal{F})\\leq\\left(32L^{2}+4L+\\frac{24L}{2-p}\\right)n^{1-\\frac{2}{p+2}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(ii)for $p=2$ , the minimax regret (58) is bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\nu_{n}(\\mathcal{F})\\leq\\left(32L^{2}+4L+3\\right)\\sqrt{n}\\log n.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(iv) for the parametric case (62), the minimax regret (58) is bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{V}_{n}(\\mathcal{F})\\leq\\left(16L^{2}+4L+12\\right)d\\log n.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(v) if the function class $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to[-L,L])$ is a finite set, the minimax regret (58) is bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{V}_{n}(\\mathcal{F})\\leq32L^{2}\\log\\left|\\mathcal{F}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "It is shown in [45] that the upper bounds (i)-(iv) on the minimax regret (58) in Theorem B.2 are tight up to logarithmic factors. See Theorem 3 therein for further details. ", "page_idx": 24}, {"type": "text", "text": "Although Theorem B.2 characterizes the rates of convergence of the minimax regret (58) in various scenarios statistically, its proof is non-constructive in the sense that the regret bounds therein are established without explicitly constructing an algorithm. In order to provide a general algorithmic framework for the problem of online non-parametric regression, we follow the abstract relaxation recipe proposed in [47]. It was shown in [47] that if one can find a sequence of mappings from the observed data to real numbers $\\mathsf{R e l}_{n}$ , often called a relaxation, satisfying some desirable conditions, then one can construct estimators based on such relaxations. To be specific, we search for a relaxation ${\\mathrm{Rel}}_{n}\\left(\\cdot,\\cdot\\right):\\left\\vert\\mathsf{d}\\right\\vert_{k=0}^{n}\\left\\{\\left(\\mathbb{X}\\times\\mathbb{A}\\right)^{k}\\times\\left\\lbrack-L,L\\right\\rbrack^{k}\\right\\}\\to\\mathbb{R}$ that satisfies the following two conditions: ", "page_idx": 24}, {"type": "text", "text": "Assumption 5 (Initial condition). The relaxation ${\\mathrm{Rel}}_{n}\\left(\\cdot,\\cdot\\right):\\left\\vert\\mathsf{d}\\right\\vert_{k=0}^{n}\\left\\{\\left(\\mathbb{X}\\times\\mathbb{A}\\right)^{k}\\times\\left\\lbrack-L,L\\right\\rbrack^{k}\\right\\}\\to\\mathbb{R}$ satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname{Rel}_{n}\\left(\\left(\\mathbf{x},\\mathbf{a}\\right)_{1:n},\\mathbf{y}_{1:n}\\right)\\geq-\\operatorname*{inf}\\left\\{\\sum_{k=1}^{n}\\left\\{y_{i}-\\mu\\left(x_{i},a_{i}\\right)\\right\\}^{2}:\\mu(\\cdot,\\cdot)\\in\\mathcal{F}\\right\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $(\\mathbf{x},\\mathbf{a})_{1:k}:=((x_{i},a_{i}):i\\in[k])\\in(\\mathbb{X}\\times\\mathbb{A})^{k}$ and $\\mathbf{y}_{1:k}:=(y_{i}:i\\in[k])\\in[-L,L]^{k}$ for every $k\\in[n]$ ", "page_idx": 24}, {"type": "text", "text": "Assumption 6 (Recursive admissibility condition). The relaxation $\\operatorname{Rel}_{n}\\left(\\cdot,\\cdot\\right)$ satisfies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{y}_{k}\\in[-L,L]}\\operatorname*{sup}_{y_{k}\\in[-L,L]}\\left\\{\\left(\\hat{y}_{k}-y_{k}\\right)^{2}+\\mathrm{Rel}_{n}\\left((\\mathbf{x},\\mathbf{a})_{1:k}\\,,\\mathbf{y}_{1:k}\\right)\\right\\}\\leq\\mathrm{Rel}_{n}\\left((\\mathbf{x},\\mathbf{a})_{1:k-1}\\,,\\mathbf{y}_{1:k-1}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for any $k\\in[n]$ and any $x_{k}\\in\\mathbb{X}$ ", "page_idx": 24}, {"type": "text", "text": "A relaxation $\\operatorname{Rel}_{n}\\left(\\cdot,\\cdot\\right):\\left\\vert\\mathsf{t}\\right\\vert_{k=0}^{n}\\left\\{\\left(\\mathbb{X}\\times\\mathbb{A}\\right)^{k}\\times\\left\\lbrack-L,L\\right\\rbrack^{k}\\right\\}\\rightarrow\\mathbb{R}$ satisfying Assumptions 5 and 6 is said to be admissible. With an admissible relaxation $\\operatorname{Rel}_{n}\\left(\\cdot,\\cdot\\right)$ in hand, one can design an algorithm for the online non-parametric regression problem with the following associated regret bound (see Algorithm 5 for a detailed description): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\overline{{\\mathrm{Regret}}}\\left(n,\\mathcal{F};\\mathrm{Alg.\\,}5\\right)}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\left\\{Y_{i}-\\widehat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\left(X_{i},A_{i}\\right)\\right\\}^{2}-\\operatorname*{inf}\\left\\{\\displaystyle\\sum_{i=1}^{n}\\left\\{Y_{i}-\\mu\\left(X_{i},A_{i}\\right)\\right\\}^{2}:\\mu\\in\\mathcal{F}\\right\\}}\\\\ &{\\leq\\mathrm{Rel}_{n}\\left(\\varnothing,\\varnothing\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We further notice that if the function $y_{i}\\,\\in\\,[-L,L]\\,\\mapsto\\,\\left({\\hat{y}}-y_{i}\\right)^{2}+\\mathsf{R e l}_{n}$ ((x,a)1:),(y1:-1,yi)) isconvexfor every $\\left({\\hat{y}},\\mathbf{x}_{1:n},\\mathbf{a}_{1:n},\\mathbf{y}_{1:i-1}\\right)\\in[-L,L]\\times\\mathbb{X}^{n}\\times{\\mathbb{A}}^{n}\\times[-L,L]^{\\imath-1}$ and $i\\,\\in\\,[n]$ , then the prediction rules (71) and (72) becomes much simpler, since the supremum over $y_{i}\\in[-L,L]$ is attained either $L$ or $-L$ . The prediction rules then can be written as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\hat{\\mu}_{1}(\\mathcal{O})(x,a)}\\\\ &{\\in\\arg\\operatorname*{min}\\left\\{\\operatorname*{max}\\left\\{\\left(\\hat{y}-L\\right)^{2}+\\mathsf{R e l}_{n}\\left((x,a),L\\right),(\\hat{y}+L)^{2}+\\mathsf{R e l}_{n}\\left((x,a),-L\\right)\\right\\}:\\hat{y}\\in\\left[-L,L\\right]\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and for $i\\in\\{2,3,\\cdots\\,,n\\}$ \uff0c ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\;\\;\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)(x,a)}\\\\ &{\\in\\;\\mathrm{arg\\,min}\\left\\{\\operatorname*{max}\\left\\{\\left(\\hat{y}-L\\right)^{2}+\\mathrm{Rel}_{n}\\left(\\left(\\left(\\mathbf{X},\\mathbf{A}\\right)_{1:i-1},(x,a)\\right),(\\mathbf{Y}_{1:i-1},L)\\right),\\right.}\\\\ &{\\;\\;\\;\\left.\\left(\\hat{y}+L\\right)^{2}+\\mathrm{Rel}_{n}\\left(\\left(\\left(\\mathbf{X},\\mathbf{A}\\right)_{1:i-1},(x,a)\\right),(\\mathbf{Y}_{1:i-1},-L)\\right)\\right\\}:\\hat{y}\\in\\left[-L,L\\right]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "One can easily observe that the prediction rules (73) and (74) can be further simplified as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{1}(\\mathcal{O})(x,a)=\\chi_{[-L,L]}\\left\\{\\frac{\\mathrm{Rel}_{n}\\left((x,a),L\\right)-\\mathrm{Rel}_{n}\\left((x,a),-L\\right)}{4L}\\right\\},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Require: a relaxation $\\operatorname{Rel}_{n}\\left(\\cdot,\\cdot\\right):\\left\\vert\\pmb{\\updownarrow}_{k=0}^{n}\\left\\{\\left(\\mathbb{X}\\times\\mathbb{A}\\right)^{k}\\times\\left[-L,L\\right]^{k}\\right\\}\\rightarrow\\mathbb{R}.$ 1: We first choose $\\hat{\\mu}_{1}(\\mathcal{D})(\\cdot,\\cdot)\\in(\\mathbb{X}\\times\\mathbb{A}^{\\bullet}\\mathbb{R})$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{\\mu}_{1}(\\emptyset)(x,a)\\in\\left\\{\\operatorname*{sup}_{y_{1}\\in[-L,L]}\\left\\{\\left(\\hat{y}-y_{1}\\right)^{2}+\\mathrm{Rel}_{n}\\left((x,a),y_{1}\\right)\\right\\}:\\hat{y}\\in[-L,L]\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "2: for $i=2,3,\\cdots\\,,n$ do ", "page_idx": 25}, {"type": "text", "text": "3: Observe a triple $(X_{i},A_{i},Y_{i})\\in\\mathbb{O}$   \n4: We compute $\\hat{\\mu}_{i}\\left(\\mathbf{O}_{i-1}\\right)\\in\\left(\\mathbb{X}\\times\\mathbb{A}\\rightarrow\\mathbb{R}\\right)$ according to the following rule: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mu_{i}\\left\\{\\mathbf{U}_{i-1}\\right\\}:x,u)}\\\\ &{\\in\\mathrm{arg}\\operatorname*{min}\\left\\{\\underset{y_{i}\\in\\left[-L,L\\right]}{\\operatorname*{sup}}\\left\\{\\left(\\hat{y}-y_{i}\\right)^{2}+\\mathrm{Rel}_{n}\\left(\\left(\\left(\\mathbf{X},\\mathbf{A}\\right)_{1:i-1},(x,a)\\right),(\\mathbf{Y}_{1:i-1},y_{i})\\right)\\right\\}:\\hat{y}\\in\\left[-L,L\\right]\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "5: end for ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "6: return the sequence of estimates $\\{\\hat{\\mu}_{i}\\,(\\mathbf{O}_{i-1})\\in(\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{R}):i\\in[n]\\}$ of the treatment effect. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Gamma}_{\\hat{\\mu}_{i}}\\left(\\mathbf{O}_{i-1}\\right)(x,a)}\\\\ &{=\\chi_{\\left[-L,L\\right]}\\left\\{\\frac{\\mathrm{Rel}_{n}\\,\\left(\\left(\\left(\\mathbf{X},\\mathbf{A}\\right)_{1:i-1},(x,a)\\right),\\,(\\mathbf{Y}_{1:i-1},L)\\right)-\\mathrm{Rel}_{n}\\,\\left(\\left(\\left(\\mathbf{X},\\mathbf{A}\\right)_{1:i-1},(x,a)\\right),\\,(\\mathbf{Y}_{1:i-1},-L)\\right)}{4L}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\chi_{[-L,L]}(\\cdot):\\mathbb{R}\\to[-L,L]$ defines a clip function onto the interval $[-L,L]$ i.e., ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\chi_{[-L,L]}(x):={\\left\\{\\begin{array}{l l}{L}&{{\\mathrm{if~}}x>L;}\\\\ {x}&{{\\mathrm{if~}}-L\\leq x\\leq L;}\\\\ {-L}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By directly using Lemma $^ \u1e0a I6 \u1e0c$ in [45], one can obtain the following significant result: ", "page_idx": 25}, {"type": "text", "text": "Theorem B.3. The relaxation $\\mathcal{R}_{n}\\left(\\cdot,\\cdot\\right):\\left\\lfloor\\right\\rfloor_{k=0}^{n}\\Big\\{\\left(\\mathbb{X}\\times\\mathbb{A}\\right)^{k}\\times\\left[-L,L\\right]^{k}\\Big\\}\\to\\mathbb{R}$ defined as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{R}_{n}\\left((\\mathbf{x},\\mathbf{a})_{1:k},\\mathbf{y}_{1:k}\\right)}\\\\ &{:=\\begin{array}{l}{\\displaystyle\\operatorname*{sup}_{\\left(\\mathbf{z},\\mathbf{m}\\right)}\\mathbb{E}_{\\epsilon_{1:n}\\sim\\mathrm{Unif}\\left(\\left\\{\\pm1\\right\\}^{n}\\right)}\\left[\\operatorname*{sup}\\left\\{\\sum_{j=k+1}^{n}\\left[4L\\epsilon_{j}\\left\\{\\mu\\left(\\mathbf{z}_{j}\\left(\\epsilon_{1:j-1}\\right)\\right)-\\mathbf{m}_{j}\\left(\\epsilon_{1:j-1}\\right)\\right\\}\\right.\\right.}\\\\ {\\displaystyle\\left.\\left.-\\left\\{\\mu\\left(\\mathbf{z}_{j}\\left(\\epsilon_{1:j-1}\\right)\\right)-\\mathbf{m}_{j}\\left(\\epsilon_{1:j-1}\\right)\\right\\}^{2}\\right]-\\sum_{j=1}^{k}\\left\\{\\mu\\left(x_{j},a_{j}\\right)-y_{j}\\right\\}^{2}:\\mu\\in\\mathcal{F}\\right\\}\\right],}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "wherethepair $(\\mathbf{z},\\mathbf{m})$ rangesover thesetTree $(\\mathbb{X}\\times\\mathbb{A},n)\\times\\operatorname{Tree}{\\left(\\mathbb{R},n\\right)},$ isanadmissiblerelaxation. As a direct consequence of the regret bound (70), Algorithm 5 using the admissible relaxation $\\mathcal{R}_{n}\\left(\\cdot,\\cdot\\right)$ as an input enjoys theregret bound of an offset Rademacher complexity: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\overline{{\\mathrm{Regret}}}}\\left(n,{\\mathcal{F}};\\mathrm{Alg.~}5\\right)}\\\\ &{\\le{\\mathcal{R}}_{n}\\left({\\mathcal{D}},{\\mathcal{O}}\\right)}\\\\ &{=\\begin{array}{l}{\\displaystyle\\operatorname*{sup}_{\\mathbf{(z,m)}}\\mathbb{E}_{\\epsilon_{1:n}\\sim\\mathrm{Unif}(\\{\\pm1\\}^{n})}\\left[\\operatorname*{sup}\\left\\{\\sum_{j=1}^{n}\\left[4L\\epsilon_{j}\\left\\{\\mu\\left(\\mathbf{z}_{j}\\left(\\epsilon_{1:j-1}\\right)\\right)-\\mathbf{m}_{j}\\left(\\epsilon_{1:j-1}\\right)\\right\\}\\right.\\right.}\\\\ {\\displaystyle\\left.\\left.\\qquad-\\left\\{\\mu\\left(\\mathbf{z}_{j}\\left(\\epsilon_{1:j-1}\\right)\\right)-\\mathbf{m}_{j}\\left(\\epsilon_{1:j-1}\\right)\\right\\}^{2}\\right]:\\mu\\in{\\mathcal{F}}\\right\\}\\right].}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since the upper bounds on the minimax regret (58) provided in Theorem B.2 are established by further upper bounding the offset Rademacher complexity $\\mathcal{R}_{n}\\left(\\boldsymbol{\\mathcal{O}},\\boldsymbol{\\mathcal{O}}\\right)$ , one can end up with the following corollary: ", "page_idx": 26}, {"type": "text", "text": "Corollary B.1. Consider any function class $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to[-L,L])$ with sequential metric entropy growth $\\log\\mathcal{N}_{2}\\left(\\beta,\\mathcal{F},n\\right)\\leq\\beta^{-p}$ for $p\\in(0,+\\infty)$ Then, Algorithm $5$ using the admissible relaxation $\\mathcal{R}_{n}\\left(\\cdot,\\cdot\\right)$ defined by (77) as an input enjoys the following regret bounds: ", "page_idx": 26}, {"type": "text", "text": "(i) for $p\\in(2,+\\infty)$ ,it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overline{{\\mathrm{Regret}}}\\,(n,\\mathcal{F};\\mathrm{Alg.\\,}5)\\leq\\left(4+\\frac{24}{p-2}\\right)L n^{1-\\frac{1}{p}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(ii) for $p\\in(0,2)$ ,it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\overline{{\\operatorname{Regret}}}}\\left(n,{\\mathcal{F}};\\operatorname{Alg.\\,}5\\right)\\leq\\left(32L^{2}+4L+{\\frac{24L}{2-p}}\\right)n^{1-{\\frac{2}{p+2}}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(ii) for $p=2$ it holdsthat ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\overline{{\\operatorname{Regret}}}\\left(n,\\mathcal{F};\\mathrm{Alg.~}5\\right)\\leq\\left(32L^{2}+4L+3\\right)\\sqrt{n}\\log n.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(iv) for the parametric case (62), it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\overline{{\\operatorname{Regret}}}}\\left(n,{\\mathcal{F}};\\operatorname{Alg.}\\,5\\right)\\leq\\left(16L^{2}+4L+12\\right)d\\log n.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(v) if the function class $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to[-L,L])$ is a finite set, it holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{\\mathrm{Regret}}}\\left(n,\\mathcal{F};\\mathrm{Alg.~}5\\right)\\leq32L^{2}\\log\\left|\\mathcal{F}\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Even though Corollary B.1 gives no-regret learning guarantees of Algorithm 5 with the admissible relaxation $\\mathcal{R}_{n}\\left(\\cdot,\\cdot\\right)$ defined by (77) for various function classes $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to[-L,L])$ , it is still NOT a practical algorithm since the relaxation $\\mathcal{R}_{n}\\left(\\cdot,\\cdot\\right)$ defined as (77) is not directly computable in general. To address this problem, [45] provided a generic schema for deriving implementable online non-parametric regression algorithms. The schema can be described as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{R}_{n}\\left((\\mathbf{x},\\mathbf{a})_{1:k}\\,,\\mathbf{y}_{1:k}\\right)\\leq\\mathrm{Rel}_{n}\\left((\\mathbf{x},\\mathbf{a})_{1:k}\\,,\\mathbf{y}_{1:k}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "forevery $(k,\\mathbf{x}_{1:n},\\mathbf{a}_{1:n},\\mathbf{y}_{1:n})\\in\\{0,1,\\cdots,n\\}\\times\\mathbb{X}^{n}\\times\\mathbb{A}^{n}\\times[-L,L]^{n}$ ', and the function $y_{k}\\;\\in\\;[-L,L]\\,\\mapsto\\,\\left(\\hat{y}-y_{k}\\right)^{2}+\\mathrm{Rel}_{n}\\left(\\left(\\left(\\mathbf{x},\\mathbf{a}\\right)_{1:k}\\right),\\right.$ $\\hat{y}-y_{k})^{2}+\\operatorname{Rel}_{n}\\left(\\left(\\left(\\mathbf{x},\\mathbf{a}\\right)_{1:k}\\right),\\left(\\mathbf{y}_{1:k-1},y_{k}\\right)\\right)\\ \\in\\ \\mathbb{R}$ isconvexfor every $(\\hat{y},\\mathbf x_{1:n},\\mathbf a_{1:n},\\mathbf y_{1:k-1})\\in[-L,L]\\times\\mathbb{X}^{n}\\times\\mathbb{A}^{n}\\times[-L,L]^{k-1}$ and $k\\in[n]$ ", "page_idx": 26}, {"type": "text", "text": "(b)  Next, we check the following condition: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{\\left(x_{k},a_{k},\\mu_{k}\\right)\\in\\mathbb{X}\\times\\mathbb{A}\\times\\Delta\\left([-L,L]\\right)}\\left\\{\\mathbb{E}_{y_{k}\\sim\\mu_{k}}\\left[\\left(\\mathbb{E}_{y_{k}\\sim\\mu_{k}}\\left[y_{k}\\right]-y_{k}\\right)^{2}\\right]+\\mathbb{E}_{y_{k}\\sim\\mu_{k}}\\left[\\mathrm{Rel}_{n}\\left(\\left(\\mathbf{x},\\mathbf{a}\\right)_{1:k},\\mathbf{y}_{1:k}\\right)\\right]\\right\\}}\\\\ &{\\displaystyle\\leq\\mathrm{Rel}_{n}\\left(\\left(\\mathbf{x},\\mathbf{a}\\right)_{1:k-1},\\mathbf{y}_{1:k-1}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(c) Implement Algorithm 5 using the relaxation $\\operatorname{Rel}_{n}\\left(\\cdot,\\cdot\\right)$ as an input. ", "page_idx": 26}, {"type": "text", "text": "The authors proved that any computable relaxation $\\operatorname{Rel}_{n}\\left(\\cdot,\\cdot\\right)$ satisfying conditions stated in (a) and (b) are admissible; see Proposition $^{17}$ therein. Consequently, any online non-parametric regression algorithm produced by the above generic schema always satisfies the regret bound (7o). Moreover, the authors established a practical online non-parametric regression algorithm with no-regret learning guarantees based on the above schema for the finite function class $\\mathcal{F}\\subseteq(\\mathbb{X}\\times\\mathbb{A}\\to[-L,L])$ and the online linear regression problem. ", "page_idx": 26}, {"type": "text", "text": "C Proofs for Section 4 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "C.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 4.1 can be established by taking the following two lemmas collectively: ", "page_idx": 27}, {"type": "text", "text": "Lemma C.1. Under Assumption 3, the local minimax risk over the class $\\mathcal{C}_{\\delta}\\left(\\mathcal{T}^{*}\\right)$ is lower bounded by ", "page_idx": 27}, {"type": "equation", "text": "$$\n{\\cal M}_{n}\\left({\\mathcal C}_{\\delta}\\left(\\mathbb{Z}^{*}\\right)\\right)\\geq\\frac{1}{2304}\\left(1-\\frac{1}{\\sqrt{2}}\\right)\\cdot\\frac{1}{n}\\mathrm{Var}_{X\\sim\\Xi^{*}}\\left[\\langle g(X,\\cdot),\\mu^{*}(X,\\cdot)\\rangle_{\\lambda_{\\Lambda}}\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "provided that $n\\geq16H_{2\\rightarrow4}^{2}$ ", "page_idx": 27}, {"type": "text", "text": "Lemma C.2. Under Assumption 4, the local minimax risk over the class $\\mathcal{C}_{\\delta}\\left(\\mathcal{T}^{*}\\right)$ is lower bounded by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{M}_{n}\\left(\\mathcal{C}_{\\delta}\\left(\\mathcal{Z}^{*}\\right)\\right)\\geq\\frac{1}{8K^{4}}\\cdot\\frac{\\left\\Vert\\boldsymbol{\\sigma}\\right\\Vert_{\\left(n\\right)}^{2}}{n}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "C.2  Proof of Lemma C.1 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The proof relies on Le Cam's two-point method by taking the outcome kernel $\\Gamma^{*}:\\mathbb{X}\\times\\mathbb{A}\\to\\Delta(\\mathbb{Y})$   \nto be fixed, and perturbing the context distribution $\\Xi^{*}(\\bar{\\cdot})\\in\\Delta(\\mathbb{X})$ : we first construct a collection $\\{\\bar{\\Xi_{s}}(\\cdot)\\in\\Delta(\\mathbb{X}):s\\in(0,+\\infty)\\}$ $s>0$ $\\Xi_{s}\\in\\mathcal{N}\\left(\\Xi^{*}\\right)$ $\\mathbb{P}_{(\\Xi_{s},\\Gamma^{*})}^{n}\\in\\Delta\\left(\\mathbb{O}^{n}\\right)$ $\\mathbb{P}_{(\\Xi^{*},\\Gamma^{*})}^{n}\\in\\Delta\\left(\\mathbb{O}^{n}\\right)$   \nare indistinguishable, but large enough such that the functional values $\\tau\\left(\\Xi_{s},\\Gamma^{*}\\right)$ and $\\tau\\left(\\Xi^{*},\\Gamma^{*}\\right)$ are   \nwell-separated. Le Cam's two-point lemma (the equation (15.14) in [62]) guarantees that the local   \nminimax risk $\\mathcal{M}_{n}\\left(\\mathcal{C}_{\\delta}\\left(\\mathcal{Z}^{\\ast}\\right)\\right)$ is lower bounded as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{M}_{n}\\left(\\mathcal{C}_{\\delta}\\left(\\mathcal{Z}^{*}\\right)\\right)\\geq\\frac{1}{4}\\left\\{1-\\mathrm{TV}\\left(\\mathbb{P}_{\\left(\\Xi_{s},\\Gamma^{*}\\right)}^{n},\\mathbb{P}_{\\left(\\Xi^{*},\\Gamma^{*}\\right)}^{n}\\right)\\right\\}\\left\\{\\tau\\left(\\Xi_{s},\\Gamma^{*}\\right)-\\tau\\left(\\Xi^{*},\\Gamma^{*}\\right)\\right\\}^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "provided that $\\Xi_{s}\\in\\mathcal{N}\\left(\\Xi^{*}\\right)$ ", "page_idx": 27}, {"type": "text", "text": "As the first step, we upper bound the total variation distance TV P\"=, \\*),IP\u201c=-,-) . Thanks to the Pinsker-Csiszar-Kullback inequality, one has ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\mathbb{P}_{(\\Xi_{s},\\Gamma^{*})}^{n},\\mathbb{P}_{(\\Xi^{*},\\Gamma^{*})}^{n}\\right)\\le\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(\\mathbb{P}_{(\\Xi_{s},\\Gamma^{*})}^{n}\\Big|\\Big|\\mathbb{P}_{(\\Xi^{*},\\Gamma^{*})}^{n}\\right)}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We can fn that the densityfction of th lw $\\mathbb{P}_{\\mathcal{T}}^{n}=\\mathbb{P}_{(\\Xi,\\Gamma)}^{n}\\in\\Delta\\left(\\mathbb{O}^{n}\\right)$ of the sample rajctory $\\mathbf{O}_{n}$ under the problem instance $\\mathcal{T}=(\\Xi,\\Gamma)\\in\\mathbb{I}$ with respect to the base measure $(\\lambda_{\\mathbb{X}}\\otimes\\lambda_{\\mathbb{A}}\\otimes\\lambda_{\\mathbb{A}})^{\\otimes n}$ is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\np_{\\mathbb{Z}}^{n}\\left(\\mathbf{o}_{n}\\right)=p_{\\left(\\mathbb{Z},\\Gamma\\right)}^{n}\\left(\\mathbf{o}_{n}\\right)=\\prod_{i=1}^{n}\\left\\{\\xi\\left(x_{i}\\right)\\pi_{i}^{*}\\left(x_{i},\\mathbf{o}_{i-1};a_{i}\\right)\\gamma\\left(\\left.y_{i}\\right|x_{i},a_{i}\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Using this fact, the KL-divergence $\\mathrm{KL}\\left(\\mathbb{P}_{(\\Xi_{s},\\Gamma^{*})}^{n}\\right|\\Big|\\mathbb{P}_{(\\Xi^{*},\\Gamma^{*})}^{n}\\right)$ can be computed as ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{KL}\\left(\\mathbb{P}_{(\\Xi\\circ,\\Gamma^{*})}^{n}\\right)\\Big\\Vert\\mathbb{P}_{(\\Xi^{*},\\Gamma^{*})}^{n}\\right)}\\\\ &{=\\mathbb{E}_{(\\Xi_{s},\\Gamma^{*})}\\left[\\log\\frac{p_{(\\Xi_{s},\\Gamma^{*})}^{n}\\left(\\mathbf{O}_{n}\\right)}{p_{(\\Xi^{*},\\Gamma^{*})}^{n}\\left(\\mathbf{O}_{n}\\right)}\\right]}\\\\ &{=\\mathbb{E}_{(\\Xi_{s},\\Gamma^{*})}\\left[\\displaystyle\\sum_{i=1}^{n}\\log\\frac{\\xi_{s}\\left(X_{i}\\right)\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)\\gamma^{*}\\left(Y_{i}|X_{i},A_{i}\\right)}{\\xi^{*}\\left(X_{i}\\right)\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)\\gamma^{*}\\left(Y_{i}|X_{i},A_{i}\\right)}\\right]}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{(\\Xi,\\Gamma^{*})}\\left[\\log\\frac{\\xi_{s}\\left(X_{i}\\right)}{\\xi^{*}\\left(X_{i}\\right)}\\right]}\\\\ &{=n\\cdot\\mathrm{KL}\\left(\\Xi_{s}\\|\\Xi^{*}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "So if one can show that $\\Xi_{s}\\in\\mathcal{N}\\left(\\Xi^{*}\\right)$ , then the equation (89) guarantees that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\left(\\mathbb{P}_{(\\Xi_{s},\\Gamma^{*})}^{n}\\right|\\Big|\\mathbb{P}_{(\\Xi^{*},\\Gamma^{*})}^{n}\\right)=n\\cdot\\mathrm{KL}\\left(\\Xi_{s}\\|\\Xi^{*}\\right)\\le1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which can be taken collectively with the bound (87) to produce the following conclusion: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname{TV}\\left(\\mathbb{P}_{(\\Xi_{s},\\Gamma^{*})}^{n},\\mathbb{P}_{(\\Xi^{*},\\Gamma^{*})}^{n}\\right)\\leq\\frac{1}{\\sqrt{2}}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "With the arguments thus far in place, it remains to construct a family $\\{\\Xi_{s}\\in\\Delta(\\mathbb{X}):s\\in(0,+\\infty)\\}$ and then choose a parameter $s>0$ such that $\\Xi_{s}\\in\\mathcal{N}\\left(\\Xi^{*}\\right)$ and the functional values $\\tau\\left(\\Xi_{s},\\Gamma^{*}\\right)$ and $\\tau\\left(\\Xi^{*},\\Gamma^{*}\\right)$ are well-separated. To this end, we consider the function $\\tilde{h}(\\cdot):\\mathbb{X}\\rightarrow\\mathbb{R}$ definedby ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\tilde{h}(x):=\\left\\{\\!\\!\\!\\begin{array}{l l}{h(x)}&{\\mathrm{if}\\,\\,\\,|h(x)|\\leq2H_{2\\to4}\\sqrt{\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[h^{2}(X)\\right]}}\\\\ {\\mathrm{sign}\\left(h(x)\\right)\\sqrt{\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[h^{2}(X)\\right]}}&{\\mathrm{otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $H_{2\\rightarrow4}\\geq1$ , one can easily find that $\\Big|\\tilde{h}(x)\\Big|\\leq|h(x)|$ for all $x\\in\\mathbb{X}$ . Now for each $s\\in(0,+\\infty)$ \uff0c we define the tilted probability measure $\\Xi_{s}(\\cdot)\\in\\Delta(\\mathbb{X})$ by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\xi_{s}(x)=\\frac{\\mathrm{d}\\Xi_{s}}{\\mathrm{d}\\lambda_{\\mathbb{X}}}(x):=\\frac{1}{\\mathcal{Z}(s)}\\xi^{*}(x)\\exp\\left(s\\tilde{h}(x)\\right),\\;\\forall x\\in\\mathbb{X},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{Z}(s):=\\int_{\\mathbb{X}}\\xi^{*}(x)\\exp\\Big(s\\widetilde{h}(x)\\Big)\\,\\mathrm{d}\\lambda_{\\mathbb{X}}(x)=\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\exp\\Big(s\\widetilde{h}(X)\\Big)\\right]}\\end{array}$ .At this point, we note for every $x\\in\\mathbb{X}$ that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\exp\\left(-s\\left\\|\\tilde{h}\\right\\|_{\\infty}\\right)\\leq\\exp\\left(s\\tilde{h}(x)\\right)\\leq\\exp\\left(s\\left\\|\\tilde{h}\\right\\|_{\\infty}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "which also immediately yields ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\exp\\left(-s\\left\\|\\tilde{h}\\right\\|_{\\infty}\\right)\\leq\\mathcal{Z}(s)=\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\exp\\left(s\\tilde{h}(X)\\right)\\right]\\leq\\exp\\left(s\\left\\|\\tilde{h}\\right\\|_{\\infty}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here we chose $\\begin{array}{r}{s=\\frac{1}{4\\left\\|h\\right\\|_{L^{2}(\\Xi^{*})}\\sqrt{n}}>0}\\end{array}$ $\\left|\\tilde{h}(x)\\right|\\leq2H_{2\\rightarrow4}\\left\\|h\\right\\|_{L^{2}(\\Xi^{*})}$ for all that ", "page_idx": 28}, {"type": "equation", "text": "$$\ns\\left\\|\\tilde{h}\\right\\|_{\\infty}=\\frac{1}{4\\sqrt{n}}\\cdot\\frac{\\left\\|\\tilde{h}\\right\\|_{\\infty}}{\\left\\|h\\right\\|_{L^{2}(\\Xi^{*})}}\\leq\\frac{H_{2\\rightarrow4}}{2\\sqrt{n}}\\overset{(\\mathrm{a})}{\\leq}\\frac{1}{8},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the step (a) follows due to the assumption that $n\\geq16H_{2\\rightarrow4}^{2}$ . Now, it's time to prove that $\\Xi_{s}\\in\\mathcal{N}\\left(\\Xi^{*}\\right)$ for the current choice of the parameter $s>0$ . Due to Theorem 5 in [14], it follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\Xi_{s}\\|\\Xi^{*}\\right)\\leq\\log\\left\\{1+\\chi^{2}\\left(\\Xi_{s}\\|\\Xi^{*}\\right)\\right\\}\\leq\\chi^{2}\\left(\\Xi_{s}\\|\\,\\Xi^{*}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "So it suffices to upper bound the $\\chi^{2}$ -divergence $\\chi^{2}\\left(\\Xi_{s}\\right\\vert\\right\\vert\\Xi^{*})$ . One can reveal that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\chi^{2}\\left(\\Xi_{s}\\|\\,\\Xi^{*}\\right)=\\mathrm{Var}_{X\\sim\\Xi^{*}}\\left[\\xi_{s}(X)\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\frac{1}{Z^{2}(s)}\\mathrm{Var}_{X\\sim\\Xi^{*}}\\left[\\exp\\left(s\\tilde{h}(X)\\right)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{1}{Z^{2}(s)}\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\left\\{\\exp\\left(s\\tilde{h}(X)\\right)-1\\right\\}^{2}\\right]}\\\\ &{\\qquad\\qquad\\overset{(b)}{\\leq}\\exp\\left(2s\\left\\|\\tilde{h}\\right\\|_{\\infty}\\right)\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\exp\\left(2s\\left|\\tilde{h}(X)\\right|\\right)\\cdot s^{2}\\tilde{h}^{2}(X)\\right]}\\\\ &{\\qquad\\overset{(c)}{\\leq}\\exp\\left(4s\\left\\|\\tilde{h}\\right\\|_{\\infty}\\right)\\cdot s^{2}\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[h^{2}(X)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the step (b) makes use of the fact (93) together with the elementary bound $|\\exp(u)-1|\\leq$ $|u|\\exp{(|u|)},\\forall u\\in\\mathbb{R}$ , and the step (c) follows from the fact $\\Big|\\tilde{h}(x)\\Big|\\,\\le\\,|h(x)|,\\forall x\\,\\in\\,\\mathbb{X}$ If we put ", "page_idx": 28}, {"type": "text", "text": "S = 41hlL2(=\\*) Vn into the bound (96), then we obtain from the fact $s\\left\\|{\\tilde{h}}\\right\\|_{\\infty}\\leq{\\frac{1}{8}}$ together with the basic inequality (95) that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\left(\\Xi_{s}\\|\\,\\Xi^{*}\\right)\\leq\\chi^{2}\\left(\\Xi_{s}\\|\\,\\Xi^{*}\\right)\\leq2s^{2}\\left\\|h\\right\\|_{L^{2}\\left(\\Xi^{*}\\right)}^{2}=\\frac{1}{8n},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "whihimpies $\\Xi_{s}\\in\\mathcal{N}\\left(\\Xi^{*}\\right)$ for thechice ofthe arameter $\\begin{array}{r}{s=\\frac{1}{4\\|h\\|_{L^{2}(\\Xi^{*})}\\sqrt{n}}}\\end{array}$ Hence,theuper bound on the total variation distance (90) turns out to be valid. ", "page_idx": 29}, {"type": "text", "text": "Next, we lower bound the gap between the functional values $\\tau\\left(\\Xi_{s},\\Gamma^{*}\\right)$ and $\\tau\\left(\\Xi^{*},\\Gamma^{*}\\right)$ . It holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\tau\\left(\\Xi_{s},\\Gamma^{*}\\right)-\\tau\\left(\\Xi^{*},\\Gamma^{*}\\right)}\\\\ &{=\\mathbb{E}_{X\\sim\\Xi_{s}}\\left[\\langle g(X,\\cdot),\\mu^{*}(X,\\cdot)\\rangle_{\\lambda_{s}}\\right]-\\tau\\left(Z^{*}\\right)}\\\\ &{=\\frac{1}{\\mathcal{Z}(s)}\\int_{\\mathbb{X}}\\xi^{*}(x)\\exp\\left(s\\tilde{h}(x)\\right)\\underbrace{\\left\\{\\langle g(x,\\cdot),\\mu^{*}(x,\\cdot)\\rangle_{\\lambda_{s}}-\\tau\\left(Z^{*}\\right)\\right\\}}_{=\\ h(x)}\\mathrm{d}\\lambda_{\\mathbb{X}}(x)}\\\\ &{=\\frac{1}{\\mathcal{Z}(s)}\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[h(X)\\exp\\left(s\\tilde{h}(X)\\right)\\right]}\\\\ &{=\\frac{\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[h(X)\\exp\\left(s\\tilde{h}(X)\\right)\\right]}{\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\exp\\left(s\\tilde{h}(X)\\right)\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $s\\left\\|{\\tilde{h}}\\right\\|_{\\infty}\\leq{\\frac{1}{8}}$ we have $s\\tilde{h}(X)\\in\\left[-\\textstyle{\\frac{1}{4}},\\textstyle{\\frac{1}{4}}\\right]$ and heret sieqlty ", "page_idx": 29}, {"type": "equation", "text": "$$\n|\\exp(u)-1-u|\\leq u^{2},\\,\\forall u\\in\\left[-\\frac{1}{4},\\frac{1}{4}\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "implies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[h(X)\\exp\\Big(s\\tilde{h}(X)\\Big)\\right]}\\\\ &{\\stackrel{(a)}{\\geq}\\underbrace{\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[h(X)\\right]+s\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[h(X)\\right]\\left|\\tilde{h}(X)\\right|\\right]-s^{2}\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[h(X)\\right]\\tilde{h}^{2}(X)\\right]}_{=0}}\\\\ &{\\stackrel{(c)}{\\geq}s\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[\\tilde{h}^{2}(X)\\right]-s^{2}\\sqrt{\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[h^{2}(X)\\right]}\\;\\underbrace{\\sqrt{\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[h^{4}(X)\\right]}}_{=H_{2+\\mathbb{Z}^{*}}\\left[h_{2}<X_{\\sim\\mathbb{Z}^{*}}\\left[h^{2}(X)\\right]\\right]}}\\\\ &{\\stackrel{(0)}{\\geq}\\frac{s}{2}\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[h^{2}(X)\\right]-s^{2}H_{2+\\mathbb{Z}^{*}}\\left(\\mathbb{E}_{X\\sim\\mathbb{Z}^{*}}\\left[h^{2}(X)\\right]\\right)^{3}}\\\\ &{=\\frac{\\|h\\|_{\\mathbb{Z}^{2}(\\mathbb{Z}^{*})}}{8}\\left(\\frac{1}{\\sqrt{n}}-\\frac{H_{2-\\mathbb{Z}^{*}}}{2n}\\right)}\\\\ &{\\stackrel{(c)}{\\geq}\\frac{\\|h\\|_{\\mathbb{Z}^{2}(\\mathbb{Z}^{*})}}{16\\mathbb{W}\\sqrt{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where the step (d) holds due to the fact that sign $(h(x))=\\mathrm{sign}\\,\\Big(\\tilde{h}(x)\\Big),\\forall x\\in\\mathbb{X}$ the step (e) makes use of the property that $\\left|{\\tilde{h}}(x)\\right|\\le|h(x)|,\\forall x\\in\\mathbb{X}$ together with the Cauchy-Schwarz inequality, the step(f) follows du toLmma7in [42], and thesp $(\\mathrm{g})$ utlizes the assumption that $n\\geq16H_{2\\rightarrow4}^{2}$ Putting the lower bound (99) into the equation (98) yields ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\tau\\left(\\Xi_{s},\\Gamma^{*}\\right)-\\tau\\left(\\Xi^{*},\\Gamma^{*}\\right)\\geq\\frac{\\left\\Vert h\\right\\Vert_{L^{2}\\left(\\Xi^{*}\\right)}}{16\\sqrt{n}\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\exp\\left(s\\tilde{h}(X)\\right)\\right]}\\overset{\\footnotesize(\\ h)}{\\geq}\\frac{\\left\\Vert h\\right\\Vert_{L^{2}\\left(\\Xi^{*}\\right)}}{24\\sqrt{n}},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Where thestep h) olds since $\\begin{array}{r}{\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\exp\\Big(s\\tilde{h}(X)\\Big)\\right]\\le\\frac{3}{2}}\\end{array}$ which followsby the faet $\\Big|s\\tilde{h}(X)\\Big|\\leq{\\textstyle{\\frac{1}{8}}}$ Finally, by taking three pieces (86), (90), and (100) collectively, one completes the proof of Lemma C.1. ", "page_idx": 29}, {"type": "text", "text": "C.3 Proof of Lemma C.2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The proof of Lemma C.2 is also heavily relies on Le Cam's two-point method. For each $(i,s,z)\\in$ $[n]\\check{\\times}\\left(0,+\\infty\\right)\\times\\{\\pm1\\}$ , we consider the function $\\mu_{i}(z s)(\\cdot,\\cdot):\\mathbb{X}^{\\cdot}\\times\\mathbb{A}\\to\\mathbb{R}$ defined by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mu_{i}(z s)(x,a):=\\mu^{*}(x,a)+\\frac{z s g(x,a)}{\\pi_{i}(x,a)}\\sigma^{2}(x,a),\\;\\forall(x,a)\\in\\mathbb{X}\\times\\mathbb{A}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Also, we define the perturbed outcome kernel $\\Gamma_{i}(z s)(\\cdot,\\cdot):\\mathbb{X}\\times\\mathbb{A}\\to\\mathbb{Y}$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Gamma_{i}(z s)\\left(\\,\\cdot\\!\\mid x,a\\right):=\\mathcal{N}\\left(\\mu_{i}(z s)(x,a),\\sigma^{2}(x,a)\\right),\\,\\forall(x,a)\\in\\mathbb{X}\\times\\mathbb{A}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, due to Le Cam's two-point lemma, the local minimax risk over the class $\\mathcal{C}_{\\delta}\\left(\\mathcal{T}^{*}\\right)$ canbe lower bounded by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{{\\mathcal M}_{n}\\left(\\mathcal C_{\\delta}\\left(\\mathbb Z^{*}\\right)\\right)\\geq\\displaystyle\\frac14\\left\\{1-\\mathrm{TV}\\left({\\mathbb P}_{\\left(\\Xi^{*},\\Gamma_{i}\\left(s\\right)\\right)}^{n},{\\mathbb P}_{\\left(\\Xi^{*},\\Gamma_{i}\\left(-s\\right)\\right)}^{n}\\right)\\right\\}}}\\\\ {{\\{\\tau\\left(\\Xi^{*},\\Gamma_{i}(s)\\right)-\\tau\\left(\\Xi^{*},\\Gamma_{i}(-s)\\right)\\}^{2}\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "provided that $\\Gamma_{i}(z s)\\in\\mathcal{N}_{\\delta}\\left(\\Gamma^{*}\\right)$ for $z\\in\\{\\pm1\\}$ ", "page_idx": 30}, {"type": "text", "text": "We frst upper bound the total vriation distance TV $\\left(\\mathbb{P}_{\\left(\\Xi^{\\ast},\\Gamma_{i}\\left(s\\right)\\right)}^{n},\\mathbb{P}_{\\left(\\Xi^{\\ast},\\Gamma_{i}\\left(-s\\right)\\right)}^{n}\\right)$ By employing the Pinsker-Csiszar-Kullback inequality, one has ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(s))}^{n},\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(-s))}^{n}\\right)\\le\\sqrt{\\frac{1}{2}\\mathrm{KL}\\left(\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(s))}^{n}\\right|\\bigg|\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(-s))}^{n}\\right)}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The KL-divergence ${\\mathrm{KL}}\\left(\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(s))}^{n}\\right)\\bigg|\\bigg|\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(-s))}^{n}\\bigg)$ can be computed as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{KL}\\left(\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(s))}^{n}\\right)\\Bigg|\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(\\cdot^{\\delta}))}^{n}\\Bigg)}\\\\ &{=\\mathbb{E}_{(\\Xi^{*},\\Gamma_{i}(s))}\\left[\\log\\frac{p_{(\\Xi^{*},\\Gamma_{i}(s))}^{n}\\left(\\mathbf{O}_{n}\\right)}{p_{(\\Xi^{*},\\Gamma_{i}(-s))}^{n}\\left(\\mathbf{O}_{n}\\right)}\\right]}\\\\ &{=\\mathbb{E}_{(\\Xi^{*},\\Gamma_{i}(s))}\\left[\\displaystyle\\sum_{i=1}^{n}\\log\\frac{\\xi^{*}\\left(X_{i}\\right)\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)\\gamma_{i}(s)\\left(Y_{i}\\big|X_{i},A_{i}\\right)}{\\xi^{*}\\left(X_{i}\\right)\\pi_{i}^{*}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)\\gamma_{i}(-s)\\left(Y_{i}\\big|X_{i},A_{i}\\right)}\\right]}\\\\ &{=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{(\\Xi^{*},\\Gamma_{i}(s))}\\left[\\log\\frac{\\gamma_{i}(s)\\left(Y_{i}\\big|X_{i},A_{i}\\right)}{\\gamma_{i}(-s)\\left(Y_{i}\\big|X_{i},A_{i}\\right)}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\log\\frac{\\gamma_{i}(s)\\,(y\\mid x,a)}{\\gamma_{i}(-s)\\,(y\\mid x,a)}}\\\\ &{=\\,-\\,\\frac{1}{2\\sigma^{2}(x,a)}\\left[\\{y-\\mu_{i}(s)(x,a)\\}^{2}-\\{y-\\mu_{i}(-s)(x,a)\\}^{2}\\right]}\\\\ &{=\\frac{s g(x,a)}{\\overline{{\\pi}}_{i}(x,a)}\\left\\{2y-\\mu_{i}(s)(x,a)-\\mu_{i}(-s)(x,a)\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "By utilizing the fact (105), one can obtain from the equation (104) that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{KL}\\left(\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(s))}^{n}\\right)\\Bigg[\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(-s))}^{n}\\right)}\\\\ &{}&{=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{(\\Xi^{*},\\Gamma_{i}(s))}\\left[\\mathbb{E}_{(\\Xi^{*},\\Gamma_{i}(s))}\\left[\\frac{s g\\left(X_{i},A_{i}\\right)}{\\overline{{\\pi}}_{i}\\left(X_{i},A_{i}\\right)}\\left\\{2Y_{i}-\\mu_{i}(s)\\left(X_{i},A_{i}\\right)-\\mu_{i}(-s)\\left(X_{i},A_{i}\\right)\\right\\}\\right]\\left(X_{i},A_{i},\\lambda_{i}\\right)\\right]}\\\\ &{}&{=\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{(\\Xi^{*},\\Gamma_{i}(s))}\\left[\\frac{s g\\left(X_{i},A_{i}\\right)}{\\overline{{\\pi}}_{i}\\left(X_{i},A_{i}\\right)}\\left\\{\\mu_{i}(s)\\left(X_{i},A_{i}\\right)-\\mu_{i}(-s)\\left(X_{i},A_{i}\\right)\\right\\}\\right]}\\\\ &{}&{=2s^{2}\\displaystyle\\sum_{i=1}^{n}\\mathbb{E}_{(\\Xi^{*},\\Gamma_{i}(s))}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\overline{{\\pi}}_{i}^{2}\\left(X_{i},A_{i}\\right)}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle=2s^{2}\\sum_{i=1}^{n}\\mathbb{E}_{Z^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\overline{{\\pi}}_{i}^{2}\\left(X_{i},A_{i}\\right)}\\right]}\\\\ &{\\displaystyle\\overset{\\mathrm{(a)}}{\\leq}2K^{2}s^{2}\\sum_{i=1}^{n}\\mathbb{E}_{Z^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]}\\\\ &{\\displaystyle=2K^{2}s^{2}n\\left\\Vert\\sigma\\right\\Vert_{\\left(n\\right)}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Where thsteafolwsby eaon5Iwepu = into the bound (106), it   \nfolowsthat $\\begin{array}{r}{\\mathrm{KL}\\left(\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(s))}^{n}\\right|\\Big|\\mathbb{P}_{(\\Xi^{*},\\Gamma_{i}(-s))}^{n}\\right)\\leq\\frac{1}{2}}\\end{array}$   \nthe basic inequality (103), we arrive at ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\mathrm{TV}}\\left(\\mathbb{P}_{(\\Xi^{\\ast},\\Gamma_{i}(s))}^{n},\\mathbb{P}_{(\\Xi^{\\ast},\\Gamma_{i}(-s))}^{n}\\right)\\leq\\frac{1}{2}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "At this point, we should note for every $(i,z,x,a)\\in[n]\\times\\{\\pm1\\}\\times\\mathbb{X}\\times\\mathbb{A}$ that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mu^{*}(x,a)-\\mu_{i}(s z)(x,a)\\right|=\\frac{s\\left|g\\left(x,a\\right)\\right|\\sigma^{2}\\left(x,a\\right)}{\\overline{{\\pi}}_{i}\\left(x,a\\right)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1}{2\\sqrt{K}}\\cdot\\frac{\\left|g\\left(x,a\\right)\\right|\\sigma^{2}\\left(x,a\\right)}{\\sqrt{n}\\overline{{\\pi}}_{i}\\left(x,a\\right)\\left\\|\\sigma\\right\\|_{\\left(n\\right)}}}\\\\ &{\\qquad\\qquad\\qquad\\stackrel{\\mathrm{(b)}}{\\leq}\\frac{\\delta\\left(x,a\\right)}{2\\sqrt{K}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\stackrel{\\mathrm{(c)}}{\\leq}\\delta(x,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the step (b) holds due to Assumption 4, and the step (c) utilizes the fact that $K\\geq1$ ,which establishesthat $\\Gamma_{i}(z s)\\in\\mathcal{N}_{\\delta}\\left(\\Gamma^{\\ast}\\right)$ for $z\\in\\{\\pm1\\}$ and thus the local minimax lower bound (102) turns out to bevalid. ", "page_idx": 31}, {"type": "text", "text": "Next, we aim at establishing a lower bound on the gap between the functional values $\\tau\\left(\\Xi^{*},\\Gamma_{i}(s)\\right)$ and $\\tau\\left(\\Xi^{*},\\Gamma_{i}\\big(-s\\big)\\right)$ . One can observe that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\tau(\\Xi^{*},\\Gamma_{i}(s))-\\tau(\\Xi^{*},\\Gamma_{i}(-s))}\\\\ &{=\\mathbb{E}_{X\\sim\\Xi^{*}}\\left[\\langle g(X,\\cdot),\\mu_{i}(s)(X,\\cdot)-\\mu_{i}(-s)(X,\\cdot)\\rangle_{\\lambda_{h}}\\right]}\\\\ &{=2s\\cdot\\mathbb{E}_{Z^{*}}\\left[\\int_{\\Lambda}\\frac{g^{2}\\big(X_{i},a\\big)\\sigma^{2}\\big(X_{i},a\\big)}{\\overline{\\pi}_{i}\\big(X_{i},a\\big)}\\mathrm{d}\\lambda_{h}(a)\\right]}\\\\ &{\\stackrel{(a)}{\\geq}\\frac{2s}{K}\\cdot\\mathbb{E}_{Z^{*}}\\left[\\int_{\\Lambda}\\frac{g^{2}\\big(X_{i},a\\big)\\sigma^{2}\\big(X_{i},a\\big)}{\\pi_{i}^{*}\\big(X_{i},0_{i-1};a\\big)}\\mathrm{d}\\lambda_{h}(a)\\right]}\\\\ &{=\\frac{2s}{K}\\cdot\\mathbb{E}_{Z^{*}}\\left[\\frac{g^{2}\\big(X_{i},A_{i}\\big)\\sigma^{2}\\big(X_{i},A_{i}\\big)}{\\big(\\pi_{i}^{*}\\big)^{2}\\big(X_{i},0_{i-1};A_{i}\\big)}\\right]}\\\\ &{=\\frac{1}{K^{2}\\sqrt{n}}\\|\\sigma\\|_{(n)}\\mathbb{E}_{Z^{*}}\\left[\\frac{g^{2}\\big(X_{i},A_{i}\\big)\\sigma^{2}\\big(X_{i},A_{i}\\big)}{\\big(\\pi_{i}^{*}\\big)^{2}\\big(X_{i},0_{i-1};A_{i}\\big)}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the step (d) holds due to the assumption (25). By taking three pieces (102), (107), and (109) collectively,wehave ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{M}_{n}\\left(\\mathcal{C}_{\\delta}\\left(\\mathcal{Z}^{*}\\right)\\right)\\geq\\frac{1}{8K^{4}n\\left\\Vert\\sigma\\right\\Vert_{\\left(n\\right)}^{2}}\\left(\\mathbb{E}_{Z^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\pi_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]\\right)^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "forevery $i\\in[n]$ . Hence, one can conclude by taking an average of the local minimax lower bound (110) over $i\\in[n]$ that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{M_{n}\\left(C_{\\delta}\\left(\\mathcal{Z}^{*}\\right)\\right)=\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}M_{n}\\left(C_{\\delta}\\left(\\mathcal{Z}^{*}\\right)\\right)}\\\\ &{\\geq\\displaystyle\\frac{1}{8K^{4}n^{2}\\left\\Vert\\sigma\\right\\Vert_{(n)}^{2}}\\sum_{i=1}^{n}\\left(\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\sigma_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]\\right)^{2}}\\\\ &{\\overset{\\textcircled{(e)}}{\\geq}\\displaystyle\\frac{1}{8K^{4}n^{3}\\left\\Vert\\sigma\\right\\Vert_{(n)}^{2}}\\left(\\sum_{i=1}^{n}\\mathbb{E}_{\\mathcal{T}^{*}}\\left[\\frac{g^{2}\\left(X_{i},A_{i}\\right)\\sigma^{2}\\left(X_{i},A_{i}\\right)}{\\left(\\sigma_{i}^{*}\\right)^{2}\\left(X_{i},\\mathbf{O}_{i-1};A_{i}\\right)}\\right]\\right)^{2}}\\\\ &{=\\displaystyle\\frac{1}{8K^{4}}\\cdot\\frac{\\left\\Vert\\sigma\\right\\Vert_{(n)}^{2}}{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the step (e) makes use of the Cauchy-Schwarz inequality. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 33}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 33}, {"type": "text", "text": "\u00b7 [NA]  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 33}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 33}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 33}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. \u00b7 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The abstract and introduction provide a good summary of our contributions. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The main limitation lies in the lower bounds where we assume the existence of a sequence of Markov policies that are close enough to the history-dependent behavioral policies. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational effciency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We provide rigorous analysis of both the upper and lower bounds in our paper. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We don't have experimental results in this paper. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPs does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: We don't have experimental results in this paper ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips .cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 35}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We don't have experimental results in this paper. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We don't have experimental results in this paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We don't have experimental results in this paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 36}, {"type": "text", "text": "\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: We don't see any violations. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper is mostly theoretical, and is not tied to a particular application. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We give full credit to the prior work. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]