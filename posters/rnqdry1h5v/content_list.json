[{"type": "text", "text": "B\u2019MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Luca Zancato\u2217 Arjun Seshadri Yonatan Dukler Aditya Golatkar Yantao Shen ", "page_idx": 0}, {"type": "text", "text": "Benjamin Bowman Matthew Trager Alessandro Achille Stefano Soatto ", "page_idx": 0}, {"type": "text", "text": "AWS AI Labs ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\u201ccontext\u201d in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B\u2019MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access shortterm eidetic memory \u201cin-context,\u201d permanent structural memory \u201cin-weights,\u201d fading memory \u201cin-state,\u201d and long-term eidetic memory \u201cin-storage\u201d by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B\u2019MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B\u2019MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B\u2019MOJO achieves perplexity comparable to similarlysized Transformers and SSMs up to 1.4B parameters, while being up to $10\\%$ faster to train. Finally, we test whether models trained inductively on a-priori bounded sequences (up to 8K tokens) can still perform transductive inference on sequences many-fold longer. B\u2019MOJO\u2019s ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In Machine Learning, data representations are parametric maps trained to re-present data either individually, by optimizing a reconstruction criterion, or collectively, by optimizing a classification criterion. A trained representation can be co-opted to map a previously unseen datum to a hypothesis, for instance a class label. Representation learning with deep neural networks has been at the core of recent progress in artificial intelligence (AI) during the past decade. This paper is instead concerned with data realizations, which are maps trained to realize, i.e., to make real, bring into existence, or generate data. Roughly speaking, realizations are \u201cgenerative representations,\u201d trained by optimizing a prediction criterion, that can be used as sequential decision or prediction maps, or as generative models for sequence data. Large language models (LLMs) and other predictive models that involve randomness in the sequential generation process are special cases of stochastic realizations [29]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Representations are the backbone of inference from inductive learning, or induction for short. Induction refers to the process of mapping properties of a particular set of training data, through the parameters of the learned map, to properties of general test data. Successful induction, leading to generalization, hinges on an assumption of stationarity, namely the existence of some unknown distribution from which both past (training) data and present (inference) data are independently and identically drawn (IID). While uniform generalization bounds provide provable guarantees for any distribution, they do so under the assumption that the distribution exists and is the same for training and testing. This assumption is routinely violated in practice (e.g., language, climate, and business data), as manifest in so-called \u201cout-of-distribution\u201d effects or \u201cdistribution shift.\u201d These lead to apparent paradoxes involving generalization and memorization [57]. ", "page_idx": 1}, {"type": "text", "text": "Realizations, on the other hand, are the backbone of transductive inference and generative AI (GenAI). Transduction refers to the process of inferring particular properties of test data by processing, at inference time, all given (particular) training data.2 The boundary between induction and transduction is blurry: Induction can be viewed as a restricted form of transduction, where training data is accessible only through the learned weights. An over-parametrized representation, when overfit to training data, could in principle store the training set in the weights, thus making induction functionally identical to transduction. However, optimal induction aims to foster generalization by using various forms of regularization to prevent memorization, leading to sub-optimal transduction. Another form of sub-optimal transductive inference is termed \u201cin-context learning\u201d \u2013 which notably involves no learning if by learning one means to \u201cimprove by experience:\u201d The same in-context task, presented multiple times, requires identical effort and leads to no improvement. In-context learning can be optimal transduction only if all the data of interest fits in the context (including the entire training set), and even then it has been proven optimal for Transformers only for simple tasks such as linear classification. In summary, memorization and specific inference computation at the core of transduction are in contrast with the biases fostered by inductive learning: Whereas inductive inference seeks to minimize memorization to avoid overftiting and to foster generalization to unseen data, transductive inference seeks to maximize memorization and forgo generalization in favor of sample-specific inference computation.3 ", "page_idx": 1}, {"type": "text", "text": "Transduction does not require the train and test distribution to be the same. Indeed, the joint distribution from which both present and past data could have been jointly drawn can change with every sample. Therefore, optimality is not measured relative to one unknown distribution, as in generalization bounds, but rather relative to all possible distributions. If the data is generated by a physically realizable process, such distributions are computable. Optimal inference measured on average over all possible computable distributions, weighted by the Universal Prior, has been described by Solomonoff [42]. Solomonoff-style inference can be thought of as the limit of transduction, and similarly involves no learning. Instead, it consists of cycling through all computable programs using a Universal Turing Machine, which requires infinite time, memory, and compute resources, which in turn renders such inference unattainable. ", "page_idx": 1}, {"type": "text", "text": "Nonetheless, this Solomonoff limit points to two directions for improving inference: (a) efficient memorization, ideally by losslessly encoding all past data, and (b) efficient test-time computation through hardware and model co-design. Ideally, the resulting realizations would be such that, if memory and compute resources were extrapolated to infinity, inference would approach the Solomonoff limit. In reality, inference is always resource bound, so if something has to grow it would have to be external to the core inference engine. Accordingly, our goal in this work is to design and analyze families of architectures that (a) natively incorporate retrieval from a growing external memory (retrieval-augmented generation, or RAG), and (b) can scale to perform efficient inference. ", "page_idx": 1}, {"type": "text", "text": "In order to design families of architectures that efficiently memorize and scale inference computation, we must exploit the structure of the data they realize. When past and present data are known to be independent and identically distributed (IID) one can encode inference computation through a fixed \u201cstateless\u201d map, or model, which is a representation that can be trained inductively regardless of the inference query. When past and future data are not IID but generated by a Markov process of known order, there exist finite-dimensional statistics, called states, that summarize all past data for the purpose of prediction [26, 23, 29]. But even if the underlying process is Markovian of bounded order, unless such an order is known a-priori optimal realization is generally not possible with constant complexity [33]. To perform optimal inference, memory has to grow, and if the data generation mechanism has finite complexity at some point an efficient encoding of past data into memory will stop growing, but it is not possible to know when [1]. Therefore, a suitable architecture has to always allow the possibility of adding new storage. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In the seventies, Stochastic Realization Theory [11] studied State Space Models (SSMs) under the known-order Markov assumption, since realizations with growing memory were unmanageable then. 4 Today, Foundation Models can ingest a large portion of the growing volume of data accessible through the Internet and make it available for inference. Current AI systems are typically hardconstrained by inference time and compute resources, but not by storage\u2014one can always add more disks. To extend Stochastic Realization beyond the IID or known-Markov cases, Foundation Models need scalable architectures that comprise short-term memory updated synchronously within the given computational constraints and long-term memory updated and accessed sparingly and asynchronously. The former includes both eidetic (lossless) and fading (lossy) memory for efficient computation, while the latter is akin to an integrated form of \u201cretrieval-augmented\u201d inference. Such architectures would seamlessly manage short-term eidetic memory \u201cin-context\u201d, fading memory \u201cin-state\u201d, long-term structural memory \u201cin-weights\u201d and long-term eidetic memory \u2018in-storage\u2019 [13]. ", "page_idx": 2}, {"type": "text", "text": "Existing architectures such as Transformers and SSMs fall short of encompassing these criteria. Transformer-based architectures use eidetic memory restricted to a finite span, \u201ccontext length\u201d [6, 24], while recent SSM-based architectures [19, 16, 51] use only fading memory in their state. In both cases, scaling requires allowing the context (and the key-value cache) or recurrent state to grow unbounded. Recent work on hybrid combinations of Transformer and State Space layers [28, 39, 10] show promise in striking a balance between eidetic memory, fading memory and compute. ", "page_idx": 2}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this work, we describe a class of models that encompasses both recent SSMs [19], Transformers [24], and hybrid architectures [28, 10] as special cases, which we call B\u2019MOJO. This model family simultaneously renders the high expressivity and recall of Transformers, and the high compute efficiency of SSMs. And, rather than assigning tokens to the attention mechanism by recency, an asynchronous selection mechanism assigns tokens based on unpredictability. That is, whenever the model processes a new token that cannot be well-explained it will append it to the registers that implement B\u2019MOJO\u2019s eidetic memory\u2014a process we call Innovation Selection. ", "page_idx": 2}, {"type": "text", "text": "We demonstrate through synthetic tasks that B\u2019MOJO outperforms existing SSM and hybrid model architectures in transductive inference. Empirically, we show that our implementation is $15\\%$ faster than similarly-sized Transformers and SSMs while achieving comparable perplexity on language model tasks up to the 1.4B scale. Finally, we show that B\u2019MOJO can operate effectively at inference time on sequences far longer (tested up to $4\\times1$ ) than those used for training. Specifically, experiments with a B\u2019MOJO architecture trained with sequences of at most 8K tokens show consistent length generalization on test sequences of 32K tokens. ", "page_idx": 2}, {"type": "text", "text": "2 Background and Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We start with a general overview of models for sequence data, whether of text tokens (Large Language Models), images (e.g., Vision Language Models), video, or other physical sensory data (World Models). Any predictive model inferred from a sequence is called a realization.5 If one assumes that there exists one \u201ctrue\u201d model, coming from a known model class, then the problem of realization reduces to System Identification [31], which is to estimate the true model parameters from data. If the model is linear and driven by Gaussian noise, the Cayley-Hamilton theorem [25] implies that the model\u2019s \u201ctrue\u201d order can be inferred with a variety of model selection criteria, including greedy selection. Otherwise, the rank of the non-linear analog of the observability matrix, the Observability Codistribution, can grow in fits and starts, making greedy model selection undecidable [22]. If not only the model class, but the model itself are known, then the problem further reduces to flitering or prediction, which for the linear-Gaussian case can be implemented as a closed-form iterative update [26]. When building a (generally non-unique) realization, all one is given is the data, which leaves complete freedom of choice of model class and order, or number of free parameters. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Stochastic Realization. A \u201csequence model\u201d is a mathematical representation of sequential data capable of predicting the next datum in the sequence given previous ones. The Wiener Filter [48] was among the earliest to be deployed, superseded by the Kalman Filter [26], the first State Space Model (SSM) with an explicit \u201cstate\u201d updated recursively. The ensuing decades saw extensions to more general models leading to Stochastic Realization Theory [29]. The general problem of stochastic realization is, given a (potentially infinite) sequence $\\left\\{\\dots,u_{t-1},u_{t}\\right\\}\\doteq u_{\\le t}$ observed up to time $t$ infer (i.e., \u201clearn\u201d) some parametric model, a function $\\phi$ with parameters $\\theta$ , $\\phi_{\\theta}(u_{\\le t})$ such that the prediction $\\hat{u}_{t+1}=\\phi_{\\theta}\\big(u_{\\le t}\\big)$ yields a residual $\\epsilon_{t+1}=u_{t+1}-\\hat{u}_{t+1}$ (\u201cinnovation process\u201d) that is as close as possible to independent and identically distributed samples (IID). In a nutshell, an optimal predictor is one that makes the prediction error unpredictable. ", "page_idx": 3}, {"type": "text", "text": "State. The state of a model is $a$ statistic that makes the future independent of the past. In particular, such a statistic (function of past data) $\\xi(u_{\\leq t})$ yields to the following conditional entropy equality $H(u_{t+1}|\\xi(u_{\\le t}),u_{\\le t})\\,=\\,H\\bar{(}u_{t+1}|\\xi(u_{\\le t}))$ relative to the joint distribution of all past and present data [31, 30]. Trivially, the data itself fits the definition of state with $\\xi(u_{\\le t})=u_{\\le t}$ , but it grows unbounded over time. Instead, one seeks a bounded complexity state given which all past data can be ignored with no loss of information. Building such a state is the core goal of stochastic realization. ", "page_idx": 3}, {"type": "text", "text": "Elementary model classes: LTI, LTV and LIV. Any finite sequence can be realized by a linear time-invariant (LTI) system driven by IID Gaussian noise [29]. It would therefore appear that this model class is sufficient for any practical purpose, and we need models no more expressive than linear time-invariant ones driven by white zero-mean Gaussian noise:6 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{x_{t+1}=A x_{t}+B u_{t}\\qquad}&{\\mathrm{LTI}}\\\\ {y_{t}=C x_{t}+v_{t}}\\end{array}\\right.\\qquad\\mathrm{LTI}\\qquad\\qquad\\left\\{\\begin{array}{l l}{x_{t+1}=A(u_{t})x_{t}+B(u_{t})u_{t}}&{}\\\\ {y_{t}=C(u_{t})x_{t}+v_{t}}&{}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Given observations $y_{0:t}$ , stochastic realization deals with the problem of inferring an equivalence class of model parameters $A,B,C$ [44] and a state $x_{t}$ along with a covariance matrix $P_{t}$ of $[u_{t},v_{t}]$ that can be propagated deterministically to approximate trajectories produced by the underlying data generation mechanism, $y_{0:t}$ [49]. However, arbitrarily long and complex time series would require a growing state, therefore making the model no longer time-invariant. More expressive model classes, such as time-varying [23] and input-varying [27] afford more efficient representation by considering $A_{t},B_{t},C_{t}$ known functions of time (Linear-Time Varying) or of their input (Linear-Input Varying, LIV). As we describe next, a special case of LIV model class where the dependency on the input is linear, resulting in a so-called bilinear realization, is gathering considerable attention [19, 10, 51]. ", "page_idx": 3}, {"type": "text", "text": "Modern realizations. Input-dependent (bilinear) models first gained popularity half a century ago starting with Brockett [5], when [27] showed that \u201cevery nonlinear system with controls entering linearly is locally almost bilinear\u201d; [11] developed a complete realization theory for this class of systems including minimal realizations, and showed that they not only minimize the dimension of the state, but also minimize the number of operations necessary for the update. Most recently, special cases of bilinear realizations are being used as building blocks in stacked architectures like Mamba [19, 46], Jamba [28] and Griffin [10]; they refer to input-dependency as \u201cselectivity\u201d [19, 20, 37] and combine Attention mechanisms and other techniques to scale up to 52B parameters. Similarly, but at a smaller scale, Block State Transformers [14], GSS [34] and H3 [16] consider using State Space Models to contextualize information that is later aggregated with Attention. All these models differ by their choices of $A,B,C$ , each with its own advantages and limitations which we describe next. ", "page_idx": 3}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/45d41b2ef5f33ae279d06b49fed05d5e0ef1e62f824452da2de0acc6efc51a11.jpg", "img_caption": ["Figure 1: B\u2019MOJO\u2019s memory management. (Left) Illustration of the B\u2019MOJO layer. (Right) B\u2019MOJO\u2019s Realization. B\u2019MOJO\u2019s fading memory is computed by a SSM that represents longrange dependencies through its state (a fixed-dimensional representation) which is later aggregated along with with the most recent past. B\u2019MOJO\u2019s eidetic memory stores tokens selected from the past using an innovation test on the SSM\u2019s state and appends them to the current sliding window. The innovation test measures how difficult it is to predict the next token using the SSM\u2019s state. If a token is difficult to predict, we store it in the eidetic memory and pass it to the attention module together with the state, a compressed summary of the past, and the most recent tokens. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "3 B\u2019MOJO ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We now introduce B\u2019MOJO, a general family of architectures based on a stackable module, designed to foster transductive inference. We represent fading memory using the state of a dynamical model whose size is fixed a-priori based on hardware constraints. Since the state of a dynamical model is a lossy memory of past data, we implement a complementary eidetic memory with shifting registers that directly encode past data as new information is processed. Although adding storage is simple and cheap, peering back into the growing history with every new query is not. Approximate retrieval [35, 52] also grows as more tokens are processed, potentially sub-linearly if optimized off-line. We propose to access information arbitrarily far in the past at a fixed compute cost by keeping only the most unpredictable tokens according to an innovation test. While we respect hard constraints on the amount of memory processed at inference-time, we impose no constraint on its time span. ", "page_idx": 4}, {"type": "text", "text": "3.1 Fading Attention ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by describing the elementary components of B\u2019MOJO, which we show to encompass existing models. We first show that the Attention mechanism implements a non-linear nil-potent system whose state grows as more samples are processed. Much like a Moving-Average (MA) system with non-linear read-out, the attention state is required to increase as its span is enlarged. Then, we show that Mamba [19] has a fixed-dimensional Auto-Regressive (AR) (fading) state and hence cannot perform exact recall. Finally, we describe a model that modulates the two forms of memory, B\u2019MOJO-Fading (B\u2019MOJO-F), effectively realizing a non-linear ARMA model [31, 54, 55]. ", "page_idx": 4}, {"type": "text", "text": "Transformers use Attention to map input to output sequences according to $\\begin{array}{r}{y_{t}=\\frac{\\sum_{i=1}^{t}\\exp(q_{t}k_{i}^{T})v_{i}}{\\sum_{i=1}^{t}\\exp(q_{t}k_{i}^{T})}}\\end{array}$ where $q_{t},k_{t},v_{t}$ are the query, key and value vectors and are all computed directly from the input tokens $u_{t}$ . We write this equation as a nil-potent dynamical system with a softmax read-out function $\\rho$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nx_{t+1}=A(u_{t})x_{t}+B(u_{t});\\qquad y_{t}=\\rho(u_{t},x_{t})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\nA(u_{t})=A_{\\mathrm{ATT}}=\\left[\\begin{array}{l l l l}{0}&{I}&&\\\\ &&{\\ddots}&\\\\ &&&{I}\\\\ &&&{0}\\end{array}\\right]\\in\\mathbb{R}^{2V N\\times2V N}\\quad\\mathrm{and}\\quad B(u_{t})=b_{\\mathrm{ATT}}(u_{t})=\\left[\\begin{array}{l}{0}\\\\ {\\vdots}\\\\ {0}\\\\ {b(u_{t})}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with $b(u_{t}):=\\left[\\!\\!\\begin{array}{l}{k_{t}\\right]\\in\\mathbb{R}^{2V}}\\\\ {v_{t}\\mathrm{~}}\\end{array}$ , where $2V$ is the embedding dimension of the KV cache and $N$ is the length of the Attention window. A Transformer has only short-term eidetic memory that is deadbeat in $N$ steps: what slides \u201cout of context\u201d is permanently removed. ", "page_idx": 4}, {"type": "text", "text": "Mamba is complementary in that it only has fading memory [19] with decoupled (diagonal) dynamics. Specifically, ", "page_idx": 5}, {"type": "equation", "text": "$$\nA(u_{t})=A_{\\mathrm{Mamba}}(u_{t})=\\left[\\begin{array}{c c c}{a_{1}(u_{t})}&&&\\\\ &{\\ddots}&\\\\ &&{a_{N}(u_{t})}\\end{array}\\right],\\quad b(u_{t})=b_{\\mathrm{Mamba}}(u_{t})=\\left[\\begin{array}{c}{b_{1}(u_{t})}\\\\ {\\vdots}\\\\ {b_{N}(u_{t})}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $N$ is picked a-priori and the output is $y_{t}=C(u_{t})x_{t}+D u_{t}$ . A Mamba model is obtained by stacking diagonal Mamba layers, hence insufficient to realize even a simple oscillator without resorting to (numerically slow) complex algebra. A marginally stable pole $[a_{i}=1]$ ) can be used as permanent memory but at an unbounded cost of encumbering the state and re-processing it at each time step. [19] emphasizes the use of \u201cselective state space,\u201d which is simply a bilinear realization [12, 11]. However, crucial to its implementation are interleaved convolutional layers that retrieve some of the eidetic functionality lost with the diagonal dynamics. In Sect. E we derive in detail the form above from the description of the paper, forgoing the unnecessary continuous time narrative, and comment on the model in relation to the actual implementation. ", "page_idx": 5}, {"type": "text", "text": "B\u2019MOJO-F bypasses the limitations of Transformer and Mamba layers by using a Controllable Canonical Form [25] to realize Equation (1) (see appendix C), that is: ", "page_idx": 5}, {"type": "equation", "text": "$$\nA_{\\mathrm{B^{\\prime}M o l o}}(u_{t})=\\left[\\begin{array}{c c c c}{{0}}&{{I}}&{{}}&{{}}\\\\ {{}}&{{}}&{{\\ddots}}&{{}}\\\\ {{}}&{{}}&{{}}&{{I}}\\\\ {{a_{1}(u_{t})}}&{{a_{2}(u_{t})}}&{{\\ldots}}&{{a_{N}(u_{t})}}\\end{array}\\right],\\quad b_{\\mathrm{B^{\\prime}M o l o}}(u_{t})=\\left[\\begin{array}{c}{{0}}\\\\ {{\\vdots}}\\\\ {{0}}\\\\ {{b(u_{t})}}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Appendix C we show that B\u2019MOJO-F is a minimal realization and, when the state dimension is fixed, it generalizes both Mamba and Attention modules. B\u2019MOJO-F uses a fixed computational budget to capture long-range dependencies far beyond the span of Attention thanks to the last rows in the state transition matrix. Specifically, the order $N$ fixes the number of tokens (the most recent ones) that are processed by a local attention window $\\rho$ thanks to the upper diagonal identities in $A_{\\mathrm{B^{\\prime}M O J O}}(u_{t})$ . On the other hand, the last rows of $A_{\\mathrm{B^{\\prime}M O J O}}(u_{t})$ aggregate information from the most recent past data, akin to \u201cattention sinks\u201d [50]. ", "page_idx": 5}, {"type": "text", "text": "3.2 B\u2019MOJO\u2019s complete functional form ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "While B\u2019MOJO-F generalizes both Transformers and Mamba models, it can only access information outside the current window with lossy fading memory. Data that becomes relevant only after a long time span would be ignored. We therefore modify our model class to detect and incorporate such tokens into the span of the local attention eidetically. We do so with a simple method that we call Innovation Selection: whenever B\u2019MOJO processes a new token that cannot be explained using the lossy fading memory, we affix it to the eidetic memory. Innovation Selection operates similarly to the mechanism behind the Lempel-Ziv-Welch (LZW) compression algorithm [58, 47].7 Like with fading memory, the tokens in the new eidetic memory are processed by the same sliding window attention we use in B\u2019MOJO-F. Algorithm 1 captures the steps we perform from input to output of each layer of B\u2019MOJO architecture, which we discuss below. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1: The B\u2019MOJO Mechanism   \nData: Input data $u_{t}$ , inner recurrent state $x_{t-1}$ , fading output $y_{t-1}$ , eidetic memory $M_{t-1}$ , window size $w$ , B\u2019MOJO\u2019s state update matrix $A(u_{t})$ , input to state vector $B(u_{t})$ , state to output vector $C(u_{t})$ , a predictor function ${\\hat{y}}(\\cdot)$ over a span of length $k$ . Result: Output $\\mathrm{out}_{t}$   \n$\\begin{array}{r l r l}&{u_{t-w:t}\\leftarrow u_{t-w-1:t-1}^{\\star}\\cup u_{t}\\;;\\;\\;}&&{\\mathrm{//~Short-term~\\mathtt{memory~\\mathtt{~window}~}}}\\\\ &{x_{t}\\leftarrow A(u_{t})x_{t-1}+b(u_{t})\\;;\\;\\;}&&{\\mathrm{//~Long-term~\\mathtt{fading~memory}~}}\\\\ &{\\epsilon_{t}\\leftarrow\\mathrm{error}(\\hat{y}(y_{t-1:t-k}),y_{t})\\;;\\;}&&{\\mathrm{//~7~Innovation~computation~}}\\\\ &{M_{t}\\leftarrow\\left\\{M_{t-1}\\cup\\left\\{u_{t},\\epsilon_{t}\\right\\}\\quad\\mathrm{if}\\;\\epsilon_{t}>\\operatorname*{min}_{\\epsilon\\in M_{t-1}}(\\epsilon)\\}}&&{\\mathrm{//~Long-term~\\mathtt{eidetic~}\\pi e m o r y~}}\\\\ &{\\mathrm{out}_{t}\\leftarrow\\mathrm{Attn}(u_{t-w:t},y_{t-w},M_{t});\\;}&&{\\mathrm{//~Model~\\mathtt{output}~}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "7In brief, LZW scans an input sequence to find the shortest sequence that is currently unknown, appends this sequence to a dictionary while returning indices for known sub-sequences. Similarly, Innovation Selection scans the input sequence to find the shortest sequence that has high prediction error, adds to the eidetic memory unpredictable inputs, while returning outputs that leverage known sub-sequences. ", "page_idx": 5}, {"type": "text", "text": "In Algorithm 1, the first line updates the short-term memory (collection of the last $w$ seen tokens) by ejecting the oldest token and appending the new token. The second line updates the SSM fading memory, which stores new information into the state $x_{t}$ . Then, we augment fading memory with our eidetic memory $M_{t}$ , a set of tokens that the model decides to keep based on their unpredictability: tokens that are difficult to predict given the past, as measured by $\\begin{array}{r}{\\mathrm{error}(\\hat{y}_{t}(y_{t-1:t-k}),y_{t})}\\end{array}$ , could be valuable far in the future when their memory has faded away, and are hence curated in the eidetic memory. The amount of information stored in $M_{t}$ can grow unbounded over time up to hardware limitations, in Section 3.3 we further discuss how to efficiently implement the predictor $\\hat{y}$ . ", "page_idx": 6}, {"type": "text", "text": "Finally, we weave fading and eidetic memory together to predict a new token through a sliding attention mechanism, which aggregates relevant information from short-term memory $u_{t-w:t}$ , the fading memory $y_{t}$ and the long term eidetic memory $M_{t}$ . ", "page_idx": 6}, {"type": "text", "text": "Differences between B\u2019MOJO, B\u2019MOJO-F and vanilla Hybrid models. Differently from vanilla hybrid models that stack SSM and Attention layers, B\u2019MOJO-F and B\u2019MOJO allows the Attention module to attend to both the input and output tokens of the SSM, allowing the Attention layer to merge information from fading \u201cmemory\u201d tokens (output of the SSM) with \u201ceidetic\u201d tokens (the layers\u2019 inputs). Differently from B\u2019MOJO, B\u2019MOJO-F does not use the innovation selection mechanism and therefore it does not implement long term eidetic memory. ", "page_idx": 6}, {"type": "text", "text": "3.3 B\u2019MOJO\u2019s efficient implementation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We efficiently implement B\u2019MOJO\u2019s tiered memory hierarchy at scale. The modelling choices that lead to B\u2019MOJO closely follow ideas from Stochastic Realization and have an obvious recurrent efficient implementation. On the other hand, during training, one is more interested in a parallel formulation that can allow processing of multiple tokens at the same time and in a single forward pass. In the following, we describe how we use chunking to develop B\u2019MOJO\u2019s parallel form. ", "page_idx": 6}, {"type": "text", "text": "Efficient sliding window and memory chunking. Computing fading memory and selecting the most unpredictable tokens to store before feeding them into an attention layer is a sequential process which is hard to parallelize. To solve this we use chunks of length $w$ and use fading and eidetic memory to summarize the whole past before the beginning of each chunk. Then, to aggregate information from the input and memory tokens, we use a sliding window over the interleaved concatenation of input chunks and their respective memory tokens as shown in Figure 7 (see Appendix B.2). Our modular and interleaved approach in Figure 7 enables us to leverage optimized kernels for both SSM [19] and sliding window attention [9], enabling fast training and inference beyond the 1B scale. In Figure 4 we show that our efficient implementation is faster than both Mamba [19] and Mistral [24]. ", "page_idx": 6}, {"type": "text", "text": "Efficient Innovation Selection. The Innovation Selection process we describe in Algorithm 1 requires the predictor $\\hat{y}(y_{t-1:t-k})$ . While building a new parametric predictor with learnable parameters is possible, in practice, this requires modifying the training loss. In our implementation, we consider a fixed predictor so no extra weights need to be learned. We fix $\\hat{y}$ as a running weighted average and implement it using short 1D grouped causal convolutions. ", "page_idx": 6}, {"type": "text", "text": "4 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To evaluate the scalability and efficiency of B\u2019MOJO, we compare it with state-of-the-art model classes (SSMs, Transformers and hybrid variants) on both synthetic and language modeling tasks. Our experimental results are of two main types, (i) language modeling scaling laws (Section 4.2) with zero-shot evaluation on short context text benchmarks (Section 4.3), and (ii) specific long context/recall-based tasks where finite-context models are ill-fit. We wish to emphasize that in this setting, and for the results of type (i), Transformers are a paragon, not a baseline, since most tasks are answerable within the context. Therefore in the results of type (ii) we leverage specific benchmarks, like synthetic tasks (Section 4.1), long context evaluation and length generalization to assess our models (Section 4.3). The goal of our novel model class is to cover the entire spectrum, i.e. perform comparably to the paragon on finite contexts while preserving higher performance than Transformers whenever the data relevant to solve the inference tasks fall outside the context window. ", "page_idx": 6}, {"type": "text", "text": "All experiments compare against three baselines: (1) Transformers, represented by a downscaled Mistral-7B [24] architecture re-trained from scratch (2) SSMs, represented by Mamba and (3) Hybrid models, implemented by stacking Mamba with a sliding window attention [10, 28]. For a fair apples-to-apples comparison all our models are trained from scratch using the same pre-training data, tokenizer [24], and context length. And, in order to abalte the contribution of eidetic memory, we consider B\u2019MOJO-F in addition to B\u2019MOJO in all our experiments. ", "page_idx": 6}, {"type": "table", "img_path": "RnQdRY1h5v/tmp/7b5be331c118bdc746cba829200fc4e06da922ab070f9ccefc042113357a2072.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/e770b576e6386243e318661031aee825d650e47827243d015f92219e3703d58b.jpg", "img_caption": ["Figure 2: (Panels 1-3) B\u2019MOJO has high memory efficiency on Associative Recall Tasks (sequence length is 256 and attention window 32). For various models, we plot accuracy on the Multi-Query Associative Recall (MQAR) task as a function of the model dimension (totaling the SSM state, eidetic memory and KV cache where applicable). The transformer paragon attains $100\\%$ accuracy because it operates on the full context. While all models benefit strongly from increased memory, B\u2019MOJO and B\u2019MOJO-F consistently achieve the best accuracies for a given memory budget. Panels 1-3 report MQAR tasks of increasing difficulty, on which the performance gap between B\u2019MOJO and other models increases, showcasing the value of eidetic memory. (Panel 4) Increases in eidetic memory size corresponds to gains in recall. We ablate the effects of eidetic memory by growing the number of eidetic memory tokens in B\u2019MOJO. Each added token contributes to an increase in recall accuracy until performance is saturated. ", "Figure 3: B\u2019MOJO language modeling scaling laws. We plot the perplexity reached by models at different scales against the number of parameters and the wall-clock training time. B\u2019MOJO is faster than Mamba and Mistral at training time while achieving better perplexity than Mamba and comparable perplexity with Mistral. The plot also exhibits a non-saturating scaling law, showing that increasing the amount of resources leads to increasingly better B\u2019MOJO models. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.1 Synthetic Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We use synthetic tasks to test B\u2019MOJO\u2019s ability to recall exact information from beyond the attention span [2, 51]. We do so with Multi-Query Associative Recall (MQAR) data in the main text, and consider other tasks such as Induction Heads [36], Selective Copying [19], Fuzzy MQAR[39], and Noisy MQAR [39] in Appendix D.1. ", "page_idx": 7}, {"type": "text", "text": "B\u2019MOJO\u2019s Memory Efficiency on Associative Recall Tasks. The MQAR task [2] has been shown to correlate well with language modeling performance [2, 39]. Compared to its peers (e.g. Induction Heads [36]) MQAR is considerably more difficult and requires strong recall capabilities. In Figure 2, we display accuracy on MQAR as we vary the size of the recurrent state for 2-layer instances of our models. Panels 1-3 consider varying numbers of key-value pairs to illustrate increasing complexity. Here, a Transformer (Mistral), with its sequence-length-sized KV cache serves as the paragon, always achieving a $100\\%$ accuracy. Should its window size be restricted not to include the KV pairs, its accuracy would drop to that of a random guess. Our results show that while every model class improves in recall accuracy as its size increases, B\u2019MOJO and B\u2019MOJO-F do so more reliably and faster than others. We explain these findings as follows. Mamba possesses only fading memory to propagate information to the future, and therefore has a limited recall capacity [2] for a given size of its recurrent state. Hybrid models leverage a sliding window attention to mitigate this issue, however they require a lengthy window to increase their span and recall the reference pairs. The strong performance of B\u2019MOJO-F showcases the value of fading memory over a simple hybrid configuration, and the even stronger performance of B\u2019MOJO, most evident in the panel on the right, highlights the added contribution of an eidetic memory for precise recall. ", "page_idx": 7}, {"type": "table", "img_path": "RnQdRY1h5v/tmp/a279e0b5f17d7a79eb0ce79409a20820d9890f2b4f4711600622b484db515509.jpg", "table_caption": ["Table 1: B\u2019MOJO\u2019s performance on downstream tasks. We compare different architectures on several zero-shot downstream tasks used to test common-sense reasoning and question-answering on relatively small contexts. These tasks, however, do not require strong recall capabilities because the input text is typically very short (results on longer contexts are reported in Table 2). On pretraining perplexity B\u2019MOJO performs on par with our pre-trained Mistral model and outperforms our pre-trained Mamba models at the largest scale we test 1.4B. However, on accuracy metrics, while B\u2019MOJO still outperforms Mamba, its gap with the Mistral model increases. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "4.2 Language Modeling Scaling laws ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We next demonstrate B\u2019MOJO\u2019s favourable scaling laws on mid-size language modeling, baselining against the same set of state-of-the art-model classes as the previous section. We report the training setting and hyper-parameters in Appendix B.1. ", "page_idx": 8}, {"type": "text", "text": "Results. In Figure 3, we report perplexity at different scales against (left) the number of parameters and (right) the wall-clock training time. B\u2019MOJO is faster than Mamba and Mistral that use efficient CUDA kernels (Flash attention and Selective Scan), outperforms our pre-trained Mamba baseline at all scales, and is comparable with our Mistral Transformer model, in Appendix D.2 we further comment our results in relation to other recent hybrid models like Griffin [10] and Zamba [18]. ", "page_idx": 8}, {"type": "text", "text": "We additionally report the scaling behavior of B\u2019MOJO-F as an ablation of the eidetic memory, as well as a vanilla hybrid architecture composed of interleaved SSM and attention layers as an ablation of both fading and eidetic memory. Despite using only short context sizes $(2\\mathbf{k})$ in Figure 3, our results show that adding fading memory strictly improves pre-training perplexity over the baseline hybrid model at all scales. Performance is further improved when eidetic memory is added despite incurring a slightly higher training time. Our results suggest that B\u2019MOJO exhibits a non saturating scaling law: increasing the amount of resources (parameters/FLOPs) leads to increasingly better models. ", "page_idx": 8}, {"type": "text", "text": "4.3 Zero Shot Evaluation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We catalog the performance of our pre-trained models on an assortment of short and long context zero-shot evaluation tasks. While perplexity captures the models\u2019 ability to predict language, these evaluations characterize their generalization capabilities to unseen tasks. We use the EleutherAI LLM Harness [17] to conduct all evaluations. ", "page_idx": 8}, {"type": "text", "text": "Short Context Evaluation. In Table 1 we report results on common-sense reasoning and questionanswering tasks that require processing both short [19] and medium-sized contexts [2, 35]. Since these language tasks do not require long range modeling we would not expect B\u2019MOJO to meaningfully outperform our baselines. Moreover, B\u2019MOJO uses a smaller sliding window (512 tokens) than Mistral (1024 tokens), placing the former on an uneven footing. Despite these caveats, we find that B\u2019MOJO still bests Mamba at the 1.4B scale and performs comparably to the Mistral model. ", "page_idx": 8}, {"type": "text", "text": "Long Context Evaluation. We next investigate the ability of B\u2019MOJO to process long contexts using the PG-19 dataset [17] and more recall-intensive natural language tasks using the SWDE, Scrolls [2, 41] benchmarks. Our results show that B\u2019MOJO outperforms Mamba and our hybrid baseline on PG-19 and SWDE, while B\u2019MOJO-F outperforms Mamba on the Scrolls datasets, in line with our previous findings on the synthetic tasks in Figure 2. These long context tasks showcase\u2014in a practical setting\u2014B\u2019MOJO\u2019s efficacy in recalling information from beyond the attention span. ", "page_idx": 8}, {"type": "table", "img_path": "", "table_caption": ["Table 2: Long range downstream tasks. We compare different architectures on several zero-shot long range recall-intensive downstream tasks [2]. Our B\u2019MOJO variants outperform Mamba since they have stronger recall capabilities. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/a996ad3d2e10832cef618eca9a0c2f0ec3a2c15e252b4e2d2f4302638bc16f08.jpg", "img_caption": ["Figure 4: Time in ms to process 2k sequences. B\u2019MOJO is faster than other efficient implementations of Mamba [19] and Transformers [19] at all scales. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/0393993b57378ee59f8d5ba24a12b8716663683c254ec65e1a7f32145b91dcea.jpg", "img_caption": ["Figure 5: Length generalization. (Left) We pre-train B\u2019MOJO 1.4B and Mamba 1.4B on $2\\mathbf{k}$ context lengths and a 1.4B Transformer baseline on 1k. (Right) We pre-train B\u2019MOJO 790M and Mamba 790M on $8\\mathbf{k}$ context length and compare models evaluating perplexity on longer sequences up to 32K tokens on PG-19. Transformers cannot length generalize (a known failure mode), on the other hand B\u2019MOJO preserves/improves in perplexity better than Mamba even on longer sequences. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/b5757cd916988e4d16f7f55fc02c60e6ba91e2eea52f9c69ad20f98541ea577a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "4.4 Length Generalization ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We evaluate the ability of B\u2019MOJO to improve its predictions with longer contexts than ones seen during training, an attribute termed length generalization [10, 51, 3]. Whereas length generalization in Transformers is limited by positional encodings and memory constraints [10], for SSMs and B\u2019MOJO, it is instead limited by the capacity of the recurrent state. In Figure 5 we report perplexity on PG-19 as the model processes contexts larger than pre-training contexts. We observe that B\u2019MOJO 1.4B and Mamba 1.4B are capable of reducing and maintaining lower perplexity levels than Mistral at long context sizes. Curiously, we find that on models trained on longer sequences (8k), length generalization still holds and allows the model to continuously reduce perplexity as more tokens are processed, up to $4\\times$ the pre-training sequence length. Additionally, the pre-training perplexity of B\u2019MOJO models trained on longer contexts is lower than identical ones trained on the same amount of tokens but on shorter contexts, showcasing that our model can properly leverage the eidetic and fading memory to process long sequences. In Appendix B.2, we report our implementation of backpropagation through time that we use to efficiently train our models on even longer contexts (beyond 16k). Our results on smaller scales (up to 370M) show that a model trained on 16k tokens can length generalize up to contexts of length 64k. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Although our experiments have been conducted up to a 1.4B scale, scaling B\u2019MOJO further requires non-trivial engineering and compute. Therefore, despite B\u2019MOJO\u2019s promising scaling laws, it is difficult to ascertain whether it could scale to even larger models and datasets, and do so competitively. Scaling our work to even larger models could result in positive societal benefits such as ease of access to information. However, these models could be used also to spread misinformation, so novel algorithms and research is important to improve controllability and reduce hallucination [15]. Despite our promising results on length generalization, we observed that Mamba checkpoints trained on more compute tend not to length generalize that well. Since B\u2019MOJO leverages a Mamba-like module to implement fading memory, we cannot exclude the possibility that it will be less effective in length generalization as we scale more. However, exploring simple time normalization techniques as mitigations is a promising area for future work [32]. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alessandro Achille and Stefano Soatto. On the learnability of physical concepts: Can a neural network understand what\u2019s real? arXiv preprint arXiv:2207.12186, 2022.   \n[2] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher R\u00e9. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024.   \n[3] Maximilian Beck, Korbinian P\u00f6ppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517, 2024.   \n[4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.   \n[5] R Brockett. On the algebraic structure of bilinear systems. theory and applica-tions of variable structure systems, 1972. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[7] Olivier Chapelle, Vladimir Vapnik, and Jason Weston. Transductive inference for estimating values of functions. Advances in Neural Information Processing Systems, 12, 1999.   \n[8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[9] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.   \n[10] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024.   \n[11] Paolo D\u2019Alessandro, Alberto Isidori, and Antonio Ruberti. Realization and structure theory of bilinear dynamical systems. SIAM Journal on Control, 12(3):517\u2013535, 1974.   \n[12] David LeRoy Elliott. Bilinear control systems: matrices in action, volume 169. Springer, 2009.   \n[13] B. Bowman et al. Inductive learning of transductive inference:. ArXiv [pending], 2021.   \n[14] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Joseph Pal, Orhan Firat, and Ross Goroshin. Block-state transformers. In Neural Information Processing Systems, 2023.   \n[15] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. arXiv preprint arXiv:2403.14003, 2024.   \n[16] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022.   \n[17] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPof,i Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.   \n[18] Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: A compact 7b ssm hybrid model. arXiv preprint arXiv:2405.16712, 2024.   \n[19] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[20] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022.   \n[21] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   \n[22] Alberto Isidori. Nonlinear control systems: an introduction. Springer, 1985.   \n[23] Andrew H Jazwinski. Stochastic processes and filtering theory. Courier Corporation, 2007.   \n[24] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[25] Thomas Kailath. Linear systems, volume 156. Prentice-Hall Englewood Cliffs, NJ, 1980.   \n[26] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.   \n[27] Arthur J Krener. Bilinear and nonlinear realizations of input-output maps. SIAM Journal on Control, 13(4):827\u2013834, 1975.   \n[28] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: A hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024.   \n[29] Anders Lindquist and Giorgio Picci. On the stochastic realization problem. SIAM Journal on Control and Optimization, 17(3):365\u2013389, 1979.   \n[30] Greta M Ljung and George EP Box. On a measure of lack of fti in time series models. Biometrika, 65(2):297\u2013303, 1978.   \n[31] Lennart Ljung. System identification toolbox: User\u2019s guide. Citeseer, 1995.   \n[32] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. arXiv preprint arXiv:2404.08801, 2024.   \n[33] Mireille Chaleyat Maurel and Dominique Michel. Results on the non existence of finite dimensional filters. Stochastics: An International Journal of Probability and Stochastic Processes, 13(1-2):83\u2013102, 1984.   \n[34] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022.   \n[35] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023.   \n[36] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-inductionheads/index.html.   \n[37] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670\u201326698. PMLR, 2023.   \n[38] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.   \n[39] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024.   \n[40] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[41] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.   \n[42] RJ Solmonoff. A formal theory of inductive inference. i. II Information and Control, 7:224\u2013254, 1964.   \n[43] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 9229\u20139248. PMLR, 2020.   \n[44] Peter Van Overschee and Bart De Moor. Subspace identification for linear systems: Theory\u2014Implementation\u2014Applications. Springer Science & Business Media, 2012.   \n[45] Vladimir Vapnik. Transductive inference and semi-supervised learning. 2006.   \n[46] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024.   \n[47] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8\u201319, 1984.   \n[48] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949.   \n[49] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681\u2013697, 1968.   \n[50] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023.   \n[51] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.   \n[52] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers. Advances in Neural Information Processing Systems, 36, 2024.   \n[53] Luca Zancato, Alessandro Achille, Tian Yu Liu, Matthew Trager, Pramuditha Perera, and Stefano Soatto. Train/test-time adaptation with retrieval. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15911\u201315921, 2023.   \n[54] Luca Zancato, Alessandro Achille, Giovanni Paolini, Alessandro Chiuso, and Stefano Soatto. Stric: Stacked residuals of interpretable components for time series anomaly detection. 2021.   \n[55] Luca Zancato and Alessandro Chiuso. A novel deep neural network architecture for non-linear system identification. IFAC-PapersOnLine, 54(7):186\u2013191, 2021. 19th IFAC Symposium on System Identification SYSID 2021.   \n[56] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   \n[57] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107\u2013 115, 2021.   \n[58] Jacob Ziv and Abraham Lempel. Compression of individual sequences via variable-rate coding. IEEE transactions on Information Theory, 24(5):530\u2013536, 1978. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Induction and Transduction ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Example A.1 (Biology). We note that biological agents have no option but to operate inductively, due to (a) hard memory bounds, and (b) evolutionary pressure towards minimizing inference latency: When faced with a threat, a biological agent is better served by a quick suboptimal decision than by reasoning over all past experience. AI built on silicon has no such limitations: Memory can grow unbounded and test-time computation can be distributed and improved by hardware design. Nonetheless, any practical realization involves some kind of constraint on inference time or compute resources. Therefore, resource-constrained optimal inference hinges on how to best use the available resources against a growing memory. ", "page_idx": 14}, {"type": "text", "text": "Example A.2 (CNN Classifiers, VAEs and GANs). A trained representation can be co-opted to generate data. For example, a CNN can be used to classify random data until one is labeled with the desired class, and the resulting sample considered as being \u201cgenerated\u201d by the CNN. Similarly, one could generate random data indirectly by feeding noise to an encoder, as done in Generative Adversarial Networks (GANs), again co-opting a representation for generating data. In a Variational Autoencoder (VAE), data is generated by perturbing the latent representation of a map trained to re-construct the dataset. ", "page_idx": 14}, {"type": "text", "text": "Example A.3 (Diffusion Models). Diffusion Models are representations, trained to re-construct the original data, but the mechanics used to reconstruct the data during training are sequential, using an artificial \u201ctime\u201d variable, akin to a realization. This makes their use as \u201cgenerative representation\u201d natural since the reconstruction process is already a stochastic realization.8 ", "page_idx": 14}, {"type": "text", "text": "Example A.4 (The Sage and the Savant). Picture the Library of Alexandria in $40\\;\\mathrm{BCE}$ , with the two best known experts, Sage and Savant. Sage had spent years reading the entire library and distilled its content down to maxims, proverbs, and various nuggets of wisdom. When asked a question, Sage would quickly return a pithy answer, although for the occasional unusual question, a generic answer. Savant was the librarian, with a preternatural ability to rapidly find content to assemble answers to any question. Savant did not have ready answers but, when asked, Savant would scour the library to retrieve relevant sources, speed-read through them, and assemble an answer. If asked the same question by the next customer, Savant would repeat the process anew, to the dismay of customers who saw Savant re-read the same material over and over, seemingly without understanding or learning anything. When voices spread that Savant produced more accurate answers, the enraged Sage burned down the library, putting Savant out of work. Sage regained the status of preeminent source of consultation, who could generalize wisdom from sources long gone, until two millennia later, when a new Savant was created from bits. ", "page_idx": 14}, {"type": "text", "text": "Sage personifies inductive learning, favoring thoughtful and time-consuming learning to enable quick inference and rapid answers to questions. Savant represents transductive inference, which requires access to memory and efficient computation that is specific and tailored to the question, at the cost of having to repeat it all over. ", "page_idx": 14}, {"type": "text", "text": "B B\u2019MOJO implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Training details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We train all the models from scratch on a common language dataset at scales from $130\\mathrm{M}$ to 1.4B using instances with 8 40GB NVIDIA A100 GPUs. For our 1.4B experiments, we use 8 instances simultaneously to train our model and perform the remaining experiments on a single instance. Models are trained using AdamW on $20\\mathrm{x}$ the number of tokens as parameters [21]. We use a batch size of $0.5\\mathrm{M}$ tokens and a cosine learning rate schedule with $5\\%$ warmup and a minimum learning rate of 1e-5. Finally, we use a weight decay of 0.1 and gradient clipping of 1 (see the Appendix for additional details). B\u2019MOJO and B\u2019MOJO-F do not use positional encodings, and both, along with Hybrid use a sliding window of 512 tokens. Furthermore, we found that the learning dynamics of our hybrid models can be improved by having two different learning rates parameters for the hidden SSM and the sliding window. We follow Mamba\u2019s learning rates [19] and GPT3\u2019s [6] for the sliding window. ", "page_idx": 14}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/31f8bffcb3446444f4a5c772d336362d0a39944518ff92597393395c53398967.jpg", "img_caption": ["Figure 6: B\u2019MOJO\u2019s Memory management. (Left) Fading Memory B\u2019MOJO fading memory is computed by a SSM that represents long-range dependencies through a fixed-dimensional representation which is later aggregated on the current tokens along with with the most recent past. (Right) Eidetic $^{+}$ Fading Memory Fading memory is handled as in the left panel while tokens from the past are selected using an innovation test over the SSM output and appended to the current sliding window. The innovation test measures how difficult a new tokens is to predict using the state of the SSM, if a tokens is difficult to predict from the state we store it in the eidetic memory and pass it to the attention module. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/d5dbcaa5efea5caace09f6c25e88fa20b3bd560a1c3ddd58fedda5cf8b69197c.jpg", "img_caption": ["Figure 7: (Left) B\u2019MOJO\u2019s implementation. We report the basic layer we use to implement B\u2019MOJO and its memory hierachy (fading and eidetic). (Right) Efficient interleaved implementation. We show to efficiently implement a sliding window attention over chunks of input, fading and eidetic tokens. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.2 B\u2019MOJO\u2019s efficient implementation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We efficiently implement B\u2019MOJO\u2019s tiered memory hierarchy at scale. The modelling choices that lead to B\u2019MOJO closely follow ideas from Stochastic Realization and have an obvious recurrent efficient implementation. On the other hand, during training one is more interested in a parallel formulation that can allow to process multiple tokens at the same time in a single sequence. In the following we describe how we use chunking to develop B\u2019MOJO\u2019s parallel form. ", "page_idx": 15}, {"type": "text", "text": "Efficient sliding window. Computing fading memory and selecting the most unpredictable tokens to store before feeding them into an attention layer is a sequential process which is hard to parallelize. To solve this we use chunks of length $w$ and use fading and eidetic memory to summarize the whole past before the beginning of each chunk. Then, to efficiently aggregate information from the input and memory tokens we use a sliding window over the interleaved concatenation of input chunks and their respective memory tokens as shown in Figure 7. To make sure that every token is computed uniformly we pick the size of the sliding window to be $K:=w+m_{f}+m_{e}$ , where $m_{f}$ is the number of fading memory tokens, and $m_{e}$ is the number of eidetic tokens. Note that the number of eidetic tokens is not known a priori however, in our experiments, we fixed it to an upper-bound. The modular and interleaved approach Figure 7 enables us to leverage optimized kernels for both the SSM [19] and the sliding window attention [9] components enabling fast training and inference beyond the 1B scale. Furthermore, one can accelerate the Attention components in B\u2019MOJO using standard techniques like GQA, which, for example, can make B\u2019MOJO 1.4B with $2\\mathbf{k}$ context length up to $7\\%$ faster than a vanilla implementation. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Efficient Innovation Selection. The Innovation Selection process we describe in Algorithm Line 1 requires a $\\hat{y}(y_{t-1:t-k})$ whose task is to predict the next output $y_{t}$ of the SSM. Whenever, it is difficult to predict $y_{t}$ using samples from the past $y_{t-1:t-k}$ the consider the most recent input token $u_{t}$ highly informative and store it. While building a new parametric predictor with learnable parameters is possible, in practice, this will require to modify the training loss. In our experiments we consider a fixed predictor so no extra weights need to be learned. We fix $\\hat{y}$ as a running weighted average and implement it using short 1D grouped causal convolutions. ", "page_idx": 16}, {"type": "text", "text": "BPTT: Back-propagation Through Time In this section we discuss our technique for backpropagation through time which enables training with arbitrary context length, potentially infinite. This enables us to treat the training data as batch of samples or as one large string. The trick to enable such training lies in splitting the data into multiple chunks, performing computations on each chunk, caching the statistics like the hidden states for SSMs or the tokens in the previous sliding window/eidetic memory, and then using them for the next chunk of data. One way to efficiently implement this is to write a custom CUDA kernel for this task which defines a Mamba layer which can process a non-zero initial hidden state (as the current implementations only support a zero initial hidden state). However, we would like to use the existing implementations and instead modify the inputs/layer weights such that we can load the cached hidden state before that start of every chunk. Let $\\bar{\\boldsymbol{x}_{t}}\\,\\in\\,\\mathbb{R}^{\\bar{b}\\times d\\times h}$ be the hidden state of an SSM layer (Mamba), where $b,d,h$ are the batch-size, model hidden dimension, state hidden dimension (channels for each state dimension) respectively, and $u_{t}\\in\\mathbb{R}^{b\\times d}$ is the input to the layer. The state update equations in Mamba evolve as $x_{t+1}=A(u_{t})*x_{t}\\!+B(u_{t})*\\widehat{u_{t}}$ , where $A(u_{t})\\in\\dot{\\mathbb{R}}^{b\\times d\\times h}$ , $B(u_{t})^{\\Big\\rightharpoonup}\\in\\mathbb{R}^{b\\times\\hat{d}\\times h}$ , $\\widehat{u_{t}}$ is $u_{t}$ stacked $h\\times$ , and $^*$ is an element-wise multiplication operation. In the traditional implementations $x_{0}=0$ , however, we would like to use a non-zero initial state $\\boldsymbol{x}_{0}^{\\prime}$ . The trick we use here is to break $\\boldsymbol{x}_{0}^{\\prime}\\in\\mathbb{R}^{b\\times d\\times h}$ into $h$ tokens of size $b\\times d$ (which is the same as the size of $u_{t}$ ) and pass them as additional prompts to the input. In practice the dimension $h$ is usually small (like 16) and as a result we can afford to break the hidden state into tokens and pass them as inputs. Let $T$ be the size of each chunk (input), then to enable processing non-zero initial hidden states (or caching) we need to process chunks of size $T+h$ where $T\\gg h$ . We perform $h$ initial steps of the SSM to load the previous hidden state, before we start processing the current chunk. To dynamics for the first $h$ timesteps are governed by the following equation for $t\\in[0,h]$ : $x_{t+1}=x_{t}+B_{t}^{\\prime}*\\widehat{x_{0,t}^{\\prime}}$ , where $\\boldsymbol{x}_{0,t}^{\\prime}$ chooses elements from the $t$ column in the final dimension, $B_{t}^{\\prime}$ is one-hot with 1 in dimension $t$ , and acts a shifting operation which ensures that the tokenized previous hidden state is correctly loaded. Thus after $h$ time-steps the hidden state $x_{h}$ is loaded with the final hidden state from the previous chunk, which can be used with gradient accumulation techniques to compute gradients on arbitrarily sized input sequences. Note that B\u2019MOJO has additional components like the eidetic memory, however, that can be efficiently cached from the previous chunk in constant time/memory cost. ", "page_idx": 16}, {"type": "text", "text": "C State Space realization Attention ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lets start from the usual expression of the attention,9 given an input sequence $\\{u_{i}\\}\\in\\mathbb{R}^{n_{c h}}$ , we create the keys, values and queries vectors as follows, $\\bar{k_{i}}:=\\{W_{K}\\bar{u}_{i}\\}\\in\\bar{\\mathbb{R}}^{n}$ , $q_{i}:=\\{W_{Q}u_{i}\\}\\in\\mathbb{R}^{n}$ , $v_{i}:=\\{W_{V}u_{i}\\}\\in\\mathbb{R}^{n}$ . Now the output of a causal attention layer is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{t}={\\frac{\\sum_{i=1}^{t}\\exp(q_{t}^{T}k_{i})v_{i}}{\\sum_{i=1}^{t}\\exp(q_{t}^{T}k_{i})}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.1 Linear attention ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "It is possible to approximate the exponential in the numerator using a kernel representation which makes the attention a linear operator (the ratio of two linear operators) onto the augmented feature space. The basic idea is to write $\\exp(q_{t}^{T}k_{i})\\approx\\phi(q_{t})^{T}\\phi(k_{i})$ , we therefore get that the linear attention can be computed as: ", "page_idx": 16}, {"type": "equation", "text": "$$\ny_{t}\\approx y_{t}^{l i n}={\\frac{\\sum_{i=1}^{t}\\phi(q_{t})^{T}\\phi(k_{i})v_{i}}{\\sum_{i=1}^{t}\\phi(q_{t})^{T}\\phi(k_{i})}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/f0b8719cb3746d804199abedf97578f4573a1775c8cb7f323a8d53960078159d.jpg", "img_caption": ["Figure 8: B\u2019MOJO\u2019s long context training using BPTT Appendix B.2. We train B\u2019MOJO with two different context sizes, 2k and 16k (BPTT) respectively and evaluate on long context task (PG-19). We show that our model trained with 2k context size is able to extrapolate for context size upto 65536 (with marginal increase in the perplexity), while model trained with $16\\mathrm{k}$ context size can handle long context much more effectively which can be seen by the lower perplexity values as the context size increases. Remarkably, as previously observed in [10], B\u2019MOJO models trained on long contexts underperform models trained on shorter ones if evaluated on fewer tokens (i.e., when the inference context size is much smaller than the training context size). "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Now consider the numerator, the denominator can be obtained by fixing the input $v_{i}=1\\;\\forall i$ . It is trivial to show that we can represent the linear attention using a Finite-Impulse Response dynamical system as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left\\{x_{t}=\\sum_{i=1}^{t}\\phi(k_{i})v_{i}\\right.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This is a linear dynamical system that evolves over time and returns the final values of the linear attention at each time instant $t$ . ", "page_idx": 17}, {"type": "text", "text": "Remark: Note that we can modify the previous equations to represent the attention mechanism by simply using a non-linear read out function (the exp) in place of the identity. However, this is not very insightful since this simple FIR system is simply a shifting register over time. ", "page_idx": 17}, {"type": "text", "text": "It is worth noticing that the state of this system is $t$ , and it always increases over time, usually up to a design parameter specified by the system design which is dictated by the available compute and memory (typical values are set to 2k). ", "page_idx": 17}, {"type": "text", "text": "A simple way to measure the state of the system is to write its state space realization, whose dimension directly informs us on the expressivity of the system. In our case we shall assume that the attention is computed on a sliding window of size $K$ . A canonical realization of the FIR described above is: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{c}{\\medskip Z_{t+1}:=\\left[\\begin{array}{c}{z^{t-K+1}}\\\\ {\\vdots}\\\\ {z^{t+1}}\\end{array}\\right]_{t+1}=\\left[\\begin{array}{c c c c}{\\medskip0}&{1}&&\\\\ &{\\ddots}&\\\\ &&&{1}\\\\ {0}&{0}&{\\ldots}&{0}\\end{array}\\right]\\left[\\begin{array}{c}{z^{t-K}}\\\\ {\\vdots}\\\\ {z^{t}}\\end{array}\\right]_{t}+\\left[\\begin{array}{c}{\\medskip0}\\\\ {0}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]_{\\textstyle1}\\phi(k_{t})v_{t}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "C.2 Connection with State Space Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we describe the connection with State Space models, in particular Mamba (input dependent state space model) and characterize how it approximates a linear attention mechanism. ", "page_idx": 17}, {"type": "text", "text": "First we state the Mamba equations: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l}{\\bar{u}_{t}=w_{0}u_{t}+w_{1}u_{t-1}+w_{2}u_{t-2}+w_{3}u_{t-3}}\\\\ {x_{t}=a(\\bar{u}_{t})x_{t-1}+b(\\bar{u}_{t})\\bar{u}_{t}}\\\\ {y_{t}=c(\\bar{u}_{t})x_{t}+d\\bar{u}_{t}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the sake of simplicity we shall now study the input to state and the read out equation. It is easy to show that this is not a strictly causal realization of a dynamical system (since the state at time $t$ is updated with the input at the same time). ", "page_idx": 18}, {"type": "text", "text": "Remark: Every single channel in a Mamba block is independent from each other and unnormalized, Mamba reduces the variability across channels with a coupling in the lower dimensional projections of the gating parameter $\\Delta$ . ", "page_idx": 18}, {"type": "text", "text": "C.2.1 Local Global factorization of the attention mechanism ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "A natural way to prevent the system matrices to grow ever larger is to approximate the FIR above using an AR component, which would essentially keep a running average of the keys for each token seen so far. This will allow the model to keep some higher level statistics of past data which would be used to summarize past information into a single dimension of the dynamical system. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle y_{t}=\\sum_{i=1}^{t}\\phi(q_{t})^{T}\\phi(k_{i})v_{i}=\\phi(q_{t})^{T}\\biggl(\\sum_{i=0}^{K-1}\\phi(k_{t-i})v_{t-i}+\\sum_{i=1}^{t-K}\\phi(k_{i})v_{i}\\biggr)}}\\\\ {{\\displaystyle\\;=\\phi(q_{t})^{T}\\biggl(\\sum_{i=0}^{K-1}\\phi(k_{t-i})v_{t-i}+\\bar{Z}_{t-K}\\biggr)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which can be written as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{x_{t}=x_{t-K}+\\sum_{i=0}^{K-1}\\phi(k_{t-i})v_{t-i}}\\\\ {y_{t}=\\phi(q_{t})^{T}x_{t}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and canonically realized as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\displaystyle Z_{t+1}:=\\left[\\begin{array}{c}{z^{t-K+1}}\\\\ {\\vdots}\\\\ {z^{t+1}}\\end{array}\\right]_{t+1}=\\left[\\begin{array}{c c c}{0}&{1}\\\\ &{\\ddots}\\\\ &&{1}\\\\ {0}&{0}\\end{array}\\right]\\left[\\begin{array}{c}{z^{t-K}}\\\\ {\\vdots}\\\\ {z^{t}}\\end{array}\\right]_{t}+\\left[\\begin{array}{c}{0}\\\\ {0}\\\\ {\\vdots}\\\\ {1}\\end{array}\\right]\\phi(k_{t})v_{t}}\\\\ {\\displaystyle\\bar{Z}_{t}=\\left[\\begin{array}{c c c}{1}&{\\hdots}&{1}\\end{array}\\right]Z_{t}+\\phi(k_{t})v_{t}}\\\\ {y_{t}=\\phi(q_{t})\\bar{Z}_{t}}\\end{array}\\right.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "C.3 Elements of Realization Theory ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "When studying dynamical systems it comes particularly helpful to study canonical forms. They are particularly well suited to assess properties of dynamical systems and to realize state space models that realize a desired input-output behavior. ", "page_idx": 18}, {"type": "text", "text": "In particular, given a transfer function of a LTI system ", "page_idx": 18}, {"type": "equation", "text": "$$\nW(z)=\\frac{\\beta_{0}+\\beta_{1}z+\\ldots+\\beta_{n-1}z^{n-1}}{\\alpha_{0}+\\alpha_{1}z+\\ldots+\\alpha_{n-1}z^{n-1}+z^{n}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "we can write its state space canonical controllable realization as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{X_{t+1}=\\left[\\begin{array}{c c c c}{0}&{1}&&\\\\ &&{\\ddots}&\\\\ &&&{1}\\\\ {-\\alpha_{0}}&{-\\alpha_{1}}&{...}&{-\\alpha_{n-1}\\Big]}&{\\!\\!\\!\\!\\left[0\\right]\\!\\!\\!\\!\\vdots\\!\\!\\!}\\\\ &&&{\\!\\!\\!\\!\\!\\left[X_{t}=\\left[\\beta_{0}\\quad...\\quad\\beta_{n-1}\\right]X_{t}+D u_{t}\\right.}&{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The first simple result to show is that any causal (but non-strictly) dynamical system in the following form ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bigg\\{x_{t}=A x_{t-1}+B u_{t}}\\\\ {y_{t}=C x_{t}+D u_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "can be rewritten in its canonical form as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\{\\hat{x}_{t}=A\\hat{x}_{t-1}+u_{t-1}}\\\\ {y_{t}=\\hat{C}\\hat{x}_{t}+\\hat{D}u_{t}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In fact, starting from the transfer function of the first system ", "page_idx": 19}, {"type": "equation", "text": "$$\nW(z)={\\frac{B}{1-A z^{-1}}}={\\frac{z B}{z-A}}=B+{\\frac{A B}{z-A}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\{\\begin{array}{l}{\\hat{x}_{t+1}=A\\hat{x}_{t}+u_{t-1}}\\\\ {x_{t}=A B\\hat{x}_{t}+B u_{t}}\\\\ {y_{t}=C x_{t}+D u_{t}}\\end{array}\\right.}&{{}\\rightarrow\\left\\{\\begin{array}{l}{\\hat{x}_{t+1}=A\\hat{x}_{t}+u_{t-1}}\\\\ {y_{t}=C A B\\hat{x}_{t}+(C B+D)u_{t}}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that the terms $C$ and $B$ only appear as the product $C B$ , which then we can rename as $\\hat{C}$ . ", "page_idx": 19}, {"type": "text", "text": "Remark: Extending the previous canonical from to Time-Varying Systems is tedious but straightforward. ", "page_idx": 19}, {"type": "text", "text": "The controllable canonical form we introduced in Equation (12) can be easily extended to TimeVarying Systems and Input-Varying as well since it encodes the algebraic properties of the relationship between \u201cpositional\u201d variables at time time instants (or of different inputs). Hence, it is straightforward to see that when setting all the coefficients on the last row of the state transition matrix to zero we get back the same nilpotent dynamical system that represents the attention mechanism (note that, differently from the linear attention case, the read-out function is non-linear). ", "page_idx": 19}, {"type": "text", "text": "On the other hand, we can use this canonical form to represent a Mamba (diagonal) model (which is non-canonical) by simply picking the coefficients of the characteristic polynomial such that its poles are the same as the diagonal entries of the Mamba block. Note however, that Mamba being non-minimal, could have some zero-pole cancellations depending on the specific values of the input matrix $B(u_{t})$ and $C(u_{t})$ , in such cases the input-output behaviour associated with the cancellation does not appear in the output of Mamba and, so long as it is stable (which is always the case thanks to Mamba\u2019s parametrization), it is guaranteed to remain bounded and decay to zero exponentially fast. ", "page_idx": 19}, {"type": "text", "text": "Hence, both Mamba and the Attention mechanism can be implemented by B\u2019MOJO for any fixed state $N$ . ", "page_idx": 19}, {"type": "text", "text": "D Further empirical results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Synthetic Tasks Beyond Associative Recall ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 3 we report results on synthetic tasks other than Multi-Query Associative Recall (MQAR). The table below expands the range of synthetic tasks from MQAR to four more synthetic tasks and compares performance across multiple scales. ", "page_idx": 19}, {"type": "text", "text": "D.2 Scaling laws and zero-shot evaluations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Figure 3, we report perplexity at different scales against (left) the number of parameters and (right) the wall-clock training time. B\u2019MOJO is faster than Mamba and Mistral that use efficient CUDA kernels (Flash attention and Selective Scan), outperforms our pre-trained Mamba baseline at all scales, and is comparable with our Mistral Transformer model. ", "page_idx": 19}, {"type": "text", "text": "While prior works like Jamba [28] (and the recently released Zamba [18]) slightly outperform pure Transformer models they used full attention, and thus retain the quadratic dependence of Transformers. In contrast, our work only uses a small 512 token sliding window attention to produce a model with linear dependency on the sequence length with a constant KV cache size. Other works like Griffin [10] also use sliding windows, but manage to slightly outperform Transformer leveraging much longer sliding windows sizes than ours. ", "page_idx": 19}, {"type": "table", "img_path": "RnQdRY1h5v/tmp/ad1445b954f55c9b6d9abf9318fba7937bd562d72e71cffceefbf90b485fdf72.jpg", "table_caption": ["Table 3: BMOJO demonstrates strong performance on a wide range of synthetic tasks, and both small and medium scale. We compare the performance of various models at the 2 layer and the 130M scale on 4 different synthetic tasks, (1) Selective Copying, a task involving recall of a specific sequence of tokens with random spacing (2) Induction Heads, the recall of a specific token amongst noisy tokens (3) Noisy MQAR, an associative recall task retrieving keys in a noisy environment and (4) Fuzzy Recall, an associative recall task involving keys and values that are multiple tokens each. We find that B\u2019MOJO models consistently outperform or match all existing baselines. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "E Strip MAMBA ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section we derive the form of MAMBA reported above from the original source, combining the published paper and software implementation provided by the authors [19]. ", "page_idx": 20}, {"type": "text", "text": "Every Mamba layer contains a State Space Model which maps sequences $(B,L,d_{i n})$ to $(B,L,d_{i n})$ Call the input of the state space models in Mamba is $(B,L,\\bar{d}_{i n})$ is $\\{u_{i}\\}_{i=0}^{L}$ . ", "page_idx": 20}, {"type": "text", "text": "The input sequence is used to generate the discretization time-step $\\Delta$ , the input matrix $B$ and the output matrix $C$ , using the following projection matrices: $\\dot{P_{\\Delta_{d o w n}}}\\,\\in\\,\\mathbb{R}^{\\Delta_{d o w n}^{\\star}\\times d_{i n}}$ , $P_{\\Delta_{u p}}\\in$ $\\mathbb{R}^{d_{i n}\\times\\Delta_{d o w n}}$ and $P_{B}\\in\\mathbb{R}^{N\\times d_{i n}}$ and $P_{C}\\in\\mathbb{R}^{N\\times d_{i n}}$ . ", "page_idx": 20}, {"type": "text", "text": "Overall, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{B(u_{t})=P_{B}u_{t}\\in\\mathbb{R}^{d_{i n}}}\\\\ {C(u_{t})=P_{C}u_{t}\\in\\mathbb{R}^{d_{i n}}}\\\\ {\\Delta(u_{t})=\\mathrm{softplus}(P_{\\Delta_{u p}}P_{\\Delta_{d o w n}}u_{t})\\in\\mathbb{R}^{d_{i n}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now, given a state representation for all $d_{i n}$ dimensions $\\boldsymbol{x}_{t}\\in\\mathbb{R}^{d_{i n}\\times N}$ and the state update matrix $A\\in\\mathbb{R}^{d_{i n}\\times N}$ , We can now write the mamba update rule as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta(u_{t})=\\mathrm{softplus}(P_{\\Delta_{u p}}P_{\\Delta_{d o w n}}u_{t})\\in\\mathbb{R}^{d_{i n}}}\\\\ &{\\begin{array}{r l}{x_{t+1}=\\exp(\\mathrm{Diag}(\\Delta(u_{t}))A)*x_{t}+\\mathrm{Diag}(\\Delta(u_{t}))u_{t}u_{t}^{T}P_{B}^{T}}\\\\ {y_{t}=x_{t}P_{C}^{T}u_{t}+D}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $^*$ denotes the element wise product. The elementwise product makes clear that Mamba\u2019s dynamics are diagonal. Revisiting the description of Mamba\u2019s diagonal dynamics in the main paper, ", "page_idx": 20}, {"type": "equation", "text": "$$\nA(u_{t})=A_{\\mathrm{Mamba}}(u_{t})=\\left[\\begin{array}{c c c}{a_{1}(u_{t})}&&&\\\\ &{\\ddots}&\\\\ &&{a_{N}(u_{t})}\\end{array}\\right],\\quad b(u_{t})=b_{\\mathrm{Mamba}}(u_{t})=\\left[\\begin{array}{c}{b_{1}(u_{t})}\\\\ {\\vdots}\\\\ {b_{N}(u_{t})}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "it is natural to see that the values of $a_{i}(u_{t})$ are elements of the matrix $\\exp(\\operatorname{Diag}(\\Delta(u_{t}))A)\\,=$ $\\exp(\\mathrm{Diag}(\\mathrm{softplus}(P_{\\Delta_{u p}}P_{\\Delta_{d o w n}}u_{t}))A)$ , and $b_{i}(u_{t})$ are elements of $\\mathrm{Diag}(\\Delta(u_{t}))u_{t}u_{t}^{T}P_{B}^{T}$ . Our nonlinear readout function $\\rho$ is simply bilinear in $x_{t}$ and $u_{t}$ . ", "page_idx": 20}, {"type": "text", "text": "E.1 Bilinear Mamba ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "With some approximations, we can further show that Mamba can be written as a bilinear system on an augmented input space. Using the first order Taylor Series, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t+1}=e^{\\mathrm{Diag}(\\Delta_{t})A}*x_{t}+\\mathrm{Diag}(\\Delta_{t})u_{t}u_{t}^{T}P_{B}}\\\\ &{\\qquad=\\bigl(\\mathrm{Diag}(\\Delta_{t})A+{\\mathbf{1}}\\mathbf{1}^{T}\\bigr)*x_{t}+\\mathrm{Diag}(\\Delta_{t})u_{t}u_{t}^{T}P_{B}}\\\\ &{\\qquad=x_{t}+\\mathrm{Diag}(\\Delta_{t})(A*x_{t}+u_{t}u_{t}^{T}P_{B})}\\\\ &{\\qquad=x_{t}+\\mathrm{Diag}(\\log(1+\\exp(P_{\\Delta}u_{t})))(A*x_{t}+u_{t}u_{t}^{T}P_{B})}\\\\ &{\\qquad=x_{t}+\\mathrm{Diag}([P_{\\Delta}u_{t}]_{+})(A*x_{t}+u_{t}u_{t}^{T}P_{B})}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second line follows from the application of the first order Taylor Series, the third from rearranging terms while applying the definition of $\\Delta_{t}$ , and the forth line follows from replacing notation for the soft hinge loss gating with $[\\cdot]_{+}$ . Now, we carry out some algebra as follows: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{t+1}=x_{t}+\\mathrm{Diag}([u_{t}]_{+})(A*x_{t}+P_{\\Delta}^{\\dagger}u_{t}u_{t}^{T}P_{\\Delta}^{T\\dagger}P_{B})}\\\\ &{\\qquad=x_{t}+\\mathrm{Diag}([u_{t}]_{+})A*x_{t}+\\mathrm{Diag}([u_{t}]_{+})P_{\\Delta}^{\\dagger}u_{t}u_{t}^{T}P_{B}}\\\\ &{\\qquad=x_{t}+\\mathrm{Diag}([u_{t}]_{+})A*x_{t}+(u_{t}^{T}\\otimes\\mathrm{Diag}([u_{t}]_{+}))\\mathrm{vec}(P_{\\Delta}^{\\dagger})\\mathrm{vec}(P_{B})^{T}(I\\otimes u_{t}^{T})^{T}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where vec vectorizes a matrix into a column vector, and $\\otimes$ is the Kronecker Product between two matrices. Now we can look at the vectorized evolution of $h$ , denoting vec $\\left(\\boldsymbol{x}_{t}\\right)$ as $\\tilde{x}_{t}\\in\\mathbb{R}^{d N}$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{\\tau}_{t+1}=\\tilde{x}_{t}+\\mathrm{vec}(\\mathrm{Diag}([u_{t}]_{+})A)*\\tilde{x}_{t}+\\mathrm{vec}((u_{t}^{T}\\otimes\\mathrm{Diag}([u_{t}]_{+}))\\mathrm{vec}(P_{\\Delta}^{\\dagger})\\mathrm{vec}(P_{B})^{T}(I\\otimes u_{t}^{T})^{T})}\\\\ {=\\tilde{x}_{t}+\\underbrace{(A^{T}\\odot I)}_{P a r a m e t e r s}\\underbrace{[u_{t}]_{+}}_{F e a t u r e s}*\\tilde{x}_{t}+\\underbrace{((I\\otimes u_{t}^{T})\\otimes(u_{t}^{T}\\otimes\\mathrm{Diag}([u_{t}]_{+})))}_{F e a t u r e s}\\underbrace{(\\mathrm{vec}(P_{B})\\otimes\\mathrm{vec}(P_{\\Delta}^{\\dagger}))}_{P a r a m e t e r s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\odot$ is the Khatri-Rao product. The purpose of this highly messy derivation is simply to demonstrate that there exists a feature map of $u_{t}$ that we will call $z_{t}$ , and sparse parameter sets $\\tilde{A}$ , $\\tilde{P}_{B},\\tilde{P}_{C}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\tilde{x}_{t+1}=\\tilde{A}z_{t}*\\tilde{x}_{t}+z_{t}\\tilde{P}_{B}}\\\\ {y_{t}=\\tilde{x}_{t}\\tilde{P}_{C}^{T}z_{t}+D,}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the system can be described as a discrete bilinear system. ", "page_idx": 21}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/ea626a39cd744bbfc2b4e14bc91d2735bd00ba2457193e670e26b1bc780cf054.jpg", "img_caption": ["Figure 9: An illustration of the Mamba architecture and main block. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "RnQdRY1h5v/tmp/1648a41912fac6506e2f559c277672d620a2c1fa77f1f87fc391e919507aeb5f.jpg", "img_caption": ["Figure 10: An illustration of the Mamba SSM block and Selective Scan operation. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our abstract and introduction proposes a novel model class leveraging both eideic and fading memory for sequence modeling. In our paper we propose the B\u2019MOJO model class, we motivate it through stochastic realization theory, and we conduct experiments demonstrating its main capabilities, all reflect what is described in the abstract and introduction. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We have a Limitations section that explicity describes multiple limitations of the model and our experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 23}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We prove our theoretical claims about transformers\u2019s eidetic memory and SSMs\u2019s fading memory constructively inline by showing the corresponding state matrix for each model in the main text of the paper. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We use the supplement to add a plethora of details to reproduce our empirical findings. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We do not provide code at submission time. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We have a section titled Training Details in the appendix that covers precisely the hyperparameters, and type of optimizer and other crucial training details. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 26}, {"type": "text", "text": "Justification: We do not report error bars in our paper, but clarify the sample sizes of our evaluation sets and other choices of hyperparameters so that the reader recognizes the significance of our results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We clearly state the compute hardware we used and the quantity we used in Training defaullts. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Upon review of the NeurIPS Code of Ethics we conform. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We discuss how if our elementary realizations were scaled to the size of foundation models there could be both positive and negative societal impacts in the limitations section. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not release models. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We properly credit the creators and original owners of assets used in the paper by citing them according to their request. We explicitly mention the license in our codebase for the paper where the creators and owners have requested we do so. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets at this time, but will thoroughly document its assets when realeased at camera ready. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]