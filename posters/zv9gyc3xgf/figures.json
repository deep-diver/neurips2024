[{"figure_path": "zv9gYC3xgF/figures/figures_8_1.jpg", "caption": "Figure 1: Left: Sublinear convergence of the likelihood loss L. Middle: Sublinear convergence of the parametric distance \u03a3i\u2208[n] \u03c0i||\u03bci \u2212 \u03bc\u2217||2 between student GMM and the ground truth. Right: Impact of different mixing weights on the convergence speed.", "description": "This figure demonstrates the sublinear convergence of gradient EM for over-parameterized Gaussian Mixture Models (GMMs).  The left panel shows the sublinear decrease in likelihood loss (L) over iterations for models with 2, 5, and 10 components. The middle panel displays the sublinear convergence of the parametric distance between the learned model and the ground truth, which is a single Gaussian. The right panel illustrates how different mixing weights in the GMM affect the convergence speed.", "section": "5 Experiments"}, {"figure_path": "zv9gYC3xgF/figures/figures_8_2.jpg", "caption": "Figure 2: Left: Gradient norm ||\u2207L(\u03bc(0))|| in the counter-example in Theorem 7 decreases exponentially fast w.r.t. dimension d. Right: The statistical error (blue line) approximately scales as ~ n-1/4 with sample size n.", "description": "The figure contains two sub-figures. The left sub-figure shows how the gradient norm in the counterexample of Theorem 7 exponentially decreases with respect to dimension d. The right sub-figure shows that the statistical error approximately scales as n^(-1/4) with the sample size n.", "section": "5 Experiments"}]