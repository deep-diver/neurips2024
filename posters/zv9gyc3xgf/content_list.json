[{"type": "text", "text": "Toward Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixture Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Weihang Xu Maryam Fazel Simon S. Du University of Washington University of Washington University of Washington xuwh@cs.washington.edu mfazel@uw.edu ssdu@cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the gradient Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) in the over-parameterized setting, where a general GMM with $n\\,>\\,1$ components learns from data that are generated by a single ground truth Gaussian distribution. While results for the special case of 2-Gaussian mixtures are well-known, a general global convergence analysis for arbitrary $n$ remains unresolved and faces several new technical barriers since the convergence becomes sub-linear and non-monotonic. To address these challenges, we construct a novel likelihood-based convergence analysis framework and ri\u221agorously prove that gradient EM converges globally with a sublinear rate $O(1/\\sqrt{t})$ . This is the first global convergence result for Gaussian mixtures with more than 2 components. The sublinear convergence rate is due to the algorithmic nature of learning overparameterized GMM with gradient EM. We also identify a new emerging technical challenge for learning general over-parameterized GMM: the existence of bad local regions that can trap gradient EM for an exponential number of steps. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Learning Gaussian Mixture Models (GMM) is a fundamental problem in machine learning with broad applications. In this problem, data generated from a mixture of $n\\geq2$ ground truth Gaussians are observed without the label (the index of component Gaussian that data is sampled from), and the goal is to retrieve the maximum likelihood estimation of Gaussian components. The Expectation Maximization (EM) algorithm is arguably the most widely-used algorithm for this problem. Each iteration of the EM algorithm consists of two steps. In the expectation (E) step, it computes the posterior probability of unobserved mixture membership label according to the current parameterized model. In the maximization (M) step, it computes the maximizer of the $Q$ function, which is the likelihood with respect to posterior estimation of the hidden label computed in the E step. ", "page_idx": 0}, {"type": "text", "text": "Gradient EM, as a popular variant of EM, is often used in practice when the maximization step of EM is costly or even intractable. It replaces the M step of EM with taking one gradient step on the $Q$ function. Learning Gaussian Mixture Models with EM/gradient EM is an important and widely-studied problem. Starting from the seminal work [Balakrishnan et al., 2014], a flurry of work Daskalakis et al. [2017], Xu et al. [2016], Dwivedi et al. [2018a], Kwon and Caramanis [2020], Dwivedi et al. [2019] have studied the convergence guarantee for EM/gradient EM in various settings. However, these works either only prove local convergence, or consider the special case of 2-Gaussian mixtures. A general global convergence analysis of EM/gradient EM on $n$ -Gaussian mixtures still remains unresolved. Jin et al. [2016] is a notable negative result in this regard, where the authors show that on GMM with $n\\geq3$ components, randomly initialized EM will get trapped in a spurious local minimum with high probability. ", "page_idx": 0}, {"type": "text", "text": "Over-parameterized Gaussian Mixture Models. Motivated by the negative results, a line of work considers the over-parameterized setting where the model uses more Gaussian components than the ground truth GMM, in the hope that it might help the global convergence of EM and bypass the negative result. In such over-parameterized regime, the best that people know so far is from [Dwivedi et al., 2018b]. This work proves global convergence of 2-Gaussian mixtures on one single Gaussian ground truth. The authors also show that EM has a unique sub-linear convergence rate in this over-parameterized setting (compared with the linear convergence rate in the exact-parameterized setting [Balakrishnan et al., 2014]). This motivates the following natural open question: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Can we prove global convergence of the EM/gradient EM algorithm on general $n$ -Gaussian mixtures in the over-parameterized regime? ", "page_idx": 1}, {"type": "text", "text": "In this paper, we take a significant step towards answering this question. Our main contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We prove global convergence of the gradient EM algorithm for learning general $n$ -component GMM on one single ground truth Gaussian distribution. This is, to the best of our knowledge, the first global convergence proof for general $n$ -component GMM. Our convergence rate is sub-linear, reflecting an inherent nature of over-parameterized GMM (see Remark 3 for details).   \n\u2022 We propose a new analysis framework that utilizes the likelihood function for proving convergence of gradient EM. Our new framework tackles several emerging technical barriers for global analysis of general GMM.   \n\u2022 We also identify a new geometric property of gradient EM for learning general $n$ -component GMM: There exists bad initialization regions that traps gradient EM for exponentially long, resulting in an inevitable exponential factor in the convergence rate of gradient EM. ", "page_idx": 1}, {"type": "text", "text": "1.1 Gaussian Mixture Model (GMM) ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the canonical Gaussian Mixture Models with weights $\\pi=(\\pi_{1},...,\\pi_{n})$ $(\\textstyle\\sum_{i=1}^{n}\\pi_{i}=1)$ ), means $\\pmb{\\mu}=(\\mu_{1}^{\\top},\\bot\\ldots,\\mu_{n}^{\\top})^{\\top}$ and unit covariance matrices $I_{d}$ in $d$ -dimensional space. Following a widely-studied setting [Balakrishnan et al., 2014, Yan et al., 2017, Daskalakis et al., 2017], we set the weights $\\pi$ and covariances $I_{d}$ in student GMM as fixed, and the means $\\pmb{\\mu}=(\\mu_{1}^{\\top},\\bot\\ldots,\\bar{\\mu}_{n}^{\\top})^{\\top}$ as trainable parameters. We use ${\\mathrm{GMM}}(\\mu)$ to denote the GMM model parameterized by $\\pmb{\\mu}$ , which can be described with probability density function (PDF) $p_{\\pmb{\\mu}}:\\mathbf{R}^{d}\\rightarrow\\mathbf{R}_{\\geq0}$ as ", "page_idx": 1}, {"type": "equation", "text": "$$\np_{\\mu}(x)=\\sum_{i\\in[n]}\\pi_{i}\\phi(x|\\mu_{i},I_{d})=\\sum_{i\\in[n]}\\pi_{i}(2\\pi)^{-d/2}\\exp\\left(-\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\phi(\\cdot|\\mu,\\Sigma)$ is the PDF of $\\mathcal{N}(\\mu,\\Sigma)$ , $\\pi_{1}+\\cdot\\cdot\\cdot+\\pi_{n}=1,\\pi_{i}>0,\\forall i\\in[n].$ . ", "page_idx": 1}, {"type": "text", "text": "1.2 Gradient EM algorithm ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The EM algorithm is one of the most popular algorithms for retrieving the maximum likelihood estimator (MLE) on latent variable models. In general, EM and gradient EM address the following problem: given a joint distribution $p_{\\mu^{*}}(x,y)$ of random variables $x,y$ parameterized by $\\pmb{\\mu}^{*}$ , observing only the distribution of $x$ , but not the latent variable $y$ , the goal of EM and gradient EM is to retrieve the maximum likelihood estimator ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\hat{\\pmb{\\mu}}_{\\mathrm{MLE}}\\in\\arg\\operatorname*{max}_{\\pmb{\\mu}}\\log p_{\\pmb{\\mu}}(x).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The focus of this paper is the non-convex optimization analysis, so we consider using population gradient EM algorithm to learn GMM (1), where the observed variable is $x\\in\\mathbf{R}^{d}$ and latent variable is the index of membership Gaussian in GMM. We follow the standard teacher-student setting where a student model ${\\mathrm{GMM}}(\\mu)$ with $n\\geq2$ Gaussian components learns from data generated from a ground truth teacher model $\\mathrm{GMM}(\\mu^{*})$ . We consider the over-parameterized setting where the ground truth model $\\mathrm{GMM}(\\mu^{*})$ is a single Gaussian distribution $\\mathcal{N}(\\mu^{*},I_{d})$ , namely $\\boldsymbol{\\mu}^{*}=(\\boldsymbol{\\mu}^{*}{}^{\\top},\\dots,\\boldsymbol{\\mu}^{*}{}^{\\top})^{\\top}$ . We can then further assume $w.l.o.g$ . that $\\mu^{*}=0$ . Our problem could be seen as a strict generalization of Dwivedi et al. [2018b], where they studied using mixture model of two Gaussians with symmetric means (they set constraint $\\mu_{2}=-\\mu_{1},$ ) to learn one single Gaussian. ", "page_idx": 1}, {"type": "text", "text": "At time step $t\\,=\\,0,1,2,\\ldots$ , given with parameters ${\\pmb{\\mu}}(t)\\,=\\,(\\mu_{1}(t)^{\\top},\\hdots,\\mu_{n}(t)^{\\top})^{\\top}$ , population gradient EM updates $\\pmb{\\mu}$ via the following two steps ", "page_idx": 1}, {"type": "text", "text": "\u2022 E step: for each $i\\in[n]$ , compute the membership weight function $\\psi_{i}:\\mathbf{R}^{d}\\rightarrow\\mathbf{R}$ defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\psi_{i}(x|\\pmb\\mu(t))=\\mathrm{Pr}[i|x]=\\frac{\\pi_{i}\\exp{\\left(-\\frac{\\|x-\\mu_{i}(t)\\|^{2}}{2}\\right)}}{\\sum_{k\\in[n]}\\pi_{k}\\exp{\\left(-\\frac{\\|x-\\mu_{k}(t)\\|^{2}}{2}\\right)}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "\u2022 M step: Define $Q(\\cdot|,\\mu(t))$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ(\\pmb{\\mu}|\\pmb{\\mu}(t))=\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\sum_{i=1}^{n}-\\psi_{i}(x|\\pmb{\\mu}(t))\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Gradient EM with step size $\\eta>0$ performs the following update: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu_{i}(t+1)=\\mu_{i}(t)-\\eta\\nabla_{\\mu_{i}}Q(\\mu(t)|\\mu(t))=\\mu_{i}(t)-\\eta\\mathbf{E}_{x\\sim N(0,I_{d})}\\left[\\psi_{i}(x|\\mu(t))(\\mu_{i}(t)-x)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The membership weight function $x\\rightarrow\\psi_{i}(x|\\mu)$ represents the posterior probability of data point $x$ being sampled from the $i^{\\mathrm{th}}$ Gaussian of ${\\mathrm{GMM}}(\\mu)$ . For ease of notation, we sometimes simply write $\\psi_{i}(x|\\pmb{\\mu})$ as $\\psi_{i}(x)$ when the choice of $\\pmb{\\mu}$ is obvious. ", "page_idx": 2}, {"type": "text", "text": "1.3 Loss function of gradient EM ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Since the task of gradient EM is to find the MLE over ground truth distribution $p_{\\pmb{\\mu}^{*}}$ , we can define the MLE loss function for gradient EM as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\mu})=D_{\\mathrm{KL}}(p_{\\pmb{\\mu}^{*}}||p_{\\pmb{\\mu}})=-\\mathbf{E}_{x\\sim p_{\\mu^{*}}}\\left[\\log\\left(\\frac{p_{\\pmb{\\mu}}(x)}{p_{\\mu^{*}}(x)}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The loss $\\mathcal{L}$ is the Kullback\u2013Leibler (KL) divergence between the ground truth GMM and the student model GMM. Since finding MLE is equivalent to minimizing the KL divergence between model and the ground truth, the goal of gradient EM is equivalent to finding the global minimum of loss $\\mathcal{L}$ . In other words, proving that gradient EM finds the MLE is equivalent with proving the convergence of $\\mathcal{L}$ to 0. However, we are going to present another reason why loss function $\\mathcal{L}$ is important, for it is also closely related to the dynamics of gradient EM. ", "page_idx": 2}, {"type": "text", "text": "Gradient EM is gradient descent on $\\mathcal{L}$ . We present the following important observation. The proof is deferred to appendix. ", "page_idx": 2}, {"type": "text", "text": "Fact 1 states that the gradient of $Q$ function that gradient EM optimizes in each iteration is identical to the gradient of loss function $\\mathcal{L}$ . This observation is very useful since it implies that gradient EM is equivalent to gradient descent (GD) algorithm on $\\mathcal{L}$ . This observation is not a new discovery of ours but actually a wide-spread folklore (see [Jin et al., 2016]). However, our new contribution is to observe Fact 1 is very helpful for analyzing gradient EM, and to construct a new convergence analysis framework for gradient EM based on it. ", "page_idx": 2}, {"type": "text", "text": "1.4 Notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this paper, we adopt the following notational conventions. We denote $\\{1,2,\\ldots,n\\}$ with $[n]$ . $\\pmb{\\mu}\\,=\\,(\\bar{\\mu}_{1}^{\\top},\\dots,\\mu_{n}^{\\top})^{\\top}\\,\\in\\,\\mathbf{R}^{n d}$ denotes the parameter vector of GMM obtained by concatenating Gaussian mean vectors $\\mu_{1},\\ldots,\\mu_{n}$ together. For any vector $\\mu$ , $\\mu(t)$ denotes its value at time step $t$ , sometimes we omit this iteration number $t$ when its choice is clear and simply abbreviate $\\mu(t)$ as $\\mu$ . We define a shorthand of expectation taken over the ground truth GMM $\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}[\\cdot]$ as $\\mathbf{E}_{x}[\\cdot]$ . For any vector $v\\ne0$ , we use ${\\overline{{v}}}\\,:=\\,v/\\|v\\|$ to denote the normalization of $v$ . We define (with a slight abuse of notation) $i_{\\operatorname*{max}}:=\\arg\\operatorname*{max}_{i\\in[n]}\\{\\|\\mu_{i}\\|\\}$ as the index of $\\mu_{i}$ with the maximum norm, and $\\begin{array}{r}{\\mu_{\\operatorname*{max}}:=\\|\\mu_{i_{\\operatorname*{max}}}\\|=\\operatorname*{max}_{i\\in[n]}\\{\\|\\mu_{i}\\|\\}}\\end{array}$ as the maximum norm of $\\mu_{i}$ . In particular, $\\mu_{\\mathrm{max}}(t)=$ $\\operatorname*{max}\\{\\|\\mu_{1}(t)\\|,\\ldots,\\|\\mu_{n}(t)\\|\\}$ . Similarly, $\\pi_{\\operatorname*{min}}\\,:=\\,\\operatorname*{min}_{i\\in[n]}\\pi_{i}$ and $\\pi_{\\operatorname*{max}}\\,:=\\,\\operatorname*{max}_{i\\in[n]}\\pi_{i}$ denotes the minimal and maximal $\\pi_{i}$ , respectively. We use $\\nabla_{\\mu_{i}}\\hat{\\mathcal{L}}$ to denote the gradient of $\\mu_{i}$ on $\\mathcal{L}$ , and $\\nabla{\\mathcal{L}}=(\\nabla_{\\mu_{1}}{\\mathcal{L}}^{\\top},\\ldots,\\nabla_{\\mu_{n}}{\\mathcal{L}})^{\\top}$ denotes the collection of all gradients. Finally we define a potential function $U:\\mathbf{R}^{n d}\\rightarrow\\mathbf{R}$ for ${\\mathrm{GMM}}(\\mu)$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\nU(\\pmb{\\mu})=\\sum_{i\\in[n]}\\|\\mu_{i}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "1.5 Technical overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Here we provide a brief summary of the major technical barriers for our global convergence analysis and our techniques for overcoming them. ", "page_idx": 3}, {"type": "text", "text": "New likelihood-based analysis framework. The traditional convergence analysis for EM/gradient EM in previous works Balakrishnan et al. [2014], Yan et al. [2017], Kwon and Caramanis [2020] proceeds by showing the distance between the model and the ground truth GMM in the parameter space contracts linearly in every iteration. This type of approach meets new challenges in the over-parameterized $n$ -Gaussian mixture setting since the convergence is both sub-linear and nonmonotonic. To address these problems, we propose a new likelihood-based convergence analysis framework: instead of proving the convergence of parameters, our analysis proceeds by showing the likelihood loss function $\\mathcal{L}$ converges to 0. The new analysis framework is more flexible and allows us to overcome the aforementioned technical barriers. ", "page_idx": 3}, {"type": "text", "text": "Gradient lower bound. The first step of our global convergence analysis constructs a gradient lower bound. Using some algebraic transformation techniques, we convert the gradient projection $\\langle\\mathcal{L}(\\pmb{\\mu}),\\pmb{\\mu}\\rangle$ into the expected norm square of a random vector $\\tilde{\\psi}(x)$ . (See Section (4) for the full definition). Although lower bounding the expectation of $\\tilde{\\psi}$ is very challenging, our key idea is that the gradient of $\\tilde{\\psi}$ has very nice properties and can be easily lower bounded, allowing us to establish the gradient lower bound. ", "page_idx": 3}, {"type": "text", "text": "Local smoothness and regularity condition. After obtaining the gradient lower bound, the missing component of the proof is a smoothness condition of the loss function $\\mathcal{L}$ . Since proving the smoothness of $\\mathcal{L}$ is hard in general, we define and prove a weaker notion of local smoothness, which suffices to prove our result. In addition, we design and use an auxiliary function $U$ to show that gradient EM trajectory satisfies the locality required by our smoothness lemma. ", "page_idx": 3}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 2-Gaussian mixtures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "There is a vast literature studying the convergence of EM/gradient EM on 2-component GMM. The initial batch of results proves convergence within a infinitesimally small local region [Xu and Jordan, 1996, Ma et al., 2000]. Balakrishnan et al. [2014] proves for the first time convergence of EM and gradient EM within a non-infinitesimal local region. Among the later works on the same problem, Klusowski and Brinda [2016] improves the basin of convergence guarantee, Daskalakis et al. [2017], Xu et al. [2016] proves the global convergence for 2-Gaussian mixtures. These works focused on the exact-parameterization scenario where the number of student mixtures is the same as that of the ground truth. More recently, Wu and Zhou [2019] proves global convergence of 2-component GMM without any separation condition. Their result can be viewed as a convergence result in the over-parameterized setting where the student model has two Gaussians and the ground truth is a single Gaussian. On the other hand, their setting is more restricted than ours because they require the means of two Gaussians in the student model to be symmetric around the ground truth mean. Weinberger and Bresler [2021] extends the convergence guarantee to the case of unbalanced weights. Another line of work Dwivedi et al. [2018b, 2019, 2018a] studies the over-parameterized setting of using 2-Gaussian mixture to learn a single Gaussian and proves global convergence of EM. Our result extends this type of analysis to the general case of $n$ -Gaussian mixtures, which requires significantly different techniques. We note that going beyond Gaussian mixture models, there are also works studying EM algorithms for other mixture models such as a mixture of linear regression Kwon et al. [2019]. ", "page_idx": 3}, {"type": "text", "text": "2.2 N-Gaussian mixtures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Another line of results focuses on the general case of $n$ Gaussian mixtures. Jin et al. [2016] provides a counter-example showing that EM does not converge globally for $n>2$ (in the exact-parameterized case). Dasgupta and Schulman [2000] prove that a variant of EM converges to MLE in two rounds for $n$ -GMM. Their result relies on a modification of the EM algorithm and is not comparable with ours. [Chen et al., 2023] analyzes the structure of local minima in the likelihood function of GMM. However, their result is purely geometric and does not provide any convergence guarantee. ", "page_idx": 3}, {"type": "text", "text": "A series of paper Yan et al. [2017], Zhao et al. [2018], Kwon and Caramanis [2020], Segol and Nadler follow the framework proposed by Balakrishnan et al. [2014] to prove the local convergence of EM for $n$ -GMM. While their result applies to the more general $n$ -Gaussian mixture ground truth setting, their framework only provides local convergence guarantee and cannot be directly applied to our setting. ", "page_idx": 4}, {"type": "text", "text": "2.3 Slowdown due to over-parameterization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This paper gives an $O\\left(1/{\\sqrt{t}}\\right)$ bound for fitting over-parameterized Gaussian mixture models to a single Gaussian. Recall that to learn a single Gaussian, if one\u2019s student model is also a single Gaussian, then one can obtain an $\\exp(-\\Omega(t))$ rate because the loss is strongly convex. This slowdown effect due to over-parameterization has been observed for Gaussian mixtures in Dwivedi et al. [2018a], Wu and Zhou [2019], but has also been observed in other learning problems, such as learning a two-layer neural network Xu and Du [2023], Richert et al. [2022] and matrix sensing problems [Xiong et al., 2023, Zhang et al., 2021, Zhuo et al., 2021]. ", "page_idx": 4}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we present our main theoretical result, which consists of two parts: In Section 3.1 we present our global convergence analysis of gradient EM, in Section 3.2 we prove that an exponentially small factor in our convergence bound is inevitable and cannot be removed. All omitted proofs are deferred to the appendix. ", "page_idx": 4}, {"type": "text", "text": "3.1 Global convergence of gradient EM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first present our main result, which states that gradient EM converges to MLE globally. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Main result). Consider training a student $n$ -component GMM initialized from $\\pmb{\\mu}(0)=$ $(\\mu_{1}(0)^{\\top},\\ldots,\\mu_{n}(0)^{\\top})^{\\top}$ to learn a single-component ground truth GMM $\\mathcal{N}(0,I_{d})$ with population gradient EM algorithm. If the step size satisfies $\\begin{array}{r}{\\eta\\le O\\left(\\frac{\\exp\\left(-8U(0)\\right)\\pi_{\\operatorname*{min}}^{2}}{n^{2}d^{2}(\\frac{1}{\\mu_{\\operatorname*{max}}(0)}+\\mu_{\\operatorname*{max}}(0))^{2}}\\right)}\\end{array}$ , then gradient EM converges globally with rate ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\mu}(t))\\leq\\frac{1}{\\sqrt{\\gamma t}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma=\\Omega\\left(\\frac{\\eta\\exp(-16U(0))\\pi_{\\operatorname*{min}}^{4}}{n^{2}d^{2}(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{d n})^{4}}\\right)\\in\\mathbf{R}^{+}}\\end{array}$ . Recall that $\\mu_{\\operatorname*{max}}(0)=\\operatorname*{max}\\{\\|\\mu_{1}(0)\\|,\\ldots,\\|\\mu_{n}(0)\\|\\}$ and $\\begin{array}{r}{U(0)=\\sum_{i\\in[n]}\\|\\mu_{i}(0)\\|^{2}}\\end{array}$ are two initialization constants. ", "page_idx": 4}, {"type": "text", "text": "Remark 3. Without over-parameterization, for learning a single Gaussian, one can obtain a linear convergence $\\exp(-\\Omega\\left(t\\right))$ . We would like to note th\u221aat the sub-linear convergence rate guarantee of gradient EM stated in Theorem 2 $({\\mathcal{L}}(\\mu(t))\\leq O(1/{\\sqrt{t}}))$ is due to the inherent nature of the algorithm. Dwivedi et al. [2018b] studied the special case of using 2 Gaussian mixtures with symmetric means to learn a single Gaussian and proved that EM has sublinear convergence rate when the weights $\\pi_{i}$ are equal. Since Theorem 2 studies the more general case of n Gaussian mixtures, this type of subexponential convergence rate is the best than we can hope for. ", "page_idx": 4}, {"type": "text", "text": "Remark 4. The convergence rate in Theorem 2 has a factor exponentially small in the initialization scale $(\\gamma\\propto\\exp(-16U(0)))$ . We would like to stress that this is again due to algorithmic nature of the problem rather than the limitation of analysis. In Section 3.2, we prove that there exists bad regions with exponentially small gradients so that when initialized from such region, gradient EM gets trapped locally for exp $\\Omega(U(0)))$ number of steps. Therefore, a convergence speed guarantee exponentially small in $U(0)$ is inevitable and cannot be improved. ", "page_idx": 4}, {"type": "text", "text": "Remark 5. Theorem 2 is fundamentally different from convergence analysis for EM/gradient EM in previous works Yan et al. [2017], Dwivedi et al. [2019], Balakrishnan et al. [2014] which proved monotonic linear contraction of parameter distance $\\|\\pmb{\\mu}(t)-\\pmb{\\mu}^{*}\\|$ . But our result also implies global convergence since loss function $\\mathcal{L}$ converging to 0 is equivalent to convergence of gradient EM to MLE. ", "page_idx": 4}, {"type": "text", "text": "Remark 6. The convergence result in Theorem 2 is for population gradient EM, but it also implies global convergence for sample-based gradient EM as the sample size tends to infinity. For a similar reduction from population EM to sample EM, see Section 2.2 of [Xu et al., 2016]. ", "page_idx": 4}, {"type": "text", "text": "3.2 Necessity of exponentially small factor in convergence rate ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section we prove that a factor exponentially small in initialization scale $(\\exp(-\\Theta(U(0))))$ is inevitable in the global convergence rate guarantee of gradient EM. Particularly, we show the existence of bad regions such that initialization from this region traps gradient EM for exponentially long time before final convergence. Our result is the following theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 7 (Existence of bad initializati\u221aon region). For a\u221any $\\begin{array}{l l l}{{n}}&{{\\geq}}&{{3,}}\\end{array}$ , define $\\begin{array}{r l r}{\\tilde{\\pmb{\\mu}}(0)}&{{}=}&{}\\end{array}$ $(\\boldsymbol{\\mu}_{1}^{\\top}(0),\\ldots,\\boldsymbol{\\mu}_{n}^{\\top}(0))$ as follows: $\\mu_{1}(0)=12{\\sqrt{d}}e_{1},\\mu_{2}(0)=-12{\\sqrt{d}}e_{1},\\mu_{3}(0)=\\cdots=\\mu_{n}(0)=0,$ where $e_{1}$ is a standard unit vector. Then population gradient EM initialized with means $\\tilde{\\pmb{\\mu}}(0)$ and equal weights $\\pi_{1}\\,=\\,.\\ldots\\,=\\,\\pi_{n}\\,=\\,1/n$ will be trapped in a bad local region around $\\tilde{\\pmb{\\mu}}(0)$ for exponentially long time ", "page_idx": 5}, {"type": "equation", "text": "$$\nT:=\\frac{1}{30\\eta}e^{d}=\\frac{1}{30\\eta}\\exp(\\Theta(U(0))).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "More rigorously, for any $0\\leq t\\leq T,\\exists i\\in[n]$ such that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\mu_{i}(t)\\|\\geq10{\\sqrt{d}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 7 states that, when initialized from some bad points $\\pmb{\\mu}(0)$ , after $\\exp(\\Theta(U(0)))$ number of time steps, gradient EM will still stay in this local region and remain $10\\sqrt{d}$ distance away from the global minimum $\\pmb{\\mu}=0$ . Therefore an exponentially small factor in convergence rate is inevitable. ", "page_idx": 5}, {"type": "text", "text": "Remark 8. Theorem 7 eliminates the possibility of proving any polynomial convergence rate of gradient EM from arbitrary initialization. However, it is still possible to prove that, with some specific smart initialization schemes, gradient EM avoids the bad regions stated in Theorem 7 and enjoys $a$ polynomial convergence rate. We leave this as an interesting open question for future analysis. ", "page_idx": 5}, {"type": "text", "text": "4 Proof overview ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide a technical overview of the proof in our main result (Theorem 2 and Theorem 7). ", "page_idx": 5}, {"type": "text", "text": "4.1 Difficulties of a global convergence proof and our new analysis framework ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Proving the global convergence of gradient EM for general $n$ -Gaussian mixture is highly nontrivial. While there have been many previous works [Balakrishnan et al., 2014, Yan et al., 2017, Dwivedi et al., 2018b] studying either local convergence or the special case of 2-Gaussian mixtures, they all focus on showing the contraction of parametric error. Namely, their proof proceeds by showing the distance between the model parameter and the ground truth contracts, usually by a fixed linear ratio, in each iteration of the algorithm. However, this kind of approach faces various challenges for our general problem where the convergence is both sublinear and non-monotonic. Since the convergence rate is sublinear (see Remark 3), showing a linear contraction per iteration is no longer possible. Since the convergence is non-monotonic1, we also cannot show a strictly decreasing parametric distance. ", "page_idx": 5}, {"type": "text", "text": "To address these challenges, we propose a new convergence analysis framework for gradient EM by proving the convergence of likelihood $\\mathcal{L}$ instead of the convergence of parameters $\\pmb{\\mu}$ . There are several benefits for considering the convergence from the perspective of MLE loss $\\mathcal{L}$ . Firstly, it naturally addresses the problem of non-monotonic and sub-linear convergence since we only need to show $\\mathcal{L}$ decreases as the algorithm updates. Also, since gradient EM is equivalent with running gradient descent on loss function $\\mathcal{L}$ (see Section 1.3), we can apply techniques from the optimization theory of gradient descent to facilitate our analysis. ", "page_idx": 5}, {"type": "text", "text": "4.2 Proof ideas for Theorem 2 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first briefly outline our proof of Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "Proof roadmap. Our proof of Theorem 2 consists of three steps. Firstly, we prove a gradient lower bound for $\\mathcal{L}$ (Theorem 12). Then we prove that the MLE $\\mathcal{L}$ is locally smooth (Theorem 13). Finally, ", "page_idx": 5}, {"type": "text", "text": "we combine the gradient lower bound and the smoothness condition to prove the global convergence of $\\mathcal{L}$ with mathematical induction. ", "page_idx": 6}, {"type": "text", "text": "Step 1: Gradient lower bound. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our first step aims to show that the gradient norm of $\\mathcal{L}(\\mu)$ is lower bounded by the distance of $\\pmb{\\mu}$ to the ground truth. To do this, we need a few preliminary results. Inspired by Chen et al. [2023], we use Stein\u2019s identity [Stein, 1981] to perform an algebraic transformation of the gradient. Recalling the definition of $\\psi_{i}$ in (2), we have the following lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 9. For any $G M M(\\mu),i\\in[n],$ , the gradient of $Q$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{\\mu_{i}}{\\mathcal{L}}(\\mu)=\\nabla_{\\mu_{i}}Q(\\mu|\\mu)=\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The gradient expression above is equivalent with the form in (3), but is easier to manipulate. Using the transformed gradient in Lemma 9, we have the following corollary. ", "page_idx": 6}, {"type": "text", "text": "Corollary 10. Define vector $\\begin{array}{r}{\\tilde{\\psi}_{\\mu}(x):=\\sum_{i\\in[n]}\\psi_{i}(x)\\mu_{i}}\\end{array}$ . For any $G M M(\\mu).$ , the projection of the gradient of $\\nabla{\\mathcal{L}}(\\mu)$ onto $\\pmb{\\mu}$ satisfies ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\langle\\nabla\\mathcal{L}(\\mu),\\mu\\rangle=\\langle\\nabla_{\\mu}Q(\\mu|\\mu),\\mu\\rangle=\\sum_{i\\in[n]}\\left\\langle\\nabla_{\\mu_{i}}Q(\\mu|\\mu),\\mu_{i}\\right\\rangle=\\mathbf{E}_{x}\\left[\\left\\Vert\\tilde{\\psi}_{\\mu}(x)\\right\\Vert^{2}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Corollary 9 is important since it converts the projection of gradient $\\nabla{\\mathcal{L}}(\\pmb{\\mu})$ onto $\\pmb{\\mu}$ to the expected norm square of a vector $\\tilde{\\psi}_{\\mu}$ . Since a lower bound of the gradient projection implies a lower bound of the gradient, we only need to construct a lower bound for $\\langle\\nabla\\mathcal{L}(\\mu),\\mu\\rangle=\\mathbf{E}_{x}\\left[\\left\\|\\tilde{\\psi}_{\\mu}(x)\\right\\|^{2}\\right]$ . Since $\\left\\|\\tilde{\\psi}_{\\mu}(x)\\right\\|^{2}$ is always non-negative, we already know that the gradient projection is non-negative. But lower bounding $\\Xi_{x}\\left[\\left|\\tilde{\\psi}_{\\mu}(x)\\right|\\right|^{2}\\right]$ is still highly nontrivial since the expression of $\\tilde{\\psi}$ is complicated and hard to handle. However, our key observation is that, although $\\tilde{\\psi}$ itself is hard to bound, its gradient has nice properties and can be handled gracefully: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\nabla_{x}\\tilde{\\psi}_{\\mu}(x)=\\frac{1}{2}\\sum_{i,j\\in[n]}\\psi_{i}(x)\\psi_{j}(x)(\\mu_{i}-\\mu_{j})(\\mu_{i}-\\mu_{j})^{\\top}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The gradient (5) is nicely-behaved. One can see immediately from (5) that the matrix $\\nabla_{x}\\tilde{\\psi}_{\\mu}(x)$ is positive-semi-definite, and its eigenvalues can be directly bounded. To utilize these properties, we use the following algebraic trick to convert the task of lower bounding $\\tilde{\\psi}$ itself into the task of lower bounding its gradient. ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{E}_{x}\\left[\\Vert\\tilde{\\psi}_{\\mu}(x)\\Vert^{2}\\right]=\\frac{1}{4}\\mathbf{E}_{x}\\left[\\left(\\int_{t=-1}^{1}\\Vert x\\Vert\\cdot\\overline{{x}}^{\\top}\\nabla\\tilde{\\psi}_{\\mu}(t x)\\overline{{x}}\\mathrm{d}t\\right)^{2}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\bar{x}\\,=\\,\\frac{x}{\\Vert x\\Vert}}\\end{array}$ . See detailed derivation in (23). Using (5), combined with the properties of $\\nabla_{x}\\tilde{\\psi}_{\\mu}(x)$ , we can obtain the following lemma (Recall that $\\begin{array}{r}{U=\\sum_{i\\in[n]}\\|\\mu_{i}\\|^{2}.)}\\end{array}$ : ", "page_idx": 6}, {"type": "text", "text": "Lemma 11. For any $G M M(\\mu)$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{E}_{x}\\left[\\Vert\\tilde{\\psi}_{\\mu}(x)\\Vert^{2}\\right]\\ge\\frac{\\exp{(-8U)}}{40000d(1+2\\mu_{\\mathrm{max}}\\sqrt{d})^{2}}\\left(\\sum_{i,j\\in[n]}\\pi_{i}\\pi_{j}\\Vert\\mu_{i}-\\mu_{j}\\Vert^{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "On top of Lemma 11, we can easily lower bound the gradient projection in the following lemma, finishing the first step of our proof. ", "page_idx": 6}, {"type": "text", "text": "Lemma 12 (Gradient projection lower bound). For any $G M M(\\mu)$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\langle\\nabla_{\\mu}Q(\\mu|\\mu),\\mu\\rangle=\\mathbf{E}_{x}[\\|\\tilde{\\psi}_{\\mu}(x)\\|^{2}]=\\Omega\\left(\\frac{\\exp\\left(-8U\\right)\\pi_{\\operatorname*{min}}^{2}}{d(1+\\mu_{\\operatorname*{max}}\\sqrt{d})^{2}}\\mu_{\\operatorname*{max}}^{4}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Step 2: Local smoothness. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To construct a global convergence analysis for gradient-based methods, after obtaining a gradient lower bound, we still need to prove the smoothness of loss $\\mathcal{L}$ . (Recall that global smoothness of function $f$ means that there exists constant $C$ such that $\\|\\nabla f(x_{1})-\\nabla f(x_{2})\\|\\leq{\\bar{C}}\\|x_{1}-x_{2}\\|,\\forall x_{1},x_{2}.)$ However, proving the smoothness for $\\mathcal{L}$ in general is very challenging since the membership function $\\psi_{i}$ cannot be bounded when $\\pmb{\\mu}$ is unbounded. To address this issue, we prove that $\\mathcal{L}$ is locally smooth, i.e., the smoothness between two points $\\pmb{\\mu}$ and $\\pmb{\\mu}^{\\prime}$ is satisfied if both $\\lVert\\pmb{\\mu}\\rVert$ and $\\|\\pmb{\\mu}-\\pmb{\\mu}^{\\prime}\\|$ are upper bounded. Our result is the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 13 (Local smoothness of loss function). At any two points $\\pmb{\\mu}\\,=\\,(\\mu_{1}^{\\top},\\ldots,\\mu_{n}^{\\top})^{\\top}$ and $\\pmb{\\mu}+\\pmb{\\delta}=((\\mu_{1}+\\delta_{1})^{\\top},\\dots,(\\mu_{n}+\\delta_{n})^{\\top})^{\\top},$ $i f$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\delta_{i}\\|\\leq\\frac{1}{\\operatorname*{max}\\left\\{6d,2\\|\\mu_{i}\\|\\right\\}},\\forall i\\in[n],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then the loss function $\\mathcal{L}$ satisfies the following smoothness property: for any $i\\in[n]$ we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mu_{i}+\\delta_{i}}\\mathcal{L}(\\mu+\\delta)-\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu)\\|\\leq n\\mu_{\\operatorname*{max}}(30\\sqrt{d}+4\\mu_{\\operatorname*{max}})\\|\\delta_{i}\\|+\\sum_{k\\in[n]}\\|\\delta_{k}\\|.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Step 3: putting everything together. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given the gradient lower bound and the smoothness condition, we still need to resolve two remaining problems. The first one is that the gradient lower bound in Lemma 12 is given in terms of $\\pmb{\\mu}$ , which we need to convert to a lower bound in terms of $\\mathcal{L}(\\mu)$ . For this we need the following upper bound of $\\mathcal{L}$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 14 (Loss function upper bound). The loss function can be upper bounded as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\mu})\\leq\\sum_{i\\in[n]}\\frac{\\pi_{i}}{2}\\|\\mu_{i}\\|^{2}\\leq\\frac{\\mu_{\\operatorname*{max}}^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The second problem is that our local smoothness theorem requires $\\pmb{\\mu}$ to be bounded, therefore we need to show a regularity condition that for each $i$ , ${\\pmb{\\mu}}_{i}(t)$ stays in a bounded region during gradient EM updates. This is not easy to prove for each individual $\\pmb{\\mu}_{i}$ due to the same non-monotonic issue mentioned in Section 4.1. To establish such a regularity condition, we use the potential function. $U$ to solve this problem. We prove that $U$ remains bounded along the gradient EM trajectory, implying each $\\pmb{\\mu}_{i}$ remains well-behaved. With this regularity condition, combined with the previous two steps, we finish the proof of Theorem 2 via mathematical induction. ", "page_idx": 7}, {"type": "text", "text": "4.3 Proof ideas for Theorem 7 ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Proving Theorem 7 is much simpler. The idea is natural: we found that there exists some bad regions where the gradient of $\\mathcal{L}$ is exponentially small, characterized by the following lemma. ", "page_idx": 7}, {"type": "text", "text": "Le\u221amma 15 (Gradient\u221anorm upper bound). For any $\\pmb{\\mu}$ satisfying $\\begin{array}{r l}{\\|\\mu_{1}\\|,\\|\\mu_{2}\\|}&{{}\\geq}\\end{array}$ $10{\\sqrt{d}},\\|\\mu_{3}\\|,\\ldots,\\|\\mu_{n}\\|\\leq{\\sqrt{d}},$ , the gradient of $\\mathcal{L}$ at $\\pmb{\\mu}$ can be upper bounded as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu)\\|\\leq2(\\|\\mu_{3}\\|+\\cdots+\\|\\mu_{n}\\|)+2\\exp(-d)(\\|\\mu_{1}\\|+\\|\\mu_{2}\\|),\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Utilizing Lemma 15, we can prove Theorem 7 by showing that initialization from these bad regions will get trapped in it for exponentially long, since the gradient norm is exponentially small. The full proof can be found in Appendix B.2. ", "page_idx": 7}, {"type": "image", "img_path": "zv9gYC3xgF/tmp/ef0cb88999551ff75374fe7a6bc8a17fa85a87769686eb27af332b73cbeae8c0.jpg", "img_caption": ["Figure 1: Left: Sublinear convergence of the likelihood loss $\\mathcal{L}$ . Middle: Sublinear convergence of the parametric distance $\\begin{array}{r}{\\sum_{i\\in[n]}\\pi_{i}\\|\\mu_{i}-\\mu^{*}\\|^{2}}\\end{array}$ between student GMM and the ground truth. Right: Impact of different mixing weights on the convergence speed. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "zv9gYC3xgF/tmp/198dbb667225446b4f66d02e53b56d075acfd8df7b32fd4bab06515abec175bf.jpg", "img_caption": ["Figure 2: Left: Gradient norm $\\|\\nabla\\mathcal{L}(\\mu(0))\\|$ in the counter-example in Theorem 7 decreases exponentially fast w.r.t. dimension $d$ . Right: The statistical error (blue line) approximately scales as $\\sim n^{-1/4}$ with sample size $n$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section we experimentally explore the behavior of gradient EM on GMMs. ", "page_idx": 8}, {"type": "text", "text": "Convergence rates. We choose the experimental setting of $d=5,\\eta=0.7.$ . We use $n=2,5,10$ Gaussian mixtures to learn data generated from one single ground truth Gaussian distribution $\\mathcal{N}(\\mu^{*},I_{d})$ , respectively. Since a closed form expression of the population gradient is intractable, we approximate the gradient step via Monte Carlo method, with sample size $3.5\\times10^{5}$ . The mixing weights of student GMM are randomly sampled from a standard Dirichlet distribution and set as fixed during gradient EM update. The covariances of all component Gaussians are set as the identity matrix. We recorded the convergence of likelihood function $\\mathcal{L}$ (estimated also by Monte Carlo method on fresh samples each iteration) and parametric distance $\\begin{array}{r}{\\sum_{i\\in[n]}\\pi_{i}\\|\\mu_{i}-\\dot{\\mu^{*}}\\|^{2}}\\end{array}$ along gradient EM trajectory. The results are reported in Figure 1 (left and middle panel). Both the likelihood $\\mathcal{L}$ and the parametric distance converges sub-linearly. ", "page_idx": 8}, {"type": "text", "text": "Weight configurations. We train 3-component GMM with 3-different weight configurations and report 4 runs each configuration in Figure 1 (right). Blue: $\\textstyle{\\big(}{\\frac{1}{3}},{\\frac{1}{3}},{\\frac{1}{3}}{\\big)}$ . Orange: $\\textstyle\\left({\\frac{1}{6}},{\\frac{7}{3}},{\\frac{1}{2}}\\right)$ , Green: $\\textstyle\\left({\\frac{1}{20}},{\\frac{1}{5}},{\\frac{3}{4}}\\right)$ . More evenly distributed weights result in faster convergence. ", "page_idx": 8}, {"type": "text", "text": "Initialization geometry. We empirically study the bad initialization point $\\pmb{\\mu}(0)$ described in Theorem $7\\,^{2}$ by plotting the gradient norm at $\\pmb{\\mu}(0)$ w.r.t. different dimension $d$ in Figure 2 (left). As theoretically analyzed, the gradient norm $\\|\\nabla{\\mathcal{L}}(\\mu(0))\\|$ at $\\pmb{\\mu}(0)$ decreases exponentially in dimension $d$ . ", "page_idx": 8}, {"type": "text", "text": "Statistical rates. The statistical rate for EM/gradient EM is another interesting research problem, which we observe empirically in Figure 2 (right). We run gradient EM on 5-component GMM with equal weights. x-axis: number of training samples, y-axis: parametric error after convergence. For each sample size, we run 50 times and report the average. The statistical errors are reported in the blue line. The red line (function $\\Theta(n^{-1/4}))$ and green line (linear regression output fitting blue points) are references. The trajectory approximately follows the law of accuracy $\\propto n^{-1/4}$ . While [Wu and Zhou, 2019] rigorously proves the asymptotic statistical rate of ${\\tilde{O}}(n^{-1/4})$ for the special case of 2-GMMs, our experiments imply that the same rate might also apply to the general case of multi-component GMMs. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper gives the first global convergence of gradient EM for over-parameterized Gaussian mixture models when the ground truth is a single Gaussian, and rate is sublinear which is exponentially slower than the rate in the exact-parameterization case. One fundamental open problem is to study when one can obtain global convergence of EM or gradient EM for Gaussian mixture models when the ground truth has multiple components. The likelihood-based convergence framework proposed in this paper might be an helpful tool towards solving this general problem. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements This work was supported in part by the following grants: NSF TRIPODS II-DMS 20231660, NSF CCF 2212261, NSF CCF 2007036, NSF AF 2312775, NSF IIS 2110170, NSF DMS 2134106, NSF IIS 2143493, and NSF IIS 2229881. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Sivaraman Balakrishnan, Martin J. Wainwright, and Bin Yu. Statistical guarantees for the em algorithm: From population to sample-based analysis, 2014. ", "page_idx": 9}, {"type": "text", "text": "Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suffice for mixtures of two gaussians. In Satyen Kale and Ohad Shamir, editors, Proceedings of the 2017 Conference on Learning Theory, volume 65 of Proceedings of Machine Learning Research, pages 704\u2013 710. PMLR, 07\u201310 Jul 2017. URL https://proceedings.mlr.press/v65/daskalakis17b. html.   \nJi Xu, Daniel J. Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. In Neural Information Processing Systems, 2016. URL https: //api.semanticscholar.org/CorpusID:6310792.   \nRaaz Dwivedi, Nhat Ho, Koulik Khamaru, Martin J. Wainwright, and Michael I. Jordan. Theoretical guarantees for em under misspecified gaussian mixture models. In Neural Information Processing Systems, 2018a. URL https://api.semanticscholar.org/CorpusID:54062377.   \nJeongyeol Kwon and Constantine Caramanis. The em algorithm gives sample-optimality for learning mixtures of well-separated gaussians. In Conference on Learning Theory, pages 2425\u20132487. PMLR, 2020.   \nRaaz Dwivedi, Nhat Ho, Koulik Khamaru, Martin J. Wainwright, Michael I. Jordan, and Bin Yu. Sharp analysis of expectation-maximization for weakly identifiable models. In International Conference on Artificial Intelligence and Statistics, 2019. URL https://api.semanticscholar.org/ CorpusID:216036378.   \nChi Jin, Yuchen Zhang, Sivaraman Balakrishnan, Martin J. Wainwright, and Michael I. Jordan. Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic consequences. In Neural Information Processing Systems, 2016. URL https: //api.semanticscholar.org/CorpusID:3200184.   \nRaaz Dwivedi, Nhat Ho, Koulik Khamaru, Michael I. Jordan, Martin J. Wainwright, and Bin Yu. Singularity, misspecification and the convergence rate of em. The Annals of Statistics, 2018b. URL https://api.semanticscholar.org/CorpusID:88517736.   \nBowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence of gradient em on multi-component mixture of gaussians. Advances in Neural Information Processing Systems, 30, 2017.   \nLei Xu and Michael I Jordan. On convergence properties of the em algorithm for gaussian mixtures. Neural computation, 8(1):129\u2013151, 1996.   \nJinwen Ma, Lei Xu, and Michael I Jordan. Asymptotic convergence rate of the em algorithm for gaussian mixtures. Neural Computation, 12(12):2881\u20132907, 2000.   \nJason M. Klusowski and W. D. Brinda. Statistical guarantees for estimating the centers of a twocomponent gaussian mixture by em. arXiv: Machine Learning, 2016. URL https://api. semanticscholar.org/CorpusID:88514434.   \nYihong Wu and Harrison H. Zhou. Rando\u221amly initialized em algorithm for two-component gaussian mixture achieves near optimality in $o({\\sqrt{n}})$ iterations, 2019.   \nNir Weinberger and Guy Bresler. The em algorithm is adaptively-optimal for unbalanced symmetric gaussian mixtures. J. Mach. Learn. Res., 23:103:1\u2013103:79, 2021. URL https: //api.semanticscholar.org/CorpusID:232404093.   \nJeongyeol Kwon, Wei Qian, Constantine Caramanis, Yudong Chen, and Damek Davis. Global convergence of the em algorithm for mixtures of two component linear regression. In Conference on Learning Theory, pages 2055\u20132110. PMLR, 2019.   \nSanjoy Dasgupta and Leonard J. Schulman. A two-round variant of em for gaussian mixtures. In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence, UAI $\\cdot_{00}$ , page 152\u2013159, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1558607099.   \nYudong Chen, Dogyoon Song, Xumei Xi, and Yuqian Zhang. Local minima structures in gaussian mixture models, 2023.   \nRuofei Zhao, Yuanzhi Li, and Yuekai Sun. Statistical convergence of the em algorithm on gaussian mixture models. arXiv preprint arXiv:1810.04090, 2018.   \nNimrod Segol and Boaz Nadler. Improved convergence guarantees for learning gaussian mixture models by EM and gradient EM. URL http://arxiv.org/abs/2101.00575.   \nWeihang Xu and Simon Du. Over-parameterization exponentially slows down gradient descent for learning a single neuron. In The Thirty Sixth Annual Conference on Learning Theory, pages 1155\u20131198. PMLR, 2023.   \nFrederieke Richert, Roman Worschech, and Bernd Rosenow. Soft mode in the dynamics of overrealizable online learning for soft committee machines. Physical Review E, 105(5):L052302, 2022.   \nNuoya Xiong, Lijun Ding, and Simon S Du. How over-parameterization slows down gradient descent in matrix sensing: The curses of symmetry and initialization. arXiv preprint arXiv:2310.01769, 2023.   \nJialun Zhang, Salar Fattahi, and Richard Y Zhang. Preconditioned gradient descent for overparameterized nonconvex matrix factorization. Advances in Neural Information Processing Systems, 34:5985\u20135996, 2021.   \nJiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. On the computational and statistical complexity of over-parameterized matrix sensing. arXiv preprint arXiv:2102.02756, 2021.   \nCharles M. Stein. Estimation of the Mean of a Multivariate Normal Distribution. The Annals of Statistics, 9(6):1135 \u2013 1151, 1981. doi: 10.1214/aos/1176345632. URL https://doi.org/10. 1214/aos/1176345632.   \nYurii Nesterov et al. Lectures on convex optimization, volume 137. Springer, 2018. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "A Missing Proofs and Auxiliary lemmas ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Proof of Fact 1. It is well known that (see Section 1 of $\\mathrm{Wu}$ and Zhou [2019]) ", "page_idx": 11}, {"type": "equation", "text": "$$\nQ(\\mu^{\\prime}|\\mu)=\\mathbf{E}_{x\\sim p_{\\mu^{*}}}\\left[\\log(p_{\\mu^{\\prime}}(x))-D_{\\mathrm{KL}}(p_{\\mu}(\\cdot|x)||p_{\\mu^{\\prime}}(\\cdot|x))-H(p_{\\mu}(\\cdot|x))\\right],\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "where $p_{\\mu}(\\cdot|x)$ denotes the distribution of hidden variable $y$ (in our case of GMM the index of Gaussian component) conditioned on $x$ , and $H$ denotes information entropy. ", "page_idx": 11}, {"type": "text", "text": "Since $\\pmb{\\mu}^{\\prime}=\\pmb{\\mu}$ is a global minimum of $D_{\\mathrm{KL}}(p_{\\pmb\\mu}(\\cdot|x)||p_{\\pmb\\mu^{\\prime}}(\\cdot|x))$ , we have $\\nabla D_{\\mathrm{KL}}(p_{\\pmb\\mu}(\\cdot|x)||p_{\\pmb\\mu}(\\cdot|x))=$ 0. Also $\\nabla H(p_{\\pmb\\mu}(\\cdot|x))=0$ since $H(p_{\\pmb\\mu}(\\cdot|x))$ is a constant. Therefore ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\nabla Q(\\mu|\\mu)=\\mathbf{E}_{x\\sim p_{\\mu^{*}}}\\left[\\nabla\\log(p_{\\mu}(x))\\right]=\\nabla\\mathcal{L}(\\mu).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The proof of Lemma 9 uses ideas from Theorem 1 of Chen et al. [2023] and relies on Stein\u2019s identity, which is given by the following lemma. ", "page_idx": 11}, {"type": "text", "text": "Lemma 16 (Stein [1981]). For $x\\sim\\mathcal{N}(\\mu,\\sigma^{2}I_{d})$ and differentiable function $g:\\mathbf{R}^{d}\\rightarrow\\mathbf{R}$ we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\mathbf{E}[g(x)(x-\\mu)]=\\sigma^{2}\\mathbf{E}[\\nabla_{x}g(x)],\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "if the two expectations in the above identity exist. ", "page_idx": 11}, {"type": "text", "text": "Now we are ready to prove Lemma 9. ", "page_idx": 11}, {"type": "text", "text": "Lemma 9. For any $G M M(\\mu),i\\in[n],$ , the gradient of $Q$ satisfies ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\nabla_{\\mu_{i}}{\\mathcal{L}}(\\mu)=\\nabla_{\\mu_{i}}Q(\\mu|\\mu)=\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}\\right].\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Proof. Applying Stein\u2019s identity (Lemma 16), for each $i\\in[n]$ we have ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{i}}Q(\\mu|\\mu)=\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\psi_{i}(x)(\\mu_{i}-x)\\right]}\\\\ &{\\qquad\\qquad\\quad=\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\psi_{i}(x)\\right]\\mu_{i}-\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\psi_{i}(x)x\\right]}\\\\ &{\\qquad\\qquad\\quad=\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\psi_{i}(x)\\right]\\mu_{i}-\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}[\\nabla_{x}\\psi_{i}(x)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "Recall that ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\psi_{i}(x)=\\operatorname*{Pr}[i|x]={\\frac{\\pi_{i}\\exp\\left(-{\\frac{\\|x-\\mu_{i}\\|^{2}}{2}}\\right)}{\\sum_{k\\in[n]}\\pi_{k}\\exp\\left(-{\\frac{\\|x-\\mu_{k}\\|^{2}}{2}}\\right)}}.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "The gradient $\\nabla_{x}\\psi_{i}(x)$ could be calculated as ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\frac{1}{\\left(\\sum_{k\\in[n]}\\pi_{k}\\exp\\left(-\\frac{\\|x-\\mu_{k}\\|^{2}}{2}\\right)\\right)^{2}}\\Bigg[\\left(\\sum_{k\\in[n]}\\pi_{k}\\exp\\left(-\\frac{\\|x-\\mu_{k}\\|^{2}}{2}\\right)\\right)\\pi_{i}\\exp\\left(-\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right)(i)}\\\\ &{\\quad-\\pi_{i}\\exp\\left(-\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right)\\left(\\sum_{k\\in[n]}\\pi_{k}\\exp\\left(-\\frac{\\|x-\\mu_{k}\\|^{2}}{2}\\right)(\\mu_{k}-x)\\right)\\Bigg]}\\\\ &{=\\psi_{i}(x)(\\mu_{i}-x)-\\psi_{i}(x)\\displaystyle\\sum_{k\\in[n]}\\psi_{i k}(x)(\\mu_{k}-x)}\\\\ &{=\\psi_{i}(x)(\\mu_{i}-x)+\\psi_{i}(x)x-\\displaystyle\\sum_{k\\in[n]}\\psi_{i k}(x)\\psi_{k}(x)\\mu_{k}}\\\\ &{=\\psi_{i}(x)\\left(\\mu_{i}-\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "note that we used $\\textstyle\\sum_{k\\in[n]}\\psi_{i}(x)=1$ . ", "page_idx": 12}, {"type": "text", "text": "Then we have ", "text_level": 1, "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\overline{{\\gamma}}_{\\mu_{i}}Q(\\mu|\\mu)=\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\right]\\mu_{i}-\\mathbf{E}_{x}\\big[\\nabla_{x}\\psi_{i}(x)\\big]}\\\\ {\\qquad\\qquad=\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\right]\\mu_{i}-\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\left(\\mu_{i}-\\displaystyle\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}\\right)\\right]=\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\displaystyle\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof of Corollary 10. ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\langle\\nabla_{\\mu}Q(\\mu|\\mu),\\mu\\rangle=\\sum_{i\\in[n]}\\left\\langle\\nabla_{\\mu_{i}}Q(\\mu|\\mu),\\mu_{i}\\right\\rangle=\\sum_{i\\in[n]}\\left\\langle\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}\\right],\\mu_{i}\\right\\rangle}}\\\\ {{\\displaystyle=\\sum_{i\\in[n]}\\sum_{k\\in[n]}\\mathbf{E}_{x}\\left\\langle\\psi_{i}(x)\\psi_{k}(x)\\mu_{k},\\mu_{i}\\right\\rangle=\\mathbf{E}_{x}\\left[\\left\\|\\sum_{i\\in[n]}\\psi_{i}(x)\\mu_{i}\\right\\|^{2}\\right]=\\mathbf{E}_{x}\\left[\\left\\|\\tilde{\\psi}_{\\mu}(x)\\right\\|^{2}\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Lemma 17. For any constant c satisfying $\\begin{array}{r}{0<c\\leq\\frac{1}{3d}}\\end{array}$ , we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\exp\\left(c\\|x\\|\\right)\\right]\\leq1+5\\sqrt{d}c.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Note that $\\mathbf{E}_{x\\sim\\mathcal{N}(0,I_{d})}\\left[\\exp\\left(c\\|x\\|\\right)\\right]=\\mathcal{M}_{\\|x\\|}(c)$ is the moment-generating function of $\\left\\|x\\right\\|$ . To upper bound the value of a moment generating function at $c$ , we use Lagrange\u2019s Mean Value Theorem: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{M}_{\\parallel x\\parallel}(c)=\\mathcal{M}_{\\parallel x\\parallel}(0)+\\mathcal{M}_{\\parallel x\\parallel}^{\\prime}(\\xi)c,\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $\\xi\\in[0,c]$ . Note that $\\mathcal{M}_{\\parallel x\\parallel}(0)=1$ , So the remaining task is to bound $M_{\\parallel x\\parallel}^{\\prime}(\\xi)$ . We bound this expectation using truncation method as: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{M_{\\|\\tau\\|}^{\\prime}(\\xi)=\\mathbf{E}_{x}\\left\\{\\|x\\|\\exp(\\xi\\|x\\|)\\right\\}\\leq\\mathbf{E}_{x}\\left\\{\\|x\\|\\exp(c\\|x\\|)\\right\\}}\\\\ &{=\\int_{x\\in\\mathbb{R}^{d}}\\|x\\|\\exp(c\\|x\\|)(2\\pi)^{-d/2}\\exp\\left(-\\frac{\\|x\\|^{2}}{2}\\right)\\mathrm{d}x}\\\\ &{=\\int_{\\|x\\|\\leq1}\\|x\\|\\exp(c\\|x\\|)(2\\pi)^{-d/2}\\exp\\left(-\\frac{\\|x\\|^{2}}{2}\\right)\\mathrm{d}x}\\\\ &{\\quad+\\int_{\\|x\\|\\geq1}\\|x\\|\\exp(c\\|x\\|)(2\\pi)^{-d/2}\\exp\\left(-\\frac{\\|x\\|^{2}}{2}\\right)\\mathrm{d}x}\\\\ &{\\leq\\exp(c)(2\\pi)^{-d/2}V_{d}+\\int_{\\|x\\|\\geq1}\\|x\\|(2\\pi)^{-d/2}\\exp\\left(c\\|x\\|-\\frac{\\|x\\|^{2}}{2}\\right)\\mathrm{d}x}\\\\ &{\\leq\\exp(c)(2\\pi)^{-d/2}V_{d}+\\int_{\\|x\\|\\geq1}\\|x\\|(2\\pi)^{-d/2}\\exp\\left(c\\|x\\|-\\frac{\\|x\\|^{2}}{2}\\right)\\mathrm{d}x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where Vd = $\\begin{array}{r}{V_{d}=\\frac{\\pi^{d/2}}{\\Gamma(d/2+1)}}\\end{array}$ is the volume of $d$ -dimensional unit sphere. ", "page_idx": 12}, {"type": "text", "text": "Since \u2225x\u2225\u22651 \u21d2c\u2225x\u2225\u2212\u2225x2\u22252\u226431d\u2225x\u2225\u2212\u2225x2\u22252\u2264\u2212\u2225(1\u22121/(22d))x\u22252, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lceil\\int_{\\|x\\|\\geq1}\\|x\\|(2\\pi)^{-d/2}\\exp\\left(c\\|x\\|-\\frac{\\|x\\|^{2}}{2}\\right)\\mathrm{d}x\\right.}\\\\ &{\\leq\\int_{\\|x\\|\\geq1}\\|x\\|(2\\pi)^{-d/2}\\exp\\left(-\\frac{\\|\\frac{2d-1}{2d}x\\|^{2}}{2}\\right)\\mathrm{d}x}\\\\ &{=\\int_{\\|y\\|\\geq\\frac{2d}{2d}}\\frac{2d}{2d-1}\\|y\\|(2\\pi)^{-d/2}\\exp\\left(-\\frac{\\|y\\|^{2}}{2}\\right)\\left(\\frac{2d}{2d-1}\\right)^{d}\\mathrm{d}y}\\\\ &{\\leq\\left(\\frac{2d}{2d-1}\\right)^{d+1}\\mathbf{E}_{y\\sim\\mathcal{N}(\\eta,L_{d})}\\left\\|y\\right\\|}\\\\ &{=\\left(\\frac{2d}{2d-1}\\right)^{d+1}\\frac{\\sqrt{2}\\Gamma\\left(\\frac{d+1}{2}\\right)}{\\Gamma\\left(\\frac{d}{2}\\right)}}\\\\ &{\\leq4\\sqrt{d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\left(\\frac{2d}{2d-1}\\right)^{d+1}\\leq4}\\end{array}$ and the log convexity of Gamma function at the last line. Plugging this back to (10), we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{M}_{\\|x\\|}^{\\prime}(\\xi)\\leq\\exp(c)(2\\pi)^{-d/2}V_{d}+\\displaystyle\\int_{\\|x\\|\\geq1}\\|x\\|(2\\pi)^{-d/2}\\exp\\left(c\\|x\\|-\\frac{\\|x\\|^{2}}{2}\\right)\\mathrm{d}x}\\\\ &{\\qquad\\qquad\\leq\\exp(1/(3d))(2\\pi)^{-d/2}+4\\sqrt{d}}\\\\ &{\\qquad\\qquad\\leq5\\sqrt{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Plugging (11) into (9), we obtain the final bound ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbf{E}_{x}\\left[\\exp\\left(2\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)-1\\right]=\\mathcal{M}_{\\|x\\|}(c)=\\mathcal{M}_{\\|x\\|}(0)+\\mathcal{M}_{\\|x\\|}^{\\prime}(\\xi)c\\le1+5\\sqrt{d}c.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Lemma 18. Recall that $\\begin{array}{r}{U=\\sum_{i\\in[n]}\\|\\mu_{i}\\|^{2}}\\end{array}$ . For any fixed $x\\in\\mathbf{R}^{d},x\\neq0$ and any $\\pmb{\\mu}$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int_{t=-1}^{1}\\psi_{i}(t x|\\mu)\\psi_{j}(t x|\\mu)\\mathrm{d}t\\geq\\frac{1}{2\\mu_{\\operatorname*{max}}\\|x\\|}\\pi_{i}\\pi_{j}\\exp\\left(-4U\\right)\\left(1-\\exp\\left(-4\\mu_{\\operatorname*{max}}\\|x\\|\\right)\\right).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{i}(t x)=\\frac{\\pi_{i}\\exp\\Big(-\\frac{\\|\\pi-\\mu_{i}\\|^{2}}{2}\\Big)}{\\sum_{k\\in[n]}\\pi_{k}\\exp\\Big(-\\frac{\\|\\pi-\\mu_{k}\\|^{2}}{2}\\Big)}}\\\\ &{\\quad=\\frac{\\pi_{i}}{\\sum_{k\\in[n]}\\pi_{k}\\exp\\Big(\\frac{1}{2}\\big(\\|x-\\mu_{i}\\|^{2}-\\big\\|\\pi-\\mu_{k}\\big\\|^{2}\\big)\\Big)}}\\\\ &{\\quad=\\frac{\\pi_{i}}{\\sum_{k\\in[n]}\\pi_{k}\\exp\\big(\\frac{1}{2}\\big(\\|x-\\mu_{i}\\|^{2}-\\big\\|\\pi-\\mu_{k}\\big\\|^{2}\\big)\\big)}}\\\\ &{\\quad=\\frac{\\pi_{i}}{\\sum_{k\\in[n]}\\pi_{k}\\exp\\big(\\frac{1}{2}\\big(\\|x-\\mu_{i}\\|^{2}-\\big\\|\\ell x-\\mu_{k}\\big\\|^{2}\\big)\\big)}}\\\\ &{\\quad=\\frac{\\pi_{i}}{\\sum_{k\\in[n]}\\pi_{k}\\exp\\big(\\frac{1}{2}\\big(2\\pi_{k}-\\mu_{i}-\\mu_{k},\\mu_{k}-\\mu_{i}\\big)\\big)}}\\\\ &{\\quad\\geq\\frac{\\pi_{i}}{\\sum_{k\\in[n]}\\pi_{k}\\exp\\big(\\frac{1}{2}\\big(2\\|x\\|+2\\mu_{\\operatorname*{max}}\\big)\\cdot2\\mu_{\\operatorname*{max}}\\big)}}\\\\ &{\\quad=\\pi_{i}\\exp\\big(-2\\mu_{\\operatorname*{max}}\\big(\\|x\\|+\\mu_{\\operatorname*{max}}\\big)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore ", "text_level": 1, "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{t=-1}^{1}\\psi_{i}(t x)\\psi_{j}(t x)\\mathrm{d}t\\geq\\int_{t=-1}^{1}\\pi_{i}\\pi_{j}\\exp\\left(-4\\mu_{\\operatorname*{max}}(\\|t x\\|+\\mu_{\\operatorname*{max}})\\right)\\mathrm{d}t}}\\\\ &{}&{\\quad=\\pi_{i}\\pi_{j}\\exp\\left(-4\\mu_{\\operatorname*{max}}^{2}\\right)\\cdot2\\int_{t=0}^{1}\\exp\\left(-4\\mu_{\\operatorname*{max}}\\|x\\|t\\right)\\mathrm{d}t}\\\\ &{}&{\\quad\\geq\\frac{1}{2\\mu_{\\operatorname*{max}}\\|x\\|}\\pi_{i}\\pi_{j}\\exp\\left(-4U\\right)\\left(1-\\exp\\left(-4\\mu_{\\operatorname*{max}}\\|x\\|\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Proofs for Section 3 and 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proofs for global convergence analysis ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Theorem 13. At any two points $\\pmb{\\mu}=(\\mu_{1}^{\\top},\\bot\\ldots,\\mu_{n}^{\\top})^{\\top}$ and $\\pmb{\\mu}+\\pmb{\\delta}=((\\mu_{1}+\\delta_{1})^{\\top},\\dots,(\\mu_{n}+\\delta_{n})^{\\top})^{\\top}$ , $i f$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\delta_{i}\\|\\leq\\frac{1}{\\operatorname*{max}\\left\\{6d,2\\|\\mu_{i}\\|\\right\\}},\\forall i\\in[n],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then the loss function $\\mathcal{L}$ satisfies the following smoothness property: for any $i\\in[n]$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mu_{i}+\\delta_{i}}\\mathcal{L}(\\mu+\\delta)-\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu)\\|\\leq n\\mu_{\\operatorname*{max}}(30\\sqrt{d}+4\\mu_{\\operatorname*{max}})\\|\\delta_{i}\\|+\\sum_{k\\in[n]}\\|\\delta_{k}\\|.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. Note that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp\\left(-\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)\\exp\\left(-\\frac{\\|\\delta_{i}\\|^{2}}{2}\\right)\\leq\\frac{\\exp\\left(-\\frac{\\|x-(\\mu_{i}+\\delta_{i})\\|^{2}}{2}\\right)}{\\exp\\left(-\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right)}=\\exp\\left(\\langle x-\\mu_{i},\\delta_{i}\\rangle-\\frac{\\|\\delta_{i}\\|^{2}}{2}\\right)}\\\\ &{\\leq\\exp\\left(\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)\\exp\\left(-\\frac{\\|\\delta_{i}\\|^{2}}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore $\\psi_{i}(x|\\pm\\delta)$ can be bounded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{i}(x|\\mu+\\delta)=\\frac{\\pi_{i}\\exp{\\left(-\\frac{\\|x-(\\mu_{i}+\\delta_{i})\\|^{2}}{2}\\right)}}{\\sum_{k\\in[n]}\\pi_{k}\\exp{\\left(-\\frac{\\|x-(\\mu_{k}+\\delta_{k})\\|^{2}}{2}\\right)}}}\\\\ &{\\leq\\frac{\\pi_{i}\\exp{\\left(-\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right)}\\exp{\\left(\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)}\\exp{\\left(-\\frac{\\|\\delta_{i}\\|^{2}}{2}\\right)}}{\\sum_{k\\in[n]}\\pi_{k}\\exp{\\left(-\\frac{\\|x-\\mu_{k}\\|^{2}}{2}\\right)}\\exp{\\left(-\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)}\\exp{\\left(-\\frac{\\|\\delta_{i}\\|^{2}}{2}\\right)}}\\leq\\exp{(2\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Similarly, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\psi_{i}(x|\\mu+\\delta)=\\frac{\\pi_{i}\\exp{\\left(-\\frac{\\|x-(\\mu_{i}+\\delta_{i})\\|^{2}}{2}\\right)}}{\\sum_{k\\in[n]}\\pi_{k}\\exp{\\left(-\\frac{\\|x-(\\mu_{k}+\\delta_{k})\\|^{2}}{2}\\right)}}}\\\\ &{\\geq\\frac{\\pi_{i}\\exp{\\left(-\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right)}\\exp{\\left(-\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)}\\exp{\\left(-\\frac{\\|\\delta_{i}\\|^{2}}{2}\\right)}}{\\sum_{k\\in[n]}\\pi_{k}\\exp{\\left(-\\frac{\\|x-\\mu_{k}\\|^{2}}{2}\\right)}\\exp{\\left(\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)}\\exp{\\left(-\\frac{\\|\\delta_{i}\\|^{2}}{2}\\right)}}\\geq\\exp{\\left(-2\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)}}\\\\ &{\\quad\\times\\frac{\\|\\delta_{i}\\|}{\\sum_{k\\in[n]}\\pi_{k}\\exp{\\left(-\\frac{\\|x-\\mu_{k}\\|^{2}}{2}\\right)}}\\exp{\\left(\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)}\\exp{\\left(-\\frac{\\|\\delta_{i}\\|^{2}}{2}\\right)}\\geq\\exp{\\left(-2\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall that by Lemma 9 we have $\\begin{array}{r}{\\nabla_{\\mu_{i}}\\mathcal L(\\mu)={\\bf E}_{x}\\left[\\psi_{i}(x|\\pmb\\mu)\\sum_{k\\in[n]}\\psi_{k}(x|\\pmb\\mu)\\mu_{k}\\right],\\mathrm{so}}\\end{array}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{x}\\times(\\Delta\\xi)-\\nabla_{x}\\xi(x)\\big(x)\\|}\\\\ &{=\\bigg\\|\\nabla_{x}\\bigg[\\mathrm{s}_{t}(x)(t+\\delta)\\bigg\\|\\mathrm{s}_{t}+\\mathrm{s}_{t}(x)(t+\\delta)\\bigg\\|\\mathrm{s}_{t}+\\mathrm{s}_{t}\\bigg)\\bigg]-\\mathrm{E}_{x}\\bigg[\\mathrm{s}_{t}(x)\\bigg\\|\\mathrm{s}_{t}\\bigg[\\mathrm{s}_{t}(x)\\bigg\\|\\mathrm{s}_{t})\\bigg]\\bigg\\|}\\\\ &{=\\bigg\\|\\mathrm{E}_{x}\\bigg[\\sum_{\\ell\\in\\mathcal{I}_{r}}(x)\\big\\|\\mathrm{s}_{\\ell}(x)+\\mathrm{s}_{\\ell}(x)(t+\\delta)\\bigg\\|\\mathrm{s}_{\\ell}\\bigg]}\\\\ &{\\qquad\\mathrm{l}_{\\ell}\\bigg[\\mathrm{s}_{\\ell}(x)\\bigg\\|\\mathrm{s}_{\\ell}(x)+\\mathrm{s}_{\\ell}(x)(t+\\delta)-\\mathrm{v}_{\\ell}(x)\\bigg\\|\\mathrm{s}_{\\ell}(x)\\bigg\\|\\bigg]\\bigg\\|}\\\\ &{\\qquad+\\mathrm{E}_{x}\\bigg[\\sum_{\\ell\\in\\mathcal{I}_{r}}(x)\\big\\|\\mathrm{s}_{\\ell}(x)+\\mathrm{partial}_{\\ell}(x)(t+\\delta)-\\mathrm{v}_{\\ell}(x)\\big\\|\\mathrm{s}_{\\ell}(x)\\bigg\\|\\mathrm{s}_{\\ell}\\bigg]\\bigg\\|}\\\\ &{\\leq\\mathrm{E}_{x}\\bigg[\\sum_{\\ell\\in\\mathcal{I}_{r}}(x)+\\mathrm{O}_{\\ell}(x)(t+\\delta)\\big\\|\\mathrm{s}_{\\ell}\\bigg]}\\\\ &{\\qquad+\\mathrm{E}_{x}\\bigg[\\sum_{\\ell\\in\\mathcal{I}_{r}}(x)\\big\\|\\mathrm{s}_{\\ell}(x)+\\mathrm{partial}_{\\ell}\\big\\|\\mathrm{s}_{\\ell}(x)+\\mathrm{\\partial}_{\\ell}\\big\\|\\mathrm{s}_{\\ell}(x)(x)(t+\\delta)\\big\\|\\mathrm{s}_{\\ell}\\bigg]}\\\\ &{\\leq\\mathrm{E}_{x}\\bigg[\\bigg\\|\\mathrm{s}_{\\ell}\\bigg]+\\sum_{\\ell\\in\\mathcal{I}_{r}}\\mathrm{E}_{x}\\|\\mathrm{s}_{\\ell}(x)+\\mathrm{O}_{\\ell}(x)(t+\\delta)-\\mathrm{O}(x)(t+\\delta)\\bigg\\|\\mathrm{s}_{\\ell}(x)\\bigg\\|\\mathrm{s}_{\\ell}\\bigg]}\\\\ &\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality is because $\\psi_{i},\\psi_{k}\\leq1$ and applying (15) and (16). ", "page_idx": 15}, {"type": "text", "text": "The remaining task is to bound $\\mathbf{E}_{x}\\left[\\exp{(2\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|))}-1\\right]$ . Since $\\begin{array}{r}{2\\|\\delta_{i}\\|\\leq\\frac{1}{3d}}\\end{array}$ , we can use Lemma 17 to bound it as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{E}_{x}\\left[\\exp\\left(2\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)-1\\right]=\\exp(2\\|\\delta_{i}\\|\\|\\mu_{i}\\|)\\mathbf{E}_{x}\\left[\\exp\\left(2\\|\\delta_{i}\\|\\cdot\\|x\\|\\right)\\right]-1}\\\\ &{\\leq\\exp(2\\|\\delta_{i}\\|\\|\\mu_{i}\\|)(1+10\\sqrt{d}\\|\\delta_{i}\\|)-1=\\exp(2\\|\\delta_{i}\\|\\|\\mu_{i}\\|)-1+10\\sqrt{d}\\|\\delta_{i}\\|\\exp(2\\|\\delta_{i}\\|\\|\\mu_{i}\\|)}\\\\ &{\\leq4\\|\\delta_{i}\\|\\|\\mu_{i}\\|+10\\sqrt{d}\\|\\delta_{i}\\|\\exp(1)\\leq(30\\sqrt{d}+4\\|\\mu_{i}\\|)\\|\\delta_{i}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we used $\\exp(1+x)\\leq1+2x,\\forall x\\in[0,1]$ at the last line. Plugging this back to (17), we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\nabla_{\\mu_{i}+\\delta_{i}}\\mathcal{L}(\\mu+\\delta)-\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu)\\|}\\\\ &{\\le\\displaystyle\\sum_{k\\in[n]}\\|\\delta_{k}\\|+\\displaystyle\\sum_{k\\in[n]}\\mathbf{E}_{x}\\left[\\exp\\left(2\\|\\delta_{i}\\|(\\|x\\|+\\|\\mu_{i}\\|)\\right)-1\\right]\\|\\mu_{k}\\|}\\\\ &{\\le\\displaystyle\\sum_{k\\in[n]}\\|\\delta_{k}\\|+\\displaystyle\\sum_{k\\in[n]}\\left(30\\sqrt{d}+4\\|\\mu_{i}\\|\\right)\\|\\delta_{i}\\|\\|\\mu_{k}\\|}\\\\ &{\\le n\\mu_{\\operatorname*{max}}\\big(30\\sqrt{d}+4\\mu_{\\operatorname*{max}}\\big)\\|\\delta_{i}\\|+\\displaystyle\\sum_{k\\in[n]}\\|\\delta_{k}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Theorem 14. The loss function can be upper bounded as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\mu})\\leq\\sum_{i\\in[n]}\\frac{\\pi_{i}}{2}\\|\\mu_{i}\\|^{2}\\leq\\frac{\\mu_{\\operatorname*{max}}^{2}}{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Since the logarithm function is concave, by Jensen\u2019s inequality we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}(\\mu)=D_{\\mathrm{KL}}\\left[p_{\\mu},\\psi_{\\mu}\\right]=-\\mathrm{E}_{x}\\left[\\log\\left(\\frac{p_{\\mu}(x)}{p_{\\mu}*(x)}\\right)\\right]}\\\\ &{\\quad=-\\mathrm{E}_{x}\\left[\\log\\left(\\frac{\\sum_{i}\\pi_{i}\\exp\\left(-\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right)}{\\exp\\left(-\\frac{\\|x\\|^{2}}{2}\\right)}\\right)\\right]}\\\\ &{\\quad\\le-\\mathrm{E}_{x}\\left[\\sum_{i}\\log\\left(\\frac{\\exp\\left(-\\frac{\\|x-\\mu_{i}\\|^{2}}{2}\\right)}{\\exp\\left(-\\frac{\\|x\\|^{2}}{2}\\right)}\\right)\\right]}\\\\ &{\\quad=-\\sum_{i}\\pi_{i}\\mathrm{E}_{x}\\left[\\langle x,\\mu_{i}\\rangle-\\frac{\\|\\mu_{i}\\|^{2}}{2}\\right]}\\\\ &{\\quad=\\sum_{i\\in[n]}\\frac{\\pi_{i}}{2}\\|\\mu_{i}\\|^{2}\\le\\frac{\\mu_{n}^{2}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 12. For any $G M M(\\mu)$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\nabla_{\\mu}Q(\\mu|\\mu),\\mu\\rangle=\\mathbf{E}_{x}[\\|\\tilde{\\psi}_{\\mu}(x)\\|^{2}]\\geq\\Omega\\left(\\frac{\\exp\\left(-8U\\right)\\pi_{\\operatorname*{min}}^{2}}{d(1+\\mu_{\\operatorname*{max}}\\sqrt{d})^{2}}\\mu_{\\operatorname*{max}}^{4}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Consider two cases: ", "page_idx": 16}, {"type": "text", "text": "Case 1. There exists $k\\in[n]$ such that $\\begin{array}{r}{\\|\\mu_{k}-\\mu_{i_{\\operatorname*{max}}}\\|\\geq\\frac{\\mu_{\\operatorname*{max}}}{2}}\\end{array}$ . Then by Lemma 19 and Lemma 11 we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{E}_{x}\\left[\\Vert\\tilde{\\psi}_{\\mu}(x)\\Vert^{2}\\right]\\geq\\frac{\\exp{(-8U)}}{40000d(1+2\\mu_{\\mathrm{max}}\\sqrt{d})^{2}}\\left(\\underset{i,j\\in[n]}{\\sum_{\\alpha}}\\pi_{i}\\pi_{j}\\Vert\\mu_{i}-\\mu_{j}\\Vert^{2}\\right)^{2}}&{}\\\\ {\\geq\\frac{\\exp{(-8U)}}{40000d(1+2\\mu_{\\mathrm{max}}\\sqrt{d})^{2}}\\left(\\frac{\\pi_{\\mathrm{min}}}{8}\\mu_{\\mathrm{max}}^{2}\\right)^{2}}&{}\\\\ {=\\frac{\\exp{(-8U)}\\,\\pi_{\\mathrm{min}}^{2}}{2560000d(1+2\\mu_{\\mathrm{max}}\\sqrt{d})^{2}}\\mu_{\\mathrm{max}}^{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Case2. For $\\forall k\\,\\in\\,[n],\\,\\|\\mu_{i_{\\operatorname*{max}}}\\,-\\,\\mu_{k}\\,\\|\\,<\\,\\frac{\\mu_{\\operatorname*{max}}}{2}$ . Then by Lemma 20 we have ${\\bf E}_{x}\\left[\\|\\tilde{\\psi}_{\\mu}(x)\\|^{2}\\right]\\ \\ge$ $\\begin{array}{r l r}{\\frac{1}{4}\\mu_{\\operatorname*{max}}^{2}}&{\\geq\\;\\Omega(\\exp(-8\\mu_{\\operatorname*{max}}^{2})\\mu_{\\operatorname*{max}}^{4})\\;\\geq\\;\\Omega(\\exp(-8U)\\mu_{\\operatorname*{max}}^{4})}&{\\geq\\;\\Omega\\left(\\frac{\\exp(-8U)\\pi_{\\operatorname*{min}}^{2}}{d(1+\\mu_{\\operatorname*{max}}\\sqrt{d})^{2}}\\mu_{\\operatorname*{max}}^{4}\\right),}\\end{array}$ $e^{-x}x\\leq1,\\forall x)$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma 19. For any $G M M(\\mu)$ , if there exists $k\\in[n]$ such that $\\begin{array}{r}{\\|\\mu_{k}-\\mu_{i_{\\operatorname*{max}}}\\|\\geq\\frac{\\mu_{\\operatorname*{max}}}{2}}\\end{array}$ , then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{i,j\\in[n]}\\pi_{i}\\pi_{j}\\|\\mu_{i}-\\mu_{j}\\|^{2}\\geq\\frac{\\pi_{\\operatorname*{min}}}{8}\\mu_{\\operatorname*{max}}^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. By Cauchy\u2013Schwarz inequality, we have $\\begin{array}{r}{\\|a\\|^{2}+\\|b\\|^{2}\\geq\\frac{1}{2}\\|a-b\\|^{2}}\\end{array}$ , so for $\\forall i\\in[n]$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{j\\in[n]}\\pi_{j}\\|\\mu_{i}-\\mu_{j}\\|^{2}\\geq\\pi_{i_{\\operatorname*{max}}}\\|\\mu_{i}-\\mu_{i_{\\operatorname*{max}}}\\|^{2}+\\pi_{k}\\|\\mu_{i}-\\mu_{k}\\|^{2}}\\\\ {\\displaystyle\\geq\\frac{\\pi_{\\operatorname*{min}}}{2}\\|(\\mu_{i}-\\mu_{i_{\\operatorname*{max}}})-(\\mu_{i}-\\mu_{k})\\|^{2}=\\frac{\\pi_{\\operatorname*{min}}}{2}\\|\\mu_{k}-\\mu_{i_{\\operatorname*{max}}}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore ", "page_idx": 16}, {"type": "text", "text": "$\\sum_{i,j\\in[n]}\\pi_{i}\\pi_{j}\\|\\mu_{i}-\\mu_{j}\\|^{2}=\\sum_{i\\in[n]}\\pi_{i}\\sum_{j\\in[n]}\\pi_{j}\\|\\mu_{i}-\\mu_{j}\\|^{2}\\geq\\sum_{i\\in[n]}\\pi_{i}\\frac{\\pi_{\\operatorname*{min}}}{2}\\|\\mu_{k}-\\mu_{i_{\\operatorname*{max}}}\\|^{2}\\geq\\frac{\\pi_{\\operatorname*{min}}}{8}\\mu_{\\operatorname*{max}}^{2},$ ", "page_idx": 16}, {"type": "text", "text": "where the last inequality is because $\\begin{array}{r}{\\|\\mu_{k}-\\mu_{i_{\\operatorname*{max}}}\\|\\geq\\frac{\\mu_{\\operatorname*{max}}}{2}}\\end{array}$ and $\\textstyle\\sum_{i}\\pi_{i}=1$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 20. For any $G M M(\\mu)$ , if for $\\forall k\\in[n]$ we have $\\begin{array}{r}{\\|\\mu_{i_{\\mathrm{max}}}-\\mu_{k}\\|<\\frac{\\mu_{\\mathrm{max}}}{2}}\\end{array}$ , then ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}_{x}\\left[\\lVert\\tilde{\\psi}_{\\mu}(x)\\rVert^{2}\\right]\\geq\\frac14\\mu_{\\mathrm{max}}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. For any $k\\in[n]$ , by Cauchy\u2013Schwarz inequality we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mu_{k},\\mu_{i_{\\mathrm{max}}}\\rangle=\\langle\\mu_{i_{\\mathrm{max}}}-(\\mu_{i_{\\mathrm{max}}}-\\mu_{k}),\\mu_{i_{\\mathrm{max}}}\\rangle=\\|\\mu_{i_{\\mathrm{max}}}\\|^{2}-\\langle\\mu_{i_{\\mathrm{max}}}-\\mu_{k},\\mu_{i_{\\mathrm{max}}}\\rangle}\\\\ &{\\qquad\\qquad\\geq\\mu_{\\mathrm{max}}^{2}-\\|\\mu_{i_{\\mathrm{max}}}-\\mu_{k}\\|\\mu_{\\mathrm{max}}>\\frac{1}{2}\\mu_{\\mathrm{max}}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the last inequality is because $\\begin{array}{r}{\\|\\mu_{i_{\\mathrm{max}}}-\\mu_{k}\\|<\\frac{\\mu_{\\mathrm{max}}}{2}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Note that (20) implies $\\langle\\mu_{k},\\overline{{\\mu_{i_{\\operatorname*{max}}}}}\\rangle>\\frac{1}{2}\\mu_{\\operatorname*{max}}$ , so for $\\forall x\\in\\mathbf{R}^{d}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\tilde{\\psi}_{\\mu}(x)\\|=\\left\\|\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}\\right\\|\\geq\\left\\langle\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k},\\overline{{\\mu_{i_{\\mathrm{max}}}}}\\right\\rangle=\\sum_{k\\in[n]}\\psi_{k}(x)\\left\\langle\\mu_{k},\\overline{{\\mu_{i_{\\mathrm{max}}}}}\\right\\rangle>\\frac{1}{2}\\mu_{\\mathrm{max}},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used $\\textstyle\\sum_{k\\in[n]}\\psi_{k}(x)=1$ at the last inequality. ", "page_idx": 17}, {"type": "text", "text": "Lemma 11. For any $G M M(\\mu)$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{E}_{x}\\left[\\Vert\\tilde{\\psi}_{\\mu}(x)\\Vert^{2}\\right]\\ge\\frac{\\exp{(-8U)}}{40000d(1+2\\mu_{\\mathrm{max}}\\sqrt{d})^{2}}\\left(\\sum_{i,j\\in[n]}\\pi_{i}\\pi_{j}\\Vert\\mu_{i}-\\mu_{j}\\Vert^{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. The key idea is to consider the gradient of $\\tilde{\\psi}_{\\mu}$ , which can be calculated as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{x}\\tilde{\\psi}_{\\mu}(x)=\\displaystyle\\sum_{i}\\mu_{i}\\left(\\frac{\\partial\\psi_{i}(x)}{\\partial x}\\right)^{\\top}}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{i}\\psi_{i}(x)\\mu_{i}\\mu_{i}^{\\top}-\\sum_{i,j}\\psi_{i}(x)\\psi_{j}(x)\\mu_{i}\\mu_{j}^{\\top}}\\\\ &{\\qquad=\\displaystyle\\sum_{i,j\\in[n]}\\psi_{i}(x)\\psi_{j}(x)\\mu_{i}\\mu_{i}^{\\top}-\\sum_{i,j}\\psi_{i}(x)\\psi_{j}(x)\\mu_{i}\\mu_{j}^{\\top}}\\\\ &{\\qquad=\\displaystyle\\sum_{i,j\\in[n]}\\psi_{i}(x)\\psi_{j}(x)\\mu_{i}(\\mu_{i}-\\mu_{j})^{\\top}}\\\\ &{\\qquad=\\displaystyle\\sum_{i,j\\in[n]}\\psi_{i}(x)\\psi_{j}(x)\\mu_{i}(\\mu_{i}-\\mu_{j})^{\\top}}\\\\ &{\\qquad=\\displaystyle\\sum_{i,j\\in[n]}\\psi_{i}(x)\\psi_{j}(x)\\frac{1}{2}\\left(\\mu_{i}(\\mu_{i}-\\mu_{j})^{\\top}+\\mu_{j}(\\mu_{j}-\\mu_{i})^{\\top}\\right)}\\\\ &{\\qquad=\\displaystyle\\sum_{i,j\\in[n]}\\psi_{i}(x)\\psi_{j}(x)(\\mu_{i}-\\mu_{j})(\\mu_{i}-\\mu_{j})^{\\top},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used (8) in the second identity. ", "page_idx": 17}, {"type": "text", "text": "By Cauchy-Schwarz inequality, we have $\\begin{array}{r}{\\|a\\|^{2}+\\|b\\|^{2}\\geq\\frac{1}{2}\\|a-b\\|^{2}}\\end{array}$ , which implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{E}_{x}\\left[\\Vert\\tilde{\\Psi}_{\\mu}(x)\\Vert^{2}\\right]=\\frac{1}{2}\\mathbf{E}_{x}\\left[\\Vert\\tilde{\\Psi}_{\\mu}(x)\\Vert^{2}+\\Vert\\tilde{\\Psi}_{\\mu}(-x)\\Vert^{2}\\right]}&{}\\\\ {\\ge\\frac{1}{4}\\mathbf{E}_{x}\\left[\\Vert\\tilde{\\Psi}_{\\mu}(x)-\\tilde{\\Psi}_{\\mu}(-x)\\Vert^{2}\\right]}&{}\\\\ {\\ge\\frac{1}{4}\\mathbf{E}_{x}\\left[\\langle\\tilde{\\Psi}_{\\mu}(x)-\\tilde{\\Psi}_{\\mu}(-x),\\overline{{x}}\\rangle^{2}\\right]}&{}\\\\ {=\\frac{1}{4}\\mathbf{E}_{x}\\left[\\left(\\int_{t=-1}^{1}\\frac{\\partial}{\\partial t}\\langle\\tilde{\\Psi}_{\\mu}(t,x),\\overline{{x}}\\rangle\\mathrm{d}t\\right)^{2}\\right]}&{}\\\\ {=\\frac{1}{4}\\mathbf{E}_{x}\\left[\\left(\\int_{t=-1}^{1}x^{\\top}\\nabla\\tilde{\\Psi}_{\\mu}(t,x)\\overline{{x}}\\mathrm{d}t\\right)^{2}\\right]}&{}\\\\ {=\\frac{1}{4}\\mathbf{E}_{x}\\left[\\left(\\int_{t=-1}^{1}\\Vert x\\Vert\\cdot\\overline{{x}}^{\\top}\\nabla\\tilde{\\Psi}_{\\mu}(t,x)\\overline{{x}}\\mathrm{d}t\\right)^{2}\\right],}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\frac{\\partial}{\\partial t}\\tilde{\\psi}_{\\mu}(t x)=\\nabla\\tilde{\\psi}_{\\mu}(t x)x}\\end{array}$ at the second to last identity. Careful readers might notice that the term $\\begin{array}{r}{\\left(\\int_{t=-1}^{1}\\|x\\|\\cdot\\overline{{x}}^{\\top}\\nabla\\tilde{\\psi}_{\\mu}(t x)\\overline{{x}}\\mathrm{d}t\\right)^{2}}\\end{array}$ is not well-defined when $x\\,=\\,0$ , but we can still calculate its expectation over the whole probability space since the integration is only singular on a zero-measure set. ", "page_idx": 18}, {"type": "text", "text": "For each $x\\neq0$ , by (22) we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{x}}^{\\top}\\nabla\\tilde{\\psi}_{\\mu}(t x)\\overline{{x}}=\\frac{1}{2}\\sum_{i,j\\in[n]}\\psi_{i}(t x)\\psi_{j}(t x)\\langle\\mu_{i}-\\mu_{j},\\overline{{x}}\\rangle^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "So ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\sigma}\\left[\\big|\\widehat{\\mathbf{y}}_{{\\widehat{\\mathbf{s}}}}(\\mu)\\big|^{2}\\right]}\\\\ &{\\geq\\frac{1}{16}\\mathbb{E}_{\\sigma}\\left[\\left(\\int_{t_{0}}^{t}\\Big|\\sum_{i,j\\in[n]}^{\\sigma}\\mathrm{e}_{i}(\\mu)\\mathrm{e}_{j}(\\alpha)\\big|\\mu_{i}-\\mu_{j},\\eta^{2}\\mathrm{2}\\mathrm{d}t\\right)^{2}\\right]}\\\\ &{=\\frac{1}{16}\\mathbb{E}_{\\sigma}\\left[\\left(\\prod_{i}\\sum_{j\\in[n]}^{\\sigma}\\big(\\mu_{i}-\\mu_{j},\\eta^{2}\\big)^{2}\\int_{t_{0}-1}^{t}\\mathrm{e}_{i}(\\mu)\\mathrm{e}_{j}(\\mu)\\mathrm{d}t\\right)^{2}\\right]}\\\\ &{\\geq\\frac{1}{16}\\mathbb{E}_{\\sigma}\\left[\\left(\\prod_{i}\\sum_{j\\in[n]}^{\\sigma}\\big(\\mu_{i}-\\mu_{j},\\eta^{2}\\big)^{2}\\frac{1}{2\\mu_{i}\\mu_{i}\\prod_{j}^{\\sigma}}\\mathrm{e}_{j}(\\nu)(-\\delta t)\\big(1-\\mathrm{exp}\\big(-4\\mu_{n n}\\big|\\tau\\big)\\big)\\right)^{2}\\right]}\\\\ &{=\\frac{\\mathrm{exp}(\\sigma-\\delta t)}{64}\\mathbb{E}_{\\sigma}\\left[\\left(\\sum_{i,j\\in[n]}^{\\sigma}\\sigma_{i},\\eta^{2}\\big(\\mu_{i}-\\mu_{j},\\eta^{2}\\big)^{2}\\frac{1-\\mathrm{exp}\\big(-4\\mu_{n n}\\big|\\tau\\big)}{\\mu_{i}\\mu_{i}}\\right)^{2}\\right]}\\\\ &{\\geq\\frac{\\mathrm{exp}(\\sigma-\\delta t)}{64}\\left(\\sum_{i,j\\in[n]}^{\\sigma}\\mathbb{E}_{\\sigma}\\big[\\mu_{i}-\\mu_{j},\\eta^{2}\\big(\\mu_{i}-\\mu_{j},\\eta^{2}\\big)^{2}\\frac{1-\\mathrm{exp}\\big(-4\\mu_{n n}\\big)\\big|\\eta-\\eta^{2}\\big(\\mu_{i}-\\mu_{j},\\eta^{2}\\big)}{\\mu_{i}\\mu_{i}}\\big]\\right)^{2}}\\\\ &{\\geq\\frac{\\mathrm{exp}(\\sigma-\\delta t\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used Lemma 18 at the fourth line and Cauchy-Schwarz inequality at the last line. ", "page_idx": 18}, {"type": "text", "text": "The last step is to lower bound $\\mathbf{E}_{x}\\left[\\langle\\mu_{i}-\\mu_{j},\\overline{{x}}\\rangle^{2}\\left(1-\\exp\\left(-4\\mu_{\\operatorname*{max}}\\|x\\|\\right)\\right)/\\mu_{\\operatorname*{max}}\\right]$ . Since $x$ is sampled from $\\mathcal{N}(0,I_{d})$ , which is spherically symmetric, we know that the two random variables $\\{\\overline{{x}},\\|x\\|\\}$ ", "page_idx": 18}, {"type": "text", "text": "are independent. Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}_{x}\\left[\\langle\\mu_{i}-\\mu_{j},\\overline{{x}}\\rangle^{2}\\frac{1-\\exp\\left(-4\\mu_{\\operatorname*{max}}\\lVert x\\rVert\\right)}{\\mu_{\\operatorname*{max}}}\\right]=\\mathbf{E}_{x}\\left[\\langle\\mu_{i}-\\mu_{j},\\overline{{x}}\\rangle^{2}\\right]\\mathbf{E}_{x}\\left[\\frac{1-\\exp\\left(-4\\mu_{\\operatorname*{max}}\\lVert x\\rVert\\right)}{\\mu_{\\operatorname*{max}}}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For the first term in (25), we have $\\mathbf{E}_{x}\\left[\\langle\\mu_{i}-\\mu_{j},\\overline{{x}}\\rangle^{2}\\right]\\;=\\;\\|\\mu_{i}\\,-\\,\\mu_{j}\\|^{2}/d$ since $\\overline{{x}}$ is spherically symmetrically distributed. By norm-concentration inequality of Gaussian [Dasgupta and Schulman, 2000] we know that $\\begin{array}{r}{\\operatorname*{Pr}\\left[\\lvert\\lvert x\\rvert\\rvert\\ge\\frac{\\sqrt{d}}{2}\\right]\\ge1/50,\\forall d}\\end{array}$ . The second term in (25) can be therefore lower bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{S}_{x}\\left[\\frac{1-\\exp\\left(-4\\mu_{\\operatorname*{max}}\\|x\\|\\right)}{\\mu_{\\operatorname*{max}}}\\right]\\ge\\operatorname*{Pr}\\left[\\|x\\|\\ge\\frac{\\sqrt{d}}{2}\\right]\\frac{1-\\exp\\left(-4\\mu_{\\operatorname*{max}}\\cdot\\frac{\\sqrt{d}}{2}\\right)}{\\mu_{\\operatorname*{max}}}\\ge\\frac{1-\\exp\\left(-2\\mu_{\\operatorname*{max}}\\sqrt{d}\\right)}{50\\mu_{\\operatorname*{max}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Plugging (26) into (25), we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}_{x}\\left[\\langle\\mu_{i}-\\mu_{j},\\overline{{x}}\\rangle^{2}\\frac{1-\\exp\\left(-4\\mu_{\\operatorname*{max}}\\|x\\|\\right)}{\\mu_{\\operatorname*{max}}}\\right]\\ge\\frac{1-\\exp\\left(-2\\mu_{\\operatorname*{max}}\\sqrt{d}\\right)}{50d\\mu_{\\operatorname*{max}}}\\|\\mu_{i}-\\mu_{j}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now we can plug (27) into (24) and get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{E}_{x}\\left[\\Vert\\tilde{\\psi}_{\\mu}(x)\\Vert^{2}\\right]\\geq\\frac{\\exp{(-8U)}}{64}\\left(\\sum_{i,j\\in[n]}\\pi_{i}\\pi_{j}\\mathrm{E}_{x}\\left[\\langle\\mu_{i}-\\mu_{j},\\bar{x}\\rangle^{2}\\frac{1-\\exp{(-4\\mu_{\\operatorname*{max}}\\|x\\|)}}{\\mu_{\\operatorname*{max}}}\\right]\\right)^{2}}\\\\ &{\\geq\\frac{\\exp{(-8U)}}{64}\\left(\\sum_{i,j\\in[n]}\\frac{\\pi_{i}\\pi_{j}}{50d\\mu_{\\operatorname*{max}}}\\frac{1-\\exp{(-2\\mu_{\\operatorname*{max}}\\sqrt{d})}}{50d\\mu_{\\operatorname*{max}}}\\Vert\\mu_{i}-\\mu_{j}\\Vert^{2}\\right)^{2}}\\\\ &{\\geq\\frac{\\exp{(-8U)}}{64}\\left(\\sum_{i,j\\in[n]}\\frac{1-\\frac{1}{5}(2\\mu_{\\operatorname*{max}}\\sqrt{d})}{50d\\mu_{\\operatorname*{max}}}\\Vert\\mu_{i}-\\mu_{j}\\Vert^{2}\\right)^{2}}\\\\ &{=\\frac{\\exp{(-8U)}}{4000d(1+2\\mu_{\\operatorname*{max}}\\sqrt{d})^{2}}\\left(\\sum_{i,j\\in[n]}\\pi_{i}\\pi_{j}\\Vert\\mu_{i}-\\mu_{j}\\Vert^{2}\\right)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the inequality $\\begin{array}{r}{\\forall t\\geq0,e^{-t}\\leq\\frac{1}{1+t}}\\end{array}$ at the second to last line. ", "page_idx": 19}, {"type": "text", "text": "Theorem 2. Consider training $a$ student $n$ -component GMM initialized from $\\begin{array}{r l r}{\\pmb{\\mu}(0)}&{{}=}&{}\\end{array}$ $(\\mu_{1}(0)^{\\top},\\ldots,\\mu_{n}(0)^{\\top})^{\\top}$ to learn a single-component ground truth GMM $\\mathcal{N}(0,I_{d})$ with population gradient EM algorithm. If the step size satisfies $\\begin{array}{r}{\\eta\\le O\\left(\\frac{\\exp(-8U(0))\\pi_{\\operatorname*{min}}^{2}}{n^{2}d^{2}(\\frac{1}{\\mu_{\\operatorname*{max}}(0)}+\\mu_{\\operatorname*{max}}(0))^{2}}\\right)}\\end{array}$ , then gradient EM converges globally with rate ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{\\mu}(t))\\leq\\frac{1}{\\sqrt{\\gamma t}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma=\\Omega\\left(\\frac{\\eta\\exp(-16U(0))\\pi_{\\operatorname*{min}}^{4}}{n^{2}d^{2}(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{d n})^{4}}\\right)\\in\\mathbf{R}^{+}}\\end{array}$ . Recall that $\\mu_{\\operatorname*{max}}(0)=\\operatorname*{max}\\{\\|\\mu_{1}(0)\\|,\\ldots,\\|\\mu_{n}(0)\\|\\}$ and $\\begin{array}{r}{U(0)=\\sum_{i\\in[n]}\\|\\mu_{i}(0)\\|^{2}}\\end{array}$ are two initialization constants. ", "page_idx": 19}, {"type": "text", "text": "Proof. We use mathematical induction to prove Theorem 2, by proving the following two conditions inductively: ", "page_idx": 19}, {"type": "equation", "text": "$$\nU(t)\\leq U(0)=\\sum_{i\\in[n]}\\|\\mu_{i}(0)\\|^{2},\\forall t.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{\\mathscr{L}^{2}(\\mu(t))}\\geq\\gamma t+\\frac{1}{\\mathscr{L}^{2}(\\mu(0))},\\forall t.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that (30) directly implies the theorem, so now we just need to prove (29) and (30) together. ", "page_idx": 20}, {"type": "text", "text": "The induction base for $t=0$ is trivial. Now suppose the conditions\u221a hold for time step $t$ , consider $t+1$ . By induction hypothesis (29) we have $\\Vert\\bar{\\mu_{i}(t)}\\Vert\\leq\\mu_{\\operatorname*{max}}(t)\\leq\\sqrt{n}\\mu_{\\operatorname*{max}}(0),\\forall t$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of (30). Since $\\nabla_{\\mu}Q(\\mu|\\mu)=\\nabla_{\\mu}\\mathcal{L}(\\mu)$ , we can apply classical analysis of gradient descent [Nesterov et al., 2018] as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lefteqn{\\mathcal{L}(\\mu(t+1))-\\mathcal{L}(\\mu(t))}}\\\\ &{=\\mathcal{L}(\\mu(t)-\\eta\\nabla\\mathcal{L}(\\mu(t)))-\\mathcal{L}(\\mu(t))}\\\\ &{=-\\int_{s=0}^{1}\\left\\langle\\nabla\\mathcal{L}(\\mu(t)-s\\eta\\nabla\\mathcal{L}(\\mu(t))),\\eta\\nabla\\mathcal{L}(\\mu(t))\\right\\rangle\\mathrm{d}s}\\\\ &{=-\\displaystyle\\int_{s=0}^{1}\\left\\langle\\nabla\\mathcal{L}(\\mu(t)),\\eta\\nabla\\mathcal{L}(\\mu(t))\\right\\rangle\\mathrm{d}s+\\int_{s=0}^{1}\\left\\langle\\nabla\\mathcal{L}(\\mu(t))-\\nabla\\mathcal{L}(\\mu(t)-s\\eta\\nabla\\mathcal{L}(\\mu(t))),\\eta\\nabla\\mathcal{L}(\\mu(t))\\right\\rangle}\\\\ &{=-\\eta\\|\\nabla\\mathcal{L}(\\mu(t))\\|^{2}+\\eta\\displaystyle\\int_{s=0}^{1}\\left\\langle\\nabla\\mathcal{L}(\\mu(t))-\\nabla\\mathcal{L}(\\mu(t)-s\\eta\\nabla\\mathcal{L}(\\mu(t))),\\nabla\\mathcal{L}(\\mu(t))\\right\\rangle\\mathrm{d}s}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the gradient norm can be upper bounded as ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))\\|=\\left\\|\\displaystyle\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}(t)\\right]\\right\\|\\leq\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\sum_{k\\in[n]}\\psi_{k}(x)\\,\\|\\mu_{k}(t)\\|\\right]}\\\\ &{\\qquad\\qquad\\leq\\sum_{k}\\|\\mu_{k}(t)\\|\\leq\\sqrt{n U(t)}\\leq n\\mu_{\\operatorname*{max}}(0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then for any $s\\in[0,1]$ , we have $\\begin{array}{r}{\\|s\\eta\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))\\|\\,\\le\\,\\eta n\\mu_{\\mathrm{max}}(0)\\,\\le\\,\\frac{1}{\\operatorname*{max}\\{6d,2\\|\\mu_{i}(t)\\|\\}}}\\end{array}$ . So we can apply Theorem 13 and get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))-\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t)-s\\eta\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t)))\\|}\\qquad}&{}\\\\ &{\\leq n\\mu_{\\operatorname*{max}}(t)(30\\sqrt{d}+4\\mu_{\\operatorname*{max}}(t))\\|s\\eta\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))\\|+\\displaystyle\\sum_{k\\in[n]}\\|s\\eta\\nabla_{\\mu_{k}}\\mathcal{L}(\\mu(t))\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore for $\\forall s\\in[0,1]$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\langle\\nabla\\mathcal{L}(\\mu(t))-\\nabla\\mathcal{L}(\\mu(t)-s\\eta\\nabla\\mathcal{L}(\\mu(t))),\\nabla\\mathcal{L}(\\mu(t))\\rangle}\\\\ &{\\leq\\displaystyle\\sum_{i\\in[n]}\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))-\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t)-s\\eta\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t)))\\|\\cdot\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))\\|}\\\\ &{\\leq\\displaystyle\\sum_{i\\in[n]}\\left(n\\mu_{\\operatorname*{max}}(t)(30\\sqrt{d}+4\\mu_{\\operatorname*{max}}(t))\\|s\\eta\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))\\|+\\displaystyle\\sum_{k\\in[n]}\\|s\\eta\\nabla_{\\mu_{k}}\\mathcal{L}(\\mu(t))\\|\\right)\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))\\|}\\\\ &{\\leq\\eta\\left(n\\mu_{\\operatorname*{max}}(t)(30\\sqrt{d}+4\\mu_{\\operatorname*{max}}(t))+n^{2}\\right)\\|\\nabla\\mathcal{L}(\\mu(t))\\|^{2}}\\\\ &{\\leq\\eta\\left(4\\ln^{2}\\mu_{\\operatorname*{max}}(0)^{2}+30\\sqrt{d}n^{3/2}\\mu_{\\operatorname*{max}}(0)+n^{2}\\right)\\|\\nabla\\mathcal{L}(\\mu(t))\\|^{2}}\\\\ &{\\leq20\\eta\\sqrt{d}n^{2}(\\mu_{\\operatorname*{max}}^{2}(0)+1)\\|\\nabla\\mathcal{L}(\\mu(t))\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Plugging (32) into (31), since $\\begin{array}{r}{\\eta\\le O\\left(\\frac{1}{\\sqrt{d}n^{2}(\\mu_{\\operatorname*{max}}^{2}(0)+1)}\\right)}\\end{array}$ we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma(\\mu(t+1))\\!-\\!\\mathcal{L}(\\mu(t))\\leq-\\eta\\Vert\\nabla\\mathcal{L}(\\mu(t))\\Vert^{2}\\!+\\!20\\eta\\sqrt{d}n^{2}(\\mu_{\\operatorname*{max}}^{2}(0)\\!+\\!1)\\Vert\\nabla\\mathcal{L}(\\mu(t))\\Vert^{2}\\leq-\\frac{\\eta}{2}\\Vert\\nabla\\mathcal{L}(\\mu(t))\\Vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla\\mathcal{L}(\\mu(t))\\|\\ge\\frac{\\langle\\nabla\\mathcal{L}(\\mu(t)),\\,\\mu(t)\\rangle}{\\|\\mu(t)\\|}\\ge\\frac{\\langle\\nabla\\mathcal{L}(\\mu(t)),\\,\\mu(t)\\rangle}{n\\mu_{\\operatorname*{max}}(t)}\\ge\\Omega\\left(\\frac{\\exp\\left(-8U(t)\\right)\\pi_{\\operatorname*{min}}^{2}}{n d(1+\\mu_{\\operatorname*{max}}(t)\\sqrt{d})^{2}}\\right)\\mu_{\\operatorname*{max}}^{3}(t)}\\\\ &{\\overset{\\mathrm{Theorem}\\,14}{\\ge}\\Omega\\left(\\frac{\\exp\\left(-8U(t)\\right)\\pi_{\\operatorname*{min}}^{2}}{n d(1+\\mu_{\\operatorname*{max}}(t)\\sqrt{d})^{2}}\\right)(2\\mathcal{L}(\\mu(t))^{3/2}\\ge\\Omega\\left(\\frac{\\exp\\left(-8U(0)\\right)\\pi_{\\operatorname*{min}}^{2}}{n d(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{d n})^{2}}\\right)\\mathcal{L}^{3/2}(\\mu(t)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining (34) and (33), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Sigma(\\mu(t+1))\\leq\\mathcal{L}(\\mu(t))-\\frac{\\eta}{2}\\|\\nabla\\mathcal{L}(\\mu(t))\\|^{2}\\leq\\mathcal{L}(\\mu(t))-\\Omega\\left(\\frac{\\eta\\exp\\left(-16U(0)\\right)\\pi_{\\operatorname*{min}}^{4}}{n^{2}d^{2}(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{d n})^{4}}\\right)\\mathcal{L}^{3}(\\mu(t)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that the above inequality implies $\\mathcal{L}(\\pmb{\\mu}(t+1))\\leq\\mathcal{L}(\\pmb{\\mu}(t))$ , therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{\\mathcal{L}^{2}(\\mu(t+1))}-\\frac{1}{\\mathcal{L}^{2}(\\mu(t))}=\\frac{\\left(\\mathcal{L}(\\mu(t))-\\mathcal{L}(\\mu(t+1))\\right)\\left(\\mathcal{L}(\\mu(t))+\\mathcal{L}(\\mu(t+1))\\right)}{\\mathcal{L}^{2}(\\mu(t))\\mathcal{L}^{2}(\\mu(t+1))}}\\\\ &{\\geq\\frac{\\left(\\mathcal{L}(\\mu(t))-\\mathcal{L}(\\mu(t+1))\\mathcal{L}(\\mu(t))\\right.\\overset{(35)}{\\geq}\\Omega\\left(\\frac{\\eta\\exp{\\left(-16U(0)\\right)\\pi_{\\operatorname*{min}}^{4}}}{n^{2}d^{2}(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{d n})^{4}}\\right)=\\gamma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, by induction hypothesis we have $\\begin{array}{r}{\\frac{1}{\\mathscr{L}^{2}(\\pmb{\\mu}(t))}\\geq\\gamma t+\\frac{1}{\\mathscr{L}^{2}(\\pmb{\\mu}(0))}}\\end{array}$ , combined with the above inequality, we have $\\begin{array}{r}{\\frac{1}{\\mathcal{L}^{2}(\\mu(t+1))}\\,\\geq\\,\\frac{1}{\\mathcal{L}^{2}(\\mu(t))}+\\gamma\\,\\geq\\,\\gamma(t+1)+\\frac{1}{\\mathcal{L}^{2}(\\mu(0))}}\\end{array}$ , which finishes the proof of (30). ", "page_idx": 21}, {"type": "text", "text": "Proof of (29). The dynamics of potential function $U$ can be calculated as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~{\\cal U}(\\mu(t+1))=\\displaystyle\\sum_{i\\in[n]}\\|\\mu_{i}(t+1)\\|^{2}}\\\\ &{=\\displaystyle\\sum_{i\\in[n]}\\|\\mu_{i}(t)-\\eta\\nabla_{\\mu_{i}}Q(\\mu(t)|\\mu(t))\\|^{2}}\\\\ &{=\\displaystyle{\\cal U}(\\mu(t))-\\eta\\sum_{i\\in[n]}\\langle\\mu_{i}(t),\\nabla_{\\mu_{i}}Q(\\mu(t)|\\mu(t))\\rangle+\\eta^{2}\\displaystyle\\sum_{i\\in[n]}\\|\\nabla_{\\mu_{i}}Q(\\mu(t)|\\mu(t))\\|^{2}}\\\\ &{\\overset{\\mathrm{Coellay~l0}}{=}{\\displaystyle\\sum_{i\\in[n]}({\\cal U}(\\mu(t))-\\underbrace{\\eta\\mathbf{E}_{x}\\left[\\|\\tilde{\\psi}_{\\mu(t)}(x)\\|^{2}\\right]}_{I_{1}}+\\underbrace{\\eta^{2}\\sum_{i\\in[n]}\\|\\nabla_{\\mu_{i}}Q(\\mu(t)|\\mu(t))\\|^{2}}_{I_{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By induction hypothesis, the first term $I_{1}$ can be bounded by Lemma 12 as ", "page_idx": 21}, {"type": "equation", "text": "$$\nI_{1}\\geq\\eta\\Omega\\left(\\frac{\\exp\\left(-8U(t)\\right)\\pi_{\\operatorname*{min}}^{2}}{d(1+\\mu_{\\operatorname*{max}}(t)\\sqrt{d})^{2}}\\right)\\mu_{\\operatorname*{max}}^{4}(t)\\geq\\eta\\Omega\\left(\\frac{\\exp\\left(-8U(0)\\right)\\pi_{\\operatorname*{min}}^{2}}{n^{2}d(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{n d})^{2}}\\right)U^{2}(\\mu(t)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The second term $I_{2}$ is a perturbation term that can be upper bounded by Lemma 9 as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{t}_{\\theta}=\\gamma\\frac{\\gamma}{\\alpha+\\theta}\\left[\\nabla_{\\theta}\\alpha(\\theta)(t)\\theta(t)\\right]^{1/2}=\\eta^{2}\\frac{\\sqrt{5}}{\\alpha+\\theta}\\left[\\kappa\\left[\\kappa\\left(\\eta\\right)\\sum_{i=1}^{\\infty}e_{i}(x)\\log\\left(a\\right)\\right]^{1}\\right]}\\\\ &{\\le\\eta^{2}\\sum_{i=1}^{\\infty}\\left[\\left\\|\\kappa(\\eta)\\sum_{i=1}^{\\infty}(x)\\left(x)\\right\\|_{i}^{1}\\right]}\\\\ &{\\le\\eta^{4}\\left[\\kappa\\left(\\eta\\right)\\sum_{i=1}^{\\infty}(x)\\left(\\log\\left(a\\right)\\right)\\right]^{2}}\\\\ &{\\le\\eta^{2}\\sum_{i=1}^{\\infty}\\left[\\kappa\\left(\\eta\\right)\\sum_{i=1}^{\\infty}(x)\\left(\\log\\left(a\\right)\\right)^{2}\\right]}\\\\ &{\\le\\eta^{2}\\sum_{i=1}^{\\infty}\\left[\\left(\\sum_{i=1}^{\\infty}e_{i}(x)e^{\\eta}\\right)\\left(\\sum_{i=1}^{\\infty}\\left[\\log(\\eta)\\right]^{2}\\right)^{2}\\right]}\\\\ &{\\le\\eta^{2}\\sum_{i=1}^{\\infty}\\left[\\kappa\\left(\\eta\\right)\\sum_{i=1}^{\\infty}(x)\\left(\\log\\left(a\\right)\\right)\\left(\\sum_{i=1}^{\\infty}\\left[\\log(\\eta)\\right]^{2}\\right)^{2}\\right]}\\\\ &{\\le\\eta^{2}\\frac{\\sqrt{5}}{\\alpha+\\theta}\\left[\\kappa\\left(\\frac{\\eta}{\\alpha+\\eta}\\right)e_{i}^{\\eta}\\left(\\log(x)\\right)e_{i}\\left[\\kappa\\left[\\sum_{i=1}^{\\infty}(\\log(\\eta)\\right]^{2}\\right)\\right.}\\\\ &{\\qquad\\left.-\\eta^{2}\\left(\\eta\\right)\\left(\\log(1\\right)\\sum_{i=1}^{\\infty}\\exp(\\eta)e_{i}^{\\eta}\\left(\\log(x)\\right)\\right]}\\\\ &{\\le\\eta^{6}\\eta^{6}\\left(\\log(1)\\right)\\kappa\\left[\\left(\\sum_{i=1}^{\\infty}\\log\\left(a\\right)\\right)\\left(\\sum_{i=1}^{\\infty}e_{i}(x)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we use triangle inequality twice at the second and third line, and Cauchy-Schwarz inequality twice at the fourth and fifth line. ", "page_idx": 22}, {"type": "text", "text": "Putting (38), (37) and (36) together, we get ", "page_idx": 22}, {"type": "equation", "text": "$$\nU(\\mu(t+1))\\le U(\\mu(t))-\\eta\\Omega\\left(\\frac{\\exp\\left(-8U(0)\\right)\\pi_{\\operatorname*{min}}^{2}}{n^{2}d(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{n d})^{2}}\\right)U^{2}(\\mu(t))+\\eta^{2}U(\\mu(t)).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consider two cases: ", "page_idx": 22}, {"type": "text", "text": "a). If $\\begin{array}{r}{\\frac{U(0)}{2}\\leq U(\\pmb{\\mu}(t))\\leq U(0)}\\end{array}$ , then   \n$\\begin{array}{r l}&{\\quad U(\\mu(t+1))\\leq U(\\mu(t))-\\eta U(\\mu(t))\\left(\\Omega\\left(\\frac{\\exp\\left(-8U(0)\\right)\\pi_{\\operatorname*{min}}^{2}}{n^{2}d(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{n d})^{2}}\\right)U(\\mu(t))-\\eta\\right)}\\\\ &{\\leq U(\\mu(t))-\\eta U(\\mu(t))\\left(\\Omega\\left(\\frac{\\exp\\left(-8U(0)\\right)\\pi_{\\operatorname*{min}}^{2}}{n^{2}d(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{n d})^{2}}\\right)\\frac{n}{2}\\mu_{\\operatorname*{max}}^{2}(0)-\\eta\\right)\\leq U(\\mu(t))\\leq n\\mu_{\\operatorname*{max}}^{2}(0),}\\end{array}$ note that we used $\\begin{array}{r}{\\eta\\leq O\\left(\\frac{\\exp(-8U(0))\\pi_{\\operatorname*{min}}^{2}}{n^{2}d(1+\\mu_{\\operatorname*{max}}(0)\\sqrt{n d})^{2}}\\right)\\frac{n}{2}\\mu_{\\operatorname*{max}}^{2}(0).}\\end{array}$ .   \nb). If $\\begin{array}{r}{U(\\pmb{\\mu}(t))<\\frac{1}{2}U(0)}\\end{array}$ , then $U(\\pmb{\\mu}(t+1))\\leq(1+\\eta^{2})U(\\pmb{\\mu}(t))\\leq2U(\\pmb{\\mu}(t))\\leq U(0).$ ", "page_idx": 22}, {"type": "text", "text": "Since (29) holds in both cases, our proof is done. ", "page_idx": 22}, {"type": "text", "text": "B.2 Proofs for Section 3.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma 15. For any $\\pmb{\\mu}$ satisfying $\\|\\mu_{1}\\|,\\|\\mu_{2}\\|\\geq10{\\sqrt{d}},\\|\\mu_{3}\\|,\\ldots,\\|\\mu_{n}\\|\\leq{\\sqrt{d}},$ , the gradient of $\\mathcal{L}$ at $\\pmb{\\mu}$ can be upper bounded as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu)\\|\\leq2(\\|\\mu_{3}\\|+\\cdots+\\|\\mu_{n}\\|)+2\\exp(-d)(\\|\\mu_{1}\\|+\\|\\mu_{2}\\|),\\forall i\\in[n].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Recall that the gradient has the form $\\begin{array}{r}{\\nabla_{\\mu_{i}}\\mathcal L(\\mu)={\\bf E}_{x}\\left[\\psi_{i}(x)\\sum_{k\\in[n]}\\psi_{k}(x)\\mu_{k}\\right]}\\end{array}$ , hence its norm can be upper bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu)\\|\\leq\\mathbf{E}_{x}\\left[\\psi_{i}(x)\\displaystyle\\sum_{k\\in[n]}\\psi_{k}(x)\\|\\mu_{k}\\|\\right]}\\\\ &{\\leq\\mathbf{E}_{x}\\left[\\displaystyle\\sum_{k\\in[n]}\\psi_{k}(x)\\|\\mu_{k}\\|\\left\\|x\\right\\|\\leq2\\sqrt{d}\\right]+\\mathbf{E}_{x}\\left[\\displaystyle\\sum_{k\\in[n]}\\psi_{k}(x)\\|\\mu_{k}\\|\\left\\|x\\right\\|>2\\sqrt{d}\\right]\\operatorname*{Pr}\\left[\\|x\\|>2\\sqrt{d}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any $\\lVert x\\rVert\\,\\leq\\,2{\\sqrt{d}}$ and $i\\ >\\ 2$ , we have $\\begin{array}{r}{\\exp(-\\|x-\\mu_{i}\\|^{2}/2)\\,\\geq\\,\\exp(-(\\|x\\|+\\|\\mu_{i}\\|)^{2}/2)\\,\\geq\\,}\\end{array}$ $\\exp(-9d/2)$ , whi\u221ale for $i\\;\\;\\in\\;\\;\\{1,2\\}$ , e $\\begin{array}{r l r}{\\mathrm{xp}(-\\|x\\|^{2}-\\,\\mu_{i}\\|^{2}/2)}&{\\leq}&{\\mathrm{exp}(-(\\|\\mu_{i}\\|\\,-\\,\\|x\\|)^{2}/2)}\\end{array}\\leq$ $\\exp(-(10\\sqrt{d}-2\\sqrt{d})^{2}/2)=\\exp(-32d)$ . Since $\\psi_{i}(x)\\propto\\exp(-\\|x-\\mu_{i}\\|^{2}/2)$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|x\\|\\leq2{\\sqrt{d}}\\Rightarrow\\psi_{i}(x)\\leq{\\frac{\\exp(-\\|x-\\mu_{i}\\|^{2}/2)}{\\exp(-\\|x-\\mu_{1}\\|^{2}/2)}}\\leq{\\frac{\\exp(-32d)}{\\exp(-9d/2)}}\\leq\\exp(-25d),{\\forall i}\\in\\{1,2\\}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore the first term in (36) can be bounded as $\\begin{array}{r}{\\mathbf{E}_{x}\\left[\\sum_{k\\in[n]}\\psi_{k}(x)\\|\\mu_{k}\\|\\left|\\|x\\|\\leq2\\sqrt{d}\\right]\\leq(\\|\\mu_{3}\\|+\\right.}\\end{array}$ $\\begin{array}{r}{\\cdot\\cdot\\cdot+\\|\\mu_{n}\\|)+\\exp(-25d)(\\|\\mu_{1}\\|+\\|\\mu_{2}\\|).}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "On the other hand, by tail bound of the norm of Gaussian vectors (see Lemma 8 of [Yan et al., 2017]) we have $\\operatorname*{Pr}\\left[\\|x\\|>2{\\sqrt{d}}\\right]\\leq\\exp(-d)$ . Putting everything together, (39) can be further bounded as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu)\\|\\leq(\\|\\mu_{3}\\|+\\cdots+\\|\\mu_{n}\\|)+\\exp(-25d)(\\|\\mu_{1}\\|+\\|\\mu_{2}\\|)+\\exp(-d)\\displaystyle\\sum_{i\\in[n]}\\|\\mu_{i}\\|}\\\\ &{\\qquad\\qquad\\leq2(\\|\\mu_{3}\\|+\\cdots+\\|\\mu_{n}\\|)+2\\exp(-d)(\\|\\mu_{1}\\|+\\|\\mu_{2}\\|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Th\u221aeorem 7. For any \u221a $n\\ \\geq\\ 3,$ , define $\\tilde{\\pmb{\\mu}}(0)\\pmb{\\Sigma}=\\pmb{\\Sigma}(\\mu_{1}^{\\top}(0),\\pmb{\\Sigma}\\cdot\\pmb{\\Sigma},\\mu_{n}^{\\top}(0))$ as follows: $\\mu_{1}(0)\\ =$ $12{\\sqrt{d}}e_{1},\\mu_{2}(0)=-12{\\sqrt{d}}e_{1},\\mu_{3}(0)=\\cdots=\\mu_{n}(0)=0$ , where $e_{1}$ is a standard unit vector. Then population gradient EM initialized with means $\\tilde{\\pmb{\\mu}}(0)$ and equal weights $\\pi_{1}\\,=\\,.\\,.\\,=\\,\\pi_{n}\\,=\\,1/n$ will be trapped in a bad local region around $\\tilde{\\pmb{\\mu}}(0)$ for exponentially long time $\\begin{array}{r l r}{T\\!\\!\\!}&{{}=}&{\\!\\!\\!\\frac{1}{30\\eta}e^{d}\\,=}\\end{array}$ $\\textstyle{\\frac{1}{30\\eta}}\\exp(\\Theta(U(0)))$ . More rigorously, for any $0\\leq t\\leq T,\\exists i\\in[n]$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mu_{i}(t)\\|\\geq10\\sqrt{d},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. We prove the following statement inductively: $\\forall\\,0\\leq t\\leq T$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{1}(t)+\\mu_{2}(t)=0,\\mu_{3}(t)=\\dots=\\mu_{n}(t)=0}\\\\ &{\\qquad\\forall i,\\ \\lVert\\mu_{i}(t)-\\mu_{i}(0)\\rVert\\leq\\eta t(60\\sqrt{d}e^{-d}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "(40) states that during the gradient EM update, $\\mu_{1}$ will keep stationary at 0. while the symmetry between $\\mu_{2},\\ldots,\\mu_{n}$ will be preserved. ", "page_idx": 23}, {"type": "text", "text": "The induction base is trivial. Now suppose (41), (40) holds for $0,1,\\ldots,t$ , we prove the case for $t+1$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of (40). Due to the induction hypothesis, one can see from direct calculation that $\\forall x$ , we have $\\psi_{i}(x|\\pmb\\mu(t))=\\psi_{i}(-x|\\pmb\\mu(t))$ for $i=3,\\ldots,n$ , and $\\psi_{1}(x|\\pmb{\\mu}(t))=\\psi_{2}(-x|\\pmb{\\mu}(t))$ . ", "page_idx": 23}, {"type": "text", "text": "Consequently for $\\forall i>2$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))=\\mathbf{E}_{x}\\left[\\psi_{i}(x|\\mu(t))\\displaystyle\\sum_{k\\in[n]}\\psi_{k}(x|\\mu(t))\\mu_{k}(t)\\right]=\\mathbf{E}_{x}\\left[\\psi_{i}(x)(\\psi_{1}(x)\\mu_{1}(t)+\\psi_{2}(x)\\mu_{2}(t))\\right]}\\\\ {=\\frac{1}{2}\\mathbf{E}_{x}\\left[\\psi_{i}(x)(\\psi_{1}(x)\\mu_{1}(t)+\\psi_{2}(x)\\mu_{2}(t))+\\psi_{i}(-x)(\\psi_{1}(-x)\\mu_{1}(t)+\\psi_{2}(-x)\\mu_{2}(t))\\right]}\\\\ {=\\frac{1}{2}\\mathbf{E}_{x}\\left[\\psi_{i}(x)(\\psi_{1}(x)(\\mu_{1}(t)+\\mu_{2}(t))+\\psi_{2}(x)(\\mu_{2}(t)+\\mu_{1}(t)))\\right]=0\\Rightarrow\\mu_{1}i t+1)=\\mu_{i}(t)=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Similarly, for $\\mu_{1},\\mu_{2}$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mu_{1}}\\mathcal{L}(\\mu(t))=\\mathbf{E}_{x}\\left[\\psi_{1}(x|\\mu(t))\\displaystyle\\sum_{k\\in[n]}\\psi_{k}(x|\\mu(t))\\mu_{k}(t)\\right]=\\mathbf{E}_{x}\\left[\\psi_{1}(x)(\\psi_{1}(x)\\mu_{1}+\\psi_{2}(x)\\mu_{2})\\right]}\\\\ &{=\\mathbf{E}_{x}\\left[\\psi_{2}(-x)(\\psi_{2}(-x)\\mu_{1}+\\psi_{1}(-x)\\mu_{2})\\right]=-\\mathbf{E}_{x}\\left[\\psi_{2}(-x)(\\psi_{2}(-x)\\mu_{2}+\\psi_{1}(-x)\\mu_{1})\\right]=-\\nabla_{\\mu_{2}}\\mathcal{L}(\\mu(t),\\lambda(\\psi_{2}(-x)\\mu_{1}+\\psi_{2}(x)\\mu_{2})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This combined with the induction hypothesis implies $\\mu_{2}(t+1)=-\\mu_{1}(t+1)$ , (40) is proved. ", "page_idx": 24}, {"type": "text", "text": "Proof of (41). ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "By induction hypothesis, we have $\\forall i$ , $\\|\\mu_{i}(t)-\\mu_{i}(0)\\|\\leq\\eta t\\cdot(60\\sqrt{d}e^{-d})\\leq\\eta T\\cdot(60\\sqrt{d}e^{-d})\\leq2\\sqrt{d}.$ $\\mathrm{So}\\,\\forall i\\in\\{1,2\\},\\|\\mu_{i}(t)\\|\\leq\\|\\mu_{i}(0)\\|+2{\\sqrt{d}}<15{\\sqrt{d}}$ . Then by Lemma 15, $\\forall i\\in[n]$ we have \u2225 $\\nabla_{\\mu_{i}}{\\mathcal L}(\\mu(t))\\|\\leq2(\\|\\mu_{3}\\|+\\cdots+\\|\\mu_{n}\\|)+2\\exp(-d)(\\|\\mu_{1}\\|+\\|\\mu_{2}\\|)\\leq4\\exp(-d)\\cdot15{\\sqrt{d}}=60{\\sqrt{d}}e^{-d},$ note that here we used $\\mu_{3}(t)=\\cdot\\cdot\\cdot=\\mu_{n}(t)=0$ . Therefore by the inductio\u221an hypothesis we have $\\begin{array}{r}{\\|\\mu_{i}(t+1)-\\mu_{i}(0)\\|\\leq\\eta t\\cdot(60\\sqrt{d}e^{-d})+\\eta\\|\\nabla_{\\mu_{i}}\\mathcal{L}(\\mu(t))\\|\\leq\\eta(t+1)\\cdot(60\\sqrt{d}e^{-d})}\\end{array}$ , (41) is proven. By (41)\u221a, $\\forall0\\,\\leq\\,t\\,\\leq\\,T$ , for $i\\,=\\,1,2$ w\u221ae have $\\|\\mu_{i}(t)\\|\\,\\geq\\,\\|\\mu_{i}(0)\\|\\,-\\,\\|\\mu_{i}(t)-\\mu_{i}(0)\\|\\,\\geq\\,12{\\sqrt{d}}\\,-$ $\\eta T(60\\sqrt{d}e^{-d})\\geq12\\sqrt{d}-2\\sqrt{d}=10\\sqrt{d}$ . Our proof is done. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See the summary of main contributions in Section 1 and main results in Section 3. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: As a theoretical work, the major assumptions and limitations of our results are presented in the introduction part of Section 1. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The settings and assumptions are introduced in Section 1. The complete proof of all theorems are provided in the Appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We give the details of our synthetic experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: We only run a small-scale experiment to verify an optimization phenomenon in our theory. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We give the details about our synthetic experiment. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper focuses on the optimization aspect. Our experiment shows the optimization phenomenon on synthetic data, and we do not study the statistical aspect. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our experiment only shows the phenomenon on small-scale synthetic data, so we did not record the computation resource used. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This is a theoretical work. It societal impact lies within its potential pratical applications. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is a theoretical work and has no such risks ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is a theoretical work and does not use existing assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: As a theoretical work, we does not release such new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: We do not involve crowdsourcing nor research with human subjects. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]