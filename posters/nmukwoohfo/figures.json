[{"figure_path": "nmUkwoOHFO/figures/figures_1_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the internal representations of large language models (LLMs) differ when using few-shot learning and fine-tuning.  The top row displays representations from layers closer to the input, while the bottom shows those closer to the output.  Three scenarios are compared: zero-shot, few-shot (in-context learning), and fine-tuned.  Few-shot learning creates more interpretable representations based on semantic content in the early layers, while fine-tuning better encodes the answers in later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_1_2.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the internal representations of LLMs differ when using few-shot learning versus fine-tuning for a question answering task.  The top row displays representations from layers closer to the input, while the bottom shows layers closer to the output.  Three scenarios are compared: zero-shot, few-shot (in-context learning), and fine-tuned.  The visualization reveals that few-shot learning leads to a more structured organization of representations, particularly in early layers.  Fine-tuning, on the other hand, seems to create more distinct, well-defined clusters representing answer identities primarily in later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_4_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the probability density landscape of hidden representations in LLMs for three scenarios: zero-shot, few-shot learning, and fine-tuning.  The top row displays the representations from layers closer to the input, and the bottom row displays those from layers closer to the output.  The visualization shows that few-shot learning develops better representations of the data's semantic content in the early layers while fine-tuning produces representations that better encode the identity of the answers in the later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_5_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure displays the probability landscape of a large language model's hidden representations when solving a question answering task using three different methods: zero-shot, few-shot learning, and fine-tuning. The top row shows the representations in layers near the input, while the bottom row shows those near the output.  The visualization highlights how different learning paradigms structure the internal representations of the model. Few-shot learning creates more interpretable, semantically organized representations in early layers, while fine-tuning develops probability modes better encoding the identity of answers in later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_6_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how in-context learning (ICL) and supervised fine-tuning (SFT) affect the internal representations of LLMs.  It compares the probability density landscapes of hidden representations in LLMs solving a question-answering task under three conditions: zero-shot, few-shot, and fine-tuned.  The results highlight a key difference in how the two learning methods structure the model's internal representations. ICL creates more interpretable, semantically-organized representations in early layers, while SFT better encodes answer identity in later layers. A sharp transition is observed in the middle of the network for both methods.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_7_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the probability landscape of hidden representations in LLMs for three scenarios: zero-shot, in-context learning (5-shot), and fine-tuning.  It compares how LLMs solve a question answering task. The top row displays the representations from the layers close to the input, and the bottom row displays representations from layers near the output. In the 5-shot scenario, early layers show better representation of the dataset's subjects while the fine-tuned model's late layers show better encoding of the answers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_8_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the probability landscape of hidden representations in LLMs for few-shot learning and fine-tuning on a question answering task.  It compares zero-shot, few-shot, and fine-tuned approaches, visualizing the probability mode distributions in early and late layers of the network. The results reveal that these two learning paradigms generate distinct internal structures; few-shot learning creates more interpretable representations in the early layers, hierarchically organized by semantic content, while fine-tuning generates fuzzier representations until late layers, where distinct probability modes better encode answer identities.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_17_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure visualizes how different learning paradigms (zero-shot, few-shot, and fine-tuned) shape the probability distributions within a large language model's hidden layers when performing a question-answering task.  It shows that early layers develop semantically organized representations for few-shot learning, while fine-tuning leads to representations better encoding the final answers in later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_17_2.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how in-context learning (ICL) and supervised fine-tuning (SFT) affect the representation landscape of LLMs.  It compares the probability distributions of hidden representations in LLMs solving a question answering task under three conditions: zero-shot, 5-shot (ICL), and fine-tuned (SFT). The top row shows early layers, revealing that ICL forms semantically organized representations, while SFT representations are fuzzier. The bottom row displays late layers, showing fine-tuned representations encoding answer identity better than ICL representations.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_18_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure visualizes how the probability distributions of hidden layer representations in LLMs change depending on the learning paradigm used (zero-shot, few-shot, and fine-tuned). It highlights the different ways LLMs process information across different layers and learning methods, showing a transition point in the middle of the network.  Early layers show better subject representation with few-shot learning, while late layers of fine-tuned models represent the answers more precisely. This demonstrates the distinct computational strategies employed by LLMs under varying conditions.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_18_2.jpg", "caption": "Figure 2: Intrinsic dimension, number of density peaks, and fraction of core points. Figure shows the ID (left), the number of density peaks (center), and the fraction of core points (right) for the last-token representation of Llama3-8b for an increasing number of few-shots and fine-tuned models. The three quantities change in the proximity of layer 17 in a two-phased fashion.", "description": "This figure displays the intrinsic dimension, number of density peaks, and fraction of core points across different layers of the Llama3-8b language model for various conditions (zero-shot, few-shot with increasing numbers of examples, and fine-tuned).  It highlights a two-phased behavior around layer 17, showing abrupt changes in these geometric properties of the model's representation. This transition suggests a shift in how the model processes information.", "section": "4.1 The geometry of LLMs' representations shows a two-phased behavior"}, {"figure_path": "nmUkwoOHFO/figures/figures_19_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the internal representations of large language models (LLMs) differ when solving a question-answering task using two different approaches: few-shot learning and fine-tuning.  The top row displays representations from layers closer to the input, while the bottom shows layers near the output.  The three columns represent zero-shot, few-shot learning (with 5 examples), and fine-tuning.  The visualization shows that few-shot learning develops better subject representations in early layers, while fine-tuning better encodes answer identities in later layers. This highlights the diverse computational strategies LLMs employ for the same task.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_19_2.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the probability landscape of hidden representations in LLMs changes depending on the learning paradigm used (zero-shot, few-shot, fine-tuned). The top row displays the representations closer to the input layer, while the bottom row illustrates the representations closer to the output layer.  It highlights differences in how these paradigms structure internal representations, particularly a transition that occurs in the middle of the network. Few-shot learning creates more interpretable representations in early layers, while fine-tuning better encodes the answers in the later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_20_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the probability landscape of hidden representations in LLMs for few-shot learning and fine-tuning.  It compares how LLMs solve a question-answering task under three conditions: zero-shot, few-shot in-context learning, and fine-tuning. The top row displays representations from layers closer to the input, while the bottom row shows those closer to the output.  The figure highlights that ICL and SFT induce different internal structures within the LLMs, with ICL showing hierarchical organization by semantic content in early layers and SFT showing fuzzier, semantically mixed representations before developing clearer answer identity encoding in later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_20_2.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the representation landscape of LLMs changes when solving a question answering task using different methods: zero-shot, few-shot learning, and fine-tuning. It visualizes the probability modes in the hidden layers of the model, showing distinct patterns for each learning paradigm in the early and late layers of the network. Few-shot learning creates more interpretable representations in early layers that are semantically organized, whereas fine-tuning results in a fuzzier representation that is more semantically mixed in the early layers, with later layers showing clear distinctions in probability modes that better encode the identity of the answers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_21_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the representation landscape of LLMs in three scenarios: zero-shot, few-shot, and fine-tuned. The top row shows the representations from layers near the input, and the bottom row shows those near the output.  The figure illustrates how different learning paradigms (zero-shot, few-shot, fine-tuned) shape the probability landscape of hidden representations within LLMs during a question-answering task. It highlights distinct patterns in early versus late layers and differences between the internal structures created by in-context learning and fine-tuning.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_21_2.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the probability landscape of hidden representations in LLMs changes when solving a question answering task using three different learning methods: zero-shot, few-shot, and fine-tuning.  The top row displays representations from layers closer to the input, while the bottom row shows representations from layers closer to the output.  The visualization reveals that each method leads to different internal structures in the network.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_22_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the internal representations of LLMs differ when using few-shot learning and fine-tuning for question answering.  It visualizes the probability distributions in the hidden layers of the model. The top row shows early layers, closer to the input, while the bottom row shows later layers closer to the output. Three scenarios are compared: zero-shot, few-shot (5 examples), and fine-tuned. The results highlight that the early layers of few-shot learning focus on representing the semantic content of the questions (subjects), while the later layers of the fine-tuned model focus on the answers themselves.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_23_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the internal representations of LLMs differ when using few-shot learning and fine-tuning for a question answering task.  It visualizes probability density distributions in the early and late layers of the model for three scenarios: zero-shot, 5-shot (few-shot learning), and fine-tuned.  Few-shot learning shows clear semantic organization in the early layers, while fine-tuning shows better encoding of the answers in the later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_24_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the probability density landscape of hidden representations in LLMs for three different learning paradigms: zero-shot, few-shot, and fine-tuned.  It compares how the models solve a question-answering task, highlighting differences in representation structure across the network layers (early vs. late). Few-shot learning creates more interpretable representations in the early layers, while fine-tuning leads to a more refined encoding of answers in the late layers. The visualization demonstrates how different learning methods impact the internal representations within LLMs.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_25_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the probability landscape of hidden representations in LLMs changes depending on the learning paradigm used (zero-shot, few-shot, fine-tuned) when solving a multiple-choice question-answering task.  The top row displays representations from the early layers (near input), and the bottom row from later layers (near output).  The results indicate that the learning paradigms create very different internal structures within the LLMs, particularly showing a sharp transition in the middle of the network.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_26_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the internal representations of LLMs differ when using few-shot learning versus fine-tuning for a question answering task.  It visualizes the probability density of hidden layer representations in the model, showing a clear distinction between the two approaches in the early layers (semantic representation) and the later layers (answer encoding). Few-shot learning creates more interpretable, semantically organized representations in early layers, whereas fine-tuning produces fuzzier representations until the later layers. The difference in representation structure is highlighted by comparing the probability density peaks in the different model scenarios.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_27_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure compares the representation landscape of LLMs trained with three different methods: zero-shot, few-shot, and fine-tuned. It visualizes the probability modes in the hidden layers of the network using density plots, showing how these modes evolve from the input to the output layers. The differences in the probability landscapes highlight the different internal strategies LLMs employ to solve the same question-answering task with different training paradigms.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_28_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the probability landscape of hidden representations in LLMs for three different learning paradigms: zero-shot, few-shot, and fine-tuned.  It visualizes how the probability density changes across layers, comparing the distributions of subject and answer representations. It highlights a transition point in the middle layers where the representation structure shifts.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_28_2.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the representation landscape of LLMs in few-shot learning and fine-tuning.  It compares zero-shot, 5-shot in-context learning, and fine-tuned models on a question-answering task.  The top row displays the representations from layers closer to the input, and the bottom row displays the representations from layers closer to the output. The visualizations highlight how different learning paradigms structure internal representations. In 5-shot learning, early layers represent subjects better, while fine-tuned models better encode answers in later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_29_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the internal representations of LLMs differ when using few-shot learning vs. fine-tuning for a question-answering task.  The top row displays the representations closer to the input layer, and the bottom row shows representations closer to the output layer.  It compares zero-shot, few-shot (5-shot), and fine-tuned models.  Few-shot learning develops semantically organized representations, while fine-tuning produces fuzzier and more semantically mixed representations in the early layers.  In the later layers, fine-tuned representations better encode the answers, while few-shot learning has less defined probability modes.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_30_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the visualization of probability modes in LLMs for three different learning scenarios: zero-shot, few-shot, and fine-tuned.  It compares how the internal representations of LLMs differ when solving a question-answering task using these three methods, showing distinct patterns in the probability landscape at different network layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_31_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows the visualization of probability modes in LLMs for three scenarios: zero-shot, few-shot learning, and fine-tuning.  The top row displays the representations from the beginning layers (near input), while the bottom displays the end layers (near output).  It shows how different learning paradigms organize internal representations, with few-shot showing interpretable, semantically organized clusters early on, and fine-tuning developing answer-specific clusters in later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_32_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure compares the representation landscape of three different learning scenarios (zero-shot, few-shot, and fine-tuned) in large language models while solving the same question-answering task. It shows the probability distributions in early and late layers of the model, revealing how the different learning strategies shape the internal representations in the model.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_33_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how in-context learning and fine-tuning affect the representation space of LLMs, comparing their probability landscape at different layers (near input vs. near output) for a question-answering task. It reveals distinct internal structures for each learning paradigm, with ICL forming hierarchically organized representations based on semantic content in the early layers and SFT generating fuzzier, semantically mixed representations.  In later layers, fine-tuning shows clearer probability modes that better encode the answers, while ICL representations are less defined.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_34_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the representation landscape of LLMs differs between few-shot learning and fine-tuning for a question answering task.  The top row displays the representations from layers closer to the input, while the bottom row shows those closer to the output. Three scenarios are compared: zero-shot, few-shot (in-context learning), and fine-tuned.  The results demonstrate that these two learning paradigms create significantly different internal structures within the model. Few-shot learning leads to semantically organized representations in the first half of the network while fine-tuning generates fuzzier, semantically mixed representations.  In contrast, the second half of the model shows fine-tuning developing clear probability modes associated with answers while few-shot learning displays less defined peaks.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_35_1.jpg", "caption": "Figure 1: The LLMs representation landscape of few-shot learning and fine-tuning. This figure illustrates the distribution of probability modes in large language models (LLMs) during a question-answering task (MMLU). The top row shows representations from layers near the input, while the bottom row shows those near the output. We compare three scenarios: zero-shot (0-shot, left), in-context learning (5-shot, center), and fine-tuning (right). In the 5-shot scenario, early layers (top, center) develop better representations of the dataset's subjects. Conversely, in the fine-tuned model, the late layers (bottom, right) more accurately reflect better the letter answers.", "description": "This figure shows how the internal representations of large language models differ when using few-shot learning vs. fine-tuning.  The top row shows early layers of the network, while the bottom shows later layers.  Three conditions are compared: zero-shot, 5-shot (few-shot learning), and fine-tuned.  Few-shot learning develops better representations of the subjects in the early layers, while fine-tuning better encodes the answers in the later layers.", "section": "4 Results"}, {"figure_path": "nmUkwoOHFO/figures/figures_36_1.jpg", "caption": "Figure 4: Density peaks in the layers that best encode the subjects in Llama3-8b. The dendrograms show the organization of the density peaks in Llama3-8b in the layers where the ARI with the subjects is highest for the 5-shot setup (top) and 0-shot set-up (bottom left) and fine-tuned model (bottom-right). In the 5-shot setup, the clusters are populated by examples from one or two related subjects, and their similarity reflects the semantic relationships between the subjects. In 0-shot and fine-tuned representations (bottom panels), some large clusters contain many subjects.", "description": "This figure shows dendrograms illustrating the organization of probability density peaks (clusters) in different layers of the Llama3-8b language model.  It compares three scenarios: a 5-shot in-context learning (ICL) setup, a 0-shot ICL setup, and a fine-tuned model. The 5-shot ICL setup shows a hierarchical clustering of subjects reflecting semantic relationships. In contrast, the 0-shot ICL and fine-tuned models exhibit less structured, more mixed clusters.", "section": "4.2 The probability landscape before the transition"}]