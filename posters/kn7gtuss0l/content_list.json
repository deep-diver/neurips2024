[{"type": "text", "text": "This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anthony Bardou Patrick Thiran Giovanni Ranieri IC, EPFL IC, EPFL IC, EPFL Lausanne, Switzerland Lausanne, Switzerland Lausanne, Switzerland anthony.bardou@epfl.ch patrick.thiran@epfl.ch giovanni.ranieri@epfl.ch ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Bayesian Optimization (BO) has proven to be very successful at optimizing a static, noisy, costly-to-evaluate black-box function $f:S\\to\\mathbb{R}$ . However, optimizing a black-box which is also a function of time (i.e., a dynamic function) $f:S\\times\\mathcal{T}\\rightarrow\\mathbb{R}$ remains a challenge, since a dynamic Bayesian Optimization (DBO) algorithm has to keep track of the optimum over time. This changes the nature of the optimization problem in at least three aspects: (i) querying an arbitrary point in $s\\times\\tau$ is impossible, (ii) past observations become less and less relevant for keeping track of the optimum as time goes by and (iii) the DBO algorithm must have a high sampling frequency so it can collect enough relevant observations to keep track of the optimum through time. In this paper, we design a Wasserstein distance-based criterion able to quantify the relevancy of an observation with respect to future predictions. Then, we leverage this criterion to build W-DBO, a DBO algorithm able to remove irrelevant observations from its dataset on the fly, thus maintaining simultaneously a good predictive performance and a high sampling frequency, even in continuous-time optimization tasks with unknown horizon. Numerical experiments establish the superiority of W-DBO, which outperforms state-of-the-art methods by a comfortable margin. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many real-world problems require the optimization of a costly-to-evaluate objective function $f:S\\subseteq$ $\\mathbb{R}^{d}\\stackrel{}{\\rightarrow}\\mathbb{R}$ with an unknown closed form (i.e., either the closed form expression of $f$ exists but remains unknown to the user, or it does not exist). Such a setting occurs frequently, and examples can be found in hyperparameters tuning [1], networking [2, 3], robotics [4] or computational biology [5]. In such applications, $f$ can be seen as a black-box and cannot be optimized by usual first-order approaches. Bayesian Optimization (BO) is an effective framework for black-box optimization. Its core idea is to leverage a surrogate model, usually a Gaussian Process (GP), to query $f$ at specific inputs. By doing so, a BO algorithm is able to simultaneously discover and optimize the objective function. ", "page_idx": 0}, {"type": "text", "text": "Since its inception, BO has proven to be very effective at optimizing black-boxes in a variety of contexts, such as high-dimensional input spaces [6, 7, 8], batch mode [9] or multi-objective optimization [10]. However, few works study BO in dynamic contexts (i.e., with a time-varying objective function), despite its critical importance. Indeed, dynamic black-box optimization problems arise whenever an optimization task is conducted within an environment that comprises exogenous factors that may vary with time and significantly impact the objective function. Dynamic black-boxes are found in network management [11], unmanned aerial vehicles tasks [12], hyperparameter tuning in online deep learning [13], online clustering [14] or crossing waypoints location in air routes [15]. ", "page_idx": 0}, {"type": "text", "text": "In a dynamic context, $f:S\\times\\mathcal{T}\\rightarrow\\mathbb{R}$ is a time-varying, black-box, costly-to-evaluate objective function with spatial domain $\\mathcal{S}\\subseteq\\mathbb{R}^{d}$ and temporal domain $\\tau\\subseteq\\mathbb{R}$ . Unlike common black-box objective functions that take as input a point $\\pmb{x}\\in S$ in a spatial domain only, the function $f$ takes as input a point $({\\mathbf{\\boldsymbol{x}}},t)\\in S\\times T$ in space and time. Since the problem of optimizing $f$ is still addressed under the BO framework, the framework is called dynamic Bayesian optimization $(D B O)$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Taking time into account does not boil down to merely adding an extra dimension to $\\boldsymbol{S}$ . It changes the nature of the optimization problem on at least three aspects: (i) at present time $t_{0}$ , a DBO algorithm can query any arbitrary point $\\pmb{x}\\in S$ but cannot query past $(i.e.,\\,t<t_{0})$ ) nor future1 (i.e., $t>t_{0},$ ) points in time, (ii) as time (only) moves forward, a previously collected observation $(({\\bf{x}},t),f({\\bf{x}},t))$ becomes less and less informative about the future values of $f$ as time goes by and (iii) the response time (i.e., the time required for hyperparameters inference and acquisition function maximization) becomes a key feature of the DBO algorithm since it has a direct impact on the sampling frequency of the algorithm and, consequently, on its ability to track the position of the optimum as $f$ changes. ", "page_idx": 1}, {"type": "text", "text": "Interestingly, (ii) and (iii) imply that each observation eventually becomes stale and, as such, a computational burden if kept in the dataset. Since a DBO task might require the optimization of an objective function over arbitrarily long periods of time, the ability to remove observations from the dataset on the fly, as soon as they become irrelevant for future predictions, is essential to prevent a prohibitive growth of the response time of DBO algorithms. Overall, (i), (ii) and (iii) require to address dynamic (i.e., space-time) problems differently from usual (i.e., space-only) problems. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of this article are twofold. First, we propose a fast and efficient method able to quantify the relevancy of an observation. Second, we leverage this method to build a DBO algorithm able to identify and delete irrelevant observations in an online fashion2. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Let us start by describing the BO framework with a GP prior, as introduced by [16]. Given an objective function $f:S\\stackrel{=}{\\subseteq}\\mathbb{R}^{d}\\to\\mathbb{R}$ , BO assumes that $f$ is a $\\mathcal{G P}\\left(0,k(\\pmb{x},\\pmb{x}^{\\prime})\\right)$ . For any $x\\in S$ , and given a dataset of observations $\\mathcal{D}=\\{(\\mathbf{\\boldsymbol{x}}_{i},y_{i})\\}_{i\\in[\\![1,n]\\!]}$ , where $x_{i}\\in S$ is a previously queried input with (noisy) function value $y_{i}\\,=\\,f(\\pmb{x}_{i})+\\epsilon,\\,\\epsilon\\,\\sim{\\mathcal{N}}\\left(0,\\sigma^{2}\\right)$ , the posterior distribution of $f(x)$ is $\\mathcal{N}(\\mu({\\pmb x}),\\sigma^{2}({\\pmb x}))$ , where ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mu(\\pmb{x})=\\pmb{k}^{\\top}(\\pmb{x},\\pmb{X})\\Delta^{-1}\\pmb{y},}}\\\\ {{\\sigma^{2}(\\pmb{x})=k(\\pmb{x},\\pmb{x})-\\pmb{k}^{\\top}(\\pmb{x},\\pmb{X})\\Delta^{-1}k(\\pmb{x},\\pmb{X})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "with $X\\ \\ =\\ \\ (x_{1},\\cdot\\cdot\\cdot\\ ,x_{n})$ , $\\pmb{y}_{}^{}~=~(y_{1},\\pmb{\\cdot}\\cdot\\cdot\\mathbf{\\nabla},y_{n})$ , $\\textbf{\\em\\Delta}=\\ k(X,X)\\,+\\,\\sigma^{2}I$ and $\\begin{array}{r l}{k(\\mathcal{X},\\mathcal{Y})}&{{}=}\\end{array}$ (k (xi, xj))xi\u2208X,xj\u2208Y. ", "page_idx": 1}, {"type": "text", "text": "To find $x_{n+1}$ , the input to query at the $(n+1)\\mathrm{th}$ iteration, a BO algorithm exploits an acquisition function $\\varphi:S\\to\\mathbb{R}$ . The acquisition function $\\varphi$ quantifies the benefits of querying the input $\\textbf{\\em x}$ in terms of (i) exploration (i.e., how much it improves the GP regression of $f$ ) and (ii) exploitation (i.e., how close it is to the optimum of $f$ according to the GP). A large variety of BO acquisition functions have been proposed, such as GP-UCB [17], Expected Improvement [18] or Probability of Improvement [19]. Formally, the BO algorithm determines its next queried input by finding $\\pmb{x}_{n+1}=\\arg\\operatorname*{max}_{\\pmb{x}\\in S}\\varphi(\\pmb{x})$ . ", "page_idx": 1}, {"type": "text", "text": "BO extends naturally to dynamic problems, by adapting the covariance function $k$ to properly capture temporal correlations (discussed later in this section). The resulting inference formulas are very similar to (1) and (2). Given a dataset of observations $\\mathcal{D}=\\{((\\mathbf{\\boldsymbol{x}}_{i},t_{i}\\mathbf{\\bar{)}}\\,,y_{i})\\}_{i\\in\\mathbb{\\lceil\\lceil1,n\\rceil}}$ , where $(\\mathbf{\\boldsymbol{x}}_{i},t_{i})\\in\\mathcal{S}\\times\\mathcal{T}$ is a previously queried input with the (noisy) function value $y_{i}=f(\\pmb{x}_{i},t_{i})+\\epsilon$ , the posterior distribution of $f({\\pmb x},t)$ is $\\mathcal{N}\\left(\\mu({\\bf{\\boldsymbol x}},t),\\sigma^{2}({\\bf{\\boldsymbol x}},t)\\right)$ for any $({\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}},t)\\in S\\times T$ , with ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mu({\\pmb x},t)={\\pmb k}^{\\top}(({\\pmb x},t)\\,,{\\pmb X}){\\pmb\\Delta}^{-1}{\\pmb y},}\\\\ {\\sigma^{2}({\\pmb x},t)=k(({\\pmb x},t)\\,,({\\pmb x},t))-{\\pmb k}^{\\top}(({\\pmb x},t)\\,,{\\pmb X}){\\pmb\\Delta}^{-1}k(({\\pmb x},t)\\,,{\\pmb X})}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "wi ${\\begin{array}{r l}&{\\textbf{h}X\\,=\\,((x_{1},t_{1})\\,,\\cdots\\,,(x_{n},t_{n})),\\,y\\,=\\,(y_{1},\\cdots\\,,y_{n}),\\,\\Delta\\,=\\,k(X,X)+\\sigma^{2}I{\\mathrm{~and}}}\\\\ &{{\\big\\langle}(x_{i},t_{i})\\,,(x_{j},t_{j}){\\big\\rangle})_{(x_{i},t_{i})\\in\\mathcal{X},(x_{j},t_{j})\\in\\mathcal{Y}}.}\\end{array}}$ ${\\pmb k}({\\mathcal{X}},{\\mathcal{Y}})\\,=$ ", "page_idx": 1}, {"type": "text", "text": "Exploiting the usual acquisition functions in a dynamic context is also straightforward. Since a DBO algorithm can only query an input at the current running time $t_{0}$ , the next queried input is simply $({\\bf\\boldsymbol{x}}_{n+1},t_{0})$ , with $\\begin{array}{r}{\\mathbf{\\boldsymbol{x}}_{n+1}=\\arg\\operatorname*{max}_{\\mathbf{\\boldsymbol{x}}\\in S}\\varphi(\\mathbf{\\boldsymbol{x}},t_{0})}\\end{array}$ . Some DBO algorithms (e.g., [20]) extend the querying horizon to the near future, that is, from $t_{0}$ to a time interval $[t_{0},t_{0}+\\delta_{t}]$ . In that case, the next queried input is $(\\pmb{x}_{n+1},t_{n+1})=\\arg\\operatorname*{max}_{(\\pmb{x},t)\\in\\mathcal{S}\\times[t_{0},t_{0}+\\delta_{t}]}\\varphi(\\pmb{x},t)\\tilde{\\mathbf{\\Psi}}_{n}$ . ", "page_idx": 2}, {"type": "text", "text": "BO is an active field of research, but relatively few works address DBO, despite the natural extension of BO to dynamic problems described above. We conclude this section by reviewing them. In [21], the objective function is allowed to evolve in time according to a simple Markov model, controlled by a hyperparameter $\\epsilon\\in[0,1]$ . On the one hand, the authors propose R-GP-UCB, which handles data staleness by resetting the dataset every $N(\\epsilon)$ iterations. On the other hand, the authors also propose TV-GP-UCB that incorporates data staleness by weighing the covariance of two queries ${\\pmb q}_{i}=({\\pmb x}_{i},t_{i})$ and ${\\pmb q}_{j}=({\\pmb x}_{j},t_{j})$ by $(1-\\epsilon)^{|i-j|/2}$ . In [22], the authors use the same model with an event-triggered reset of the dataset. Although less relevant to this work, let us mention [23] and [24] for the sake of completeness. Under frequentist assumptions, they also propose DBO algorithms that forget irrelevant observations by either resetting their datasets or by using decreasing covariance weights. However, they assume that the variational budget of the objective function is fixed, which has the drawback of requiring the objective function to become asymptotically static. This is a very different setting than the one of interest in this paper, which does not make this requirement. ", "page_idx": 2}, {"type": "text", "text": "The aforementioned algorithms all work with discrete, evenly-spaced time steps. This setting simplifies the regret analysis of DBO algorithms through the use of proof techniques similar to the ones used for static BO. However, it also overlooks a critical effect of the response times of their algorithms. In fact, the response time of a BO algorithm heavily depends on its dataset size $n$ , since BO inference is in ${\\mathcal{O}}(n^{3})$ . Although it is reasonable to ignore this for classical BO because the objective function $f$ is static, DBO algorithms cannot make this simplification as it directly impacts their ability to track the optimal argument of the objective function through time. Many algorithms (e.g., see [21, 23, 24]) recommend to keep all the collected observations in their datasets, whereas in practice, their response times would asymptotically become prohibitive. Other algorithms (e.g., see [21, 22, 23]) propose to reset their datasets, either periodically or once an event is triggered. This probably deletes some relevant observations in the process. More importantly, these algorithms necessarily estimate their covariance function hyperparameters beforehand and keep them fixed during the optimization. This lack of adaptivity of the estimation might lead to severely under-optimal characterization of the function by the hyperparameters, especially when optimizing an ever-changing objective function on an infinite time horizon. ", "page_idx": 2}, {"type": "text", "text": "To the best of our knowledge, only one work acknowledges these problems. It proposes ABO [20], an algorithm that uses a decomposable spatio-temporal covariance function $\\bar{k}((\\bar{\\mathbf{x}},t),(\\mathbf{x}^{\\prime},t^{\\prime}))\\,=$ $k_{S}(||\\bar{\\boldsymbol{x}}-\\boldsymbol{x}^{\\prime}||_{2})k_{T}(|t-t^{\\prime}|)$ to accurately model complex spatio-temporal correlations and samples the objective function only when deemed necessary. Although this reduces the size of ABO\u2019s dataset, ABO does not propose a way to remove stale observations, it only adds new observations less frequently. Therefore, using ABO will still become prohibitive in the long run. ", "page_idx": 2}, {"type": "text", "text": "The most relevant methods to quantify the relevancy of an observation can be found in the sparse GPs literature (e.g., see [25, 26]). However, they require non-trivial adjustments to account for the particular nature of the time dimension. As far as we know, there is no method in the DBO literature able to quantify the relevancy of an observation in an online setting. As mentioned before, such a method is much needed as it would allow a DBO algorithm to remove stale data on the fly while preserving the predictive performance of the algorithm. We bridge this gap by providing a sound criterion to measure the relevancy of an observation and an algorithm exploiting this criterion to remove stale data from its dataset. ", "page_idx": 2}, {"type": "text", "text": "3 A Wasserstein Distance-Based Criterion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Core Assumptions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To address the DBO problem under suitable smoothness conditions, let us make the usual assumption of BO, using a Gaussian Process (GP) as a surrogate model for $f$ (see [16]). ", "page_idx": 2}, {"type": "text", "text": "Assumption 3.1. $f$ is a $\\mathcal{G P}\\left(0,k((\\mathbf{\\boldsymbol{x}},t)\\,,(\\mathbf{\\boldsymbol{x}}^{\\prime},t^{\\prime})\\right)$ , whose mean is 0 (without loss of generality) and whose covariance function is denoted by $k:S\\times T\\times S\\times T\\to\\mathbb{R}_{+}$ . ", "page_idx": 2}, {"type": "text", "text": "In order to accurately model complex spatio-temporal dynamics, we make the same assumption on the decomposition and isotropy in time and space of the covariance function $k$ as in [20]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 3.2. ", "page_idx": 3}, {"type": "equation", "text": "$$\nk((\\pmb{x},t),(\\pmb{x}^{\\prime},t^{\\prime}))=\\lambda k_{S}(||\\pmb{x}-\\pmb{x}^{\\prime}||_{2},l_{S})k_{T}(|t-t^{\\prime}|,l_{T}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $\\lambda>0$ , $k_{S}:\\mathbb{R}_{+}\\rightarrow[0,1]$ and $k_{T}:\\mathbb{R}_{+}\\rightarrow[0,1]$ two positive correlation functions, parameterized by lengthscales $l_{S}>0$ and $l_{T}>0$ , respectively. The factor $\\lambda>0$ scales the product of the two correlation functions and hence, controls the magnitude of the covariance function $k$ . The lengthscales $l_{S}$ and $l_{T}$ control the correlation lengths of the GP (see [27] for more details) in space and in time, respectively. ", "page_idx": 3}, {"type": "text", "text": "Although the covariance function $k$ is able to model temporal correlations with $k_{T}$ , it does not accurately measure the relevancy of an observation. The next section addresses this question. ", "page_idx": 3}, {"type": "text", "text": "3.2 Measuring the Relevancy of an Observation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "By definition, when an irrelevant observation gets removed from the dataset, the GP posterior experiences hardly any change. Therefore, we propose to measure the relevancy of an observation $\\pmb{o}_{i}\\overline{{=}}\\left((\\pmb{x}_{i},t_{i}),y_{i}\\right)$ by measuring the impact that the removal of $\\pmb{o}_{i}$ has on the GP posterior. ", "page_idx": 3}, {"type": "text", "text": "Let $\\mathcal{G P}_{\\mathcal{D}}$ be the GP conditioned on the dataset $\\mathcal{D}=\\{((\\mathbf{}x_{i},t_{i}),y_{i})\\}_{i\\in[\\![1,n]\\!]}$ , with $(\\mathbf{\\boldsymbol{x}}_{i},t_{i})\\in\\mathcal{S}\\times\\mathcal{T}$ and $y_{i}\\,=\\,f(\\pmb{x}_{i},t_{i})+\\epsilon,\\epsilon\\,\\sim\\mathcal{N}\\left(0,\\sigma^{2}\\right)$ . Without loss of generality, let us measure the impact of removing $((\\mathbf{\\boldsymbol{x}}_{1},t_{1}),y_{1})$ from the dataset on the GP posterior. Clearly, the measure should be defined on the domain of future predictions at time $t_{0}$ , denoted by $\\mathcal{F}_{t_{0}}$ , which must include the whole space $\\boldsymbol{S}$ and only the future time interval $[t_{0},+\\infty)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t_{0}}=\\mathcal{S}\\times[t_{0},+\\infty).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We compare a GP conditioned on the whole dataset, denoted by $\\mathcal{G P}_{\\mathcal{D}}$ , with a GP conditioned on $\\tilde{\\mathcal{D}}$ , the dataset without $(\\pmb{x}_{1},t_{1},y_{1})$ , denoted by $\\mathcal{G P}_{\\tilde{\\mathcal{D}}}$ . For an arbitrary point $(\\mathbf{\\boldsymbol{x}},t)\\ \\in\\ \\mathcal{F}_{t_{0}}$ , $\\mathcal{G P}_{\\mathcal{D}}$ provides a posterior distribution $\\mathcal{N}_{\\mathcal{D}}(\\mathbf{\\boldsymbol{x}},t)\\;=\\;\\mathcal{N}\\left(\\mu_{\\mathcal{D}}(\\mathbf{\\boldsymbol{x}},t),\\sigma_{\\mathcal{D}}^{2}(\\mathbf{\\boldsymbol{x}},t)\\right)$ , and so does $\\mathcal{G P}_{\\tilde{D}}$ with $\\mathcal{N}_{\\tilde{\\mathcal{D}}}(\\mathbf{\\boldsymbol{x}},t)\\,=\\mathcal{N}\\left(\\mu_{\\tilde{\\mathcal{D}}}(\\mathbf{\\boldsymbol{x}},t),\\sigma_{\\tilde{\\mathcal{D}}}^{2}(\\mathbf{\\boldsymbol{x}},t)\\right)$ . We compare these two distributions by using the 2- Wasserstein distance [28], given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{2}\\left(\\mathcal{N}_{\\mathcal{D}}(x,t),\\mathcal{N}_{\\mathcal{\\tilde{D}}}(x,t)\\right)=\\left((\\mu_{\\mathcal{D}}(x,t)-\\mu_{\\mathcal{\\tilde{D}}}(x,t))^{2}+(\\sigma_{\\mathcal{D}}(x,t)-\\sigma_{\\mathcal{\\tilde{D}}}(x,t))^{2}\\right)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A natural extension of the 2-Wasserstein distance from a point $(\\pmb{x},t)\\in\\mathcal{F}_{t_{0}}$ to the domain $\\mathcal{F}_{t_{0}}$ is ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{2}\\left(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\tilde{\\mathcal{D}}}\\right)=\\left(\\oint_{\\mathcal{S}}\\int_{t_{0}}^{\\infty}W_{2}^{2}\\left(\\mathcal{N}_{\\mathcal{D}}(\\underline{{x}},t),\\mathcal{N}_{\\tilde{\\mathcal{D}}}(\\underline{{x}},t)\\right)d\\pmb{x}d t\\right)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Observe that (8) is a criterion that effectively captures the impact of removing the observation $\\pmb{o}_{1}=((\\pmb{x}_{1},t_{1}),y_{1})$ from the dataset on the GP posterior. However, as discussed in Appendix F, the covariance function hyperparameters $\\pmb\\theta=(\\lambda,l_{S},l_{T})$ control the magnitude of (8). This is illustrated by Figure 1, which depicts two couples of GP posteriors that achieve the same value (8). Depending on the lengthscale magnitude, the posteriors may be quite different or, conversely, very similar. As a result, (8) cannot be directly used as a gauge of observation relevancy. ", "page_idx": 3}, {"type": "text", "text": "To remove this ambiguity, we normalize (8) by $W_{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\varnothing})$ (i.e., the 2-Wasserstein distance between the GP conditioned on $\\mathcal{D}$ and the prior GP), and we obtain the ratio ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(\\mathcal{G P_{D}},\\mathcal{G P_{\\tilde{D}}})=\\frac{W_{2}(\\mathcal{G P_{D}},\\mathcal{G P_{\\tilde{D}}})}{W_{2}(\\mathcal{G P_{D}},\\mathcal{G P_{\\emptyset}})}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuitively, $W_{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\varnothing})$ measures the impact of resetting the whole dataset $\\mathcal{D}$ on the GP posterior and serves as a baseline that puts into perspective the distance measured by (8). Technically, taking the ratio (9) successfully cancels out the influence of the covariance function hyperparameters on the magnitude of the Wasserstein distances, as further discussed in Appendix F. As a direct consequence, (9) is an unambiguous indication of how relevant an observation is. This is illustrated by Figure 2, which depicts couples of GP posteriors under different contexts. When (9) is small (resp., large), the posteriors are similar (resp., dissimilar) regardless of the magnitude of the lengthscale. ", "page_idx": 3}, {"type": "image", "img_path": "kN7GTUss0l/tmp/c86d2275e6ecd9acb2c13e557c39bca27e3cd20a7bce01d1cf08ea52b4128ab2.jpg", "img_caption": ["Figure 1: Similar values of Wasserstein distance, different effect on posteriors. For visualization purposes, only the posterior means of two posterior GPs (blue for $\\mu_{\\mathscr D}$ and orange for $\\mu_{\\tilde{D}}$ ) are depicted, along a single dimension (e.g., time). The Wasserstein distance between the two posteriors is shown by the green shaded area. The GPs have a small lengthscale (left) or, conversely, a large lengthscale (right) for the chosen dimension. "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "kN7GTUss0l/tmp/41a2ea53efff32098f5d234c887439bf5a583011ce18b2f7ca7ac4b3a2cf87fa.jpg", "img_caption": ["Figure 2: Normalized Wasserstein distances. Similarly to Figure 1, a few couples of GP posterior means $(\\mu_{\\mathscr D},\\mu_{\\tilde{\\mathscr D}})$ are depicted. The top (resp., bottom) row depicts couples of posteriors that yield a small (resp., large) ratio (9). The left (resp., right) column depicts couples of posteriors controlled by a small (resp., large) lengthscale. The prior GP mean $\\mu_{\\emptyset}=0$ is shown as a black dashed line, and the Wasserstein distance between the posterior and the prior as a gray shaded area. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Exploiting this criterion is straightforward. When (9) is small, one can infer that the observation $\\pmb{O}_{1}$ can be safely removed from the dataset since it will have virtually no impact on the posterior. Conversely, when (9) is large, one can infer that removing $\\pmb{o}_{1}$ would alter the posterior too much, and conclude that it is a relevant observation that must remain in the dataset. The exploitation of (9) is discussed in more details in Section 4.2. ", "page_idx": 5}, {"type": "text", "text": "The criterion (9) is useful for a DBO algorithm if and only if it can be computed on the fly, in an online setting. In the next section, we show that (9) can be approximated efficiently, and we describe a DBO algorithm able to exploit the criterion. ", "page_idx": 5}, {"type": "text", "text": "4 Using the Criterion in Practice ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Computational Tractability ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In [29], the authors provide an algorithm to approximate the 2-Wasserstein distance between two GPs up to an arbitrary precision level. However, the computational cost of this algorithm is too expensive in an online setting, where it is crucial to keep the per-iteration cost as small as possible to ensure a high sampling frequency. In this section, we put this issue to rest by deriving an explicit approximation of (9). These formulas are computationally cheap enough to be exploited on the fly. ", "page_idx": 5}, {"type": "text", "text": "In Appendix A, we show that (7) can be computed efficiently. Next, in Appendix B, we apply these results to obtain an upper bound of (8). The key observation for deriving this result is to approximate the integrals in (8) by a convolution of the covariance functions with themselves in space and time. The same trick can be used for approximating $W_{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\varnothing})$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Let $t_{0}$ be the present time and $\\mathcal{D}=\\{((\\mathbf{}x_{i},t_{i}),y_{i})\\}_{i\\in[\\![1,n]\\!]}$ be a dataset of observations made before $t_{0}$ . Let $\\tilde{\\boldsymbol{D}}\\,=\\,\\{((\\boldsymbol{x}_{i},t_{i}),y_{i})\\}_{i\\in[2,n]}$ be the dataset without the first observation and $\\mathcal{F}_{t_{0}}=S\\times[t_{0},+\\infty)$ be the domain of future p red ictions. Then, an upper bound for $W_{2}^{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\tilde{D}})$ on $\\mathcal{F}_{t_{0}}$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{W}_{2}^{2}\\left(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\tilde{\\mathcal{D}}}\\right)=\\lambda^{2}(a^{2}+E)C((x_{1},t_{1}),(x_{1},t_{1}))+\\lambda^{2}(2a b+c)C((x_{1},t_{1}),\\tilde{\\mathcal{D}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,\\lambda^{2}\\operatorname{tr}\\left(\\left(b b^{\\top}+M\\right)C(\\tilde{\\mathcal{D}},\\tilde{\\mathcal{D}})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C(\\mathcal{X},\\mathcal{Y})=\\left((k_{S}*k_{S})(\\pmb{x}_{j}-\\pmb{x}_{i})\\cdot(k_{T}*k_{T})_{t_{0}-t_{i}}^{+\\infty}(t_{j}-t_{i})\\right)_{(\\pmb{x}_{i},t_{i})\\in\\mathcal{X}_{i}}$ , where $(f*g)$ denotes $(\\pmb{x}_{j},t_{j})\\!\\in\\!\\mathcal{D}$ the convolution between $f$ and $g$ , and $(f*g)_{a}^{b}$ denotes the convolution between $f$ and $g$ restricted to the interval $[a,b]$ . The terms $a,\\,b,\\,c,\\,E$ and $_M$ are explicited in Appendices $A$ and $B$ . ", "page_idx": 5}, {"type": "text", "text": "Moreover, an upper bound for $W_{2}^{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\varnothing})$ on $\\mathcal{F}_{t_{0}}$ is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{W}_{2}^{2}\\left(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\varnothing}\\right)=\\lambda^{2}\\left(\\pmb{y}^{\\top}\\pmb{\\Delta}^{-\\top}\\mathcal{C}\\left(\\mathcal{D},\\mathcal{D}\\right)\\pmb{\\Delta}^{-1}\\pmb{y}+\\operatorname{tr}\\left(\\pmb{\\Delta}^{-1}\\mathcal{C}\\left(\\mathcal{D},\\mathcal{D}\\right)\\right)\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This theorem provides the analytic form of an upper bound for the Wasserstein distance between $\\mathcal{G P}_{\\mathcal{D}}$ and $\\mathcal{G P}_{\\tilde{D}}$ and the Wasserstein distance between $\\mathcal{G P}_{\\mathcal{D}}$ and ${\\mathcal{G P}}_{\\varnothing}$ on the domain of future predictions $\\mathcal{F}_{t_{0}}$ . Using it, we can compute an approximation R\u02c6 of the relative criterion (9), that is ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{R}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\tilde{\\mathcal{D}}})=\\frac{\\hat{W}_{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\tilde{\\mathcal{D}}})}{\\hat{W}_{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\varnothing})}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In Appendix C, we study the error between the criterion (9) and the approximation (12). In essence, we bound the approximation error caused by estimating the integrals in (8) by a self-convolution of the covariance functions $k_{S}$ and $k_{T}$ (i.e., $k_{S}*k_{S}$ and $k_{T}*k_{T})$ ). Furthermore, we provide numerical evidence that the approximation errors in the numerator and the denominator of (12) compensate each other at least in part, making (12) a decent approximation for (9). ", "page_idx": 5}, {"type": "text", "text": "In practice, the upper bounds (10) and (11) can only be computed efficiently if the convolutions of the covariance functions can themselves be computed efficiently. The analytic forms for the convolution of two usual covariance functions listed in Table 1, namely Squared-Exponential (SE) and Mat\u00e9rn [30], are provided in Appendix D together with Tables 3 and 4 that list the self-convolutions for the spatial (resp., temporal) covariance function. Their detailed computations are also provided in this appendix. In a nutshell, the formulas are obtained first in the Fourier domain by computing the square of the spectral densities of the covariance functions, and next by computing their inverse Fourier transform. ", "page_idx": 5}, {"type": "table", "img_path": "kN7GTUss0l/tmp/4633be5237e36b07ec413e753d8d39d46e3e6ee016b016c060ff3f576baab415.jpg", "table_caption": ["Table 1: Usual covariance functions. $\\Gamma$ is the Gamma function and $K_{\\nu}$ is a modified Bessel function of the second kind of order $\\nu$ . "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Together, Tables 3, 4 in Appendix D and Theorem 4.1 show that the approximation of (9) given by (12) can be computed efficiently in an online setting. In an effort to generalize our results to a class of covariance functions that extends beyond Assumption 3.2, we also discuss how to compute the selfconvolution of an anisotropic spatial SE covariance function in Appendix E. We now leverage (12) to propose a DBO algorithm able to pinpoint and remove irrelevant observations in its dataset. ", "page_idx": 6}, {"type": "text", "text": "4.2 W-DBO", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The metric (9) and its approximation (12) can be seen as a relative error (or drift), expressed as a percentage, that separates $\\mathcal{G P}_{\\mathcal{D}}$ and $\\mathcal{G P}_{\\tilde{D}}$ . Indeed, the Wasserstein distance $W\\left(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\tilde{\\mathcal{D}}}\\right)$ is scaled by the Wasserstein distance $W\\left(\\mathcal{G P}_{\\mathcal{D}}^{\\mathcal{-}},\\mathcal{G P}_{\\varnothing}\\right)$ , that is, the distance between $\\mathcal{G P}_{\\mathcal{D}}$ and the prior. In other words, (9) and its approximation (12) measure the relative drift from $\\mathcal{G P}_{\\mathcal{D}}$ to $\\mathcal{G P}_{\\tilde{\\mathcal{D}}}$ caused by the removal of one observation. When removing multiple observations, the relative drifts naturally accumulate in a multiplicative way (similarly to the way relative errors accumulate). As a consequence, removing multiple observations could, in the worst case, make $\\mathcal{G P}_{\\tilde{D}}$ drift exponentially fast from $\\mathcal{G P}_{\\mathcal{D}}$ . To keep this exponential drift under control, one can use a removal budget $b(t)=(1+\\alpha)^{t}$ that allows a maximal relative drift from $\\mathcal{G P}_{\\mathcal{D}}$ of $\\alpha$ per unit of time (e.g., if $\\alpha=0.1$ , the allowed maximal drift is $10\\,\\%$ per unit of time). The cost of removing an observation is given by (12). ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 describes W-DBO, a DBO algorithm exploiting (12) to remove irrelevant observations on the fly. As described above, the removal budget is controlled by a single hyperparameter $\\alpha$ and grows exponentially as time goes by (see line 24). At each iteration, (12) is used to compute the relevancy of each observation in the dataset (see lines 10-13). The relevancy of the least relevant observation is then compared to the removal budget, and the observation is removed if the budget allows it (see lines 14-17). This process is repeated until all the budget is consumed. Such a greedy observation removal policy causes W-DBO to overestimate the impact of removing multiple observations3. We discuss and motivate the expression of the removal budget in Appendix G. The sensitivity analysis conducted in Section 5.1 supports this removal budget, by showing that the same value of the hyperparameter $\\alpha$ is valid for a large set of different objective functions. ", "page_idx": 6}, {"type": "text", "text": "Finally, note that using (12) to remove irrelevant observations on the fly can be performed in conjunction with any BO algorithm, because it can be appended at the end of each optimization step as a simple post-processing stage. This agnostic property of W-DBO is supported by the ability of Algorithm 1 to take as inputs any GP model ${\\mathcal{G P}}$ and any acquisition function $\\varphi$ . Similarly, observe that lines 5-8 in Algorithm 1 describe the usual BO optimization loop, without any modification. ", "page_idx": 6}, {"type": "text", "text": "5 Numerical Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we study the empirical performance of W-DBO. To measure the quality of the queries made by the DBO solutions, we compute the average regret (lower is better). For the sake of realistic evaluation, two iterations of a solution are seperated by its response time (i.e., the time taken to infer its hyperparameters and optimize its acquisition function). Furthermore, all covariance function ", "page_idx": 6}, {"type": "text", "text": "Algorithm 1 W-DBO ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "1: Input: ${\\mathcal{G P}}$ , acquisition function $\\varphi$ , hyperparameter $\\alpha$ , clock $\\mathcal{C}$ , initial observations $\\mathcal{D}$ .   \n2: Get current time $t$ from clock $\\mathcal{C}$   \n3: $b=1$   \n4: while true do   \n5: Find $\\pmb{x}_{t}=\\arg\\operatorname*{max}_{\\pmb{x}}\\varphi(\\pmb{x},t)$   \n6: Observe $y_{t}=f(\\mathbf{\\boldsymbol{x}}_{t},t)+\\epsilon,\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2})$   \n7: $\\mathcal{D}=\\mathcal{D}\\cup\\{(x_{t},t,y_{t})\\}$   \n8: Condition ${\\mathcal{G P}}$ on $\\mathcal{D}$ and get the MLE parameters $(\\lambda,l_{S},l_{T},\\hat{\\sigma}^{2})$   \n9: while $b>1$ do   \n10: for all $(x_{i},t_{i},y_{i})\\in\\mathcal{D}$ [in parallel] do   \n11: $\\tilde{\\mathcal{D}}=\\mathcal{D}\\setminus\\{({\\mathbf{x}}_{i},t_{i},y_{i})\\}$   \n12: Compute $\\hat{R}_{i}$ using (12)   \n13: end for   \n14: $\\begin{array}{r l}&{i^{**}=\\arg\\operatorname*{min}_{i\\in[\\![1,|\\mathcal{D}|]\\!]}\\hat{R}_{i}}\\\\ &{{\\mathbf{if}}\\,b>1+\\hat{R}_{i^{*}}\\,\\mathbf{then}}\\\\ &{\\quad\\mathcal{D}=\\mathcal{D}\\setminus\\left\\{({\\boldsymbol{x}}_{i^{*}}\\,,t_{i^{*}}\\,,y_{i^{*}})\\right\\}}\\\\ &{\\quad b=b/(1+\\hat{R}_{i^{*}})}\\end{array}$   \n15:   \n16:   \n17:   \n18: else   \n19: break   \n20: end if   \n21: end while   \n22: $t^{\\prime}=t$   \n23: Get current time $t$ from clock $\\mathcal{C}$   \n24: $b=b(1+\\alpha)^{(t-t^{\\prime})/l_{T}}$   \n25: end while ", "page_idx": 7}, {"type": "image", "img_path": "kN7GTUss0l/tmp/b9839cab28b621b58987e3181bd926ccc1a88e9f55ecc77c321d80a0db84d963.jpg", "img_caption": ["Figure 3: (Left) Sensitivity analysis on the Eggholder function. (Right) Aggregation of sensitivity analyses of W-DBO made on 10 synthetic functions and a real-world experiment. For aggregation purposes, the average regrets in each experiment have been normalized between 0 (lowest average regret) and 1 (largest average regret). The average performance of W-DBO over all the experiments is shown in black. Standard errors are depicted with colored bars (left) and shaded areas (right). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "parameters and the noise level are estimated on the fly. Please refer to Appendix H.1 for further experimental details, and to Appendix H.2 for a detailed description of the dynamic benchmarks. ", "page_idx": 7}, {"type": "text", "text": "5.1 Sensitivity Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We start by studying the impact of the W-DBO hyperparameter $\\alpha$ on the average regret. Recall that we take into account the response time of the algorithm. This evaluation protocol reveals that a trade-off must be achieved between having an accurate model of the objective function (which requires a large dataset) and being able to track the optimal argument of the function as it evolves (which requires a high sampling frequency, thus a moderately-sized dataset). ", "page_idx": 7}, {"type": "text", "text": "Table 2: Comparison of W-DBO with competing methods. The average regret over 10 independent replications is reported (lower is better). The performance of the best algorithm is written in bold text. The performance of algorithms whose confidence intervals overlap the best performing algorithm\u2019s confidence interval is underlined. ", "page_idx": 8}, {"type": "table", "img_path": "kN7GTUss0l/tmp/0ebe4e668510c1122f30469b35bc05763a83db6c308b35c4fe19bc055df17d8e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "kN7GTUss0l/tmp/cf4bb1b9f2d8bc9ff4b24b03f7c18710effb5735b704b0a52e21d51b11e397dc.jpg", "img_caption": ["Figure 4: (Left) Average regrets of the DBO solutions during the optimization of the Ackley synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Ackley function. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "To study this trade-off, we compare the performance of W-DBO with several values of its hyperparameter $\\alpha$ , as illustrated by the left side of Figure 3. We apply this protocol on 11 different benchmarks (described in Appendix H.2). The aggregated results (see the right side of Figure 3) show that achieving a trade-off between the size of the dataset and the sampling frequency can significantly improve the performance of W-DBO. Clearly, the sweet spot is reached for $\\begin{array}{r}{\\alpha=\\frac{1}{4}}\\end{array}$ . This hyperparameter value is used to evaluate W-DBO in the next section. ", "page_idx": 8}, {"type": "text", "text": "5.2 Comparison with Baselines ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The competing baselines are the relevant algorithms reported in Section 2, namely R-GP-UCB and TV-GP-UCB [21], ET-GP-UCB [22] and ABO [20]. We also consider vanilla BO with the GP-UCB acquisition function, which only considers spatial correlations. For comparison purposes, all the results are gathered in Table 2. The benchmarks comprise ten synthetic functions and two real-world experiments. All figures (including standard errors) are provided in Appendix H.2, and the performance of each DBO solution is discussed at length in Appendix H.3. Furthermore, the provided supplementary animated visualizations are discussed in Appendix H.4. ", "page_idx": 8}, {"type": "text", "text": "In this section. we only depict the performance of the DBO solutions on the Ackley synthetic function in Figure 4, because it illustrates best the singularity of W-DBO. The Ackley function is known for its almost flat outer region (with lots of local minima) and its deep hole at the center of its domain. Observe that most DBO solutions miss that hole, as their average regrets skyrocket between 200 and ", "page_idx": 8}, {"type": "image", "img_path": "kN7GTUss0l/tmp/93de7550a91690b1ef5a542f52c6b277f610d0ae3529f08e9c871a39b76e29ea.jpg", "img_caption": ["Figure 5: Visual summary of the results reported in Table 2. For aggregation purposes, the average regrets in each experiment have been normalized between 0 (lowest average regret) and 1 (largest average regret). The average performance of the DBO solutions is shown in black. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "400 seconds. In contrast, W-DBO manages to rapidly exploit the hole at the center of the function domain, thereby maintaining a low average regret. ", "page_idx": 9}, {"type": "text", "text": "This performance gap can be explained by studying the dataset size of W-DBO (see the right side of Figure 4). At first, the dataset size increases since most collected observations are relevant to predict the outer region of the Ackley function. After 200 seconds, the dataset size plateaus as W-DBO begins to realize that some previously collected observations are irrelevant to predict the shape of the hole that lies ahead. Between 300 and 400 seconds, the dataset size is halved because most previously collected observations are deemed irrelevant. Eventually, after 400 seconds, W-DBO explores the flatter outer region of the Ackley function again. Consequently, its dataset size increases again. ", "page_idx": 9}, {"type": "text", "text": "For a summary of the performance of the DBO solutions across all our benchmarks, please refer to the last row of Table 2, and to the visual summary in Figure 5. In our experimental setting, ABO and ET-GP-UCB obtain roughly the same performance as vanilla BO. R-GP-UCB shows slightly better average performance than GP-UCB, while TV-GP-UCB appears significantly better than the aforementioned algorithms. Remarkably, W-DBO shows significantly better performance than TV-GP-UCB and outperforms the other DBO solutions by a comfortable margin. In fact, it obtains the lowest average regret on almost every benchmark. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The ability to remove irrelevant observations from the dataset of a DBO algorithm is essential to ensure a high sampling frequency while preserving its predictive performance. To address this difficult problem, we have proposed (i) a criterion based on the Wasserstein distance to measure the relevancy of an observation, (ii) a computationally tractable approximation of this criterion to allow its use in an online setting and (iii) a DBO algorithm, W-DBO, that exploits this approximation. We have evaluated W-DBO against the state-of-the-art of DBO on a variety of benchmarks comprising synthetic functions and real-world experiments. The evaluation was conducted in the most challenging settings, where time is continuous, the time horizon is unknown, as well as the covariance functions hyperparameters. We observe that W-DBO outperforms the state-of-the-art of DBO by a comfortable margin. We explain this significant performance gap by the ability of W-DBO to quantify the relevancy of each of its observations, which is not shared with any other DBO algorithm, to the best of our knowledge. As a result, W-DBO can remove irrelevant observations in a smoother and more appropriate way than by simply triggering the erasure of the whole dataset. By doing so, W-DBO simultaneously ensures a high sampling frequency and a very good predictive performance. ", "page_idx": 9}, {"type": "text", "text": "In addition to its impact on DBO itself, we believe that W-DBO can have a significant impact on the fields that make heavy use of DBO (e.g., computer networks, robotics). As a future work, we plan on exploring these applications of W-DBO. Furthermore, we plan on better understanding the excellent performance of W-DBO by addressing the difficult problem of deriving a regret bound that holds in a continuous time setting and incorporates the effect of the sampling frequency of the DBO algorithm as well as the deletion of irrelevant observations. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International conference on machine learning, pages 115\u2013123. PMLR, 2013.   \n[2] Gregory Hornby, Al Globus, Derek Linden, and Jason Lohn. Automated antenna design with evolutionary algorithms. In American Institute of Aeronautics and Astronautics, 2006.   \n[3] Anthony Bardou and Thomas Begin. INSPIRE: Distributed Bayesian Optimization for ImproviNg SPatIal REuse in Dense WLANs. In Proceedings of the 25th International ACM Conference on Modeling Analysis and Simulation of Wireless and Mobile Systems, pages 133\u2013142, 2022.   \n[4] Daniel Lizotte, Tao Wang, Michael Bowling, and Dale Schuurmans. Automatic gait optimization with gaussian process regression. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI\u201907, page 944\u2013949, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.   \n[5] Javier Gonz\u00e1lez, Joseph Longworth, David C. James, and Neil D. Lawrence. Bayesian optimization for synthetic gene design. In NIPS Workshop on Bayesian Optimization in Academia and Industry, 2014.   \n[6] David Eriksson, Michael Pearce, Jacob Gardner, Ryan D Turner, and Matthias Poloczek. Scalable global optimization via local bayesian optimization. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[7] Johannes Kirschner, Mojmir Mutny, Nicole Hiller, Rasmus Ischebeck, and Andreas Krause. Adaptive and safe bayesian optimization in high dimensions via one-dimensional subspaces. In International Conference on Machine Learning, pages 3429\u20133438. PMLR, 2019.   \n[8] Anthony Bardou, Patrick Thiran, and Thomas Begin. Relaxing the additivity constraints in decentralized no-regret high-dimensional bayesian optimization. In The Twelfth International Conference on Learning Representations, 2024.   \n[9] Javier Gonz\u00e1lez, Zhenwen Dai, Philipp Hennig, and Neil Lawrence. Batch bayesian optimization via local penalization. In Artificial intelligence and statistics, pages 648\u2013657. PMLR, 2016.   \n[10] Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Multi-objective bayesian optimization over high-dimensional search spaces. In Uncertainty in Artificial Intelligence, pages 507\u2013517. PMLR, 2022.   \n[11] Seokhyun Kim, Kimin Lee, Yeonkeun Kim, Jinwoo Shin, Seungwon Shin, and Song Chong. Dynamic control for on-demand interference-managed wlan infrastructures. IEEE/ACM Transactions on Networking, 28(1):84\u201397, 2019.   \n[12] Aurelio G Melo, Milena F Pinto, Andre LM Marcato, Leonardo M Hon\u00f3rio, and Fabr\u00edcio O Coelho. Dynamic optimization and heuristics based online coverage path planning in 3d environment for uavs. Sensors, 21(4):1108, 2021.   \n[13] Doyen Sahoo, Quang Pham, Jing Lu, and Steven CH Hoi. Online deep learning: learning deep neural networks on the fly. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 2660\u20132666, 2018.   \n[14] Charu C Aggarwal, Jiawei Han, Jianyong Wang, and Philip S Yu. A framework for projected clustering of high dimensional data streams. In Proceedings of the Thirtieth international conference on Very large data bases-Volume 30, pages 852\u2013863, 2004.   \n[15] Xiao Mingming, Zhang Jun, Cai Kaiquan, Cao Xianbin, and Tang Ke. Cooperative co-evolution with weighted random grouping for large-scale crossing waypoints locating in air route network. In 2011 IEEE 23rd International Conference on Tools with Artificial Intelligence, pages 215\u2013222. IEEE, 2011.   \n[16] Christopher Williams and Carl Rasmussen. Gaussian processes for regression. Advances in neural information processing systems, 8, 1995.   \n[17] Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Informationtheoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory, 58(5):3250\u20133265, 2012.   \n[18] Jonas Mockus. Application of bayesian approach to numerical methods of global and stochastic optimization. Journal of Global Optimization, 4:347\u2013365, 1994.   \n[19] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455\u2013492, 1998.   \n[20] Favour M Nyikosa, Michael A Osborne, and Stephen J Roberts. Bayesian optimization for dynamic problems. arXiv preprint arXiv:1803.03432, 2018.   \n[21] Ilija Bogunovic, Jonathan Scarlett, and Volkan Cevher. Time-varying gaussian process bandit optimization. In Artificial Intelligence and Statistics, pages 314\u2013323. PMLR, 2016.   \n[22] Paul Brunzema, Alexander von Rohr, Friedrich Solowjow, and Sebastian Trimpe. Eventtriggered time-varying bayesian optimization. arXiv preprint arXiv:2208.10790, 2022.   \n[23] Xingyu Zhou and Ness Shroff. No-regret algorithms for time-varying bayesian optimization. In 2021 55th Annual Conference on Information Sciences and Systems (CISS), pages 1\u20136. IEEE, 2021.   \n[24] Yuntian Deng, Xingyu Zhou, Baekjin Kim, Ambuj Tewari, Abhishek Gupta, and Ness Shroff. Weighted gaussian process bandits for non-stationary environments. In International Conference on Artificial Intelligence and Statistics, pages 6909\u20136932. PMLR, 2022.   \n[25] Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate gaussian process regression. The Journal of Machine Learning Research, 6:1939\u20131959, 2005.   \n[26] Henry B Moss, Sebastian W Ober, and Victor Picheny. Inducing point allocation for sparse gaussian processes in high-throughput bayesian optimisation. In International Conference on Artificial Intelligence and Statistics, pages 5213\u20135230. PMLR, 2023.   \n[27] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006.   \n[28] Leonid V Kantorovich. Mathematical methods of organizing and planning production. Management science, 6(4):366\u2013422, 1960.   \n[29] Anton Mallasto and Aasa Feragen. Learning from uncertain curves: The 2-wasserstein metric for gaussian processes. Advances in Neural Information Processing Systems, 30, 2017.   \n[30] Emilio Porcu, Moreno Bevilacqua, Robert Schaback, and Chris J Oates. The mat\\\u2019ern model: A journey through statistics, numerical analysis and machine learning. arXiv preprint arXiv:2303.02759, 2023.   \n[31] Izrail Solomonovich Gradshteyn and Iosif Moiseevich Ryzhik. Table of integrals, series, and products. Academic press, 2014.   \n[32] Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gordon Wilson, and Eytan Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. In Advances in Neural Information Processing Systems 33, 2020.   \n[33] Wenzel Jakob, Jason Rhinelander, and Dean Moldovan. pybind11 \u2013 seamless operability between $c{+}{+}11$ and python, 2017. https://github.com/pybind/pybind11.   \n[34] JHB Kemperman. On the shannon capacity of an arbitrary channel. In Indagationes Mathematicae (Proceedings), volume 77, pages 101\u2013115. North-Holland, 1974. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Wasserstein Distance at a Point in $\\mathcal{F}_{t_{0}}$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "At the core of (8) lies (7). In this appendix, we provide explicit expressions for (7). Let us start by proving the following lemma. ", "page_idx": 12}, {"type": "text", "text": "Lemma A.1. ", "text_level": 1, "page_idx": 12}, {"type": "equation", "text": "$$\n\\pmb{\\Delta}_{\\mathcal{D}}^{-1}=\\left(\\begin{array}{l l}{E}&{G}\\\\ {H}&{F}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with ${\\pmb{\\Delta}}_{\\mathcal{D}}={\\pmb{k}}({\\mathcal{D}},{\\mathcal{D}})+\\sigma^{2}{\\pmb{I}}$ and $E,F,G,H$ defined as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\boldsymbol{E}=\\left(\\lambda+\\sigma^{2}-k^{\\top}\\left((x_{1},t_{1}),\\tilde{\\mathcal{D}}\\right)\\Delta_{\\bar{\\mathcal{D}}}^{-1}\\boldsymbol{k}\\left((x_{1},t_{1}),\\tilde{\\mathcal{D}}\\right)\\right)^{-1},}\\\\ &{\\boldsymbol{F}=\\left(\\Delta_{\\bar{\\mathcal{D}}}-\\frac{1}{\\lambda+\\sigma^{2}}k\\left((x_{1},t_{1}),\\tilde{\\mathcal{D}}\\right)k^{\\top}\\left((x_{1},t_{1}),\\tilde{\\mathcal{D}}\\right)\\right)^{-1},}\\\\ &{\\boldsymbol{G}=-E k^{\\top}\\left((x_{1},t_{1}),\\tilde{\\mathcal{D}}\\right)\\Delta_{\\bar{\\mathcal{D}}}^{-1},}\\\\ &{\\boldsymbol{H}=-\\frac{1}{\\lambda+\\sigma^{2}}F k\\left((x_{1},t_{1}),\\tilde{\\mathcal{D}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{k\\left(\\mathcal{X},\\mathcal{Y}\\right)=\\left(k\\left(\\left(\\mathbf{x}_{i},t_{i}\\right),(\\mathbf{x}_{j},t_{j})\\right)\\right)_{\\left(\\mathbf{x}_{i},t_{i}\\right)\\in\\mathcal{X}}a n d\\,\\Delta_{\\tilde{\\mathcal{D}}}=k(\\tilde{\\mathcal{D}},\\tilde{\\mathcal{D}})+\\sigma^{2}I.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. The proof is trivial using the inverse of a block matrix: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{{\\left(\\!\\!\\begin{array}{c c}{{A}}&{{B}}\\\\ {{C}}&{{D}}\\end{array}\\!\\!\\right)^{-1}=\\left(\\left(A-B D^{-1}C\\right)^{-1}\\!\\!\\!\\begin{array}{c c}{{0}}&{{\\!\\!\\!\\!I}}\\\\ {{\\left(D-C A^{-1}B\\right)^{-1}\\right)\\left(-C A^{-1}}&{{I}}^{{-1}}\\end{array}\\!\\!\\right)}}\\\\ {{}}&{{=\\left(\\!\\!\\begin{array}{c c}{{\\left(A-B D^{-1}C\\right)^{-1}}}&{{-\\left(A-B D^{-1}C\\right)^{-1}B D^{-1}}}\\\\ {{\\left(D-C A^{-1}B\\right)^{-1}C A^{-1}}}&{{\\left(D-C A^{-1}B\\right)^{-1}}}\\end{array}\\!\\!\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that $\\pmb{A}$ and $_{D}$ must be invertible. ", "page_idx": 12}, {"type": "text", "text": "We can use (18) to write $\\Delta_{\\mathcal{D}}^{-1}$ as a function of $\\Delta_{\\tilde{\\mathcal{D}}}^{-1}$ , since ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{\\Delta}_{\\mathcal{D}}^{-1}=\\left(k((\\alpha_{1},t_{1}),(\\pmb{x}_{1},t_{1}))+\\sigma^{2}\\quad\\pmb{k}^{\\top}((\\alpha_{1},t_{1}),\\tilde{\\mathcal{D}})\\right)^{-1}}\\\\ &{\\qquad\\qquad k((\\pmb{x}_{1},t_{1}),\\tilde{\\mathcal{D}})\\qquad\\qquad\\pmb{\\Delta}_{\\tilde{\\mathcal{D}}}}\\\\ &{\\qquad=\\left(\\underset{k((\\alpha_{1},t_{1}),\\tilde{\\mathcal{D}})}{\\lambda+\\sigma^{2}}\\quad\\pmb{k}^{\\top}((\\alpha_{1},t_{1}),\\tilde{\\mathcal{D}})\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Note that, in (19), $\\lambda+\\sigma^{2}$ and $\\Delta_{\\tilde{\\mathcal{D}}}$ are invertible. Therefore, (18) can be applied to (19), and this yields the desired result. \u53e3 ", "page_idx": 12}, {"type": "text", "text": "To compute (7), we provide the following results. ", "page_idx": 12}, {"type": "text", "text": "Proposition A.2. ", "text_level": 1, "page_idx": 12}, {"type": "equation", "text": "$$\n\\mu_{\\mathcal{D}}({\\pmb x},t)-\\mu_{\\tilde{\\mathcal{D}}}({\\pmb x},t)=a k(({\\pmb x},t),({\\pmb x}_{1},t_{1}))+{b}k\\left(({\\pmb x},t),\\tilde{\\mathcal{D}}\\right),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with $\\begin{array}{r}{a=E y_{1}+G\\tilde{\\pmb{y}},\\,\\pmb{b}^{\\top}=\\pmb{H}^{\\top}y_{1}+\\tilde{\\pmb{y}}^{\\top}\\left(\\pmb{F}-\\pmb{\\Delta}_{\\tilde{\\mathcal{D}}}^{-1}\\right)}\\end{array}$ and $\\tilde{\\pmb{y}}=(y_{2},\\cdot\\cdot\\cdot\\,,y_{n})$ . ", "page_idx": 12}, {"type": "text", "text": "Proposition A.3. ", "text_level": 1, "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(\\sigma_{\\mathcal{D}}(x,t)-\\sigma_{\\tilde{\\mathcal{D}}}(x,t))^{2}\\leq{E k}((x,t),(x_{1},t_{1}))^{2}+{k}((x,t),(x_{1},t_{1}))c k((x,t),\\tilde{\\mathcal{D}})}\\\\ &{}&{\\quad+\\,k^{\\top}((x,t),\\tilde{\\mathcal{D}})M k((x,t),\\tilde{\\mathcal{D}}),\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with $c=G+H^{\\top}$ and $M=F-\\Delta_{\\tilde{\\mathcal{D}}}^{-1}$ . ", "page_idx": 12}, {"type": "text", "text": "We now prove Proposition A.2. ", "page_idx": 12}, {"type": "text", "text": "Proof. According to (3) ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{\\tilde{\\mathcal{D}}}(\\mathbf{\\boldsymbol{x}},t)=\\pmb{k}^{\\top}((\\mathbf{\\boldsymbol{x}},t),\\tilde{\\mathcal{D}})\\Delta_{\\tilde{\\mathcal{D}}}^{-1}\\tilde{\\pmb{y}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Applying the same definition for $\\mu_{ \u1e0a }\\_ \u1e0a }(\\pmb{x},t)$ we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\mathcal{D}}(\\pmb{x},t)=\\left(k((\\pmb{x},t),(\\pmb{x}_{1},t_{1})),k^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\right)\\pmb{\\Delta}_{\\mathcal{D}}^{-1}\\pmb{y}}\\\\ &{\\qquad\\quad=\\left(k((\\pmb{x},t),(\\pmb{x}_{1},t_{1})),k^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\right)\\left(\\pmb{R}\\begin{array}{c}{G}\\\\ {H}\\end{array}\\right)\\pmb{y}}\\\\ &{\\qquad\\quad=\\left(k((\\pmb{x},t),(\\pmb{x}_{1},t_{1})),k^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\right)\\left(\\pmb{R}_{\\mathcal{Y}_{1}}+\\pmb{G}\\tilde{\\mathcal{y}}\\right)}\\\\ &{\\qquad\\quad=k((\\pmb{x},t),(\\pmb{x}_{1},t_{1}))\\left(\\pmb{E}_{\\mathcal{Y}_{1}}+\\pmb{G}\\tilde{\\mathcal{y}}\\right)+k^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\left(\\pmb{H}\\mathscr{y}_{1}+\\pmb{F}\\tilde{\\mathcal{y}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where (23) follows from Lemma A.1. ", "page_idx": 13}, {"type": "text", "text": "Finally, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\mathcal{D}}(\\pmb{x},t)-\\mu_{\\tilde{\\mathcal{D}}}(\\pmb{x},t)=k\\big((\\pmb{x},t),(\\pmb{x}_{1},t_{1})\\big)\\left(\\pmb{E}y_{1}+\\pmb{G}\\tilde{y}\\right)+\\pmb{k}^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\left(\\pmb{H}y_{1}+\\pmb{F}\\tilde{y}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\pmb{k}^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\Delta_{\\tilde{\\mathcal{D}}}^{-1}\\tilde{y}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "which can be reduced to ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{\\mathcal{D}}(\\pmb{x},t)-\\mu_{\\tilde{\\mathcal{D}}}(\\pmb{x},t)=a k((\\pmb{x},t),(\\pmb{x}_{1},t_{1}))+b\\pmb{k}((\\pmb{x},t),\\tilde{\\mathcal{D}})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, we prove Proposition A.3. ", "page_idx": 13}, {"type": "text", "text": "Proof. As $(\\sigma_{\\mathcal{D}}(\\mathbf{\\boldsymbol{x}},t)-\\sigma_{\\tilde{\\mathcal{D}}}(\\mathbf{\\boldsymbol{x}},t))^{2}$ is hard to integrate, to get (8), we upper bound it by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\sigma_{\\mathcal{D}}(\\pmb{x},t)-\\sigma_{\\tilde{\\mathcal{D}}}(\\pmb{x},t))^{2}=\\sigma_{\\mathcal{D}}^{2}(\\pmb{x},t)+\\sigma_{\\tilde{\\mathcal{D}}}^{2}(\\pmb{x},t)-2\\sigma_{\\mathcal{D}}(\\pmb{x},t)\\sigma_{\\tilde{\\mathcal{D}}}(\\pmb{x},t)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\sigma_{\\mathcal{D}}^{2}(\\pmb{x},t)+\\sigma_{\\tilde{\\mathcal{D}}}^{2}(\\pmb{x},t)-2\\sigma_{\\mathcal{D}}^{2}(\\pmb{x},t)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\sigma_{\\mathcal{D}}^{2}(\\pmb{x},t)-\\sigma_{\\mathcal{D}}^{2}(\\pmb{x},t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where (26) follows from $\\sigma_{\\mathcal{D}}(\\mathbf{\\boldsymbol{x}},t)\\leq\\sigma_{\\tilde{\\mathcal{D}}}(\\mathbf{\\boldsymbol{x}},t)$ . ", "page_idx": 13}, {"type": "text", "text": "Now, (4) yields that for $X={\\tilde{D}}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{\\tilde{\\mathcal{D}}}^{2}(\\pmb{x},t)=\\lambda-\\pmb{k}^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\Delta_{\\tilde{\\mathcal{D}}}^{-1}\\pmb{k}((\\pmb{x},t),\\tilde{\\mathcal{D}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and for $X=\\tilde{D}=\\tilde{\\mathcal{D}}\\cup\\{(\\mathbf{\\alpha}_{1},t_{1})\\}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{D}^{2}(\\pmb{x},t)=\\lambda-\\left(k((\\pmb{x},t),(\\pmb{x}_{1},t_{1})),\\pmb{k}^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\right)\\pmb{\\Delta}_{\\mathcal{D}}^{-1}\\left(\\begin{array}{c}{k((\\pmb{x},t),(\\pmb{x}_{1},t_{1}))}\\\\ {k((\\pmb{x},t),\\tilde{\\mathcal{D}})}\\end{array}\\right)}\\\\ &{\\qquad\\qquad=\\lambda-\\left(k((\\pmb{x},t),(\\pmb{x}_{1},t_{1})),\\pmb{k}^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\right)\\left(\\begin{array}{c c}{\\pmb{E}}&{\\pmb{G}}\\\\ {\\pmb{H}}&{\\pmb{F}}\\end{array}\\right)\\left(\\begin{array}{c}{k((\\pmb{x},t),(\\pmb{x}_{1},t_{1}))}\\\\ {k((\\pmb{x},t),\\tilde{\\mathcal{D}})}\\end{array}\\right)\\qquad\\qquad}\\\\ &{\\qquad\\qquad=\\lambda-\\left(k((\\pmb{x},t),(\\pmb{x}_{1},t_{1})),\\pmb{k}^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})\\right)\\left(\\begin{array}{c}{\\pmb{E}k((\\pmb{x},t),(\\pmb{x}_{1},t_{1}))+\\pmb{G}k((\\pmb{x},t),\\tilde{\\mathcal{D}})}\\\\ {\\pmb{H}k((\\pmb{x},t),(\\pmb{x}_{1},t_{1}))+\\pmb{F}k((\\pmb{x},t),\\tilde{\\mathcal{D}})}\\end{array}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where (29) follows from Lemma A.1. Developing the dot product in (30), we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{{D}}^{2}(\\mathbf{x},t)=\\lambda-E k^{2}((\\mathbf{x},t),(\\mathbf{x}_{1},t_{1}))-k((\\mathbf{x},t),(\\mathbf{x}_{1},t_{1}))G k((\\mathbf{x},t),\\tilde{{D}})}\\\\ &{\\qquad\\qquad\\qquad-\\,k((\\mathbf{x},t),(\\mathbf{x}_{1},t_{1}))H^{\\top}k((\\mathbf{x},t),\\tilde{{D}})-k^{\\top}((\\mathbf{x},t),\\tilde{{D}})F k((\\mathbf{x},t),\\tilde{{D}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Combining (28) and (31), we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{\\mathcal{\\bar{D}}}^{2}(\\pmb{x},t)-\\sigma_{\\mathcal{D}}^{2}(\\pmb{x},t)=E k((\\pmb{x},t),(\\pmb{x}_{1},t_{1}))^{2}+k((\\pmb{x},t),(\\pmb{x}_{1},t_{1}))c k((\\pmb{x},t),\\tilde{\\mathcal{D}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,k^{\\top}((\\pmb{x},t),\\tilde{\\mathcal{D}})M k((\\pmb{x},t),\\tilde{\\mathcal{D}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $c=G+H^{\\top}$ and $M=F-\\Delta_{\\tilde{\\mathcal{D}}}^{-1}$ . ", "page_idx": 13}, {"type": "text", "text": "Combining (27) and (32) concludes the proof. ", "page_idx": 13}, {"type": "text", "text": "B Wasserstein Distance on $\\mathcal{F}_{t_{0}}$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we extend the computation of the Wasserstein distance between two posterior Gaussian distributions at an arbitrary point in $\\mathcal{F}_{t_{0}}$ (see Propositions A.2 and A.3) to the Wasserstein distance between $\\mathcal{G P}_{\\mathcal{D}}$ and $\\mathcal{G P}_{\\tilde{D}}$ on $\\mathcal{F}_{t_{0}}$ . We also provide an upper bound for the Wasserstein distance between a posterior GP conditioned on $\\mathcal{D}$ (i.e., $\\mathcal{G P}_{\\mathcal{D}}$ ) and the prior GP (i.e., ${\\mathcal{G P}}_{\\varnothing}$ ). ", "page_idx": 14}, {"type": "text", "text": "Let us start by proving the following lemma. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1. Let $t_{0}$ be the present time and $\\mathcal{D}=\\{((\\mathbf{}x_{i},t_{i}),y_{i})\\}_{i\\in[\\![1,n]\\!]}$ be a dataset of observations before $t_{0}$ . Let $\\tilde{\\mathcal{D}}=\\{((\\boldsymbol{x}_{i},t_{i}),y_{i})\\}_{i\\in[\\![2,n]\\!]}$ and $\\mathcal{F}_{t_{0}}=S\\!\\times\\![t_{0},+\\infty)$ b e th e domain of future predictions. Then, for any pair of observations $(\\bar{\\pmb{x}}_{i},\\bar{t}_{i},\\cdot),(\\pmb{x}_{j},t_{j},\\cdot).$ from $\\mathcal{D}$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\oint_{S}\\int_{t_{\\circ}}^{\\infty}k\\left((\\mathbf{x},t),(\\mathbf{x}_{i},t_{i})\\right)k\\left((\\mathbf{x},t),(\\mathbf{x}_{j},t_{j})\\right)d\\mathbf{x}d t\\leq\\lambda^{2}(k_{S}*k_{S})(\\mathbf{x}_{j}-\\mathbf{x}_{i})(k_{T}*k_{T})_{t_{0}-t_{i}}^{+\\infty}(t_{j}-t_{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "with $(f*g)$ the convolution between $f$ and $g$ and $(f*g)_{a}^{b}$ the convolution between $f$ and $g$ restricted to the interval $[a,b]$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. For the sake of brevity, let us denote by $I$ the LHS of (33). According to Assumption 3.2, Fubini\u2019s theorem and since $\\bar{\\mathcal{F}_{t_{0}}}=\\bar{S}\\times[t_{0},+\\infty)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nI=\\lambda^{2}\\oint_{S}k_{S}(||\\mathbf x-\\mathbf x_{i}||_{2})k_{S}(||\\mathbf x-\\mathbf x_{j}||_{2})d x\\int_{t_{0}}^{+\\infty}k_{T}(|t-t_{i}|)k_{T}(|t-t_{j}|)d t.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let us focus on the integral on the spatial domain $\\boldsymbol{S}$ . Depending on the expression of the covariance function, this integral can be quite difficult to compute. Let us turn this expression into a more pleasant upper bound ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\oint_{S}k_{S}(||\\mathbf x-\\mathbf x_{i}||_{2})k_{S}(||\\mathbf x-\\mathbf x_{j}||_{2})d x\\leq\\oint_{\\mathbb{R}^{d}}k_{S}(||\\mathbf x-\\mathbf x_{i}||_{2})k_{S}(||\\mathbf x-\\mathbf x_{j}||_{2})d x}\\\\ {=\\oint_{\\mathbb{R}^{d}}k_{S}(||\\mathbf u||_{2})k_{S}(||\\mathbf x_{j}-\\mathbf x_{i}-\\mathbf u||_{2})d u}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (35) holds since $k_{S}$ is positive and (36) comes from the change of variable ${\\pmb u}\\,=\\,{\\pmb x}\\,-\\,{\\pmb x}_{i}$ . Obviously, this upper bound remains interesting because $k_{S}$ is usually an exponentially decreasing function (see Table 1). We discuss this point in more details in Appendix $\\mathbf{C}$ . ", "page_idx": 14}, {"type": "text", "text": "Observe that (36) is actually the convolution $(k_{S}*k_{S})(\\pmb{x}_{j}-\\pmb{x}_{i})$ . A similar change of variable can be made regarding the time integral, with $v=t-t_{i}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int_{t_{0}}^{+\\infty}k_{T}(|t-t_{i}|)k_{T}(|t-t_{j}|)d t=\\int_{t_{0}-t_{i}}^{+\\infty}k_{T}(|v|)k_{T}(|t_{j}-t_{i}-v|)d v.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Note that (37) is also a convolution, but restricted to the interval $[t_{0}-t_{i},+\\infty)$ . We denote this convolution on a restricted interval $(k_{T}*k_{T})_{t_{0}-t_{i}}^{+\\infty}(t_{j}-t_{i})$ . ", "page_idx": 14}, {"type": "text", "text": "Combining (34), (36) and (37) yields Lemma B.1. ", "page_idx": 14}, {"type": "text", "text": "We now prove the following lemma. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. Let $t_{0}$ be the present time and $\\mathcal{D}=\\{((\\mathbf{}x_{i},t_{i}),y_{i})\\}_{i\\in[\\![1,n]\\!]}$ be a dataset of observations before $t_{0}$ . Let $\\tilde{\\mathcal{D}}=\\{((\\boldsymbol{x}_{i},t_{i}),y_{i})\\}_{i\\in[\\![2,n]\\!]}$ and $\\mathcal{F}_{t_{0}}=S\\times[t_{0},+\\infty)$ the  domain of future predictions. Then, we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\oint_{\\cal S}\\int_{t_{0}}^{\\infty}(\\mu_{\\mathcal{D}}(x,t)-\\mu_{\\tilde{\\mathcal{D}}}(x,t))^{2}d x d t\\leq\\lambda^{2}a^{2}C((x_{1},t_{1}),(x_{1},t_{1}))+2\\lambda^{2}a b C((x_{1},t_{1}),\\tilde{\\mathcal{D}})}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad+\\,\\lambda^{2}\\operatorname{tr}\\left(b b^{\\top}C(\\tilde{\\mathcal{D}},\\tilde{\\mathcal{D}})\\right),\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathit{w i t h\\_a}\\quad=\\quad E y_{1}\\ +\\ G\\tilde{y}\\quad\\mathit{a n d\\_b}^{\\top}\\quad=\\quad H^{\\top}y_{1}\\ +\\ \\tilde{y}^{\\top}\\left(F-\\Delta_{\\bar{\\mathcal{D}}}^{-1}\\right),\\quad\\mathit{o t h e r w i s e}}\\\\ &{\\left((k_{S}*k_{S})(x_{j}-x_{i})(k_{T}*k_{T})_{t_{0}-t_{i}}^{+\\infty}(t_{j}-t_{i})\\right)_{(x_{i},t_{i})\\in\\mathcal{X}},\\quad1\\quad\\mathit{t h e}\\quad\\mathit{c o n f o r m a b l e}\\quad\\nu e c o n}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad(x_{j},t_{j})\\in\\mathcal{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$(f*g)$ the convolution between $f$ and $g$ and $(f*g)_{a}^{b}$ the convolution between $f$ and $g$ restricted to the interval $[a,b]$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. By Proposition A.2, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\oint_{S}\\int_{t_{0}}^{\\infty}(\\mu_{\\mathcal{D}}(x,t)-\\mu_{\\bar{\\mathcal{D}}}(x,t))^{2}d x d t=\\oint_{S}\\int_{t_{0}}^{\\infty}\\Big(a k((x,t),(x_{1},t_{1}))+b k\\left((x,t),\\tilde{\\mathcal{D}}\\right)\\Big)^{2}\\,d x d t\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with $a=E y_{1}+G\\tilde{y}$ and $\\pmb{b}^{\\top}=\\pmb{H}^{\\top}\\boldsymbol{y}_{1}+\\tilde{\\pmb{y}}^{\\top}\\left(\\pmb{F}-\\pmb{\\Delta}_{\\tilde{D}}^{-1}\\right)$ ", "page_idx": 15}, {"type": "text", "text": "Expanding the square, we get three different integrals, denoted $A,B$ and $C$ with ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle A=\\oint_{S}\\int_{t_{0}}^{\\infty}a^{2}k^{2}((\\boldsymbol{x},t),(\\boldsymbol{x}_{1},t_{1}))d\\boldsymbol{x}d t,}\\\\ {\\displaystyle B=\\oint_{S}\\int_{t_{0}}^{\\infty}2a b\\boldsymbol{k}((\\boldsymbol{x},t),(\\boldsymbol{x}_{1},t_{1}))\\boldsymbol{k}((\\boldsymbol{x},t),\\tilde{\\mathcal{D}})d\\boldsymbol{x}d t,}\\\\ {\\displaystyle C=\\oint_{S}\\int_{t_{0}}^{\\infty}b\\boldsymbol{k}\\left((\\boldsymbol{x},t),\\tilde{\\mathcal{D}}\\right)\\boldsymbol{k}^{\\top}\\left((\\boldsymbol{x},t),\\tilde{\\mathcal{D}}\\right)\\boldsymbol{b}^{\\top}d\\boldsymbol{x}d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using Lemma B.1, computing an upper bound for $A,B$ and $C$ is immediate. In fact, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A\\leq\\lambda^{2}a^{2}C\\big((\\pmb{x}_{1},t_{1}),(\\pmb{x}_{1},t_{1})\\big)}\\\\ &{B\\leq2\\lambda^{2}a b C\\big((\\pmb{x}_{1},t_{1}),\\tilde{D}\\big)}\\\\ &{C\\leq\\lambda^{2}\\mathbf{1}^{\\top}\\left(b b^{\\top}\\odot C(\\tilde{D},\\tilde{D})\\right)\\mathbf{1}}\\\\ &{\\phantom{D}=\\lambda^{2}\\operatorname{tr}\\left(b b^{\\top}C(\\tilde{D},\\tilde{D})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\odot$ is the Hadamard product and 1 the conformable vector of ones. ", "page_idx": 15}, {"type": "text", "text": "Adding (40), (41) and (42) together concludes our proof. ", "page_idx": 15}, {"type": "text", "text": "To get the first part of Theorem 4.1, we prove the following lemma. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.3. Let $t_{0}$ be the present time and $\\mathcal{D}=\\{((\\mathbf{}x_{i},t_{i}),y_{i})\\}_{i\\in[\\![1,n]\\!]}$ be a dataset of observations before $t_{0}$ . Let $\\tilde{\\mathcal{D}}=\\{((\\boldsymbol{x}_{i},t_{i}),y_{i})\\}_{i\\in[\\![2,n]\\!]}$ and $\\mathcal{F}_{t_{0}}=S\\times[t_{0},+\\infty)$ the  domain of future predictions. Then, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\oint_{\\cal S}\\int_{t_{0}}^{\\infty}(\\sigma_{\\mathcal D}({\\pmb x},t)-\\sigma_{\\tilde{\\mathcal D}}({\\pmb x},t))^{2}d{\\pmb x}d t\\leq\\lambda^{2}E C(({\\pmb x}_{1},t_{1}),({\\pmb x}_{1},t_{1}))+\\lambda^{2}c C(({\\pmb x}_{1},t_{1}),\\tilde{\\mathcal D})}\\\\ &{}&{+\\,\\lambda^{2}\\,\\mathrm{tr}\\left(M C(\\tilde{\\mathcal D},\\tilde{\\mathcal D})\\right),~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r l}&{w i t h\\quad\\;c\\quad\\;=\\quad\\;G\\quad+\\quad H^{\\top},\\quad\\;M\\quad\\;=\\quad\\;\\;F\\quad-\\quad\\Delta_{\\bar{\\mathcal{D}}}^{-1},\\quad\\;C(\\mathcal{X},\\mathcal{y})}\\\\ &{\\left((k_{S}*k_{S})(x_{j}-x_{i})(k_{T}*k_{T})_{t_{0}-t_{i}}^{+\\infty}(t_{j}-t_{i})\\right)_{(x_{i},t_{i})\\in\\mathcal{X}}\\;,\\;\\;(f\\;*\\;g)\\quad t h e\\;\\;c o n v o l u t i o n\\;\\;b e t w e e n\\;\\;=\\;\\frac{t h^{2}}{2\\pi\\bar{t}},}\\end{array}$ $=$ $f$ and $g$ and $(f*g)_{a}^{b}$ the convolution between $f$ and $g$ restricted to the interval $[a,b]$ . ", "page_idx": 15}, {"type": "text", "text": "Proof. This proof is conceptually identical to the previous one. By Proposition A.3, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{S}\\int_{t_{0}}^{\\infty}(\\sigma_{\\mathcal{D}}(x,t)-\\sigma_{\\mathcal{\\Bar{D}}}(x,t))^{2}d x d t\\le\\oint_{S}\\int_{t_{0}}^{\\infty}(E k((x,t),(x_{1},t_{1}))^{2}+k((x,t),(x_{1},t_{1}))c k((x,t),\\eta)}\\\\ &{}&{\\qquad+\\,k^{\\top}((x,t),\\boldsymbol{\\Tilde{\\mathcal{D}}})M k((x,t),\\boldsymbol{\\Tilde{\\mathcal{D}}}))d x d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $c=G+H^{\\top}$ and $M=F-\\Delta_{\\tilde{\\mathcal{D}}}^{-1}$ . ", "page_idx": 15}, {"type": "text", "text": "By linearity of the integral, the RHS of (44) can be split into three different integrals $A,B$ and $C$ where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{A=\\oint_{S}\\int_{t_{0}}^{\\infty}E k((x,t),(x_{1},t_{1}))^{2}d x d t}}}\\\\ {{\\displaystyle{B=\\oint_{S}\\int_{t_{0}}^{\\infty}k((x,t),(x_{1},t_{1}))c k((x,t),\\tilde{D})d x d t}}}\\\\ {{\\displaystyle{C=\\oint_{S}\\int_{t_{0}}^{\\infty}k^{\\top}((x,t),\\tilde{D})M k((x,t),\\tilde{D})d x d t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Once again, Lemma B.1 can be used to compute an upper bound for $A,B$ and $C$ . In fact, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A\\leq\\lambda^{2}E C((\\pmb{x}_{1},t_{1}),(\\pmb{x}_{1},t_{1}))}\\\\ &{B\\leq\\lambda^{2}c C((\\pmb{x}_{1},t_{1}),\\tilde{D})}\\\\ &{C\\leq\\lambda^{2}\\mathbf{1}^{\\top}\\left(M\\odot C(\\tilde{\\mathcal{D}},\\tilde{\\mathcal{D}})\\right)\\mathbf{1}}\\\\ &{\\quad=\\mathrm{tr}\\left(M C(\\tilde{\\mathcal{D}},\\tilde{\\mathcal{D}})\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\odot$ is the Hadamard product and 1 is the conformable vector of ones. ", "page_idx": 16}, {"type": "text", "text": "Adding (45), (46) and (47) together concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "Together, Lemmas B.1, B.2 and B.3 yield the first part of Theorem 4.1. For the second part of Theorem 4.1, we prove the following lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.4. Let $t_{0}$ be the present time and $\\mathcal{D}=\\{((\\mathbf{}x_{i},t_{i}),y_{i})\\}_{i\\in[\\![1,n]\\!]}$ be a dataset of observations before $t_{0}$ . Let $\\mathcal{F}_{t_{0}}=S\\times[t_{0},+\\infty)$ the domain of future prediction s. T hen, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{2}^{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\varnothing})\\leq\\lambda^{2}\\left(\\pmb{a}^{\\top}C\\left(\\mathcal{D},\\mathcal{D}\\right)\\pmb{a}+\\operatorname{tr}\\left(\\pmb{\\Delta}^{-1}C\\left(\\mathcal{D},\\mathcal{D}\\right)\\right)\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with $\\pmb{a}\\,=\\,\\pmb{\\Delta}^{-1}\\pmb{y}$ and C(X, Y) = (kS \u2217kS)(xj \u2212xi)(kT \u2217kT )t+0\u2212\u221eti(tj \u2212ti) (xi,ti)\u2208X, $(f*g)$ $(\\pmb{x}_{j},t_{j})\\!\\in\\!\\mathcal{D}$ the convolution between $f$ and $g$ and $(f*g)_{a}^{b}$ the convolution between $f$ and $g$ restricted to the interval $[a,b]$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Recall that, according to ${\\mathcal{G P}}_{\\varnothing}$ , $f(\\mathbf{\\boldsymbol{x}},t)\\sim\\mathcal{N}\\left(0,\\lambda\\right)$ for any point $(\\pmb{x},t)\\in\\mathcal{F}_{t_{0}}$ . Consequently, ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\emptyset})=\\left(\\oint_{S}\\int_{t_{0}}^{\\infty}\\mu_{\\mathcal{D}}^{2}(\\mathbf{x},t)d x d t+\\oint_{S}\\int_{t_{0}}^{\\infty}\\left(\\sqrt{\\lambda}-\\sigma_{\\mathcal{D}}(\\mathbf{x},t)\\right)^{2}d x d t\\right)^{\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "These two integrals in (49) can be computed with the same techniques as above. For the mean integral, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\oint_{S}\\int_{t_{0}}^{\\infty}\\mu_{D}^{2}(x,t)d x d t=\\oint_{S}\\int_{t_{0}}^{\\infty}y^{\\top}\\Delta^{-\\top}k^{\\top}((x,t),\\mathcal{D})k((x,t),\\mathcal{D})\\Delta^{-1}y d x d t}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad=y^{\\top}\\Delta^{-\\top}\\left(\\oint_{S}\\int_{t_{0}}^{\\infty}k^{\\top}((x,t),\\mathcal{D})k((x,t),\\mathcal{D})d x d t\\right)\\Delta^{-1}y}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\leq\\lambda^{2}y^{\\top}\\Delta^{-\\top}C(\\mathcal{D},\\mathcal{D})\\Delta^{-1}y.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Regarding the variance integral in (49), we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\oint_{S}\\int_{t_{0}}^{\\infty}\\left(\\sqrt{\\lambda}-\\sigma_{\\mathcal{D}}(\\pmb{x},t)\\right)^{2}d\\pmb{x}d t=\\oint_{S}\\int_{t_{0}}^{\\infty}\\left(\\lambda-2\\sqrt{\\lambda}\\sigma_{\\mathcal{D}}(\\pmb{x},t)+\\sigma_{\\mathcal{D}}^{2}(\\pmb{x},t)\\right)d\\pmb{x}d t}}\\\\ &{}&{\\leq\\oint_{S}\\int_{t_{0}}^{\\infty}\\left(\\lambda-\\sigma_{\\mathcal{D}}^{2}(\\pmb{x},t)\\right)d\\pmb{x}d t}\\\\ &{}&{=\\oint_{S}\\int_{t_{0}}^{\\infty}k^{\\top}((\\pmb{x},t),\\mathcal{D})\\Delta^{-1}k((\\pmb{x},t),\\mathcal{D})d\\pmb{x}d t}\\\\ &{}&{\\leq\\lambda^{2}\\operatorname{tr}\\left(\\Delta^{-1}C\\left(\\mathcal{D},\\mathcal{D}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (51) holds because $\\sqrt{\\lambda}\\sigma_{\\mathscr D}({\\pmb x},t)\\geq\\sigma_{\\mathscr D}^{2}({\\pmb x},t)$ and (52) holds because of (4). ", "page_idx": 17}, {"type": "text", "text": "Together, (50) and (53) conclude the proof. ", "page_idx": 17}, {"type": "text", "text": "By combining Lemmas B.2 and B.3, the proof of Theorem 4.1 is immediate. ", "page_idx": 17}, {"type": "text", "text": "C Approximation Error ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Appendix B, we provided a computationally tractable upper bound of $W_{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\tilde{\\mathcal{D}}})$ and $W_{2}(\\mathcal{G P}_{\\mathcal{D}},\\mathcal{G P}_{\\varnothing})$ . In this appendix, we provide the expression of the corresponding approximation error and we study its magnitude with the Squared-Exponential (SE) covariance function. ", "page_idx": 17}, {"type": "text", "text": "First, and without loss of generality, assume $\\begin{array}{r}{\\mathcal{S}=[0,1]^{d}}\\end{array}$ . In Appendix B, we have to approximate the Wasserstein distance because the integration over $\\boldsymbol{S}$ in (34) is difficult to compute. The upper bound proposed in (35) is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\oint_{[0,1]^{d}}k_{S}(||\\mathbf x-\\mathbf x_{i}||_{2})k_{S}(||\\mathbf x-\\mathbf x_{j}||_{2})d x\\leq\\oint_{\\mathbb R^{d}}k_{S}(||\\mathbf x-\\mathbf x_{i}||_{2})k_{S}(||\\mathbf x-\\mathbf x_{j}||_{2})d x.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Recall that the upper bounded quantity is a product of functions which are decreasing exponentially (see Table 1). As a consequence, their product decreases exponentially as well, so that extending the integration from $\\mathcal{S}=[0,1]^{d}$ to $\\mathbb{R}^{d}$ has a bounded impact on the result. Clearly, a first absolute approximation error for (54) is ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\oint_{(\\mathbb{R}\\backslash[0,1])^{d}}k_{S}(||\\pmb{x}-\\pmb{x}_{i}||_{2})k_{S}(||\\pmb{x}-\\pmb{x}_{j}||_{2})d\\pmb{x}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Because the upper bounded quantity is a product of two correlation functions $k_{S}$ on a hypercube of volume 1, the upper bound can be capped to 1 as well. This leads to the more refined absolute approximation error: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A(x_{i},x_{j};l_{S})=\\operatorname*{min}\\Bigg\\{\\displaystyle\\oint_{(\\mathbb{R}\\backslash[0,1])^{d}}k_{S}(||x-x_{i}||_{2})k_{S}(||x-x_{j}||_{2})d x,}\\\\ {1-\\displaystyle\\oint_{[0,1]^{d}}k_{S}(||x-x_{i}||_{2})k_{S}(||x-x_{j}||_{2})d x\\Bigg\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Obtaining a closed-form for the approximation error (55) is difficult. However, because the spatial lengthscale controls the correlation lengths in the spatial domain, it is clear that the left term in (55) is an increasing function with respect to $l_{S}$ . Conversely, the right term in (55) is a decreasing function with respect to $l_{S}$ . This observation allows us to derive the spatial lengthscale for which (55) is maximal. ", "page_idx": 17}, {"type": "text", "text": "Proposition C.1. Let $(x_{i},\\pmb{x}_{j})\\in S^{2}$ , with $\\begin{array}{r}{\\mathcal{S}=[0,1]^{d}}\\end{array}$ . Let $k_{S}$ be a $S E$ kernel with lengthscale $l_{S}$ Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathop{\\arg\\operatorname*{max}}_{l_{S}\\in\\mathbb{R}^{+}}A(\\pmb{x}_{i},\\pmb{x}_{j};l_{S})=\\frac{1}{\\sqrt{\\pi}}e^{\\frac{1}{2}W_{0}\\left(\\frac{\\pi||\\pmb{x}_{i}-\\pmb{x}_{j}||_{2}^{2}}{2d}\\right)},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $W_{0}$ the principal branch of the Lambert function. ", "page_idx": 17}, {"type": "text", "text": "Proof. Because the two terms in (55) are respectively increasing and decreasing with respect to the spatial lengthscale $l_{S}$ , (55) is maximal when both terms are equal. Therefore, from (55), we have the following relation ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\oint_{(\\mathbb{R}\\setminus[0,1])^{d}}k_{S}(||\\mathbf{x}-\\mathbf{x}_{i}||_{2})k_{S}(||\\mathbf{x}-\\mathbf{x}_{j}||_{2})d x=1-\\oint_{[0,1]^{d}}k_{S}(||\\mathbf{x}-\\mathbf{x}_{i}||_{2})k_{S}(||\\mathbf{x}-\\mathbf{x}_{j}||_{2})d x,}}\\\\ &{}&{\\oint_{\\mathbb{R}^{d}}k_{S}(||\\mathbf{x}-\\mathbf{x}_{i}||_{2})k_{S}(||\\mathbf{x}-\\mathbf{x}_{j}||_{2})d x=1,}\\\\ &{}&{\\pi^{\\frac{d}{2}}l_{S}^{d}e^{\\frac{-||\\mathbf{x}_{i}-\\mathbf{x}_{j}||_{2}^{2}}{4l_{S}^{2}}}=1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "kN7GTUss0l/tmp/10260304525ff0d7a2ee3ffa4887fb52225ad3263aa112262ed49a1f5a4b8c25.jpg", "img_caption": ["Figure 6: (Top row) Absolute approximation error (55) with respect to the spatial lengthscale $l_{S}$ for a 1, 3 and 5-dimensional spatial domain. Both error terms in (55) are shown in orange and green dashed lines, respectively. Finally, the critical lengthscale (56) is shown as a red vertical line. In this example, $k_{S}$ is a SE correlation function. (Bottom row) Relative approximation error with respect to the spatial lengthscale $l_{S}$ . The color codes are the same. "], "img_footnote": [], "page_idx": 18}, {"type": "image", "img_path": "kN7GTUss0l/tmp/02ec8a02a0f4d4d879a0eee7aaf9e6a495885755cf741e0d4fe5fef8d12864c7.jpg", "img_caption": ["Figure 7: Relative error between the criterion (9) and its approximation (12), with respect to the spatial lengthscale $l_{S}$ for a 1, 3 and 5-dimensional spatial domain. The relative error computed with both terms in (55) are shown as orange and green dashed lines, respectively. In this example, $k_{S}$ is a SE correlation function. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "where (57) uses a result derived in Appendix $\\mathrm{D}$ and reported in Table 3. Solving for $l_{S}$ concludes the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Let us illustrate (55) and Proposition C.1 by plotting the relative approximation error (55) with respect to the spatial lengthscale $l_{S}$ . The integral over $\\boldsymbol{S}$ is computed with a Monte-Carlo numerical integration technique. The results are shown in Figure 6. Looking at the top row, we see that the absolute approximation error peaks at a spatial lengthscale $l_{S}^{*}$ given by (56), as anticipated above. For $l_{S}<l_{S}^{*}$ , the error is given by the first error term in (55), and conversely, the error is given by the second error term in (55) for $l_{S}>l_{S}^{*}$ . The bottom row of Figure 6 shows that the same observations apply to the relative errors. ", "page_idx": 18}, {"type": "text", "text": "Figure 6 shows that even though it is bounded, the approximation error of the upper bound (54) is non-negligible. This is particularly noticeable when looking at the relative errors in the bottom row of Figure 6, which clearly increases in magnitude with the dimensionality of the spatial domain. ", "page_idx": 18}, {"type": "text", "text": "Nevertheless, recall that we seek to approximate the ratio (9) with a ratio of upper bounded Wasserstein distances (12) that involve (54) both in the numerator and in the denominator. Because the spatial lengthscale $l_{S}$ does not vary when computing the numerator and the denominator, the errors are of similar magnitude and point in the same direction (both the numerator and the denominator are upper ", "page_idx": 18}, {"type": "table", "img_path": "kN7GTUss0l/tmp/1801db58b5c39bfb6e3193866ca3bc22d07b205b74bea6411d66ccf4364dd7e6.jpg", "table_caption": ["Table 3: Analytic forms for the convolution of usual spatial covariance functions. $\\Gamma$ is the Gamma function, $J_{\\alpha}$ is a Bessel function of the first kind of order $\\alpha$ , $K_{\\alpha}$ is a modified Bessel function of the second kind of order $\\alpha$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 4: Analytic forms for the convolution of usual temporal covariance functions on the interval $[t_{0}-t_{i},+\\infty)$ . Note that erf is the error function. Also, for the sake of brevity, the terms $C_{k_{1}k_{2}},P_{k_{1}k_{2}}$ and $Q_{k_{1}k_{2}}$ are defined in Appendix D.2.2. ", "page_idx": 19}, {"type": "table", "img_path": "kN7GTUss0l/tmp/9ab7f2d8068135c001efa9316f8cb4c4edbae0d0673a69caeb575ff1289cc688.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "bounds). As a consequence, the approximation errors compensate each other (at least in part) when computing (12). To verify this observation numerically, we compute the relative approximation error between the criterion (9) and its approximation (12). The results are shown in Figure 7. Although the approximation errors in the numerator and the denominator do not entirely compensate each other, the approximation (12) appears to have lost most of its dependency to the dimensionality of the spatial domain and to the spatial lengthscale, making (12) a decent approximation of (9) regardless of $d$ or $l_{S}$ . In the main paper, Section 5 corroborates this observation by demonstrating the usefulness of the approximation (12) in practice. ", "page_idx": 19}, {"type": "text", "text": "D Convolutions of Usual Covariance Functions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this appendix, we derive the analytic forms of the convolution of usual covariance functions listed in Tables 3 and 4, which are used to compute the criterion (12). ", "page_idx": 19}, {"type": "text", "text": "D.1 Spatial Covariance Functions ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this subsection, we compute specifically the analytic forms for the convolution of usual spatial covariance functions. We rely on a direct consequence of the convolution theorem, that is $(k*k)({\\pmb x})=$ $\\mathcal{F}^{-1}\\left(\\mathcal{F}^{2}\\left(k\\right)\\right)\\left(\\pmb{x}\\right)$ , with $\\mathcal{F}(f)$ denoting the Fourier transform of $f$ and ${\\mathcal{F}}^{-1}(f)$ the inverse Fourier transform of $f$ . ", "page_idx": 19}, {"type": "text", "text": "The Fourier transform $\\mathcal{F}(k)$ of a stationary covariance function $k$ is called the spectral density of $k$ , and is usually denoted $S$ . Both functions are Fourier duals of each other (see [27] for more details). Furthermore, it is known that if $k$ is isotropic (i.e. it can be written as a function of $r=||\\pmb{x}||_{2})$ , then its spectral density $S(s)$ can be written as a function of $s=||\\pmb{\\mathscr{s}}||_{2}$ . In that case, the two functions are linked by the pair of transforms (see [27]) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle k(\\boldsymbol{r})=\\frac{2\\pi}{r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}\\boldsymbol{S}(s)J_{\\frac{d}{2}-1}(2\\pi r s)s^{\\frac{d}{2}}d s}\\\\ {\\displaystyle S(s)=\\frac{2\\pi}{s^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}k(\\boldsymbol{r})J_{\\frac{d}{2}-1}(2\\pi r s)r^{\\frac{d}{2}}d r}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $J_{\\frac{d}{2}-1}$ is a Bessel function of the first kind and of order $d/2-1$ ", "page_idx": 19}, {"type": "text", "text": "As an immediate consequence of (58), (59) and the convolution theorem, we have the following corollary. ", "page_idx": 19}, {"type": "text", "text": "Corollary D.1. Let $k$ be a stationary, isotropic covariance function with spectral density $S$ . Let $r=||{\\boldsymbol{x}}||_{2}$ and $s=||\\pmb{\\mathscr{s}}||_{2}$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n(k*k)(r)=\\frac{2\\pi}{r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}S^{2}(s)J_{\\frac{d}{2}-1}(2\\pi r s)s^{\\frac{d}{2}}d s\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $J_{\\frac{d}{2}-1}$ is a Bessel function of the first kind and of order $d/2-1$ ", "page_idx": 20}, {"type": "text", "text": "We now derive the analytic forms for the convolutions of usual spatial covariance functions $k_{S}$ . ", "page_idx": 20}, {"type": "text", "text": "D.1.1 Squared-Exponential Covariance Function ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma D.2. Let $k_{S}$ be a Squared-Exponential covariance function (see Table $^{\\,I}$ ), with lengthscale $l_{S}>0$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(k_{S}*k_{S})(\\pmb{x})=\\pi^{\\frac{d}{2}}l_{S}^{d}e^{\\frac{-\\vert\\vert\\pmb{x}\\vert\\vert_{2}^{2}}{4l_{S}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The spectral density of a Squared-Exponential covariance function $k_{S}$ is (see [27]) ", "page_idx": 20}, {"type": "equation", "text": "$$\nS(s)=(2\\pi l_{S}^{2})^{\\frac{d}{2}}e^{-2\\pi^{2}l_{S}^{2}s^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "According to Corollary D.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(k_{S}*k_{S})({\\pmb x})=\\displaystyle\\frac{2\\pi}{r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}S^{2}(s)J_{\\frac{d}{2}-1}(2\\pi r s)s^{\\frac{d}{2}}d s}}\\\\ {{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\frac{2\\pi}{r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}(2\\pi l_{S}^{2})^{d}e^{-4\\pi^{2}l_{S}^{2}s^{2}}J_{\\frac{d}{2}-1}(2\\pi r s)s^{\\frac{d}{2}}d s}}\\\\ {{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\displaystyle\\frac{(2\\pi)^{d+1}l_{S}^{2d}}{r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}e^{-4\\pi^{2}l_{S}^{2}s^{2}}J_{\\frac{d}{2}-1}(2\\pi r s)s^{\\frac{d}{2}}d s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $r=||\\pmb{x}||_{2}$ and $J_{\\frac{d}{2}-1}$ is a Bessel function of the first kind of order $\\textstyle{\\frac{d}{2}}-1$ ", "page_idx": 20}, {"type": "text", "text": "It is known (see [31]) that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\int_{0}^{\\infty}e^{-\\alpha x^{2}}x^{\\nu+1}J_{\\nu}(\\beta x)d x=\\frac{\\beta^{\\nu}}{(2\\alpha)^{\\nu+1}}e^{\\frac{-\\beta^{2}}{4\\alpha}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(k_{S}*k_{S})(\\pmb{x})=\\frac{(2\\pi)^{d+1}\\,l_{S}^{2d}}{r^{\\frac{d}{2}-1}}\\frac{(2\\pi r)^{\\frac{d}{2}-1}}{(8\\pi^{2}l_{S}^{2})^{\\frac{d}{2}}}e^{\\frac{-4\\pi^{2}r^{2}}{16\\pi^{2}l_{S}^{2}}}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=\\pi^{\\frac{d}{2}}l_{S}^{d}e^{\\frac{-r^{2}}{4l_{S}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Replacing $r$ by $||\\pmb{x}||_{2}$ in (64) concludes the proof. ", "page_idx": 20}, {"type": "text", "text": "D.1.2 Mat\u00e9rn Covariance Function ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma D.3. Let $k_{S}$ be a Mat\u00e9rn covariance function (see Table $^{\\,l}$ ), with smoothness parameter $\\nu>0$ and lengthscale $l_{S}>0$ . Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n(k_{S}*k_{S})(\\pmb{x})=\\frac{2^{\\frac{d}{2}-2\\nu+1}\\pi^{\\frac{d}{2}}\\Gamma(\\nu+\\frac{d}{2})^{2}}{\\Gamma(\\nu)^{2}\\Gamma(2\\nu+d)}\\left(\\frac{\\sqrt{2\\nu}}{l_{S}}\\right)^{2\\nu-\\frac{d}{2}}\\left|\\left|\\pmb{x}\\right|\\right|_{2}^{2\\nu+\\frac{d}{2}}K_{2\\nu+\\frac{d}{2}}\\left(\\frac{\\left|\\left|\\pmb{x}\\right|\\right|_{2}\\sqrt{2\\nu}}{l_{S}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Gamma$ is the Gamma function and $K_{\\alpha}$ is a modified Bessel function of the second kind of order $\\alpha$ . Proof. The spectral density of a Mat\u00e9rn covariance function $k_{S}$ is (see [27]) ", "page_idx": 20}, {"type": "equation", "text": "$$\nS(s)=\\frac{2^{d}\\pi^{\\frac{d}{2}}\\Gamma(\\nu+\\frac{d}{2})(2\\nu)^{\\nu}}{\\Gamma(\\nu)l_{S}^{2\\nu}}\\left(\\frac{2\\nu}{l_{S}^{2}}+4\\pi^{2}s^{2}\\right)^{-\\nu-\\frac{d}{2}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Gamma$ is the Gamma function. ", "page_idx": 21}, {"type": "text", "text": "According to Corollary D.1, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(k_{S}*k_{S})(x)=\\frac{2\\pi}{r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}S^{2}(s)J_{\\frac{d}{2}-1}(2\\pi r s)s^{\\frac{d}{2}}d s}\\\\ &{\\qquad\\qquad\\qquad=\\frac{2\\pi}{r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}\\frac{2^{2d}\\pi^{d}\\Gamma^{2}(\\nu+\\frac{d}{2})(2\\nu)^{2\\nu}}{\\Gamma^{2}(\\nu)l_{S}^{4\\nu}}\\left(\\frac{2\\nu}{l_{S}^{2}}+4\\pi^{2}s^{2}\\right)^{-2\\nu-d}J_{\\frac{d}{2}-1}(2\\pi r s)s^{\\frac{d}{2}}d s}\\\\ &{\\qquad\\qquad=\\frac{2^{2d+1}\\pi^{d+1}\\Gamma^{2}(\\nu+\\frac{d}{2})(2\\nu)^{2\\nu}}{\\Gamma^{2}(\\nu)l_{S}^{4\\nu}r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}\\left(\\frac{2\\nu}{l_{S}^{2}}+4\\pi^{2}s^{2}\\right)^{-2\\nu-d}J_{\\frac{d}{2}-1}(2\\pi r s)s^{\\frac{d}{2}}d s}\\\\ &{\\qquad\\qquad=\\frac{2^{\\frac{3d}{2}}\\pi^{\\frac{d}{2}}\\Gamma^{2}(\\nu+\\frac{d}{2})(2\\nu)^{2\\nu}}{\\Gamma^{2}(\\nu)l_{S}^{4\\nu}r^{\\frac{d}{2}-1}}\\int_{0}^{\\infty}\\left(\\frac{2\\nu}{l_{S}^{2}}+u^{2}\\right)^{-2\\nu-d}J_{\\frac{d}{2}-1}(r u)u^{\\frac{d}{2}}d u}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ${\\cal J}_{\\frac{d}{2}-1}$ is a Bessel function of the first kind of order $\\textstyle{\\frac{d}{2}}-1$ , and (67) comes from the change of variable $u=2\\pi s$ . ", "page_idx": 21}, {"type": "text", "text": "It is known (see [31]) that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{0}^{\\infty}(a^{2}+x^{2})^{-(\\mu+1)}J_{\\alpha}(b x)x^{\\alpha+1}d x={\\frac{a^{\\alpha-\\mu}b^{\\mu}}{2^{\\mu}\\Gamma(\\mu+1)}}K_{\\mu-\\alpha}(a b)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $K_{\\mu-\\alpha}$ is a modified Bessel function of the second kind of order $\\mu-\\alpha$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{(k_{S}*k_{S})(\\pmb{x})=\\frac{2^{\\frac{3d}{2}}\\pi^{\\frac{d}{2}}\\Gamma^{2}(\\nu+\\frac{d}{2})(2\\nu)^{2\\nu}}{\\Gamma^{2}(\\nu)l_{S}^{4\\nu}r^{\\frac{d}{2}-1}}\\left(\\frac{\\sqrt{2\\nu}}{l_{S}}\\right)^{-2\\nu-\\frac{d}{2}}\\frac{r^{2\\nu+d-1}}{2^{2\\nu+d-1}\\Gamma(2\\nu+d)}K_{2\\nu+\\frac{d}{2}}\\left(\\frac{r\\sqrt{2\\nu}}{l_{S}}\\right)}\\\\ &{}&{=\\frac{2^{\\frac{d}{2}-2\\nu+1}\\pi^{\\frac{d}{2}}\\Gamma(\\nu+\\frac{d}{2})^{2}}{\\Gamma(\\nu)^{2}\\Gamma(2\\nu+d)}\\left(\\frac{\\sqrt{2\\nu}}{l_{S}}\\right)^{2\\nu-\\frac{d}{2}}r^{2\\nu+\\frac{d}{2}}K_{2\\nu+\\frac{d}{2}}\\left(\\frac{r\\sqrt{2\\nu}}{l_{S}}\\right).\\qquad\\qquad\\quad(68)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Replacing $r$ by $||\\pmb{x}||_{2}$ in (68) concludes the proof. ", "page_idx": 21}, {"type": "text", "text": "D.2 Temporal Covariance Functions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we derive the analytic expression of the convolutions of the most popular temporal covariance functions $k_{T}$ restricted to the interval $[t_{0}\\mathrm{~-~}t_{i},+\\infty)$ . Therefore, we compute many integrals of the form ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{t_{0}-t_{i}}^{+\\infty}k_{T}(|t|)k_{T}(|t_{j}-t_{i}-t|)d t,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which can be rewritten as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int_{t_{0}-t_{i}}^{+\\infty}k_{T}(t)k_{T}(t+t_{i}-t_{j})d t.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "since $t\\geq0$ and $t_{j}-t_{i}-t\\leq0$ for all $t\\in[t_{0}-t_{i},+\\infty)$ . The form (69) will be used in every proof of this section. ", "page_idx": 21}, {"type": "text", "text": "D.2.1 Squared-Exponential Covariance Function ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma D.4. Let $k_{T}$ be a Squared-Exponential covariance function (see Table $^{\\,I}$ ), with lengthscale $l_{S}>0$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n(k_{T}*k_{T})_{t_{0}-t_{i}}^{+\\infty}(t_{i}-t_{j})=\\frac{\\sqrt{\\pi}l_{T}}{2}e^{\\frac{-(t_{i}-t_{j})^{2}}{2l_{T}^{2}}}\\left(1-e r f\\bigg(\\frac{2t_{0}-t_{i}-t_{j}}{2l_{T}}\\bigg)\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where erf is the error function. ", "page_idx": 21}, {"type": "text", "text": "Proof. Since $k_{T}$ is a Squared-Exponential function, (69) becomes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\int_{t_{0}-t_{i}}^{+\\infty}e^{\\frac{-t_{2}^{2}}{2t_{T}^{2}}}e^{\\frac{-(t-t_{j}+t_{i})^{2}}{2l_{T}^{2}}}d t}\\\\ &{=\\int_{t_{0}-t_{i}}^{+\\infty}e^{\\frac{-(2t^{2}-2(t_{j}-t_{i})t+t_{j}^{2}+t_{i}^{2}-2t_{i}t_{j})}{2l_{T}^{2}}}d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is known (see [31]) that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\int e^{-(a x^{2}+2b x+c)}d x={\\frac{1}{2}}{\\sqrt{\\frac{\\pi}{a}}}e^{\\frac{b^{2}-a c}{a}}\\mathrm{erf}\\left({\\sqrt{a}}x+{\\frac{b}{\\sqrt{a}}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where erf is the error function. ", "page_idx": 22}, {"type": "text", "text": "Therefore, (71) becomes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{\\pi}l_{T}}{2}e^{\\frac{-(t_{i}-t_{j})^{2}}{2l_{T}^{2}}}\\left(1-\\mathrm{erf}\\left(\\frac{2t_{0}-t_{i}-t_{j}}{2l_{T}}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "D.2.2 Mat\u00e9rn Covariance Function ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Lemma D.5. Let $k_{T}$ be a Mat\u00e9rn covariance function (see Table $^{\\,l}$ ), with smoothness parameter $\\begin{array}{r}{\\nu=p+\\frac{1}{2},p\\in\\mathbb{N}}\\end{array}$ and lengthscale $l_{T}>0$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n(k_{T}*k_{T})_{t_{0}-t_{i}}^{l_{T}+t_{0}-t_{i}}(t_{i}-t_{j})=\\sum_{k_{1}=0}^{p}\\sum_{k_{2}=0}^{p}C_{k_{1}k_{2}}e^{\\frac{-\\sqrt{2p+1}(2t_{0}-t_{i}-t_{j})}{l_{T}}}P_{k_{1}k_{2}}(t_{0},t_{i},t_{j})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C_{k_{1}k_{2}}=\\displaystyle\\left(\\frac{p!}{(2p)!}\\right)^{2}\\displaystyle\\frac{(p+k_{1})!(p+k_{2})!}{k_{1}!k_{2}!(p-k_{1})!(p-k_{2})!}\\left(\\frac{2\\sqrt{2p+1}}{l_{T}}\\right)^{2p-k_{1}-k_{2}-1},}}\\\\ {{P_{k_{1}k_{2}}(t_{0},t_{i},t_{j})=\\displaystyle\\sum_{k_{3}=0}^{2p-k_{1}-k_{2}}\\left(\\frac{l_{T}}{2\\sqrt{2p+1}}\\right)^{k_{3}}P^{(k_{3})}(t_{0}-t_{i}),}}\\\\ {{P(t)=t^{p-k_{1}}(t-t_{j}+t_{i})^{p-k_{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and $P^{(k)}$ the kth derivative of $P(t)$ with respect to $t$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. The Mat\u00e9rn covariance function has a simpler form when its smoothness parameter $\\nu$ is a half-integer, that is $\\begin{array}{r}{\\nu=p+\\frac{1}{2},p\\in\\mathbb{N}}\\end{array}$ (see [30]). In that case, ", "page_idx": 22}, {"type": "equation", "text": "$$\nk_{T}(t)=e^{\\frac{-\\sqrt{2p+1}t}{l_{T}}}\\frac{p!}{(2p)!}\\sum_{k_{1}=0}^{p}\\frac{(p+k_{1})!}{k_{1}!(p-k_{1})!}\\left(\\frac{2\\sqrt{2p+1}t}{l_{T}}\\right)^{p-k_{1}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, ", "page_idx": 22}, {"type": "equation", "text": "$$\nk_{T}(t)k_{T}(t+t_{i}-t_{j})=\\frac{2\\sqrt{2p+1}}{l_{T}}\\sum_{k_{1}=0}^{p}\\sum_{k_{2}=0}^{p}C_{k_{1}k_{2}}e^{\\frac{-\\sqrt{2p+1}(2t-t_{j}+t_{i})}{l_{T}}}P(t)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with $C_{k_{1}k_{2}}$ defined in (73) and $P(t)$ defined in (75). ", "page_idx": 22}, {"type": "text", "text": "Integrating (76), we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{2\\sqrt{2p+1}}{l_{T}}\\sum_{k_{1}=0}^{p}\\sum_{k_{2}=0}^{p}C_{k_{1}k_{2}}e^{\\frac{-\\sqrt{2p+1}(t_{i}-t_{j})}{l_{T}}}\\int_{t_{0}-t_{i}}^{+\\infty}e^{\\frac{-2\\sqrt{2p+1}t}{l_{T}}}P(t)d t\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "thanks to the linearity of the integral. ", "page_idx": 22}, {"type": "text", "text": "It is known (see [31]) that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int P(x)e^{a x}d x={\\frac{e^{a x}}{a}}\\sum_{k=0}^{m}(-1)^{k}{\\frac{P_{m}^{(k)}(x)}{a^{k}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $P_{m}$ is a polynomial of degree $m$ and $P_{m}^{(k)}$ is the $k$ th derivative of $P_{m}$ ", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int e^{\\frac{-2\\sqrt{2p+1}t}{l_{T}}}P(t)d t=-\\frac{l_{T}}{2\\sqrt{2p+1}}e^{-\\frac{2\\sqrt{2p+1}t}{l_{T}}}\\sum_{k_{3}=0}^{2p-k_{1}-k_{2}}\\frac{P^{(k_{3})}(t)l_{T}^{k_{3}}}{\\big(2\\sqrt{2p+1}\\big)^{k_{3}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining (77) and (78) we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n(k_{T}*k_{T})_{t_{0}-t_{i}}^{+\\infty}(t_{i}-t_{j})=\\sum_{k_{1}=0}^{p}\\sum_{k_{2}=0}^{p}C_{k_{1}k_{2}}e^{\\frac{-\\sqrt{2p+1}(2t_{0}-t_{i}-t_{j})}{l_{T}}}P_{k_{1}k_{2}}(t_{0},t_{i},t_{j})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with $C_{k_{1}k_{2}}$ defined in (73) and $P_{k_{1}k_{2}}$ defined in (74). ", "page_idx": 23}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "E Extension to Anisotropic Spatial Kernels ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this appendix, we illustrate how Theorem 4.1 could be extended to anisotropic spatial kernels by considering an Automatic Relevance Detection (ARD) Squared-Exponential (SE). It has the following form: ", "page_idx": 23}, {"type": "equation", "text": "$$\nk_{S}(\\pmb{x},\\pmb{y})=e^{-\\frac{1}{2}(\\pmb{x}-\\pmb{y})^{\\top}M^{-2}(\\pmb{x}-\\pmb{y})}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\boldsymbol{M}\\,=\\,\\mathrm{diag}\\,(l_{1},\\cdots\\,,l_{d})$ is a diagonal matrix that gathers a different lengthscale for each dimension. Observe that the isotropic SE with lengthscale $l_{S}$ is retrieved by setting $M=l_{S}I$ . ", "page_idx": 23}, {"type": "text", "text": "Because the ARD SE kernel (79) is anisotropic, the convolution with itself ", "page_idx": 23}, {"type": "equation", "text": "$$\n(k_{S}*k_{S})({\\pmb x}-{\\pmb y})=\\oint_{\\mathbb{R}^{d}}k_{S}({\\pmb x},z)k_{S}({\\pmb y},z)d z\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "cannot be simplified to a one-dimensional integral through a change to polar coordinates, as done in Corollary D.1. The integral becomes more complex, but can still be computed exactly for some kernel such as the ARD SE. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.1. Let $k_{S}$ be an ARD $S E$ covariance function with parameter $_M$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n(k_{S}*k_{S})(\\pmb{x}-\\pmb{y})=\\pi^{\\frac d2}\\operatorname*{det}\\left(\\pmb{M}\\right)e^{-\\frac14(\\pmb{x}-\\pmb{y})^{\\top}\\pmb{M}^{-2}(\\pmb{x}-\\pmb{y})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For the ARD SE kernel with parameter $\\boldsymbol{M}=\\mathrm{diag}\\left(l_{1},\\cdots\\,,l_{d}\\right)$ , the convolution (80) is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle k_{S}\\ast k_{S}\\rangle(x-y)=\\displaystyle\\oint_{\\mathbb R^{d}}e^{-\\frac{1}{2}(x-z)^{\\top}M^{-2}(x-z)}e^{-\\frac{1}{2}(y-z)^{\\top}M^{-2}(y-z)}d z}\\\\ &{\\quad\\quad\\quad\\quad\\quad=e^{-\\frac{1}{2}\\left(x M^{-2}x+y M^{-2}y\\right)}\\displaystyle\\oint_{\\mathbb R^{d}}e^{-z^{\\top}M^{-2}z+z^{\\top}M^{-2}(x+y)}d z}\\\\ &{\\quad\\quad\\quad\\quad=e^{-\\frac{1}{2}\\left(x M^{-2}x+y M^{-2}y\\right)}\\displaystyle\\oint_{\\mathbb R^{d}}e^{\\sum_{k=1}^{d}z_{k}(x_{k}+y_{k}-z_{k})/l_{k}^{2}}d z_{1}\\cdot\\cdot\\cdot d z_{d}}\\\\ &{\\quad\\quad\\quad\\quad=e^{-\\frac{1}{2}\\left(x M^{-2}x+y M^{-2}y\\right)}\\displaystyle\\prod_{k=1}^{d}\\int_{-\\infty}^{+\\infty}e^{z_{k}(x_{k}+y_{k}-z_{k})/l_{k}^{2}}d z_{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Integrating the $k$ -th term in (82), we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int_{-\\infty}^{+\\infty}e^{z_{k}(x_{k}+y_{k}-z_{k})/l_{k}^{2}}d z_{k}=\\frac{1}{2}\\sqrt{\\pi}l_{k}e^{\\frac{(x_{k}+y_{k})^{2}}{4l_{k}^{2}}}\\left[\\mathrm{erf}\\left(\\frac{t}{l_{k}}+\\frac{x_{k}+y_{k}}{2l_{k}}\\right)\\right]_{-\\infty}^{+\\infty}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Injecting (83) into (82), we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(k_{S}*k_{S})({\\pmb x}-{\\pmb y})=e^{-\\frac{1}{2}\\left({\\pmb x}{\\pmb M}^{-2}{\\pmb x}+{\\pmb y}{\\pmb M}^{-2}{\\pmb y}\\right)}\\displaystyle\\prod_{k=1}^{d}\\sqrt{\\pi}l_{k}e^{\\frac{(x_{k}+y_{k})^{2}}{4l_{k}^{2}}}}}\\\\ {{\\qquad\\qquad\\qquad\\qquad\\qquad=\\pi^{\\frac{d}{2}}\\operatorname*{det}\\left({\\pmb M}\\right)e^{\\frac{1}{4}({\\pmb x}+{\\pmb y}){\\pmb M}^{-2}({\\pmb x}+{\\pmb y})-\\frac{1}{2}\\left({\\pmb x}{\\pmb M}^{-2}{\\pmb x}+{\\pmb y}{\\pmb M}^{-2}{\\pmb y}\\right)}}}\\\\ {{\\qquad=\\pi^{\\frac{d}{2}}\\operatorname*{det}\\left({\\pmb M}\\right)e^{-\\frac{1}{4}({\\pmb x}-{\\pmb y})^{\\top}{\\pmb M}^{-2}({\\pmb x}-{\\pmb y})}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (84) holds because the determinant of a diagonal matrix is the product of its diagonal elements. ", "page_idx": 24}, {"type": "text", "text": "As a safety check, observe that Lemma D.2 is a special case of Lemma E.1 where $M=l_{S}I$ , that is, when $k_{S}$ is an isotropic SE kernel. ", "page_idx": 24}, {"type": "text", "text": "F Relative Quantification of Relevancy ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this appendix, we discuss how (9) and its approximation (12) address the dependency on the covariance function hyperparameters introduced by (8). For the sake of this discussion, we take $k_{S}$ and $k_{T}$ as two Squared-Exponential (SE) covariance functions (see Table 1). A similar reasoning can be conducted with Mat\u00e9rn covariance functions. ", "page_idx": 24}, {"type": "text", "text": "Let us start by rewriting the product of spatial and temporal convolutions $C(({\\bf{x}},t),({\\bf{x}}^{\\prime},t^{\\prime}))$ with the formulas provided in Tables 3 and 4 for the SE covariance functions. We get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{C((\\pmb{x},t),(\\pmb{x}^{\\prime},t^{\\prime}))=\\pi^{\\frac{d}{2}}l_{S}^{d}e^{\\frac{-||\\pmb{x}-\\pmb{x}^{\\prime}||_{2}^{2}}{4l_{S}^{2}}}\\frac{\\sqrt{\\pi}}{2}l_{T}e^{\\frac{-(t-t^{\\prime})^{2}}{2l_{T}^{2}}}\\left(1-\\mathrm{erf}\\left(\\frac{2t_{0}-t-t^{\\prime}}{2l_{T}}\\right)\\right)}}\\\\ {{=\\frac{1}{2}\\pi^{\\frac{d+1}{2}}l_{S}^{d}l_{T}e^{\\frac{-||\\pmb{x}-\\pmb{x}^{\\prime}||_{2}^{2}}{4l_{S}^{2}}-\\frac{-(t-t^{\\prime})^{2}}{2l_{T}^{2}}}\\left(1-\\mathrm{erf}\\left(\\frac{2t_{0}-t-t^{\\prime}}{2l_{T}}\\right)\\right)}}\\\\ {{=\\frac{1}{2}\\pi^{\\frac{d+1}{2}}l_{S}^{d}l_{T}C^{*}((\\pmb{x},t),(\\pmb{x}^{\\prime},t^{\\prime})).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The dependency on the covariance function hyperparameters $\\lambda,l_{S},l_{T}$ appears clearly in (86). Both $l_{S}$ and $l_{T}$ are used, not only to scale the spatial distance $||\\pmb{x}-\\pmb{x}^{\\prime}||_{2}$ and the temporal distance $|t-t^{\\prime}|$ in $C^{*}$ , but also as a scaling constant of the magnitude of the output of the product of convolutions itself. Because $C(({\\bf{x}},t),({\\bf{x}}^{\\prime},t^{\\prime}))$ is involved in every term of (10) and (11) in Theorem 4.1, ${\\frac{1}{2}}\\pi^{\\frac{d+1}{2}}l_{S}^{d}l_{T}$ can be factored out of (10) and (11). Overall, both equations have in common the factor $\\textstyle{\\frac{1}{2}}\\pi^{\\frac{d+1}{2}}\\lambda^{2}l_{S}^{d}l_{T}$ . Clearly, this shows how the covariance function hyperparameters $\\pmb\\theta=(\\lambda,l_{S},l_{T})$ may control the magnitude of the Wasserstein distances. ", "page_idx": 24}, {"type": "text", "text": "To capture the intrinsic relevancy of an observation regardless of the hyperparameters values, one can compute (12), that is the ratio between (10) and (11). Doing so, the factors which are common to the two equations cancel out. Considering the application of Theorem 4.1 with $k_{S}$ and $k_{T}$ being SE covariance functions, the undesirable factor $\\textstyle{\\frac{1}{2}}\\pi^{\\frac{d+1}{2}}\\lambda^{2}l_{S}^{d}l_{T}$ is removed. Clearly, (12) remains a function of $l_{S}$ and $l_{T}$ , but the hyperparameters are only used to scale the spatial and temporal distances, that is to control correlation lengths. However, the undesirable scaling exposed in (86) no longer exists. ", "page_idx": 24}, {"type": "text", "text": "G Removal Budget ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this appendix, we discuss why the removal budget of W-DBO (see Algorithm 1), denoted $b_{t}$ at a given time $t$ , has the form ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\{b_{0}\\begin{array}{l l}{{}}&{{=1,}}\\\\ {{b_{t+\\Delta{t}}}}&{{=b_{t}(1+\\alpha)^{\\Delta{t}/l_{T}}}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with $l_{T}$ the temporal lengthscale (see Assumption 3.2) and $\\alpha$ the hyperparameter of W-DBO. Crucially, $l_{T}$ and $t$ must be expressed in the same unit of time. ", "page_idx": 24}, {"type": "table", "img_path": "kN7GTUss0l/tmp/050e961dfaaa60875823cc536dc1091c322d84998382dc647fed876486244252.jpg", "table_caption": ["Table 5: Comparison of removal budgets (87) and (88) when doing experiments of different durations on the Hartmann3d synthetic function. All experiments use the same time domain $[0,1]$ . "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "First, note that the expression of the budget is intuitive because (9) measures a ratio, expressed as a percentage. Therefore, the budget must accumulate in a multiplicative way, leading to the exponential form (87). ", "page_idx": 25}, {"type": "text", "text": "More interestingly, let us discuss the exponent $\\Delta t/l_{T}$ . Arguably, an alternative, more intuitive form of the removal budget would be ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{b_{0}}&{=1,}\\\\ {b_{t+\\Delta t}}&{=b_{t}(1+\\alpha)^{\\Delta t}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Although easier to understand, the budget (88) presents a major problem since it depends on arbitrary choices made by the user. This is illustrated by Table 5, where the same synthetic function (Hartmann3d) is optimized under three different durations. When the duration varies, the removal budget (88), which depends on the number of elapsed seconds only, also varies. Conversely, the budget (87) remains the same. This is because the number of temporal lengthscales elapsed during the experiment remains constant, regardless of the experiment duration. ", "page_idx": 25}, {"type": "text", "text": "Using the removal budget (88) becomes really troublesome when it comes to making a recommendation for the hyperparameter $\\alpha$ . If the analysis in Section 5.1 had used the budget (88), its recommendation $\\alpha^{*}$ would have been a function of the temporal lengthscale, and it would have been valid only for experiments with the same duration (e.g., ten minutes). Any other experiment duration would have required another sensitivity analysis. ", "page_idx": 25}, {"type": "text", "text": "Conversely, the recommendation made in Section 5.1, using the budget (87), is a single number that is valid regardless of experiment duration. This is a much more general insight. ", "page_idx": 25}, {"type": "text", "text": "H Empirical Results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "H.1 Experimental Settings ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In each experiment, the $d$ -dimensional spatial domain is scaled in $\\begin{array}{r}{{\\cal S}^{\\prime}=[0,1]^{d}}\\end{array}$ and the temporal domain (viewed as the $(d+1)$ th dimension) is normalized in [0, 1]. Additionally, each optimization task lasts 600 seconds (10 minutes). ", "page_idx": 25}, {"type": "text", "text": "Unless stated otherwise, each DBO algorithm exploits a Matern- $\\cdot5/2$ kernel as its spatial covariance function. GP-UCB, R-GP-UCB and ET-GP-UCB do not explicitly take into account temporal correlations, while TV-GP-UCB uses its own temporal covariance function. Eventually, ABO and W-DBO exploits a Matern- $3/2$ kernel as their temporal covariance function. ", "page_idx": 25}, {"type": "text", "text": "Each DBO algorithm begins its optimization task with 15 initial observations, uniformly sampled in $S^{\\prime}\\times\\left[0,\\frac{1}{40}\\right]$ . At each iteration (at time $t$ ), (i) the noise level as well as the kernel parameters are estimated, and (ii) the GP-UCB acquisition function is optimized to get the next query. The sum of the times taken to perform tasks (i) and (ii) is the response time of the DBO algorithm, denoted by $\\Delta t$ . Clearly, $\\Delta t$ is a function of the dataset size of the DBO algorithm. Consequently, it varies throughout the optimization, getting larger when the DBO algorithm adds a new observation to its dataset, and getting smaller when the DBO algorithm removes at least one point. Once (i) and (ii) are performed, the objective function is immediately sampled (except for ABO which can decide to sample $f$ at a specific time in the future) and a Gaussian noise with variance equal to $5\\,\\%$ of the signal variance is added. Then, the next iteration begins at time $t+\\Delta t$ (except for ABO if it decides to sample $f$ later). ", "page_idx": 25}, {"type": "image", "img_path": "kN7GTUss0l/tmp/e17b37dab8dc00ed15fcba0129c1f922ad780b231c02e557413546ef9eff3c6d.jpg", "img_caption": ["Figure 8: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Rastrigin synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Rastrigin synthetic function. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "For the sake of benchmarking fairness, all the solutions have been implemented using the same popular BO Python library, namely BOTorch [32] (MIT License). To comply with the technical choices (i.e., Python front-end, $C++$ back-end), the computationally-heavy part of W-DBO (i.e., the evaluation of the formulas in Section 4) have been implemented in $C++$ and bound to the Python code with PyBind11 [33] (BSD License). All experiments have been independently replicated 10 times on a laptop equipped with an Intel Core i9-9980HK $\\textcircled{a}\\ 2.40\\,\\mathrm{GHz}$ with 8 cores (16 threads). ", "page_idx": 26}, {"type": "text", "text": "H.2 Benchmarks and Figures ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide here a detailed description of each implemented benchmark and the associated figures. There are two figures associated with each benchmark, showing their average regrets and the size of their datasets throughout the experiment. ", "page_idx": 26}, {"type": "text", "text": "In the following, the synthetic benchmarks will be described as functions of a point $_{\\textit{z}}$ in the $d+1$ - dimensional spatio-temporal domain $s\\times\\tau$ . More precisely, the point $_{\\textit{z}}$ is explicitly given by $\\pmb{z}=(x_{1},\\cdots\\,,\\bar{x}_{d},t)$ . Also, we will write $d^{\\prime}=d+1$ for the sake of brevity. ", "page_idx": 26}, {"type": "text", "text": "Rastrigin. The Rastrigin function is $d^{\\prime}$ -dimensional, and has the form ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(z)=a d^{\\prime}+\\sum_{i=1}^{d^{\\prime}}z_{i}^{2}-a\\cos\\left(2\\pi z_{i}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the numerical evaluation, we set $a=10$ , $d^{\\prime}=5$ and we optimized the function on the domain $[-4,4]^{d^{\\prime}}$ . The results are provided in Figure 8. ", "page_idx": 26}, {"type": "text", "text": "Schwefel. The Schwefel function is $d^{\\prime}$ -dimensional, and has the form ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(z)=418.9829d^{\\prime}-\\sum_{i=1}^{d^{\\prime}}z_{i}\\sin\\left({\\sqrt{|z_{i}|}}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For the numerical evaluation, we set $d^{\\prime}\\ =\\ 4$ and we optimized the function on the domain $[-500,500]^{d^{\\prime}}$ . The results are provided in Figure 9. This benchmark has also been used to replicate our results using the ARD covariance function studied in Appendix E. The results are provided in Figure 10. ", "page_idx": 26}, {"type": "text", "text": "Styblinski-Tang. The Syblinski-Tang function is $d^{\\prime}$ -dimensional, and has the form ", "page_idx": 26}, {"type": "equation", "text": "$$\nf(z)=\\frac{1}{2}\\sum_{i=1}^{d^{\\prime}}z_{i}^{4}-16z_{i}^{2}+5z_{i}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "image", "img_path": "kN7GTUss0l/tmp/00e4652086a1b6d5e8547795db3793b4c717bb16a10e46356b9035f33674737b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 9: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Schwefel synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Schwefel synthetic function. ", "page_idx": 27}, {"type": "image", "img_path": "kN7GTUss0l/tmp/5e09982aa1d42e258b2881f5c5f72e377ff39cf1b9e2183cb2001432bbf4fba6.jpg", "img_caption": ["Figure 10: (Left) Average response time and average regrets of the DBO solutions using an ARD SE kernel during the optimization of the Schwefel synthetic function. (Right) Dataset sizes of the DBO solutions using an ARD SE kernel during the optimization of the Schwefel synthetic function. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "For the numerical evaluation, we set $d^{\\prime}=4$ and we optimized the function on the domain $[-5,5]^{d^{\\prime}}$ .   \nThe results are provided in Figure 11. ", "page_idx": 27}, {"type": "text", "text": "Eggholder. The Eggholder function is 2-dimensional, and has the form ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(z)=-(z_{2}+47)\\sin\\left(\\sqrt{|z_{2}+\\frac{z_{1}}{2}+47|}\\right)-z_{1}\\sin\\left(\\sqrt{|z_{1}-z_{2}-47|}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the numerical evaluation, we optimized the function on the domain $[-512,512]^{2}$ . The results are provided in Figure 12. ", "page_idx": 27}, {"type": "text", "text": "Ackley. The Ackley function is $d^{\\prime}$ -dimensional, and has the form ", "page_idx": 27}, {"type": "equation", "text": "$$\nf(z)=-a\\exp\\left(-b\\sqrt{\\frac{1}{d^{\\prime}}\\sum_{i=1}^{d^{\\prime}}z_{i}^{2}}\\right)-\\exp\\left(\\frac{1}{d^{\\prime}}\\sum_{i=1}^{d^{\\prime}}\\cos(c z_{i})\\right)+a+\\exp(1).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For the numerical evaluation, we set $a=20$ , $b=0.2$ , $c=2\\pi$ , $d^{\\prime}=4$ and we optimized the function on the domain $[-32,32]^{d^{\\prime}}$ . The results are provided in Figure 13. This benchmark has also been used to replicate our results using the ARD covariance function studied in Appendix E. The results are provided in Figure 14. ", "page_idx": 27}, {"type": "image", "img_path": "kN7GTUss0l/tmp/c1f3a0c801efe99f4043c5304961e060a764102ae4da77a709e462e1e1a49f1a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 11: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Styblinski-Tang synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Styblinski-Tang synthetic function. ", "page_idx": 28}, {"type": "image", "img_path": "kN7GTUss0l/tmp/9aa7077a4c77e56530c0775f7d8e1af39cac053fe8dec0a2f0e4478f4c7237c9.jpg", "img_caption": ["Figure 12: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Eggholder synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Eggholder synthetic function. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Rosenbrock. The Rosenbrock function is $d^{\\prime}$ -dimensional, and has the form ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(z)=\\sum_{i=1}^{d^{\\prime}-1}100(z_{i+1}-z_{i}^{2})^{2}+(z_{i}-1)^{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For the numerical evaluation, we set $d^{\\prime}=3$ and we optimized the function on the domain $[-1,1.5]^{d^{\\prime}}$ .   \nThe results are provided in Figure 15. ", "page_idx": 28}, {"type": "text", "text": "Shekel. The Shekel function is 4-dimensional, and has the form ", "page_idx": 28}, {"type": "equation", "text": "$$\nf(z)=-\\sum_{i=1}^{m}\\left(\\sum_{j=1}^{4}(z_{j}-C_{j i})^{2}+\\beta_{i}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For the numerical evaluation, we set $m=10$ , $\\begin{array}{r}{\\beta=\\frac{1}{10}\\,(1,2,2,4,4,6,3,7,5,5),}\\end{array}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\nC=\\left(\\!\\!\\begin{array}{c c c c c c c c c c c}{{4}}&{{1}}&{{8}}&{{6}}&{{3}}&{{2}}&{{5}}&{{8}}&{{6}}&{{7}}\\\\ {{4}}&{{1}}&{{8}}&{{6}}&{{7}}&{{9}}&{{3}}&{{1}}&{{2}}&{{3.6}}\\\\ {{4}}&{{1}}&{{8}}&{{6}}&{{3}}&{{2}}&{{5}}&{{8}}&{{6}}&{{7}}\\\\ {{4}}&{{1}}&{{8}}&{{6}}&{{7}}&{{9}}&{{3}}&{{1}}&{{2}}&{{3.6}}\\end{array}\\!\\!\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and we optimized the function on the domain $[0,10]^{4}$ . The results are provided in Figure 16. ", "page_idx": 28}, {"type": "image", "img_path": "kN7GTUss0l/tmp/1d7de5e13e5a0599ff0b6429954c13d217de4b25cdbb7c6d46aa25b3fc0cf4fb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 13: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Ackley synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Ackley synthetic function. ", "page_idx": 29}, {"type": "image", "img_path": "kN7GTUss0l/tmp/21549748f754a29fa94379121587c8418dc2c1634c79246e8cf6cb12d3a45ec3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 14: (Left) Average response time and average regrets of the DBO solutions using an ARD SE kernel during the optimization of the Ackley synthetic function. (Right) Dataset sizes of the DBO solutions using an ARD SE kernel during the optimization of the Ackley synthetic function. ", "page_idx": 29}, {"type": "text", "text": "Hartmann-3. The Hartmann-3 function is 3-dimensional, and has the form ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(z)=-\\sum_{i=1}^{4}\\alpha_{i}\\exp\\left(-\\sum_{j=1}^{3}A_{i j}(z_{j}-P_{i j})^{2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "For the numerical evaluation, we set $\\pmb{\\alpha}=(1.0,1.2,3.0,3.2)$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A=\\left(\\!\\!\\begin{array}{c c c}{3}&{10}&{30}\\\\ {0.1}&{10}&{35}\\\\ {3}&{10}&{30}\\\\ {0.1}&{10}&{35}\\end{array}\\!\\!\\right),P=10^{-4}\\left(\\!\\!\\begin{array}{c c c}{3689}&{1170}&{2673}\\\\ {4699}&{4387}&{7470}\\\\ {1091}&{8732}&{5547}\\\\ {381}&{5743}&{8828}\\end{array}\\!\\!\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and we optimized the function on the domain $[0,1]^{3}$ . The results are provided in Figure 17. ", "page_idx": 29}, {"type": "text", "text": "Hartmann-6. The Hartmann-6 function is 6-dimensional, and has the form ", "page_idx": 29}, {"type": "equation", "text": "$$\nf(z)=-\\sum_{i=1}^{4}\\alpha_{i}\\exp\\left(-\\sum_{j=1}^{6}A_{i j}(z_{j}-P_{i j})^{2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "image", "img_path": "kN7GTUss0l/tmp/ba1bcf97f7954d7d9412ca9c4235508a008bb0017a2400964fb9934945ab6c1f.jpg", "img_caption": ["Figure 15: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Rosenbrock synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Rosenbrock synthetic function. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "kN7GTUss0l/tmp/36c34a65ccbd5408f4e9d35a94cff385f332550a1a00c9090852e5b9ee072f6b.jpg", "img_caption": ["Figure 16: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Shekel synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Shekel synthetic function. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "For the numerical evaluation, we set $\\pmb{\\alpha}=(1.0,1.2,3.0,3.2)$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbf{4}={\\left(\\begin{array}{l l l l l l}{10}&{3}&{17}&{3.50}&{1.7}&{8}\\\\ {0.05}&{10}&{17}&{0.1}&{8}&{14}\\\\ {3}&{3.5}&{1.7}&{10}&{17}&{8}\\\\ {17}&{8}&{0.05}&{10}&{0.1}&{14}\\end{array}\\right)}\\,,P=10^{-4}\\,{\\left(\\begin{array}{l l l l l l}{1312}&{1696}&{5569}&{124}&{8283}&{5886}&{1.05}\\\\ {2329}&{4135}&{8307}&{3736}&{1004}&{9991}&{1.05}\\\\ {2348}&{1451}&{3522}&{2883}&{3047}&{6650}&{2.15}\\\\ {4047}&{8828}&{8732}&{5743}&{1091}&{381}&{1.05}\\end{array}\\right)}\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and we optimized the function on the domain $[0,1]^{6}$ . The results are provided in Figure 18. ", "page_idx": 30}, {"type": "text", "text": "Powell. The Powell function is $d^{\\prime}$ -dimensional, and has the form ", "page_idx": 30}, {"type": "equation", "text": "$$\nf(z)=\\sum_{i=1}^{d^{\\prime}/4}(z_{4i-3}+10z_{4i-2})^{2}+5(z_{4i-1}-z_{4i})^{2}+(z_{4i-2}-2z_{4i-1})^{4}+10(z_{4i-3}-z_{4i})^{4}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For the numerical evaluation, we set $d^{\\prime}=4$ and we optimized the function on the domain $[-4,5]^{d^{\\prime}}$ .   \nThe results are provided in Figure 19. ", "page_idx": 30}, {"type": "text", "text": "Temperature. This benchmark comes from the temperature dataset collected from 46 sensors deployed at Intel Research Berkeley. It is a famous benchmark, used in other works such as [21, 22]. The goal of the DBO task is to activate the sensor with the highest temperature, which will vary with time. To make the benchmark more interesting, we interpolate the data in space-time. With this interpolation, the algorithms can activate any point in space-time, making it a 3-dimensional benchmark (2 spatial dimensions for a location in Intel Research Berkeley, 1 temporal dimension). ", "page_idx": 30}, {"type": "image", "img_path": "kN7GTUss0l/tmp/15387b670edf842cd9adee3b71b9e53d0463fab7ea7320f144cb53a2a26bc0e0.jpg", "img_caption": ["Figure 17: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Hartmann-3 synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Hartmann-3 synthetic function. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "kN7GTUss0l/tmp/771e9fb4802b4841e62a220677127a1c5efa5825c0879b20b3122a07c742b528.jpg", "img_caption": ["Figure 18: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Hartmann-6 synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Hartmann-6 synthetic function. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "For the numerical evaluation, we used the first day of data. The results are provided in Figure 20. ", "page_idx": 31}, {"type": "text", "text": "WLAN. This benchmark aims at maximizing the throughput of a Wireless Local Area Network (WLAN). 18 moving end-users are associated with one of 4 fixed nodes and continuously stream a large amount of data. As they move in space, they change the radio environment of the network, which should adapt accordingly to improve its performance. To do so, each node has a power level that can be tuned for the purpose of reaching the best trade-off between serving all its users and not causing interference for the neighboring nodes. ", "page_idx": 31}, {"type": "text", "text": "The performance of the network is computed as the sum of the Shannon capacities for each pair of node and associated end-users. The Shannon capacity [34] sets a theoretical upper bound on the throughput of a wireless communication. We denote it $C(i,j)$ , we express it in bits per second (bps). It depends on $S_{i j}$ the Signal-to-Interference plus Noise Ratio (SINR) of the communication between node $i$ and end-user $j$ , as well as on $W$ , the bandwidth of the radio channel (in $\\mathrm{Hz}$ ): ", "page_idx": 31}, {"type": "equation", "text": "$$\nC_{i j}(\\pmb{x},t)=W\\log_{2}(1+S_{i j}(\\pmb{x},t)).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Then, the objective function is ", "page_idx": 31}, {"type": "equation", "text": "$$\nf(\\mathbf{x},t)=\\sum_{i=1}^{4}\\sum_{j\\in\\mathcal{N}_{i}}C_{i j}(\\mathbf{x},t),\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with ${\\mathcal{N}}_{i}$ the end-users associated with node $i$ . ", "page_idx": 31}, {"type": "image", "img_path": "kN7GTUss0l/tmp/8add4aa7a081bc64f472c0b0c6d89b0cc4c40d92a6f846d2188bff6c877a3878.jpg", "img_caption": ["Figure 19: (Left) Average response time and average regrets of the DBO solutions during the optimization of the Powell synthetic function. (Right) Dataset sizes of the DBO solutions during the optimization of the Powell synthetic function. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "kN7GTUss0l/tmp/a8128baf282c05271bc16cc857462b7bda9a3e5e69b8bbafefaa59fc4d4e3564.jpg", "img_caption": ["Figure 20: (Left) Average response time and average regrets of the DBO solutions during the Temperature real-world experiment. (Right) Dataset sizes of the DBO solutions during the Temperature real-world experiment. "], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "For the numerical evaluation, we optimized the power levels $\\textbf{\\em x}$ in the domain $[10^{0.1},10^{2.5}]^{4}$ . For this experiment, the DBO solutions were evaluated with a Mat\u00e9rn- $\\cdot5/2$ for the spatial covariance function and a Mat\u00e9rn-1/2 for the temporal covariance function. The results are provided in Figure 21. ", "page_idx": 32}, {"type": "text", "text": "H.3 Discussion on Empirical Performance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we discuss the performance achieved by all the DBO solutions on the benchmarks introduced in the previous section. ", "page_idx": 32}, {"type": "text", "text": "GP-UCB. This baseline, which does not take into account temporal correlations, obtains surprisingly good performance in this experimental setting (continuous time, hyperparameters estimated on the fly). Its simple behavior (i.e., keep all observations in the dataset until the end of the experiment) hampers its response time, but this drawback is balanced by the fact that it has only three parameters to estimate with MLE $(i.e.,\\,\\lambda,l_{S},\\sigma^{2})$ . Overall, it is dominated by R-GP-UCB, TV-GP-UCB and W-DBO, but behaves surprisingly well against ABO and ET-GP-UCB (see Figure 5). ", "page_idx": 32}, {"type": "text", "text": "ABO. ABO performs poorly in this experimental setting. We explain this poor performance by the fact that the hyperparameters (including the spatial and temporal lengthscales) have to be estimated on the fly. Since ABO can decide to postpone its next query to the near future (a fraction of the temporal lengthscale $l_{T}$ away), overestimating $l_{T}$ may cause ABO to wait for a long time before querying $f$ again. This interpretation is supported by the fact that the functions ABO performs the poorest on are the ones with the largest temporal lengthscales $l_{T}$ , e.g., Rosenbrock (see Figure 15) and Powell (see Figure 19). Conversely, ABO obtains competitive performance on functions with smaller temporal lengthscales, e.g. Eggholder (see Figure 12) or Shekel (see Figure 16). These results highlight the lack of robustness of ABO. ", "page_idx": 32}, {"type": "image", "img_path": "kN7GTUss0l/tmp/e6034116eb9815668d2b772ee5d4d2a43274ff3010406388b56f28438825bc7b.jpg", "img_caption": ["Figure 21: (Left) Average response time and average regrets of the DBO solutions during the WLAN real-world experiment. (Right) Dataset sizes of the DBO solutions during the WLAN real-world experiment. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "ET-GP-UCB. Like GP-UCB, ET-GP-UCB does not take into account temporal correlations, and deals with stale observations by resetting its dataset each time a condition is met. Our experimental setting exposes the lack of robustness of ET-GP-UCB, since its performance is quite poor on most benchmarks. This is mainly due to the fact that, because the hyperparameters (including the observational noise level $\\sigma^{2}$ ) are inferred on the fly, the MLE explains the variance in the observations with an increasingly large observational noise level $\\sigma^{2}$ as time goes by. However, by construction of ET-GP-UCB, the greater $\\sigma^{2}$ , the less the dataset will be reset. As a consequence, on some benchmarks, the event is never triggered (or not triggered enough) and the performance of ET-GP-UCB is close to (sometimes worse than) the performance of GP-UCB. For examples, refer to Hartmann6d (see Figure 18), Shekel (see Figure 16) or Ackley (see Figure 13). Some other times, the variance in the observations cannot be explained by an increasingly large observational noise. In these cases, the triggering occurs properly and ET-GP-UCB obtains competitive performance, e.g., with Powell (see Figure 19). ", "page_idx": 33}, {"type": "text", "text": "R-GP-UCB. R-GP-UCB deals with stale data by resetting its dataset (like ET-GP-UCB). It indirectly accounts for temporal correlations by estimating an hyperparameter $\\epsilon$ , and the reset is triggered each time the dataset size exceeds $N(\\epsilon)$ (given in [21]). More often than not, its performance is better than GP-UCB, because stale data is frequently removed from the dataset. As a consequence, R-GP-UCB has the lowest average response time of all the DBO solutions. Overall, because of its low response time, R-GP-UCB obtains very good performance on some benchmarks, e.g., Eggholder (see Figure 12), Hartmann3d (see Figure 17) or Temperature (see Figure 20). ", "page_idx": 33}, {"type": "text", "text": "TV-GP-UCB. TV-GP-UCB directly accounts for temporal correlations by computing a specific covariance function controlled by an hyperparameter $\\epsilon$ . Although the temporal covariance function is based on a distance between indices instead of a distance between points in time, the DBO solution turns out to be quite robust in our experimental setting. However, its response time is hampered by the irrelevant observations that are kept in the dataset. Because of them, TV-GP-UCB has one of the largest response time on many benchmarks, e.g., Rastrigin (see Figure 8), Schwefel (see Figure 9) or Shekel (see Figure 16). Nevertheless, its average performance is significantly better than the other state-of-the-art DBO solutions. ", "page_idx": 33}, {"type": "text", "text": "W-DBO. Because of its ability to measure the relevancy of its observations and to remove irrelevant observations, W-DBO achieves simultaneously good predictive performance and a low response time. Depending on the benchmark, its dataset size follows different patterns. When the objective function evolves smoothly, e.g., Powell (see Figure 19) or Rosenbrock (see Figure 15), W-DBO behaves roughly like GP-UCB and TV-GP-UCB and keeps most of its observations in its dataset (although it manages to identify and delete some irrelevant observations). When the objective function\u2019s variations are more pronounced, the dataset size of W-DBO experiences sudden drops, as can be seen with Ackley (see Figure 13), Shekel (see Figure 16) or Temperature (see Figure 20). This suggests that W-DBO is also able to \"reset\" its dataset, although in a more refined way as it is able to keep the few observations still relevant for future predictions. Thanks to its ability to adapt in very different contexts, W-DBO outperforms state-of-the-art DBO solutions by a comfortable margin. This performance gap can be seen in its average performance across all benchmarks (see Figure 5), but also on most of the benchmarks themselves, e.g., Schwefel (see Figure 9), Ackley (see Figure 13), Shekel (see Figure 16), Hartmann-6 (see Figure 18) or Powell (see Figure 19). ", "page_idx": 33}, {"type": "image", "img_path": "kN7GTUss0l/tmp/944bf7a56e58284df7cdb290a6028aad41649517f725a5dee6d5a98f322c60af.jpg", "img_caption": ["Figure 22: Snapshot from one of the videos showing the optimization conducted by W-DBO. The normalized temporal dimension is shown on the $\\mathbf{X}$ -axis and the normalized spatial dimension is shown on the y-axis. The observations that are in the dataset are depicted as red dots, while the deleted observations are depicted as black crosses. The maximal arguments $\\{\\arg\\operatorname*{max}_{x\\in S}f(x,t),t\\in T\\}$ are depicted with a cyan curve. The predictions of W-DBO are shown with a contour plot. Finally, the present time is depicted as a black vertical line labelled $t_{0}$ . "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "H.4 Animated Visualizations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "In this section, we describe and discuss the two animated visualizations provided as supplementary material for the paper. These videos show W-DBO optimizing two 2-dimensional synthetic functions. They depict W-DBO\u2019s predictions, collected observations and deleted observations into the spatiotemporal domains of the functions. ", "page_idx": 34}, {"type": "text", "text": "One of the videos depict the optimization of the Six-Hump Camel function4 on the domain $[-2,2]^{2}$ . The SHC function is ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{SHC}(x,t)=\\left(4-2.1x^{2}+{\\frac{x^{4}}{3}}\\right)x^{2}+x t+\\left(-4+4t^{2}\\right)t^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "To study how W-DBO reacts to sudden changes in the objective function, the other video depict the optimization of the piecewise function ", "page_idx": 34}, {"type": "equation", "text": "$$\nf(x,t)={\\left\\{\\operatorname{SHC}(x,t)\\quad{\\mathrm{if~}}t<-{\\frac{1}{2}},\\right.}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "A snapshot from the latter can be found in Figure 22. It illustrates that the benefits brought by W-DBO are substantial, since the algorithm is able to track $\\operatorname*{max}_{x\\in S}f(x,t)$ over the time $t$ while simultaneously deleting a significant portion of collected observations. Indeed, many observations are deemed irrelevant, either because (i) they have become stale (there are only a few observations collected at the start of the experiment that have been kept in the dataset) or because (ii) they are redundant with observations that are already in the dataset (many observations are located near the maximal argument, and many of them are deleted soon after being collected). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "I Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "For the sake of completeness, we explicitly discuss the limitations of W-DBO in this appendix. Four limitations were identified: ", "page_idx": 35}, {"type": "text", "text": "\u2022 As for any BO algorithm, W-DBO exploits a GP as a surrogate model (see Assumption 3.1). If the objective function $f$ cannot be properly approximated by a GP, we expect the performance of W-DBO to decline.   \n\u2022 As for any BO algorithm, W-DBO conducts GP inference, which causes it to manipulate inverses of Gram matrices that scale with the dataset size. Although the main motivation of introducing W-DBO is to reduce the dataset size, the cubic complexity of matrix inversion algorithms can still constitute a limitation if too many observations are kept in the dataset.   \n\u2022 We also introduce a structure for spatio-temporal correlations with Assumption 3.2. Although less restrictive than the one enforced by [21, 22], equivalent to the one in [20] and partially relaxed in Appendix E, this is still a limitation since we expect the performance of W-DBO to worsen if the objective function does not meet this assumption.   \n\u2022 Finally, W-DBO is not exempt from the effects of the sampling frequency. In fact, as for any DBO algorithm, the performance of W-DBO will drop if the function varies too much between observations. As an example, if $f$ evolves so rapidly that two successive observations become basically independent, W-DBO will not be able to infer anything meaningful about the objective function. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The abstract states that the paper introduces a criterion that measures the relevancy of the observations and a DBO algorithm that exploits the criterion to improve upon state-of-the-art. These contributions can be found in Sections 3, 4 and 5. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The limitations are explicitly discussed in Appendix I and the approximation error of our proposed upper bounds is discussed at length in Appendix C. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The assumptions are explicitly given in the main text (see Section 3), and the proofs for each result are provided in Appendices A, B and D. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 36}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The experimental details are discussed briefly in the main paper (see Section 5) and discussed at length in Appendix H.1. Furthermore, the benchmarks are detailed in Appendix H.2. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: A GitHub repository with a minimal working example and a detailed documentation is provided along with the camera-ready version.f ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 37}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: As previously mentioned, all experimental details are discussed in Appendix H. A sensitivity study and a discussion about the hyperparameter of W-DBO are also provided in Section 5.1 and Appendix G, respectively. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: Standard errors are depicted graphically in Section 5 and Appendix H.2. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: The computing resources are described in Appendix H.1. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The research conducted in this paper does not violate the NeurIPS Code of Ethics. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: This paper proposes a DBO algorithm that can be applied to many different applied research areas (e.g., robotics, computer networks, biology). These impacts are briefly mentioned in Section 6. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: In our opinion, the paper poses no such risks. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper uses BOTorch and PyBind11, which are explicitly mentioned in Appendix H.1 and whose licenses allow free exploitation for scientific research purposes. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: The paper does not introduce new assets yet. Upon acceptance, a GitHub repository with a detailed documentation will be provided. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 40}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The research does not involve crowdsourcing nor human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The research does not involve crowdsourcing nor human subjects. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]