[{"figure_path": "xoc4QOvbDs/tables/tables_7_1.jpg", "caption": "Table 1: Dataset summary.", "description": "This table summarizes the six multi-view datasets used in the paper's experiments.  For each dataset, it lists the number of views, the number of samples, and the number of clusters.", "section": "4.1 Datasets, Metrics and Experimental settings"}, {"figure_path": "xoc4QOvbDs/tables/tables_7_2.jpg", "caption": "Table 2: Analyzing view contribution under InfoNCE+Kmeans and ProImp frameworks on CUB and Caltech101-7 datasets.", "description": "This table presents the results of analyzing view contributions using two different multi-view clustering methods (InfoNCE+Kmeans and ProIMP) on two datasets (CUB and Caltech101-7).  It shows the individual contributions of each view (\u03a61 and \u03a62), as calculated by the Shapley value method,  along with the overall clustering performance metrics: Accuracy (ACC), Normalized Mutual Information (NMI), and Adjusted Rand Index (ARI).  The purpose is to demonstrate how the Shapley value approach helps evaluate view contributions and to assess the impact of view cooperation enhancement on clustering results.", "section": "4.2 Evaluate on Alignment-based Methods(RQ1 & RQ2)"}, {"figure_path": "xoc4QOvbDs/tables/tables_8_1.jpg", "caption": "Table 4: Sensitive analysis on UCI-digit and STL10 datasets. The optimal results are marked in bold.", "description": "This table presents the results of a sensitivity analysis performed on the UCI-digit and STL10 datasets. The analysis focuses on the impact of the hyperparameter  \u03c4  on the performance of the proposed SCE-MVC model.  The table shows the ACC, NMI, and ARI metrics for different values of  \u03c4 , allowing for an assessment of the model's robustness to changes in this parameter. The optimal results for each dataset and metric are highlighted in bold.", "section": "4.4 Sensitive Analysis(RQ5)"}, {"figure_path": "xoc4QOvbDs/tables/tables_16_1.jpg", "caption": "Table 5: View Contribution Comparison w/o SCE", "description": "This table presents a comparison of view contributions with and without the Shapley-based Cooperation Enhancing (SCE) module across six different datasets.  It highlights the differences in the contribution of each view before and after applying SCE.  The results showcase how SCE increases the participation of underrepresented views and leads to a more balanced distribution of view contributions in the fusion process.", "section": "B Further Experiments"}, {"figure_path": "xoc4QOvbDs/tables/tables_16_2.jpg", "caption": "Table 6: Three views training on the UCI-digit dataset and fuse the results of different views. The optimal results are marked in bold, and the suboptimal values are underlined.", "description": "This table presents the results of an experiment using the UCI-digit dataset with three views. The experiment compares the clustering accuracy (ACC) achieved using different combinations of views, both with and without the Shapley-based Cooperation Enhancing (SCE) module. The optimal results (highest ACC values) are highlighted in bold, and suboptimal results are underlined. The table demonstrates the impact of the SCE module on improving the clustering accuracy by better integrating information from multiple views.", "section": "4.3 Evaluate then Cooperate on Joint Methods(RQ3 & RQ4)"}, {"figure_path": "xoc4QOvbDs/tables/tables_16_3.jpg", "caption": "Table 7: Three views are used for training in the first row, and then selected two of them each time for training, and compared the fusion results.", "description": "This table shows the results of experiments where different combinations of two out of the three views from UCI-digit dataset are used for training. The table demonstrates the impact of using different views in the training process on the final clustering accuracy (ACC). The first row shows that training with all three views yields the highest ACC of 0.873. Removing one of the three views results in slightly lower ACC.", "section": "B.2 Detailed Discussions of View Roles"}, {"figure_path": "xoc4QOvbDs/tables/tables_17_1.jpg", "caption": "Table 8: Three additional datasets summary.", "description": "This table presents a summary of three additional datasets used in the experiments.  For each dataset, it shows the number of views, the number of samples, and the number of clusters.", "section": "4.1 Datasets, Metrics and Experimental settings"}, {"figure_path": "xoc4QOvbDs/tables/tables_17_2.jpg", "caption": "Table 3: Multi-view clustering performance on six benchmark datasets. The optimal results are marked in bold, and the suboptimal values are underlined. O/M denotes out-of-memory error encountered during the training process.", "description": "This table presents the performance of various multi-view clustering methods on six benchmark datasets (CUB, Caltech101-7, UCI-digit, HandWritten, STL10, Reuters). The performance is measured using three metrics: ACC (accuracy), NMI (normalized mutual information), and ARI (adjusted Rand index).  The best results for each dataset and metric are shown in bold, and the second-best results are underlined.  O/M indicates that the method ran out of memory during training.", "section": "4.2 Evaluate on Alignment-based Methods(RQ1 & RQ2)"}]