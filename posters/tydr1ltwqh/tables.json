[{"figure_path": "tYdR1lTWqh/tables/tables_1_1.jpg", "caption": "Table 1: Example LLM responses to queries for different data knowledge along training process. Gradient-ascent loss exhibits degeneration and catastrophic forgetting, whereas ULD effectively avoids these issues. Responses are selected after epoch 1, 5, and 10. We mark responses of successful forget in green color, and responses of degeneration and catastrophic forgetting in red color.", "description": "This table presents example outputs from different LLMs at various training epochs in response to queries about data from forget documents (information the model should unlearn), retain documents (information the model should keep), and new information not contained in either.  It compares the performance of a baseline gradient-ascent model with KL regularization to the proposed Unlearning from Logit Difference (ULD) method, highlighting the baseline model's issues of degeneration (nonsensical answers) and catastrophic forgetting (loss of previously learned knowledge), which ULD avoids.", "section": "1 Introduction"}, {"figure_path": "tYdR1lTWqh/tables/tables_4_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of the proposed Unlearning from Logit Difference (ULD) method and several baseline methods on the TOFU dataset for three different forget data sizes (1%, 5%, and 10%).  The metrics reported include Forget Quality (F.Q.), measuring how well the model forgets the unwanted information; Model Utility (M.U.), measuring how well the model retains its overall knowledge; and ROUGE-L (R-L), a metric assessing the overlap between generated and reference texts.  The table compares ULD's performance against various baseline methods, demonstrating its effectiveness in balancing the trade-off between forgetting unwanted information and preserving useful knowledge. The asterisks (*) indicate values that differ from the original paper, attributed to random seed sensitivity.", "section": "3.2 Experiments on TOFU"}, {"figure_path": "tYdR1lTWqh/tables/tables_5_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of different LLM unlearning methods on the TOFU benchmark dataset.  It compares the performance of several baseline methods (gradient ascent, direct preference optimization, negative preference optimization, and their combinations with different retain loss functions) against the proposed ULD method.  The metrics used are Forget Quality (F.Q.), representing how well the model forgets the unwanted information; Model Utility (M.U.), indicating how well the model retains its overall capabilities; and ROUGE-L (R-L), a metric for evaluating the quality of generated text.  The results are shown for three different settings of the TOFU dataset (1%, 5%, and 10% of data used for forgetting), allowing for a comparison of performance under varying levels of knowledge removal.", "section": "3.2 Experiments on TOFU"}, {"figure_path": "tYdR1lTWqh/tables/tables_6_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of different LLM unlearning methods on the TOFU benchmark dataset.  It compares the performance of several baseline methods (e.g., GA+KL, DPO+KL, NPO+KL) against a proposed method (ULD) across three different forgetting ratios (1%, 5%, and 10%).  The metrics used to evaluate performance are Forget Quality (F.Q.), representing the effectiveness of removing unwanted knowledge; Model Utility (M.U.), reflecting the preservation of useful knowledge; and ROUGE-L (R-L), measuring the overlap between generated text and reference text. The table demonstrates the trade-off between forgetting unwanted information and retaining useful information, highlighting the superiority of the ULD method.", "section": "3.2 Experiments on TOFU"}, {"figure_path": "tYdR1lTWqh/tables/tables_8_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of different LLM unlearning methods on the TOFU benchmark dataset.  It compares the performance of various methods across three different settings (TOFU-1%, TOFU-5%, TOFU-10%), each varying in the percentage of data used for forgetting. The metrics used to evaluate performance include Forget Quality (F.Q.), Model Utility (M.U.), and ROUGE-L (R-L).  Higher F.Q. and M.U. scores indicate better unlearning performance, while higher ROUGE-L suggests higher similarity between the generated and reference text. The results highlight that ULD achieves superior performance in all settings.", "section": "3.2 Experiments on TOFU"}, {"figure_path": "tYdR1lTWqh/tables/tables_16_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of the proposed ULD method and several baseline methods on the TOFU dataset.  The TOFU dataset is designed to evaluate LLM unlearning by measuring the ability of a model to forget information about fictional writers while retaining knowledge about other topics. The table shows the performance of each method across three different settings (TOFU-1%, TOFU-5%, TOFU-10%), each representing a different percentage of fictional writers to be forgotten.  Metrics include Forget Quality (F.Q.), which measures how well the model forgets the target information; Model Utility (M.U.), which indicates how well the model retains other knowledge; and ROUGE-L, which is a metric assessing the overlap between the model's generated text and the reference text.  The best performance for each metric in each setting is highlighted in bold. The table also includes the performance of the original LLM and a retain LLM (trained only on the retain data) as references.", "section": "3.2 Experiments on TOFU"}, {"figure_path": "tYdR1lTWqh/tables/tables_19_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of different LLM unlearning methods on the TOFU benchmark dataset.  It compares the Forget Quality (F.Q.), Model Utility (M.U.), and ROUGE-L scores across various methods and different dataset sizes (TOFU-1%, TOFU-5%, TOFU-10%).  Higher F.Q. indicates better forgetting of the target knowledge, higher M.U. represents better retention of other knowledge, and higher ROUGE-L signifies better response quality. The table includes results for the original LLM and a retain-only LLM as baselines for comparison.  Note that some values are lower than in the original paper due to variations caused by the use of random seeds in the experiment.", "section": "3.2 Experiments on TOFU"}, {"figure_path": "tYdR1lTWqh/tables/tables_20_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of different LLM unlearning methods on the TOFU benchmark dataset.  It compares the performance of various methods across three different forget data sizes (1%, 5%, and 10% of the dataset), evaluating both the Forget Quality (how well the model forgets the target information) and Retain Performance (how well the model retains other knowledge).  Metrics include Forget Quality (F.Q.), ROUGE-L (R-L) for both forget and retain, and Model Utility (M.U.).  The best performing method for each metric in each setting is highlighted in bold.  The table also shows the original LLM's performance and a retain-only LLM as baselines for comparison.", "section": "3.2 Experiments on TOFU"}, {"figure_path": "tYdR1lTWqh/tables/tables_21_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of the proposed ULD method and several baseline methods on the TOFU dataset.  It shows the forget quality (F.Q.), measured by how well the model forgets the unwanted knowledge, and the retain performance (M.U.), which assesses how well the model retains useful knowledge.  ROUGE-L (R-L) scores are provided for both forget and retain tasks. The table compares performance across three different forget data sizes (TOFU-1%, TOFU-5%, TOFU-10%). The best results for each metric are highlighted in bold.  The original LLM and a retain-only LLM are included for comparison.", "section": "3.2 Experiments on TOFU"}, {"figure_path": "tYdR1lTWqh/tables/tables_22_1.jpg", "caption": "Table 9: TOFU-10% performance for ULD with different ratio of regular retain data.", "description": "This table presents the performance of the ULD model on the TOFU-10% dataset when varying the ratio of regular retain data used during training. It compares the forget quality (how well the model forgets the knowledge in the forget data) and model utility (how well the model retains its knowledge on other tasks) of ULD with different percentages of the regular retain data (0%, 25%, 50%, 75%).  The results are compared against a target LLM (fully trained) and a retain LLM (trained only on retain data) to provide a baseline.", "section": "D.2 Effect of retain data about knowledge to retain on ULD"}, {"figure_path": "tYdR1lTWqh/tables/tables_23_1.jpg", "caption": "Table 2: Performance on TOFU dataset. F.Q., M.U., and R-L represent forget quality, model utility and ROUGE-L respectively. The best results are marked in bold. We include the original LLM and retain LLM for reference. *: We notice these values are lower than those in the original paper, due to sensitivity to random seeds.", "description": "This table presents the quantitative results of different LLM unlearning methods on the TOFU benchmark dataset.  It compares the performance across three different settings (TOFU-1%, TOFU-5%, TOFU-10%) representing varying amounts of data to be forgotten. The key metrics are Forget Quality (F.Q.), indicating how well the model forgets the unwanted knowledge; Model Utility (M.U.), showing the model's performance on data it should retain; and ROUGE-L (R-L), measuring the overlap between generated and reference text. The table helps in assessing the trade-off between forgetting unwanted information and preserving useful knowledge across different unlearning techniques.  The asterisk (*) notes that some results vary from previous work likely due to random seed differences.", "section": "3.2 Experiments on TOFU"}]