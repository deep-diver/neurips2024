[{"figure_path": "MocRdX0n7B/figures/figures_1_1.jpg", "caption": "Figure 1: (a): The base detector failed to recognize objects. (b, c) However, when IIM is employed with a simple edge feature, the object is identified. (d, e) Furthermore, the full IIM utilizes a task-driven learnable kernel to extract illumination-invariant features that are richer and more suitable for the detection task than simple edge features.", "description": "This figure shows the effectiveness of the proposed Illumination-Invariant Module (IIM) in object detection under low-light conditions.  (a) shows a baseline detector failing to recognize objects in a low-light image. (b) demonstrates that using the IIM with a simple edge feature allows the detector to identify an object. Finally, (d) and (e) illustrate that using the full IIM with a learned kernel produces richer, more robust illumination-invariant features leading to improved object recognition, surpassing the performance of the simpler edge-based approach.", "section": "1 Introduction"}, {"figure_path": "MocRdX0n7B/figures/figures_2_1.jpg", "caption": "Figure 2: The overall pipeline of YOLA.YOLA extracts illumination-invariant features via IIM and integrates them with original images by leveraging a fuse convolution block for the subsequent detector.", "description": "This figure illustrates the architecture of the YOLA object detection framework.  It shows how illumination-invariant features are extracted using the Illumination Invariant Module (IIM). These features are then fused with the original image using a fusion convolution block before being fed into the detector head for final object detection. The IIM processes the image to generate illumination-invariant feature maps using learnable kernels. The bottom half of the figure shows a detailed view of the IIM process, highlighting the use of convolutional kernels and element-wise addition.", "section": "3 Method"}, {"figure_path": "MocRdX0n7B/figures/figures_5_1.jpg", "caption": "Figure 3: Qualitative comparisons of TOOD detector on both ExDark and UG2+DARK FACE dataset, where the top 2 rows visualize the detection results from ExDark, and the bottom 2 rows show the results from UG2+DARK FACE. The images are being replaced with enhanced images generated by LLIE or low-light object methods. Red dash boxes highlight the inconspicuous cases. Zoom in red dash boxes for the best view.", "description": "This figure shows a qualitative comparison of the TOOD object detector's performance on the ExDark and UG2+DARK FACE datasets.  The top two rows display results on ExDark, while the bottom two rows show results on UG2+DARK FACE.  Each image shows the ground truth (GT) bounding boxes along with results from several different methods (baseline, DENet, MAET, KIND, SMG, NERCO) and the proposed YOLA method.  Red dashed boxes highlight cases where objects were missed or poorly detected by the various methods.", "section": "4 Experiments"}, {"figure_path": "MocRdX0n7B/figures/figures_8_1.jpg", "caption": "Figure 1: (a): The base detector failed to recognize objects. (b, c) However, when IIM is employed with a simple edge feature, the object is identified. (d, e) Furthermore, the full IIM utilizes a task-driven learnable kernel to extract illumination-invariant features that are richer and more suitable for the detection task than simple edge features.", "description": "This figure demonstrates the effectiveness of the Illumination-Invariant Module (IIM) in object detection under low-light conditions.  It shows that using the IIM, even with a simple edge feature (b, c), improves object recognition compared to a baseline detector that fails entirely (a).  The full IIM, employing a learned kernel, extracts richer, more effective features, further enhancing detection performance (d, e).", "section": "1 Introduction"}, {"figure_path": "MocRdX0n7B/figures/figures_13_1.jpg", "caption": "Figure 3: Qualitative comparisons of TOOD detector on both ExDark and UG2+DARK FACE dataset, where the top 2 rows visualize the detection results from ExDark, and the bottom 2 rows show the results from UG2+DARK FACE. The images are being replaced with enhanced images generated by LLIE or low-light object methods. Red dash boxes highlight the inconspicuous cases. Zoom in red dash boxes for the best view.", "description": "This figure shows a qualitative comparison of the TOOD object detector's performance on the ExDark and UG2+DARK FACE datasets.  The top two rows display results on ExDark, while the bottom two rows show results on UG2+DARK FACE.  The images used are enhanced using various low-light image enhancement (LLIE) techniques or low-light object detection methods. Red boxes highlight areas where the detectors had difficulty, particularly with inconspicuous objects.", "section": "4 Experiments"}, {"figure_path": "MocRdX0n7B/figures/figures_14_1.jpg", "caption": "Figure 3: Qualitative comparisons of TOOD detector on both ExDark and UG2+DARK FACE dataset, where the top 2 rows visualize the detection results from ExDark, and the bottom 2 rows show the results from UG2+DARK FACE. The images are being replaced with enhanced images generated by LLIE or low-light object methods. Red dash boxes highlight the inconspicuous cases. Zoom in red dash boxes for the best view.", "description": "This figure presents a qualitative comparison of the TOOD object detector's performance on the ExDark and UG2+DARK FACE datasets.  The top two rows show results on ExDark, and the bottom two rows show results on UG2+DARK FACE. Each image shows the ground truth (GT) bounding boxes and results from different methods, including the baseline, DENet, MAET, KIND, SMG, NeRCo, and the proposed YOLA method.  Red boxes highlight areas where the detectors had difficulty.", "section": "4. Experiments"}, {"figure_path": "MocRdX0n7B/figures/figures_15_1.jpg", "caption": "Figure 3: Qualitative comparisons of TOOD detector on both ExDark and UG2+DARK FACE dataset, where the top 2 rows visualize the detection results from ExDark, and the bottom 2 rows show the results from UG2+DARK FACE. The images are being replaced with enhanced images generated by LLIE or low-light object methods. Red dash boxes highlight the inconspicuous cases. Zoom in red dash boxes for the best view.", "description": "This figure shows qualitative results comparing the TOOD object detector's performance on the ExDark and UG2+DARK FACE datasets with and without the proposed YOLA method. The top two rows display ExDark results, and the bottom two rows display UG2+DARK FACE results. Red boxes highlight areas where the baseline detector failed but YOLA succeeded. This demonstrates YOLA's improved object detection in low-light conditions.", "section": "4 Experiments"}]