[{"figure_path": "ybLXvqJyQA/figures/figures_2_1.jpg", "caption": "Figure 1: A deep network model for predicting ground state properties. Given a vector x \u2208 [-1,1]m that parameterizes a quantum many-body Hamiltonian H(x), the algorithm uses geometric structure to create \u201clocal\u201d neural network models fppt . The ML algorithm then combines the outputs of these local models to predict a property tr(Op(x)), where p(x) is the ground state of H(x). Here, we decompose O = \u03a3\u03b1\u03c1 Pi for Pauli operators P\u2081, where the final layer takes a linear combination of the outputs of the local models weighted by some trainable parameters wp, that intuitively should approximate the Pauli coefficients ap.", "description": "This figure illustrates the deep neural network model used for predicting ground state properties. It consists of multiple local neural network models, each predicting the contribution of a specific Pauli operator to the ground state property. The final output is a weighted sum of these local predictions, where the weights are trainable parameters. The model leverages the geometric structure of the Hamiltonian to improve efficiency.", "section": "Rigorous guarantees for neural networks"}, {"figure_path": "ybLXvqJyQA/figures/figures_9_1.jpg", "caption": "Figure 2: Numerical experiments. (Left) Comparison with previous methods. Each point indicates the prediction error (RMSE) of our deep learning model or the regression model of [2], fixing the training set size N = 3686 and the size of the local neighorbood \u03b4\u2081 = 0 (Equation (2.3)). We train both algorithms on either LDS or uniformly random points. (Center) Scaling with training size. Each point indicates the prediction error of our deep learning model given LDS training data for various \u03b4\u2081 and training data sizes. (Right) Neural network weights and training error. Blue points correspond to the training error of the neural network model. Red points correspond to the l\u2081 norm of parameters in the last layer or the largest absolute value of the parameters of the neural network, fixing N = 3686 and \u03b4\u2081 = 1. This shows that the assumptions in Theorem 5 are achieved in practice. The shaded areas denote the 1-sigma error bars across the assessed ground state properties.", "description": "This figure presents the results of numerical experiments comparing the performance of the proposed deep learning model to a previous regression model. The left panel compares the RMSE prediction error for both methods with a fixed training set size and local neighborhood size. The center panel shows how the deep learning model scales with different training set sizes and local neighborhood sizes. The right panel shows the relationship between the neural network's training error and the magnitude of its parameters, demonstrating that the assumptions of Theorem 5 are satisfied.", "section": "4 Numerical experiments"}, {"figure_path": "ybLXvqJyQA/figures/figures_39_1.jpg", "caption": "Figure 3: Transformed low-discrepancy sequences. The blue circles correspond to two-dimensional uniform Sobol points x. The orange triangles indicate the corresponding Sobol points with respect to the CDF of the standard normal distribution, denoted by \u03a6. The latter forms a low-discrepancy-sequence with respect to the Borel measure \u03bc = \u03a6.", "description": "This figure shows a comparison between two sets of points in a 2D space. The blue circles represent points sampled from a uniform distribution using a Sobol sequence, a type of low-discrepancy sequence. The orange triangles depict points generated using the same Sobol sequence but transformed by the CDF of a standard normal distribution. This transformation alters the distribution of points, making them more concentrated around the center of the space and less dense near the boundaries. This transformation is to illustrate how to generate low-discrepancy sequences for arbitrary distributions.", "section": "C.3 Prediction on general distributions"}, {"figure_path": "ybLXvqJyQA/figures/figures_54_1.jpg", "caption": "Figure 2: Numerical experiments. (Left) Comparison with previous methods. Each point indicates the prediction error (RMSE) of our deep learning model or the regression model of [2], fixing the training set size N = 3686 and the size of the local neighorbood \u03b4\u2081 = 0 (Equation (2.3)). We train both algorithms on either LDS or uniformly random points. (Center) Scaling with training size. Each point indicates the prediction error of our deep learning model given LDS training data for various \u03b4\u2081 and training data sizes. (Right) Neural network weights and training error. Blue points correspond to the training error of the neural network model. Red points correspond to the l\u2081 norm of parameters in the last layer or the largest absolute value of the parameters of the neural network, fixing N = 3686 and \u03b4\u2081 = 1. This shows that the assumptions in Theorem 5 are achieved in practice. The shaded areas denote the 1-sigma error bars across the assessed ground state properties.", "description": "This figure presents a comparison of the deep learning model's performance against previous methods, its scaling behavior with training size, and an analysis of the neural network's weights and training error. The left panel compares the deep learning model with the regression model from [2]. The center panel shows how prediction error scales with the training set size. The right panel displays the neural network weights and training error, confirming the assumptions of Theorem 5.", "section": "Numerical experiments"}, {"figure_path": "ybLXvqJyQA/figures/figures_54_2.jpg", "caption": "Figure 2: Numerical experiments. (Left) Comparison with previous methods. Each point indicates the prediction error (RMSE) of our deep learning model or the regression model of [2], fixing the training set size N = 3686 and the size of the local neighorbood \u03b4\u2081 = 0 (Equation (2.3)). We train both algorithms on either LDS or uniformly random points. (Center) Scaling with training size. Each point indicates the prediction error of our deep learning model given LDS training data for various \u03b4\u2081 and training data sizes. (Right) Neural network weights and training error. Blue points correspond to the training error of the neural network model. Red points correspond to the l\u2081 norm of parameters in the last layer or the largest absolute value of the parameters of the neural network, fixing N = 3686 and \u03b4\u2081 = 1. This shows that the assumptions in Theorem 5 are achieved in practice. The shaded areas denote the 1-sigma error bars across the assessed ground state properties.", "description": "This figure presents a comparison of the proposed deep learning model with previous methods in terms of prediction error (RMSE), investigating the scaling of prediction error with respect to training set size and the impact of the local neighborhood size, and examining the neural network weights and training error to validate the assumptions made in Theorem 5. The left panel shows a comparison of the deep learning model and a regression model from a prior study for different training data types and neighborhood size values. The middle panel shows how prediction error changes as the training set size increases for different values of the neighborhood parameter.  The right panel examines the relationship between training error and the norm of the neural network weights, providing visual evidence for the fulfillment of the assumption in Theorem 5.", "section": "4 Numerical experiments"}, {"figure_path": "ybLXvqJyQA/figures/figures_55_1.jpg", "caption": "Figure 2: Numerical experiments. (Left) Comparison with previous methods. Each point indicates the prediction error (RMSE) of our deep learning model or the regression model of [2], fixing the training set size N = 3686 and the size of the local neighorbood \u03b4\u2081 = 0 (Equation (2.3)). We train both algorithms on either LDS or uniformly random points. (Center) Scaling with training size. Each point indicates the prediction error of our deep learning model given LDS training data for various \u03b4\u2081 and training data sizes. (Right) Neural network weights and training error. Blue points correspond to the training error of the neural network model. Red points correspond to the l\u2081 norm of parameters in the last layer or the largest absolute value of the parameters of the neural network, fixing N = 3686 and \u03b4\u2081 = 1. This shows that the assumptions in Theorem 5 are achieved in practice. The shaded areas denote the 1-sigma error bars across the assessed ground state properties.", "description": "This figure presents a comparison of the deep learning model's performance against a regression model, illustrating the scaling of prediction error with training set size and the relationship between training error, parameter norm and the  \u03b4\u2081 parameter. The left panel compares the RMSE of both models for different data distributions and a fixed training set size; the center panel examines prediction error for varying training sizes and \u03b4\u2081 values using LDS data, and the right panel visually demonstrates the relationship between training error and the l\u2081-norm of the neural network's weights.", "section": "4 Numerical experiments"}, {"figure_path": "ybLXvqJyQA/figures/figures_56_1.jpg", "caption": "Figure 2: Numerical experiments. (Left) Comparison with previous methods. Each point indicates the prediction error (RMSE) of our deep learning model or the regression model of [2], fixing the training set size N = 3686 and the size of the local neighorbood \u03b4\u2081 = 0 (Equation (2.3)). We train both algorithms on either LDS or uniformly random points. (Center) Scaling with training size. Each point indicates the prediction error of our deep learning model given LDS training data for various \u03b4\u2081 and training data sizes. (Right) Neural network weights and training error. Blue points correspond to the training error of the neural network model. Red points correspond to the l\u2081 norm of parameters in the last layer or the largest absolute value of the parameters of the neural network, fixing N = 3686 and \u03b4\u2081 = 1. This shows that the assumptions in Theorem 5 are achieved in practice. The shaded areas denote the 1-sigma error bars across the assessed ground state properties.", "description": "This figure presents a comparison of the deep learning model's performance against a regression model from a previous study [2] for predicting ground state properties of quantum systems.  The left panel shows the root mean square error (RMSE) for various system sizes, using both low-discrepancy sequences (LDS) and randomly generated data. The center panel demonstrates how the RMSE scales with the size of the training dataset for various choices of a parameter (\u03b4\u2081). Finally, the right panel shows the training error and the magnitude of the weights in the neural network, corroborating that the assumptions for the theoretical guarantee hold.", "section": "4 Numerical experiments"}]