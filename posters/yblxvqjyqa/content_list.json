[{"type": "text", "text": "Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Marc Wanner ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Laura Lewis ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science and Engineering Chalmers University of Technology and University of Gothenburg wanner@chalmers.se ", "page_idx": 0}, {"type": "text", "text": "Applied Mathematics and Theoretical Physics University of Cambridge Cambridge, United Kingdom llewis@alumni.caltech.edu ", "page_idx": 0}, {"type": "text", "text": "Chiranjib Bhattacharyya ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Devdatt Dubhashi ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science and Automation Indian Institute of Science Bangalore, India chiru@iisc.ac.in ", "page_idx": 0}, {"type": "text", "text": "Computer Science and Engineering Chalmers University of Technology and University of Gothenburg dubhashi@chalmers.se ", "page_idx": 0}, {"type": "text", "text": "Alexandru Gheorghiu ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computer Science and Engineering Chalmers University of Technology and University of Gothenburg aleghe@chalmers.se ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A fundamental problem in quantum many-body physics is that of finding ground states of local Hamiltonians. A number of recent works gave provably efficient machine learning (ML) algorithms for learning ground states. Specifically, Huang et al. in [1], introduced an approach for learning properties of the ground state of an $n$ -qubit gapped local Hamiltonian $H$ from only $\\bar{n}^{\\bar{\\mathcal{O}}(1)}$ data points sampled from Hamiltonians in the same phase of matter. This was subsequently improved by Lewis et al. in [2], to ${\\mathcal{O}}(\\log n)$ samples when the geometry of the $n$ -qubit system is known. In this work, we introduce two approaches that achieve a constant sample complexity, independent of system size $n$ , for learning ground state properties. Our first algorithm consists of a simple modification of the ML model used by Lewis et al. and applies to a property of interest known in advance. Our second algorithm, which applies even if a description of the property is not known, is a deep neural network model. While empirical results showing the performance of neural networks have been demonstrated, to our knowledge, this is the first rigorous sample complexity bound on a neural network model for predicting ground state properties. We also perform numerical experiments on systems of up to 45 qubits that confirm the improved scaling of our approach compared to [1, 2]. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "One of the most important problems in quantum many-body physics is that of finding ground states of quantum systems. This is due to the fact that the ground state describes the behavior of electronic systems (e.g., metals, magnets, etc.) at room temperature well. Thus, understanding the ground state can provide insights into, for example, chemical properties of molecules, leading to many potential applications in chemistry and materials science. However, despite extensive research [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], an efficient classical algorithm solving this problem in full generality remains out of reach. On the other hand, researchers have successfully leveraged classical machine learning (ML) techniques to solve (albeit largely heuristically) the ground state problem and other related quantum many-body problems [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]. Rather than solving these problems directly from first principles, ML algorithms are given some training data collected from physical experiments and are asked to generalize it to new inputs. Intuitively, this additional data can make the problem easier and thus may open the door to obtaining provably efficient classical ML algorithms for finding ground states. This data-driven approach is in some sense necessary, since finding the ground state from the Hamiltonian alone is known to be QMA-hard in general [47], and thus out of reach for both efficient classical and quantum algorithms. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In a recent work [1], Huang et al. proposed the first provably efficient ML algorithm for predicting ground state properties of gapped geometrically local Hamiltonians. In particular, the algorithm in [1] uses an amount of training data (or sample complexity) that scales as $O(n^{1/\\epsilon})$ , where $n$ is the system size and $\\epsilon$ is the prediction error of the ML algorithm. Recently, [2] improved this guarantee, achieving $\\mathcal{O}(\\log(n)2^{\\mathrm{polylog}(1/\\epsilon)})$ , an exponential improvement with respect to the system size $n$ . The same sample complexity was obtained by [48] for the task of learning thermal state properties with exponential decay of correlations. Moreover, [49] extended this to Lindbladian phases of matter [50] with local rapid mixing, including both ground states of gapped Hamiltonians and thermal states. The work of [51] obtains a similar guarantee assuming the continuity of quantum states in the parameter range of interest but focusing on the scaling with respect to $1/\\epsilon$ rather than system size. ", "page_idx": 1}, {"type": "text", "text": "These previous works drastically improve the sample complexity of the original Huang et al. result [1], but none prove sample complexity lower bounds for their respective tasks, leaving open the possibility of further reducing the sample complexity. In addition, [2, 48, 49] all use fairly simple learning models, i.e., regularized linear regression or taking empirical averages of classical shadows [52], respectively. With the emergence of neural networks as a popular model in practical ML, one may wonder if these more powerful ML tools may be useful to predict ground state properties as well. In fact, recent works [37, 36] empirically demonstrate a favorable sample complexity using neural-network-based ML algorithms. However, there are currently no rigorous theoretical guarantees regarding the amount of training data needed to achieve a desired prediction error. These remarks lead us to the following two central questions of this work. ", "page_idx": 1}, {"type": "text", "text": "Question 1. Can classical ML algorithms predict ground state properties with even less than $\\bar{\\mathcal{O}}(\\log(n)2^{\\mathrm{polylog}(1/\\epsilon)})$ data? ", "page_idx": 1}, {"type": "text", "text": "This is especially relevant for systems approaching the thermodynamic limit, where the system size can be arbitrarily large. Needing fewer samples also means less work for experimentally preparing ground states of the system. The second question, stated as an open question in [1, 2] is: ", "page_idx": 1}, {"type": "text", "text": "Question 2. Can we obtain rigorous sample complexity guarantees for neural-network-based ML algorithms for predicting ground state properties? ", "page_idx": 1}, {"type": "text", "text": "Our results ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We give positive answers to both questions. We consider the same assumptions as [2] with minimal additional ones that we mention here. First, we show that a simple modification to the approach in [2] allows us to achieve a sample complexity that is independent of the system size. This does, however require knowledge of the property we wish to predict in advance, whereas this is not a requirement in [2]. We view this as a reasonable assumption, since in practice we can imagine preparing ground states of some system in order to measure a specific property of interest. We show the following theorem, stated informally here. The formal statement, including all the assumptions required for proving the result, can be found in Appendix B. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1 (Informal). Let $H(x)$ be an $n$ -qubit gapped, geometrically local Hamiltonian with ground state $\\rho(x)$ . Given an observable $O$ , with a known decomposition as a sum of local Pauli operators and given training data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ sampled from an arbitrary distribution, with $y_{\\ell}\\approx\\mathrm{tr}(\\bar{O}\\rho(x_{\\ell}))$ , there is an ML algorithm for predicting ground state properties $\\mathrm{tr}(O\\rho(x))$ to within precision $\\epsilon>0$ using $N=\\mathcal{O}\\left(2^{\\mathrm{polylog}(1/\\epsilon)}\\right)$ training samples. ", "page_idx": 1}, {"type": "image", "img_path": "ybLXvqJyQA/tmp/049b670d28bed4f681bac70e43f757801d83ca832600467b73f6c4f9a006d024.jpg", "img_caption": ["Figure 1: A deep network model for predicting ground state properties. Given a vector $x\\in$ $[-1,1]^{m}$ that parameterizes a quantum many-body Hamiltonian $H(x)$ , the algorithm uses geometric structure to create \u201clocal\u201d neural network models fP\u03b8P i. The ML algorithm then combines the outputs of these local models to predict a property $\\mathrm{tr}(O\\bar{\\rho}(x))$ , where $\\rho(x)$ is the ground state of $H(x)$ . Here, we decompose O = iM=1 \u03b1PiPi for Pauli operators Pi, where the final layer takes a linear combination of the output s of the local models weighted by some trainable parameters $w_{P_{i}}$ that intuitively should approximate the Pauli coefficients $\\alpha_{P_{i}}$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Note that the number of samples $N$ depends only on the desired prediction error $\\epsilon$ and is independent of the system size. In particular, this means that for a fixed prediction error our algorithm requires only a constant amount of training data. Moreover, the computational complexity of our algorithm improves upon [2], having ${\\mathcal{O}}(n)$ runtime, compared to the previous $\\mathcal{O}(n\\log n)$ . While removing the $\\log n$ factor may seem like a small improvement, in practice this can make a significant difference. For instance, for a system of $n\\sim1000$ qubits, removing the $\\log n$ factor would result in a ten-fold reduction in training data and time. ", "page_idx": 2}, {"type": "text", "text": "Much like in [2], this result also extends to learning classical representations of $\\rho(x)$ . In other words, if the algorithm is instead given classical shadows [52] of the ground state as training data, it can then predict a classical representation of $\\rho(x)$ for new parameters $x$ . This can mitigate the requirement that the observable is known in Theorem 1, as predicting properties from a classical representation clearly requires knowledge of the observable. ", "page_idx": 2}, {"type": "text", "text": "Our second result shows the same sample complexity guarantee for a neural network ML algorithm (Figure 1) [53], in which one does not need to know the observable being measured in advance. An additional constraint that we require in this case is that the training data is not sampled according to an arbitrary distribution, but a distribution satisfying some technical assumptions. We note that these assumptions are satisfied for common distributions such as uniform and Gaussian. ", "page_idx": 2}, {"type": "text", "text": "Theorem 2 (Informal). Let $H(x)$ be an $n$ -qubit gapped, geometrically local Hamiltonian with ground state $\\rho(x)$ . For any observable $O$ , expressible as a sum of local Pauli operators and given training data $\\{(\\dot{x}_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ , sampled from a distribution satisfying certain assumptions with $y_{\\ell}\\approx\\mathrm{tr}(O\\rho(x_{\\ell}))$ , there is a neural network ML algorithm for predicting ground state properties $\\mathrm{tr}(O\\rho(x))$ , for uniform $x$ , to within precision $\\epsilon>0$ using $N=\\mathcal{O}\\left(2^{\\mathrm{polylog}(1/\\epsilon)}\\right)$ training samples under mild assumptions on training. ", "page_idx": 2}, {"type": "text", "text": "We prove this result by making use of the Koksma-Hlawka inequality from quasi-Monte Carlo theory [54, 55, 56, 57, 58] and combining it with the spectral flow formalism [59, 60, 61]. ", "page_idx": 2}, {"type": "text", "text": "Similar to Theorem 1, we can also extend this result to learning classical representations of $\\rho(x)$ when given classical shadow training data. The formal statement and its proof can be found in Appendix C. We also remark that, much like the setting in [51], a more favorable scaling with respect to $\\epsilon$ can be achieved if the number of parameters that the Hamiltonian depends on is constant. In particular, it was shown in [51] that if the number of parameters is constant, the sample complexity scales as $N\\,=\\,\\mathsf{p o l y}(1/\\epsilon,\\log(n))$ . For our results, this similarly yields $N\\,=\\,\\mathsf{p o l y}(1/\\epsilon)$ , preserving the independence on the system size while also achieving a polynomial scaling in $1/\\epsilon$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Furthermore, we perform numerical experiments on system sizes of up to 45 qubits, which support our theoretical findings, and show that, in practice, our deep learning algorithm outperforms previous methods [2]. We describe them in detail in Appendix D, and they are illustrated in Figure 2. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "2.1 Problem statement ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "First, we formally describe the problem setting, which is the same as [2]. We consider a family of $n$ -qubit Hamiltonians $H(x)$ smoothly parameterized by an $m$ -dimensional vector $x\\in[-1,1]^{m}$ We assume that these Hamiltonians are gapped for all choices of parameters $x\\,\\in\\,[-1,1]^{m}$ and geometrically local such that they can be written as a sum of local terms ", "page_idx": 3}, {"type": "equation", "text": "$$\nH(x)=\\sum_{j=1}^{L}h_{j}(\\vec{x}_{j}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the parameter vector $x$ is a concatenation of the constant-dimensional vectors $\\vec{x}_{1},\\ldots,\\vec{x}_{L}$ . Each of these constant-dimensional vectors ${\\vec{x}}_{j}$ parameterizes the local interaction term $h_{j}(\\vec{x}_{j})$ . Crucially, we assume that each local term $h_{j}$ only depends on a constant number of parameters rather than the entire parameter vector $x$ . We also assume that the geometry of the $n$ -qubit system is known. ", "page_idx": 3}, {"type": "text", "text": "Throughout this work, we use $\\rho(x)$ to denote the ground state of the Hamiltonian $H(x)$ and $O$ to denote an observable that can be written as a sum of geometrically local observables with bounded spectral norm $\\|O\\|_{\\infty}\\leq1$ . Here, the ground states $\\rho(x)$ form a gapped quantum phase of matter. Given samples of quantum states drawn from this phase, we wish to predict expectation values of observables $O$ with respect to other states in the same phase. In other words, we are given training data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ , w here $y_{\\ell}\\approx\\mathrm{tr}(O\\rho(x_{\\ell}))$ approximates the ground state property for a parameter choice $x_{\\ell}\\in[-1,1]^{m}$ sampled from some distribution over the parameter space. We aim to learn a function $h^{*}(x)$ that approximates the ground state property $\\mathrm{tr}(\\bar{O}\\rho(x))$ for some unseen parameter $x$ while minimizing the amount of training data, or sample complexity, $N$ . How well we learn the ground state property is quantified by the average prediction error ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\vert h^{*}(x)-\\mathrm{tr}(O\\rho(x))\\vert^{2}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We describe the precise conditions under which Theorems 1 and 2 hold in the following sections. ", "page_idx": 3}, {"type": "text", "text": "2.2 Review of previous algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we review the previous algorithm from [2], as our proofs rely on similar ideas. For full details, we refer the reader to [2] and our more detailed presentation in Appendix A.1. The ML algorithm proposed in [2] requires some geometric definitions. Fix a geometrically local Pauli observable $P\\in\\{\\bar{I},X,Y,Z\\}^{\\otimes n}$ . ", "page_idx": 3}, {"type": "text", "text": "Let $\\delta_{1},B>0$ be efficiently-computable hyperparameters that we define in Appendix A.1. Define the set $I_{P}$ of coordinates $c$ such that $x_{c}$ parameterizes some local term $h_{j(c)}$ that is close to the Pauli $P$ Here, the distance between two observables $d_{\\mathrm{obs}}$ is defined as the minimum distance between the qubits that the observables act on, where the distance between qubits is given by the geometry of the system, which we assume to be known. Formally, we define this set of local coordinates as ", "page_idx": 3}, {"type": "equation", "text": "$$\nI_{P}\\triangleq\\{c\\in\\{1,\\dots,m\\}:d_{\\mathrm{obs}}(h_{j(c)},P)\\leq\\delta_{1}\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $h_{j(c)}$ is the local term in the Hamiltonian $H(x)$ whose parameters $\\vec{x}_{j(c)}$ include the variable $x_{c}$ . The intuition behind this set of coordinates is that it indexes the parameters $x_{c}$ that influence the ground state property $\\operatorname{tr}(P\\rho(x))$ corresponding to the Pauli $P$ . The algorithm consists of two steps. First, it maps the parameter space $[-1,1]^{m}$ to a high dimensional space via a nonlinear feature map $\\phi$ . Second, it runs $\\ell_{1}$ -regularized linear regression (LASSO) [62, 63, 64] over the feature space. ", "page_idx": 3}, {"type": "text", "text": "This first step encodes the geometry of the problem. The feature map intuitively projects a given parameter vector $x$ onto the local parameter space $\\{x_{c}\\,:\\,c\\,\\in\\,I_{P}\\}$ . We define this precisely in ", "page_idx": 3}, {"type": "text", "text": "Appendix A.1. Following the feature mapping, the $\\mathrm{ML}$ algorithm uses LASSO [62, 63, 64] to learn functions of the form $\\{\\bar{h}(x)=\\mathbf{w}\\cdot\\phi(x)\\,\\bar{:}\\,\\|\\mathbf{\\bar{w}}\\|_{1}\\leq B\\}$ for a chosen hyperparameter $B>0$ . We denote the learned function by $h^{*}(x)=\\mathbf{w}^{*}\\cdot\\phi(x)$ . For our purposes, we set $B=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ . This algorithm obtains the following rigorous guarantee. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 (Theorem 1 in [2]). Given $n,\\delta>0$ , $\\mathit{\\Pi}_{e}^{1}>\\epsilon>0$ and a training data set $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 4}, {"type": "equation", "text": "$$\nN=\\log(n/\\delta)^{\\mathrm{polylog}(1/\\epsilon)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $x_{\\ell}$ is sampled from an unknown distribution $\\mathcal{D}$ and $|y_{\\ell}\\!-\\!\\mathrm{tr}(O\\rho(x_{\\ell}))|\\leq\\epsilon$ . With a proper choice of the efficiently computable hyperparameters $\\delta_{1},\\delta_{2}$ , and $B$ , the learned function $h^{*}(x)=\\mathbf{w}^{*}\\cdot\\phi(x)$ satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left|h^{*}(x)-\\mathrm{tr}(O\\rho(x))\\right|^{2}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with probability at least $1-\\delta$ . The training and prediction time of the classical ML model are bounded by $\\mathcal{O}(n N)=n\\log(n/\\delta)2^{\\mathrm{polylog}(1/\\epsilon)}$ . ", "page_idx": 4}, {"type": "text", "text": "A crucial step in the proof is that ground state properties can indeed be approximated by linear functions over the feature space. Along the way, [2] proves that the ground state property can be approximated by a linear combination of \u201clocal functions,\u201d which are local in that they only depend on parameters with coordinates in the set $I_{P}$ . We relegate further details to Appendix A.1 and [2]. ", "page_idx": 4}, {"type": "text", "text": "3 Main results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we discuss our rigorous guarantees for predicting ground state properties with constant sample complexity and with neural-network-based ML algorithms. ", "page_idx": 4}, {"type": "text", "text": "3.1 Constant sample complexity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we show that a simple modification of the algorithm from [2] can achieve a sample complexity that is independent of the system size $n$ , under the additional assumption that the observable $O$ is known. In practice, a scientist often has a specific ground state property in mind that they wish to study, so we view this as a natural assumption. Moreover, this is still an interesting learning problem, as when obtaining the training data via quantum experiments, preparing the ground state $\\rho(x)$ in the laboratory for a new choice of parameters $x$ may be difficult experimentally. This in turn means that accurately predicting some property $\\mathrm{tr}(O\\rho(\\bar{x}))$ for a new choice of $x$ may be challenging, even if the property of interest, $O$ , is known. ML algorithms can allow us to circumvent this issue and generalize from the results of few training data points without needing to prepare the ground state directly. ", "page_idx": 4}, {"type": "text", "text": "Let $\\begin{array}{r}{O=\\sum_{P\\in\\{I,X,Y,Z\\}\\otimes n}\\alpha_{P}P}\\end{array}$ be an observable that can be written as a sum of geometrically local observables. Because $O$ is assumed to be known, we can find this decomposition of $O$ in terms of the Pauli observables $P$ . The overall structure of the algorithm remains the same: perform a nonlinear feature mapping followed by linear regression. However, there are two key differences from the previous algorithm [2]. First, we change the feature mapping of [2] to incorporate the Pauli coefficients $\\alpha_{P}$ . We define this new feature mapping $\\tilde{\\phi}$ in Appendix B. The second difference from [2] is that we use ridge regression [65, 66] instead of LASSO [62, 63, 66]. Recall that LASSO learns hypothesis functions of the form $\\{h(x)=\\mathbf{w}\\cdot\\tilde{\\phi}(x):\\|\\mathbf{w}\\|_{1}\\leq B\\}$ for some hyperparameter $B>0$ . In contrast, ridge regression replaces the $\\ell_{1}$ -norm constraint $\\|\\mathbf{\\dot{w}}\\|_{1}\\leq B$ with an $\\ell_{2}$ -norm constraint: $\\|\\mathbf{w}\\|_{2}\\leq\\Lambda$ , for some hyperparameter $\\Lambda>0$ . Namely, for a chosen efficiently-computable hyperparameter $\\Lambda>0$ , ridge regression finds a vector $\\mathbf{w}^{*}$ that minimizes the training error subject to the constraint that $\\|\\mathbf{w}\\|_{2}\\leq\\Lambda$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{m_{\\phi}}}\\frac{1}{N}\\sum_{\\ell=1}^{N}|\\mathbf{w}\\cdot\\tilde{\\phi}(x_{\\ell})-y_{\\ell}|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Standard results in machine learning theory give sample complexity upper bounds for ridge regression in terms of $\\Lambda$ and the $\\ell_{2}$ -norm of the feature vector $\\tilde{\\phi}(x)$ [65, 66]. The key idea is that with our new feature mapping, we can still approximate the ground state property by a linear function over the feature space, as in [2], to obtain a low training error. Meanwhile, by incorporating the Pauli coefficients $\\alpha_{P}$ into the feature map, we can bound the $\\ell_{2}$ -norm of $\\tilde{\\phi}(x)$ by a quantity independent of system size, leveraging bounds on the $\\ell_{1}$ -norm of the Pauli coefficients [2, 67]. We note that naively applying ridge regression with the feature map from [2] does not achieve the same guarantees and in fact gives worse scaling than [2]. Similarly, we can also choose a suitable $\\Lambda>0$ independent of system size. Thus, we obtain the following guarantee. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Theorem 4 (Constant sample complexity). Given $n,\\delta>0,\\,1/e>\\epsilon>0$ and a training data set $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 5}, {"type": "equation", "text": "$$\nN=\\log(1/\\delta)2^{\\mathrm{polylog}(1/\\epsilon)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{\\ell}$ is sampled from an unknown distribution $\\mathcal{D}$ and $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\,\\le\\,\\epsilon.$ . With a proper choice of the efficiently computable hyperparameters $\\delta_{1},\\delta_{2},\\Lambda,$ the learned function $h^{*}(x)$ satisfies ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|h^{*}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with probability least $1-\\delta$ . The training and prediction time of the classical ML model are bounded by $\\mathcal{O}(n)\\mathrm{polylog}(1/\\delta)2^{\\mathrm{polylog}(1/\\epsilon)}$ . ", "page_idx": 5}, {"type": "text", "text": "We compare this result to Theorem 3. For a constant prediction error $\\epsilon\\,=\\,\\mathcal{O}(1)$ , our proposed algorithm achieves a constant sample complexity $N=\\mathcal{O}(1)$ , compared to the logarithmic sample complexity $N\\;=\\;\\mathcal{O}(\\log n)$ of [2]. Moreover, we also improve the computational complexity, achieving a linear-in- ${\\cdot n}$ runtime, compared to the previous $\\mathcal{O}(n\\log n)$ . The scaling with respect to the prediction error $\\epsilon$ is the same as the previous algorithm [2]. This means that regardless of how large our quantum system is, we need the same amount of samples to predict ground state properties well. This is especially important for settings in which obtaining training data for large systems is difficult. ", "page_idx": 5}, {"type": "text", "text": "Thus far, we have only considered the setting in which we learn a specific ground state property $\\mathrm{tr}(O\\rho(x))$ for a fixed observable $O$ . Because our training data is given in the form $\\{(x_{\\ell},\\dot{y}_{\\ell})\\}_{\\ell=1}^{N}$ where $y_{\\ell}$ approximates $\\mathrm{tr}(O\\rho(x))$ for this fixed observable $O$ , if we want to predict a new property for the same ground state $\\rho(x)$ , we would need to generate new training data. Thus, it may be more useful to learn a ground state representation, from which we could predict $\\mathrm{tr}(O\\rho(x))$ for many different choices of observables $O$ without requiring new training data. In this case, suppose we are instead given training data $\\{x_{\\ell},\\sigma_{T}(\\rho(x_{\\ell}))\\}_{\\ell=1}^{N}$ , where $\\sigma_{T}(\\rho(x_{\\ell}))$ is a classical shadow representation [52, 68, 69, 70, 71] of the ground state $\\bar{\\rho(x_{\\ell})}$ . An immediate corollary of Theorem 4 is that we can predict ground state representations with the same sample complexity. This follows from the same proof as Corollary 5 in [2]. ", "page_idx": 5}, {"type": "text", "text": "Corollary 1 (Learning representations of ground states). Let $n,\\delta>0,\\,1/e>\\epsilon>0$ and $\\delta>0$ . Given training data $\\{(\\tilde{x}_{\\ell},\\dot{\\sigma}_{T}(\\rho(x_{\\ell}))\\}_{\\ell=1}^{N}$ of size ", "page_idx": 5}, {"type": "equation", "text": "$$\nN=\\log(1/\\delta)2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon))},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $x_{\\ell}$ is sampled from $\\mathcal{D}$ and $\\sigma_{T}(\\rho(x_{\\ell})$ is the classical shadow representation of the ground state $\\rho(x_{\\ell})$ using $T$ randomized Pauli measurements. For $T=\\tilde{\\mathcal{O}}(\\log(n/\\delta)/\\epsilon^{2}),$ , with probability at least $1-\\delta$ , the ML algorithm will produce a ground state representation $\\hat{\\rho}_{N,T}(x)$ that achieves ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|\\,\\mathrm{tr}(O\\hat{\\rho}_{N,T}(x))-\\mathrm{tr}(O\\rho(x))|^{2}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for any observable with $\\|O\\|_{\\infty}\\leq1$ that can be written as a sum of geometrically local observables. ", "page_idx": 5}, {"type": "text", "text": "3.2 Rigorous guarantees for neural networks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we prove the existence of a deep neural network model that can predict ground state properties using a constant number of training samples. In particular, we prove that after training on a constant number of samples from a distribution $\\mathcal{D}$ on $[-1,1]^{m}$ satisfying certain technical assumptions, our model can achieve a low prediction error under mild assumptions on training. In this case, for predicting properties $\\mathrm{tr}(O\\rho(x))$ , the observable $O$ need not be known in advance. However, we need to assume that all mixed first order derivatives of the Hamiltonian $\\left\\|\\partial^{m}H(x)/\\partial x_{1}\\cdot\\cdot\\cdot\\partial x_{m}\\right\\|_{\\infty}\\leq1$ exist and are bounded. This is not much stronger than [2], which assumes that directional derivatives $\\partial h_{j}/\\partial{\\hat{u}}$ are bounded by one for any direction $\\hat{u}$ . Moreover, we also need the training data to be sampled from a distribution $\\mathcal{D}$ with probability density function $g$ satisfying the following assumptions: $g$ has full support and is continuously differentiable on $[-1,1]^{m}$ . Also, $g$ is of the form $\\begin{array}{r}{g(x)=\\prod_{j=1}^{L}g_{j}(\\vec{x}_{j})}\\end{array}$ . This resembles our assumption on the form of the Hamiltonian $H(x)$ . Furthermore, the average prediction error is measured with respect to the same distribution $\\mathcal{D}$ . We note that these assumptions are satisfied for common distributions such as uniform and Gaussian. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "As in the previous algorithm [2], we leverage the geometry of the $n$ -qubit system to approximate the ground state properties by a linear combination of smooth local functions, which only depend on parameters with coordinates in the local coordinate set $I_{P}$ defined in Equation (2.3). Crucially, the size $\\tilde{m}\\triangleq|I_{P}|$ of the domains of these local functions is independent of the system size. ", "page_idx": 6}, {"type": "text", "text": "Instead of using a feature map and linear regression to learn the ground state properties, we utilize a deep neural network model defined as follows. Inspired by the local approximation of ground state properties, we define \u201clocal models\u201d $f_{P}^{\\theta_{P}}:[-1,1]^{\\tilde{m}}\\rightarrow\\mathbb{R},$ , which are neural networks consisting of three layers of affine transformations and applications of a nonlinear activation function. In particular, $f_{P}^{\\theta_{P}}$ has two hidden layers with the affine transformations given by the trainable weights and biases denoted by $\\theta_{P}$ . We take hyperbolic tangent, tanh, as the activation function. These local models are then combined into a model $f^{\\Theta,w}:[-\\overline{{1}},1]^{m}\\to\\mathbb{R}$ given by ", "page_idx": 6}, {"type": "equation", "text": "$$\nf^{\\Theta,w}(x)=\\sum_{P\\in S^{(\\mathrm{geo})}}w_{P}f_{P}^{\\theta_{P}}(x),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $w_{P}\\in\\mathbb{R}$ are the weights in the last layer and $\\Theta=\\{\\theta_{P}\\}_{P\\in S^{(\\mathrm{geo})}}$ . This model is schematically illustrated in Figure 1. We refer to Definition 6 in Appendix C for a full description of the model. ", "page_idx": 6}, {"type": "text", "text": "Consider training data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ , where $x_{\\ell}$ are sampled according to a distribution $\\mathcal{D}$ satisfying the assumptions described above and $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\leq\\epsilon$ . The ML algorithm first initializes the weights via standard deep learning initialization procedures, e.g., Xavier initialization [72]. Then, the algorithm performs quasi-Monte Carlo training given the training data, e.g., Adam [73], to find weights $\\Theta^{*},w^{*}$ which minimize the training objective function ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{\\ell=1}^{N}|f^{\\Theta,w}(x_{\\ell})-y_{\\ell}|^{2}+\\lambda\\|w\\|_{1},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\lambda$ is some regularization parameter that may depend on $\\epsilon$ . For this algorithm, we prove the following theorem bounding the average prediction error of our deep neural network model. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5 (Neural network sample complexity guarantee). Let $1/e>\\epsilon>0$ . Let $\\mathcal{D}$ be a distribution with probability density function $g$ satisfying the properties stated above. Let $f^{\\Theta^{*},w^{*}}:[-1,1]^{m}\\to\\mathbb{R}$ be a neural network model trained on data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 6}, {"type": "equation", "text": "$$\nN=\\mathcal{O}\\left(\\log(1/\\delta)2^{\\mathrm{polylog}(1/\\epsilon)}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the $x_{\\ell}$ \u2019s are sampled from $\\mathcal{D}$ and $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\,\\le\\,\\epsilon$ . Suppose that $f^{\\Theta^{*},w^{*}}$ achieves a value no larger than $O(\\epsilon)$ on the training objective (Equation (3.7)) with $\\lambda(\\epsilon)={\\mathcal O}(\\epsilon)$ . Additionally, suppose that all parameters $\\Theta_{i}^{*}$ of $f^{\\Theta^{*},w^{*}}$ satisfy $|\\Theta_{i}^{*}|\\;\\leq\\;W_{\\operatorname*{max}},$ , for some $W_{m a x}\\,>\\,0$ that is independent of $n$ . Then ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}|f^{\\Theta^{*},w^{*}}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similar to Theorem 4, for a constant prediction error $\\epsilon=\\mathcal{O}(1)$ , the deep neural network algorithm achieves constant sample complexity $N\\,=\\,\\mathcal{O}(1)$ . In contrast to Theorem 4, we do not require knowledge about the observable, $O$ . This is a direct consequence of the regularity of $w_{P}$ , which is achieved when the training objective is small. Theorem 6 guarantees, that a model with such regularity can yield a small prediction error. ", "page_idx": 6}, {"type": "text", "text": "There are, however, some caveats compared to the previous result. First, the training data is restricted to being sampled from a distribution satisfying our technical assumptions stated previously, in contrast to Theorem 4 which holds for data sampled from any arbitrary unknown distribution. Second, in regards to the model, the weights must be bounded by a constant $W_{\\mathrm{max}}$ . Finally, we cannot guarantee a priori that the network will indeed achieve a low training error. This is due to the fact that our training objective is non-convex and thus, globally optimal weights cannot be found efficiently in general [74]. Even so, we are still able to prove the existence of suitable weights such that the resulting network approximates $\\mathrm{tr}(O\\rho(x))$ for any $x\\in[-1,1]^{m}$ (see Theorem 6 in the next section). ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "However, we view the assumptions made in Theorem 5 as being mild in practice. Small training objectives are commonly achieved in deep learning so we expect our training algorithm to produce a model which fulfills the assumptions of Theorem 5 after $\\mathcal{O}(1)$ training steps and ${\\mathcal{O}}(n)$ runtime. Moreover, it is known that gradient descent provably converges to the global optimum for overparametrized deep neural networks, while the weights remain small, when properly initialized [75]. We verify that these conditions are satisfied in practice through our numerical experiments in Figure 2 and ?? . To our knowledge, Theorem 5 is the first rigorous sample complexity bound on a neural network model for predicting ground state properties. ", "page_idx": 7}, {"type": "text", "text": "We also note that if the training data is instead sampled according to a low-discrepancy sequence (LDS) [55, 56, 76, 77, 78, 57, 79, 58, 80, 81], we can obtain better guarantees, but these improvements are hidden in the polylogarithmic factors in the exponential. We discuss learning given data from a LDS in Appendix C. Intuitively, a LDS is a collection of points in the parameter space that covers the space such that there are no large gaps, or discrepancies. ", "page_idx": 7}, {"type": "text", "text": "Similar to Corollary 1, if we are instead given training data $\\{x_{\\ell},\\sigma_{T}(\\rho(x_{\\ell}))\\}_{\\ell=1}^{N}$ , where $\\sigma_{T}(\\rho(x_{\\ell}))$ is a classical shadow representation [52, 68, 69, 70, 71] of the ground state $\\rho(x_{\\ell})$ , then we obtain the following immediate corollary of Theorem 5. ", "page_idx": 7}, {"type": "text", "text": "Corollary 2 (Learning representations of ground states with neural networks). Let $n,\\delta>0,1/e>$ $\\epsilon>0$ and $\\delta>0$ . Given training data $\\{(\\bar{x_{\\ell}},\\sigma_{T}(\\rho(x_{\\ell}))\\}_{\\ell=1}^{N}$ of size ", "page_idx": 7}, {"type": "equation", "text": "$$\nN=\\mathcal{O}\\left(\\log(1/\\delta)2^{\\mathrm{polylog}(1/\\epsilon)}\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $x_{\\ell}$ is sampled from a distribution $\\mathcal{D}$ satisfying the same assumptions as Theorem 5 and $\\sigma_{T}(\\rho(x_{\\ell})$ is the classical shadow representation of the ground state $\\rho(x_{\\ell})$ using $T$ randomized Pauli measurements. For $T={\\tilde{\\mathcal{O}}}(\\log(n/\\delta)/\\epsilon^{2})$ , with probability at least $1-\\delta$ , the ML algorithm will produce a ground state representation $\\hat{\\rho}_{N,T}(x)$ that achieves ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|\\,\\mathrm{tr}(O\\hat{\\rho}_{N,T}(x))-\\mathrm{tr}(O\\rho(x))|^{2}\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for any observable with $\\|O\\|_{\\infty}\\leq1$ that can be written as a sum of geometrically local observables. ", "page_idx": 7}, {"type": "text", "text": "3.2.1 Proof ideas for neural network guarantee ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To prove Theorem 5, we first show that our neural network model $f^{\\Theta,w}$ can approximate the ground state properties well. In particular, we show that there exist weights \u0398\u2032, w\u2032 such that f \u0398,w approximates the ground state properties and thus achieves small value for the training objective (Equation (3.7)). Then, we bound the prediction error using tools from deep learning and quasi-Monte Carlo theory [54, 55, 56, 57, 58]. We ensure the existence of $f^{\\Theta^{\\prime},w^{\\prime}}$ in the following theorem. ", "page_idx": 7}, {"type": "text", "text": "Theorem 6. For any $1/e>\\epsilon>0$ and width $W$ , there exist weights $\\Theta^{\\prime},w^{\\prime}$ such that the neural network model f \u0398\u2032,w\u2032s atisfies ", "page_idx": 7}, {"type": "equation", "text": "$$\n|f^{\\Theta^{\\prime},w^{\\prime}}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\leq\\epsilon,\\quad\\forall x\\in[-1,1]^{m}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Moreover, each parameter $\\Theta_{i}$ of the network has a magnitude of $|\\Theta_{i}|=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon))}$ . ", "page_idx": 7}, {"type": "text", "text": "This implies that for a suitable choice of regularization parameter $\\lambda=\\mathcal{O}(\\epsilon)$ , the training objective from Equation (3.7) is also small. We prove this statement by combining results in deep learning regarding tanh neural networks approximating functions [53] with the geometric locality of the system and smoothness of the ground state properties. We note that the weights $\\Theta^{\\prime},w^{\\prime}$ in Theorem 6 are not necessarily the weights $\\Theta^{*},w^{*}$ found via the neural network training procedure in Theorem 5. Because the training objective is non-convex, we cannot guarantee convergence to these weights $\\Theta^{\\prime},w^{\\prime}$ . However, assuming that $f^{\\Theta^{*},w^{*}}$ does indeed achieve a low training error (which is often satisfied in practice), we are able to rigorously guarantee that the model will generalize well and achieve a low prediction error in Theorem 5. ", "page_idx": 7}, {"type": "text", "text": "Notice that the guarantee of Theorem 6 holds for all $x$ and, in particular, does not require our assumptions on the distribution $\\mathcal{D}$ . The assumption that the network is trained on such data only becomes relevant when bounding the prediction error. While not explicitly stated here, we also note that Theorem 6 gives a bound on the number of trainable parameters $|\\Theta_{i}|$ that has a similar dependence on $\\epsilon$ as the model in [2]. Furthermore, the parameters are independent of system size, $n$ . Additional smoothness assumptions on the Hamiltonian $H(x)$ can yield mild improvements on the dependence in terms of $\\epsilon$ , as briefly discussed in Appendix C.1. Moreover, because of this bound on $|\\Theta_{i}|$ , applying an additional penalty on the $\\ell_{2}$ -norm of the weights $\\Theta$ can help ensure that the weights remain small. In practice, this is usually satisfied during training when the weights are initialized properly and the inputs are regularized, e.g. [53]. Thus, the condition that $\\vert\\Theta_{i}^{\\ast}\\vert\\leq W_{\\mathrm{max}}$ is often satisfied in practice and is not considered a strong assumption in deep learning. ", "page_idx": 8}, {"type": "text", "text": "To prove the prediction error bound in Theorem 5 assuming that a low training error is achieved, we combine techniques from quasi-Monte Carlo theory applied to deep learning [54] (see Appendix A.2 for a review) along with our knowledge of the geometry of the $n$ -qubit system. In contrast to [54], we need to characterize the dimension of the input domain in our approach. The reason for doing this is that the approximation error depends on the size $\\tilde{m}=|I_{P}|$ (Equation (2.3)) of our local models $f_{P}^{\\theta_{P}}$ . ", "page_idx": 8}, {"type": "text", "text": "The central result we use here is the Koksma-Hlawka inequality [56] (see Theorem 10 in Appendix A.2) from quasi-Monte Carlo theory. This produces a bound on the prediction error in terms of the star-discrepancy (see Definition 2 in Appendix A.2) and the Hardy-Krause variation. The star-discrepancy can be controlled by known bounds on the star-discrepancy of random points [82]. We bound the Hardy-Krause variation by explicitly computing the mixed derivatives of the local models $f_{P}^{\\theta_{P}}$ and the ground state properties $\\operatorname{tr}(O\\rho(x))$ . In particular, we bound the latter using tools from the spectral flow formalism [59, 60, 61], and this is where the assumption that the mixed first order derivatives of the Hamiltonian are bounded is needed. Putting these steps together, we arrive at the rigorous guarantee in Theorem 5. ", "page_idx": 8}, {"type": "text", "text": "4 Numerical experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct numerical experiments to observe the performance of our model in practice. The results demonstrate that our assumptions in Theorem 5 are often satisfied in practice and that our deep learning algorithm outperforms the previous best-known method [2]. Moreover, we generate and utilize significantly more training data than in prior works [1, 2]. The code can be found at https://github.com/marcwannerchalmers/learning_ground_states.git. ", "page_idx": 8}, {"type": "text", "text": "We consider the classical neural network model discussed in the previous section and defined formally in Definition 6. For each of the local models $f_{P}^{\\theta_{P}}$ , we use fully connected deep neural networks with five hidden layers of width 200. We train the model with the AdamW optimization algorithm [83]. We measure the training error and prediction error via the root-mean-square error (RMSE). The model is discussed further in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "As in [2], we consider the two-dimensional antiferromagnetic random Heisenberg model on between 20 to 45 qubits and predict two-body correlation functions. The corresponding Hamiltonian is ", "page_idx": 8}, {"type": "equation", "text": "$$\nH=\\sum_{\\langle i j\\rangle}J_{i j}(X_{i}X_{j}+Y_{i}Y_{j}+Z_{i}Z_{j}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\langle i j\\rangle$ denotes all pairs of neighboring sites on the lattice. The coupling terms $J_{i j}$ correspond to the parameters $x$ of the Hamiltonian and are sampled uniformly from $[0,2]$ . ", "page_idx": 8}, {"type": "text", "text": "We generate training data similarly to [1, 2], using the density-matrix renormalization group (DMRG) [9] based on matrix-product-states (MPS) [84]. To assess the performance of our model, we consider both uniformly randomly distributed $J_{i j}$ and coupling parameters, which are distributed as a Sobol sequence. It is easy to see that the distributions and $H$ satisfy the requirements of Theorem 5. ", "page_idx": 8}, {"type": "text", "text": "In Figure 2 (Left), we see that our deep learning algorithm consistently outperforms the previous best-known ML algorithm from [2], achieving approximately half the prediction error on the same training data. The prediction error also exhibits a constant scaling with respect to system size, agreeing with our rigorous guarantee in Theorem 5. Another noteworthy observation is that the ML algorithm\u2019s performance on LDS is nearly equivalent to its performance on uniformly random points. We discuss a potential reason for this in Appendix D. ", "page_idx": 8}, {"type": "image", "img_path": "ybLXvqJyQA/tmp/8d5bb85068cc4bcc963f20a938579dcc9a304ab59d55efe84125a5ef0892ea34.jpg", "img_caption": ["Figure 2: Numerical experiments. (Left) Comparison with previous methods. Each point indicates the prediction error (RMSE) of our deep learning model or the regression model of [2], fixing the training set size $N=3686$ and the size of the local neighorbood $\\delta_{1}=0$ (Equation (2.3)). We train both algorithms on either LDS or uniformly random points. (Center) Scaling with training size. Each point indicates the prediction error of our deep learning model given LDS training data for various $\\delta_{1}$ and training data sizes. (Right) Neural network weights and training error. Blue points correspond to the training error of the neural network model. Red points correspond to the $\\ell_{1}$ norm of parameters in the last layer or the largest absolute value of the parameters of the neural network, fixing $N=3686$ and $\\delta_{1}=1$ . This shows that the assumptions in Theorem 5 are achieved in practice. The shaded areas denote the 1-sigma error bars across the assessed ground state properties. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2 (Center) illustrates the prediction error scaling with respect to the training set size for various choices of $\\delta_{1}$ (size of the local neighborhood from Equation (2.3)). For $\\delta_{1}=0$ , the error arising from approximating the ground state property via local functions dominates. For $\\delta_{1}>0$ , we observe a smaller local approximation error and thus achieve a smaller prediction error for sufficiently large training sets. This is consistent with our theoretical results. ", "page_idx": 9}, {"type": "text", "text": "Finally, Figure 2 (Right) illustrates that our assumptions in Theorem 5 are mild in practice. Namely, the blue points show that a small training error can be achieved. The red points also demonstrate that the $\\ell_{1}$ -norm of the parameters in the last layer and the largest absolute value of the parmeters in the trained neural network remain small. In particular, in Figure 2, the weights exhibit a scaling independent of system size $n$ . Hence, we find that the assumptions needed to guarantee the prediction error bound in Theorem 5, namely that the training objective is small and the weights of the neural network are small and independent of system size, are fulfilled in our numerical experiments. We provide further details of the numerical experiments in Appendix D. ", "page_idx": 9}, {"type": "text", "text": "5 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have shown that we can construct ML models for predicting ground state properties that require only a constant number of training samples, for a fixed prediction error. Specifically, we showed that a simple modification to the linear regression model in [2] only requires $2^{\\mathsf{p o l y l o g}(\\epsilon^{-1})}$ samples in order to achieve a prediction error of $\\epsilon$ , provided that we know a decomposition of the observable of interest in terms of Pauli operators. We then showed that a neural network model which is trained on $2^{\\mathsf{p o l y l o g}(\\epsilon^{-1})}$ training samples and which achieves $O(\\epsilon)$ training error on these samples will also have a prediction error of at most $\\epsilon$ . In this case, knowledge of the observable $O$ is no longer required. ", "page_idx": 9}, {"type": "text", "text": "Our work leaves open several avenues for future exploration. First, it would be desirable to understand the conditions under which we can prove convergence for the training error. For instance, could the model be changed so as to use a convex objective, thereby avoiding the issues associated with finding a global optimum in a non-convex landscape? Following [49, 50], we would also like to know whether the results obtained for neural networks can be extended to thermal states or Lindbladian phases of matter. Finally, for both results it would be desirable to improve the scaling with respect to the error $\\epsilon$ . Currently, the models have quasipolynomial scaling in $1/\\epsilon$ and the only case in which we know how to achieve poly $(1/\\epsilon)$ scaling is when the number of parameters, $m$ , is constant (as in [51]). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "MW and DD are supported by SSF (Swedish Foundation for Strategic Research), grant number FUS21-0063. LL is supported by a Marshall Scholarship. CB is partially supported by a grant from DIA-COE. AG is supported by the Knut and Alice Wallenberg Foundation through the Wallenberg Centre for Quantum Technology (WACQT). This work was done in part while a subset of the authors were visiting the Simons Institute for the Theory of Computing. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Hsin-Yuan Huang, Richard Kueng, Giacomo Torlai, Victor V Albert, and John Preskill. Provably efficient machine learning for quantum many-body problems. Science, 377(6613):eabk3333, 2022. [2] Laura Lewis, Hsin-Yuan Huang, Viet T Tran, Sebastian Lehner, Richard Kueng, and John Preskill. Improved machine learning algorithm for predicting ground state properties. Nature Communications, 15(1):895, 2024. [3] P. Hohenberg and W. Kohn. Inhomogeneous electron gas. Phys. Rev., 136:B864\u2013B871, 1964. [4] W. Kohn. Nobel lecture: Electronic structure of matter\u2014wave functions and density functionals. Rev. Mod. Phys., 71:1253\u20131266, 1999. [5] Anders W. Sandvik. Stochastic series expansion method with operator-loop update. Phys. Rev. B, 59:R14157\u2013R14160, 1999. [6] David Ceperley and Berni Alder. Quantum Monte Carlo. Science, 231(4738):555\u2013560, 1986. [7] Federico Becca and Sandro Sorella. Quantum Monte Carlo Approaches for Correlated Systems. Cambridge University Press, 2017. [8] James Gubernatis, Naoki Kawashima, and Philipp Werner. Quantum Monte Carlo Methods. Cambridge University Press, 2016. [9] Steven R White. Density matrix formulation for quantum renormalization groups. Physical review letters, 69(19):2863, 1992.   \n[10] Steven R White. Density-matrix algorithms for quantum renormalization groups. Phys. Rev. B, 48(14):10345, 1993.   \n[11] Guifr\u00e9 Vidal. Class of quantum many-body states that can be efficiently simulated. Physical review letters, 101(11):110501, 2008.   \n[12] Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J Love, Al\u00e1n Aspuru-Guzik, and Jeremy L O\u2019brien. A variational eigenvalue solver on a photonic quantum processor. Nat. Commun., 5:4213, 2014.   \n[13] J Ignacio Cirac, David Perez-Garcia, Norbert Schuch, and Frank Verstraete. Matrix product states and projected entangled pair states: Concepts, symmetries, theorems. Reviews of Modern Physics, 93(4):045003, 2021.   \n[14] Toby S Cubitt. Dissipative ground state preparation and the dissipative quantum eigensolver. arXiv preprint arXiv:2303.11962, 2023.   \n[15] Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, and Lenka Zdeborov\u00e1. Machine learning and the physical sciences. Rev. Mod. Phys., 91:045002, 2019.   \n[16] Juan Carrasquilla. Machine learning for quantum matter. Adv. Phys.: X, 5(1):1797528, 2020.   \n[17] Dong-Ling Deng, Xiaopeng Li, and S. Das Sarma. Machine learning topological states. Phys. Rev. B, 96:195145, 2017.   \n[18] Juan Carrasquilla and Roger G. Melko. Machine learning phases of matter. Nat. Phys., 13:431, 2017.   \n[19] Giuseppe Carleo and Matthias Troyer. Solving the quantum many-body problem with artificial neural networks. Science, 355(6325):602\u2013606, 2017.   \n[20] Giacomo Torlai and Roger G. Melko. Learning thermodynamics with Boltzmann machines. Physical Review B, 94(16):165134, 2016.   \n[21] Yusuke Nomura, Andrew S. Darmawan, Youhei Yamaji, and Masatoshi Imada. Restricted boltzmann machine learning for solving strongly correlated quantum systems. Phys. Rev. B, 96:205152, 2017.   \n[22] Evert P. L. van Nieuwenburg, Ye-Hua Liu, and Sebastian D. Huber. Learning phase transitions by confusion. Nat. Phys., 13:435, 2017.   \n[23] Lei Wang. Discovering phase transitions with unsupervised learning. Phys. Rev. B, 94:195105, 2016.   \n[24] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.   \n[25] Giacomo Torlai, Guglielmo Mazzola, Juan Carrasquilla, Matthias Troyer, Roger Melko, and Giuseppe Carleo. Neural-network quantum state tomography. Nat. Phys., 14(5):447\u2013450, 2018.   \n[26] Rodrigo A Vargas-Hern\u00e1ndez, John Sous, Mona Berciu, and Roman V Krems. Extrapolating quantum observables with machine learning: inferring multiple phase transitions from properties of a single phase. Physical review letters, 121(25):255702, 2018.   \n[27] KT Sch\u00fctt, Michael Gastegger, Alexandre Tkatchenko, K-R M\u00fcller, and Reinhard J Maurer. Unifying machine learning and quantum chemistry with a deep neural network for molecular wavefunctions. Nat. Commun., 10(1):1\u201310, 2019.   \n[28] Ivan Glasser, Nicola Pancotti, Moritz August, Ivan D. Rodriguez, and J. Ignacio Cirac. Neuralnetwork quantum states, string-bond states, and chiral topological states. Phys. Rev. X, 8:011006, 2018.   \n[29] Matthias C Caro, Hsin-Yuan Huang, Nicholas Ezzell, Joe Gibbs, Andrew T Sornborger, Lukasz Cincio, Patrick J Coles, and Zo\u00eb Holmes. Out-of-distribution generalization for learning quantum dynamics. arXiv preprint arXiv:2204.10268, 2022.   \n[30] Joaquin F Rodriguez-Nieva and Mathias S Scheurer. Identifying topological order through unsupervised machine learning. Nat. Phys., 15(8):790\u2013795, 2019.   \n[31] Zhuoran Qiao, Matthew Welborn, Animashree Anandkumar, Frederick R Manby, and Thomas F Miller III. Orbnet: Deep learning for quantum chemistry using symmetry-adapted atomic-orbital features. J. Chem. Phys., 153(12):124111, 2020.   \n[32] Kenny Choo, Antonio Mezzacapo, and Giuseppe Carleo. Fermionic neural-network states for ab-initio electronic structure. Nat. Commun., 11(1):2368, May 2020.   \n[33] Hiroki Kawai and Yuya O Nakagawa. Predicting excited states from ground state wavefunction by supervised quantum machine learning. Machine Learning: Science and Technology, 1(4):045027, 2020.   \n[34] Javier Robledo Moreno, Giuseppe Carleo, and Antoine Georges. Deep learning the hohenbergkohn maps of density functional theory. Physical Review Letters, 125(7):076402, 2020.   \n[35] Korbinian Kottmann, Philippe Corboz, Maciej Lewenstein, and Antonio Ac\u00edn. Unsupervised mapping of phase diagrams of 2d systems from infinite projected entangled-pair states via deep anomaly detection. SciPost Physics, 11(2):025, 2021.   \n[36] Haoxiang Wang, Maurice Weber, Josh Izaac, and Cedric Yen-Yu Lin. Predicting properties of quantum systems with conditional generative models. arXiv preprint arXiv:2211.16943, 2022.   \n[37] Viet T Tran, Laura Lewis, Hsin-Yuan Huang, Johannes Kofler, Richard Kueng, Sepp Hochreiter, and Sebastian Lehner. Using shadows to learn ground state properties of quantum hamiltonians. Machine Learning and Physical Sciences Workshop at the 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.   \n[38] Kyle Mills, Michael Spanner, and Isaac Tamblyn. Deep learning and the schr\u00f6dinger equation. Phys. Rev. A, 96:042113, Oct 2017.   \n[39] N Saraceni, S Cantori, and S Pilati. Scalable neural networks for the efficient learning of disordered quantum systems. Physical Review E, 102(3):033301, 2020.   \n[40] Cancan Huang and Brenda M Rubenstein. Machine learning diffusion monte carlo forces. The Journal of Physical Chemistry A, 127(1):339\u2013355, 2022.   \n[41] Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert M\u00fcller, and O Anatole Von Lilienfeld. Fast and accurate modeling of molecular atomization energies with machine learning. Physical review letters, 108(5):058301, 2012.   \n[42] Felix A Faber, Luke Hutchison, Bing Huang, Justin Gilmer, Samuel S Schoenholz, George E Dahl, Oriol Vinyals, Steven Kearnes, Patrick F Riley, and O Anatole Von Lilienfeld. Prediction errors of molecular machine learning models lower than hybrid dft error. Journal of chemical theory and computation, 13(11):5255\u20135264, 2017.   \n[43] Benno S Rem, Niklas K\u00e4ming, Matthias Tarnowski, Luca Asteria, Nick Fl\u00e4schner, Christoph Becker, Klaus Sengstock, and Christof Weitenberg. Identifying quantum phase transitions using artificial neural networks on experimental data. Nature Physics, 15(9):917\u2013920, 2019.   \n[44] Xiao-Yu Dong, Frank Pollmann, Xue-Feng Zhang, et al. Machine learning of quantum phase transitions. Physical Review B, 99(12):121104, 2019.   \n[45] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine learning. Nature, 549(7671):195\u2013202, 2017.   \n[46] Luuk Coopmans and Marcello Benedetti. On the sample complexity of quantum boltzmann machine learning. arXiv preprint arXiv:2306.14969, 2023.   \n[47] Julia Kempe, Alexei Kitaev, and Oded Regev. The complexity of the local hamiltonian problem. Siam journal on computing, 35(5):1070\u20131097, 2006.   \n[48] Cambyse Rouz\u00e9, Daniel Stilck Fran\u00e7a, Emilio Onorati, and James D. Watson. Efficient learning of ground and thermal states within phases of matter. Nature Communications, 15(1), September 2024.   \n[49] Emilio Onorati, Cambyse Rouz\u00e9, Daniel Stilck Fran\u00e7a, and James D Watson. Provably efficient learning of phases of matter via dissipative evolutions. arXiv preprint arXiv:2311.07506, 2023.   \n[50] Andrea Coser and David P\u00e9rez-Garc\u00eda. Classification of phases for mixed states via fast dissipative evolution. Quantum, 3:174, 2019.   \n[51] Yanming Che, Clemens Gneiting, and Franco Nori. Exponentially improved efficient machine learning for quantum many-body states with provable guarantees. arXiv preprint arXiv:2304.04353, 2023.   \n[52] Hsin-Yuan Huang, Richard Kueng, and John Preskill. Predicting many properties of a quantum system from very few measurements. Nat. Phys., 16:1050\u2013\u20131057, 2020.   \n[53] Tim De Ryck, Samuel Lanthaler, and Siddhartha Mishra. On the approximation of functions by tanh neural networks. Neural Networks, 143:732\u2013750, November 2021.   \n[54] Siddhartha Mishra and T. Konstantin Rusch. Enhancing accuracy of deep learning algorithms by training with low-discrepancy sequences. SIAM Journal on Numerical Analysis, 59(3):1811\u2013 1834, 2021.   \n[55] Stanislaw K Zaremba. The mathematical basis of monte carlo and quasi-monte carlo methods. SIAM review, 10(3):303\u2013314, 1968. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[56] Russel E Cafilsch. Monte carlo and quasi-monte carlo methods. Acta numerica, 7:1\u201349, 1998. ", "page_idx": 13}, {"type": "text", "text": "[57] Harald Niederreiter. Random number generation and quasi-Monte Carlo methods. SIAM, 1992.   \n[58] Pierre L\u2019Ecuyer and Christiane Lemieux. Recent advances in randomized quasi-monte carlo methods. Modeling uncertainty: An examination of stochastic theory, methods, and applications, pages 419\u2013474, 2002.   \n[59] Sven Bachmann, Spyridon Michalakis, Bruno Nachtergaele, and Robert Sims. Automorphic equivalence within gapped phases of quantum lattice systems. Commun. Math. Phys., 309(3):835\u2013871, 2012.   \n[60] Matthew B Hastings and Xiao-Gang Wen. Quasiadiabatic continuation of quantum states: The stability of topological ground-state degeneracy and emergent gauge invariance. Phys. Rev. B, 72(4):045141, 2005.   \n[61] Tobias J Osborne. Simulating adiabatic evolution of gapped spin systems. Phys. Rev. A, 75(3):032321, 2007.   \n[62] Fadil Santosa and William W. Symes. Linear inversion of band-limited reflection seismograms. SIAM Journal on Scientific and Statistical Computing, 7(4):1307\u20131330, 1986.   \n[63] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267\u2013288, 1996.   \n[64] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. The MIT Press, 2018.   \n[65] Craig Saunders, Alexander Gammerman, and Volodya Vovk. Ridge regression learning algorithm in dual variables. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 515\u2013521, 1998.   \n[66] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.   \n[67] Hsin-Yuan Huang, Sitan Chen, and John Preskill. Learning to predict arbitrary quantum processes. PRX Quantum, 4(4):040337, 2023.   \n[68] Andreas Elben, Richard Kueng, Hsin-Yuan Huang, Rick van Bijnen, Christian Kokail, Marcello Dalmonte, Pasquale Calabrese, Barbara Kraus, John Preskill, Peter Zoller, and Beno\u00eet Vermersch. Mixed-state entanglement from local randomized measurements. Phys. Rev. Lett., 125:200501, 2020.   \n[69] Andreas Elben, Steven T Flammia, Hsin-Yuan Huang, Richard Kueng, John Preskill, Beno\u00eet Vermersch, and Peter Zoller. The randomized measurement toolbox. arXiv preprint arXiv:2203.11374, 2022.   \n[70] Kianna Wan, William J Huggins, Joonho Lee, and Ryan Babbush. Matchgate shadows for fermionic quantum simulation. arXiv preprint arXiv:2207.13723, 2022.   \n[71] Kaifeng Bu, Dax Enshan Koh, Roy J Garcia, and Arthur Jaffe. Classical shadows with pauliinvariant unitary ensembles. arXiv preprint arXiv:2202.03272, 2022.   \n[72] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.   \n[73] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[74] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.   \n[75] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pages 1675\u20131685. PMLR, 2019.   \n[76] Ilya M Sobol. Uniformly distributed sequences with an additional uniform property. USSR Computational mathematics and mathematical physics, 16(5):236\u2013242, 1976.   \n[77] Il\u2019ya Meerovich Sobol\u2019. On the distribution of points in a cube and the approximate evaluation of integrals. Zhurnal Vychislitel\u2019noi Matematiki i Matematicheskoi Fiziki, 7(4):784\u2013802, 1967.   \n[78] Harald Niederreiter. Point sets and sequences with small discrepancy. Monatshefte f\u00fcr Mathematik, 104:273\u2013337, 1987.   \n[79] Harald Niederreiter. Low-discrepancy and low-dispersion sequences. Journal of number theory, 30(1):51\u201370, 1988.   \n[80] John H Halton. On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals. Numerische Mathematik, 2:84\u201390, 1960.   \n[81] Art B Owen. Monte carlo variance of scrambled net quadrature. SIAM Journal on Numerical Analysis, 34(5):1884\u20131910, 1997.   \n[82] Christoph Aistleitner and Markus Hofer. Probabilistic discrepancy bound for monte carlo point sets, 2012.   \n[83] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.   \n[84] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Mach. Learn., 20(3):273\u2013297, 1995.   \n[85] Kjetil O Lye, Siddhartha Mishra, and Deep Ray. Deep learning observables in computational fluid dynamics. Journal of Computational Physics, 410:109339, 2020.   \n[86] Art B Owen. Multidimensional variation for quasi-monte carlo. In Contemporary Multivariate Analysis And Design Of Experiments: In Celebration of Professor Kai-Tai Fang\u2019s 65th Birthday, pages 49\u201374. World Scientific, 2005.   \n[87] Christoph Aistleitner and Josef Dick. Functions of bounded variation, signed measures, and a general koksma-hlawka inequality, 2014.   \n[88] E. Hlawka and R. M\u00fcck. \u00dcber eine transformation von gleichverteilten folgen ii. Computing, 9(2):127\u2013138, Jun 1972.   \n[89] Mohammad Mahdi Bejani and Mehdi Ghatee. A systematic review on overfitting control in shallow and deep neural networks. Artificial Intelligence Review, pages 1\u201348, 2021.   \n[90] George Casella and Roger L Berger. Statistical lnference. Duxbury press, 2002.   \n[91] Murray Rosenblatt. Remarks on a multivariate transformation. The Annals of Mathematical Statistics, 23(3):470\u2013472, 1952.   \n[92] Mei Zhang, Aijun Zhang, and Yongdao Zhou. Construction of Uniform Designs on Arbitrary Domains by Inverse Rosenblatt Transformation, pages 111\u2013126. 05 2020.   \n[93] 2. Quasi-Monte Carlo Methods for Numerical Integration, pages 13\u201322.   \n[94] Ohad Shamir. A variant of azuma\u2019s inequality for martingales with subgaussian tails. arXiv preprint arXiv:1110.2392, 2011.   \n[95] Howard E Haber. Notes on the matrix exponential and logarithm. Santa Cruz Institute for Particle Physics, University of California: Santa Cruz, CA, USA, 2018.   \n[96] Steven R. White. Density matrix formulation for quantum renormalization groups. Phys. Rev. Lett., 69:2863\u20132866, 1992.   \n[97] De Huang, Jonathan Niles-Weed, Joel A Tropp, and Rachel Ward. Matrix concentration for products. arXiv preprint arXiv:2003.05437, 2020. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A Preliminaries 16 ", "page_idx": 15}, {"type": "text", "text": "A.1 Review of previous algorithm and proof 16   \nA.1.1 ML Algorithm 17   \nA.1.2 Proof Ideas . 18   \nA.2 Deep learning with low-discrepancy sequences 20 ", "page_idx": 15}, {"type": "text", "text": "B Constant sample complexity 23 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Training error bound 24   \nB.2 Prediction error bound 25   \nB.3 Computational time for training and prediction 27 ", "page_idx": 15}, {"type": "text", "text": "C Rigorous guarantees for neural networks 27 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Approximation of ground state properties by neural networks . . 30   \nC.2 Prediction error bound 33   \nC.3 Prediction on general distributions 39   \nC.4 Bound on the mixed derivatives . . . 49 ", "page_idx": 15}, {"type": "text", "text": "D Details of numerical experiments 53 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 Experimental setup 53   \nD.2 Additional experiments and discussion 54   \nD.3 Experiments with non-geometrically-local Hamiltonians 56 ", "page_idx": 15}, {"type": "text", "text": "These appendices provide the technical details of the ideas discussed in the main text. In Appendix A, we review several important concepts for our proofs such as the algorithm and rigorous guarantee from [2] in Appendix A.1 and background on classical deep learning techniques in Appendix A.2. In Appendix B, we build on [2] to obtain a sample complexity upper bound for predicting ground state properties independent of system size. In Appendix C, we prove our guarantee for predicting ground state properties using neural networks. ", "page_idx": 15}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Review of previous algorithm and proof ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we review the previous algorithm from [2] along with intermediate results we use throughout our proofs. For full details, we refer the reader to [2]. Throughout this section, let $1/e>\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}>0$ . One can think of $\\epsilon_{1}$ as the approximation error caused by the hypothesis of our ML algorithm not exactly capturing the ground state property; $\\epsilon_{2}$ represents the noise in the training data; $\\epsilon_{3}$ corresponds to the generalization error. ", "page_idx": 15}, {"type": "text", "text": "Recall that we consider a family of $n$ -qubit Hamiltonians $H(x)$ smoothly parameterized by an $m$ -dimensional vector $x\\,\\in\\,[-1,1]^{m}$ that satisfies the following assumptions, which we restate from [2]. ", "page_idx": 15}, {"type": "text", "text": "(a) Physical system: We consider $n$ finite-dimensional quantum systems that are arranged at locations, or sites, in a $d$ -dimensional space, e.g., a spin chain $[d=1]$ ), a square lattice $[d=2]$ ), or a cubic lattice $\\boldsymbol{d}=3$ ). Unless specified otherwise, our big- $\\mathcal{O},\\Omega,\\Theta$ notation is with respect to the thermodynamic limit $n\\to\\infty$ .   \n(b) Hamiltonian: $H(x)$ decomposes into a sum of geometrically local terms $H(x)\\ =$ $\\begin{array}{r}{\\sum_{j=1}^{L}h_{j}(\\vec{x}_{j})}\\end{array}$ , each of which only acts on an $O(1)$ number of sites in a ball of $\\mathcal{O}(1)$ radius. Here, $\\vec{x}_{j}\\in\\mathbb{R}^{q},q=\\mathcal{O}(1)$ and $x$ is the concatenation of $L$ vectors $\\vec{x}_{1},\\ldots,\\vec{x}_{L}$ with dimension $m=L q=\\mathcal{O}(n)$ . Individual terms $h_{j}(\\vec{x}_{j})$ obey $\\|h_{j}(\\vec{x}_{j})\\|_{\\infty}\\leq1$ and also have bounded directional derivative: $\\|\\partial h_{j}/\\partial\\hat{u}\\|_{\\infty}\\leq1$ , where $\\hat{u}$ is a unit vector in parameter space.   \n(c) Ground-state subspace: We consider the ground state $\\rho(x)$ for the Hamiltonian $H(x)$ to be defined as $\\rho(x)=\\mathrm{lim}_{\\beta\\to\\infty}\\,e^{-\\beta H(x)}/\\,\\mathrm{tr}\\big(e^{-\\beta H(x)}\\big)$ . This is equivalent to a uniform mixture over the eigenspace of $H(x)$ with the minimum eigenvalue.   \n(d) Observable: $O$ can be written as a sum of few-body observables $O=\\textstyle\\sum_{j}O_{j}$ , where each $O_{j}$ only acts on an $\\mathcal{O}(1)$ number of sites. Hence, we can also write $\\begin{array}{r}{O=\\sum_{P\\in S^{(\\mathrm{geo})}}\\alpha_{P}P}\\end{array}$ , where $P\\in\\{I,X,Y,Z\\}^{\\otimes n}$ and $S^{(\\mathrm{geo})}$ is the set of all geometrically local Pauli observables. We focus on $O$ given as a sum of geometrically local observables $\\textstyle\\sum_{j}O_{j}$ , where each $O_{j}$ only acts on an ${\\mathcal O}(1)$ number of sites in a ball of $\\mathcal{O}(1)$ radius. Moreover, $O$ has $\\left\\lVert O\\right\\rVert_{\\infty}=O(1)$ . ", "page_idx": 16}, {"type": "text", "text": "We also assume that the spectral gap of $H(x)$ is bounded from below by some constant $\\gamma$ for all choices of parameters $x\\in[-1,1]^{m}$ . ", "page_idx": 16}, {"type": "text", "text": "The ML algorithm is given a training data set $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ , where $x_{\\ell}$ is sampled from some distribution $\\mathcal{D}$ over the parameter space $[-1,1]^{m}$ and $y_{\\ell}$ approximates the ground state property: $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\,\\le\\,\\epsilon_{2}$ . The goal is to learn some function $h^{*}(x)$ that achieves a low average prediction error ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}|h^{*}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "A.1.1 ML Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The ML algorithm proposed in [2] requires several geometric definitions. We use $S^{(\\mathrm{geo})}$ to denote the set of all geometrically local Pauli observables throughout. ", "page_idx": 16}, {"type": "text", "text": "Let $\\delta_{1},\\delta_{2},B\\,>\\,0$ be efficiently-computable hyperparameters that we define later. Then, define the set $I_{P}$ of coordinates $c$ that parameterize some local term $h_{j(c)}$ that is close to a Pauli $P\\ \\in$ $\\{I,X,Y,Z\\}^{\\otimes n}$ . Here, the distance between two observables $d_{\\mathrm{obs}}$ is defined as the minimum distance between the qubits that the observables act on, where the distance between qubits is given by the geometry of the system, which we assume to be known. Formally, we define the set of local coordinates as ", "page_idx": 16}, {"type": "equation", "text": "$$\nI_{P}\\triangleq\\{c\\in\\{1,\\dots,m\\}:d_{\\mathrm{obs}}(h_{j(c)},P)\\leq\\delta_{1}\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $h_{j(c)}$ is the local term in the Hamiltonian $H(x)$ whose parameters $\\vec{x}_{j(c)}$ include the variable $x_{c}$ . The intuition behind this set of coordinates is that it indexes the parameters $x_{c}$ that influence the ground state property $\\operatorname{tr}(P\\rho(x))$ corresponding to this Pauli $P$ . Using this intuition, because these parameters $x_{c}$ for $c\\in I_{P}$ matter most for the property we are trying to learn (as [2] proves and we give the ideas for later), then we can define a new effective parameter space in which all other parameters are set to zero. Moreover, parameters $x_{c}$ for $c\\in I_{P}$ can be discretized to lie on a lattice. This gives the following set $X_{P}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nX_{P}\\triangleq\\left\\{^{x\\in[-1,1]^{m}:\\mathrm{if}\\,c\\,\\notin\\,I_{P},\\,x_{c}=0}\\right.\\quad}\\\\ {\\left.\\quad\\qquad\\qquad\\mathrm{if}\\,c\\in I_{P},\\,x_{c}\\in\\{0,\\pm\\delta_{2},\\pm2\\delta_{2},\\ldots,\\pm1\\}\\quad\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We can also define a set $T_{x,P}$ for each vector $x\\in X_{P}$ which is the set of parameters $x^{\\prime}$ that are close to $x$ for coordinates in $I_{P}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nT_{x,P}\\triangleq\\left\\{x^{\\prime}\\in[-1,1]^{m}:-\\frac{\\delta_{2}}{2}<x_{c}-x_{c}^{\\prime}\\leq\\frac{\\delta_{2}}{2},\\;\\forall c\\in I_{P}\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With these definitions in place, we set the hyperparameters as follows. Define $\\delta_{1}$ as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\delta_{1}\\triangleq\\operatorname*{max}\\left(C_{\\mathrm{max}}\\log^{2}(2C/\\epsilon_{1}),C_{4},C_{5},\\frac{\\operatorname*{max}(5900,\\alpha,7(d+11),\\theta)}{b}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $b,C_{\\operatorname*{max}},C_{4},C_{5},\\alpha,\\theta,C$ are all constants. We refer to the supplementary information of [2] for a full description of these constants. Moreover, $\\delta_{2}$ is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{2}\\triangleq\\frac{1}{\\left\\lceil\\frac{2\\sqrt{C^{\\prime}|I_{P}|}}{\\epsilon_{1}}\\right\\rceil},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C^{\\prime}$ is a constant. Finally, we define an additional hyperparameter $B>0$ as ", "page_idx": 17}, {"type": "equation", "text": "$$\nB\\triangleq2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The ML algorithm from [2] utilizes these objects to encode the geometric locality of the system. The algorithm consists of two steps. First, it maps the parameter space $[-1,1]^{m}$ to a high dimensional space $\\mathbb{R}^{m_{\\phi}}$ for ", "page_idx": 17}, {"type": "equation", "text": "$$\nm_{\\phi}\\triangleq\\sum_{P\\in S^{\\mathrm{(geo)}}}|X_{P}|=\\mathcal{O}(n)2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "via a nonlinear feature map $\\phi$ . Second, it runs $\\ell_{1}$ -regularized linear regression (LASSO) over the feature space. ", "page_idx": 17}, {"type": "text", "text": "This first step encodes the geometry of the problem. In particular, the feature map is defined as follows, where each coordinate of $\\phi(x)$ is indexed by $x^{\\prime}\\in X_{P}$ and $P\\in S^{(\\mathrm{geo})}$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\phi(x)_{x^{\\prime},P}\\triangleq\\mathbb{1}[x\\in T_{x^{\\prime},P}].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In this way, the feature map $\\phi(x)$ identifies the nearest lattice point to $x$ . The idea is that one can approximate the ground state property well by only approximating it at these representative points and summing up. We make this intuition rigorous in the following section. ", "page_idx": 17}, {"type": "text", "text": "Following the feature mapping, our ML algorithm uses LASSO [62, 63, 64] to learn functions of the form $\\{h(x)=\\mathbf{w}\\cdot\\phi(x):\\|w\\|_{1}\\leq B\\}$ . In particular, for a chosen hyperparameter $B>0$ , LASSO finds a coefficient vector $\\mathbf{w}^{*}$ that solves the following optimization problem minimizing the training error subject to the constraint that $\\|\\boldsymbol{w}\\|_{1}\\le B$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{m_{\\phi}}}\\frac{1}{N}\\sum_{\\ell=1}^{N}|\\mathbf{w}\\cdot\\phi(x_{\\ell})-y_{\\ell}|^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We denote the learned function by $h^{*}(x)=\\mathbf{w}^{*}\\cdot\\phi(x)$ . Note that the learned function does not need to achieve the minimum training error, but can be some amount $\\epsilon_{3}/2$ above it. For our purposes, we set B = 2O(polylog(1/\u03f51)). ", "page_idx": 17}, {"type": "text", "text": "This algorithm obtains the following rigorous guarantee. ", "page_idx": 17}, {"type": "text", "text": "Theorem 7 (Theorem 5 in [2]). Let $1/e~>~\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}~>~0$ and $\\delta\\ >\\ 0$ . Given training data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 17}, {"type": "equation", "text": "$$\nN=\\log(n/\\delta)2^{\\mathcal{O}(\\log(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $x_{\\ell}$ is sampled from $\\mathcal{D}$ and y\u2113is an estimator of $\\operatorname{tr}(O\\rho(x_{\\ell}))$ such that $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\le\\epsilon_{2}$ , the ML algorithm can produce $h^{*}(x)$ that achieves prediction error ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|h^{*}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability at least $1\\ -\\ \\delta$ . The training time for constructing the hypothesis function $h$ and the prediction time for computing $h^{*}(x)$ are upper bounded by $\\begin{array}{r l}{\\mathcal{O}(n N)}&{{}=}\\end{array}$ n log(n/\u03b4)2O(log(1/\u03f53)+polylog(1/\u03f51)). ", "page_idx": 17}, {"type": "text", "text": "A.1.2 Proof Ideas ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "This rigorous guarantee is proven by first showing that the training error ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{R}(h)=\\operatorname*{min}_{\\mathbf{w}}\\frac{1}{N}\\sum_{\\ell=1}^{N}|h(x_{\\ell})-y_{\\ell}|^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is small. ", "page_idx": 17}, {"type": "text", "text": "Lemma 1 (Lemma 15 in [2]). The function ", "page_idx": 18}, {"type": "equation", "text": "$$\ng(x)=\\sum_{P\\in S^{(\\mathrm{geo})}}\\sum_{x^{\\prime}\\in X_{P}}f_{P}(x^{\\prime})\\mathbb{1}[x\\in T_{x^{\\prime},P}]=\\mathbf{w}^{\\prime}\\cdot\\phi(x),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "achieves training error ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{R}(g)\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The proof of this consists of three different steps. First, one can show that $\\mathrm{tr}(O\\rho(x))$ can be approximated by a sum of smooth local functions, denoted as $\\begin{array}{r}{f(x)\\,=\\,\\sum_{P\\in S^{(\\mathrm{geo})}}\\,\\dot{f}_{P}(\\dot{x})}\\end{array}$ , where $f_{P}(x)=\\alpha_{P}\\operatorname{tr}(P\\rho(\\chi_{P}(x)))$ for $\\begin{array}{r}{O=\\sum_{P\\in\\{I,X,Y,Z\\}\\otimes n}\\alpha_{P}P}\\end{array}$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\chi_{P}(x)_{c}=\\left\\{\\!\\!\\begin{array}{l l}{x_{c},}&{c\\in I_{P}}\\\\ {0}&{c\\notin I_{P}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $c\\in\\{1,\\ldots,m\\}$ . In other words, parameters that parameterize local terms $h_{j}$ far away a Pauli $P$ ( $\\scriptstyle x_{c}$ for $c\\notin I_{P}{\\mathrm{,}}$ ) do not contribute much to the ground state property, and thus we can simply set them to zero. Formally, this approximation is given in the following lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 2 (Corollary 2 in [2]). Consider a class of local Hamiltonians $\\{H(x):x\\in[-1,1]^{m}\\}$ and an observable $\\textstyle O=\\sum_{P\\in\\{I,X,Y,Z\\}\\otimes n}\\alpha_{P}P$ satisfying assumptions (a)- $(d)$ . There exists a constant $C>0$ such that for any $1/e>\\epsilon_{1}>0,$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\operatorname{tr}(O\\rho(x))-f(x)|\\leq C\\epsilon_{1}\\left(\\sum_{P\\in S^{\\mathrm{(geo)}}}|\\alpha_{P}|\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{f(x)=\\sum_{P\\in S^{(\\mathrm{geo})}}f_{P}(x)}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "Second, one can also show that this sum of local functions $\\begin{array}{r}{f(x)=\\sum_{P\\in S^{(\\mathrm{geo})}}f_{P}(x)}\\end{array}$ can in turn be approximated by a linear function over the feature space $g(x)=\\mathbf{w}^{\\prime}\\cdot\\phi(x)$ , where $\\mathbf{w}^{\\prime}$ is a vector with entries indexed by $P\\in S^{(\\mathrm{geo})}$ and $x^{\\prime}\\in X_{P}$ given by $\\mathbf{w}_{x^{\\prime},P}^{\\prime}=f_{P}(x^{\\prime})$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 3 (Corollary 3 in [2]). For $g(x)=\\mathbf{w}^{\\prime}\\cdot\\phi(x)$ and $\\begin{array}{r}{f(x)=\\sum_{P\\in S^{(\\mathrm{geo})}}f_{P}(x),}\\end{array}$ , then writing an observable $\\begin{array}{r}{O=\\sum_{P\\in\\{I,X,Y,Z\\}\\otimes n}\\alpha_{P}P}\\end{array}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n|g(x)-f(x)|<\\epsilon_{1}\\left(\\sum_{P\\in S^{\\mathrm{(geo)}}}|\\alpha_{P}|\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any $x$ . ", "page_idx": 18}, {"type": "text", "text": "This tells us that the hypothesis functions of the ML algorithm indeed approximate the ground state properties well. The final piece needed is a norm inequality bounding the $\\ell_{1}$ -norm of the Pauli coefficients. This allows us to bound the terms involving $\\left|\\alpha_{P}\\right|$ in Lemma 2 and Lemma 3. In particular, we have the following bound. ", "page_idx": 18}, {"type": "text", "text": "Theorem 8 (Corollary 4 in [2]). Let $\\begin{array}{r}{O\\,=\\,\\sum_{P\\in\\{I,X,Y,Z\\}\\otimes n}\\alpha_{P}P}\\end{array}$ be an observable that can be written as a sum of geometrically local observables. Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{P}|\\alpha_{P}|=\\mathcal{O}(1).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Given these results, Lemma 1 follows directly by triangle inequality and rescaling $\\epsilon_{1}$ when using Lemma 2 and Lemma 3. Finally, to prove Theorem 7, it remains to bound the generalization error by $\\epsilon_{3}$ . This follows directly from known sample complexity guarantees for the LASSO algorithm [64], which learns $\\ell_{1}$ -regularized linear functions. In order to apply this known result, one needs to provide a regularization parameter, i.e., some $B>0$ such that the ML algorithm learns functions of the form $h(x)=\\mathbf{w}\\cdot\\phi(x)$ for $\\|\\mathbf{w}\\|_{1}\\leq B$ . To choose such a $B$ , Lewis et al. bound the $\\ell_{1}$ -norm of $\\mathbf{w}^{\\prime}$ , where recall $\\mathbf{w}_{x^{\\prime},P}^{\\prime}=f_{P}(x^{\\prime})$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma 4 (Lemma 14 in [2]). Let $\\mathbf{w}^{\\prime}$ be the vector of coefficients defined by $\\mathbf{w}_{x^{\\prime},P}^{\\prime}=f_{P}(x^{\\prime})$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\mathbf{w}^{\\prime}\\|_{1}=\\sum_{P\\in S^{\\mathrm{(geo)}}}\\sum_{x^{\\prime}\\in X_{P}}|f_{P}(x^{\\prime})|=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using $B=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ in the known guarantees for LASSO [64] gives Theorem 7. ", "page_idx": 18}, {"type": "text", "text": "A.2 Deep learning with low-discrepancy sequences ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we review results in classical deep learning theory for obtaining rigorous guarantees when learning from data sampled according to a low-discrepancy sequence (LDS) [55, 56, 76, 77, 78, 57, 79, 58, 80, 81]. For this discussion, we follow [54, 85]. ", "page_idx": 19}, {"type": "text", "text": "We consider a neural network or multi-layer perceptron model as a composition of several layers of affine transformations and nonlinear activation functions. Namely, let $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a nonlinear activation function. Then, a neural network with $L$ layers is defined as follows. Let $d_{0},\\ldots,d_{L}\\in\\mathbb{N}$ be the dimension (number of neurons or width) of each layer $k\\in\\{0,\\ldots,L\\}$ . Here, the zeroth layer is the input layer and the $L$ th layer is the output layer. At each layer $k\\in\\{0,\\ldots,L-1\\}$ except for the output layer, we define an affine transformation $W_{k}:\\mathbb{R}^{d_{k}}\\rightarrow\\mathbb{R}^{d_{k+1}}$ by $W_{k}(x)=A_{k}x+b_{k}$ for a matrix of weights $A_{k}\\in\\mathbb{R}^{d_{k+1}\\times d_{k}}$ and a vector of biases $b_{k}\\in\\mathbb{R}^{d_{k+1}}$ . Then, a neural network is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\nf_{\\theta}(x)=(W_{L-1}\\circ\\sigma\\circ\\cdots\\circ\\sigma\\circ W_{0})(x),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\sigma$ is applied element-wise and $\\theta=\\left((A_{0},b_{0}),\\dots,(A_{L-1},b_{L-1})\\right)$ . The hidden layers are the first $L-1$ layers. Here, $\\theta$ are the trainable parameters of the neural network, which can be iteratively updated through training on data. A deep neural network is a neural network with at least three layers: $L\\geq3$ . In this work, we consider the activation function ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sigma(x)=\\operatorname{tanh}(x)={\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We refer to such neural networks with this activation function as tanh neural networks. ", "page_idx": 19}, {"type": "text", "text": "Suppose a neural network $f_{\\theta}$ aims to approximate some target function $f$ , given training data $\\{(\\overline{{x_{\\ell}}},f(x_{\\ell})\\}_{\\ell=1}^{N}$ . Then, the training error is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{R}(\\theta)=\\frac{1}{N}\\sum_{\\ell=1}^{N}|f(x_{\\ell})-f_{\\theta}(x_{\\ell})|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The prediction error is then defined over the whole domain, including unseen data, as ", "page_idx": 19}, {"type": "equation", "text": "$$\nR(\\theta)=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}|f(x)-f_{\\theta}(x)|^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the training data is sampled from some distribution $\\mathcal{D}$ . A canonical result in deep learning theory [66] is that the generalization error can be bounded by roughly ", "page_idx": 19}, {"type": "equation", "text": "$$\nR(\\theta)\\lesssim\\hat{R}(\\theta)+{\\mathcal O}\\left(\\frac{1}{\\sqrt{N}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\lesssim$ indicates that we only state this result schematically. Importantly, this means that in order for the neural network $f_{\\theta}$ to approximate $f$ with high accuracy, many training data points $N$ are needed, which is undesirable. In order to fix this issue, [54] combines ideas from deep learning with tools from quasi-Monte Carlo methods [55, 56, 57, 58] to achieve a generalization error bound of ", "page_idx": 19}, {"type": "equation", "text": "$$\nR(\\theta)\\lesssim\\hat{R}(\\theta)+\\tilde{\\mathcal{O}}\\left(\\frac{1}{N}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\tilde{\\mathcal{O}}$ indicates that we are suppressing polylogarithmic factors. The key tool used here is lowdiscrepancy sequences [55, 56, 76, 77, 78, 57, 79, 58, 80, 81]. Intuitively, this is a collection of points that covers domain of the function $f$ in such a way that there are no large gaps, or discrepancies. By fliling these gaps, one can ensure that the training data accurately represents the target function, more so than even uniformly random data. We leverage these ideas to obtain our rigorous guarantee on the sample complexity of a deep learning algorithm for predicting ground state properties. In the following, we formally define low-discrepancy sequences and a key inequality in quasi-Monte Carlo theory for obtaining our generalization bound. ", "page_idx": 19}, {"type": "text", "text": "First, we define the discrepancy of a sequence, which is a measure of uniformity. ", "page_idx": 19}, {"type": "text", "text": "Definition 1 (Discrepancy [56]). Let $\\lambda$ be the Lebesgue measure, $N\\in\\mathbb{N}.$ . Let $x=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ be a sequence of points with $x_{\\ell}\\in[0,1]^{d}$ for all \u2113. The discrepancy of the sequence $x$ is defined as ", "page_idx": 19}, {"type": "equation", "text": "$$\nD_{N}(d)=\\operatorname*{sup}_{J\\in E}|R_{N}(J)|,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\nR_{N}(J)=\\frac{1}{N}\\sum_{\\ell=1}^{N}\\mathbb{1}\\{x_{\\ell}\\in J\\}-\\lambda(J)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for a Lebesgue-measurable set $J\\subseteq[0,1]^{d}$ . Also, $E$ is the set of all rectangular subsets of $[0,1]^{d}$ , i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\nE=\\left\\{\\prod_{i=1}^{d}[a_{i},b_{i}):0\\leq a_{i}<b_{i}\\leq1\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Intuitively, one can consider the discrepancy as a measure of how well the sequence flils rectangular subsets of $[0,1]^{d}$ . If the discrepancy is small, this means that the sequence fills these subsets well. We can similarly define the star-discrepancy, where the supremum is instead taken over rectangular subsets of $[0,1]^{\\dot{d}}$ such that one endpoint is 0. ", "page_idx": 20}, {"type": "text", "text": "Definition 2 (Star-discrepancy [56]). Let $\\lambda$ be the Lebesgue measure, $N\\in\\mathbb{N}.$ . Let $x=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ be $a$ sequence of points with $x_{\\ell}\\in[0,1]^{d}$ for all \u2113. The star-discrepancy of the sequence $x$ is defined as ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{N}^{*}(d)=\\operatorname*{sup}_{J\\in E^{*}}|R_{N}(J)|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $R_{N}$ is defined in Equation (A.28) for a Lebesgue-measurable set $J\\subseteq[0,1]^{d}$ . Also, $E^{*}$ is the set of all rectangular subsets of $[0,1]^{d}$ , i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\nE^{*}=\\left\\{\\prod_{i=1}^{d}[0,b_{i}):0<b_{i}\\leq1\\right\\}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "With these definitions, we can define low-discrepancy sequences. ", "page_idx": 20}, {"type": "text", "text": "Definition 3 (Low-discrepancy sequence [56]). A sequence of points $x=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ with $x_{\\ell}\\in[0,1]^{d}$ for all $\\ell$ is $a$ low-discrepancy sequence $i f$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{N}^{*}(d)\\leq C\\frac{(\\log N)^{d}}{N},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $C$ is a constant that possibly depends on $d$ but is independent of $N$ . ", "page_idx": 20}, {"type": "text", "text": "The value of the constant $C$ in this definition depends on the construction of the low-discrepancy sequence. Several constructions of low-discrepancy sequences are known [80, 77, 81, 57]. In this work, we consider Sobol sequences in base 2 [57]. For these sequences, we have the following guarantee ", "page_idx": 20}, {"type": "text", "text": "Theorem 9 (Theorem 4.17 in [57]). Let $N\\in\\mathbb{N}.$ . If $x=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ is a Sobol sequence in base 2 with $x_{\\ell}\\in[0,1]^{d}$ for all $\\ell_{i}$ , then the star-discrepancy satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{N}^{*}(d)\\leq C(d){\\frac{(\\log N)^{d}}{N}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $C(d)$ is a constant satisfying ", "page_idx": 20}, {"type": "equation", "text": "$$\nC(d)<\\frac{1}{d!}\\left(\\frac{d}{\\log(2d)}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We state this result without proof and refer to [57] for details on this construction. Another important known discrepancy bound that we will use in Appendix C.3 is the following bound on the stardiscrepancy of uniformly random points. ", "page_idx": 20}, {"type": "text", "text": "Lemma 5 (Corollary 1 in [82]). For any $d\\geq1,N\\,\\geq\\,1$ and $\\delta\\,\\in\\,(0,1)$ a (uniformly) randomly generated $d$ -dimensional point set $(x_{1},\\hdots,x_{N})$ satisfies ", "page_idx": 20}, {"type": "equation", "text": "$$\nD_{N}^{*}(d)\\leq5.7{\\sqrt{4.9+\\log\\left({\\frac{1}{\\delta}}\\right)}}{\\frac{\\sqrt{d}}{\\sqrt{N}}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 20}, {"type": "text", "text": "As discussed earlier, low-discrepancy sequences allow us to obtain better sample complexity guarantees for neural networks. The key result in quasi-Monte Carlo theory that enables this is the Koksma-Hlawka inequality [56]. In order to properly state it, we first need to define the HardyKrause variation. A full technical definition can be found in, e.g., [54], but for our purposes, it suffices to consider the following upper bound [86]. Let $f$ be a \u201csufficiently smooth\u201d function. Then, its Hardy-Krause variation can be upper bounded by ", "page_idx": 21}, {"type": "equation", "text": "$$\nV_{H K}(f)\\leq\\hat{V}_{H K}=\\int_{[0,1]^{d}}\\left|\\frac{\\partial^{d}f(y)}{\\partial y_{i}\\cdot\\cdot\\cdot\\partial y_{d}}\\right|\\,d y+\\sum_{i=1}^{d}\\hat{V}_{H K}(f_{1}^{(i)}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $f_{1}^{(i)}$ is the restriction of the function $f$ to the boundary $y_{i}=1$ . If all of the mixed partial derivatives are continuous, then this inequality is actually an equality [56]. Now, we can state the Koksma-Hlawka inequality. ", "page_idx": 21}, {"type": "text", "text": "Theorem 10 (Koksma-Hlawka inequality). Let $f:[0,1]^{d}\\rightarrow\\mathbb{R}$ be a function whose mixed derivatives are absolutely integrable over its domain with bounded Hardy-Krause variation $V_{H K}(f)<\\infty$ . Let $x=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ be a sequence of $N\\,d$ -dimensional points in $[0,1]^{d}$ with star-discrepancy $D_{N}^{*}(d)$ . Then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left|\\int_{[0,1]^{d}}f(x)\\,d x-{\\frac{1}{N}}\\sum_{\\ell=1}^{N}f(x_{\\ell})\\right|\\leq V_{H K}(f)D_{N}^{*}(d).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This theorem is used in quasi-Monte Carlo methods to estimate the error of approximating an integral of a function $f$ by the empirical average of $f$ evaluated on a sequence of points. Notice that if the sequence $x$ is a low-discrepancy sequence, then by definition, we can upper bound the star-discrepancy $D_{N}^{*}$ . Moreover, recalling the definitions of prediction error and training error (Equations (A.23) and (A.24)), one can see how this relates to our task of bounding the prediction error. ", "page_idx": 21}, {"type": "text", "text": "To generalize our results to a wider class of distributions, we need to extend these tools for arbitrary measures, rather than just the Lebesgue measure. First, we restate the definition of discrepancy and star-discrepancy [87]. ", "page_idx": 21}, {"type": "text", "text": "Definition 4 (General Discrepancy [88]). Let $\\mu$ be a normalized Borel measure on $[0,1]^{d}$ . Let $x=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ be a sequence of points with $x_{\\ell}\\in[0,1]^{d}$ for all $\\ell$ . The discrepancy with respect to $\\mu$ of the sequence $x$ is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{N}(d;\\mu)=\\operatorname*{sup}_{J\\in E}|R_{N}(J;\\mu)|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\nR_{N}(J;\\mu)=\\frac{1}{N}\\sum_{\\ell=1}^{N}\\mathbb{1}\\{x_{\\ell}\\in J\\}-\\mu(J)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for a Borel-measurable set $J\\subseteq[0,1]^{d}$ . Also, $E$ is the set of all rectangular subsets of $[0,1]^{d}$ , i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\nE=\\left\\{\\prod_{i=1}^{d}[a_{i},b_{i}):0\\leq a_{i}<b_{i}\\leq1\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Definition 5 (General Star-Discrepancy [87]). Let $\\mu$ be a normalized Borel measure on $[0,1]^{d}$ , and let $N\\in\\ensuremath{\\mathbb{N}}$ . Let $x=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ be a sequence of points with $x_{\\ell}\\in[0,1]^{d}$ for all $\\ell$ . The star-discrepancy with respect to $\\mu$ of the sequence $x$ is defined as ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{N}^{*}(d;\\mu)=\\operatorname*{sup}_{J\\in E^{*}}\\left|{R_{N}(J;\\mu)}\\right|,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $R_{N}$ is defined in Equation (A.39) for a Borel-measurable set $J\\subseteq[0,1]^{d}$ . Also, $E^{*}$ is the set of all rectangular subsets of $[0,1]^{d}$ , i.e., ", "page_idx": 21}, {"type": "equation", "text": "$$\nE^{*}=\\left\\{\\prod_{i=1}^{d}[0,b_{i}):0<b_{i}\\leq1\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "These definitions coincide with Definition 1 and Definition 2 when $\\mu$ is the Lebesgue measure $\\lambda$ . Moreover, we can define general low-discrepancy sequences similarly to Definition 3 with respect this general star-discrepancy. There is also a generalized Koksma-Hlawka inequality [87], which we state below. ", "page_idx": 21}, {"type": "text", "text": "Theorem 11 (Generalized Koksma-Hlawka inequality; Theorem 1 in [87]). Let $f:[0,1]^{d}\\rightarrow\\mathbb{R}$ be a measurable function whose mixed derivatives are absolutely integrable over its domain with bounded Hardy-Krause variation $V_{H K}(f)<\\infty$ . Let $\\mu$ be a normalized Borel measure on $[0,1]^{d}$ , and let $\\boldsymbol{x}\\,=\\,\\{x_{\\ell}\\}_{\\ell=1}^{N}$ be a sequence of $N$ $d$ -dimensional points in $[0,1]^{d}$ with general star-discrepancy $D_{N}^{*}(d;\\mu)$ . Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{N}\\sum_{\\ell=1}^{N}f(x_{\\ell})-\\int_{[0,1]^{d}}f(x)\\,d\\mu(x)\\right|\\leq V_{H K}(f)D_{N}^{*}(d;\\mu).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B Constant sample complexity ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we show that with a simple modification of the algorithm from [2], we can reduce the sample complexity to $\\mathcal{O}(1)$ for a constant prediction error. We consider all of the same definitions/notation as in Appendix A.1. This section is similar to Section IV in the Supplementary Information of [2]. As in [2], our algorithm first maps the parameter space $[-1,1]^{m}$ into a high-dimensional feature space $\\mathbb{R}^{m_{\\phi}}$ for $m_{\\phi}$ given in Equation (A.8) via a feature map $\\phi$ . Our simple modification is to use the feature map defined by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{\\phi}(x)_{x^{\\prime},P}\\triangleq\\mathrm{sign}(\\alpha_{P})\\sqrt{|\\alpha_{P}|}\\mathbb{1}\\{x\\in T_{x^{\\prime},P}\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where each coordinate of $\\phi(x)$ is indexed by $P\\in S^{(\\mathrm{geo})},x^{\\prime}\\in X_{P}$ . Note that defining the feature map in this way requires knowledge of the observable $\\begin{array}{r}{O=\\sum_{P}\\alpha_{P}P}\\end{array}$ corresponding to the ground state property to be predicted. However, in practice, this is a n atural assumption. The hypothesis class for our proposed ML algorithm consists of linear functions in this feature space, i.e., functions of the form $h(x)=\\mathbf{w}\\cdot\\phi(x)$ . Then, our algorithm learns these functions via ridge regression [65, 66]. For a chosen hyperparameter $\\Lambda>0$ , ridge regression finds a vector $\\mathbf{w}^{*}$ that solves the following optimization problem minimizing the training error subject to the constraint that $\\|\\mathbf{w}\\|_{2}\\leq\\Lambda$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{m_{\\phi}}}\\frac{1}{N}\\sum_{\\ell=1}^{N}|\\mathbf{w}\\cdot\\tilde{\\phi}(x_{\\ell})-y_{\\ell}|^{2},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $y_{\\ell}$ approximates $\\operatorname{tr}(O\\rho(x_{\\ell}))$ . We denote the learned function by $h^{*}(x)=\\mathbf{w}^{*}\\cdot\\tilde{\\phi}(x)$ . Note that the learned function does not need to achieve the minimum training error, but can be some amount say $\\epsilon_{3}/2$ above it. For our purposes, we choose the hyperparameter to be $\\Lambda=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ , which we justify in the next section. ", "page_idx": 22}, {"type": "text", "text": "Note that there are two main differences from the algorithm in [2]. First, recall from Appendix A.1 that the feature map was previously defined as $\\phi(x)_{x^{\\prime},P}=\\mathbb{1}\\{x\\in T_{x^{\\prime},P}\\}$ for $P\\in S^{(\\mathrm{geo})},x^{\\prime}\\in X_{P}$ . Second, instead of using LASSO $\\mathcal{l}_{1}$ -regularized regression), our proposed algorithm uses ridge regression. ", "page_idx": 22}, {"type": "text", "text": "With this algorithm, we obtain the following guarantee. ", "page_idx": 22}, {"type": "text", "text": "Theorem 12 (Constant sample complexity; Detailed restatement of Theorem 4). Let $1/e~>$ $\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}>0$ and $\\delta>0$ . Given training data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 22}, {"type": "equation", "text": "$$\nN=\\log(1/\\delta)2^{\\mathcal{O}(\\log(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $x_{\\ell}$ is sampled from $\\mathcal{D}$ and y\u2113is an estimator of $\\operatorname{tr}(O\\rho(x_{\\ell}))$ such that $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\le\\epsilon_{2}$ , the ML algorithm can produce $h^{*}(x)$ that achieves prediction error ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|h^{*}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with probability at least $1-\\delta$ . The training time for constructing the hypothesis function $h^{*}$ and the prediction time for computing $h^{*}(x)$ are upper bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathcal{O}(n)\\mathrm{polylog}(1/\\delta)2^{\\mathcal{O}(\\log(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Comparing to Theorem 7, notice that our sample complexity guarantee is completely independent of system size $n$ . ", "page_idx": 22}, {"type": "text", "text": "The theorem in the main text corresponds to $\\epsilon_{1}\\;=\\;0.2\\epsilon,\\epsilon_{2}\\;=\\;\\epsilon$ , and $\\epsilon_{3}~=~0.4\\epsilon$ . In this way, $(\\epsilon_{1}+\\epsilon_{2})^{2}\\leq1.44\\epsilon^{2}\\leq0.53\\epsilon$ and $(\\epsilon_{1}\\dot{+}\\epsilon_{2})^{2}+\\epsilon_{3}\\leq\\epsilon$ . ", "page_idx": 23}, {"type": "text", "text": "So far, we have only considered the setting in which we learn a specific ground state property $\\mathrm{tr}(O\\rho(x))$ for a fixed observable $O$ . Because our training data is given in the form $\\{(x_{\\ell},\\dot{y}_{\\ell})\\}_{\\ell=1}^{N}$ , where $y_{\\ell}$ approximates $\\mathrm{tr}(O\\rho(x))$ for this fixed observable $O$ , if we want to predict a new property for the same ground state $\\rho(x)$ , we would need to generate new training data. Thus, it may be more useful to learn a ground state representation, from which we could predict $\\mathrm{tr}(O\\rho(x))$ for many different choices of observables $O$ without requiring new training data. In this case, suppose we are instead given training data $\\{x_{\\ell},\\sigma_{T}(\\rho(x_{\\ell}))\\}_{\\ell=1}^{N}$ , where $\\sigma_{T}(\\rho(\\bar{x}_{\\ell}))$ is a classical shadow representation [52, 68, 69, 70, 71] of the ground state $\\rho(x_{\\ell})$ . An immediate corollary of Theorem 12 is that we can predict ground state representations with the same sample complexity. This follows from the same proof as Corollary 5 in [2]. ", "page_idx": 23}, {"type": "text", "text": "Corollary 3 (Learning representations of ground states; detailed restatement of Corollary 1). Let $1/e>\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}>0$ and $\\delta>0$ . Given training data $\\{(x_{\\ell},\\sigma_{T}(\\rho(x_{\\ell}))\\}_{\\ell=1}^{N}$ of size ", "page_idx": 23}, {"type": "equation", "text": "$$\nN=\\log(1/\\delta)2^{\\mathcal{O}(\\log(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $x_{\\ell}$ is sampled from $\\mathcal{D}$ and $\\sigma_{T}(\\rho(x_{\\ell})$ is the classical shadow representation of the ground state $\\rho(x_{\\ell})$ using $T$ randomized Pauli measurements. For $T=\\mathcal{O}(\\log(n N/\\delta)/\\epsilon_{2}^{2})=\\tilde{\\mathcal{O}}(\\log(n/\\delta)/\\epsilon_{2}^{2})$ , the ML algorithm can produce a ground state representation $\\hat{\\rho}_{N,T}(x)$ that achieves ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|\\,\\mathrm{tr}(O\\hat{\\rho}_{N,T}(x))-\\mathrm{tr}(O\\rho(x))|^{2}\\le(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with probability at least $1-\\delta$ , for any observable with eigenvalues between $-1$ and 1 that can be written as a sum of geometrically local observables. ", "page_idx": 23}, {"type": "text", "text": "We note that the number of measurements $T$ needed to generate the training data scales as $\\log(n)$ , but the amount of training data still remains constant with respect to system size. We do not consider the number of measurements as contributing to the sample complexity because in our setting, the ML algorithm is given this training data as input and does not generate it itself. ", "page_idx": 23}, {"type": "text", "text": "B.1 Training error bound ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To prove Theorem 12, we first derive a bound on the training error. Recall that the training error is defined as ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\hat{R}(h)=\\operatorname*{min}_{\\mathbf{w}}\\frac{1}{N}\\sum_{\\ell=1}^{N}|h(x_{\\ell})-y_{\\ell}|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Define the vector $\\tilde{\\mathbf{w}}$ with entries indexed by $P\\in S^{(\\mathrm{geo})},x^{\\prime}\\in X_{P}$ by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{w}}_{x^{\\prime},P}\\triangleq\\sqrt{|\\alpha_{P}|}\\,\\mathrm{tr}(P\\rho(\\chi_{P}(x))),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\chi_{P}(x)$ is defined in Equation (A.16). Then, notice that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{g}(x)\\triangleq\\tilde{\\mathbf{w}}\\cdot\\tilde{\\tilde{g}}(x)}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{P\\in S^{(\\mathrm{geo})}}\\sum_{x^{\\prime}\\in X_{P}}\\mathrm{sign}(\\alpha_{P})|\\alpha_{P}|\\,\\mathrm{tr}\\big(P\\rho(\\chi_{P}(x))\\big)\\mathbb{1}\\{x\\in T_{x^{\\prime},P}\\}}\\\\ &{\\quad\\quad=\\displaystyle\\sum_{P\\in S^{(\\mathrm{geo})}}\\sum_{x^{\\prime}\\in X_{P}}\\alpha_{P}\\,\\mathrm{tr}\\big(P\\rho(\\chi_{P}(x))\\big)\\mathbb{1}\\{x\\in T_{x^{\\prime},P}\\}}\\\\ &{\\quad\\quad=\\mathbf{w}^{\\prime}\\cdot\\phi(x)}\\\\ &{\\quad\\quad=g(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $g(x)=\\mathbf{w}^{\\prime}{\\cdot}\\phi(x)$ with $\\mathbf{w}_{x^{\\prime},P}^{\\prime}=\\alpha_{P}\\operatorname{tr}(P\\rho(\\chi_{P}(x)))$ , $\\phi(x)_{x^{\\prime},P}=\\mathbb{1}\\{x\\in T_{x^{\\prime},P}\\}$ . By Lemma 1, we know that $g(x)$ approximates the ground state property with low training error, and thus, in turn, ${\\tilde{g}}(x)$ also approximates the ground state property well. The existence of $\\tilde{\\mathbf{w}}$ such that $\\tilde{g}(x)=\\tilde{\\mathbf{w}}\\cdot\\tilde{\\phi}(x)$ guarantees that the function $h^{*}(x)=\\mathbf{w}^{*}\\cdot\\tilde{\\phi}(x)$ found by performing via ridge regression will also yield a small training error. More formally, we have the following guarantee ", "page_idx": 23}, {"type": "text", "text": "Lemma 6 (Training error). The function ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{g}(x)=\\tilde{\\mathbf{w}}\\cdot\\tilde{\\phi}(x)=\\sum_{P\\in S^{\\mathrm{(geo)}}}\\sum_{x^{\\prime}\\in X_{P}}\\alpha_{P}\\operatorname{tr}(P\\rho(\\chi_{P}(x)))\\mathbb{1}\\{x\\in T_{x^{\\prime},P}\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "achieves training error ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{R}(\\tilde{g})\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since ${\\tilde{g}}(x)=g(x)$ , this follows directly from Lemma 1. Moreover, we can obtain an $\\ell_{2}$ -norm bound on w\u02dc. We can utilize this upper bound to choose the hyperparameter $\\Lambda>0$ such that $\\|\\mathbf{w}\\|_{2}\\leq\\Lambda$ . Thus, we have the following lemma, ", "page_idx": 24}, {"type": "text", "text": "Lemma 7 ( $\\ell_{2}$ -Norm bound). Let w\u02dc be the vector of coefficients defined in Equation (B.9). Then, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{w}}\\|_{2}^{2}=\\sum_{P\\in S^{\\mathrm{(geo)}}}\\sum_{x^{\\prime}\\in X_{P}}|\\alpha_{P}||\\operatorname{tr}(P\\rho(\\chi_{P}(x)))|^{2}=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. This is a simple consequence of Lemma 4. Explicitly, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{\\mathbf{w}}\\|_{2}=\\displaystyle\\sum_{P\\in S^{(\\mathrm{geo})}}\\displaystyle\\sum_{x^{\\prime}\\in X_{P}}|\\alpha_{P}||\\,\\mathrm{tr}(P\\rho(\\chi_{P}(x)))|^{2}}\\\\ &{\\qquad\\le\\displaystyle\\sum_{P\\in S^{(\\mathrm{geo})}}\\displaystyle\\sum_{x^{\\prime}\\in X_{P}}|\\alpha_{P}||\\,\\mathrm{tr}(P\\rho(\\chi_{P}(x)))|}\\\\ &{\\qquad=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the second line follows because $\\mathrm{tr}(P\\rho(\\chi_{P}(x)))\\leq1$ and the last line follows by Lemma 4. ", "page_idx": 24}, {"type": "text", "text": "This justifies our choice of $\\Lambda\\,=\\,2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ . Now consider the learned function $h^{*}(x)\\,=$ $\\mathbf{w}^{*}\\cdot\\tilde{\\phi}(x)$ , where $\\mathbf{w}^{*}$ is found by minimizing the training error subject to the constraint that $\\|\\mathbf{w}\\|_{2}\\leq\\Lambda$ . We do not require the learned function to achieve the minimum training error, but it can be some amount $\\epsilon_{3}/2$ above it, i.e., ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{R}(h^{*})\\leq\\frac{\\epsilon_{3}}{2}+\\operatorname*{min}_{\\|\\mathbf{w}\\|_{2}\\leq\\Lambda}\\frac{1}{N}\\sum_{\\ell=1}^{N}|\\mathbf{w}\\cdot\\tilde{\\phi}(x_{\\ell})-y_{\\ell}|^{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since we chose $\\Lambda=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ and we showed in Lemma 7 that $\\|\\tilde{\\mathbf{w}}\\|_{2}\\leq\\Lambda$ , then the minimum training error is at most $\\hat{R}(\\tilde{g})$ . We also know that this is bounded by $(\\epsilon_{1}+\\epsilon_{2})^{2}$ by Lemma 6. This then implies ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{R}(h^{*})\\leq\\frac{\\epsilon_{3}}{2}+\\hat{R}(g)\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}+\\frac{\\epsilon_{3}}{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.2 Prediction error bound ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "To prove Theorem 12, it remains to bound the prediction error. We can use a standard result from machine learning theory on the prediction error of ridge regression algorithms [66, 64]. ", "page_idx": 24}, {"type": "text", "text": "Theorem 13 (Theorem 26.12 in [66]). Suppose that $\\mathcal{D}$ is a distribution over $\\mathcal X\\times\\mathcal X$ such that with probability 1 we have that $\\|\\mathbf{x}\\|_{2}\\leq R$ . Let $\\mathcal{H}=\\left\\{\\mathbf{x}\\mapsto\\mathbf{w}\\cdot\\mathbf{x}:\\|\\mathbf{w}\\|_{2}\\leq\\Lambda\\right\\}$ and let $\\ell:\\mathcal{H}\\times Z\\to\\mathbb{R}$ be a loss function of the form $\\ell(\\mathbf{w},(\\mathbf{x},y))=\\phi(\\mathbf{\\dot{w}}\\cdot\\mathbf{x},y)$ such that for all $y\\in\\mathcal{P},a\\mapsto\\phi(a,y)$ is a $\\rho$ -Lipschitz function and such that $\\mathrm{max}_{a\\in[-\\Lambda R,\\Lambda R]}\\,|\\phi(a,y)|\\leq c$ . Then, for any $\\delta\\in(0,1)$ , with probability of at least $1-\\delta$ over the choice of an i.i.d. sample of size $N$ , for all $h\\in\\mathcal H$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\nR(h)\\leq\\hat{R}_{S}(h)+\\frac{2\\rho\\Lambda R}{\\sqrt{N}}+c\\sqrt{\\frac{2\\log(2/\\delta)}{N}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, $R(h)$ denotes the prediction error for the hypothesis $h$ . With this, we can complete the proof of Theorem 12. ", "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 12. First, let us reframe the theorem in our setting. Consider the input space $\\mathcal{X}$ to be the parameter space $[-1,1]^{m}$ and our input variable is $\\mathbf{x}=\\tilde{\\phi}\\bar{(}x)$ . Since the observables we consider have spectral norm at most 1, the output space fulfils ${\\mathcal{V}}\\subseteq[-1,1]$ . The hypothesis set is $\\mathcal{\\bar{H}}=\\{x\\mapsto\\mathbf{w}\\cdot\\bar{\\boldsymbol{\\phi}}(x):\\|\\mathbf{w}\\|_{2}\\leq\\Lambda\\}$ , where in the previous section, we set $\\Lambda=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ . ", "page_idx": 25}, {"type": "text", "text": "It remains to check the conditions of the theorem. We begin by showing that $\\|\\mathbf{x}\\|_{2}\\leq R$ for some $R>0$ . We have the following computation: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|{\\mathbf x}\\|_{2}=\\tilde{\\phi}(x)\\cdot\\tilde{\\phi}(x)=\\left\\|\\tilde{\\phi}(x)\\right\\|_{2}^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{P\\in S^{(\\mathrm{eqs})}}\\sum_{x^{\\prime}\\in X_{P}}|\\mathrm{sign}(\\alpha_{P})\\sqrt{|\\alpha_{P}|}\\mathbb{1}\\{x\\in T_{x^{\\prime},P}\\}|^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{P\\in S^{(\\mathrm{eqs})}}\\sum_{x^{\\prime}\\in X_{P}}|\\alpha_{P}|\\mathbb{1}\\{x\\in T_{x^{\\prime},P}\\}}\\\\ &{\\qquad=\\displaystyle\\sum_{P\\in S^{(\\mathrm{eqs})}}|\\alpha_{P}|}\\\\ &{\\qquad=\\mathcal{O}(1),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second to last line follows because for a given $P$ , $x\\in T_{x^{\\prime},P}$ for exactly one $x^{\\prime}\\in X_{P}$ . This is shown in Corollary 3 of [2]. Also, the last line follows by Theorem 8. Thus, we can take $R=\\mathcal{O}(1)$ . ", "page_idx": 25}, {"type": "text", "text": "Finally, note that $\\ell(\\mathbf{w},(\\mathbf{x},y))\\,=\\,|\\mathbf{w}\\cdot\\mathbf{x}-y|\\,=\\,\\phi(\\mathbf{w}\\cdot\\mathbf{x},y)$ . Therefore, $\\phi(a,y)$ is a 1-Lipschitz function and fulfils ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in[-\\Lambda R,\\Lambda R]}|\\phi(a,y)|=\\operatorname*{max}_{a\\in[-\\Lambda R,\\Lambda R]}|a-y|\\leq\\Lambda R+1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we can consider $\\rho=1$ and $c=\\mathcal{O}(1)\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}+1$ . ", "page_idx": 25}, {"type": "text", "text": "By Equation (B.22), we know that the learned model $h^{*}(x)=\\mathbf{w}^{*}\\cdot\\tilde{\\phi}(x)$ achieves ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{R}(h^{*})\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}+\\frac{\\epsilon_{3}}{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Plugging in $R\\,=\\,\\mathcal{O}(1)$ , $\\rho\\,=\\,1$ , $\\Lambda=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ and $c=\\mathcal{O}(1)\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}+1$ into Theorem 13, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{R(h^{*})\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}+\\displaystyle\\frac{\\epsilon_{3}}{2}}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{1}{\\sqrt{N}}\\left(2\\mathcal{O}(1)\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}+\\left(\\mathcal{O}(1)\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}+1\\right)\\sqrt{2\\log(2/\\delta)}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with probability at least $1-\\delta$ . In order to bound the prediction error by $(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}$ , we need $N$ to be large enough such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{N}}\\left(2\\mathcal{O}(1)\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}+\\left(\\mathcal{O}(1)\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}+1\\right)\\sqrt{2\\log(2/\\delta)}\\right)\\leq\\frac{\\epsilon_{3}}{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Solving for $N$ in this inequality and simplifying we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nN=\\frac{4}{\\epsilon_{3}^{2}}2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}(1+\\sqrt{\\log(1/\\delta)})^{2}=2^{\\mathcal{O}(\\log(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))}\\log(1/\\delta).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, for this $N$ , we can guarantee that $R(h^{*})\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}$ , as claimed. ", "page_idx": 25}, {"type": "text", "text": "On another note, when considering a scenario with a fixed number of parameters $m=\\mathcal{O}(1)$ , much like the setting in [51], the expression derived from the result in Lemma 10 exhibits polynomial dependence on $\\epsilon$ . One can incorporate the constant number of parameters by setting ${\\tilde{m}}=m$ . Thus, we recover the exact ground state properties $\\operatorname{tr}(P\\rho(x))$ in $f_{P}$ and the approximation error resulting from applying Lemma 2 vanishes completely. Furthermore, we can slightly adapt the proof of Lemma 7 and obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{w}}\\|_{2}^{2}=\\sum_{P\\in S^{\\mathrm{(geo)}}}\\sum_{x^{\\prime}\\in X_{P}}|\\alpha_{P}||\\operatorname{tr}(P\\rho(\\chi_{P}(x)))|=\\operatorname*{max}_{P\\in S^{\\mathrm{(geo)}}}|X_{P}|\\sum_{Q\\in S^{\\mathrm{(geo)}}}|\\alpha_{P}|=\\mathcal{O}(\\epsilon^{-m}),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last step is performed similarly as in the proof of Lemma 4. ", "page_idx": 25}, {"type": "text", "text": "B.3 Computational time for training and prediction ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "It remains to analyze the computational time for the ML algorithm\u2019s training and prediction. ", "page_idx": 26}, {"type": "text", "text": "Proof of computational time in Theorem $^{12}$ . The training time is dominated by the time required for ridge regression over the feature space defined by the feature map $\\phi$ . Recall that the optimization problem under considerations is ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{w}\\in\\mathbb{R}^{m_{\\phi}}}\\frac{1}{N}\\sum_{\\ell=1}^{N}|\\mathbf{w}\\cdot\\tilde{\\phi}(x_{\\ell})-y_{\\ell}|^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "One can show that this is a convex optimization problem so that we can solve its equivalent dual problem instead. This dual optimization problem is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\alpha\\in\\mathbb{R}^{N}}-\\alpha^{\\mathsf{T}}(K+\\lambda I)\\alpha+2\\alpha\\cdot Y,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the kernel matrix is $K\\,=\\,X^{\\mathsf{T}}X$ , for the feature matrix $X~\\in~\\mathbb{R}^{m_{\\phi}\\times N}$ defined by $X\\,=$ $(\\tilde{\\phi}(x_{1})~\\cdot\\cdot\\cdot~\\tilde{\\phi}(x_{N}))$ and the response vector $\\boldsymbol{Y}=(y_{1},\\boldsymbol{\\cdot}\\boldsymbol{\\cdot},y_{N})^{\\intercal}$ . If $\\kappa$ is the maximum time it takes to compute a kernel entry $K(x,\\stackrel{\\cdot}{x^{\\prime}})=\\tilde{\\phi}(x)\\cdot\\tilde{\\phi}(x^{\\prime})$ , then one can show that the time to solve this dual problem is $\\mathcal{O}(\\kappa N^{2}+\\bar{N^{3}})$ . Moreover, prediction can be executed in $\\mathcal{O}(\\kappa N)$ . For more details in this analysis, we refer the reader to, e.g., Section 11.3.2 of [64]. ", "page_idx": 26}, {"type": "text", "text": "In our case, $\\kappa=\\mathcal{O}(m_{\\phi})$ since $\\tilde{\\phi}(\\boldsymbol{x})\\in\\mathbb{R}^{m_{\\phi}}$ and the kernel is simply the dot product of two of these vectors. By Equation (A.8), we know that ", "page_idx": 26}, {"type": "equation", "text": "$$\nm_{\\phi}=\\mathcal{O}(n)2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "so that $\\kappa=\\mathcal{O}(n)2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ . Moreover, by Theorem 12, ", "page_idx": 26}, {"type": "equation", "text": "$$\nN=\\log(1/\\delta)2^{\\mathcal{O}(\\log(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging this into the time required to solve the dual problem for kernel ridge regression, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}(\\kappa N^{2}+N^{3})=\\mathcal{O}(n)\\mathrm{polylog}(1/\\delta)2^{\\mathcal{O}(\\log(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, the prediction time is given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{O}(\\kappa N)=\\mathcal{O}(n)\\mathrm{polylog}(1/\\delta)2^{\\mathcal{O}(\\log(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "C Rigorous guarantees for neural networks ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we derive a rigorous guarantee on the sample complexity of a deep-learning based model for predicting ground state properties. Similarly to the previous sections, let $1/e>\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}>$ 0 throughout. One can think of $\\epsilon_{1}$ as the approximation error caused by our neural network model not exactly capturing the ground state property; $\\epsilon_{2}$ represents the noise in the training data; $\\epsilon_{3}$ corresponds to the generalization error. ", "page_idx": 26}, {"type": "text", "text": "Recall again the setup, where we consider a family of $n$ -qubit Hamiltonians $\\begin{array}{r}{H(x)=\\sum_{j=1}^{L}h_{j}(\\vec{x}_{j})}\\end{array}$ parameterized by an $m$ -dimensional vector $x\\in[-1,1]^{m}$ , which satisfies the assumptions (a)-(c) in Appendix A.1. Let $\\rho(x)$ denote the ground state of $H(x)$ . We consider the task of predicting ground state properties $\\operatorname{tr}(O\\rho(x))$ for some observable $O$ that satisfies assumption (d) in Appendix A.1, where we are given training data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ with $y_{\\ell}\\,\\approx\\,\\operatorname{tr}(O\\rho(x_{\\ell}))$ . In particular, suppose $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\,\\le\\,\\epsilon_{2}$ . Furthermore, we also assume that all mixed partial derivatives of order $\\tilde{m}\\triangleq|I_{P}|$ of $h_{j}$ are bounded as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial^{\\tilde{m}}}{\\partial x_{1}\\partial x_{2}\\ldots\\partial x_{\\tilde{m}}}h_{j}(x)\\right\\|_{\\infty}\\leq1,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $I_{P}$ is the set of local coordinates defined in Equation (A.2). Here, we denote the number of local coordinates by $\\tilde{m}=|I_{P}|$ for ease of notation. This is similar in spirit to assumption (b), in which we assume that the local terms have bounded directional derivatives: $\\left\\|\\partial h_{j}/\\partial\\hat{u}\\right\\|_{\\infty}\\leq1$ , where $\\hat{u}$ is a unit vector in parameter space. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "Let $S^{(\\mathrm{geo})}$ denote the set of geometrically local Pauli observables. Our deep neural network model consists of $\\vert S^{\\mathrm{(geo)}}\\vert\\,=\\,\\mathcal{O}(n)$ \u201clocal\u201d multi-layer perceptron models (defined in generally in Appendix A.2) with two hidden layers and tanh activation functions. Their outputs are combined through a linear layer without activation function. Formally, our model is defined as follows. ", "page_idx": 27}, {"type": "text", "text": "Definition 6 (Deep neural network model). The neural network model is given by a function $f^{\\Theta,w}:[-1,1]^{m}\\overset{}{\\rightarrow}\\mathbb{R}$ defined by ", "page_idx": 27}, {"type": "equation", "text": "$$\nf^{\\Theta,w}(x)=\\sum_{P\\in S^{(\\mathrm{geo})}}w_{P}f_{P}^{\\theta_{P}}(x),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the \u201clocal models\u201d $f_{P}^{\\theta_{P}}:[-1,1]^{\\tilde{m}}\\to\\mathbb{R}$ are given by ", "page_idx": 27}, {"type": "equation", "text": "$$\nf_{P}^{\\theta_{P}}(x)=\\left(W_{\\mathrm{out}}\\circ\\operatorname{tanh}\\circ W_{\\mathrm{hidden}}\\circ\\operatorname{tanh}\\circ W_{\\mathrm{in}}\\circ\\tau^{-1}\\right)(x),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $\\tau^{-1}(x)\\,=\\,(x+1)/2$ and \u03b8 $P_{\\mathrm{...}}\\mathrm{=}_{\\mathrm{..}}[(W_{\\mathrm{in}},b_{\\mathrm{in}}),(W_{\\mathrm{M}\\mathrm{idden}},b_{\\mathrm{hidden}}),(W_{\\mathrm{out}},b_{\\mathrm{out}})]$ . Here, $W_{\\mathrm{in}}\\,\\in$ $\\mathbb{R}^{\\tilde{m}\\times W}$ , $\\boldsymbol{b}_{\\mathrm{in}}^{\\mathrm{~\\,~}}\\in\\mathbb{R}^{W}$ , $W_{\\mathrm{hidden}}\\,\\in\\,\\mathbb{R}^{W\\times W}$ , $b_{\\mathrm{hidden}}\\,\\in\\,\\mathbb{R}^{W}$ , $W_{\\mathrm{out}}\\,\\in\\,\\mathbb{R}^{W\\times1}$ and $b_{\\mathrm{out}}\\in\\mathbb{R},$ where $W$ denotes the width of the hidden layers. The weights are given by $\\Theta=\\{\\theta_{P}:P\\in S^{(\\mathrm{geo})}\\}$ in the local models and $w\\in\\mathbb R$ in the last layer. Furthermore, we denote the individual parameters by $\\Theta_{i}\\in\\mathbb{R}.$ ", "page_idx": 27}, {"type": "text", "text": "Using this model, we can establish an objective function that we aim to minimize during the training process. Specifically, this objective function comprises the mean square error along with a lasso penalty applied to the weights $w$ in the final layer. ", "page_idx": 27}, {"type": "text", "text": "Definition 7 (Training objective). Let $f^{\\Theta,w}$ be a neural network model as in Definition 6. Let $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ be the training data set and $\\lambda>0$ be some regularization parameter that may depend $o n\\;\\epsilon_{1},\\epsilon_{2}>0$ . The training objective is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{\\ell=1}^{N}|f^{\\Theta,w}(x_{\\ell})-y_{\\ell}|^{2}+\\lambda\\|w\\|_{1}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Our proposed ML algorithm then operates as in Algorithm 1 ", "page_idx": 27}, {"type": "text", "text": "Algorithm 1: Deep learning-based prediction of ground state properties ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Sample $N$ low-discrepancy points $\\{x_{\\ell}\\}_{\\ell=1}^{N}$ ; ", "page_idx": 27}, {"type": "text", "text": "Collect training labels $\\{y_{\\ell}\\}_{\\ell=1}^{N}$ , where $y_{\\ell}\\approx\\mathrm{tr}(O\\rho(x_{\\ell}))$ ; ", "page_idx": 27}, {"type": "text", "text": "Data: $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ ; ", "page_idx": 27}, {"type": "text", "text": "Fix $\\left|{I_{P}}\\right|$ ", "page_idx": 27}, {"type": "text", "text": "Initialize model architecture according to Definition 6 with appropriate hyperparameter $\\delta_{1}$ , width $W$ as in Theorem 16 and weights $\\Theta,w$ using an appropriate initialization method (e.g., Xavier initialization [72]); ", "page_idx": 27}, {"type": "text", "text": "Train with respect to the objective in Definition 7 with appropriate hyperparameter $\\lambda>0$ using a quasi-Monte Carlo training algorithm, e.g., Adam [73] until convergence; Obtain locally optimal parameters $\\Theta^{*},w^{*}$ ; ", "page_idx": 27}, {"type": "text", "text": "Result: Classical representation $f^{\\Theta^{*},w^{*}}$ ", "page_idx": 27}, {"type": "text", "text": "After training our model using Algorithm 1, we obtain the following rigorous guarantee. ", "page_idx": 27}, {"type": "text", "text": "Theorem 14 (Neural network sample complexity guarantee). Let $1/e>\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}>0.$ . Let $f^{\\Theta^{*},w^{*}}$ : $[-1,1]^{m}\\to\\mathbb{R}$ be a neural network model produced from Algorithm $^{\\,l}$ trained on data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 27}, {"type": "equation", "text": "$$\nN=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1})+\\mathrm{polylog}(1/\\epsilon_{3}))}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the $x_{\\ell}$ \u2019s form a low-discrepancy Sobol sequence and $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\le\\epsilon_{2}$ . Suppose that $f^{\\Theta^{*},w^{*}}$ achieves a training error of at most $((\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3})/2$ . Additionally, suppose that all parameters $\\Theta_{i}^{*}$ of $f^{\\Theta^{*},w^{*}}$ satisfy $|\\Theta_{i}^{*}|\\leq W_{\\operatorname*{max}},$ , for some $W_{\\mathrm{max}}>0$ that is independent of the system size $n$ . Then the neural network $f^{\\Theta^{*},w^{*}}$ achieves prediction error ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}|f^{\\Theta^{*},w^{*}}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\leq2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $x\\sim U[-1,1]^{m}$ denotes sampling $x$ from a uniform distribution over $[-1,1]^{m}$ . ", "page_idx": 27}, {"type": "text", "text": "We prove this theorem in the next two sections (Appendices C.1 and C.2). As a corollary of this, we obtain the theorem stated in the main text. We discuss the assumptions that the distribution $\\mathcal{D}$ must satisfy in depth and prove the corollary in Appendix C.3. The theorem in the main text (Theorem 5) corresponds to $\\epsilon_{1}=\\epsilon_{3}=0.1\\epsilon$ and $\\epsilon_{2}=\\epsilon$ . Hence, $2(\\epsilon_{1}+\\epsilon_{2})^{2}\\leq2.44\\epsilon^{2}\\leq0.9\\epsilon$ for $1/e>\\epsilon>0$ and so $2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}\\leq\\epsilon$ . ", "page_idx": 28}, {"type": "text", "text": "Corollary 4 (Neural network sample complexity guarantee; detailed restatement of Theorem 5). Let $1/e>\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}>0$ , $\\mathcal{D}$ a distribution with $P D F$ $g$ satisfying the following properties: $g$ has full support and is continuously differentiable on $[-1,1]^{m}$ . Moreover, $g$ is of the form ", "page_idx": 28}, {"type": "equation", "text": "$$\ng(x)=\\prod_{j=1}^{L}g_{j}(\\vec{x}_{j}).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let $f^{\\Theta^{*},w^{*}}:[-1,1]^{m}\\to\\mathbb{R}$ be a neural network model produced from Algorithm $^{\\,l}$ trained on data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 28}, {"type": "equation", "text": "$$\nN=\\log(1/\\delta)2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1})+\\mathrm{polylog}(1/\\epsilon_{3}))},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the $x_{\\ell}\\sim\\mathcal{D}$ and $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\,\\le\\,\\epsilon_{2}$ . Suppose that $f^{\\Theta^{*},w^{*}}$ achieves a training error of at most $((\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3})/2$ . Additionally, suppose that all parameters $\\Theta_{i}^{*}$ of $f^{\\Theta^{*},w^{*}}$ satisfy $|\\Theta_{i}^{*}|\\leq W_{\\operatorname*{max}},$ , for some $W_{\\mathrm{max}}>0$ that is independent of the system size $n$ . Then the neural network f \u0398 ,w achieves prediction error ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|f^{\\Theta^{*},w^{*}}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\le2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 28}, {"type": "text", "text": "Moreover, while we can show the existence of suitable parameters that achieve a low training error, quantified by our training objective in Definition 7, in general, we cannot prove that Algorithm 1 converges to parameters close to this optimum because our training objective is not convex. Thus, to obtain the guarantee in Theorem 14, we need to assume that a low training error is indeed achieved by Algorithm 1. This is commonly satisfied in practice. ", "page_idx": 28}, {"type": "text", "text": "Similar to Corollary 3, if we are instead given training data $\\{x_{\\ell},\\sigma_{T}(\\rho(x_{\\ell}))\\}_{\\ell=1}^{N}$ , where $\\sigma_{T}(\\rho(x_{\\ell}))$ is a classical shadow representation [52, 68, 69, 70, 71] of the ground state $\\rho(x_{\\ell})$ , then an immediate corollary of Theorem 14 is that we can predict ground state representations with the same sample complexity. This follows from the same proof as Corollary 5 in [2]. ", "page_idx": 28}, {"type": "text", "text": "Corollary 5 (Learning representations of ground states with neural networks; detailed restatement of Corollary 2). Let $1/e>\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}>0$ and $\\delta>0$ . Given training data $\\{(x_{\\ell},\\sigma_{T}(\\rho(x_{\\ell}))\\}_{\\ell=1}^{N}$ of size ", "page_idx": 28}, {"type": "equation", "text": "$$\nN=\\log(1/\\delta)2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{3})+\\mathrm{polylog}(1/\\epsilon_{1}))},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $x_{\\ell}$ is sampled from a distribution $\\mathcal{D}$ satisfying the same assumptions as Corollary 4 and $\\sigma_{T}(\\rho(x_{\\ell})$ is the classical shadow representation of the ground state $\\rho(x_{\\ell})$ using $T$ randomized Pauli measurements. For $T=\\mathcal{O}(\\log(n N/\\delta)/\\epsilon_{2}^{2})=\\tilde{\\mathcal{O}}(\\log(n/\\delta)/\\epsilon_{2}^{2})$ , the ML algorithm can produce $a$ ground state representation $\\hat{\\rho}_{N,T}(x)$ that achieves ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}|\\operatorname{tr}(O\\hat{\\rho}_{N,T}(x))-\\operatorname{tr}(O\\rho(x))|^{2}\\leq2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with probability at least $1-\\delta$ , for any observable with eigenvalues between $-1$ and 1 that can be written as a sum of geometrically local observables. ", "page_idx": 28}, {"type": "text", "text": "We review low-discrepancy sequences and techniques in quasi-Monte Carlo theory in Appendix A.2, which we use in our proof. To prove Theorem 14, we first show that there exists weights $\\bar{\\Theta}^{\\prime},w^{\\prime}$ such that our proposed neural network $f^{\\Theta^{\\prime},w^{\\prime}}$ achieves a low training error, i.e., it approximates the ground state properties $\\mathrm{tr}(O\\rho(x))$ well. We show this using results in classical deep learning theory about approximating arbitrary functions with neural networks [53]. Then, we use the Koksma-Hlawka inequality (Theorem 10) to bound the prediction error of our model, similarly to [54]. As we want to derive a bound which is independent of the size of the physical system, our approach requires some additional steps. Since the dimension of the input domain of our model depends on the size of the physical system, we cannot treat it as constant as in [54]. Therefore, we bound the prediction error with respect to local functions, whose domain size is independent of the system size. ", "page_idx": 28}, {"type": "text", "text": "Moreover, recall that the Koksma-Hlawka inequality produces a bound in terms of the star-discrepancy (Definition 2) and the Hardy-Krause variation. The star-discrepancy can be bounded by considering low-discrepancy sequences (Definition 3), and the Hardy-Krause variation can be bounded by Equation (A.36). We derive explicit bounds for the Hardy-Krause variation of the ground state properties $\\mathrm{tr}(O\\rho(x))$ , using tools from the spectral flow formalism [59, 60, 61]. To obtain Corollary 4, we follow a similar proof but use results relating the discrepancy with respect to the Lebesgue measure to the discrepancy with respect to arbitrary measures and bounds on the discrepancy of uniformly random points (Appendix A.2). ", "page_idx": 29}, {"type": "text", "text": "In Appendix C.1, we prove that our model approximates the ground state properties well. Then, in Appendix C.2, we use the Koksma-Hlawka inequality to bound the prediction error of our model. Technical results explicitly bounding the mixed partial derivatives of the ground state properties are found in Appendix C.4. We use these in Appendix C.2 to bound the Hardy-Krause variation. We then generalize this to data sampled from different distributions, as in Corollary 4, in Appendix C.3. ", "page_idx": 29}, {"type": "text", "text": "C.1 Approximation of ground state properties by neural networks ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we prove that when choosing the number of parameters and width of the model appropriately, there exists a parameter set for which the deep neural network model approximates the ground state properties well. This shows the existence of a neural network with low training error. The proof is a direct application of the main result from [53], which proves that tanh neural networks can approximate sufficiently smooth functions, in combination with the bounds on the mixed derivative of $\\operatorname{tr}(P\\rho(x))$ we derived in Appendix C.4. ", "page_idx": 29}, {"type": "text", "text": "We consider the local functions defined as in Appendix A.1.2. Namely, define $\\;\\!f(x)\\;\\;=$ $\\textstyle\\sum_{P\\in S^{(\\mathrm{geo})}}\\alpha_{P}f_{P}(x)$ , where $f_{P}(x)=\\mathrm{tr}(P\\rho(\\chi_{P}(x)))$ for $\\begin{array}{r}{O=\\sum_{P\\in\\{I,X,Y,Z\\}\\otimes n}\\alpha_{P}P}\\end{array}$ and ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\chi_{P}(x)_{c}=\\left\\{\\!\\!\\begin{array}{l l}{x_{c},}&{c\\in I_{P}}\\\\ {0}&{c\\notin I_{P}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for all $c\\in\\{1,\\ldots,m\\}$ , for $I_{P}$ defined in Equation (A.2). Note that here, we slightly alter the definition from Appendix A.1.2, where we do not include the coefficient $\\alpha_{P}$ in the definition of $f_{P}(x)$ . Because all parameters with coordinates not in $I_{P}$ are set to 0, we can view $f_{P}$ as a function taking inputs in $[-\\dot{1},1]^{\\tilde{m}}$ , where recall that we use $\\tilde{m}=|I_{P}|$ to denote the number of local coordinates. ", "page_idx": 29}, {"type": "text", "text": "We show that there exists a neural network that can approximate these local functions $f_{P}$ well. In order to apply the result from [53] to approximate $\\mathrm{tr}(P\\rho(\\chi_{P}(x)))$ , we need to transform its inputs, such that it becomes a map $[0,1]^{\\tilde{m}}\\overset{\\cdot\\cdot}{\\rightarrow}\\mathbb{R}$ . Therefore, we introduce an appropriate function $\\tau(x)=2x-1$ . To avoid confusion when considering the different domains $[-1,1]^{m}$ versus $[0,1]^{m}$ , if an input $x\\in[-1,1]^{m}$ , we simply denote it by $x$ . If $x\\in[0,1]^{m}$ , we denote it by $\\textstyle{\\bar{x}}$ . We similarly use this notation for other domain dimensions. ", "page_idx": 29}, {"type": "text", "text": "In the following lemma, we use $W^{k,\\infty}(\\Omega)$ for $\\Omega\\subseteq\\mathbb{R}^{m}$ to denote the Sobolev space ", "page_idx": 29}, {"type": "equation", "text": "$$\nW^{k,\\infty}(\\Omega)\\triangleq\\left\\{f\\in L^{\\infty}(\\Omega):\\frac{\\partial^{|\\alpha|}f}{\\partial x_{1}^{\\alpha_{1}}\\cdot\\cdot\\cdot\\partial x_{m}^{\\alpha_{m}}}\\in L^{\\infty}(\\Omega)\\mathrm{~for~all~}\\alpha\\in\\mathbb{N}_{0}^{m}\\mathrm{~with~}|\\alpha|\\leq k\\right\\}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "so that the Sobolev norm is defined as ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|f\\|_{W^{k,\\infty}(\\Omega)}\\triangleq\\operatorname*{max}_{|\\alpha|=s}\\left\\|\\frac{\\partial^{|\\alpha|}f}{\\partial x_{1}^{\\alpha_{1}}\\cdot\\cdot\\cdot\\partial x_{m}^{\\alpha_{m}}}\\right\\|_{L^{\\infty}(\\Omega)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for $\\alpha\\in\\ensuremath{\\mathbb{N}}_{0}^{m}$ and the $L^{\\infty}$ -norm is defined by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|f\\|_{L^{\\infty}(\\Omega)}=\\operatorname*{sup}_{x\\in\\Omega}\\|f\\|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Lemma 8 (Existence of approximating neural network). Let $\\epsilon_{1}\\;\\;>\\;\\;0,\\;\\;s,M\\;\\;\\in\\;\\;\\mathbb{N}.$ . Let $\\|H(x)\\|_{W^{s,\\infty}([-1,1]^{m})}\\leq1$ . Define functions $f_{P}:[0,1]^{\\tilde{m}}\\to\\mathbb{R}$ as $f_{P}(\\tau(\\bar{x}))=\\mathrm{tr}(P\\rho(\\chi_{P}(\\tau(\\bar{x}))))$ , where $\\tau(\\bar{x})=2\\bar{x}-1$ . Then, there exist neural networks $\\hat{f}_{P}^{M}$ , such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left\\|f_{P}\\circ\\tau-\\hat{f}_{P}^{M}\\right\\|_{L^{\\infty}([0,1]^{\\tilde{m}})}\\leq\\epsilon_{1}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with at most $\\epsilon_{1}^{-\\frac{\\tilde{m}+1}{s}}2^{\\mathcal{O}(\\tilde{m}\\log(\\tilde{m}))}$ parameters. Furthermore, the weights scale as $2^{\\mathrm{poly}(\\log(1/\\epsilon_{1}),\\tilde{m},s)}$ . ", "page_idx": 29}, {"type": "text", "text": "To prove this, we utilize the main result from [53], which states that a neural network with tanh activation functions can approximate effectively any function. ", "page_idx": 30}, {"type": "text", "text": "Theorem 15 (Theorem 5.1 in [53]). Let $d,s\\in\\mathbb{N},$ , $R>0$ , $d>0$ and $f\\in W^{s,\\infty}([0,1]^{d})$ . There exist constants $\\mathcal{C}(d,k,s,f)$ , $N_{0}(d)>0$ , such that for every $N\\in\\ensuremath{\\mathbb{N}}$ with $N>N_{0}(d)$ there exists $a$ tanh neural network $\\hat{f}^{N}$ with two hidden layers, one of width at most $3{\\lceil}\\frac{s}{2}{\\rceil}|P_{s-1,d+1}|+d(N-1)$ (where $|P_{n,d}|={\\binom{n+d-1}{n}},$ and another of width at most $3\\lceil\\frac{d+2}{2}\\rceil|P_{d+1,d+1}|N^{d}\\,(o r\\,3\\lceil\\frac{s}{2}\\rceil+N-1$ and $6N$ for $d=1$ ), such that, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|f-\\hat{f}^{N}\\|_{L^{\\infty}([0,1]^{d})}\\leq(1+\\delta)\\frac{\\mathcal{C}(d,0,s,f)}{N^{s}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and for $k=1,\\ldots,s-1$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|f-\\hat{f}^{N}\\|_{W^{k,\\infty}([0,1]^{d})}\\leq3^{d}(1+\\delta)(2(k+1))^{3k}\\operatorname*{max}\\left\\{R^{k},\\ln^{k}\\left(\\beta N^{s+d+2}\\right)\\right\\}\\frac{\\mathcal{C}(d,k,s,f)}{N^{s-k}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where we define ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\beta=\\frac{k^{3}2^{d}\\sqrt{d}\\operatorname*{max}\\{1,\\|f\\|_{W^{k,\\infty}([0,1]^{d})}^{1/2}\\}}{\\delta\\operatorname*{min}\\{1,\\sqrt{\\mathcal{C}(d,k,s,f)}\\}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "If $f\\in C^{s}([0,1]^{d}),$ , then it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{C}(d,k,s,f)=\\operatorname*{max}_{0\\le l\\le k}\\frac{1}{(s-l)!}\\left(\\frac{3d}{2}\\right)^{s-l}|f|_{W^{s,\\infty}([0,1]^{d})},\\qquad N_{0}(d)=\\frac{3d}{2},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and else it holds that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{C}(d,k,s,f)=\\operatorname*{max}_{0\\leq l\\leq k}\\frac{\\pi^{1/4}\\sqrt{s}}{(s-l-1)!}\\left(5d^{2}\\right)^{s-l}|f|_{W^{s,\\infty}([0,1]^{d})},\\qquad N_{0}(d)=5d^{2}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In addition, the weights of $\\hat{f}^{N}$ scale as $\\mathcal{O}\\Big(\\mathcal{C}^{-s/2}N^{d(d+s^{2}+k^{2})/2}(s(s+2))^{3s(s+2)}\\Big).$ ", "page_idx": 30}, {"type": "text", "text": "The proof of Lemma 8 follows by an application of Theorem 15. ", "page_idx": 30}, {"type": "text", "text": "Proof of Lemma 8. We directly apply Theorem 15, with the input space dimension $\\tilde{m}$ , where $\\tilde{m}$ is the number of local parameters $\\tilde{m}=|I_{P}|$ . By Corollary 8, then we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\left\\|f_{P}\\circ\\tau-\\hat{f}_{P}^{M}\\right\\|_{L^{\\infty}([0,1]^{\\tilde{m}})}\\leq\\frac{(1+\\delta)}{s!}\\left(\\frac{3\\tilde{m}C s^{2}}{2M}\\right)^{s}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We want to show that this is bounded above by $\\epsilon_{1}$ . By rearranging, we find that this holds when ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\epsilon_{1}^{-\\frac{1}{s}}\\left(\\frac{(1+\\delta)}{s!}\\right)^{\\frac{1}{s}}\\frac{3}{2}\\tilde{m}C s^{2}\\leq M.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Note that by composing with $f_{P}$ with $\\tau$ , we acquire an extra factor of $2^{s}$ , which can be considered a component of $C$ . Using $M=\\mathcal{O}(\\epsilon_{1}^{-\\frac{1}{s}}\\tilde{m}s^{2})$ , this results in the widths of the two layers being ", "page_idx": 30}, {"type": "equation", "text": "$$\n3\\left\\lceil\\frac{s}{2}\\right\\rceil\\left(\\!\\!\\!\\begin{array}{c}{{s+\\tilde{m}}}\\\\ {{\\tilde{m}+1}}\\end{array}\\!\\!\\right)+\\tilde{m}(M-1)\\mathrm{~and~}\\left\\lceil\\frac{\\tilde{m}+2}{2}\\right\\rceil\\left(\\!\\!\\!\\begin{array}{c}{{2\\tilde{m}+2}}\\\\ {{d+1}}\\end{array}\\!\\!\\right)M^{\\tilde{m}},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and therefore at most ", "page_idx": 30}, {"type": "equation", "text": "$$\n(c_{1}\\tilde{m})^{c_{2}\\tilde{m}}\\epsilon_{1}^{-\\frac{\\tilde{m}+1}{s}}=2^{\\mathcal{O}(\\tilde{m}(\\log(\\tilde{m})+\\log(1/\\epsilon_{1})/s))}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "trainable weights in the neural network. The constants $c_{1}$ and $c_{2}$ are independent of $\\tilde{m}$ , but may depend on $s$ . By Theorem 15, the weights scale as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathcal{O}\\left(\\mathcal{C}^{-s/2}\\left(\\epsilon_{1}^{-\\frac{1}{s}}\\tilde{m}s^{2}\\right)^{\\tilde{m}(\\tilde{m}+s^{2}+k^{2})/2}(s(s+2))^{3s(s+2)}\\right)=\\epsilon_{1}^{-\\frac{\\tilde{m}+1}{s}}2^{\\mathcal{O}(\\tilde{m}\\log(\\tilde{m}))}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Although the dependence on $s$ is not relevant for our final statement, it is important to comment on the effect of the smoothness of $H(x)$ . The result states that the dependence of the required parameters with respect to $1/\\epsilon_{1}$ improves with the highest degree for which all mixed derivatives of $H(x)$ are bounded. When $H(x)$ is analytic, $s$ can be chosen to be very large so that the number of parameters in the neural network almost scales as $\\mathcal{O}(\\tilde{m}s^{\\log(\\tilde{m})})$ . The constant scales rather poorly with $s$ ; however, this effect is only be visible for very small $\\epsilon_{1}$ . ", "page_idx": 31}, {"type": "text", "text": "Using Lemma 8, we can show that there exist parameters such that the complete model approximates $\\mathrm{tr}(O\\bar{\\rho}(x))$ and obtains a small training objective (defined in Definition 7). The theorem (Theorem 6) in the main text corresponds to $\\epsilon_{1}=0.2\\epsilon$ , $\\epsilon_{2}=\\epsilon$ so that $(\\epsilon_{1}+\\epsilon_{2})^{2}\\leq1.44\\epsilon^{2}\\leq0.53\\epsilon\\leq\\epsilon$ . ", "page_idx": 31}, {"type": "text", "text": "Theorem 16 (Detailed restatement of Theorem 6). For any $1/e>\\epsilon_{1},\\epsilon_{2}>0$ and appropriate width $W$ , there exist weights $\\Theta^{\\prime},w^{\\prime}$ such that the neural network model $f^{\\Theta^{\\prime},w^{\\prime}}$ satisfies ", "page_idx": 31}, {"type": "equation", "text": "$$\n|f^{\\Theta^{\\prime},w^{\\prime}}(x)-\\mathrm{tr}({\\cal O}\\rho(x))|\\le\\epsilon_{1}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for any $x\\in[-1,1]^{m}$ . In particular, for any collection of $N$ training data points $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ with $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\stackrel{\\cdot}{\\leq}\\epsilon_{2}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{1}{N}\\sum_{\\ell=1}^{N}|f^{\\Theta^{\\prime},w^{\\prime}}(x_{\\ell})-y_{\\ell}|^{2}+\\lambda\\|w^{\\prime}\\|_{1}\\leq(\\epsilon_{1}+\\epsilon_{2})^{2}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for a suitable choice of regularization parameter $\\lambda=\\mathcal{O}(\\epsilon_{1}^{2})$ . Moreover, each parameter $\\Theta_{i}$ of the network has a magnitude of |\u0398i| = 2O(polylog(1/\u03f51)). ", "page_idx": 31}, {"type": "text", "text": "Proof. Write $\\begin{array}{r}{O=\\sum_{P}\\alpha_{P}\\operatorname{tr}(P\\rho(x))}\\end{array}$ . By Theorem 8, let $D=\\mathcal{O}(1)$ be a constant such that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{P}|\\alpha_{P}|\\leq D.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "For every Pauli $P$ , then by Lemma 8, there exist weights $\\theta_{P}^{\\prime}$ such that a neural network $\\bar{f}_{P}^{\\theta^{\\prime}}:$ $[0,1]^{\\tilde{m}}\\to\\mathbb{R}$ with two hidden layers as in Definition 6 approximates the local functions $f_{P}(\\tau({\\bar{x}}))=$ $\\operatorname{ir}({\\bar{P}}\\rho(\\chi_{P}(\\tau({\\bar{x}}))))$ , where $\\tau(\\bar{x})\\overset{\\cdot}{=}2\\bar{x}-1$ , up to an error $\\epsilon_{1}/(4D)$ , when the width of their hidden layers is chosen as $W=\\epsilon_{1}^{-\\frac{\\tilde{m}+1}{s}}2^{\\mathcal{O}(\\tilde{m}\\log(\\tilde{m}))}$ , where the number of local coordinates is given by $\\tilde{m}=|I_{P}|$ and by the smoothness assumption Item (d), $s\\geq1$ . In other words, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\bar{f}_{P}^{\\theta_{P}^{\\prime}}(\\bar{x})-\\mathrm{tr}(P\\rho(\\chi_{P}(\\tau(\\bar{x}))))\\right|\\leq\\frac{\\epsilon_{1}}{4D},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\bar{x}\\in[0,1]^{\\tilde{m}}$ . Because $\\tau$ is simply a coordinate transformation, then we also obtain ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|f_{P}^{\\theta_{P}^{\\prime}}(x)-\\mathrm{tr}(P\\rho(\\chi_{P}(x)))\\right|\\leq\\frac{\\epsilon_{1}}{4D},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for $x\\in[-1,1]^{\\tilde{m}}$ . ", "page_idx": 31}, {"type": "text", "text": "Furthermore, by Lemma 2 in Appendix A.1.2, the sum of the local functions $\\begin{array}{r}{f(x)=\\sum_{P}\\alpha_{P}f_{P}(x)}\\end{array}$ approximates the ground state property $\\begin{array}{r}{\\mathrm{tr}(O\\rho(x))=\\sum_{P}\\alpha_{P}\\,\\mathrm{tr}(P\\rho(x))}\\end{array}$ well. In particular, combining Lemma 2 with Theorem 8, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left|\\sum_{P}\\alpha_{P}\\operatorname{tr}(P\\rho(\\chi_{P}(x)))-\\sum_{P}\\alpha_{P}\\operatorname{tr}(P\\rho(x))\\right|\\leq{\\frac{\\epsilon_{1}}{4}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "This holds when choosing the local radius $\\delta_{1}$ (defined in Equation (A.5)) to be $\\delta_{1}=4C\\log^{2}(1/\\epsilon_{1})$ for some constant $C$ . This implies that $\\tilde{m}=|I_{P}|=\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))$ (e.g., Equation (S33) of [2]). Thus, for the model $f^{\\Theta^{\\prime},w^{\\prime}}$ with architecture defined in Definition 6 and weights $w_{P}^{\\prime}=\\alpha_{P}$ and $\\Theta^{\\prime}=\\{\\theta_{P}^{\\prime}\\}_{P}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle|f^{\\Theta^{\\prime},w^{\\prime}}(x)-\\mathrm{tr}(O\\rho(x))|}}\\\\ {{\\displaystyle=\\left|\\sum_{P}\\alpha_{P}f_{P}^{\\theta_{P}^{\\prime}}(x)-\\sum_{P}\\mathrm{tr}(P\\rho(\\chi_{P}(x)))+\\sum_{P}\\mathrm{tr}(P\\rho(\\chi_{P}(x)))-\\sum_{P}\\mathrm{tr}(P\\rho(x))\\right|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\sum_{P}|\\alpha_{P}|\\frac{\\epsilon_{1}}{4D}+\\left|\\displaystyle\\sum_{P}\\mathrm{tr}(P\\rho(\\chi_{P}(x)))-\\sum_{P}\\mathrm{tr}(P\\rho(x))\\right|}\\\\ &{\\leq\\displaystyle\\frac{\\epsilon_{1}}{4}+\\left|\\displaystyle\\sum_{P}\\mathrm{tr}(P\\rho(\\chi_{P}(x)))-\\sum_{P}\\mathrm{tr}(P\\rho(x))\\right|}\\\\ &{\\leq\\displaystyle\\frac{\\epsilon_{1}}{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Moreover, by definition of the training data, we have $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\,\\le\\,\\epsilon_{2}$ . Thus, by triangle inequality and choosing regularization parameter $\\lambda=\\epsilon_{1}^{2}/(2D)$ , we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac1N\\sum_{\\ell=1}^{N}|f^{\\mathrm{e}^{\\prime},w^{\\prime}}(x_{\\ell})-y_{\\ell}|^{2}+\\lambda\\|w^{\\prime}\\|_{1}}\\\\ {\\displaystyle=\\frac1N\\sum_{\\ell=1}^{N}|f^{\\mathrm{e}^{\\prime},w^{\\prime}}(x_{\\ell})-\\mathrm{tr}(O\\rho(x_{\\ell}))+\\mathrm{tr}(O\\rho(x_{\\ell}))-y_{\\ell}|^{2}+\\lambda\\|w^{\\prime}\\|_{1}}\\\\ {\\le(\\epsilon_{1}/2+\\epsilon_{2})^{2}+\\lambda\\|w^{\\prime}\\|_{1}}\\\\ {\\displaystyle\\le\\left(\\frac{\\epsilon_{1}}{2}+\\epsilon_{2}\\right)^{2}+\\frac{\\epsilon_{1}^{2}}{2}}\\\\ {\\le(\\epsilon_{1}+\\epsilon_{2})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Finally, plugging in $\\tilde{m}=\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))$ , by Lemma 8, then we obtain $|\\Theta_{i}|=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1}))}$ , as required. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "C.2 Prediction error bound ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "In this section, we derive our result on the prediction error to complete the proof of Theorem 14. The central result we use is the Koksma-Hlawka inequality (Theorem 10) from quasi-Monte Carlo theory, which produces a bound in terms of the star-discrepancy (Definition 2) and the Hardy-Krause variation (Equation (A.36)). We review these tools in Appendix A.2. To bound the star-discrepancy, we consider a specific low-discrepancy sequence with guarantees described in Appendix A.2. The Hardy-Krause variation can be bounded by considering the mixed derivatives of our target function $\\mathrm{tr}(O\\dot{\\rho}(x))$ and our neural network model. We relegate the bounds on the mixed derivatives of $\\mathrm{tr}(O\\rho(x))$ to Appendix C.4, as the discussion is fairly technical. To bound the mixed derivatives of our model, we consider the following lemma. ", "page_idx": 32}, {"type": "text", "text": "Lemma 9 (Bound on mixed derivatives of neural network). Let $k,d\\in\\mathbb{N}.$ . Let $\\hat{f}:[-1,1]^{d}\\to\\mathbb{R}$ be $a$ tanh neural network with two hidden layers of width $W\\geq d$ and maximal weight $W_{\\mathrm{max}}$ . Then ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\hat{f}\\|_{W^{k,\\infty}([-1,1]^{d})}=2^{\\mathcal{O}(k^{2}\\log(k)+k\\log(d W W_{\\operatorname*{max}}))}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Recall that a tanh deep neural network with two hidden layers is defined as a function $\\hat{f}:[-1,1]^{d}\\to\\mathbb{R}$ such that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{f}(x)=(W_{\\mathrm{out}}\\circ\\operatorname{tanh}\\circ W_{\\mathrm{hidden}}\\circ\\operatorname{tanh}\\circ W_{\\mathrm{in}})(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the activation function tanh is applied element-wise. Note that this result holds for any tanh neural network with two hidden layers, where this neural network does not necessarily have to be the same model as Definition 6. ", "page_idx": 32}, {"type": "text", "text": "Let $W_{L}\\,\\in\\,\\{W_{\\mathrm{in}},W_{\\mathrm{hidden}},W_{\\mathrm{out}}\\}$ denote the layers of the neural network that perform an affine transformation for $L\\in\\{\\mathrm{in,hidden,out}\\}$ . We can also use $L\\in\\{0,1,2\\}$ , where 0 corresponds to in, 1 corresponds to hidden, and 2 corresponds to out. Let $d_{L}$ denote the width (number of input neurons) in each layer, where we define $d_{0}=d_{\\mathrm{in}},d_{1}=d_{2}=W,d_{3}=d_{\\mathrm{out}}$ . In this way, $W_{L}:\\mathbb{R}^{d_{L}^{\\star}}\\rightarrow\\mathbb{R}^{d_{L+1}}$ for $L\\in\\{0,1,2\\}$ . Explicitly, we have $W_{\\mathrm{in}}:\\mathbb{R}^{d_{\\mathrm{in}}}\\to\\mathbb{R}^{W}$ , $W_{\\mathrm{hidden}}:\\mathbb{R}^{W}\\to\\mathbb{R}^{W}$ , $W_{\\mathrm{out}}:\\mathbb{R}^{W}\\to\\mathbb{R}^{d_{\\mathrm{out}}}$ . Since $W_{L}$ performs an affine transformation, we can write it has $W_{L}(x)=(f_{1}(x),\\dots,f_{d_{L+1}}(x))$ , where $\\boldsymbol{x}\\in\\mathbb{R}^{d_{L}}$ and $f_{i}$ are linear functions $f_{i}(\\boldsymbol{x})=w_{i}^{\\mathsf{T}}\\boldsymbol{x}+b_{i}$ for $w_{i}\\in\\mathbb{R}^{d_{L}},b_{i}\\in\\mathbb{R}$ . For these linear layers, we observe for any function $g:\\mathbb{R}^{d_{g}}\\rightarrow\\mathbb{R}^{d_{L}}$ with input dimension $d_{g}$ and for $L\\in\\{0,1,2\\}$ , we have ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\operatorname*{max}_{1\\leq i\\leq d_{L+1}}\\|(W_{L}\\circ g)_{i}\\|_{W^{k,\\infty}([-1,1]^{d})}=\\displaystyle\\operatorname*{max}_{1\\leq i\\leq d_{L+1}}\\|f_{i}(g(x))\\|_{W^{k,\\infty}([-1,1]^{d})}}&{}\\\\ {\\displaystyle}&{\\leq\\displaystyle\\sum_{i=1}^{d_{L+1}}\\|w_{i}^{T}g(x)+b_{i}\\|_{W^{k,\\infty}([-1,1]^{d})}}\\\\ &{\\leq\\displaystyle\\operatorname*{max}_{1\\leq j\\leq d_{g}}\\|W_{L}\\|_{1}\\|g(x)_{j}\\|_{W^{k,\\infty}([-1,1]^{d})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where we use the notation ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|W_{L}\\|_{1}\\triangleq\\sum_{i=1}^{d_{L+1}}\\left(|b_{i}|+\\sum_{j=1}^{d_{L}}|w_{i,j}|\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $w$ is a matrix with rows given by the vectors $w_{i}^{\\mathsf{T}}$ , $w_{i}\\in\\mathbb{R}^{d_{L}}$ . To show this inequality, we used H\u00f6lder\u2019s inequality in the last step. With this, by factoring out one layer $W_{L}$ at a time, we can bound the Sobolev norm of $\\hat{f}$ . In particular, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\hat{f}\\|_{W^{k,\\infty}([-1,1]^{d})}}\\\\ &{=\\|(W_{\\mathrm{out}}\\circ\\mathrm{tanh~o~}W_{\\mathrm{hidden}}\\circ\\mathrm{tanh}\\circ W_{\\mathrm{in}})(x)\\|_{W^{k,\\infty}([-1,1]^{d})}}\\\\ &{\\leq\\|W_{\\mathrm{out}}\\|_{1}\\underset{1\\leq j\\leq W}{\\operatorname*{max}}\\|(\\mathrm{tanh}\\circ W_{\\mathrm{hidden}}\\circ\\mathrm{tanh}\\circ W_{\\mathrm{in}})_{j}\\|_{W^{k,\\infty}([-1,1]^{d})}}\\\\ &{\\leq\\|W_{\\mathrm{out}}\\|_{1}16(e^{2}k^{4}d^{2})^{k}(2k)^{k(k+1)}\\underset{1\\leq j\\leq W}{\\operatorname*{max}}\\|(W_{\\mathrm{hidden}}\\circ\\mathrm{tanh}\\circ W_{\\mathrm{in}})_{j}\\|_{W^{k,\\infty}([-1,1]^{d})}^{k}}\\\\ &{\\leq\\|W_{\\mathrm{out}}\\|_{1}\\|W_{\\mathrm{hidden}}\\|_{1}^{k}\\cdot16(e^{2}k^{4}d^{2})^{k}(2k)^{k(k+1)}\\underset{1\\leq j\\leq W}{\\operatorname*{max}}\\|(\\mathrm{tanh}\\circ W_{\\mathrm{in}})_{j}\\|_{W^{k,\\infty}([-1,1]^{d})}^{k}}\\\\ &{\\leq\\|W_{\\mathrm{out}}\\|_{1}\\|W_{\\mathrm{hidden}}\\|_{1}^{k}\\cdot16^{2}(e^{2}k^{4}d^{2})^{2k}(2k)^{2k(k+1)}\\|W_{\\mathrm{in}}\\|_{1}^{k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "In the second line, we used Equation (C.47). In the third line, we used the two following inequalities: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|{\\frac{d^{m}}{d x^{m}}}\\operatorname{tanh}(x)\\right|\\leq(2m)^{m+1}\\operatorname*{min}\\{\\exp(-2x),\\exp(2x)\\}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for all $x\\in\\mathbb{R},m\\in\\mathbb{N}$ (see Lemma A.4 in [53]), and ", "page_idx": 33}, {"type": "equation", "text": "$$\n||g\\circ f||_{W^{n,\\infty}}\\leq16(e^{2}n^{4}m d^{2})^{n}||g||_{W^{n,\\infty}}\\operatorname*{max}_{1\\leq i\\leq m}||(f)_{i}||_{W^{n,\\infty}}^{n}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "for any functions $f\\,\\in\\,C^{n}(\\Omega_{1};\\Omega_{2})$ and $g\\,\\in\\,C^{n}(\\Omega_{2};\\mathbb{R})$ defined on $\\Omega_{1}\\,\\subset\\,\\mathbb{R}^{d},\\,\\Omega_{2}\\,\\subset\\,\\mathbb{R}^{m}$ with $d,m,n\\in\\mathbb{N}$ (see Lemma A.7 in [53]). In the fourth and fifth lines, we used Equation (C.47) and these inequalities again. Furthermore, we used that our inputs are absolutely bounded by 1 in the last step. ", "page_idx": 33}, {"type": "text", "text": "We can further bound this term using that $W_{\\mathrm{max}}$ is the maximal weight of $\\hat{f}$ and the width of the hidden layers is lower bounded by $W\\geq d$ . ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|\\hat{f}\\|_{W^{k,\\infty}([-1,1]^{d})}\\leq16^{2}(e^{2}k^{4}d^{2})^{2k}(2k)^{2k(k+1)}W_{\\operatorname*{max}}^{2k+1}W^{3k+1}d^{k}=2^{\\mathcal{O}(k(k\\log(k)+\\log(d W W_{\\operatorname*{max}})))}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Now we have all the necessary tools in order to derive a bound on the generalization error for our neural network model of the form given in Definition 6. In our proof, we first bound the prediction error in terms of functions with $2\\tilde{m}$ -dimensional domain and on which we can directly apply the Koksma-Hlawka inequality. Then, we use the previous result to obtain an explicit bound. Due to the regularity of the parameters $\\alpha_{P}$ and our model parameters $w_{P}$ , this prediction error bound is independent of the system size $n$ . ", "page_idx": 33}, {"type": "text", "text": "Before stating the formal result bounding the prediction error, we introduce some notation. We define the prediction error of a neural network $f^{\\Theta,{\\bar{w}}}$ with weights given by $\\Theta,w$ as ", "page_idx": 33}, {"type": "equation", "text": "$$\nR(\\Theta)\\triangleq\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}|f^{\\Theta,w}(x)-\\mathrm{tr}(O\\rho(x))|^{2},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where in our case, $x\\sim U[-1,1]^{m}$ denotes $x$ sampled from a uniform distribution over $[-1,1]^{m}$ . We suppress $w$ in the notation to avoid cluttering. For a neural network $f^{\\Theta,w}$ generated from training on some data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ , we can define the training error as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{R}(\\Theta)\\triangleq\\frac{1}{N}\\sum_{\\ell=1}^{N}|f^{\\Theta,w}(x_{\\ell})-y_{\\ell}|^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Moreover, as in our analysis in Appendix C.1, we rely on an approximation of the ground state property $\\mathrm{tr}(O\\rho(x))$ by a sum of smooth local functions $\\dot{\\sum}_{P}\\,\\alpha_{P}\\bar{f_{P}(x)}$ (Lemma 2). Namely, combining Lemma 2 and Theorem 8, we have that for $\\epsilon_{1}>0$ ,  then choosing $\\delta_{1}>0$ as in Equation (A.5), i.e., $\\delta_{1}=\\mathcal{O}(\\log^{2}(1/\\epsilon_{1}))$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\sum_{P}\\alpha_{P}f_{P}(x)-\\mathrm{tr}(O\\rho(x))\\right|\\leq{\\frac{\\epsilon_{1}}{2}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that here, again, we slightly alter the definition from Appendix A.1.2, where we do not include the coefficient $\\alpha_{P}$ in the definition of $f_{P}(x)$ . With these definitions, we have the following guarantee on the prediction error. ", "page_idx": 34}, {"type": "text", "text": "Lemma 10 (Prediction error bound). Let $1/e\\;>\\;\\epsilon_{1},\\epsilon_{2}\\;>\\;0.$ . Consider a tanh neural network $f^{\\Theta,w}:[-1,\\dot{1}]^{m}\\to\\mathbb{R}$ with architecture defined in Definition 7 with weights $\\Theta_{i}\\leq W_{\\operatorname*{max}}$ for some $W_{\\mathrm{max}}>0$ independent of the system size n and weights w in the last layer. Suppose we train $f^{\\Theta,w}$ on data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size $N$ , where the $x_{\\ell}$ \u2019s form a low-discrepancy sequence with star-discrepancy $D_{N}^{*}$ and $|y_{\\ell}-\\mathrm{tr}(\\mathsf{\\bar{O}}\\rho(x_{\\ell}))|\\leq\\epsilon_{2}$ . Then, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nR(\\Theta)\\le\\hat{R}(\\Theta)+\\frac{\\epsilon_{1}^{2}}{2}+\\epsilon_{2}^{2}+(\\|w\\|_{1}+\\|w\\|_{1}^{2})D_{N}^{*}\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(W W_{\\operatorname*{max}}/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\tilde{m}=|I_{P}|=\\mathcal{O}(\\log^{2}(1/\\epsilon_{1}))$ for $I_{P}$ defined in Equation (A.2). ", "page_idx": 34}, {"type": "text", "text": "Proof. Recall the definition of our neural network model in Definition 6. In particular, our model is given by a function $f^{\\Theta,w}:[-1,1]^{m}\\to\\mathbb{R}$ defined by ", "page_idx": 34}, {"type": "equation", "text": "$$\nf^{\\Theta,w}(x)=\\sum_{P\\in S^{(\\mathrm{geo})}}w_{P}f_{P}^{\\theta_{P}}(x),\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we refer to $f_{P}^{\\theta_{P}}\\,:\\,[-1,1]^{\\tilde{m}}\\,\\to\\,\\mathbb{R}$ as the local models. For $f_{P}(x)\\;=\\;\\mathrm{tr}(P\\rho(\\chi_{P}(x)))$ as considered in Equation (C.60), we can define the following quantities. Define the training error with respect to this local approximation by ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\hat{R}_{\\mathrm{loc}}(\\Theta)\\triangleq\\frac{1}{N}\\sum_{\\ell=1}^{N}\\left|f^{\\Theta,w}(x_{\\ell})-\\sum_{P}\\alpha_{P}f_{P}(x_{\\ell})\\right|^{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Also define the prediction error with respect to the local approximation as ", "page_idx": 34}, {"type": "equation", "text": "$$\nR_{\\mathrm{loc}}(\\Theta)\\triangleq\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}\\left|f^{\\Theta,w}(x)-\\sum_{P}\\alpha_{P}f_{P}(x)\\right|^{2},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $x\\sim U[-1,1]^{m}$ means that $x$ is sampled according to the uniform distribution. ", "page_idx": 34}, {"type": "text", "text": "By Lemma 2, for our choice of $\\delta_{1}$ , we have Equation (C.60): ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left|\\sum_{P}\\alpha_{P}f_{P}(x)-\\mathrm{tr}(O\\rho(x))\\right|\\leq{\\frac{\\epsilon_{1}}{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By the triangle inequality, we can bound the prediction error as ", "page_idx": 34}, {"type": "equation", "text": "$$\nR(\\Theta)=\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}\\left|f^{\\Theta,w}(x)-\\sum_{P}\\alpha_{P}f_{P}(x)+\\sum_{P}\\alpha_{P}f_{P}(x)-\\mathrm{tr}(O\\rho(x))\\right|^{2}\\leq R_{\\mathrm{loc}}(\\Theta)+\\frac{\\epsilon_{1}^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By applying the reverse triangle inequality, we can further bound this as ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle R(\\Theta)\\leq\\frac{e_{1}^{2}}{4}+\\hat{R}_{\\mathrm{loc}}(\\Theta)+|R_{\\mathrm{loc}}(\\Theta)-\\hat{R}_{\\mathrm{loc}}(\\Theta)|\\,}\\\\ {\\displaystyle\\qquad=\\frac{e_{1}^{2}}{4}+\\hat{R}_{\\mathrm{loc}}(\\Theta)\\~}\\\\ {\\displaystyle\\qquad+\\left|\\sum_{x\\sim U[-1,1]^{\\mathrm{loc}}}\\left|f^{\\Theta,w}(x)-\\sum_{P}\\alpha_{P}f_{P}(x)\\right|^{2}-\\frac{1}{N}\\sum_{\\ell=1}^{N}\\left|f^{\\Theta,w}(x_{\\ell})-\\alpha_{P}f_{P}(x_{\\ell})\\right|^{2}\\right|\\qquad\\displaystyle(\\mathbb{C}\\xi\\otimes f^{\\Theta})\\left(\\left|f^{\\Theta,w}(x_{\\ell})-\\alpha_{P}f_{P}(x_{\\ell})\\right|^{2}\\right)}\\\\ {\\displaystyle\\qquad=\\frac{e_{1}^{2}}{4}+\\hat{R}_{\\mathrm{loc}}(\\Theta)\\~}\\\\ {\\displaystyle\\qquad+\\left|\\sum_{x\\sim U[-1,1]^{\\mathrm{loc}}}\\left(\\sum_{P}w_{P}f_{P}^{\\Theta_{P}}(x)-\\alpha_{P}f_{P}(x)\\right)^{2}-\\frac{1}{N}\\sum_{\\ell=1}^{N}\\left(\\sum_{P}w_{P}f_{P}^{\\Theta_{P}}(x_{\\ell})-\\alpha_{P}f_{P}(x_{\\ell})\\right)^{2}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We can expand the term in the expectation/sum as follows ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\displaystyle\\sum_{P}w_{P}f_{P}^{\\theta_{P}}(x)-\\alpha_{P}f_{P}(x)\\right)^{2}}\\\\ &{\\quad=\\left(\\displaystyle\\sum_{P}w_{P}f_{P}^{\\theta_{P}}(x)\\right)^{2}-2\\left(\\displaystyle\\sum_{P}w_{P}f_{P}^{\\theta_{P}}(x)\\right)\\left(\\sum_{P}\\alpha_{P}f_{P}(x)\\right)+\\left(\\sum_{P}\\alpha_{P}f_{P}(x)\\right)^{2}}\\\\ &{=\\displaystyle\\sum_{P_{1},P_{2}}w_{P_{1}}f_{P_{1}}^{\\theta_{P_{1}}}(x)w_{P_{2}}f_{P_{2}}^{\\theta_{P_{2}}}(x)-2w_{P_{1}}f_{P_{1}}^{\\theta_{P_{1}}}(x)\\alpha_{P_{2}}f_{P_{2}}(x)+\\alpha_{P_{1}}f_{P_{1}}(x)\\alpha_{P_{2}}f_{P_{2}}(x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Plugging this into the absolute value term in Equation (C.71) and upper bounding it with the triangle inequality, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{R_{\\mathrm{loc}}(\\Theta)-\\hat{R}_{\\mathrm{loc}}(\\Theta)|\\leq\\displaystyle\\sum_{P_{1},P_{2}}|w_{P_{1}}||w_{P_{2}}|\\left|\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}[f_{P_{1}}^{\\theta_{P_{1}}}(x)f_{P_{2}}^{\\theta_{P_{2}}}(x)]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(x_{\\ell})f_{P_{2}}^{\\theta_{P_{2}}}(x_{\\ell})\\right|}}\\\\ &{+\\left.2|w_{P_{1}}|\\alpha_{P_{2}}\\right|\\left|\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}[f_{P_{1}}^{\\theta_{P_{1}}}(x)f_{P_{2}}(x)]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(x_{\\ell})f_{P_{2}}(x_{\\ell})\\right|}\\\\ &{+\\left.|\\alpha_{P_{1}}||\\alpha_{P_{2}}\\right|\\left|\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}[f_{P_{1}}(x)f_{P_{2}}(x)]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}(x_{\\ell})f_{P_{2}}(x_{\\ell})\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Notice that in the expectation over $[-1,1]^{m}$ , we can replace this by an expectation over the set of local parameters, i.e., the parameters with coordinates in $I_{P_{1}}\\cup I_{P_{2}}$ , which we denote by $S_{P_{1},P_{2}}$ This is because the functions in the expectations are local functions that only depend on these local parameters. The dimension of the domain we integrate over thus becomes independent of the system size $n$ , as $|S_{P_{1},P_{2}}|\\leq2\\tilde{m}=2|I_{P}|$ . ", "page_idx": 35}, {"type": "text", "text": "We can now bound this term further using the Koksma-Hlawka inequality (Theorem 10). We apply a simple variable transformation $\\tau(x)\\,=\\,2x\\mathrm{~-~}1$ so that the domain of $f_{P}\\circ\\tau$ becomes $[0,\\dot{1}]^{\\tilde{m}}$ . Furthermore, we denote the domain associated with $S_{P_{1},P_{2}}$ by $\\Omega_{P_{1},P_{2}}\\triangleq[0,1]^{|S_{P_{1},P_{2}}|}$ . Starting with the first term in Equation (C.75), we obtain ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\left\\lvert{\\hphantom{-}\\mathbb{E}_{\\sim U[0,1]^{m}}\\big[f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}))\\big]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}_{\\ell}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}_{\\ell}))}\\right\\rvert}}\\\\ {{\\displaystyle=\\left\\lvert{\\hphantom{-}\\mathbb{E}_{\\sim U(\\Omega_{P_{1},P_{2}})}\\big[f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}))\\big]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}_{\\ell}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}_{\\ell}))}\\right\\rvert}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\Bigg|\\displaystyle\\int_{S_{P_{1},P_{2}}}f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}))d x-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}_{\\ell}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}_{\\ell}))\\Bigg|}\\\\ &{\\le D_{N}^{*}(2\\tilde{m})V_{H K}\\left((f_{P_{1}}^{\\theta_{P_{1}}}\\cdot f_{P_{2}}^{\\theta_{P_{2}}})\\circ\\tau\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\bar{x}_{\\ell}=\\tau^{-1}(x_{\\ell})$ , such that Equation (C.76) matches the expression referenced in Equation (C.75). Note that we also applied in the last step that the star-discrepancy is increasing with respect to the dimension of the sequence. By application of the chain rule and the Cauchy-Schwartz inequality in the definition of the Hardy-Krause variation (Equation (A.36)), it is easy to see that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V_{H K}\\left(\\big(f_{P_{1}}^{\\theta_{P_{1}}}\\cdot f_{P_{2}}^{\\theta_{P_{2}}}\\big)\\circ\\tau\\right)\\leq2^{2\\tilde{m}}V_{H K}\\big(f_{P_{1}}^{\\theta_{P_{1}}}\\cdot f_{P_{2}}^{\\theta_{P_{2}}}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For all subsets $S^{\\prime}\\subseteq S_{P_{1},P_{2}}$ , applying the product rule yields ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\partial^{|S^{\\prime}|}}{\\partial x_{S^{\\prime}}}}(f_{P_{1}}^{\\theta_{P_{1}}}\\cdot f_{P_{2}}^{\\theta_{P_{2}}})\\right|\\leq\\sum_{A\\subseteq S^{\\prime}}\\left|{\\frac{\\partial^{|A|}}{\\partial x_{A}}}f_{P_{1}}^{\\theta_{P_{1}}}\\right|\\left|{\\frac{\\partial^{|S^{\\prime}\\backslash A|}}{\\partial x_{S^{\\prime}\\backslash A}}}f_{P_{2}}^{\\theta_{P_{2}}}\\right|=2^{\\mathcal{O}(\\tilde{m}\\log(W W_{\\operatorname*{max}})+\\tilde{m}^{2}\\log(\\tilde{m}))},\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last equality follows from applying Lemma 9 from Appendix C.4 with $d=k=2\\tilde{m}$ and $|\\{A:A\\subseteq S^{\\prime}\\}|=2^{2{\\tilde{m}}}$ . Here, we are using the notation $\\frac{\\partial^{|B|}}{\\partial{{x}_{B}}}$ to denote the mixed derivative with respect to all parameters $x_{i}\\in B$ for some set $B$ . Thus, applying Lemma 9 again, we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\nV_{H K}(f_{P_{1}}^{\\theta_{P_{1}}}\\cdot f_{P_{2}}^{\\theta_{P_{2}}})\\leq\\sum_{S^{\\prime}\\subseteq S_{P_{1},P_{2}}}\\left|\\frac{\\partial^{|S^{\\prime}|}}{\\partial x_{S^{\\prime}}}(f_{P_{1}}^{\\theta_{P_{1}}}\\cdot f_{P_{2}}^{\\theta_{P_{2}}})\\right|=2^{\\mathcal{O}(\\tilde{m}\\log(W W_{\\operatorname*{max}})+\\tilde{m}^{2}\\log(\\tilde{m}))}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, putting it all together, we see that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}[f_{P_{1}}^{\\theta_{P_{1}}}(x)f_{P_{2}}^{\\theta_{P_{2}}}(x)]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(x_{\\ell})f_{P_{2}}^{\\theta_{P_{2}}}(x_{\\ell})\\right|}\\\\ &{\\leq2^{2\\tilde{m}}D_{N}^{*}(2\\tilde{m})2^{\\mathcal{O}(\\tilde{m}\\log(W W_{\\operatorname*{max}})+\\tilde{m}^{2}\\log(\\tilde{m}))}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The remaining terms in Equation (C.75) can be bounded similarly using Lemma 22 from Appendix C.4. This lemma is applicable to $f_{P}$ because the derivatives are with respect to the local parameters. In this way, we can upper bound Equation (C.75) by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|R_{\\mathrm{loc}}(\\Theta)-\\hat{R}_{\\mathrm{loc}}(\\Theta)|\\leq\\displaystyle\\sum_{P_{1},P_{2}}\\Big((|w_{P_{1}}||w_{P_{2}}|+|w_{P_{1}}||\\alpha_{P_{2}}|)2^{\\mathcal{O}(\\tilde{m}\\log(W W_{\\mathrm{max}})+\\tilde{m}^{2}\\log(\\tilde{m}))}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+|\\alpha_{P_{1}}||\\alpha_{P_{2}}|2^{\\mathcal{O}(\\tilde{m}\\log(\\tilde{m}))}\\Big)\\,D_{N}^{*}(2\\tilde{m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Plugging this back in to Equation (C.67), we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\Theta)\\leq\\frac{\\epsilon_{1}^{2}}{4}+\\hat{R}_{\\mathrm{loc}}(\\Theta)+\\displaystyle\\sum_{P_{1},P_{2}}\\left((|w_{P_{1}}||w_{P_{2}}|+|w_{P_{1}}||\\alpha_{P_{2}}|)2^{\\mathcal{O}(\\tilde{m}\\log(W W_{\\operatorname*{max}})+\\tilde{m}^{2}\\log(\\tilde{m}))}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+|\\alpha_{P_{1}}||\\alpha_{P_{2}}|2^{\\mathcal{O}(\\tilde{m}\\log(\\tilde{m}))}\\right)D_{N}^{*}(2\\tilde{m}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Lastly, we can bound $\\hat{R}_{\\mathrm{loc}}(\\Theta)\\leq\\epsilon_{1}^{2}/4+\\epsilon_{2}^{2}+\\hat{R}(\\Theta)$ in the same way as in Equation (C.66): ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\hat{R}_{\\mathrm{loc}}(\\Theta)=\\frac{1}{N}\\sum_{\\ell=1}^{N}\\bigg|f^{\\Theta,w}(\\boldsymbol{x}_{\\ell})-\\sum_{P}\\alpha_{P}f_{P}\\big(\\chi_{P}(\\boldsymbol{x}_{\\ell})\\big)\\bigg|^{2}}}\\\\ &{}&{\\leq\\frac{1}{N}\\sum_{\\ell=1}^{N}\\left|f^{\\Theta,w}(\\boldsymbol{x}_{\\ell})-y_{\\ell}\\right|^{2}+|y_{\\ell}-\\mathrm{tr}({O}\\rho(\\boldsymbol{x}_{\\ell}))|^{2}+\\left|\\mathrm{tr}({O}\\rho(\\boldsymbol{x}_{\\ell}))-\\sum_{P}\\alpha_{P}f_{P}\\big(\\chi_{P}(\\boldsymbol{x}_{\\ell})\\big)\\right|^{2}}\\\\ &{}&{\\stackrel{}{\\mathrm{(C.90~\\!~\\!-\\!\\!~\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}}}\\\\ &{}&{\\leq\\hat{R}(\\Theta)+\\epsilon_{2}^{2}+\\frac{\\epsilon_{1}^{2}}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Inserting $\\tilde{m}=\\left|I_{P}\\right|=\\mathcal{O}\\left(\\mathrm{polylog}\\left(1/\\epsilon_{1}\\right)\\right)$ and using that $\\begin{array}{r}{\\sum_{P}|\\alpha_{P}|=\\mathcal{O}(1)}\\end{array}$ (Theorem 8) yields ", "page_idx": 37}, {"type": "equation", "text": "$$\nR(\\Theta)\\leq\\hat{R}(\\Theta)+\\frac{\\epsilon_{1}^{2}}{2}+\\epsilon_{2}^{2}+(\\|w\\|_{1}+\\|w\\|_{1}^{2})D_{N}^{*}(2\\tilde{m})2^{\\mathcal{O}(\\mathrm{polylog}(W W_{\\operatorname*{max}}/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using the previous result and the results from low-discrepancy theory (see Appendix A.2 for a review), we can now show that Algorithm 1 will, under mild assumptions for training, output a model, which yields low prediction error. Thus, using Lemma 10, we can easily prove Theorem 14. ", "page_idx": 37}, {"type": "text", "text": "Proof of Theorem 14. By Theorem 9, we know that for Sobol sequences in base 2 with points in $[0,1]^{d}$ , the star-discrepancy is bounded by ", "page_idx": 37}, {"type": "equation", "text": "$$\nD_{N}^{*}(d)\\leq C(d)\\frac{\\log(N)^{d}}{N},\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $C(d)$ is a constant such that ", "page_idx": 37}, {"type": "equation", "text": "$$\nC(d)<\\frac{1}{d!}\\left(\\frac{d}{\\log(2d)}\\right).\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since $C(d)=o(1)$ , there exists a constant $C$ , such that $C\\geq C(d)$ for all $d>0$ . In our case, $d=2\\tilde{m}$ , so we have ", "page_idx": 37}, {"type": "equation", "text": "$$\nD_{N}^{*}(2\\tilde{m})\\leq C\\frac{\\log(N)^{2\\tilde{m}}}{N}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Using the assumption that the training objective is not larger than $((\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3})/2$ , by Lemma 10, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\Theta^{*})=\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}|f^{\\Theta^{*},w^{*}}(x)-\\mathrm{tr}(O\\rho(x))|^{2}}\\\\ &{\\qquad\\le\\frac{\\epsilon_{1}^{2}}{2}+\\epsilon_{2}^{2}+\\frac{\\left(\\epsilon_{1}+\\epsilon_{2}\\right)^{2}+\\epsilon_{3}}{2}+C^{\\prime}\\frac{\\log(N)^{\\mathrm{polylog}(1/\\epsilon_{1})}2^{O(\\mathrm{polylog}(W_{\\mathrm{max}}/\\epsilon_{1}))}}{N}}\\\\ &{\\qquad\\le2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\frac{\\epsilon_{3}}{2}+C^{\\prime}\\frac{\\log(N)^{\\mathrm{polylog}(1/\\epsilon_{1})}2^{\\mathcal{O}(\\mathrm{polylog}(W_{\\mathrm{max}}/\\epsilon_{1}))}}{N},}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where $C^{\\prime}$ is a constant. We also used here that $\\tilde{m}=|I_{P}|=\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1})$ . Since the training data has size $N=\\mathcal{O}\\left(2^{\\mathrm{polylog}(1/\\epsilon_{1})+\\mathrm{polylog}(1/\\epsilon_{3})}\\right)$ , $W_{\\mathrm{max}}$ can be chosen with respect to $\\epsilon_{1},\\epsilon_{3}$ and independent of the system size $n$ such that ", "page_idx": 37}, {"type": "equation", "text": "$$\nC^{\\prime}\\frac{\\log(N)^{\\mathrm{polylog(1/\\epsilon_{1})}}2^{\\mathcal{O}(\\mathrm{polylog}(W_{\\mathrm{max}}/\\epsilon_{1}))}}{N}\\leq\\frac{\\epsilon_{3}}{4}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In this way, we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\nR(\\Theta^{*})\\leq2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Since the training objective from Definition 7 is non-convex, we cannot guarantee that our algorithm converges to a neural network with low training error. However, the assumptions made in Theorem 14 are rather mild in practice. Small training errors are a well-known phenomenon in deep learning and usually come at the expense of a larger prediction error, which is referred to as overftiting. Overftiting may arise due to excessive model complexity [89], i.e. too many trainable parameters. This is reflected by Lemma 10, since the generalization error increases with the width $W$ of the layers. The major challenge in practice lies in finding an appropriate balance between achieving a small training objective and model complexity, rather than only the latter. Furthermore, when the inputs are regularized, the weights usually remain small during training when initialized properly. This was for example observed in [53]. ", "page_idx": 37}, {"type": "text", "text": "Finally, it is worth noting that in a scenario with a constant number of parameters $m=\\mathcal{O}(1)$ , similar to the setup in [51], the expression derived from the outcome in Lemma 10 exhibits nearly linear dependence on $\\epsilon$ . When incorporating the constant number of parameters by setting $\\tilde{m}=m$ , we recover the exact ground state properties $\\operatorname{tr}(P\\rho(x))$ in $f_{P}$ . Thus, $\\epsilon_{1}$ in Lemma 10 becomes 0. Hence, the ability of LDS training to overcome the curse of dimensionality can unfold its full potential, since the domain dimension becomes independent of $\\epsilon$ and expression Equation (C.88) reduces to a constant multiplied by $D_{N}^{*}(2m)$ . By Equation (C.95), we obtain $R(\\Theta)=\\mathcal{O}(\\epsilon^{-(1+\\delta)})$ for any $\\delta>0$ and $\\epsilon$ small enough, when the conditions of Theorem 5 are fulfilled. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "C.3 Prediction on general distributions ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "In this section, we generalize our results to hold for a wider class of distributions. Recall that our rigorous guarantee proven so far (Theorem 14) holds when the training data is generated according to a low-discrepancy sequence and the prediction error is measured with respect to the uniform distribution. We want to extend this result for different choices of both training and prediction error distributions. Notice that our prediction error bound (Lemma 10) is the only place that requires these assumptions on the distributions. Thus, in this section, we establish bounds on the expected prediction error for a more general family of distributions. We consider the following two cases. ", "page_idx": 38}, {"type": "text", "text": "1. The training data is generated according to a general low-discrepancy sequence (in the sense of Definitions 4 and 5), and the prediction error is measured with respect to some distribution $\\mathcal{D}$ .   \n2. The training data consists of independently and identically distributed (i.i.d.) random samples according to a distribution $\\mathcal{D}$ , and the prediction error is measured with respect to the same distribution $\\mathcal{D}$ . ", "page_idx": 38}, {"type": "text", "text": "There are some conditions on the distributions that we discuss shortly. In Case 1, suppose for example that we want to provide rigorous guarantees on the prediction error when the parameters $x\\in[-1,1]^{m}$ are sampled from a standard normal distribution (restricted to $[-1,1]^{m}$ and normalized appropriately). As normally distributed test samples are more densely populated around the mean and more sparse around the boundary of the input domain, we need to predict more accurately around the mean than close to the boundary. When using a uniform low-discrepancy sequence for training, as in Algorithm 1, the predictive capabilities of our model are not exploited properly. To remedy this, we consider the training data to form a general low-discrepancy sequence, where it is low-discrepancy with respect to a normal distribution. We can relate this general low-discrepancy sequence to an LDS with respect to the Lebesgue measure, which are the sequences considered in Appendix C.2, via the probability integral transform (see, e.g., [90]). We sometimes refer to LDS with respect to the Lebesgue measure as uniform low-discrepancy sequences. Formally, for any random variable $X$ which follows some probability distribution $P(X\\geq x)\\triangleq F_{X}(x)$ , the random variable $Y=F_{X}(X)$ follows a uniform distribution. It turns out that the same transformation on LDS produces LDS with respect to other measures than the Lebesgue measure, as illustrated in Figure 3. Moreover, under some assumptions on the distribution, we can bound the discrepancy with respect to other measures in terms of the discrepancy with respect to the Lebesgue measure, which we know how to bound as in Appendix C.2. ", "page_idx": 38}, {"type": "text", "text": "In the following, we formalize this argument and adapt it to our problem setting. We refer the reader to Appendix A.2 to review the necessary concepts of generalized (star-)discrepancy, the KoksmaHlawka inequality, and related results. Then, we demonstrate that a generalization of Lemma 10 and Theorem 14 can be achieved by incorporating these findings with slight adjustments to the proofs. ", "page_idx": 38}, {"type": "text", "text": "In Case 2, we consider training data sampled i.i.d. from some distribution $\\mathcal{D}$ and prediction error measured with respect to the same distribution $\\mathcal{D}$ . To obtain a rigorous guarantee on the prediction error in this case, we leverage a probabilistic bound on the discrepancy of uniformly random points from [82]. Utilizing the previously established framework from Case 1, we can bound the discrepancy of points sampled from $\\mathcal{D}$ in terms of the discrepancy of uniformly random points. This allows us to establish similar guarantees for Case 2. ", "page_idx": 38}, {"type": "text", "text": "Before proving each of these cases, we set up our probabilistic framework and define the Borel measure with respect to which our low-discrepancy sequence is defined. Let $g\\triangleq\\mathrm{PDF}({\\mathcal{D}})$ be the probability density function (PDF) of the data distribution and let $G\\triangleq\\mathrm{CDF}({\\cal D})$ be the corresponding cumulative distribution function (CDF). In the following, assume that the $\\mathrm{PDF}\\,g$ satisfies the following properties. ", "page_idx": 38}, {"type": "image", "img_path": "ybLXvqJyQA/tmp/581ae5faf745cfdc860dfa46f774b820078a39b935d2b15ae8907174110b83e4.jpg", "img_caption": ["Figure 3: Transformed low-discrepancy sequences. The blue circles correspond to two-dimensional uniform Sobol points $x$ . The orange triangles indicate the corresponding Sobol points with respect to the CDF of the standard normal distribution, denoted by $\\Phi$ . The latter forms a low-discrepancysequence with respect to the Borel measure $\\mu=\\Phi$ . "], "img_footnote": [], "page_idx": 39}, {"type": "text", "text": "(a) Strict positivity: $g$ has full support on $[-1,1]^{m}$ , i.e., $g(x)>0$ if $x\\in[-1,1]^{m}$ and $g(x)=0$ otherwise. ", "page_idx": 39}, {"type": "text", "text": "(b) Continuity: $g(x)$ is continuously differentiable on $[-1,1]^{m}$ . ", "page_idx": 39}, {"type": "text", "text": "(c) Component-wise independence: The (random) variables $\\vec{x}_{i},\\vec{x}_{j}$ upon which different local terms $h_{i}(\\vec{x}_{i}),h_{j}(\\vec{x}_{j})$ of the Hamiltonian depend on, are independent. Hence, the PDF $g$ is of the form ", "page_idx": 39}, {"type": "equation", "text": "$$\ng(x)=\\prod_{j=1}^{L}g_{j}(\\vec{x}_{j})\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "for PDFs gj. ", "page_idx": 39}, {"type": "text", "text": "We implicitly assume that $g$ also satisfies all properties of a probability density function. It should be noted that Assumptions (a), (b) could technically be relaxed. We expand more on this later. Notice that if $g:[-1,1]^{m}\\to\\mathbb{R}$ meets these requirements, the same holds for $\\bar{g}\\triangleq g\\circ\\tau:[0,1]^{m}\\to\\mathbb{R}$ . Here, we use the notation from the previous section, where a bar denotes that we are working in the domain $[0,1]^{m}$ as opposed to $[-1,1]^{m}$ , and $\\tau(\\bar{(\\boldsymbol x)})=2\\bar{\\boldsymbol x}-1$ . Since the available results hold on $[0,1]^{m}$ , we will mostly work with $\\bar{g}$ and the corresponding $\\mathrm{CDF}\\,\\bar{G}$ . ", "page_idx": 39}, {"type": "text", "text": "We continue to set up the necessary notation to formally state our prediction error bound for Case 1. Let $S_{P_{1},P_{2}},\\,\\Omega_{P_{1},P_{2}}$ be as in the proof of Lemma 10. Namely, let $S_{P_{1},P_{2}}$ be the parameters with coordinates in $I_{P_{1}}\\cup I_{P_{2}}$ , where $I_{P}$ is defined in Equation (A.2), and let $\\Omega_{P_{1},P_{2}}\\,=\\,[0,1]^{|S_{P_{1},P_{2}}|}$ Additionally, define \u00b5P1,P2 \u225c j\u2208SP,P as the probability measure of the marginal for all (random) variables with indices in $S_{P_{1},P_{2}}$ . Due to Assumption (c), $\\mu{_{P_{1},P_{2}}}$ depends on at most $2\\tilde{m}$ variables. Furthermore, we define ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mu^{*}\\triangleq\\underset{\\mu_{P_{i},P_{j}}}{\\arg\\operatorname*{max}}\\,D_{N}(|S_{P_{i},P_{j}}|;\\mu_{P_{i},P_{j}}),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and denote by $S^{*}$ the corresponding coordinate set. $S^{*}$ forms the domain of $\\mu^{*}$ , and we use $d^{\\ast}$ to denote the dimension of the domain. ", "page_idx": 39}, {"type": "text", "text": "In both Case 1 and Case 2, the idea is to define a transformation $F$ that maps random variables with an arbitrary distribution to uniformly random variables. Namely, we construct a mapping $\\phi$ such that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}[u(x)]=\\underset{x\\sim U[-1,1]^{m}}{\\mathbb{E}}[u(\\phi(x))]\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for any function $u$ . In the following, we introduce the transform $F\\triangleq\\phi^{-1}$ , as has been introduced in [91, 88, 92]. $F$ can nicely be characterized using $\\bar{g}$ and $\\bar{G}$ , and assumptions on $F$ are easy to verify for a given data distribution. In fact, if $F$ satisfies a Lipschitz condition, then known results [88] bound the discrepancy with respect to an arbitrary measure in terms of the discrepancy with respect to the Lebesgue measure, i.e., we can directly upper-bound $D_{N}(d^{*};\\mu^{*})$ in terms of $D_{N}(d^{*})$ . Our prediction error bound for more general distributions follows from this result and the results from Appendix C.2. ", "page_idx": 40}, {"type": "text", "text": "Let $g^{*}$ be defined such that $d\\mu^{*}(x)=g^{*}(x)d x$ . Also, let $A,B\\subseteq S^{*}$ be such that $A\\cap B=\\emptyset$ and $C=S^{*}\\setminus(A\\cup B)$ . Then, we define the conditional marginal PDF as ", "page_idx": 40}, {"type": "equation", "text": "$$\ng^{*}(x_{A}|X_{B}=x_{B})\\triangleq{\\frac{\\int_{[0,1]^{|C|}}g^{*}(x)d x_{C}}{\\int_{[0,1]^{|A|+|C|}}\\int_{0}^{(x_{B})_{1}}\\cdot\\cdot\\cdot\\int_{0}^{(x_{B})_{|B|}}g^{*}(x)d x}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and the corresponding CDF as ", "page_idx": 40}, {"type": "equation", "text": "$$\nG^{*}(X_{A}=x_{A}|X_{B}=x_{B})\\triangleq\\int_{0}^{(x_{A})_{1}}\\cdot\\cdot\\cdot\\int_{0}^{(x_{A})_{|A|}}g^{*}(x_{A}|x_{B})d x_{A}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "For convenience, we refer to the indices of $x$ in $S^{*}$ via $x_{1},x_{2},\\ldots,x_{d^{*}}$ . We can do this without loss of generality by permuting the order of the parameters. Using these definitions, we can now define the reverse transformation as $F:[0,1]^{d^{*}}\\rightarrow[0,1]^{d^{*}}$ , where the indices of $F$ are given by ", "page_idx": 40}, {"type": "equation", "text": "$$\nF_{j}(x)\\triangleq G^{*}(X_{j}=x_{j}|X_{1}=x_{1},\\ldots,X_{j-1}=x_{j-1}).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "If random variables are distributed as $X\\sim G^{*}$ , then $F(X)\\sim U[-1,1]^{d^{*}}$ (or equivalently $U[0,1]^{d^{*}}$ under the variable transformation $\\tau(x)=2x-1)$ , since $X_{1}$ , $X_{2}|X_{1},\\ldots,X_{d^{*}}|X_{1},\\ldots,X_{d^{*}-1}$ are independent and ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\prod_{j}F_{j}(X)=G^{*}(X).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Finally, with this notation set up, we can formally state our result for Case 1. ", "page_idx": 40}, {"type": "text", "text": "Corollary 6 (Neural network sample complexity guarantee; generalization of Theorem 14). Let $1/e>\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}>0,$ , and let $\\mathcal{D}$ be a distribution with PDF g fulfilling assumptions (a)-(c) and $F$ according to Equation (C.106). Let $f^{\\Theta^{*},w^{*}}:[-1,1]^{m}\\to\\mathbb{R}$ be a neural network model produced from Algorithm $^{\\,I}$ trained on data $\\{(\\hat{x}_{\\ell},\\hat{y}_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 40}, {"type": "equation", "text": "$$\nN=2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1})+\\mathrm{polylog}(1/\\epsilon_{3}))},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the x\u2113\u2019s form a low-discrepancy Sobol sequence, $\\hat{x}_{\\ell}=F^{-1}(x_{\\ell})$ and $|\\hat{y}_{\\ell}-\\mathrm{tr}(O\\rho(\\hat{x}_{\\ell}))|\\le\\epsilon_{2}$ . Suppose that $f^{\\Theta^{*},w^{*}}$ achieves a training error of at most $((\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3})/2$ . Additionally, suppose that all parameters $\\Theta_{i}^{*}$ of $f^{\\Theta^{*},w^{*}}$ satisfy $|\\Theta_{i}^{*}|\\leq W_{\\operatorname*{max}},$ for some $W_{\\mathrm{max}}>0$ that is independent of the system size $n$ . Then the neural network $f^{\\Theta^{*},w^{*}}$ achieves prediction error ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|f^{\\Theta^{*},w^{*}}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\le2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Similarly, we also have a guarantee for Case 2, which is the version we state in the main text and the beginning of this appendix. ", "page_idx": 40}, {"type": "text", "text": "Corollary 7 (Neural network sample complexity guarantee; generalization of Corollary 6 for random data). Let $1/e\\,>\\epsilon_{1},\\epsilon_{2},\\epsilon_{3}\\,>\\,0$ , $\\mathcal{D}$ a distribution with $P D F\\,g$ satisfying assumptions (a)-(c). Let $f^{\\Theta^{*},w^{*}}:[-1,1]^{m}\\to\\mathbb{R}$ be a neural network model produced from Algorithm $^{\\,l}$ trained on data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size ", "page_idx": 40}, {"type": "equation", "text": "$$\nN=\\log(1/\\delta)2^{\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1})+\\mathrm{polylog}(1/\\epsilon_{3}))},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where the $x_{\\ell}\\sim\\mathcal{D}$ and $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\,\\le\\,\\epsilon_{2}$ . Suppose that $f^{\\Theta^{*},w^{*}}$ achieves a training error of at most $((\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3})/2$ . Additionally, suppose that all parameters $\\Theta_{i}^{*}$ of $f^{\\Theta^{*},w^{*}}$ satisfy $|\\Theta_{i}^{*}|\\leq W_{\\operatorname*{max}},$ for some $W_{\\mathrm{max}}>0$ that is independent of the system size $n$ . Then the neural network f \u0398\u2217,w\u2217achieves prediction error ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|f^{\\Theta^{*},w^{*}}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\le2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 41}, {"type": "text", "text": "We first prove Corollary 6 similarly to how we proved Theorem 14. In particular, we can prove a generalized version of Lemma 10, where we are given a low-discrepancy sequence with respect to $\\mu^{*}$ and wish to bound the prediction error with respect to $\\mathcal{D}$ , as in Case 1. Define the prediction error of a neural network $f^{\\Theta,w}$ with weights given by $\\Theta,w$ with respect to a distribution $\\mathcal{D}$ as ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{\\mathcal{D}}(\\Theta)\\triangleq\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}|f^{\\Theta,w}(x)-\\mathrm{tr}(O\\rho(x))|^{2}=\\int_{[-1,1]^{m}}|f^{\\Theta,w}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\,d G(x),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $x\\sim\\mathcal{D}$ denotes $x$ sampled from the distribution $\\mathcal{D}$ over $[-1,1]^{m}$ and $d G(x)=g(x)d x$ . Again, we suppress $w$ in the notation to avoid cluttering. Then, we have the following lemma. ", "page_idx": 41}, {"type": "text", "text": "Lemma 11 (Generalized prediction error bound). Let $1/e>\\epsilon_{1},\\epsilon_{2}>0$ . Consider a tanh neural network $f^{\\Theta,w}:[-1,1]^{m}\\,\\^{\\!}\\to\\mathbb{R}$ with architecture defined in Definition 7 with weights $\\Theta_{i}\\leq W_{\\operatorname*{max}}$ for some $W_{\\mathrm{max}}\\,>\\,0$ independent of the system size $n$ and weights w in the last layer. Assume that $G$ satisfies assumptions (a)- $(c)$ and $|y_{\\ell}-\\mathrm{tr}(O\\rho(x_{\\ell}))|\\,\\le\\,\\epsilon_{2}$ . Furthermore, suppose we train $f^{\\Theta,w}$ on data $\\{(x_{\\ell},y_{\\ell})\\}_{\\ell=1}^{N}$ of size $N$ , where the $\\tau^{-1}(x_{\\ell})\\,^{\\prime}s$ from a set with star-discrepancy at most $D_{N}^{*}(d;\\mu^{*})$ in each dimension . Then, we have ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{\\mathcal{D}}(\\Theta)\\leq\\hat{R}(\\Theta)+\\frac{\\epsilon_{1}^{2}}{2}+\\epsilon_{2}^{2}+(\\|w\\|_{1}+\\|w\\|_{1}^{2})D_{N}^{*}(d^{*};\\mu^{*})\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(W W_{\\operatorname*{max}}/\\epsilon_{1}))}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Moreover, if there exist constants $b_{1},b_{2}$ such that $\\begin{array}{r}{D_{N}^{*}(d)\\leq b_{1}\\sqrt{b_{2}+\\log(1/\\delta)}\\sqrt{\\frac{d}{N}}}\\end{array}$ with probability at least $1-\\delta$ , then there exists a constant $\\tilde{b}_{1}$ such that ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{\\mathcal{D}}(\\Theta)\\leq\\hat{R}(\\Theta)\\!+\\!\\frac{\\epsilon_{1}^{2}}{2}\\!+\\!\\epsilon_{2}^{2}\\!+\\!(\\|w\\|_{1}\\!+\\!\\|w\\|_{1}^{2})\\tilde{b}_{1}\\sqrt{1+\\log(1/\\delta)}\\sqrt{\\frac{\\tilde{m}}{N}}\\cdot2^{\\mathcal{O}(\\mathrm{polylog}(W W_{\\operatorname*{max}}/\\epsilon_{1}))}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "with probability at least $1-\\delta$ . ", "page_idx": 41}, {"type": "text", "text": "This lemma can be proven in the same fashion as Lemma 10 with two minor adjustments. One change is the use of a generalization of the Koksma-Hlawka inequality, which we discuss in Theorem 11 in Appendix A.2. To prove the second part of Lemma 11, we need a technical lemma (Lemma 14) to handle the probability of failure in the bound on the star-discrepancy. We relegate this to the end of the section, as it is mainly a technicality. ", "page_idx": 41}, {"type": "text", "text": "Proof. We proceed in the same way as in Lemma 10, replacing $\\mathcal{R}(\\Theta)$ with $\\mathcal{R}_{\\mathcal{D}}(\\Theta)$ and replacing $\\mathcal{R}_{\\mathrm{loc}}(\\Theta)$ with ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{\\mathrm{loc},\\mathcal{D}}(\\Theta)\\triangleq\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\left|f^{\\Theta,w}(x)-\\sum_{P}\\alpha_{P}f_{P}(x)\\right|^{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We follow the proof of Lemma 10 until Equation (C.75). This gives us ", "page_idx": 41}, {"type": "equation", "text": "$$\nR_{\\mathcal{D}}(\\Theta)\\leq\\frac{\\epsilon_{1}^{2}}{4}+\\hat{R}_{\\mathrm{loc}}(\\Theta)+|R_{\\mathrm{loc},\\mathcal{D}}(\\Theta)-\\hat{R}_{\\mathrm{loc}}(\\Theta)|,\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where recall that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\hat{R}_{\\mathrm{loc}}(\\Theta)=\\frac{1}{N}\\sum_{\\ell=1}^{N}\\left|f^{\\Theta,w}(x_{\\ell})-\\sum_{P}\\alpha_{P}f_{P}(x_{\\ell})\\right|^{2},\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "as in Equation (C.63). Moreover, we also have the adjusted version of Equation (C.75) ", "page_idx": 41}, {"type": "equation", "text": "$$\n|R_{\\mathrm{loc},\\mathcal{D}}(\\Theta)-\\hat{R}_{\\mathrm{loc}}(\\Theta)|\\leq\\sum_{P_{1},P_{2}}|w_{P_{1}}||w_{P_{2}}|\\left|\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}[f_{P_{1}}^{\\theta_{P_{1}}}(x)f_{P_{2}}^{\\theta_{P_{2}}}(x)]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(x_{\\ell})f_{P_{2}}^{\\theta_{P_{2}}}(x_{\\ell})\\right|\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle+\\,2|w_{P_{1}}||\\alpha_{P_{2}}|\\left|\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}[f_{P_{1}}^{\\theta_{P_{1}}}(x)f_{P_{2}}(x)]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(x_{\\ell})f_{P_{2}}(x_{\\ell})\\right|}}\\\\ {{\\displaystyle+\\,|\\alpha_{P_{1}}||\\alpha_{P_{2}}|\\left|\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}[f_{P_{1}}(x)f_{P_{2}}(x)]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}(x_{\\ell})f_{P_{2}}(x_{\\ell})\\right|.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "To bound the first term, we use the generalized Koksma-Hlawka inequality (Theorem 11) to obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{x\\to D}{\\mathbb{E}}[f_{P_{1}}^{\\theta_{P_{1}}}(x)f_{P_{2}}^{\\theta_{P_{2}}}(x)]-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(x_{\\ell})f_{P_{2}}^{\\theta_{P_{2}}}(x_{\\ell})\\Bigg|}\\\\ &{=\\left|\\int_{[-1,1]^{n}}|f^{\\Theta,w}(x)-\\mathrm{tr}(O\\rho(x))|^{2}\\,d G(x)-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(x_{\\ell})f_{P_{2}}^{\\theta_{P_{2}}}(x_{\\ell})\\right|}\\\\ &{=\\left|\\int_{[0,1]^{n}}f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}))\\prod_{i=1}^{L}\\bar{g}_{i}(\\vec{x}_{i})d\\bar{x}-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}_{\\ell}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}_{\\ell}))\\right|}\\\\ &{=\\left|\\int_{\\Omega_{P_{1}},P_{2}}f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}))d\\mu_{P_{1},P_{2}}(\\bar{x})-\\frac{1}{N}\\sum_{\\ell=1}^{N}f_{P_{1}}^{\\theta_{P_{1}}}(\\tau(\\bar{x}_{\\ell}))f_{P_{2}}^{\\theta_{P_{2}}}(\\tau(\\bar{x}_{\\ell}))\\right|}\\\\ &{\\le D_{\\nu}^{*}(d^{*};\\mu^{*})V_{H K}\\left((f_{P_{1}}^{\\theta_{P_{1}}}\\cdot f_{P_{2}}^{\\theta_{P_{2}}})\\circ\\tau\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Here, in the first equality, we use Assumption (c) on the structure of the $\\mathrm{PDF~}g$ and use the variable transformation $\\tau(x)=2x-1$ . In the second equality, similarly to Lemma 10, we notice that because the functions in the expectation are local functions that only depend on parameters in $S_{P_{1},P_{2}}$ , we can replace the expectation over the whole domain $[0,1]^{m}$ with an expectation over just the domain $\\Omega_{P_{1},P_{2}}$ . This step also crucially uses Assumption (c), where the factorization of the $\\mathrm{PDF~}g$ due to independence is needed. The last line uses the generalized Koksma-Hlawka inequality (Theorem 11). The remainder of the proof follows in the same way as Lemma 10. ", "page_idx": 42}, {"type": "text", "text": "The second part of the statement is a direct consequence of Lemma 14. Specifically, following the proof of Lemma 10, we use Lemma 14 to bound the term in Equation (C.88). This is necessary because the upper bound on the star-discrepancy only holds probabilistically, so we must show that we can still use this upper bound on the sum of several star-discrepancy terms. This is more complicated than a simple union bound, and we relegate the proof and statement of Lemma 14 to the end of this section. \u53e3 ", "page_idx": 42}, {"type": "text", "text": "In addition to this lemma, we also need a result from [88], adapted to our definitions above. In the following, we use $D_{N}(\\omega;d)$ to denote the discrepancy with respect to the Lebesgue measure of a specific sequence $\\omega$ of length $N$ and dimension $d$ , as in Definition 1. Similarly, we use $D_{N}(\\omega;d;\\mu)$ to denote the discrepancy with respect to a measure $\\mu$ of a specific sequence $\\omega$ of length $N$ and dimension $d$ , as in Definition 4. ", "page_idx": 42}, {"type": "text", "text": "Lemma 12 (Theorem 2 in [88]). Let $\\omega~=~\\{x_{\\ell}\\}_{\\ell=1}^{N}$ be an arbitrary sequence on the open $d_{\\cdot}$ dimensional unit cube with discrepancy $D_{N}(\\omega,d)$ , and let $\\hat{\\omega}=\\{\\hat{x}_{\\ell}\\}_{\\ell=1}^{N}$ be the sequence defined by $F\\hat{x}_{\\ell}=x_{\\ell}$ , where $F$ is defined in Equation (C.106). Moreover, let $g$ be a strictly positive, $d$ -times continuously differentiable PDF, such that $g(x)\\geq m>0$ for all $x$ . Let $G$ be the corresponding probability measure (i.e., $C D F,$ ). Furthermore, let $F$ satisfy ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\|F(x)-F(y)\\|\\leq K\\|x-y\\|.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Then, the discrepancy of $\\hat{\\omega}$ with respect to $G$ is bounded as ", "page_idx": 42}, {"type": "equation", "text": "$$\nD_{N}(\\hat{\\omega};d;G)\\leq c\\left(D_{N}(\\omega;d)\\right)^{\\frac{1}{d}},\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "where $c=2d\\cdot3^{d}(K+1)^{d-1}$ . ", "page_idx": 42}, {"type": "text", "text": "The authors in [88] note that the assumption on $F$ in Equation (C.124) is certainly fulfliled when $g$ is continuously differentiable. This is where Assumption (b) is used, where technically, we only require this Lipschitz condition on $F$ . With Lemma 11 and Lemma 12, we are ready to prove Corollary 6. ", "page_idx": 42}, {"type": "text", "text": "Proof of Corollary $^{6}$ . We proceed similarly as in the proof of Theorem 14 but this time using Lemma 11 instead of Lemma 10. First, we bound the nonuniform discrepancy of our training inputs $\\hat{\\omega}=\\{\\hat{x}_{\\ell}\\}_{\\ell=1}^{N}$ . Recall that $\\hat{x}_{\\ell}\\,=\\,F^{-1}(x_{\\ell})$ for $x_{\\ell}$ generated according to a low-discrepancy Sobol sequence (i.e., low-discrepancy with respect to the Lebesgue measure). By definition of , then $\\hat{\\omega}$ has star-discrepancy $D_{N}^{*}(\\hat{\\omega};d^{*};\\mu^{*})$ . By Assumption (b), we can apply Lemma 12 to obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\nD_{N}^{*}(\\hat{\\omega};d^{*};\\mu^{*})\\leq D_{N}(\\hat{\\omega};d^{*};\\mu^{*})\\leq c\\left(D_{N}(\\omega;d^{*})\\right)^{\\frac{1}{d^{*}}}\\leq2^{d^{*}}c\\left(D_{N}^{*}(\\omega;d^{*})\\right)^{\\frac{1}{d^{*}}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Here, the first inequality follows because $D_{N}^{*}(d)\\leq D_{N}(d)$ , and the second follows by Lemma 12. Finally, the last inequality follows from $D_{N}(d)\\leq2^{d}D_{N}^{*}(d)$ (see, e.g., [93]). Because $d^{*}\\leq2\\tilde{m}$ , we can proceed as in the proof of Theorem 14 using the bound above. ", "page_idx": 43}, {"type": "text", "text": "By Theorem 9, we know that for Sobol sequences in base 2 with points in $[0,1]^{d^{*}}$ , the star-discrepancy is bounded by ", "page_idx": 43}, {"type": "equation", "text": "$$\nD_{N}^{*}(d^{*})\\leq C(d^{*})\\frac{\\log(N)^{d^{*}}}{N},\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $C(d)$ is a constant such that ", "page_idx": 43}, {"type": "equation", "text": "$$\nC(d)<\\frac{1}{d!}\\left(\\frac{d}{\\log(2d)}\\right).\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Since $C(d)\\;=\\;o(1)$ , there exists a constant $C$ , such that $C\\,\\geq\\,C(d)$ for all $d\\,>\\,0$ . Using the assumption that the training objective is not larger than $((\\epsilon_{1}+\\epsilon_{2})^{2}+\\dot{\\epsilon}_{3})/2$ , by Lemma 11, we have ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{R_{\\mathcal{D}}(\\Theta^{*})}=\\underset{x\\sim\\mathcal{D}}{\\mathbb{E}}\\,|\\,f^{\\Theta^{*},w^{*}}(x)-\\mathrm{tr}(O\\rho(x))|^{2}}\\\\ &{\\qquad\\quad\\le\\frac{\\epsilon_{1}^{2}}{2}+\\epsilon_{2}^{2}+\\frac{\\left(\\epsilon_{1}+\\epsilon_{2}\\right)^{2}+\\,\\epsilon_{3}}{2}+{C^{\\prime}}\\frac{\\log(N)2^{\\mathcal{O}(\\mathrm{polylog}(W_{\\mathrm{max}}/\\epsilon_{1}))}}{N^{1/d^{*}}}}\\\\ &{\\qquad\\quad\\le2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\frac{\\epsilon_{3}}{2}+{C^{\\prime}}\\frac{\\log(N)2^{\\mathcal{O}(\\mathrm{polylog}(W_{\\mathrm{max}}/\\epsilon_{1}))}}{N^{1/\\mathrm{polylog}(1/\\epsilon_{1})}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where $C^{\\prime}$ is a constant. We also used here that $d^{*}\\leq2\\tilde{m}$ and $\\tilde{m}=|I_{P}|=\\mathcal{O}(\\mathrm{polylog}(1/\\epsilon_{1})$ . Since the training data has size $N=\\mathcal{O}\\left(2^{\\mathrm{polylog}(1/\\epsilon_{1})+\\mathrm{polylog}(1/\\epsilon_{3})}\\right)$ , $W_{\\mathrm{max}}$ can be chosen with respect to $\\epsilon_{1},\\epsilon_{3}$ and independent of the system size $n$ such that ", "page_idx": 43}, {"type": "equation", "text": "$$\nC^{\\prime\\prime}\\frac{\\log(N)2^{\\mathcal{O}(\\mathrm{polylog}(W_{\\mathrm{max}}/\\epsilon_{1}))}}{N^{1/\\mathrm{polylog}(1/\\epsilon_{1})}}\\leq\\frac{\\epsilon_{3}}{4}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "for some constant $C^{\\prime\\prime}$ . In this way, we obtain ", "page_idx": 43}, {"type": "equation", "text": "$$\nR_{\\mathcal{D}}(\\Theta^{*})\\leq2(\\epsilon_{1}+\\epsilon_{2})^{2}+\\epsilon_{3}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Finally, we prove Case 2, where the training data is sampled i.i.d. according to a distribution $\\mathcal{D}$ and the prediction error is also measured with respect to $\\mathcal{D}$ . Note that we can drop the assumption that $F^{-\\dagger}$ is efficiently computable for this case. The key result we need for this is a bound on the star-discrepancy for uniformly random points (Lemma 5). ", "page_idx": 43}, {"type": "text", "text": "Proof of Corollary 7. Let $\\hat{x}_{\\ell}=F x_{\\ell}$ , where $F$ is as in Equation (C.106). As stated in [91, 92], $F$ transforms random variables with distribution $\\mathcal{D}$ into standard uniform random variables. Hence, similarly to the proof of Corollary 6, if $\\omega=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ has star-discrepancy $D_{N}^{*}(\\omega;d^{*};\\mu^{*})$ , we can bound it with respect to the discrepancy of $\\hat{\\omega}=\\{\\hat{x}_{\\ell}\\}_{\\ell=1}^{N}$ : ", "page_idx": 43}, {"type": "equation", "text": "$$\nD_{N}^{*}(\\omega;d^{*};\\mu^{*})\\leq D_{N}(\\omega;d^{*};\\mu^{*})\\leq c\\left(D_{N}(\\hat{\\omega};d^{*})\\right)^{\\frac{1}{d^{*}}}\\leq2^{d^{*}}c\\left(D_{N}(\\hat{\\omega};d^{*})\\right)^{\\frac{1}{d^{*}}}.\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Here, again we use $D_{N}^{*}(d)\\leq D_{N}(d)\\leq2^{d}D_{N}^{*}(d)$ and Lemma 12. Then, by Lemma 5, for uniformly random points, i.e., $\\hat{\\omega}=\\{\\hat{x}_{\\ell}\\}_{\\ell=1}^{N}$ , we have ", "page_idx": 43}, {"type": "equation", "text": "$$\nD_{N}^{*}(d)\\leq5.7\\sqrt{4.9+\\log(1/\\delta)}\\sqrt{\\frac{d}{N}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "with probability at least $1-\\delta$ . The rest of the proof follows in the same way as Corollary 6, but using the above discrepancy bound. Note that because this discrepancy bound only holds probabilistically, we need to use the second part of Lemma 11. \u53e3 ", "page_idx": 43}, {"type": "text", "text": "Our prediction error bounds for Case 1 and Case 2 (in Corollaries 6 and 7, respectively) look rather similar. However, one can verify that Corollary 6 only requires about square root of the number of samples Corollary 7 uses to achieve a certain risk bound with small enough $\\epsilon$ , but this advantage is hidden in the polylogarithmic factors in the exponent. Hence, low-discrepancy data yields better theoretical guarantees. However, they did not yield an improvement in our numerical experiments, as discussed in Appendix D. The size $N$ of the training set seems to be very large for low-discrepancy data to have practical effects. In the main text, we present Corollary 7 because it is the more general theoretical statement. ", "page_idx": 44}, {"type": "text", "text": "In fact, one can also improve the polylogarithmic factors in Corollary 6 by imposing stronger assumptions on the distribution. In particular, the result in Lemma 12 seems surprisingly weak at first glance. Multidimensional transformations do, however, constitute a major challenge, since they generally do not preserve properties such as lines remaining straight or parallel. The boxes over which one optimizes in order to compute the discrepancy can thus change severely in shape, which can strongly alter the discrepancy and makes it difficult to analyze. This can result in a rather poor scaling in terms of the discrepancy with respect to the Lebesgue measure and thus $N$ . However, when $g$ fulflils additional assumptions, we obtain a much better dependence on $N$ by directly applying the Koksma-Hlawka inequality (with respect to the Lebesgue measure) to $f\\circ\\phi$ . Unsurprisingly, this is possible when the mixed derivative of $F^{-1}=\\phi$ is bounded on $[0,1]$ . This follows, when $g$ \u2019s mixed derivative is bounded [88]. We restate this result below. ", "page_idx": 44}, {"type": "text", "text": "Lemma 13 (Theorem 1 in [88]). Let $\\omega=\\{x_{\\ell}\\}_{\\ell=1}^{N}$ of size $N$ be an arbitrary sequence on the open $d$ -dimensional unit cube with discrepancy $D_{N}(\\omega,d)$ and $\\hat{\\omega}\\,=\\,\\{\\hat{x}_{\\ell}\\}_{\\ell=1}^{N}$ the sequence defined by $F\\hat{x}_{\\ell}=x_{\\ell}$ , where $F$ is defined in Equation (C.106). Moreover, let $g$ be a strictly positive, $d$ -times continuously differentiable PDF, such that $g(x)\\geq m>0$ for all $x$ . Let $G$ be the corresponding probability measure (i.e., $C D F,$ ). Furthermore, let $F$ satisfy ", "page_idx": 44}, {"type": "equation", "text": "$$\n{\\frac{\\partial^{|A|}F_{j}}{\\partial x_{A}}}\\leq M\\qquad1\\leq j\\leq d,\\quad A\\subseteq\\{1,\\ldots,d\\}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Then ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\left|\\int_{[0,1^{d}]}f(\\hat{x})d G(\\hat{x})-\\frac{1}{N}\\sum_{\\ell=1}^{N}f(\\hat{x}_{\\ell})\\right|\\leq d!\\left(\\frac{M}{m}\\right)^{2d-1}D_{N}^{*}(\\omega;d)V_{H K}(f).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "On a high level, the proof works via the observation that the Jacobian of $F$ has $g$ as its determinant, which is strictly positive. Since $F\\circ\\phi$ is the identity, one can write the Jacobian of $F\\circ\\phi$ as a linear system of equations with the derivatives of $\\phi$ as solution. Using Cramer\u2019s rule and the assumption on $F$ , one can upper bound the derivative of $\\phi$ . Applying this iteratively, one can show via induction that the mixed derivatives of $\\phi$ are also bounded when the mixed derivatives of $F$ are bounded. Note that (as stated in [88]) when $\\mathcal{D}$ is a product of independent distributions, i.e. $\\begin{array}{r}{g(x)=\\prod_{i=1}^{m}g_{i}(x_{i})}\\end{array}$ and fulfills assumptions (a)-(c), the conditions for Lemma 13 are also fulfilled. It is important to emphasize that the additional assumption used in Lemma 13 yield a much better dependence of $\\epsilon$ on $N$ . However, this improvement is hidden in the polylogarithmic factors. ", "page_idx": 44}, {"type": "text", "text": "We dedicate the last part of this section to the proof the following statement, which we used in the proof of Lemma 11. At a high level, this shows that when we have a probabilistic upper bound on the star-discrepancy, we can still upper bound a sum of star-discrepancies with high probability. ", "page_idx": 44}, {"type": "text", "text": "Lemma 14. Suppose there exist constants $b_{1},b_{2}$ such that $\\begin{array}{r}{D_{N}^{*}(d)\\leq b_{1}\\sqrt{b_{2}+\\log(1/\\delta)}\\sqrt{\\frac{d}{N}}\\ w i t h}\\end{array}$ h probability $1-\\delta$ . Then, there exists a constant $\\tilde{b}_{1}$ , such that for any $t>0$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathrm{Pr}\\left(\\displaystyle\\sum_{P_{1},P_{2}\\in S^{(\\mathrm{geo})}}(c_{1}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|+c_{2}(|w_{P_{1}}||\\alpha_{P_{2}}|+|w_{P_{1}}||w_{P_{2}}|))D_{N}^{*}(x_{P_{1},P_{2}})\\geq t\\right)}\\\\ &{\\leq\\exp\\left(-\\frac{N t^{2}}{\\tilde{b}_{1}(c_{1}||\\alpha||_{1}^{2}+c_{2}(\\|w\\|_{1}\\|\\alpha\\|_{1}+\\|w\\|_{1}^{2}))^{2}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where recall $S_{P_{1},P_{2}}$ is the set of parameters with coordinates in $I_{P_{1}}\\cup I_{P_{2}}$ (Equation (A.2)), $c_{1}=$ $2^{\\mathcal{O}(\\tilde{m}\\log(\\tilde{m}))}$ and $c_{2}=2^{\\mathcal{O}(\\tilde{m}\\log(W W_{\\operatorname*{max}})+\\tilde{m}^{2}\\log(\\tilde{m}))}$ . Thus $D_{N}^{*}(x_{P_{1},P_{2}})$ denotes the star-discrepancy of this set of parameters in the training data. ", "page_idx": 44}, {"type": "text", "text": "First, we introduce two useful tools for the proof. ", "page_idx": 45}, {"type": "text", "text": "Theorem 17 (Azuma\u2019s Inequality for Martingales with Subgaussian Tails; Adapted from Theorem 2 in [94]). Let $Z_{1},Z_{2},\\ldots,Z_{n}$ be a martingale difference sequence with respect to a sequence $X_{1},X_{2},\\ldots,X_{n},$ , and suppose there are constants $b>1$ , $c_{1},\\ldots,c_{n}>0,$ , such that for any $j$ and any $t>0$ it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(Z_{j}>t|X_{1},\\ldots,X_{j-1})\\leq b\\exp\\left(-{\\frac{t^{2}}{c_{j}^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Then, it holds that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{j=1}^{n}Z_{j}>t\\right)\\leq\\exp\\left(-{\\frac{t^{2}}{28b\\sum_{j=1}^{n}c_{j}^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Proof of Theorem $I7.$ . Following the steps of the proof of Theorem 2 in [94], but taking the sum over $Z_{j}$ instead of the empirical average, we obtain for any $s>0$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{j=1}^{n}Z_{j}>t\\right)\\leq e^{-s t}e^{7b c_{n}^{2}s^{2}}\\operatorname{\\mathbb{E}}\\left[\\prod_{j=1}^{n}e^{s Z_{j}}\\bigg|X_{1},\\ldots,X_{n-1}\\right]\\leq e^{-s t+7b s^{2}\\sum_{j=1}^{n}c_{j}^{2}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We refer to [94] for further details of this calculation. Choosing $\\begin{array}{r}{s=t/\\left(14b\\sum_{j=1}^{n}c_{j}^{2}\\right)}\\end{array}$ , the expression above equals $e^{-t^{2}/\\left(28b\\sum_{j=1}^{n}c_{j}^{2}\\right)}$ , and we get the claim: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\sum_{j=1}^{n}Z_{j}>t\\right)\\leq\\exp\\left(-{\\frac{t^{2}}{28b\\sum_{j=1}^{n}c_{j}^{2}}}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "We also need the following two small lemmas. ", "page_idx": 45}, {"type": "text", "text": "Lemma 15. Let $\\delta>0$ . Let $X:\\Omega_{X}\\to X$ and $Y:\\Omega_{Y}\\rightarrow\\mathcal{Y}$ be independent random variables and $f:\\mathcal{X}\\times\\mathcal{Y}\\to\\mathbb{R}_{+}$ be a function such that $\\operatorname*{Pr}_{X Y}\\!\\left(f(X,Y)\\geq t\\right)\\leq\\delta$ for $t>0$ . Then, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathbb{E}[f(X,Y)|X]\\leq{\\frac{t}{2}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "with probability at least $1-2\\delta$ . ", "page_idx": 45}, {"type": "text", "text": "Proof. First, we show that $\\operatorname*{Pr}(f(X,Y)\\geq t|X)\\geq1/2$ with high probability. Then, we show that this implies the claim by Markov\u2019s inequality. ", "page_idx": 45}, {"type": "text", "text": "First, suppose for the sake of contradiction that $\\begin{array}{r}{\\operatorname*{Pr}\\bigl(\\mathbb{E}[\\mathbb{1}\\{f(X,Y)\\geq t\\}|X]\\geq\\frac{1}{2}\\bigr)>2\\delta}\\end{array}$ . Using the independence of $X$ and $Y$ applying Markov\u2019s inequality, we obtain ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\mathrm{v}_{}(f(X,Y)\\geq t)=\\mathbb{E}[\\mathbb{E}[\\mathbb{I}\\{f(X,Y)\\geq t\\}|X]]\\geq\\frac{1}{2}\\operatorname*{Pr}\\biggl(\\mathbb{E}\\left[\\mathbb{I}\\{f(X,Y)\\geq t\\}|X|\\geq\\frac{1}{2}\\right)>2\\delta\\cdot\\frac{1}{2}=\\delta,\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "which contradicts our initial assumption. Therefore, with probability at most $2\\delta$ (w.r.t. $X_{.}$ ), $\\operatorname*{Pr}(f(X,Y)\\geq t|X)\\geq{\\frac{1}{2}}$ and hence ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\frac{1}{2}}\\leq\\operatorname*{Pr}(f(X,Y)\\geq t|X)\\leq{\\frac{\\mathbb{E}[f(X,Y)|X]}{t}}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "by Markov\u2019s inequality. The result follows immediately. ", "page_idx": 45}, {"type": "text", "text": "Lemma 16. Let $j\\in[m]$ be a coordinate of the parameters. Then, $|\\{P\\in S^{(\\mathrm{geo})}:j\\in I_{P}\\}|=t i l d e m,$ , where $\\tilde{m}={\\mathcal{O}}(|I_{P}|)$ . ", "page_idx": 45}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 46}, {"type": "equation", "text": "$$\nI_{P}=\\{c\\in\\{1,\\dots,m\\}:d_{\\mathrm{obs}}(h_{j(c)},P)\\leq\\delta_{1}\\}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Fixing $c$ instead of $P$ also results in a set of geometrically local terms in a radius $\\delta_{1}$ around a geometrically local term. Hence, the size of the set $\\{P\\in S^{(\\mathrm{geo})}:j\\in I_{P}\\}$ also scales as $\\left|{I_{P}}\\right|$ , which is at most $\\tilde{m}$ . \u53e3 ", "page_idx": 46}, {"type": "text", "text": "Now we are able to provide a partial proof to Lemma 14. ", "page_idx": 46}, {"type": "text", "text": "Lemma 17. Let $S_{P_{1},P_{2}}$ be the set of parameters with coordinates in $I_{P_{1}}\\cup I_{P_{2}}$ and let $x_{P_{1},P_{2}}\\triangleq$ $\\{\\{x\\,\\in\\,S_{P_{1},P_{2}}\\}_{\\ell}\\}_{\\ell=1}^{N}$ denote the training data set only for these local parameters. If there exist constants $b_{1},b_{2}$ such that $\\begin{array}{r}{D_{N}^{*}(d)\\leq b_{1}\\sqrt{b_{2}+\\log(1/\\delta)}\\sqrt{\\frac{d}{N}}}\\end{array}$ with probability at least $1-\\delta$ , then ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\sum_{P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})\\geq t\\right)\\leq\\exp\\left(-\\frac{N t^{2}}{224\\|\\alpha\\|_{2}^{2}(\\tilde{m})^{2}\\exp(b_{1})b_{2}}\\right)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "for any $P_{1}\\in S^{(\\mathrm{geo})}$ and any $t>0$ . ", "page_idx": 46}, {"type": "text", "text": "Proof. Let $P_{1}\\in S^{(\\mathrm{geo})}$ . Define ", "page_idx": 46}, {"type": "equation", "text": "$$\nX_{j}\\triangleq\\mathbb{E}\\left[\\sum_{P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|Y_{j-1},\\ldots,Y_{0}\\right],\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where we omit the dependence $x_{P_{1},P_{2}}=x_{P_{1},P_{2}}(Y_{1},\\ldots,Y_{m})$ and $Y_{j}\\triangleq\\{(x_{j})_{\\ell}\\}_{\\ell=1}^{N}$ and $x_{j}$ parameterize $h_{j}$ . We consider all increments, which are not contained in $I_{P_{1}}$ . Hence, with slight abuse of notation, let index $j\\,=\\,0$ refer to all coordinates in $I_{P_{1}}$ and ${\\cal Y}_{0}\\;\\triangleq\\;\\{\\{(x_{j})\\,:\\,j\\;\\in\\;I_{P_{1}}\\}_{\\ell}\\}_{\\ell=1}^{N}$ . Furthermore, let ", "page_idx": 46}, {"type": "equation", "text": "$$\nX_{0}\\triangleq\\mathbb{E}\\left[\\sum_{P_{2}\\in S^{(\\mathrm{geo})}}|\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|Y_{0}\\right]\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "and $Z_{1}\\triangleq X_{1}-X_{0}$ . Clearly, $X_{0},\\ldots,X_{m}$ is a martingale sequence and $Z_{1},\\ldots,Z_{m}$ the respective martingale difference sequence. Furthermore, note that $\\begin{array}{r}{X_{m}=\\sum_{P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})}\\end{array}$ and by definition of $Y_{0},\\,j\\notin I_{P_{1}}$ for all $j>0$ . Now, since $D_{N}^{*}(x_{P_{1},P_{2}})\\geq0$ and $|\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})$ cancel out if $j\\notin I_{P_{2}}$ , ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Z_{j}\\le\\mathbb{E}\\left[\\sum_{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}\\backslash I_{P_{1}}}\\vert\\alpha_{P_{2}}\\vert D_{N}^{*}(x_{P_{1},P_{2}})\\Bigg\\vert Y_{j-1},\\ldots,Y_{0}\\right]}}\\\\ &{=}&{\\sum_{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}\\backslash I_{P_{1}}}\\vert\\alpha_{P_{2}}\\vert\\,\\mathbb{E}\\left[D_{N}^{*}(x_{P_{1},P_{2}})\\Bigg\\vert Y_{j-1},\\ldots,Y_{0}\\right]}\\\\ &{=}&{\\sum_{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}\\backslash I_{P_{1}}}\\vert\\alpha_{P_{2}}\\vert\\,\\mathbb{E}\\left[D_{N}^{*}(x_{P_{1},P_{2}})\\Bigg\\vert(Y_{k})_{k<j,k\\in I_{P_{1}}}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then, for any $t>0$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Pr}(Z_{j}\\geq t)\\leq\\mathrm{Pr}\\left(\\underset{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}\\backslash I_{P_{1}}}{\\sum}|\\alpha_{P_{2}}|\\,\\mathbb{E}\\left[D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|(Y_{k})_{k<j,k\\in I_{P_{1}}\\cup I_{P_{2}}}\\right]\\geq t\\right)}\\\\ &{\\qquad\\qquad\\leq\\underset{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}}{\\sum}\\mathrm{Pr}\\bigg(\\mathbb{E}\\left[D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|(Y_{k})_{k<j,k\\in I_{P_{1}}\\cup I_{P_{2}}}\\right]\\geq\\frac{t}{|\\alpha_{P_{2}}|}\\bigg)}\\\\ &{\\qquad\\qquad\\leq2\\underset{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}}{\\sum}\\mathrm{Pr}\\bigg(D_{N}^{*}(x_{P_{1},P_{2}})\\geq\\frac{t}{2|\\alpha_{P_{2}}|}\\bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\leq2\\sum_{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}}\\exp(b_{1})\\exp\\biggl(-\\frac{N t^{2}}{4|\\alpha_{P_{2}}|^{2}b_{2}\\tilde{m}}\\biggr)}}&{{\\displaystyle(\\mathrm{C})^{2}\\exp\\biggl(-\\frac{N t^{2}}{4|\\alpha_{P_{2}}|^{2}b_{2}\\tilde{m}}\\biggr)}}\\\\ {{\\displaystyle\\leq2|\\{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}\\}|\\exp(b_{1})\\exp\\biggl(-\\frac{N t^{2}}{4b_{2}\\tilde{m}\\sum_{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}}|\\alpha_{P_{2}}|^{2}}\\biggr)\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n\\leq2\\tilde{m}\\exp(b_{1})\\exp\\left(-\\frac{N t^{2}}{4b_{2}\\tilde{m}\\sum_{P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{2}}}|\\alpha_{P_{2}}|^{2}}\\right),\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "In the second line, we use a union bound. In the third line, we use Lemma 15 with $X\\,=$ $(Y_{k})_{k<j,k\\in I_{P_{1}}\\cup I_{P_{2}}}$ and $Y\\,=\\,(Y_{0},\\ldots,Y_{j-1})$ . In the fourth line, we use a rearrangement of the probabilistic upper bound on the star-discrepancy. In the last line, we use the definition of $\\tilde{m}$ . Now, by Theorem 17, for any $t>0$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\sum_{P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})\\geq t\\right)\\leq\\exp\\left(-\\frac{t^{2}}{28b^{\\prime}\\sum_{i=1}^{m}c_{i}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "with $b^{\\prime}=2\\tilde{m}\\exp(b_{1})$ and $\\begin{array}{r}{c_{i}^{2}=\\frac{4b_{2}\\tilde{m}}{N}\\sum_{P_{2}\\in S^{(\\mathrm{geo})}:i\\in I_{P_{2}}}|\\alpha_{P_{2}}|^{2}}\\end{array}$ . Furthermore, ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{m}\\sum_{P_{2}\\in S^{\\mathrm{(geo)}}:i\\in I_{P_{2}}}|\\alpha_{P_{2}}|^{2}=\\sum_{P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{2}}|^{2}\\sum_{i=1}^{m}\\mathbb{1}\\{i\\in I_{P_{2}}\\}=\\tilde{m}\\sum_{P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{2}}|^{2}=\\tilde{m}\\|\\alpha\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "The result follows from this. ", "page_idx": 47}, {"type": "text", "text": "Now, we are finally able to prove Lemma 14. ", "page_idx": 47}, {"type": "text", "text": "Proof of Lemma 14. We need to bound the weighted sum of the star-discrepancies we consider. This requires an extra step, since the star-discrepancy may vary among the sequences in the sum. Note that simply applying the union bound would result in a $\\log(n)$ -factor. Luckily, only the sum needs to be small, rather than all individual terms needing to be small at once. Recall that we use $S_{P_{1},P_{2}}$ to denote the set of parameters with coordinates in $I_{P_{1}}\\cup I_{P_{2}}$ . In the following, we use $D_{N}^{*}(x_{P_{1},P_{2}})$ to denote the star-discrepancy of $\\{x\\in S_{P_{1},P_{2}}\\}_{\\ell=1}^{N}$ , i.e.,  the training data points restricted to local parameters in $S_{P_{1},P_{2}}$ . We aim to apply Theorem 17 to $\\begin{array}{r}{\\sum_{P_{1},P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})}\\end{array}$ , $\\begin{array}{r}{\\sum_{P_{1},P_{2}}|w_{P_{1}}||\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})}\\end{array}$ and $\\begin{array}{r}{\\sum_{P_{1},P_{2}\\in S^{\\mathrm{(geo)}}}|w_{P_{1}}||w_{P_{2}}|D_{N}^{*}\\big(\\Bar{x}_{P_{1},P_{2}}^{-}\\big))}\\end{array}$ . ", "page_idx": 47}, {"type": "text", "text": "For illustrative purposes, we only consider the first term for now. We proceed similarly to the proof of Lemma 17. Define ", "page_idx": 47}, {"type": "equation", "text": "$$\nX_{j}\\triangleq\\mathbb{E}\\left[\\sum_{P_{1},P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|Y_{j-1},\\ldots,Y_{1}\\right],\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the expectation is with respect to the inputs $Y_{j}\\triangleq\\{(x_{j})_{\\ell}\\}_{\\ell=1}^{N}$ and $x_{j}$ parametrize $h_{j}$ . Furthermore, let ", "page_idx": 47}, {"type": "equation", "text": "$$\nX_{0}\\triangleq\\mathbb{E}\\left[\\sum_{P_{1},P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|D_{N}^{*}(x_{S_{P_{1}},P_{2}})\\right]\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "and $Z_{1}\\triangleq X_{1}-X_{0}$ . Clearly, $X_{0},\\ldots,X_{m}$ is a martingale sequence and $Z_{1},\\ldots,Z_{m}$ the respective martingale difference sequence. Furthermore, $\\begin{array}{r}{X_{m}=\\sum_{P_{1},P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})}\\end{array}$ . Now, since $D_{N}^{*}(x_{P_{1},P_{2}})\\geq0$ and $\\lvert\\alpha_{P_{1}}\\rvert\\rvert\\alpha_{P_{2}}\\lvert D_{N}^{*}(x_{P_{1},P_{2}})$ cancel out if $j\\notin I_{P_{1}}\\cup I_{P_{2}}$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Z_{j}\\ge\\mathbb{E}\\left[\\sum_{\\substack{P_{1},P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{1}}\\cup I_{P_{2}}}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|Y_{j-1},\\ldots,Y_{1}\\right]}\\\\ {=\\sum_{\\substack{P_{1},P_{2}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{1}}\\cup I_{P_{2}}}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|\\,\\mathbb{E}\\left[D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|Y_{j-1},\\ldots,Y_{1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "equation", "text": "$$\n=2\\sum_{P_{1}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{1}}}\\sum_{P_{2}\\in S^{(\\mathrm{geo})}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|\\,\\mathbb{E}\\left[D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|Y_{j-1},\\ldots,Y_{1}\\right].\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "In the last step, we used the observation that for $j$ to be contained in $I_{P_{1}}\\cup I_{P_{2}}$ , it has to be contained in at least one of the two sets. Hence, we can enumerate the admissible coordinate sets by fixing $P_{1}$ , such that $I_{P_{1}}$ contains $j$ and combine it with all $I_{P_{2}}$ . The factor two arises from doing the same with $P_{1}$ when fixing $P_{2}$ . ", "page_idx": 48}, {"type": "text", "text": "Now, for any $t>0$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}(Z_{j}\\geq t)\\leq\\operatorname*{Pr}\\left(2\\sum_{\\substack{P_{1}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{1}}\\,P_{2}\\in S^{(\\mathrm{geo})}}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|\\operatorname{\\mathbb{E}}\\left[D_{N}^{*}(x_{P_{1},P_{2}})\\bigg|Y_{j-1},\\ldots,Y_{1}\\right]\\geq t\\right)\n$$", "text_format": "latex", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le2\\displaystyle\\sum_{P_{1}\\in S^{(\\mathrm{spos})};j\\in P_{1}}\\operatorname*{Pr}_{1}\\left(\\displaystyle\\sum_{P_{2}\\in S^{(\\mathrm{spos})}}|\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})\\ge\\frac{t}{4|\\alpha_{P_{1}}|}\\right)~~~~~~~~~~~~~~~~~(\\mathbb{C}.168)}\\\\ &{\\le2\\displaystyle\\sum_{P_{1}\\in S^{(\\mathrm{spos})};j\\in P_{1}}\\exp\\left(-\\frac{N t^{2}}{16\\cdot224|\\alpha_{P_{1}}|^{2}||\\mathbb{Z}||\\mathbb{Z}|^{2}(\\bar{m})^{2}\\exp(b_{1})b_{2}}\\right)~~~~~~~~~~~~~~~(\\mathbb{C}.169)}\\\\ &{\\le2\\|\\{P_{1}\\in S^{(\\mathrm{geo})}:j\\in I_{P_{1}}\\}|\\exp\\left(-\\frac{N t^{2}}{16\\cdot224||\\alpha||_{2}^{2}(\\bar{m})^{2}\\exp(b_{1})b_{2}\\sum_{P_{1}\\in S^{(\\mathrm{geo})};j\\in P_{1}}|\\alpha_{P_{1}}|^{2}}\\right)}\\\\ &{\\le2\\bar{m}\\exp\\left(-\\frac{N t^{2}}{16\\cdot224||\\alpha||_{2}^{2}(\\bar{m})^{2}\\exp(b_{1})b_{2}\\sum_{P_{2}\\in S^{(\\mathrm{geo})};j\\in P_{1}}|\\alpha_{P_{2}}|^{2}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "In second line, we use the union bound and Lemma 15 with $X\\,=\\,(Y_{k})_{k<j,k\\in I_{P_{1}}\\cup I_{P_{2}}}$ and $Y=$ $\\left(Y_{k}\\right)_{k>j,k\\in I_{P_{1}}\\cup I_{P_{2}}}.$ In the third line, we use Lemma 17. In the last line, we use the definition of $\\tilde{m}$ . Applying Theorem 17 and bounding $\\textstyle\\sum_{j}c_{j}^{2}$ exactly as in the proof of Lemma 17 yields ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left(\\sum_{P_{1},P_{2}\\in S^{\\mathrm{(geo)}}}|\\alpha_{P_{1}}||\\alpha_{P_{2}}|D_{N}^{*}(x_{P_{1},P_{2}})\\geq t\\right)\\leq\\exp\\left(-\\frac{N t^{2}}{\\tilde{b}_{1}\\|\\alpha\\|_{2}^{4}}\\right).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "One can similarly repeat this argument for the remaining terms of ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{P_{1},P_{2}\\in S^{(\\mathrm{geo})}}(c_{1}|\\alpha P_{1}||\\alpha P_{2}|+c_{2}(|w_{P_{1}}||\\alpha P_{P_{2}}|+|w_{P_{1}}||w_{P_{2}}|))D_{N}^{*}(x_{P_{1},P_{2}})).\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Using that $\\lVert\\alpha\\rVert_{2}^{2}\\leq\\lVert\\alpha\\rVert_{1}^{2}$ and solving for the appropriate $\\delta$ yields the desired result. ", "page_idx": 48}, {"type": "text", "text": "C.4 Bound on the mixed derivatives ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Let $\\begin{array}{r}{O=\\sum_{P\\in\\{I,X,Y,Z\\}\\otimes n}\\alpha_{P}P}\\end{array}$ be an observable that can be written as a sum of geometrically local observables. In the following, we derive an expression for the mixed partial derivatives of $\\operatorname{tr}(P\\rho(x))$ , using tools from the spectral flow formalism [59, 60, 61]. This allows us to bound the Hardy-Krause variation (Equation (A.36)) in Appendix C.2. Let the spectral gap of $H(x)$ be lower bounded by some constant $\\gamma$ for all choices of parameters $x\\in[-1,1]^{m}$ . Then, by the spectral flow formalism [59, 60, 61], the directional derivative of a ground state of $\\boldsymbol{\\cal H}(\\boldsymbol{x})$ in the direction defined by the parameter unit vector $\\hat{u}$ is given by ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\hat{u}}\\rho(x)=\\hat{u}\\cdot\\nabla_{x}\\rho(x)=i[D_{\\hat{u}}(x),\\rho(x)]\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where ", "page_idx": 48}, {"type": "equation", "text": "$$\nD_{\\hat{u}}(x)=\\int_{-\\infty}^{+\\infty}W_{\\gamma}(t)e^{i t H(x)}\\frac{\\partial H}{\\partial\\hat{u}}(x)e^{-i t H(x)}d t,\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "and $W_{\\gamma}(t)$ is defined by ", "page_idx": 49}, {"type": "equation", "text": "$$\n|W_{\\gamma}(t)|\\leq\\left\\{\\frac{\\frac{1}{2}}{35e^{2}(\\gamma|t|)^{4}e^{-\\frac{2}{7}\\frac{\\gamma|t|}{\\log^{2}(\\gamma|t|)}}}\\quad0\\leq\\gamma|t|\\leq\\theta,\\right.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "The parameter $\\theta$ is chosen to be the largest real solution of $\\begin{array}{r}{35e^{2}(\\gamma|t|)^{4}\\exp\\!\\left(-\\frac{2}{7}\\frac{\\gamma|t|}{\\log^{2}(\\gamma|t|)}\\right)=1/2}\\end{array}$ ", "page_idx": 49}, {"type": "text", "text": "This allows to us to obtain an expression of the first order derivative of $\\rho(x)$ with respect to some parameter $x_{k}$ . Consider the unit vector $\\hat{u}=\\hat{e}_{k}\\triangleq(0,\\ldots0,1,0,\\ldots0)^{T}$ , where the 1 is in the kth position. Then, the directional derivative in the direction given by $e_{k}$ is ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\hat{e}_{k}}\\rho(x)=\\boldsymbol{\\hat{e}}_{k}\\cdot{\\boldsymbol{\\nabla}}_{x}\\rho(x)=\\frac{\\partial}{\\partial x_{k}}\\rho(x)=i[D_{\\hat{e}_{k}}(x),\\rho(x)].\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Hence, we obtain ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial x_{1}}\\operatorname{tr}(P\\rho(x))=\\operatorname{tr}\\biggr(P\\frac{\\partial}{\\partial x_{1}}\\rho(x)\\biggr)=i\\operatorname{tr}(P[D_{\\hat{e}_{1}}(x),\\rho(x)])=i\\operatorname{tr}([P,D_{\\hat{e}_{1}}(x)]\\rho(x)).\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "In order to compute the mixed derivative of second order, we now apply the product rule to this expression, which yields ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial^{2}}{\\partial x_{1}\\partial x_{2}}\\alpha_{P}\\operatorname{tr}(P\\rho(x))=\\frac{\\partial}{\\partial x_{2}}i\\alpha_{P}\\operatorname{tr}([P,D_{\\hat{e}_{1}}(x)]\\rho(x))}}\\\\ &{}&{=i\\alpha_{P}\\left(\\operatorname{tr}\\left(\\left[P,\\frac{\\partial}{\\partial x_{2}}D_{\\hat{e}_{1}}(x)\\right]\\rho(x)\\right)-\\operatorname{tr}\\left([P,D_{\\hat{e}_{1}}(x)]\\frac{\\partial}{\\partial x_{2}}\\rho(x)\\right)\\right)}\\\\ &{}&{=\\alpha_{P}\\operatorname{tr}\\left(i\\left[P,\\frac{\\partial}{\\partial x_{2}}D_{\\hat{e}_{1}}(x)\\right]\\rho(x)\\right)-\\operatorname{tr}([P,D_{\\hat{e}_{1}}(x)],D_{\\hat{e}_{2}}(x)]\\rho(x)).}\\\\ &{}&{=\\alpha_{P}\\operatorname{tr}\\left(i\\left[P,\\frac{\\partial}{\\partial x_{2}}D_{\\hat{e}_{1}}(x)\\right]\\rho(x)\\right)-\\operatorname{tr}([[P,D_{\\hat{e}_{1}}(x)],D_{\\hat{e}_{2}}(x)]\\rho(x)).}\\\\ &{}&{:=\\dots.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Note that the terms of this expression can be treated similarly as the first partial derivative. For each additional partial derivative, we obtain terms consisting of the product with nested commutators with $\\rho(x)$ under the trace. The nested commutators contain $D_{\\hat{e}_{j}}(x)$ or partial derivatives of it, for which we will later derive an explicit form. Hence, we can apply the same scheme until we arrive at the $k$ -th partial derivative. In order to formalize this statement, we need to introduce some additional notation. ", "page_idx": 49}, {"type": "text", "text": "Throughout the rest of this section, we use the notation \u2202\u2202|xBB| to denote the mixed derivative with respect to all parameters $x_{i}\\in B$ for some set $B$ . ", "page_idx": 49}, {"type": "text", "text": "Definition 8. Let $k\\in\\mathbb{N}.$ Let $A\\subseteq[k]$ be a set of size $|A|=m$ . Define an ordering $l_{1}<l_{2}<\\cdot\\cdot<l_{m}$ over the elements $l_{1},l_{2},\\ldots,l_{m}\\in A$ of $A$ . Then, we define ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\bigoplus_{l\\in A}\\partial^{B_{l}}l\\triangleq i^{m}\\operatorname{tr}\\left(\\left[\\left[\\cdots\\left[\\left[P,\\frac{\\partial^{|B_{l_{1}}|}}{\\partial x_{B_{l_{1}}}}D_{\\hat{e}_{l_{1}}}(x)\\right],\\frac{\\partial^{|B_{l_{2}}|}}{\\partial x_{B_{l_{2}}}}D_{\\hat{e}_{l_{2}}}(x)\\right]\\cdots\\right],\\frac{\\partial^{|B_{l_{m}}|}}{\\partial x_{B_{l_{m}}}}D_{\\hat{e}_{l_{m}}}(x)\\right]\\rho(x)\\right),\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $B_{j}\\subset[k]$ . We refer to the nested commutators under the trace as summands. The set $A$ and collection $\\{B_{l}\\}_{l\\in A}\\triangleq B_{A}$ satisfy the following conditions: ", "page_idx": 49}, {"type": "text", "text": "1. Each summand contains $D_{\\hat{e}_{1}}(x)$ . ", "page_idx": 49}, {"type": "text", "text": "This notation gives a compact way of expressing the terms of the mixed derivative and allows us to address each mixed derivative of the terms $D_{\\hat{e}_{j}}$ individually. Each term $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ contains the product of $m+1$ matrix-valued functions (including $\\rho(x)$ , which depend on $x$ . The set $A$ denotes the partial derivatives on the factor $\\rho(x)$ , which have been differentiated using Equation (C.174) when applying the product rule. We will address the partial derivatives on $D_{\\hat{e}_{j}}$ later in this section, when we derive an upper bound for $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ . ", "page_idx": 49}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "The first condition underlines that the first partial derivative on $\\rho(x)$ is necessarily computed via Equation (C.174) and thus contained in each term. The second condition reflects that each partial derivative operates on exactly one factor in each summand when applying the product rule. The third condition arises from the order, by which the partial derivatives are computed. For example, when we apply\u2202x \u2202\u2032 after $\\frac{\\partial}{\\partial x_{j}}$ \u2202xj , the $\\textstyle{\\frac{\\partial}{\\partial x_{j}}}D_{{\\hat{e}}_{j}^{\\prime}}$ can not occur in any term, since no term contained $D_{\\hat{e}_{j}^{\\prime}}$ when the partial derivatives $\\frac{\\partial}{\\partial x_{j}}$ were computed. ", "page_idx": 50}, {"type": "text", "text": "We can show that the mixed partial derivatives of $\\alpha P\\operatorname{tr}(P\\rho(x))$ can be written in terms of $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ . Lemma 18 (Mixed derivative). Let ${\\mathcal{A}}_{k}=\\{A\\subseteq[k]:1\\in A\\}$ and $B_{A}$ be as in Definition 8. The mixed derivative of the ground state property $\\operatorname{tr}(P\\rho(x))$ is given by ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{\\partial^{k}}{\\partial x_{1}\\ldots\\partial x_{k}}\\alpha_{P}\\operatorname{tr}(P\\rho(x))=\\alpha_{P}\\sum_{A\\in\\mathcal{A}_{k}}\\sum_{(B_{1},\\ldots,B_{|A|})\\in\\mathcal{B}_{A}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! \n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. We proceed via induction. First, we verify that $\\begin{array}{r}{\\frac{\\partial}{\\partial x_{k}}\\,\\textcircled{=}_{l\\in A}\\partial^{B_{l}}l}\\end{array}$ with $A\\in A_{k-1}$ and $A\\cup$ $\\cup_{j=1}^{m}B_{j}\\,=\\,[k\\,-\\,1]$ yield summands which fulfill the criteria for summands of the $k$ -th partial derivative stated in Definition 8. Then, we show that each summand of the $k$ -th derivative stems from a unique summand from the $(k-1)$ -th partial derivative. ", "page_idx": 50}, {"type": "text", "text": "For the first part, let $|A|=m$ . Furthermore, let ", "page_idx": 50}, {"type": "equation", "text": "$$\nI_{s}(j)={\\binom{\\{j\\}}{\\emptyset}}\\quad\\mathrm{if}\\;s=k\\;.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Then, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial x_{k}}\\underset{l\\in A}{\\underbrace{\\mathbf{\\Theta}\\otimes}}\\partial^{B_{l}}l=\\sum_{j=1}^{m}\\underset{l_{j}\\in A}{\\underbrace{\\mathbf{\\Theta}\\otimes}}\\partial^{B_{l_{j}}\\cup I_{j}(l)}l+\\underset{l\\in A\\cup\\{k\\}}{\\underbrace{\\mathbf{\\Theta}\\otimes}}\\partial^{B_{l}}l,\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "where each summand fulfills the properties, since $k>l$ for all $l\\in A$ . ", "page_idx": 50}, {"type": "text", "text": "Next, let $A^{\\prime}\\in\\mathcal A_{k}$ and let $B_{A^{\\prime}}$ be the corresponding collection. Then, it is easy to see that, if $k\\in B_{l_{j}}$ , it stems from the summand $\\circledcirc_{l\\in A^{\\prime}}\\partial^{B_{l}}l$ with $B_{l_{1}},\\ldots,B_{l_{j}}\\setminus\\{k\\},\\ldots,B_{l_{m}}$ . If $k\\in A^{\\prime}$ , it stems from $\\circledcirc_{l\\in A\\setminus\\{k\\}}\\partial^{B_{l}}l$ . \u53e3 ", "page_idx": 50}, {"type": "text", "text": "With this expression in hand, we can move forward to bounding the mixed derivative, which we need in order to bound the Hardy-Krause variation. First, we will upper bound the number of terms in $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ . Then, we derive an upper bound on the individual terms. For the first step, we exploit the conditions on the sets defining the mixed derivative. When we drop the third requirement in definition Definition 8, $|\\boldsymbol{{B}}_{A}|$ corresponds to the number of ways of assigning $k-m$ distinct balls to $m$ bins. Thus, we obtain the following result on the number of terms $\\bar{\\circledcirc}_{l\\in A}\\,\\bar{\\partial}^{B_{l}}l$ in the $k$ -th mixed derivative. ", "page_idx": 50}, {"type": "text", "text": "Lemma 19. Let $\\boldsymbol{\\mathcal{A}}$ denote the set of all subsets of $[k]$ . Then, the number of summands in the expression $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ is upper bounded by ", "page_idx": 50}, {"type": "equation", "text": "$$\n|\\mathcal{B}_{A}|\\leq\\sum_{s=1}^{k}{\\binom{k}{s}}s^{k-s}.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Proof. There are $\\begin{array}{r}{|\\mathcal{A}|=\\sum_{s=1}^{k}\\binom{k}{s}_{.}}\\end{array}$ different subsets of $[k]$ . For each set $A$ with $|{\\boldsymbol{A}}|=s$ , there are $s$ sets $B_{l}$ . Dropping the third condition in Definition 8, we observe that each of the $k-s$ elements in $[k]\\setminus A$ can be in any of the $s$ sets. Thus, we obtain the claimed upper bound. \u53e3 ", "page_idx": 50}, {"type": "text", "text": "In the next step, we aim to bound each individual term of the mixed derivative $\\circledcirc_{l\\in A}\\partial^{B_{l}}l.$ . Therefore, a crucial step is to bound the spectral norm of each factor. We first derive a preliminary result on the mixed derivatives of the factors in $D_{\\hat{e}_{j}}(x)$ , which depend on $x$ . This can be done using Duhamel\u2019s Formula for the derivative of the exponential map on $e^{H(x)}$ , where we exploit that we only compute the derivative with respect to one parameter at a time, such that we can treat $H(x)$ as a function, which only depends on one parameter. ", "page_idx": 50}, {"type": "text", "text": "", "page_idx": 51}, {"type": "text", "text": "Theorem 18 (Derivative of the exponential map; Theorem 3a in [95])). Let $A(t):\\mathbb{R}\\to\\mathbb{C}^{n\\times n}$ . Then, ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\frac{d}{d t}e^{A(t)}=\\int_{0}^{1}e^{(1-s)A(t)}\\left(\\frac{d A(t)}{d t}\\right)e^{s A(t)}d s.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Lemma 20. Let k \u2208[n], B \u2286[n] \\ {k}, such that  \u2202|\u2202Cx|Chj $\\begin{array}{r}{\\left\\lVert\\frac{\\partial^{|C|}h_{j}}{\\partial x_{C}}\\right\\rVert_{\\infty}\\leq1\\,\\forall C\\subseteq B\\cup\\{k\\}}\\end{array}$ . Then ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial^{|B|}}{\\partial x_{B}}\\left(e^{i t H(x)}\\left(\\frac{\\partial h_{j}}{\\partial x_{k}}\\right)e^{-i t H(x)}\\right)\\right\\|_{\\infty}\\leq2^{|B|+1}(|B|+1)^{|B|+1}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof. By Theorem 18, the mixed derivative equals the sum of terms of the form ", "page_idx": 51}, {"type": "equation", "text": "$$\nT=\\int_{0}^{1}\\cdot\\cdot\\cdot\\int_{0}^{1}\\prod_{l}f_{l}(s_{l})d s_{l}\\cdot\\cdot\\cdot d s_{1},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where fl(sl) can be any of e(1\u2212sl)iH(x), esliH(x), \u2202\u2202xhj or 1. By our assumption and the CauchySchwartz inequality, each term $T$ satisfies $\\|T\\|_{\\infty}\\leq1$ . Furthermore, by the product rule, the number of terms is smaller than $\\Pi_{j=1}^{|B|}(2j+1)$ . Since each term of the $l$ th partial derivative (including $k$ ) is the product of at most $2l+1$ factors depending on $x$ , such that the $(l+1)$ th derivative contains at most $2l+1$ -times as many factors. Thus, the number of terms is bounded above by ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\prod_{j=1}^{|B|}(2j+1)\\leq\\prod_{j=1}^{|B|+1}(2j)=2^{n}n!\\leq2^{|B|+1}(|B|+1)^{|B|+1},\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "as required. ", "page_idx": 51}, {"type": "text", "text": "Now we can bound the terms $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ . ", "page_idx": 51}, {"type": "text", "text": "Lemma 21 (Bound components of the derivative). Let $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ be as in Definition 8. Then ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left|\\bigotimes_{l\\in A}\\partial^{B_{l}}l\\right|\\leq(2C_{\\gamma})^{|A|}\\prod_{s=1}^{|A|}2^{|B_{l s}|+1}(|B_{l s}|+1)^{|B_{l s}|+1}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Proof. Recall that ", "page_idx": 51}, {"type": "equation", "text": "$$\nD_{\\hat{u}}(x)=\\int_{-\\infty}^{+\\infty}W_{\\gamma}(t)e^{i t H(x)}\\frac{\\partial H}{\\partial\\hat{u}}(x)e^{-i t H(x)}d t,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $W_{\\gamma}(t)$ , such that ", "page_idx": 51}, {"type": "equation", "text": "$$\n|W_{\\gamma}(t)|\\leq\\left\\{\\frac{\\frac{1}{2}}{35e^{2}(\\gamma|t|)^{4}e^{-\\frac{2}{7}\\frac{\\gamma|t|}{\\log^{2}(\\gamma|t|)}}}\\right.\\quad\\gamma|t|>\\theta,\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "where $\\theta$ is chosen to be the largest real solution of $\\begin{array}{r}{35e^{2}(\\gamma|t|)^{4}\\exp\\!\\left(-\\frac{2}{7}\\frac{\\gamma|t|}{\\log^{2}(\\gamma|t|)}\\right)=1/2}\\end{array}$ . It is also useful to note that $\\operatorname*{sup}_{t}|W_{\\gamma}(t)|=1/2$ . ", "page_idx": 51}, {"type": "text", "text": "By definition of the terms $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ , the Cauchy-Schwartz inequality, and $\\|[A,B]\\|_{\\infty}\\;\\;\\leq$ $2\\|A\\|_{\\infty}\\|B\\|_{\\infty}$ , we obtain ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\left|\\underset{l\\in A\\;}{\\bigcap}\\partial^{B_{l}}l\\right|\\leq\\left(\\int_{-\\infty}^{+\\infty}|W_{\\gamma}(t)|d t\\right)^{|A|}2^{|A|}\\prod_{s=1}^{|A|}\\operatorname*{sup}_{t}\\left\\|\\frac{\\partial^{|B_{l s}|}}{\\partial x_{B_{l s}}}\\left(e^{i t H(x)}\\left(\\frac{\\partial h_{j_{s}}}{\\partial x_{s}}\\right)e^{-i t H(x)}\\right)\\right\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "We bound each term individually. For the first term, we proceed in a similar manner as in [2] (Lemma 3). Namely, by Equation (S32) in [2], we can bound this integral by ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\int_{t^{*}}^{+\\infty}|W_{\\gamma}(t)|d t\\leq\\frac{245}{2}e^{2}\\gamma^{-1}\\left(\\frac{1}{1-\\frac{35\\log^{2}(\\gamma t^{*})}{\\gamma t^{*}}}\\right)(\\gamma t^{*})^{10}e^{-\\frac{2}{7}\\frac{\\gamma t^{*}}{\\log^{2}(\\gamma t^{*})}}\\triangleq C_{\\gamma}^{\\prime},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "by choosing $t^{*}$ such that $\\gamma t^{*}=\\operatorname*{max}(5900,\\alpha,7(d+11),\\theta)$ for some constant $\\alpha$ . Here, we use $C_{\\gamma}^{\\prime}$ to denote a constant that depends only on $\\gamma$ . Moreover, since $|W_{\\gamma}(t)|\\leq\\frac{1}{2}$ , we can conclude that ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\int_{-\\infty}^{+\\infty}|W_{\\gamma}(t)|d t\\leq\\int_{-t^{*}}^{t^{*}}\\frac{1}{2}\\,d t+2\\int_{t^{*}}^{+\\infty}|W_{\\gamma}(t)|\\,d t\\leq\\frac{\\operatorname*{max}(5900,\\alpha,7(d+11),\\theta)}{\\gamma}+2C_{\\gamma}^{\\prime}\\triangleq C_{\\gamma},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $C_{\\gamma}$ is also a constant that only depends on $\\gamma$ . By Lemma 20, we obtain the desired statement. ", "page_idx": 52}, {"type": "text", "text": "Lemma 22 (Bounding the $k$ -th mixed derivative). The $k$ -th mixed derivative of $\\alpha_{P}\\operatorname{tr}(P\\rho(x))$ is bounded by ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\partial^{k}}{\\partial x_{1}\\dots\\partial x_{k}}}\\alpha_{P}\\operatorname{tr}(P\\rho(x))\\right|\\leq\\left|\\alpha_{P}\\right|2^{\\mathcal{O}(k\\log(k))}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. First, we derive an upper bound on the terms $\\circledcirc_{l\\in A}\\partial^{B_{l}}l$ , which is independent of $A$ and $B_{A}$ . Proceeding from the result of Lemma 21, we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n(2C_{\\gamma})^{|A|}\\prod_{s=1}^{|A|}2^{|B_{l s}|+1}(|B_{l s}|+1)^{|B_{l s}|+1}\\leq(2C_{\\gamma})^{|A|}2^{k}\\prod_{s=1}^{|A|}k^{|B_{l s}|+1}\\leq(C_{1}k)^{k},\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where we used $|A|\\leq k$ and $\\begin{array}{r}{\\sum_{l}(|B_{l}|+1)=k}\\end{array}$ and $C_{1}=4C_{\\gamma}$ . Furthermore, from Lemma 19, it is easy to see that $|\\beta_{\\mathcal{A}}|\\leq k^{k}$ . Thus, we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial^{k}}{\\partial x_{1}\\ldots\\partial x_{k}}\\alpha_{P}\\operatorname{tr}(P\\rho(x))\\right|\\leq|\\mathcal{B}_{A}||\\alpha_{P}|(C_{1}k)^{k}\\leq|\\alpha_{P}|C_{1}^{k}k^{2k}=|\\alpha_{P}|2^{\\mathcal{O}(k\\log(k))}.\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Note that when deriving this result, we do not require that the parameters for the mixed derivatives are distinct. Assuming that $\\|H(x)\\|_{W^{k,\\infty}([-1,1]^{m})}\\leq1$ , we can induce an order recover the above bound for any mixed derivative of order $k$ . ", "page_idx": 52}, {"type": "text", "text": "Corollary 8. If $\\begin{array}{r}{\\lceil\\|H(x)\\|_{W^{k,\\infty}([-1,1]^{m})}\\leq1,t h e n\\,\\|\\alpha_{P}\\operatorname{tr}(P\\rho(x))\\|_{W^{k,\\infty}}\\leq|\\alpha_{P}|2^{\\mathcal{O}(k\\log(k))}.}\\end{array}$ ", "page_idx": 52}, {"type": "text", "text": "Proof. Note that the bound from Lemma 22 is agnostic to the explicit directions $\\hat{e}_{j}$ of the derivatives. Thus, we can choose any mixed derivative $\\lambda\\:\\in\\:\\mathbb{N}_{0}^{k}$ such that $\\sum_{j=1}^{k}\\lambda_{j}\\ =\\ k$ and fix an order $o:\\mathrm{dom}(\\lambda)\\to[k]$ . Then, we can bound the mixed derivative on $o(\\lambda)$ using the same approach as in Lemma 22 to obtain the bound. \u53e3 ", "page_idx": 52}, {"type": "text", "text": "D Details of numerical experiments ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "In this section, we discuss the numerical experiments in detail. ", "page_idx": 52}, {"type": "text", "text": "D.1 Experimental setup ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "As in [2], we consider the two-dimensional antiferromagnetic Heisenberg model with spin- $1/2$ particles placed on sites in a two-dimensional lattice. The corresponding Hamiltonian is ", "page_idx": 52}, {"type": "equation", "text": "$$\nH=\\sum_{\\langle i j\\rangle}J_{i j}(X_{i}X_{j}+Y_{i}Y_{j}+Z_{i}Z_{j}),\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "where $\\langle i j\\rangle$ denotes all pairs of neighboring sites on the lattice. The coupling terms $J_{i j}$ correspond to the parameters $x$ of the Hamiltonian and are sampled uniformly from $[0,2]$ (and then mapped to lie in $[-1,1]$ for our ML algorithm). The goal of the numerical experiment is to predict the two-body correlation functions, i.e., the expectation value of ", "page_idx": 53}, {"type": "equation", "text": "$$\nC_{i j}=\\frac{1}{3}(X_{i}X_{j}+Y_{i}Y_{j}+Z_{i}Z_{j})\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "for all neighboring sites $\\langle i j\\rangle$ . ", "page_idx": 53}, {"type": "text", "text": "To this end, we generate data similarly to [1, 2], approximating the ground state and corresponding correlation functions for the Hamiltonian Equation (D.1) of different lattice sizes and choices of coupling parameters $J_{i j}$ . We consider lattice sizes of $4\\times5\\,=\\,20$ up to $9\\times5\\,=\\,45$ . For each lattice size, we generate two datasets of size 4096, one with uniformly randomly distributed $J_{i j}$ and one where the coupling parameters are distributed as a Sobol sequence. We obtained the data by approximating the ground state using the density-matrix renormalization group (DMRG) [96] based on matrix-product-states (MPS) [97], as has been done in [1, 2]. The simulations were performed on Nvidia T4 and A40 graphical processing units (GPUs). The former were used for lattice sizes from $4\\times5$ up to $7\\times5$ while the latter were used for lattice sizes $8\\times5$ and $9\\times5$ . Depending on system size, we required between $\\approx50$ and 200 hours on the respective hardware component to simulate one dataset of size 4096. ", "page_idx": 53}, {"type": "text", "text": "Our deep learning model was also trained on Nvidia T4 and A40 GPUs. We trained the models for all respective correlation terms in parallel, by training a full model $f_{i j}^{\\Theta,w}$ (we omit the indices for the model\u2019s parameters) for each term and minimizing the combined loss function ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\sum_{\\langle i j\\rangle}\\sum_{\\ell=1}^{N}\\bigl|f_{i j}^{\\Theta,w}(x_{\\ell})-(C_{i j})_{\\ell}\\bigr|^{2}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "for the sake of time efficiency. For each data point, we trained a combined model for 500 epochs. For the terms of the local models $f_{P}^{\\theta_{P}}$ , as defined in Definition 6, we used fully connected deep neural networks with five hidden layers of width 200. For training, we used the AdamW optimization algorithm [83]. Depending on the system size and the amount of training data, this took between 0.5 and 20 hours. As a baseline, we compared against the best model from [2]. The code can be found at https://github.com/marcwannerchalmers/learning_ground_states.git. ", "page_idx": 53}, {"type": "text", "text": "D.2 Additional experiments and discussion ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "In this section, we discuss the results of the numerical experiments and additional experiments performed that are not mentioned in the main text. ", "page_idx": 53}, {"type": "text", "text": "First, we perform additional experiments that analyze the scaling of the training/prediction error with respect to various parameters such as system size, local neighborhood size, and training set size (Figures 4 to 6). Importantly, in each of these, we see that the training error is small, as required by Theorem 5. Thus, as discussed in the main text, this assumption is satisfied in practice. ", "page_idx": 53}, {"type": "text", "text": "Moreover, as shown in Figure 2 (Left), the empirical prediction accuracy (RMSE) of the deep learning model is approximately constant with respect to the size of the lattice. Figure 4 (Right) further underlines this statement. The slight increase in prediction error for $\\delta_{1}>0$ (size of the local neighborhood in Equation (A.2)) present in Figure 4 (Right) when increasing the system size from $4\\times5$ to $5\\times5$ may occur due to numerical errors in the data. From system size $5\\times5$ onwards, we rather witness random fluctuations in test errors than a systematic increase. ", "page_idx": 53}, {"type": "text", "text": "Furthermore, we observe that the deep learning model significantly outperforms the regression model with random Fourier features from [2]. On the one hand, we notice that the performance of the latter could be improved, since the hyperparameters considered for hyperparameter tuning were selected for a substantially smaller dataset. This is underlined by the drop in RMSE for the regression model on Figure 5 for $\\delta_{1}=1$ , whereas a smaller RMSE is possible when choosing $\\delta_{1}=0$ . On the other hand, we think that the vast body of deep learning research also offers room for practical improvement of our deep learning model. ", "page_idx": 53}, {"type": "text", "text": "For $\\delta_{1}=0$ , we believe that our model achieves the best possible prediction error. For training set size larger than 2048, there is little improvement on the prediction error, as opposed to all experiments with $\\delta_{1}>0$ (see Figure 2 (Center)). Furthermore, the training error remains relatively large compared to other choices of $\\delta_{1}$ (see Figure 4). Hence, we conclude that the error arising from approximating the ground state property via local functions dominates the prediction error. ", "page_idx": 53}, {"type": "image", "img_path": "ybLXvqJyQA/tmp/4d931420eac672ff3c4839a3c3e998235d06ce86b9ffb4a78923372136b09b74.jpg", "img_caption": ["Figure 4: Training/Prediction Error vs. System Size. This figure shows the scaling of the training (left) and prediction (right) RMSE with respect to system size for different values of $\\delta_{1}$ . All training sets are distributed as Sobol sequences and were trained on $N=3686$ samples. The shaded areas denote the 1-sigma error bars across the assessed ground state properties. "], "img_footnote": [], "page_idx": 54}, {"type": "image", "img_path": "ybLXvqJyQA/tmp/d49378399bf026876f23e51c954de4eb4e0a4b11fc57365a8c57128b75d4d760.jpg", "img_caption": ["Figure 5: Training/Prediction Error vs. Local Neighborhood Size. This figure shows the scaling of the training (left) and prediction (right) RMSE with respect to the local neighborhood size $\\delta_{1}$ . All training sets are of size $N=3686$ with system size $9\\times5$ . The shaded areas denote the 1-sigma error bars across the assessed ground state properties. "], "img_footnote": [], "page_idx": 54}, {"type": "text", "text": "", "page_idx": 54}, {"type": "text", "text": "When increasing $\\delta_{1}$ , we witness an increase in prediction error, especially for small training sets. This is consistent with Lemma 10, which states that the bound on the prediction error is a combination of the training error and a term proportional to the star-discrepancy (and thus increases with the dimension of the domain of the local models). Our experimental results underline the balance which must be achieved between the two in order to obtain a small prediction error. This can clearly be observed in Figure 6. The training error decreases when increasing $\\delta_{1}$ and increases with the size of the training set. Meanwhile, the test error increases when increasing $\\delta_{1}$ and decreases with the size of the training set. ", "page_idx": 54}, {"type": "text", "text": "Another interesting observation is that the ML algorithm\u2019s performance on LDS seem to be almost the same as that of uniformly random points. We believe this is due to the dominance of the local approximation error for small $\\delta_{1}$ and the drastic increase in dimensionality of the local models with increasing $\\delta_{1}$ outweighing the benefit of using LDS in practice. The dominance of approximation error is also a possible explanation for the slight decrease in prediction error with respect to the system size in Figure 2 (Left) and Figure 5. For our concrete choice of lattice shape and ground state properties, the local approximation error may be decreasing with respect to system size. However, we do not expect this to be the case in general. ", "page_idx": 54}, {"type": "image", "img_path": "ybLXvqJyQA/tmp/d9f012114431cc8df2d5d684e5f281a5d248b9d61bb124d3f6796d27d8aa77b6.jpg", "img_caption": ["Figure 6: Training/Prediction Error vs. Training Set Size. This figure shows training (left) and prediction (right) RMSE with respect to training set size for different values of $\\delta_{1}$ . All training sets are distributed as Sobol sequences and the grid size is $9\\times5$ . The shaded areas denote the 1-sigma error bars across the assessed ground state properties. "], "img_footnote": [], "page_idx": 55}, {"type": "text", "text": "D.3 Experiments with non-geometrically-local Hamiltonians ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "In this section, we assess the necessity of the geometric locality assumption by conducting numerical experiments for non-geometrically-local systems. We conclude that geometric locality is necessary for our theoretical results. ", "page_idx": 55}, {"type": "text", "text": "We conduct experiments on for a Hamiltonian given by ", "page_idx": 55}, {"type": "equation", "text": "$$\nH=\\sum_{j<i}J_{i j}(X_{i}X_{j}+Y_{i}Y_{j}+Z_{i}Z_{j}).\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The difference between this Hamiltonian and Equation (D.1) is that the sites $i$ and $j$ are not required to be neighboring, thus violating the geomtric locality assumption needed for our rigorous guarantees. We predict the same ground state properties as in the previous section, i.e., two-body correlation functions on neighboring sites. Our ML model still uses the local coordinate set $I_{P}$ from Equation (2.3). However, notice that the non-geometric-locality of the terms in the Hamiltonian impacts the number of parameters used. In other words, a larger number of parameters now affects a site in the neighborhood of each local Pauli. Furthermore, our adapted ML model assumes observables with 2-local terms1. Hence, the adapted ML model reads ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sum_{P\\in S^{\\left(2\\mathrm{-}\\mathrm{local}\\right)}}f_{P}^{\\theta_{P}}.\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Due to the lack of geometric locality and the larger number of terms of the Hamiltonian, the ground state properties are substantially harder to simulate, compared to the previous ones. We limit ourselves to uniformly random parameters and lattice shapes $4\\times5,5\\times5$ and $6\\times5$ . The former two were simulated on Nvidia T4 GPUs and the latter on Nvidia A40 GPUs, using approximately $100-500$ hours per data set of size 4096. We also notice that the approximation error due to MPS may be larger in this dataset than in the previous one. As for the previous results, we trained the models for each ground state property in parallel, by optimizing the sum of their training objectives. For the local models $f_{P}^{\\theta_{P}}$ , we used fully connected neural networks with five hidden layers of width 100. This may not be optimal, but sufficient for the purpose of assessing the scaling of the prediciton error. We trained the models for different training set sizes using $\\delta_{1}=0$ . Since the adapted models consisted substantially more terms than the previous ones, training them for 500 epochs took between 5 and 35 hours on Nvidia T4 GPUs for lattice shapes $4\\times5$ and $5\\times5$ and on Nvidia A40 GPUs on a $6\\times5$ -lattice. ", "page_idx": 55}, {"type": "text", "text": "In Figure 7 (Right), we witness system size-dependent prediction error for the smallest training set size we investigate. Since the respective training error is very small, the respective prediction errors arise due to overftiting. This effect diminishes for larger training sets. This is what one would expect when directly applying the techniques of our theoretical results to this setting. Since the number of terms increases quadratically in system size, the norm of the weights in the final layer can not be bounded by a constant anymore. Furthermore, the properties of the local approximation do not hold true anymore. Hence, the predictive capabilities of a model with $\\delta_{1}=0$ may be more limited here than in the geometrically local case. However, the prediction error may also be impacted by possible numerical errors in the training data, as well as the architecture of the local deep neural networks. Overall, these experiments illustrate the necessity of the geometric locality assumption in our theoretical results. ", "page_idx": 55}, {"type": "image", "img_path": "ybLXvqJyQA/tmp/0fbd7a370dff2f92d2d2aac84d21a28732dfb120041af28b9da1636d3835240f.jpg", "img_caption": ["Figure 7: Training/Prediction Error vs. System Size for Non-Geometrically-Local Systems. This figure shows training (left) and prediction (right) RMSE with respect to the system size for the model given in Equation (D.4), which violates geometric locality. All training sets are of size $N=3686$ and $\\delta_{1}=0$ . The shaded areas denote the 1-sigma error bars across the assessed ground state properties. "], "img_footnote": [], "page_idx": 56}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: We state, albeit informally, our results and assumptions in the abstract and instruction. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 57}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Justification: Section 3 includes a detailed discussion of the assumptions needed in the paper. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 57}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The assumptions are detailed in Section 3. The appendices contain the full proofs. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 58}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 58}, {"type": "text", "text": "Justification: The code and data for the numerical experiments are provided in the supplementary material. The details of the numerical experiments are also described in Section D of the appendices. ", "page_idx": 58}, {"type": "text", "text": "Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 58}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: The code and data for the numerical experiments is provided in the supplemental material, and the details regarding the experiments are described in the Appendix D. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 59}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: This is detailed in Appendix D. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 59}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: Error bars are included in the numerical experiments. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 59}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 60}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: This is detailed in Appendix D. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 60}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: This research conforms with the code of ethics. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 60}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 60}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 60}, {"type": "text", "text": "Justification: There are no societal impacts of the work performed. Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 60}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 61}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 61}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 61}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 61}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: When using data or parts of code from previous papers, this is cited. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 61}, {"type": "text", "text": "", "page_idx": 62}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 62}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 62}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 62}]