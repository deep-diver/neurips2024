{"references": [{"fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023", "reason": "This paper introduces Pythia, a benchmark suite used to analyze LLMs, which is crucial to the evaluation process of the Star-Agents framework."}, {"fullname_first_author": "Rohan Taori", "paper_title": "Stanford Alpaca: An instruction-following LLaMA model", "publication_date": "2023", "reason": "This paper introduces Alpaca, a foundational instruction-following model, providing a benchmark and dataset used in comparisons within the Star-Agents framework."}, {"fullname_first_author": "Lichang Chen", "paper_title": "Alpagasus: Training a better alpaca with fewer data", "publication_date": "2023", "reason": "This paper proposes Alpagasus, a method to improve instruction-tuned LLMs by enhancing data quality, an area that Star-Agents also addresses."}, {"fullname_first_author": "Ming Li", "paper_title": "From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning", "publication_date": "2023", "reason": "This paper introduces a self-guided data selection approach to enhance LLM performance through instruction tuning; Star-Agents similarly focuses on data quality optimization."}, {"fullname_first_author": "Can Xu", "paper_title": "WizardLM: Empowering large language models to follow complex instructions", "publication_date": "2023", "reason": "This paper introduces WizardLM, a benchmark and dataset which is important for evaluating instruction-following capabilities, directly relevant to Star-Agents' goals."}]}