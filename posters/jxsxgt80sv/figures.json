[{"figure_path": "jXsxGt80sv/figures/figures_2_1.jpg", "caption": "Figure 1: The diagram of the Star-Agents Framework. Step 1 is designed to gather diverse instructions and responses as shown in Appendix A.3. Step 2 focuses on selecting high-quality, tailored data from the data collected in Step 1. Finally, Step 3 aims to enhance the effectiveness and efficiency of the data generation process by evolving the Star-Agents framework.", "description": "This figure illustrates the Star-Agents framework, a three-step process for automatic data optimization. Step 1 uses multiple LLM agents to generate diverse instruction data. Step 2 evaluates the generated data using a dual-model approach to assess both difficulty and quality, selecting high-quality samples. Step 3 dynamically refines the process by prioritizing more effective LLMs, leading to improved data quality over time. The framework is designed to automate the enhancement of data quality across datasets through multi-agent collaboration and assessment.", "section": "3 Star-Agents"}, {"figure_path": "jXsxGt80sv/figures/figures_4_1.jpg", "caption": "Figure 2: Performance comparison of varied-scale models on the Alpaca and Evol-Instruct datasets. The tasks from the Evol-Instruct dataset are more complex than those from Alpaca.", "description": "This figure shows the performance of different sized language models (50M, 100M, 150M parameters) on two datasets: Alpaca and Evol-Instruct.  The y-axis represents the performance, and the x-axis shows the model size in millions of parameters.  The results indicate that the Evol-Instruct dataset, containing more complex tasks, leads to better performance as the model size increases.  The Alpaca dataset shows a less significant performance increase with larger models.", "section": "4.2 Main Results"}, {"figure_path": "jXsxGt80sv/figures/figures_4_2.jpg", "caption": "Figure 3: Illustration of dual-model evaluation. Data with a significant gap between the IFD scores of the small and large models will be prioritised.", "description": "This figure illustrates the core concept of the dual-model evaluation strategy used in the Star-Agents framework.  It shows two curves representing the Instruction Following Difficulty (IFD) scores as a function of instruction complexity for a small language model (e.g., the target model being trained) and a large language model (used for evaluation). The shaded region highlights the optimal range of complexity: data points falling within this area have a significantly different IFD for the small and large models, suggesting that the instruction is challenging enough for the small model to benefit from but not too difficult to be out of the small model's scope. This selection strategy aims to improve data quality and enhance the performance of the target model.", "section": "3.2 Evaluating Tailored Data via a Dual-model Strategy"}, {"figure_path": "jXsxGt80sv/figures/figures_7_1.jpg", "caption": "Figure 4: Radar plot of detailed scores for Llama-2-7B-star_instrcut against the major baseline on different subtasks of (a) Vicuna-Bench and (b) MT-Bench.", "description": "This figure presents a comparison of the performance of the Llama-2-7B model fine-tuned with data optimized by the Star-Agents framework (Llama-2-7B-star_instruct) against the baseline model (Llama-2-7B-evol_instruct) across various subtasks within two benchmark datasets: Vicuna-bench and MT-bench.  Each dataset evaluates different aspects of language model capabilities, such as reasoning, coding, and common sense. The radar plots visually represent the performance differences across multiple subtasks for both models.  The Llama-2-7B-star_instruct consistently outperforms the baseline across a majority of the subtasks, highlighting the efficacy of the Star-Agents data optimization approach.", "section": "4.2 Main Results"}, {"figure_path": "jXsxGt80sv/figures/figures_8_1.jpg", "caption": "Figure 5: Evolution of the typical Agent-Pairs.", "description": "This figure shows the evolution of the sampling probability of four different agent-pairs over 70,000 iterations.  The sampling probability is adjusted dynamically based on the quality of the data generated by each agent-pair.  The Mistral-ChatGPT pair consistently demonstrates high quality, resulting in an increased sampling probability. Conversely, the Phi2-ChatGPT pair shows decreasing probability due to lower-quality data generation. The ChatGLM3-ChatGPT and ChatGPT-0613-ChatGPT pairs exhibit relatively stable trajectories.", "section": "3.3 Evolving Star Agents"}]