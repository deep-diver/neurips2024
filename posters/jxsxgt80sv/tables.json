[{"figure_path": "jXsxGt80sv/tables/tables_6_1.jpg", "caption": "Table 1: Typical LLMs utilized in Star-Agents.", "description": "This table lists the Large Language Models (LLMs) used in the Star-Agents framework.  It provides details on each model, including its family, size (in parameters), the size of the data it was trained on, the training method used (either solely pretraining or a combination of Supervised Fine-Tuning and Reinforcement Learning from Human Feedback), and the source or developer of the model.", "section": "4.1 Setups"}, {"figure_path": "jXsxGt80sv/tables/tables_7_1.jpg", "caption": "Table 2: Results of different models on Vicuna-bench, WizardLM testset and MT-Bench.", "description": "This table presents the performance comparison of various LLMs on three benchmark datasets: Vicuna-bench, WizardLM testset, and MT-Bench.  The models are categorized into 1B parameter models and 7B parameter models.  For each model, the scores on each benchmark are shown, along with an average score across all three benchmarks.  The table allows for a comparison of model performance based on model size and the training data used (Alpaca, Evol-Instruct, and Star-Instruct).", "section": "4.2 Main Results"}, {"figure_path": "jXsxGt80sv/tables/tables_8_1.jpg", "caption": "Table 4: Imapct of the selection method.", "description": "This table presents the average scores achieved by the Pythia-1B model when trained using different data selection methods: Evol-Instruct, IFD, Random, and Star-instruct.  The Star-Instruct method, proposed by the authors, significantly outperforms the other methods, demonstrating its effectiveness in improving model performance by selecting high-quality data samples.", "section": "4.3 Ablation Study"}, {"figure_path": "jXsxGt80sv/tables/tables_8_2.jpg", "caption": "Table 2: Results of different models on Vicuna-bench, WizardLM testset and MT-Bench.", "description": "This table presents the performance comparison of various LLMs on three different benchmark datasets: Vicuna-bench, WizardLM testset, and MT-Bench.  The models are grouped by their size (1B and 7B parameters) and include both baselines (trained on Alpaca and Evol-Instruct datasets) and models optimized using the Star-Agents framework (Star-Instruct dataset). The table shows the average score across all three benchmarks for each model, as well as individual scores for each benchmark. The results illustrate the effectiveness of the Star-Agents framework in improving the performance of LLMs.", "section": "4.2 Main Results"}, {"figure_path": "jXsxGt80sv/tables/tables_13_1.jpg", "caption": "Table 2: Results of different models on Vicuna-bench, WizardLM testset and MT-Bench.", "description": "This table presents a comparison of various LLMs' performance across three benchmark datasets: Vicuna-bench, WizardLM testset, and MT-Bench.  The models are categorized by their size (1B or 7B parameters). For each model and benchmark, the table shows the achieved score.  This allows for a comprehensive evaluation of the models' capabilities in various tasks, including reasoning, commonsense, and coding. The \"Average\" column provides an aggregated score reflecting overall performance across the three benchmarks.", "section": "4.2 Main Results"}, {"figure_path": "jXsxGt80sv/tables/tables_15_1.jpg", "caption": "Table 2: Results of different models on Vicuna-bench, WizardLM testset and MT-Bench.", "description": "This table presents a comparison of various language models' performance across three benchmark datasets: Vicuna-bench, WizardLM testset, and MT-Bench.  The results showcase the average scores achieved by different models (including those trained with different data optimization techniques such as Alpaca, Evol-Instruct, and Star-Instruct) and different model sizes (1B and 7B parameters).  The table helps evaluate the effectiveness of the Star-Agents framework in improving model performance on instruction-following tasks.", "section": "4.2 Main Results"}, {"figure_path": "jXsxGt80sv/tables/tables_16_1.jpg", "caption": "Table 2: Results of different models on Vicuna-bench, WizardLM testset and MT-Bench.", "description": "This table presents the performance comparison of various LLMs across three benchmark datasets: Vicuna-bench, WizardLM testset, and MT-Bench.  Models are evaluated based on their instruction-following capabilities. The table shows the average scores for each model across all three benchmarks, allowing for a comprehensive comparison of performance.", "section": "4.2 Main Results"}]