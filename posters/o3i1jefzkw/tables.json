[{"figure_path": "o3i1JEfzKw/tables/tables_3_1.jpg", "caption": "Table 1: Comparison of the theoretical guarantees with and without privileged information. PI: privileged information; STD: structural assumptions on transition dynamics, e.g., deterministic transition or reachability of all states; SL: supervised learning; FA: function approximation; WSE: well-separated emission.", "description": "This table compares the theoretical guarantees for Partially Observable Markov Decision Processes (POMDPs) and Partially Observable Stochastic Games (POSGs) with and without privileged information.  It shows the sample and time complexities for various algorithms under different model assumptions.  The assumptions vary in restrictiveness, ranging from strong structural assumptions (like deterministic transitions) to weaker ones (like well-separated emissions).  The table highlights that privileged information can significantly improve the efficiency of algorithms for certain POMDP/POSG classes.", "section": "2 Preliminaries"}, {"figure_path": "o3i1JEfzKw/tables/tables_46_1.jpg", "caption": "Table 2: Rewards of different approaches for POMDPs under the deterministic filter condition.", "description": "This table presents the rewards obtained by four different reinforcement learning algorithms on four different POMDP problem instances (Cases 1-4). The algorithms are: Asymmetric optimistic NPG, Expert policy distillation, Asymmetric Q-learning, and Vanilla AAC.  The POMDPs are categorized into Deterministic POMDP and Block MDP, representing different structural properties affecting the complexity of solving them.  The results show the average reward and standard deviation achieved by each algorithm across multiple runs for each problem instance.", "section": "Numerical Validation"}]