[{"figure_path": "m0DS4OOmSY/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration about the model editing and its pitfalls in retaining edited knowledge. Left panel: model editing methods can efficiently update knowledge within language models; Right panel: when scaling editing to thousands, the model can't retain edited knowledge, see [17] for details.", "description": "This figure shows the impact of model editing on knowledge retention. The left panel demonstrates that model editing methods effectively update knowledge within language models by showing an example question and its correct answer after editing. The right panel illustrates the pitfalls of scaling model editing.  When many edits are introduced, the model struggles to maintain the edited knowledge.", "section": "1 Introduction"}, {"figure_path": "m0DS4OOmSY/figures/figures_4_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model's abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure displays the performance of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K).  The x-axis represents the number of edits applied, and the y-axis shows the model's score on each benchmark task. The results show that PMET and MEND methods are more robust to editing and maintain high performance, even with a high number of edits. However, the KN method demonstrates a significant performance decline with a very small number of edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_5_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure displays the performance of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base language model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K). The x-axis represents the number of edits performed, while the y-axis shows the model's score on each benchmark task. The results show that PMET and MEND are more robust to editing and maintain good performance across tasks, even with a large number of edits.  In contrast, KN exhibits a drastic drop in performance with fewer than ten edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_5_2.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure presents the performance of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base language model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K).  The x-axis represents the number of edits applied to the model, and the y-axis shows the model's score on each benchmark. The results show that PMET and MEND methods maintain relatively high performance even with a large number of edits, whereas KN shows a significant drop in performance with only a small number of edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_6_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure displays the performance of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (MMLU, GSM8K, BBH, CSQA, and Safety). Each graph shows the performance of a single method, plotted against the number of edits.  The results demonstrate that PMET and MEND consistently maintain the model\u2019s performance across all tasks, even with increasing numbers of edits, while KN shows significant performance degradation very early. This illustrates the varying robustness of different editing methods to edits and highlights the susceptibility of some methods to performance degradation with even small numbers of edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_7_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model's abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure shows the performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K). The results indicate that PMET and MEND are the most effective at preserving the model's abilities across all tasks, even with a large number of edits. In contrast, KN shows a significant performance drop with fewer than ten edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_16_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure shows the performance of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base language model across five different benchmark tasks (CSQA, MMLU, BBH, GSM8K). The x-axis represents the number of edits applied to the model, and the y-axis represents the model's performance on each benchmark task. The results indicate that PMET and MEND methods are most effective in preserving the model's overall abilities, even with a relatively large number of edits. In contrast, the KN method shows a significant performance drop with less than 10 edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_17_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure shows the performance of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K).  The x-axis represents the number of edits, and the y-axis represents the score achieved on each benchmark task. The figure demonstrates that PMET and MEND methods show better performance retention even with increased number of edits, while KN shows a rapid performance drop.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_18_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model's abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "The figure shows the performance of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five different benchmark tasks (CSQA, MMLU, BBH, GSM8K). The x-axis represents the number of edits performed, and the y-axis represents the score achieved on each benchmark. The results show that PMET and MEND are more robust to editing and maintain the model's general abilities even with a large number of edits. In contrast, KN shows a significant performance drop after only a few edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_19_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure displays the performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K, and a combined score).  The results show that PMET and MEND consistently maintain the model's abilities across all tasks, even as the number of edits increases. In contrast, the KN method shows a significant performance drop with fewer than ten edits.  This demonstrates the varying robustness of different model editing techniques when scaling the number of edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_20_1.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure displays the performance of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base language model.  The models were evaluated on multiple benchmark tasks across various numbers of edits.  The results show that PMET and MEND maintain good performance across all tasks even with many edits, while KN shows a significant drop in performance with only a small number of edits.  The graph visually represents the robustness and limitations of different editing techniques.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "m0DS4OOmSY/figures/figures_20_2.jpg", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This figure displays the performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five different benchmark tasks (CSQA, MMLU, BBH, GSM8K).  The x-axis represents the number of edits applied to the model, and the y-axis shows the performance score on each benchmark. The results demonstrate that PMET and MEND methods are more robust and effectively preserve the model's capabilities across various tasks even with a larger number of edits, while KN shows a significant performance drop even with fewer than ten edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}]