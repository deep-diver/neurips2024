[{"figure_path": "DAO2BFzMfy/figures/figures_0_1.jpg", "caption": "Figure 1: weights2weights (w2w) space enables controllable creation of new customized diffusion models. We model a manifold of customized diffusion models as a subspace of weights that encodes different instances of a broad visual concept (e.g., human identities, dog breeds, etc.). This forms a space that supports inverting the subject (e.g., identity) from a single image into a model, editing the subject encoded in the model, and sampling new models that encode new instances of the visual concept. Each of these operations results in a new model that can consistently generate the subject.", "description": "This figure illustrates the concept of weights2weights (w2w) space.  It shows how a manifold of customized diffusion models can be represented as a subspace of weights. This subspace allows for three key operations: single image inversion (creating a model from a single image), identity editing (modifying an existing model), and sampling (generating new models with different instances of the same visual concept).  Each operation consistently produces models that generate the intended subject.", "section": "Introduction"}, {"figure_path": "DAO2BFzMfy/figures/figures_1_1.jpg", "caption": "Figure 2: The weights2weights space operates as a meta-latent space. Unlike a traditional generative latent space, w2w space controls the model itself rather than single image instances. New identity-encoding models can be sampled from the space and edited by linearly traversing along semantic directions in weight space. Additionally, a single image can be inverted into the space to produce a model that consistently generates that identity.", "description": "This figure illustrates the weights2weights (w2w) space as a meta-latent space.  It contrasts the traditional generative latent space which controls single image instances with the w2w space which controls the model itself. The figure shows three applications of w2w space: sampling new models, editing existing models, and inverting a single image into a model. Each operation results in a new model that consistently generates the subject.", "section": "1 Introduction"}, {"figure_path": "DAO2BFzMfy/figures/figures_2_1.jpg", "caption": "Figure 3: Building weights2weights (w2w) space. We create a dataset of model weights where each model is personalized to a specific identity using low-rank updates (LoRA). These model weights lie on a weights manifold that we further project into a lower-dimensional subspace spanned by its principal components. We train linear classifiers to find disentangled edit directions in this space.", "description": "This figure illustrates the process of creating the weights2weights (w2w) space.  It starts with a dataset of identities, each of which is used to fine-tune a diffusion model using Low-Rank Adaptation (LoRA). This results in a set of model weights. Dimensionality reduction (using PCA) is then applied, creating a lower-dimensional subspace called the w2w space.  Finally, linear classifiers are trained to find interpretable directions within this space for editing.", "section": "3 Method"}, {"figure_path": "DAO2BFzMfy/figures/figures_5_1.jpg", "caption": "Figure 13: Sampled identity-encoding models from w2w space and their nearest neighbor models. The sampled identities share some characteristics with the nearest neighbors, but are still distinct. These identities can also be composed into novel contexts like a standard customized diffusion model.", "description": "This figure shows three examples of identities sampled from the w2w space, along with their nearest neighbor identities from the training set.  The goal is to illustrate that the sampling process generates novel identities that are similar but not identical to existing ones. The figure also demonstrates that these new identities can be used in various contexts, just like the original training set identities.", "section": "A Sampling"}, {"figure_path": "DAO2BFzMfy/figures/figures_5_2.jpg", "caption": "Figure 5: Qualitative comparison. w2w edits preserve identity while being disentangled and semantically aligned. Concept Sliders [17] tends to exaggerate effects which induces artifacts and degrades identity, while prompting the subject with the desired edit has unexpected effects.", "description": "This figure compares three methods for editing subject attributes in generated images:  prompting, Concept Sliders, and weights2weights (w2w). Each column shows the results of applying an edit (Woman, Chubby, Narrow Eyes) using each method. The w2w method is shown to preserve identity, apply the edit cleanly, and avoid unintended side effects. Concept Sliders tends to exaggerate the edits, leading to artifacts, and prompting often leads to unpredictable changes.", "section": "4.3 Editing Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_6_1.jpg", "caption": "Figure 6: Composing edits in w2w space. Each column represents fixed seed samples from an edited model. Multiple edits in w2w space minimally degrade the original identity or interfere with other concepts, while maintaining edit appearance across different samples.", "description": "This figure demonstrates that multiple edits can be composed linearly in the w2w space. Each column shows samples generated from a model with different combinations of edits applied. The results show that combining multiple edits does not significantly affect the original identity or interfere with other visual elements, and that the edits remain consistent across different generations.", "section": "4.3 Editing Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_7_1.jpg", "caption": "Figure 7: Single image inversion reconstructs identity and enables editing in w2w space. We present generated samples from the inverted models. These inverted identities can be composed in novel contexts and edited using our discovered semantic directions in weight space. These edits persist in appearance across generation seeds and prompts.", "description": "This figure shows the results of inverting single images into the weights2weights (w2w) space.  The leftmost column shows the input images. Subsequent columns demonstrate that the inverted models can generate realistic and consistent identities across different generation seeds and prompts. Moreover, the identities can be seamlessly edited by linearly traversing along semantic directions in the w2w space. These edits also persist across different generations.", "section": "4.4 Inverting Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_7_2.jpg", "caption": "Figure 8: Projecting out-of-distribution identities. We show that our inversion method can convert unrealistic identities into realistic renderings with in-domain facial features. Each image represents a generated sample from the inverted model. The resulting identities can be composed in novel scenes, such as playing tennis or rendered into other artistic domains.", "description": "This figure demonstrates the ability of the w2w space to project out-of-distribution identities (e.g., cartoon, painting, animation) into realistic renderings with in-domain facial features. The generated samples highlight the model's capacity to capture and reproduce key characteristics of the identities, even from unconventional sources.  Further, these inverted identities maintain consistency when composed into novel scenes or rendered in different artistic styles.", "section": "4.4 Inverting Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_8_1.jpg", "caption": "Figure 9: Scaling dataset of models further disentangles classifier directions. We highlight the trend in disentanglement of three examples where attributes may be strongly correlated among identities. As the number of models is increased, the features are less entangled.", "description": "This figure shows the effect of scaling the number of models used to create the weights2weights space on the disentanglement of attribute directions and identity preservation.  The left plot demonstrates how the entanglement (measured by cosine similarity between classifier directions) decreases as the number of models increases. The right plot shows how identity preservation (measured by ID score) increases as the number of models increases, approaching the performance of a multi-image DreamBooth model. Three specific attribute pairs are highlighted to illustrate the disentanglement trend.", "section": "4.6 Effect of Number of Models Spanning w2w Space"}, {"figure_path": "DAO2BFzMfy/figures/figures_9_1.jpg", "caption": "Figure 11: weights2weights linear subspaces can be created for other visual concepts. We follow the same procedure of applying PCA and finding edit directions with linear classifiers on datasets of models encoding dog breeds and models encoding car types.", "description": "This figure shows that the weights2weights method can be applied to visual concepts beyond human faces.  Two examples are given: dog breeds and car types. For each, the authors created a dataset of models fine-tuned to different instances of the concept (dog breeds or car types). PCA was applied to the weights to find a lower-dimensional subspace. Linear classifiers were then trained to identify directions in this subspace corresponding to semantic attributes (like size for dogs or car type for cars). The resulting subspaces allow for controllable creation of models representing new variations of the concepts. ", "section": "5 Extending to Other Domains"}, {"figure_path": "DAO2BFzMfy/figures/figures_9_2.jpg", "caption": "Figure 7: Single image inversion reconstructs identity and enables editing in w2w space. We present generated samples from the inverted models. These inverted identities can be composed in novel contexts and edited using our discovered semantic directions in weight space. These edits persist in appearance across generation seeds and prompts.", "description": "This figure demonstrates the ability of the w2w space to reconstruct an identity from a single image and further edit it by applying linear transformations in the weight space. The reconstructed identities maintain consistency across different generation seeds and prompts, highlighting the model's ability to generate diverse and realistic images while preserving the core identity.", "section": "4.4 Inverting Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_15_1.jpg", "caption": "Figure 13: Sampled identity-encoding models from w2w space and their nearest neighbor models. The sampled identities share some characteristics with the nearest neighbors, but are still distinct. These identities can also be composed into novel contexts like a standard customized diffusion model.", "description": "This figure shows several examples of identities generated from the w2w model, which is a model of the weight space of fine-tuned diffusion models.  Each sampled identity is compared to its nearest neighbor from the training dataset.  The comparison highlights that while the sampled identities share some characteristics with their nearest neighbors, they are also distinct and novel. This demonstrates the ability of the w2w model to generate diverse and realistic identities. The figure also shows that these sampled identities can be used in a variety of contexts, just like standard customized diffusion models.", "section": "A Sampling"}, {"figure_path": "DAO2BFzMfy/figures/figures_16_1.jpg", "caption": "Figure 14: Multiple edits can be controlled in a continuous manner.", "description": "This figure shows that multiple edits in the w2w space can be controlled in a continuous manner.  The edits are applied to different identities, demonstrating that the edits are disentangled and do not interfere with other attributes. The edits are also persistent, meaning that they appear consistently across different generations.", "section": "B Model Editing in w2w Space"}, {"figure_path": "DAO2BFzMfy/figures/figures_17_1.jpg", "caption": "Figure 6: Composing edits in w2w space. Each column represents fixed seed samples from an edited model. Multiple edits in w2w space minimally degrade the original identity or interfere with other concepts, while maintaining edit appearance across different samples.", "description": "This figure demonstrates that multiple edits can be applied to a single identity model using the w2w method.  The edits are shown to be disentangled, meaning they don't negatively affect other aspects of the generated image, and semantically aligned, meaning the changes accurately reflect the intended edit.  The consistent appearance of the edits across different image samples highlights the reliability and stability of the w2w approach for generating edited images.", "section": "4.3 Editing Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_17_2.jpg", "caption": "Figure 6: Composing edits in w2w space. Each column represents fixed seed samples from an edited model. Multiple edits in w2w space minimally degrade the original identity or interfere with other concepts, while maintaining edit appearance across different samples.", "description": "This figure demonstrates that multiple edits can be composed in the w2w space without significantly affecting the original identity or interfering with other concepts. The edits are consistent across different generations of images, using fixed random seeds.", "section": "4.3 Editing Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_18_1.jpg", "caption": "Figure 31: Traversal along the first three principal components in w2w space. The directions encode various entangled identity attributes such as age, gender, and facial hair.", "description": "This figure visualizes the effect of traversing along the first three principal components of the w2w space.  Each row shows a series of images generated by varying the value of a single principal component while holding the others constant. The images demonstrate that the principal components influence multiple attributes of the generated identities, such as age, gender, and facial hair, highlighting the entanglement of these features in the model's weight space. This entanglement motivates the use of linear classifiers to disentangle these attributes for more controlled editing.", "section": "Visualizing Principal Components"}, {"figure_path": "DAO2BFzMfy/figures/figures_18_2.jpg", "caption": "Figure 16: Additional examples of composing multiple edits. We provide more examples of semantic edits based on labels available from CelebA.", "description": "This figure shows more examples of applying multiple edits to the same subject in the w2w space.  The edits are shown in a sequential manner demonstrating how different attributes can be combined.  Each row showcases a different subject and different sets of edits.  The consistency of the identity across these edits is emphasized.", "section": "4.3 Editing Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_19_1.jpg", "caption": "Figure 1: weights2weights (w2w) space enables controllable creation of new customized diffusion models. We model a manifold of customized diffusion models as a subspace of weights that encodes different instances of a broad visual concept (e.g., human identities, dog breeds, etc.). This forms a space that supports inverting the subject (e.g., identity) from a single image into a model, editing the subject encoded in the model, and sampling new models that encode new instances of the visual concept. Each of these operations results in a new model that can consistently generate the subject.", "description": "This figure illustrates the weights2weights (w2w) space, which is a subspace of weights from a collection of customized diffusion models. It shows how this space allows for three main operations: single image inversion (creating a model from a single image), identity editing (modifying characteristics within an existing model), and sampling (generating new models with novel instances of the visual concept).", "section": "1 Introduction"}, {"figure_path": "DAO2BFzMfy/figures/figures_19_2.jpg", "caption": "Figure 18: Example question from the identity editing user study.", "description": "This figure shows an example question from the user study on identity editing. Users are presented with an original image and three edited versions of the same image, each created using a different method. They are asked to choose the edited image that best satisfies the criteria of semantic alignment (in this case, \"chubby\"), identity preservation, and disentanglement. This helps to evaluate the performance of each method in terms of creating edits that are both semantically correct and visually pleasing.", "section": "4.3 Editing Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_20_1.jpg", "caption": "Figure 19: Inversion into w2w space preserves identity and realism. We compare against Dreambooth fine-tuning with LoRA on multiple images and a single image.", "description": "This figure compares the results of three different methods for inverting a single image into a generative model: Dreambooth with LoRA trained on multiple images, Dreambooth with LoRA trained on a single image, and the proposed w2w inversion method.  The results show that w2w inversion produces images that are more consistent with the input image in terms of identity and realism compared to the other methods. The w2w method is particularly effective at preserving the identity of individuals across varied poses and settings, highlighting its superior performance in generating realistic and consistent image generation.", "section": "4 Experiments"}, {"figure_path": "DAO2BFzMfy/figures/figures_21_1.jpg", "caption": "Figure 20: Inversion into w2w space preserves identity and realism (cont.).", "description": "This figure shows a qualitative comparison of the inversion results obtained using different methods: Dreambooth fine-tuned with LoRA on multiple images, Dreambooth fine-tuned with LoRA on a single image, and the proposed w2w inversion. For each method, the figure presents several generated images from the same input image.  The results demonstrate that the w2w inversion method is better at preserving the identity and realism of the input image compared to the other two methods.", "section": "4 Experiments"}, {"figure_path": "DAO2BFzMfy/figures/figures_22_1.jpg", "caption": "Figure 21: Qualitative comparison of single-shot personalization methods.", "description": "This figure compares the results of three different single-shot personalization methods: Celeb-Basis, IP-Adapter FaceID, and the proposed w2w method.  The original image is shown on the left, and the results of each method are shown on the right, using the same prompt. It visually demonstrates the differences in identity preservation, image diversity, and alignment with the prompt between the methods.", "section": "4.4 Inverting Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_23_1.jpg", "caption": "Figure 18: Example question from the identity editing user study.", "description": "This figure shows an example question used in a user study to evaluate the quality of identity edits.  Users were presented with an original image and three edited versions (using different methods) and asked to select the best edit based on three criteria: semantic alignment, identity preservation, and disentanglement.", "section": "4.3 Editing Subjects"}, {"figure_path": "DAO2BFzMfy/figures/figures_24_1.jpg", "caption": "Figure 24: Projection into w2w space generalizes to a variety of inputs. A range of styles and entities can be inverted into a realistic identity in this space. Once a model is obtained, it can be prompted to generate the identity in a variety of contexts.", "description": "This figure shows three examples of out-of-distribution images (a portrait of Shakespeare, a dog, and a painting) being successfully inverted into the weights2weights space.  The resulting models, when prompted, generate realistic images of the subjects in diverse settings. This demonstrates the ability of the model to handle various input styles while maintaining the core identity.", "section": "Out-of-Distribution Projection"}, {"figure_path": "DAO2BFzMfy/figures/figures_25_1.jpg", "caption": "Figure 25: Fine-tuning on synthetic examples allows Dreambooth fine-tuning to distill a consistent identity. The left column shows a CelebA image used to condition generation of a set of identity-consistent images in the right column associated with that identity using [67]. The consistent appearance of the identity enables a more consistent identity encoding.", "description": "This figure shows how synthetic datasets were created for fine-tuning the model for different identities.  It highlights the use of a CelebA image as a starting point, which is then used to generate a consistent set of images for training. This process ensures that the model learns a consistent representation of each identity.", "section": "E Identity Datasets"}, {"figure_path": "DAO2BFzMfy/figures/figures_26_1.jpg", "caption": "Figure 26: Histogram of principal component coefficients. The first three principal component coefficients appear approximately Gaussian.", "description": "This figure shows the histograms of the coefficients for the first three principal components obtained through PCA on the model weights.  The distributions appear roughly Gaussian, suggesting independence among the principal components which supports the modelling assumption used to sample new models.", "section": "F Principal Component Basis"}, {"figure_path": "DAO2BFzMfy/figures/figures_26_2.jpg", "caption": "Figure 27: Pairwise joint histogram of principal component coefficients. We rescale the first three principal component coefficients and plot the pairwise joint distributions for visualization purposes. Given that the marginals are roughly Gaussian, the circular appearance of the joint suggests pairwise independence for the first three components.", "description": "This figure shows the pairwise joint distributions of the first three principal components after rescaling them to unit variance. The near-circular shapes in the plots indicate that the components are roughly independent, which supports the assumption made in the paper that they can be modeled as independent Gaussians.", "section": "F Principal Component Basis"}, {"figure_path": "DAO2BFzMfy/figures/figures_27_1.jpg", "caption": "Figure 28: Edit results with varying number of Principal Components. Training classifiers to find semantic weight space directions with the first 1000 Principal Components achieves the most semantically aligned and disentangled results.", "description": "This figure shows the results of applying different numbers of principal components to the task of adding a goatee to faces in images. Using only 100 principal components, the results are coarse and change several attributes at once. With 1000 principal components, the editing is much more focused and only changes the addition of a goatee. Using 10,000 principal components doesn't improve results significantly.", "section": "3.3 Finding Interpretable Weight Space Directions"}, {"figure_path": "DAO2BFzMfy/figures/figures_27_2.jpg", "caption": "Figure 29: Identity inversion results with varying number of principal components. We optimize the coefficients for the first 1000, 10,000, and 20,000 Principal Component. Each column indicates a fixed generation seed and prompt. Inversion with the first 10,000 components balances parameter efficiency, realism, and identity preservation without overfitting to the single image.", "description": "This figure shows the results of inverting a single image into the w2w space using different numbers of principal components (PCs). The image on the left is the input image. The results on the right show that using 10,000 PCs produces images that are both realistic and preserve the identity of the input image, without overfitting. Using fewer PCs (1000) underfits the data and using more PCs (20,000) overfits, causing the generated images to be less realistic and consistent.", "section": "3.4 Inversion into w2w Space"}, {"figure_path": "DAO2BFzMfy/figures/figures_27_3.jpg", "caption": "Figure 30: Identity preservation vs. number of principal components used for w2w inversion. We optimize the coefficients for the first N principal components up to 20,000 and measure the average ID score for 100 inverted FFHQ identities.", "description": "This figure shows a plot of identity preservation (ID score) against the number of principal components used in the w2w inversion method. The x-axis represents the number of principal components, while the y-axis represents the average ID score achieved on 100 inverted FFHQ (high-resolution face) images. The plot demonstrates that as the number of principal components increases, identity preservation improves, suggesting that a higher-dimensional subspace better captures the nuances of individual identities.", "section": "4.6 Effect of Number of Models Spanning w2w Space"}, {"figure_path": "DAO2BFzMfy/figures/figures_28_1.jpg", "caption": "Figure 31: Traversal along the first three principal components in w2w space. The directions encode various entangled identity attributes such as age, gender, and facial hair.", "description": "This figure shows how traversing along the first three principal components of the weights2weights space affects the generated images. Each row represents a traversal along one of the components, showing changes in attributes like age, gender, and facial hair.  The entanglement of these attributes highlights the need for disentanglement techniques, like using linear classifiers to find separating hyperplanes, to control individual features more effectively.", "section": "Visualizing Principal Components"}, {"figure_path": "DAO2BFzMfy/figures/figures_29_1.jpg", "caption": "Figure 11: weights2weights linear subspaces can be created for other visual concepts. We follow the same procedure of applying PCA and finding edit directions with linear classifiers on datasets of models encoding dog breeds and models encoding car types.", "description": "This figure shows that the proposed method can be extended to other visual concepts besides human identities.  Two examples are given: dog breeds and car types. For each concept, a linear subspace is created using PCA on a dataset of fine-tuned diffusion models. Linear classifiers then identify semantic edit directions within these subspaces, enabling controlled modifications of the generated models (e.g., changing a dog's size or a car's color).", "section": "Extending to Other Domains"}, {"figure_path": "DAO2BFzMfy/figures/figures_30_1.jpg", "caption": "Figure 33: Merging w2w models with non-identity models. Here, another model is fine-tuned to map V\u2082 to \\\"Pixar\\\" style. The two models are merged with simple addition.", "description": "This figure demonstrates the merging of two different types of models:  one from the weights2weights (w2w) space, and another fine-tuned to generate images in a specific style (Pixar).  The merging process is shown to be a simple addition of the model weights, resulting in images that combine elements of both models.  This highlights the potential for combining models from different training processes and conceptual spaces.", "section": "H Multi-Concept Merging"}, {"figure_path": "DAO2BFzMfy/figures/figures_30_2.jpg", "caption": "Figure 34: Injecting edited weights at varying timesteps. Using the edited weights at a smaller timestep T better preserves context at the expense of edit strength and fidelity.", "description": "This figure shows the results of injecting edited weights at different timesteps (T) during the image generation process.  It demonstrates a trade-off between preserving the original context of the image and the strength of the applied edit.  Using smaller timesteps (e.g., T=600) results in better context preservation, but the edit is less pronounced. Conversely, larger timesteps (e.g., T=1000) result in a stronger edit, but might negatively impact the overall context of the image.", "section": "I Timestep Analysis"}]