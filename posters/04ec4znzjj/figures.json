[{"figure_path": "04EC4ZnZJj/figures/figures_1_1.jpg", "caption": "Figure 1: FLOPs with different model hidden size and sequence lengths.", "description": "This figure shows the comparison of computational complexity (FLOPs) between the proposed MemoryFormer and a baseline Transformer model.  The x-axis represents the model's hidden size, while the y-axis represents the FLOPs per block (in billions).  Two lines are plotted for each model (Transformer and MemoryFormer), one for a sequence length of 1024 and another for a sequence length of 2048.  The results clearly demonstrate that MemoryFormer achieves significantly lower FLOPs compared to the Transformer model, especially as the hidden size and sequence length increase. This reduction in FLOPs is a key advantage of MemoryFormer, indicating its enhanced computational efficiency.", "section": "1 Introduction"}, {"figure_path": "04EC4ZnZJj/figures/figures_3_1.jpg", "caption": "Figure 2: A demonstration with T = 2 and K = 3, where z\u2081 is hashed to the bucket2 of T\u2081, z\u2082 is hashed to the bucket1 of T\u2082, z\u2083 is hashed to the bucket2 of T\u2083.", "description": "This figure demonstrates the Locality-Sensitive Hashing (LSH) process used in MemoryFormer.  It shows how three sub-vectors (z1, z2, z3) are hashed into different buckets of hash tables (T1, T2, T3). Each bucket stores a representative vector, and similar sub-vectors are mapped to the same bucket.  This illustration helps visualize how the LSH approach enables efficient retrieval of similar vectors with a reduced computational cost compared to traditional fully-connected layers.", "section": "2.2 Compute-less Locality-Sensitive Hashing"}, {"figure_path": "04EC4ZnZJj/figures/figures_4_1.jpg", "caption": "Figure 3: Left: The schematic diagram of the Memory Layer. Right: One building block of the MemoryFormer.", "description": "The figure shows two diagrams. The left diagram shows the internal structure of the Memory Layer, which is a core component of the MemoryFormer model.  It illustrates the process of hashing input vectors, retrieving a subset of vectors from memory tables (T1 to Tk), and then computing a weighted sum of the retrieved vectors to generate the output. The right diagram depicts a single building block of the MemoryFormer architecture, illustrating how the Memory Layer is integrated with the multi-head attention mechanism. The building block takes an input (X), processes it through Memory Layers to obtain query (Q), key (K), and value (V) matrices, performs a multi-head attention operation, and finally outputs a transformed representation (Y).", "section": "2.4 Architecture of MemoryFormer"}, {"figure_path": "04EC4ZnZJj/figures/figures_8_1.jpg", "caption": "Figure 4: The frequency at which each bucket in the hash table is retrieved.", "description": "This figure visualizes the distribution of hash bucket retrievals in the Memory Layer.  It shows the frequency with which each bucket in the hash tables (for Q, K, V projections and the two layers of the FFN module) is accessed.  A uniform distribution across buckets indicates that the hashing function is working effectively and the embedding space is well-utilized.  Deviations from uniformity might suggest issues with the hash function or data imbalance.", "section": "3.4 Visualization"}]