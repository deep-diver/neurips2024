[{"figure_path": "04EC4ZnZJj/tables/tables_6_1.jpg", "caption": "Table 1: Zero-shot evaluation results on public NLP benchmarks. We use \"MF\" as the abbreviation for MemoryFormer. \"Attn.\" refers to the computation of o(QK<sup>T</sup>)V. Inference FLOPs are measured for one block with sequence length of 2048.", "description": "This table presents the zero-shot evaluation results of various models on six public NLP benchmarks.  The models compared include Pythia models of different sizes (70M, 160M, 410M parameters) and their corresponding MemoryFormer variants.  The table shows the number of layers, hidden size, FLOPs (with and without attention), and the average accuracy across the six benchmarks (PIQA, WinoGrande, WSC, ARC-E, ARC-C, LogiQA).  Inference FLOPs are calculated for a single transformer block with a sequence length of 2048, to allow for comparison of computational efficiency.", "section": "3.1 Evaluation Across Different Scales"}, {"figure_path": "04EC4ZnZJj/tables/tables_6_2.jpg", "caption": "Table 2: Comparison of different efficient transformer methods based on Pythia-410M. Inference FLOPs are measured for one block with sequence length of 2048.", "description": "This table compares the performance and computational efficiency (in FLOPs) of different efficient transformer models against a baseline Pythia-410M model.  The comparison includes Linformer, Cosformer, Performer, and MemoryFormer-base, all using a sequence length of 2048 for inference FLOPs calculation.  The performance is evaluated across six NLP benchmarks: PIQA, WinoGrande, WSC, ARC-E, ARC-C, and LogiQA. The average performance across these benchmarks is also provided for each model.", "section": "3.2 Comparison with Efficient Transformers"}, {"figure_path": "04EC4ZnZJj/tables/tables_7_1.jpg", "caption": "Table 3: Ablation study on different \u03c4 and K. Memory Size refer to the storage space required by the Memory Layer Q.", "description": "This table presents an ablation study on the hyperparameters \u03c4 (number of bits) and K (number of hash tables) in the Memory Layer of the MemoryFormer model.  It shows the validation perplexity (Val. PPL), floating-point operations (FLOPs), and memory size required for different combinations of \u03c4 and K.  The results demonstrate the trade-off between model performance, computational cost, and memory usage when adjusting these hyperparameters.", "section": "3.3 Ablation Study"}, {"figure_path": "04EC4ZnZJj/tables/tables_7_2.jpg", "caption": "Table 4: Val. PPL at 8000 training steps with various LR.", "description": "This table presents the results of an ablation study on the effect of different learning rates (LR) on the validation perplexity (Val. PPL) of the MemoryFormer model.  The experiment was run for 8000 training steps, and the learning rates tested were 1e-3, 2e-3, 3e-3, and 4e-3.  The table shows that a learning rate of 3e-3 achieved the lowest validation perplexity.", "section": "3.3 Ablation Study"}, {"figure_path": "04EC4ZnZJj/tables/tables_7_3.jpg", "caption": "Table 5: Different expanding bits of Memory Block. #Expanding Bit = T' denotes the number of extra bit of zk after expansion. Memory Size denotes the storage space required by Memory Block.", "description": "This table presents the ablation study on the expanding bits in the Memory Block of the MemoryFormer model.  It shows the validation perplexity (Val. PPL), the size of the hash tables (TM1 and TM2), and the total memory size used by the Memory Block for different numbers of expanding bits.  The expanding bits refers to the additional bits added to the sub-vector zk during the expansion process in the second Memory Layer.  As the number of expanding bits increases, the model performance improves, but the memory consumption increases exponentially.", "section": "3.3 Ablation Study"}, {"figure_path": "04EC4ZnZJj/tables/tables_8_1.jpg", "caption": "Table 6: Ablation study on whether to use the non-linearity in the Memory Block.", "description": "This ablation study investigates the impact of removing the GeLU activation function from the Memory Block in the MemoryFormer model.  The table compares the performance (average accuracy across several NLP benchmarks: PIQA, WinoGrande, WSC, ARC-E, ARC-C, LogiQA) of the MemoryFormer-tiny model with and without GeLU activation between the two Memory layers of the block. Results show minimal performance difference, suggesting that the GeLU function may be redundant due to the inherent nonlinearity of the hashing operation.", "section": "3.3 Ablation Study"}]