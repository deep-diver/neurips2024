[{"type": "text", "text": "MemoryFormer: Minimize Transformer Computation by Removing Fully-Connected Layers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ning Ding1\u2217, Yehui $\\mathbf{Tang^{2*}}$ , Haochen $\\mathbf{Qin^{2}}$ , Zhenli $\\mathbf{Zhou^{1}}$ , Chao $\\mathbf{X}\\mathbf{u}^{1}$ , Lin $\\mathbf{Li}^{3}$ , Kai $\\mathbf{Han}^{2}$ , Heng Liao3, Yunhe $\\mathbf{Wang^{2\\dagger}}$ ", "page_idx": 0}, {"type": "text", "text": "1 State Key Lab of General AI, School of Intelligence Science and Technology, Peking University 2 Huawei Noah\u2019s Ark Lab. 3 Huawei HiSilicon dingning@stu.pku.edu.cn xuchao@cis.pku.edu.cn {yehui.tang, yunhe.wang}@huawei.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In order to reduce the computational complexity of large language models, great efforts have been made to to improve the efficiency of transformer models such as linear attention and flash-attention. However, the model size and corresponding computational complexity are constantly scaled up in pursuit of higher performance. In this work, we present MemoryFormer, a novel transformer architecture which significantly reduces the computational complexity (FLOPs) from a new perspective. We eliminate nearly all the computations of the transformer model except for the necessary computation required by the multi-head attention operation. This is made possible by utilizing an alternative method for feature transformation to replace the linear projection of fully-connected layers. Specifically, we first construct a group of in-memory lookup tables that store a large amount of discrete vectors to replace the weight matrix used in linear projection. We then use a hash algorithm to retrieve a correlated subset of vectors dynamically based on the input embedding. The retrieved vectors combined together will form the output embedding, which provides an estimation of the result of matrix multiplication operation in a fully-connected layer. Compared to conducting matrix multiplication, retrieving data blocks from memory is a much cheaper operation which requires little computations. We train MemoryFormer from scratch and conduct extensive experiments on various benchmarks to demonstrate the effectiveness of the proposed model. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The Transformer model has made magnificent achievement in deep learning community since it was made public. The Transformer not only successfully leads the revolution in the field of natural language processing due to its excellent performance, but also motivates the innovation in model architecture in other fields such as computer vision and speech recognition. Recently, large language models (LLMs), transformers that are extremely scaled up in size, have drawn remarkable attention of both researchers and non-researchers across the globe. The unprecedented emergent abilities that LLMs demonstrate attract an increasing number of investments and researches, which point out a potential pathway for the artificial general intelligence. ", "page_idx": 0}, {"type": "text", "text": "However, what comes along with the scaling lay is not only more intelligence, but also greater consumption of computing resources. The ever-increasing computational complexity is currently the main obstacle hindering the application and popularization of LLMs. In response to this situation, great efforts have been made towards optimizing the architecture of transformer model by the research community. Some works using traditional methods such as model pruning and weight quantization are able to lower the computational complexity of LLMs to some degree. Another line of works that are specialized for transformer re-design the self-attention mechanism, which is the key to sequence modeling. They use sliding-windows or kernel function to reduce the complexity from quadratic to sub-quadratic or even linear with respect to the sequence length, while maintaining a comparable performance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "According to our observation, in most application scenarios, only a small proportion of the computational complexity comes from the multi-head attention (MHA) operation, while the majority of the computation comes from the fully-connected (FC) layers in the transformer model. Specifically, given a standard transformer model with the hidden size being $d$ and the length of input sequence being $s$ , the amount of floating-point computation of the MHA operation is $2s^{\\breve{2}}d$ , and the computation in all the FC layers is $12s d^{\\breve{2}}$ . The computation required by the MHA becomes dominant only when $s>6d$ . That\u2019s to say, for an LLM with hidden size $d=4096$ , the sequence length $s$ needs to be larger than 24K. ", "page_idx": 1}, {"type": "text", "text": "Another observation we have is that, at present, the inference stage of deep neural network relies on the parallel-computing cores of the graphics processing unit (GPU), while the CPU and the randomaccess memory (RAM) resources in the computer system are left almost unused, despite the fact that the size of the RAM easily reaches terabytes and the CPU has hundreds of cores (e.g. NVIDIA DGX A100 has 2TB RAM and 128 CPU cores [21]). What\u2019s more, CPU manufacturers have started developing tensor core to accelerate parallel-computing, which might make the low-latency CPU inference feasible in the future. ", "page_idx": 1}, {"type": "text", "text": "Based on the above observations, we present a novel transformer architecture in this work, named MemoryFormer, to minimize the required computational complexity from a new perspective. Instead of the fully-connected layers used in a standard transformer, MemoryFormer uses the Memory Layer to process the feature vector (the embedding of token). Specifically, the Memory Layer contains a group of in-memory hash tables which store a large amount of discrete vectors. It uses localitysensitive hashing (LSH) algorithm to retrieve a subset of vectors from the hash tables that are correlated with the input token embedding. These retrieved vectors are then aggregated with different weights to form the output of the Memory Layer. The result of hashing-and-aggregating operation performed by the Memory Layer provides an estimation of the matrix multiplication of the fullyconnected layer. We also design a method to make all the vectors stored in hash tables learnable via the back-propagation of gradients. Therefore, the MemoryFormercan be trained from scratch in an end-to-end manner. ", "page_idx": 1}, {"type": "text", "text": "Since the amount of computation produced by the hash operation is negligible, the absolute majority of the computations now results from the matrixmultiplications within the multi-head attention. Figure 1 demonstrates the comparison of the computational complexity of one block between the proposed MemoryFormer (red) and the baseline transformer (blue). The MemoryFormer block only requires $\\mathord{\\sim}19\\%$ of the FLOPs compared with the baseline transformer block when sequence length $s\\,=\\,2048$ and hidden size $d\\,=\\,2048$ . The effect of FLOPs-reduction is going to be more significant as the model size is scaled up. Replacing all fullyconnected layers with Memory Layers allows us to trade the memory resources for less computational complexity. ", "page_idx": 1}, {"type": "image", "img_path": "04EC4ZnZJj/tmp/fbbfea64e5dddb25bb1bbc8b966d531062845c0057d416709e0e3cb7d088e058.jpg", "img_caption": ["Figure 1: FLOPs with different model hidden size and sequence lengths. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "This work not only proposes a new FLOPs-reduction strategy that is different from existing approaches, but also provides guiding significance for the hardware design (e.g. bigger bus width and higher cache hit rate) of the next-generation parallel-computing platform. We train a series of MemoryFormers of different sizes and validate the effectiveness of these models on multiple public benchmarks. The experiment results show that our method can achieve performance comparable to the baseline Transformer with significantly less computation. ", "page_idx": 1}, {"type": "text", "text": "2 MemoryFormer ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In the standard Transformer model, there are two main types of operations to perform feature transformation on the sequence of token embeddings. One is the multi-head attention (MHA), the most important operation in a transformer block which captures the long-range inter-relationships among different tokens within the sequence. The other is the ubiquitous fully-connected (FC) layer that performs linear projection on each token in the sequence separately. In addition to the projections of $\\{\\mathbf{\\bar{W}}_{Q},\\mathbf{W}_{K},\\mathbf{W}_{V}\\}$ prior to the MHA, the feed-forward network (FFN) is also composed of multiple FC layers. ", "page_idx": 2}, {"type": "text", "text": "Let $\\mathbf{x}\\in\\mathbb{R}^{d}$ be a row vector representing any token embedding, a fully-connected layer parameterized by weight matrix $\\mathbf{W}\\in\\mathbb{R}^{d\\times h}$ applies a linear projection to $\\mathbf{x}$ formulated as $\\mathbf{y}=\\mathbf{x}\\mathbf{W}$ , where $\\mathbf{y}\\in\\mathbb{R}^{h}$ is the output token embedding. For a sequence composed of $s$ tokens, this would become a matrix multiplication in Eq. (1) with computational complexity of $O(s d h)$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Y}=\\mathbf{X}\\mathbf{W},\\ \\ \\mathbf{X}\\in\\mathbb{R}^{s\\times d},\\ \\mathbf{y}\\in\\mathbb{R}^{s\\times h}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In finite-dimensional vector space, the fully-connected layer is a continuous linear operator, which means, for two adjacent input feature vector $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ , given the same weight matrix $\\mathbf{W}$ , the projected vectors $\\mathbf{y}_{1}=\\mathbf{x}_{1}\\mathbf{W}$ and $\\mathbf{y}_{2}=\\mathbf{x}_{2}\\mathbf{W}$ are most likely to be similar as well. In this work, we want to find an alternative mapping function which has much less computational complexity than $O(s d h)$ yet generally in accord with the properties of the linear projection. If this is achieved, we can use this alternative method to replace all the FC layers to perform feature transformation while reducing computation. ", "page_idx": 2}, {"type": "text", "text": "In this paper, we use normal-font lowercase letters to denote scalars (e.g. $d$ and $h$ ), bold-font lowercase letters to denote vectors (e.g. x and $\\mathbf{y}$ ), and bold-font uppercase letters to denote matrices (e.g. W). We use the notation $[\\cdot]_{i}$ to represent the $i$ -th row of a matrix, and also use $[\\cdot]_{i}$ to represent the $i$ -th entry of a vector. ", "page_idx": 2}, {"type": "text", "text": "2.2 Compute-less Locality-Sensitive Hashing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unlike ordinary hash functions that are designed to avoid collisions for different encoded items, the aim of locality-sensitive hashing (LSH) is to map similar items to the same hash bucket (memory location) in the hash table. For example, in the text-to-image search system, several different descriptions for the same image are expected to have an identical hash code after encoded by the LSH function, and thus retrieve the same image. ", "page_idx": 2}, {"type": "text", "text": "We decide to apply LSH function in the embedding space to encode any input feature vectors. Assuming $\\mathbf{x}$ is hashed to a specific bucket that stores a vector $\\hat{\\mathbf{y}}$ by the LSH function, then the hashing result for some adjacent neighbor vectors of $\\mathbf{x}$ will correspond to the same hash bucket and retrieve $\\hat{\\mathbf{y}}$ as well. If the value $\\hat{\\mathbf{y}}$ in the hash table is an approximation of the result of linear operation $\\mathbf{y}=\\mathbf{x}\\mathbf{W}$ for the input vector $\\mathbf{x}$ , we can use this method, that is to find an estimated result in a hash table, to replace the fully-connected layer, while only using the FLOPs of the hashing operation. ", "page_idx": 2}, {"type": "text", "text": "Firstly, we construct an in-memory hash table parameterized by the matrix $\\textbf{T}\\in\\,\\mathbb{R}^{2^{d}\\times h}$ which stores $2^{d}$ vectors $[\\mathbf{T}]_{i}\\in\\mathbb{R}^{h}$ . Traditional LSH functions, such as Hyperplane-LSH [5], incorporate multiple linear projections to generate the hash code for table indexing. To avoid any unnecessary computations, we utilize a much simpler LSH function to generate the hash code $h(\\mathbf{x})$ . Specifically, the process of hashing and retrieving the result from table $\\mathbf{T}$ is formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle h({\\bf x})=\\mathrm{integer}(\\mathrm{sign}({\\bf x})),}}\\\\ {{\\displaystyle\\mathrm{sign}([{\\bf x}]_{i})=\\left\\{\\begin{array}{l l}{{\\displaystyle-1,\\;\\mathrm{if}\\;[{\\bf x}]_{i}<0,}}\\\\ {{\\displaystyle1,\\;\\mathrm{if}\\;[{\\bf x}]_{i}\\geq0,}}\\end{array}\\right.}}\\\\ {{\\displaystyle\\mathrm{integer}({\\bf s})=\\sum_{i=0}^{d-1}\\frac{[{\\bf s}]_{i}+1}{2}\\cdot2^{i},}}\\\\ {{\\displaystyle\\hat{\\bf y}=[{\\bf T}]_{h({\\bf x})},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}\\in\\mathbb{R}^{d}$ is the input vector, $\\mathbf{s}=\\mathrm{sign}(\\mathbf{x})\\in\\{-1,1\\}^{d}$ is the corresponding binary representation (hash code), integer $(\\cdot)$ function converts the binary representation s to the corresponding non-negative integer used as the index number of the hash table, $h(\\mathbf{x})\\in\\{0,1,2,\\cdots,2^{d}-1\\}$ . Notably, the space complexity of such a hash table is $O(2^{d}h)$ . The required memory space would be $\\sim10^{145}$ terabytes (TB) when using float16 datatype with $d=h=512$ , which is impractical for any modern computer system. ", "page_idx": 3}, {"type": "text", "text": "To tackle this problem, we propose to evenly split $\\mathbf{x}\\in\\mathbb{R}^{d}$ into $K$ non-overlapping chunks and handle them separately: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{z}_{k}=\\mathrm{split}(\\mathbf{x},\\;\\mathrm{num\\_chunk}=K),\\;\\;k=1,2,\\cdot\\cdot\\cdot\\;,K,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{z}_{k}\\in\\mathbb{R}^{\\tau}$ , $\\textstyle\\tau={\\frac{d}{K}}$ , and $d$ is evenly divisible by $K$ . We then set up a hash table $\\mathbf{T}_{k}\\in\\mathbb{R}^{2^{\\tau}\\times h}$ for each sub-vector $\\mathbf{z}_{k}$ , respectively. Therefore, the output result is ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{y}}=\\sum_{k=1}^{K}\\,[\\,\\mathbf{T}_{k}\\big]_{h(\\mathbf{z}_{k})},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Since $\\mathbf{z}_{k}$ has a smaller bit width after binarization and thus the corresponding hash table ${\\bf T}_{k}$ would comsume less memory space for storage. The space complexity of Eq. (7) is $\\mathcal{O}(K2^{\\tau}h)$ . When $d=h=512$ , $\\tau=8,K=64$ and data type is float16, the storage required by all $K$ hash tables is $\\sim16$ MegaBytes(MB). Figure 2 is a simple demonstration of the proposed locality-sensitive hashing. ", "page_idx": 3}, {"type": "text", "text": "2.3 Memory Layer ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "So far, the above-mentioned formulation is able to simulate the forward pass of fully-connected layer. And the values store in the hash tables can be updated via back-propagation. The derivative of the loss function $L$ with respect to the hash table is \u2202[Tk]h(z ) $\\begin{array}{r}{\\frac{\\partial L}{\\partial[\\mathbf{T}_{k}]_{h(\\mathbf{z}_{k})}}\\ =\\ \\frac{\\partial L}{\\partial\\hat{\\mathbf{y}}}\\frac{\\partial\\hat{\\mathbf{y}}}{\\partial[\\mathbf{T}_{k}]_{h(\\mathbf{z}_{k})}}}\\end{array}$ . However, the input vector $\\mathbf{x}$ is unable to have gradient since it\u2019s hashed to multiple integers $h(\\mathbf{z}_{k})$ used as the index number for retrieval, which is a non-differentiable operation. If we can reformulate Eq. (7) as $\\begin{array}{r}{\\hat{\\mathbf{y}}=\\sum_{k=1}^{K}\\,p(\\mathbf{z}_{k})\\cdot\\bar{[\\mathbf{T}_{k}]}_{h(\\mathbf{z}_{k})}}\\end{array}$ to add a coefficient $p(\\mathbf{z}_{k})$ to weight each retrieved item, where $p(\\mathbf{z}_{k})$ is a function of the variable $\\mathbf{z}_{k}$ , the gradients can be back-propagated to the input $\\mathbf{x}$ via $\\big[\\mathbf{T}_{k}\\big]_{h(\\mathbf{z}_{k})}$ . ", "page_idx": 3}, {"type": "text", "text": "As we can observe from Figure 2, many sub-vectors with various directions and amplitudes can still be hashed to the same bucket as long as their signs are identical, but the angle between the bucket\u2019s representative binary vector (each entry is either 1 or $^-1$ ) ", "page_idx": 3}, {"type": "image", "img_path": "04EC4ZnZJj/tmp/78c2378d52c0d0d72e331b5cee3e1cf3dea0603dee33ff244971e7957e44ce37.jpg", "img_caption": ["Figure 2: A demonstration with $\\tau\\,=\\,2$ and $K\\,=\\,3$ , where $\\mathbf{z}_{1}$ is hashed to the bucket2 of ${\\bf T}_{1}$ , $\\mathbf{z}_{2}$ is hashed to the bucket1 of ${\\bf T}_{2}$ , $\\mathbf{z}_{3}$ is hashed to the bucket2 of ${\\bf T}_{3}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "and these sub-vectors are different, which is defined by the cosine value $\\mathrm{cos}(\\mathbf{z}_{k},\\mathrm{sign}(\\mathbf{z}_{k}))$ ). We use a scaled cosine similarity, which takes into account both the direction and amplitude of $\\mathbf{z}_{k}$ , to measure the relevance between $\\mathbf{z}_{k}$ and its corresponding hash bucket $h(\\mathbf{z}_{k})$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sin(\\mathbf{z}_{k},h(\\mathbf{z}_{k}))=\\|\\mathbf{z}_{k}\\|_{2}\\cdot\\|\\mathrm{sign}(\\mathbf{z}_{k})\\|_{2}\\cdot\\mathrm{cos}(\\mathbf{z}_{k},\\mathrm{sign}(\\mathbf{z}_{k}))=\\langle\\mathbf{z}_{k},\\mathrm{sign}(\\mathbf{z}_{k})\\rangle,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\langle,\\rangle$ computes the inner-product of two vectors. Considering all the $2^{\\tau}$ buckets that $\\mathbf{z}_{k}$ is possibly hashed to in the lookup table ${\\bf T}_{k}$ simultaneously, we define the probability that $\\mathbf{z}_{k}$ is specifically mapped to the $h(\\mathbf{z}_{k})$ -th hash bucket: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\mathbf{z}_{k})=\\frac{e x p\\left[\\sin\\left(\\mathbf{z}_{k},h(\\mathbf{z}_{k})\\right)/t\\;\\right]}{2\\tau-1}=\\frac{e x p\\left[\\left\\langle\\mathbf{z}_{k},\\mathrm{sign}\\left(\\mathbf{z}_{k}\\right)\\right\\rangle/t\\;\\right]}{\\displaystyle\\sum_{i=0}^{\\tau}\\,e x p\\left[\\sin\\left(\\mathbf{z}_{k},i\\right)/t\\;\\right]},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $t$ is the temperature hyper-parameter, i $\\mathrm{\\mathfrak{teger}}_{\\tau}^{-1}(i)\\,\\in\\,\\{-1,1\\}^{\\tau}$ is a function that maps an non-negative integer $0\\ \\leq\\ i\\ <\\ 2^{\\tau}$ to the corresponding $\\tau$ -bit binary representation. Note that $\\langle\\cdot,\\mathrm{integer}_{\\tau}^{-1}(i)\\rangle$ operator takes the summation after elementwise selective sign-flipping over any $\\tau$ -dimensional vector, therefore we have ", "page_idx": 3}, {"type": "image", "img_path": "04EC4ZnZJj/tmp/b3c550ebd82308c5b68023a7347d6b18f272fe2c275c806bed47d34ce6f4f237.jpg", "img_caption": ["Figure 3: Left: The schematic diagram of the Memory Layer. Right: One building block of the MemoryFormer. "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf{z}_{k},\\mathrm{sign}(\\mathbf{z}_{k})\\rangle=\\displaystyle\\sum_{i=0}^{\\tau-1}|[\\mathbf{z}_{k}]_{i}|\\,,}\\\\ &{\\displaystyle\\sum_{i=0}^{\\tau-1}e x p[\\langle\\mathbf{z}_{k},\\mathrm{integer}_{\\tau}^{-1}(i)\\rangle]=\\displaystyle\\prod_{i=0}^{\\tau-1}[e x p([\\mathbf{z}_{k}]_{i})+e x p(-[\\mathbf{z}_{k}]_{i})]\\,,}\\\\ &{p(\\mathbf{z}_{k})=\\displaystyle\\frac{e x p(\\sum_{i=0}^{\\tau-1}|[\\mathbf{z}_{k}]_{i}|/t\\,)}{\\prod_{i=0}^{\\tau-1}[e x p([\\mathbf{z}_{k}]_{i}/t\\,)+e x p(-[\\mathbf{z}_{k}]_{i}/t\\,)]}=\\displaystyle\\frac{1}{\\prod_{i=0}^{\\tau-1}[1+e x p(-2|[\\mathbf{z}_{k}]_{i}|/t\\,)]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This way, we can formulated the Memory Layer as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\sum_{k=1}^{K}\\,p(\\mathbf{z}_{k})\\cdot[\\mathbf{T}_{k}]_{h(\\mathbf{z}_{k})}=\\sum_{k=1}^{K}\\,\\frac{[\\mathbf{T}_{k}]_{h(\\mathbf{z}_{k})}}{\\,\\prod_{i=0}^{\\tau-1}[1+e x p(-2|[\\mathbf{z}_{k}]_{i}|/t\\,)]\\,}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The left part of Figure 3 illustrates the schematic of the Memory Layer. For a sequence composed of $s\\ d$ -dimensional tokens, and the dimensionality of output embeddings is $h$ , the computation complexity of a Memory Layer in Eq. (11) is $\\begin{array}{r}{\\mathcal{O}(s(\\tau+\\bar{h})K)\\approx\\mathcal{O}(\\frac{s d\\bar{h}}{\\tau})}\\end{array}$ . This is one order of magnitude smaller than the fully-connected layer which is $O(s d h)$ when $\\tau=10$ . ", "page_idx": 4}, {"type": "text", "text": "According to Eq. (11), we can compute the derivative of the loss function $L$ with respect to both the hash tables and the input vector as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial L}{\\partial[\\mathbf{T}_{k}]_{i}}=\\left\\{\\begin{array}{l l}{p(\\mathbf{z}_{k})\\frac{\\partial L}{\\partial\\mathbf{y}},\\;\\mathrm{if}\\;h(\\mathbf{z}_{k})=i,}&{\\displaystyle i\\in\\{0,1,\\cdots,2^{\\tau}-1\\},}\\\\ {\\displaystyle0,\\quad\\quad\\;\\mathrm{if}\\;h(\\mathbf{z}_{k})\\neq i,}\\end{array}\\right.}\\\\ {\\displaystyle\\frac{\\partial L}{\\partial\\mathbf{x}}=\\mathrm{concat}(\\mathbf{\\boldsymbol{[}\\mathbf{\\nabla}.\\boldsymbol{\\cdot}\\frac{\\partial L}{\\partial y}[\\mathbf{T}_{k}]_{h(\\mathbf{z}_{k})}^{\\top}\\frac{\\partial p(\\mathbf{z}_{k})}{\\partial\\mathbf{z}_{k}}\\boldsymbol{\\cdot}...\\,\\mathrm{for}\\;k\\;\\mathrm{in}\\;\\mathrm{range}(1,K+1)\\mid).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2.4 Architecture of MemoryFormer ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We follow the generic design paradigm of the standard transformer architecture [25] using $_\\mathrm{N}$ stacked blocks to build the MemoryFormer. The right part of Figure 3 depicts one building block. ", "page_idx": 4}, {"type": "text", "text": "Multi-Head Attention Given an input sequence $\\mathbf{X}=(\\mathbf{x}_{1},\\mathbf{x}_{2},\\cdot\\cdot\\cdot\\mathbf{\\mu},\\mathbf{x}_{s})^{\\top}\\in\\mathbb{R}^{s\\times d}$ , a $\\operatorname{Norm}(\\cdot)$ layer first normalizes the input. There Memory Layers transform the normalized $\\mathbf{X}$ into Q, K, $\\mathbf{V}\\in\\mathbb{R}^{s\\times d}$ , respectively. The tokens in $\\mathbf{Q}$ , K, $\\mathbf{V}$ are then evenly split into multiple sub-vectors for multi-head purpose. The calculation of multi-head attention remains untouched as in [25]. Therefore, any other efficiet self-attention techniques such as Flash Attention [10], Linear Attention [16] and KV-Cache can be seamlessly incorporated into MemoryFormer to further increase the forward-time efficiency. ", "page_idx": 4}, {"type": "text", "text": "Memory Block In MemoryFormer we use the Memory Block to replace the Feed-Forward Network used in standard transformers. The Memory Block is composed of 2 consecutive Memory Layers, each of which is preceded by a Norm(\u00b7) layer. The norm layer is vital by setting a zero-mean distribution for the input embedding before the hashing operation, and thus the sign function in Eq. (3) ", "page_idx": 4}, {"type": "text", "text": "can generate -1 and $+1$ evenly. Therefore, the output of Eq. (4) can have an uniform distribution so that every bucket in the hash table will be retrieved with equiprobability. ", "page_idx": 5}, {"type": "text", "text": "Another detail about the Memory Block is that we omit the intermediate activation function (e.g. ReLU, GELU). The hashing operation is a non-linear operation itself. An extra nonlinear function is thus redundant. We have verified through experiments that discarding the non-linear function between the two Memory Layers has no effect on the performance. ", "page_idx": 5}, {"type": "text", "text": "In a traditional FFN module, in order to increase the model capacity and performance, the dimensionality of the output token embeddings of the first FC layer is expanded by 4 times, and then restored to hidden size $d$ by the second FC layer. To keep aligned with this design pattern, we set the output dimensionality of the first Memory Layer to be $(\\tau+2)\\cdot K$ . Remeber that $d=\\tau K$ , that is, the size of each hash table is $\\mathbf{T}_{k}^{1}\\in\\mathbb{R}^{2^{\\tau}\\times(\\tau+2)\\cdot K}$ in the first Memory Layer of the Memory Block. Therefore, the bit width of $K$ sub-vectors $\\mathbf{z}_{k}$ in the second Memory Layer is 2 bits larger than that of the sub-vectors in the first layer. The size of hash tables in the second layer is T2k \u2208R2(\u03c4+2)\u00d7d, which leads to a capacity 4 times larger than the first layer while restores the dimensionality of the output embeddings back to $d$ . ", "page_idx": 5}, {"type": "text", "text": "Computational Complexity. So far, the above computing process of one MemoryFormer block is formulated as follows: ", "page_idx": 5}, {"type": "text", "text": "$\\mathbf{X}=\\operatorname{Norm}(\\mathbf{X})$ , $\\begin{array}{r l}&{\\mathbf{Q}=\\mathbf{MemoryLayer}_{Q}(\\mathbf{X}),\\;\\;\\mathbf{K}=\\mathbf{MemoryLayer}_{K}(\\mathbf{X}),\\;\\;\\mathbf{V}=\\mathbf{MemoryLayer}_{V}(\\mathbf{X}),}\\\\ &{\\;\\;\\mathbf{Z}=\\mathbf{X}+\\mathbf{MultiHeadAttention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V}),}\\\\ &{\\;\\;\\mathbf{Y}=\\mathbf{Z}+\\mathbf{MemoryLayer}_{2}(\\mathbf{Norm}(\\mathbf{MemoryLayer}_{1}(\\mathbf{Norm}(\\mathbf{X})))).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "The amount of floating-point computation of a standard transformer block is $2s^{2}d+12s d^{2}$ , while the amount of computation of a MemoryFormer block is only about $2s^{2}d+\\textstyle{\\frac{6}{\\tau}}s d^{2}=2s^{2}d+6K s d$ . The computations originating from FC layers in standard transformer are eliminated by an order of magnitude. The absolute majority of the computational workload now comes from the MultiHeadAttention. ", "page_idx": 5}, {"type": "text", "text": "3 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we conduct thorough experiments on multiple NLP benchmarks to validate the efficiency and effectiveness of the MemoryFormer across different scales. We also compare our model with existing efficient transformer methods. ", "page_idx": 5}, {"type": "text", "text": "Given the fact that most large language models only open-source the checkpoint without providing detailed training information, reproducing them is unachievable. Therefore, we employ Pythia [3], a well-developed LLM training framework with completely available dataset and detailed model hyper-parameters, to implement our method. As for training data, the Pile [12] dataset contains 825 GiB corpus with 22 diverse high-quality subsets, which either pre-exists or is constructed from professional and academic fields. We use exactly the same optimizer, scheduler and other hyper-parameters following the setting of Pythia to conduct fair comparisons. ", "page_idx": 5}, {"type": "text", "text": "We choose six widely-used evaluation task for our approach: PIQA [4], WinoGrande [23], WSC [24], ARC-E, ARC-C [9], and LogiQA[19]. These tasks range from knowledge to reasoning, forming a comprehensive benchmark for evaluating the all-round capability of large language models. ", "page_idx": 5}, {"type": "text", "text": "3.1 Evaluation Across Different Scales ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We choose Pythia-70M, Pythia-160M, Pythia-410M as the baseline models upon which we build our MemoryFormers. Specifically, MemoryFormer-tiny has the same hidden size and number of layers as Phythia-70M, MemoryFormer-small has the same hidden size and number of layers as Phythia-160M and MemoryFormer-base has the same hidden size and number of layers as Phythia-410M. As for the hyper-parameter of Memory Layer, we fix the value of $\\tau$ to be 8, while the number of hash tables $K$ is 64, 96 and 128 respectively for MemoryFormer-tiny, -small and -base model. Notably, considering the sparsity of gradients of the hash tables, we set the learning rate to be 3 times of the baseline learning rate used by the corresponding Pythia model. This is ablated in Sec. 3.3. It\u2019s worth noting that, the only one fully-connected layer in the MemoryFormer is the classifier head. ", "page_idx": 5}, {"type": "text", "text": "Tab. 1 reports both the computational complexity and the evaluation results of our models compared to baseline. The FLOPs is calculated for one transformer block with the input sequence length of ", "page_idx": 5}, {"type": "table", "img_path": "04EC4ZnZJj/tmp/17e49c03801cfd90423ba11dd4180bc99ea60c8ac597188f110f7c6499ae641a.jpg", "table_caption": ["Table 1: Zero-shot evaluation results on public NLP benchmarks. We use \"MF\" as the abbreviation for MemoryFormer. \"Attn.\" refers to the computation of $\\sigma(\\mathbf{Q}\\mathbf{K}^{\\top})\\mathbf{V}$ . Inference FLOPs are measured for one block with sequence length of 2048. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "04EC4ZnZJj/tmp/200fd6ab7f0391d804c6ad2f3675a96aeea5f3cbdfc415eb3f41b8fa0968efe7.jpg", "table_caption": ["Table 2: Comparison of different efficient transformer methods based on Pythia-410M. Inference FLOPs are measured for one block with sequence length of 2048. "], "table_footnote": ["2048. The experiment results show that MemoryFormer has the minimum computation except for the necessary computation of self-attention. Across all three different model sizes, we achieve better average accuracy on the benchmark than the Pythia baseline. This suggests that the proposed method is able to greatly reduce the computational complexity without compromising the performance. "], "page_idx": 6}, {"type": "text", "text": "3.2 Comparison with Efficient Transformers ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We also compare our models with existing efficient transformer methods to show the superiority of MemoryFormer in both performance and efficiency. We choose Pythia-410M as the baseline, and replace the multi-head attention module of Pythia with the ones proposed by Linformer [26], Cosformer [22], and Performer [8], respectively. Tab. 2 demonstratse the experiment results on the benchmark and the inference FLOPs of each model. We measure the FLOPs using one transformer block with the sequence length of 2048. As shown in Tab. 2, these efficient attention methods can obtain FLOPs-reduction but with considerable performance degradation, while MemoryFormer significantly eliminates the computations and gains better performance. On the other hand, even though existing efficient transformer methods can reduce the computation cost of the self-attention operation, yet we can observe from Tab. 2 that the majority of the computation originates from the fullyconnected layers in both the MHA and FFN module as discussed previously. The linear projection operation accounts for the biggest part of the workload when the sequence length is not extremely large in most of the practical scenarios. Utilizing Memory Layer in the embedding space to replace FC layers does provide a new solution to minimize the FLOPs of LLMs. ", "page_idx": 6}, {"type": "text", "text": "3.3 Ablation Study ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tradeoff between $\\tau$ and $K$ . In Eq. (6) we split an input embedding x in to K sub-vectors to avoid the explosive growth of memory usage of the hash tables. We study the model performance with different $(\\tau,K)$ combination by controlling the model hidden size to be the same. We report the experiment results in Tab. 3 along with the computation in FLOPs of the corresponding Memory Layer with sequence length of 2048. As is shown in Tab. 3, as the bit width of ${\\bf z}$ increases, the model performance continues to grow due to the exponentially enlarged capacity of hash tables. However, the memory consumption of the Memory Layer also increases drastically and its computational complexity soon reaches the lower bound. To trade off between the efficiency and memory usage, we conjecture that $\\tau=8$ is a good option for MemoryFormer. ", "page_idx": 6}, {"type": "table", "img_path": "04EC4ZnZJj/tmp/1bc80060244c58e29030f46dbbfaa55ec96ef7dad19ba317a38380afec4cc991.jpg", "table_caption": ["Table 3: Ablation study on different $\\tau$ and $K$ . Memory Size refer to the storage space required by the Memory Layer Q. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "04EC4ZnZJj/tmp/bc06b742aaaad023db21a22d94025e548f95eea7bbae77f3d0e0c4dcc9f79f84.jpg", "table_caption": ["Table 4: Val. PPL at 8000 training steps with various LR. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "04EC4ZnZJj/tmp/b0b0d135e9edfd67f90c55cc91407e7cb5fce0342c5b22a6d5578799470832da.jpg", "table_caption": ["Table 5: Different expanding bits of Memory Block. #Expanding $\\mathrm{Bit}{=}\\;\\tau^{\\prime}-\\tau$ denotes the number of extra bit of $\\mathbf{z}_{k}$ after expansion. Memory Size denotes the storage space required by Memory Block. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Larger learning rate. From Eq. (12) we can observe that the gradients of the hash table are sparse during backward propagation. Some buckets in the hash table might not get updated in one training step. We conjecture that larger learning rate will help remedy the lack-of-gradients situation. We train a MemoryFormer-tiny for 8000 steps with different LR and report the PPL of validation set in Tab. 4. We use initial $L\\mathrm{R}{=}1\\mathrm{e}{-}3$ as the baseline learning rate following the settings of Pythia-70M. The best performance is achieved with the learning rate 3 times of the baseline. ", "page_idx": 7}, {"type": "text", "text": "Expanding bit in the Memory Block. As mentioned in Sec. 2.4, in order to increase the model capacity, we enlarge the dimensionality of the output embedding of the first layer of the Memory Block, which consequently expands the bit-width of the the sub-vector $\\mathbf{z}_{k}$ in the second layer of the Memory Block. We use MemoryFormer-tiny with hidden size $d=512$ , bit-width $\\tau=8$ and number of hash tables $K=64$ as the baseline model, and report the perplexity results of using different number of expanding bits after 8000 training steps in Tab. 5. We use $\\mathbf{T}_{k}^{\\bar{M}1}$ to denote the hash tables of the 1st layer of the Memory Block, $\\mathbf{T}_{k}^{M2}$ to denote the hash tables of the 2nd layer, and use $\\tau^{\\prime}$ to denote the bit-width of $\\mathbf{z}_{k}$ in the second layer after expansion. As shown in Tab. 5, the validation PPL continuously decreases as $\\tau^{\\prime}$ gets larger. However, the memory space consumed by the hash tables keeps increasing exponentially. Therefore, we choose 2 as the number of expanding bit in the Memory Block for the trade-off between the space complexity and performance. ", "page_idx": 7}, {"type": "text", "text": "Removing non-linearity in the Memory Block. We also mentioned in Sec. 2.4 that all activation functions are discarded in the MemoryFormer. We trained a MemoryFormer-tiny on PILE dataset, where we insert a GeLU layer between the two consecutive Memory Layers of the Memory Block for each building block, and report the testing scores on multiple tasks. As shown in Tab. 6, adding an extra GeLU into the Memory Block leads to almost identical result to the baseline MemoryFormer. ", "page_idx": 7}, {"type": "text", "text": "3.4 Visualization ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Distribution of Hash Bucket. In a Memory Layer, we do not want most of the sub-vector $\\mathbf{z}_{k}$ hashed to a few popular buckets while the rest of buckets in the table are rarely selected. We expect $\\mathbf{z}_{k}$ to be hashed to all the buckets with the same probability. If so, the output space of the Memory Layer would be diversified enough to enlarge the capacity of MemoryFormer. We use 2048 sequences of 1024 token length to visualize the distribution of the frequency that each bucket is retrieved within a hash table in Figure 4. Specially, we choose the first table $\\mathbf{T}_{1}$ and the last table $\\mathbf{T}_{64}$ of the $\\mathrm{^Q}$ , ", "page_idx": 7}, {"type": "table", "img_path": "04EC4ZnZJj/tmp/c793f154550db5dc321a7548fb9d69b2bd47330db7ed5c1001a4af2b643ae4ab.jpg", "table_caption": ["Table 6: Ablation study on whether to use the non-linearity in the Memory Block. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "04EC4ZnZJj/tmp/c219578ab84c771d9cba0286ee86b767bb48e77458082dbdfd502011b07174b2.jpg", "img_caption": ["Figure 4: The frequency at which each bucket in the hash table is retrieved. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "K, V projection layers and the two layers in the FFN module from the first building block of the MemoryFormer-tiny model. As shown in Figure 4, the number of times each bucket is hashed to by ${\\bf z}$ is generally uniform. ", "page_idx": 8}, {"type": "text", "text": "4 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Locality Sensitive Hashing. Locality sensitive hashing [5, 11, 1] (LSH) is a special kind of hash function which is designed to maximize the collision probability for similar input items. It\u2019s wildly adopted in deep learning-based applications. Large scale image retrieval system [18, 15, 2, 30] uses LSH to locate the similar images. Reformer[17] and YOSO [28] both use LSH algorithm to reduce the memory consumption and improve the computational efficiency of the self-attention module. LookupFFN [29] adopt the LSH to accelerate the inference speed of the Feed-Forward Network. SLIDE [7] and MONGOOSE [6] improve the converging speed of neural network training process with the help of locality sensitive hashing algorithms. ", "page_idx": 8}, {"type": "text", "text": "Efficient Transformers. Minimizing the computational complexity of the transformer model has always been a center task for the deep learning community. Many previous works are dedicated to reducing the complexity of multi-head attention module to sub-quadratic, such as CosFormer [22], PerFormer [8], LinFormer [26] and so on. [14] uses a sliding window to constrain the attention map within a local range. Besides, there are some researches [13, 20, 27] exploiting the sparsity of the intermediate activation in the FFN (MLP) module to reduce the computation. Recently, with the development of large language models, practical engineering method like FlashAttention [10] brings substantial optimization for the self-attention mechanism. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we propose MemoryFormer, a novel transformer architecture that significantly reduces the computational complexity (FLOPs) of transformer model from a new perspective. Unlike existing methods that opt to optimize the computation of the multi-head attention operation, this work provides a new solution from a new perspective. We observe that, in most scenarios the vast majority of computations originates from the fully-connected layers in the transformer model. Thus we focus on removing them. The MemoryFormeruses Memory Layer, which uses locality-sensitive hashing algorithm to perform feature transformation in the embedding space, to replace all the FC layers. It achieves a similar function as the computation-heavy matrix multiplication operation but with much less FLOPs. We successfully eliminate nearly all the computations of the transformer model except for the necessary ones required by the self-attention operation. We validate the efficiency and the effectiveness of MemoryFormer via extensive experiments on public NLP benchmarks. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Key R&D Program of China under Grant No.2022ZD0160300 and the National Natural Science Foundation of China under Grant No.62276007. We gratefully acknowledge the support of MindSpore, CANN and Ascend AI Processor used in this research. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alexandr Andoni, Piotr Indyk, Huy L Nguyen, and Ilya Razenshteyn. Beyond locality-sensitive hashing. In Proceedings of the twenty-ffith annual ACM-SIAM symposium on Discrete algorithms, pages 1018\u20131028. SIAM, 2014. [2] Prabavathy Balasundaram, Sriram Muralidharan, and Sruthi Bijoy. An improved content based image retrieval system using unsupervised deep neural network and locality sensitive hashing. In 2021 5th International Conference on Computer, Communication and Signal Processing (ICCCSP), pages 1\u20137. IEEE, 2021. [3] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023.   \n[4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020. [5] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380\u2013388, 2002.   \n[6] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable lsh framework for efficient neural network training. In International Conference on Learning Representations, 2020. [7] Beidi Chen, Tharun Medini, James Farwell, Charlie Tai, Anshumali Shrivastava, et al. Slide: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems. Proceedings of Machine Learning and Systems, 2:291\u2013306, 2020.   \n[8] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.   \n[9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[10] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35: 16344\u201316359, 2022.   \n[11] Anirban Dasgupta, Ravi Kumar, and Tam\u00e1s Sarl\u00f3s. Fast locality-sensitive hashing. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1073\u20131081, 2011.   \n[12] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[13] Matteo Grimaldi, Darshan C Ganji, Ivan Lazarevich, and Sudhakar Sah. Accelerating deep neural networks via semi-structured activation sparsity. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1179\u20131188, 2023.   \n[14] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[15] Ke Jiang, Qichao Que, and Brian Kulis. Revisiting kernelized locality-sensitive hashing for improved largescale image retrieval. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4933\u20134941, 2015.   \n[16] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156\u20135165. PMLR, 2020.   \n[17] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.   \n[18] Brian Kulis and Kristen Grauman. Kernelized locality-sensitive hashing for scalable image search. In 2009 IEEE 12th international conference on computer vision, pages 2130\u20132137. IEEE, 2009.   \n[19] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020.   \n[20] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137\u201322176. PMLR, 2023.   \n[21] NVIDIA. Nvidia dgx a100 user guide, 2023. URL https://docs.nvidia.com/dgx/pdf/ dgxa100-user-guide.pdf.   \n[22] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. arXiv preprint arXiv:2202.08791, 2022.   \n[23] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.   \n[24] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[26] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.   \n[27] Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli, et al. Hire: High recall approximate top- $\\cdot k$ estimation for efficient llm inference. arXiv preprint arXiv:2402.09360, 2024.   \n[28] Zhanpeng Zeng, Yunyang Xiong, Sathya Ravi, Shailesh Acharya, Glenn M Fung, and Vikas Singh. You only sample (almost) once: Linear cost self-attention via bernoulli sampling. In International conference on machine learning, pages 12321\u201312332. PMLR, 2021.   \n[29] Zhanpeng Zeng, Michael Davies, Pranav Pulijala, Karthikeyan Sankaralingam, and Vikas Singh. Lookupffn: making transformers compute-lite for cpu inference. In International Conference on Machine Learning, pages 40707\u201340718. PMLR, 2023.   \n[30] Zheng Zhang, Jianning Wang, Lei Zhu, Yadan Luo, and Guangming Lu. Deep collaborative graph hashing for discriminative image retrieval. Pattern Recognition, 139:109462, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 11}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 11}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction reflect the paper\u2019s contributions and scope accurately. ", "page_idx": 11}, {"type": "text", "text": "Guidelines: ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 11}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 11}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Justification: The limitations are discussed in the Conclusion. ", "page_idx": 11}, {"type": "text", "text": "Guidelines: ", "page_idx": 11}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 11}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 11}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 11}, {"type": "text", "text": "Justification: The paper does not include theoretical results Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 12}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: All the information needed to reproduce the main experimental results are provided in the Sec. 3. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a LLM), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 12}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 12}, {"type": "text", "text": "Answer: [No] ", "page_idx": 13}, {"type": "text", "text": "Justification: The code and data will not be provided. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 13}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The details are provided in the Sec. 3. Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 13}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 13}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: The error bar is not provided. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The details are provided in the Sec. 3. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 14}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: It is conformed. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 14}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 14}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 15}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 15}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: There is no risk in the paper. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 15}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: CC-BY 4.0 is included for each asset ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 15}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 16}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: ", "page_idx": 16}, {"type": "text", "text": "Guidelines: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 16}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 16}]