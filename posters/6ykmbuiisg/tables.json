[{"figure_path": "6YKMBUiIsG/tables/tables_8_1.jpg", "caption": "Table 1: Text summarization task with LLaMa-7b model [42] as target model and LLaMa-68m model [25] as reference model.", "description": "This table presents the results of a text summarization experiment using the Llama-7b model as the target and Llama-68m as the reference model.  It compares several methods, including basic sampling, vanilla unbiased watermarking (VUW), vanilla speculative sampling (VSpS), maintaining watermark strength (MWS), and maintaining sampling efficiency (MSE). For each method, the table shows the average accepted tokens per step (AATPS), per token time (PTT), log perplexity (LOGPPL), average negative log p-value per token (ANLPPT) using the maximin-LLR score, and average negative log p-value per token (ANLPPT) using the U score.  Different watermarking schemes (DeltaGumbel and Gamma) are evaluated and results are shown for different draft sequence lengths (K). The data shows the trade-offs between watermark strength and sampling efficiency.", "section": "6 Experiments"}, {"figure_path": "6YKMBUiIsG/tables/tables_23_1.jpg", "caption": "Table 1: Text summarization task with LLaMa-7b model [42] as target model and LLaMa-68m model [25] as reference model.", "description": "This table presents the results of a text summarization experiment using the Llama-7b model as the target and Llama-68m as the reference model.  Different watermarking and speculative sampling methods are compared across various metrics. The metrics include Average Accepted Tokens Per Step (AATPS) to measure sampling efficiency, Per Token Time (PTT) for inference speed, Log Perplexity (LOGPPL) to evaluate output quality, and Average Negative Log P-value Per Token (ANLPPT) for watermark strength, using both maximin-LLR and U scores for different reweighting schemes (DeltaGumbel and Gamma). The results are shown for different draft sequence lengths (K=1,2,3,4) for each method.", "section": "6 Experiments"}, {"figure_path": "6YKMBUiIsG/tables/tables_24_1.jpg", "caption": "Table 1: Text summarization task with LLaMa-7b model [42] as target model and LLaMa-68m model [25] as reference model.", "description": "This table presents the results of a text summarization task using different methods.  It compares basic sampling, vanilla unbiased watermarking (VUW), vanilla speculative sampling (VSpS), methods that maintain watermark strength (MWS), and methods that maintain sampling efficiency (MSE). For each method, the table shows the average accepted tokens per step (AATPS) as a measure of sampling efficiency, the per-token time (PTT) as a measure of computational efficiency, the log perplexity (LOGPPL) as a measure of output quality, and the average negative log P-value per token (ANLPPT) as a measure of watermark strength.  Different reweighting schemes (DeltaGumbel and Gamma) are used to evaluate the watermarking strength and the P-value is computed based on likelihood-based tests (maximin-LLR) and likelihood-agnostic tests (U Score). The experiment is conducted with different draft sequence lengths (K=1,2,3,4).", "section": "6 Experiments"}, {"figure_path": "6YKMBUiIsG/tables/tables_25_1.jpg", "caption": "Table 1: Text summarization task with LLaMa-7b model [42] as target model and LLaMa-68m model [25] as reference model.", "description": "This table presents the results of text summarization experiments using different methods.  The methods are compared across several metrics, including Average Accepted Tokens Per Step (AATPS) representing sampling efficiency, Per Token Time (PTT) representing inference latency, and Log Perplexity (LOGPPL) representing the quality of generated text.  Additionally, watermark strength is measured using the Average Negative Log P-value Per Token (ANLPPT), calculated via both maximin-LLR score and U score,  for different watermarking schemes (DeltaGumbel and Gamma reweight). The table shows results for different draft sequence lengths (K).", "section": "6 Experiments"}, {"figure_path": "6YKMBUiIsG/tables/tables_26_1.jpg", "caption": "Table 1: Text summarization task with LLaMa-7b model [42] as target model and LLaMa-68m model [25] as reference model.", "description": "This table presents the results of a text summarization experiment using two language models: LLaMa-7b as the target and LLaMa-68m as the reference.  It compares various methods (Basic Sampling, VUW, VSpS, MSE, and MWS) across different draft sequence lengths (K=1, 2, 3, 4) and reweighting techniques (No Reweight, DeltaGumbel, Gamma). For each method and configuration, it reports the Average Accepted Tokens Per Step (AATPS), Per Token Time (PTT), Log Perplexity (LOGPPL), Average Negative Log P-value Per Token (ANLPPT) using the U score and the maximin-LLR score.  The AATPS indicates the efficiency of the speculative sampling method, the PTT is the average time taken per token, the LOGPPL measures the quality of the generated text, while the ANLPPT (using U score and maximin-LLR score) reflects the watermark strength.", "section": "6 Experiments"}]