[{"figure_path": "67K3Xlvw8L/tables/tables_3_1.jpg", "caption": "Table 1: Change in model's response type before (t) and after (t + 1) alignment for honesty. Take a \u201cD\u201d response as an example: the model Mt is capable of providing the correct answer to the question, yet Mt+1 refrains from doing so, which implies that the aligned model may display an excessive level of caution.", "description": "This table shows changes in the model's response type (correct, wrong, or I don't know) before and after applying the honesty alignment.  Each cell represents a possible combination of response types before and after alignment. The table helps in calculating metrics like prudence and over-conservativeness to quantitatively evaluate honesty improvement.", "section": "2.3 Evaluation Methodology"}, {"figure_path": "67K3Xlvw8L/tables/tables_7_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main experimental results on the TriviaQA dataset, comparing different methods for aligning LLMs with honesty.  It shows the prudence score (measuring the model's ability to correctly refuse to answer unknown questions), the over-conservativeness score (measuring the tendency to refuse to answer known questions), the honesty score (a combined metric of prudence and over-conservativeness), and the accuracy of the model on the dataset. The table compares the performance of an unaligned baseline, a fine-tuned baseline, a prompt-based method, and three proposed honesty-oriented fine-tuning methods.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_8_1.jpg", "caption": "Table 4: Out-of-distribution performance on the three free-form QA datasets. Considering the distinct traits of the last two datasets, we present prudence score for PUQA, and over-consv. score and accuracy for PKQA. Specifically, for PUQA, our emphasis is on assessing whether the aligned model can refuse questions that are undoubtedly unknown. Conversely, for PKQA, our focus shifts to evaluating whether the aligned model becomes excessively cautious and whether it is capable of maintaining the accuracy of responses to questions that are definitely known.", "description": "This table presents the out-of-distribution evaluation results of different honesty-oriented fine-tuning methods on three datasets: Non-AmbigQA, PUQA, and PKQA.  It compares the Prudence, Over-Conservativeness, Honesty, and Accuracy scores across the methods and datasets.  PUQA focuses on the models' ability to correctly decline answering unknown questions, while PKQA assesses their ability to answer known questions without excessive caution. Non-AmbigQA serves as a baseline.", "section": "4.4 Exp II: Out-of-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_8_2.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the Honesty-oriented fine-tuning methods on the TriviaQA evaluation dataset.  It compares the performance of three baselines (UNALIGNED, FINE-TUNED, PROMPT-BASED) with three proposed methods (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, and MULTISAMPLE). The metrics used for comparison are Prudence, Over-Conservativeness, Honesty, and Accuracy.  The table highlights the best-performing method in terms of Honesty score and the second-best in terms of Accuracy.  The caption clearly explains the abbreviations used.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_17_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the in-distribution evaluation on the TriviaQA dataset.  It compares the performance of different methods for aligning LLMs for honesty: an unaligned baseline, a fine-tuned baseline, a prompt-based method, and three proposed methods (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, and MULTISAMPLE). The metrics used for comparison are Prudence, Over-Conservativeness, Honesty, and Accuracy.  The best honesty score is highlighted in bold, while the second-best accuracy is underlined.  The table provides a quantitative assessment of the effectiveness of each method in improving the honesty of LLMs without sacrificing performance.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_18_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the experiments conducted on the TriviaQA evaluation set.  It compares the performance of four different approaches: the unaligned baseline, a fine-tuned baseline, a prompt-based approach, and three proposed honesty-oriented fine-tuning methods (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, and MULTISAMPLE). The metrics used for comparison are Prudence, Over-Conservativeness, Honesty, and Accuracy. The best Honesty score is highlighted in bold, and the second-best Accuracy score is underlined.  The table helps demonstrate the effectiveness of the proposed honesty-oriented fine-tuning methods in improving model honesty without significantly sacrificing accuracy.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_19_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the experiments conducted on the TriviaQA evaluation dataset. It compares four different methods for aligning LLMs with honesty: UNALIGNED BASELINE, FINE-TUNED BASELINE, PROMPT-BASED, and ABSOLUTE.  For each method, the table shows the prudence score, over-conservativeness score, honesty score, and accuracy.  The best honesty score is highlighted in bold, and the second-best accuracy is underlined. The table provides quantitative evidence to evaluate the effectiveness of different alignment strategies in improving the honesty of LLMs.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_20_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the experiments conducted on the TriviaQA evaluation set.  It compares the performance of four different methods: UNALIGNED BASELINE, FINE-TUNED BASELINE, PROMPT-BASED, and three variations of the proposed honesty-oriented fine-tuning method (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, and MULTISAMPLE). For each method, the table shows the prudence score, over-conservativeness score, honesty score, and accuracy.  The best honesty score is highlighted in bold, and the second-best accuracy is underlined.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_20_2.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the Honesty-oriented fine-tuning methods on the TriviaQA evaluation set.  It compares the performance of four different methods: UNALIGNED BASELINE (no alignment), FINE-TUNED BASELINE (supervised fine-tuning without honesty focus), PROMPT-BASED (training-free method using only prompts), and ABSOLUTE (a specific honesty-oriented fine-tuning method). The table shows the Prudence, Over-Conservativeness, Honesty scores and Accuracy for each method.  Higher Prudence and Honesty scores are better, while a lower Over-Conservativeness score is preferred. The best Honesty score for each model is highlighted in bold, and the second best accuracy is underlined.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_21_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the experiments conducted on the TriviaQA dataset.  It compares the performance of several different methods for aligning LLMs with honesty, including a training-free method (PROMPT-BASED) and several supervised fine-tuning methods (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, MULTISAMPLE). The results are shown in terms of Prudence, Over-Conservativeness, Honesty, and Accuracy.  The table highlights which method achieved the best honesty score, and which achieved the second-best accuracy.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_23_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the in-distribution evaluation on the TriviaQA dataset.  It compares the performance of several methods for aligning LLMs with honesty, including a baseline without alignment, a fine-tuned baseline, a training-free method using prompts only, and three proposed honesty-oriented fine-tuning methods (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, and MULTISAMPLE). The metrics used for comparison are Prudence, Over-Conservativeness, Honesty, and Accuracy. The best performing method according to the Honesty score is highlighted in bold, and the second-best performing method according to Accuracy is underlined.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_23_2.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the in-distribution evaluation on the TriviaQA dataset.  It compares the performance of several methods for aligning LLMs with honesty, including a training-free method (PROMPT-BASED), a supervised fine-tuning baseline (FINE-TUNED), and three variations of a proposed honesty-oriented supervised fine-tuning approach (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, MULTISAMPLE).  The metrics used are Prudence, Over-Conservativeness, Honesty, and Accuracy.  The table highlights the effectiveness of the proposed methods in significantly improving honesty scores without severely compromising accuracy.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_23_3.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the Honesty-oriented fine-tuning methods on the TriviaQA evaluation dataset.  It compares the performance of several methods: UNALIGNED (no alignment), FINE-TUNED (supervised fine-tuning without honesty focus), PROMPT-BASED (a simple prompt-based approach), and three variations of the proposed supervised fine-tuning methods (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, and MULTISAMPLE). The metrics used are Prudence, Over-Conservativeness, Honesty, and Accuracy.  The table highlights the best Honesty score and second best Accuracy score achieved.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_24_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the honesty-oriented fine-tuning experiments on the TriviaQA dataset.  It compares the performance of three baselines (UNALIGNED, FINE-TUNED, PROMPT-BASED) with three proposed methods (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, MULTISAMPLE). The metrics used are Prudence, Over-Conservativeness, Honesty, and Accuracy.  The best Honesty score for each model is highlighted in bold, and the second-best Accuracy is underlined.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_24_2.jpg", "caption": "Table 22: Detailed results on Eval-P using GPT-4.", "description": "This table presents the detailed helpfulness scores assessed by GPT-4 for the three models (UNALIGNED, CONFIDENCE-VERB, and MULTISAMPLE) across seven different task categories within the Eval-P dataset.  Each score represents the average helpfulness rating (on a scale of 1 to 10) given by GPT-4 for each model's responses within a specific task category. This allows for a granular comparison of the models' helpfulness performance across various task types, considering the potential impact of honesty-oriented fine-tuning on overall helpfulness.", "section": "4.6 Exp III: Alignment Tax"}, {"figure_path": "67K3Xlvw8L/tables/tables_24_3.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the in-distribution evaluation on the TriviaQA dataset.  It compares the performance of different methods for aligning LLMs with honesty: the unaligned baseline, a fine-tuned baseline, a prompt-based method, and three proposed methods (ABSOLUTE, CONFIDENCE-NUM, CONFIDENCE-VERB, and MULTISAMPLE). The metrics used for comparison are Prudence, Over-Conservativeness, Honesty, and Accuracy.  The table highlights the best-performing method in terms of honesty and accuracy.", "section": "4.4 Exp-I: In-distribution Evaluation"}, {"figure_path": "67K3Xlvw8L/tables/tables_25_1.jpg", "caption": "Table 3: Main results on the TriviaQA evaluation set. UNALIGNED refers to UNALIGNED BASELINE, FINE-TUNED refers to FINE-TUNED BASELINE, and PROMPT-BASED refers to the training-free method that adopts the prompt alone. ABSOLUTE applies m = 10 and \u03c4 = 0.1. The best honesty score is in bold, and the second-highest accuracy is underlined.", "description": "This table presents the main results of the honesty-oriented fine-tuning methods on the TriviaQA evaluation set.  It compares the performance of four different methods: UNALIGNED BASELINE (no alignment), FINE-TUNED BASELINE (supervised fine-tuning without honesty focus), PROMPT-BASED (training-free method using prompts only), and ABSOLUTE (supervised fine-tuning with honesty focus using the ABSOLUTE method).  The table shows the prudence score, over-conservativeness score, honesty score, and accuracy for each method. The best honesty score is highlighted in bold, and the second-best accuracy is underlined, indicating a trade-off between honesty and performance.", "section": "4.4 Exp-I: In-distribution Evaluation"}]