[{"figure_path": "ejWvCpLuwu/figures/figures_1_1.jpg", "caption": "Figure 1: Intuitive illustration of the distribution shifting problem. The 3-dimensional map represents a trained GNN model f, where (h1, h2) represents the embedding distribution of the graph in two dimensions, and Y represents the prediction value of the graph through f. The red and blue lines represent the distribution of the original training graph set and the corresponding explanation sub-graph set, respectively. The distribution of G* shifts away from the original distribution, resulting in shifted prediction values.", "description": "This figure illustrates the distribution shift problem encountered when using a GNN model trained on a set of original graphs to predict the labels of explanation subgraphs. The 3D plot shows the model's predictions (Y-axis) as a function of the graph embeddings (h1 and h2 axes).  The red points represent the original graph data, while the blue points represent the distribution of the explanation sub-graphs. The key observation is that the distribution of the subgraphs (blue) is shifted considerably compared to the distribution of the original graphs (red), which can lead to inaccurate predictions when using the trained GNN model on the subgraphs.", "section": "1 Introduction"}, {"figure_path": "ejWvCpLuwu/figures/figures_3_1.jpg", "caption": "Figure 2: Intuitive illustration about why I(G*;Y) > I(Y*;Y). G* contains more mutual information as having more overlapping area with Y than the overlapping area between Y* and Y.", "description": "This figure provides a visual explanation for why the mutual information between the explanation subgraph G* and the label Y is greater than the mutual information between the prediction label Y* and Y. It uses Venn diagrams to represent the information shared by these variables. The larger overlapping area between G* and Y illustrates that G* contains more information about Y than Y* does.", "section": "4.1 GIB for Explaining Graph Regression"}, {"figure_path": "ejWvCpLuwu/figures/figures_4_1.jpg", "caption": "Figure 3: Visualization of distribution shifting problem on four graph regression datasets. The points represent the regression value, where the blue points mean ground truth label Y, red points mean prediction f(G), and the green points mean prediction f(G*) on the four datasets. The x-axis is the indices of the graph, sorted by the value of the label Y.", "description": "This figure shows the distribution shift problem in graph regression.  Four graphs, one for each dataset (BA-Motif-Volume, BA-Motif-Counting, Triangles, Crippen), illustrate the distribution of ground truth labels (blue), predictions from the original graph (red), and predictions from the explanation subgraph (green). The x-axis represents the graph index sorted by the label value. The significant shift between the red and green points across all datasets demonstrates the distribution shift problem, where the model's performance on the original graph differs substantially from its performance on the explanation subgraph.", "section": "4.2 Distribution Shifting Problem in Graph Regression"}, {"figure_path": "ejWvCpLuwu/figures/figures_5_1.jpg", "caption": "Figure 4: Illustration of RegExplainer. G is the to-be-explained graph, G+ and G\u00af are the randomly sampled positive and negative neighbors. The explanation of the graph is produced by the explainer model. Then graph G* is mixed with (G+)^ = G+ \u2212(G+)* and (G\u00af)^ = G--(G\u00af)* respectively to produce G(mix)+ and G(mix)-. Then the graphs are fed into the trained GNN model to retrieve the embedding vectors h\u207a, h\u00af, h(mix)+ and h(mix)-, where h(mix)+ \u2248 h(mix)- due to the same label-preserving sub-graph G*. We use InfoNCE loss to minimize the distance between G(mix)+ and the positive sample and maximize the distance between G(mix)- and the negative sample. The explainer is trained with the GIB objective and self-supervised contrastive loss.", "description": "This figure illustrates the RegExplainer framework.  It shows how the explainer model generates an explanation subgraph (G*) for a given graph (G). To address the distribution shift problem, a mix-up approach is used, combining G* with subgraphs from similar (G+) and dissimilar (G\u2212) graphs. The resulting mixed graphs, G(mix)+ and G(mix)\u2212, are then fed into a GNN to obtain their embeddings.  The InfoNCE loss is employed to guide the training process.", "section": "4.3 Mix-up Approach with Contrastive Learning"}, {"figure_path": "ejWvCpLuwu/figures/figures_7_1.jpg", "caption": "Figure 5: Ablation study of RegExplainer. We evaluated the AUC performance of the original RegExplainer and its variants that exclude the mix-up approach, InfoNCE loss, or MSE loss, respectively. The black solid line shows the standard deviation.", "description": "This figure shows the ablation study results for the RegExplainer model.  The AUC performance (Area Under the Curve of the Receiver Operating Characteristic curve) is shown for four different datasets.  Each bar represents the performance of a variant of the model where a component (mix-up, InfoNCE loss, or MSE loss) is removed. The original RegExplainer model's performance is shown for comparison, highlighting the contribution of each removed component to overall performance. Error bars show standard deviations.", "section": "5.3 Ablation Study and Hyper-parameter Sensitivity Study (RQ2)"}, {"figure_path": "ejWvCpLuwu/figures/figures_7_2.jpg", "caption": "Figure 6: Hyper-parameters study of \u03b1 and \u03b2 on four datasets with RegExplainer. In both figures, the x-axis is the value of different hyper-parameter settings, and the y-axis is the value of the average AUC score over ten runs with different random seeds.", "description": "This figure shows the results of a hyperparameter sensitivity analysis performed on the RegExplainer model.  Two hyperparameters, \u03b1 and \u03b2, were varied across a range of values, and the resulting average AUC score (Area Under the Receiver Operating Characteristic curve) was calculated across ten runs for each setting.  The x-axis represents the values of the hyperparameters, and the y-axis represents the average AUC score. The plot helps to understand the impact of these hyperparameters on model performance and to identify optimal values. Each line on the plot represents one of the four datasets used in the study.", "section": "5.3 Ablation Study and Hyper-parameter Sensitivity Study (RQ2)"}, {"figure_path": "ejWvCpLuwu/figures/figures_17_1.jpg", "caption": "Figure 3: Visualization of distribution shifting problem on four graph regression datasets. The points represent the regression value, where the blue points mean ground truth label Y, red points mean prediction f(G), and the green points mean prediction f(G*) on the four datasets. The x-axis is the indices of the graph, sorted by the value of the label Y.", "description": "This figure shows the distribution shift problem in graph regression tasks. It visualizes the regression values of the original graph (blue points), the GNN prediction of the original graph (red points), and the GNN prediction of the explanation sub-graph (green points) for four datasets (BA-Motif-Volume, BA-Motif-Counting, Triangles, and Crippen). The x-axis represents the graph indices sorted by their labels (Y). The figure demonstrates that the distribution of the explanation sub-graph predictions significantly shifts away from the distribution of the original graph predictions, highlighting the distribution shift problem which hinders the effectiveness of existing explanation methods for graph regression.", "section": "4.2 Distribution Shifting Problem in Graph Regression"}]