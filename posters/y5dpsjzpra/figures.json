[{"figure_path": "Y5DPSJzpra/figures/figures_1_1.jpg", "caption": "Figure 1: Design of existing SSL algorithms relies on heuristics. (A) Augmentation graphs are common in vision pretraining, providing generalizable features for downstream tasks. (B) We propose an equivalent loss function for SSL pretraining that recovers the same eigenfunctions more efficiently than existing approaches.", "description": "This figure illustrates the design of self-supervised learning (SSL) algorithms. Panel A shows the common use of augmentation graphs in vision pretraining, where augmentations of images provide generalizable features for downstream tasks. Panel B presents the authors' proposed equivalent loss function for SSL pretraining.  This new loss function aims to recover the same eigenfunctions as existing approaches but with improved efficiency.", "section": "1 Introduction"}, {"figure_path": "Y5DPSJzpra/figures/figures_6_1.jpg", "caption": "Figure 2: Low-dimensional projectors can yield good representations. We demonstrate that using a higher orthogonality constraint, \u03b2, for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions (d).", "description": "This figure shows that using a higher orthogonality constraint (\u03b2) with lower-dimensional projectors can achieve similar performance to higher-dimensional projectors across various datasets (CIFAR-10, STL-10, and Imagenet-100) and algorithms (BarlowTwins and VICReg).  The results support the recommendation to use lower-dimensional projectors with appropriate orthogonalization.", "section": "4.1 Low-dimensional projectors can yield good representations"}, {"figure_path": "Y5DPSJzpra/figures/figures_6_2.jpg", "caption": "Figure 2: Low-dimensional projectors can yield good representations. We demonstrate that using a higher orthogonality constraint, \u03b2, for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions (d).", "description": "This figure empirically validates the theoretical finding that using stronger orthogonalization with lower-dimensional projectors can achieve comparable performance to high-dimensional projectors in self-supervised learning.  It shows that by tuning the hyperparameter beta (\u03b2), similar test accuracy can be obtained across different projector dimensionalities (d), disproving the common heuristic that very high-dimensional projection heads are necessary.", "section": "4.1 Low-dimensional projectors can yield good representations"}, {"figure_path": "Y5DPSJzpra/figures/figures_7_1.jpg", "caption": "Figure 4: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure empirically demonstrates the impact of using multiple augmentations in improving sample efficiency.  It shows that with the same effective dataset size (number of augmentations multiplied by the number of unique samples), using more augmentations results in better performance across various datasets (CIFAR-10, STL-10, and Imagenet-100) and algorithms.  However, the benefit plateaus in data-scarce scenarios.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_9_1.jpg", "caption": "Figure 5: Using > 2 augmentations with a fraction of the dataset improves overall Pareto frontier, speeding runtime up to ~ 2x.", "description": "This figure shows the Pareto frontier for sample efficiency in self-supervised learning.  The x-axis represents runtime (in minutes), and the y-axis represents the error rate (%).  Different curves represent different training scenarios using various numbers of augmentations (2, 4, and 8) and different fractions of the full training dataset. The multi-augmentation approach (black circles) shows improved performance (lower error) at a reduced runtime compared to the baseline of 2 augmentations (blue triangles). The figure demonstrates that using more augmentations improves the Pareto frontier by enabling faster convergence or achieving similar performance with fewer samples.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_22_1.jpg", "caption": "Figure 6: Schematic of augmentation graph. (A) Augmentations from each image span a region in the image space which could overlap with the augmentation span of other images. (B) An augmentation graph schematic that uses probabilities to characterize the interactions among augmentation spans of different instances.", "description": "This figure illustrates the concept of an augmentation graph, which represents the relationships between augmented versions of images. Panel (A) shows how augmentations from a single image form a cluster in the feature space, and these clusters can overlap. Panel (B) provides a more detailed view, using probabilities to model the connections between different augmentations.", "section": "C Multi-Augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_24_1.jpg", "caption": "Figure 7: Empirical verification of the subsampling Ansatz.", "description": "This figure empirically validates the subsampling ansatz introduced in the paper. The ansatz pertains to the eigenvalues of a sampled augmentation graph, particularly in scenarios with few augmentations per example.  It shows that when the augmentation space is not sufficiently sampled, the eigenvalues corresponding to class information and pixel-level global information become very close to each other, and eigenvalues associated with augmentations changing both class and global information approach zero. This suggests that when only a few augmentations are used, the learning process may suppress class information in favor of pixel-level information, leading to increased smoothness in the learned feature space. The figure plots the eigenvalues and compares them between the true values from the complete augmentation graph and the values obtained from subsampled versions of this graph.  Error bars demonstrate the variability of the eigenvalues due to the stochasticity of the subsampling process. ", "section": "3.1 Features in terms of data augmentation kernels"}, {"figure_path": "Y5DPSJzpra/figures/figures_24_2.jpg", "caption": "Figure 2: Low-dimensional projectors can yield good representations. We demonstrate that using a higher orthogonality constraint, \u03b2, for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions (d).", "description": "This figure empirically validates that low-dimensional projectors can achieve performance comparable to high-dimensional ones, provided an appropriate orthogonality constraint (\u03b2) is applied.  It shows the test accuracy achieved across different projector dimensions (d) with both a fixed \u03b2 and an optimized \u03b2 for each d. The results support the claim that using a stronger orthogonality constraint with lower-dimensional projectors can be as effective as using a high-dimensional projector.", "section": "4.1 Low-dimensional projectors can yield good representations"}, {"figure_path": "Y5DPSJzpra/figures/figures_25_1.jpg", "caption": "Figure 8: Low-dimensional projectors can yield good representations for both BarlowTwins and VICReg. We demonstrate that using a higher orthogonality constraint, \u03b2, for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions (d). Note that for VICReg, we plot the ratio of the coefficient of the covariance loss to the coefficient of the invariance loss, i.e. \u03b2 = a*/\u03bc, where \u03bc is the coefficient of the invariance loss. (See Equation (49) for details of the loss formulation.)", "description": "This figure shows that using low-dimensional projectors with a stronger orthogonality constraint can achieve similar performance to high-dimensional projectors for both BarlowTwins and VICReg algorithms.  It highlights that the optimal orthogonality constraint (\u03b2) is inversely proportional to the projector dimensionality (d).", "section": "4.1 Low-dimensional projectors can yield good representations"}, {"figure_path": "Y5DPSJzpra/figures/figures_26_1.jpg", "caption": "Figure 2: Low-dimensional projectors can yield good representations. We demonstrate that using a higher orthogonality constraint, \u03b2, for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions (d).", "description": "This figure empirically validates the theoretical finding that using stronger orthogonalization with lower-dimensional projectors can achieve performance comparable to higher-dimensional ones.  It shows the test accuracy on CIFAR-10, STL-10 and ImageNet-100 datasets for different projector dimensions (d) using both fixed and optimized orthogonality constraints (\u03b2). The results support the recommendation to use lower-dimensional projectors with appropriately tuned orthogonality constraints for improved efficiency.", "section": "4.1 Low-dimensional projectors can yield good representations"}, {"figure_path": "Y5DPSJzpra/figures/figures_26_2.jpg", "caption": "Figure 1: Design of existing SSL algorithms relies on heuristics. (A) Augmentation graphs are common in vision pretraining, providing generalizable features for downstream tasks. (B) We propose an equivalent loss function for SSL pretraining that recovers the same eigenfunctions more efficiently than existing approaches.", "description": "This figure illustrates the difference between existing self-supervised learning (SSL) algorithms and the proposed method. (A) shows that existing SSL algorithms rely on heuristic choices regarding data augmentations and the projection dimensionality of the embedding network.  In contrast, (B) illustrates that the proposed method is theoretically grounded and offers an equivalent, more efficient loss function for SSL pretraining, focusing on the eigenfunctions of the augmentation-defined data similarity kernel.", "section": "1 Introduction"}, {"figure_path": "Y5DPSJzpra/figures/figures_27_1.jpg", "caption": "Figure 11: BarlowTwins pretraining on full Imagenet-100 dataset with 2, 4 and 8 augmentations.", "description": "This figure shows the results of BarlowTwins pretraining on the full Imagenet-100 dataset using 2, 4, and 8 augmentations.  The x-axis represents either training epochs or training time (in minutes), and the y-axis represents the test accuracy.  The plot visually compares the convergence speed and final accuracy achieved with different numbers of augmentations. The goal is to demonstrate the impact of using multiple augmentations on both the speed and performance of the self-supervised learning process.", "section": "4.2 Multiple Augmentations Improve Performance and Convergence"}, {"figure_path": "Y5DPSJzpra/figures/figures_27_2.jpg", "caption": "Figure 10: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins and VICReg pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C) or wall clock time (D-F). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure shows the impact of using multiple augmentations on sample efficiency in self-supervised learning.  It demonstrates that with more augmentations, similar performance can be achieved using a smaller subset of the training data.  The results are shown across three datasets (CIFAR-10, STL-10, and Imagenet-100) and for two algorithms (BarlowTwins and VICReg).  While the multi-augmentation approach shows promise in data-scarce situations, a trade-off point may be reached where excessive augmentations fail to further enhance performance.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_28_1.jpg", "caption": "Figure 2: Low-dimensional projectors can yield good representations. We demonstrate that using a higher orthogonality constraint, \u03b2, for lower projector dimensionality can achieve similar performance over a wide range of projector dimensions (d).", "description": "This figure shows the test accuracy achieved by BarlowTwins and VICReg models with different projector dimensionalities (d) on CIFAR-10, STL-10, and Imagenet-100 datasets.  It demonstrates that using a higher orthogonality constraint (\u03b2) with lower-dimensional projectors can achieve similar performance to higher-dimensional projectors, suggesting that high-dimensional projectors might be unnecessary. The optimal \u03b2 value is inversely proportional to the projector dimensionality (d), as predicted by theory.", "section": "4.1 Low-dimensional projectors can yield good representations"}, {"figure_path": "Y5DPSJzpra/figures/figures_28_2.jpg", "caption": "Figure 10: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins and VICReg pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure empirically demonstrates the effect of using multiple augmentations in self-supervised learning (SSL). It shows that with a fixed effective dataset size (the number of augmentations multiplied by the number of unique samples), increasing the number of augmentations can lead to similar or even better performance in downstream tasks.  This indicates that multiple augmentations can act as a form of data augmentation, making the training more sample-efficient. However, the figure also suggests that beyond a certain point, using too many augmentations may not lead to further performance gains, particularly in low-data regimes.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_28_3.jpg", "caption": "Figure 10: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins and VICReg pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure empirically shows that using multiple augmentations in self-supervised learning improves sample efficiency.  The results across three different datasets (CIFAR-10, STL-10, and Imagenet-100) and two algorithms (BarlowTwins and VICReg) demonstrate that achieving similar performance to using a full dataset and two augmentations can be done with smaller datasets and more augmentations.  However, this benefit plateaus in very low data settings.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_29_1.jpg", "caption": "Figure 4: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure empirically demonstrates the impact of using multiple augmentations on sample efficiency in self-supervised learning (SSL).  It shows that by increasing the number of augmentations, similar performance can be achieved even with a smaller proportion of the original training data. The results are presented for three different datasets: CIFAR-10, STL-10, and Imagenet-100, each using BarlowTwins as the SSL algorithm.  The figure highlights a trade-off: while multiple augmentations improve sample efficiency, they might not provide the same level of benefit when the initial dataset is already very small.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_29_2.jpg", "caption": "Figure 4: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure empirically shows that using multiple augmentations in self-supervised learning can lead to improved sample efficiency.  The results, shown across three different datasets (CIFAR-10, STL-10, and Imagenet-100), demonstrate that maintaining a constant effective dataset size (number of augmentations multiplied by the number of unique samples) while increasing the number of augmentations results in similar or even better performance compared to using fewer augmentations with a larger dataset.  The figure highlights a trade-off:  while this benefit is observed in data-rich settings, it may not hold true when dealing with extremely limited data.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_29_3.jpg", "caption": "Figure 10: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins and VICReg pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure empirically demonstrates the impact of using multiple augmentations in self-supervised learning.  It shows that by increasing the number of augmentations, similar performance can be achieved even with a smaller number of unique training samples.  The results are shown across three datasets (CIFAR-10, STL-10, and Imagenet-100) using two different self-supervised learning algorithms (BarlowTwins and VICReg).  While multiple augmentations generally improve performance, a trade-off is observed in data-scarce scenarios.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_30_1.jpg", "caption": "Figure 19: BarlowTwins pretraining on full CIFAR-10 dataset for 400 epochs.", "description": "This figure shows the test error rate of BarlowTwins model with different number of augmentations (2, 4, and 8) over 400 epochs.  It demonstrates the impact of using multiple augmentations on the model's performance and training convergence. The plot shows test error (%) against epochs on the left and training time in minutes on the right.", "section": "E.1 Longer Pretraining to determine early stopping"}, {"figure_path": "Y5DPSJzpra/figures/figures_30_2.jpg", "caption": "Figure 10: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins and VICReg pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure empirically demonstrates the impact of using multiple augmentations in self-supervised learning.  It shows that using more augmentations allows for achieving comparable performance with a smaller number of unique training samples, showcasing improved sample efficiency. This effect is observed across different datasets (CIFAR-10, STL-10, and ImageNet-100) and with two different self-supervised learning algorithms (BarlowTwins and VICReg). While increasing the number of augmentations generally improves performance, there's a trade-off in very data-scarce regimes.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}, {"figure_path": "Y5DPSJzpra/figures/figures_31_1.jpg", "caption": "Figure 10: Multi-augmentation improves sample efficiency, recovering similar performance with significantly fewer unique samples in the pretraining dataset. Across BarlowTwins and VICReg pretraining on CIFAR-10, STL-10 and Imagenet-100 for the same effective dataset size (#augs \u00d7 #unique_samples), using more patches improves performance at the same epoch (A-C). However, a tradeoff exists wherein more data augmentations fail to improve performance in the scarce data regime.", "description": "This figure empirically demonstrates that using multiple augmentations in self-supervised learning (SSL) can significantly improve sample efficiency.  The results are shown across three different datasets (CIFAR-10, STL-10, and Imagenet-100) and for two different SSL algorithms (Barlow Twins and VICReg).  While using more augmentations does increase the overall training time for the same number of epochs, the experiments reveal that comparable or even better performance can be obtained with fewer unique samples by leveraging more data augmentations.", "section": "4.3 Sample Efficient Multi-augmentation Learning"}]