[{"figure_path": "a6em980M9x/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between FNO and AM-FNO: FNO assigns each value at the discretized frequencies of the Fourier-transformed kernel function as a learnable parameter, while AM-FNO utilizes neural network parameterization (MLP or KAN) to approximate the mapping between frequencies and function values. The frequencies are embedded using a set of orthogonal basis functions before being processed by the MLP.", "description": "This figure compares the frequency-specific parameterization used in traditional Fourier Neural Operators (FNOs) with the amortized neural parameterization introduced in the proposed AM-FNO.  FNOs use a separate parameter for each discretized frequency, leading to a large number of parameters, especially for high-dimensional problems.  The truncation of high frequencies further limits the representation capability of FNOs. In contrast, AM-FNO uses either a Multi-Layer Perceptron (MLP) or a Kolmogorov-Arnold Network (KAN) to learn a mapping between frequencies and kernel function values. This allows AM-FNO to handle arbitrarily many frequency modes using a fixed number of parameters, while also leveraging orthogonal embedding to improve the representation of frequency information by MLPs. ", "section": "4 Method"}, {"figure_path": "a6em980M9x/figures/figures_4_1.jpg", "caption": "Figure 2: AM-FNO structure for 2D PDEs: The input function a is mapped to a higher-dimensional space. Stacked operators and activation functions are applied for function propagation. Within the operator layers, a linear transformation R is applied to h(1) after FFT, followed by a feed-forward network (FFN) after the Inverse Fast Fourier Transform (IFFT). The values of R result from KAN or multiplying the MLP transformations of selected one-dimensional orthogonal basis functions (w denotes linear weights.). Finally, the function is projected to the solution dimension space.", "description": "This figure illustrates the architecture of the Amortized Fourier Neural Operator (AM-FNO) for solving 2D Partial Differential Equations (PDEs).  It shows how the input function is processed through multiple operator layers, each involving a Fast Fourier Transform (FFT), a kernel transformation (R), an Inverse FFT (IFFT), and a feed-forward network (FFN). The kernel transformation R is implemented using either a Kolmogorov-Arnold Network (KAN) or a Multi-Layer Perceptron (MLP) with orthogonal basis functions to efficiently handle various frequencies. This allows the model to learn the mapping between input functions and output solutions without explicitly parameterizing each frequency mode, leading to a more efficient and generalizable model.", "section": "4.4 AM-FNO Architecture"}, {"figure_path": "a6em980M9x/figures/figures_6_1.jpg", "caption": "Figure 3: Comparison of L2 norm error on different frequency modes on CFD-1D benchmark.", "description": "This figure compares the L2 norm error across different frequency modes for the CFD-1D benchmark.  It shows the performance of AM-FNO (KAN), AM-FNO (MLP), FNO, FNO+ (FNO without truncation), and U-FNO. The x-axis represents the frequency mode, and the y-axis shows the L2 error.  The figure highlights that AM-FNO models consistently outperform the baseline methods across all frequency ranges, especially in the lower frequencies. The figure also indicates that the errors decrease as the frequency increases, and that the error from the truncated frequencies becomes negligible.", "section": "5.3 Frequency-Based Error Analysis"}, {"figure_path": "a6em980M9x/figures/figures_8_1.jpg", "caption": "Figure 4: L2 relative error varies w.r.t. the number of basis functions (Left), hidden layer size of KANs (middle), and grid size of KANs (right) on Darcy and Airfoil benchmarks.", "description": "This figure shows ablation study results on the impact of hyperparameters of the Kolmogorov-Arnold Networks (KANs) used in AM-FNO.  Three subplots show how the L2 relative error changes with respect to the number of basis functions, the hidden layer size of the KAN, and the grid size of the KAN, respectively.  Results are shown for both the Darcy and Airfoil benchmarks.", "section": "5.4 Ablation Experiments"}, {"figure_path": "a6em980M9x/figures/figures_8_2.jpg", "caption": "Figure 2: AM-FNO structure for 2D PDEs: The input function a is mapped to a higher-dimensional space. Stacked operators and activation functions are applied for function propagation. Within the operator layers, a linear transformation R is applied to h(1) after FFT, followed by a feed-forward network (FFN) after the Inverse Fast Fourier Transform (IFFT). The values of R result from KAN or multiplying the MLP transformations of selected one-dimensional orthogonal basis functions (w denotes linear weights.). Finally, the function is projected to the solution dimension space.", "description": "This figure illustrates the architecture of the Amortized Fourier Neural Operator (AM-FNO) for solving 2D Partial Differential Equations (PDEs).  It shows how the input function is processed through multiple operator layers, each involving a Fast Fourier Transform (FFT), a learned kernel transformation (R), an Inverse Fast Fourier Transform (IFFT), and a feed-forward network (FFN).  The kernel transformation R is either learned by a Kolmogorov-Arnold Network (KAN) or by an MLP that takes in orthogonalized frequency embeddings. The process repeats for multiple layers, finally projecting to the solution space.", "section": "4.4 AM-FNO Architecture"}]