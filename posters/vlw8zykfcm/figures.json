[{"figure_path": "VLw8ZyKfcm/figures/figures_3_1.jpg", "caption": "Figure 1: The overall architecture of Latent Neural Operator.", "description": "The figure illustrates the architecture of the Latent Neural Operator (LNO), a model designed for solving both forward and inverse Partial Differential Equation (PDE) problems. The LNO consists of four main modules: an embedding layer, an encoder, a series of Transformer blocks for operator learning in latent space, and a decoder. The embedding layer transforms the input function data into higher-dimensional representations that consider both spatial coordinates and physical quantity values. The encoder utilizes Physics-Cross-Attention (PhCA) to map the input from geometric space to a learnable latent space. Transformer blocks then operate on this latent representation to learn the underlying PDE operator.  The decoder uses PhCA to transform the latent representation back to geometric space, allowing prediction at any arbitrary point. This architecture is designed for improved accuracy and efficiency, particularly for inverse problems where interpolation and extrapolation are crucial.", "section": "3 Method"}, {"figure_path": "VLw8ZyKfcm/figures/figures_5_1.jpg", "caption": "Figure 2: The working mechanism of Physics-Cross-Attention in encoder and decoder respectively.", "description": "This figure illustrates the Physics-Cross-Attention (PhCA) module, a core component of the Latent Neural Operator (LNO).  The PhCA module acts as both an encoder and decoder, transforming data representations between the real-world geometric space and a learnable latent space.  The diagram shows the process for both the encoder (left) and decoder (right).  In the encoder, input position embeddings (X) and value embeddings (Y) are processed by the attention projector, along with learnable query positions.  The resulting latent representation (Z<sup>0</sup>) is then passed to subsequent processing stages. The decoder performs the inverse transformation, mapping the latent space representation (Z<sup>L</sup>) back to the real-world geometric space.", "section": "3.3 Physics-Cross-Attention"}, {"figure_path": "VLw8ZyKfcm/figures/figures_9_1.jpg", "caption": "Figure 2: The working mechanism of Physics-Cross-Attention in encoder and decoder respectively.", "description": "This figure illustrates the detailed architecture of the Physics-Cross-Attention (PhCA) module, which is a core component of the Latent Neural Operator (LNO).  The PhCA module acts as both an encoder and decoder, transforming data representations between the real-world geometric space and a learnable latent space. The figure displays the process of encoding the input function into the latent space and the subsequent decoding of the output function back to the real-world space, highlighting the decoupling property and the learnable latent space. Specifically, it shows how the input's position and quantity values are encoded into the latent space by the encoder's PhCA, and then how the decoder's PhCA utilizes learnable latent space positions to produce the corresponding output function. The learnable matrices (W1, W2) in the encoder and decoder share the same parameters for efficiency, which is different from the Transolver architecture.", "section": "3.3 Physics-Cross-Attention"}, {"figure_path": "VLw8ZyKfcm/figures/figures_13_1.jpg", "caption": "Figure 1: The overall architecture of Latent Neural Operator.", "description": "This figure illustrates the overall architecture of the Latent Neural Operator (LNO) model. It consists of four main modules: an embedding layer that converts the input data into a higher-dimensional representation; an encoder that transforms the input data into a learnable latent space using the Physics-Cross-Attention (PhCA) mechanism; a series of Transformer blocks that learn the PDE operator within the latent space; and a decoder that transforms the output from the latent space back into the original geometric space, also using the PhCA mechanism.  The figure clearly shows the flow of data through these modules, including the input function, observation positions, prediction positions, and output function. The PhCA module is highlighted as a key component of the LNO, responsible for the transformation between the geometric space and latent space, and enabling the model to operate on latent representations of input/output functions.", "section": "3.2 Latent Neural Operator"}, {"figure_path": "VLw8ZyKfcm/figures/figures_13_2.jpg", "caption": "Figure 1: The overall architecture of Latent Neural Operator.", "description": "This figure shows the overall architecture of the Latent Neural Operator (LNO).  The LNO consists of four main modules: an embedding layer, an encoder using Physics-Cross-Attention (PhCA) to transform data to a latent space, a series of Transformer blocks to learn the operator in the latent space, and a decoder (also using PhCA) to transform data back to the real-world geometric space.  The figure highlights the flow of data through these modules, starting with the input function and ending with the output function. The key innovation is the use of the PhCA module which decouples input and output sample locations, enabling flexibility in predicting values at positions not seen during training.", "section": "3.2 Latent Neural Operator"}, {"figure_path": "VLw8ZyKfcm/figures/figures_14_1.jpg", "caption": "Figure 6: Visualization of Burgers' equation with different observation situations in different regions. We propose a two-stage strategy. First, we interpolate the solution in the subdomain. Then, we extrapolate from the subdomain to the whole domain.", "description": "This figure visualizes the results of solving the Burgers' equation using different observation strategies. The leftmost panel shows a random observation in a subdomain, the middle panel shows the complete solution interpolated within the same subdomain, and the rightmost panel shows the complete solution extrapolated to the entire domain.  The figure illustrates the two-stage approach (interpolation followed by extrapolation) used in the inverse problem to reconstruct the solution from sparse observations.", "section": "4.2 Accuracy for the inverse problem"}, {"figure_path": "VLw8ZyKfcm/figures/figures_14_2.jpg", "caption": "Figure 7: Visualization of Burgers' equation in the fixed observation situation with different temporal and spatial sampling intervals.", "description": "This figure visualizes the results of solving the Burgers' equation using different temporal and spatial sampling intervals in a fixed observation scenario.  It demonstrates how the accuracy of the solution is affected by varying the density of the observed data points. The figure shows three subplots, each representing a different level of sampling density (intervals=4, 8, and 16), and a colorbar indicating the values of the solution, u. Each subplot displays the spatial and temporal locations of the observed data points as small dots. The plot shows that increasing sampling interval decreases the accuracy of the result in the extrapolation region, especially the temporal sampling interval.", "section": "4.2 Accuracy for the inverse problem"}, {"figure_path": "VLw8ZyKfcm/figures/figures_14_3.jpg", "caption": "Figure 2: The working mechanism of Physics-Cross-Attention in encoder and decoder respectively.", "description": "This figure illustrates the Physics-Cross-Attention (PhCA) mechanism used in both the encoder and decoder of the Latent Neural Operator (LNO).  The encoder transforms data from the geometric space to the latent space, while the decoder performs the reverse transformation.  The diagram highlights the key components: the attention mechanism (softmax), projection layers, and the interaction between query (H or P), key (X), and value (Y) matrices.  The use of learnable parameters in these matrices emphasizes the data-driven learning aspect of PhCA, allowing the model to automatically learn efficient mappings between function spaces.", "section": "3.3 Physics-Cross-Attention"}]