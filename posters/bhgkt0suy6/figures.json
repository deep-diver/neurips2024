[{"figure_path": "bHgkT0sUy6/figures/figures_1_1.jpg", "caption": "Figure 1: DUPLEX data flow while training three policies over a set of two contexts. At each iteration, we provide three inputs to our multi-policy agent: (i) a context vector describing task requirements and environment dynamics in the current episode c; (ii) an encoding of the policy used in the episode z; (iii) and the current state of the environment s. The critic network returns estimates for the intrinsic and extrinsic rewards and successor features to drive diverse behavior discovery. Finally, the algorithm samples policies in \u03a0 uniformly and rolls them out to collect more experience.", "description": "This figure illustrates the DUPLEX algorithm's data flow during the training of three different policies across two distinct contexts.  The algorithm uses three inputs: context vector (c), policy encoding (z), and current environment state (s). A critic network then processes this information to output intrinsic and extrinsic reward estimates and successor features.  These outputs guide the policy learning process, promoting diversity and near-optimality.  Finally, policies are sampled uniformly for rollouts to gather further experience, iteratively refining the learning process.", "section": "1 Introduction"}, {"figure_path": "bHgkT0sUy6/figures/figures_6_1.jpg", "caption": "Figure 2: Results in GT. a) is the reward-diversity trade off, DUPLEX/DOMiNO_best_policy refers to maxr. (\u03c0\u2208 \u03a0) of every run, while DUPLEX/DOMiNO_avg refers to avgr\u300f(\u03c0\u2208 \u03a0) of that run. Differently from DOMINO, DUPLEX is able to generate both competitive and diverse behaviors. Figure b) illustrates the number of policies that finish laps (active). Figure c) min lap times of the diverse policies. Note that the number of active policies is important to showcase that all policies are actively searching for near-optimal diverse behaviors through different epochs. Applying our soft lower bound (\u03b2 \u2260 0) is key to focusing diversity in the region of interest.", "description": "This figure displays the results of the DUPLEX and DOMINO algorithms on the Gran Turismo 7 (GT) racing simulator.  Subfigure (a) shows the reward-diversity trade-off, illustrating that DUPLEX achieves both high rewards and high diversity, unlike DOMINO. Subfigure (b) demonstrates the number of active policies (those that complete laps) over training epochs, highlighting DUPLEX's ability to maintain a diverse set of active policies.  Subfigure (c) presents the minimum lap times achieved by each policy, further showcasing the diverse performance of DUPLEX.", "section": "5 Experiments"}, {"figure_path": "bHgkT0sUy6/figures/figures_7_1.jpg", "caption": "Figure 2: Results in GT. a) is the reward-diversity trade off, DUPLEX/DOMiNO_best_policy refers to maxr. (\u03c0\u2208 \u03a0) of every run, while DUPLEX/DOMiNO_avg refers to avgr\u300f(\u03c0\u2208 \u03a0) of that run. Differently from DOMINO, DUPLEX is able to generate both competitive and diverse behaviors. Figure b) illustrates the number of policies that finish laps (active). Figure c) min lap times of the diverse policies. Note that the number of active policies is important to showcase that all policies are actively searching for near-optimal diverse behaviors through different epochs. Applying our soft lower bound (\u03b2 \u2260 0) is key to focusing diversity in the region of interest.", "description": "This figure presents the results of the DUPLEX and DOMINO algorithms on the Gran Turismo 7 (GT) racing simulator.  Panel (a) shows the reward-diversity trade-off, illustrating that DUPLEX achieves both high rewards and high diversity, unlike DOMINO. Panel (b) displays the number of active (finishing) policies over training epochs, showing DUPLEX maintains a diverse set of active policies. Panel (c) shows the minimum lap times achieved by each policy, highlighting the diversity in performance across policies. The use of a soft lower bound (\u03b2) in DUPLEX is crucial for focusing the search for diverse policies in a near-optimal region.", "section": "5 Experiments"}, {"figure_path": "bHgkT0sUy6/figures/figures_8_1.jpg", "caption": "Figure 2: Results in GT. a) is the reward-diversity trade off, DUPLEX/DOMiNO_best_policy refers to maxr. (\u03c0\u2208 \u03a0) of every run, while DUPLEX/DOMiNO_avg refers to avgr\u300f(\u03c0\u2208 \u03a0) of that run. Differently from DOMINO, DUPLEX is able to generate both competitive and diverse behaviors. Figure b) illustrates the number of policies that finish laps (active). Figure c) min lap times of the diverse policies. Note that the number of active policies is important to showcase that all policies are actively searching for near-optimal diverse behaviors through different epochs. Applying our soft lower bound (\u03b2 \u2260 0) is key to focusing diversity in the region of interest.", "description": "This figure presents the results of the experiments conducted in Gran Turismo 7 (GT).  Panel (a) shows the reward-diversity trade-off, comparing DUPLEX and DOMINO.  DUPLEX shows a better balance between reward and diversity than DOMINO. Panel (b) illustrates the number of active policies (those that complete laps) over training epochs, highlighting the sustained diversity of DUPLEX.  Panel (c) displays the minimum lap times achieved by the diverse policies learned by each algorithm, further demonstrating the diverse and competitive driving behaviors learned by DUPLEX.", "section": "5 Experiments"}, {"figure_path": "bHgkT0sUy6/figures/figures_8_2.jpg", "caption": "Figure 2: Results in GT. a) is the reward-diversity trade off, DUPLEX/DOMiNO_best_policy refers to maxr. (\u03c0\u2208 \u03a0) of every run, while DUPLEX/DOMiNO_avg refers to avgr\u300f(\u03c0\u2208 \u03a0) of that run. Differently from DOMINO, DUPLEX is able to generate both competitive and diverse behaviors. Figure b) illustrates the number of policies that finish laps (active). Figure c) min lap times of the diverse policies. Note that the number of active policies is important to showcase that all policies are actively searching for near-optimal diverse behaviors through different epochs. Applying our soft lower bound (\u03b2 \u2260 0) is key to focusing diversity in the region of interest.", "description": "This figure shows the results of the DUPLEX algorithm in the Gran Turismo 7 (GT) environment.  Panel (a) presents a reward-diversity trade-off curve, comparing DUPLEX and DOMINO, highlighting DUPLEX's ability to achieve both high reward and high diversity. Panel (b) illustrates the number of active policies (those that complete a lap) over training epochs, demonstrating that DUPLEX maintains a diverse set of active policies.  Finally, panel (c) shows the minimum lap times achieved by the diverse policies, further emphasizing the competitive performance of all policies learned by DUPLEX.", "section": "5 Experiments"}, {"figure_path": "bHgkT0sUy6/figures/figures_13_1.jpg", "caption": "Figure 3: Walker single-task results. Both DUPLEX and DOMINO do not estimate SFs and use dav. On the left, we show results from the best policy in each variant while on the right, the average reward across all policies. Dots and lines represent mean and standard deviation, respectively. It is worth reporting that the vanilla SAC implementation scores 5576.24\u00b1256.783 when evaluated at the same training epoch.", "description": "This figure displays the performance of DUPLEX and DOMINO on the MuJoCo Walker environment for a single task.  Both algorithms used the average occupancy metric (dav) instead of estimating successor features (SFs).  The left panel shows the reward obtained by the best-performing policy for each method, while the right panel shows the average reward across all policies. The results highlight the superior performance of DUPLEX in achieving both high rewards and diversity.  A baseline result using vanilla SAC is also provided for context.", "section": "5 Experiments"}]