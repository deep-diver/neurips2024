[{"type": "text", "text": "Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Guillaume Huguet1,2,3,\u2217 James Vuckovic1,\u2217 Kilian Fatras1,\u2020 Eric Thibodeau-Laufer1,\u2020 Pablo Lemos1, Riashat Islam1, Cheng-Hao Liu1,3,4, Jarrid Rector-Brooks1,2,3,   \nTara Akhound-Sadegh1,2,4, Michael Bronstein1,5,6, Alexander Tong1,2,3,\u2021 Avishek Joey Bose1,5\u2021   \n1Dreamfold, 2Universit\u00e9 de Montr\u00e9al, 3Mila, 4McGill University, 5University of Oxford, 6Aithyra ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Proteins are essential for almost all biological processes and derive their diverse functions from complex 3D structures, which are in turn determined by their amino acid sequences. In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FOLDFLOW- $2^{4}$ , a novel sequence-conditioned SE(3)-equivariant flow matching model for protein structure generation. FOLDFLOW-2 presents substantial new architectural features over the previous FOLDFLOW family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase diversity and novelty of generated samples\u2014crucial for de-novo drug design\u2014we train FOLDFLOW-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through flitering. We further demonstrate the ability to align FOLDFLOW-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FOLDFLOW-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FOLDFLOW-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Rational design of novel protein structures via generative modeling holds significant promise for accelerating computational drug discovery [Chevalier et al., 2017, Ebrahimi and Samanta, 2023]. In particular, the ability to design proteins with a pre-specified functional property is arguably one of the principal tools in addressing global health challenges such as COVID-19 [Cao et al., 2020, Gainza et al., 2023], influenza [Strauch et al., 2017], and cancer [Silva et al., 2019]. In many instances, designing function involves the design of both the 3D geometric structure of the protein as well as its specific chemical interactions. In proteins, the amino-acid sequences determine the interaction between protein backbones and side chains, which fold into a distribution of protein structures. Consequently, the functional properties of protein structures can be inferred from its sequence. ", "page_idx": 0}, {"type": "text", "text": "The representation of proteins plays a key aspect in any computational approach to protein engineering. The 3D structure of proteins can be mathematically represented on the space of rotation and translation invariant $\\mathrm{SE}(3)^{N}$ . Several unconditional protein generative models have been developed recently to generate new protein backbones [Yim et al., 2023b, Bose et al., 2024]. While these models demonstrate the ability to design new proteins, they are insufficiently tailored for downstream drug discovery applications, where the primary challenge lies in generating proteins that are specifically tailored to interact effectively with a given target. In real-world drug design problems, one often knows the target protein (its amino acid sequence and often an experimentally verified 3D structure). In machine learning terms, the design of new proteins (\u201cde novo\u201d design) that can drug the given target can be framed as a conditional generation problem. This raises the following research question: ", "page_idx": 1}, {"type": "text", "text": "How can we leverage the structure and sequence of a target to inform de novo protein design? ", "page_idx": 1}, {"type": "text", "text": "Current work. In this paper, we introduce FOLDFLOW-2 a novel protein structure generative model that is additionally conditioned on protein sequences. FOLDFLOW-2 is built on the foundations of FOLDFLOW [Bose et al., 2024] and is an $\\mathrm{SE}(3)^{N}$ -invariant generative model for protein backbone generation that handles multi-modal data by design. Specifically, FOLDFLOW-2 introduces several new architectural components over previous protein structure generative models that enable it to process both 3D structure and discrete sequences. These include (1) a joint structure and sequence encoder; (2) a multi-modal fusion trunk that combines the representations from each modality in a shared representation space; and (3) a transformer-based geometric decoder. In contrast to prior efforts to incorporate sequences in structure-based generative models [Campbell et al., 2024], FOLDFLOW-2 leverages the representational power of a large pre-trained protein language model in ESM [Lin et al., 2022] enabling it to make use of the rich biological inductive bias found in sequences but at a scale far beyond ground-truth experimental 3D structures found in the Protein Data Bank (PDB). ", "page_idx": 1}, {"type": "text", "text": "As a sequence-conditioned model, FOLDFLOW-2 is able to tackle a suite of new tasks beyond simple unconditional generation. Specifically, our model can additionally be used for protein folding by simply generating structures conditioned on sequence as well as hard, biologically motivated conditional design problems. For instance, our model can perform partial structure generation by conditioning on a masked sequence, i.e., structure in-painting. This enables FOLDFLOW-2 to be better equipped than prior structure-only generative models to tackle the key challenges in de novo drug design. For example, in settings where we aim to engineer a structure that binds and neutralizes a desired target protein structure and sequence pair; this is precisely a structure and sequence in-painting problem. ", "page_idx": 1}, {"type": "text", "text": "As diversity and quantity of training samples play a crucial role in downstream generative modeling performance on conditional design tasks, we construct a new large dataset\u2014an order of magnitude larger than PDB\u2014of high-quality synthetic structures filtered from SwissProt [Jumper et al., 2021, Varadi et al., 2021]. We further investigate the impact of fine-tuning FOLDFLOW-2 using Reinforced Fine-Tuning (ReFT), a new approach that aligns flow-matching generative models to arbitrary rewards. In the context of protein backbone generation, we apply fine-tuning to improve the properties of generated backbones, such as optimizing for the diversity of secondary structures, as well as improving the performance on conditional generation tasks like generating scaffolds around a target motif. ", "page_idx": 1}, {"type": "text", "text": "Main results. We summarize the main empirical results obtained using FOLDFLOW-2 below: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We empirically demonstrate that FOLDFLOW-2 achieves state-of-the art performance for unconditional generation and leads to the most designable, novel, and diverse proteins. In particular, FOLDFLOW-2 improves over RFDiffusion [Watson et al., 2023] and FoldFlow [Bose et al., 2024]. \u2022 We find FOLDFLOW-2 closes the gap in performance with purpose-built folding models like ESMFold and improves by a factor of $\\approx4\\times$ compared to MultiFlow [Campbell et al., 2024], the most comparable protein structure generative model that also leverages sequences. \u2022 We use FOLDFLOW-2 to solve a biologically relevant conditional design problem in motif scaffolding. We find that a fine-tuned FOLDFLOW-2 is able to solve all $24/24$ scaffolds in the benchmark dataset from Watson et al. [2023]. On challenging VHH nanobodies, it solves $9/25$ refoldable motifs in comparison to $5/25$ for the previous best approach RFDiffusion. \u2022 We hypothesize that FOLDFLOW-2 is able to perform zero-shot equilibrium conformation sampling on unseen proteins in the ATLAS molecular dynamics (MD) dataset [Vander Meersche et al., 2024] based on conformation variation seen within the protein data bank. We observe that FOLDFLOW-2 is able to capture different modes of the equilibrium conformation comparably to ESMFlow-MD [Jing et al., 2024], a model fine-tuned on MD data, but lags behind AlphaFlow-MD [Jing et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "2 Background and preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Protein backbone and sequence parametrizations ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Sequence representation. Protein sequences correspond to the chain of amino acids, which for a protein of length $N$ is identified by a discrete token $a^{i}\\,\\in\\,\\{1,\\dots,20\\}\\,=:{\\cal A}$ . As is customary in protein language models [Lin et al., 2022], these discrete tokens are encoded using a one-hot representation. We denote the entire amino acid sequence associated with a protein as A \u2208RN\u00d720. ", "page_idx": 2}, {"type": "text", "text": "Structure representation. The 3D structure of protein backbones can be represented as rigid frames associated with each residue in an amino acid sequence [Jumper et al., 2021]. Each residue, $i$ , within a protein backbone of length $N$ consists of idealized coordinates of their 4 heavy atoms $\\mathbf{N}^{*},\\mathbf{C}_{\\alpha}^{*},\\mathbf{C}^{*},\\mathbf{O}^{*}\\in\\mathbb{R}^{3}$ , with ${\\cal C}_{\\alpha}^{*}=(0,0,0)$ . The defining property of rigid frames is that they can be viewed as elements of the special Euclidean group SE(3) and as such each frame $x=(r,s)\\in\\mathrm{SE}(3)$ contains a rotation $r$ and translation $s$ component. Applying a rigid transformation $x^{i}$ to the idealized coordinates of the heavy atoms allows us to represent the rigid frame of a given residue, $[\\mathrm{N},\\mathrm{C}_{\\alpha},\\mathrm{C},\\mathrm{O}]^{i}=x^{i}\\circ[\\mathrm{N}^{\\ast},\\mathrm{C}_{\\alpha}^{\\ast},\\mathrm{C}^{\\ast},\\mathrm{O}^{\\ast}]$ , where $\\circ$ is the binary operator associated to the group, which for SE(3) is simply matrix multiplication. This leads to a structure representation of the complete 3D coordinates associated with all heavy atoms of a protein as the tensor $\\mathbf{X}\\in\\mathbb{R}^{N\\times4\\times3}$ . ", "page_idx": 2}, {"type": "text", "text": "$\\mathrm{SE}(3)$ : the group of rigid motions. The special Euclidean group SE(3) contains rotations and translations in three dimensions and can be thought of in several ways. It is a Lie group, i.e., a differentiable manifold endowed with a group structure. SE(3) can be seen as the group of rigid frames, representing 3D rotations and translations. As a Lie group, SE(3) can be uniquely identified with its Lie algebra, the tangent space at the identity element of the group. SE(3) is also a matrix Lie group, meaning that its elements can be represented with matrices. It can formally be written as the semidirect product of the rotation and the translation groups, $\\mathrm{SE}(3)\\cong\\mathrm{SO}(3)\\ltimes\\bar{(}\\mathbb{R}^{3},+)$ . A more detailed introduction to Riemannian manifolds and Lie theory, with an emphasis on $\\mathrm{SE}(3)$ is provided in $\\S\\mathrm{A}$ ", "page_idx": 2}, {"type": "text", "text": "2.2 Flow matching on the SE(3) group ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As Lie groups are smooth manifolds, they can also be equipped with a Riemannian metric, which can be used to define distances and geodesics on the manifold. On SE(3), a natural choice of the metric decomposes into the metrics on its constituent subgroups, SO(3) and $\\mathbb{R}^{3}$ [Bose et al., 2024, Yim et al., 2023b]. This allows us to build independent flows on the group of rotations and translations and induce a flow directly on SE(3). As flow matching on Euclidean spaces is well-studied [Albergo and Vanden-Eijnden, 2023, Lipman et al., 2023, Liu et al., 2023], we restrict our focus on reviewing flows, conditional probability paths, and vector fields over the group SO(3). ", "page_idx": 2}, {"type": "text", "text": "Probability paths on SO(3). Given two densities $\\rho_{0},\\rho_{1}\\in\\mathrm{SO(3)}$ , a probability path $\\rho_{t}:[0,1]\\to$ $\\mathbb{P}(\\mathrm{SO}(3))$ is an interpolation, parametrized by time, $t$ , between the two densities in probability space. Without loss of generality, we may consider $\\rho_{0}$ to be the target data distribution and $\\rho_{1}$ an easy-tosample source distribution. A flow is a one-parameter diffeomorphism in $t$ , $\\psi_{t}:\\mathrm{SO}(3)\\to\\mathrm{SO}(3)$ . It is the solution to the ordinary differential equation (ODE): $\\begin{array}{r}{\\frac{d}{d t}\\psi_{t}(r)\\,=\\,u_{t}\\,(\\psi_{t}(r))}\\end{array}$ , with initial condition $\\psi_{0}(r)=r$ , where $u_{t}$ is the time-dependent smooth vector $u_{t}:[0,1]\\times\\mathrm{SO(3)}\\to\\mathrm{SO(3)}$ . It is said that $\\psi_{t}$ generates $\\rho_{t}$ if it induces a pushforward map $\\rho_{t}=[\\psi_{t}]_{\\#}(\\dot{\\rho}_{0})$ . ", "page_idx": 2}, {"type": "text", "text": "Matching vector fields on $\\mathrm{SO}(3)$ . The framework of Riemannian flow matching [Chen and Lipman, 2024] can also accommodate Lie groups such as SO(3). Consequently, to learn a continuous normalizing flow (CNF) that pushes forward samples $r_{0}\\ \\sim\\ \\rho_{0}$ to $r_{1}\\ \\sim\\ \\rho_{1}$ we must regress a parametric vector field $v_{\\theta}\\in\\mathfrak{X}(\\mathrm{SO}(3))$ in the tangent space of the manifold to the target conditional vector field $u_{t}(r_{t}|r_{0},r_{1})$ , for all $t~\\in~[0,1]$ . Conveniently, the target $u_{t}(r_{t}|r_{0},\\bar{r_{1}})$ is the time derivative of a point $r_{t}$ along the shortest path between $r_{0}$ and $r_{1}$ \u2014i.e., the geodesic interpolant $r_{t}\\ =\\ \\exp_{r_{0}}(t\\log_{r_{0}}(r_{1}))$ . Furthermore, for $\\mathrm{SO}(3)$ the target conditional vector field admits a closed-form expression $u_{t}(r_{t}|r_{0},r_{1})\\,=\\,\\log_{r_{t}}(r_{0})/t$ as the exponential and logarithmic maps are numerically computable using the axis-angle representation of the group elements [Bose et al., 2024]. Given these ingredients, we can formulate the flow matching objective for SO(3) as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{SO(3)}}(\\theta)=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1),q(r_{0},r_{1}),\\rho_{t}(r_{t}|r_{0},r_{1})}\\left\\|v_{\\theta}(t,r_{t})-\\log_{r_{t}}(r_{0})/t\\right\\|_{\\mathrm{SO(3)}}^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In eq. (1), $q(r_{0},r_{1})$ is any coupling between samples from the source and target distributions. An optimal choice is to set $\\bar{q}(r_{0},\\bar{r_{1}})\\doteq\\pi(r_{0},r_{1})$ which is the coupling, $\\pi$ , that solves the Riemannian ", "page_idx": 2}, {"type": "text", "text": "optimal transport problem using minibatches [Bose et al., 2024, Tong et al., 2023, Fatras et al., 2020]. Finally, the generation of samples is done by first drawing from a source sample $r_{1}\\sim\\rho_{1}$ and integrating the ODE backward in time using the learned vectorfield $v_{\\theta}$ . ", "page_idx": 3}, {"type": "text", "text": "3 FOLDFLOW-2 ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now present FOLDFLOW-2, our sequenceconditioned structure generative model. FOLDFLOW-2 operates on protein backbones $x_{0}\\ \\sim\\ \\rho_{0}$ which are parametrized as $N$ rigid frames as well as their corresponding sequence $a$ . As protein backbones contain symmetries from SE(3), we design FOLDFLOW-2 as an $\\mathrm{SE}(3)^{N}$ -invariant density using a flow-matching objective. We achieve translation invariance by constructing the flow on the subspace $\\mathrm{SE}(3)_{0}^{N}$ , where the center of mass of the inputs is removed. Additionally, we can focus on building flows on the group of rotations $\\mathrm{SO}(3)$ ", "page_idx": 3}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/d4ea2935c5467a4dd63d1967ba1a85c1baf14091558ed182f7f649e976da041b.jpg", "table_caption": ["Table 1: Overview of the conditioning capability of unconditional $(\\varnothing)$ , folding (A), and inpainting $(\\mathrm{A},\\mathrm{X})$ of various protein backbone generation models. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "and translations $\\mathbb{R}^{3}$ , for each of the $N$ residues independently, as $\\mathrm{SE}(3)_{0}^{N}$ can be viewed as a product manifold consisting of $N$ copies of $\\mathrm{SE}(3)_{0}$ . The overall loss function for the model decomposes into per residue rotation and translation losses $\\mathcal{L}=\\mathcal{L}_{\\mathrm{SO(3)}}+\\mathcal{L}_{\\mathbb{R}^{3}}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1),\\rho_{t}(x_{t}|x_{0},x_{1},\\bar{a})}\\left[\\left\\|v_{\\theta}(t,r_{t},\\bar{a})-\\log_{r_{t}}\\frac{r_{0}}{t}\\right\\|_{\\mathrm{SO}(3)}^{2}+\\left\\|v_{\\theta}(t,s_{t},\\bar{a})-\\frac{(s_{t}-s_{0})}{t}\\right\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the pair $(x_{0},x_{1})\\sim\\pi(x_{0},x_{1})$ is sampled from the optimal transport plan $\\pi$ . In addition, the sequence ${\\bar{a}}=a\\odot m$ corresponds to $x_{0}$ and is masked completely, with a mask $m$ , with a probability Bern(0.5). Operationally, this means $50\\%$ of the time the model is trained unconditionally with no sequence information, i.e., $\\bar{\\boldsymbol{a}}=[\\varnothing]^{N}$ , while the other $50\\%$ the model has access to the full sequence ${\\bar{a}}=a$ . Optimizing the loss in eq. (2) is equivalent to maximizing the conditional log-likelihood of observing protein structures given their sequences lo $\\mathrm{g}\\,p(\\mathrm{X}|\\mathrm{A})$ when the sequence is not masked and maximizing the unconditional log-likelihood $\\log{p(\\mathrm{X})}$ when the sequence is fully masked. Due to the ability to mask sequences, FOLDFLOW-2 enables new modeling capabilities in comparison to existing models as outlined in table 1. More precisely, FOLDFLOW-2 trained using masked sequences can perform a diverse set of tasks, outlined in table 2, beyond simple unconditional backbone generation which aids in tackling more biologically relevant problems that require conditional generation such as mimicking a protein folding model and designing the 3D scaffolds around a target motif. ", "page_idx": 3}, {"type": "text", "text": "With the breadth of tasks T1\u2013T3 (table 2) FOLDFLOW-2 unlocks new structural design capabilities beyond the simple unconditional generation ability of FOLDFLOW. We next outline the architectural components of FOLDFLOW-2 in $\\S3.1$ before detailing the training procedure in $\\S3.2$ , which includes key design decisions regarding the construction of our new scaled dataset of ground truth PDBs ", "page_idx": 3}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/24d4a7583425db22c3f3dfd4c78d226de713119ed830a3eb9e4e64b28945c0b3.jpg", "table_caption": ["Table 2: By manipulating the input modalities, FOLDFLOW-2 is able to perform a diverse set of conditional and unconditional generation tasks including biologically relevant tasks such as designing scaffolds. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "and filtered AlphaFold2 synthetic structures. We also outline the inference procedure for sampling in $\\S B.4$ . We conclude by discussing various techniques to fine-tune FOLDFLOW-2, including methods based on filtering with auxiliary rewards for supervised fine-tuning $\\S3.3$ to align protein structures. ", "page_idx": 3}, {"type": "text", "text": "3.1 FOLDFLOW-2 Architecture ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The FOLDFLOW-2 architecture is comprised of three core components: (1) Structure and sequence encoder: An encoder which encodes both structures and sequences; (2) Multi-modal fusion trunk: the trunk which combines the multi-modal representations of the encoded structure and sequences; and (3) Geometric Decoder: a decoder that consumes the fused representation from the trunk and outputs a generated structure. The overall architecture of FOLDFLOW-2 is depicted in fig. 1. ", "page_idx": 3}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/6d8248280b4371dc470fd65ac2459550b9836b312670eff66643da6ab10d3c53.jpg", "img_caption": ["Figure 1: FOLDFLOW-2 architecture which processes sequence and structure and outputs $\\mathrm{SE}(3)_{0}^{N}$ vectorfields. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Structure and Sequence Encoder. We leverage existing state-of-the-art architectures to encode the structure and sequence modalities separately. For structure encoding, we rely on the invariant point attention (IPA) transformer architecture [Jumper et al., 2021], which is SE(3)-equivariant. The benefit of the IPA architecture is that it is highly flexible and can both consume and produce a structure\u2014i.e., $N$ rigid frames\u2014and also output single and pair representations of the input structure. ", "page_idx": 4}, {"type": "text", "text": "To encode amino-acid sequences, we use a large pre-trained protein language model: the 650M variant of the ESM2 sequence model [Lin et al., 2022]. Large protein language models have a strong inductive bias on atomic-level predictions of protein structures while exhibiting strong generalization properties beyond any known experimental structures\u2014which we argue is highly correlated with goals of de novo structure design. Moreover, the ESM2 architecture also produces single and pair representations for an encoded sequence of amino acids, which conceptually correspond to the single and pair representations from the structure encoder. Consequently, the output space of each modality prescribes a natural fusion of representations into a joint single and pair latent space for a given input protein. ", "page_idx": 4}, {"type": "text", "text": "Multi-Modal fusion trunk. After encoding both input structure and sequence, we construct a joint representation for the single and pair representation using a \u201cproject and concatenate\u201d combiner module with simple MLPs, see fig. 1. We use LayerNorm [Ba et al., 2016] throughout the architecture as it is essential to accommodate differently-scaled inputs. The joint representations are further processed by a series of Folding blocks [Lin et al., 2023], which refines the single and pair representations via triangular self-attention updates. ", "page_idx": 4}, {"type": "text", "text": "Geometric decoder. To decode the joint representations of the inputs into $\\mathrm{SE}(3)_{0}^{N}$ vector fields, we once again leverage the IPA Transformer architecture. The decoder takes as input the single, pair outputs of the trunk and the rigid representations from the structure encoder. One of our major findings is that including a skip-connection between the structure encoder and the decoder is essential for good performance as the temporal information is only given to the structure encoder. ", "page_idx": 4}, {"type": "text", "text": "Given each component, we stack $2-2-2$ blocks for the encoder, trunk, and decoder components. ", "page_idx": 4}, {"type": "text", "text": "3.2 Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We train FOLDFLOW-2 by alternating between both folding and unconditional generation tasks using a novel sequence-and-structure flow matching procedure, described below. ", "page_idx": 4}, {"type": "text", "text": "Dataset construction. The generalization ability of generative models trained using maximum likelihood is determined by the quality and diversity of curated training data [Kadkhodaie et al., ", "page_idx": 4}, {"type": "text", "text": "2024]. Due to the limited size of ground truth structures in the Protein Data Bank (PDB) we aim to improve training set diversity by additionally curating a dataset of flitered AlphaFold2 structures from SwissProt [Jumper et al., 2021, Varadi et al., 2021]. To ensure FOLDFLOW-2 is trained on high-quality synthetic structures, we employ a set of stringent flitering techniques that remove many undesignable proteins from SwissProt. After flitering, our final dataset consists of $160K$ structures and constitutes approximately an $8\\times$ fold increase compared to prior works [Yim et al., 2023b, Bose et al., 2024]. Our exact layered flitering strategy for synthetic structures in SwissProt is outlined by the following steps: ", "page_idx": 5}, {"type": "text", "text": "(Step 1) Filtering low-confidence structures. We use per-residue local confidence metrics like the average pLDDT to filter out low-confidence structures from the initial SwissProt dataset.   \n(Step 2) Masking low-confidence residues. Globally high-confident structures may include low-confidence residues with disordered regions that can impede training. We use a per-residue pLDDT threshold of 70 to mask such \u201clow-quality\" residues during training.   \n(Step 3) Filter high-confidence, low-quality structures. The nature of synthetic data means that even following steps 1 and 2 low-quality data persists in a curated dataset. To combat this we further filter structures by learning a light-weight structure prediction model trained on structural features predictive of protein quality. ", "page_idx": 5}, {"type": "text", "text": "We report a detailed analysis of each step in the filtration process in $\\S B.1$ which includes examples of low-quality structures that were filtered as illustrative examples. The impact of these findings is empirically corroborated in by analyzing generated samples from FOLDFLOW-2 in $\\S C.2$ . ", "page_idx": 5}, {"type": "text", "text": "During training, we set the fraction of synthetic samples that may be seen during an epoch to $2/3$ of the epoch. This prevents the model from overftiting to the remaining noise in the synthetic data, and is also common practice when training with synthetic data [Hsu et al., 2022, Lin et al., 2023]. Anecdotally, we did not notice an improvement from using a smaller proportion of synthetic structures. Finally, in the FOLDFLOW-2 architecture, we keep the ESM pre-trained language model fixed during training and train all other components (encoder, trunk, and decoder) from scratch. The results presented in table 3 and $\\S4.4$ use PDB data only, as this displayed the best performance for designability scores. ", "page_idx": 5}, {"type": "text", "text": "3.3 Fine-Tuning FOLDFLOW-2 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We explore the efficacy of fine-tuning FOLDFLOW-2 with preferential alignment. We take a supervised fine-tuning approach [Wei et al., 2022] that uses an additional fine-tuning dataset which is filtered using pre-specified auxiliary rewards $r_{\\mathrm{aux}}$ to create a preferential dataset $\\mathcal{D}_{\\mathrm{pref}}$ . We term this Reinforced FineTuning (ReFT) since fine-tuning in this manner can be considered aligning FOLDFLOW-2 generations to the auxiliary reward. Summarizing this in three steps: (1) We take a curated dataset of proteins with desirable metrics; (2) We use $r_{\\mathrm{aux}}$ to score the samples from step 1 and fliter them to get a subset of high-scoring samples; (3) We then improve FOLDFLOW-2 by SFT on the filtered subset. Finetuning with ReFT optimizes the following optimization objective ${\\mathcal{L}}_{\\mathrm{REFT}}(\\theta)$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{p_{\\theta}}\\mathcal{L}_{\\mathrm{REFT}}(\\theta)=\\mathbb{E}_{(x,a)\\sim\\mathcal{D}_{\\mathrm{pref}}}\\left[r_{\\mathrm{aux}}(x)\\log p_{\\theta}(x|a)\\right].\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Compared to recent alignment methods based on reward models, as in RLHF [Bai et al., 2022], ReFT uses a filtered structure dataset to fine-tune FOLDFLOW-2. Standard RL approaches seek to fine-tune generative model-based model-generated data and assume access to evaluating the reward function. Our approach maximizing ${\\mathcal{L}}_{\\mathrm{REFT}}(\\theta)$ requires constructing $\\mathcal{D}_{\\mathrm{pref}}$ with auxiliary reward $r_{\\mathrm{aux}}$ , demonstrated by the improvement in secondary structure diversity in $\\S4.2$ . ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We evaluate FOLDFLOW-2 on multiple protein design tasks including unconditional generation, motif scaffolding, folding, fine-tuning to improve secondary structure diversity, and equilibrium conformation sampling from molecular dynamics trajectories. We provide implementation details in $\\S B$ . ", "page_idx": 5}, {"type": "text", "text": "Baselines. As our main baselines for the unconditional generation task we rely on pre-trained versions of FrameDiff [Yim et al., 2023b], Chroma [Ingraham et al., 2023], Genie [Yeqing and Mohammed, 2023], MultiFlow [Campbell et al., 2024], and RFDiffusion which is the current gold standard [Watson et al., 2023]. In conditional generation tasks like motif scaffolding, we compare against a conditional variant of FrameFlow [Yim et al., 2023a] as well as RFDiffusion. For protein folding, we focus on comparing against ESMFold [Lin et al., 2022] and MultiFlow which also leverages sequence information. Lastly, for conformational sampling the principal baselines are ESMFlow and AlphaFlow [Jing et al., 2024]. ", "page_idx": 5}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/331b3ca246df5d24ff08e37901411b6b36ad83e09e5c311b73728eebdb8b771a.jpg", "img_caption": ["Figure 2: Uncurated designable $\\mathrm{(scRMSD<2\\mathring{A})}$ ) length 100 structures with ESMFold refolded structure from FOLDFLOW-2 and RFDiffusion colored by secondary structure assignment. FOLDFLOW-2 is significantly more diverse in terms of secondary structure composition where we see RFDiffusion generates mostly $\\alpha$ -helices. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.1 Unconditional protein backbone generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate unconditional structure generation using metrics that assess the designability, novelty, and diversity of generated structures. For each method, we generate 50 proteins at lengths $\\{100,150,2\\dot{0}0,25\\dot{0},300\\}$ (c.f. FOLDFLOW-2 samples in fig. 12). Designability is computed by using the self-consistency metric which compares the refolded proteins (with ProteinMPNN [Dauparas et al., 2022] and ESMFold [Lin et al., 2022]) with the original one. Novelty is computed using: 1.) the fraction of designable proteins with TM-score $<0.3$ and 2.) the average maximum TM-score of designable generated proteins to the training data. Finally, diversity uses the average pairwise TM-score designable samples averaged across lengths as well as the maximum number of clusters. ", "page_idx": 6}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/96848899db0ec71865fa21a86091aadf8babd14e572e1c2e90734e6b873aa175.jpg", "table_caption": ["Table 3: Comparison of Designability (fraction with $\\mathrm{scRMSD}<2.0\\mathring{\\mathrm{A}}\\!^{\\circ}$ ), Novelty (max. TM-score to PDB and fraction of proteins with averaged max. TM-score $<0.3$ and $\\mathrm{scRMSD}<2.0\\mathring{\\mathrm{A}}\\!\\!\\}$ ), and Diversity (avg. pairwise TM-score and MaxCluster fraction). Designability and Novelty metrics include standard errors. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Results. We see that FOLDFLOW-2 outperforms all other methods on all metrics\u2014crucially without requiring a pretrained folding model as part of the architecture like RFDiffusion. In particular, we observe that FOLDFLOW-2 produces the most designable samples with $97.6\\%$ of samples being refolded by ESMFold to within $<\\,2\\mathrm{\\mathring{A}}$ . We also find that FOLDFLOW-2 novelty improves over RFDiffusion by an absolute $25.2\\%$ in the fraction of designable samples with TM-score $<\\,0.3$ Furthermore, we observe $19.9\\%$ and $102.3\\%$ relative improvement in the diversity of FOLDFLOW-2 over RFDiffusion as measured by the pairwise TM-score and Max Cluster fraction. This places FOLDFLOW-2 as the current most designable, novel, and diverse protein structure generative model. ", "page_idx": 6}, {"type": "text", "text": "We present uncurated generated samples of FOLDFLOW-2 and RFDiffusion in fig. 2. We further visualize the distribution of secondary structures of all methods in fig. 3. We see a clear indication that FOLDFLOW-2 is able to produce the most diverse secondary structures\u2014more closely matching the training distribution (see fig. 3e)\u2014and improving over RFDiffusion. We further observe increased amounts of $\\beta$ -sheets and coils which are particularly challenging for models like FrameDiff and FOLDFLOW that primarily generate $\\alpha$ -helices. We also include multiple ablations on architectural choices, inference annealing, and sequence conditioning in table 14. ", "page_idx": 6}, {"type": "text", "text": "4.2 Increasing secondary structure diversity with finetuning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We investigate ReFT based data filtering to improve diversity of secondary structures in generated samples. We use a diversity score based auxiliary reward for filtering, based on weighted entropy on the proportions of each residue belonging to each type of secondary structure\u2014 i.e., alpha-helices $(\\alpha)$ , coils c, beta-sheets $\\beta$ in the set $\\boldsymbol{S}$ , that can be analytically written as $\\begin{array}{r}{r_{\\mathrm{diversity}}\\,=\\,\\left(\\sum_{s\\in\\mathcal{S}}p_{s}w_{s}\\right)}\\end{array}$ $\\begin{array}{r}{\\left(1+\\sum_{s\\in\\mathcal{S}}p_{s}\\log p_{s}\\right)}\\end{array}$ . Due to models producing increasing amounts of helices, we use $w_{\\alpha}=1$ , $w_{c}=0.5$ and $w_{\\beta}=2$ , and take top $25\\%$ of samples according to the rdiversity. Experimental results in fig. 3f with generated samples in fig. 12 demonstrate that protein at all lengths benefit from training with ReFT as measured by diversity of generated samples, and produces most amount of $\\beta$ -sheets, and can surpass diversity improvement already obtained by training using synthetic structures as in fig. 3. ", "page_idx": 6}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/7398cba14d0b6055e9d4d54f8a64cf331dd865c679549a185bd8557a74a2d643.jpg", "img_caption": ["Figure 3: Distribution of secondary structure elements ( $\\alpha$ -helices, $\\beta$ -sheets, and coils) of designable (scRMSD $<2.0)$ proteins generated by various models. FOLDFLOW-2 generates more diverse designable backbones. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.3 Folding sequences ", "text_level": 1, "page_idx": 7}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/8da9b7a9b509bd249ebee1722b0ed7283d4dd68101bd3071781bbe44361dae93.jpg", "table_caption": ["Table 4: Folding model evaluation on a test set of 268 proteins from PDB. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Given that FOLDFLOW-2 is sequence conditioned, we can perform protein folding by providing a valid sequence during inference. During training, FOLDFLOW-2 tries to transform a $\\mathrm{SE}(3)_{0}^{N}$ noise sample into the given sequence\u2019s 3D structure. Therefore, we aim to measure the generalization properties of our model to fold unseen sequences. We evaluate folding on a test set of 268 unseen proteins from the PDB dataset. We compare the folding capabilities of FOLDFLOW-2, ESMFold, and Multiflow. In table 4, we report the aligned RMSD between the predicted backbone and the ground truth backbone. We find that FOLDFLOW-2, trained for structure generation, approaches the performance of ESMFold which is a purpose-built folding model. We contextualize this result by noting that FOLDFLOW-2 $\\approx4\\times$ is better at folding than the most comparable model in MultiFlow [Campbell et al., 2024] which is a multi-modal flow matching model using sequences. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.4 Motif Scaffolding ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In motif scaffolding, we are tasked with designing a subset of residues, termed \u201cscaffolds\", around one or more subsections of a (\u201cmotif\") protein structure that have known biologically-important functions through its interaction with a target. This enables the design of proteins with a priori functional sites using generative models [Wang et al., 2021, Watson et al., 2023]. The motifs can be small and have non-specific shapes (e.g. a helix), and hence it is important for the generative model to understand the chemical information it carries on top of its geometry. We thus consider the task of motif scaffolding as an example of how our model can be fine-tuned for conditional generation tasks. We consider two datasets for evaluating motif scaffolding performance: the benchmark proposed in Watson et al. [2023] consisting of 24 single-chain motifs, and a new benchmark based on scaffolding the Complimentary Determining Regions (CDRs) of VHH nanobodies, as found in the Structural Antibody Database [Schneider et al., 2022]. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Motif Scaffolding Benchmark. We use the scaffolding benchmark from Watson et al. [2023] and follow the pseudo-label fine-tuning procedure described in Yim et al. [2023b] by randomly generating motifs from proteins by training on both the motif structure and sequence. For inference, we sample the scaffold lengths for each motif and provide both the both partially masked structure and sequence to the model. We follow the same evaluation procedure used in RFDiffusion (c.f. $\\S B.6$ for details). Our results in table 5 show that both FOLDFLOW-2 and RFDiffusion solve all $24/24$ motifs. ", "page_idx": 8}, {"type": "text", "text": "CDR Scaffolding. VHH antibodies, also known as nanobodies, have shown significant promise in protein design and therapeutics due to their unique properties [Muyldermans, 2021]. They are composed of a single variable domain derived from camelid heavy-chain antibodies, featuring three complementarity-determining regions (CDRs) that confer specificity and variability in antigen binding. As a result, creating effective scaffolds for nanobodies is challenging due to the need to maintain the designability of the CDRs and especially because any scaffolding effort must avoid altering these characteristics to preserve binding functionality. We treat this as a conditional generative modeling problem and fix the motif atoms, and mask the scaffold sequence information. Exact training and experimental details along with additional metrics are provided in $\\S B.6$ . Our results are found in table 5, where the average motif scRMSD is much higher than the average scaffold scRMSD. The result is a much lower number of solved motif scaffolding. ", "page_idx": 8}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/eeae4e19559f7288d04ba67f4e9a0f895bb79540e5331cc1207d6a2ccd5dffa8.jpg", "table_caption": ["Table 5: Motif-scaffolding benchmarks. FrameFlow does not have public code for motif-scaffolding and thus cannot be evaluated on the VHH benchmark. $\\mathrm{^{\\bullet}+F T^{\\bullet}}$ indicates \u201cwith fine-tuning\u201d. \\*Using reported numbers with AlphaFold2 instead of ESMFold used in our evaluation procedure; c.f. $\\S B.6$ for further discussion. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.5 Zero-shot Equilibrium Conformation Sampling ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now test FOLDFLOW-2 on zero-shot equilibrium conformation sampling task. Starting from a sequence, we generate multiple conformations of the same proteins and compare the distribution of conformations with the ones from molecular dynamic simulations. We compare FOLDFLOW-2 with AlphaFlow-MD and ESMFlow-MD; two folding models fine-tuned on a molecular dynamic dataset, and non finetuned models Eigenfold [Jing et al.] and STR2STR [Lu et al., 2024]. In table 6, we report the pairwise and global RMSD, the root mean square fluctuation (RMSF), and the 2-Wasserstein on the top two principal components. For both RMSD and the RMSF metrics, we report the Pearson correlation ", "page_idx": 8}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/6eca18052bccaa68e330c7889ca7862b34a12fe6c58c19cb431691d602a6723e.jpg", "img_caption": ["Figure 4: Protein conformation ensembles from the ATLAS dataset, ESMFlow-MD and FOLDFLOW-2. Proteins are colored by their secondary structure with $\\alpha$ -helices in blue, $\\beta$ -sheets in red, and coils in green. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "between the values from the generated ensemble and those of the ground truth ensemble (the procedure is detailed in full in $\\S B.7\\rangle$ ). ", "page_idx": 8}, {"type": "text", "text": "We use the same test set as in Jing et al. [2024] restricted to proteins of length at most 400 amino acids. Notably FOLDFLOW-2 performs similarly or better than the comparable model ESMFlow-MD across all metrics without any fine-tuning and with significantly fewer parameters on molecular dynamics data, indicating that the base model trained only on PDB already captures similar information about protein dynamics as models given explicit access to this data. Moreover, we observed that FOLDFLOW2 requires $4.5\\times$ less GPU hours for training and $33\\times$ less trainable parameters while allowing for $6\\times$ faster inference steps than ESMFlow-MD as reported in table 13, improving FOLDFLOW-2\u2019s prospects as a practical base model for future work on capturing protein dynamics. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Table 6: Zero-shot performance of the base FOLDFLOW-2 model on the ATLAS dataset of MD trajectories compared to ESMFlow and AlphaFlow models fine-tuned on ATLAS. FOLDFLOW-2 is competitive to the comparable model ESMFlow across all metrics. $r$ denotes Pearson\u2019s correlation coefficient. Time is per sample (Time / sample (s)) on length 300 protein 7c45_A. Eigenfold only models $\\mathbf{C}\\alpha$ atom so we compare on PCA $\\mathcal{W}_{2}$ -dist on the $\\mathbf{C}\\alpha$ atom only. ", "page_idx": 9}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/aa46312136416ab238651e73935c581d2dfe5933f83783d04c644aeca5f5373d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Protein design. Physics-based protein structure design yielded the first de novo proteins [Huang et al., 2016]. For example, structure-based biophysics approaches have previously resulted in several drug candidates [R\u00f6thlisberger et al., 2008, Fleishman et al., 2011, Cao et al., 2020].This was followed by language models [Hie et al., 2022, Ferruz et al., 2022] and geometric deep learning [Gainza et al., 2020] for protein structure design. Recently, diffusion [Wu et al., 2024, Yim et al., 2023b, Watson et al., 2023, Ingraham et al., 2023, Wang et al., 2024, Frey et al., 2024] and flow-based models [Bose et al., 2024, Yim et al., 2023a, Jing et al., 2024] have risen to prominence. These methods employ a backbone-first approach with the notable exception of MultiFlow [Campbell et al., 2024] which uses sequence to perform co-generation. ", "page_idx": 9}, {"type": "text", "text": "RLHF and Supervised Fine-Tuning (SFT). Aligning the outputs of language models with RLHF has recently gained interest [Ouyang et al., 2022, Stiennon et al., 2020, Bai et al., 2022]. These methods learn a reward model for post-training alignment to desired behavior [Mishra et al., 2022], which can prove challenging for protein design [Zhou et al., 2024]. SFT on hand-crafted data has proven to be effective in enhancing performance but requires high-quality data [Rozi\u00e8re et al., 2023, Yuan et al., 2023]. Filtering real data using auxiliary rewards serve as a substitute for steering the desired properties of the generated samples. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce a new sequence-conditioned protein structure generative model called FOLDFLOW-2. FOLDFLOW-2 leverages a protein language model to condition Flow Matchingbased protein generative models with sequences. Our model achieves state-of-the-art results on unconditional generation and generates diverse and novel proteins, especially when trained on our new dataset. Conditioning over sequences allows our model to perform novel tasks such as folding sequences and motif-scaffolding tasks and we show its competitiveness on those tasks. Regarding the limitations of our model, we note that it requires a competitive pre-trained language model to be sequence conditioned, which can be hard to acquire. We also note that ProteinMPNN, used in our evaluation pipeline, has been trained only on PDBs. Therefore, it is possible that our models trained on our new dataset generates designable proteins which are not correctly processed by ProteinMPNN. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank Alexandre Stein, Maksym Korablyov, and the entire DreamFold team for providing a vibrant workspace that enabled this research. The authors would like to acknowledge Anyscale, Amazon AWS, and Google GCP for providing computational resources for the protein experiments. ", "page_idx": 9}, {"type": "text", "text": "Contribution statement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Architecture design was led by G.H. Infrastructure development was led by J.V. The experiments were divided as follows: Unconditional (K.F., A.T.), diversity (E.T.L., R.I., C.L.), folding (G.H.), motif-scaffolding (G.H., J.V., E.T.L, C.L.), and equilibrium conformation (J.R.B, G.H., T.A.S.). Dataset preparation including synthetic structure filtering (J.V., P.L., K.F.). A.J.B. drove the writing of the paper with contributions from all other authors. A.J.B. and A.T. cosupervised the project with guidance from M.B. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "M. S. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic interpolants. International Conference on Learning Representations (ICLR), 2023. (Cited on page 3) ", "page_idx": 10}, {"type": "text", "text": "J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. Advances in neural information processing systems, 2016. (Cited on page 5) Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. E. Showk, N. Elhage, Z. HatfieldDodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv, 2022. (Cited on pages 6 and 10) H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N. Shindyalov, and P. E. Bourne. The protein data bank. Nucleic Acids Research, 28(1):235\u2013242, Jan. 2000. (Cited on page 17) A. J. Bose, T. Akhound-Sadegh, K. Fatras, G. Huguet, J. Rector-Brooks, C.-H. Liu, A. C. Nica, M. Korablyov, M. Bronstein, and A. Tong. Se(3)-stochastic flow matching for protein backbone generation. In International Conference on Learning Representations (ICLR), 2024. (Cited on pages 2, 3, 4, 6, 10, 17, 19, 20, and 21) A. Campbell, J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. International Conference on Machine Learning (ICML), 2024. (Cited on pages 2, 4, 6, 8, and 10) L. Cao, I. Goreshnik, B. Coventry, J. B. Case, L. Miller, L. Kozodoy, R. E. Chen, L. Carter, A. C. Walls, Y.-J. Park, et al. De novo design of picomolar sars-cov-2 miniprotein inhibitors. Science,   \n370(6515):426\u2013431, 2020. (Cited on pages 1 and 10) S. R. Carter, S. Curtis, C. Emerson, J. Gray, I. C. Haydon, A. Hebbeler, C. Qureshi, N. Randolph, A. Rives, and L. Stuart. Community values, guiding principles, and commitments for the responsible development of ai for protein design, 2024. (Cited on page 16) R. T. Q. Chen and Y. Lipman. Flow matching on general geometries. In The Twelfth International Conference on Learning Representations, 2024. (Cited on page 3) A. Chevalier, D.-A. Silva, G. J. Rocklin, D. R. Hicks, R. Vergara, P. Murapa, S. M. Bernard, L. Zhang, K.-H. Lam, G. Yao, et al. Massively parallel de novo protein design for targeted therapeutics. Nature, 550(7674):74\u201379, 2017. (Cited on page 1) J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan, B. Koepnick, H. Nguyen, A. Kang, B. Sankaran, A. K. Bera, N. P. King, and D. Baker. Robust deep learning\u2013based protein sequence design using ProteinMPNN. Science, 378(6615):49\u201356,   \n2022. (Cited on pages 7, 20, and 21) S. B. Ebrahimi and D. Samanta. Engineering protein-based therapeutics through structural and chemical design. Nature Communications, 14(1):2411, 2023. (Cited on page 1)   \nK. Fatras, Y. Zine, R. Flamary, R. Gribonval, and N. Courty. Learning with minibatch wasserstein : asymptotic and gradient properties. International Conference on Artificial Intelligence and Statistics (AISTATS), 2020. (Cited on pages 4 and 17)   \nK. Fatras, Y. Zine, S. Majewski, R. Flamary, R. Gribonval, and N. Courty. Minibatch optimal transport distances; analysis and applications. arXiv, 2021. (Cited on page 17)   \nN. Ferruz, S. Schmidt, and B. H\u00f6cker. Protgpt2 is a deep unsupervised language model for protein design. Nature communications, 13(1):4348, 2022. (Cited on page 10)   \nS. J. Fleishman, T. A. Whitehead, D. C. Ekiert, C. Dreyfus, J. E. Corn, E.-M. Strauch, I. A. Wilson, and D. Baker. Computational design of proteins targeting the conserved stem region of influenza hemagglutinin. Science, 332(6031):816\u2013821, 2011. (Cited on page 10)   \nN. C. Frey, D. Berenberg, K. Zadorozhny, J. Kleinhenz, J. Lafrance-Vanasse, I. Hotzel, Y. Wu, S. Ra, R. Bonneau, K. Cho, et al. Protein discovery with discrete walk-jump sampling. International Conference on Learning Representations (ICLR), 2024. (Cited on page 10)   \nP. Gainza, F. Sverrisson, F. Monti, E. Rodola, D. Boscaini, M. M. Bronstein, and B. E. Correia. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Nature Methods, 17(2):184\u2013192, 2020. (Cited on page 10)   \nP. Gainza, S. Wehrle, A. Van Hall-Beauvais, A. Marchand, A. Scheck, Z. Harteveld, S. Buckley, D. Ni, S. Tan, F. Sverrisson, et al. De novo design of protein interactions with learned surface fingerprints. Nature, pages 1\u20139, 2023. (Cited on page 1)   \nB. C. Hall. Lie groups, Lie algebras, and representations. Springer, 2013. (Cited on page 16)   \nA. Herbert and M. Sternberg. Maxcluster: a tool for protein structure comparison and clustering, 2008. (Cited on page 21)   \nB. Hie, S. Candido, Z. Lin, O. Kabeli, R. Rao, N. Smetanin, T. Sercu, and A. Rives. A high-level programming language for generative protein design. bioRxiv, pages 2022\u201312, 2022. (Cited on page 10)   \nC. Hsu, R. Verkuil, J. Liu, Z. Lin, B. Hie, T. Sercu, A. Lerer, and A. Rives. Learning inverse folding from millions of predicted structures. International conference on machine learning, 2022. (Cited on page 6)   \nP.-S. Huang, S. E. Boyken, and D. Baker. The coming of age of de novo protein design. Nature, 537 (7620):320\u2013327, 2016. (Cited on page 10)   \nJ. B. Ingraham, M. Baranov, Z. Costello, K. W. Barber, W. Wang, A. Ismail, V. Frappier, D. M. Lord, C. Ng-Thow-Hing, E. R. Van Vlack, et al. Illuminating protein space with a programmable generative model. Nature, 623(7989):1070\u20131078, 2023. (Cited on pages 4, 6, 10, and 21)   \nB. Jing, E. Erives, P. Pao-Huang, G. Corso, B. Berger, and T. S. Jaakkola. Eigenfold: Generative protein structure prediction with diffusion models. In ICLR 2023-Machine Learning for Drug Discovery workshop. (Cited on page 9)   \nB. Jing, B. Berger, and T. Jaakkola. Alphafold meets flow matching for generating protein ensembles. International Conference on Machine Learning (ICML), 2024. (Cited on pages 2, 7, 9, 10, 23, and 24)   \nJ. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. \u017d\u00eddek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021. (Cited on pages 2, 3, 4, 5, 6, 18, and 19)   \nZ. Kadkhodaie, F. Guth, E. P. Simoncelli, and S. Mallat. Generalization in diffusion models arises from geometry-adaptive harmonic representation. In International Conference on Learning Representations (ICLR), 2024. (Cited on page 5)   \nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. (Cited on page 19)   \nZ. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022. (Cited on pages 2, 3, 5, 6, 7, 19, and 20)   \nZ. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123\u20131130, 2023. (Cited on pages 5, 6, and 21)   \nY. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. In International Conference on Learning Representations (ICLR), 2023. (Cited on page 3)   \nX. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. International Conference on Learning Representations (ICLR), 2023. (Cited on page 3)   \nJ. Lu, B. Zhong, Z. Zhang, and J. Tang. Str2str: A score-based framework for zero-shot protein conformation sampling. International Conference on Learning Representations (ICLR), 2024. (Cited on page 9)   \nS. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3470\u20133487. Association for Computational Linguistics, 2022. (Cited on page 10)   \nL. S. Mitchell and L. J. Colwell. Comparative analysis of nanobody sequence and structure data. Proteins: Structure, Function, and Bioinformatics, 86(7):697\u2013706, 2018. (Cited on page 23)   \nS. Muyldermans. Applications of nanobodies. Annual review of animal biosciences, 9:401\u2013421, 2021. (Cited on page 9)   \nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS, 2022. (Cited on page 10)   \nF. C. Park and R. W. Brockett. Kinematic dexterity of robotic mechanisms. The International Journal of Robotics Research, 13(1):1\u201315, 1994. (Cited on page 17)   \nA.-A. Pooladian, H. Ben-Hamu, C. Domingo-Enrich, B. Amos, Y. Lipman, and R. T. Chen. Multisample flow matching: Straightening flows with minibatch couplings. International Conference on Learning Representations (ICLR), 2023. (Cited on page 27)   \nD. R\u00f6thlisberger, O. Khersonsky, A. M. Wollacott, L. Jiang, J. DeChancie, J. Betker, J. L. Gallaher, E. A. Althoff, A. Zanghellini, O. Dym, et al. Kemp elimination catalysts by computational enzyme design. Nature, 453(7192):190\u2013195, 2008. (Cited on page 10)   \nB. Rozi\u00e8re, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong, A. D\u00e9fossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. (Cited on page 10)   \nC. Schneider, M. I. Raybould, and C. M. Deane. Sabdab in the age of biotherapeutics: updates including sabdab-nano, the nanobody structure tracker. Nucleic acids research, 50(D1):D1368\u2013 D1372, 2022. (Cited on page 9)   \nD.-A. Silva, S. Yu, U. Y. Ulge, J. B. Spangler, K. M. Jude, C. Lab\u00e3o-Almeida, L. R. Ali, A. QuijanoRubio, M. Ruterbusch, I. Leung, et al. De novo design of potent and selective mimics of il-2 and il-15. Nature, 565(7738):186\u2013191, 2019. (Cited on page 1)   \nN. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS, 2020. (Cited on page 10)   \nE.-M. Strauch, S. M. Bernard, D. La, A. J. Bohn, P. S. Lee, C. E. Anderson, T. Nieusma, C. A. Holstein, N. K. Garcia, K. A. Hooper, et al. Computational design of trimeric influenza-neutralizing proteins targeting the hemagglutinin receptor binding site. Nature biotechnology, 35(7):667\u2013671, 2017. (Cited on page 1)   \nA. Tong, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, K. Fatras, G. Wolf, and Y. Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint 2302.00482, 2023. (Cited on pages 4 and 27)   \nY. Vander Meersche, G. Cretin, A. Gheeraert, J.-C. Gelly, and T. Galochkina. ATLAS: protein flexibility description from atomistic molecular dynamics simulations. Nucleic Acids Research, 52 (D1):D384\u2013D392, 11 2023. (Cited on page 23)   \nY. Vander Meersche, G. Cretin, A. Gheeraert, J.-C. Gelly, and T. Galochkina. Atlas: protein flexibility description from atomistic molecular dynamics simulations. Nucleic Acids Research, 52(D1): D384\u2013D392, 2024. (Cited on page 2)   \nM. Varadi, S. Anyango, M. Deshpande, S. Nair, C. Natassia, G. Yordanova, D. Yuan, O. Stroe, G. Wood, A. Laydon, A. \u017d\u00eddek, T. Green, K. Tunyasuvunakool, S. Petersen, J. Jumper, E. Clancy, R. Green, A. Vora, M. Lutf,i M. Figurnov, A. Cowie, N. Hobbs, P. Kohli, G. Kleywegt, E. Birney, D. Hassabis, and S. Velankar. AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. Nucleic Acids Research, 50(D1):D439\u2013D444, 11 2021. (Cited on pages 2 and 6)   \nC. Wang, Y. Qu, Z. Peng, Y. Wang, H. Zhu, D. Chen, and L. Cao. Proteus: exploring protein structure generation for enhanced designability and efficiency. International Conference on Machine Learning (ICML), 2024. (Cited on page 10)   \nJ. Wang, S. Lisanza, D. Juergens, D. Tischer, I. Anishchenko, M. Baek, J. L. Watson, J. H. Chun, L. F. Milles, J. Dauparas, et al. Deep learning methods for designing proteins scaffolding functional sites. BioRxiv, 2021. (Cited on page 8)   \nJ. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, N. Hanikel, S. J. Pellock, A. Courbet, W. Sheffler, J. Wang, P. Venkatesh, I. Sappington, S. V. Torres, A. Lauko, V. De Bortoli, E. Mathieu, S. Ovchinnikov, R. Barzilay, T. S. Jaakkola, F. DiMaio, M. Baek, and D. Baker. De novo design of protein structure and function with RFdiffusion. Nature, 620(7976):1089\u20131100, 2023. (Cited on pages 2, 4, 6, 8, 9, 10, 19, 20, 21, 22, and 23)   \nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. International Conference on Learning Representations (ICLR), 2022. (Cited on page 6)   \nK. E. Wu, K. K. Yang, R. van den Berg, S. Alamdari, J. Y. Zou, A. X. Lu, and A. P. Amini. Protein structure generation via folding diffusion. Nature Communications, 15(1):1059, 2024. (Cited on page 10)   \nL. Yeqing and A. Mohammed. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds. International Conference on Machine Learning (ICML), 2023. (Cited on pages 6, 20, and 21)   \nJ. Yim, A. Campbell, A. Y. Foong, M. Gastegger, J. Jim\u00e9nez-Luna, S. Lewis, V. G. Satorras, B. S. Veeling, R. Barzilay, T. Jaakkola, et al. Fast protein backbone generation with se (3) flow matching. arXiv preprint arXiv:2310.05297, 2023a. (Cited on pages 4, 6, 10, and 19)   \nJ. Yim, B. L. Trippe, V. De Bortoli, E. Mathieu, A. Doucet, R. Barzilay, and T. Jaakkola. Se (3) diffusion model with application to protein backbone generation. In Proceedings of the 40th International Conference on Machine Learning, pages 40001\u201340039, 2023b. (Cited on pages 2, 3, 4, 6, 9, 10, 19, and 21)   \nJ. Yim, A. Campbell, E. Mathieu, A. Y. Foong, M. Gastegger, J. Jim\u00e9nez-Luna, S. Lewis, V. G. Satorras, B. S. Veeling, F. No\u00e9, et al. Improved motif-scaffolding with se (3) flow matching. Transactions on Machine Learning Research, 2024. (Cited on page 21)   \nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv, 2023. (Cited on page 10)   \nY. Zhang, I. A. Hubner, A. K. Arakaki, E. Shakhnovich, and J. Skolnick. On the origin and highly likely completeness of single-domain protein structures. Proceedings of the National Academy of Sciences, 103(8):2605\u20132610, 2006. (Cited on page 21)   \nX. Zhou, D. Xue, R. Chen, Z. Zheng, L. Wang, and Q. Gu. Antigen-specific antibody design via direct energy-based preference optimization. arXiv, 2024. (Cited on page 10) ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Broader impact ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The development of generative AI for protein backbone design holds significant promise for the fields of biotechnology and medicine. By enabling the precise engineering of protein structures, these models can accelerate the discovery of novel therapeutics and vaccines, potentially leading to more effective treatments for a wide range of diseases. Multi-modal and conditional generative models in the space of biotechnology are seeing rapid improvements, from which we have yet to see the full impact on modern and future medicine. We also acknowledge the potential for dual use of our results but, as discussed in Carter et al. [2024], the benefits of public research into computational drug discovery outweigh the potential drawbacks. ", "page_idx": 15}, {"type": "text", "text": "A A short review of Riemannian geometry, Lie groups, and optimal transport ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Riemannian manifolds ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Informally, a topological manifold, $\\mathcal{M}$ is a topological space that is locally Euclidean (i.e. homeomorphic to a Euclidean space). The manifold is said to be smooth or differentiable if it additionally is $C^{p}$ differential for all $p$ . An important notion is the tangent space, $\\mathcal{T}_{x}\\mathcal{M}$ , which is attached to every point on the manifold, $x\\in{\\mathcal{M}}$ . The disjoint union of all the tangent spaces is called a tangent bundle, $\\mathfrak{X}(\\mathcal{M})$ . If in addition, the manifold is equipped with a Riemannian metric, $g_{x}$ , it is said to be a Riemannian manifold. The notion of a Riemannian metric is used to define inner products on the tangent space at each point of the manifold. This means that for $\\mathfrak{x}_{1},\\mathfrak{x}_{2}\\in\\mathcal{T}_{x}\\mathcal{M}$ , $g_{x}(\\mathfrak{x}_{1},\\mathfrak{x}_{2}):=\\langle\\mathfrak{x}_{1},\\mathfrak{x}_{2}\\rangle$ . Similar to how inner products can be used to define key geometric properties, such as length and distance, the Riemannian metric allows us to define such notions on an arbitrary Riemannian manifold: the length of ${\\mathfrak{x}}$ , a vector in the tangent space of the manifold is defined by $|{\\mathfrak{x}}|=\\langle{\\mathfrak{x}},{\\mathfrak{x}}\\rangle^{1/2}$ . Finally, an important property on Riemannian manifolds is geodesics, which generalizes the notion of shortest paths in Euclidean spaces. While in Euclidean space the shortest path between two points is the length of a straight line between them, on a manifold, the idea is to find the shortest smooth curve between two points, which is called a geodesic. ", "page_idx": 15}, {"type": "text", "text": "A.2 Lie groups ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Symmetries refer to transformations of an object that preserve a certain structure. A set of continuous symmetries, paired with a composition operation satisfying group axioms is a Lie group $(G,\\circ)$ . More precisely, a group is a set paired with a group operation, $\\circ:G\\times G\\to G$ , which is associative, has an identity element and there is an inverse element for every element of the set. In addition to this group structure, a Lie group is also a smooth manifold, where the group operations of multiplication, $\\bar{(x,y)}\\to x y$ for $x,y\\in G$ , and inversion, $x\\to x^{-1}$ , are smooth maps. ", "page_idx": 15}, {"type": "text", "text": "Given $y\\,\\in\\,G$ , we can define a diffeomorphism, $L_{y}\\;:\\;G\\;\\to\\;G$ defined by $x\\mapsto y x$ , known as left multiplication. Given a vector field $X$ on the group, we say that it is left invariant, if, under this left multiplication, it is left-invariant, meaning that $L_{y}^{*}X=X$ , $\\forall y\\in G$ . Note that $L_{y}^{*}$ is the differential of left action, naturally identifying the tangent spaces, $\\tau_{y}\\to\\tau_{y x}$ . Therefore, given the group multiplication, we can uniquely define a left-invariant vector field with its values on the tangent space at the identity element of the group, $\\tau_{e}$ . We can additionally equip this vector space, $V$ , with a bilinear operation known as the Lie bracket, $[\\cdot,\\cdot]:V\\times V\\rightarrow V$ , that satisfies the Jacobi identity and is anticommutative. Such a vector space is called a Lie algebra. The tangent space of Lie groups form Lie algebras and are denoted with $\\mathfrak{G}$ . The elements of the Lie algebra can be mapped into group elements using an invertible map called the exponential map, $\\exp:\\mathfrak{G}\\to G$ . The inverse is called the logarithmic map allowing us to go from the group elements to their corresponding elements in the Lie algebra, $\\log:G\\to\\mathfrak{G}$ . ", "page_idx": 15}, {"type": "text", "text": "Finally, we note that the set of $n\\times n$ non-singular matrices forms a Lie group. The group operation is matrix multiplication and it can be seen as a smooth manifold that is an open subset of $\\mathbb{R}^{2\\bar{n}}$ . This group is known as the General Linear Group, $G L(n)$ . Any closed subgroup of $G L(n)$ is known as a matrix Lie group, which are perhaps some of the most important Lie groups that are studied. In the case of these matrix Lie groups, the exponential and logarithmic maps also coincide with the matrix exponential and logarithm. For a more detailed overview of this subject, we refer the reader to Hall [2013]. ", "page_idx": 15}, {"type": "text", "text": "A.3 The Special Euclidean Group in 3 Dimensions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "One of the closed subgroups of $G L(n)$ that has been studied extensively in various fields is the $3D$ Special Orthogonal group, SO(3). The elements of this group are $3\\times3$ rotation matrices, namely ${\\dot{S O}}(n)=\\{r\\ {\\bar{\\in}}\\ G L{\\bar{(n)}}:{\\dot{r}}^{T}r={\\dot{I}},d e t(r)=1\\}.$ . Additionally, the translations of an object in $3D$ space by a translation vector $s$ can also be seen as a matrix Lie group by considering the translation matrix, $\\binom{I}{0}\\binom{s}{1}$ , where $I$ is an $3\\times3$ identity matrix. With the group operation being translations, this group is also a matrix group, denoted as $(\\mathbb{R}^{3},+)$ . ", "page_idx": 16}, {"type": "text", "text": "Combining these, we can represent the rigid transformations of objects in $3D$ space with a group that encompasses both rotations and translations. This group is known as the Special Euclidean group in $3D$ and is the semidirect product of the rotation and translation groups, $\\mathrm{SE}(3)\\cong\\mathrm{SO}(3)\\ltimes\\bar{(}\\mathbb{R}^{3},+)$ . This group is also a matrix group and its elements can be written as $\\mathrm{SE}(3)=\\left\\{(r,s)=\\binom{r}{0}\\right.\\right.$ $r\\in\\mathrm{SO}(3),s\\in(\\mathbb{R}^{3},+)\\}$ . Finally, we note that given a suitable choice of metric [Park and Brockett, 1994] the inner product on SE(3) decomposes naturally into inner products on $\\mathrm{SO}(3)$ and $(\\mathbb{R}^{3},+)$ , i.e. $\\langle\\mathfrak{x}_{1},\\mathfrak{x}_{2}\\rangle_{\\mathrm{SE}(3)}=\\langle\\mathfrak{x}_{1},\\mathfrak{r}_{2}\\rangle_{\\mathrm{SO}(3)}+\\langle\\mathfrak{s}_{1},\\mathfrak{s}_{2}\\rangle_{(\\mathbb{R}^{3},+)}$ , where we have denoted the elements of tangent spaces of $\\mathrm{SO}(3)$ and $\\mathbb{R}^{3}$ with $\\mathfrak{x}$ and r respectively and have omitted to do so for the translation group as the tangent space coincides with the space itself. ", "page_idx": 16}, {"type": "text", "text": "A.4 Optimal Transport ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Optimal transport (OT) focuses on finding the most efficient way to move mass or resources from one distribution to another. It involves minimizing a cost function $c(x,y)$ that quantifies the expense of transporting mass from point $x$ in one space to point $y$ in another space. This problem is framed as an optimization problem, often using the Kantorovich formulation: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\int_{X\\times Y}c(x,y)\\,d\\gamma(x,y)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\Pi(\\mu,\\nu)$ is the set of all joint distributions $\\gamma$ with marginals $\\mu$ and $\\nu$ . The goal is to find a transportation plan $\\gamma$ that minimizes the total cost. Optimal transport has significant applications in probability theory, where it is used to compare probability distributions. One common measure is the Wasserstein distance, defined as: ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{p}(\\mu,\\nu)=\\left(\\operatorname*{inf}_{\\gamma\\in\\Pi(\\mu,\\nu)}\\int_{X\\times Y}d(x,y)^{p}\\,d\\gamma(x,y)\\right)^{1/p}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $d(x,y)$ represents the distance between points $x$ and $y$ and $p\\geq1$ is a parameter that defines the type of Wasserstein distance. ", "page_idx": 16}, {"type": "text", "text": "In machine learning, optimal transport enhances algorithms in tasks such as domain adaptation, where it helps align different data distributions, and in generative modeling, where it aids in generating data samples that match a given distribution. In the case of FOLDFLOW-2 and as used in [Bose et al., 2024], OT is used to sample tuples of source noisy sample and target protein $(x_{0},x_{1})$ to perform Flow Matching. It takes the form of using the OT plan for the joint distribution $q(x_{0},x_{1})$ between minibatches $(X_{0},X_{1})$ . The OT variant performs here corresponds to what is called minibatch OT as studied in [Fatras et al., 2021, 2020]. ", "page_idx": 16}, {"type": "text", "text": "B Experimental Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Dataset filtering ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1.1 PDB Structures ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use a subset of PDB with resolution $<5\\mathrm{\\mathring{A}}$ downloaded from the PDB [Berman et al., 2000] on July 20, 2023. We performed standard filtering to remove any proteins with $>50\\%$ loops. During preprocessing, we also removed any non-organic residues at either end of the structure. In previous works, these residues are typically kept but masked during training, however they contributed to the total forward pass FLOPs and therefore decrease training efficiency. By removing these residues during preprocessing, we are able to increase the number of training examples per batch. Finally, we re-clustered the PDB dataset using mmseqs2 at the $50\\%$ sequence identity threshold to obtain 6,593 clusters during training. The PDB is made available under a CC0 1.0 Universal (CC0 1.0) Public Domain Dedication. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "B.1.2 Synthetic Structures ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide more detail on the filtering procedure used in our curation of synthetic data for training. We began with a SwissProt data dump consistent of 532,003 structures predicted by AlphaFold2 [Jumper et al., 2021] accessed in February 2024. The AlphaFold2 predicted structure database is made available under a CC-BY-4.0 license for academic and commercial uses. ", "page_idx": 17}, {"type": "text", "text": "Global pLDDT Filtering. We first filtered out globally low-confident structures, as measured by average and standard deviation of pLDDT taken across all residues in a predicted structure. Despite SwissProt already being a curated set of high-confidence structures, we still found considerable variation in quality. See fig. 5a for an empirical analysis average pLDDT for a random sample of 500 proteins from SwissProt. Our final filtering criteria were (avg pLDDT $>~85$ ) && (std pLDDT $<$ 15) to keep only \u201cconsistently good\u201d structures, see fig. 5b for a graphical representation. ", "page_idx": 17}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/262b2c6c162dd503d7e27a444b8f3f832d8c25c3ba7320cac01f7ff6e3c98459.jpg", "img_caption": ["Figure 5: Analysis of global pLDDT distribution on a sample of 500 proteins from SwissProt. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "High-Confidence Low-Quality Filtering. Despite the global pLDDT filtering, there were still low-quality structures in the training after global pLDDT flitering. Some examples of these structures can be seen in fig. 6. They are characterized by having high overall confidence and good local qualities but unrealistic global structures or sub-chain interactions. Our finding is that these structures easily corrupt the training data and cause a model to produce similarly \u201cunfolded\u201d generations. ", "page_idx": 17}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/06d926981814b6599faa13b9284e50b6a4da56a7d6bda889d3f62b7e7791bd30.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: Examples of high-confidence, low-quality structures that were filtered out of the training set. Images are from the AlphaFold Protein Structure Database https://alphafold.ebi.ac.uk/ accessed in May 2024; identifiers are the UniProt IDs of each example. ", "page_idx": 17}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/f5aa719cf245ab9fb9668386b92fee621bc38c8c08ad0b6372ab8f1778a7258b.jpg", "table_caption": ["Table 7: Overview of Training Setup "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/15f436ffdf0482db65c74f8695777f359f30e36cb57e29cb458c0590b014ab73.jpg", "table_caption": ["Table 8: An overview of training time. \\*RFDiffusion initializes from RoseTTAFold, and we include that training time in the estimates. \\*\\*We recall that FOLDFLOW-2 uses frozen ESM2-650M which was trained on 512 GPUs for 8 days. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.2 Model architecture details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide details for the IPA blocks and Folding blocks. For the IPA blocks, we follow the setting developed in [Yim et al., 2023b] by adding to the original IPA [Jumper et al., 2021] a skip connection and transformer layer on the node representation. The IPA modules takes as inputs the single and pair representations and a structure. For the structure encoder, we initialize the single and pair representations with positional and time embedding passed to MLPs. In our experiment, we used a node embedding dimension of 256, an edge dimension of 128, and a hidden dimension of 256. These settings are the same for both the encoder and decoder. We have use a skip connection between the representations of the structure encoder and decoder. ", "page_idx": 18}, {"type": "text", "text": "We combine the single and pair representations of different modalities by projecting each with a linear layer of output dimensions 128 for the single representations and 64 for the pair representation. We then concatenate all modalities\u2019 representation to obtain a single representation of dimension 128 and pair representation of dimension 256. ", "page_idx": 18}, {"type": "text", "text": "The Folding blocks are taken from Lin et al. [2022]. They are composed of 2 Triangular SelfAttention Blocks with single and pair head width of size 32. Finally, the pair and single representation dimensions are of 128. ", "page_idx": 18}, {"type": "text", "text": "The structure decoder\u2019s single and pair representations inputs are an average between the refined ones from the Folding blocks and the initial ones. All other architectural details not specified here are set to the defaults of IPA or ESM, respectively. ", "page_idx": 18}, {"type": "text", "text": "B.3 Training Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Hyperparameters. See table 7 for an overview of the experimental setup. We train with the \u201clength batching\u201d scheme described in Yim et al. [2023a] in which each batch consists of the same protein sampled at different times. The number of samples in a batch is variable and is approximately $\\lceil\\mathrm{num\\_residues}^{2}/M\\rceil$ where $M$ is a hyperparameter in table 7. Other training details such as the loss computation are the same as Bose et al. [2024]. ", "page_idx": 18}, {"type": "text", "text": "Training hardware setup. FOLDFLOW-2 is coded in PyTorch and was trained on 2 A100 40GB NVIDIA GPUs for 4 days. Initial tests runs were trained in a similar setting. ", "page_idx": 18}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/281724a3318ae442682b53ad547073c02276400ac1361dbb23084a31d8324a18.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 7: Schematic of designability calculation. First a generative flow model is used to generate a protein backbone from an initial structure (possibly noise) and (optionally) a protein sequence. This is then fed to an inverse folding model (ProteinMPNN Dauparas et al. [2022]) eight times to generate eight sequences for the structure. Then all eight sequences are fed back to ESMFold to produce a structure for each sequence. All eight structures are compared using scRMSD from the ESMFold \u201crefolded\u201d structure to the generated structure, and the minimum is taken as the \u201cdesignability\u201d of the generated structure with at least one structure with error $<2.0\\mathring\\mathrm{A}$ being classified as designable following Watson et al. [2023]. ", "page_idx": 19}, {"type": "text", "text": "B.4 Inference details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The inference is performed with Euler integration steps. In our experiments, we found that 50 steps gave state-of-the-art results. We use the Inference Annealing trick from Bose et al. [2024] multiplying the rotation vector by some time dependent scaling function $i(t)$ where $t\\in[0,1]$ . We tried different scaling parameters and as found in [Bose et al., 2024], $10t$ provided the best performance. In practice this means greatly speeding up the rotation components at the beginning of inference $t\\approx1$ ) and slowing it down at the end of inference $\\left.t\\approx0\\right.$ ). ", "page_idx": 19}, {"type": "text", "text": "B.5 Unconditional protein backbone generation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Metrics. We compute several quantity of interest to measure the performance of FOLDFLOW2. i) We start with designability. We measure designability using the self-consistency metric with ProteinMPNN [Dauparas et al., 2022] and ESMFold [Lin et al., 2022], counting the fraction of proteins that refold $\\left(C_{\\alpha}{\\mathrm{-RMSD}}\\left({\\mathrm{scRMSD}}\\right)<2.0{\\mathring{\\mathrm{A}}}\\right)$ over 50 proteins at lengths $\\{100,150,200,250,300\\}$ . For our ablation study and sensitivity analysis in $\\S\\,\\mathrm{C}.2$ , we also provide the average self-consistency rmsd over all generated proteins. ", "page_idx": 19}, {"type": "text", "text": "ii) In order to use generative models for drug discovery applications, we want to measure how different and novel are the generated data compared to training data. We measure novelty using two metrics: 1.) the fraction of generated proteins that are both designable and are dissimilar to PDB structures (quantified by template-match score to PDB, i.e., PDB-TM score) $<0.3$ , as used in Yeqing and Mohammed [2023], higher is better) and 2.) for designable proteins, the average closest similarity to training data (quantified by the maximum TM score, lower is better). We note that the threshold for similarity has been studied previously, where the average TM-score on random structure pairs is $\\sim0.3$ [Zhang et al., 2006]. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "iii) Finally we want a model that generates diverse proteins and not just of the same type. Proteins are usually gathered into different clusters during training. So for diversity, we use the number of generated clusters with a TM-score threshold of 0.5 [Herbert and Sternberg, 2008](higher is better) as well as the average pairwise TM-score of the designable generated samples averaged across lengths as our diversity metric (lower is better). Note that in certain model, designability is inversely correlated with diversity as these models can produce unrealistic (e.g. unfolded) proteins that are \u201cdiverse\u201d because they do not align well with each other. ", "page_idx": 20}, {"type": "text", "text": "Baselines. On the unconditional backbone generation task we compare to pre-trained versions of FrameDiff [Yim et al., 2023b], Chroma [Ingraham et al., 2023], Genie [Yeqing and Mohammed, 2023], FoldFlow [Bose et al., 2024], and RFDiffusion [Watson et al., 2023]. We use the default parameters for each model including # of Euler steps for inference and default noise levels (0.1 for RFDiffusion and FrameDiff). We use the OT version of FoldFlow as it is the most similar to our setup and achieved the highest designability. ", "page_idx": 20}, {"type": "text", "text": "In table 3 we use 30 Euler steps for inference to better match the diversity levels of the baseline models for more accurate comparison on these metrics. Results for the 50 step model can be seen in table 17 with slightly worse designability but improved novelty and diversity. ", "page_idx": 20}, {"type": "text", "text": "B.6 Motif scaffolding ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Evaluation procedure. We follow the same evaluation procedure as in Watson et al. [2023], Yim et al. [2023b]. In particular, to evaluate the designability of a scaffold, we use ProteinMPNN [Dauparas et al., 2022] to decode 8 sequences and then re-fold those sequences, fixing the motif sequence which is known a priori. Given these re-folded structures, we compare three numbers: ", "page_idx": 20}, {"type": "text", "text": "Global RMSD: the overall aligned RMSD between the entire generated structure and the refolded structure.   \n2. Motif RSMD: the RMSD of the refolded motif residues aligned to the original motif residues.   \n3. Scaffold RMSD: the RMSD of the refolded scaffold residues aligned to the generated scaffold residues. ", "page_idx": 20}, {"type": "text", "text": "Following Watson et al. [2023], a scaffold is considered \u201cdesignable\u201d if the Global RMSD is $<2$ AND the motif $\\mathrm{RMSD}<1$ AND the scaffold $\\mathrm{RMSD}<2$ . A detailed breakdown of our results can be found in table 9, with some samples of designable motifs in fig. 8. ", "page_idx": 20}, {"type": "text", "text": "We note that the choice of folding model appears to have a nontrivial impact on this metric. In their original papers, Watson et al. [2023], Yim et al. [2023b] used AlphaFold2 with no MSA and 0 recycles to refold their structures; however ESMFold is known to be significantly more accurate when no MSAs are provided [Lin et al., 2023]. Given this, we generated new samples from RFDiffusion (FrameFlow doesn\u2019t have public code for generating scaffolds) and re-folded them with ESMFold. The result is that RFDiffusion is able to solve all 24 examples; an increase of 4 vs. their reported numbers. Moreover, the proportion of solved increases relative to their reported results, suggesting that the accuracy of the folding model significantly impacts the ability to measure scaffold quality in silico. ", "page_idx": 20}, {"type": "text", "text": "B.6.1 Pseudo-label training ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We follow a similar data augmentation procedure as in [Yim et al., 2024, Watson et al., 2023], with the only modification of adding a minimum number of contiguous residues per motif. We use the same min length and adding an absolute minimum length of 2 for a motif. We continue training FOLDFLOW-2 for 330,000 steps on the same dataset, with a learning rate of 10e-5. This was done using 2 NVIDIA A100 80G, for 2.85 days. ", "page_idx": 20}, {"type": "text", "text": "B.6.2 VHH CDR ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "From the Structural Antibody Database we used 615 nanobody sequences, yielding 1831 chains from the PDB as training set, and for testing we used 40 sequences appearing in 106 PDB chains. ", "page_idx": 20}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/46d1117f9ec698732718f86b044437a64bde46ad31ee65062102fe00a70f1b33.jpg", "table_caption": ["Table 9: A detailed breakdown of FOLDFLOW-2 motif scaffolding performance using ESMFold to refold all structures. All numbers are out of 100 samples. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 8: Samples of solved motif scaffolding problems from the benchmark of Watson et al. [2023]. The motif is in red, the designed scaffold is in blue, and the refolded structure from ESMFold is in gray. ", "page_idx": 21}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/524eb169dbb90fb326e6707bd56afd57f19dde3f8a062afeeef5410de0d3d3f0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/10f45387eb6c126a2c7dd6a1a9f9c29fdbf0c41bf1bf8ffa031d8b9555d6f3a8.jpg", "table_caption": ["Table 10: A detailed breakdown of FOLDFLOW-2 motif scaffolding performance applied to refoldable VHH structures. The same comments as table 9 apply to evaluation. All numbers are out of 25 samples. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "With both the sequence and CDRs readily available, we can build the appropriate mask for training, which is used to fix the motif atoms, and mask the scaffold sequence information. For testing, we sampled the scaffold segment lengths based on the median value of each scaffold segment, $\\pm\\ 5$ . Empirically the scaffold segment lengths varied less than that amount; nanobodies are known to exhibit less variability across their framework regions, both in sequence and structure [Mitchell and Colwell, 2018]. We continue training FOLDFLOW-2 using this dataset, training for 10,000 steps with a learning rate of 10e-5. Using a single A100 80G this process takes 3.5 hours. We observed rapid increase but rapid tapering in performance using the VHH dataset; this is likely due to the lack of variability of the scaffolds themselves. ", "page_idx": 22}, {"type": "text", "text": "We note the designability metric used here and in other papers shows certain limitations when applied to this dataset and this task. Using the test set sequence and structures themselves, we compute the same scRMSD scores, in order to benchmark what should be, in theory, the best possible performance. We observe that out of the 106 testing chains, only 25 of them are \"solved\" according to Watson et al. [2023]\u2019s criteria. This raises the question as to whether or not this particular criteria and setup is applicable to any motif scaffolding task. We provide the full set of results in table 12 on all 106 chains, and on a subset of size 25 in table 11, with generated samples in fig. 9. In addition, the particular number of solved samples are reported in table 10. ", "page_idx": 22}, {"type": "text", "text": "B.7 Zero-shot Molecular Dynamics ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To evaluate FOLDFLOW-2\u2019s ability to capture protein dynamics we evaluated its performance on the test set of ATLAS [Vander Meersche et al., 2023] molecular dynamics used by Jing et al. [2024] but restricted to proteins at most 40 amino acids in length. To measure performance we used the following metrics. All metrics were computed exclusively over backbone atoms. ", "page_idx": 22}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/abe96c52494a3416caa58e0b41909984b231b091313944561117072f1292b15e.jpg", "table_caption": ["Table 11: VHH Motif Scaffolding results on the re-foldable examples only. Reported are the global, motif, and scaffold RMSD along with the number of solved tasks. "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/e41cbcdba9a5276612664682a9afe97a9424fc9382aad45d02a1de4ec815c2f2.jpg", "table_caption": ["Table 12: VHH Motif Scaffolding results on all samples, same numbers as table 11 "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "1. Pairwise RMSD $r$ . To measure FOLDFLOW-2\u2019s ability to capture protein flexibility we first compute the average pairwise RMSD between every pair of conformations generated by each method. We then evaluate the average pairwise RMSD for the ground truth data and report the Pearson correlation $r$ between the average pairwise RMSD per generated protein and ground truth data. ", "page_idx": 23}, {"type": "text", "text": "2. Global and per-target RMSF $r$ . To further investigate flexibility, we measure the RMSF both globally and per target and compute the Pearson correlation $r$ to ground truth data. Global RMSF is computed by, for each target calculating the backbone RMSF and taking its average, then measuring Pearson correlation between generated and ground truth samples over the sequence of averaged RMSFs. For per-target RMSF we instead compute the Pearson correlation between generated and ground truth backbone atom RMSFs and report the average taken over all targets. ", "page_idx": 23}, {"type": "text", "text": "3. PCA $\\mathcal{W}_{2}$ . Following Jing et al. [2024] we seek to measure the distributional accuracy of our generated samples by evaluating the 2-Wasserstein distance using the first two principal components given by ground truth data. We use a PCA as evaluating $\\mathcal{W}_{2}$ on all atom coordinates would yield inaccurate measurements due to $\\mathcal{W}_{2}$ needing samples exponential in dimensionality in order to obtain reasonable estimates. We compute the PCA $\\mathcal{W}_{2}$ by, for each target, computing the first two principal components of backbone atom coordinates from ground truth MD data and projecting the backbone coordinates of each method\u2019s generated samples onto these coordinates. We then compute the $\\mathcal{W}_{2}$ per target and report the averaged $\\mathcal{W}_{2}$ over all targets. ", "page_idx": 23}, {"type": "text", "text": "As the test set contains 30,000 frames per protein computing test metrics using all ground truth conformations would be computationally infeasible. As such, following Jing et al. [2024] we randomly sample 300 conformations for each protein to be used as the test set. table 6 reports the mean and standard deviation over 5 resamplings of these test sets. Samples were generated from FOLDFLOW-2 using 50 inference steps and the inference annealing trick wherein the rotation vector is multiplied by 10t. ", "page_idx": 23}, {"type": "text", "text": "We report in table 13 details on the resources required for training FOLDFLOW-2 compared to AlphaFlow-MD and ESMFlow-MD. We see that FOLDFLOW-2 requires an order of magnitude less time per inference step than ESMFlow-MD and AlphaFlow-MD while attaining results competitive with ESMFlow-MD while using 4.5X less GPU hours for training and 33X less trainable parameters. FOLDFLOW-2, ESMFlow-MD, and AlphaFlow-MD were all done on NVIDIA A100s. Inference time benchmarks were done on an NVIDIA A100, performing inference on a single protein of length 300 amino acids. Finally, in appendix B.7 we provide additional generated conformational ensembles from the test set, ESMFlow, and FOLDFLOW-2. ", "page_idx": 23}, {"type": "text", "text": "Figure 9: Samples of scaffolds for VHHs. Motif (i.e. CDR) is in red, scaffold is in blue, and refolded structure from ESMFold is in gray. ", "page_idx": 24}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/12ada24c74e1962141d3e68a19f97904158a84ef2422c48ed3507e1529ad5531.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/a338588637795a83588dd4cdd4fe364bbf8fb06ca5363b5588e1e9f1fede473c.jpg", "table_caption": ["Table 13: Molecular dynamics experiment training details. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "C.1 Unconditional generation diversity and novelty exploration ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We next provide several additional results on unconditional generation to give a better understanding of the behavior of FOLDFLOW-2 relative to the baselines. In fig. 11 we can see that FOLDFLOW-2 creates many more novel proteins at all thresholds of what is considered novel as compared to ", "page_idx": 24}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/37e2dd2bbfd1f80ff309bfd5881f141722bb0d7511d64dcee9e82e729fb9d157.jpg", "img_caption": ["", "Figure 10: Additional conformation generation task samples. Proteins are colored by secondary structure with $\\alpha$ -helices in blue, $\\beta$ -sheets in red, and loops in green. ", ""], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/ff8edb7444ea0a55fc84236929dc14eb379fa4e49f22fc31bc354e3eb25f432f.jpg", "img_caption": ["Figure 11: Curve showing the number of designable proteins that are at least some distance away from the PDB. FOLDFLOW-2 has many more novel and designable proteins than baselines. We report designability fraction at TM-score $=0.3$ in table 3. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "previous methods. We also depict more generated samples of all lengths in fig. 12. We can see that FOLDFLOW-2 creates more diversity of structures, especially at shorter lengths. With synthetic data or diversity fine-tuning this is expanded to all lengths 100-300. ", "page_idx": 25}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/8321dc1594c2959b7486dc69f4c30c40442b9ebb80f4764e9af8a1db2557c597.jpg", "img_caption": ["Figure 12: Designable samples from various methods. Overlayed in silver are refolded ESMFold structures. FOLDFLOW-2 exhibits significantly more diversity in secondary structure at shorter lengths than RFDiffusion with fine-tuned models able to produce diverse proteins across lengths. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "C.2 Ablation study and sensitivity analysis ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we provide several ablations of our FOLDFLOW-2 method. In table 14, we provide the unconditional generation performance for the different architecture components of our FOLDFLOW-2 method. In table 15 and table 16, we compare the performance achieved by our model FOLDFLOW-2 when we use different inference annealing $t$ values for respectively unconditional generation and folding. In table 17 and in table 18, we compared the performance achieved by our model FOLDFLOW", "page_idx": 25}, {"type": "text", "text": "2 when using different numbers of Euler steps at inference for respectively unconditional generation and folding. ", "page_idx": 26}, {"type": "text", "text": "Architecture ablation. In table 14 we seek to understand the effects of architecture and dataset on the performance across our main designability, novelty, and diversity metrics for unconditional backbone generation. Starting from FOLDFLOW-2 we first investigate the effect of replacing the Folding Block with a simple MLP with FOLDFLOW-2 (- F. Block) and removing the sequence conditioning entirely (- ESM2). In this comparison we find that both the folding block and structure conditioning significantly improve the results. We find that the Folding block improves all metrics while the structure conditioning improves designability and diversity at the cost of novelty. ", "page_idx": 26}, {"type": "text", "text": "Dataset and Training. In table 14, we look at how adding synthetic data or stochastic flow matching affects designability, diversity, and novelty metrics. We find that both these additions actually hurt these metrics, although this is likely due to the change in composition of their generated structures. Overall, we find that these two models create more diverse proteins. ", "page_idx": 26}, {"type": "text", "text": "Number of inference steps & inference annealing scale. We also studied the influence of number of Euler steps on the generated proteins in table 17. We note that our model performs quite well at a relatively small number of steps, although performance starts to drop off under 25 steps. We attribute this to the optimal transport approach which is known to increase quality of generation especially with few inference steps Tong et al. [2023], Pooladian et al. [2023]. We find an interesting tradeoff between designability and diversity in table 17 and visually in fig. 13. Specifically that more steps increases the diversity of samples at the cost of designability. ", "page_idx": 26}, {"type": "text", "text": "We also studied the impact of the inference annealing scale factor in table 15. We see that small scaling (or none at all, corresponding to a value of 1) produce highly undesignable proteins, but designability improves quickly and beyond the optimal value of 10 it is somewhat stable. We notice the opposite effect in diversity as measured by the MaxCluster metric: no time scaling yields a score which is $64\\%$ larger than the same metric with scaling 10, and this trend is clearly anti-correlated with the scaling value. ", "page_idx": 26}, {"type": "text", "text": "Impact of synthetic augmented dataset on diversity. One of the interesting benefits of using our synthetic augmented dataset is that it increases diversity among designable generated data. Proteins have different secondary structures such as helices, beta-sheets and coil. While our model generates a lot of helices, it is important for drug discovery applications to generate other secondary structure such as beta-sheets. As shown in fig. 13, we can see that our synthetic augmented dataset leads to an increased of beta-sheets. ", "page_idx": 26}, {"type": "text", "text": "Impact of rotation time scaling & number of inference steps on folding. We conducted an analysis of inference parameters on FOLDFLOW-2\u2019s ability to fold proteins. In table 16 we sweep over the rotation time scaling parameter and measure its impact on folding RMSD. We see a similar trend to the unconditional case in table 15: very small scaling factors (e.g., 2) produce worse results, but the performance improves quickly as scaling increases and remains somewhat stable in a neighbourhood of 10. ", "page_idx": 26}, {"type": "text", "text": "We also conducted a sweep over the number of inference steps during generation on folding RMSD in table 18. We again see a similar trend to the counterpart for unconditional generation in table 17: a small number of Euler steps near 30-50 is best, with performance degrading significantly beyond 50 steps. ", "page_idx": 26}, {"type": "image", "img_path": "paYwtPBpyZ/tmp/41ee74c91ac5be2b9d0d180ee6b4cfe234eeacb30acd0ffb72c63f2f68e9e5f1.jpg", "img_caption": ["Figure 13: Secondary structure elements distributions $\\alpha$ -helices, $\\beta$ -sheets, and coils) of designable (scRMSD $<2.0\\$ proteins, along with the data\u2019s distribution. We find more steps leads to more diverse designs with fewer $\\alpha$ -helix only generations. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 14: Ablation study on FOLDFLOW-2 (FF-2) using: synthetic data, folding blocks, and stochastic flow matching (SFM). We generated 250 proteins (50 of length 100, 150, 200, 250) and compared Designability (fraction with $\\mathrm{scRMSD}<2.0\\mathring\\mathrm{A})$ , Novelty (max. TM-score to PDB and fraction of proteins with averaged max. TMscore $<0.3$ and $\\mathrm{scRMSD}<2.0\\mathring{\\mathrm{A}}\\rangle$ ), and Diversity (avg. pairwise TMscore and MaxCluster fraction). ", "page_idx": 27}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/5d9697d9cd2f1ce22638285e119286e869cd5c8f41bce227eeb2dd6a1686f7cb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/10b77e1fc661dac3fee3e084fc74ea65deee5c264db3b81fd7c0d9fcf1212439.jpg", "table_caption": ["Table 15: Comparison of Designability (fraction with $\\mathrm{scRMSD}<2.0\\mathring{\\mathrm{A}}\\!\\!\\}$ ), Novelty (max. TM-score to PDB and fraction of proteins with averaged max. TMscore $<0.3$ and $\\mathrm{scRMSD}<2.0\\mathring\\mathrm{A})$ ), and Diversity (avg. pairwise TMscore and MaxCluster fraction) for different inference annealing functions $i(t)$ . "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/e01844ee7af5328047a97986c825a1b6abf4661e9ee8b4da9c3abc3b87476d42.jpg", "table_caption": ["Table 16: Speed of the integration on rotations. Integrating with a faster time for rotations compared to translation leads to more designable structures. Reporting the mean $\\pm$ std. on 278 test samples. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/bad1a21e5e770c8af38e5afad41111036a99af60f999cad594bce5224903d75a.jpg", "table_caption": ["Table 17: Comparison of Designability (fraction with $\\mathrm{scRMSD}<2.0\\mathring\\mathrm{A})$ ), Novelty (max. TM-score to PDB and fraction of proteins with averaged max. TMscore $<0.3$ and $\\mathrm{scRMSD}<2.0\\mathring\\mathrm{A}\\!\\!\\!^{\\circ}$ ), and Diversity (avg. pairwise TMscore and MaxCluster fraction) for different number of Euler steps at inference. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "paYwtPBpyZ/tmp/ffb8aca18a98b07d20816377bc483a383b4350ed511834162b02b55c34fb677f.jpg", "table_caption": ["Table 18: Effect of the number of integration steps on the aligned RMSD between the generated and ground truth backbone. Reporting the mean $\\pm$ std. on 278 test samples. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction are supported by extensive experiments in $\\S4$ and in the Appendix $\\S C$ . ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We mention limitations of our model and our evaluation in the conclusion $\\S6$ In $\\S B.6.2$ we also mention the difficulties of evaluating VHH with the current best folding models. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Detailed information regarding experimental setup and model parameters is made available in the appendix. We introduce a new architecture which is described graphically in fig. 1, and is described in the main text with more details provided in the appendix $\\S B$ . ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] and No ", "page_idx": 28}, {"type": "text", "text": "Justification: Repreducible code is available under a CC-BY-NC 4.0 license at https: //github.com/DreamFold/FoldFlow. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: High-level experimental details are discussed in the main text in $\\S3.1{,}4$ (architecture, training setup, etc) and extensive experimental details (hyperparameters, dataset details, training time, etc) are provided in $\\S B$ . ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: When applicable, in all tables we reported the standard deviation of the scores.   \nWe provide details on the metrics computation in $\\S4.1$ , $\\S B.7$ , and $\\S B.6$ . ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide details on the training resources and initial runs in $\\S B.3$ , as well as resources for finetuning in $\\S B.7$ , and $\\S B.6.1$ . ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work uses publicly-available data and does not involve human or animal participants. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The positive societal impacts of accelerated drug discovery are obvious and discussed in numerous places in our paper. This work represents an advance in developing tools for drug research and development, but does not have immediate impact to the broader public. This is discussed in $\\S6$ . ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work investigates protein design techniques, which do not pose a high risk of misuse. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: In $\\S B$ we detail the creators and owners of the datasets used in this work. We mention the license and copyright information. We intend to repackage the AlphaFold2 structure database to curate structures which are useful for improved training of generative protein design models. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not release new assets at this time. ", "page_idx": 29}, {"type": "text", "text": "14. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}]