{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational for the field of large language models, establishing the effectiveness of few-shot learning which is relevant to the current work."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-01", "reason": "This paper introduces reinforcement learning from human feedback (RLHF) which is a key technique in aligning language models, a topic this paper also explores."}, {"fullname_first_author": "Collin Burns", "paper_title": "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision", "publication_date": "2023-12-01", "reason": "This paper directly addresses the problem of generalization from weak to strong models, a challenge this paper attempts to solve with easy-to-hard generalization."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-01", "reason": "This paper introduces the concept of training separate reward models to evaluate solutions, a key component of the proposed easy-to-hard generalization method."}, {"fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "publication_date": "2023-05-01", "reason": "This paper explores process-supervision in mathematical reasoning, which is directly relevant to the process-supervised reward models used in this paper."}]}