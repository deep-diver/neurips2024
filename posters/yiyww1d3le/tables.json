[{"figure_path": "YiYww1d3lE/tables/tables_5_1.jpg", "caption": "Table 1: Zero-shot image classification. We report top-1 accuracy (%) for each dataset. The \"Train\" column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot.", "description": "This table presents the results of zero-shot image classification experiments across 14 datasets.  It compares the performance of AWT against several state-of-the-art methods.  The top-1 accuracy is reported for each dataset, and the 'Train' column indicates whether each method required additional training (including test-time training).  Methods trained on ImageNet are excluded from the zero-shot results and indicated in grey.", "section": "4 Experiments"}, {"figure_path": "YiYww1d3lE/tables/tables_6_1.jpg", "caption": "Table 2: Out of distribution generalization.", "description": "This table presents the results of out-of-distribution generalization experiments on four ImageNet variants (ImageNet-A, ImageNet-V2, ImageNet-R, ImageNet-Sketch).  The performance of different methods (CLIP, TPT, DiffTPT, CuPL, VisDesc, WaffleCLIP, and AWT) is evaluated using top-1 accuracy.  The OOD column represents the average performance across these four datasets, showing AWT's superiority in handling out-of-distribution samples.", "section": "4 Experiments"}, {"figure_path": "YiYww1d3lE/tables/tables_6_2.jpg", "caption": "Table 3: Zero-shot video action recognition.", "description": "This table presents the results of zero-shot video action recognition on three benchmark datasets: UCF101, HMDB51, and Kinetics-600.  Two evaluation protocols are used for UCF101 and HMDB51 (EP1 and EP2), while Kinetics-600 uses three validation sets. The table compares the performance of AWT against several state-of-the-art methods, highlighting AWT's superior performance in this task.", "section": "4.2 Zero-shot Video Tasks"}, {"figure_path": "YiYww1d3lE/tables/tables_7_1.jpg", "caption": "Table 1: Zero-shot image classification. We report top-1 accuracy (%) for each dataset. The \"Train\" column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot.", "description": "This table presents the zero-shot image classification results on 14 datasets.  It compares the performance of AWT against several state-of-the-art (SOTA) methods,  including prompt-based, test-time prompt tuning, and other augmentation-based approaches. The \"Train\" column indicates whether each method requires any additional training, highlighting AWT's zero-shot capability.  Numbers in gray indicate methods pre-trained on ImageNet, which are not considered zero-shot.", "section": "4.1 Zero-shot Image Tasks"}, {"figure_path": "YiYww1d3lE/tables/tables_7_2.jpg", "caption": "Table 1: Zero-shot image classification. We report top-1 accuracy (%) for each dataset. The \"Train\" column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot.", "description": "This table presents the zero-shot image classification results on 14 datasets using various methods.  It compares AWT's performance to other state-of-the-art (SOTA) methods, highlighting its ability to achieve high accuracy without requiring additional training. The \"Train\" column indicates whether a method uses additional training data, differentiating zero-shot approaches from others.  The table provides a comprehensive comparison of different methods across multiple datasets, offering a clear view of AWT's effectiveness.", "section": "4.1 Zero-shot Image Tasks"}, {"figure_path": "YiYww1d3lE/tables/tables_9_1.jpg", "caption": "Table 5: Failure case analysis. We focus on low-resolution datasets CIFAR-10 and CIFAR-100 and evaluate the effectiveness of AWT when equipped with different image augmentation techniques.", "description": "This table presents the results of experiments on CIFAR-10 and CIFAR-100 datasets using different image augmentation techniques.  The baseline performance of CLIP is shown for comparison, along with results when applying AWT using the traditional random resized cropping and flipping technique, and again using AWT with DALL-E 2 generated images.  The performance change (increase or decrease) relative to the baseline is indicated using arrows and numerical values.", "section": "4.1 Zero-shot Image Tasks"}, {"figure_path": "YiYww1d3lE/tables/tables_18_1.jpg", "caption": "Table 1: Zero-shot image classification. We report top-1 accuracy (%) for each dataset. The \"Train\" column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot.", "description": "This table presents the results of zero-shot image classification on 14 datasets using different methods.  The top-1 accuracy is reported for each dataset and method.  The \"Train\" column indicates whether the method requires additional training, differentiating between zero-shot and non-zero-shot approaches.  Numbers in gray indicate that the method was pre-trained on ImageNet, which is not considered a zero-shot scenario.", "section": "4 Experiments"}, {"figure_path": "YiYww1d3lE/tables/tables_19_1.jpg", "caption": "Table 7: Fine-tuned modalities in few-shot learning. The model's performance is evaluated with the adapter module applied to visual, textual, or both modalities. Performance metrics are reported for the ImageNet test set.", "description": "This table presents the results of a few-shot learning experiment on the ImageNet dataset using a multi-modal adapter. The adapter was applied to either the visual modality, the textual modality, or both. The table shows the top-1 accuracy for each configuration (1-shot, 4-shot, 16-shot), demonstrating that the multi-modal adapter consistently outperforms the single-modality adapters.", "section": "B.3 Study on the multi-modal adapter"}, {"figure_path": "YiYww1d3lE/tables/tables_19_2.jpg", "caption": "Table 8: Domain generalization. All methods are trained on 16-shot ImageNet and directly evaluated on four datasets with domain shifts. We report the top-1 accuracy (%) of each method.", "description": "This table presents the results of domain generalization experiments.  All the methods listed were trained on 16-shot ImageNet and then directly evaluated on four other datasets with different domain characteristics (ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-Sketch). The results show the top-1 accuracy for each method on each dataset, providing a measure of how well each method generalizes to unseen data with different distributions.", "section": "B.4 Domain generalization"}, {"figure_path": "YiYww1d3lE/tables/tables_19_3.jpg", "caption": "Table 9: Study on different LLMs in the description generation process.", "description": "This table compares the performance of AWT when using different large language models (LLMs) for generating class descriptions.  It shows the top-1 accuracy achieved on various image datasets (IN-1K, IN-A, IN-V2, DTD, Cars, UCF, Cal101, Food) for three different LLMs: GPT-3.5, GPT-4, and Qwen-Plus. The results demonstrate the robustness of AWT to different LLMs and highlight that the more advanced LLMs do not always yield better performance.", "section": "B.5 Study on different LLMs"}, {"figure_path": "YiYww1d3lE/tables/tables_20_1.jpg", "caption": "Table 10: Compare AWT-base with other description generation methods.", "description": "This table compares the performance of the AWT framework using different methods for generating class descriptions. The baseline AWT method uses a single global question to generate descriptions.  Other methods utilize various prompting strategies, including using 3 global questions or 4.56 dataset-specific questions. The results are evaluated using out-of-distribution (OOD) average accuracy across 14 datasets. The table demonstrates that the proposed two-step dataset-aware prompting approach in AWT yields the best performance.", "section": "B.6 Description generation without dataset description"}, {"figure_path": "YiYww1d3lE/tables/tables_20_2.jpg", "caption": "Table 1: Zero-shot image classification. We report top-1 accuracy (%) for each dataset. The \"Train\" column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot.", "description": "This table presents the zero-shot image classification results on 14 datasets using various methods.  The top-1 accuracy is reported for each dataset and method. The \"Train\" column indicates whether each method required additional training (including test-time training), highlighting the zero-shot capability of the methods.  Numbers in gray indicate methods that were pre-trained on ImageNet, making them not truly zero-shot.", "section": "4.1 Zero-shot Image Tasks"}, {"figure_path": "YiYww1d3lE/tables/tables_21_1.jpg", "caption": "Table 12: Few-shot transfer learning settings. We select the model from the final epoch for testing.", "description": "This table shows the hyperparameters used for few-shot transfer learning experiments with different numbers of shots (1, 2, 4, 8, and 16).  It details settings for the adapter layers, downsample rate, learning rate, warmup epochs, weight decay, training epochs, batch size, optimizer, momentum, number of image views and class descriptions for training and testing, and GPU numbers used.  The augmentation strategy is also specified.", "section": "C Additional Details"}, {"figure_path": "YiYww1d3lE/tables/tables_22_1.jpg", "caption": "Table 1: Zero-shot image classification. We report top-1 accuracy (%) for each dataset. The \"Train\" column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot.", "description": "This table presents the results of zero-shot image classification experiments on 14 datasets using various methods.  The top-1 accuracy is reported for each dataset and each method. The \"Train\" column indicates whether the method required any training, including test-time training. Methods trained on ImageNet are indicated in gray, as they are not considered zero-shot.", "section": "4.1 Zero-shot Image Tasks"}, {"figure_path": "YiYww1d3lE/tables/tables_23_1.jpg", "caption": "Table 1: Zero-shot image classification. We report top-1 accuracy (%) for each dataset. The \"Train\" column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot.", "description": "This table presents the zero-shot image classification results on 14 datasets.  It compares the performance of AWT against several state-of-the-art methods. The accuracy is reported as top-1 accuracy (%).  The \"Train\" column indicates whether a method used any additional training data.  Gray numbers represent methods that were trained on ImageNet, thus not considered zero-shot.", "section": "4 Experiments"}]