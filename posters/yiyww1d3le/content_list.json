[{"type": "text", "text": "AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuhan Zhu1 Yuyang Ji1 Zhiyu Zhao1,2 Gangshan $\\mathbf{W}\\mathbf{u}^{1}$ Limin Wang1,2\u2217 ", "page_idx": 0}, {"type": "text", "text": "1State Key Laboratory for Novel Software Technology, Nanjing University 2Shanghai AI Laboratory https://github.com/MCG-NJU/AWT ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks. However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes. To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through image transformations and language models; dynamically weighting inputs based on the prediction entropy; and employing optimal transport to mine semantic correlations in the vision-language space. AWT can be seamlessly integrated into various VLMs, enhancing their zero-shot capabilities without additional training and facilitating few-shot learning through an integrated multimodal adapter module. We verify AWT in multiple challenging scenarios, including zero-shot and few-shot image classification, zero-shot video action recognition, and out-of-distribution generalization. AWT consistently outperforms the state-of-the-art methods in each setting. In addition, our extensive studies further demonstrate AWT\u2019s effectiveness and adaptability across different VLMs, architectures, and scales. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advances in vision-language models (VLMs) [1\u20138], which undergo extensive pre-training on web-scale image-text pairs, have exhibited remarkable success in various classification tasks. VLMs are trained to associate images with relevant textual descriptions. In the standard protocol (Fig. 1(a)), raw images and class names are projected into a joint vision-language embedding space, where the class with the shortest distance to the image representation is selected as the prediction result. ", "page_idx": 0}, {"type": "text", "text": "However, directly using raw images and class names in testing has limitations [1, 9]. Visually, the broad scope of pre-training compels VLMs to analyze all image elements, lacking capability of focusing on specific interested regions. For instance, a model might miss critical facial features of a cat while unnecessarily focusing on irrelevant elements like \u201cbench\u201d and \u201cgrass\u201d (Fig. 1(a)). Textually, since VLM pre-training associates visual elements with diverse and rich textual descriptions (e.g., colors and textures), merely using class names during test falls short of capturing the full spectrum of visual content. To enhance input effectiveness, the literature focuses on post-training prompts [9\u201333] (Fig. 1(b)) that provide contextual cues, thereby helping the model in prioritizing relevant features, such as cat\u2019s attributes. However, this approach often depends on the availability of training resources, which may not be always practical. ", "page_idx": 0}, {"type": "text", "text": "In this study, we are interested in enhancing inputs for better adaptation of VLMs without training prompts. We advocate for data augmentation as a simple yet effective strategy, as depicted in Fig. 1(c). ", "page_idx": 0}, {"type": "image", "img_path": "YiYww1d3lE/tmp/dd200496330efd054845d6eac977e8a5e7959f6aa674caadd1a6d9467590a00e.jpg", "img_caption": ["Figure 1: (a) Standard protocol directly calculates distances between raw images and class names in the joint V-L space. (b) Prompt-based methods enhance inputs with post-trained visual or textual prompts to provide the task-specific context. (c) Augment-based method enriches raw inputs with image transformations and class descriptions, requiring no additional training. Upon this, we propose AWT, which considers both intra-modal importance variations and cross-modal semantic correlations. (d) AWT is evaluated against SOTA methods across four tasks: zero-shot and few-shot image classification, out-of-distribution generalization, and zero-shot video action recognition. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Techniques like random resized cropping and image flipping enrich the input with varied and multiscale perspectives, while detailed textual descriptions for each class provide richer visual narratives. Although manually crafting diverse descriptions for each class is expensive, employing Large Language Models (LLMs) [34\u201336] presents an efficient alternative. ", "page_idx": 1}, {"type": "text", "text": "Nonetheless, several challenges remain. First, the intra-modal importance of each augmented image and description needs assessment, as not all views contribute equally to class recognition\u2014some may be irrelevant background elements or non-visual descriptors such as the cat\u2019s personality. Second, the inter-modal interaction requires consideration, as descriptions such as \u201cdark face\u201d or \u201clight-colored body\u201d might have direct semantic correlations with some image crops (Fig. 1(c)). ", "page_idx": 1}, {"type": "text", "text": "To tackle these challenges, we propose AWT, a novel framework that augments raw inputs into diverse views, weights view importance in each modality dynamically, and transports semantic correlations across modalities. Initially, AWT augments raw inputs via image transformations and LLMs. Subsequently, it weights the importance of each view on the fly based on its prediction entropy, as more confident predictions typically indicate higher accuracy [37]. This method allows AWT to identify and prioritize significant views, and adjust the importance distribution dynamically according to the task-specific context (e.g., candidate class names). AWT then formulates the image-text distance calculation as an optimal transport problem [38, 39], considering each augmented view as a quantity of sand. The importance assessed for each view determines the mass of its corresponding sand pile, and distances are calculated using cosine similarity. This formulation can effectively discover cross-modal correlations by solving the optimal transport problem\u2014which minimizes the effort required to transport sand from one modality to another. Additionally, generating class descriptions from LLMs using a simple prompt like \u201cDescribe a {class}.\u201d often results in overly generic descriptions. Inspired by chain-of-thought approach [40], we introduce a two-step, dataset-aware prompting method. This approach encourages LLMs to produce class descriptions that are both diverse and dataset-relevant. ", "page_idx": 1}, {"type": "text", "text": "We implement AWT using the CLIP model [1] and evaluated its performance across 21 datasets covering four challenging tasks: zero-shot and few-shot image classification, out-of-distribution generalization, and zero-shot video action recognition. As shown in Fig. 1(d), AWT consistently surpasses the existing state-of-the-art methods in each setting. Our extensive analysis further examines AWT\u2019s flexibility with diverse architectures, its scalability with different model sizes, and its potential applicability to other VLMs. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Vision-Language Models. Leveraging the extensive pre-training on web-scale text-image pairs, vision-language models (VLMs) such as CLIP [1] and ALIGN [6] excel in acquiring versatile representations that span multiple modalities. These models adeptly embed texts and images into a shared vision-language feature space, enabling the proximity of inputs with analogous semantics. The inherent flexibility of natural language allows VLMs to be effectively utilized across a wide range of open-set tasks including image classification [1, 9, 13], object detection [41\u201343], image generation [44, 45], video action recognition [46\u201348]. However, such general-purpose models often fail to focus on task-specific details, which can result in sub-optimal performance. This study aims to overcome this limitation by proposing a novel adaptation framework, namely AWT, for VLMs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Adapt VLMs to downstream tasks. Direct adaptation of pre-trained VLMs to downstream tasks often results in suboptimal and unstable performance [9]. To overcome this, the existing literature has primarily focused on the use of post-training to enrich task context. This includes strategies such as few-shot prompt learning [9, 11, 12, 29, 32], cross-dataset prompt generalization [13\u201322, 25\u2013 27, 30, 31, 33], unsupervised prompt tuning [23, 24, 28], test-time prompt tuning [10, 49\u201354], and adapter tuning [55\u201359]. Conversely, other approaches aim to augment inputs using various resources such as the WordNet relationship hierarchy [60], Large Language Models [61\u201364], or Stable Diffusion models [50, 65, 66]. Nonetheless, these methods mainly enhance only one modality. In contrast, our study innovatively applies augmentation to both visual and textual modalities and addresses significant challenges in dual-modality augmentation scenarios. ", "page_idx": 2}, {"type": "text", "text": "Optimal Transport (OT). Optimal transport (OT), originating from the Monge problem [38] in the eighteenth century, serves as a metric for quantifying the distance between mathematical entities [67] while considering their intricate geometric structures [39]. Historically rediscovered in various forms, OT first gained fame in computer vision under the name of earth mover\u2019s distances [68]. The development of efficient approximate solvers [69] has recently propelled a resurgence in OT\u2019s popularity, broadening its utility across multiple domains, including object detection [70, 71], domain adaptation [72\u201374], generative modeling [75\u201378], semantic correspondence [79], point clouds [80\u2013 82], prompt learning [83\u201385] and video understanding [86, 87]. Of particular relevance to our study are PLOT [83] and Wang et al. [85], which leverage OT for fine-grained prompt learning to enhance VLMs. Distinct from these two studies, our research diverges by eschewing the need for additional training resources, opting instead for an augmentation-based direction. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Contrastive Language-Image Pre-training (CLIP). CLIP [1] integrates dual encoders\u2014an image encoder $f(\\cdot)$ and a text encoder $g(\\cdot),$ \u2014to map images and textual descriptions into a shared visionlanguage (V-L) embedding space. CLIP is designed to minimize the cosine distance between embeddings of semantically related image-text pairs. Thanks to the flexibility of natural language, CLIP enables direct application to classification tasks without the need for task-specific training. For instance, given an image $X\\in\\mathbb{R}^{3\\times H\\times W}$ and a set of candidate class names $\\{\\bar{t_{i}}\\}_{i=1}^{C}$ , where $C$ denotes the class count. CLIP computes the embeddings $I\\in\\mathbb{R}^{d}$ for the image and $\\{e_{i}\\}_{i=1}^{C}\\in\\mathbb{R}^{C\\times d}$ for all class names, where $d$ is the feature dimension. Subsequently, the classification probability for image $X$ being of class $t_{i}$ can be formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\np\\left(t_{i}\\mid X\\right)=\\frac{\\exp\\left(\\cos\\left(e_{i},I\\right)/\\tau\\right)}{\\sum_{j=1}^{C}\\exp\\left(\\cos\\left(e_{j},I\\right)/\\tau\\right)},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau$ is a temperature parameter. ", "page_idx": 2}, {"type": "text", "text": "Optimal Transport (OT). Optimal transport (OT) theory, originating from the Monge problem [38], provides a framework for structural distance measurements. This theory conceptualizes scenarios such as relocating sand at a construction site with the goal of minimizing effort. Mathematically, the initial and target distributions of sands are modeled as discrete measures: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\alpha=\\sum_{i=1}^{N}\\mathbf{a}_{i}\\delta_{x_{i}}\\quad{\\mathrm{~and~}}\\quad\\beta=\\sum_{j=1}^{M}\\mathbf{b}_{j}\\delta_{y_{j}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\delta_{x_{i}}$ denotes the Dirac with a concentrated mass ${\\bf a}_{i}$ centered at $x_{i}$ , and similarly for $\\beta$ . Here, $N$ and $M$ represent the number of source and target locations, respectively. The cost of transporting sands from any source location $x_{i}$ to any target location $y_{j}$ is given by the cost function $c(x_{i},y_{j})$ . To extend the application to broader and more intricate scenarios, e.g., cross-modal correlation, the Kantorovich relaxation [88] is employed. This relaxation introduces flexibility in the transport plan and ensures symmetric transport solutions. The transport plan $\\mathbf{P}\\,\\in\\,\\mathbb{R}_{+}^{N\\times M}$ , where element $\\mathbf{P}_{i,j}$ indicates the mass transported from $x_{i}$ to $y_{j}$ , must satisfy the constraints: ", "page_idx": 2}, {"type": "image", "img_path": "YiYww1d3lE/tmp/0609f353de15fd7552bb1b56740f669c10819999e34e10f670fedc66eb86cba2.jpg", "img_caption": ["Figure 2: Pipeline of AWT: Augment, Weight, then Transport. Given an image and candidate class names, we first augment each input into diverse views. These views are then fed into the CLIP model to obtain coarse predictions. To assess the importance of each view, we use prediction confidence as a proxy and introduce an entropy-based weighting mechanism. Next, we measure the distance between image-text view sets by solving an optimal transport (OT) problem. Finally, the resulting OT distance is used to represent the distance between the input image and each class name. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{U}(\\mathbf{a},\\mathbf{b})\\ {\\stackrel{\\mathrm{def.}}{=}}\\ \\left\\{\\mathbf{P}\\in\\mathbb{R}_{+}^{N\\times M}:\\mathbf{P}\\mathbb{1}_{M}=\\mathbf{a}\\quad{\\mathrm{~and~}}\\quad\\mathbf{P}^{\\mathrm{T}}\\mathbb{1}_{N}=\\mathbf{b}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Kantorovich\u2019s formulation seeks to minimize the total transportation cost: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c}(\\alpha,\\beta)\\;\\stackrel{\\mathrm{def.}}{=}\\operatorname*{min}_{\\mathbf{P}\\in\\mathbf{U}(\\mathbf{a},\\mathbf{b})}\\langle\\mathbf{C},\\mathbf{P}\\rangle\\;\\stackrel{\\mathrm{def.}}{=}\\;\\sum_{i,j}\\mathbf{C}_{i,j}\\mathbf{P}_{i,j},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{C}_{i,j}=c(x_{i},y_{j})$ defines the cost matrix. ", "page_idx": 3}, {"type": "text", "text": "3.2 AWT: Augment, Weight, then Transport ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Pre-trained VLMs often underperform when adapted to new concepts due to insufficient information about new classes. Moreover, their extensive pre-training scope leads them to analyze all elements of an image, causing them to miss contextually important cues crucial for specific downstream applications. To overcome these limitations, we introduce a novel framework, termed AWT (Augment, Weight, then Transport), to enhance the adaptability of VLMs without additional training. The AWT framework, as depicted in Fig. 2, consists of three critical components: augmenting raw inputs to generate diverse and content-rich views, weighting the significance of these views within each modality, and transporting semantically correlated elements across modalities. ", "page_idx": 3}, {"type": "text", "text": "3.2.1 Augment Raw Inputs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The augmentation process begins with an image $X\\,\\in\\,\\mathbb{R}^{3\\times H\\times W}$ and the class name set $\\{t_{i}\\}_{i=1}^{C}$ , aiming to transform these inputs into various views that offer different perspectives and details. ", "page_idx": 3}, {"type": "text", "text": "For visual augmentation, we apply standard data augmentation including random resized cropping and random flipping to produce a set of varied views $\\{X^{n}\\}_{n=1}^{N+1}$ . This set includes $N$ augmented images alongside the original (denoted as the 0 index), enriching the input with diverse and multiscale perspectives. An illustrative example is shown in Fig. 2. ", "page_idx": 3}, {"type": "text", "text": "To enrich the textual modality, we utilize Large Language Models (LLMs) to generate class descriptions. Typical prompts like \u201cDescribe a {class}.\u201d often result in descriptions that are either vague\u2014lacking in specific visual details\u2014or contextually misaligned. For instance, in contexts such as classifying sketches, generic descriptions of a category may not correspond well with the sketch images. To address this, we adopt a two-step, dataset-aware prompt strategy. Initially, we prompt LLMs to generate multiple questions that probe different aspects of the category, which is crucial for eliciting detailed and varied descriptions. To ensure the queries are aligned with the visual content, we incorporate a dataset-level description into the initial prompts. Specifically, we start by asking LLMs to \u201cGenerate questions to classify images from a dataset, which {dataset descirption}.\u201d. Using the dataset-related questions generated from the first step, we ionbctlauidn itnagi laorn eadd ddietsicorniaplt ivoinesw.  fTohre mseedt  obfy  atuhge mraewnt celda svsi enwasm feo. r Tehaicsh  mcleatshso $t_{i}$ eins sduerneso tbeodt ha s $\\{t_{i}^{m}\\}_{m=1}^{M+1}$ the descriptions and their relevance to the visual content. More details can be found in Appendix C.2. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2.2 Weight Augmented Views ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Following augmentation, it is essential to assess the significance of each augmented view, as not all views contribute equally to classification. Some views may be critical while others might be less informative or even noisy. To address this variation, we introduce an entropy-based weighting mechanism to quantify each view\u2019s importance. Our key insight is that the impact of a view on classification confidence\u2014a metric often correlated with accuracy [37]\u2014can serve as a proxy for its importance. A view that significantly enhances classification confidence is considered more vital. ", "page_idx": 4}, {"type": "text", "text": "To assess the importance of $n$ -th image view $X^{n}$ , we maintain a constant text context and compute the averaged embedding for each class as $\\begin{array}{r}{\\{\\bar{e}_{i}=\\frac{1}{M+1}\\sum_{m=1}^{M+1}e_{i}^{m}\\}_{i=1}^{C}}\\end{array}$ , where $e_{i}^{m}$ is the CLIP embedding of $t_{i}^{m}$ . The classification probability $p(t\\mid X^{n})$ is then calculated using the image embedding $I^{n}$ and text embeddings $\\{\\bar{e}_{i}\\}_{i=1}^{C}$ , following Eq. (1). Predictive confidence is then quantified using the entropy formula $\\begin{array}{r}{H_{n}(t)=-\\sum_{t}p(t\\mid X^{n})\\log p(t\\mid X^{n})}\\end{array}$ . Lower entropy indicates higher confidence, allowing us to evaluate view importance through the negative entropy as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{a}_{n}=\\frac{\\exp\\left(-H_{n}(t)/\\gamma_{v}\\right)}{\\sum_{j=1}^{N+1}\\exp\\left(-H_{j}(t)/\\gamma_{v}\\right)},\\quad n=1,\\ldots,N+1,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\gamma_{v}$ is a temperature parameter adjusting the distribution\u2019s sharpness. ", "page_idx": 4}, {"type": "text", "text": "Similarly, to determine the importance of $m$ -th description for $i$ -th class, i.e., $t_{i}^{m}$ , we calculate the classification probability $p_{m}^{i}(t\\mid X^{0})$ , with the image embedding $I^{0}$ and text embeddings $\\{e_{i}^{m}\\}\\cup$ $\\{\\bar{e}_{j}\\}_{j=1,j\\neq i}^{C}$ . The classification entropy is given by $\\begin{array}{r}{H_{m}^{i}(t)\\,=\\,-\\sum_{t}p_{m}^{i}\\left(t\\mid X^{0}\\right)\\log p_{m}^{i}\\left(t\\mid\\dot{X}^{0}\\right)}\\end{array}$ We then calculate the importance scores for all descriptions within the $i$ -th class as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{b}_{m}^{i}=\\frac{\\exp\\left(-H_{m}^{i}(t)/\\gamma_{t}\\right)}{\\sum_{k=1}^{M+1}\\exp\\left(-H_{k}^{i}(t)/\\gamma_{t}\\right)},\\quad m=1,\\ldots,M+1,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\gamma_{t}$ is the temperature parameter. This entropy-based weighting mechanism ensures the prioritization of the contextually significant views. By dynamically adjusting the importance based on the direct impact on classification confidence, the augmented view sets can be well-prepared for the optimal transport process. ", "page_idx": 4}, {"type": "text", "text": "3.2.3 Transport Across Modalities ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our primary goal is to precisely measure the distance between an image and its candidate names. Through the augmentation, we have transformed each original image or class name into a set of augmented views. Typically, the distance between these sets is measured by simply averaging the embeddings within each set. However, such practice often fails to capture the dynamic correlation across modalities. Consider the scenario depicted in Fig. 2, where specific textual descriptions such as \u201chas a dome-shaped top\u201d might correlate directly with certain image crops. The conventional averaging strategy typically overlooks these intuitive and meaningful correlations. ", "page_idx": 4}, {"type": "text", "text": "To address this issue, we propose a novel approach by formulating distance measurement as an optimal transport (OT) problem, which facilitates richer interactions between modalities. We model each view within the V-L space as a mass located at its embedding position: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\alpha=\\sum_{n=1}^{N+1}\\mathbf{a}_{n}\\delta_{I^{n}}\\quad\\mathrm{~and~}\\quad\\left\\{\\beta^{i}=\\sum_{m=1}^{M+1}\\mathbf{b}_{m}^{i}\\delta_{e_{i}^{m}}\\right\\}_{i=1}^{C}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "table", "img_path": "YiYww1d3lE/tmp/b635e5a9854f7d861da93c1425f026a1fed0c765715cc107504bd451ede19861.jpg", "table_caption": ["Table 1: Zero-shot image classification. We report top-1 accuracy $(\\%)$ for each dataset. The \"Train\" column indicates whether the methods necessitate additional training (including test-time training). Numbers in grey indicate that the method was trained on ImageNet and is therefore not zero-shot. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "Here, the importance weight of each view, derived from Eqs. (5) and (6), determines the mass allocation. The transportation cost between any two points (e.g., an image and a textual description) is quantified using the cosine distance between their embeddings, $\\mathbf{C}=1-\\cos(I,e)$ , which serves as an intuitive measure of semantic similarity. The goal of optimal transport is to minimize the total cost of transporting mass from visual modality into textual modality. Specifically, the distance between tbheet iwmeeang iaenwd , $\\{X^{n}\\}_{n=1}^{N+1}$ aatendd $i$ -nt hE cql. a(s4s ).d esWceri petimopnl osyet $\\{t_{i}^{m}\\}_{m=1}^{M+1}$ isA rlegdoeriftinhemd  [a6s 9a]n t oO eTf pfricoieblnetlmy $\\alpha$ $\\beta_{i}$   \napproximate the solution, denoted as $\\tilde{\\mathbf{P}}$ . Consequently, the classification probability can be expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\mathrm{OT}}\\left(t_{i}\\mid X\\right)=\\frac{\\exp\\left(\\mathbf{s}_{i}/\\tau\\right)}{\\sum_{j=1}^{C}\\exp\\left(\\mathbf{s}_{j}/\\tau\\right)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{{\\bf s}=\\sum_{i}\\sum_{j}\\tilde{\\bf P}_{i j}\\left(1-{\\bf C}\\right)_{i j}}\\end{array}$ . By employing the optimal transport framework, we ensure that semantically related views receive more attention, enhancing the accuracy and relevance of the classification process. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Zero-shot Image Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. For zero-shot image tasks, we consider image classification and out-of-distribution (OOD) generalization. Our study encompasses 18 datasets that span a wide array of recognition tasks: ImageNet [89], Caltech101 [95] and Caltech256 [101] for generic object recognition, OxfordPets [92], StanfordCars [93], OxfordFlowers [90], Food101 [96], FGVCAircraft [98], Birdsnap [100] and CUB [102] for fine-grained classification, SUN397 [97] for scene recognition, DTD [91] for texture classification, EuroSAT [99] for satellite recognition, and UCF101 [94] for action recognition. Besides, four ImageNet variant datasets are involved to assess the model\u2019s capability for OOD generalization: ImageNet-A [103], ImageNetV2 [104], ImageNet-R [105] and ImageNet-Sketch [106]. ", "page_idx": 5}, {"type": "text", "text": "Competitive methods. We mainly compare three distinct categories of approaches: 1) Prompt learning methods: These involve additional data to post-train visual or textual prompts, including CoOp [9], CoCoOp [13], MaPLe [15], $\\mathrm{PLOT++}$ [83], POMP [22]. 2) Test-time prompt tuning methods: These optimize prompts during inference, such as TPT [49], DiffTPT [50], and PromptAlign [52]. 3) Augment-based method: These use LLMs or diffusion models to augment inputs, including CuPL [63], VisDesc [61], WaffleCLIP [62], and SuS-X-SD [65]. ", "page_idx": 5}, {"type": "text", "text": "Implementation details. We implemented the AWT framework using the CLIP-B/16 model [1]. Image augmentations include random resized cropping and flipping, and class descriptions are generated via GPT-3.5 [35]. We set the number of augmented images $N$ and descriptions $M$ to 50 each. Dataset-level descriptions are provided in Appendix C. For both visual and textual modalities, we configured the importance distribution temperatures at $\\gamma_{v}\\,=\\,1/2$ and $\\gamma_{t}=1/2$ . The optimal transport problem is approximated using Sinkhorn\u2019s Algorithm with an $\\epsilon$ of 0.1 [69]. All experiments are conducted on one NVIDIA A100-SXM4-80GB GPU. ", "page_idx": 6}, {"type": "text", "text": "Results. In Tab. 1, we compare AWT with three categories of CLIP adaptation methods: prompt learning, test-time prompt tuning, and existing augmentation-based methods. Remarkably, without additional training, AWT outperforms all existing methods by a significant margin, achieving state-of-the-art performance on 13 out of 14 datasets and surpassing the previous best results ", "page_idx": 6}, {"type": "table", "img_path": "YiYww1d3lE/tmp/9584353893a9da0907c7e72ae7e2e171da8e7656978f6553bd17e2fc515ecbb3.jpg", "table_caption": ["Table 2: Out of distribution generalization. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "by an average accuracy of $2.05\\%$ . Further, the out-of-distribution (OOD) generalization capabilities of AWT are detailed in Table 2. Leveraging dataset-aware prompting and a dynamic weighting approach that adjusts in real-time during testing, AWT effectively manages complex scenarios encountered in OOD. Consequently, AWT stands out by delivering the highest performance across all four OOD datasets, surpassing the previous arts by an average accuracy improvement of $3.62\\%$ . ", "page_idx": 6}, {"type": "text", "text": "4.2 Zero-shot Video Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setup. Here, we focus on the zero-shot video action recognition task, using three representative datasets: UCF101 [94], HMDB51 [107], and Kinetics-600 [108]. For UCF101 and HMDB51, we adopt two evaluation protocols: 1) EP1: test model on all 101 UCF classes and 51 HMDB classes [47, 117], and report the top-1 accuracy. 2) EP2: evaluate the model using three official splits and averaging the results of each split. The average top-1 accuracy and standard deviation are reported. For Kinetics-600, the top-1 accuracy and ", "page_idx": 6}, {"type": "table", "img_path": "YiYww1d3lE/tmp/6b1d5c7b3b35445d3c9dfd0d27ff839043e8f00016d952a15dbc1ed9a8fb00ba.jpg", "table_caption": ["Table 3: Zero-shot video action recognition. "], "table_footnote": ["standard deviation are reported on three validation sets split by Chen and Huang [118]. "], "page_idx": 6}, {"type": "text", "text": "Implementation details. To model temporal dynamics, we follow Open-VCLIP [47] to use neighbor-frame attention and fine-tune CLIP on Kinetics-400 [119]. Note that three test subsets of Kinetics-600 have a disjoint class set compared to Kinetics-400. All AWT configurations are the same for zero-shot image tasks except for the visual augmentation, in which we directly use the different sampled temporal and cropped video frames. ", "page_idx": 6}, {"type": "text", "text": "Results. In Tab. 3, we present a comparison of our AWT with existing CLIP-based zero-shot video action recognition methods. Although AWT was not originally tailored for video tasks, it sets new records in this domain, outperforming the recent state-of-the-art method, FROSTER, by $1.6\\%$ and $2.4\\%$ on HMDB51, and by $1.3\\%$ on Kinetics-600. These results suggest that our AWT framework could be effectively extended to video understanding tasks. ", "page_idx": 6}, {"type": "text", "text": "4.3 Few-shot Image Tasks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Setup. We assessed the few-shot transfer capabilities of our method across 11 datasets: ImageNet [89], Caltech101 [95], OxfordPets [92], StanfordCars [93], OxfordFlowers [90], Food101 [96], FGVCAircraft [98], SUN397 [97], DTD [91], EuroSAT [99], and UCF101 [94]. We trained our model using 1, 2, 4, 8, and 16 shots. Results are averaged over three runs. ", "page_idx": 6}, {"type": "text", "text": "Implementation details. All AWT configurations are the same as zero-shot image tasks. In this task, we introduced a multi-modal Adapter for efficient learning, inserted after each Multi-Head ", "page_idx": 6}, {"type": "table", "img_path": "YiYww1d3lE/tmp/59cf7bd930d85d9cbcf2654c29e3ad6bfc394d1c4ee5ef7dd3d4bd0973ed8a54.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "YiYww1d3lE/tmp/7555eae89b9142791783cea17e356bd3b92899d6811a0b3ff98658402182a419.jpg", "table_caption": ["Table 4: Ablation experiments across 18 image datasets. The default configuration is colored grey . (a) Main component analysis: Aug-(b) Number of augmented image (c) Number of generated class ment(A), Weight(W) and Transport(T). views $N$ . descriptions $M$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Self-Attention and MLP block in every layer. We adopt a distillation technique similar to [21] to prevent overfitting. Further details about the Adapter are available in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Results. In Fig. 3, we compare the performance of AWT with existing methods in the few-shot transfer learning task. Impressively, AWT surpasses the previous state-of-the-art by average accuracies of $2.76\\%$ , $2.16\\%$ , $1.62\\%$ , $1.57\\%$ , and $1.75\\%$ for 1, 2, 4, 8, and 16 shots respectively. Notably, on the ImageNet dataset, AWT significantly outperforms all prior methods. While $\\mathrm{PLOT++}$ also utilizes optimal transport, it is limited to local image features and class names, neglecting the multiscale image perspectives and the rich textual semantics, leading to suboptimal transfer capabilities. In contrast, AWT leverages diverse augmented views, effectively maintaining intra-modal importance trade-offs while establishing dynamical cross-modal correlations, achieving superior few-shot performance. ", "page_idx": 7}, {"type": "text", "text": "4.4 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Main component analysis. In Tab. 4a, we analyze the key components of AWT. Initially, we augment raw inputs and apply basic ensembling. The results (rows two and four) show that directly augmenting images is ineffective, likely due to background image crops. Conversely, textual enhancements significantly boost performance, thanks to our carefully designed prompt strategies for LLMs. We then shift from basic ensembling to optimal transport (OT), leading to consistent improvements across two tasks. However, the full potential of OT is not realized due to ineffective mass (i.e., importance) weighting. By incorporating our entropy-based weighting method, which accurately assesses the importance of each view, we again achieve substantial performance gains. ", "page_idx": 7}, {"type": "image", "img_path": "YiYww1d3lE/tmp/f8e6f5e9fe5831a1217ac53bad1c30dbfb778a9a68b45f78a60fb8edf6b62030.jpg", "img_caption": ["Figure 4: Versatility analysis of AWT. Average top-1 accuracy $(\\%)$ on 18 image datasets is reported. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Number of augmentation views. We present the study on the augmented view quantities for both visual and textual sides in Tabs. 4b and 4c, respectively. The results clearly demonstrate that performance tends to increase with the number of views. Our findings suggest that about 50 views per modality are sufficient to achieve decent performance. The number of augmentation views is crucial for AWT\u2019s effectiveness. Given that AWT is augmentation-driven, this correlation is intuitive. However, increasing the augmented view quantities can also lead to higher computational costs during inference, we also include an efficiency-performance trade-off analysis in Fig. 9. ", "page_idx": 8}, {"type": "text", "text": "LLM prompting strategy. We evaluated the effectiveness of our LLM prompting strategy, detailed in Tab. 4d. Our method is compared with two established approaches: VisDesc [61] and CuPL [63]. VisDesc uses a uniform prompt template across different datasets, while CuPL employs a tailored, dataset-specific manual prompting strategy, enriching context for LLMs. We developed a refined twostep process that enhances context comprehension through dataset-level descriptions and increases diversity by utilizing chain-of-thought queries. Our strategy consistently outperforms the existing methods in both evaluated tasks. ", "page_idx": 8}, {"type": "text", "text": "Temperature in weighting. We evaluated our entropy-based weighting method by conducting an ablation study on the softmax function\u2019s temperature parameter (see Eqs. (5) and (6)). A higher temperature creates a more uniform importance distribution. The findings for both modalities are presented in Tabs. 4e and 4f, respectively. Our results reveal that a very high temperature (e.g., 100) leads to suboptimal performance, likely due to insufficient emphasis on contextually significant views. Conversely, lowering the temperature enhances focus on these important views, improving performance. Empirically, a temperature of $1/2$ has been identified as optimal for both modalities. For a clearer understanding of our weighting strategy, visualizations are provided in Appendix A. ", "page_idx": 8}, {"type": "text", "text": "4.5 Versatility Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our AWT is applicable to any VLM using dual encoders to map images and text into a joint space with appropriate distance metrics (e.g.. cosine similarity). Therefore, it is crucial to assess AWT\u2019s effectiveness across various scenarios. We conduct evaluations with both ResNet [120] and ViT [121] architectures, explore AWT\u2019s scalability from ViT-B/32 to $\\mathrm{ViT-L}/14@336$ , and assess its generalizability across three VLMs: ALIGN [6], SigLIP [122], and EVA-CLIP [123]. We conduct experiments across 18 image datasets and present the results in Fig. 4. Our findings reveal that AWT consistently achieves performance gains in all tested scenarios, highlighting its broad applicability. ", "page_idx": 8}, {"type": "text", "text": "4.6 Failure Case Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Although AWT has shown success across various datasets and tasks, we identified certain limitations when applying it to the CIFAR datasets. As detailed in Tab. 5, AWT resulted in performance declines of $0.91\\%$ and $0.17\\%$ on two CIFAR datasets, respectively. To investigate this issue, we analyzed the images produced by the transformations used in our study. We discovered that with low-resolution images, such as $32\\times32$ pixels, the random resized crop operation tends to overly blur the images, obscuring the objects within them, as illustrated in Fig. 5. To address this, we integrated a diffusion model, specifically DALL\u00b7E 2 [44], as a substitute for traditional data augmentations. Fig. 5 shows examples of enhanced image views generated by DALL\u00b7E 2, which produce sharper images and offer a variety of perspectives. By incorporating this advanced technique into the AWT framework, we have significantly improved its performance. The updated benchmark results, presented in Tab. 5, demonstrate that AWT now consistently achieves performance gains over baselines. ", "page_idx": 8}, {"type": "image", "img_path": "YiYww1d3lE/tmp/8b3be0fa06fce631b05c0e57b1ecaca48f07db52444b25e53a28530a2e559a08.jpg", "img_caption": ["Figure 5: Comparison of image augmentation techniques on low-resolution images. We present images from the CIFAR-10/100 datasets, where each image is $32\\times32$ pixels. The comparison includes images generated by traditional image transformations and DALL\u00b7E 2. "], "img_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "YiYww1d3lE/tmp/16151d3eae051f2f8f11f605e5345a1db5c5454ce65adcc8a10784da7dcd7cd7.jpg", "table_caption": ["Table 5: Failure case analysis. We focus on low-resolution datasets CIFAR-10 and CIFAR-100 and evaluate the effectiveness of AWT when equipped with different image augmentation techniques. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have introduced the AWT (Augment, Weight, then Transport) framework, designed to enhance the transferability of pre-trained vision-language models (VLMs). Rather than using raw images and class names directly, our approach enriches the inputs by augmenting them with diverse visual perspectives and detailed class descriptions. We further develop an entropy-based weighting strategy to dynamically prioritize these augmented views and employ optimal transport to measure the cross-modal distance in the structured visual-language space. The AWT framework not only boosts the zero-shot performance of VLMs without the need for additional training but also facilitates few-shot transfer learning via an integrated multimodal adapter module. Our evaluations across four challenging tasks demonstrate that AWT significantly outperforms existing state-of-the-art methods. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. This work is supported by the National Key R&D Program of China (No. 2022ZD0160900), the National Natural Science Foundation of China (No. 62076119), the Fundamental Research Funds for the Central Universities (No. 020214380119), the Nanjing UniversityChina Mobile Communications Group Co., Ltd. Joint Institute, and the Collaborative Innovation Center of Novel Software Technology and Industrialization. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021. 1, 2, 3, 6, 7, 10, 19, 23   \n[2] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [3] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. [4] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [5] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195\u201322206, 2024.   \n[6] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021. 2, 9   \n[7] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022. [8] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022. 1   \n[9] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision (IJCV), 2022. 1, 3, 6, 20, 21, 23   \n[10] Yuhan Zhu, Guozhen Zhang, Chen Xu, Haocheng Shen, Xiaoxin Chen, Gangshan Wu, and Limin Wang. Efficient test-time prompt tuning for vision-language models. arXiv preprint arXiv:2408.05775, 2024. 3, 6   \n[11] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5206\u20135215, 2022. 3   \n[12] Feng Wang, Manling Li, Xudong Lin, Hairong Lv, Alex Schwing, and Heng Ji. Learning to decompose visual features with latent textual prompts. In The Eleventh International Conference on Learning Representations, 2022. 3   \n[13] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3, 6, 20, 21   \n[14] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-guided context optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6757\u20136767, 2023. 20   \n[15] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19113\u201319122, 2023. 6, 20   \n[16] Adrian Bulat and Georgios Tzimiropoulos. Lasp: Text-to-text optimization for language-aware soft prompting of vision & language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23232\u201323241, 2023.   \n[17] Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyeong Choi, Sanghyeok Lee, and Hyunwoo J Kim. Read-only prompt optimization for vision-language few-shot learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1401\u20131411, 2023.   \n[18] Baoshuo Kan, Teng Wang, Wenpeng Lu, Xiantong Zhen, Weili Guan, and Feng Zheng. Knowledge-aware prompt tuning for generalizable vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15670\u201315680, 2023.   \n[19] Hantao Yao, Rui Zhang, and Changsheng Xu. Tcp: Textual-based class-aware prompt tuning for visual-language model. arXiv preprint arXiv:2311.18231, 2023.   \n[20] Ji Zhang, Shihan Wu, Lianli Gao, Hengtao Shen, and Jingkuan Song. Dept: Decoupled prompt tuning. arXiv preprint arXiv:2309.07439, 2023.   \n[21] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15190\u201315200, 2023. 8, 21   \n[22] Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, Mu Li, Alexander J Smola, and Xu Sun. Prompt pre-training with twenty-thousand classes for open-vocabulary visual recognition. Advances in ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Neural Information Processing Systems, 36, 2024. 3, 6, 20   \n[23] Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, and Wangmeng Zuo. Texts as images in prompt tuning for multi-label image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2808\u20132817, 2023. 3   \n[24] Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, Luc Van Gool, and Federico Tombari. Learning to prompt with text only supervision for vision-language models. arXiv preprint arXiv:2401.02418, 2024. 3   \n[25] Cheng Shi and Sibei Yang. Logoprompt: Synthetic text images can be good visual prompts for visionlanguage models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2932\u20132941, 2023. 3   \n[26] Zhengbo Wang, Jian Liang, Ran He, Nan Xu, Zilei Wang, and Tieniu Tan. Improving zero-shot generalization for clip with synthesized prompts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3032\u20133042, 2023.   \n[27] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15659\u201315669, 2023. 3, 20   \n[28] Tony Huang, Jack Chu, and Fangyun Wei. Unsupervised prompt learning for vision-language models. arXiv preprint arXiv:2204.03649, 2022. 3   \n[29] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Unified vision and language prompt learning. arXiv preprint arXiv:2210.07225, 2022. 3   \n[30] Sifan Long, Zhen Zhao, Junkun Yuan, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang, and Jingdong Wang. Task-oriented multi-modal mutual leaning for vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21959\u201321969, 2023. 3   \n[31] Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, Wei Ji, Qi Tian, Tat-Seng Chua, and Yueting Zhuang. Gradient-regulated meta-prompt learning for generalizable visionlanguage models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2551\u20132562, 2023. 3   \n[32] Chen Xu, Yuhan Zhu, Haocheng Shen, Boheng Chen, Yixuan Liao, Xiaoxin Chen, and Limin Wang. Progressive visual prompt learning with contrastive feature re-formation. International Journal of Computer Vision, pages 1\u201316, 2024. 3, 6, 21   \n[33] Chen Xu, Yuhan Zhu, Guozhen Zhang, Haocheng Shen, Yixuan Liao, Xiaoxin Chen, Gangshan Wu, and Limin Wang. Dpl: Decoupled prompt learning for vision-language models. arXiv preprint arXiv:2308.10061, 2023. 1, 3   \n[34] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 2   \n[35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022. 7, 20   \n[36] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api.semanticscholar. org/CorpusID:257532815. 2, 20   \n[37] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2020. 2, 5   \n[38] Gaspard Monge. M\u00e9moire sur la th\u00e9orie des d\u00e9blais et des remblais. Mem. Math. Phys. Acad. Royale Sci., pages 666\u2013704, 1781. 2, 3   \n[39] Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6):355\u2013607, 2019. 2, 3   \n[40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022. 2   \n[41] Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin Wei, Weidi Xie, and Lin Ma. Promptdet: Towards open-vocabulary detection using uncurated images. In European Conference on Computer Vision, pages 701\u2013717. Springer, 2022. 3   \n[42] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Zero-shot detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2(3):4, 2021.   \n[43] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. Learning to prompt for openvocabulary object detection with vision-language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14084\u201314093, 2022. 3   \n[44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 3, 10   \n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022. 3   \n[46] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: A new paradigm for video action recognition. arXiv preprint arXiv:2109.08472, 2021. 3, 7   \n[47] Zejia Weng, Xitong Yang, Ang Li, Zuxuan Wu, and Yu-Gang Jiang. Open-vclip: Transforming clip to an open-vocabulary video model via interpolated weight optimization. In ICML, 2023. 7   \n[48] Xiaohu Huang, Hao Zhou, Kun Yao, and Kai Han. Froster: Frozen clip is a strong teacher for openvocabulary action recognition. arXiv preprint arXiv:2402.03241, 2024. 3, 7   \n[49] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems, 35:14274\u201314289, 2022. 3, 6, 7, 20, 23   \n[50] Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmentation with diffusions for effective test-time prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2704\u20132714, 2023. 3, 6, 7, 20   \n[51] Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. Swapprompt: Test-time prompt adaptation for vision-language models. Advances in Neural Information Processing Systems, 36, 2024.   \n[52] Jameel Abdul Samadh, Mohammad Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muhammad Muzammal Naseer, Fahad Shahbaz Khan, and Salman H Khan. Align your prompts: Test-time prompting with distribution alignment for zero-shot generalization. Advances in Neural Information Processing Systems, 36, 2024. 6, 20   \n[53] Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, and Lei Zhang. Dual memory networks: A versatile adaptation approach for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 28718\u201328728, 2024.   \n[54] Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, and Eric Xing. Efficient test-time adaptation of vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14162\u201314171, 2024. 3   \n[55] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, pages 1\u201315, 2023. 3   \n[56] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In European Conference on Computer Vision, pages 493\u2013510. Springer, 2022.   \n[57] Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao, and Peng Gao. Not all features matter: Enhancing few-shot clip with adaptive prior refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2605\u20132615, 2023. 21   \n[58] Yicheng Xu, Yuxin Chen, Jiahao Nie, Yusong Wang, Huiping Zhuang, and Manabu Okumura. Advancing cross-domain discriminability in continual learning of vison-language models. arXiv preprint arXiv:2406.18868, 2024.   \n[59] Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, and Xiaohua Xie. Mma: Multi-modal adapter for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23826\u201323837, 2024. 3   \n[60] Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, and Jiaping Zhao. Improving zero-shot generalization and robustness of multi-modal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11093\u201311101, 2023. 3   \n[61] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. In The Eleventh International Conference on Learning Representations, 2022. 3, 6, 7, 8, 9, 21   \n[62] Karsten Roth, Jae Myung Kim, A Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata. Waffilng around for performance: Visual classification with random words and broad concepts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15746\u201315757, 2023. 6, 7   \n[63] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15691\u201315701, 2023. 6, 7, 8, 9, 21   \n[64] Zachary Novack, Julian McAuley, Zachary Chase Lipton, and Saurabh Garg. Chils: Zero-shot image classification with hierarchical label sets. In International Conference on Machine Learning, pages 26342\u201326362. PMLR, 2023. 3   \n[65] Vishaal Udandarao, Ankush Gupta, and Samuel Albanie. Sus-x: Training-free name-only transfer of vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2725\u20132736, 2023. 3, 6   \n[66] Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu Qiao, Peng Gao, and Hongsheng Li. Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15211\u2013 15222, 2023. 3   \n[67] Abdelwahed Khamis, Russell Tsuchida, Mohamed Tarek, Vivien Rolland, and Lars Petersson. Scalable optimal transport methods in machine learning: A contemporary survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3   \n[68] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover\u2019s distance as a metric for image retrieval. International journal of computer vision, 40:99\u2013121, 2000. 3   \n[69] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. 3, 6, 7   \n[70] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun. Ota: Optimal transport assignment for object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 303\u2013312, 2021. 3   \n[71] Henri De Plaen, Pierre-Fran\u00e7ois De Plaen, Johan AK Suykens, Marc Proesmans, Tinne Tuytelaars, and Luc Van Gool. Unbalanced optimal transport: A unified framework for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3198\u20133207, 2023. 3   \n[72] Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853\u20131865, 2016. 3   \n[73] Yuguang Yan, Wen Li, Hanrui Wu, Huaqing Min, Mingkui Tan, and Qingyao Wu. Semi-supervised optimal transport for heterogeneous domain adaptation. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 2969\u20132975, 2018.   \n[74] Bharath Bhushan Damodaran, Benjamin Kellenberger, R\u00e9mi Flamary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. In Proceedings of the European conference on computer vision (ECCV), pages 447\u2013463, 2018. 3   \n[75] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214\u2013223. PMLR, 2017. 3   \n[76] Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal transport. In International Conference on Learning Representations, 2018.   \n[77] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017.   \n[78] Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert St\u00f6ter. Slicedwasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, pages 4104\u20134113. PMLR, 2019. 3   \n[79] Yanbin Liu, Linchao Zhu, Makoto Yamada, and Yi Yang. Semantic correspondence as an optimal transport problem. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4463\u20134472, 2020. 3   \n[80] Gilles Puy, Alexandre Boulch, and Renaud Marlet. Flot: Scene flow on point clouds guided by optimal transport. In European conference on computer vision, pages 527\u2013544. Springer, 2020. 3   \n[81] Zhengyang Shen, Jean Feydy, Peirong Liu, Ariel H Curiale, Ruben San Jose Estepar, Raul San Jose Estepar, and Marc Niethammer. Accurate point cloud registration with robust optimal transport. Advances in Neural Information Processing Systems, 34:5373\u20135389, 2021.   \n[82] Ruibo Li, Guosheng Lin, and Lihua Xie. Self-point-flow: Self-supervised scene flow estimation from point clouds with optimal transport and random walk. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15577\u201315586, 2021. 3   \n[83] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Plot: Prompt learning with optimal transport for vision-language models. In The Eleventh International Conference on Learning Representations, 2022. 3, 6, 20, 21, 23   \n[84] Kwanyoung Kim, Yujin Oh, and Jong Chul Ye. Zegot: Zero-shot segmentation through optimal transport of text prompts. arXiv preprint arXiv:2301.12171, 2023.   \n[85] Dongsheng Wang, Miaoge Li, Xinyang Liu, MingSheng Xu, Bo Chen, and Hanwang Zhang. Tuning multi-mode token-level prompt alignment across modalities. Advances in Neural Information Processing Systems, 36, 2024. 3   \n[86] Tongjia Chen, Hongshan Yu, Zhengeng Yang, Zechuan Li, Wei Sun, and Chen Chen. Ost: Refining text knowledge with optimal spatio-temporal descriptor for general video recognition. arXiv preprint arXiv:2312.00096, 2023. 3   \n[87] Kaihua Zhang, Zicheng Zhao, Dong Liu, Qingshan Liu, and Bo Liu. Deep transport network for unsupervised video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8781\u20138790, 2021. 3   \n[88] L. Kantorovich. On the translocation of masses. Journal of Mathematical Sciences, 133, 03 2006. doi: 10.1007/s10958-006-0049-2. 4   \n[89] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009. 6, 7, 20, 21   \n[90] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722\u2013729. IEEE, 2008. 6, 7, 21   \n[91] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613, 2014. 6, 7, 20, 21   \n[92] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012. 6, 7, 21, 23   \n[93] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013. 6, 7, 20, 21   \n[94] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 6, 7, 20, 21   \n[95] Li Fei-Fei. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178\u2013178. IEEE, 2004. 6, 7, 20, 21, 23   \n[96] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014. 6, 7, 20, 21   \n[97] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Largescale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010. 6, 7, 21   \n[98] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 6, 7, 21   \n[99] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217\u20132226, 2019. 6, 7, 21, 23   \n[100] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L. Alexander, David W. Jacobs, and Peter N. Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In Proc. Conf. Computer Vision and Pattern Recognition (CVPR), June 2014. 6   \n[101] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. 6, 23   \n[102] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 6   \n[103] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271, 2021. 6, 7, 20, 23   \n[104] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389\u20135400. PMLR, 2019. 6, 7, 20, 23   \n[105] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349, 2021. 6, 7, 20, 23   \n[106] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. 6, 7, 20, 23   \n[107] Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision, pages 2556\u20132563. IEEE, 2011. 7, 23   \n[108] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018. 7, 23   \n[109] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. In European Conference on Computer Vision, pages 1\u201318. Springer, 2022. 7   \n[110] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. In European Conference on Computer Vision, pages 105\u2013124. Springer, 2022. 7   \n[111] Wenhao Wu, Zhun Sun, and Wanli Ouyang. Revisiting classifier: Transferring vision-language models for video recognition. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 2847\u20132855, 2023. 7   \n[112] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu Li. Aim: Adapting image models for efficient video action recognition. In The Eleventh International Conference on Learning Representations, 2022. 7   \n[113] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-to-video transfer learning. Advances in Neural Information Processing Systems, 35:26462\u201326477, 2022. 7   \n[114] Syed Talal Wasim, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah. Vita-clip: Video and text adaptive clip via multimodal prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23034\u201323044, 2023. 7   \n[115] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned clip models are efficient video learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6545\u20136554, 2023. 7   \n[116] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 35:16664\u201316678, 2022. 7 [117] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zeroshot video classification: End-to-end training for realistic applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4613\u20134623, 2020. 7 [118] Shizhe Chen and Dong Huang. Elaborative rehearsal for zero-shot action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13638\u201313647, 2021. 7 [119] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 7, 23 [120] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n9 [121] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. 9 [122] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages   \n11975\u201311986, 2023. 9 [123] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 9 [124] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 10 [125] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,   \n2021. 19 [126] Shuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang Zhao, and Xu Sun. Delving into the openness of clip. In Findings of the Association for Computational Linguistics: ACL 2023, pages 9587\u20139606, 2023. 20 [127] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 20 [128] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2016. 22 [129] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139\u20131147. PMLR, 2013. 22 [130] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,   \n2016. 22 [131] Guozhen Zhang, Jingyu Liu, Shengming Cao, Xiaotong Zhao, Kevin Zhao, Kai Ma, and Limin Wang. Dynamic and compressive adaptation of transformers from images to videos. arXiv preprint arXiv:2408.06840, 2024. 25 [132] Xinhao Li and Limin Wang. Zeroi2v: Zero-cost adaptation of pre-trained transformers from image to video. arXiv preprint arXiv:2310.01324, 2023. 25 [133] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5682\u20135692, 2023. 25 [134] Guozhen Zhang, Chunxu Liu, Yutao Cui, Xiaotong Zhao, Kai Ma, and Limin Wang. Vfimamba: Video frame interpolation with state space models. arXiv preprint arXiv:2407.02315, 2024. [135] Chunxu Liu, Guozhen Zhang, Rui Zhao, and Limin Wang. Sparse global matching for video frame interpolation with large motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19125\u201319134, 2024. 25 [136] Mengcheng Lan, Xinjiang Wang, Yiping Ke, Jiaxing Xu, Litong Feng, and Wayne Zhang. Smooseg: Smoothness prior for unsupervised semantic segmentation. Advances in Neural Information Processing Systems, 36:11353\u201311373, 2023. 25 [137] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Proxyclip: Proxy attention improves clip for open-vocabulary segmentation. arXiv preprint arXiv:2408.04883, 2024. [138] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Clearclip: Decomposing clip representations for dense vision-language inference. arXiv preprint arXiv:2407.12442,   \n2024. 25 [139] Yuhan Zhu, Guozhen Zhang, Jing Tan, Gangshan Wu, and Limin Wang. Dual detrs for multi-label temporal action detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18559\u201318569, 2024. 25 [140] Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Accelerating image generation with sub-path linear approximation model. arXiv preprint arXiv:2404.13903, 2024. 25 [141] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847,   \n2023. 25 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The appendix provides supplementary material including additional visualization results, comparative studies, ablation experiments, detailed methodologies, examples of failure cases, and a discussion on the limitations of our research. The contents are structured as follows: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Visualizations of our weighting and transportation processes (Appendix A).   \n\u2022 Extended experiments and ablation studies (Appendix B).   \n\u2022 Method details and experimental procedures (Appendix C).   \n\u2022 Discussion on societal impacts (Appendix D).   \n\u2022 Discussion on limitations and future research directions (Appendix E). ", "page_idx": 16}, {"type": "text", "text": "A Visualization ", "text_level": 1, "page_idx": 16}, {"type": "image", "img_path": "YiYww1d3lE/tmp/d17584ccf187f5e1db9b2320b58750b8cbbccc33ae4350d5938a3f3ead590aeb.jpg", "img_caption": ["Figure 6: Visualization of weighting image views. We show the weights assigned to the same image view set under varying candidate class names. Our dynamic weighting strategy effectively allocates importance to contextually relevant image views. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.1 Entropy-based Weight Strategy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Pre-trained VLMs often struggle to adequately focus on contextually significant features due to their open-set design, which requires processing every element within an image. To overcome this limitation, we introduce an entropy-based weighting strategy that dynamically evaluates the importance of each image view based on the prediction entropy. This method allows us to prioritize relevant views while diminishing the impact of less pertinent ones. ", "page_idx": 16}, {"type": "text", "text": "Visualization of weighting image views. We have included visualizations to demonstrate the effectiveness of our entropy-based weighting strategy in managing image views, as shown in Fig. 6. ", "page_idx": 16}, {"type": "text", "text": "Through these visualizations, we observe two primary benefits: 1) Priority to Significant Views: The strategy effectively prioritizes image views that are intuitively significant, while efficiently excluding irrelevant background crops. 2) Dynamic weight assignment: In different contexts, such as varying candidate class names, the strategy dynamically assigns weights, emphasizing views that are contextually important. ", "page_idx": 17}, {"type": "image", "img_path": "YiYww1d3lE/tmp/2ba213828026f08029ec164c78adb5af5696da94fa6f3d7c0a043d7eb24fd8d8.jpg", "img_caption": ["Figure 7: Visualization of weighting class descriptions. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Visualization of weighting class descriptions. The strategy not only enhances image view management but also optimizes the handling of textual descriptions. In Fig. 7, we observe the following: 1) For descriptions related to the ground-truth class, our strategy prioritizes key textual elements while effectively filtering out irrelevant content. 2) For non-matching classes, the importance levels are relatively uniform, indicating a general irrelevance to the specific image content. ", "page_idx": 17}, {"type": "text", "text": "A.2 Optimal Transport ", "text_level": 1, "page_idx": 17}, {"type": "image", "img_path": "YiYww1d3lE/tmp/1c0ae2c7d65593c271ab06705b0e34030e625d204134ed250077025f4ac9e2af.jpg", "img_caption": ["Figure 8: Visualization of cross-modal correlations captured by optimal transport. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "After augmenting the input image and class names to create diverse views, there is a potential for these views to exhibit direct and meaningful correlations across modalities. For instance, image crops that focus on the eyes of a cat could closely correlate with textual descriptions concerning the eyes. In our main paper, we introduce the use of optimal transport (OT) to effectively handle these correlations. To illustrate this, we provide a visualization of the meaningful correlations captured by the OT approach. As depicted in Fig. 8, the visualization of the transport plan clearly shows that image and text pairs with direct semantic relationships are given priority. Conversely, pairs that are semantically irrelevant are typically disregarded by the transport plan. ", "page_idx": 17}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "YiYww1d3lE/tmp/2c54681e16fb9c330be7f64f61343499788d9e1bcbfc6ceaf9a3c6a0f2122438.jpg", "table_caption": ["B.1 Error bar analysis ", "Table 6: Error bar analysis on 18 image datasets. "], "table_footnote": ["We conducted an analysis of error bars across 18 image datasets, performing three runs each to ensure robust statistical evaluation. The results, which include both the mean and the standard deviation of the top-1 accuracy, are presented in Tab. 6. Overall, AWT demonstrates robust performance, exhibiting low variability across most datasets. "], "page_idx": 18}, {"type": "text", "text": "B.2 Performance-efficiency trade-off ", "text_level": 1, "page_idx": 18}, {"type": "image", "img_path": "YiYww1d3lE/tmp/fcfe396617b3a58d0a946a8c0d58deb1c61297f981d9baad78656b559cb71306.jpg", "img_caption": ["Figure 9: Performance-efficiency comparison. AWT is assessed using 2, 5, 10, 25, and 50 image views to present its trade-off between performance and computational efficiency. We show the results on both zero-shot (left) and few-shot (right) tasks. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Our AWT introduces additional computational costs during inference by utilizing multiple views. Although text embeddings can be pre-computed as suggested in [1], the primary expense arises from processing additional image views. In our study, we explore the trade-off between performance and efficiency by employing 2, 5, 10, 25, and 50 augmented views of the input image. We assess this trade-off by reporting both the frames per second (FPS) and the average accuracy in zero-shot and few-shot settings, as shown in Fig. 9. ", "page_idx": 18}, {"type": "text", "text": "The results indicate that even with a limited number of views, AWT can substantially outperform existing methods. As the number of views increases, there is a consistent enhancement in performance, albeit at the expense of reduced inference speed. This analysis enables practitioners to tailor AWT application to real-world scenarios by selecting an optimal balance between accuracy and processing speed, based on specific requirements. ", "page_idx": 18}, {"type": "text", "text": "In the few-shot settings, we incorporate two additional adapter modules at each transformer layer to facilitate transfer learning. This configuration initially makes our method slower than other prompt learning approaches, likely due to the adapters requiring sequential processing, whereas prompts are processed in parallel. Considering the computational costs, we propose that AWT could be more efficiently transferred using test-time zero-cost methods like LoRA [125], which allow for structural reparameterization after training. ", "page_idx": 18}, {"type": "text", "text": "Table 7: Fine-tuned modalities in few-shot learning. The model\u2019s performance is evaluated with the adapter module applied to visual, textual, or both modalities. Performance metrics are reported for the ImageNet test set. ", "page_idx": 19}, {"type": "table", "img_path": "YiYww1d3lE/tmp/b8a7064ac776fe5839d93fd5cb21dcfeb5ec887fe945371a08495b42f91903f7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In this study, we explore the impact of applying multi-modal adapters to both visual and textual modalities. We conducted experiments using an unimodal adapter either on the visual or textual side and analyzed their performance on the ImageNet dataset. The results are detailed in Tab. 7. Our findings indicate that when training samples are extremely limited, such as in a 1-shot scenario, fine-tuning the visual adapter yields more benefits. However, as the number of training samples increases, fine-tuning the textual adapter becomes more advantageous. This shift is likely due to the need to reduce the similarity between class embeddings as suggested in [126]. Despite these variations, our results ultimately show that fine-tuning both modalities simultaneously delivers the best overall performance. ", "page_idx": 19}, {"type": "text", "text": "B.4 Domain generalization ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "YiYww1d3lE/tmp/886a9cc339f37351fa2e610899accfe7fba391e496a6fabadfe6d66bded111af.jpg", "table_caption": ["Table 8: Domain generalization. All methods are trained on 16-shot ImageNet and directly evaluated on four datasets with domain shifts. We report the top-1 accuracy $(\\%)$ of each method. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Previous studies [9, 13] suggested assessing method robustness by directly applying a model trained on ImageNet to four variant datasets derived from ImageNet. We adhere to this protocol and present our findings in Tab. 8. The results demonstrate that our approach, AWT, not only excels in few-shot performance on ImageNet but also outperforms in domain generalization robustness across the four ImageNet variant datasets, effectively balancing the adaptation-generalization trade-off. ", "page_idx": 19}, {"type": "text", "text": "B.5 Study on different LLMs ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "YiYww1d3lE/tmp/ba6d1e234f95f626a81b79dfe74959a930bb2957dcb543dacbc97bcac2b8ef61.jpg", "table_caption": ["Table 9: Study on different LLMs in the description generation process. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "In this study, we utilize GPT-4o [36] and Qwen-Plus [127] for our description generation process, integrating them into AWT. The benchmark accuracy is presented in Tab. 9. Overall, AWT maintains robust performance across different LLMs. Interestingly, more advanced LLMs do not necessarily lead to significant performance improvements. This may be because AWT primarily relies on the simple instruction-following and knowledge-recall capabilities of LLMs, for which models like GPT-3.5 are sufficient. ", "page_idx": 19}, {"type": "text", "text": "B.6 Description generation without dataset description ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "YiYww1d3lE/tmp/48ac10e9f8e360187725100fe559b3d6eb25bb21443943149549866c2136ae50.jpg", "table_caption": ["Table 10: Compare AWT-base with other description generation methods. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Here, we examine the performance of AWT when no dataset-level description is available, a situation often encountered in real-world applications. We introduce a base version of AWT, which replaces our original instruction with \"Generate questions to classify images.\" while leaving other components unchanged. The performance of AWT-base is detailed in Tab. 10. Despite its simplicity, AWT-base demonstrates favorable performance compared to previous methods. ", "page_idx": 20}, {"type": "text", "text": "B.7 Numerical values in few-shot learning ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For the benefti of future research, we present numerical values of our experimentation in the few-shot transfer learning setting in Tab. 11. ", "page_idx": 20}, {"type": "table", "img_path": "YiYww1d3lE/tmp/96902d13e27fc0c1722f10e49affcfe487494e78b04b3cdc5725352d31602363.jpg", "table_caption": ["Table 11: Few-shot image classification results of different methods on 11 datasets. All results are averaged over three runs. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "YiYww1d3lE/tmp/d2684fbf46cafc5118c6fd2037a4a9dec6a01977e7c8f85848e8a5beb2280737.jpg", "img_caption": ["Figure 10: Architecture of the adapter module and its integration with the Transformer. "], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "YiYww1d3lE/tmp/fbecc942a03a4abed26ffae03a576e62bc0bc17e43a3c94e474c44e4511bd2bc.jpg", "table_caption": ["Table 12: Few-shot transfer learning settings. We select the model from the final epoch for testing. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "C Additional Details ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "C.1 Few-shot learning with the multi-modal adapter module ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For few-shot learning, we employ a multi-modal adapter to achieve parameter-efficient transfer learning. The architecture of the multi-modal adapter is depicted in Fig. 10. We integrate the adapter module subsequent to the Multi-Head Self-Attention and the MLP module within each transformer layer. The adapter module is applied to both the visual and textual side of the CLIP model. The core of the adapter module is a bottleneck structure designed to efficiently manage model parameters while maintaining performance. This structure comprises a down-projection layer $W_{\\mathrm{down}}\\in\\mathbb{R}^{d\\times\\frac{d}{\\lambda}}$ that reduces the dimensionality of the input features, and an up-projection layer $W_{\\mathrm{up}}\\in\\mathbb{R}^{\\frac{d}{\\lambda}\\times d}$ that restores the original dimensions. Between these layers, a GeLU activation layer [130] introduces non-linearity. A learnable scale parameter $s$ is used to modulate the output of the adapter. The feature dimension is denoted by $d$ . For an input feature vector $\\mathbf{x}\\in\\mathbb{R}^{L\\times d}$ , where $L$ is the sequence length, the adapter\u2019s forward process is expressed as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{x}}=s\\cdot\\mathrm{GeLU}(\\mathbf{x}\\cdot W_{\\mathrm{down}})\\cdot W_{\\mathrm{up}}+\\mathbf{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Detailed settings and hyperparameters for our few-shot learning experiments are outlined in Tab. 12. ", "page_idx": 21}, {"type": "table", "img_path": "YiYww1d3lE/tmp/fb5fa5113beb7740088ff1edd08ad85468c1124754058c9223483fc28c59697a.jpg", "table_caption": ["Table 13: Dataset-level descriptions employed in the first step of our prompt strategy are provided, along with corresponding source weblinks for each description. ImageNet-A and ImageNetV2 are omitted as they share the same dataset-level descriptions as ImageNet. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.2 Two-step dataset-aware prompting for LLMs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Here, we provide additional details of our proposed two-step dataset-aware prompting strategy for LLMs. Initially, in the first step, we leverage a dataset-level description for each dataset to inspire the generation of diverse and contextually relevant questions. The descriptions for each dataset are sourced from their official websites or from the paperswithcode website, and we include the dataset description and the reference to the source\u2019s URL in Tab. 13. ", "page_idx": 22}, {"type": "text", "text": "To better illustrate the effectiveness of our strategy, we present a sample of questions generated in the first step. For each dataset, we list three examples in Tab. 14. From these examples, we can observe the following: 1) the questions generated are not only more diverse compared to traditional prompts such as \u201cDescribe a {class}\u201d, but they also delve into more detailed inquiries, and 2) the questions incorporate dataset-specific information, thereby aiding LLMs in producing more accurate and relevant descriptions. ", "page_idx": 22}, {"type": "text", "text": "C.3 License information of the assets used in this work ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Datasets. Below are the datasets used in this paper that have known license information: MIT License: ImageNet-A [103], ImageNetV2 [104], ImageNet-R [105], ImageNet-Sketch [106], EuroSAT [99]. ", "page_idx": 22}, {"type": "text", "text": "CC BY 4.0 License: Caltech101 [95], Caltech256 [101], HMDB51 [107], K400 [119], K600 [108].   \nCC BY-SA 4.0 License: OxfordPets [92]. ", "page_idx": 22}, {"type": "text", "text": "Source code. Source code used in this paper are under the MIT License: CLIP [1], CoOp [9], TPT [49], PLOT [83]. ", "page_idx": 22}, {"type": "table", "img_path": "YiYww1d3lE/tmp/201091a9df827590939854069282947a939521fce07ba442747bcc026403cbd8.jpg", "table_caption": ["Table 14: A selection of generated questions for each dataset. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D Societal Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This paper introduces AWT to enhance the transferability of vision-language foundation models. We have evaluated the effectiveness of AWT across a variety of classification tasks, employing different types, scales, and architectures of VLMs. Our findings suggest that AWT holds significant potential for integration into diverse downstream applications involving VLMs. Beyond classification tasks, we anticipate that the insights gained from this study will stimulate further research into other areas of computer vision, such as object detection and semantic segmentation. Currently, we are not aware of any ethical issues associated with the real-world applications of this technology. Nonetheless, continuous monitoring and evaluation are recommended to ensure responsible deployment. ", "page_idx": 24}, {"type": "text", "text": "E Limitations and Future Work ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "While the AWT framework achieves commendable performance using a limited number of augmented image views, attaining higher performance necessitates an increase in the number of views. This expansion, however, inevitably leads to additional computational costs. Notably, the visual elements in different augmented views often contain duplicates, particularly when they are all derived from the same original image (e.g., random cropping). An interesting avenue for future research would be to explore strategies for reducing duplicates among batches to enhance inference speed. Furthermore, the application of test-time augmentation extends beyond our current scope and holds potential for various other domains, such as video understanding [131, 132], frame interpolation [133\u2013135], semantic segmentation [136\u2013138], and action detection [139]. Another intriguing area for future research involves the integration of efficient and controlled diffusion models [140, 141] to enhance the quality of visual augmentations. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See our results and discussion in Section 4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: See Appendix E. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See implementation details in Section 4 and Appendix C. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [No] ", "page_idx": 27}, {"type": "text", "text": "Justification: We will release our code. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: See experimental setting/details in Section 4. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: See Appendix B.1. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Section 4 and Appendix B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper conforms with the NeurIPS Code of Ethics. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: See Appendix D. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 28}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have cited the creators and mentioned the license of the assets (See Appendix C.3). ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve such experiments. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not involve such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]