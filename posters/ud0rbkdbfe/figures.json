[{"figure_path": "ud0RBkdBfE/figures/figures_1_1.jpg", "caption": "Figure 1: An illustration of SFL framework, and there are two major algorithms, i.e., SFL-V1 (left) and SFL-V2 (right) [27]. More discussions on SFL-V1 and SFL-V2 are given in Sec. 2.", "description": "The figure illustrates the architecture of Split Federated Learning (SFL), comparing two main variants: SFL-V1 and SFL-V2.  In both, a global model is split into client-side and server-side components. SFL-V1 has a separate server-side model for each client, updated in parallel. SFL-V2 uses a single server-side model updated sequentially.  The figure showcases the parallel processing of client-side models and the interaction between clients and servers.", "section": "1 Introduction"}, {"figure_path": "ud0RBkdBfE/figures/figures_7_1.jpg", "caption": "Figure 2: Impact of the choice of cut layer on SFL performance.", "description": "This figure shows the impact of the choice of cut layer on the performance of SFL-V1 and SFL-V2 algorithms.  The x-axis represents the training round, and the y-axis represents the accuracy.  There are four subfigures, each showing the results for a different combination of dataset (CIFAR-10 or CIFAR-100) and algorithm (SFL-V1 or SFL-V2). Within each subfigure, multiple lines represent different choices of cut layer (Lc), illustrating how the performance varies as the model split point changes. The results demonstrate how the choice of the cut layer influences the convergence speed and overall accuracy of the split federated learning.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_8_1.jpg", "caption": "Figure 3: Impact of data heterogeneity on SFL performance.", "description": "The figure shows the impact of data heterogeneity on the performance of two Split Federated Learning (SFL) algorithms (SFL-V1 and SFL-V2).  Different values of beta (\u03b2) in a Dirichlet distribution, representing varying levels of data heterogeneity among clients, are used. Beta values of 0.1, 0.5, 1, and \u221e represent increasingly homogeneous data, with \u221e representing IID data. The graphs illustrate the accuracy achieved by each SFL algorithm over training rounds for CIFAR-10 and CIFAR-100 datasets. The results demonstrate that as data heterogeneity increases (smaller \u03b2), the accuracy of both SFL algorithms decreases, indicating a more challenging training scenario.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_8_2.jpg", "caption": "Figure 4: Impact of client participation on SFL performance.", "description": "This figure shows the impact of client participation rate on the performance of SFL-V1 and SFL-V2 algorithms on CIFAR-10 and CIFAR-100 datasets.  Different levels of participation (q = 0.2, 0.5, 1) are tested, showing how the availability of clients affects the algorithms' convergence and accuracy.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_9_1.jpg", "caption": "Figure 5: Performance comparison on CIFAR-10.", "description": "This figure compares the performance of SFL-V1, SFL-V2, FL, and SL under different combinations of data heterogeneity (\u03b2 \u2208 {0.1, 0.5}) and the number of clients (N \u2208 {10, 50, 100}).  The results show that when data is mildly heterogeneous (\u03b2 = 0.5), SFL and FL have similar convergence rates and accuracy performance, with SL underperforming. However, under highly heterogeneous data (\u03b2 = 0.1), SFL-V2 outperforms both FL and SL, especially as the number of clients increases.  This highlights SFL-V2's effectiveness in handling highly heterogeneous data with many clients.", "section": "4.3 Comparison among SFL, FL, and SL"}, {"figure_path": "ud0RBkdBfE/figures/figures_13_1.jpg", "caption": "Figure 1: An illustration of SFL framework, and there are two major algorithms, i.e., SFL-V1 (left) and SFL-V2 (right) [27]. More discussions on SFL-V1 and SFL-V2 are given in Sec. 2.", "description": "This figure illustrates the architecture of the Split Federated Learning (SFL) framework.  It shows two variants of SFL: SFL-V1 and SFL-V2.  Both versions involve splitting the global model into client-side and server-side components.  SFL-V1 has a separate server-side model for each client, while SFL-V2 uses a single server-side model. The figure highlights the parallel training at the client-side and the interaction between clients and servers (fed server and main server).", "section": "1 Introduction"}, {"figure_path": "ud0RBkdBfE/figures/figures_59_1.jpg", "caption": "Figure 6: Performance comparison on CIFAR-10.", "description": "This figure compares the performance of SFL-V1, SFL-V2, FL, SL, FedProx, and FedOpt on CIFAR-10 dataset.  Two subfigures are shown, one for \u03b2 = 0.1 and N = 10 (a), and the other for \u03b2 = 0.1 and N = 100 (b). Each line represents the accuracy over training rounds for a specific algorithm. The results show that SFL-V2 generally outperforms the other methods, especially when the number of clients (N) increases, indicating its effectiveness for heterogeneous data in large-scale federated learning.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_59_2.jpg", "caption": "Figure 7: Impact of local iteration on SFL performance.", "description": "The figure shows the impact of the number of local epochs (E) on the performance of split federated learning (SFL) algorithms (SFL-V1 and SFL-V2).  The x-axis represents the training round, and the y-axis represents the accuracy.  Four subfigures show results on CIFAR-10 (SFL-V1 and SFL-V2) and CIFAR-100 (SFL-V1 and SFL-V2), respectively. Each subfigure plots three curves, each corresponding to different values of E (E=2, E=5, and E=10). The results demonstrate that SFL generally converges faster with a larger E, highlighting the benefit of SFL in practical distributed systems. ", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_60_1.jpg", "caption": "Figure 2: Impact of the choice of cut layer on SFL performance.", "description": "The figure shows the impact of the choice of cut layer on the performance of split federated learning (SFL). The x-axis represents the training round, and the y-axis represents the accuracy. There are four lines in each subplot, each representing a different choice of cut layer (Lc = 1, 2, 3, 4). The subplots are arranged in a 2x2 grid, with each row representing a different dataset (CIFAR-10 and CIFAR-100) and each column representing a different SFL algorithm (SFL-V1 and SFL-V2). The results show that the performance of SFL generally increases as the cut layer moves towards the later layers, although the optimal cut layer may vary depending on the dataset and algorithm.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_60_2.jpg", "caption": "Figure 2: Impact of the choice of cut layer on SFL performance.", "description": "This figure shows the impact of the choice of cut layer (Lc) on the performance of Split Federated Learning (SFL). The x-axis represents the training rounds, and the y-axis represents the accuracy.  Four different cut layers (Lc = 1, 2, 3, 4) are tested with both SFL-V1 and SFL-V2 algorithms on CIFAR-10 and CIFAR-100 datasets.  The results demonstrate how the choice of the cut layer impacts the convergence and accuracy of SFL.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_61_1.jpg", "caption": "Figure 2: Impact of the choice of cut layer on SFL performance.", "description": "This figure visualizes the impact of the cut layer (Lc) on the performance of Split Federated Learning (SFL). The cut layer determines where the global model is divided into client-side and server-side models. The figure shows the training accuracy for both SFL-V1 and SFL-V2 algorithms on the CIFAR-10 and CIFAR-100 datasets across different cut layers (Lc = 1, 2, 3, 4).  The results indicate how the choice of the cut layer influences the algorithm's ability to converge and achieve high accuracy. It highlights the performance differences between SFL-V1 and SFL-V2 under varying data heterogeneity and model splitting strategies.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_61_2.jpg", "caption": "Figure 4: Impact of client participation on SFL performance.", "description": "This figure visualizes the effect of varying client participation rates (q = {0.2, 0.5, 1}) on the performance of SFL-V1 and SFL-V2 across different datasets (CIFAR-10 and CIFAR-100).  Each subfigure shows the training accuracy over 200 rounds.  The results demonstrate that reducing client participation (lower q values) leads to slower convergence and lower overall accuracy for both SFL algorithms and across both datasets.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_61_3.jpg", "caption": "Figure 2: Impact of the choice of cut layer on SFL performance.", "description": "This figure visualizes the impact of different cut layer choices (Lc = {1, 2, 3, 4}) on the performance of the two SFL algorithms (SFL-V1 and SFL-V2).  The x-axis represents the training round, and the y-axis represents the accuracy. Separate subplots are provided for each algorithm and for two datasets (CIFAR-10 and CIFAR-100).  The results show how the choice of cut layer affects the convergence speed and final accuracy of the algorithms.", "section": "4 Experimental Results"}, {"figure_path": "ud0RBkdBfE/figures/figures_61_4.jpg", "caption": "Figure 5: Performance comparison on CIFAR-10.", "description": "The figure compares the performance of SFL-V1, SFL-V2, FL, and SL on CIFAR-10 under different combinations of data heterogeneity (\u03b2 \u2208 {0.1, 0.5}) and cohort sizes (N \u2208 {10, 50, 100}).  It shows that when data is mildly heterogeneous (\u03b2 = 0.5), SFL and FL perform similarly, while SL underperforms. However, under highly heterogeneous data (\u03b2 = 0.1) and large client numbers, SFL-V2 outperforms FL and SL, highlighting its effectiveness in handling client drift and catastrophic forgetting issues.", "section": "4 Experimental Results"}]