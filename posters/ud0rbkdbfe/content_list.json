[{"type": "text", "text": "Convergence Analysis of Split Federated Learning on Heterogeneous Data ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Pengchao Han \\* Guangdong University of Technology, China hanpengchao@gdut.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Chao Huang \\* Montclair State University, USA huangch@montclair.edu ", "page_idx": 0}, {"type": "text", "text": "Geng Tian Southern University of Science and Technology, China 12332463@mail.sustech.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Ming Tang t Xin Liu Southern University of Science and Technology, China University of California, Davis, USA tangm3@sustech.edu.cn xinliu@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of federated learning (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and ", "page_idx": 0}, {"type": "text", "text": "$O(1/\\sqrt[3]{T})$ ,respectively,where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and the scenario where some clients may be unavailable during training. Experimental experiments validate our theoretical results and show that SFL outperforms FL and split learning (SL) when data is highly heterogeneous across a large number of clients. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1.1 Motivation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Federated learning (FL) [18, 9] allows distributed clients to train a global machine learning model collaboratively without sharing raw data. FL leverages the parallel computing capabilities of clients to enhance model training efficiency. However, FL is usually computationally intensive. Clients need to train the entire global model multiple times, which can be infeasible for resource-constrained edge devices. This challenge is further exacerbated as the trend towards increasingly larger model architectures demands more substantial resources [1]. Moreover, FL suffers from the client drift problem when clients\u2019 data distributions are heterogeneous, aka non-identically and independently distributed (non-IID). A large number of studies have proposed algorithms to address the client drift issue, e.g., [15, 10, 14, 25]. ", "page_idx": 0}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/dfd8283f6a925f52cbdf650f13230ef538f9106c8868987bf8142b0a8de78e8d.jpg", "img_caption": ["Figure 1: An illustration of SFL framework, and there are two major algorithms, i.e., SFL-V1 (left) and SFL-V2 (right) [27]. More discussions on SFL-V1 and SFL-V2 are given in Sec. 2. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Split learning (SL) [28] is another distributed approach. By splitting the model across clients and a main server, SL can substantially reduce the computational workload on edge devices. Moreover, recent studies in [34, 17] show that SL can outperform FL when data is highly heterogeneous. However, SL's sequential training among clients can lead to high latency in each training round and potential performance loss (e.g., caused by catastrophic forgetting), which impedes its practical applicability in real-world distributed systems. ", "page_idx": 1}, {"type": "text", "text": "In light of above challenges, Thapa et. al in [27] proposed split federated learning (SFL) as a hybrid approach that synergizes the strengths of both FL and SL. SFL combines parallel training of FL with partial model training of SL. They proposed two major SFL algorithms: SFL-V1 and SFL-V2. An illustration of these SFL algorithms are shown in Fig. 1. Specifically, the global model (to be trained) is first split at a cut layer into two parts: a client-side model and a server-side model. Then, the clients are responsible for training only the client-side model under the coordination of a fed server (similar to FL). Another server, known as the main server, is tasked with training the server-side model by collaborating with the clients (similar to SL). SFL aims to leverage parallel processing to reduce latency, while benefiting from the reduced computational workloads and enhanced data heterogeneity handling of SL. ", "page_idx": 1}, {"type": "text", "text": "Following [27], there has been an emerging volume of empirical studies on SFL. e.g., [22, 21, 3, 23, 8, 31, 5]. However, a convergence analysis of SFL is missing in the literature, and this paper aims to provide a comprehensive convergence analysis under different conditions. Convergence theory is crucial for understanding the learning performance of SFL, particularly in the context of heterogeneous data and partial participation scenarios. In practical distributed systems, clients are prone to have different data distributions. Moreover, not all clients may be active or available at all times. These two issues can significantly affect the learning performance of SFL. We aim to provide convergence guarantees for SFL on heterogeneous data (under both full and partial participation). We further compare the results to FL and SL, which provides insights into the practical deployment of various distributed approaches. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Convergence theories of FL and SL. There are many convergence results on FL. Most studies focus on data heterogeneity, e.g., [30, 16, 11, 10, 12]. Some studies look at partial participation, e.g.. [35, 29, 26]. There are also convergence results on Mini-Batch SGD, e.g., [24, 33, 32], where [33] argued that the key difference between FL and Mini-Batch SGD is the communication frequency. ", "page_idx": 1}, {"type": "text", "text": "To our best knowledge, there is only one recent study [17] discussing the convergence of SL. The major difference to SL analysis lies in the sequential training manner across clients, while SFL clients perform parallel training. ", "page_idx": 1}, {"type": "text", "text": "1.3  Challenges and Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Challenges of SFL convergence analysis. When data is homogeneous (IID) across clients, the convergence theory in [12] (mainly developed for FL) can be applied to SFL. When data is heterogeneous, however, the theory cannot be directly applied due to the client drift problem. The challenge is intensified with clients\u2019 partial participation, which induces bias in the training process. Despite that prior FL theories have handled data heterogeneity [16] and partial participation [29], SFL convergence analysis imposes unique challenges due to the dual-paced model aggregation and model updates at the client-side and server-side. More specifically, ", "page_idx": 2}, {"type": "text", "text": "Dual-paced model aggregation in SFL-Vl: In SFL-V1, the main server maintains one server-side model for each client, and it periodically aggregates the server-side models. When the main server aggregates its models at the same frequency as the clients, the analysis is the same to that of FL. However, FL analysis cannot be applied when aggregations occur at different frequencies, and it is challenging to analyze the impact of such discrepancy on SFL convergence. ", "page_idx": 2}, {"type": "text", "text": "Dual-paced model updates in SFL-V2: In SFL-V2, the main server only maintains one version of server-side model. The clients update the client-side models in a parallel manner while the main server updates the server-side model in a sequential fashion. Hence, each client's local update depends on the randomness of the previous clients who have interacted with the main server. While [17] handled sequential client training, their theory cannot be applied to SFL-V2 as they did not consider the aggregation of client-side models. This makes our analysis more challenging than FL and SL. ", "page_idx": 2}, {"type": "text", "text": "Contributions. We summarize our contributions as follows: ", "page_idx": 2}, {"type": "text", "text": "\u00b7 We provide the first comprehensive convergence analysis of SFL. The analysis is more challenging than prior FL analysis due to the dual-paced model aggregation and model updates. To this end, we derive a key decomposition result (Proposition 3.5) that enables us to analyze the convergence from the server-side and client-side separately. \u00b7 Based on the decomposition result, we prove that the convergence guarantees of both SFLV1 and SFL-V2 are ${\\cal O}(1/T)$ for strongly convex objective and $O(1/\\sqrt[3]{T})$ for general convex objective,where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and more practical scenarios where some clients may be unavailable during training. \u00b7 We conduct simulations on various datasets. We show that the results are consistent with our theories. We further show two surprising results: (i) SFL achieves a better performance when clients maintain a larger portion of the global model; (ii) SFL-V2 outperforms FL and SL when clients have highly heterogeneous data and the number of client is large. ", "page_idx": 2}, {"type": "text", "text": "The rest of the paper is organized as follows. Sec. 2 formulates the SFL model. Sec. 3 presents the convergence results for SFL. We conduct experiments in Sec. 4 and conclude in Sec. 5. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider a set of clients $\\mathcal{N}=\\{1,2,\\cdot\\cdot\\cdot,N\\}$ , where each client $n\\in\\mathcal N$ has a local private dataset $\\mathcal{D}_{n}$ of size $D_{n}=|D_{n}|$ . Suppose the global model parameterized by $\\textbf{\\em x}$ has $L$ layers. In SFL, the global model is split at the $L_{c}$ -th layer (i.e., the cut layer) into two segments: a client-side model $x_{c}$ (from the first layer to layer $L_{c.}$ ) and a server-side model $\\pmb{x}_{s}$ (from layer $L_{c}+1$ to layer $L$ ) where $\\pmb{x}=[\\pmb{x}_{c};\\pmb{x}_{s}]$ . Let $\\mathbf{\\boldsymbol{x}}_{c,n}$ denote the local client-side model of client $n$ . The clients train models with the help of two servers: (i) fed server, which periodically aggregates clients? local models $\\pmb{x}_{c,n}$ (similar to FL), and (ii) main server, who trains the server-side model $\\pmb{x}_{s}$ . In this work, we consider two major SFL algorithms: SFL-V1 and SFL-V2 [27]. In SFL-V1, the main server maintains a separate server-side model $\\mathbf{\\boldsymbol{x}}_{s,n}$ corresponding to each client $n$ . In comparison, in SFL-V2, the main server only maintains one model $\\pmb{x}_{s}$ ", "page_idx": 2}, {"type": "text", "text": "Let $F_{n}(\\mathbf{x};\\zeta_{n})$ denote the loss of model $\\textbf{\\em x}$ over client $n$ 's mini-batch instance $\\zeta_{n}$ , which is randomly sampled from client $n$ 's dataset $\\mathcal{D}_{n}$ . Let $F_{n}(\\pmb{x})\\triangleq\\mathbb{E}_{\\zeta_{n}\\sim\\mathcal{D}_{n}}[F_{n}(\\pmb{x};\\zeta_{n})]$ denote the expected loss of model $\\textbf{\\em x}$ over client $n$ 's dataset. The goal of SFL is to minimize the expected loss of the model $\\textbf{\\em x}$ ", "page_idx": 2}, {"type": "text", "text": "over the datasets of all clients: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}}f(\\pmb{x})=\\sum_{n=1}^{N}a_{n}F_{n}\\left(\\pmb{x}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Where $a_{n}~\\in~[0,1]$ is the weight of client $n$ satisfying $\\textstyle\\sum_{n\\in{\\mathcal N}}a_{n}\\ =\\ 1$ Typically, $\\textstyle a_{n}=$ $D_{n}/\\sum_{n^{\\prime}\\in\\mathcal{N}}D_{n^{\\prime}}$ ,where aclient with alarger data size is assigned alarger weight [3]. ", "page_idx": 3}, {"type": "text", "text": "2.2 Algorithm Description ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We provide a brief description of SFL. Refer to Appendix $\\mathbf{B}$ for a more detailed discussion. SFL takes a total number of $T$ rounds to solve (1). At the beginning of each round $t$ , clients download the recent global client-side model from the fed server, where the model is an aggregated version of the client-side models of the clients from the previous round $t-1$ . Each round $t$ contains two stages: ", "page_idx": 3}, {"type": "text", "text": "Stage 1: model training. Clients and the main server train the full global model for $\\tau$ iterations in each round. In each iteration $i<\\tau$ , there are three steps: ", "page_idx": 3}, {"type": "text", "text": "Step 1: client forward propagation. Each client $n$ samples a mini-batch of data $\\zeta_{n}^{t,i}$ from $\\mathcal{D}_{n}$ computes the intermediate features (e.g., activation values at the cut layer) over its current model $\\pmb{x}_{c,n}^{t,i}$ , and sends the activation to the main server. The clients perform forward propagation in parallel. ", "page_idx": 3}, {"type": "text", "text": "Step 2: main server training. Upon receiving the activation of each client $n$ ", "page_idx": 3}, {"type": "text", "text": "\u00b7 SFL-V1: the main server computes the loss using the current server-side mdel $\\pmb{x}_{s,n}^{t,i}$ It then computes the gradients over $\\pmb{x}_{s,n}^{t,i}$ to update the model. It also computes the gradient over the activation at the cut layer, and sends it to client $n$   \n\u00b7 SFL-V2: the main server computes the loss $F_{n}(\\left\\{\\mathbf{x}_{c,n}^{t,i},\\mathbf{x}_{s}^{t,i}\\right\\})$ , based on which it then updates the server-side model $\\pmb{x}_{s}^{t,i}$ . It also computes and sends the gradient over activation at the cut layer to client $n$ . Note that the main server sequentially interacts with the clients in a randomized order. ", "page_idx": 3}, {"type": "text", "text": "Step 3: client backward propagation. Receiving gradient at the cut layer, each client $n$ computes the client-side gradient using the chain rule, and then updates its model $\\bar{\\mathbf{x}}_{c,n}^{t,i}$ ", "page_idx": 3}, {"type": "text", "text": "Stage 2: model aggregation. Model aggregation can occur for both client-side and server-side models. For the client side, after $\\tau$ iterations of model training (i.e., at the end of round $t$ ),eachclient sends its current client-side model to the fed server. The fed server aggregates the clients\u2019 models (e.g., weighted averaging), which will be downloaded in the next round $t+1$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\pmb x}_{c}^{t+1}\\gets\\sum_{n\\in\\mathcal{N}}a_{n}{\\pmb x}_{c,n}^{t,\\tau}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For the server side, (i) in SFL-V1, after $\\tilde{\\tau}$ iterations of training, the main server aggregates all serverside models. Note that $\\tilde{\\tau}$ does not necessarily need to equal $\\tau$ , but when equality holds, SFL-V1 can be regarded as FL (despite the model splitting). (i) In SFL-V2, no aggregation occurs since the main server only maintains one model. ", "page_idx": 3}, {"type": "text", "text": "2.3  Client Participation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider two cases: (i) full participation where all clients are available during training. This can model the scenarios where clients are organizations or companies who likely have sufficient computation and communication resources [7]; (i) partial participation where some clients may be unavailable during training. This can model the cases where clients are edge devices (e.g., mobile phones) that are usually resource-constrained and may be disconnected from the SFL process. ", "page_idx": 3}, {"type": "text", "text": "To model partial participation, we consider independent participation probabilities for each client, allowing for arbitrary and heterogeneous participation probabilities. Specifically, we use $q_{n}\\in[0,1]$ to denote client $n$ 's participation level (or probability), and $\\pmb{q}\\,=\\,(\\Bar{q_{n}},n\\in\\mathcal{N})$ . If $q_{n}\\,=\\,1$ , client $n$ participates in every_ round of SFL with probability one. If $q_{n}<1$ , client $n$ is unavailable in some rounds. Denote $\\stackrel{\\triangledown^{t}}{\\mathcal{P}^{t}}({\\boldsymbol{q}})$ as the set of participating clients in round $t$ . In the presence of partial participation, we need to modify (2) (and the potential server-side aggregation) to offset the incurred bias: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\pmb x}_{c}^{t+1}\\gets\\sum_{n\\in\\mathcal{P}^{t}({\\pmb q})}\\frac{a_{n}}{q_{n}}{\\pmb x}_{c,n}^{t,\\tau}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3  Convergence Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first make technical assumptions in Sec. 3.1. Then, we present a key technical result in Sec. 3.2 to support the SFL convergence analysis. Finally, we provide the convergence results under full participation and partial participation in Sec. 3.3 and Sec. 3.4, respectively. ", "page_idx": 4}, {"type": "text", "text": "3.1 Assumptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We start with some conventional assumptions for convergence analysis in the FL literature. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1. $S$ -Smoothness) Each client $n$ 's loss function $F_{n}$ is $S$ -smooth. That is, for all $\\pmb{x},\\pmb{y}\\in\\bar{\\mathbb{R}}^{d}$ \uff0c ", "page_idx": 4}, {"type": "equation", "text": "$$\nF_{n}(\\pmb{y})\\leq F_{n}(\\pmb{x})+\\langle\\nabla F_{n}(\\pmb{x}),\\pmb{y}-\\pmb{x}\\rangle+\\frac{S}{2}\\|\\pmb{y}-\\pmb{x}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The smoothness assumption holds for many loss functions in, for example, logistic regression, softmax classifier, and $l_{2}$ -norm regularized linear regression [16]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.2. (Unbiased and bounded stochastic gradients with bounded variance) The stochastic gradients $g_{n}(\\cdot)$ $F_{n}\\left(\\cdot\\right)$ is unbiased with thevarianceboundedby $\\sigma_{n}^{2}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\zeta_{n}\\sim\\mathcal{D}_{n}}\\left[\\pmb{g}_{n}\\left(\\pmb{x},\\zeta_{n}\\right)\\right]=\\nabla F_{n}\\left(\\pmb{x}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\zeta_{n}\\sim\\mathcal{D}_{n}}\\left[\\left\\|g_{n}\\left(\\pmb{x},\\zeta_{n}\\right)-\\nabla F_{n}\\left(\\pmb{x}\\right)\\right\\|^{2}\\right]\\leq\\sigma_{n}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 3.3. (Bounded gradients) The expected squared norm of stochastic gradients is bounded by $G^{2}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\zeta_{n}\\sim\\mathcal{D}_{n}}\\left\\|\\pmb{g}_{n}\\left(\\pmb{x},\\zeta_{n}\\right)\\right\\|^{2}\\leq G^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The value of $\\sigma_{n}$ measures the level of stochasticity. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.4. (Heterogeneity) There exists an $\\epsilon^{2}$ such that the divergence between local and global gradients is bounded by e?. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\nabla F_{n}\\left(\\mathbf{\\boldsymbol{x}}\\right)-\\nabla f\\left(\\mathbf{\\boldsymbol{x}}\\right)\\right\\|^{2}\\leq\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "A larger $\\epsilon^{2}$ indicates a larger degree of data heterogeneity. ", "page_idx": 4}, {"type": "text", "text": "3.2 Decomposition ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As discussed in Sec. 1.3, analyzing the performance bound of SFL can be more challenging than that of conventional FL counterparts due to the dual-paced model aggregation and model updates. To address this challenge, we decompose the convergence analysis into the server-side and client-side updates, respectively. We give the decomposition below. ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.5. (Convergence decomposition) Let $\\pmb{x}^{*}\\triangleq[\\pmb{x}_{c}^{*};\\pmb{x}_{s}^{*}]$ denote the optimal global model that minimizes $f(\\cdot)$ and $\\pmb{x}^{T}\\triangleq[\\pmb{x}_{c}^{T};\\pmb{x}_{s}^{T}]$ is the global model obtained after $T$ rounds of SFL training. Under Assumption 3.1, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\pmb{x}^{T})\\right]-f(\\pmb{x}^{*})\\le\\frac{S}{2}\\left(\\mathbb{E}||\\pmb{x}_{s}^{T}-\\pmb{x}_{s}^{*}||^{2}+\\mathbb{E}||\\pmb{x}_{c}^{T}-\\pmb{x}_{c}^{*}||^{2}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The proof is given in Appendix C.4. Proposition 3.5 is particularly useful. It shows that despite the challenging dual-paced updates, to bound the SFL performance gap, it suffices to separately bound the gap at the server-side and client-side models. Note that our decomposition can be easily applied to other distributed approaches such as SL. In addition, such a decomposition is not necessarily loose, as our derived bounds for SFL achieve the same order as in FL (see Appendix H.2 for details). ", "page_idx": 4}, {"type": "text", "text": "3.3  Results under Full Participation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Built upon Proposition 3.5, we first present the convergence results under full participation. For convenience, define ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{I^{\\mathrm{err}}\\triangleq\\left\\Vert x^{0}-x^{*}\\right\\Vert^{2},\\quad\\gamma\\triangleq8S/\\mu-1,\\quad\\tau_{\\mathrm{min}}\\triangleq\\operatorname*{min}\\{\\tau,\\tilde{\\tau}\\},\\quad\\tau_{\\mathrm{max}}\\triangleq\\operatorname*{max}\\{\\tau,\\tilde{\\tau}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and let $\\eta^{t}$ represent the learning rate at round $t$ .Let $f^{*}$ denotes the optimal global loss, i.e., $f^{*}\\triangleq f(x^{*})$ . All results are obtained based on Assumptions 3.1-3.4. The convergence results for SFL-V1 and SFL-V2 are summarized in Theorems 3.6 and 3.7, respectively1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.6. ( SFL-V1: full participation) ", "page_idx": 5}, {"type": "text", "text": "$\\mu$ strongly coex: LtAsumtions 3.1 -33hold and $\\begin{array}{r}{\\eta^{t}=\\frac{4}{\\mu\\tilde{\\tau}(\\gamma+t)}}\\end{array}$ for client-side model and $\\begin{array}{r}{\\eta^{t}=\\frac{4}{\\mu\\tau(\\gamma+t)}}\\end{array}$ \u03bcr(\\*+t) for server-side model,. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(x^{T})\\right]-f^{*}\\leq\\frac{8S N\\sum_{n=1}^{N}\\ensuremath{a_{n}}^{2}\\big(2\\sigma_{n}^{2}+G^{2}\\big)}{\\mu^{2}\\left(\\gamma+T\\right)}+\\frac{768S^{2}\\sum_{n=1}^{N}\\ensuremath{a_{n}}\\big(2\\sigma_{n}^{2}+G^{2}\\big)}{\\mu^{3}\\left(\\gamma+T\\right)\\left(\\gamma+1\\right)}+\\frac{S(\\gamma+1)I^{\\mathrm{err}}}{2(\\gamma+T)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "General convex: Let Assumtions 3.1- 3.3 hold, and n \u2264 2Sx ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\boldsymbol{x}^{T}\\right)\\right]-f^{*}\\le\\!\\frac{S I^{\\mathrm{err}}}{2(T+1)}+\\frac12\\left(\\frac{(\\tilde{\\tau}^{2}+\\tau^{2})I^{\\mathrm{err}}N}{\\tau_{\\mathrm{min}}^{2}(T+1)}\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac12}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\frac1{2}\\left(\\frac{24(\\tilde{\\tau}^{2}+\\tau^{2})S I^{\\mathrm{err}}}{\\tau_{\\mathrm{min}}^{2}(T+1)}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac13}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Non-convex: Let Assumptions 3.1 3.2, and 3.4 hold, and $\\begin{array}{r}{\\eta^{t}\\leq\\operatorname*{min}\\bigg\\{\\frac{1}{16S\\tau_{\\mathrm{max}}},\\frac{\\tau_{\\mathrm{min}}}{8S N\\tau_{\\mathrm{max}}^{2}\\sum_{n=1}^{N}a_{n}^{2}}\\bigg\\},}\\end{array}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\eta^{t}\\mathbb{E}\\Big[\\big\\lVert\\nabla_{x}f\\left(x^{t}\\right)\\big\\rVert^{2}\\Big]\\leq\\frac{4}{T\\tau_{\\mathrm{min}}}\\big(f\\left(x^{0}\\right)-f^{*}\\big)+\\frac{8N S(\\tau^{2}+\\tilde{\\tau}^{2})}{T\\tau_{\\mathrm{min}}}\\sum_{n=1}^{N}a_{n}^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)\\sum_{t=0}^{T-1}\\left(\\eta^{t}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Theorem 3.7. ( SFL-V2: full participation) ", "page_idx": 5}, {"type": "text", "text": "$\\mu$ strongly convex: Let Assumptions 3.1 - 3.3 hold, and $\\begin{array}{r}{\\eta^{t}=\\frac{4}{\\mu\\tilde{\\tau}(\\gamma+t)}}\\end{array}$ for client-side model and $\\begin{array}{r}{\\eta^{t}=\\frac{4}{\\mu\\tau(\\gamma+t)}}\\end{array}$ \u03bcr(\\*+t) for server-side model, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\natural\\big[f(x^{T})\\big]-f^{\\ast}\\leq\\frac{8S N\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{2}\\left(\\gamma+T\\right)}+\\frac{768S^{2}\\sum_{n=1}^{N}(a_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma+T\\right)\\left(\\gamma+1\\right)}+\\frac{S(\\gamma+1)I^{\\mathrm{err}}}{2(\\gamma+T)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "General convex: Let Assumptions 3.1 - 3.3 hold, and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\natural\\big[f(\\mathbf{x}^{T})\\big]-f^{*}\\leq\\frac{S I^{\\mathrm{err}}}{2(T+1)}+\\frac{1}{2}\\left(\\frac{N I^{\\mathrm{err}}}{T+1}\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac{1}{2}}+\\frac{1}{2}\\left(\\frac{24S I^{\\mathrm{err}}}{T+1}\\sum_{n=1}^{N}(a_{n}+1)\\big(2\\sigma_{n}^{2}+G^{2}\\big)\\right)^{\\frac{1}{3}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Non-convex: Let Assumptions 3.1, 3.2, and 3.4 hold, and $\\begin{array}{r}{\\eta^{t}\\le\\operatorname*{min}\\left\\{\\frac{1}{16S\\tau},\\frac{1}{8S N^{2}\\tau}\\right\\}}\\end{array}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\eta^{t}\\mathbb{E}\\Big[\\big\\lVert\\nabla_{x}f\\left(\\pmb{x}^{t}\\right)\\big\\rVert^{2}\\Big]\\leq\\frac{4}{T\\tau}\\big(f\\left(\\pmb{x}^{0}\\right)-f^{*}\\big)+\\frac{8N S\\tau}{T}\\sum_{n=1}^{N}(a_{n}^{2}+1)\\big(\\sigma_{n}^{2}+\\epsilon^{2}\\big)\\sum_{t=0}^{T-1}\\left(\\eta^{t}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "'Following many existing works in FL (e.g., [1o]), we consider $\\mathbb{E}\\left[f(\\pmb{x}^{T})\\right]\\;\\;-\\;\\;f^{*}$ and $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=0}^{T-1}\\eta^{t}\\mathbb{E}[\\|\\nabla_{x}f(\\pmb{x}^{t})\\|^{2}]}\\end{array}$ a the performane metriesfor strongly convex and n-convexojetives respectively. ", "page_idx": 5}, {"type": "text", "text": "Proofs of Theorems 3.6-3.7 are given in Appendices D-E, respectively. We summarize the key findings below. ", "page_idx": 6}, {"type": "text", "text": "Convergence rate. The convergence bounds of both SFL-V1 and SFL-V2 achieve an order of ${\\cal O}(1/T)$ on strongly convex (and non-convex) objectives. For general convex objectives, the convergence rate becomes $O(1/\\sqrt[3]{T})$ .2 Note that our bounds match the existing bounds for FL and SL (in terms of the orderof $T$ ) on heterogeneous data for strongly convex objectives. For a more detailed comparison, please refer to Appendix H.2.3 ", "page_idx": 6}, {"type": "text", "text": "Impact of data heterogeneity. The convergence bounds increase as the level of data heterogeneity increases. For example, in (13), the bound increases in $\\epsilon^{2}$ (see Assumption 3.4). This means that SFL tends to perform worse when clients\u2019 data are more heterogeneous, which is a commonly observed phenomenon in distributed learning, e.g., FL. ", "page_idx": 6}, {"type": "text", "text": "Choice of learning rate. One should use a smaller learning rate when the number of local iteration $\\tau$ increases. This bears a similar spirit to [16]. In addition, our results indicate that a proper choice of constant learning rate sufices for SFL convergence. It would be an interesting direction to investigate whether diminishing learning rates are able to achieve faster convergence. ", "page_idx": 6}, {"type": "text", "text": "Comparison between SFL-V1 and SFL-V2. The convergence results between the two SFL versions are very similar, except that $a_{n}^{2}$ (and $a_{n}$ ) in SFL-V1 are replaced by $a_{n}^{2}+1$ (and $a_{n}+1)$ inSFL-V2. See (11) and (14) for an inspection. We will show in Sec. 4 that SFL-V1 and SFL-V2 achieve similar accuracy (except under highly heterogeneous data). ", "page_idx": 6}, {"type": "text", "text": "3.4   Results under Partial Participation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now, we present the results under partial participation. ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.8. ( SFL-V1: partial participation) ", "page_idx": 6}, {"type": "text", "text": "$\\mu$ $3.1\\cdot3.3\\$ $\\begin{array}{r}{\\eta^{t}=\\frac{4}{\\mu\\tilde{\\tau}(\\gamma+t)}}\\end{array}$ $\\begin{array}{r}{\\eta^{t}=\\frac{4}{\\mu\\tau(\\gamma+t)}}\\end{array}$ ur(\\*+t) for server-side model, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\pmb{x}^{T})\\right]-f^{*}\\leq\\frac{8S N\\sum_{n=1}^{N}a_{n}^{2}\\Big(2\\sigma_{n}^{2}+G^{2}+\\frac{G^{2}}{q_{n}}\\Big)}{\\mu^{2}\\left(\\gamma+T\\right)}+\\frac{768S^{2}\\sum_{n=1}^{N}a_{n}\\big(2\\sigma_{n}^{2}+G^{2}\\big)}{\\mu^{3}\\left(\\gamma+T\\right)\\left(\\gamma+1\\right)}+\\frac{S(\\gamma+1)I^{\\mathrm{err}}}{2(\\gamma+T)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "General convex: Let Assumptions 3.1 - 3.3 hold, and n \u2264 2Smx ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mathbb{E}\\left[f\\left({\\pmb x}^{T}\\right)\\right]-f^{*}\\leq\\frac{S I^{\\mathrm{err}}}{2(T+1)}+\\frac{1}{2}\\left(\\frac{(\\tilde{\\tau}^{2}+\\tau^{2})I^{\\mathrm{err}}N}{\\tau_{\\mathrm{min}}^{2}(T+1)}\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}+\\frac{G^{2}}{q_{n}}\\right)\\right)^{\\frac12}}}\\\\ {{\\displaystyle+\\frac{1}{2}\\left(\\frac{24(\\tilde{\\tau}^{2}+\\tau^{2})S I^{\\mathrm{err}}}{\\tau_{\\mathrm{min}}^{2}(T+1)}\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac13}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Non-coex: LAos.a34hold d\u2264m 8N Nan (2+2)(mt)\u00b2. (19) t=0 TTmin n=1 In t=0 ", "page_idx": 6}, {"type": "text", "text": "Theorem 3.9. ( SFL-V2: partial participation) ", "page_idx": 6}, {"type": "text", "text": "$\\mu$ strongly convex: Let Assumptions $3.1\\cdot3.3\\$ hold, and $\\begin{array}{r}{\\eta^{t}=\\frac{4}{\\mu\\tilde{\\tau}(\\gamma+t)}}\\end{array}$ for client-side model and $\\begin{array}{r}{\\eta^{t}=\\frac{4}{\\mu\\tau(\\gamma+t)}}\\end{array}$ ur(+t) for server-side model, ", "page_idx": 6}, {"type": "equation", "text": "$$\nz[f\\left(x^{T}\\right)]-f^{*}\\le\\frac{8S N\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}+\\frac{G^{2}}{q_{n}}\\right)}{\\mu^{2}\\left(\\gamma+T\\right)}+\\frac{768S^{2}\\sum_{n=1}^{N}(a_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma+T\\right)\\left(\\gamma+1\\right)}+\\frac{S(\\gamma+1)I^{\\mathrm{erf}}\\left(a_{n}+1\\right)}{2(\\gamma+T)}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/1b421c92756fa6811139825abaf940fc1df5c4ae4dfa34c49deab2fcda85ca9f.jpg", "img_caption": [], "img_footnote": ["(a) SFL-V1 on CIFAR-10. (b) SFL-V2 on CIFAR-10. (c) SFL-V1 on CIFAR-100.(d) SFL-V2 on CIFAR-100. "], "page_idx": 7}, {"type": "text", "text": "General convex: Let Assumptions 3.1 - 3.3 hold, and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\mathbb{E}\\left[f\\left({\\pmb x}^{T}\\right)\\right]-f^{*}\\leq\\frac{S I^{\\mathrm{err}}}{2(T+1)}+\\frac{1}{2}\\left(\\frac{N I^{\\mathrm{err}}}{T+1}\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}+\\frac{G_{n}^{2}}{{q_{n}}}\\right)\\right)^{\\frac{1}{2}}}}\\\\ {{\\displaystyle+\\,\\frac{1}{2}\\left(\\frac{24S I^{\\mathrm{err}}}{T+1}\\sum_{n=1}^{N}(a_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac{1}{3}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Non-convex: Let Assumptions 3.1, 3.2, and 3.4 hold, and $\\begin{array}{r}{\\eta^{t}\\leq\\operatorname*{min}\\bigg\\{\\frac{1}{16S\\tau},\\frac{1}{8S N^{2}\\tau\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}}\\bigg\\},}\\end{array}$ $\\frac{1}{T}\\sum_{t=0}^{T-1}\\eta^{t}\\mathbb{E}\\left[\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{4}{T\\tau}\\left(f\\left(x_{0}\\right)-f^{*}\\right)+\\frac{8N S\\tau}{T}\\sum_{n=1}^{N}\\frac{a_{n}^{2}+1}{q_{n}}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)\\sum_{t=0}^{T-1}\\left(\\eta^{t}\\right)^{2}.$ (22) ", "page_idx": 7}, {"type": "text", "text": "The proofs are given in Appendices F-G. ", "page_idx": 7}, {"type": "text", "text": "Impact of partial participation. In practical cross-device settings, some clients may not participate in all rounds of training, i.e., $q_{n}<1$ for some $n$ . This brings an additional term $G^{2}/\\dot{q}_{n}$ to theconvergence bound (e.g., see (12) and (18)), meaning that partial participation worsens SFL performance. This is also observed in FL literature (e.g., [29]) and is consistent with our experimental results. ", "page_idx": 7}, {"type": "text", "text": "4   Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1  Setup", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We conduct experiments on CIFAR-10 and CIFAR-100 [13].4 To simulate data heterogeneity, we adopt the widely used Dirichlet distribution [6] with a controlling parameter $\\beta$ . Here, a smaller $\\beta$ corresponds to a higher level of data heterogeneity across clients. We use ResNet-18, which contains four blocks, as the model structure and consider four types of model splitting represented by $L_{c}\\,=\\,\\{1,2,3,4\\}$ ,where $L_{c}\\,=\\,n$ means the model is split after the $n$ -th residual block. We consider two major distributed approaches as the benchmark, i.e., FL (in particular FedAvg [18]) and SL [28]. The learning rates for SFL-V1, SFL-V2, FL, and SL are set as 0.01. The batchsize $b_{s}$ is 128, and we run experiments for $T\\,=\\,200$ rounds. Unless stated otherwise, we use $N=10$ $\\beta=0.1$ \uff0c $E=5$ , where $E$ is the number of local epochs for client-side model aggregation (i.e., every $E$ times of training performed over each client's dataset, their client-side models are aggregated at the fed server), and hence $\\begin{array}{r}{\\tau=\\left\\lceil\\frac{D_{n}}{b_{s}}\\right\\rceil\\times E}\\end{array}$ . We set $\\tau=\\tilde{\\tau}$ for the fair comparison to vanilla FL. The experiments are run on a CPU (Intel(R) Xeon(R) Gold 5320 at 2.20GHz) and a GPU (A100-PCIE-80GB). Our codes are provided in https : //github . com/TIANGeng708/ Convergence-Analysis-of-Split-Federated-Learning-on-Heterogeneous-Data. ", "page_idx": 7}, {"type": "text", "text": "4.2  Impact of system parameters on SFL performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Impact of cut layer. We first investigate how the choice of the cut layer $L_{c}$ affectstheSFL performance. The results are reported in Fig. 2. We observe that for both SFL-V1 and SFL-V2, the performance increases in $L_{c}$ (i.e., clients have a larger proportion of the global model). This is associated with our empirical observation that the average client gradient variance gets smaller with $L_{c}$ . Intuitively, a smaller gradient variance implies a lower degree of the client drift issue, which leads to a better algorithm performance.5 Based on this observation, we use $L_{c}=4$ for SFL (and SL) for the following experiments. ", "page_idx": 7}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/c66a50677418592e8284d5acd3940748b3e7a58ed33ba8503d99751a2bd2ed6d.jpg", "img_caption": ["(a) SFL-V1 on CIFAR-10. (b) SFL-V2 on CIFAR-10. (c) SFL-V1 on CIFAR-100. (d) SFL-V2 on CIFAR-100. Figure 3: Impact of data heterogeneity on SFL performance. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/bcd595ac9e70e437e1474d109dd7c659f56a908b6bcb4cd0763c2f1f8829e87f.jpg", "img_caption": ["(a) SFL-V1 on CIFAR-10. (b) SFL-V2 on CIFAR-10. (c) SFL-V1 on CIFAR-100.(d) SFL-V2 on CIFAR-100. ", "Figure 4: Impact of client participation on SFL performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Impact of data heterogeneity. We study the impact of data heterogeneity on SFL performance, where we use $\\beta\\in\\{0.1,0.5,1,\\infty\\}$ , and $\\beta=\\infty$ means clients have IID data. The results are reported in Fig. 3. We observe that a higher level of data heterogeneity (i.e., a smaller $\\beta$ )leads to slower algorithm convergence and a lower accuracy for both SFL-V1 and SFL-V2. The observation is consistent with our convergence bound, e.g., in (16), the performance bound increases in $\\epsilon^{2}$ .Note that the negative impact of heterogeneity is commonly observed in distributed learning literature including FL [7] and SL [21]. ", "page_idx": 8}, {"type": "text", "text": "Impact of partial participation. We study the impact of client participation and let $q_{n}\\,=\\,q\\,\\in$ $\\{0.2,0.5,1\\},\\forall n$ . The results are reported in Fig. 4. We observe that a lower level of participation leads to less stable convergence and also a smaller accuracy. This is consistent with our convergence results, e.g., in (20), the bound decreases in clients' participation level $q_{n}$ . Partial participation is expected in practical cross-device scenarios where clients are resourced-constrained edge devices. It is important to develop efficient algorithms as well as effective incentive mechanisms to encourage clients\u2032 participation in SFL. ", "page_idx": 8}, {"type": "text", "text": "4.3 Comparison among SFL, FL, and SL. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now compare SFL to $\\mathrm{FL}$ and SL. We consider different combinations of data heterogeneity $\\beta\\in\\{0.1,0.5\\}$ and cohort sizes $N\\in\\{10,50,100\\}$ . The results are reported in Fig. 5. When data is mildly heterogeneous (i.e., $\\beta=0.5)$ 0, SFL and FL have similar convergence rates and accuracy performance. Note that SL seems to under-perform SFL and FL. We think this is mainly due to the catastrophic forgetting issue, which has been observed in [21, 2]. ", "page_idx": 8}, {"type": "text", "text": "SFL outperforms FL and SL under highly heterogeneous data and a large client number. When data becomes more non-ID (i.e., $\\beta=0.1)$ , SFL-V2 tends to outperform FL and SL. The improvement becomes more significant as the cohort size gets larger. The bottleneck of FL is the client drift issue caused by data heterogeneity. The bottleneck of SL is associated with the catastrophic forgetting. SFL-V2 is a hybrid combination of $\\mathrm{FL}$ and SL, which can lead to a better tradeoff between client drift and forgetting. By appropriately choosing the cut layer, SFL-V2 outperforms ", "page_idx": 8}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/bb18795021551387629c67155bee740602b7bc21f3fd9b4e0d7afbf0532fbb6b.jpg", "img_caption": ["Figure 5: Performance comparison on CIFAR-10. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "FL and SL. This observation also indicates that SFL-V2 can be a more appealing solution than FL for practical cross-device systems, as it achieves a better performance while requiring smaller computation overheads from edge devices. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we provided the first comprehensive convergence analysis of SFL for strongly convex, general-convex, and non-convex objectives on heterogeneous data. One key challenge is the dualpaced model updates. We get around this issue by decomposing the performance gap of the global model into the client-side and server-side gaps. We further extend our analysis to the more practical scenario with partial client participation. Experimental experiments validate our theories and further show that SFL can outperform FL and SL under highly heterogeneous data and a large client number. One limitation of our work is that our bounds for SFL achieve the same order (in terms of training rounds) as in FL, yet the experiments showed that SFL outperforms FL under high heterogeneity. This is possibly due to that tighter bounds for SFL are to be derived, which is an important future work. For future work, one can apply our derived bounds to optimize SFL system performance, considering model accuracy, communication overhead, and computational workload of clients. It is also interesting to theoretically analyze how the choice of the cut layer affects the SFL performance. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ahmed M Abdelmoniem, Atal Narayan Sahu, Marco Canini, and Suhaib A Fahmy. Refl: Resource-effcient federated learning. In Proceedings of the Eighteenth European Conference on Computer Systems, pages 215-232, 2023.   \n[2] Yansong Gao, Minki Kim, Sharif Abuadbba, Yeonjae Kim, Chandra Thapa, Kyuyeon Kim, Seyit A Camtepe, Hyoungshick Kim, and Surya Nepal. End-to-end evaluation of federated learning and split learning for internet of things. arXiv preprint arXiv:2003.13376, 2020.   \n[3] Dong-Jun Han, Hasnain Irshad Bhatti, Jungmoon Lee, and Jaekyun Moon. Accelerating federated learning with split learning on locally generated losses. In ICML workshop on federated learning for user privacy and data confidentiality, 2021.   \n[4] Dong-Jun Han, Do- Yeon Kim, Minseok Choi, Christopher G Brinton, and Jaekyun Moon Splitgp: Achieving both generalization and personalization in federated learning. Proc. of IEEE INFOCOM, 2023.   \n[5] Pengchao Han, Chao Huang, Xingyan Shi, Jianwei Huang, and Xin Liu. Incentivizing participation in splitfed learning: Convergence analysis and model versioning. In 2024 IEEE 44th International Conference on Distributed Computing Systems (ICDCS), pages 846-856, 2024.   \n[6]  Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.   \n[7] Chao Huang, Shuqi Ke, and Xin Liu. Duopoly business competition in cross-silo federated learning. IEEE Transactions on Network Science and Engineering, 2023.   \n[8]  Chao Huang, Geng Tian, and Ming Tang. When minibatch sgd meets splitfed learning: Convergence analysis and performance evaluation. arXiv preprint arXiv:2308.11953, 2023.   \n[9]  Yang Jiao, Kai Yang, Tiancheng Wu, Chengtao Jian, and Jianwei Huang. Provably convergent federated trilevel learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 12928-12937, 2024.   \n[10] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International conference on machine learning, pages 5132-5143. PMLR, 2020.   \n[11]  Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pages 4519-4529. PMLR, 2020.   \n[12]  Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized sgd with changing topology and local updates. In International Conference on Machine Learning, pages 5381-5393. PMLR, 2020.   \n[13]  Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.   \n[14]  Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10713-10722, 2021.   \n[15] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems, 2:429-450, 2020.   \n[16] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In Proc. of ICLR, 2020.   \n[17]  Yipeng Li and Xinchen Lyu. Convergence analysis of sequential federated learning on heterogeneous data. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[18] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273-1282. PMLR, 2017.   \n[19] Sashank Reddi, Zachary Charles, Manzil Zaher, Zachary Garrett, Keith Rush, Jakub Koneny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.   \n[20] Sashank Reddi, Zachary Burr Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and Brendan McMahan, editors. Adaptive Federated Optimization, 2021.   \n[21] Jinglong Shen, Nan Cheng, Xiucheng Wang, Feng Lyu, Wenchao Xu, Zhi Liu, Khalid Aldubaikhy, and Xuemin Shen. Ringsf: An adaptive split federated learning towards taming client heterogeneity. IEEE Transactions on Mobile Computing, 2023.   \n[22] Chamani Shiranthika, Zahra Hafezi Kafshgari, Parvaneh Saeedi, and Ivan V Bajic. Splitfed resilience to packet loss: Where to split,that is the question. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 367-377. Springer, 2023.   \n[23]  Veronika Stephanie, Ibrahim Khalil, and Mohammed Atiquzzaman. Digital twin enabled asynchronous splitfed learning in e-healthcare systems. IEEE Journal on Selected Areas in Communications, 41(11):3650-3661, 2023.   \n[24]  Sebastian U Stich. Unifed optimal analysis of the (stochastic) gradient method. arXiv preprint arXiv:1907.04232, 2019.   \n[25] Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. Federated learning on non-id graphs via structural knowledge sharing. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 9953-9961, 2023.   \n[26] Ming Tang and Vincent WS Wong. Tackling system induced bias in federated learning: Stratification and convergence analysis. In Proc. of IEEE INFOCOM, pages 1-10, 2023.   \n[27] Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, Seyit Camtepe, and Lichao Sun. Splitfed: When federated learning meets split learning. In Proc. of AAAl, volume 36, pages 8485-8493, 2022.   \n[28] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for health: Distributed deep learning without sharing raw patient data. ICLR Workshop on AIfor Social Good, 2019.   \n[29]  Shiqiang Wang and Mingyue Ji. A unified analysis of federated learning with arbitrary client participation. Advances in Neural Information Processing Systems, 35:19124-19137, 2022.   \n[30] Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin K Leung, Christian Makaya, Ting He, and Kevin Chan. Adaptive federated learning in resource constrained edge computing systems. IEEE journal on selected areas in communications, 37(6):1205-1221, 2019.   \n[31] Dinah Waref and Mohammed Salem. Split federated learning for emotion detection. In 2022 4th Novel Intelligent and Leading Emerging Sciences Conference (NILES), pages 112-115. IEEE, 2022.   \n[32] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International Conference on Machine Learning, pages 10334-10343. PMLR, 2020.   \n[33] Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local sgd for heterogeneous distributed learning. Advances in Neural Information Processing Systems, 33:6281-6292, 2020.   \n[34] Wen Wu, Mushu Li, Kaige Qu, Conghao Zhou, Xuemin Shen, Weihua Zhuang, Xu Li, and Weisen Shi. Split learning over wireless networks: Parallel design and resource manageent. IEEE Journal on Selected Areas in Communications, 41(4):1051-1066, 2023.   \n[35] Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-id federated learning. Proc. of ICLR, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix / supplemental material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We organize the entire appendix file as follows: ", "page_idx": 12}, {"type": "text", "text": "In Sec. B, we provide detailed algorithmic descriptions.   \nIn Sec. C, we provide notations and some technical lemmas.   \n\u00b7 In Sec. C.1, we provide some notations   \n\u00b7 In Sec. C.2, we recall SFL-V1 and SFL-V2   \n\u00b7 In Sec. C.3, we recall the assumptions   \n\u00b7 In Sec. C.4, we provide some useful technical lemmas together with their proofs ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "In Sec. D, we prove Theorem 3.6, i.e., convergence of SFL-V1 under full participation. ", "page_idx": 12}, {"type": "text", "text": "\u00b7 In Sec. D.1, we prove the strongly convex case \u00b7 In Sec. D.2, we prove the general convex case \u00b7 In Sec. D.3, we prove the non-convex case ", "page_idx": 12}, {"type": "text", "text": "In Sec. E, we prove Theorem 3.7, i.e., convergence of SFL-V2 under full participation. ", "page_idx": 12}, {"type": "text", "text": "\u00b7 In Sec. E.1, we prove the strongly convex case \u00b7 In Sec. E.2, we prove the general convex case \u00b7 In Sec. E.3, we prove the non-convex case ", "page_idx": 12}, {"type": "text", "text": "In Sec. F, we prove Theorem 3.8, i.e., convergence of SFL-V1 under partial participation. ", "page_idx": 12}, {"type": "text", "text": "\u00b7 In Sec. F.1, we prove the strongly convex case \u00b7 In Sec. F.2, we prove the general convex case \u00b7 In Sec. F.3, we prove the non-convex case ", "page_idx": 12}, {"type": "text", "text": "In Sec. G, we prove Theorem 3.9, i.e., convergence of SFL-V2 under partial participation. ", "page_idx": 12}, {"type": "text", "text": "\u00b7 In Sec. G.1, we prove the strongly convex case \u00b7 In Sec. G.2, we prove the general convex case \u00b7 In Sec. G.3, we prove the non-convex case ", "page_idx": 12}, {"type": "text", "text": "In Sec. H, we SFL to other distributed approaches, i.e., FL, SL, and Mini-Batch SGD. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u00b7 In Sec. H.2, we compare their convergence bounds \u00b7 In Sec. H.3, we compare their overheads in terms of communication and computation ", "page_idx": 12}, {"type": "text", "text": "In Sec. I, we provide more experimental results. ", "page_idx": 12}, {"type": "text", "text": "For version 1, the client-side model parameter and M-server-side model parameter are aggregated   \nevery $\\tau$ and $\\tilde{\\tau}$ iterations, respectively. In iteration $i$ of round $t$ , each client $n$ samples a mini-batch   \nof data $\\zeta_{n}^{t,i}$ from $\\mathcal{D}_{n}$ , computes the intermediate features $h(\\pmb{x}_{c,n}^{t,i};\\zeta_{n}^{t,i})$ (e.g, activation values at   \nthecut layeerisuremdl $n$ hM-servercmputesthlos $F_{n}(h(\\mathbf{x}_{c,n}^{t,i};\\zeta_{n}^{t,i}),\\mathbf{x}_{s,n}^{t,i})$ ${\\pmb x}_{c,n}^{t,i}$ ,and sends ot,n) based on cs $h(\\pmb{x}_{c,n}^{t,i};\\zeta_{n}^{t,i})$ \u4e00 to he M-server Foreach client rt,n. Let V denote a gradient $F$ $\\pmb{w}$ $g_{s,n}^{t,i}(x_{s,n}^{t,i};\\zeta_{n}^{\\hat{t},i})=\\nabla_{\\mathbf{x}_{s}}\\bar{F_{n}}\\big(h(x_{c,n}^{t,i};\\zeta_{n}^{t,i}),x_{s,n}^{t,i}\\big)$   \n(activations) at the cut layer rt,n ( $\\begin{array}{r}{\\mathbf{\\boldsymbol{r}}_{c,n}^{t,i}(\\mathbf{\\boldsymbol{x}}_{s,n}^{t,i};\\zeta_{n}^{t,i})=\\nabla_{h}F_{n}\\big(h(\\mathbf{\\boldsymbol{x}}_{c,n}^{t,i};\\zeta_{n}^{t,i}),\\mathbf{\\boldsymbol{x}}_{s,n}^{t,i}\\big)}\\end{array}$ and sends ${\\pmb r}_{c,n}^{t,i}({\\pmb x}_{s,n}^{t,i};\\zeta_{n}^{t,i})$   \nto client $n$ Each client $n$ computes the client-side gradient ${\\pmb g}_{c,n}^{t,i}({\\pmb x}_{c,n}^{t,i};\\zeta_{n}^{t,i})$ based on ${\\pmb r}_{c,n}^{t,i}({\\pmb x}_{s,n}^{t,i};\\zeta_{n}^{t,i})$   \nusing the chain rule. ", "page_idx": 13}, {"type": "text", "text": "For version 2, the client-side model is aggregated every $\\tau$ iterations, while the M-server trains only one version of the M-server-side model. ", "page_idx": 13}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/34f12e14cba2420d9d75280ae0037007751b9d619c9486d8ad1a5b2024eed093.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Input: $\\tau,T$ , and learning rate $\\eta_{t}$   \nOutput: Global model $\\bar{\\pmb{x}}^{T}=\\{\\pmb{x}_{c}^{T},\\pmb{x}_{s}^{T}\\}$   \n1  Initialize ${\\pmb x}^{0}=\\{{\\pmb x}_{c}^{0},{\\pmb x}_{s}^{0}\\}$ .\uff0c   \n2 for $t=0,\\dots,T-1$ do   \n3 Determine participating client set $\\mathcal{P}^{t}\\subseteq\\mathcal{N}$ according to $q_{n}$   \nPhase 1: model training.   \n4 each client $n\\in\\mathcal{P}^{t}$ \uff1a   \n5 $\\pmb{x}_{c,n}^{t,0}\\gets\\pmb{x}_{c}^{t}$   \n6 for $i=0,\\ldots,\\tau-1$ do   \n7 Sample a mini-batch $\\zeta_{n}^{t,i}$ .\uff0c   \n8 Send $h(\\pmb{x}_{c,n}^{t,i};\\zeta_{n}^{t,i})$ to the M-server;   \n9 the M-server:   \n10 Compute $F_{n}\\big(h(\\mathbf{\\boldsymbol{x}}_{c,n}^{t,i};\\zeta_{n}^{t,i}),\\mathbf{\\boldsymbol{x}}_{s}^{t,i}\\big),\\mathbf{\\boldsymbol{g}}_{s,n}^{t,i}(\\mathbf{\\boldsymbol{x}}_{s}^{t,i};\\zeta_{n}^{t,i})$ and   \n${\\pmb r}_{c,n}^{t,i}({\\pmb x}_{s}^{t,i};\\zeta_{n}^{t,i})$ toclertr   \n11 ${\\pmb r}_{c,n}^{t,i}({\\pmb x}_{s}^{t,i};\\zeta_{n}^{t,i})$ $n\\in\\mathcal{P}^{t}$   \n12 $\\begin{array}{r}{\\pmb{x}_{s}^{t,i+1}\\gets\\pmb{x}_{s}^{t,i}-\\frac{\\eta_{t}}{q_{n}}\\pmb{g}_{s,n}^{t,i}(\\pmb{x}_{s}^{t,i};\\zeta_{n}^{t,i});}\\end{array}$   \n13 Compute $\\pmb{g}_{c,n}^{t,i}(\\pmb{x}_{c,n}^{t,i};\\zeta_{n}^{t,i})$   \n14 $\\pmb{x}_{c,n}^{t,i+1}\\gets\\pmb{x}_{c,n}^{t,i}-\\eta_{t}\\pmb{g}_{c,n}^{t,i}(\\pmb{x}_{c,n}^{t,i};\\zeta_{n}^{t,i})$   \n15 the M-server:   \n16 $\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $   \nPhase 2: model aggregation.   \n17 each client $n\\in\\mathcal{P}^{t}$ \uff1a   \n18 $\\lfloor$ Send $\\pmb{x}_{c,n}^{t,\\tau}$ to the F-server;   \n19 the F-server:   \n20 $\\begin{array}{r}{\\underline{{\\pmb{x}}}_{c}^{t+1}\\leftarrow\\sum_{n\\in\\mathcal{P}^{t}}\\frac{a_{n}}{q_{n}}\\pmb{x}_{c,n}^{t,\\tau}.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "C   Notations and technical lemmas ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1  Notations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall that the objective of SFL is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pmb{x}}f(\\pmb{x}):=\\sum_{n=1}^{N}a_{n}F_{n}(\\pmb{x})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We define ", "page_idx": 15}, {"type": "text", "text": "$x_{c}$ and $\\pmb{x}_{s}$ : global model parameter on the clients and server sides, respectively. \u00b7 ${\\mathbf{}}{\\mathbf{}}{\\mathbf{}}{\\mathbf{}}_{x,n}$ and $\\mathbf{\\boldsymbol{x}}_{s,n}\\mathbf{\\boldsymbol{:}}$ local forms of parameter on client $n$ and on the main server corresponding to client $n$ (in SFL-V1). $\\nabla F_{c,n}\\left(\\cdot\\right)$ and $\\nabla F_{s,n}\\left(\\cdot\\right)$ : the gradients of $F_{n}\\left(\\cdot\\right)$ over $x_{c}$ and $\\pmb{x}_{s}$ , respectively. $g_{c,n}\\left(\\cdot\\right)$ and $g_{s,n}\\left(\\cdot\\right)$ : the stochastic gradients of $F_{n}$ () over $x_{c}$ and $\\pmb{x}_{s}$ , respectively. ", "page_idx": 15}, {"type": "text", "text": "For convenience, we omit the notation for mini-batch training data when referring to stochastic gradients. ", "page_idx": 15}, {"type": "text", "text": "Further, we recall how SFL-V1 and SFL-V2 update models below. ", "page_idx": 15}, {"type": "text", "text": "C.2 SFL-V1 and SFL-V2 model updates ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Let $q_{n}$ denote the participating probability of client $n$ and define $\\pmb q:=\\{q_{1},\\dots,q_{N}\\}$ . We denote $\\mathbf{I}_{n}^{t}$ as a binary variable, taking 1 if client $n$ participates in model training in round $t$ , and O otherwise. $\\mathbf{I}_{n}^{t}$ follows a Bernoulli distribution with an expectation of $q_{n}$ . Denote $\\bar{\\mathcal{P}}^{t}\\left(\\pmb{q}\\right)$ as the set of participating clients in round $t$ ", "page_idx": 15}, {"type": "text", "text": "Parameter update for SFL-V1: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Local raning of lent $n$ ${\\pmb x}_{c,n}^{t,0}\\gets{\\pmb x}_{c}^{t}$ $\\begin{array}{r}{{\\bf x}_{c}^{t},{\\bf x}_{c,n}^{t,i+1}\\gets{\\pmb x}_{c,n}^{t,i}-\\eta^{t}{\\pmb g}_{c,n}^{t,i}\\,\\big({\\pmb x}_{c,n}^{t,i}\\big),{\\pmb x}_{c,n}^{t+1}\\gets{\\pmb x}_{c,n}^{t,\\tau};}\\end{array}$   \n\u00b7 Client-side global aggregation: - Full participation: $\\begin{array}{r}{\\pmb{x}_{c}^{t+1}\\gets\\pmb{x}_{c}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{N}}a_{n}\\sum_{i=0}^{\\tau}g_{c,n}^{t,i}\\left(\\pmb{x}_{c,n}^{t,i}\\right);}\\end{array}$ - Partal paricipation $\\begin{array}{r}{\\pmb{x}_{c}^{t+1}\\gets\\pmb{x}_{c}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\frac{a_{n}}{q_{n}}\\sum_{i=0}^{\\tau}\\pmb{g}_{c,n}^{t,i}\\left(\\pmb{x}_{c,n}^{t,i}\\right);}\\end{array}$   \n\u00b7 M-server-side model update: - Full partcipaton: - Partial pricipation $\\begin{array}{r l}&{z_{s}^{t+1}\\gets x_{s}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{N}}a_{n}\\sum_{i=0}^{\\tilde{\\tau}-1}g_{s,n}^{t,i}\\left(x_{s,n}^{t,i}\\right);}\\\\ &{:x_{s}^{t+1}\\gets x_{s}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\frac{a_{n}}{q_{n}}\\sum_{i=0}^{\\tilde{\\tau}-1}g_{s,n}^{t,i}\\left(x_{s,n}^{t,i}\\right).}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Parameter update for SFL-V2: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Local rainin o client $n$ $\\pmb{x}_{c,n}^{t,0}\\gets\\pmb{x}_{c}^{t}$ $,\\pmb{x}_{c,n}^{t,i+1}\\leftarrow\\pmb{x}_{c,n}^{t,i}-\\eta^{t}\\pmb{g}_{c,n}^{t,i}\\left(\\pmb{x}_{c,n}^{t,i}\\right),\\pmb{x}_{c,n}^{t+1}\\leftarrow\\pmb{x}_{c,n}^{t,\\tau};$   \n\u00b7 Client-side global aggregation: - Ful articipation: $\\begin{array}{r}{\\pmb{x}_{c}^{t+1}\\gets\\pmb{x}_{c}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{N}}a_{n}\\sum_{i=0}^{\\tau}g_{c,n}^{t,i}\\left(\\pmb{x}_{c,n}^{t,i}\\right);}\\end{array}$ - Partial paticipation: $\\begin{array}{r}{\\pmb{x}_{c}^{t+1}\\gets\\pmb{x}_{c}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\frac{a_{n}}{q_{n}}\\sum_{i=0}^{\\tau}\\pmb{g}_{c,n}^{t,i}\\left(\\pmb{x}_{c,n}^{t,i}\\right);}\\end{array}$   \n\u00b7 M-server-side model update: $\\begin{array}{r}{\\pmb{x}_{s}^{t+1}\\leftarrow\\pmb{x}_{s}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{N}}\\sum_{i=0}^{\\tau-1}\\pmb{g}_{s,n}^{t,i}\\left(\\pmb{x}_{s,n}^{t,i}\\right);}\\end{array}$ - Partial prticiation: $\\begin{array}{r}{\\pmb{x}_{s}^{t+1}\\gets\\pmb{x}_{s}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\frac{1}{q_{n}}\\sum_{i=0}^{\\tau-1}\\pmb{g}_{s,n}^{t,i}\\left(\\pmb{x}_{s,n}^{t,i}\\right)\\!.}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "C.3  Assumptions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We further recall the following assumptions for clients\u2019 loss functions in the proof. ", "page_idx": 15}, {"type": "text", "text": "Assumption C.1. For each client $n\\in\\mathcal N$ ", "page_idx": 15}, {"type": "text", "text": "\u00b7 The loss $F_{n}\\left(\\cdot\\right)$ is $S$ -smooth: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\nabla F_{n}\\left(\\pmb{x}\\right)-\\nabla F_{n}\\left(\\pmb{y}\\right)\\right\\|\\leq S\\left\\|\\pmb{x}-\\pmb{y}\\right\\|,\\forall\\pmb{x},\\pmb{y},}\\\\ &{F_{n}(\\pmb{y})\\leq F_{n}(\\pmb{x})+\\langle\\nabla F_{n}(\\pmb{x}),\\pmb{y}-\\pmb{x}\\rangle+\\displaystyle\\frac{S}{2}\\|\\pmb{y}-\\pmb{x}\\|^{2},\\forall\\pmb{x},\\pmb{y}\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u00b7 The stochastic gradients of $F_{n}\\left(\\cdot\\right)$ are unbiased with the variance bounded by $\\sigma_{n}^{2}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathbb{E}\\left[\\pmb{\\mathscr{g}}_{n}\\left(\\pmb{x}\\right)\\right]=\\nabla F_{n}\\left(\\pmb{x}\\right),}\\\\ {\\mathbb{E}\\left[\\left\\vert\\pmb{\\mathscr{g}}_{n}\\left(\\pmb{x}\\right)-\\nabla F_{n}\\left(\\pmb{x}\\right)\\right\\vert\\right]^{2}\\right]\\leq\\sigma_{n}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u00b7 The expected squared norm of stochastic gradients is bounded by $G^{2}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left\\|\\pmb{{g}}_{n}\\left(\\pmb{{x}}\\right)\\right\\|^{2}\\leq G^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "\u00b7 (Bounded gradient divergence) There exists a constant $\\epsilon\\,>\\,0$ , such that the divergence between local and global gradients is bounded by $\\epsilon^{2}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|\\nabla F_{n}\\left(\\mathbf{x}\\right)-\\nabla f\\left(\\mathbf{x}\\right)\\right\\|^{2}\\leq\\epsilon^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assumption C.2. For each client $n\\in\\mathcal N$ ", "page_idx": 16}, {"type": "text", "text": "\u00b7 The loss $F_{n}\\left(\\cdot\\right)$ is $\\mu$ -strongly convex for some $\\mu\\geq0$ ", "page_idx": 16}, {"type": "equation", "text": "$$\nF_{n}(\\pmb{y})\\geq F_{n}(\\pmb{x})+\\langle\\nabla F_{n}(\\pmb{x}),\\pmb{y}-\\pmb{x}\\rangle+\\frac{\\mu}{2}\\|\\pmb{y}-\\pmb{x}\\|^{2},\\forall\\pmb{x},\\pmb{y}\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, we allow that $\\mu=0$ , referring to this case of the general convex. ", "page_idx": 16}, {"type": "text", "text": "C.4 Technical Lemmas ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma C.3. [Lemma $^{5}$ in [1oj] The following holds for any $S$ -smooth and $\\mu$ -strongly convex function $h$ and any $x,y,z$ in the domain of $h$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\langle\\nabla h({\\pmb x}),{\\pmb z}-{\\pmb y}\\rangle\\geq h({\\pmb z})-h({\\pmb y})+\\frac{\\mu}{4}\\|{\\pmb y}-{\\pmb z}\\|^{2}-S\\|{\\pmb z}-{\\pmb x}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 3.5 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proposition 3.5 (Convergence decomposition) Let $\\pmb{x}^{*}\\triangleq[\\pmb{x}_{c}^{*};\\pmb{x}_{s}^{*}]$ denote the optimal global model that minimizes $f(\\cdot)$ and $\\pmb{x}^{T}\\triangleq[\\pmb{x}_{c}^{T};\\pmb{x}_{s}^{T}]$ is the global model obtained after $T$ rounds of SFL training. Under Assumption 3.1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f(\\pmb{x}^{T})\\right]-f(\\pmb{x}^{*})\\le\\frac{S}{2}\\left(\\mathbb{E}||\\pmb{x}_{s}^{T}-\\pmb{x}_{s}^{*}||^{2}+\\mathbb{E}||\\pmb{x}_{c}^{T}-\\pmb{x}_{c}^{*}||^{2}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Since $F_{n}$ 's are $S$ -smooth, it is easy to show that the global loss function $f(\\cdot)$ is also $S$ -smooth. Thus, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[f({\\pmb x}^{T})\\right]-f({\\pmb x}^{*})\\!\\leq\\!\\mathbb{E}\\big[\\!\\langle{\\pmb x}^{T}-{\\pmb x}^{*},\\nabla f({\\pmb x}^{*})\\rangle\\big]\\!+\\!\\frac{S}{2}\\mathbb{E}\\left[||{\\pmb x}^{T}-{\\pmb x}^{*}||^{2}\\right]\\!=\\!\\frac{S}{2}\\mathbb{E}\\left[||{\\pmb x}^{T}-{\\pmb x}^{*}||^{2}\\right].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since $\\pmb{x}^{T}\\triangleq[\\pmb{x}_{c}^{T};\\pmb{x}_{s}^{T}]$ and $\\pmb{x}^{*}\\triangleq[\\pmb{x}_{c}^{*};\\pmb{x}_{s}^{*}]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}\\left[||\\pmb{x}^{T}-\\pmb{x}^{*}||^{2}\\right]=\\mathbb{E}\\left[||[\\pmb{x}_{c}^{T};\\pmb{x}_{s}^{T}]-[\\pmb{x}_{c}^{*};\\pmb{x}_{s}^{*}]||^{2}\\right]}\\\\ &{=\\!\\mathbb{E}\\left[||[\\pmb{x}_{c}^{T}-\\pmb{x}_{c}^{*};\\pmb{x}_{s}^{T}-\\pmb{x}_{s}^{*}]||^{2}\\right]=\\mathbb{E}\\left[||\\pmb{x}_{c}^{T}-\\pmb{x}_{c}^{*}||^{2}\\right]+\\mathbb{E}\\left[||\\pmb{x}_{s}^{T}-\\pmb{x}_{s}^{*}||^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Substituting (34) into (33), we complete the proof. ", "page_idx": 16}, {"type": "text", "text": "Proposition C.4 (Decomposition in each round). Under Assumption C.1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f\\left(\\pmb{x}^{t+1}\\right)\\right]-f\\left(\\pmb{x}^{t}\\right)}\\\\ &{~~\\leq\\mathbb{E}\\left[\\left\\langle\\nabla_{\\pmb{x}_{c}}f\\left(\\pmb{x}^{t}\\right),\\pmb{x}_{c}^{t+1}-\\pmb{x}_{c}^{t}\\right\\rangle\\right]\\!+\\!\\displaystyle\\frac{S}{2}\\mathbb{E}\\left[\\left\\|\\pmb{x}_{c}^{t+1}-\\pmb{x}_{c}^{t}\\right\\|^{2}\\right]+}\\\\ &{~~\\mathbb{E}\\left[\\left\\langle\\nabla_{\\pmb{x}_{s}}f\\left(\\pmb{x}^{t}\\right),\\pmb{x}_{s}^{t+1}-\\pmb{x}_{s}^{t}\\right\\rangle\\right]\\!+\\!\\displaystyle\\frac{S}{2}\\mathbb{E}\\left[\\left\\|\\pmb{x}_{s}^{t+1}\\!-\\pmb{x}_{s}^{t}\\right\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma C.5. [Multiple iterations of local training in each round] Under Assumption C.1, if we let $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{\\sqrt{6}S\\tau}}\\end{array}$ andrun clientn's localmodel for $\\tau$ iterationcontinuouslyinanyround $t$ wehave ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert x_{n}^{t,i}-x^{t}\\right\\Vert^{2}\\right]\\leq12\\tau^{3}\\left(\\eta^{t}\\right)^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Similar to Lemma 3 in [20], we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad_{1}\\left\\{\\begin{array}{l l}{1}&{0:}\\\\ {\\zeta^{(\\frac{\\nu}{2})-\\nu}}&{\\leq\\zeta^{(\\frac{\\nu}{2})}\\left[\\frac{\\zeta^{(\\frac{\\nu}{2})-\\nu}}{\\zeta^{(\\frac{\\nu}{2})}}-z^{\\frac{\\nu}{2}}\\right]^{\\frac{\\nu}{2}}}\\\\ &{\\leq\\left\\{\\left[\\frac{\\zeta^{(\\frac{\\nu}{2})-\\nu}}{\\zeta^{(\\frac{\\nu}{2})}}-z^{-\\frac{\\nu}{2}}\\right]^{\\frac{\\nu}{2}}\\left[\\zeta_{\\frac{{\\nu}}{2}}^{(\\frac{\\nu}{2})-1}-\\nabla_{\\nu}\\int_{\\mathbb{R}}\\left(z_{\\frac{{\\nu}}{2}}^{(\\frac{\\nu}{2})-1}\\right)+\\nabla_{\\nu}F_{\\nu}\\left(z_{\\frac{\\nu}{2}}^{(\\frac{\\nu}{2})-1}\\right)-\\nabla_{\\nu}F_{\\nu}\\left(z_{\\frac{\\nu}{2}}^{(\\frac{\\nu}{2})}+\\nabla_{\\nu}F_{\\nu}\\left(z_{\\frac{\\nu}{2}}^{(\\frac{\\nu}{2})}\\right)\\right]\\right\\}^{2}\\right\\}}\\\\ &{\\leq\\left(1+\\frac{\\nu}{2}\\right)\\left[\\left[\\frac{\\zeta^{(\\frac{\\nu}{2})-\\nu}}{\\zeta^{(\\frac{\\nu}{2})}}\\right]^{\\frac{\\nu}{2}}+3(1+\\frac{\\nu}{2})\\left[\\left[\\frac{\\zeta^{(\\frac{\\nu}{2})-\\nu}}{\\zeta^{(\\frac{\\nu}{2})}}-\\nabla_{\\nu}F_{\\nu}\\left(z_{\\frac{\\nu}{2}}^{(\\frac{\\nu}{2})-1}\\right)\\right]^{\\frac{\\nu}{2}}\\right]}\\\\ &{\\leq\\lambda(1+\\frac{\\nu}{2})\\left[\\left[\\frac{\\zeta^{(\\frac{\\nu}{2})}}{\\zeta^{(\\frac{\\nu}{2})}}\\left[\\zeta_{\\frac{{\\nu}}{2}}^{(\\frac{\\nu}{2})-1}-\\nabla_{\\nu}F_{\\nu}\\left(z_{\\frac{\\nu}{2}}^{(\\frac{\\nu}{2})}\\right)\\right]\\right]^{\\frac{\\nu}{2}}+3(1+\\frac{\\nu}{2})\\left[\\left[\\\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we use Assumption C.1, $\\begin{array}{r}{(X+Y)^{2}\\le(1+a)\\,X^{2}+\\left(1+\\frac{1}{a}\\right)Y^{2}}\\end{array}$ for some positive $a$ , and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{\\sqrt{6}S\\tau}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{t,i}:=\\mathbb{E}\\left[\\left\\lVert x_{n}^{t,i}-x^{t}\\right\\rVert^{2}\\right]}\\\\ &{B:=6\\tau\\left(\\eta^{t}\\right)^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{C:=1+\\displaystyle\\frac{2}{\\tau}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have ", "page_idx": 17}, {"type": "equation", "text": "$$\nA^{t,i}\\leq C A^{t,i-1}+B\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can show that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A^{t,1}\\leq C A^{t}+B}\\\\ &{A^{t,2}\\leq C A^{t,1}+B\\leq C^{2}A^{t}+C B+B}\\\\ &{A^{t,3}\\leq C A^{t,2}+B\\leq C^{3}A^{t}+C^{2}B+C B+B}\\\\ &{\\cdots}\\\\ &{A^{t,i}\\leq C^{i}A^{t}+B\\displaystyle\\sum_{j=0}^{i-1}C^{j}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that $A^{t}:=A^{t,0}=\\mathbb{E}\\left[\\left\\lVert\\pmb{x}^{t}-\\pmb{x}^{t}\\right\\rVert^{2}\\right]=0$ .Accumulate the above for $\\tau$ iterations, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{r-1}\\mathbb{E}\\left[\\left\\|\\mathbf{x}_{n}^{t,i}-\\mathbf{x}^{t}\\right\\|^{2}\\right]=\\sum_{i=0}^{r-1}B_{\\sum}^{i-1}C^{j}}\\\\ &{=\\displaystyle B\\sum_{i=0}^{r-1}\\frac{C^{i-1}}{C}-1\\,=\\frac{B_{0}}{C-1}\\sum_{i=0}^{r}(C^{i}-1)=\\frac{B}{C-1}\\left(\\frac{C^{r}-1}{C-1}-\\tau\\right)}\\\\ &{=\\displaystyle\\frac{B}{\\frac{B}{\\frac{1}{\\frac{1}{\\rho}}}}\\left(\\frac{1+\\frac{\\sigma}{2}}{\\frac{\\sigma}{2}}\\right)^{r}-1}\\\\ &{\\leq\\frac{\\tau^{2}B}{2}\\left(\\frac{\\epsilon^{2}-1}{2}-1\\right)}\\\\ &{\\leq2\\tau^{2}B}\\\\ &{\\leq2\\tau^{2}\\delta\\tau\\left(\\eta^{\\mathrm{f}}\\right)^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{\\leq12\\tau^{3}\\left(\\eta^{\\mathrm{f}}\\right)^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The frs inequaityis due to $\\begin{array}{r}{\\sum_{i=0}^{N-1}x^{i}=\\frac{x^{N}-1}{X-1}}\\end{array}$ andthe third lneresut rom $\\textstyle(1+{\\frac{n}{x}})^{x}\\leq e^{n}$ Thus, we finish the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Lemma C.6. [Multiple iterations of local training in each round] Under Assumption C.1, if we let $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{\\sqrt{8}S\\tau}}\\end{array}$ andrunclient $n$ 's local model for $\\tau$ iteration continuousy in any round $t$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert x_{n}^{t,i}-x^{t}\\right\\Vert^{2}\\right]\\leq2\\tau^{2}\\left(8\\tau\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\left\\|\\mathbf{z}_{j}^{t,\\varepsilon}-\\mathbf{z}^{t}\\right\\|^{2}\\right\\|^{2}}\\\\ &{\\leq\\mathbb{E}\\left\\|\\left\\|\\mathbf{z}_{j}^{t,\\varepsilon-1}-\\mathbf{z}^{t}\\widehat{\\mathbf{g}}_{\\theta}^{t,\\varepsilon-1}-\\mathbf{z}^{t}\\right\\|^{2}}\\\\ &{\\leq\\mathbb{E}\\left\\|\\left\\|\\mathbf{z}_{j}^{t,\\varepsilon-1}-\\mathbf{z}^{t}\\cdot\\widehat{\\mathbf{g}}_{\\theta}^{t}\\right\\|^{2}}\\\\ &{\\leq\\mathbb{E}\\left\\|\\left\\|\\mathbf{z}_{j}^{t,\\varepsilon-1}-\\mathbf{z}^{t}\\cdot\\widehat{\\mathbf{g}}_{\\theta}^{t}\\right\\|^{2}+\\mathbb{E}\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{j}^{t,\\varepsilon-1}\\right)}\\\\ &{+\\mathbb{V}\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{j}^{t,\\varepsilon+1}\\right)-\\mathbb{V}\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{j}^{t}\\right)+\\mathbb{V}\\mathbf{z}_{\\theta}\\left(\\mathbf{z}^{t}\\right)-\\mathbb{V}\\mathbf{z}_{\\theta}\\left(\\mathbf{z}^{t}\\right)+\\mathbb{V}_{\\phi}f\\left(\\mathbf{z}^{t}\\right)\\right\\|^{2}}\\\\ &{\\leq\\left(1+\\frac{1}{2}\\right)\\mathbb{E}\\left\\|\\left\\|\\mathbf{z}_{j}^{t,\\varepsilon-1}-\\mathbf{z}^{t}\\right\\|^{2}+\\mathbb{E}\\mathbb{z}\\left[\\left\\|\\left|\\mathbf{z}^{t}\\left(\\mathbf{z}_{j}^{t,\\varepsilon-1}-\\mathbf{z}_{j}\\mathbf{z}_{j}\\right)\\right\\|^{2}\\right]}\\\\ &{+\\mathbb{E}\\mathbb{E}\\left\\|\\left\\|\\left\\langle\\mathbf{z}_{j}^{t}\\left(\\mathbf{z}_{j}^{t,\\varepsilon}\\mathbf{z}_{j}^{t,\\varepsilon-1}\\right)-\\mathbf{z}^{t}\\mathbf{z}^{t}\\mathbf{z}^{t}\\right\\|^{2}\\right\\|^{2}+\\mathbb{E}\\mathbb{z}\\left[\\left\\|\\left|\\mathbf{z}^{t}\\left(\\mathbf{z}_{j}^{t,\\varepsilon-1}-\\mathbf{z}_{j}\\mathbf{z}^{t}\\right)\\right\\|^{2}\\right]}\\\\ &{+\\mathbb{E}\\mathbb{z}\\left\\|\\left\\|\\mathbf{z}_{j}^{t}\\left(\\mathbf{z}_{j}^{t,\\varepsilon}\\right)\\right\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we have applied Assumption C.1, $\\begin{array}{r}{\\left(X+Y\\right)^{2}\\leq\\left(1+a\\right)X^{2}+\\left(1+\\frac{1}{a}\\right)Y^{2}}\\end{array}$ for some positive a, and nt \u2264 V8st\u00b7 ", "page_idx": 18}, {"type": "text", "text": "Let ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{t,i}:=\\mathbb{E}\\left[\\left\\lvert\\lvert\\mathbf{\\boldsymbol{x}}_{n}^{t,i}-\\mathbf{\\boldsymbol{x}}^{t}\\right\\rvert\\right\\rvert^{2}\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{B:=8\\tau\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x}f\\left(\\pmb{x}^{t}\\right)\\right\\Vert^{2}}\\\\ {C:=1+\\displaystyle\\frac{2}{\\tau}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We have ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{t,i}\\leq C A_{t,i-1}+B\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can show that ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{t,i}\\leq C^{i}A_{t}+B\\sum_{j=0}^{i-1}C^{j}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $A_{t}=\\mathbb{E}\\left[\\left\\|\\pmb{x}^{t}-\\pmb{x}^{t}\\right\\|^{2}\\right]=0$ . Accumulate the above for $\\tau$ iterations, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\|x_{n}^{t,i}-x^{t}\\right\\|^{2}\\right]=\\displaystyle\\sum_{i=0}^{\\tau-1}B\\sum_{j=0}^{i-1}C^{j}}\\\\ &{\\leq2\\tau^{2}B}\\\\ &{\\leq2\\tau^{2}\\left(8\\tau\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\left\\|\\nabla_{x}f\\left(x^{t}\\right)\\right\\|^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we use $\\begin{array}{r}{\\sum_{i=0}^{N-1}x^{i}=\\frac{x^{N}-1}{X-1}}\\end{array}$ and $(1+\\textstyle{\\frac{n}{x}})^{x}\\leq e^{n}$ Therfore, w ", "page_idx": 19}, {"type": "text", "text": "Lemma C.7. [Multiple iterations of local gradient acumulation in each round] Under Assumption C.l,if welet $\\begin{array}{r}{\\eta^{\\bar{t}}\\leq\\frac{\\hat{1}}{2S\\tau}}\\end{array}$ andrun client n's local model for $\\tau$ iteration continuously in any round $t$ have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert g_{n}^{t,i}-g_{n}^{t}\\right\\Vert^{2}\\right]\\leq8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x}F_{n}\\left(\\pmb{x}^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k}-\\mathbf{g}_{t}^{k}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k}-\\mathbf{g}_{t}^{k,\\delta}-\\mathbf{g}_{t}^{k,-1}-\\mathbf{g}_{t}^{k,1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq(1+\\tau)\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,\\delta}-\\mathbf{g}_{t}^{k,-1}\\right\\Vert^{2}\\right]+\\left(1+\\frac{1}{\\tau}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,\\delta-1}-\\mathbf{g}_{t}^{k}\\right\\Vert^{2}\\right]}\\\\ &{\\leq(1+\\tau)\\delta^{2}\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,\\delta}-\\mathbf{g}_{t}^{k,-1}\\right\\Vert^{2}\\right]+\\left(1+\\frac{1}{\\tau}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,1}-\\mathbf{g}_{t}^{k,1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq(1+\\tau)\\left(\\eta^{2}\\right)^{2}\\delta^{2}2\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,-1}\\right\\Vert^{2}\\right]+\\left(1+\\frac{1}{\\tau}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,-1}-\\mathbf{g}_{t}^{k,1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq(1+\\tau)\\left(\\eta^{2}\\right)^{2}\\delta^{2}2\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,\\delta-1}-\\mathbf{g}_{t}^{k,+}\\mathbf{g}_{t}^{k}\\right\\Vert^{2}\\right]+\\left(1+\\frac{1}{\\tau}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,-1}-\\mathbf{g}_{t}^{k,1}\\right\\Vert^{2}\\right]}\\\\ &{\\leq2(1+\\tau)\\left(\\eta^{2}\\right)^{2}\\delta^{2}\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,\\delta-1}-\\mathbf{g}_{t}^{k,1}\\right\\Vert^{2}\\right]+2(1+\\tau)\\left(\\eta^{2}\\right)^{2}\\delta^{2}\\mathbb{E}\\left[\\left\\Vert\\mathbf{g}_{t}^{k,-1}-\\mathbf{g}_{t}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We define the following notation for simplicity: ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{t,i}:=\\mathbb{E}\\left[\\left\\lvert\\pmb{\\mathscr{g}}_{n}^{t,i}-\\pmb{\\mathscr{g}}_{n}^{t}\\right\\rvert\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{B:=2\\left(1+\\tau\\right)\\left(\\eta^{t}\\right)^{2}S^{2}\\mathbb{E}\\left[\\left\\Vert g_{n}^{t}\\right\\Vert^{2}\\right]}\\\\ {C:=\\left(1+\\displaystyle\\frac{2}{\\tau}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We have ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{t,i}\\leq C A_{t,i-1}+B\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can show that ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{t,i}\\leq C^{i}A_{t}+B\\sum_{j=0}^{i-1}C^{j}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that $A_{t}=\\mathbb{E}\\left[\\left\\|\\pmb{{g}}_{n}^{t}-\\pmb{{g}}_{n}^{t}\\right\\|^{2}\\right]=0$ For the second par, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert g_{n}^{t,i}-g_{n}^{t}\\right\\Vert^{2}\\right]=\\displaystyle\\sum_{i=0}^{\\tau-1}B\\sum_{j=0}^{i-1}C^{j}\\leq2\\tau^{2}B}\\\\ &{\\leq4\\tau^{2}\\left(1+\\tau\\right)\\left(\\eta^{t}\\right)^{2}S^{2}\\mathbb{E}\\left[\\left\\Vert g_{n}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\mathbb{E}\\left[\\left\\Vert g_{n}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "D Proof for Theorem 3.6 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We organize the proof of Theorem 3.6 as follows: ", "page_idx": 21}, {"type": "text", "text": "\u00b7 In Sec. D.1, we prove the strongly convex case.   \n\u00b7 In Sec. D.2, we prove the general convex case.   \n\u00b7 In Sec. D.3, we prove the non-convex case. ", "page_idx": 21}, {"type": "text", "text": "D.1 Strongly convex case for SFL-V1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "D.1.1 One-round Parallel Update for M-Server-Side Model ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma D.1. Under Assumptions C.1 and C.2, if $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tilde{\\tau}}}\\end{array}$ ,inround $t$ . the M-server-side model evolvesas ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\left(1-\\frac{\\eta^{t}\\tilde{\\tau}\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tilde{\\tau}\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\left(\\tilde{\\tau}\\right)^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\left(\\tilde{\\tau}\\right)^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We prove Lemma D.1 as follows. ", "page_idx": 21}, {"type": "text", "text": "Prof. We use $\\pmb{x}_{s,n}^{t,i}$ as the M-serverside model when the M-server interacts with clent $n$ fof the -th iteration of model training at round Using the (sequential) gradient update rule of $\\begin{array}{r}{\\pmb{x}_{s}^{t+1}=\\pmb{x}_{s}^{t}-\\eta^{t}\\sum_{n=1}^{N}\\sum_{i=0}^{\\tilde{\\tau}-1}a_{n}\\pmb{g}_{s,n}^{t,i}\\left(\\left\\{\\pmb{x}_{c,n}^{t,i},\\pmb{x}_{s,n}^{t,i}\\right\\}\\right)}\\end{array}$ wehave ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{-\\mathbb{E}\\left[\\left|\\left|x_{c}^{t}-x_{c}^{t}\\right|^{\\frac{t}{2}}\\!\\!\\!\\right|_{0}^{t}\\!\\!\\!\\right|_{0}^{t}-x_{c}^{t}-\\eta_{c}^{t+1}\\sum_{\\ell=0}^{t-1}\\mathbb{E}\\left[\\left\\{x_{c}^{t},x_{c}^{t,\\ell+1}\\right\\}\\right]+\\eta_{c}^{t-1}\\sum_{\\ell=0}^{t-1}\\mathbb{E}\\left[\\left\\{x_{c}^{t},x_{c}^{t,\\ell+1}\\right\\}\\right]\\right|}\\\\ &{=\\mathbb{E}\\left[\\Bigg|x_{c}^{t}-x_{c}^{t}-\\eta_{c}^{t+1}\\mathbb{E}_{x}\\int\\left\\{\\left(x_{c}^{t},x_{c}^{t}\\right)\\right\\}\\Bigg|\\right]^{\\frac{t}{2}}}\\\\ &{+2\\eta^{t}\\mathbb{E}\\left[\\Bigg\\langle x_{c}^{t}-x_{c}^{t}-\\eta_{c}^{t+1}\\!\\!\\!\\Bigg\\rangle_{0}^{t-1}\\mathbb{E}_{x}f\\left(\\left\\{x_{c}^{t,\\ell},x_{c}^{t,\\ell}\\right\\}\\right)\\Bigg|\\right]}\\\\ &{+\\mathbb{E}\\left[\\left\\{\\eta_{c}^{t}\\right\\}\\Bigg|\\left|\\sum_{0}^{t-1}\\!\\!\\!\\!\\right|_{0}^{t}\\!\\!\\!\\right|_{0}^{t}\\!\\!\\!\\right|_{0}^{t-1}\\!\\!\\!\\!\\!\\!-\\!\\!\\mathbb{E}_{\\eta}f\\left\\{\\left\\{x_{c}^{t,\\ell},x_{c}^{t,\\ell}\\right\\}\\right\\}\\Bigg|\\right]^{\\frac{t}{2}-1}\\!\\!\\!\\!\\nabla_{\\alpha}f\\left(\\left\\{x_{c}^{t,\\ell},x_{c}^{t,\\ell}\\right\\}\\right)-\\frac{t-1}{\\sum_{0}^{t}\\theta_{c}}\\eta_{c}^{t,\\ell}\\Bigg\\}\\Bigg]}\\\\ &{+\\mathbb{E}\\left[\\left(\\eta_{c}^{t}\\right)\\Bigg|\\left|\\sum_{0}^{t-1}\\!\\!\\!\\!\\right|_{0}^{t}\\!\\!\\!\\right|_{0}^{t-1}\\!\\!\\!\\!\\!-\\!\\!\\frac{t-1}{\\sum_{0}^{t}}\\nabla_{\\alpha}f\\left(\\left\\{x_{c}^{t,\\ell},x_{\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the second equality is from $\\left(a+b\\right)^{2}\\;=\\;a^{2}\\,+\\,2a b\\,+\\,b^{2}$ and the last inequality is due to $\\mathbb{E}\\left[\\nabla_{{\\pmb x}_{s}}f\\left(\\left\\{{\\pmb x}_{c}^{t,i},{\\pmb x}_{s}^{t,i}\\right\\}\\right)-{\\pmb g}_{s}^{t,i}\\right]=0$ ", "page_idx": 21}, {"type": "text", "text": "The first part in (54) is ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|x_{s}^{t}-x_{s}^{*}-\\eta^{t}\\underset{i=0}{\\overset{\\sim}{\\sum}}\\nabla_{x_{s}}f\\left(\\left\\{x_{c}^{t,i},x_{s}^{t,i}\\right\\}\\right)\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\mathbb{E}\\left[\\left\\|x_{s}^{t}-x_{s}^{*}\\right\\|^{2}\\right]+\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}N\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tilde{\\tau}-1}a_{n}^{2}\\mathbb{E}\\left[\\left\\|\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\|^{2}\\right]}\\\\ {\\displaystyle-\\,2\\eta^{t}\\mathbb{E}\\left[\\sum_{n=1}^{N}\\sum_{i=0}^{\\tilde{\\tau}-1}a_{n}\\left\\langle x_{s}^{t}-x_{s}^{*},\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c}^{t,i},x_{s}^{t,i}\\right\\}\\right)\\right\\rangle\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "wherewe use $\\begin{array}{r}{\\nabla_{x_{s}}f\\left(\\left\\{x_{c},x_{s}\\right\\}\\right)=\\sum_{n=1}^{N}a_{n}\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c},x_{s}\\right\\}\\right)\\!.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "For (55), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}N\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tilde{\\tau}-1}a_{n}^{2}\\mathbb{E}\\left[\\left\\lVert\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\rVert^{2}\\right]}\\\\ &{\\displaystyle=\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}N\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tilde{\\tau}-1}a_{n}^{2}\\mathbb{E}\\left[\\left\\lVert\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right.}\\\\ &{\\displaystyle\\left.-g_{s,n}^{t,i}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\rVert^{2}\\right]+\\mathbb{E}\\left[\\left\\lVert g_{s,n}^{t,i}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\rVert^{2}\\right]}\\\\ &{\\displaystyle\\leq\\left(\\eta^{t}\\right)^{2}\\left(\\tilde{\\tau}\\right)^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(\\sigma_{n}^{2}+G^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality applies triangle inequality. In the last inequality, we apply the bound of variance and expected squared norm for stochastic gradients in Assumption C.1. ", "page_idx": 22}, {"type": "text", "text": "Since $F_{n}(x)$ is $S$ -smooth and $\\mu$ -strongly convex, using Lemma C.3 we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle-\\,2\\eta^{t}\\mathbb{E}\\left[\\sum_{n=1}^{N}\\sum_{i=0}^{\\widetilde{\\tau}-1}a_{n}\\left\\langle\\pmb{x}_{s}^{t}-\\pmb{x}_{s}^{*},\\nabla_{x_{s}}F_{n}\\left(\\left\\{\\pmb{x}_{c,n}^{t,i},\\pmb{x}_{s,n}^{t,i}\\right\\}\\right)\\right\\rangle\\right]}\\\\ &{\\displaystyle\\leq-2\\eta^{t}\\sum_{n=1}^{N}\\sum_{i=0}^{\\widetilde{\\tau}-1}a_{n}\\mathbb{E}\\left[\\left(F_{n}\\left(\\pmb{x}^{t}\\right)-F_{n}\\left(\\pmb{x}^{*}\\right)\\right.\\right.}\\\\ &{\\displaystyle\\left.\\left.+\\frac{\\mu}{4}\\left\\|\\pmb{x}_{s}^{t}-\\pmb{x}_{s}^{*}\\right\\|^{2}-S\\left\\|\\pmb{x}_{s,n}^{t,i}-\\pmb{x}_{s}^{t}\\right\\|^{2}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma C.5, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{N}\\sum_{i=0}^{\\tilde{\\tau}-1}a_{n}\\mathbb{E}\\left[\\left\\|x_{s,n}^{t,i}-x_{s}^{t}\\right\\|^{2}\\right]\\leq12\\sum_{n=1}^{N}a_{n}\\left(\\tilde{\\tau}\\right)^{3}\\left(\\eta^{t}\\right)^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "From Assumption C.1, the second part in (54) is bounded by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tilde{\\tau}-1}a_{n}{g}_{s,n}^{t,i}-\\sum_{i=0}^{\\tilde{\\tau}-1}\\nabla_{x_{s}}f\\left(\\left\\{{x}_{c}^{t,i},{x}_{s}^{t,i}\\right\\}\\right)\\right\\|^{2}}\\\\ &{\\quad\\leq\\tilde{\\tau}\\displaystyle\\sum_{i=0}^{\\tilde{\\tau}-1}\\mathbb{E}\\left\\|\\displaystyle\\sum_{n=1}^{N}a_{n}\\left({g}_{s,n}^{t,i}-\\nabla_{x_{s}}F_{n}\\left(\\left\\{{x}_{c}^{t,i},{x}_{s}^{t,i}\\right\\}\\right)\\right)\\right\\|^{2}}\\\\ &{\\quad\\leq N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\sigma_{n}^{2}\\left(\\tilde{\\tau}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, by $\\textstyle\\sum_{n=1}^{N}a_{n}=1$ (54) becomes ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\|\\pmb{x}_{s}^{t+1}-\\pmb{x}_{s}^{*}\\right\\|^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\|\\pmb{x}_{s}^{t}-\\pmb{x}_{s}^{*}\\right\\|^{2}\\right]+\\left(\\eta^{t}\\right)^{2}\\left(\\tilde{\\tau}\\right)^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(\\sigma_{n}^{2}+G^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{-2\\eta^{t}\\frac{\\Bar{\\tau}}{\\tau_{n}-1}\\alpha_{n}\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}}\\\\ &{-\\frac{\\mu\\Bar{\\tau}\\eta^{t}\\sum_{n=1}^{N}\\alpha_{n}}{2}\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}+2\\eta^{t}\\left(12\\sum_{n=1}^{N}\\alpha_{n}S\\left(\\Bar{\\tau}\\right)^{3}\\left(\\eta^{t}\\right)^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)}\\\\ &{+N\\displaystyle\\sum_{n=1}^{N}\\alpha_{n}^{2}\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}\\left(\\Bar{\\tau}\\right)^{2}}\\\\ &{\\le\\left(1-\\frac{\\eta^{t}\\Bar{\\tau}\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\Bar{\\tau}\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}\\left(\\Bar{\\tau}\\right)^{2}N\\displaystyle\\sum_{n=1}^{N}\\alpha_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\left(\\Bar{\\tau}\\right)^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}\\alpha_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We now prove the convergence error. Let $\\Delta^{t+1}\\triangleq\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-x_{s}^{*}\\right\\|^{2}\\right]$ We can rewrite (60) as: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta^{t}\\tilde{\\tau}\\mu}{2}\\right)\\Delta^{t}-2\\eta^{t}\\tilde{\\tau}\\mathbb{E}\\left[f\\left({\\pmb x}^{t}\\right)-f\\left({\\pmb x}^{*}\\right)\\right],}}\\\\ {{\\displaystyle\\qquad+\\left(\\eta^{t}\\right)^{2}\\left(\\tilde{\\tau}\\right)^{2}N\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\left(\\tilde{\\tau}\\right)^{3}\\left(\\eta^{t}\\right)^{3}\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right),}}\\\\ {{\\displaystyle\\qquad\\leq\\left(1-\\frac{\\eta^{t}\\tilde{\\tau}\\mu}{2}\\right)\\Delta^{t}+\\frac{\\left(\\eta^{t}\\right)^{2}\\left(\\tilde{\\tau}\\right)^{2}}{4}B_{1}+\\frac{\\left(\\eta^{t}\\right)^{3}\\left(\\tilde{\\tau}\\right)^{3}}{8}B_{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\begin{array}{r}{B_{1}:=4N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\end{array}$ and $\\begin{array}{r}{B:=192S\\,\\sum_{n=1}^{N}a_{n}\\,\\bigl(2\\sigma_{n}^{2}+G^{2}\\bigr).}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "ef \uff0cie\uff0c $\\begin{array}{r}{\\frac{\\eta^{t}\\tilde{\\tau}}{2}\\,=\\,\\frac{\\beta}{\\gamma+t}}\\end{array}$ $\\begin{array}{r}{\\beta\\,=\\,\\frac{2}{\\mu},\\gamma\\,=\\,\\frac{8S}{\\mu}\\,-\\,1}\\end{array}$ $\\begin{array}{r}{\\eta^{t}\\,\\leq\\,\\frac{1}{2S\\tilde{\\tau}}}\\end{array}$ $t$ $\\begin{array}{r}{\\Delta^{t+1}\\,\\le\\,\\frac{v}{\\gamma+t+1}}\\end{array}$ $v=$ $\\begin{array}{r}{\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}(\\gamma+1)},(\\gamma+1)\\Delta^{0}\\right\\}}\\end{array}$ $v$ enses that it holds for $t=-1$ . Assume the conclusion holds for some $t$ , it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta\\sqrt{t}}{2}\\right)\\Delta^{t}+\\frac{(\\eta^{\\prime})^{2}(\\frac{\\eta^{\\prime}}{2})^{2}}{4}B_{1}+\\frac{(\\eta^{\\prime})^{3}(\\frac{\\eta^{\\prime}}{2})^{3}}{8}B_{2}}\\\\ &{\\leq\\left(1-\\frac{\\eta\\beta}{\\sqrt{4}}\\right)\\frac{\\tau}{\\eta}+\\frac{(\\eta^{\\prime})^{4}(\\frac{\\eta^{\\prime}}{2})^{2}}{4}B_{1}+\\frac{(\\eta^{\\prime})^{3}(\\frac{\\eta^{\\prime}}{2})^{3}}{8}B_{2}}\\\\ &{=\\frac{\\tau+t-1}{(\\gamma+t)^{2}}\\nu+\\left[\\left(\\frac{\\beta^{2}B_{1}}{(\\gamma+t)^{2}}+\\frac{\\beta^{3}B_{2}}{(\\gamma+t)^{3}}-\\frac{\\beta\\mu-1}{(\\gamma+t)^{2}}\\right)\\right.}\\\\ &{=\\frac{\\tau+t-1}{(\\gamma+t)^{2}}\\nu+\\left[\\left(\\frac{\\beta^{2}B_{1}}{(\\gamma+t)^{2}}+\\frac{\\beta^{3}B_{2}}{(\\gamma+t)^{3}}-\\frac{\\beta\\mu-1}{(\\gamma+t)^{2}}\\operatorname*{max}\\left\\{\\frac{\\beta B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}(\\gamma+t)},(\\gamma+1)\\Delta^{9}\\right\\}\\right]\\right.}\\\\ &{=\\frac{\\gamma+t-1}{(\\gamma+t)^{2}}\\nu+\\frac{\\left.\\beta^{2}B_{1}}{(\\gamma+t)^{2}}+\\frac{\\beta^{3}B_{2}}{(\\gamma+t)^{3}}-\\frac{\\beta\\mu-1}{(\\gamma+t)^{2}}\\operatorname*{max}\\left\\{\\frac{\\beta^{2}B_{1}}{\\beta\\mu-1}+\\frac{\\beta^{3}B_{2}}{(\\beta\\mu-1)(\\gamma+t)},(\\gamma+1)\\Delta^{9}\\right\\}}\\\\ &{\\leq\\frac{\\gamma+t-1}{(\\gamma+t)^{2}}\\nu+\\frac{1}{(\\gamma+t)^{3}}-\\frac{\\beta\\mu-1}{(\\gamma+t)^{2}}\\operatorname*{max}\\left\\{\\frac{\\beta^{2}B_\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, we have proven that $\\begin{array}{r}{\\Delta^{t}\\leq\\frac{v}{\\gamma+t},\\forall t}\\end{array}$ . Therefore, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]=\\Delta^{t}\\leq\\frac{v}{\\gamma+t}=\\frac{\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}(\\gamma+1)},(\\gamma+1)\\mathbb{E}\\left[\\left\\Vert x_{s}^{0}-x_{s}^{*}\\right\\Vert^{2}\\right]\\right\\}}{\\gamma+t}}\\\\ &{\\leq\\frac{16N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{2}\\left(\\gamma+t\\right)}+\\frac{1536S\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma+t\\right)\\left(\\gamma+1\\right)}+\\frac{(\\gamma+1)\\mathbb{E}\\left[\\left\\Vert x_{s}^{0}-x_{s}^{*}\\right\\Vert^{2}\\right]}{\\gamma+t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.1.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Under Assumptions C.1 and C.2, if $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , in round $t$ , Lemma D.1 gives ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\left(1-\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\left(\\tau\\right)^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\left(\\tau\\right)^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\Delta^{t+1}\\triangleq\\mathbb{E}\\left[\\left\\|\\mathbf{}x_{c}^{t+1}-x_{c}^{*}\\right\\|^{2}\\right]$ We can rewrite (70) as: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\Delta^{t}+\\frac{\\left(\\eta^{t}\\right)^{2}\\left(\\tau\\right)^{2}}{4}B_{1}+\\frac{\\left(\\eta^{t}\\right)^{3}\\left(\\tau\\right)^{3}}{8}B_{2}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\begin{array}{r}{B_{1}:=4N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\end{array}$ and $\\begin{array}{r}{B_{2}:=192S\\,\\sum_{n=1}^{N}a_{n}\\,\\bigl(2\\sigma_{n}^{2}+G^{2}\\bigr).}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Consider diminishingsepize $\\begin{array}{r}{\\eta^{t}=\\frac{2\\beta}{\\tau(\\gamma+t)}}\\end{array}$ ,ie, $\\begin{array}{r}{\\frac{\\eta^{t}\\tau}{2}=\\frac{\\beta}{\\gamma+t}}\\end{array}$ #, where \u03b2 =2 =  - 1. Itis easy to show that $\\begin{array}{r}{\\eta^{t}\\,\\leq\\,\\frac{1}{2S\\tau}}\\end{array}$ for all $t$ For [B + +1); (/ + 1)], we can prove that $\\begin{array}{r}{\\Delta^{t}\\leq\\frac{v}{\\gamma+t},\\forall t}\\end{array}$ Therefore, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]=\\Delta^{t}\\leq\\frac{v}{\\gamma+t}=\\frac{\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}(\\gamma+1)},(\\gamma+1)\\mathbb{E}\\left[\\left\\Vert x_{c}^{0}-x_{c}^{*}\\right\\Vert^{2}\\right]\\right\\}}{\\gamma+t}}\\\\ &{\\leq\\frac{16N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{2}\\left(\\gamma+t\\right)}+\\frac{1536S\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma+t\\right)\\left(\\gamma+1\\right)}+\\frac{(\\gamma+1)\\mathbb{E}\\left[\\left\\Vert x_{c}^{0}-x_{c}^{*}\\right\\Vert^{2}\\right]}{\\gamma+t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.1.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We merge the M-server-side and client-side models in (69) and (72) using Proposition 3.5 by setting $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\operatorname*{max}\\left\\{\\tau,\\tilde{\\tau}\\right\\}}}\\end{array}$ Wehave ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(\\mathbf{x}^{T})\\right]-f(\\mathbf{x}^{*})}\\\\ &{\\leq\\displaystyle\\frac{S}{2}\\left(\\mathbb{E}||\\mathbf{x}_{s}^{T}-\\mathbf{x}_{s}^{*}||^{2}+\\mathbb{E}||\\mathbf{x}_{c}^{T}-\\mathbf{x}_{c}^{*}||^{2}\\right)}\\\\ &{\\leq\\displaystyle\\frac{8S N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{2}\\left(\\gamma+T\\right)}+\\frac{768S^{2}\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma+T\\right)\\left(\\gamma+1\\right)}+\\frac{S(\\gamma+1)\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}^{0}-\\mathbf{x}^{*}\\right\\Vert^{2}\\right]}{2(\\gamma+T)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "D.2 General convex case for SFL-V1 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "D.2.1  One-round Parallel Update for M-Server-Side Model ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "By Lemma D.1 with $\\mu=0$ and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tilde{\\tau}}}\\end{array}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tilde{\\tau}\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D.2.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "By Lemma D.1 with $\\mu=0$ and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D.2.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We merge the M-server-side and client-side models in (74) and (75) as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big\\|x^{t+1}-x^{*}\\Big\\|^{2}\\leq\\mathbb{E}\\Big[\\|x_{s}^{t+1}-x_{s}^{*}\\|^{2}\\Big]+\\mathbb{E}\\Big[\\|x_{c}^{t+1}-x_{s}^{*}\\|^{2}\\Big],}\\\\ &{\\leq\\mathbb{E}\\Big[\\|x_{s}^{t}-x_{s}^{*}\\|^{2}\\Big]-2\\eta^{t}\\mathbb{E}\\big[f^{*}(x^{t})-f(x^{*})\\big]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}\\tilde{\\eta}^{2}\\Lambda\\sum_{n=1}^{M}\\alpha_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S^{2}\\,\\big(\\eta^{t}\\big)^{3}\\sum_{n=1}^{N}\\alpha_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{+\\mathbb{E}\\Big[\\|x_{c}^{t}-x_{s}^{*}\\|^{2}\\Big]-2\\eta^{t}\\mathbb{E}\\big[f^{*}(x^{t})-f(x^{*})\\big]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\sum_{n=1}^{M}\\alpha_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S^{2}\\,\\big(\\eta^{t}\\big)^{3}\\sum_{n=1}^{N}\\alpha_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{=\\mathbb{E}\\Big[\\big\\|x^{t}-x^{*}\\big\\|^{2}\\Big]-4\\eta^{t}\\operatorname*{min}\\left\\{\\tau,\\tilde{\\tau}\\right\\}\\mathbb{E}\\left[f^{*}\\left(x^{*}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}N\\sum_{n=1}^{N}\\alpha_{n}^{2}\\big(\\eta^{2}+\\tau^{2}\\big)\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\,\\big(\\eta^{t}\\big)^{3}\\sum_{n=1}^{N}\\alpha_{n}\\left(\\tilde{\\tau}^{3}+\\tau^{3}\\right)\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then, we can obtain the relation between $\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}^{t+1}-\\mathbf{x}^{*}\\right\\Vert^{2}\\right]$ and $\\mathbb{E}\\left[\\left\\|\\pmb{x}^{t}-\\pmb{x}^{*}\\right\\|^{2}\\right]$ , which is related to E[f (xt) - f (a\\*)] Applying Lemma 8in [17] and let Tmin := min {,T} and n \u2264 2Smax(,], we obtain the performance bound as ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[f\\left(x^{T}\\right)\\right]-f\\left(x^{*}\\right)}\\\\ {\\displaystyle\\leq\\frac{1}{2}\\left(\\frac{\\tilde{\\tau}^{2}+\\tau^{2}}{\\tau_{\\mathrm{min}}^{2}}N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac{1}{2}}\\left(\\frac{\\left\\Vert x^{0}-x^{*}\\right\\Vert^{2}}{T+1}\\right)^{\\frac{1}{2}}}\\\\ {\\displaystyle+\\,\\frac{1}{2}\\left(\\frac{\\tilde{\\tau}^{2}+\\tau^{2}}{\\tau_{\\mathrm{min}}^{2}}24S\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac{1}{3}}\\left(\\frac{\\left\\Vert x^{0}-x^{*}\\right\\Vert^{2}}{T+1}\\right)^{\\frac{1}{3}}+\\frac{S\\left\\Vert x^{0}-x^{*}\\right\\Vert^{2}}{2(T+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D.3Non-convex case for SFL-V1 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "D.3.1 One-round Parallel Update for M-Server-Side Model ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "For the server, we have ", "page_idx": 26}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\left[\\nabla_{x}f\\left(x^{t}\\right)\\right]x^{t+1}-x^{t}\\right\\}}\\\\ &{\\leq\\mathbb{E}\\left\\{\\nabla_{x}f\\left(x^{t}\\right)\\Big\\}x^{t}-\\int_{0}^{1}\\sum_{u=1}^{t-1}\\mathcal{E}\\left[\\mathcal{F}_{u,t}f\\left(x^{t}\\right)-f^{\\prime}\\left(T^{t}\\mathcal{E}_{u},f^{\\prime}\\right)\\right]}\\\\ &{\\leq\\mathbb{E}\\left\\{\\nabla_{x}f\\left(x^{t}\\right)\\Big\\}^{2}\\mathbb{E}\\left[\\mathcal{F}_{u,t}f\\left(x^{t}\\right),x^{t}-x^{t}+\\eta^{2}\\mathbb{E}\\left\\{\\left.T^{t}\\mathcal{E}_{u}f\\left(x^{t}\\right)\\right\\}-\\left.\\mathcal{F}\\left(f^{\\prime}\\right)\\right]\\right\\}}\\\\ &{\\leq\\Big\\{\\nabla_{x}f\\left(x^{t}\\right)\\int_{0}^{1}\\sum_{u=1}^{t-1}\\mathcal{E}_{u}f_{u,t}^{\\prime}\\Big\\}+\\eta^{2}\\mathbb{E}\\Psi_{u,t}\\int_{0}^{1}\\bigg\\}\\times\\frac{\\eta^{2}}{2}\\int_{0}^{1}\\mathbb{E}\\Psi_{u,t}\\int_{0}^{1}\\Big\\}}\\\\ &{\\leq\\bigg\\{\\nabla_{x}f\\left(x^{t}\\right)\\leq\\left[\\int_{0}^{1}\\sum_{u=1}^{t-1}\\sum_{u=1}^{t-1}\\mathcal{E}_{u}f_{u,t}^{\\prime}\\left(x^{t}\\right)\\right]+\\eta^{2}\\Psi_{u,t}\\int_{0}^{1}\\bigg\\}-\\eta^{2}\\left\\{\\nabla_{x}f\\left(x^{t}\\right)\\right\\}-\\eta^{2}\\left\\{\\right\\}}\\\\ &{\\leq\\bigg\\{\\nabla_{x}f\\left(x^{t}\\right)\\leq\\bigg\\}^{2}\\sqrt{-\\bigg\\lvert\\frac{\\eta^{2}}{2}}\\sum_{u=1}^{t-1}\\nabla_{x}f_{u,t}\\left(\\left(x_{u,t}^{t}\\right)\\right)}\\\\ &{\\leq\\bigg\\{\\nabla_{x}f\\left(x^{t}\\right)\\leq\\int_{0}^{1}\\sum_{u=1}^{t-1}\\sum_{u=1}^{t-1}\\Phi_{u,t}\\int_{0}^{1}\\left(\\left(\\frac{\\eta^{2}}{\\epsilon}\\right)x^{t}-x^{t}\\right)+\\eta^{2}$ $\\begin{array}{r l}&{\\quad\\biggl<\\nabla_{\\mathbf{x}}\\int_{\\mathbf{x}}^{\\infty}\\mathrm{e}^{-\\mathbf{u}}[\\mathrm{e}^{-\\mathbf{u}}]}\\\\ &{\\le\\biggl<\\nabla_{\\mathbf{x}}\\int_{\\mathbf{x}}^{\\infty}\\biggl|\\nabla_{\\mathbf{x}}f\\left(x\\right)^{2}\\frac{1}{2}\\Biggl|\\sum_{i=1}^{N}\\mathrm{e}_{\\mathbf{x}}\\int_{\\mathbf{x}_{i}}P_{i}\\left(x_{i+\\infty}^{\\varepsilon,i},x_{i}^{\\varepsilon}\\right)+\\eta_{\\infty}^{\\varepsilon}\\sum_{i=1}^{N}\\mathrm{e}_{\\mathbf{x}}\\int_{\\mathbf{x}_{i}}P_{i}\\left(x_{i}^{\\varepsilon}\\right)\\biggr|\\Biggr>}\\\\ &{\\quad-\\eta^{\\varepsilon}\\mathrm{e}^{-\\mathbf{u}}[\\mathrm{e},f]\\left[\\mathrm{e},f^{\\varepsilon}\\right]^{2}\\Biggr|}\\\\ &{\\le\\biggl<\\nabla_{\\mathbf{x}}\\bigg/^{\\varepsilon}\\biggl<\\nabla_{\\mathbf{x}}\\int_{\\mathbf{x}}^{\\infty}\\mathrm{e}^{-\\mathbf{u}}[\\mathrm{e},f^{\\varepsilon}]-\\frac{1}{2}\\sum_{i=1}^{N}\\mathrm{e}_{\\mathbf{x}}\\int_{\\mathbf{x}_{i}}P_{i}\\left(x_{i+\\infty}^{\\varepsilon,i},x_{i}^{\\varepsilon}\\right)+\\frac{1}{2}\\sum_{i=1}^{N}\\mathrm{e}_{\\mathbf{x}}^{-\\mathbf{u}}[\\mathrm{e},f^{\\varepsilon}]\\biggr|\\Biggr>}\\\\ &{\\quad-\\eta^{\\varepsilon}\\mathrm{e}^{-\\mathbf{u}}[\\mathrm{e},f^{\\varepsilon}]\\biggr|^{2}}\\\\ &{\\le\\frac{\\eta_{\\mathbf{x}}^{2}}{2}\\biggl|\\nabla_{\\mathbf{x}}f\\left(x\\right)^{2}\\biggr|^{2}+\\frac{\\eta_{\\mathbf{x}}^{2}}{2}\\mathbb{E}\\Biggl[\\left|\\sum_{i=1}^{N}\\mathrm{e}_{\\mathbf{x}}^{-\\mathbf{u}}[\\mathrm{e},f^{\\varepsilon}]-[\\mathrm{e},f^{\\varepsilon}]\\right|^{2}\\Biggr]-\\frac{\\eta_{\\mathbf{x}}^{2}}{2}\\mathrm{e}_{\\mathbf{x}}^{-\\mathbf{u}}[\\mathrm{e},f^{\\varepsilon}]\\Biggr|^{2}\\Biggr|}\\\\ &{\\quad-\\eta^{\\varepsilon}\\mathrm{e}^{-\\mathbf{$ ", "page_idx": 26}, {"type": "text", "text": "wherewe applyAssumptionC.1, $\\begin{array}{r}{\\nabla_{x_{s}}f\\left(\\pmb{x}^{t}\\right)=\\sum_{n=1}^{N}a_{n}\\nabla_{\\pmb{x}_{s}}F_{n}\\left(\\pmb{x}^{t}\\right)}\\end{array}$ and $\\begin{array}{r}{\\langle a,b\\rangle\\leq\\frac{a^{2}+b^{2}}{2}}\\end{array}$ ", "page_idx": 26}, {"type": "text", "text": "By Lemma C.6 with $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{\\sqrt{8}S\\tilde{\\tau}}}\\end{array}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tilde{\\tau}-1}\\mathbb{E}\\left[\\left\\Vert x_{s,n}^{t,i}-x_{s}^{t}\\right\\Vert^{2}\\right]\\leq2\\tilde{\\tau}^{2}\\left(8\\tilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\tilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\tilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x_{s}^{t}\\right)\\right\\Vert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, (78) becomes ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla_{x_{s}}f\\left(x^{t}\\right),x_{s}^{t+1}-x_{s}^{t}\\right\\rangle\\right]}\\\\ &{\\le-\\frac{\\eta^{t}\\widetilde{\\tau}}{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\frac{N\\eta^{t}S^{2}}{2}\\underset{n=1}{\\overset{N}{\\sum}}a_{n}^{2}\\widetilde{\\tau}^{2}\\left(8\\widetilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\widetilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\widetilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x_{s}^{t}\\right)\\right\\Vert^{2}\\right)}\\\\ &{\\le\\left(-\\frac{\\eta^{t}\\widetilde{\\tau}}{2}+8N\\left(\\eta^{t}\\right)^{3}\\widetilde{\\tau}^{3}S^{2}\\underset{n=1}{\\overset{N}{\\sum}}a_{n}^{2}\\right)\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+8N\\eta^{t}S^{2}\\widetilde{\\tau}^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(\\eta^{t}\\right)^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{S}{2}\\mathbb{E}\\left[\\left\\|\\mathbf{z}_{1}^{k+1}-\\mathbf{z}_{1}^{k}\\right\\|^{2}\\right]}\\\\ &{=\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{N}{\\sum_{m}^{\\infty}}\\mathbb{E}\\left[\\left\\|\\frac{1}{\\sum_{l=0}^{k}}\\alpha_{l,n}\\mathbf{z}_{l}^{k}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{N}{\\sum_{m}^{\\infty}}\\alpha_{m}^{2}\\mathbb{E}\\left[\\left\\|\\frac{1}{\\sum_{l=0}^{k}\\theta_{m}^{\\star}}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{\\tilde{j}}{\\sum_{m}^{\\infty}}\\frac{S\\cdot w_{m}^{2}\\left\\|\\mathbf{z}_{l}^{k}\\right\\|^{2}}{S\\cdot w_{m}^{2}}\\left\\|\\left\\|\\frac{1}{S}\\right\\|^{2}}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{\\tilde{j}}{\\sum_{m}^{\\infty}}\\frac{S\\cdot w_{m}^{2}}{S_{m}^{\\infty}}\\mathbb{E}\\left[\\left\\|\\mathbf{z}_{l}^{k}-\\mathbf{z}_{m}^{k},\\mathbf{z}_{m}^{k},\\mathbf{z}_{m}^{k}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{S\\cdot w_{m}^{2}\\left\\|\\mathbf{z}_{m}^{k}-\\mathbf{z}_{m}^{k}\\right\\|^{2}}{\\sum_{m}^{\\infty}\\frac{S\\cdot w_{m}^{2}}{S_{m}^{\\infty}}\\left(\\mathbb{E}\\left[\\left\\|\\mathbf{z}_{m}^{k}-\\mathbf{z}_{m}^{k}\\right\\|^{2}\\right]+\\mathbb{E}\\left[\\left\\|\\mathbf{z}_{m}^{k}\\right\\|^{2}\\right]\\right)}}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{\\tilde{j}}{\\sum_{m}^{\\infty}}\\frac{S\\cdot w_{m}^{2}}{S_{m}^{\\infty}}\\left(\\mathbb{E}\\left[\\left\\|\\mathbf{z}_{m}^{\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last line uses Assumption C.1 and $\\mathbb{E}\\left[\\lvert\\lvert\\mathbf{z}\\rvert\\rvert^{2}\\right]=\\lvert\\lvert\\mathbb{E}[\\mathbf{z}]\\rvert\\rvert^{2}+\\mathbb{E}[\\lvert\\lvert\\mathbf{z}-\\mathbb{E}[\\mathbf{z}]\\rvert\\rvert^{2}]$ for any random variable $\\mathbf{z}$ ", "page_idx": 27}, {"type": "text", "text": "By Lemma C.7 with $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tilde{\\tau}}}\\end{array}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tilde{\\tau}-1}\\mathbb{E}\\left[\\left\\Vert g_{s,n}^{t,i}-g_{s,n}^{t}\\right\\Vert^{2}\\right]\\leq8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(\\pmb{x}^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, (81) becomes ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{S}{2}\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\sum_{n=1}^{N}\\alpha_{n}^{2}\\left(8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)+\\tilde{\\tau}\\mathbb{E}\\left[\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right]\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\sum_{n=1}^{N}\\alpha_{n}^{2}\\left(\\tilde{\\tau}+8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\sum_{n=1}^{N}\\alpha_{n}^{2}\\left(\\tilde{\\tau}+8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)-\\nabla_{x_{s}}f\\left(x^{t}\\right)+\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\sum_{n=1}^{N}\\alpha_{n}^{2}\\left(\\tilde{\\tau}+8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(2\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)-7\\,x_{s}\\right\\Vert^{2}+2\\epsilon^{2}+\\sigma_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "D.3.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The analysis of the client-side model update is similar to the server. Thus, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla_{x_{c}}f\\left(x^{t}\\right),x_{c}^{t+1}-x_{c}^{t}\\right\\rangle\\right]}\\\\ &{\\le\\left(-\\displaystyle\\frac{\\eta^{t}\\tau}{2}+8N\\left(\\eta^{t}\\right)^{3}\\tau^{3}S^{2}\\sum_{n=1}^{N}a_{n}^{2}\\right)\\left\\Vert\\nabla_{x_{c}}f\\left(x^{t}\\right)\\right\\Vert^{2}+8N\\eta^{t}S^{2}\\tau^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(\\eta^{t}\\right)^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ \uff0c", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{S}{2}\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{t}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\sum_{n=1}^{N}a_{n}^{2}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(2\\left\\|\\nabla_{x_{c}}f\\left(\\pmb{x}^{t}\\right)\\right\\|^{2}+2\\epsilon^{2}+\\sigma_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "D.3.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Applying (80), (83), (85) and (84) into (36) in Proposition C.4 and define $\\tau_{\\operatorname*{min}}\\,\\triangleq\\,\\operatorname*{min}\\left\\{\\tau,\\tilde{\\tau}\\right\\}$ $\\tau_{\\operatorname*{max}}\\triangleq\\operatorname*{max}\\left\\{\\tau,\\tilde{\\tau}\\right\\}$ wehave ", "page_idx": 28}, {"type": "text", "text": "E[f(xt+1)]-f(\u03b1t)  \n<E[<Vf(a\uff09),+1-]+[+-]+E[<Vf(x),1-\u221e>]  \n+[]  \n$\\begin{array}{r l}&{\\quad_{2}^{\\ast}\\sum_{j=1}^{\\infty}\\left\\{F_{j}\\left(x_{t}-\\nu_{t}\\right)\\right\\}}\\\\ &{\\leq\\left(-\\frac{\\eta(\\mu_{0};j+1)}{2}+\\kappa(t)\\right)^{3}(\\tan{(\\nu_{0}\\,\\gamma_{-}t)})^{3}\\varepsilon\\frac{x_{0}^{2}}{\\omega_{x}^{2}}\\Bigg\\vert\\mathbb{P}_{j}\\mathcal{L}(e)\\Bigg\\vert^{2}}\\\\ &{\\quad+8\\pi\\beta^{2}\\delta^{2}\\left(\\nu_{0}^{2}+\\nu_{1}^{2}\\right)\\frac{\\sum_{k=1}^{\\infty}\\left(\\nu_{1}^{2}+\\nu_{2}^{2}\\right)}{\\sum_{k=1}^{\\infty}\\left(\\alpha_{0}^{2}+\\nu_{2}^{2}\\right)}}\\\\ &{\\quad+\\frac{8\\pi\\beta^{2}}{32}\\frac{\\left(\\eta^{2}+\\nu_{3}\\right)\\left(\\alpha_{1}^{2}+\\nu_{3}^{2}\\right)}{2}\\frac{x_{0}^{2}}{\\omega_{x}^{2}}\\Bigg\\{8\\alpha\\left(3\\varepsilon-\\nu_{1}^{2}+8\\left(\\tan{(\\nu_{1}\\,\\gamma_{-}t)}+\\nu_{2}^{2}\\right)(\\nu_{1}^{2}\\,\\sigma_{2}^{2}\\right)\\mathbb{P}_{j}\\left(\\nu_{1}^{2}\\right)\\right\\}^{2}}\\\\ &{\\quad+\\frac{8\\pi\\beta^{2}}{32}\\frac{x_{0}^{2}}{\\nu_{x}^{2}}\\frac{x_{0}^{4}}{\\omega_{x}^{2}}\\Bigg(\\nu_{1}^{2}\\delta^{2}\\left(8\\nu_{0}^{2}+8\\nu_{1}^{2}\\right)\\left(3\\varepsilon^{2}+\\nu_{2}^{2}\\right)}\\\\ &{\\quad+\\frac{8\\pi\\beta^{2}}{32}\\frac{x_{0}^{2}}{\\nu_{x}^{2}}\\frac{x_{0}^{4}}{\\omega_{x}^{2}}\\Big(\\nu_{1}^{2}\\delta^{2}\\left(8\\nu_{0}^{2}+8\\nu_{1}^{2}\\right)(7)\\delta^{2}\\left(\\nu_{2}^{2}+\\nu_{2}^{2}\\right)}\\\\ &{\\quad+\\frac $  \n$\\begin{array}{r l}&{\\quad_{1}\\leq\\frac{5\\sqrt{\\binom{3}{2}}^{3}}{2}\\sum_{i=1}^{\\infty}\\sum_{s=0}^{\\infty}\\Bigg(a^{2}+s^{2}\\bigg)^{(2)}\\Bigg)\\Bigg(3^{2}+a^{2}\\Bigg)}\\\\ &{\\leq\\Bigg(\\frac{a^{2}c^{2}\\ln(1+s)}{2}\\cos^{2}{\\left(1+s\\right)}\\frac{a^{2}}{2}\\sin{\\left(\\frac{3}{2}\\frac{3}{2}\\frac{3}{2}+3\\sqrt{\\binom{3}{2}}{4}\\right)}\\cos{\\left(\\frac{5}{2}\\frac{3}{2}\\frac{9}{2}\\left(\\cos{\\left(4\\pi+s^{2}\\right)}\\left(\\sqrt{\\frac{3}{2}}\\right)\\right)\\right)}\\Bigg)\\Bigg|\\nabla_{2}d(z)}\\\\ &{\\quad+8\\sqrt{\\pi\\beta^{2}}\\left(\\nu^{2}+s^{2}\\right)\\Bigg)\\frac{5}{2}\\sqrt{\\frac{4}{2}\\left(\\nu^{2}+\\beta^{2}\\right)}\\left(\\nu^{2}+\\alpha^{2}\\right)}\\\\ &{\\quad+\\frac{1}{2^{2}}\\mathrm{Se}\\left(\\nu\\right)^{2}\\Bigg(a^{2}+s^{2}\\left(b^{2}+s^{2}\\right)\\frac{a^{2}}{2}\\Bigg)\\frac{1}{2}\\mathrm{Se}^{\\frac{2}{3}}\\left(\\beta^{2}+\\alpha^{2}\\right)+\\frac{1}{2^{2}}\\mathrm{Se}\\left(\\nu\\right)^{2}\\left(a^{2}+s^{2}\\left(b^{2}+s^{2}\\right)\\right)\\frac{a^{2}}{2}\\mathrm{Se}^{\\frac{2}{3}}\\Bigg(\\nu^{2}+\\alpha^{2}\\right)}\\\\ &{\\quad\\times\\Bigg(\\frac{a^{2}c^{2}\\ln(1+s)}{2}\\cos{\\left(\\frac{3}{2}\\frac{3}{2}\\frac{3}{2}+3\\sqrt{\\binom{3}{2}}{4}\\right)}\\cos{\\left(\\frac{3}{2}\\frac{3}{2}\\frac{3}{2}+3\\sqrt{\\binom{3}{2}}{4}\\right)}\\cos{\\frac{3}{2}\\frac{3}{2}\\alpha^{2}}+8\\sqrt{\\binom{3}{2}+\\beta^{2}\\frac{3}{2}}\\Bigg)\\Bigg|\\nabla_{2}d(z)}\\\\ &{\\quad+8\\$ \uff09\u00a52\u865f2.m\u00b2", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Phi_{\\varepsilon}=\\frac{\\mathcal{F}_{\\mathrm{shon}}^{(\\varepsilon)}}{2}\\left(-1-2\\nu\\nabla_{\\theta}^{2}\\frac{\\nu_{\\theta}^{2}}{\\nu_{\\infty}^{2}}\\frac{\\nu_{\\theta}^{2}}{\\nu_{\\infty}^{2}}e^{\\frac{2\\nu}{\\nu_{\\theta}}\\left(1+8\\nu\\rho^{\\prime}+8\\nu^{\\prime}\\left(\\nu^{2}\\right)^{2}\\right)}\\right)\\left\\vert\\nabla_{\\theta}\\int\\psi_{\\varepsilon}\\right\\vert\\psi_{\\varepsilon}\\right\\vert}\\\\ &{+\\frac{\\mathcal{F}_{\\mathrm{shon}}^{(\\varepsilon)}}{2}\\left(1-2\\nu\\nabla_{\\theta}^{2}\\frac{\\nu_{\\theta}^{2}}{\\nu_{\\infty}^{2}}\\frac{\\nu_{\\theta}^{2}}{\\nu_{\\infty}^{2}}e^{-\\frac{\\nu_{\\theta}^{2}}{2}\\nu\\left(1+8\\nu\\rho^{\\prime}\\left(\\nu^{2}\\right)^{2}\\right)}\\right)\\frac{\\sqrt{\\nu}_{\\theta}^{2}}{\\nu_{\\infty}^{2}}e^{\\frac{2\\nu}{\\nu_{\\theta}}\\left(1-\\nu\\right)}}\\\\ &{+\\left(8\\nu\\left(\\nu^{2}\\right)^{2}+4\\nu\\nabla_{\\theta}^{2}\\left(1^{3}\\nu^{2}+18\\nu\\rho^{\\prime}\\left(\\nu^{2}\\right)^{2}+\\frac{\\nu_{\\theta}^{2}}{\\nu_{\\infty}^{2}}\\right)\\right)\\frac{\\sqrt{\\nu}_{\\theta}^{2}}{\\nu_{\\infty}^{2}}e^{2\\nu\\left(\\nu^{2}\\right)}}\\\\ &{+\\frac{\\mathcal{F}_{\\mathrm{shon}}^{(\\varepsilon)}}{2}\\left(\\nu^{2}\\right)^{2}e^{\\frac{2\\nu}{\\nu_{\\phi}}\\left(1-8\\nu^{\\prime}\\left(\\nu^{2}\\right)^{2}\\right)}\\psi_{\\varepsilon}^{2}+\\Lambda^{2}\\Re\\left(\\nu^{2}\\right)^{2}\\left(1-\\nu\\right)\\frac{\\sqrt{\\nu}_{\\theta}^{2}}{\\nu_{\\infty}^{2}}e^{2\\nu\\left(\\nu^{2}\\right)}}\\\\ &{+\\left(8\\nu(\\nu^{2})^{2}+\\nu(\\nu^{2})^{2}\\right)e^{\\frac{\\nu}{\\mu}\\left(1+\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we first let $\\begin{array}{r}{\\eta^{t}\\le\\frac{1}{16S\\tau_{\\mathrm{max}}}}\\end{array}$ and then let $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{8S N\\frac{\\tau_{\\operatorname*{max}}^{2}}{\\tau_{\\operatorname*{min}}}\\sum_{n=1}^{N}a_{n}^{2}}}\\end{array}$ We also use $\\left\\|\\nabla_{x}f\\left(\\pmb{x}^{t}\\right)\\right\\|^{2}=$ $\\left\\|\\nabla_{x_{c}}f\\left(x^{t}\\right)\\right\\|^{2}+\\left\\|\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\|^{2}.$ ", "page_idx": 29}, {"type": "text", "text": "Rearranging the above we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\eta^{t}\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\leq\\frac{4}{\\tau_{\\mathrm{min}}}\\left(f\\left(x^{t}\\right)-\\mathbb{E}\\left[f\\left(x_{s}^{t+1}\\right)\\right]\\right)+8N S\\left(\\eta^{t}\\right)^{2}\\frac{\\tau^{2}+\\tilde{\\tau}^{2}}{\\tau_{\\mathrm{min}}}\\sum_{n=1}^{N}a_{n}^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Taking expectation and averaging over all $t$ ,we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\eta^{t}\\mathbb{E}\\left[\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{4}{T\\tau_{\\mathrm{min}}}\\left(f\\left(x_{0}\\right)-f^{*}\\right)+\\frac{8N S\\frac{\\tau^{2}+\\tilde{\\tau}^{2}}{\\tau_{\\mathrm{min}}}}{T}\\sum_{n=1}^{N}a_{n}^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)\\sum_{t=0}^{T-1}\\left(\\eta^{t}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "(89) ", "page_idx": 29}, {"type": "text", "text": "EProof of Theorem 3.7 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u00b7 In Sec. E.1, we prove the strongly convex case.   \n\u00b7 In Sec. E.2, we prove the general convex case.   \n\u00b7 In Sec. E.3, we prove the non-convex case. ", "page_idx": 30}, {"type": "text", "text": "E.1  Strongly convex case for SFL-V2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "E.1.1  One-round Sequential Update for M-Server-Side Model ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma E.1. Under Assumptions $C.I$ and C.2, if $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , in round $t,$ . the M-server-side model evolves as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\le\\left(1-\\displaystyle\\frac{N\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We prove Lemma E.1 as follows. ", "page_idx": 30}, {"type": "text", "text": "Proof. We use $\\pmb{x}_{s,n}^{t,i}$ as theM-server-sidemodel whn th M-erver interacts withlt $n$ fof mt+1 = \u221es -n \u2211n=1\u2211=0 g ({\u221en,a ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{\\Xi}\\left[\\left[\\mathbf{x}_{i}^{t}-\\tau_{i}^{\\star}\\mathbf{\\Xi}\\right]\\mathbf{\\Xi}\\alpha_{i}^{t}-\\mathbf{x}_{i}^{t}-\\tau_{i}^{\\star}\\sum_{\\in\\atop\\_{i=0}}^{t-1}\\mathbb{E}_{\\mathbf{z}_{i}}f\\left(\\mathbf{x}_{i}^{t},\\mathbf{x}_{i}^{t}\\right)\\right]+\\eta_{i}^{\\star}\\sum_{\\in\\atop\\_{i=0}}^{t-1}\\mathbf{\\Xi}\\mathbf{\\Xi}\\mathbf{\\Xi}\\mathbf{\\alpha}_{i}f\\left(\\mathbf{x}_{i}^{t},\\mathbf{x}_{i}^{t}\\right)\\right]}\\\\ &{=\\mathbf{\\Xi}\\left[\\left[\\mathbf{x}_{i}^{t}-\\tau_{i}^{\\star}-\\eta_{i}^{\\star}\\sum_{\\in\\atop\\_{i=0}}^{t-1}\\mathbb{E}_{\\mathbf{z}_{i}}f\\left(\\left(\\mathbf{x}_{i}^{t},\\mathbf{x}_{i}^{t}\\right)\\right)\\right]\\right]}\\\\ &{+\\eta_{i}^{\\prime}\\mathbb{E}\\left[\\left(\\mathbf{z}_{i}^{t}-\\mathbf{x}_{i}^{t}-\\eta_{i}^{\\star}\\right)\\sum_{\\in\\atop\\_{i=0}}^{t-1}\\mathbb{E}_{\\mathbf{z}_{i}}f\\left(\\left(\\mathbf{z}_{i}^{t},\\mathbf{z}_{i}^{t}\\right)\\right)\\right]\\sum_{\\in\\atop\\_{i=0}}^{t-1}\\mathbb{E}_{\\mathbf{z}_{i}}f\\left(\\left(\\mathbf{z}_{i}^{t},\\mathbf{x}_{i}^{t}\\right)\\right)-\\frac{\\Gamma_{\\infty}^{\\star}}{\\log{\\delta}_{i}^{\\star}}\\eta_{i}^{\\star}\\right]\\right]}\\\\ &{+\\mathbb{E}\\left[\\left(\\eta_{i}^{\\star}\\right)\\left[\\left[\\mathbf{z}_{i}^{t}\\right]\\right]\\sum_{\\in\\atop\\_{i=0}}^{t-1}\\theta_{i}^{\\star}-\\frac{\\Gamma_{\\infty}^{\\star}}{\\log{\\delta}_{i}}f\\left(\\left(\\mathbf{z}_{i}^{t},\\mathbf{z}_{i}^{t}\\right)\\right)\\right]\\left[\\begin{array}{l}{\\mathbf{\\Xi}\\left[\\mathbf{z}_{i}^{t}}\\\\ {\\right]}\\\\ {\\right]}\\\\ {\\zeta\\leq\\left[\\left[\\mathbf{z}_{i}^{t}-\\mathbf{\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the first equality is from $(a+b)^{2}\\ =\\ a^{2}\\,+\\,2a b\\,+\\,b^{2}$ and the last inequality is due to $\\mathbb{E}\\left[\\nabla_{\\mathbf{\\boldsymbol{x}}_{s}}f\\left(\\left\\{\\mathbf{\\boldsymbol{x}}_{c}^{t,i},\\mathbf{\\boldsymbol{x}}_{s}^{t,i}\\right\\}\\right)-\\mathbf{\\boldsymbol{g}}_{s}^{t,i}\\right]=0$ ", "page_idx": 30}, {"type": "text", "text": "The first part in (91) is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|x_{s}^{t}-x_{s}^{*}-\\eta^{t}\\underset{i=0}{\\overset{\\tau-1}{\\sum}}\\nabla_{x_{s}}f\\left(\\left\\{x_{c}^{t,i},x_{s}^{t,i}\\right\\}\\right)\\right\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\mathbb{E}\\left[\\left\\|x_{s}^{t}-x_{s}^{*}\\right\\|^{2}\\right]+\\left(\\eta^{t}\\right)^{2}\\tau N\\sum_{n=1}^{N}\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\|\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\|^{2}\\right]}\\\\ {\\displaystyle-\\,2\\eta^{t}\\mathbb{E}\\left[\\sum_{n=1}^{N}\\sum_{i=0}^{\\tau-1}\\left\\langle x_{s}^{t}-x_{s}^{*},\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c}^{t,i},x_{s}^{t,i}\\right\\}\\right)\\right\\rangle\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "whereweuse $\\begin{array}{r}{\\nabla_{\\mathbf{x}_{s}}f\\left(\\left\\{x_{c},{\\mathbf{x}_{s}}\\right\\}\\right)=\\sum_{n=1}^{N}\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c},{\\mathbf{x}_{s}}\\right\\}\\right)\\!.}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "For (92), we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\eta^{t}\\right)^{2}\\tau N\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\lvert\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\rvert^{2}\\right]}\\\\ &{=\\left(\\eta^{t}\\right)^{2}\\tau N\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\lvert\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right.}\\\\ &{\\left.-g_{s,n}^{t,i}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\rvert^{2}\\right]+\\mathbb{E}\\left[\\left\\lVert g_{s,n}^{t,i}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\rVert^{2}\\right]}\\\\ &{\\leq\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}\\left(\\sigma_{n}^{2}+G^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the first inequality applies triangle inequality. In the last inequality, we apply the bound of variance and expected squared norm for stochastic gradients in Assumption C.1. ", "page_idx": 31}, {"type": "text", "text": "Since $F_{n}(x)$ is $S$ -smooth and $\\mu$ -strongly convex, using Lemma C.3 we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;2\\eta^{t}\\mathbb{E}\\left[\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tau-1}\\left\\langle x_{s}^{t}-x_{s}^{*},\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\rangle\\right]}\\\\ &{\\leq-2\\eta^{t}\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left(F_{n}\\left(x^{t}\\right)-F_{n}\\left(x^{*}\\right)\\right.\\right.}\\\\ &{\\left.\\left.+\\frac{\\mu}{4}\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}-S\\left\\Vert x_{s,n}^{t,i}-x_{s}^{t}\\right\\Vert^{2}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By Lemma C.5, we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{N}\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert x_{s,n}^{t,i}-x_{s}^{t}\\right\\Vert^{2}\\right]\\leq12\\sum_{n=1}^{N}\\tau^{3}\\left(\\eta^{t}\\right)^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "From Assumption C.1, the second part in (91) is bounded by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\|\\displaystyle\\sum_{n=1}^{N}\\sum_{i=0}^{\\tau-1}g_{s,n}^{t,i}-\\sum_{i=0}^{\\tau-1}\\nabla_{x_{s}}f\\left(\\left\\{x_{c}^{t,i},x_{s}^{t,i}\\right\\}\\right)\\right\\|^{2}}\\\\ &{\\leq\\tau\\displaystyle\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left\\|\\displaystyle\\sum_{n=1}^{N}g_{s,n}^{t,i}-\\nabla_{x_{s}}F_{n}\\left(\\left\\{x_{c}^{t,i},x_{s}^{t,i}\\right\\}\\right)\\right\\|^{2}}\\\\ &{\\leq N\\displaystyle\\sum_{n=1}^{N}\\sigma_{n}^{2}\\tau^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus, (91) becomes ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\|\\pmb{x}_{s}^{t+1}-\\pmb{x}_{s}^{*}\\right\\|^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\|\\pmb{x}_{s}^{t}-\\pmb{x}_{s}^{*}\\right\\|^{2}\\right]+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}\\left(\\sigma_{n}^{2}+G^{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle-\\left.2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]\\right.}\\\\ {\\displaystyle-\\left.\\frac{\\mu N\\tau\\eta^{t}}{2}\\left\\|x_{s}^{t}-x_{s}^{*}\\right\\|^{2}+2\\eta^{t}\\left(12\\sum_{n=1}^{N}S\\tau^{3}\\left(\\eta^{t}\\right)^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)\\right.}\\\\ {\\displaystyle+\\left.N\\sum_{n=1}^{N}\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}\\tau^{2}}\\\\ {\\displaystyle\\leq\\left(1-\\frac{\\eta^{t}N\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\|x_{s}^{t}-x_{s}^{*}\\right\\|^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ {\\displaystyle+\\left.\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Using the above lemma, we can prove the convergence error. Let $\\Delta^{t+1}\\triangleq\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-x_{s}^{*}\\right\\|^{2}\\right]$ We can rewrite (97) as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta^{t}N\\tau\\mu}{2}\\right)\\Delta^{t}-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left({\\pmb x}^{t}\\right)-f\\left({\\pmb x}^{*}\\right)\\right],}}\\\\ {{\\displaystyle\\qquad+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right),}}\\\\ {{\\displaystyle\\qquad\\leq\\left(1-\\frac{\\eta^{t}N\\tau\\mu}{2}\\right)\\Delta^{t}+\\frac{\\left(\\eta^{t}\\right)^{2}\\tau^{2}}{4}B_{1}+\\frac{\\left(\\eta^{t}\\right)^{3}\\tau^{3}}{8}B_{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $\\begin{array}{r}{B_{1}:=4N\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\end{array}$ and $B:=192S\\:\\:\\textstyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)$ ", "page_idx": 32}, {"type": "text", "text": "Consider diminishing stepsise $\\begin{array}{r}{\\eta^{t}=\\frac{2\\beta}{N\\tau(\\gamma_{s}+t)}}\\end{array}$ $\\begin{array}{r}{\\frac{N\\eta^{t}\\tau}{2}=\\frac{\\beta}{\\gamma_{s}+t}}\\end{array}$ where $\\begin{array}{r}{\\beta\\,=\\,\\frac{2}{\\mu},\\gamma_{s}\\,=\\,\\frac{8S}{N\\mu}\\,-\\,1}\\end{array}$ It is easy to show that $\\begin{array}{r}{\\eta^{t}\\,\\leq\\,\\frac{1}{2S\\tau}}\\end{array}$ for all $t$ Next, we ill prove that t $\\begin{array}{r}{\\Delta^{t+1}\\;\\le\\;\\frac{v}{\\gamma_{s}+t+1}}\\end{array}$ s+++1, where $\\begin{array}{r}{v\\,=\\,\\operatorname*{max}\\left\\lbrace\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}\\left(\\gamma_{s}+1\\right)},(\\gamma_{s}+1)\\Delta^{0}\\right\\rbrace}\\end{array}$ +( + 1)We pove tisby inductonFrst, te dention of ensures that it holds for $t=-1$ . Assume the conclusion holds for some $t$ , it follows that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta^{t+1}\\leq\\bigg(1-\\frac{N\\eta\\tau^{\\prime}\\varepsilon\\mu}{2}\\bigg)\\Delta^{t}+\\frac{\\left(\\eta^{\\prime}\\right)^{2}\\tau^{2}}{4}B_{1}+\\frac{\\left(\\eta^{\\prime}\\right)^{3}\\tau^{3}}{8}B_{2}}\\\\ &{\\leq\\left(1-\\frac{\\beta\\varepsilon\\delta}{\\gamma}\\right)\\frac{\\tau^{\\prime}}{2}+\\frac{\\left(\\eta^{\\prime}\\right)^{2}\\tau^{2}}{4}B_{1}+\\frac{\\left(\\eta^{\\prime}\\right)^{3}\\tau^{3}}{8}B_{2}}\\\\ &{=\\frac{\\gamma_{1}+t-1}{\\left(\\gamma_{1}+t\\right)^{2}}\\gamma+\\frac{\\beta^{2}\\hbar}{\\left(\\gamma_{1}+t\\right)^{2}}+\\frac{\\beta^{3}\\varepsilon_{2}}{\\left(\\gamma_{1}+t\\right)^{3}}-\\frac{\\beta\\mu}{\\left(\\gamma_{1}+t\\right)^{2}}}\\\\ &{=\\frac{\\gamma_{1}+t-1}{\\left(\\gamma_{1}+t\\right)^{2}}\\tau+t\\left[\\frac{\\beta^{2}\\hbar}{\\gamma_{1}+t}+\\frac{\\beta^{3}\\varepsilon_{2}}{\\left(\\gamma_{1}+t\\right)^{3}}-\\frac{\\beta\\mu-1}{\\left(\\gamma_{1}+t\\right)^{2}}\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\beta^{2}}+\\frac{8B_{2}}{\\mu^{6}\\left(\\gamma_{1}+t\\right)},\\langle\\gamma_{1}+1\\rangle\\Delta\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.\\left(\\frac{\\beta^{2}\\hbar}{\\gamma_{1}+t}\\right)^{2}+\\frac{\\beta^{3}\\hbar}{\\left(\\gamma_{1}+t\\right)^{3}}-\\frac{\\beta\\mu-1}{\\left(\\gamma_{1}+t\\right)^{2}}\\operatorname*{max}\\left\\{\\frac{\\beta^{2}\\hbar}{\\beta\\mu-1},\\frac{\\beta^{3}\\varepsilon_{2}}{\\left(\\beta\\mu-1\\right)\\left(\\gamma_{1}+t\\right)^{3}},\\langle\\gamma_{1}+1\\rangle\\Delta\\right\\}\\right]}\\\\ &{=\\frac{\\gamma_{1}+t-1}{\\left(\\gamma_{1}+t\\right)^{2}}\\tau^{\\prime}\\left(\\frac{\\beta^{2}\\hbar}{\\gamma_{1}+t}+\\frac{\\beta \n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Hence, we have proven that $\\begin{array}{r}{\\Delta^{t}\\le\\frac{v}{\\gamma_{s}+t},\\forall t}\\end{array}$ . Therefore, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]=\\Delta^{t}\\leq\\frac{v}{\\gamma_{s}+t}=\\frac{\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}\\left(\\gamma_{s}+1\\right)},\\left(\\gamma_{s}+1\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{0}-x_{s}^{*}\\right\\Vert^{2}\\right]\\right\\}}{\\gamma_{s}+t}}\\\\ &{\\leq\\frac{16N\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{2}\\left(\\gamma_{s}+t\\right)}+\\frac{1536S\\,\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma_{s}+t\\right)\\left(\\gamma_{s}+1\\right)}+\\frac{\\left(\\gamma_{s}+1\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{0}-x_{s}^{*}\\right\\Vert^{2}\\right]}{\\gamma_{s}+t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "E.1.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Under Assumptions C.1 and C.2, if $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , in round $t$ , Lemma D.1 gives ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\left(1-\\displaystyle\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Let $\\Delta^{t+1}\\triangleq\\mathbb{E}\\left[\\left\\|\\mathbf{}x_{c}^{t+1}-x_{c}^{*}\\right\\|^{2}\\right]$ We can rewrite (107) as: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\Delta^{t}+\\frac{\\left(\\eta^{t}\\right)^{2}\\tau^{2}}{4}B_{1}+\\frac{\\left(\\eta^{t}\\right)^{3}\\tau^{3}}{8}B_{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r}{B_{1}:=4N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\end{array}$ and $\\begin{array}{r}{B_{2}:=192S\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\!.}\\end{array}$ ", "page_idx": 33}, {"type": "text", "text": "Consider a diminishing stepsize $\\begin{array}{r}{\\eta^{t}=\\frac{2\\beta}{\\tau(\\gamma_{c}+t)}}\\end{array}$ ,i.e,=\u03b2 , where $\\begin{array}{r}{\\beta=\\frac{2}{\\mu},\\gamma_{c}=\\frac{8S}{\\mu}-1}\\end{array}$ It is easy to show that $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ for all $t$ For $\\begin{array}{r}{v=\\operatorname*{max}\\left\\lbrace\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}(\\gamma_{c}+1)},(\\gamma_{c}+1)\\Delta^{0}\\right\\rbrace}\\end{array}$ , we can prove that $\\begin{array}{r}{\\Delta^{t}\\leq\\frac{v}{\\gamma_{c}+t},\\forall t}\\end{array}$ Therefore, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|x_{c}^{t}-x_{c}^{*}\\right\\|^{2}\\right]=\\Delta^{t}\\leq\\frac{v}{\\gamma_{c}+t}=\\frac{\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}\\left(\\gamma_{c}+1\\right)},(\\gamma_{c}+1)\\mathbb{E}\\left[\\left\\|x_{c}^{0}-x_{c}^{*}\\right\\|^{2}\\right]\\right\\}}{\\gamma_{c}+t}}\\\\ &{\\leq\\frac{16N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{2}\\left(\\gamma_{c}+t\\right)}+\\frac{1536S\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma_{c}+t\\right)\\left(\\gamma_{c}+1\\right)}+\\frac{\\left(\\gamma_{c}+1\\right)\\mathbb{E}\\left[\\left\\|x_{c}^{0}-x_{c}^{*}\\right\\|^{2}\\right]}{\\gamma_{c}+t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "E.1.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "We merge the M-server-side and client-side models in (106) and (109) using Proposition 3.5. For $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1^{\\breve{\\bf\\Delta}}}{2S\\tau}}\\end{array}$ and $\\textstyle\\gamma={\\frac{8S}{\\mu}}-1$ 8S-1,wehave ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(\\mathbf{x}^{T})\\right]-f(\\mathbf{x}^{*})}\\\\ &{\\leq\\displaystyle\\frac{S}{2}\\left(\\mathbb{E}||\\mathbf{x}_{s}^{T}-\\mathbf{x}_{s}^{*}||^{2}+\\mathbb{E}||\\mathbf{x}_{c}^{T}-\\mathbf{x}_{c}^{*}||^{2}\\right)}\\\\ &{\\leq\\displaystyle\\frac{8S N\\sum_{n=1}^{N}(a_{n}^{2}+1)\\big(2\\sigma_{n}^{2}+G^{2}\\big)}{\\mu^{2}\\left(\\gamma+T\\right)}+\\frac{768S^{2}\\sum_{n=1}^{N}(a_{n}+1)\\big(2\\sigma_{n}^{2}+G^{2}\\big)}{\\mu^{3}\\left(\\gamma+T\\right)\\left(\\gamma+1\\right)}+\\frac{S\\left(\\gamma+1\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}^{0}-\\mathbf{x}^{*}\\right\\Vert^{2}\\right]\\right)}{2\\left(\\gamma+T\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "E.2.1  One-round Sequential Update for M-Server-Side Model ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "By Lemma E.1 with $\\mu=0$ and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "E.2.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "By Lemma D.1 with $\\mu=0$ and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "E.2.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We merge the M-server-side and client-side models in (111) and (112) as follows ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Big[\\big\\|x^{t+1}-x^{*}\\big\\|^{2}\\Big]\\leq\\mathbb{E}\\Big[\\big\\|x_{s}^{t+1}-x_{s}^{*}\\big\\|^{2}\\Big]+\\mathbb{E}\\Big[\\big\\|x_{c}^{t+1}-x_{c}^{*}\\big\\|^{2}\\Big],}\\\\ &{\\leq\\mathbb{E}\\left[\\big\\|x_{s}^{t}-x_{s}^{*}\\big\\|^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+\\mathbb{E}\\left[\\big\\|x_{c}^{t}-x_{c}^{*}\\big\\|^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}(a_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{=\\mathbb{E}\\left[\\big\\|x^{t}-x^{*}\\big\\|^{2}\\right]-4\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}(a_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then, we can obtain the relation between $\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}^{t+1}-\\mathbf{x}^{*}\\right\\Vert^{2}\\right]$ and $\\mathbb{E}\\left[\\left\\|\\pmb{x}^{t}-\\pmb{x}^{*}\\right\\|^{2}\\right]$ , which is related to $\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]$ . Applying Lemma 8 in [i7], we obtain the performance bound as ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[f\\left(\\pmb{x}^{T}\\right)\\right]-f\\left(\\pmb{x}^{*}\\right)}\\\\ {\\leq\\frac{1}{2}\\left(N\\displaystyle\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac{1}{2}}\\left(\\frac{\\left\\Vert\\pmb{x}^{0}-\\pmb{x}^{*}\\right\\Vert^{2}}{T+1}\\right)^{\\frac{1}{2}}}\\\\ {+\\displaystyle\\frac{1}{2}\\left(24S\\displaystyle\\sum_{n=1}^{N}(a_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac{1}{3}}\\left(\\frac{\\left\\Vert\\pmb{x}^{0}-\\pmb{x}^{*}\\right\\Vert^{2}}{T+1}\\right)^{\\frac{1}{3}}+\\frac{S\\left\\Vert\\pmb{x}^{0}-\\pmb{x}^{*}\\right\\Vert^{2}}{2\\left(T+1\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "E.3.1 One-round Sequential Update for M-Server-Side Model ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "For the server, we have ", "page_idx": 35}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad_{2}=_{1}^{T}\\sum_{t=1}^{T}\\left[\\nabla_{x}f\\left(x^{t}\\right)\\right]^{2}+\\frac{\\alpha^{2}}{2}\\underset{x=1}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\begin{array}{l}{\\vdots}\\\\ {0}\\end{array}}\\\\ {\\vdots\\underset{0}{\\overset{n}{\\sum}}\\end{array}}\\end{array}]\\Bigg]\\Bigg]^{\\top}+\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\begin{array}{l}{\\vdots}\\\\ {0}\\end{array}}\\\\ {\\vdots\\underset{0}{\\overset{n}{\\sum}}\\end{array}}\\end{array}]\\Bigg]\\Bigg]^{\\top}}\\\\ &{\\quad-\\gamma^{\\prime}\\left[\\gamma\\right]\\nabla_{x}f\\left(x^{t}\\right)\\right]^{2}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\end{array}{l}\\Bigg[\\begin{array}{l}{\\bigg[u_{0}^{t}}\\\\ {\\vdots}\\end{array}\\end{array}}\\end{array}\\times\\frac{l}{\\overset{n}{\\sum}}\\end{array}\\Bigg]\\Bigg]+\\frac{\\alpha^{2}}{2}\\underset{0}{\\overset{n}{\\sum}}\\int_{\\mathbb{R}_{n}}f\\left(x^{t}\\right)\\right]\\Bigg]^{\\top}}\\\\ &{\\quad-\\gamma^{\\prime}\\left[\\gamma\\right]\\nabla_{x}f\\left(x^{t}\\right)\\right]^{2}+\\frac{\\alpha^{2}}{2!}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\underset{0}{\\overset{n}{\\sum}}\\end{array}{l}}\\end{array}\\times\\frac{l}{\\overset{n}{\\sum}}\\Bigg]\\Bigg]-\\underset{0}{\\overset{n}{\\sum}}\\Bigg[\\nabla_{x}f\\left(x$ $\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\mathbb{P}_{T}\\int\\left(\\mathbf{S}_{T},T\\right)\\xi^{\\mathcal{N}/2}-\\xi\\right\\}+\\eta\\sqrt{\\eta}\\exp\\left(\\frac{\\eta}{\\xi}\\right)\\exp\\left(-\\eta\\right)\\mathcal{H}_{-1}\\left(\\sigma\\right)}\\\\ &{\\mathbb{E}\\left\\{\\left(\\eta,T\\right)\\xi^{\\mathcal{N}/2}-\\xi\\right\\}+\\frac{\\eta}{\\sqrt{2}\\eta}\\exp\\left(\\frac{\\eta}{\\xi}\\right)-\\xi\\left(T^{\\mathcal{N}},T\\right)\\xi^{\\mathcal{N}/2}\\exp\\left(\\frac{\\eta}{\\xi}\\right)\\right\\}}\\\\ &{\\leq\\left\\{\\mathcal{N}_{T}\\int\\eta\\in\\mathcal{E}_{T}\\right\\}\\left(-\\frac{T^{\\mathcal{N}}}{2}\\sum_{k=1}^{\\infty}\\xi\\right)\\sqrt{\\eta}\\exp\\left(\\frac{\\eta}{\\xi}\\right)+\\eta\\sqrt{\\eta}\\exp\\left(\\int\\left(\\eta\\right)\\right)-\\eta\\sqrt{\\eta}\\exp\\left(\\frac{T}{\\xi}\\right)}\\\\ &{\\leq\\bigg\\{\\mathcal{N}_{T}\\int\\eta\\in\\mathcal{E}_{T}\\bigg\\}\\left[\\frac{\\eta}{\\xi}\\sum_{k=1}^{\\infty}\\xi\\left(\\frac{\\eta}{\\xi}\\right)\\xi^{\\mathcal{N}/2}+\\eta\\sqrt{\\eta}\\exp\\left(\\int\\left(\\eta\\right)\\right)-\\eta\\sqrt{\\eta}\\exp\\left(\\int\\eta\\right)\\xi^{\\mathcal{N}/2}}\\\\ &{\\leq\\bigg\\{\\mathcal{N}_{T}\\int\\eta\\in\\mathcal{E}_{T}\\bigg\\}\\left[\\frac{\\eta}{\\xi}\\sum_{k=1}^{\\infty}\\xi\\left(\\frac{\\eta}{\\xi}\\right)\\xi^{\\mathcal{N}/2}+\\eta\\sqrt{\\eta}\\exp\\left(\\frac{\\eta}{\\xi}\\right)\\right]+\\eta\\sqrt{\\eta}\\exp\\left(\\frac{T}{\\xi}\\right)\\sqrt{\\eta}\\exp\\left(-\\frac{T^{\\mathcal{N}/2}}{\\xi}\\right)}\\\\ &{\\leq\\bigg\\{\\mathcal{N}_{T}\\int\\eta\\in\\mathcal{E}_{T}\\bigg\\}\\xi\\left(\\frac{\\eta}{\\xi}\\right)\\sum_{k=1}^{\\infty}\\sum_{i=1}^{\\infty}\\xi\\left((\\xi\\eta\\xi\\xi\\eta^{\\mathcal{N}}(\\xi\\xi\\eta^{\\mathcal{N}}(\\xi\\xi))+\\frac{\\eta}{\\sqrt{2}\\eta}\\sum_{k=1}^{\\infty}\\xi\\eta\\left(\\xi\\eta\\right)\\right)\\bigg)}\\\\ &{-\\eta\\sqrt{\\eta}\\exp\\left(\\frac{T}{\\xi}\\right)\\xi^ $ 1 ", "page_idx": 35}, {"type": "text", "text": "where we apply Assumption C.1, $\\begin{array}{r}{\\nabla_{\\mathbf{x}_{s}}f\\left(\\mathbf{x}^{t}\\right)=\\sum_{n=1}^{N}\\nabla_{\\mathbf{x}_{s}}F_{n}\\left(\\mathbf{x}^{t}\\right)}\\end{array}$ , and $\\begin{array}{r}{\\langle a,b\\rangle\\leq\\frac{a^{2}+b^{2}}{2}}\\end{array}$ ", "page_idx": 35}, {"type": "text", "text": "By Lemma C.6 with $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{\\sqrt{8}S\\tau}}\\end{array}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert x_{s,n}^{t,i}-x_{s}^{t}\\right\\Vert^{2}\\right]\\leq2\\tau^{2}\\left(8\\tau\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x_{s}^{t}\\right)\\right\\Vert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Thus, (115) becomes ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla_{x_{s}}f\\left(x^{t}\\right),x_{s}^{t+1}-x_{s}^{t}\\right\\rangle\\right]}\\\\ &{\\le\\displaystyle\\frac{\\eta^{t}\\tau}{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\displaystyle\\frac{N\\eta^{t}S^{2}}{2}\\sum_{n=1}^{N}2\\tau^{2}\\left(8\\tau\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x_{s}^{t}\\right)\\right\\Vert^{2}\\right)}\\\\ &{\\le\\left(-\\displaystyle\\frac{\\eta^{t}\\tau}{2}+8N^{2}\\left(\\eta^{t}\\right)^{3}\\tau^{3}S^{2}\\right)\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+8N\\eta^{t}S^{2}\\tau^{3}\\sum_{n=1}^{N}\\left(\\eta^{t}\\right)^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).\\qquad\\mathrm{(or~}\\qquad\\epsilon)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Furthermore, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{3}{2}\\mathbb{E}\\left[\\left\\|x_{t}^{+1}-x_{t}^{+}\\right\\|^{2}\\right]}\\\\ &{=\\frac{S(\\eta^{2})}{2}\\sum_{n=1}^{N}\\mathbb{E}\\left[\\left\\|\\sum_{\\ell=0}^{\\lfloor\\eta_{\\ell}\\rfloor}e^{\\frac{t}{2}\\rfloor}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S(\\eta^{2})^{2}}{2}\\sum_{n=1}^{N}\\mathbb{E}\\left[\\left\\|\\sum_{\\ell=0}^{\\lfloor\\eta_{\\ell}\\rfloor}e^{\\frac{t}{2}\\rfloor}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S(\\eta^{2})^{2}}{2}\\sum_{n=1}^{N}\\sum_{i=0}^{N}\\mathbb{E}\\left[\\left\\|g_{i,n}^{\\ell}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S(\\eta^{2})^{2}}{2}\\sum_{n=1}^{N}\\sum_{i=0}^{N}\\mathbb{E}\\left[\\left\\|g_{i,n}^{\\ell}-g_{i,n}^{\\ell}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S(\\eta^{2})^{2}}{2}\\sum_{n=1}^{N}\\sum_{i=0}^{N}\\mathbb{E}\\left[\\left\\|g_{i,n}^{\\ell}-g_{i,n}^{\\ell}+g_{i,n}^{\\ell}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S(\\eta^{2})^{2}}{2}\\sum_{n=1}^{N}\\sum_{i=0}^{N}\\left(\\mathbb{E}\\left[\\left\\|g_{i,n}^{\\ell}-g_{i,n}^{\\ell}\\right\\|^{2}\\right]+\\mathbb{E}\\left[\\left\\|g_{i,n}^{\\ell}\\right\\|^{2}\\right]\\right)}\\\\ &{\\leq\\frac{S(\\eta^{2})^{2}}{2}\\sum_{n=1}^{N}\\sum_{i=0}^{N}\\left(\\mathbb{E}\\left[\\left\\|g_{i,n}^{\\ell}-g_{i,n}^{\\ell}\\right\\|^{2}\\right]+\\mathbb{E}\\left[\\left\\|\\nabla_{\\ell}\\right\\|^{2}+\\sigma_{i,n}^{\\ell}\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last line uses Assumption C.1 and $\\mathbb{E}\\left[||\\mathbf{z}||^{2}\\right]=||\\mathbb{E}[\\mathbf{z}]||^{2}+\\mathbb{E}[||\\mathbf{z}-\\mathbb{E}[\\mathbf{z}]||^{2}]$ for any random variable $\\mathbf{z}$ ", "page_idx": 36}, {"type": "text", "text": "By Lemma C.7 with $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert g_{s,n}^{t,i}-g_{s,n}^{t}\\right\\Vert^{2}\\right]\\leq8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(\\pmb{x}^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, (118) becomes ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{S}{2}\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}}{2}\\frac{N}{n\\cosh}\\left(8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)+\\tau\\mathbb{E}\\left[\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right]\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\frac{N}{n-1}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\frac{N}{n-1}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)-\\nabla_{x_{s}}f\\left(x^{t}\\right)+\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\frac{N}{\\sum_{n=1}^{N}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)}\\left(2\\left\\Vert\\nabla_{x_{s}}f_{n}\\left(x^{t}\\right)-\\nabla_{x_{s}}f\\left(x^{t}\\right)+\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\frac{N}{\\sum_{n=1}^{N}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)}\\left(2\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+2\\epsilon^{2}+\\sigma_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "E.3.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "The analysis of the client-side model update is the same as the client's model update in version 1. Thus,wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla_{x_{c}}f\\left(x^{t}\\right),x_{c}^{t+1}-x_{c}^{t}\\right\\rangle\\right]}\\\\ &{\\le\\left(-\\displaystyle\\frac{\\eta^{t}\\tau}{2}+8N\\left(\\eta^{t}\\right)^{3}\\tau^{3}S^{2}\\sum_{n=1}^{N}a_{n}^{2}\\right)\\left\\Vert\\nabla_{x_{c}}f\\left(x^{t}\\right)\\right\\Vert^{2}+8N\\eta^{t}S^{2}\\tau^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(\\eta^{t}\\right)^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ \uff0c", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{S}{2}\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{t}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\sum_{n=1}^{N}a_{n}^{2}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(2\\left\\|\\nabla_{x_{c}}f\\left(\\mathbf{x}^{t}\\right)\\right\\|^{2}+2\\epsilon^{2}+\\sigma_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "E.3.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Applying (117), (120), (122) and (121) into (36) in Proposition C.4, we have $\\begin{array}{r l}&{\\mathbb{E}\\{f(\\eta^{(k)})\\}-f(x^{t})-\\mathbb{E}\\{\\left|\\mathcal{F}_{q}\\right|^{2}\\}\\times\\mathbb{E}\\{\\eta_{k}^{(k)}\\}\\alpha f(x^{t})\\alpha f^{(k)}}\\\\ &{\\le\\mathbb{E}\\{\\frac{\\eta^{(k)}}{2}\\xi\\xi\\left(\\eta_{k}^{(k)}\\right)^{2}-\\xi\\alpha_{k}^{(k)}\\xi\\left[\\exp^{\\frac{3}{2}}\\sum_{i=1}^{N}\\mathbb{E}\\{\\exp_{s^{\\prime}}f(x^{t})\\}^{2}\\right]\\xi^{2}}\\\\ &{\\le\\left(-\\frac{\\eta^{(k)}}{2}\\xi+\\Delta\\xi^{(k)}\\right)\\left(\\eta_{k}^{(k)}\\right)^{2}\\le\\alpha^{\\frac{3}{2}}\\Bigg\\}\\Bigg\\}\\Bigg|\\nabla f\\left(\\xi^{(k)}\\right)^{2}}\\\\ &{\\le3\\eta\\sqrt{\\eta^{(k)}}\\frac{\\xi}{2}\\exp^{\\frac{3}{2}}\\Bigg\\{\\left(\\eta_{k}^{(k)}+\\xi\\right)^{2}+\\eta_{k}^{(k)}\\xi^{(k)}}\\\\ &{+\\frac{5\\eta\\sqrt{\\eta^{(k)}}}{2}\\frac{\\eta^{(k)}}{2}\\Big(\\eta_{k}^{(k)}+\\xi^{(k)}\\eta_{k}^{(k)}\\Big)^{2}\\le\\alpha^{\\frac{3}{2}}\\Bigg\\}}\\\\ &{\\le\\frac{5\\eta\\sqrt{\\eta^{(k)}}}{2}\\frac{\\eta^{(k)}}{2}\\sum_{i=1}^{N}\\bigg(\\eta_{k}^{(k)}+1\\bigg(\\eta_{k}^{(k)}\\sqrt{\\eta}^{(k)}\\bigg)\\left(\\eta^{(k)}+\\xi^{(k)}\\right)}\\\\ &{\\le\\left(-\\frac{\\eta^{(k)}}{2}+\\Delta\\xi^{(k)}\\right)\\left(\\eta_{k}^{(k)}\\right)^{2}\\eta^{(k)}+3\\eta\\sqrt{\\eta}\\Bigg\\}^{2}\\frac{\\eta^{(k)}}{2}\\Big(\\eta^{(k)}+\\eta^{(k)}\\Big)^{2}}\\\\ &{+\\frac{5\\eta\\sqrt{\\eta^{(k)}}}{2}\\frac{\\eta^{(k)}}{2}\\frac{\\eta^{(k)}}{2}\\Bigg\\{\\left(\\eta_{k}^{(k)}+1\\right)$ 1\u2014 $\\begin{array}{r l}&{\\quad+8\\times15\\times87.5\\times(1^{7}\\times(2^{3}+6\\times(1)+(\\rho_{6}^{2}+1))^{6}+67)}\\\\ &{\\quad+\\frac{1}{2}\\times87(1)^{2}\\tau\\Big(r^{2}+(5\\times4)^{2}(7)^{3}\\tau\\Big)\\times67+12\\{r^{2}+41/(2^{2}+\\alpha^{2})\\}}\\\\ &{\\quad+\\Big(\\frac{9}{2}\\tau^{4}+85\\times(67)^{3}\\tau^{2}+368\\Big)\\Big(\\frac{3}{2}\\tau^{3}+36\\times15\\Big)\\times20\\times(9)^{4}\\tau^{3}+Big]\\Big|\\tau_{2}\\Big(r^{2}\\Big)\\tau^{4}}\\\\ &{\\quad+8\\times15\\times10^{-3}\\times87.5\\times(6\\times10^{-3}+187)^{3}\\tau^{2}+8\\times15\\times10^{-3}\\times10^{-2}\\times10^{-4}\\times11}\\\\ &{\\quad+8\\times17\\big(1^{7}\\times\\tau^{2}\\big)\\tau^{2}\\times(12\\times(1+1)^{2}\\tau^{3}+15\\times(7)\\big)\\times5\\times\\frac{11}{2}\\big(\\alpha^{2}+11\\big)}\\\\ &{\\quad+8\\times17\\big(1^{7}\\times\\tau^{2}\\big)\\tau^{6}\\times(1+12\\times(7)\\tau^{3})\\tau^{5}\\times\\frac{12}{3}\\big(\\alpha^{2}+11\\big)\\times20\\times15\\quad\\mathrm{(a)}}\\\\ &{\\quad+8\\times5\\times17\\big(1^{7}\\times\\tau^{2}\\big)\\tau^{8}\\big(\\tau^{4}+1\\big)+48\\times15\\big(9\\big)^{2}\\tau^{3}\\times\\frac{12}{3}\\big(\\alpha^{2}+11\\big)\\tau^{2}}\\\\ &{\\quad+8\\times15\\times10^{-4}\\times10^{-3}\\tau^{3}\\big(\\tau^{2}+1\\big)+8\\times15\\big(9\\big)^{3}\\tau^{2}\\times\\frac{12}{3}\\big(\\alpha^{2}+11\\big)\\tau^{2}}\\\\ &{\\quad-\\frac{9}{2}\\tau^{6}\\left(1-28\\times10^{7}\\tau^$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle+\\,N S\\left(\\eta^{t}\\right)^{2}\\tau^{2}\\left(\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{64}\\right)\\sum_{n=1}^{N}(a_{n}^{2}+1)\\sigma_{n}^{2}+2S N\\left(\\eta^{t}\\right)^{2}\\tau^{2}\\left(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{64}\\right)\\sum_{n=1}^{N}(a_{n}^{2}+1)\\epsilon^{2}}}\\\\ {{\\displaystyle\\leq-\\frac{\\eta^{t}\\tau}{2}\\left(1-4N^{2}S\\eta^{t}\\frac{\\tau^{2}}{\\tau}\\right)\\left\\|\\nabla_{x}f\\left(x^{t}\\right)\\right\\|^{2}+2N S\\left(\\eta^{t}\\right)^{2}\\sum_{n=1}^{N}\\left(\\tau^{2}a_{n}^{2}+\\tau^{2}\\right)\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)}}\\\\ {{\\displaystyle\\leq-\\frac{\\eta^{t}\\tau}{4}\\left\\|\\nabla_{x}f\\left(x^{t}\\right)\\right\\|^{2}+2N S\\left(\\eta^{t}\\right)^{2}\\tau^{2}\\sum_{n=1}^{N}\\left(a_{n}^{2}+1\\right)\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)}}&{{\\displaystyle\\:(123)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where we first let $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{16S\\tau}}\\end{array}$ and then let $\\begin{array}{r}{\\eta^{t}\\le\\frac{1}{8S N^{2}\\tau}}\\end{array}$ . We have applied $\\begin{array}{r}{\\sum_{n=1}^{N}a_{n}^{2}\\leq N}\\end{array}$ Rearranging the above we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\eta^{t}\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\leq\\frac{4}{\\tau}\\left(f\\left(x^{t}\\right)-\\mathbb{E}\\left[f\\left(x_{s}^{t+1}\\right)\\right]\\right)+8N S\\left(\\eta^{t}\\right)^{2}\\tau\\sum_{n=1}^{N}\\frac{a_{n}^{2}+1}{\\tau}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Taking expectation and averaging over all $t$ ,we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\eta^{t}\\mathbb{E}\\left[\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{4}{T\\tau}\\left(f\\left(x_{0}\\right)-f^{*}\\right)+\\frac{8N S\\tau}{T}\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)\\sum_{t=0}^{T-1}\\left(\\eta^{t}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "FProof of Theorem 3.8 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "\u00b7 In Sec. F.1, we prove the strongly convex case.   \n\u00b7 In Sec. F.2, we prove the general convex case.   \n\u00b7 In Sec. F.3, we prove the non-convex case. ", "page_idx": 39}, {"type": "text", "text": "F.1 Strongly convex case for SFL-V1 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "F.1.1  One-round Parallel Update for M-Server-Side Model ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "We first bound the M-server-side model update in one round for full participation ( $g_{n}=1$ for all $n$ and then compute the difference between full participation and partial participation ${q_{n}<1}$ for some $n$ ). We denote $\\mathbf{I}_{n}^{t}$ as a binary variable, taking 1 if client $n$ participates in model training in round $t$ and O otherwise. Practically, $\\mathbf{I}_{n}^{t}$ follows a Bernoulli distribution with an expectation of $q_{n}$ ", "page_idx": 39}, {"type": "text", "text": "For full participation, Lemma D.1 gives ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert\\overline{{\\boldsymbol{x}}}_{s}^{t+1}-\\boldsymbol{x}_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\left(1-\\frac{\\eta^{t}\\tilde{\\tau}\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\boldsymbol{x}_{s}^{t}-\\boldsymbol{x}_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tilde{\\tau}\\mathbb{E}\\left[f\\left(\\boldsymbol{x}^{t}\\right)-f\\left(\\boldsymbol{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\left(\\tilde{\\tau}\\right)^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\left(\\tilde{\\tau}\\right)^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Considering that each client $n$ participates in model training with a probability $q_{n}$ ,we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-\\overline{{x}}_{s}^{t+1}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-x_{s}^{t}+x_{s}^{t}-\\overline{{x}}_{s}^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-x_{s}^{t}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{1=1}^{N}\\eta^{\\underline{{t}}}\\frac{\\eta^{t}\\ln_{1}^{\\frac{t}{t}-1}}{q_{1}}\\xi_{s,n}^{t,i}\\left(\\xi_{c,n}^{t,i},x_{s,n}^{t,i}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq N\\displaystyle\\frac{N}{n-1}(\\eta^{t})^{2}\\frac{\\alpha_{s}^{2}-1}{q_{1}}\\mathbb{E}\\left[\\left\\|g_{s,n}^{t,i}\\left(\\xi_{c,n}^{t,i},x_{s,n}^{t,i}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq N(\\bar{\\tau})^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{\\alpha_{s}^{2}}{q_{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "$\\begin{array}{r l r l}&{\\mathrm{where~~~we~~~use~~~}\\mathbb{E}\\left\\|X-\\mathbb{E}X\\right\\|^{2}\\quad\\leq\\quad\\mathbb{E}\\left\\|X\\right\\|^{2},}&{\\mathbb{E}\\left[\\mathbf{I}_{n}^{t}\\right]}&{{}={}}&{q_{n},}\\\\ &{\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(q)}\\sum_{i=0}^{\\tilde{\\tau}-1}\\frac{a_{n}^{2}}{q_{n}}g_{s,n}^{t,i}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\!.}\\end{array}$ and $\\begin{array}{r l r}{{\\pmb x}_{s}^{t+1}}&{{}=}&{{\\pmb x}_{s}^{t}\\ -}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Combining the above gives ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-\\overline{{x}}_{s}^{t+1}+\\overline{{x}}_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\left(1-\\frac{\\eta^{t}\\widetilde{\\tau}\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}(\\widetilde{\\tau})^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\left(\\widetilde{\\tau}\\right)^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{+N\\left(\\widetilde{\\tau}\\right)^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Let $\\Delta^{t+1}\\triangleq\\mathbb{E}\\left[\\left\\|\\mathbf{}x_{s}^{t+1}-x_{s}^{*}\\right\\|^{2}\\right]$ . We can rewrite (128) as: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta^{t}\\tilde{\\tau}\\mu}{2}\\right)\\Delta^{t}+\\frac{\\left(\\eta^{t}\\right)^{2}\\left(\\tilde{\\tau}\\right)^{2}}{4}B_{1}+\\frac{\\left(\\eta^{t}\\right)^{3}\\left(\\tilde{\\tau}\\right)^{3}}{8}B_{2}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Iwhere $\\begin{array}{r}{B_{1}:=4N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+4N G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\mathrm{~and~}B:=192S\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}$ $\\begin{array}{r}{\\eta^{t}=\\frac{2\\beta}{\\tilde{\\tau}(\\gamma_{s}+t)}}\\end{array}$ $\\begin{array}{r}{\\frac{\\eta^{t}\\tilde{\\tau}}{2}=\\frac{\\beta}{\\gamma_{s}+t}}\\end{array}$ $\\begin{array}{r}{\\beta=\\frac{2}{\\mu},\\gamma_{s}=\\frac{8S}{\\mu}-1}\\end{array}$ easy to show that $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tilde{\\tau}}}\\end{array}$ for all $t$ We can prove that $\\begin{array}{r}{\\Delta^{t}\\le\\frac{v}{\\gamma_{s}+t},\\forall t}\\end{array}$ Therefoe, we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]=\\Delta^{t}\\leq\\frac{v}{\\gamma_{s}+t}=\\frac{\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}\\left(\\gamma_{s}+1\\right)},\\left(\\gamma_{s}+1\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{0}-x_{s}^{*}\\right\\Vert^{2}\\right]\\right\\}}{\\gamma_{s}+t}}\\\\ &{\\leq\\frac{16N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+16N G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}}{\\mu^{2}\\left(\\gamma_{s}+t\\right)}+\\frac{1536S\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma_{s}+t\\right)\\left(\\gamma_{s}+1\\right)}}\\\\ &{\\ +\\,\\frac{\\left(\\gamma_{s}+1\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{0}-x_{s}^{*}\\right\\Vert^{2}\\right]}{\\gamma_{s}+t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "F.1.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Define $\\begin{array}{r}{\\overline{{\\pmb{x}}}_{t}^{c}\\,=\\,\\sum_{n=1}^{N}a_{n}\\pmb{x}_{c,n}^{t}}\\end{array}$ $t$ ticipation. Using a similar derivation as the M-server side, we first bound the client-side model update in one round for full participation $\\mathbb{E}\\left[\\left\\|\\overline{{\\boldsymbol{x}}}_{c}^{t+1}-\\boldsymbol{x}_{c}^{*}\\right\\|^{2}\\right]$ and then bound the difference of client-side model parameters between full participation and partial participation $\\mathbb{E}\\left[\\left\\|\\mathbf{\\boldsymbol{x}}_{c}^{t+1}-\\mathbf{\\boldsymbol{x}}_{c}^{*}\\right\\|^{2}\\right]$ . The overall gradient update rule of clients in each training round is $x_{c}^{t+1}=$ $\\begin{array}{r}{\\pmb{x}_{c}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\overline{{\\sum}}_{i=0}^{\\tau-1}\\frac{a_{n}}{q_{n}}\\pmb{g}_{c,n}^{t,i}\\left(\\left\\{\\pmb{x}_{c,n}^{t,i},\\pmb{x}_{s,n}^{t,i}\\right\\}\\right)\\!.}\\end{array}$ ", "page_idx": 40}, {"type": "text", "text": "Under Assumptions C.1 and C.2, if $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , in round $t$ , Lemma D.1 gives ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert\\overline{{\\boldsymbol{x}}}_{c}^{t+1}-\\boldsymbol{x}_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\left(1-\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\boldsymbol{x}_{c}^{t}-\\boldsymbol{x}_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\boldsymbol{x}^{t}\\right)-f\\left(\\boldsymbol{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\left(\\tau\\right)^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\left(\\tau\\right)^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Considering that each client $n$ participates in model training with a probability $q_{n}$ ,wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-\\overline{{x}}_{c}^{t+1}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{t}+x_{c}^{t}-\\overline{{x}}_{c}^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{t}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|\\sum_{1=1}^{p}\\eta_{\\overline{{q}}}^{t}\\frac{n_{\\overline{{q}}}\\Pi_{1}^{t}}{\\alpha_{1}}\\stackrel{(-1)}{\\sum_{0}}g_{\\varepsilon,n}^{t,i}\\left(\\left\\{x_{c,n}^{t},x_{s,n}^{t,i}\\right\\}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq N\\tau\\displaystyle\\sum_{n=1}^{N}\\left(\\eta^{t}\\right)^{2}\\frac{\\alpha_{2}^{2}}{q_{n}}\\stackrel{\\gamma-1}{\\sum_{0}}\\left[\\left\\|g_{\\varepsilon,n}^{t,i}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq N\\left(\\tau\\right)^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{\\alpha_{2}^{2}}{q_{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where we use EX-EXI? \u2264 EXI, E[I] =qn\uff0cand $\\begin{array}{r l r}{{\\pmb x}_{c}^{t+1}}&{{}=}&{{\\pmb x}_{c}^{t}\\ -}\\end{array}$$\\begin{array}{r}{\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\sum_{i=0}^{\\tau-1}\\frac{a_{n}}{q_{n}}\\pmb{g}_{c,n}^{t,i}\\left(\\left\\{\\pmb{x}_{c,n}^{t,i},\\pmb{x}_{s,n}^{t,i}\\right\\}\\right)}\\end{array}$", "page_idx": 41}, {"type": "text", "text": "We obtain the client-side model parameter update in one round for partial participation by combining the two terms and we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-\\overline{{x}}_{c}^{t+1}+\\overline{{x}}_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\left(1-\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}(\\tau)^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\left(\\tau\\right)^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{+N\\left(\\tau\\right)^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where we consider $\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]\\geq0$ ", "page_idx": 41}, {"type": "text", "text": "Let $\\Delta^{t+1}\\triangleq\\mathbb{E}\\left[\\left\\|\\mathbf{}x_{c}^{t+1}-x_{c}^{*}\\right\\|^{2}\\right]$ We can rewrite(163) as: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\Delta^{t}+\\frac{\\left(\\eta^{t}\\right)^{2}\\left(\\tau\\right)^{2}}{4}B_{1}+\\frac{\\left(\\eta^{t}\\right)^{3}\\left(\\tau\\right)^{3}}{8}B_{2}.}\\\\ {B_{1}\\qquad:=\\qquad4N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\quad+\\quad4N G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\quad\\mathrm{~and~}\\quad B_{2}}\\\\ {\\qquad\\times N\\qquad\\omega\\quad_{\\infty}\\circ\\quad_{\\infty}\\circ\\dag}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Consider a diminishing stepsize (2e+t)i.e\uff0c $\\begin{array}{r}{\\frac{\\eta^{t}\\tau}{2}=\\frac{\\beta}{\\gamma_{c}+t}}\\end{array}$ , where $\\begin{array}{r}{\\beta=\\frac{2}{\\mu},\\gamma_{c}=\\frac{8S}{\\mu}-1}\\end{array}$ It is easy to show that $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ for all $t$ For $\\begin{array}{r}{v=\\operatorname*{max}\\left\\lbrace\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}\\left(\\gamma_{c}+1\\right)},(\\gamma_{c}+1)\\Delta^{0}\\right\\rbrace}\\end{array}$ -e+1)(Y+ 1)o, wecanprove that $\\begin{array}{r}{\\Delta^{t}\\leq\\frac{v}{\\gamma_{c}+t},\\forall t}\\end{array}$ Therefore,we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]=\\Delta^{t}\\leq\\frac{v}{\\gamma_{c}+t}=\\frac{\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}\\left(\\gamma_{c}+1\\right)},(\\gamma_{c}+1)\\mathbb{E}\\left[\\left\\Vert x_{c}^{0}-x_{c}^{*}\\right\\Vert^{2}\\right]\\right\\}}{\\gamma_{c}+t}}\\\\ &{\\leq\\frac{16N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+16N G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}}{\\mu^{2}\\left(\\gamma_{c}+t\\right)}+\\frac{1536S\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma_{c}+t\\right)\\left(\\gamma_{c}+1\\right)}}\\\\ &{+\\,\\frac{\\left(\\gamma_{c}+1\\right)\\mathbb{E}\\left[\\left\\Vert x_{c}^{0}-x_{c}^{*}\\right\\Vert^{2}\\right]}{\\gamma_{c}+t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "F.1.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "We merge the M-server-side and client-side models in (130) and (135) using Proposition 3.5. For $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\operatorname*{max}\\left\\{\\tau,\\tilde{\\tau}\\right\\}}}\\end{array}$ $\\begin{array}{r}{\\gamma=\\frac{8S}{\\mu}-1}\\end{array}$ 8S-1,wehave ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(\\mathbf{x}^{T})\\right]-f(\\mathbf{x}^{*})}\\\\ &{\\leq\\displaystyle\\frac{S}{2}\\left(\\mathbb{E}||\\mathbf{x}_{s}^{T}-\\mathbf{x}_{s}^{*}||^{2}+\\mathbb{E}||\\mathbf{x}_{c}^{T}-\\mathbf{x}_{c}^{*}||^{2}\\right)}\\\\ &{\\leq\\displaystyle\\frac{8S N\\sum_{n=1}^{N}a_{n}^{2}\\Big(2\\sigma_{n}^{2}+G^{2}+\\frac{G^{2}}{q_{n}}\\Big)}{\\mu^{2}\\left(\\gamma+T\\right)}+\\frac{768S^{2}\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma+T\\right)\\left(\\gamma+1\\right)}+\\frac{S(\\gamma+1)\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}^{0}-\\mathbf{x}^{*}\\right\\Vert\\right]^{2}}{2\\left(\\gamma+T\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "F.2 General convex case for SFL-V1 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "F.2.1 One-round Parallel Update for M-Server-Side Model ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "By Lemma D.1 with $\\mu=0$ and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tilde{\\tau}}}\\end{array}$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert\\overline{{\\mathbf{x}}}_{s}^{t+1}-\\mathbf{x}_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{s}^{t}-\\mathbf{x}_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\widetilde{\\tau}\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\widetilde{\\tau}^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\widetilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Considering that each client $n$ participates in model training with a probability $q_{n}$ ,we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-\\overline{{x}}_{s}^{t+1}\\right\\|^{2}\\right]\\leq N\\tilde{\\tau}^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tilde{\\tau}\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{\\ +\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+N\\tilde{\\tau}^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "F.2.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "By Lemma D.1 with $\\mu=0$ and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert\\overline{{\\mathbf{x}}}_{c}^{t+1}-\\mathbf{x}_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{c}^{t}-\\mathbf{x}_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\mathbf{x}^{t}\\right)-f\\left(\\mathbf{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Considering that each client $n$ participates in model training with a probability $q_{n}$ ,wehave ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-\\overline{{x}}_{c}^{t+1}\\right\\|^{2}\\right]\\leq N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Thus, we have ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{\\ +\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "F.2.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We merge the M-server-side and client-side models in (139) and (142) as follows ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left\\{\\left\\|x^{t+1}-x^{*}\\right\\|^{2}\\leq\\mathbb{E}\\left\\|\\left\\|x_{s}^{t+1}-x_{s}^{*}\\right\\|\\right\\}+\\mathbb{E}\\left\\{\\left\\|x_{c}^{t+1}-x_{s}^{*}\\right\\|^{2}\\right\\},}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|x_{s}^{t}-x_{s}^{*}\\right\\|^{2}\\right]-2\\eta^{\\prime}\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+(\\eta^{\\prime})^{2}\\frac{2}{\\eta^{\\prime}\\Delta}\\sum_{n=1}^{M}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+2457^{3}\\left(\\eta^{\\prime}\\right)^{3}\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+N\\tilde{\\eta}^{2}\\left(\\eta^{\\prime}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}}\\\\ &{+\\mathbb{E}\\left[\\left\\|x_{c}^{t}-x_{s}^{*}\\right\\|^{2}\\right]-2\\eta^{\\prime}\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+\\left(\\eta^{\\prime}\\right)^{2}\\tau^{2}\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+2457^{3}\\left(\\eta^{\\prime}\\right)^{3}\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+N\\tau^{2}\\left(\\eta^{\\prime}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}}\\\\ &{=\\mathbb{E}\\left[\\left\\|x^{t}-x^{*}\\right\\|^{2}\\right]-4\\eta^{\\prime}\\operatorname*{min}\\left\\{\\tau,\\tilde{\\tau}\\right\\}\\mathbb{E}\\left[f\\left(x^{*}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{+(\\eta^{\\prime})^{2}\\sum_{n=1}^{N}a_{n}^{2}\\big(\\eta^{\\prime}\\big)^{2}+245\\left(\\eta^{\\prime}\\right)^{3}\\sum_{n \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Then, we can obtain the relation between $\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}^{t+1}-\\mathbf{x}^{*}\\right\\Vert^{2}\\right]$ and $\\mathbb{E}\\!\\left[\\!\\|x^{t}\\!-\\!x^{*}\\|^{2}\\right]$ , which is related to $\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]$ . Applying Lemma 8 in [17] and let $\\tau_{\\mathrm{min}}\\;:=\\;\\operatorname*{min}\\left\\{\\tilde{\\tau},\\tau\\right\\}$ , we obtain the performance bound as ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[f\\left(x^{T}\\right)\\right]-f\\left(x^{*}\\right)}\\\\ {\\displaystyle\\leq\\frac{1}{2}\\left(\\frac{\\tilde{\\tau}^{2}+\\tau^{2}}{\\tau_{\\operatorname*{min}}^{2}}N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}+\\frac{G_{n}^{2}}{q_{n}}\\right)\\right)^{\\frac{1}{2}}\\left(\\frac{\\left\\Vert x^{0}-x^{*}\\right\\Vert^{2}}{T+1}\\right)^{\\frac{1}{2}}}\\\\ {\\displaystyle+\\,\\frac{1}{2}\\left(\\frac{\\tilde{\\tau}^{2}+\\tau^{2}}{\\tau_{\\operatorname*{min}}^{2}}24S\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac{1}{3}}\\left(\\frac{\\left\\Vert x^{0}-x^{*}\\right\\Vert^{2}}{T+1}\\right)^{\\frac{1}{3}}+\\frac{S\\left\\Vert x^{0}-x^{*}\\right\\Vert^{2}}{2(T+1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "F.3Non-convex case for SFL-V1 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "F.3.1One-round Parallel Update for M-Server-Side Model ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "For the server, we have ", "page_idx": 44}, {"type": "text", "text": "E[<Va\u3002f(axt),mt+1 -\u03b1t>]  \n\u2264E[<Vf (x),+1 -+nVf(\u221et)-nVsf(\u221e))]  \n\u2264E[<V\u03b1f(\u03b1t),ct+1 \u2014+nVmf(xt)-(f(),nVf(xt)]  \n$\\begin{array}{r l}&{\\le\\left\\langle\\mathbf{v}_{i},f(t)\\rangle\\times\\left[-\\frac{1}{\\sqrt{2}}\\sum_{t=1}^{\\infty}\\mathbf{g}_{t}\\mathbf{g}_{t}\\right]+\\mu(\\mathbf{g}_{t},f(t))\\times\\left\\langle\\mathbf{v}_{i},f(t)\\right\\rangle=\\left\\langle\\mathbf{f}(\\mathbf{v}_{i},f(t))\\right\\rangle}\\\\ &{\\le\\left\\langle\\mathbf{v}_{i},f(t)\\right\\rangle\\times\\left[\\frac{1}{\\sqrt{2}}\\sum_{t=1}^{\\infty}\\mathbf{g}_{t}\\mathbf{g}_{t}\\right]}\\\\ &{\\le\\left\\langle\\mathbf{v}_{i},f(t)\\right\\rangle\\times\\left\\langle\\mathbf{v}_{i},f(t)\\right\\rangle}\\\\ &{\\ \\ \\ \\times\\left\\langle\\mathbf{f}(\\mathbf{v}_{i},f)\\right\\rangle}\\\\ &{\\ \\ \\ \\times\\left\\langle\\mathbf{f}(\\mathbf{v}_{i},f)\\right\\rangle}\\\\ &{\\ \\ \\ \\times\\left\\langle\\mathbf{f}(\\mathbf{v}_{i},f)\\right\\rangle}\\\\ &{\\ \\ \\ \\times\\left\\langle\\mathbf{f}(\\mathbf{v}_{i},f)\\right\\rangle}\\\\ &{\\le\\left\\langle\\mathbf{f}(\\mathbf{v}_{i},f)\\right\\rangle}\\\\ &{\\ \\ \\ \\ \\times\\left\\langle\\mathbf{f}(\\mathbf{v}_{i},f)\\right\\rangle}\\\\ &{\\ \\ \\ \\ \\times\\left\\langle\\mathbf{f}(\\mathbf{v}_{i},f)\\right\\rangle}\\\\ &{\\le\\frac{1}{\\sqrt{2}}\\sum_{t=1}^{\\infty}\\mathbf{g}_{t}\\mathbf{v}_{i},\\mathbf{f}\\left(\\mathbf{v}_{i}^{t},\\mathbf{f}(\\mathbf{v}_{i}^{t},\\mathbf{f}_{t}^{t})\\right)+\\frac{1}{\\sqrt{2}}\\sum_{t=1}^{\\infty}\\mathbf{g}_{t}\\mathbf{v}_{i},f(\\mathbf{v}_{i}^{t})\\right\\rangle}\\\\ &{\\le\\frac{1}{\\sqrt{2}}\\sum_{t=1}^{\\infty}\\int_{0}^{1}\\mathbf{g}_{t}\\mathbf{v}_{i},\\mathbf{f}\\left(\\mathbf{v}_{i}^{t},\\mathbf{f}(\\mathbf{v}_{i}^{t},\\mathbf{f}_{t}^{t})\\right)+\\frac{1}{\\sqrt{2}}\\sum_{t=1}^{\\infty}\\mathbf{g}_{t}\\mathbf{v}_{i},f(\\mathbf{v}_{i}^{t})}\\\\ &{\\ \\ \\ \\times\\frac{1}{\\sqrt{2}}\\sum_{t=1}^{\\infty}\\int_$ N-12 > >E (145)", "page_idx": 44}, {"type": "text", "text": "where we apply AssumptonC.1,Vzf (at)= m=1 anVzFn (at),(a,b)\u2264 \u00b2b, , and $\\mathbb{E}\\left[\\mathbf{I}_{n}^{t}\\right]=$ In\u00b7 ", "page_idx": 44}, {"type": "text", "text": "By Lemma C.6 with $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{\\sqrt{8}S\\tilde{\\tau}}}\\end{array}$ , we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tilde{\\tau}-1}\\mathbb{E}\\left[\\left\\Vert x_{s,n}^{t,i}-x_{s}^{t}\\right\\Vert^{2}\\right]\\leq2\\tilde{\\tau}^{2}\\left(8\\tilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\tilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\tilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x_{s}^{t}\\right)\\right\\Vert^{2}\\right).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Thus, (145) becomes ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla_{x_{s}}f\\left(x^{t}\\right),x_{s}^{t+1}-x_{s}^{t}\\right\\rangle\\right]}\\\\ &{\\le-\\frac{\\eta^{t}\\widetilde{\\tau}}{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\frac{N\\eta^{t}S^{2}}{2}\\displaystyle\\sum_{n=1}^{N}\\!\\frac{a_{n}^{2}}{q_{n}}2\\widetilde{\\tau}^{2}\\left(8\\widetilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}\\!+\\!8\\widetilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}\\!+\\!8\\widetilde{\\tau}\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x_{s}^{t}\\right)\\right\\Vert^{2}\\right)}\\\\ &{\\le\\left(-\\frac{\\eta^{t}\\widetilde{\\tau}}{2}+8N\\left(\\eta^{t}\\right)^{3}\\widetilde{\\tau}^{3}S^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\right)\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+8N\\eta^{t}S^{2}\\widetilde{\\tau}^{3}\\displaystyle\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\left(\\eta^{t}\\right)^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Furthermore, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{S}{2}\\mathbb{E}\\left[\\left\\|\\mathbf{z}^{k+1}-\\mathbf{z}^{k}\\right\\|^{2}\\right]}\\\\ &{=\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{N}{\\sum_{m}^{2}}\\left[\\left\\|\\frac{1-\\alpha}{\\sum_{s=0}^{k}{\\theta_{s}}}\\mathbf{z}_{s,j}^{k}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{N}{\\sum_{m}^{2}}\\frac{\\alpha}{\\|\\mathbf{z}^{k}-\\mathbf{z}^{k}\\|^{2}}\\left[\\left\\|\\frac{1-\\alpha}{\\sum_{s=0}^{k}{\\theta_{s}}}\\mathbf{z}_{s,j}^{k}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{\\hat{f}}{\\sum_{m}^{2}}\\frac{\\alpha}{\\|\\mathbf{z}^{k}-\\mathbf{z}^{k}\\|^{2}}\\left[\\left\\|\\frac{1-\\alpha}{\\sum_{s=0}^{k}{\\theta_{s}}}\\mathbf{z}_{s,j}^{k}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{\\hat{f}}{\\sum_{m}^{2}}\\frac{\\alpha}{\\|\\mathbf{z}^{k}-\\mathbf{z}^{k}\\|^{2}}\\left[\\left\\|\\mathbf{z}_{s,m}^{k}-\\mathbf{z}_{s,m}^{k}\\right\\|^{2}\\right]}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{\\hat{f}}{\\sum_{m}^{2}}\\frac{\\alpha}{\\|\\mathbf{z}^{k}-\\mathbf{z}^{k}\\|^{2}}\\left(\\mathbb{E}\\left[\\left\\|\\frac{\\mathbf{z}_{s,m}^{k}-\\mathbf{z}_{s,m}^{k}}{\\theta_{s}}\\right\\|^{2}+\\mathbb{E}\\left[\\left\\|\\mathbf{z}_{s,m}^{k}\\right\\|^{2}\\right]\\right)}\\\\ &{\\leq\\frac{S\\cdot W\\left(\\eta^{2}\\right)^{2}}{2}\\frac{\\alpha}{\\|\\mathbf{z}^{k}-\\mathbf{z}^{k}\\|^{2}}\\frac{\\alpha}{\\|\\mathbf{z}^{k}\\|^{2}}\\left(\\mathbb{E}\\left[\\left\\|\\mathbf\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the last line uses Assumption C.1 and $\\mathbb{E}\\left[||\\mathbf{z}||^{2}\\right]=||\\mathbb{E}[\\mathbf{z}]||^{2}+\\mathbb{E}[||\\mathbf{z}-\\mathbb{E}[\\mathbf{z}]||^{2}]$ for any random variable $\\mathbf{z}$ ", "page_idx": 45}, {"type": "text", "text": "By Lemma C.7 with $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tilde{\\tau}}}\\end{array}$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tilde{\\tau}-1}\\mathbb{E}\\left[\\left\\Vert g_{s,n}^{t,i}-g_{s,n}^{t}\\right\\Vert^{2}\\right]\\leq8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(\\mathbf{x}^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thus, (148) becomes ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{S}{2}\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\frac{N}{n-1}\\frac{a_{n}^{2}}{q_{n}}\\left(8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)+\\tilde{\\tau}\\mathbb{E}\\left[\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right]\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\frac{N}{n-1}\\frac{a_{n}^{2}}{q_{n}}\\left(\\tilde{\\tau}+8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\frac{N}{n-1}\\frac{a_{n}^{2}}{q_{n}}\\left(\\tilde{\\tau}+8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)-\\nabla_{x_{s}}f\\left(x^{t}\\right)+\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\frac{N}{\\sum_{n=1}^{N}}\\frac{a_{n}^{2}}{q_{n}}\\left(\\tilde{\\tau}+8\\tilde{\\tau}^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(2\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)-\\nabla_{x_{s}}f\\left(x^{t}\\right)+\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tilde{\\tau}}{2}\\frac{N}{\\sum_{n \n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "F.3.2One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "The analysis of the client-side model update is similar to the server. Thus, we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla_{x_{c}}f\\left(x^{t}\\right),x_{c}^{t+1}-x_{c}^{t}\\right\\rangle\\right]}\\\\ &{\\le\\left(-\\displaystyle\\frac{\\eta^{t}\\tau}{2}+8N\\left(\\eta^{t}\\right)^{3}\\tau^{3}S^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\right)\\left\\Vert\\nabla_{x_{c}}f\\left(x^{t}\\right)\\right\\Vert^{2}+8N\\eta^{t}S^{2}\\tau^{3}\\displaystyle\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\left(\\eta^{t}\\right)^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "For $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ \uff0c", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{S}{2}\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{t}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(2\\left\\|\\nabla_{x_{c}}f\\left(\\mathbf{x}^{t}\\right)\\right\\|^{2}+2\\epsilon^{2}+\\sigma_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "F.3.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Applying (147), (150), (152) and (151) into (36) in Proposition C.4 and define $\\tau_{\\operatorname*{min}}\\triangleq\\operatorname*{min}\\left\\{\\tau,\\tilde{\\tau}\\right\\}$ $\\tau_{\\operatorname*{max}}\\triangleq\\operatorname*{max}\\left\\{\\tau,\\tilde{\\tau}\\right\\}$ we have ", "page_idx": 46}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad+\\frac{559}{12}(\\gamma^{2}\\log\\left(1+\\gamma\\right)\\Biggr|\\leq\\arcsin\\left(\\pi\\left(\\nu+\\gamma\\right)\\right)+\\arctan\\left(\\pi\\left(\\nu+\\gamma\\right)\\right)\\Biggr|\\leq\\gamma^{2}\\exp\\left(\\pi\\left(\\nu\\right)\\right)^{2}\\Biggr|\\leq\\gamma\\Biggl|\\mathcal{E}_{-1}\\Biggr|}\\\\ &{\\quad+\\frac{559}{22}\\gamma^{2}\\frac{59}{16}\\Biggl(\\kappa^{2}+\\kappa^{3}\\left(\\gamma^{2}\\right)\\Biggr|\\mathcal{E}_{-1}\\Biggr|\\leq\\gamma^{2}\\exp\\left(\\pi\\left(\\nu+\\gamma\\right)\\right)+\\frac{59}{2}\\gamma^{2}\\frac{59}{16}\\Biggl(\\kappa^{3}\\Biggl(\\kappa^{2}+\\kappa^{3}\\left(\\phi_{0}^{2}\\right)\\right)\\Biggr|\\leq\\rho^{2}+\\kappa^{5}}\\\\ &{\\quad+\\Biggl(\\frac{\\gamma^{2}\\pi^{2}\\kappa^{4}}{12}+\\kappa\\left(\\nu\\right)\\Biggr|\\mathcal{E}_{-3}\\Biggr|\\leq\\gamma\\frac{59}{62}\\Biggr)+\\kappa^{5}\\left(\\nu\\right)^{2}\\Biggr|\\leq\\gamma^{2}\\Biggl|\\frac{59}{16}\\Biggr|\\leq\\Biggl|\\frac{59}{2}\\Biggr|\\leq\\gamma^{4}\\Biggr|}\\\\ &{\\quad+\\kappa\\left(\\gamma^{3}\\pi\\left(\\nu+\\nu\\right)\\right)\\Biggr|\\leq\\frac{59}{16}\\Biggr|\\bigg(\\gamma^{2}\\Biggl)+\\Biggl|\\frac{59}{2}\\Biggr|}\\\\ &{\\quad+\\frac{159}{2}\\Biggr|\\mathcal{E}_{-1}\\Biggr|\\leq\\gamma^{6}\\Biggl|\\bigg(\\pi\\left(\\nu+\\nu\\right)\\Biggr|^{2}\\Biggr)\\Biggr|\\leq\\frac{59}{16}\\Biggr|\\leq\\gamma^{2}\\Biggl|\\frac{59}{16}\\Biggr|\\leq\\gamma^{4}\\Biggr|}\\\\ &{\\quad+\\frac{1}{2}\\Biggr|3\\Biggl|\\mathcal{E}_{-1}\\Biggr|\\leq\\gamma^{6}\\Biggl|\\bigg(\\gamma^{8}\\Biggr)\\Biggr|\\leq\\Biggl|\\frac{59}{16}\\Biggr|\\leq\\gamma^{6}\\Biggl|\\left(\\beta^{2}+\\gamma\\right)\\Biggr|}\\\\ &{\\quad+\\frac{1}{2}\\Biggr|3\\Biggl|\\mathcal{E}_{-1}\\Biggr|\\leq\\gamma^{8}\\Biggl|\\left(\\gamma^{3}\\Biggr)\\Biggr|\\leq\\Biggl|\\frac{59}{16}\\Biggr|\\leq\\gamma^{6$ $\\begin{array}{r l}&{\\mathbb{E}\\{f(t^{n+1})-\\int_{t}^{t}\\Big(x^{t}\\Big)\\Big\\}}\\\\ &{\\le\\mathbb{E}\\Big[\\mathbb{P}_{t}\\Big\\{f(t^{n})+x^{t}-x_{t}^{t}\\Big\\}+\\frac{S}{2}\\Big\\|\\Big|\\frac{x^{t}}{\\alpha}\\Big\\|_{t}^{4-t}-x_{t}^{t}\\Big\\}\\Big\\|_{t}^{2}+\\mathbb{E}\\Big[\\mathbb{P}_{t}\\Big\\{f(t^{n}),\\frac{x^{t}+1}{2}-\\alpha_{t}^{t}\\Big\\}+\\frac{S}{2}\\Big\\|\\Big\\}\\Big\\|\\nabla_{t^{n}}f^{t^{n}}+\\cdot\\cdot\\nabla f(t^{n})\\Big\\}}\\\\ &{\\le\\Bigg(-\\frac{\\eta\\sin{\\theta}}{2}\\sum_{t^{n}}\\Big(\\frac{t}{\\alpha}\\cdot t^{n})+S\\Big(t^{n}\\Big)^{2}\\Big)\\sin{\\theta}\\Big[\\Big(t^{n}+\\frac{t}{2}\\Big)^{3}\\frac{S}{2}\\frac{S}{2}\\frac{S}{2}\\frac{S}{2}\\Bigg\\|\\Big\\}\\tau_{t^{n}}\\Big(t^{n})^{2}}\\\\ &{+\\mathbb{E}\\Big[\\mathbb{P}_{t}\\Big\\{f^{n}\\}+\\frac{\\mu}{2}\\Big\\}\\frac{S}{2}\\Big\\langle\\frac{1}{\\alpha}\\Big\\rangle\\Big\\|\\frac{x^{t}}{\\alpha}\\Big\\|_{t}^{2}\\cos^{2}(t^{n})}\\\\ &{+\\underbrace{S\\}\\Big(\\eta^{2})^{2}\\tan{\\theta}[\\tau]\\frac{\\sqrt{t}}{2}\\frac{S}{2}\\frac{S}{2}\\frac{S}{2}\\frac{S}{2}\\frac{S}{2}\\frac{S}{2}\\alpha_{t}^{t}\\Big(\\sin\\{\\theta}\\Big[\\tau]+\\sin\\Big(\\tau.\\tau_{t}\\Big)^{3}\\Big(t^{n})^{2}\\Big)\\Big\\}\\Big(\\eta^{2}\\tau\\Big)\\int_{t}\\mathbb{C}f\\Big(t^{n}\\Big)\\Big\\}^{2}}\\\\ &{+\\underbrace{S\\frac{\\eta}{2}\\sqrt{t}\\frac{S}{2}}_{\\mathrm{~t~n~a~p~}}\\frac{\\alpha_{t}^{t}}{2}\\Big(t^{n}+s^{2}\\Big)\\sin{\\theta}\\Big[\\eta^{2}\\tau]+\\alpha_{t}^{2}\\alpha_{$ + 8Ns3(m) 2 a2 n=1 9n n=1 n=1 Qn 1 n=1 Qn n=1 Qn ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad{\\mathbb{Z}}\\quad\\quad\\quad\\quad\\operatorname{\\mathbb{Z}}\\quad\\quad\\operatorname{max}_{\\phi\\to0}\\operatorname{max}_{\\phi\\to+\\infty}\\operatorname{in}\\operatorname{max}_{\\phi\\to0}(1,\\cdots)}\\\\ &{=\\left(\\frac{1}{2}\\nu({\\phi})e^{2\\nu^{2}+3K({\\phi})^{2}}\\nu^{3}e^{4\\nu(3\\phi)}+\\operatorname{in}^{2(\\phi)}\\left(e^{1-\\phi)}\\frac{1}{2}\\frac{e^{2\\nu^{2}+\\phi}}{1+\\nu^{2}}\\right.}\\\\ &{\\quad+\\left.({\\Lambda})e^{(1)\\phi}\\right)^{-\\frac{1}{2}+{\\Lambda}\\phi}\\left(e^{2\\phi}\\right)^{-\\frac{1}{2}+{\\Lambda}\\phi}\\Lambda^{2}\\nu^{4}\\nu^{5}e^{4\\nu(1)\\phi}\\right)\\frac{1}{{\\sqrt{2}}}\\frac{e^{{\\phi}}}{{\\sqrt{2}}}e^{4\\nu^{4}}}\\\\ &{=\\left(\\frac{1}{2}\\nu({\\phi})e^{2\\phi}+3K(e^{2\\phi})\\right)^{-\\frac{1}{2}}e^{4\\nu(3\\phi)}+3{\\Lambda}^{\\mathrm{O}}\\nu^{5}(\\nu)\\left(\\nu^{1}\\right)^{-\\frac{1}{2}}\\frac{e^{2\\nu}}{{\\sqrt{2}}}e^{4\\nu}}\\\\ &{\\quad+\\left({\\Lambda}\\right)e^{(1)\\phi}\\left(e^{2\\phi}+3K(e^{2\\phi})\\right)\\frac{1}{2}\\nu^{3}e^{4\\nu}+4{\\Lambda}^{\\mathrm{O}}\\nu^{5}(\\nu)\\left(\\nu^{1}\\right)^{-\\frac{1}{2}}\\frac{e^{2\\nu}}{{\\sqrt{2}}}e^{4\\nu}}\\\\ &{\\quad+\\left.({\\Lambda})\\left(e^{3\\phi}+\\operatorname{in}^{2(\\phi)}+3{\\Lambda}^{\\mathrm{O}}\\nu^{5}(\\nu)\\right)\\left(\\nu^{1}\\right)\\right)\\frac{1}{\\sqrt{2}}\\frac{e^{{\\phi}}}{{\\sqrt{2}}}e^{4\\nu}}\\\\ &{\\leq\\frac{\\sqrt{2}\\nu\\rho}{2}\\left(1-2{\\Lambda}\\nu\\rho\\frac{\\nu\\rho}{2}\\frac{e^{2\\phi}}{1+\\nu^{2}}\\frac{{\\Lambda}^{ \n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where we first let $\\begin{array}{r}{\\eta^{t}\\le\\frac{1}{16S\\tau_{\\mathrm{max}}}}\\end{array}$ and then let $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{8S N\\frac{\\tau_{\\operatorname*{max}}^{2}}{\\tau_{\\operatorname*{min}}}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}}}\\end{array}$ We also use $\\left\\|\\nabla_{x}f\\left(\\pmb{x}^{t}\\right)\\right\\|^{2}=$ $\\left\\|\\nabla_{x_{c}}f\\left(x^{t}\\right)\\right\\|^{2}+\\left\\|\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\|^{2}.$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\eta^{t}\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\leq\\frac{4}{\\tau_{\\mathrm{min}}}\\left(f\\left(x^{t}\\right)-\\mathbb{E}\\left[f\\left(x_{s}^{t+1}\\right)\\right]\\right)+8N S\\left(\\eta^{t}\\right)^{2}\\frac{\\tau^{2}+\\tilde{\\tau}^{2}}{\\tau_{\\mathrm{min}}}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Taking expectation and averaging over all $t$ we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\eta^{t}\\mathbb{E}\\left[\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{4}{T\\tau_{\\mathrm{min}}}\\left(f\\left(x_{0}\\right)-f^{*}\\right)+\\frac{8N S\\frac{\\tau^{2}+\\tilde{\\tau}^{2}}{T_{\\mathrm{min}}}}{T}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)\\sum_{t=0}^{T-1}\\left(\\eta^{t}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "GProof of Theorem 3.9 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "\u00b7 In Sec. G.1, we prove the strongly convex case.   \n\u00b7 In Sec. G.2, we prove the general convex case.   \n\u00b7 In Sec. G.3, we prove the non-convex case. ", "page_idx": 48}, {"type": "text", "text": "G.1  Strongly convex case for SFL-V2 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "G.1.1  One-round Sequential Update for M-Server-Side Model ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We first bound the M-server-side model update in one round for full participation ( $g_{n}=1$ for all $n$ and then compute the difference between full participation and partial participation ${q_{n}<1}$ for some $n$ ). We denote $\\mathbf{I}_{n}^{t}$ as a binary variable, taking 1 if client $n$ participates in model training in round $t$ and O otherwise. ", "page_idx": 48}, {"type": "text", "text": "For full participation, Lemma E.1 gives ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert\\overline{{{\\pmb x}}}_{s}^{t+1}-{\\pmb x}_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\left(1-\\frac{N\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert{\\pmb x}_{s}^{t}-{\\pmb x}_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left({\\pmb x}^{t}\\right)-f\\left({\\pmb x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Considering that each client $n$ participates in model training with a probability $q_{n}$ ,we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-\\overline{{x}}_{s}^{t+1}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-x_{s}^{t}+x_{s}^{t}-\\overline{{x}}_{s}^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-x_{s}^{t}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{n=1}^{N}\\eta_{q}^{\\frac{t}{n}}\\sum_{i=0}^{\\tau-1}\\theta_{s,n}^{t}\\left(\\lbrace x_{c,n}^{t,i},x_{s,n}^{t,i}\\rbrace\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq N\\tau\\displaystyle\\sum_{n=1}^{N}\\left(\\eta^{t}\\right)^{2}\\frac{1}{q_{n}}\\sum_{i=0}^{n}\\mathbb{E}\\left[\\left\\|g_{s,n}^{t,i}\\left(\\lbrace x_{c,n}^{t,i},x_{s,n}^{t,i}\\rbrace\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{1}{q_{n}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "$\\begin{array}{r l r}{\\mathbb{E}\\left\\|X-\\mathbb{E}X\\right\\|^{2}}&{{}\\leq}&{\\mathbb{E}\\left\\|X\\right\\|^{2}}\\end{array}$ \uff0c $\\begin{array}{r l r}{\\mathbb{E}\\left[{\\bf I}_{n}^{t}\\right]}&{{}=}&{q_{n},}\\end{array}$ and $\\begin{array}{r l r}{{\\pmb x}_{s}^{t+1}}&{{}=}&{{\\pmb x}_{s}^{t}\\ -}\\end{array}$$\\begin{array}{r}{\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\sum_{i=0}^{\\tau-1}\\frac{1}{q_{n}}\\pmb{g}_{s,n}^{t,i}\\left(\\left\\{\\pmb{x}_{c,n}^{t,i},\\pmb{x}_{s,n}^{t,i}\\right\\}\\right)}\\end{array}$", "page_idx": 48}, {"type": "text", "text": "Combining the above gives ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]=\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-\\overline{{x}}_{s}^{t+1}+\\overline{{x}}_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\left(1-\\frac{N\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ &{+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{+N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{1}{q_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Let $\\Delta^{t+1}\\triangleq\\mathbb{E}\\left[\\left\\|\\mathbf{}x_{s}^{t+1}-x_{s}^{*}\\right\\|^{2}\\right]$ We can rewrite (158) as: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta^{t}N\\tau\\mu}{2}\\right)\\Delta^{t}+\\frac{\\left(\\eta^{t}\\right)^{2}\\tau^{2}}{4}B_{1}+\\frac{\\left(\\eta^{t}\\right)^{3}\\tau^{3}}{8}B_{2}.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r}{:B_{1}:=4N\\sum_{n=1}^{N}\\big(2\\sigma_{n}^{2}+G^{2}\\big)+4N G^{2}\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\mathrm{~and~}B:=192S\\ \\sum_{n=1}^{N}\\big(2\\sigma_{n}^{2}+G^{2}\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Consider adiminising stpsize $\\begin{array}{r}{\\eta^{t}=\\frac{2\\beta}{N\\tau(\\gamma_{s}+t)}}\\end{array}$ ie, $\\begin{array}{r}{\\frac{N\\eta^{t}\\tau}{2}=\\frac{\\beta}{\\gamma_{s}+t}}\\end{array}$ ,where $\\begin{array}{r}{\\beta=\\frac{2}{\\mu},\\gamma_{s}=\\frac{8S}{N\\mu}-1}\\end{array}$ - 1. It is easy to show that $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ for all $t$ . We can prove that $\\begin{array}{r}{\\Delta^{t}\\le\\frac{v}{\\gamma_{s}+t},\\forall t}\\end{array}$ . Therefore, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]=\\Delta^{t}\\leq\\frac{v}{\\gamma_{s}+t}=\\frac{\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}\\left(\\gamma_{s}+1\\right)},\\left(\\gamma_{s}+1\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{0}-x_{s}^{*}\\right\\Vert^{2}\\right]\\right\\}}{\\gamma_{s}+t}}\\\\ &{\\leq\\frac{16N\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+16N G^{2}\\sum_{n=1}^{N}\\frac{1}{q_{n}}}{\\mu^{2}\\left(\\gamma_{s}+t\\right)}+\\frac{1536S\\,\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma_{s}+t\\right)\\left(\\gamma_{s}+1\\right)}}\\\\ &{\\ +\\,\\frac{\\left(\\gamma_{s}+1\\right)\\mathbb{E}\\left[\\left\\Vert x_{s}^{0}-x_{s}^{*}\\right\\Vert^{2}\\right]}{\\gamma_{s}+t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "G.1.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Defne $\\begin{array}{r}{\\overline{{\\pmb{x}}}_{t}^{c}\\,=\\,\\sum_{n=1}^{N}a_{n}\\pmb{x}_{c,n}^{t}}\\end{array}$ $t$ model update in one round for full participation $\\mathbb{E}\\left[\\left\\|\\overline{{\\boldsymbol{x}}}_{c}^{t+1}-\\boldsymbol{x}_{c}^{*}\\right\\|^{2}\\right]$ and then bound the difference of client-side model parameters between full participation and partial participation $\\mathbb{E}\\left[\\left\\|\\mathbf{\\boldsymbol{x}}_{c}^{t+1}-\\mathbf{\\boldsymbol{x}}_{c}^{*}\\right\\|^{2}\\right]$ . The overall gradient update rule of clients in each training round is $x_{c}^{t+1}=$ $\\begin{array}{r}{\\pmb{x}_{c}^{t}-\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\overleftarrow{\\sum_{i=0}^{\\tau-1}\\frac{a_{n}}{q_{n}}}\\pmb{g}_{c,n}^{t,i}\\left(\\left\\{\\pmb{x}_{c,n}^{t,i},\\pmb{x}_{s,n}^{t,i}\\right\\}\\right)\\!.}\\end{array}$ ", "page_idx": 49}, {"type": "text", "text": "Under Assumptions C.1 and C.2, if $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , in round $t$ , Lemma D.1 gives ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert\\overline{{\\mathbf{x}}}_{c}^{t+1}-\\mathbf{x}_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\left(1-\\displaystyle\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{c}^{t}-\\mathbf{x}_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\mathbf{x}^{t}\\right)-f\\left(\\mathbf{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Considering that each client $n$ participates in model training with a probability $q_{n}$ ,we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-\\overline{{x}}_{c}^{t+1}\\right\\|^{2}\\right]}\\\\ &{=\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{t}+x_{c}^{t}-\\overline{{x}}_{c}^{t+1}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{t}\\right\\|^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\|\\displaystyle\\sum_{1=1}^{N}\\eta^{\\underline{{t}}}\\frac{\\eta^{\\alpha}\\Pi_{1}^{t}}{q_{1}}\\sum_{i=0}^{-1}\\varrho_{c,n}^{t,i}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq N\\tau\\displaystyle\\sum_{n=1}^{N}\\left(\\eta^{t}\\right)^{2}\\frac{\\alpha_{n}^{2}}{q_{1}}\\sum_{i=0}^{-1}\\mathbb{E}\\left[\\left\\|g_{c,n}^{t,i}\\left(\\left\\{x_{c,n}^{t,i},x_{s,n}^{t,i}\\right\\}\\right)\\right\\|^{2}\\right]}\\\\ &{\\leq N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{\\alpha_{n}^{2}}{q_{1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where we use EX -EXI\u00b2 \u2264 EXI, E[I] = Qn\uff0c and $\\begin{array}{r l r}{{\\pmb x}_{c}^{t+1}}&{{}=}&{{\\pmb x}_{c}^{t}\\ -}\\end{array}$$\\begin{array}{r}{\\eta^{t}\\sum_{n\\in\\mathcal{P}^{t}(\\pmb{q})}\\sum_{i=0}^{\\tau-1}\\frac{a_{n}}{q_{n}}\\pmb{g}_{c,n}^{t,i}\\left(\\left\\{\\pmb{x}_{c,n}^{t,i},\\pmb{x}_{s,n}^{t,i}\\right\\}\\right)}\\end{array}$", "page_idx": 50}, {"type": "text", "text": "We obtain the client-side model parameter update in one round for partial participation by combining the two terms and we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{*}\\right\\|^{2}\\right]=\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-\\overline{{x}}_{c}^{t+1}+\\overline{{x}}_{c}^{t+1}-x_{c}^{*}\\right\\|^{2}\\right]}\\\\ {\\displaystyle\\leq\\left(1-\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\mathbb{E}\\left[\\left\\|x_{c}^{t}-x_{c}^{*}\\right\\|^{2}\\right]}\\\\ {\\displaystyle+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ {\\displaystyle+\\left.N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Let $\\Delta^{t+1}\\triangleq\\mathbb{E}\\left[\\left\\|\\mathbf{}x_{c}^{t+1}-x_{c}^{*}\\right\\|^{2}\\right]$ We can rewrite (163) as: ", "page_idx": 50}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}{\\Delta^{t+1}\\leq\\left(1-\\frac{\\eta^{t}\\tau\\mu}{2}\\right)\\Delta^{t}+\\frac{\\left(\\eta^{t}\\right)^{2}\\tau^{2}}{4}B_{1}+\\frac{\\left(\\eta^{t}\\right)^{3}\\tau^{3}}{8}B_{2}.}&{}&&{(164.62\\mathrm{~x~})^{2}+(64.76\\left(\\eta^{2}\\frac{\\Delta^{t}}{2}\\right)^{2}+\\left(18.62\\mathrm{~x~}\\right))^{2}+(72.6\\mathrm{~x~})^{3}}\\\\ {B_{1}\\qquad:=\\quad}&{4N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\quad+}&{4N G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\quad\\mathrm{~and~}\\quad B_{2}\\quad}&{:=0.}\\\\ {\\neg{N}\\quad}&{(\\neg{a}-2)\\quad\\mathcal{N}^{2}}&{}&&{(164.62\\mathrm{~x})^{2}+(64.76\\left(\\eta^{2}\\frac{\\Delta^{t}}{2}\\right)^{2}+(76.36\\mathrm{~x}))^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Consider a diminishing stepsize $\\begin{array}{r}{\\eta^{t}=\\frac{2\\beta}{\\tau(\\gamma_{c}+t)}}\\end{array}$ ,i.e,=, , where \u03b2 = 2e=  -1. Itis easy to show that $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ for all $t$ For $\\begin{array}{r}{v=\\operatorname*{max}\\left\\lbrace\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}(\\gamma_{c}+1)},(\\gamma_{c}+1)\\Delta^{0}\\right\\rbrace}\\end{array}$ , we can prove that $\\begin{array}{r}{\\Delta^{t}\\leq\\frac{v}{\\gamma_{c}+t},\\forall t}\\end{array}$ Therefore, we have ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]=\\Delta^{t}\\leq\\frac{v}{\\gamma_{c}+t}=\\frac{\\operatorname*{max}\\left\\{\\frac{4B_{1}}{\\mu^{2}}+\\frac{8B_{2}}{\\mu^{3}\\left(\\gamma_{c}+1\\right)},(\\gamma_{c}+1)\\mathbb{E}\\left[\\left\\Vert x_{c}^{0}-x_{c}^{*}\\right\\Vert^{2}\\right]\\right\\}}{\\gamma_{c}+t}}\\\\ &{\\leq\\frac{16N\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+16N G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}}{\\mu^{2}\\left(\\gamma_{c}+t\\right)}+\\frac{1536S\\,\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma_{c}+t\\right)\\left(\\gamma_{c}+1\\right)}}\\\\ &{+\\,\\frac{\\left(\\gamma_{c}+1\\right)\\mathbb{E}\\left[\\left\\Vert x_{c}^{0}-x_{c}^{*}\\right\\Vert^{2}\\right]}{\\gamma_{c}+t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "G.1.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We merge the M-server-side and client-side models in (160) and (165) using Proposition 3.5. For $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1^{\\breve{\\bf\\Delta}}}{2S\\tau}}\\end{array}$ and $\\textstyle\\gamma={\\frac{8S}{\\mu}}-1$ 8S-1, wehave ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[f(\\mathbf{x}^{T})\\right]-f(\\mathbf{x}^{*})}\\\\ &{\\leq\\frac{S}{2}\\left(\\mathbb{E}||\\mathbf{x}_{s}^{T}-\\mathbf{x}_{s}^{*}||^{2}+\\mathbb{E}||\\mathbf{x}_{c}^{T}-\\mathbf{x}_{c}^{*}||^{2}\\right)}\\\\ &{\\leq\\frac{8S N\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}+\\frac{G^{2}}{q_{n}}\\right)}{\\mu^{2}\\left(\\gamma+T\\right)}+\\frac{768S^{2}\\,\\sum_{n=1}^{N}(a_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)}{\\mu^{3}\\left(\\gamma+T\\right)\\left(\\gamma+1\\right)}}\\\\ &{\\phantom{=}+\\frac{S(\\gamma+1)\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}_{c}^{0}-\\mathbf{x}_{c}^{*}\\right\\Vert^{2}\\right]}{2(\\gamma+T)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "G.2  General convex case for version 2 ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "G.2.1  One-round Sequential Update for M-Server-Side Model ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "By Lemma E.1 with $\\mu=0$ and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{\\ +\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Considering that each client $n$ participates in model training with a probability $q_{n}$ ,wehave ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|x_{s}^{t+1}-\\overline{{x}}_{s}^{t+1}\\right\\|^{2}\\right]\\leq N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{1}{q_{n}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Thus, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\Vert x_{s}^{t}-x_{s}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{1}{q_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "G.2.2  One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "By Lemma D.1 with $\\mu=0$ and $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ {\\leq\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]}\\\\ {+\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Considering that each client $n$ participates in model training with a probability $q_{n}$ ,wehave ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-\\overline{{x}}_{c}^{t+1}\\right\\|^{2}\\right]\\leq N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Thus, we have ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\Vert x_{c}^{t+1}-x_{c}^{*}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\left\\Vert x_{c}^{t}-x_{c}^{*}\\right\\Vert^{2}\\right]-2\\eta^{t}\\tau\\mathbb{E}\\left[f\\left(x^{t}\\right)-f\\left(x^{*}\\right)\\right]}\\\\ &{\\ +\\left(\\eta^{t}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}a_{n}^{2}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S\\tau^{3}\\left(\\eta^{t}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}a_{n}\\left(2\\sigma_{n}^{2}+G^{2}\\right)+N\\tau^{2}\\left(\\eta^{t}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "G.2.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "We merge the M-server-side and client-side models in (169) and (172) as follows ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Big[\\big\\|\\pmb{x}^{t+1}\\!-\\!\\pmb{x}^{*}\\big\\|^{2}\\Big]\\!\\leq\\!\\mathbb{E}\\Big[\\big\\|\\pmb{x}_{s}^{t+1}\\!-\\!\\pmb{x}_{s}^{*}\\big\\|^{2}\\Big]+\\!\\mathbb{E}\\Big[\\big\\|\\pmb{x}_{c}^{t+1}\\!-\\!\\pmb{x}_{c}^{*}\\big\\|^{2}\\Big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\mathbb{E}\\left[\\left\\|x_{\\varepsilon}^{\\star}-x_{\\varepsilon}^{\\star}\\right\\|^{2}\\right]-2\\eta^{\\prime}\\tau\\mathbb{E}\\left[f\\left(x^{\\prime}\\right)-f\\left(x^{\\star}\\right)\\right]}\\\\ &{+\\mathbb{E}\\left[\\left\\|x_{\\varepsilon}^{\\star}-x_{\\varepsilon}^{\\star}\\right\\|^{2}\\right]-2\\eta^{\\prime}\\tau\\mathbb{E}\\left[f\\left(x^{\\prime}\\right)-f\\left(x^{\\star}\\right)\\right]}\\\\ &{+\\left(\\eta^{\\prime}\\right)^{2}\\tau^{2}X\\displaystyle\\sum_{n=1}^{N}\\alpha_{n}^{2}+1\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S^{\\ge3}\\left(\\eta^{\\prime}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}(\\alpha_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{+N\\tau^{2}\\left(\\eta^{\\prime}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\displaystyle\\frac{\\alpha_{n}^{2}+1}{q_{n}}}\\\\ &{=\\mathbb{E}\\left[\\left\\|x^{\\star}-x^{\\star}\\right\\|^{2}\\right]-4\\eta^{\\prime}\\tau\\mathbb{E}\\left[f\\left(x^{\\prime}\\right)-f\\left(x^{\\star}\\right)\\right]}\\\\ &{+\\left(\\eta^{\\prime}\\right)^{2}\\tau^{2}N\\displaystyle\\sum_{n=1}^{N}(\\alpha_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)+24S^{\\ge3}\\left(\\eta^{\\prime}\\right)^{3}\\displaystyle\\sum_{n=1}^{N}(\\alpha_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)}\\\\ &{+N\\tau^{2}\\left(\\eta^{\\prime}\\right)^{2}G^{2}\\sum_{n=1}^{N}\\displaystyle\\sum_{n=1}^{\\alpha_{n}^{2}+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Then, we can obtain the relation between $\\mathbb{E}\\left[\\left\\Vert\\mathbf{x}^{t+1}-\\mathbf{x}^{*}\\right\\Vert^{2}\\right]$ and $\\mathbb{E}\\left[\\left\\|\\pmb{x}^{t}-\\pmb{x}^{*}\\right\\|^{2}\\right]$ , which is related to $\\mathbb{E}\\left[f\\left(\\pmb{x}^{t}\\right)-f\\left(\\pmb{x}^{*}\\right)\\right]$ . Applying Lemma 8 in [i7], we obtain the performance bound as ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[f\\left({\\pmb x}^{T}\\right)\\right]-f\\left({\\pmb x}^{*}\\right)}\\\\ {\\displaystyle\\leq\\frac{1}{2}\\left(N\\sum_{n=1}^{N}(a_{n}^{2}+1)\\left(2\\sigma_{n}^{2}+G^{2}+\\frac{G_{n}^{2}}{{q}_{n}}\\right)\\right)^{\\frac{1}{2}}\\left(\\frac{\\left\\|{\\pmb x}^{0}-{\\pmb x}^{*}\\right\\|^{2}}{T+1}\\right)^{\\frac{1}{2}}}\\\\ {\\displaystyle+\\,\\frac{1}{2}\\left(24S\\sum_{n=1}^{N}(a_{n}+1)\\left(2\\sigma_{n}^{2}+G^{2}\\right)\\right)^{\\frac{1}{3}}\\left(\\frac{\\left\\|{\\pmb x}^{0}-{\\pmb x}^{*}\\right\\|^{2}}{T+1}\\right)^{\\frac{1}{3}}+\\frac{S\\left\\|{\\pmb x}^{0}-{\\pmb x}^{*}\\right\\|^{2}}{2\\left(T+1\\right)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "G.3  Non-convex case for version 2 ", "page_idx": 53}, {"type": "text", "text": "G.3.1 One-round Sequential Update for M-Server-Side Model ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "For the server, we have ", "page_idx": 53}, {"type": "text", "text": "E[<Vmf (xt),\u03b1t+1-\u221e>]   \n\u2264E[<Vmf(at),+1-+nVf(xt)-nVmsf (x)]   \n\u2264E[<Vf (\u03b1\uff09,\u221e+1 +nVsf(xt\uff09)-<f(),nTVsf (xt)]   \n$\\begin{array}{r l}&{\\varphi\\left\\langle\\mathbf{v}_{j,t}(\\mathbf{r}),\\left\\vert\\mathbf{x}\\right\\rangle=\\sum_{t=1}^{\\infty}\\sum_{s=1}^{t-1}\\mathbf{g}\\left\\langle\\mathbf{v}_{j,t}^{t}\\left(\\mathbf{x}_{t}^{t},\\mathbf{x}_{t}^{t}\\right)+\\eta^{\\top}\\varphi_{j,t}\\left(\\mathbf{x}_{t}^{t},\\rho\\right)\\right\\rangle-\\eta^{\\top}\\left\\vert\\mathbf{P}_{0},I_{0}\\right\\rangle^{\\perp}}\\\\ &{\\le\\left\\langle\\mathbf{v}_{j,t}^{t}\\left\\vert\\mathbf{x}_{t}^{t},\\left\\vert\\mathbf{x}_{t}^{t}\\right\\rangle=\\sum_{t=1}^{\\infty}\\mathbf{v}_{j,t}\\left\\langle\\mathbf{v}_{j,t}^{t}\\left(\\mathbf{x}_{t}^{t},\\mathbf{x}_{t}^{t}\\right)\\right\\vert+\\eta^{\\top}\\varphi_{j,t}\\left(\\mathbf{x}_{t}^{t}\\right)\\right\\rangle-\\eta^{\\top}\\left\\vert\\mathbf{P}_{0},I_{0}\\right\\rangle^{\\perp}}\\\\ &{\\le\\left\\langle\\mathbf{v}_{j,t}^{t}\\left\\vert\\mathbf{x}_{t}^{t},\\left\\vert\\mathbf{x}_{t}^{t}\\right\\rangle=\\sum_{t=1}^{\\infty}\\mathbf{v}_{j,t}\\left\\vert\\mathbf{x}_{t}^{t},\\left\\vert\\mathbf{x}_{t}^{t},\\left\\vert\\mathbf{x}_{t}^{t},\\rho\\right\\vert+\\eta^{\\top}\\right\\vert\\mathbf{x}_{t}^{t},\\rho\\right\\vert}\\\\ &{\\varphi\\left\\vert\\mathbf{v}_{j,t}^{t}\\left(\\mathbf{x}_{t}^{t},\\rho\\right)\\right\\vert\\right\\rangle}\\\\ &{\\le\\eta^{\\top}\\left\\langle\\mathbf{v}_{j,t}^{t}\\left\\vert\\mathbf{v}_{j,t}^{t}\\right\\vert\\right\\rangle^{2}}\\\\ &{\\le\\eta^{\\top}\\left\\langle\\mathbf{v}_{j,t}^{t}\\left\\vert\\mathbf{x}_{t}^{t},\\left\\vert\\mathbf{x}_{t}^{t}\\right\\vert,\\frac{1}{2}\\sum_{s=1}^{\\infty}\\mathbf{v}_{j,t},F_{0}\\left(\\mathbf{x}_{t}^{t},\\mathbf{x}_{t}^{t}\\right)\\right)+\\frac{1}{2}\\sum_{t=1}^{\\infty}\\mathbf{v}_{j,t}\\left\\langle\\mathbf{v}_{j}^{t}\\right\\vert\\right\\rangle}\\\\ &{\\ \\ \\ \\ \\left\\langle $ (175) qn =0 ", "page_idx": 53}, {"type": "text", "text": "where weapply AsumptionC.1,Vzf(a)=Vm,Fn(z),(\u03b1b)\u2264, \u201c, and E[I] = $q_{n}$ ", "page_idx": 53}, {"type": "text", "text": "By Lemma C.6 with $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{\\sqrt{8}S\\tau}}\\end{array}$ ,we have ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert x_{s,n}^{t,i}-x_{s}^{t}\\right\\Vert^{2}\\right]\\leq2\\tau^{2}\\left(8\\tau\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}+8\\tau\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x_{s}^{t}\\right)\\right\\Vert^{2}\\right)\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Thus, (175) becomes ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla_{x_{s}}f\\left(x^{t}\\right),x_{s}^{t+1}-x_{s}^{t}\\right\\rangle\\right]}\\\\ &{\\le-\\displaystyle\\frac{\\eta^{t}\\tau}{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\displaystyle\\frac{N\\eta^{t}S^{2}}{2}\\sum_{n=1}^{N}\\!\\frac{1}{q_{n}}2\\tau^{2}\\left(8\\tau\\left(\\eta^{t}\\right)^{2}\\sigma_{n}^{2}\\!+\\!8\\tau\\left(\\eta^{t}\\right)^{2}\\epsilon^{2}\\!+\\!8\\tau\\left(\\eta^{t}\\right)^{2}\\left\\Vert\\nabla_{x_{s}}f\\left(x_{s}^{t}\\right)\\right\\Vert^{2}\\right)}\\\\ &{\\le\\left(-\\displaystyle\\frac{\\eta^{t}\\tau}{2}+8N\\left(\\eta^{t}\\right)^{3}\\tau^{3}S^{2}\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\right)\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+8N\\eta^{t}S^{2}\\tau^{3}\\displaystyle\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\left(\\eta^{t}\\right)^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Furthermore, we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{5}{2}\\left[\\left\\Vert\\mathbf{z}_{j}^{\\bot+1}-\\mathbf{z}_{j}^{\\bot}\\right\\Vert^{2}\\right]}\\\\ &{=\\frac{S\\kappa\\left(\\eta_{j}^{\\bot}\\right)^{2}}{2}\\frac{\\mathcal{N}}{w_{0}^{2}}\\left\\{\\left[\\left\\Vert\\mathbf{z}_{j}^{\\bot}-\\mathbf{z}_{j}^{\\bot}\\mathbf{z}_{j}^{\\bot}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{S\\kappa\\left(\\eta_{j}^{\\bot}\\right)^{2}}{2}\\sum_{n=1}^{\\infty}\\frac{1}{w_{0}^{2}}\\left[\\left\\Vert\\mathbf{\\widehat{z}}_{j}^{\\bot}-\\mathbf{z}_{j}^{\\bot}\\mathbf{z}_{j}^{\\bot}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{S\\kappa\\left(\\eta_{j}^{\\bot}\\right)^{2}\\tau_{j}^{2}}{2}\\frac{\\mathcal{N}}{w_{0}^{2}}\\frac{1}{w_{0}^{2}}\\left\\{\\left\\Vert\\mathbf{z}_{j}^{\\bot}-\\mathbf{z}_{j}^{\\bot}\\right\\Vert^{2}\\right\\}}\\\\ &{\\leq\\frac{S\\kappa\\left(\\eta_{j}^{\\bot}\\right)^{2}\\tau_{j}^{2}}{2}\\frac{\\mathcal{N}}{w_{0}^{2}}\\frac{1}{w_{0}^{2}}\\frac{1}{w_{0}^{2}}\\left\\{\\left\\Vert\\mathbf{z}_{j}^{\\bot}-\\mathbf{z}_{j}^{\\bot}\\mathbf{z}_{j}^{\\bot}\\right\\Vert^{2}\\right\\}}\\\\ &{\\leq\\frac{S\\kappa\\left(\\eta_{j}^{\\bot}\\right)^{2}\\tau_{j}^{2}}{2}\\frac{\\mathcal{N}}{w_{0}^{2}}\\frac{1}{w_{0}^{2}}\\frac{1}{w_{0}^{2}}\\left\\{\\left[\\left\\Vert\\mathbf{z}_{j}^{\\bot}-\\mathbf{z}_{j}^{\\bot}\\mathbf{z}_{j}^{\\bot}+\\mathbf{z}_{j}^{\\bot}\\right\\Vert^{2}\\right]}{2\\left\\Vert\\mathbf{z}_{j}^{\\bot}-\\mathbf{z}_{j}^{\\bot}\\mathbf{z}_{j}^{\\bot}\\right\\Vert^{2}\\right\\}}\\\\ &{\\leq\\frac{S\\kappa(\\eta_{j}^{\\bot})^{2}\\tau_{j}^{2}}{2}\\frac{ \n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where the last line uses Assumption C.1 and $\\mathbb{E}\\left[||\\mathbf{z}||^{2}\\right]=||\\mathbb{E}[\\mathbf{z}]||^{2}+\\mathbb{E}[||\\mathbf{z}-\\mathbb{E}[\\mathbf{z}]||^{2}]$ for any random variable $\\mathbf{z}$ ", "page_idx": 54}, {"type": "text", "text": "By Lemma C.7 with $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ , we have ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\sum_{i=0}^{\\tau-1}\\mathbb{E}\\left[\\left\\Vert g_{s,n}^{t,i}-g_{s,n}^{t}\\right\\Vert^{2}\\right]\\leq8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(\\pmb{x}^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right).\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Thus, (178) becomes ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{S_{\\mathbb{R}}}{2}\\left[\\left\\Vert x_{s}^{t+1}-x_{s}^{t}\\right\\Vert^{2}\\right]}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\displaystyle\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\left(8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)+\\tau\\mathbb{E}\\left[\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right]\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\displaystyle\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\displaystyle\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(\\left\\Vert\\nabla_{x_{s}}F_{n}\\left(x^{t}\\right)-\\nabla_{x_{s}}f\\left(x^{t}\\right)+\\nabla_{x_{s}}f\\left(x^{t}\\right)\\right\\Vert^{2}+\\sigma_{n}^{2}\\right)}\\\\ &{\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\displaystyle\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(2\\left\\Vert\\nabla_{x_{s}}f\\left(x^{t}\\right)-7\\sigma_{x}^{t}\\right\\Vert^{2}+2\\epsilon_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "G.3.2   One-round Parallel Update for Client-Side Models ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "The analysis of the client-side model update is the same as the client's model update in version 1. Thus,wehave ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\left\\langle\\nabla_{x_{c}}f\\left(x^{t}\\right),x_{c}^{t+1}-x_{c}^{t}\\right\\rangle\\right]}\\\\ &{\\le\\left(-\\frac{\\eta^{t}\\tau}{2}+8N\\left(\\eta^{t}\\right)^{3}\\tau^{3}S^{2}\\displaystyle\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\right)\\left\\Vert\\nabla_{x_{c}}f\\left(x^{t}\\right)\\right\\Vert^{2}+8N\\eta^{t}S^{2}\\tau^{3}\\displaystyle\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\left(\\eta^{t}\\right)^{2}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "For $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{2S\\tau}}\\end{array}$ \uff0c", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{S}{2}\\mathbb{E}\\left[\\left\\|x_{c}^{t+1}-x_{c}^{t}\\right\\|^{2}\\right]}\\\\ &{\\displaystyle\\leq\\frac{S N\\left(\\eta^{t}\\right)^{2}\\tau}{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}}{q_{n}}\\left(\\tau+8\\tau^{3}\\left(\\eta^{t}\\right)^{2}S^{2}\\right)\\left(2\\left\\|\\nabla_{x_{c}}f\\left(\\mathbf{x}^{t}\\right)\\right\\|^{2}+2\\epsilon^{2}+\\sigma_{n}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "G.3.3 Superposition of M-Server and Clients ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Applying (177), (180), (182) and (181) into (36) in Proposition C.4, we have ", "page_idx": 55}, {"type": "text", "text": "$\\begin{array}{r l}&{\\quad_{1}\\exp\\left(x\\left(x^{3},\\frac{x}{2}\\right)^{2}\\right)}\\\\ &{=\\frac{1}{2}\\sum_{j=1}^{N}\\left(t\\left(t^{3}-\\nu\\right)^{2}\\right)^{2}\\frac{x}{\\sum_{k=1}^{N}}\\left(t\\left(t^{2}\\right)^{2}\\left(x^{2}+t^{2}\\right)\\right.}\\\\ &{\\quad\\left.+\\frac{1}{2}\\sum_{k=1}^{N}\\left(t\\left(t^{3}-t^{3}\\right)^{2}\\right)^{2}\\frac{x}{\\sum_{k=1}^{N}}\\left(t\\left(t^{2}+\\nu\\right)^{2}\\right)\\right.}\\\\ &{\\quad\\times\\left.\\left(-\\frac{x^{2}}{2}t^{2}+N\\left(t\\left(t^{2}\\right)^{2}\\frac{x}{2}\\frac{x}{2}+\\textrm{S i e f}\\left(t^{3}\\right)^{2}\\right)\\frac{x}{\\sum_{k=1}^{N}}+b\\nabla\\left(t^{3}\\right)^{2}\\nu^{\\frac{3}{2}}+\\frac{\\nu\\cdot\\frac{1}{2}}{\\sum_{k=1}^{N}}\\right)\\right\\Vert\\nabla_{\\tau_{k}},}\\\\ &{\\quad+\\textrm{S i e f}\\left(t^{3}\\right)^{2}x^{2}+b\\frac{\\nu\\cdot\\frac{1}{2}}{\\sqrt{6}}\\frac{x}{2}\\frac{x+1}{\\sqrt{6}}\\left(x^{2}+t^{2}\\right)+b\\nabla\\left(t^{3}\\right)^{2}x^{2}x^{3}\\frac{x}{2}\\frac{x}{2}\\frac{x+1}{\\sqrt{6}}\\left(x^{2}+t^{2}\\right)}\\\\ &{\\quad\\times\\left.\\textrm{S i e f}\\left(t^{2}\\right)^{2}x^{2}\\frac{x}{2}\\frac{x+1}{\\sqrt{6}}+\\frac{1}{2}\\sum_{k=1}^{3}\\left(t\\left(t^{2}\\right)^{2}x^{2}\\frac{x+1}{\\sqrt{6}}+\\frac{\\nu\\cdot\\frac{1}{2}}{\\sqrt{6}}\\right)\\right.}\\\\ &{\\quad\\times\\left.\\textrm{S i e f}\\left(t^{3}\\right)^{2}+t^{2}\\frac{x}{2}\\frac{x+1}{\\sqrt{6}}+\\frac{1}{2}\\left(x^{3}\\right)^{2}\\left(t^{2}+\\frac{\\nu\\cdot\\frac{1}{2}}{\\sqrt{6 $ $\\begin{array}{r l}&{\\quad+\\frac{559\\left(\\eta^{2}\\right)^{5}}{2}\\sum_{\\stackrel{n=1}{\\eta\\leq0}}^{\\infty}\\left(r+\\kappa^{3}\\left(r\\right)^{2}\\eta^{3}\\right)\\left)\\left[\\eta_{-}\\left(r\\right)\\right]^{2}}\\\\ &{\\quad+\\frac{58\\pi\\left(\\eta^{2}\\right)^{5}}{2}\\sum_{\\stackrel{n=1}{\\eta\\leq0}}^{\\infty}\\frac{1}{r}\\frac{\\eta_{-}^{4}}{r}\\frac{1}{r}\\frac{1}{r}\\frac{\\left(1+\\kappa^{3}\\left(\\eta^{3}\\right)^{2}\\right)\\left(\\eta^{2}\\right)^{2}}{2}\\kappa^{-4}\\kappa^{2}}\\\\ &{\\quad+\\frac{58\\pi\\left(\\eta^{2}\\right)^{5}}{2}\\sum_{\\stackrel{n=1}{\\eta\\leq0}}^{\\infty}\\frac{1}{r}\\frac{\\left(1+\\kappa^{3}\\left(\\eta^{3}\\right)^{2}\\right)\\left(\\eta_{-}\\left(\\eta^{2}\\right)^{2}\\right)}{2}\\frac{1}{r^{\\frac{3}{2}}\\frac{1}{r^{\\frac{3}{2}}}\\frac{\\left(1+\\kappa^{3}\\left(\\eta^{2}\\right)^{3}\\right)}{2}}\\left|\\Gamma_{\\kappa,\\frac{r}{2}}\\left(\\pi\\right)\\right|^{2}}\\\\ &{\\quad+8\\pi\\beta^{2}\\kappa^{-1}\\frac{52\\pi\\left(\\eta^{2}\\right)^{4}}{r^{\\frac{5}{2}}}\\frac{\\left(1+\\kappa^{3}\\left(\\eta^{2}\\right)^{2}\\right)\\left(\\eta_{-}^{2}\\left(\\eta^{2}\\right)^{2}\\right)}{2}\\frac{1}{r^{\\frac{6}{2}}\\sin^{2}\\left(\\eta^{2}\\right)}}\\\\ &{\\quad+\\frac{358\\pi\\left(\\eta^{2}\\right)^{5}}{2}\\gamma^{-\\frac{6}{2}}\\left(r+\\pi^{5}\\left(r^{2}\\right)^{2}\\gamma^{\\frac{8}{2}}\\right)\\frac{1}{r^{\\frac{9}{2}}}\\frac{\\left(1+\\kappa^{3}\\left(\\eta^{2}\\right)^{2}\\right)}{r^{\\frac{9}{2}}\\sin^{2}\\left(\\eta^{2}\\right)}}\\\\ &{\\quad\\times\\left(\\frac{\\eta^{2}\\$ $\\begin{array}{r l}&{\\mathbb{E}\\{f(t^{(k)})\\}-f(x)}\\\\ &{\\ \\ \\leq\\mathbb{E}\\{\\mathbb{J}_{q}^{\\alpha},f(t^{(k)})\\}+\\frac{\\mathcal{O}(\\alpha^{2}\\!-\\!\\alpha_{q}^{2})}{\\alpha^{2}}\\frac{\\mathcal{S}}{\\alpha^{2}}\\Bigg\\{\\|\\alpha^{4}\\|^{2}\\!-\\!\\alpha_{q}^{2}\\!\\|^{2}\\!\\}\\times\\!\\mathbb{E}\\{\\nabla_{q}f(t^{(k)})\\}^{2}f^{(k)}\\!-\\!x_{q}^{2}\\!\\|^{2}\\!+\\!\\frac{\\mathcal{O}(\\alpha^{2}\\!-\\!\\alpha_{q}^{2})}{\\alpha^{2}}\\frac{\\mathcal{S}}{\\alpha^{2}}\\mathbb{E}\\{\\mu^{4}\\}^{-1}}\\\\ &{\\quad\\leq\\Bigg(\\frac{\\mathcal{O}(\\alpha^{2}\\!-\\!\\alpha_{q}^{2})}{\\alpha^{2}}+\\ln((t^{(k)})^{2}\\!-\\!\\frac{\\mathcal{O}(\\alpha^{2}\\!-\\!\\alpha_{q}^{2})}{\\alpha^{2}}\\Bigg)\\Bigg\\}\\Bigg\\}\\Bigg\\}\\end{array}$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le-\\displaystyle\\frac{\\eta^{t}\\tau}{2}\\left(1-2N S\\eta^{t}\\frac{\\tau^{2}}{\\tau}\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\left(1+\\frac{1}{2}+\\frac{1}{32}\\right)\\right)\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}}\\\\ &{+N S\\left(\\eta^{t}\\right)^{2}\\tau^{2}\\left(\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{64}\\right)\\sum_{n=1}^{N}\\frac{a_{n}^{2}+1}{q_{n}}\\sigma_{n}^{2}+2S N\\left(\\eta^{t}\\right)^{2}\\tau^{2}\\left(\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{64}\\right)\\sum_{n=1}^{N}\\frac{a_{n}^{2}+1}{q_{n}}\\epsilon^{2}}\\\\ &{\\le-\\displaystyle\\frac{\\eta^{t}\\tau}{2}\\left(1-4N^{2}S\\eta^{t}\\frac{\\tau^{2}}{\\tau}\\sum_{n=1}^{N}\\frac{1}{q_{n}}\\right)\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}+2N S\\left(\\eta^{t}\\right)^{2}\\tau^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}+1}{q_{n}}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)}\\\\ &{\\le-\\displaystyle\\frac{\\eta^{t}\\tau}{4}\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}+2N S\\left(\\eta^{t}\\right)^{2}\\tau^{2}\\sum_{n=1}^{N}\\frac{a_{n}^{2}+1}{q_{n}}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right),}&{\\quad{\\scriptstyle(1\\forall}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "$\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{16S\\tau}}\\end{array}$ $\\begin{array}{r}{\\eta^{t}\\leq\\frac{1}{8S N^{2}\\tau\\sum_{n=1}^{N}\\frac{1}{q_{n}}}}\\end{array}$ We have applied $\\begin{array}{r}{\\sum_{n=1}^{N}a_{n}^{2}\\leq N}\\end{array}$ Rearranging the above we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\eta^{t}\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\leq\\frac{4}{\\tau}\\left(f\\left(x^{t}\\right)-\\mathbb{E}\\left[f\\left(x_{s}^{t+1}\\right)\\right]\\right)+8N S\\left(\\eta^{t}\\right)^{2}\\tau\\sum_{n=1}^{N}\\frac{a_{n}^{2}+1}{q_{n}}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Taking expectation and averaging over all $t$ we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\frac{1}{T}\\sum_{t=0}^{T-1}\\eta^{t}\\mathbb{E}\\left[\\left\\Vert\\nabla_{x}f\\left(x^{t}\\right)\\right\\Vert^{2}\\right]\\leq\\frac{4}{T\\tau}\\left(f\\left(x_{0}\\right)-f^{*}\\right)+\\frac{8N S\\tau}{T}\\sum_{n=1}^{N}\\frac{a_{n}^{2}+1}{q_{n}}\\left(\\sigma_{n}^{2}+\\epsilon^{2}\\right)\\sum_{t=0}^{T-1}\\left(\\eta^{t}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "H  Comparative Analysis ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "H.1 Main technical results ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "We conclude the convergence results in our paper in Table 1. ", "page_idx": 57}, {"type": "table", "img_path": "ud0RBkdBfE/tmp/d967ff00cd7bf67f8f45620da59b0a2daf402a0b4cec186b36b0b5b1e072e0bb.jpg", "table_caption": [""], "table_footnote": [], "page_idx": 57}, {"type": "text", "text": "H.2 Comparison of Bounds ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "We compare our derived bounds for SFL to other distributed approaches. For simplicity, we let $a_{n}=1/N$ in (1) and $\\sigma_{n}=\\sigma$ for all $n$ in (6). The result are summarized in Table 2. Since different convergence theories make slightly different assumptions, we clarify them below. ", "page_idx": 57}, {"type": "text", "text": "In [33], $\\sigma_{*}^{2}$ is the variance of  the stochastic gradient  at  the optimum: $\\begin{array}{r l}{\\mathbb{E}_{\\zeta_{n}\\sim\\mathcal{D}_{n}}\\left[\\left\\|g_{n}\\left(\\pmb{x}^{*},\\zeta_{n}\\right)-\\nabla F_{n}\\left(\\pmb{x}^{*}\\right)\\right\\|^{2}\\right]\\;\\leq\\;\\sigma_{*}^{2}}\\end{array}$ In [16], $\\begin{array}{r}{\\Gamma\\ =\\ f^{*}\\ -\\ \\sum_{n=1}^{N}F_{n}^{*}/N}\\end{array}$ characterizes the client heterogeneity. In [17], $\\epsilon_{*}^{2}$ characterizes the client heterogeneity at the optimum, similar to [12], i.e., $\\begin{array}{r}{\\frac{1}{N}\\sum_{n=1}^{N}||\\nabla F_{n}({\\pmb x}^{*})||^{2}=\\epsilon_{*}^{2}}\\end{array}$ ", "page_idx": 57}, {"type": "text", "text": "Table 2: Performance upper bounds for strongly convex objectives with full client participation. Here, absolute constants and polylogarithmic factors are omitted. We further relax the upper bounds of SFL-V2 for an easier comparison. ", "page_idx": 57}, {"type": "table", "img_path": "ud0RBkdBfE/tmp/7eb93e530951e0af5adf02b8dc50771b6fddb73a39fba23d53ec67a74bd64b8c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 57}, {"type": "text", "text": "The key observation is that our derived bounds match the other distributed approaches in the order of $T$ and they all achieve ${\\cal O}(1/T)$ ", "page_idx": 57}, {"type": "text", "text": "Furthermore, we will compare the convergence upper bounds of SFL to those of distributed SGD in [12] (with parameters $p=1$ and $\\bar{\\zeta}^{2}=0,$ asfollows: ", "page_idx": 58}, {"type": "table", "img_path": "ud0RBkdBfE/tmp/3a104beda34f11840ba840654ed01b37ee5a2e70dc65bf0ee0d621d75e7881eb.jpg", "table_caption": ["Table 3: Performance upper bounds for different objectives with full client participation. "], "table_footnote": [], "page_idx": 58}, {"type": "text", "text": "In the aforementioned table, we denote $\\sigma_{n}=\\sigma$ define $I^{\\mathrm{err}}\\triangleq||\\pmb{x}^{0}-\\pmb{x}^{*}||^{2}$ ,and represent $\\pmb{F}^{e r r}$ $f({\\pmb x}^{0})-f^{*}$ . The absolute constants and polylogarithmic factors are omitted for brevity. Our SFL algorithms show the same convergence rate as the distributed SGD. ", "page_idx": 58}, {"type": "text", "text": "H.3 Comparison of Communication and Computation Overheads ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "There have been are some papers discussing the overhead of SFL, e.g., [27, 4]. We mainly use the analysis from [27]. ", "page_idx": 58}, {"type": "text", "text": "We start with the definitions. Let $K$ represent the total number of clients involved, $D$ denote the aggregate size of the data, and $v$ indicate the size of the smashed layer. The rate of communication is given by $R$ , while $T_{\\mathrm{fb}}$ signifies the duration required for a complete forward and backward propagation cycle on the entire model for a dataset of size $D$ , applicable across various architectures. The time needed to aggregate the full model is expressed as $T_{\\mathrm{fedavg}}$ . The full model's size is denoted by $\\lvert W\\rvert$ and $r$ reflects the proportion of the full model's size that is accessible to a client in SFL, specifically, $|W_{C}|=r|W|$ . The factor $2r|W|$ in the communication per client arises from the necessity for clients to download and upload their model updates before and after the training process. These findings are encapsulated in Table 4. It is observed that as $K$ escalates, the cumulative cost of training time tends to rise following the sequence: SFL-V2 being less than SFL-V1. ", "page_idx": 58}, {"type": "table", "img_path": "ud0RBkdBfE/tmp/d1353481610e7f333f83f5c76d5b1889d097a7ef376b20677ff78706baf73451.jpg", "table_caption": ["Table 4: Communication and computation comparison between FL, SL, and SFL "], "table_footnote": [], "page_idx": 58}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/bbb34a31e9753157fb7e90a2628e0bdee34c6915f6b96be14fcfbb1a995100be.jpg", "img_caption": ["Figure 6: Performance comparison on CIFAR-10. "], "img_footnote": [], "page_idx": 59}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/202ee9888464f9dfc6e5927903d705d0a1591e244944a4be10c85083ba78d397.jpg", "img_caption": ["(a) SFL-V1 on CIFAR-10. (b) SFL-V2 on CIFAR-10. (c) SFL-V1 on CIFAR-100. (d) SFL-V2 on CIFAR-100. "], "img_footnote": [], "page_idx": 59}, {"type": "text", "text": "Figure 7: Impact of local iteration on SFL performance. ", "page_idx": 59}, {"type": "text", "text": "1  Additional Experiments ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "1.1 More comparing methods ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "We compare the SFL methods with the benchmarks, i.e., FedProx [15] and FedOpt [19]. We have used the same hyperparameters, and trained the models on CIFAR-10. The results are provided in Figure 6. From the figures, we observe that SFL-V2 continues to be the best-performing algorithm. This is consistent with our observations in the main paper. ", "page_idx": 59}, {"type": "text", "text": "1.2Impact of local iteration ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "We further study the impact of local epoch number $E$ on the SFL performance. The results are reported in Fig. 7. We observe that SFL generally converges faster with a larger $\\tau$ , demonstrating the benefit of SFL in practical distributed systems. ", "page_idx": 59}, {"type": "text", "text": "1.3  Results using loss metric ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "We further report the results using the loss metric. More specifically: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 Impact of cut layer: The results are reported in Figs. 8 and 9.   \n\u00b7 Impact of data heterogeneity: The results are reported in Fig. 10.   \n\u00b7 Impact of partial participation: The results are reported in Fig. 11. ", "page_idx": 59}, {"type": "text", "text": "In general, we see similar (but opposite) trends with the observations in the main paper. That is, a higher accuracy is associated with a smaller loss. These results are again consistent with our theories. ", "page_idx": 59}, {"type": "text", "text": "1.4  Results on the impact of the position of cut layer. ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "We can observe from the results that the performance of SFL-V1 and SFL-V2 increases in $L_{c}$ We look at the impact of the position of cut layer from the gradient perspective. We plot the gradient divergence in Fig. 12: ", "page_idx": 59}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/faa3acfebb0d1208a6caee4f371927d7291912e216acedfd887e75ff8f951abe.jpg", "img_caption": ["(a) SFL-V1 on CIFAR-10. (b) SFL-V2 on CIFAR-10. (c) SFL-V1 on CIFAR-100. (d) SFL-V2 on CIFAR-100. Figure 8: Impact of the choice of cut layer on SFL training loss. "], "img_footnote": [], "page_idx": 60}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/46016e0413ea41614d39e7cbb3440f6c105d311565930b5734b503c488013115.jpg", "img_caption": ["(a) SFL-V1 on CIFAR-10. (b) SFL-V2 on CIFAR-10. (c) SFL-V1 on CIFAR-100.(d) SFL-V2 on CIFAR-100. Figure 9: Impact of the choice of cut layer on SFL test loss. "], "img_footnote": [], "page_idx": 60}, {"type": "text", "text": "We see for SFL-V1 and SFL-V2, the gradient divergence decreases as we choose a latter cut layer (a larger $L_{c}$ ). This means that the client drift issue is less severe and hence the performance increases. In addition, from a theoretical perspective, if we write $\\epsilon^{2}$ (i.e., upper bound of gradient divergence defined in Assumption 3.3) as a function of $L_{c}$ , we can see that the upper bounds of performance loss decrease in $L_{c}$ . This provides a theoretical angle that the performance of SFL-V1 and SFL-V2 increases in $L_{c}$ ", "page_idx": 60}, {"type": "text", "text": "1.5 Results on FEMNIST dataset ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "We have conducted more simulations on a larger dataset FEMNIST. In particular, we consider $N=100$ and train FL, SFL-V1, SFL-V2. Note that the data come from different sources and are heterogeneous across clients. The results are reported in Fig. 13. ", "page_idx": 60}, {"type": "text", "text": "We note that our key observation continues to hold. That is, SFL-V2 outperforms FL and SFL-V1 under-performs FL under heterogeneous data and a large number of clients. ", "page_idx": 60}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/90c238cbf5a49e3308d17f82d8329e1818365e12bddddfb4b9907169305972cc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 61}, {"type": "text", "text": "(a) SFL-V1 on CIFAR-10. (b) SFL-V2 on CIFAR-10. (c) SFL-V1 on CIFAR-100.(d) SFL-V2 on CIFAR-100. Figure 10: Impact of data heterogeneity on SFL training loss. ", "page_idx": 61}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/436aa110fd49cab925cec1989f8beea059d8966b949c97542dbcd9a8d61f7da1.jpg", "img_caption": [], "img_footnote": [], "page_idx": 61}, {"type": "text", "text": "(a) SFL-V1 on CIFAR-10. (b) SFL-V2 on CIFAR-10. (c) SFL-V1 on CIFAR-100.(d) SFL-V2 on CIFAR-100. Figure 11: Impact of client participation on SFL training loss. ", "page_idx": 61}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/7cab635d07ffb6bdfc5f0086627e5ef266b40d2dfbd146f858a2b3e1c9bd577e.jpg", "img_caption": ["Figure 12: Results on the impact of the position of cut layer. "], "img_footnote": [], "page_idx": 61}, {"type": "image", "img_path": "ud0RBkdBfE/tmp/abf5fe245b6deaa43db04dfe5a84ce51e42c30a5f818e75b70de44848e009cc0.jpg", "img_caption": ["Figure 13: Results on FEMNIST. "], "img_footnote": [], "page_idx": 61}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: We claim that we provide convergence analysis of split-federated learning (SFL) for strongly convex, general convex, and non-convex objectives on heterogeneous data in the abstract and introduction. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 62}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer:[Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: In Section 5, we claim that it is also important to theoretically analyze how the choice of the cut layer affects the SFL performance. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 62}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: Section 3 presents the theoretical results with full set of assumptions. The proof of Proposition 3.5 is given in Appendix C.4. The complete proofs of Theorems 3.6-3.7 are given in Appendices D-E, respectively. Proofs of Theorems 3.6-3.7 are given in Appendices D-E, respectively. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 63}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: Section 4.1 presents the setup of our experiments to reproduce the experimental results. We further provide our codes in the supplementary material. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 63}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We provide our codes in the supplementary material. We will provide open access to the data and code if the paper is accepted. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so ^No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 64}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We present the setup of our experiments, including the datasets and hyperparameters, in Section 4.1 and provide our codes in the supplementary material. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 64}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 64}, {"type": "text", "text": "Answer: [No] ", "page_idx": 64}, {"type": "text", "text": "Justification: This is a theory paper. Due to limit of computation resources and time, we were not able to run more experiments. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 65}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 65}, {"type": "text", "text": "Justification: We have used one CPU and one GPU. Specifically, the CPU is Intel(R) Xeon(R) Gold 5320 CPU with 2.20GHz, and the GPU is A100-PCIE-80GB. Memory: 256GB Storage: 10TB. Each experiment curve takes about 10 hours to train. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 65}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Justification: We have conformed to all aspects of code of ethics with this submission. Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 65}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 65}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Justification: The goal of this work is to provide a comprehensive analysis on the convergence of SFL. Our theoretical results have two potential impacts. First, we provide a thorough understanding on the performance of SFL, which potentially guides the implementation of SFL (e.g., the choice between FL and SFL, the choice of hyper-parameters and cut layers). ", "page_idx": 65}, {"type": "text", "text": "Second, the convergence results can be used for modeling the training performance of SFL. Together with an effective modeling of communication and computation overheads of clients, researchers will be able to perform SFL system optimization. Due to the reduced clients' training loads in SFL, such a system optimization can potentially minimize the burden at clients (e.g., human mobile devices) while maintaining client privacy and satisfactory training performance. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 66}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 66}, {"type": "text", "text": "Justification: The paper poses no such risks. We use open-access datasets and models Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 66}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 66}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 66}, {"type": "text", "text": "Justification: Our codes are based on the codes provided in [27], which was cited in the main paper. ", "page_idx": 66}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 67}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: The paper does not release new assets ", "page_idx": 67}, {"type": "text", "text": "Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 67}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 67}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 67}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 67}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 67}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 67}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 67}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: ", "page_idx": 68}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 68}]