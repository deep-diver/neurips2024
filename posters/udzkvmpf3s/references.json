{"references": [{"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces chain-of-thought prompting, a key technique used in the target paper, and is thus foundational to the current work."}, {"fullname_first_author": "William Merrill", "paper_title": "The expressive power of transformers with chain of thought", "publication_date": "2024-01-01", "reason": "This paper provides theoretical underpinnings for chain-of-thought prompting, enhancing the understanding of the method's capabilities."}, {"fullname_first_author": "Xi Ye", "paper_title": "The unreliability of explanations in few-shot prompting for textual reasoning", "publication_date": "2022-12-01", "reason": "This paper highlights the unreliability of LLMs' explanations, a problem addressed by the target paper's proposed calibration method."}, {"fullname_first_author": "Miles Turpin", "paper_title": "Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting", "publication_date": "2023-05-01", "reason": "This paper discusses the phenomenon of unfaithful reasoning in LLMs, directly motivating the development of the internal consistency calibration method."}, {"fullname_first_author": "Qing Lyu", "paper_title": "Faithful chain-of-thought reasoning", "publication_date": "2023-01-01", "reason": "This paper focuses on the faithfulness of chain-of-thought reasoning, a concept directly related to the internal consistency approach proposed in the target paper."}]}