[{"type": "text", "text": "SAND: Smooth Imputation Of Sparse And Noisy Functional Data With Transformer Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Junwen Yao ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ju-Sheng Hong   \nDepartment of Statistics   \nUniversity of California Davis   \nDavis, CA 95616   \njsdhong@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Department of Statistics University of California Davis Davis, CA jwyao@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Jonas Mueller ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Cleanlab Cambridge, MA jonaswmueller@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Jane-Ling Wang   \nDepartment of Statistics   \nUniversity of California Davis   \nDavis, CA   \njanelwang@ucdavis.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Although the transformer architecture has come to dominate other models for text and image data, its application to irregularly-spaced longitudinal data has been limited. We introduce a variant of the transformer that enables it to more smoothly impute such functional data. We augment the vanilla transformer with a simple module we call SAND (self-attention on derivatives), which naturally encourages smoothness by modeling the sub-derivative of the imputed curve. On the theoretical front, we prove the number of hidden nodes required by a network with SAND to achieve an $\\epsilon$ prediction error bound for functional imputation. Extensive experiments over various types of functional data demonstrate that transformers with SAND produce better imputations than both their standard counterparts as well as transformers augmented with alternative approaches to encode the inductive bias of smoothness. SAND also outperforms standard statistical methods for functional imputation like kernel smoothing and PACE. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Functional data analysis (FDA) offers a framework for analyzing complex data sampled from random functions or curves. Such data are encountered in many applications including air pollution studies, fMRI scans, growth curves, and sensor data from wearable devices. FDA has thus grown into an established field in statistics [14, 6, 9, 24]. Functional data intrinsically have infinite dimensions but are tractably handled by assuming they were generated from a smooth underlying process. The simplest form is a univariate random curve $\\mathbf{x}(t)$ defined on a compact set over the real line $\\mathbb{R}$ . Without loss of generality, it is typically assumed that $\\mathbf{x}(t)$ is a smooth stochastic process over the $[0,1]$ interval domain. Let $x_{i}({\\bar{t}}),i\\in\\dot{\\{1,\\dots,n\\}}$ be a random sample of $\\mathbf{x}(t)$ , for instance, growth curves corresponding to different subjects in a population. In practice, each $x_{i}(t)$ can only be observed discretely over the time interval $[0,1]$ , and often only at a few irregularly spaced $n_{i}$ time points, namely $\\{t_{i j}\\}_{j=1}^{n_{i}}$ . Moreover, the observed measurements are often noisy. Thus, the observed data are $\\mathcal{D}=\\{\\{({t_{i j}},y_{i j})\\}_{j=1}^{n_{i}}\\}_{i=1}^{n}$ , where $y_{i j}=x_{i}(t_{i j})+\\epsilon_{i j}$ and $\\epsilon_{i j}$ are random noise effects. For many applications, it is reasonable to assume that the noise is independent over $t$ and across subjects and follows a zero mean distribution with finite variance. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The objective of this paper is to recover the latent processes $x_{i}(t),i\\ \\in\\ \\{1,\\ldots,n\\}$ , using the noisy/sparse observed data $\\mathcal{D}$ . We introduce an estimator that is effective for both finely and sparsely measured functional data (i.e. regardless of whether the number of measurements for the $i$ -th subject, $n_{i}$ , tends to infinity or is uniformly bounded by a constant). The latter case, sup $n_{i}<\\infty$ , is common in longitudinal studies and called sparse functional data in FDA. Accurate recovery (imputation) of the underlying function from noisy/sparse observations can improve analyses of longitudinal data across many sciences. ", "page_idx": 1}, {"type": "text", "text": "The conventional approach to imputing longitudinal data is to employ a parametric model with mixedeffects, such as the linear mixed-effects model. This approach lacks the flexibility to capture the shapes of more complex trajectories, so a nonparametric method is preferred for imputing functional data. For instance, MICE [21] uses a chained equation approach, where the imputation of each missing value is conditioned on the imputed values of the other variables, resulting in more accurate and stable imputations compared to single imputation methods. However, the resulting imputed functions may not be smooth and to get stable imputations, many iterations are required if the percentage of missing values is high [17, 25]. ", "page_idx": 1}, {"type": "text", "text": "A standard FDA approach that avoids this drawback is to first reduce each function to a finite but possibly growing dimensional vector and then reconstruct the trajectory of each subject using a particular set of basis functions associated with the dimension reduction approach [28]. Two widely used dimension reduction methods are the basis expansion approach [16, 3] and Functional Principal Component Analysis (FPCA) [2, 15, 18, 10, 27, 13, 12, 26]. Among these, FPCA provides the most parsimonious approach. The most popular FPCA approach is PACE [24, 4], which estimates the top principal component functions and the corresponding scores of each subject to reconstruct the latent process of this subject through the Karhunen-Lo\u00e8ve decomposition. ", "page_idx": 1}, {"type": "text", "text": "All aforementioned approaches use the combined data from all subjects to conduct the imputation for each particular individual $x_{i}(\\cdot)$ . Another approach that works well for densely observed functional data is to apply a smoothing method to interpolate the data from an individual. While such smoothing does not borrow information across subjects, it can be effective when the number of measurements $(n_{i})$ is large for all subjects (proper smoothing can denoise the observations). ", "page_idx": 1}, {"type": "text", "text": "Recent advancements in deep learning for functional data imputation include techniques like neural processes [8] and GAIN [29]. Their applications to real data are however dwarfed by the widespread use of transformer networks [23]. Using a self-attention mechanism to discerningly attend to regional information across a domain, transformers offer a natural architecture for learning the right information to rely on in order to best estimate $x_{i}(t)$ at $t$ . Specifically, encoder-decoder transformers use representations generated by the encoder to produce properly contextualized estimations through the decoder. We note that imputing sparsely observed functional data can be viewed as a sequenceto-sequence learning task, for which transformers should be well-suited. However, imputation via transformers fails to yield a naturally smooth function. In this paper, we introduce a small modification of transformers that addresses this issue and produces more accurate imputations. ", "page_idx": 1}, {"type": "text", "text": "Summary of our contributions. (1) We show empirically that a vanilla transformer is promising for functional data imputation \u2013 it outperforms PACE and neural processes but the imputations are not differentiable/smooth. (2) We introduce a novel network layer, SAND (self-attention on derivatives), that can be learned end-to-end with the rest of a transformer architecture. Extending the attention module, our SAND layer guarantees smooth imputations and does not require any assumptions on the data distribution. (3) We prove that imputations from SAND are first-order differentiable (Theorem 1), and derive an analytical expression relating the number of hidden nodes and prediction error of a SAND network fit via empirical risk minimization (Theorem 2). ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Sparse and intensively measured functional data ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In most longitudinal studies involving human subjects, the number of measurements $n_{i}$ is often small (bounded). This type of data is called sparse functional data. When $n_{i}$ is large and grows with the sample size $n$ to infinity, such functional data are called intensively measured functional data. We note that they are not time series data, which is a collection of data observed discretely on an equally spaced time grid for a single subject. There is no smooth latent process that generates this time series. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "For intensively measured functional data, we can apply a kernel smoother to smooth the noisy observations individually for each subject to recover the underlying curve $x_{i}(t)$ and perform subsequent data analysis. Let $K(\\cdot)$ be a kernel function (often a symmetric p.d.f). $\\dot{x}_{i}(t)$ is estimated by the weighted sum of observed data, $\\{x_{i}(t_{i j})\\}_{j=1}^{n_{i}}$ , where weights are computed using kernel $K(\\cdot)$ : $\\begin{array}{r}{\\widehat{x}_{i}(t)=\\sum_{j=0}^{n_{i}}K_{h}(t_{i j}-t)x_{i}(t_{i j})}\\end{array}$ with $K_{h}(\\cdot)=K(\\cdot/h)/h$ . The bandwidth $h$ that determines the size of the related neighborhood can be decided by generalized cross-validation [31]. ", "page_idx": 2}, {"type": "text", "text": "It is not possible to consistently reconstruct the underlying curves for sparse functional data, due to the limited number of available observations per subject. It is however possible to consistently estimate the mean $\\mu(t)=\\mathbb{E}[{\\mathbf{x}}(t)]$ and covariance function $G(s,t)=\\mathrm{Cov}(\\mathtt{x}(s),\\mathtt{x}(t))$ from sparse functional data [27]. Once we have a consistent estimate of the covariance function, we can perform functional PCA and get the imputation of $x_{i}(t)$ as done in the popular PACE method [27]. Letting $\\{(\\lambda_{k},\\phi_{k}(\\cdot))\\}_{k=1}^{\\infty}$ denote the eigen-components of the kernel operator defined by $G(s,t)$ (which can all be estimated consistently), PACE uses the Karhunen-Lo\u00e9ve series expansion of individual curves, ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{i}(t)=\\mu(t)+\\sum_{k=1}^{\\infty}\\xi_{i k}\\phi_{k}(t)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and replaces all unknowns with their estimates. To estimate the principal component scores $\\xi_{i k}$ , PACE uses the best linear predictor $\\lambda_{k}\\phi_{i k}^{\\mathsf{T}}\\Sigma_{y_{i}}^{-1}({\\pmb y}_{i}-{\\pmb\\mu}_{i})$ , where $\\bar{\\phi_{i k}}=(\\bar{\\phi_{k}}(t_{i j}))_{j=1}^{\\bar{n_{i}}}$ , $\\pmb{y}_{i}=(y_{i j})_{j=1}^{\\dot{n_{i}}}$ . ", "page_idx": 2}, {"type": "text", "text": "When $x_{i}(\\cdot)$ is a Gaussian process, we expect the best linear predictor PACE to shine and this is reflected in the empirical results in Tables S9 and S10. These results further reveal that PACE continues to outperform existing imputation methods, such as MICE [22], CNP [7] and GAIN [29], when $x_{i}(\\cdot)$ is not Gaussian. This outstanding performance underscores the status quo, that PACE is the state-of-the-art imputation method for functional data. However, PACE is a linear imputation method, which may restrict its ability to estimate a more complex data structure. In this work, we aim to replace this constraint with a more flexible model that uses our custom self-attention module. ", "page_idx": 2}, {"type": "text", "text": "2.2 Neural processes ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The first study of deep learning for functional data imputation led to neural processes (NP) [7], which perform imputation by modeling the distribution of a target $x_{i}(t)$ conditioning on the observed contexts $\\{(t_{i j},y_{i j})\\}$ via latent variables. From a set of observed data, the NP estimates a distribution over functions from which these data points could have been sampled. This estimation is done more flexibly than traditional Gaussian Process inference by utilizing a learned neural network. Variants of this approach include the conditional neural process (CNP) [7] and attentive neural process [11]. ", "page_idx": 2}, {"type": "text", "text": "While the NP appears promising, we empirically find that straightforward sequence-to-sequence modeling via transformers yields more accurate functional imputations. Our proposed transformer extension learns to recover smooth trajectories from a fixed set of sparse observations in a single-shot. NP models are more complex and must learn their conditional distributions by iteratively sampling observations from the process. At each training iteration of an NP, one first samples a random function and then some observations from the function, repeating this procedure many times. In contrast, our method specifically aims to recover smooth trajectories from a given (fixed) set of observations, rather than relying on iterative sampling. ", "page_idx": 2}, {"type": "text", "text": "2.3 Self-attention ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Transformer [23] comprises an encoder and a decoder, each consisting of stacks of attention and feed-forward layers. [30] highlight the roles of these layers: self-attention captures contextual mappings of the inputs and the feed-forward layer maps them to the desired output values. To provide a general overview of the self-attention module, we consider a generic real-valued input matrix $\\mathbf{\\deltaX}$ of size $q\\times m$ . Let $h_{d}$ be the number of hidden nodes and $H$ be the number of heads used in a selfattention module. For each head $h$ , parameters $W_{O}^{(h)}\\in\\mathbb{R}^{q\\times h_{d}}$ and the key-value-query component ${W_{V}^{(h)},W_{Q}^{(h)},W_{K}^{(h)}\\in\\mathbb{R}^{h_{d}\\times q}}$ are learnable. Denote softmax $(X)$ as applying the softmax operation to columns of $\\mathbf{\\deltaX}$ . A self-attention module defines a mapping from $\\mathbb{R}^{q\\times m}$ to $\\mathbb{R}^{q\\times m}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Attn}(\\boldsymbol{X})=\\boldsymbol{X}+\\sum_{h=1}^{H}\\left\\{\\boldsymbol{W}_{O}^{(h)}\\left(\\boldsymbol{W}_{V}^{(h)}\\boldsymbol{X}\\right)\\mathrm{softmax}\\left[\\left(\\boldsymbol{W}_{K}^{(h)}\\boldsymbol{X}\\right)^{\\top}\\left(\\boldsymbol{W}_{Q}^{(h)}\\boldsymbol{X}\\right)\\left/\\sqrt{h_{d}}\\right]\\right\\}^{\\top}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The input $\\mathbf{\\deltaX}$ can be viewed as a $2\\times n_{i}$ matrix with the first and the second rows being $(y_{i j})_{j=1}^{n_{i}}$ and $(t_{i j})_{j=1}^{n_{i}}$ , respectively. The softmax operation generates a matrix with non-negative entries, where the columns sum up to 1. By multiplying softmax on the right of $\\mathbf{\\deltaX}$ , we obtain a weighted sum of the columns of $\\mathbf{\\deltaX}$ . This can be viewed as an interpolation method aiming to recover the underlying process $x_{i}(t)$ simultaneously for all subjects, by pooling all observed data together to estimate the weights $W_{K}$ and $W_{Q}$ . This approach proves to be more efficient and distinct from individually interpolating each subject through a weighted sum of $\\mathbf{\\deltaX}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As our objective is to smoothly impute curves, a transformer with ReLU activation function (RT) may not be the optimal choice as ReLU is not differentiable at 0. An alternative is to consider a GeLU-activated transformer (GT). Since GT outperforms RT in our benchmarks (Tables S2 and S3), we investigate GT and its variations and propose an architecture that equips GT with our SAND module in the rest of this section. ", "page_idx": 3}, {"type": "text", "text": "3.1 Functional imputation via the vanilla transformer network ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\begin{array}{r}{\\mathcal{T}=\\{0,\\frac{1}{M-1},\\frac{2}{M-1},\\dots,1\\}}\\end{array}$ be a discretization of $[0,1]$ where $M$ is the number of points, $\\widetilde{T}_{i}$ be an imputation on an output grid $\\mathcal{T}$ for subject $i$ , and $\\mathbb{A}$ be the training index set with $\\lvert\\lvert\\mathbb{A}\\rvert\\rvert_{0}$ being the cardinality. A transformer network $f$ defines a mapping from $\\mathbb{R}^{2\\times n_{i}}$ to $\\mathbb{R}^{M}$ through $\\widetilde{T}_{i}=f(S_{i})$ where $S_{i}$ (source) is the input and $\\widetilde{T}_{i}$ (target) is the output. During training, the mean squared error (MSE) loss between the observati ons $\\pmb{y}_{i}$ and the imputations on $\\bar{\\{t_{i j}\\}}_{j=1}^{n_{i}}$ is minimized. Let $\\|{\\pmb x}\\|_{2}$ denote the $\\ell_{2}$ norm of $\\textbf{\\em x}$ . We use the same $\\widetilde{T}_{i}$ as the imputation projected on $(t_{i j})_{j=1}^{n_{i}}$ and minimize ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{MSE_{A}\\;=\\;\\sum_{\\boldsymbol{i}\\in\\mathbb{A}}\\|\\widetilde{T}_{\\boldsymbol{i}}-\\pmb{y}_{\\boldsymbol{i}}\\|_{2}^{2}/\\|\\mathbb{A}\\|_{0}.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "During the imputation, we select a sufficiently large $M=101$ to ensure thatT accurately represents the imputation within the interval $[0,1]$ . However, due to the universal approximation capabilities of transformers [30] and the presence of noisy data during training, well-trained models can often produce zigzag-like imputations that mimic the noise in the training data. Meanwhile, it\u2019s essential to note that when these models encounter testing data, which is also observed with noise, they may continue to generate zigzag patterns if they have learned to replicate the noise patterns present in their training data, as shown in the first two plots of Figure 1. This can result in imputations that fail to capture the underlying, noise-free structure of the data. ", "page_idx": 3}, {"type": "text", "text": "A natural way to counter a non-smooth issue is to add a penalty. Let $\\beta$ denote the collection of all parameters used in GT. By minimizing $\\mathrm{MSE}_{\\mathbb{A}}+\\lambda\\|\\beta\\|_{2}^{2}$ where $\\lambda$ is chosen by the validation dataset, we reduce the complexity of the model by shrinking the parameters towards zero. This results in a model that is less likely to fit the noise. However, it does not guarantee the smoothness of the target function (see the third plot of Figure 1). Alternatively, we explicitly penalize the output function by considering the objective function as $\\mathrm{MSE_{\\mathbb{A}}}+\\lambda\\sum_{i\\in\\mathbb{A}_{-}}\\int_{0}^{1}[\\widetilde{T}_{i}^{(2)}(t)]^{2}\\mathop{d t}/\\|\\mathbb{A}\\|_{0}^{2}$ . This objective function penalizes the curvature of the imputation and restricts its behavior. However, as illustrated in the fourth plot of Figure 1, this penalization approach fails to resolve the non-smooth issue. ", "page_idx": 3}, {"type": "text", "text": "Another approach is to apply statistical methods that ensure the differentiability of the imputations from transformers (either penalized or not). For instance, after the training of the network, we may apply PACE to estimate the eigen-components of the non-smooth outputs and use them to reconstruct the prediction output. A kernel smoothing can also individually smooth each of the outputs. However, transformers with these methods are not typically preferred due to their lack of end-to-end training. Related numerical results can be found in Section 5.1. ", "page_idx": 3}, {"type": "text", "text": "RT GT GT+I2w GT+I20 ATT -SAND ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Figure 1: Close-ups of imputations generated by transformer variants (red curves) for testing data. Green lines are the true underlying functions from which observations (blue dots) are sampled with measurement errors. $\\mathrm{GT}{+}12\\mathrm{w}$ and $\\mathrm{GT}{+}120$ are GTs regularized by the $\\ell_{2}$ norm of its parameters and the curvature of imputations, respectively. SAND is our proposed method. ", "page_idx": 4}, {"type": "text", "text": "Remark 1. Here, we incorporate positional encoding for functional data and prepare it as input sequences for a transformer. Let $n_{\\mathrm{max}}=\\operatorname*{max}_{i}n_{i}$ . For each subject $i$ , the locations $\\pmb{t}_{i}=(t_{i j})_{j=1}^{\\bar{n}_{i}}$ and values $\\pmb{y}_{i}=(y_{i j})_{j=1}^{n_{i}}$ are both padded with $\\left(n_{\\operatorname*{max}}-n_{i}\\right)$ zeros. To prevent attention modules from attending to the padding, we use mask vectors in the encoder and decoder components. To incorporate positional encoding, we define $E_{i}\\,\\in\\,\\mathbb{R}^{p\\times n_{\\operatorname*{max}}}$ as the encoding of $\\pmb{t}_{i}$ , with $p$ being the encoding dimension. $(E_{i})_{j k}$ is computed as s $\\mathrm{in}\\left((M-1)t_{i k}/10000^{(j+1)/p}\\right)$ for odd values of $j$ , and cos $\\left((M-1)t_{i k}/10000^{j/p}\\right)$ for even values of $j$ . Finally, we stack $S_{i}=[y_{i};t_{i};E_{i}]$ as an input. ", "page_idx": 4}, {"type": "text", "text": "3.2 Self-attention on derivative ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To overcome the lack of smoothness in imputations, we introduce a new module called Self-AtteNtion on Derivative (SAND). SAND is stacked after GT, taking the imputations from GT as inputs, and outputting a smoother imputation in two steps: it first learns the derivative of the original imputation through a novel operator $\\mathrm{Diff}(\\cdot)$ , which fathoms the derivative information and is inspired by the self-attention module in (1); then, it reconstructs a smooth version of the imputation via the numerical integral operator $\\mathrm{Intg}(\\cdot)$ . We describe these two operators after introducing necessary notations. ", "page_idx": 4}, {"type": "text", "text": "Let $\\widetilde{T}$ be an imputation on grid $\\mathcal{T}$ with the first element $(\\widetilde{T})_{1}$ and $\\widetilde{T}_{c}=\\widetilde{T}-(\\widetilde{T})_{1}$ as $\\widetilde{T}$ being shifted by $(\\widetilde{T})_{1}$ . Subsequently, let $\\tilde{\\pmb{T}}$ and $\\widetilde{\\pmb{T}}_{c}$ be $(1+p)$ -by- $M$ matrices where the first rows are $\\widetilde{T}$ and $\\widetilde{T}_{c}$ , resp ectively, and the rest of  rows a re the $p$ -dim positional encoding of $\\mathcal{T}$ . The symbol cum sum $(x)$ is ,c thaen dc tibvee  lseuarmnmabaltieo pna oraf $\\textbf{\\em x}$ .e tIetrs $j$ -Tthh ee leDimffe natn $[\\mathrm{cumsum}({\\pmb x})]_{j}$ r si s $\\scriptstyle\\sum_{k=1}^{j}x_{k}$ . Let ${W}_{O}^{(h)}$ , ${W}_{V}^{(h)}$ ${W}_{K}^{(h)}$ $W_{Q}^{(h)}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Diff}(\\widetilde{T})\\;=\\;\\widetilde{D}\\;=\\;\\displaystyle\\sum_{h=1}^{H}W_{O}^{(h)}\\left(W_{V}^{(h)}\\widetilde{T}_{c}\\right)\\left[\\left(W_{K}^{(h)}\\widetilde{T}_{c}\\right)^{\\top}\\left(W_{Q}^{(h)}\\widetilde{T}_{c}\\right)\\bigg/\\sqrt{h_{d}}\\right],}\\\\ &{\\mathrm{Intg}(\\widetilde{D})\\;=\\;\\mathrm{cumsum}\\,[\\widetilde{D}/(M-1)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Denote $\\widehat{T}$ as the output from SAND. Then, SAND can be precisely summarized as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{T}:=\\mathrm{SAND}(\\widetilde{T})\\;=\\;(\\widetilde{T})_{1}+\\mathrm{Intg}[\\mathrm{Diff}(\\widetilde{T})].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Figure 2 depicts the role of SAND: it takes an output $\\widetilde{T}$ from a transformer, learns its derivative via Diff and Intg, and recovers the noise-free trajectory $\\widehat{T}$ by minimizing the $\\ell_{2}$ distance of $\\widehat{T}$ and $\\widetilde{T}$ . ", "page_idx": 4}, {"type": "image", "img_path": "MXRO5kukST/tmp/8656dc051a4afd3672cd56b4ff5eb668d18e95d53a04d6cbc7587409ea57fadb.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: SAND\u2019s pipeline. Dashed lines are underlying processes, dots are observations sampled with errors, solid red curves are imputations, and solid orange curves are learned derivatives. ", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{Diff}(\\cdot)$ operator. The structure \u221aof $\\mathrm{Diff}(\\cdot)$ is similar to the attention module defined in (1) where the key-query product is scaled by $\\sqrt{h_{d}}$ to prevent the product from growing with the hidden dimension. However, $\\mathrm{Diff}(\\cdot)$ removes the softmax operator on the key-query product because the product is used as weights while approximating derivatives. It should not be restricted to be positive only as the approximation of $\\begin{array}{r}{f^{\\prime}(a)\\approx\\frac{f(b)-f(a)}{b-a}}\\end{array}$ , for any differentiable $f$ and provided that $b$ is close to $a$ involves a weighted sum of $f(a)$ and $f(b)$ with both positive and negative weights. We remark that despite we don\u2019t have the observed derivative information to guide $\\mathrm{Diff}(\\cdot)$ during the training process, we conduct a comparison using simulated data. Figure S1 compares the output from Diff with the derivative of the underlying process and indicates that $\\mathrm{Diff}(\\cdot)$ adeptly captures the overall shape. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "$\\mathbf{Intg}(\\cdot)$ operator. The function $\\mathrm{{Intg}(\\cdot)}$ approximates the integral $\\textstyle f(b)-f(a)=\\int_{a}^{b}f^{\\prime}(x)\\,d x$ , where $f(\\cdot)$ is differentiable. At a high level, because $\\mathrm{Diff}(\\cdot)$ is a continuous operator and the integration of a continuous function is differentiable, the output of $\\mathrm{Intg}(\\cdot)$ is continuously differentiable. This result is formally developed in Theorem 1. Furthermore, the $\\mathrm{{Intg}(\\cdot)}$ operator allows SAND to impute the target process from any index $k_{0}$ . We let the $k_{0}$ th location serve the purpose of $(a,f(a))$ in the integral at the beginning of this paragraph, then the imputation from SAND at index $k_{1}$ is $\\begin{array}{r}{\\widetilde{T}_{k_{0}}+(-\\dot{1})^{I(k_{1}<k_{0})}\\sum_{k\\in\\mathcal{K}}^{\\^{\\bullet}}(\\widetilde{D})_{k}/(M-1)}\\end{array}$ where $I(k_{1}<k_{0})$ is 1 if $k_{1}<k_{0}$ and is 0 otherwise, and $\\kappa$ is the set of indices b etween $k_{0}$ and $k_{1}$ (inclusive). Hence, SAND imputes trajectories in a bidirectional fashion. A similar approach, scheduled sampling, was explored by [1]. ", "page_idx": 5}, {"type": "text", "text": "Remark 2. Conceptually, any machine learning model that handles vector inputs and outputs could be utilized to enhance the coarse imputation from GT, as there are numerous viable options. The attention module is adapted for two reasons: ", "page_idx": 5}, {"type": "text", "text": "Computational Efficiency. SAND is computationally efficient compared to other well-established vector-to-vector models like recurrent networks or convolutional networks. This efficiency is crucial in our choice, as highlighted by [23]. The attention module itself has demonstrated effectiveness across diverse applications, thanks in great part to its scalability. ", "page_idx": 5}, {"type": "text", "text": "Achieving Smooth Outputs. Our objective is to produce a smooth curve, which requires that its first derivative be continuous. Since most neural networks inherently model continuous functions, they are suited to this task. The first derivative $\\begin{array}{r}{f^{\\prime}(a)\\approx\\frac{f(b)-f(a)}{b-a}}\\end{array}$ f(bb)\u2212\u2212fa(a)involves a linear combination of function values. In constructing SAND, we chose the attention module because it inherently performs a weighted summation of its inputs, aligning well with our needs for modeling derivative. ", "page_idx": 5}, {"type": "text", "text": "4 Theoretical analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorem 1 demonstrates that the imputations from SAND are continuously differentiable. Theorem 2 establishes a crucial finding that highlights the improved accuracy of SAND: it can be viewed as a dimension reduction technique. Proofs of all results in the section are provided in the supplement. To begin Theorem 1, let $C^{1}(\\mathcal{T})$ denote the set of continuously differentiable functions defined on $\\mathcal{T}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let $\\widehat{T}$ denote the outputs from SAND. Then, $\\widehat{T}$ is in $C^{1}([0,1])$ when $M\\rightarrow\\infty$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 implies that the imputation from SAND is continuously differentiable. This is in contrast to GT, which is prone to overfitting and non-smoothness issues, as discussed in Section 3.1. The assumption that $M\\rightarrow\\infty$ is fairly mild as it could tend to $\\infty$ at any rate, not depending on the sample size $n$ . In simulations, we validate our choice of $M$ by reporting the total variation (TV) of the difference between an imputation and its underlying process. This measure is defined as the average of absolute differences between adjacent elements in the difference. ", "page_idx": 5}, {"type": "text", "text": "Since SAND aims at minimizing the $\\ell_{2}$ distance between its output $(\\widehat{T})$ and the imputation derived from GT (its input,T), it is possible to drive its training error to zero by using a large number of hidden nodes $(h_{d})$ . However, this needs to be avoided as the inputT contains random noise. Instead, the goal for SAND should be to effectively disentangle the inherent smooth process from the random noise in $\\widetilde{T}$ . Therefore, a small training error $\\epsilon$ should be incorporated in SAND. The choice of $\\epsilon$ is tied to the number of hidden nodes $h_{d}$ , which can be determined by the fraction of variance unexplained in FPCA [27]. We formally encapsulate this intuition in Theorem 2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. Define A, $\\tilde{T}_{i}$ , and $\\widetilde{T}_{i,c}$ as in section 3. Suppose that $\\{\\lambda_{j}\\}_{j=1}^{M}$ are the non-increasing eigenvalues of $\\sum_{i}\\widetilde{T}_{i,c}^{\\intercal}\\widetilde{T}_{i,c}$ . For any $\\epsilon\\geq0,$ , let $d_{\\epsilon}=\\operatorname*{min}\\{d\\mid\\sum_{j=1}^{d}\\lambda_{j}/\\sum_{j=1}^{M}\\lambda_{j}\\geq1-\\epsilon\\}$ . Then, there exists a SAND  sat isfying: ( $I)\\,h_{d}\\ge d_{\\epsilon}$ ; and (2) the positional encoding dimension being , so that for any imputation output $\\widehat{T}_{i}$ from SAND, we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{1}{\\|\\mathbb{A}\\|_{0}}\\sum_{i\\in\\mathbb{A}}\\|d_{i}^{m a s k}\\odot(\\widetilde{T}_{i}-\\widehat{T}_{i})\\|_{2}^{2}\\;\\leq\\;\\frac{\\epsilon}{\\|\\mathbb{A}\\|_{0}}\\sum_{i\\in\\mathbb{A}}\\|\\widetilde{T}_{i,c}\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $d_{i}^{m a s k}$ , the decoder masking for ith subject, is a vector of length M whose jth element is 1 if $\\begin{array}{r}{t=\\frac{j}{M-1}}\\end{array}$ is observed in subject $i$ and is $O$ otherwise and u $\\odot$ v represents the element-wise multiplication between vectors u and $v$ .   \nThe eigenvalues of $\\sum_{i}\\widetilde{T}_{i,c}^{\\intercal}\\widetilde{T}_{i,c}$ in Theorem 2 are used to determine the number of hidden nodes in SAND. As a conse que nce  of Theorem 2, the following corollary shows the connection between SAND and dimension reduction techniques of FPCA. ", "page_idx": 6}, {"type": "text", "text": "Corollary 1. Let SAND be defined as in Theorem 2 with $h_{d}=d_{\\epsilon}$ . Suppose that for all subjects, we have full observations on the output grid $\\mathcal{T}_{:}$ , i.e., all elements in $d_{i}^{m a s k}$ is $^{\\,I}$ . Then, SAND minimizes ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbb{P}:r a n k(\\mathbb{P})=d_{\\epsilon}}\\frac{1}{\\|\\mathbb{A}\\|_{0}}\\sum_{i\\in\\mathbb{A}}\\|\\widetilde{T}_{i,c}-\\widetilde{T}_{i,c}\\mathbb{P}\\|_{2}^{2}=\\frac{\\epsilon}{\\|\\mathbb{A}\\|_{0}}\\sum_{i\\in\\mathbb{A}}\\|\\widetilde{T}_{i,c}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Remark 3. Corollary 1 shows that SAND can be seen as a dimension reduction technique by viewing $h_{d}$ and $\\epsilon$ as the fraction of variance unexplained and the number of principal components needed to attain an $\\epsilon$ -error in FPCA, respectively. However, because of the presence of the decoder masking, $h_{d}$ and $\\epsilon$ do not generally play the two roles. Still, Corollary 1 provides an intuitive insights of them. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "SAND is compared to seven baseline methods: PACE [27], FACE [26], mFPCA [13], MICE [22], CNP [7], GAIN [29], and 1DS (one-dimensional kernel smoothing to impute each subject separately). PACE estimates the Karhunen-Lo\u00e9ve presentation based on the function input. FACE imputes the missing data through a different covariance smoothing method from PACE. mFPCA uses a likelihoodbased approach to estimate the functional principal components. MICE imputes the missing data based on the fully conditional specification, where each incomplete variable is imputed by a separate model. CNP parametrizes distribution over imputations giving a distributed representation of pairs of time points and functional values. GAIN imputes the missing data using generative adversarial nets. Due to the lack of available code, we did not include the methods of [16] and [10]. We discuss more on them along with [13] in the supplementary materials. ", "page_idx": 6}, {"type": "text", "text": "Each baseline involves the choice of hyperparameters. In PACE and mFPCA, we let the fraction of variance explained be $99\\%$ . FACE can select all hyperparameter automatically. In MICE, we impute 20 versions of data and take their mean, as suggested in [25]. In CNP, we implement [7] [link] using PyTorch to train the model for 50,000 epochs. The code for GAIN is provided in [29][link]. We train the model for a max of 10,000 epochs with 15 different random seed and take their mean. Finally, in 1DS, the bandwidth of the smoothing window is selected by cross-validation. ", "page_idx": 6}, {"type": "text", "text": "All transformers in Section 3 were trained for 5,000 epochs using a mini-batch of size 256 with a learning rate of $3\\times10^{-4}$ and a $15\\%$ dropout rate. The penalty parameters are picked from a data-driven grid containing 8 points. In SAND, $h_{d}$ is set to 64 and the dropout rate is $5\\%$ . See the code in the supplement for details of the model configuration. To quantify the uncertainty of the imputations, we add the pinball loss with the 10th and 90th target quantiles to the MSE in (2). Training takes 1 day on one Nvidia GeForce GTX 1080 Ti with 10,000 samples. ", "page_idx": 6}, {"type": "text", "text": "Data splitting and evaluation of imputations. The dataset is divided into three parts: the first $90\\%$ is for training, the last $5\\%$ for testing, and the remaining $5\\%$ for validation. The error between prediction and the underlying process on the testing data is reported for comparison. To evaluate the performance of each method, we report the MSE on testing data using (2) along with its standard error (SE). To validate Theorem 1 and our choice of $M$ , we provide the mean and SE of the TV of the difference between any imputation $\\widetilde{T}_{i}$ and the underlying process $x_{i}(\\cdot)$ . ", "page_idx": 6}, {"type": "text", "text": "5.1 Simulation studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Through simulations, we show that transformer-based models outperform other imputation methods. Extensive simulation schemes varying in the number of basis functions, the number of points sampled in a curve, the size of random noises, and the independence of the sampled points are considered. Our simulations are purposely designed to probe the shorts of all baseline methods and neural networks. ", "page_idx": 6}, {"type": "text", "text": "Data-generating process. Let $\\begin{array}{r}{x_{i}(t)=\\sum_{k=1}^{K}\\left[a_{i k}\\,\\sin(2\\pi k t)+b_{i k}\\,\\cos(2\\pi k t)\\right]/k}\\end{array}$ where $t\\in[0,1]$ , $i\\in\\{1,\\ldots,10^{4}\\}$ and $a_{i k}$ , $b_{i k}$ follow a  zero-mean distribution. The actual observed data of the ith curve is a discrete data $\\{y_{i j}\\}_{j=1}^{n_{i}}$ , where $y_{i j}=x_{i}(t_{i j})+\\varepsilon_{i j}$ and $\\varepsilon_{i j}$ follows $N(0,\\sigma^{2})$ independently. The output grid [0, 1] is discretized in $\\{j/100\\}_{j=0}^{100}$ in all simulation. The experiment consists of 12 scenarios, varying in the distribution of the eigenvalues $\\lambda_{i k}$ and $b_{i k}$ ), the number of basis functions $2K$ , and the magnitude of $\\sigma^{2}$ . The full simulation is provided in the supplement (Table S1). In the main text, we only focus on the scenario when the eigenvalues follow an exponential distribution, which stimulates heavy-tailed processes, the number of basis functions is set to 40, the time points within any subject are sampled independently, and the signal-to-noise ratio (SNR) is set to 4. For a function input $f(t)$ , we define $\\operatorname{SNR}(f)$ as $\\textstyle\\int_{T}^{\\cdot}f^{2}(t)\\,d t/\\sigma^{\\overline{{2}}}$ . Finally, each scenario comprises three cases determined by $n_{i}$ : either $n_{i}=30$ , $n_{i}=8$ to 12, or $n_{i}=3,4,5$ . ", "page_idx": 6}, {"type": "table", "img_path": "MXRO5kukST/tmp/15a9565bca4016fe3d252f349ba433e7b88ea740267b90dbe350584e5c7dc168.jpg", "table_caption": ["Table 1: MSE(SE) & TV(SE) on simulated data. Bold values indicate the top 2 performing methods. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Transformer-based models. In Table 1, GT1 and GT2 are GTs penalized with $\\ell_{2}$ norm of parameters and the curvatures of the imputations, respectively, where the penalties are chosen by the validation dataset. For $i$ being 1 or 2, GTiP, GTiS and GTiT use PACE, kernel smoothing and trend flitering [19], respectively, to subsequently smooth the imputations obtained from $\\mathrm{GT}i$ . GT variants are discussed in Section 3.1. Moreover, we investigate the impact of adding an extra self-attention module after the GT as it bears a similar structure to an interpolation method. We refer it as \u2018ATT\u2019 in our analysis. ", "page_idx": 7}, {"type": "text", "text": "Results. Table 1 presents the imputation MSEs and TVs (scaled by 1,000) along with their SEs for all 4 cases. Transformer-based models consistently perform the best across all benchmarks, indicating their efficacy for functional data imputation. We begin by examining GTs, which empirically outperform other baseline methods (Table 1). Despite this, imputations from penalized transformers still lack smoothness (Figure 1). Table 1 reinforces this observation by showing significantly higher TV in the imputations from GT1 and GT2 compared to imputations from PACE, FACE, and CNP. This lack of smoothness can be further mitigated by subsequently applying methods like PACE or individual smoothing to further refine GT imputations. Table 1 confirms that total variations in all GTs decrease, yet a closer look at the table reveals that PACE (GT1P and GT2P) can generally lower MSE, but falls short, particularly when $n_{i}$ is large, where PACE\u2019s posterior smoothing dramatically escalates the MSE. Individual smoothing (GT1S and GT2S) can improve imputations for dense functional data, yet has limited beneftis for sparsely observed data. Although trend flitering methods (GT1T and GT2T) consistently reduce MSE and TV, their performance is ultimately surpassed by SAND. Lastly, using a self-attention module to boost GT doesn\u2019t yield positive results in our simulation. Figure 1 still displays non-smooth imputations, and MSEs increase across all scenarios, compared to GT variants. ", "page_idx": 7}, {"type": "table", "img_path": "MXRO5kukST/tmp/89a6a9b8edc0da469ca44f3295c300a18b31bdd73f7893d31f185541ba969429.jpg", "table_caption": ["Table 2: MSE(SE) and TV(SE) on real datasets. Bold values indicate the top 2 performing methods. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In contrast, the SAND method we proposed consistently outperforms all other approaches, yielding the lowest MSEs and TVs across all scenarios. Table 1 reflects that SAND effectively reduces MSEs and TVs by an average of around $8\\%$ and $13\\%$ , respectively, compared to GT1. This is further substantiated by Figure 1, which visually demonstrates the smoothness of SAND-generated outputs, offering empirical support for the validity of Theorem 1. Within the domain of transformer-based models, our focus is not solely on reducing MSE but also on improving the smoothness of imputations. For this end, we recommend examining the SEs of TVs among different transformer-based models. Upon reviewing TV columns in Table 1, it becomes evident that SAND significantly outperforms all other transformer imputation methods. This highlights the effectiveness of SAND in enhancing the smoothness of imputations. Furthermore, SAND achieves smaller TVs compared to PACE and FACE, suggesting its capacity to capture underlying processes while maintaining low prediction errors. ", "page_idx": 8}, {"type": "text", "text": "5.2 Application to real datasets ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "UK household electricity dataset. We consider a dataset that records every half-hour the energy consumption of 5,494 randomly selected households in London. This results in a total of 96 observations from 11/13 (Wed) to 11/14 (Thurs) of $2013^{3}$ . See [20] for details of the dataset. Our objective is to recover the underlying process of households\u2019 electricity usage where the underlying process is the Gaussian 6-hour moving average of the complete data. To align with our simulation designs, we randomly select the data into $n_{i}=30$ , $n_{i}=8$ to 12, or $n_{i}=3,4,5$ and we sample the time points in two ways - independently and dependently. Tables 2 and S11 report that SAND consistently produces the best performance across all settings. When the observed data contain errors, SAND excels in generating smooth trajectories, as illustrated in Figure S2. They also reveal a substantial reduction in TVs when comparing SAND to GT in all cases. This shows the effectiveness of SAND in enhancing the smoothness of imputations. ", "page_idx": 8}, {"type": "text", "text": "Framingham heart study. We apply SAND and the baseline methods to the longitudinal data from the Framingham Heart Study [5]. We focus on $n=890$ subjects who were live at age 70. Our goal is to reconstruct BMI trajectories from ages 40 to 60 based on their irregular and sparse measurements within this range $\\boldsymbol{n}_{i}\\,=\\,3$ to 11). Here the MSE (marked by asterisks in Table 2) are measured based on the observed data. Figure S3 shows the imputations of two testing data from SAND and its main competitors (PACE, FACE, CNP and GT) and Table 2 highlights the substantial advantage of employing SAND, which reduces MSE by at least $75\\%$ for all competitors except for GT. Here GT has a lower MSE possibly due to overftiting as reflected in its TV and Figure S3. Regarding TV, which represents the total variation of the imputation in this analysis, PACE, FACE and CNP have smaller TVs than GT and SAND. Figure S3 indicates that they might undersmooth the trajectories. Considering both MSE and TV, SAND emerges as the most robust model across various scenarios. ", "page_idx": 8}, {"type": "text", "text": "5.3 Advantage of SAND: Enhancing Downstream Prediction Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section demonstrates better imputations lead to better downstream predictions in two scenarios: (1) predicting the average energy consumption on November 15, 2013 based on the imputed trajectories of energy consumption of the previous two days, (2) forecasting the average BMI between age 61 and 65 in the Framingham Heart Study using the imputed trajectory of the same subject between age 40 and 60. To predict those outcomes, we leverage AdaFNN [28], a machine learning model capable of learning optimal basis representations between the imputations and the targets. Table 3 shows the result of SAND along with the three top competitors in imputation tasks, PACE, FACE and GT. It\u2019s evident that SAND consistently achieves the lowest MSE across all prediction tasks and the principle that better imputations yield more accurate downstream predictions. These findings position SAND as a promising tool for enhancing downstream prediction tasks across diverse domains. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "table", "img_path": "MXRO5kukST/tmp/56882f408ef07113aaa88713350eb2bef718d08d1cda0b4d9a022d97bd373649.jpg", "table_caption": ["Table 3: MSE(SE) on downstream tasks. Bold font marks the smallest MSE across methods. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper introduces a novel SAND layer in a transformer for imputing sparse and noisy functional data. SAND, when paired with a vanilla transformer, constitutes an end-to-end learning mechanism that involves two main operations: Diff (3) and Intg (4). Initially, Diff takes the noisy imputed function from a vanilla transformer and concentrates on the sub-derivative of the imputation. Following this, Intg constructs a differentiable imputation, utilizing the fundamental theorem of calculus. SAND minimizes the $\\ell_{2}$ distance between the imputed and the original transformer-estimated functions, eliminating the need to estimate the derivative of a noisy function. ", "page_idx": 9}, {"type": "text", "text": "From a theoretical perspective, we prove the smoothness of the imputation from SAND and explicitly determine the necessary number of hidden nodes for SAND to achieve an $\\epsilon$ prediction error boundary on training data (up to a constant). Interestingly, the error boundary serves the role of the fraction of variance unexplained in FPCA literature. Traditional imputation methods, such as FPCA, employ a dimension-reduction strategy to learn trajectories via a finite-dimensional vector. Empirical benchmarks reveal that SAND surpasses FPCA when the curves aren\u2019t a Gaussian process with a low number of basis function. SAND effectively smooths noisy imputations from a vanilla transformer, and yields more accurate imputations than other techniques to encourage smoothness. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Bengio, S., O. Vinyals, N. Jaitly, and N. Shazeer (2015). Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems.   \n[2] Besse, P. and J. O. Ramsay (1986). Principal components analysis of sampled functions. Psychometrika 51(2), 285\u2013311.   \n[3] Cardot, H., F. Ferraty, and P. Sarda (2003). Spline estimators for the functional linear model. Statistica Sinica, 571\u2013591.   \n[4] Chen, K., X. Zhang, A. Petersen, and H.-G. M\u00fcller (2017). Quantifying infinite-dimensional data: Functional data analysis in action. Statistics in Biosciences 9, 582\u2013604.   \n[5] D\u2019Agostino, R. B., S. Grundy, L. M. Sullivan, P. Wilson, C. R. P. Group, et al. (2001). Validation of the framingham coronary heart disease prediction scores: results of a multiple ethnic groups investigation. Jama 286(2), 180\u2013187.   \n[6] Ferraty, F. and P. Vieu (2006). Nonparametric Functional Data Analysis: Theory and Practice, Volume 76. Springer.   \n[7] Garnelo, M., D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. Rezende, and S. A. Eslami (2018). Conditional neural processes. In International Conference on Machine Learning.   \n[8] Garnelo, M., J. Schwarz, D. Rosenbaum, F. Viola, D. J. Rezende, S. Eslami, and Y. W. Teh (2018). Neural processes. In ICML workshop on Theoretical Foundations and Applications of Deep Generative Models.   \n[9] Hsing, T. and R. Eubank (2015). Theoretical Foundations of Functional Data Analysis, with an Introduction to Linear Operators, Volume 997. John Wiley & Sons.   \n[10] James, G. M., T. J. Hastie, and C. A. Sugar (2000). Principal component models for sparse functional data. Biometrika 87(3), 587\u2013602.   \n[11] Kim, H., A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y. W. Teh (2019). Attentive neural processes. In International Conference on Learning Representations.   \n[12] Li, Y. and T. Hsing (2010). Uniform convergence rates for nonparametric regression and principal component analysis in functional/longitudinal data. The Annals of Statistics.   \n[13] Peng, J. and D. Paul (2009). A geometric approach to maximum likelihood estimation of the functional principal components from sparse longitudinal data. Journal of Computational and Graphical Statistics 18(4), 995\u20131015.   \n[14] Ramsay, J. and B. Silverman (2002). Applied Functional Data Analysis. Methods and Case Studies, Volume 77. Springer.   \n[15] Rice, J. A. and B. W. Silverman (1991). Estimating the mean and covariance structure nonparametrically when the data are curves. Journal of the Royal Statistical Society: Series B (Methodological) 53(1), 233\u2013243.   \n[16] Rice, J. A. and C. O. Wu (2001). Nonparametric mixed effects models for unequally sampled noisy curves. Biometrics 57(1), 253\u2013259.   \n[17] Schafer, J. L. and J. W. Graham (2002). Missing data: our view of the state of the art. Psychological Methods 7(2), 147.   \n[18] Silverman, B. W. (1996). Smoothed functional principal components analysis by choice of norm. The Annals of Statistics 24(1), 1\u201324.   \n[19] Tibshirani, R. J. (2014). Adaptive piecewise polynomial estimation via trend filtering. Annals of Statistics 42(1), 285\u2013323.   \n[20] UK Power Networks (2015). Smartmeter energy consumption data in london households.   \n[21] Van Buuren, S., H. C. Boshuizen, and D. L. Knook (1999). Multiple imputation of missing blood pressure covariates in survival analysis. Statistics in medicine 18(6), 681\u2013694.   \n[22] Van Buuren, S. and K. Groothuis-Oudshoorn (2011). mice: Multivariate imputation by chained equations in R. Journal of Statistical Software 45, 1\u201367.   \n[23] Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin (2017). Attention is all you need. In Advances in Neural Information Processing Systems.   \n[24] Wang, J.-L., J.-M. Chiou, and H.-G. M\u00fcller (2016). Functional data analysis. Annual Review of Statistics and Its Application 3, 257\u2013295.   \n[25] White, I. R., P. Royston, and A. M. Wood (2011). Multiple imputation using chained equations: issues and guidance for practice. Statistics in Medicine 30(4), 377\u2013399.   \n[26] Xiao, L., C. Li, W. Checkley, and C. Crainiceanu (2018). Fast covariance estimation for sparse functional data. Statistics and computing 28, 511\u2013522.   \n[27] Yao, F., H.-G. M\u00fcller, and J.-L. Wang (2005). Functional data analysis for sparse longitudinal data. Journal of the American Statistical Association 100(470), 577\u2013590.   \n[28] Yao, J., J. Mueller, and J.-L. Wang (2021). Deep learning for functional data analysis with adaptive basis layers. In International Conference on Machine Learning.   \n[29] Yoon, J., J. Jordon, and M. Schaar (2018). Gain: Missing data imputation using generative adversarial nets. In International conference on machine learning, pp. 5689\u20135698. PMLR.   \n[30] Yun, C., S. Bhojanapalli, A. S. Rawat, S. Reddi, and S. Kumar (2020). Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations.   \n[31] Zhang, J.-T. and J. Chen (2007). Statistical inferences for functional data. The Annals of Statistics 35(3), 1052\u20131079. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: Section 3.1 addresses the limitations of using a vanilla transformer for imputation tasks. Section 3.2 introduces SAND as a solution. Theoretical properties of SAND are rigorously discussed in Section 4, and numerical study results are presented in Section 5. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 12}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 12}, {"type": "text", "text": "Justification: The limitations of SAND are discussed in Section 2.3 of the supplementary material due to space constraints in the main text. ", "page_idx": 12}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 12}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 12}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The assumptions for the theoretical results are stated in Section 4, and the complete proofs are provided in the supplementary materials. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 13}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The supplementary zip file includes the code for generating simulated data with a fixed seed, ensuring reproducibility. The real data is provided in the designated folder, with explicit file names. A sketch of the training instructions is given in Section 5.1, and detailed training instructions are available in the README file. Additionally, the random seed is fixed in the training procedure to ensure reproducibility. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in ", "page_idx": 13}, {"type": "text", "text": "some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The supplementary materials include open access to both the data and the code. Detailed instructions for reproducing the main experimental results are provided, including a fixed random seed for the training procedure to ensure reproducibility. The real data is organized in clearly labeled folders, and comprehensive training instructions are outlined in Section 5.1 and detailed further in the README file. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 14}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: Key training settings are described in Section 5.1, with detailed information provided in the README file. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 14}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: The standard error of the mean square error estimation is provided in all tables. Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 15}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: Sufficient information on the computer resources is provided in the second paragraph of Page 7 in the main text. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 15}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The authors cite all references to the datasets used and include the licenses for the code employed to run some of the baseline methods. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 15}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: This paper focuses on proposing a new layer and primarily emphasizes theoretical and simulation results for imputing missing values in functional data. Given the scope and focus of the paper, the discussion of negative societal impacts is deemed irrelevant and therefore not included. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 16}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: This paper does not involve text or image datasets, nor does it deal with high-risk models. The focus is solely on functional data imputation, which does not present significant misuse risks. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 16}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper properly credits the creators and original owners of all assets used. Links to the relevant GitHub repositories are provided in the main text, and the licenses are explicitly included in the supplementary zip file. All usage complies with the respective licenses. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 17}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not introduced any new assets. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 17}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not involve experiments or research with human subjects. Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 17}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 18}]