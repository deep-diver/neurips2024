[{"figure_path": "ogk236hsJM/figures/figures_1_1.jpg", "caption": "Figure 1: Time for a Human Preference Study! Could you please tell us which one is better? Hint: the rightmost column is the one-step Latent Consistency Model of PixelArt-a; The left two columns are randomly placed, with one generated from our one-step SIM-DiT-600M model, and another generated from the 14-step PixelArt-a teacher diffusion model. We put the answer in Appendix B.1.", "description": "This figure presents a human preference study comparing image generation quality. Three images are shown for three different prompts. The rightmost column uses the one-step Latent Consistency Model of PixelArt-a. The left two columns show images generated by the proposed SIM-DiT-600M model and the 14-step PixelArt-a teacher diffusion model, respectively, for each prompt. The user is asked to identify the best quality image, with the answer provided in the appendix.", "section": "1 Introduction"}, {"figure_path": "ogk236hsJM/figures/figures_7_1.jpg", "caption": "Figure 2: Left Two: Comparison of distillation methods with a batch size of 256 and a learning rate of 1e-4. (Left): the FID value. (Right): the Inception Scores. Right Two: Comparison of distillation methods with a batch size of 256 and a learning rate of 1e \u2013 5. (Left): the FID value. (Right): the Inception Scores. All methods are constrained to the same settings except for the distillation methods.", "description": "This figure compares the performance of different distillation methods (SIM, SiD) under various learning rates (1e-4, 1e-5) and batch sizes (256).  The left two subfigures show FID (Frechet Inception Distance) and Inception Score values, respectively, for the 1e-4 learning rate experiments.  The right two subfigures display the same metrics, but for the 1e-5 learning rate experiments.  The results demonstrate that SIM is more robust to the choice of hyperparameters than SiD, achieving faster convergence and more stable performance across both learning rates.", "section": "4 Experiments"}, {"figure_path": "ogk236hsJM/figures/figures_8_1.jpg", "caption": "Figure 3: Qualitative comparison of SIM-DiT-600M against other few-step text-to-image models. Please zoom in to check details, lighting, and aesthetic performances. Prompts in Appendix B.7.", "description": "This figure compares the image generation quality of SIM-DiT-600M against other leading few-step text-to-image models, including SDXL-Hyper, SDXL-TCD, SDXL-Turbo, SDXL-Lightning, and LCM-PixArt-alpha. The comparison focuses on image details, lighting, and overall aesthetic quality.  The prompts used to generate these images can be found in Appendix B.7 of the paper.", "section": "4.2 Transformer-based One-step Text-to-Image Generator"}, {"figure_path": "ogk236hsJM/figures/figures_8_2.jpg", "caption": "Figure 1: Time for a Human Preference Study! Could you please tell us which one is better? Hint: the rightmost column is the one-step Latent Consistency Model of PixelArt-a; The left two columns are randomly placed, with one generated from our one-step SIM-DiT-600M model, and another generated from the 14-step PixelArt-a teacher diffusion model. We put the answer in Appendix B.1.", "description": "This figure presents a human preference study comparing image generation quality. Three columns showcase images: the rightmost shows a one-step Latent Consistency Model, while the leftmost two are randomly ordered images from a one-step SIM-DiT-600M model and a 14-step PixelArt-a diffusion model. Users are asked to identify the better-quality image, with the correct answer provided in the appendix. This study highlights the ability of the one-step SIM model to produce comparable image quality to the 14-step model.", "section": "1 Introduction"}, {"figure_path": "ogk236hsJM/figures/figures_28_1.jpg", "caption": "Figure 2: Left Two: Comparison of distillation methods with a batch size of 256 and a learning rate of 1e-4. (Left): the FID value. (Right): the Inception Scores. Right Two: Comparison of distillation methods with a batch size of 256 and a learning rate of 1e \u2013 5. (Left): the FID value. (Right): the Inception Scores. All methods are constrained to the same settings except for the distillation methods.", "description": "This figure compares the performance of different distillation methods (SIM and SiD) under various settings.  The left two subfigures show FID and Inception Scores for a batch size of 256 and learning rate of 1e-4, highlighting SIM's robustness to hyperparameters. The right two subfigures present the same metrics but with a smaller learning rate of 1e-5, further emphasizing SIM's superior performance and faster convergence compared to SiD.", "section": "4 Experiments"}, {"figure_path": "ogk236hsJM/figures/figures_29_1.jpg", "caption": "Figure 1: Time for a Human Preference Study! Could you please tell us which one is better? Hint: the rightmost column is the one-step Latent Consistency Model of PixelArt-a; The left two columns are randomly placed, with one generated from our one-step SIM-DiT-600M model, and another generated from the 14-step PixelArt-a teacher diffusion model. We put the answer in Appendix B.1.", "description": "This figure presents a human preference study comparing image generation quality. Three columns of images are shown, each generated using a different method.  The rightmost column shows images from a one-step Latent Consistency Model, while the left two columns feature images from a one-step SIM-DiT-600M model and a 14-step PixelArt-a diffusion model (the order is randomized). Readers are prompted to choose which images they prefer, with the answer revealed in Appendix B.1. This comparison highlights the effectiveness of the SIM-DiT-600M model in achieving one-step generation quality close to that of a multi-step method.", "section": "1 Introduction"}, {"figure_path": "ogk236hsJM/figures/figures_29_2.jpg", "caption": "Figure 6: One-step SIM model on CIFAR10-conditional. FID=1.96.", "description": "This figure shows the images generated by a one-step Score Implicit Matching (SIM) model on the CIFAR10 dataset, conditioned on class labels. The FID score of 1.96 indicates high-quality image generation.  The figure demonstrates the model's ability to generate realistic and diverse images within a single step, a significant improvement over traditional diffusion models that require many steps for comparable results. ", "section": "4 Experiments"}, {"figure_path": "ogk236hsJM/figures/figures_30_1.jpg", "caption": "Figure 6: One-step SIM model on CIFAR10-conditional. FID=1.96.", "description": "This figure shows the images generated by a one-step generator trained using the Score Implicit Matching (SIM) method on the CIFAR10 dataset.  The images are class-conditional, meaning that the generator was trained to produce images of a specific class (e.g., cat, dog, bird). The FID score (Frechet Inception Distance) of 1.96 indicates high-quality image generation, comparable to the performance of more computationally expensive multi-step diffusion models.", "section": "4 Experiments"}, {"figure_path": "ogk236hsJM/figures/figures_30_2.jpg", "caption": "Figure 2: Left Two: Comparison of distillation methods with a batch size of 256 and a learning rate of 1e-4. (Left): the FID value. (Right): the Inception Scores. Right Two: Comparison of distillation methods with a batch size of 256 and a learning rate of 1e \u2013 5. (Left): the FID value. (Right): the Inception Scores. All methods are constrained to the same settings except for the distillation methods.", "description": "This figure compares the performance of SIM, SiD, and DI under different learning rates (1e-4 and 1e-5) and batch size (256) in terms of FID and Inception Score.  The left two subfigures show the results with a learning rate of 1e-4, while the right two subfigures show the results with a learning rate of 1e-5.  The plots demonstrate SIM's robustness to higher learning rates and faster convergence compared to the other methods.", "section": "4.1 One-step CIFAR10 Generation"}]