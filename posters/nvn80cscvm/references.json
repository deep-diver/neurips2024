{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models, introducing the concept of few-shot learning and significantly advancing the capabilities of LLMs."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-15", "reason": "This paper introduces the concept of compute-optimal LLMs, which has significantly influenced the design and training of efficient and effective LLMs."}, {"fullname_first_author": "Susan Zhang", "paper_title": "OPT: Open pre-trained transformer language models", "publication_date": "2022-05-01", "reason": "This paper introduces the OPT family of language models, which is extensively used in the experiments and forms a key basis for comparing different LLM sizes and capabilities."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-04-08", "reason": "This paper introduces visual instruction tuning for multi-modal LLMs, which is directly relevant to the multi-modal evaluation methods presented in the paper."}, {"fullname_first_author": "Jun Chen", "paper_title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning", "publication_date": "2023-10-09", "reason": "This paper presents MiniGPT-v2, a multi-modal LLM used in the paper's experiments, contributing significantly to the evaluation of multi-modal capabilities."}]}