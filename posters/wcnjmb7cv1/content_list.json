[{"type": "text", "text": "Learning to Assist Humans without Inferring Rewards ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Vivek Myers1 Evan Ellis1 ", "page_idx": 0}, {"type": "text", "text": "Sergey Levine1 Benjamin Eysenbach2 Anca Dragan1 ", "page_idx": 0}, {"type": "text", "text": "1UC Berkeley 2Princeton University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Assistive agents should make humans\u2019 lives easier. Classically, such assistance is studied through the lens of inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human\u2019s intention and then selects actions to help the human reach that goal. This approach requires inferring intentions, which can be difficult in high-dimensional settings. We build upon prior work that studies assistance through the lens of empowerment: an assistive agent aims to maximize the influence of the human\u2019s actions such that they exert a greater control over the environmental outcomes and can solve tasks in fewer steps. We lift the major limitation of prior work in this area \u2014 scalability to high-dimensional settings \u2014 with contrastive successor representations. We formally prove that these representations estimate a similar notion of empowerment to that studied by prior work and provide a ready-made mechanism for optimizing it. Empirically, our proposed method outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a cooperative game setting. Theoretically, our work connects ideas from information theory, neuroscience, and reinforcement learning, and charts a path for representations to play a critical role in solving assistive problems.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "AI agents deployed in the real world should be helpful to humans. When we know the utility function of the humans an agent could interact with, we can directly train assistive agents through reinforcement learning with the known human objective as the agent\u2019s reward. In practice, agents rarely have direct access to a scalar reward corresponding to human preferences (if such a consistent model even exists) [1], and must infer them from human behavior [2, 3]. This inference can be challenging, as humans may act suboptimally with respect to their stated goals, not know their goals, or have changing preferences [4]. Optimizing a misspecified reward function can have poor consequences [5]. ", "page_idx": 0}, {"type": "text", "text": "An alternative paradigm for assistance is to train agents that are intrinsically motivated to assist humans, rather than directly optimizing a model of their preferences. An analogy can be drawn to a parent raising a child. A good parent will empower the child to make impactful decisions and flourish, rather than proscribing an \u201coptimal\u201d outcome for the child. Likewise, AI agents might seek to empower the human agents they interact with, maximizing their capacity to change the environment [6]. In practice, concrete notions of empowerment can be difficult to optimize as an objective, requiring extensive modeling assumptions that don\u2019t scale well to the high-dimensional settings deep reinforcement learning agents are deployed in. ", "page_idx": 0}, {"type": "text", "text": "What is a good intrinsic objective for assisting humans that doesn\u2019t require these assumptions? We propose a notion of assistance based on maximizing the influence of the human\u2019s actions on the environment. This approach only requires one structural assumption: the AI agent is interacting with an environment where there is a notion of actions taken by the human agent\u2014a more general setting than the case where we model the human actions as the outcome of some optimization procedure, as in inverse RL [7, 8] or preference-based RL [9]. ", "page_idx": 0}, {"type": "image", "img_path": "WCnJmb7cv1/tmp/9610d7b991a6114f9e4e10f1c90496fc9a54fbc4c9218022ecbf3c20b11ff3cd.jpg", "img_caption": ["Figure 1: We propose an algorithm training assistive agents to empower human users\u2014the assistant should take actions that enable human users to visit a wide range of future states, and the human\u2019s actions should exert a high degree of influence over the future outcomes. Our algorithm scales to high-dimensional settings, opening the door to building assistive agents that need not directly reason about human intentions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Prior work has studied many effective objectives for empowerment. For instance, Du et al. [6] approximates human empowerment as the variance in the final states of random rollouts. Despite excellent results in certain settings, this approach can be challenging to scale to higher dimensional settings, and does not necessarily enable human users to achieve the goals the want to achieve. By contrast, our approach exclusively empowers the human with respect to the distribution of (useful) behaviors induced by their current policy, and can be implemented through a simple objective derived from contrastive successor features, which can then be optimized with scalable deep reinforcement learning (Fig. 1). We provide a theoretical framework connecting our objective to prior work on empowerment and goal inference, and empirically show that agents trained with this objective can assist humans in the Overcooked environment [10] as well as the obstacle gridworld assistance benchmark proposed by Du et al. [6]. ", "page_idx": 1}, {"type": "text", "text": "Our core contribution is a novel objective for training agents that are intrinsically motivated to assist humans without requiring a model of the human\u2019s reward function. Our objective, Empowerment via Successor Representations (ESR), maximizes the influence of the human\u2019s actions on the environment, and, unlike past approaches for assistance without reward inference, is based on a scalable model-free objective that can be derived from learned successor features that encode which states the human is likely to want to reach given their current action. Our objective empowers the human to reach the desired states, not all states, without assuming a human model. We analyze this objective in terms of empowerment and goal inference, drawing novel mathematical connections between time-series representations, decision-making, and assistance. We empirically show that agents trained with our objective can assist humans in two benchmarks proposed by past work: the Overcooked environment [10] and an obstacle-avoidance gridworld [6]. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our approach broadly connects ideas from contrastive contrastive representation learning and intrinsic motivation to the problem of assisting humans. ", "page_idx": 1}, {"type": "text", "text": "Assistive Agents. There are two lines of past work on assistive agents that are most relevant. ", "page_idx": 1}, {"type": "text", "text": "The first line of work focuses on the setting of an assistance game [2], where a robot (AI) agent tries to optimize a human reward of which it is initially unaware. Practically, inverse reinforcement learning (IRL) can be used in such a setting to infer the human\u2019s reward function and assist the human in achieving their goals [3]. The key challenge with this approach is that it requires modeling the human\u2019s reward function. This can be difficult in practice, especially if the human\u2019s behavior is not well-modeled by the reward architecture. Slightly mispecified reward functions can lead to catastrophic outcomes (i.e., directly harmful behavior in the assistance context) [11\u201313]. By contrast, our approach does not require modeling the human\u2019s reward function. ", "page_idx": 2}, {"type": "text", "text": "The second line of work focuses on empowerment-like objectives for assistance and shared autonomy. Empowerment generally refers to a measure of an agent\u2019s ability to influence the environment [14, 15]. In the context of assistance, Du et al. [6] show one such approximation of empowerment (AvE) can be approximated in simple environments through random rollouts to assist humans. Meanwhile, empowerment-like objectives have been used in shared autonomy settings to assist humans with teleoperation [16] and general assistive interfaces [17]. A key limitation of these approaches for general assistance is they only model empowerment over one time step. Our approach enables a more scalable notion of empowerment that can be computed over multiple time steps. ", "page_idx": 2}, {"type": "text", "text": "Intrinsic Motivation. Intrinsic motivation broadly refers to agents that accomplish behaviors in the absence of an externally-specified reward or task [18]. Common applications of intrinsic motivation in single-agent reinforcement learning include exploration and skill discovery [19\u201321], empowerment [15, 14], and surprise minimization [22, 23, 15]. When applied to settings with humans, these objectives may lead to antisocial behavior [5]. Our approach applies intrinsic motivation to the setting of assisting humans, where the agent\u2019s goal is an empowerment objective\u2014to maximize the human\u2019s ability to change the environment. ", "page_idx": 2}, {"type": "text", "text": "Information-theoretic Decision Making. Information-theoretic approaches have seen broad applicability across unsupervised reinforcement learning [24, 15, 19]. These methods have been applied to goal-reaching [25], skill discovery [26, 27, 20, 28, 29], and exploration [21, 30, 31]. In the context of assisting humans, information-theoretic methods have primarily been used to reason about the human\u2019s goals or rewards [32\u201334]. ", "page_idx": 2}, {"type": "text", "text": "Our approach is made possible by advances in contrastive representation learning for efficient estimation of the mutual information of sequence data [35]. While these methods have been widely used for representation learning [36, 37] and reinforcement learning [38\u201341], to the best of our knowledge prior work has not used these contrastive techniques for learning assistive agents. ", "page_idx": 2}, {"type": "text", "text": "3 The Information Geometry of Empowerment ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We will first state a general notion of an assistive setting, then show how an empowerment objective based on learned successor representations can be used to assist humans without making assumptions about the human following an underlying reward function. In Section 5, we provide empirical evidence supporting these claims. ", "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Formally, we adapt the notation of Hadfield-Menell et al. [2], and assume a \u201crobot\u201d $(\\bf R)$ and \u201chuman\u201d $(\\mathbf{H})$ policy are training together in an MDP $M=(S,\\mathcal{A}_{\\bf H},\\mathcal{A}_{\\bf R},R,\\mathrm{P},\\gamma)$ . The states $s$ consist of the joint states of the robot and the human; we do not have separate observations for the human and robot. At any state $s\\in S$ , the robot policy selects actions distributed according to $\\pi_{\\mathbf{R}}(a^{\\mathbf{R}}\\mid s)$ for $a^{\\mathbf{R}}\\in\\mathcal{A}_{\\mathbf{R}}$ and the human selects actions from $\\pi_{\\mathbf{H}}(a^{\\mathbf{H}}\\mid s)$ for $a^{\\mathbf{H}}\\in\\mathcal{A}_{\\mathbf{H}}$ . The transition dynamics are defined by a distribution $\\underline{{\\mathbf{P}}}(s^{\\prime}\\mid s,a^{\\boldsymbol{\\mathbf{H}}},a^{\\boldsymbol{\\mathbf{R}}})$ over the next state $s^{\\prime}\\in\\mathcal{S}$ given the current state $s\\in S$ and actions $a^{\\mathbf{H}}\\in\\mathcal{A}_{\\mathbf{H}}$ and $a^{\\mathbf{R}}\\in\\dot{\\mathcal{A}}_{\\mathbf{R}}$ , as well as an initial state distribution $\\mathrm{P}(s_{0})$ . For notational convenience, we will additionally define random variables $\\mathfrak{s}_{t}$ to represent the state at time $t$ , and $\\mathfrak{a}_{t}^{\\mathbf{R}}\\sim\\pi_{\\mathbf{R}}(\\bullet\\mid\\mathfrak{s}_{t})$ and $\\mathfrak{a}_{t}^{\\mathbf{H}}\\sim\\pi_{\\mathbf{H}}(\\bullet\\mid\\dot{\\mathfrak{s}_{t}})$ to represent the human and robot actions at time $t$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Empowerment. Our work builds on a long line of prior methods that use information theoretic objectives for RL. Specifically, we adopt empowerment as an objective for training an assistive agent [6, 42, 43]. This section provides the mathematical foundations for empowerment, as developed in prior work. Our work will build on the prior work by (1) providing an information geometric interpretation of what empowerment does (Section 3.3) and (2) providing a scalable algorithm for estimating and optimizing empowerment, going well beyond the gridworlds studied in prior work. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "The idea behind empowerment is to think about the changes that an agent can effect on a world; an agent is more empowered if it can effect a larger degree of change over future outcomes. Following prior work [25, 43, 42], we measure empowerment by looking at how much the actions taken now affect outcomes in the future. An agent with a high degree of empowerment exerts a high degree of control of the future states by simply changing the actions taken now. Like prior work, we measure this degree of control through the mutual information $I({\\mathfrak{s}}^{+};a^{\\mathbf{H}})$ between the current action $a^{\\mathbf{H}}$ and the future states ${\\mathfrak{s}}^{+}$ . Note that these future states might occur many time steps into the future. ", "page_idx": 3}, {"type": "text", "text": "Empowerment depends on several factors: the environment dynamics, the choice of future actions, the current state, and other agents in the environment. Different problem settings involve maximizing empowerment using these different factors. In this work, we study the setting where a \u201chuman\u201d agent and a \u201crobot\u201d agent collaborate in an environment; the robot will aim to maximize the empowerment of the human. This problem setting was introduced in prior work [6]. Compared with other mathematical frameworks for learning assistive agents [44], framing the problem in terms of empowerment means that the assistive agent need not infer the human\u2019s underlying intention, an inference problem that is typically challenging [45, 46]. ", "page_idx": 3}, {"type": "text", "text": "We now define our objective. To do this, we introduce random variable ${\\mathfrak{s}}^{+}$ , which corresponds to a state sampled $K\\sim\\mathrm{Geom}(1-\\gamma)$ steps into the future under the behavior policies $\\pi\\mathbf{H}$ and $\\pi_{\\mathbf{R}}$ . We will use $\\bar{\\rho(\\mathfrak{s}^{+}\\mid s_{t})}$ to denote the density of this random variable; this density is sometimes referred to as the discounted state occupancy measure. We will use mutual information to measure how much the action $a_{t}$ at time $t$ changes this distribution: ", "page_idx": 3}, {"type": "equation", "text": "$$\nI(a_{t}^{\\mathbf{H}};\\mathfrak{s}^{+}\\mid s_{t})\\triangleq\\mathbb{E}_{s_{t},s_{t+k},a_{t}^{\\mathbf{H}},a_{t}^{\\mathbf{R}}}\\left[\\log\\frac{\\mathrm{P}(\\mathfrak{s}_{t+K}=s_{t+k}\\mid\\mathfrak{s}_{t}=s_{t},\\mathfrak{a}_{t}^{\\mathbf{H}}=a_{t})}{\\mathrm{P}(\\mathfrak{s}_{t+K}=s_{t+k}\\mid\\mathfrak{s}_{t}=s_{t})}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Our overall objective is empowerment, $\\mathcal{E}(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})$ : the mutual information between the human\u2019s actions and the future states ${\\mathfrak{s}}^{+}$ while interacting with the robot: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})=\\mathbb{E}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}I(\\mathfrak{a}_{t}^{\\mathbf{H}};\\mathfrak{s}^{+}\\mid\\mathfrak{s}_{t})\\Big].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that this objective resembles an RL objective: we do not just want to maximize this objective greedily at each time step, but rather want the assistive agents to take actions now that help the human agent reach states where it will have high empowerment in the future. ", "page_idx": 3}, {"type": "image", "img_path": "WCnJmb7cv1/tmp/49883b2c6c234aaf367fa237678f25a0b41efe8b84d052fe866e289661a7550f.jpg", "img_caption": ["(a) State marginal polytope ", "(b) Mutual information ", "(c) Maximizing empowerment "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: The Information Geometry of Empowerment, illustrating the analysis in Section 3.3. (Left) For a given state $s_{t}$ and assistant policy $\\pi_{\\mathbf{R}}$ , we plot the distribution over future states for 6 choices of the human policy $\\pi\\mathbf{H}$ . In a 3-state MDP, we can represent each policy as a vector lying on the 2-dimensional probability simplex. We refer to the set of all possible state distributions as the state marginal polytope. (Center) Mutual information corresponds to the distance between the center of the polytope and the vertices that are maximally far away. (Right) Empowerment corresponds to maximizing the size of this polytope. For example, when an assistive agent moves an obstacle out of a human user\u2019s way, the human user can spend more time at desired state. ", "page_idx": 3}, {"type": "text", "text": "3.2 Intuition and Geometry of Empowerment ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Intuitively, the assistive agent should aim to maximize the number of future outcomes. We will mathematically quantify this in terms of the discounted state occupancy measure, $\\rho^{\\pi}(s^{+}~|~s)$ . Intuitively, an agent has a large empowerment if the future states for one action are very different from the future actions after taking a different action; i.e., when $\\rho(a_{t}=a_{1};\\mathfrak{s}^{+}\\mid s_{t})$ is quite different from $\\rho(a_{t}\\mid s_{2};\\mathfrak{s}^{+}\\mid s_{t})$ for actions $a_{1}\\neq a_{2}$ . The mutual information (Eq. (1)) quantifies this degree of control: $I(a_{t};{\\mathfrak{s}}^{+}\\mid s_{t})$ . ", "page_idx": 4}, {"type": "text", "text": "One way of understanding this mutual information is through information geometry [47, 48, 48, 49]. For a fixed current state $s_{t}$ , assistant policy $\\pi_{\\mathbf{R}}$ and human policy $\\pi\\mathbf{H}$ , each potential action $a_{t}$ that the human takes induces a different distribution over future states: $\\rho^{\\pi_{\\mathbf{R}},\\pi_{\\mathbf{H}}}(\\mathfrak{s}^{+}\\mid s_{t},a_{t})$ . We can think about the set of these possible distributions: $\\left\\{\\rho^{\\pi_{\\mathbf{R}},\\pi_{\\mathbf{H}}}(\\mathfrak{s}^{+}\\mid s_{t},\\overset{\\cdot}{a}_{t})\\mid a_{t}\\in{\\mathcal{A}}\\right\\}$ . Figure 2 $(L e f t)$ visualizes this distribution on a probability simplex for 6 choices of action $a_{t}$ . If we look at any possible distribution over actions, then this set of possible future distributions becomes a polytope (see orange polygon in Fig. 2 (Center)). ", "page_idx": 4}, {"type": "text", "text": "Intuitively, the mutual information $I(a_{t};{\\mathfrak{s}}^{+}\\mid s_{t})$ used to define our empowerment objective corresponds to the size or volume of this state marginal polytope. This intuition can be formalized by using results from information geometry [50\u201352]. The human policy $\\pi_{\\mathbf{H}}(a_{t}\\mid s_{t})$ places probability mass on the different points in Figure 2 (Center). Maximizing the mutual information corresponds to \u201cpicking out\u201d the state distributions that are maximally spread apart (see probabilities in Fig. 2 (Center)). To make this formal, define ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\rho}(\\mathfrak{s}^{+}\\mid s_{t})\\triangleq\\mathbb{E}_{\\pi(a_{t}\\mid s_{t})}[{\\rho}(\\mathfrak{s}^{+}\\mid s_{t},a_{t})]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "as the average state distribution from taking the human\u2019s actions (see green square in Fig. 2 (Center)). ", "page_idx": 4}, {"type": "text", "text": "Remark 3.1. Mutual information corresponds to the distance between the average state distribution $\\left(E q.\\;3\\right)$ and the furthest achievable state distributions: ", "page_idx": 4}, {"type": "equation", "text": "$$\nI(a_{t};\\mathfrak{s}^{+}\\mid s_{t})=\\operatorname*{max}_{a_{t}}D_{K L}\\bigl(\\rho(a_{t};\\mathfrak{s}^{+}\\mid s_{t})\\,\\bigr\\Vert\\,\\rho(\\mathfrak{s}^{+}\\mid s_{t})\\bigr)\\triangleq d_{m a x}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This distance is visualized as the black lines in Fig. 2. When we talk about the \u201csize\u201d of the state marginal polytope, we are specifically referring to the length of these black lines (as measured with a KL divergence). ", "page_idx": 4}, {"type": "text", "text": "This sort of mutual information is a way for measuring the degree of control that an agent exerts on an environment. This measure is well defined for any agent/policy; that agent need not be maximizing mutual information, and could instead be maximizing some arbitrary reward function. This point is important in our setting: this means that the assistive agent can estimate and maximize the empowerment of the human user without having to infer what reward function the human is trying to maximize. ", "page_idx": 4}, {"type": "text", "text": "Finally, we come back to our empowerment objective, which is a discounted sum of the mutual information terms that we have been analyzing above. This empowerment objective says that the human is more empowered when this set has a larger size \u2014 i.e., the human can visit a wider range of future state (distributions). The empowerment objective says that the assistive agent should act to try to maximize the size of this polytope. Importantly, this maximization problem is done sequentially: the assistive agent wants the size of this polytope to be large both at the current state and at future states; the human\u2019s actions should exert a high degree of influence over the future outcomes both now and in the future. Thus, our overall objective looks at a sum of these mutual informations. ", "page_idx": 4}, {"type": "text", "text": "Not only does this analysis provides a geometric picture for what empowerment is doing, it also lays the groundwork for formally relating empowerment to reward. ", "page_idx": 4}, {"type": "text", "text": "3.3 Relating Empowerment to Reward ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section we take aim at the question: when humans are well-modeled as optimizing a reward function, when does maximizing empowerment help humans maximize their rewards? Answering this question is important because for empowerment to be a safe and effective assistive objective, it should enable the human to better achieve their goals. We show that under certain assumptions, empowerment yields a provable lower bound on the average-case reward achieved by the human for suffiently long-horizon empowerment (i.e., $\\gamma\\rightarrow1$ ). ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "For constructing the formal bound, we suppose the human is Boltzmann-rational [53, 54] with respect to some reward function $R\\sim\\mathcal{R}$ , where $\\mathcal{R}$ is some distribution that could be interpreted as a prior over the human\u2019s objective, a set of skills the human may try and carry out, or a population of humans with different objectives that the agent could be interacting with. Our quantity of interest, the average-case reward achieved by the human with our empowerment objective, is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{I}_{\\pi_{\\mathbf{R}}}^{\\gamma}(\\pi_{\\mathbf{H}})=\\mathbb{E}_{R\\sim\\mathcal{R}}\\left[V_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(s_{0})\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $V_{R,\\gamma}^{\\pi_{\\bf H},\\pi_{\\bf R}}(s_{0})$ is the value function of the human policy $\\pi\\mathbf{H}$ under the reward function $R$ when interacting with $\\pi_{\\mathbf{R}}$ . Recalling Eq. (2), we will express the overall empowerment objective we are trying to relate to Eq. (5) as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\gamma}(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})=\\mathbb{E}\\!\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}I(\\mathfrak{s}^{+};\\mathfrak{a}_{t}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})\\right]\\!.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This notation is formalized in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "The two key assumptions used in our analysis are Assumption 3.1, which states that the human will optimize for behaviors that uniformly cover the state space, and Assumption 3.2, which simply states that with infinite time, the human will be able to reach any state in the state space. ", "page_idx": 5}, {"type": "text", "text": "Assumption 3.1 (Skill Coverage). The rewards $R\\sim\\mathcal{R}$ are uniformly distributed over the scaled $|{\\mathcal{S}}|$ -simplex $\\Delta^{\\left|S\\right|}$ such that: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(R+\\frac{1}{|S|}\\right)\\left(\\frac{1}{1-\\gamma}\\right)\\sim\\mathrm{Unif}\\big(\\Delta^{|S|}\\big)=\\mathrm{Dirichlet}(\\underbrace{1,1,\\dots,1}_{|S|\\;t i m e s}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Assumption 3.2 (Ergodicity). For some $\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{P}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(\\mathfrak{s}^{+}=s\\mid\\mathfrak{s}_{0})>0\\quad f o r\\,a l l\\,s\\in S,\\gamma\\in(0,1).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Our main theoretical result is Theorem 3.1, which shows that under these assumptions, maximizing empowerment yields a lower bound on the (squared) average-case reward achieved by the human for sufficiently large $\\gamma$ . In other words, for a sufficiently long empowerment horizon, the empowerment objective Eq. (2) is a meaningful proxy for reward maximization. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1. Under Assumption 3.1 and Assumption 3.2, for sufficiently large $\\gamma$ and any $\\beta>0$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\gamma}(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})^{1/2}\\leq(\\beta/e)\\,\\mathcal{J}_{\\pi_{\\mathbf{R}}}^{\\gamma}(\\pi_{\\mathbf{H}}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The proof is in Appendix B.1 To the best of our knowledge, this result provides the first formal link between empowerment maximization and reward maximization. This motivates us to develop a scalable algorithm for empowerment maximization, which we introduce in the following section. ", "page_idx": 5}, {"type": "text", "text": "4 Estimating and Maximizing Empowerment with Contrastive Representations ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Directly computing Eq. (2) would require access to the human policy, which we don\u2019t have. Therefore, we want a tractable estimation that still performs well in large environments which are more difficult to model due to the exponentially increasing set of possible future states. To better-estimate empowerment, we learn contrastive representations that encode information about which future states are likely to be reached from the current state. These contrastive representations learn to model mutual information between the current state, action, and future state, which we then use to compute the empowerment objective. ", "page_idx": 5}, {"type": "text", "text": "4.1 Estimating Empowerment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To estimate this empowerment objective, we need a way of learning the probability ratio inside the expectation. Prior methods such as Du et al. [6] and Salge et al. [42] rollout possible future states and compute a measure of their variance as a proxy for empowerment, however this doesn\u2019t scale when the environment becomes complex. Other methods learn a dynamics model, which also doesn\u2019t scale when dynamics become challenging to model [27]. Modeling these probabilities directly is challenging in settings with high-dimensional states, so we opt for an indirect approach. Specifically, we will learn representations that encode two probability ratios. Then, we will be able to compute the desired probability ratio by combining these other probability ratios. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Our method learns three representations: ", "page_idx": 6}, {"type": "text", "text": "1. $\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{H}})$ \u2014 This representation can be understood as a sort of latent-space model, predicting the future representation given the current state $s$ and the human\u2019s current action $\\dot{a}^{\\mathbf{H}}$ as well as the robot\u2019s current action $a^{\\mathbf{R}}$ .   \n2. $\\phi^{\\prime}(s,a^{\\mathbf{R}})$ \u2014 This representation can be understood as an uncontrolled model, predicting the representation of a future state without reference to the current human action $a^{\\mathbf{H}}$ . This representation is analogous to a value function.   \n3. $\\bar{\\psi(}s^{+})$ \u2014 This is a representation of a future state. ", "page_idx": 6}, {"type": "text", "text": "We will learn these three representations with two contrastive losses, one that aligns $\\phi(s,a^{R},a^{H})\\leftrightarrow$ $\\psi(s^{+})$ and one that aligns $\\dot{\\phi}^{\\prime}(s,a^{\\mathbf{R}})\\leftrightarrow\\psi(s^{+})$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi,\\phi^{\\prime},\\psi}\\mathbb{E}_{\\{(s_{i},a_{i},s_{i}^{\\prime})\\sim\\mathbb{P}(\\mathfrak{s}_{t},\\mathfrak{a}_{t}^{\\mathrm{H}},\\mathfrak{s}_{t+k})\\}_{i=1}^{N}}\\big[\\mathcal{L}_{c}\\big(\\big\\{\\phi(s_{i},a_{i})\\big\\},\\{\\psi(s_{j}^{\\prime})\\}\\big)+\\mathcal{L}_{c}(\\big\\{\\phi^{\\prime}(s_{i})\\big\\},\\{\\psi(s_{j}^{\\prime})\\}\\big)\\big],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the contrastive loss $\\scriptstyle{\\mathcal{L}}_{\\mathsf{c}}$ is the symmetrized infoNCE objective [35]: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathsf{c}}(\\{x_{i}\\},\\{y_{j}\\})\\triangleq\\sum_{i=1}^{N}\\left[\\log\\left(\\frac{e^{x_{i}^{T}y_{i}}}{\\sum_{j=1}^{N}e^{x_{i}^{T}y_{j}}}\\right)+\\log\\left(\\frac{e^{x_{i}^{T}y_{i}}}{\\sum_{j=1}^{N}e^{x_{j}^{T}y_{i}}}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We have colored the index $j$ for clarity. At convergence, these representations encode two probability ratios [24], which we will ultimately be able to use to estimate empowerment (Eq. 2): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{H}})^{T}\\psi(g)=\\log\\left[{\\frac{\\mathrm{P}(\\mathfrak{s}_{t+K}=g\\mid\\mathfrak{s}_{t}=\\mathfrak{s},\\mathfrak{a}_{t}^{\\mathbf{H}}=a^{\\mathbf{H}},\\mathfrak{a}_{t}^{\\mathbf{R}}=a^{\\mathbf{R}})}{C_{1}\\,\\mathrm{P}(\\mathfrak{s}_{t+K}=g)}}\\right]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\phi^{\\prime}(s,a^{\\mathbf{R}})^{T}\\psi(g)=\\log\\left[\\frac{\\mathrm{P}(\\mathfrak{s}_{t+K}=s_{t+k}\\mid\\mathfrak{s}_{t}=s_{t},\\mathfrak{a}_{t}^{\\mathbf{R}}=a^{\\mathbf{R}})}{C_{2}\\,\\mathrm{P}(\\mathfrak{s}_{t+K}=g)}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that our definition of empowerment (Eq. 2) is defined in terms of similar probability ratios. The constants $C_{1}$ and $C_{2}$ will mean that our estimate of empowerment may be off by an additive constant, but that constant will not affect the solution to the empowerment maximization problem. ", "page_idx": 6}, {"type": "text", "text": "4.2 Estimating Empowerment with the Learned Representations ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To estimate empowerment, we will look at the difference between these two inner products: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(s_{t+K},a^{\\mathbf{R}},a^{\\mathbf{H}})^{T}\\psi(g)-\\phi(s_{t+K},a^{\\mathbf{R}})^{T}\\psi(g)}\\\\ &{=\\log\\mathrm{P}(s_{t+K}\\mid s,a^{\\mathbf{H}})-\\log C_{1}-\\underbrace{\\log\\mathrm{P}(s_{t+K})}_{=\\log{\\mathrm{P}(s_{t+K})}}-\\log\\mathrm{P}(s_{t+K}\\mid s)+\\log C_{2}+\\underbrace{\\log\\mathrm{P}(s_{t+K})}_{=\\log{\\mathrm{P}(s_{t+K})}}}\\\\ &{=\\log\\frac{\\mathrm{P}(s_{t+K}\\mid s,a^{\\mathbf{H}})}{\\mathrm{P}(s_{t+K}\\mid s)}+\\log\\frac{C_{2}}{C_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that the expected value of the first term is the conditional mutual information $I(s_{t+K};a^{\\mathbf{H}}\\mid s)$ . Our empowerment objective corresponds to averaging this mutual information across all the visited states. In other words, our objective corresponds to an RL problem, where empowerment corresponds to the expected discounted sum of these log ratios: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal E(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})=\\mathbb{E}_{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}I(s^{+};a_{t}^{\\mathbf{H}}\\mid s_{t})\\right]}\\ ~}\\\\ {{\\displaystyle~~~~~~~~~~~~~\\approx\\mathbb{E}_{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}(\\phi(s_{t},a^{\\mathbf{R}},a^{\\mathbf{H}})-\\phi(s_{t},a^{\\mathbf{R}}))^{T}\\psi(g)-\\log\\frac{C_{2}}{C_{1}}\\right]}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The approximation above comes from function approximation in learning the Bayes optimal representations. Again, note that the constants $C_{1}$ and $C_{2}$ do not change the optimization problem. Thus, to maximize empowerment we will apply RL to the assistive agent $\\pi_{\\mathbf{R}}(a\\mid s)$ using a reward function ", "page_idx": 6}, {"type": "equation", "text": "$$\nr(s,a^{\\boldsymbol{\\mathbf{R}}})=(\\phi(s_{t},a^{\\boldsymbol{\\mathbf{R}}},a^{\\boldsymbol{\\mathbf{H}}})-\\phi(s_{t},a^{\\boldsymbol{\\mathbf{R}}}))^{T}\\psi(g).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Algorithm 1: Empowerment via Successor Representations (ESR) ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input: Human policy $\\pi_{\\mathbf{H}}(a\\mid s)$   \nRandomly initialize assistive agent policy $\\pi_{\\mathbf{R}}(a\\mid s)$ , and representations $\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{H}})$ , $\\psi(s,a^{T})$ , and $\\psi(g)$ .   \nInitialize replay buffer $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ .   \nwhile not converged do Collect a trajectory of experience with human policy and assistive agent policy, store in replay buffer $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . Update representations $\\dot{\\phi(s,a^{\\boldsymbol{\\mathbf{R}}},a^{\\boldsymbol{\\mathbf{H}}})}$ , $\\psi(s,a^{T})$ , and $\\psi(g)$ with the contrastive losses in Eq. (10). Update $\\pi_{\\mathbf{R}}(a\\mid s)$ with RL using reward function $r(s,a^{\\boldsymbol{\\mathbf{R}}},a^{\\boldsymbol{\\mathbf{H}}})=(\\phi(s,a^{\\boldsymbol{\\mathbf{R}}},a^{\\boldsymbol{\\mathbf{H}}})-\\phi^{\\prime}(s,a^{\\boldsymbol{\\mathbf{R}}}))^{T}\\psi(g)$ .   \nReturn: Assistive policy $\\pi_{\\mathbf{R}}(a\\mid s)$ . ", "page_idx": 7}, {"type": "text", "text": "4.3 Algorithm Summary ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We propose an actor-critic method for learning the assistive agent. Our method will alternate between updating these contrastive representations and using them to estimate a reward function (Eq. (13)) that is optimized via RL. We summarize the algorithm in Algorithm 1. In practice, we use SAC [55] as our RL algorithm. In our experiments, we will also study the setting where the human user updates their policy alongside the assistive agent. ", "page_idx": 7}, {"type": "image", "img_path": "WCnJmb7cv1/tmp/469bb118828b237a6074d0ef3a21840bd9a0677943f58b796b1acf3a21a4efe4.jpg", "img_caption": ["Figure 3: We apply our method to the benchmark proposed in prior work [6], visualized in Fig. 4a. The four subplots show variant tasks of increasing complexity (more blocks), ( $\\pm1$ SE). We compare against AvE [6], the Goal Inference baseline from [6] which assumes access to a world model, and Reward Inference [56] where we recover the reward from a learned $\\mathbf{q}$ -value. These prior approaches fail on all except the easiest task, highlighting the importance of scalability. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "WCnJmb7cv1/tmp/5619b062f000af8a09497a96eaf77b26a9f65cf0baf3e8c57a20eed882e5005e.jpg", "img_caption": ["Figure 4: (a) The modified environment from Du et al. [6] scaled to $N=7$ blocks, and $(b,c)$ the two layouts of the Overcooked environment [10]. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "WCnJmb7cv1/tmp/dae058cb9321fd8ab53685e2e0eb11f57b72503875d42be1e22e0a29ec6e5610.jpg", "img_caption": ["Figure 5: In Coordination Ring, our ESR agent learns to wait for the human to add an onion to the pot, and then adds one itself. There is another pot at the top which is nearly full, but the empowerment agent takes actions to maximize the impact of the human\u2019s actions, and so follows the lead of the human by filling the empty pot. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We seek to answer two questions with our experiments. First, does our approach enable assistance in standard cooperation benchmarks? Second, does our approach scale to harder benchmarks where prior methods fail? ", "page_idx": 8}, {"type": "text", "text": "Our experiments will use two benchmarks designed by prior work to study assistance: the obstacle gridworld [6] and Overcooked [10]. Our main baseline is AvE [6], a prior empowerment-based method. Our conjecture is that both methods will perform well on the lower-dimensional gridworld task, and that our method will scale more gracefully to the higher dimensional Overcooked environment. We will also compare against a na\u00efve baseline where the assistive agent acts randomly. ", "page_idx": 8}, {"type": "text", "text": "5.1 Do contrastive successor representations effectively estimate empowerment? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We test our approach in the assistance benchmark suggested in Du et al. [6]. The human (orange) is tasked with reaching a goal state (green) while avoiding the obstacles (purple). The AI assistant can move blocks one step at a time in any direction [6]. While the original benchmark used $N=2$ obstacles, we will additionally evaluate on harder versions of this task with $N=5,7,10$ obstacles. We show results in Fig. 3. On the easiest task, both our method and AvE achieve similar asymptotic reward, though our method learns more slowly than AvE. However, on the tasks with moderate and high degrees of complexity, our approach (ESR) achieves significantly higher rewards than AvE, which performs worse than a random controller. These experiments support our claim that contrastive successor representations provide an effective means for estimating empowerment, and hint that ESR might be well suited for solving higher dimensional tasks. ", "page_idx": 8}, {"type": "text", "text": "Our second set of experiments look at scaling ESR to the image-based Overcooked environment. Since contrastive learning is often applied to image domains, we conjectured that ESR would scale gracefully to this setting. We will evaluate our approach in assisting a human policy trained with behavioral cloning taken from Laidlaw and Dragan [57]. The human prepares dishes by picking up ingredients and cooking them on a stove, while the AI assistant moves ingredients and dishes around the kitchen. We focus on two environments within this setting: a cramped room where the human must pass ingredients and dishes through a narrow corridor, and a coordination ring where the human must pass ingredients and dishes around a ring-shaped kitchen (Figs. 4b and $4\\mathrm{c}$ ). As before, we compare with AvE as well as a na\u00efve random controller. We report results in Table 1. On both tasks, we observe that our approach achieves higher rewards than AvE baseline, which performs no better than a random controller. In Fig. 5, we show an example of one of the collaborative behaviors learned by ESR. Taken together with the results in the previous setting, these results highlight the scalability of ESR to higher dimensional problems. ", "page_idx": 9}, {"type": "table", "img_path": "WCnJmb7cv1/tmp/03beed253cb9cdd2f626a996a8d9779e6b1b041933146f1b2e39c61ab2e0f71b.jpg", "table_caption": ["Table 1: Overcooked Results "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "One of the most important problems in AI today is equipping AI agents with the capacity to assist humans achieve their goals. While much of the prior work in this area requires inferring the human\u2019s intention, our work builds on prior work in studying how an assistive agent can empower a human user without inferring their intention. Relative to prior methods, we demonstrate how empowerment can be readily estimated using contrastive learning, paving the way for deploying these techniques on high-dimensional problems. ", "page_idx": 9}, {"type": "text", "text": "Limitations. One of the main limitations of our approach is the assumption that the assistive agent has access to the human\u2019s actions, which could be challenging to observe in practice. Automatically inferring the human\u2019s actions remains an important problem for future work. A second limitation is that the method is currently an on-policy method, in the sense that the assistive agent has to learn by trial and error. A third limitation is that the ESR formulation assumes that both agents share the same state space. In many cases the empowerment objective will still lead to desirable behavior, however, care must be taken in cases where the agent can restrict the information in its own observations, which could lead to reward hacking. Finally, our experiments do not test our method against real humans, whose policies may differ from the simulated policies. In the future, we plan to investigate techniques from off-policy evaluation and cooperative game theory to enable faster learning of assistive agents with fewer trials. We also plan to test the ESR objective in environments with partial observability over the human\u2019s state. ", "page_idx": 9}, {"type": "text", "text": "Safety risks. Perhaps the main risk involved with maximizing empowerment is that it may be at odds with a human\u2019s agents goal, especially in contexts where the pursuit of that goal limits the human\u2019s capacity to pursue other goals. For example, a family choosing to have a kid has many fewer options over where they can travel for vacation, yet we do not want assistive agents to stymie families from having children. ", "page_idx": 9}, {"type": "text", "text": "One key consideration is whom should be empowered. The present paper assumes there is a single human agent. Equivalently, this can be seen as maximizing the empowerment of all exogenous agents. However, it is easy to adapt the proposed method to maximize the empowerment of a single target individual. Given historical inequities in the distribution of power, practitioners must take care when considering whose empowerment to maximize. Similarly, while we focused on maximizing empowerment, it is trivial to change the sign so that an \u201cassistive\u201d agent minimizes empowerment. One could imagine using such a tool in policies to handicap one\u2019s political opponents. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. We would like to thank Micah Carroll and Cameron Allen for their helpful feedback, as well as Niklas Lauffer for suggesting JaxMARL. We especially thank the fantastic NeurIPS reviewers for their constructive comments and suggestions. This research was partly supported by ARL DCIST CRA W911NF-17-2-0181 and ONR N00014-22-1-2773, as well as NSF HCC 2310757, the Jump Cocosys Center, Princeton Research Computing, and the DoD through the NDSEG Fellowship Program. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open Problems and Fundamental Limitations of Reinforcement Learning From Human Feedback. 2023. arXiv:2307.15217. [2] Dylan Hadfield-Menell, Stuart J. Russell, Pieter Abbeel, and Anca Dragan. Cooperative Inverse Reinforcement Learning. Advances in Neural Information Processing Systems, 29, 2016. [3] Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J. Russell, and Anca Dragan. Inverse Reward Design. Advances in Neural Information Processing Systems, 30, 2017.   \n[4] Micah Carroll, Dylan Hadfield-Menell, Stuart Russell, and Anca Dragan. Estimating and Penalizing Preference Shift in Recommender Systems. In ACM Conference on Recommender Systems, RecSys \u201921, pp. 661\u2013667. 2021.   \n[5] Alexander Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. Optimal Policies Tend to Seek Power. In Neural Information Processing Systems. 2023.   \n[6] Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter Abbeel, and Anca Dragan. Ave: Assistance via Empowerment. In Advances in Neural Information Processing Systems, volume 33, pp. 4560\u20134571. 2020.   \n[7] Stuart Russell. Learning Agents for Uncertain Environments (Extended Abstract). In Annual Conference on Computational Learning Theory, pp. 101\u2013103. 1998.   \n[8] Saurabh Arora and Prashant Doshi. A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress. Artificial Intelligence, 297:103500, 2021. [9] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F\u00fcrnkranz. A Survey of Preference-based Reinforcement Learning Methods. Journal of Machine Learning Research, 18(136):1\u201346, 2017.   \n[10] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia, Pieter Abbeel, and Anca Dragan. on the Utility of Learning About Humans for Human-ai Coordination. In Conference on Neural Information Processing Systems. 2019.   \n[11] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. the Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models. In International Conference on Learning Representations, arXiv:2201.03544. 2022.   \n[12] Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D. Dragan, and Daniel S. Brown. Causal Confusion and Reward Misidentification in Preference-based Reward Learning. In International Conference on Learning Representations. 2023.   \n[13] Cassidy Laidlaw, Shivam Singhal, and Anca Dragan. Preventing Reward Hacking With Occupancy Measure Regularization. 2024. arXiv:2403.03185.   \n[14] Christoph Salge, Cornelius Glackin, and Daniel Polani. Empowerment \u2013 an Introduction. 2013. arXiv:1310.1863.   \n[15] Ildefons Magrans de Abril and Ryota Kanai. A Unified Strategy for Implementing Curiosity and Empowerment Driven Reinforcement Learning. 2018. arXiv:1806.06505.   \n[16] Sean Chen, Jensen Gao, Siddharth Reddy, Glen Berseth, Anca D. Dragan, and Sergey Levine. ASHA: Assistive Teleoperation via Human-in-the-loop Reinforcement Learning. In International Conference on Robotics and Automation. 2022.   \n[17] Siddharth Reddy, Sergey Levine, and Anca Dragan. First Contact: Unsupervised Humanmachine Co-adaptation via Mutual Information Maximization. Advances in Neural Information Processing Systems, 35:31542\u201331556, 2022.   \n[18] Andrew G. Barto. Intrinsic Motivation and Reinforcement Learning. In Gianluca Baldassarre and Marco Mirolli, editors, Intrinsically Motivated Learning in Natural and Artificial Systems, pp. 17\u201347. Springer, 2013.   \n[19] Arthur Aubret, Laetitia Matignon, and Salima Hassas. A Survey on Intrinsic Motivation in Reinforcement Learning. In Entropy. 2023.   \n[20] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity Is All You Need: Learning Skills Without a Reward Function. In International Conference on Learning Representations (ICLR). 2019.   \n[21] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by Random Network Distillation. In International Conference on Machine Learning. 2023.   \n[22] Karl Friston. the Free-energy Principle: A Unified Brain Theory? Nature Reviews Neuroscience, 11(2):127\u2013138, 2010.   \n[23] Glen Berseth, Daniel Geng, Coline Devin, Nicholas Rhinehart, Chelsea Finn, Dinesh Jayaraman, and Sergey Levine. Smirl: Surprise Minimizing Reinforcement Learning in Unstable Environments. In International Conference on Learning Representations. 2019.   \n[24] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. on Variational Bounds of Mutual Information. In International Conference on Machine Learning. 2019.   \n[25] Jongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, and Shixiang Shane Gu. Variational Empowerment as Representation Learning for Goal-conditioned Reinforcement Learning. In International Conference on Machine Learning, pp. 1953\u20131963. 2021.   \n[26] Shakir Mohamed and Danilo Jimenez Rezende. Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 28. 2015.   \n[27] Tobias Jung, Daniel Polani, and Peter Stone. Empowerment for Continuous Agent\u2014 environment Systems. Adaptive Behavior, 19(1):16\u201339, 2011.   \n[28] Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, and Pieter Abbeel. CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery. 2022. arXiv:2202.00161.   \n[29] Seohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, and Gunhee Kim. Lipschitzconstrained Unsupervised Skill Discovery. In International Conference on Learning Representations. 2021.   \n[30] Susanne Still and Doina Precup. an Information-theoretic Approach to Curiosity-driven Reinforcement Learning. Theory in Biosciences, 131:139\u2013148, 2012.   \n[31] Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. InformationDirected Exploration for Deep Reinforcement Learning. In International Conference on Learning Representations. 2019.   \n[32] Erdem Biyik, Dylan P. Losey, Malayandi Palan, Nicholas C. Landolfi, Gleb Shevchuk, and Dorsa Sadigh. Learning Reward Functions From Diverse Sources of Human Feedback: Optimally Integrating Demonstrations and Preferences. In Int. J. Robotics Res. 2022.   \n[33] Vivek Myers, Erdem Biyik, Nima Anari, and Dorsa Sadigh. Learning Multimodal Rewards From Rankings. In Conference on Robot Learning, pp. 342\u2013352. 2021.   \n[34] Neil Houlsby, Ferenc Husz\u00e1r, Zoubin Ghahramani, and M\u00e1t\u00e9 Lengyel. Bayesian Active Learning for Classification and Preference Learning. 2011. arXiv:1112.5745.   \n[35] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning With Contrastive Predictive Coding. 2019. arXiv:1807.03748.   \n[36] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In International Conference on Machine Learning. 2020.   \n[37] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised Feature Learning via Non-parametric Instance Discrimination. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 3733\u20133742. 2018.   \n[38] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive Unsupervised Representations for Reinforcement Learning. In International Conference on Machine Learning, pp. 5639\u20135650. 2020.   \n[39] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R. Salakhutdinov. Contrastive Learning as Goal-conditioned Reinforcement Learning. In Advances in Neural Information Processing Systems, volume 35, pp. 35603\u201335620. 2022.   \n[40] Peter Dayan. Improving Generalization for Temporal Difference Learning: the Successor Representation. Neural Computation, 5(4):613\u2013624, 1993.   \n[41] Ida Momennejad, Evan M Russek, Jin H Cheong, Matthew M Botvinick, Nathaniel Douglass Daw, and Samuel J Gershman. the Successor Representation in Human Reinforcement Learning. Nature Human Behaviour, 1(9):680\u2013692, 2017.   \n[42] Christoph Salge, Cornelius Glackin, and Daniel Polani. Empowerment\u2013an Introduction. Guided Self-organization: Inception, pp. 67\u2013114, 2014.   \n[43] Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. Empowerment: A Universal Agent-centric Measure of Control. In 2005 IEEE Congress on Evolutionary Computation, volume 1, pp. 128\u2013135. 2005.   \n[44] Siddharth Reddy, Anca D Dragan, and Sergey Levine. Shared Autonomy via Deep Reinforcement Learning. 2018. arXiv:1802.01744.   \n[45] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum Margin Planning. In 23rd International Conference on Machine Learning - ICML \u201906, pp. 729\u2013736. 2006.   \n[46] Pieter Abbeel and Andrew Y. Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In Twenty-first International Conference on Machine Learning - ICML \u201904, p. 1. 2004.   \n[47] Thomas M Cover. Elements of Information Theory. John Wiley & Sons, 1999.   \n[48] Nihat Ay, J\u00fcrgen Jost, H\u00f4ng V\u00e2n L\u00ea, and Lorenz Schwachh\u00f6fer. Information Geometry, volume 64 of Ergebnisse Der Mathematik Und Ihrer Grenzgebiete 34. Springer International Publishing, 2017.   \n[49] Frank Nielsen. an Elementary Introduction to Information Geometry. Entropy, 22(10):1100, 2020.   \n[50] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. the Information Geometry of Unsupervised Reinforcement Learning. In International Conference on Learning Representations. 2022.   \n[51] Robert G. Gallager. Source Coding With Side Information and Universal Coding. 1979.   \n[52] Boris Yakovlevich Ryabko. Coding of a Source With Unknown but Ordered Probabilities. Problems of Information Transmission, 15(2):134\u2013138, 1979.   \n[53] R. Duncan Luce. Individual Choice Behavior. Individual Choice Behavior. John Wiley, 1959.   \n[54] Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum Entropy Inverse Reinforcement Learning. In AAAI Conference on Artificial Intelligence. 2008.   \n[55] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft Actor-critic Algorithms and Applications. 2018. arXiv:1812.05905.   \n[56] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn: Inverse Soft-Q Learning for Imitation. In Advances in Neural Information Processing Systems, volume 34, pp. 4028\u20134039. 2021.   \n[57] Cassidy Laidlaw and Anca Dragan. the Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models. In International Conference on Learning Representations. 2022.   \n[58] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. 2017. arXiv:1412.6980.   \n[59] Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, Akbir Khan, et al. Jaxmarl: Multi-agent RL Environments in JAX. In International Conference on Autonomous Agents and Multiagent Systems. 2024.   \n[60] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier Nonlinearities Improve Neural Network Acoustic Models. In International Conference on Machine Learning, volume 30, p. 3. 2013.   \n[61] Tongzhou Wang and Phillip Isola. Understanding Contrastive Representation Learning Through Alignment and Uniformity on the Hypersphere. In International Conference on Machine Learning. 2020.   \n[62] Benjamin Eysenbach, Vivek Myers, Ruslan Salakhutdinov, and Sergey Levine. Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference. In Advances in Neural Information Processing Systems. 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Experimental Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We ran all our experiments on NVIDIA RTX A6000 GPUs with 48GB of memory within an internal cluster. Each evaluation seed took around 5-10 hours to complete. Our losses (Eqs. 10 and 13) were computed and optimized in JAX with Adam [58]. We used a hardware-accelerated version of the Overcooked environment from the JaxMARL package [59]. The experimental results described in Section 5 were obtained by averaging over 5 seeds for the Overcooked coordination ring layout, 15 for the cramped room layout, and 20 for the obstacle gridworld environment. Specific hyperparameter values can be found in our code, which is available at https://github.com/ vivekmyers/empowerment_successor_representations. ", "page_idx": 13}, {"type": "text", "text": "A.1 Network Architecture ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In the obstacle grid environment, we used a network with 2 convolutional and 2 fully connected layers and SiLU activations. In Overcooked, we adapted the policy architecture from past work [4], using 3 convolutional layers followed by 4 MLP layers with Leaky ReLU activations [60]. We concatenate in $a^{\\mathbf{R}}$ and $a^{\\mathbf{H}}$ to the state as one-hot encoded channels, i.e. if the action is 5, 6 additional channels will be concatenated to the state with all set to 0s except the 5th channel which is set to 1s. ", "page_idx": 13}, {"type": "text", "text": "B Theoretical Analysis of Empowerment ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To connect our empowerment objective to reward, we will extend the notation in Section 3.1 to include a distribution over possible tasks the human might be trying to solve, $\\mathcal{R}$ , such that each $R\\sim\\mathcal{R}$ defines a distinct reward function $R:S\\rightarrow\\mathbb{R}$ . We assume $\\pi_{\\mathbf{R}}$ tries to maximize the $\\gamma$ -discounted empowerment\u201d of the human, defined as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\gamma}(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})=\\mathbb{E}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}I(\\mathfrak{s}_{+}^{\\gamma};\\mathfrak{a}_{t}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})\\Big]\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "for ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathfrak{H}_{+}^{\\gamma}\\triangleq\\{\\mathfrak{s}_{k}\\quad\\mathrm{for}\\quad k\\sim\\operatorname{Geom}(1-\\gamma)\\}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We additionally define $\\bar{\\mathfrak{s}}_{t}$ to be the full history of states up to time $t$ and $\\bar{\\mathfrak{a}}_{t}^{\\mathbf{H}}$ to be the full history of human actions up to time $t$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{\\mathfrak{s}}_{t}=\\{\\mathfrak{s}_{i}\\}_{i=0}^{t},\\ }\\\\ {\\bar{\\mathfrak{a}}_{t}^{\\mathbf{H}}=\\{\\mathfrak{a}_{i}^{\\mathbf{H}}\\}_{i=0}^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then, $\\widetilde{\\mathfrak{s}}_{t}$ is the full history of states and past human actions up to time $t$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\widetilde{\\mathfrak{s}}_{t}=\\bar{\\mathfrak{s}}_{t}\\cup\\bar{\\mathfrak{a}}_{t-1}^{\\mathbf{H}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that the definition of empowerment in Eq. (6) differs slightly from the original construction Eq. (2) \u2014 we condition on the full history of human actions, not just the most recent one. This distinction becomes irrelevant in practice if our MDP maintains history in the state, in which case we can equivalently use $\\mathfrak{s}_{t}$ in place of $\\widetilde{\\mathfrak{s}}_{t}$ . ", "page_idx": 13}, {"type": "text", "text": "Meanwhile, for any fixed $\\pi_{\\mathbf{R}}$ and $\\beta>0$ , the human is Boltzmann-rational with respect to the robot\u2019s policy: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{\\mathbf{H}}(a_{t}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})\\propto\\exp\\bigl(\\beta Q_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(s_{t},a_{t}^{\\mathbf{H}})\\bigr)}\\\\ {\\mathrm{where}\\,}&{Q_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(s_{t},a_{t}^{\\mathbf{H}})=\\mathbb{E}\\Bigl[\\sum_{k=0}^{\\infty}\\gamma^{k}R(s_{t+k})\\textbf{\\Bigl|}s_{t},a_{t}^{\\mathbf{H}}\\Bigr].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Equivalently, we can define the human\u2019s (soft) Q-function and value as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(s_{t},a_{t}^{\\mathbf{H}})=R(s_{t})+\\gamma\\mathbb{E}\\!\\left[V_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(s_{t+1})\\;\\middle|\\;s_{t},a_{t}^{\\mathbf{H}}\\right]}\\\\ &{\\mathrm{for}\\;V_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(s_{t})=\\mathbb{E}\\!\\left[R(s_{t})+\\gamma R(s_{t+1})+\\gamma^{2}R(s_{t+2})+...\\;\\middle|\\;s_{t},a_{t}^{\\mathbf{H}}\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The overall human objective is to maximize the expected soft value: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{J}_{\\pi_{\\mathbf{R}}}^{\\gamma}(\\pi_{\\mathbf{H}})=\\mathbb{E}_{R\\sim\\mathcal{R}_{0}}\\left[V_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(s_{0})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that this definition of $\\pi\\mathbf{H}$ depends on $R$ and $\\pi_{\\mathbf{R}}$ and is bounded $0\\leq\\mathcal{J}_{\\pi_{\\mathbf{R}}}^{\\gamma}(\\pi_{\\mathbf{H}})\\leq1$ . As in the CIRL setting [2], we assume robot is unable to access the true human reward $\\bar{R}:\\mathcal{S}\\rightarrow\\mathbb{R}$ . One way to think of the robot\u2019s task is as finding a Nash equilibrium between the objectives Eq. (6) and the human best response in Eq. (17). ", "page_idx": 14}, {"type": "text", "text": "For convenience, we will also define a multistep version of Q\u03c0R,H\u03b3,\u03c0 ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}\\!\\left(s_{t},a_{t}^{\\mathbf{H}},\\ldots,a_{t+K}^{\\mathbf{H}}\\right)=\\mathbb{E}\\!\\left[\\sum_{k=0}^{\\infty}\\gamma^{k}R(\\mathfrak{s}_{t+k})\\;\\middle|\\;\\mathfrak{s}_{t},a_{t}^{\\mathbf{H}},\\ldots,a_{t+K}^{\\mathbf{H}}\\right]\\!.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B.1 Connecting Empowerment to Reward ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our approach will be to first relate the empowerment (influence of ${\\mathfrak{a}}_{t}^{\\mathbf{H}}$ on $\\mathfrak{s}_{+}^{\\gamma}$ ) to the mutual information between ${\\mathfrak{a}}_{t}^{\\mathbf{H}}$ and the reward $R$ . ", "page_idx": 14}, {"type": "text", "text": "Then, we will connect this quantity to a notion of \u201cadvantage\u201d for the human (Eq. 27), which in turn can be related to the expected reward under the human\u2019s policy. In its simplest form, this argument will require an assumption over the reward distribution: ", "page_idx": 14}, {"type": "text", "text": "Assumption 3.1 (Skill Coverage). The rewards $R\\sim\\mathcal{R}$ are uniformly distributed over the scaled $|{\\mathcal{S}}|$ -simplex $\\Delta^{\\left|S\\right|}$ such that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(R+\\frac{1}{|S|}\\right)\\left(\\frac{1}{1-\\gamma}\\right)\\sim\\mathrm{Unif}\\big(\\Delta^{|S|}\\big)=\\mathrm{Dirichlet}(\\underbrace{1,1,\\dots,1}_{|S|\\;t i m e s}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, Assumption 3.1 says our prior over the human\u2019s reward function is uniform with zero mean. This is not the only prior for which this argument works, but for general $\\mathcal{R}$ we will need a correction term to incentivize states that are more likely across the distribution of $\\mathcal{R}$ . Another way to view Assumption 3.1 is that the human is trying to execute diverse \u201cskills\u201d $z\\sim\\mathrm{Unif}(\\Delta^{|S|})$ . ", "page_idx": 14}, {"type": "text", "text": "We also assume ergodicity (Assumption 3.2). In the special case of an MDP that resets to some distribution with full support over $\\boldsymbol{S}$ , this assumption is automatically satisfied. ", "page_idx": 14}, {"type": "text", "text": "Assumption 3.2 (Ergodicity). For some $\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{P}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(\\mathfrak{s}_{+}^{\\gamma}=s\\ |\\ s_{0})>0\\ \\ \\,f o r\\,a l l\\,s\\in S,\\gamma\\in(0,1).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Our main result connects empowerment directly to a (lower bound on) the human\u2019s expected reward. ", "page_idx": 14}, {"type": "text", "text": "Theorem 3.1. Under Assumption 3.1 and Assumption 3.2, for sufficiently large $\\gamma$ and any $\\beta>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\gamma}(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})^{1/2}\\leq(\\beta/e)\\,\\mathcal{J}_{\\pi_{\\mathbf{R}}}^{\\gamma}(\\pi_{\\mathbf{H}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Theorem 3.1 says that for a long enough horizon (i.e., $\\gamma$ close to 1), the robot\u2019s empowerment objective will lower bound the (squared, MaxEnt) human objective. ", "page_idx": 14}, {"type": "text", "text": "We make use of the following lemmas in the proof. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1. For $t\\sim\\mathrm{Geom}(1-\\gamma)$ and any $K\\ge0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\to1}I(\\mathfrak{s}_{+}^{\\gamma};\\mathfrak{a}_{t}^{\\mathbf{H}},\\cdot\\cdot\\cdot,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})\\le I(R;\\mathfrak{a}_{t}^{\\mathbf{H}},\\cdot\\cdot,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof. For sufficiently large $\\gamma,\\mathfrak{s}_{+}^{\\gamma}$ will approach the stationary distribution of $\\mathbf{\\mathrm{P}^{\\pi}}\\mathbf{H}^{,\\pi_{\\mathbf{R}}}$ for a fixed $\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}$ , irrespective of $\\widetilde{\\mathfrak{s}}_{t}$ and $\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}}$ from Assumption 3.2. So, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{\\gamma\\to1}\\operatorname*{inf}_{\\substack{I\\left(\\mathfrak{s}_{+}^{\\gamma};\\mathfrak{a}_{t}^{\\mathbf{H}},\\,\\cdot\\,\\cdot\\,,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\;\\right|\\,\\widetilde{\\mathfrak{s}}_{t}\\right)\\leq I\\left(\\operatorname*{lim}_{\\gamma\\to\\infty}\\mathfrak{s}_{+}^{\\gamma};\\mathfrak{a}_{t}^{\\mathbf{H}},\\,\\cdot\\,\\cdot\\,,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\;\\right|\\,\\widetilde{\\mathfrak{s}}_{t}\\right)}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since each $R,\\pi_{\\mathbf{R}},\\gamma$ defines a human policy $\\pi\\mathbf{H}$ via Eq. (17), we can express the dependencies as the following Markov chain: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\mathbf{a}}_{t}\\,\\longrightarrow\\,R\\,\\longrightarrow\\,\\operatorname*{lim}_{\\gamma\\to1}{\\mathfrak{s}}_{+}^{\\gamma}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying the data processing inequality [47], we get ", "page_idx": 14}, {"type": "equation", "text": "$$\nI\\biggl(\\operatorname*{lim}_{\\gamma\\to\\infty}\\mathfrak{s}_{+}^{\\gamma}\\,;\\,\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\,\\bigg|\\,\\widetilde{\\mathfrak{s}}_{t}\\biggr)\\leq I\\bigl(R;\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t}\\bigr),\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "from which Eq. (21) follows. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2. Suppose we have $k$ logits, denoted by the map $\\alpha:\\{1\\ldots k\\}\\rightarrow[0,1].$ . For any $\\beta>0$ , we can construct the (softmax) distribution ", "page_idx": 15}, {"type": "equation", "text": "$$\np_{\\beta}(i)\\propto\\exp\\bigl(\\beta\\alpha(i)\\bigr).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{H}(p_{\\beta})\\geq\\log k-\\bigg(\\frac{\\beta}{e}\\bigg)^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. We lower bound the \u201cworst-case\u201d of the RHS, $\\alpha=(1,0,\\ldots,0)$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{H}(p_{\\beta})=\\frac{\\displaystyle(1-n)\\log\\bigl(\\frac{1}{k+e^{\\beta}-1}\\bigr)}{\\displaystyle k+e^{\\beta}-1}-\\frac{e^{\\beta}\\log\\bigl(\\frac{e^{\\beta}}{k+e^{\\beta}-1}\\bigr)}{\\displaystyle k+e^{\\beta}-1}}\\\\ &{\\qquad=\\frac{\\displaystyle(k+e^{\\beta}-1)\\log(k+e^{\\beta}-1)-e^{\\beta}\\log(e^{\\beta})}{\\displaystyle k+e^{\\beta}-1}}\\\\ &{\\qquad=\\log(k+e^{\\beta}-1)-\\frac{e^{\\beta}\\log(e^{\\beta})}{\\displaystyle k+e^{\\beta}-1}}\\\\ &{\\qquad\\geq\\log k-(\\beta/e)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Lemma B.3. For any $t$ and $K\\geq0$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(R;\\mathfrak{a}_{t}^{\\mathbf{H}},\\mathfrak{c}_{\\mathfrak{c}},\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})\\le\\operatorname*{lim}_{\\gamma\\to1}\\Bigl(\\frac{\\beta}{e}\\mathbb{E}\\bigl[Q_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}\\bigl(s_{t},\\mathfrak{a}_{t}^{\\mathbf{H}},\\mathfrak{c}_{\\mathfrak{c}},\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\bigr)\\bigr]\\Bigr)^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Denote by $\\hat{\\mathbf{u}}_{t}^{\\mathbf{H}}\\cdot\\cdot\\cdot\\hat{\\mathbf{u}}_{t+K}\\sim\\operatorname{Unif}(A^{\\mathbf{H}})$ a sequence of $K$ random actions. From Lemma B.2: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I(R;\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})=\\mathcal{H}(\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})-\\mathcal{H}(\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid R,\\widetilde{\\mathfrak{s}}_{t})}\\\\ &{\\quad\\quad\\leq\\log(K|\\mathcal{A}|)-\\mathcal{H}\\big(\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid R,\\widetilde{\\mathfrak{s}}_{t}\\big)}\\\\ &{\\quad\\quad\\leq\\underset{\\gamma\\to1}{\\operatorname*{lim}}\\Big(\\frac{\\beta}{e}\\mathbb{E}\\big[Q_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}\\big(s_{t},\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\big)-Q_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}\\big(s_{t},\\hat{\\mathfrak{a}}_{t}^{\\mathbf{H}},\\ldots,\\hat{\\mathfrak{a}}_{t+K}^{\\mathbf{H}}\\big)\\big]\\Big)^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last inequality follows from Lemma B.2 and $Q_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}(...)\\leq1$ . We also have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{0\\leq Q_{R,\\gamma}^{\\pi\\mathbf{H},\\pi_{\\mathbf{R}}}(s_{t},\\hat{\\mathbf{a}}_{t}^{\\mathbf{H}},\\ldots,\\hat{\\mathbf{a}}_{t+K}^{\\mathbf{H}})\\leq Q_{R,\\gamma}^{\\pi\\mathbf{H},\\pi_{\\mathbf{R}}}(s_{t},\\mathfrak{a}_{t}^{\\mathbf{H}},\\ldots,\\mathfrak{a}_{t+K}^{\\mathbf{H}})\\leq1,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which lets us conclude from Eq. (28) that ", "page_idx": 15}, {"type": "equation", "text": "$$\nI(R;\\mathfrak{a}_{t}^{\\mathbf{H}},\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})\\le\\Bigl(\\frac{\\beta}{e}\\mathbb{E}\\bigl[Q_{R,\\gamma}^{\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}}}\\bigl(\\mathfrak{s}_{t},\\mathfrak{a}_{t}^{\\mathbf{H}},\\mathfrak{\\cdot}\\,\\mathfrak{\\cdot}\\,,\\mathfrak{a}_{t+K}^{\\mathbf{H}}\\bigr)\\bigr]\\Bigr)^{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We can now prove Theorem 3.1 directly by combining Lemmas B.1 and B.3. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 3.1. Simplifying the limit in Eq. (9), we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\gamma\\rightarrow1}{\\operatorname*{liminf}}\\ \\mathcal{E}_{\\gamma}(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})\\leq\\underset{\\gamma\\rightarrow1}{\\operatorname*{liminf}}\\left(\\sum_{t=0}^{\\infty}\\gamma^{t}I(\\boldsymbol{\\mathfrak{s}}_{+}^{\\gamma};\\mathbf{a}_{t}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leq\\operatorname*{liminf}I(\\boldsymbol{\\mathfrak{s}}_{+}^{\\gamma};\\mathbf{a}_{t}^{\\mathbf{H}},\\ldots,\\boldsymbol{\\mathfrak{a}}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\ \\ \\ \\ \\ \\ }\\\\ &{\\leq I(R;\\mathbf{a}_{t}^{\\mathbf{H}},\\ldots,\\boldsymbol{\\mathfrak{a}}_{t+K}^{\\mathbf{H}}\\mid\\widetilde{\\mathfrak{s}}_{t})}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It follows that for sufficiently large $\\gamma$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{E}_{\\gamma}(\\pi_{\\mathbf{H}},\\pi_{\\mathbf{R}})^{1/2}\\leq(\\beta/e)\\,\\mathcal{J}_{\\pi_{\\mathbf{R}}}^{\\gamma}(\\pi_{\\mathbf{H}}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "WCnJmb7cv1/tmp/13c57d00a37bd8175a8b6e3a42e473cef85b97fc639640d591c4f90bc45248c0.jpg", "img_caption": ["Figure 6: We evaluate our method with and without conditioning on the robot action $a^{\\mathbf{R}}$ . Conditioning aids learning significantly, which we theorize is because it removes uncertainty in the classification. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "C Additional Ablations and Qualitative Results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section we evaluate additional ablations and qualitative results for the ESR method. ", "page_idx": 16}, {"type": "text", "text": "C.1 Learning Representations without the Robot Action ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In our estimation of empowerment (Eq. 12) we supply the robot action $a^{\\mathbf{R}}$ when learning both $\\phi$ and $\\phi$ , however, the theoretical empowerment formulation in Section 3.3 does not require it. ", "page_idx": 16}, {"type": "text", "text": "To evaluate the impact of including $a^{\\mathbf{R}}$ , we run an additional ablation without it on the gridworld environment, shown in Fig. 6. This ablation shows that the inclusion of $a^{\\mathbf{R}}$ is most impactful in more challenging (higher number of boxes) environments. We hypothesize that conditioning the representations on the robot action reduces the noise in the mutual information estimation, and also reduces the difficulty of classifying true future states. ", "page_idx": 16}, {"type": "text", "text": "D Greedy Empowerment Policy ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All of our experiments have used Soft-Q learning to learn a policy from the empowerment estimation. Here, we additionally study a greedy empowerment policy which takes the most empowering action at each step. We model this by setting the q-learning gamma to 0 to fully discount future rewards. ", "page_idx": 16}, {"type": "text", "text": "Results for this ablation are shown in Fig. 7. Unsurprisingly, the greedy optimization vastly underperforms the policy with $\\gamma=0.9$ . ", "page_idx": 16}, {"type": "image", "img_path": "WCnJmb7cv1/tmp/8992072346a42f040fc8627f571359a967296644065580a86a421abf8b88a21d.jpg", "img_caption": ["Figure 7: We compare a greedy policy $(\\gamma=0,$ ) against our standard policy $(\\gamma=0.9)$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.1 ESR Training Example ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Fig. 8, we show the mutual information during training of the ESR agent in the gridworld environment with 5 obstacles. he mutual information quickly becomes positive and remains so throughout training. As long as the mutual information is positive, the classifier is able to reward the agent for taking actions that empower the human. ", "page_idx": 17}, {"type": "text", "text": "E Simplifying the Objective ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The reward function in Eq. (13) is itself a random variable because it depends on future states $g$ . This subsection describes how this randomness can be removed. To do this, we follow prior work [61, 62] in arguing that the learned representations $\\psi(g)$ follow a Gaussian distribution: ", "page_idx": 17}, {"type": "text", "text": "Assumption E.1 (Based on Wang and Isola [61]). The representations of future states $\\psi(g)$ learned by contrastive learning have a marginal distribution that is Gaussian: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{P}(\\psi)=\\int\\mathrm{P}(g)\\delta(\\psi=\\psi(g))\\,\\mathrm{d}g\\stackrel{d}{=}\\mathcal{N}(0,I).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "With this assumption, we can remove the random sampling of $g$ from the reward function. We start by noting that the learned representations tell us the relative likelihood of seeing a future state Eq. (12)). Assumption E.1 will allow us to convert these relative likelihoods into likelihoods. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\mathbf{P}(s^{+}\\mid s,a^{\\mathbf{R}},a^{\\mathbf{H}})}\\big[r(s,a^{\\mathbf{R}})\\big]=\\mathbb{E}_{\\mathbf{P}(s^{+})}\\Bigg[\\frac{\\mathbf{P}(s^{+}\\mid s,a^{\\mathbf{R}},a^{\\mathbf{H}})}{\\mathbf{P}(s^{+})}r(s,a^{\\mathbf{R}})\\Bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}_{\\mathbf{P}(s^{+})}\\Big[C_{1}e^{\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{H}})^{T}\\phi(s^{+})}r(s,a^{\\mathbf{R}})\\Big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=C_{1}\\mathbb{E}_{\\psi\\sim\\mathbf{P}(\\phi(s^{+}))}\\Big[e^{\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{H}})^{T}\\psi}(\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{H}})-\\phi(s,a^{\\mathbf{R}}))^{T}\\psi\\Big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "image", "img_path": "WCnJmb7cv1/tmp/eab2c33806f29f434abe2054b57e037db870dbc85e08d3125e4d78e39a450314.jpg", "img_caption": ["Figure 8: Visualizing training empowerment in a $5\\mathrm{x}5$ Gridworld with 10 obstacles. Our empowerment objective maximizes the influence of the human\u2019s actions on the future state, preferring the state where the human can reach the goal to the trapped state. This corresponds to maximizing the volume of the state marginal polytope, which is proportional to the number of states that the human can reach from their current position. To visualize the representations, we set the latent dimension to 3 instead of 100. "], "img_footnote": [], "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=C_{1}\\left(\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})-\\phi(s,a^{\\mathbf{R}})\\right)^{T}}\\\\ &{\\qquad\\qquad\\int\\frac{1}{(2\\pi)^{d/2}}e^{-\\frac{1}{2}\\|\\psi\\|_{2}^{2}+\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})^{T}\\psi}\\,\\mathrm{d}\\psi}\\\\ &{=C_{1}\\Bigl(\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})-\\phi(s,a^{\\mathbf{R}})\\Bigr)^{T}e^{\\frac{1}{2}\\|\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}},a^{\\mathbf{R}})\\|_{2}^{2}}}\\\\ &{\\qquad\\qquad\\int\\frac{1}{(2\\pi)^{d/2}}e^{-\\frac{1}{2}\\|\\psi\\|_{2}^{2}+\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})^{T}\\psi-\\frac{1}{2}\\|\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})\\|_{2}^{2}}\\psi\\,\\mathrm{d}\\psi}\\\\ &{=C_{1}\\Bigl(\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})-\\phi(s,a^{\\mathbf{R}})\\Bigr)^{T}}\\\\ &{\\qquad\\qquad\\quad e^{\\frac{1}{2}\\|\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})\\|_{2}^{2}}\\mathbb{E}_{\\psi\\sim N(\\mu-\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}}),\\Sigma=I)}\\Bigl[\\psi\\Bigr]}\\\\ &{=C_{1}e^{\\frac{1}{2}\\|\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})\\|_{2}^{2}}\\Bigl(\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}})-\\phi(s,a^{\\mathbf{R}})\\Bigr)^{T}\\phi(s,a^{\\mathbf{R}},a^{\\mathbf{R}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This simplification may be attractive in in cases where the computed empowerment bonuses have high variance, or when the empowerment horizon is large (i.e., $\\gamma\\rightarrow1$ , as in Section 3.3). Empirically, we found this version of the objective to be less effective in practice due to the additional representation structure required by Assumption E.1. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Yes, our goals in the abstract and introduction are well-specified and accurately reflect the rest of the paper and the experimental results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: Yes, we have a limitation subsection that addresses the core limitations of the approach. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we clearly cite our assumptions and prior work that we are building on for theoretical results. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we fully describe how to reproduce our method by detailing the different contrastive networks, the exact loss functions we use, the environments we train on, and the human policies we are training against. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, our code is released at https://anonymous.4open.science/r/ esr-7e94 ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, all of the training details are included in depth to help understand the results, and also the code containing all of this is released. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes standard error bars are included and explained, and the number of seeds are given. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 22}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we provide the experimental details including the compute resources needed to reproduce the experiments in the appendix. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we comply to the Code of Ethics in every way. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We extensively discuss various safety risks, and how empowerment plays into that, in the Discussion section of the paper. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 23}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The data and models are not at risk of misuse, because they are in a gridworld and Overcooked, and policies trained on these environments do not pose a safety issue. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, we cite all the assets used, including code for environments, experiments, and algorithms, and graphics describing the environments. We additionally credit the authors of the behavioral cloning policies we use as human models, link to their repository/license, and mention that they use an MIT License. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The code that we release is well documented and the license is provided alongside the code repository. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper involves neither crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 25}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]