[{"figure_path": "WCnJmb7cv1/figures/figures_1_1.jpg", "caption": "Figure 1: We propose an algorithm training assistive agents to empower human users the assistant should take actions that enable human users to visit a wide range of future states, and the human's actions should exert a high degree of influence over the future outcomes. Our algorithm scales to high-dimensional settings, opening the door to building assistive agents that need not directly reason about human intentions.", "description": "This figure is a visual representation of the core concept of the paper: empowering human users through assistive agents. The left panel (\"Without Empowerment\") shows a scenario where an assistive agent's actions provide limited benefit to the human user, leaving the human constrained in their possible actions. In contrast, the right panel (\"Empowerment\") illustrates how the assistive agent's actions empower the human, enabling them to achieve a wider range of outcomes, which is the goal of the proposed algorithm.", "section": "1 Introduction"}, {"figure_path": "WCnJmb7cv1/figures/figures_3_1.jpg", "caption": "Figure 2: The Information Geometry of Empowerment, illustrating the analysis in Section 3.3. (Left) For a given state st and assistant policy \u03c0R, we plot the distribution over future states for 6 choices of the human policy \u03c0H. In a 3-state MDP, we can represent each policy as a vector lying on the 2-dimensional probability simplex. We refer to the set of all possible state distributions as the state marginal polytope. (Center) Mutual information corresponds to the distance between the center of the polytope and the vertices that are maximally far away. (Right) Empowerment corresponds to maximizing the size of this polytope. For example, when an assistive agent moves an obstacle out of a human user's way, the human user can spend more time at desired state.", "description": "This figure illustrates the concept of empowerment using information geometry.  The left panel shows the state marginal polytope, representing the possible future state distributions for different human actions given a fixed state and robot policy. The center panel illustrates mutual information as the distance between the average state distribution and the furthest achievable state distributions. The right panel shows that maximizing empowerment corresponds to maximizing the size of this polytope, enabling the human to reach a wider range of desired states.", "section": "3 The Information Geometry of Empowerment"}, {"figure_path": "WCnJmb7cv1/figures/figures_7_1.jpg", "caption": "Figure 3: We apply our method to the benchmark proposed in prior work [6], visualized in Fig. 4a. The four subplots show variant tasks of increasing complexity (more blocks), (\u00b11 SE). We compare against AvE [6], the Goal Inference baseline from [6] which assumes access to a world model, and Reward Inference [56] where we recover the reward from a learned q-value. These prior approaches fail on all except the easiest task, highlighting the importance of scalability.", "description": "This figure compares the performance of the proposed Empowerment via Successor Representations (ESR) method against several baselines on a benchmark task with increasing difficulty.  The task involves assisting a human agent in navigating an obstacle course to reach a goal. The four subplots show the results for different numbers of obstacles (2, 5, 7, and 10).  The ESR method outperforms the baselines, especially as the task complexity increases, demonstrating its scalability to more challenging scenarios.", "section": "Experiments"}, {"figure_path": "WCnJmb7cv1/figures/figures_8_1.jpg", "caption": "Figure 4: (a) The modified environment from Du et al. [6] scaled to N = 7 blocks, and (b, c) the two layouts of the Overcooked environment [10].", "description": "This figure shows three different experimental environments used in the paper.  (a) is a modified version of the obstacle gridworld from a previous work, scaled up to 7 blocks.  (b) and (c) show two different layouts of the Overcooked game environment, known for its cooperative gameplay requiring coordination between agents.", "section": "Experiments"}, {"figure_path": "WCnJmb7cv1/figures/figures_8_2.jpg", "caption": "Figure 5: In Coordination Ring, our ESR agent learns to wait for the human to add an onion to the pot, and then adds one itself. There is another pot at the top which is nearly full, but the empowerment agent takes actions to maximize the impact of the human\u2019s actions, and so follows the lead of the human by filling the empty pot.", "description": "This figure shows a sequence of actions in the Overcooked Coordination Ring environment, illustrating how the ESR agent assists the human. The human and robot take turns adding onions to the pots.  The key observation is that the ESR agent waits for the human to perform an action before taking its own, maximizing the impact of human action on the environment. This demonstrates the agent's ability to learn collaborative behaviors without explicitly modeling the human's reward.", "section": "Experiments"}, {"figure_path": "WCnJmb7cv1/figures/figures_16_1.jpg", "caption": "Figure 3: We apply our method to the benchmark proposed in prior work [6], visualized in Fig. 4a. The four subplots show variant tasks of increasing complexity (more blocks), (\u00b11 SE). We compare against AvE [6], the Goal Inference baseline from [6] which assumes access to a world model, and Reward Inference [56] where we recover the reward from a learned q-value. These prior approaches fail on all except the easiest task, highlighting the importance of scalability.", "description": "This figure compares the performance of the proposed Empowerment via Successor Representations (ESR) method against other baseline methods (AvE, Goal Inference, Reward Inference) on a benchmark task with varying difficulty levels (2, 5, 7, and 10 obstacles).  The results show that ESR significantly outperforms the baselines in more complex scenarios, demonstrating its scalability to higher-dimensional problems.", "section": "5.1 Do contrastive successor representations effectively estimate empowerment?"}, {"figure_path": "WCnJmb7cv1/figures/figures_17_1.jpg", "caption": "Figure 3: We apply our method to the benchmark proposed in prior work [6], visualized in Fig. 4a. The four subplots show variant tasks of increasing complexity (more blocks), (\u00b11 SE). We compare against AvE [6], the Goal Inference baseline from [6] which assumes access to a world model, and Reward Inference [56] where we recover the reward from a learned q-value. These prior approaches fail on all except the easiest task, highlighting the importance of scalability.", "description": "This figure compares the performance of the proposed Empowerment via Successor Representations (ESR) method against several baselines on an obstacle gridworld assistance task. The task involves a human user trying to reach a goal while avoiding obstacles, and an assistive agent helping the user.  The four subplots show results for increasing numbers of obstacles.  The ESR method significantly outperforms the baselines (AvE, Goal Inference, Reward Inference) as the task complexity increases.  The results highlight ESR's scalability to higher-dimensional problems compared to previous methods.", "section": "Experiments"}, {"figure_path": "WCnJmb7cv1/figures/figures_18_1.jpg", "caption": "Figure 8: Visualizing training empowerment in a 5x5 Gridworld with 10 obstacles. Our empowerment objective maximizes the influence of the human's actions on the future state, preferring the state where the human can reach the goal to the trapped state. This corresponds to maximizing the volume of the state marginal polytope, which is proportional to the number of states that the human can reach from their current position. To visualize the representations, we set the latent dimension to 3 instead of 100.", "description": "This figure visualizes how the proposed empowerment objective affects the agent's learning process in a gridworld environment. The top part shows two scenarios: a \"free state\" where the human can easily reach the goal and a \"trapped state\" where the human is blocked. The plots show the difference in future state distributions between these two scenarios, highlighting the agent's ability to distinguish between states based on the human's ability to reach the goal. The lower plots show the mutual information (a measure of empowerment), the average number of reachable states, and the overall reward achieved during training. These plots demonstrate how maximizing empowerment leads to improved performance in the task.  The latent dimension is reduced for visualization purposes.", "section": "Experiments"}]