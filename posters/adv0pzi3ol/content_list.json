[{"type": "text", "text": "Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tang Li Mengmeng Ma Xi Peng ", "page_idx": 0}, {"type": "text", "text": "DeepREAL Lab: https://deep-real.github.io Department of Computer & Information Science, University of Delaware {tangli, mengma, xipeng}@udel.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large pretrained foundation models demonstrate exceptional performance and, in some high-stakes applications, even surpass human experts. However, most of these models are currently evaluated primarily on prediction accuracy, overlooking the validity of the rationales behind their accurate predictions. For the safe deployment of foundation models, there is a pressing need to ensure double-correct predictions, i.e., correct prediction backed by correct rationales. To achieve this, we propose a two-phase scheme: First, we curate a new dataset that offers structured rationales for visual recognition tasks. Second, we propose a rationale-informed optimization method to guide the model in disentangling and localizing visual evidence for each rationale, without requiring manual annotations. Extensive experiments and ablation studies demonstrate that our model outperforms state-of-the-art models by up to $10.1\\%$ in prediction accuracy across a wide range of tasks. Furthermore, our method significantly improves the model\u2019s rationale correctness, improving localization by $7.5\\%$ and disentanglement by $36.5\\%$ . Our dataset, source code, and pretrained weights: https://github.com/deep-real/DCP ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large foundation models, such as CLIP [1] and GPT4V [2], exhibit exceptional performance or even surpass human experts in some high-stakes applications, such as medical diagnosis [3] and autonomous driving [4, 5]. However, most of these models are currently evaluated primarily on prediction accuracy, overlooking a critical aspect for ensuring safety, i.e., the validity of the reasons behind their accurate predictions. Understanding the rationales - the \u201chow\u201d and \u201cwhy\u201d behind model predictions - is crucial for developing safe predictions. Fig. 1 shows typical examples of unsafe predictions: CLIP might predict accurately yet based on wrong rationales, whereas GPT-4V might make wrong predictions based on rationales that are plausible to humans. To build trust in real-world deployment, a natural question arises: Can models make double-correct predictions, i.e., correct predictions backed by correct rationales? ", "page_idx": 0}, {"type": "text", "text": "Correct rationales generally align with how humans would reason about the same decision and are based on valid visual evidence [6, 7, 8]. There are existing attempts to provide rationales for machine learning models\u2019 predictions. They either explicitly force the models to make decisions based on human-understandable concepts by introducing bottleneck layers [9, 10], or implicitly inject commonsense knowledge into models by contrastive learning between similar yet distinct textual concepts [11, 12]. However, none of them ensures double-correct predictions. Observations from our previous research [13] and recent studies in the field [14, 15] reveal that these models might provide incorrect rationales, as they fail to base the rationales on valid visual evidence. ", "page_idx": 0}, {"type": "image", "img_path": "ADV0Pzi3Ol/tmp/343fd9b25a602ac369178ea96cb0eabf9609a860fba985df45011fc3311f7608.jpg", "img_caption": ["Figure 1: Unsafe prediction examples. Correct prediction, incorrect rationale: CLIP identifies a red light, but wrongly based on red balloons. Incorrect prediction, correct rationale: GPT-4V incorrectly predicts a closed door, yet based on plausible visual evidence. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To this end, we develop double-correct predictions by focusing on two foundational aspects: ", "page_idx": 1}, {"type": "text", "text": "i) \u201cWhat\u201d are the correct rationales? Structured rationale acquisition. Existing vision datasets typically provide ground truth labels of predictions, whereas missing the rationales behind these decisions [16, 17]. To fill this gap, we curate a new dataset that offers over 4,000 unique textual rationales designed for predicting the 1,000 categories in ImageNet [18], structured in a tree format. This design differs from existing knowledge graphs [19, 20, 21], which either provide irrelevant knowledge for the vision task or are too coarse-grained, providing insufficient information. Our rationale dataset is tailored to capture the detailed reasoning processes for visual recognition. ", "page_idx": 1}, {"type": "text", "text": "ii) \u201cWhere\u201d are the correct rationales? Rationale-informed optimization. The other challenge in developing double-correct predictions is the absence of pixel-wise annotations for rationales\u2019 visual evidence. Although some datasets provide segmentation masks of object parts [17, 22], they lack sufficient rationale coverage and are limited to small-scale use cases [23]. To address this issue, we propose a rationale-informed optimization method to guide the model in disentangling and localizing the visual evidence of rationales, without requiring manual annotations. Our method can be integrated into the existing model training process without architectural changes and extra parameters. ", "page_idx": 1}, {"type": "text", "text": "We evaluate the proposed method on a wide range of benchmark datasets and tasks. For prediction correctness, our model outperforms state-of-the-art models in zero-shot, linear probe, and fine-tuning settings by $2.6\\%$ , $2.0\\%$ , and $10.1\\%$ . For rationale correctness, the empirical results exhibit that our model significantly improves ground truth rationale localization and rationale disentanglability by $7.5\\%$ and $36.5\\%$ . Furthermore, the extensive qualitative results and ablation studies demonstrate the effectiveness of the proposed method. ", "page_idx": 1}, {"type": "text", "text": "Our contribution includes: 1) We curate a new structured rationale dataset. 2) A faithful explanation method tailored for explaining CLIP-ViT predictions. 3) A principled optimization method that seamlessly integrates structured rationale information to develop double-correct predictions. 4) Empirical results in a wide range of benchmark datasets and tasks including image classification and retrieval demonstrate the superior prediction and rationale correctness of our model. ", "page_idx": 1}, {"type": "text", "text": "2 Problem Formulation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we first formally define rationales, then provide the mathematical formulation of the double-correct prediction problem. ", "page_idx": 1}, {"type": "text", "text": "Definition 1 (Rationales) Given a category y, rationales are a set of $K$ underlying abstract notions $\\{r_{k}^{y}\\}_{k=1}^{K}$ and relations that capture the reasoning process leading to the recognition of $y$ . ", "page_idx": 1}, {"type": "text", "text": "In the real world, rationales can be represented through textual descriptions [24, 25]. For example, when recognizing a specific breed of dog in an image, the rationales could be a set of concepts such as the shape of the ears, the color of the fur, and the size of the dog. Mathematically, given a textual rationale $r$ , we assume the existence of a ground truth labeling function $V(x,r)$ that can provide the pixel-wise annotations of visual evidence corresponding to $r$ on an input $x$ . ", "page_idx": 1}, {"type": "text", "text": "Definition 2 (Double-Correct Predictions) $A$ correct prediction is double-correct when it is backed by correct rationales that are based on valid visual evidence. ", "page_idx": 1}, {"type": "text", "text": "Denote $(x,y)\\sim P(X,Y)$ as a data point sampled from the training distribution $P(X,Y),\\,g(\\cdot)$ as an explanation method that attributes the prediction of text $r$ to a group of pixels in input $x$ depending on model $f,\\ell(\\cdot)$ as the task-specific loss function, and $\\mathcal{F}$ as a function class that is model-agnostic for the prediction task. To ensure the model $f$ makes double-correct prediction, we propose to solve the following constrained optimization problem: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{f\\in\\mathcal{F}}\\mathcal{R}(f):=\\mathbb{E}_{(x,y)\\sim P(X,Y)}[\\ell(f(x),y)]\\quad\\mathrm{~s.t.~}\\,g(x,r;f)=V(x,r),\\;\\;\\forall r\\in\\{r_{k}^{y}\\}_{k=1}^{K}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "nTohre  tpor othblee gmr oinu nEdq .t r1u tish  clahbalelleinnggi nfug ntcot isoonlvs .c e Twhee rnee iatrhee re xhiasvtien agc catetses mtop ttsh et hraatt ieonmaplleos $\\{r_{k}^{y}\\}_{k=1}^{K}$ , $V(\\cdot)$ experts to manually collect textual descriptions of rationales [22, 26], or pixel-wise annotations of object parts on the image [17]. However, these approaches are often limited to small-scale datasets, and impractical in large-scale settings due to the high cost of fine-grained annotations [27, 23]. ", "page_idx": 2}, {"type": "text", "text": "3 Double-Correct Predictions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To bridge the gaps, in Sec. 3.1 we present how to acquire rationales $\\{r_{k}^{y}\\}_{k=1}^{K}$ , in Sec. 3.2 we propose a new explanation method $g(\\cdot)$ , and in Sec. 3.3 we develop double-correct predictions without $\\bar{V}(\\cdot)$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Structured Rationale Dataset ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we curate a new rationale dataset to offer $\\{r_{k}^{y}\\}_{k=1}^{K}$ in Eq. 1. According to Def. 1, rationales are structured human knowledge. Therefore, ontologies that encapsulate complex, interconnected information while maintaining semantic relationships between entities [28, 29], present a proper tool to represent rationales. The beneftis are bi-directional: i) in the human-to-machine direction, it offers a standardized, machine-readable format; ii) in the machine-tohuman direction, ontology structure mirroring how humans organize and retrieve information to explain the model\u2019s decision-making process. ", "page_idx": 2}, {"type": "image", "img_path": "ADV0Pzi3Ol/tmp/45cc1ce31292e9bf863249688f1a5c980859f66eacf1296206fabf3f4a0a5da7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Our structured rationales capture the major attributes and their sub-attributes that lead to the recognition of objects. Our dataset offers over 4,000 unique rationales covering all 1,000 categories from ImageNet [18]. ", "page_idx": 2}, {"type": "text", "text": "Acquire structured rationales: Different from existing works that are limited to small-scale manual annotation, we generate our rationale dataset in a scalable manner. Specifically, we utilize Large Language Models (LLMs) like GPT-4 [2] to extract the structured rationales. Existing studies prove that GPT-4 has expert-level expertise in commonsense [30] and domain knowledge [31]. However, we find that directly querying LLMs would yield inconsistent tree structures that can hardly be used by machine learning models. To address this issue, we provide a series of exemplary structured rationales before the query, employing in-context learning [32] to extract standardized rationales in a .JSON format. See Appendix A for our full prompt and rationale examples. ", "page_idx": 2}, {"type": "text", "text": "Rationale dataset statistics: Our dataset covers all 1,000 categories in the ImageNet [18]. For each category, we generate an ontology tree with a maximum height of two. As illustrated in Fig. 2, the root node is the category, the children of the root are the attributes, and the leaves are the sub-attributes. The edges represent the relationships between nodes. Combining attributes and sub-attributes, our dataset contains over 4,000 unique rationales. Our rationale ontology trees capture the reasoning processes leading to the recognition of the corresponding root categories. ", "page_idx": 2}, {"type": "text", "text": "Can we trust the rationales extracted from GPT-4? Although there are plenty of works showing GPT-4\u2019s remarkable capabilities [30, 31], it still could suffer from hallucinations [33, 34]. However, evaluations on the generation quality are largely missing from existing works that generate data from LLMs [9, 35, 36]. To fill this gap and ensure the quality of our rationale data, we conduct comprehensive human and machine evaluations. As detailed in Sec. 4.1, on a 5-point Likert scale across three metrics, 964 out of 1,000 categories are scored as having high-quality rationales $(\\ge\\!4.0)$ . ", "page_idx": 2}, {"type": "text", "text": "In contrast to existing Knowledge Graphs [19, 20, 21] that either offer knowledge unrelated to the visual prediction task, or are too coarse-grained that provide insufficient information, our structured rationales are tailored for visual recognition tasks in a fine-grained attribute level. Furthermore, our dataset can expand to accommodate new rationales, providing flexibility to dealing with evolving datasets where more data becomes available. For example, our rationale ontologies can be seamlessly integrated following the ImageNet [18] category ontology derived from WordNet [20]. ", "page_idx": 2}, {"type": "text", "text": "3.2 Faithful Explanation Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we develop a new explanation method to implement $g(\\cdot)$ in Eq. 1. To incorporate both image and text inputs, we instantiate the model $f$ using the CLIP-ViT architectures [37] because of ", "page_idx": 2}, {"type": "image", "img_path": "ADV0Pzi3Ol/tmp/6e8fa1d3af9416006106b1693b7fc5c55be47a37f6b6346987c9639731eba7dd.jpg", "img_caption": ["Figure 3: Multi-head Self Attention (MSA) accumulated mean-ablation study. Based on Eq. 2, we replace the direct effects of MSAs up to a specific layer with their mean values calculated across the ImageNet [18] validation set. Most of the performance gains can be attributed to the final layers of the ViT. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Table 1: Weakly-supervised segmentation accuracy on ImageNet-Seg [45]. We threshold explanation heatmaps from CLIP-ViT-L-14 as segmentation masks. Our method outperforms existing explanation methods in segmentation accuracy, demonstrating the high faithfulness of our explanations. ", "page_idx": 3}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/a7259a035fbe82b457f8f5f2256b395456d367bec1ec1548c52472ab74124874.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "their proven capability [1, 38]. Existing methods for explaining the ViT model either directly use the attention maps as explanations [39], or weigh them using gradients [40, 41]. However, these methods might be unfaithful to the ViT predictions. This is because the computation of each ViT prediction involves queries, keys, and values, whereas the attention maps only capture the inner products of queries and keys, ignoring information in values that also affect predictions [42, 40]. Therefore, explanations based on attention maps might not fully reflect the reasons behind ViT predictions. ", "page_idx": 3}, {"type": "text", "text": "Decompose ViT outputs: Recent works [43, 44] prove that, for ViT models, the image embeddings can be decomposed into the contributions of each token within each attention head. Let $\\phi$ and $\\theta$ parameterize the image- and text-encoder of the CLIP-ViT model, $P$ is the projection matrix, $L,M$ , $N$ are the numbers of layers, heads, and image tokens, $a_{i}^{l,m}$ is the output of the $m$ -th attention head in layer $l$ for the $i$ -th image token, then the embedding of image $I$ can be decomposed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{e_{I}=f_{\\phi}(I)=P\\mathrm{ViT}(I)=\\sum_{l=1}^{L}\\!\\sum_{m=1}^{M}\\!\\sum_{i=0}^{N}\\!P a_{i}^{l,m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "By contracting along layers and heads, [44] calculates the contribution of the $i$ -th image token to the final image embedding using $\\textstyle\\sum_{l=1}^{L}\\sum_{m=1}^{M}P a_{i}^{l,m}$ ", "page_idx": 3}, {"type": "text", "text": "Faithful explanations weighted by mean-ablation results: As indicated by our mean-ablation results in Fig. 3, the final layers contribute the most to the predictions, whereas the earlier layers have minimal impact. Thus, noise from early layers could obscure key information by a naive summation across all layers as in [44]. To address this issue, we weigh each layer\u2019s contribution based on its importance, measured by the corresponding performance drop in the mean-ablation study. Denote the performance drop of layer $l$ as $\\Delta_{l}$ , we calculate the contribution of the $i$ -th image token by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\boldsymbol{e}_{i}=\\sum_{l=1}^{L}w^{l}\\sum_{m=1}^{M}\\!P a_{i}^{l,m},\\quad\\mathrm{where\\\\}\\boldsymbol{w}^{l}=\\frac{\\boldsymbol{\\Delta}_{l}}{\\sum_{j=1}^{L}\\boldsymbol{\\Delta}_{j}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that $e_{i}$ is projected onto the image-text embedding space by $P$ . Thus, we can use $g(I,r)=$ $\\{\\langle e_{i},f_{\\theta}(r)\\rangle\\}_{i\\in I}$ to calculate the explanations of rationale $r$ on an image $I$ , i.e., visual evidence. ", "page_idx": 3}, {"type": "text", "text": "Our method significantly improves the explanation accuracy, as shown in Tab. 1. In contrast to attention-based explanations [39], our method fully utilizes the information from queries, keys, and values that are used for ViT predictions. Compared to gradient-weighted attention maps [40, 41], our method cuts down the computational complexity from $O(n^{2})$ to $O(n)$ over $n$ image tokens. ", "page_idx": 3}, {"type": "text", "text": "3.3 Rationale-informed Optimization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we develop double-correct predictions by disentangling and localizing rationales without pixel-wise human annotations $V(\\cdot)$ in Eq. 1. ", "page_idx": 3}, {"type": "text", "text": "Disentanglement via reconstruction: Drawing insights from our previous research [47, 13], we propose to contrast between explanation heatmaps of rationales to guide the model training in a selfsupervised manner. Specifically, we enforce the following two constraints: i) the image embeddings for different rationales within the same category are disentangled, and ii) the aggregated image embedding of all rationales within the same category aligns with the text embedding of the category. ", "page_idx": 3}, {"type": "text", "text": "Mathematically, the backbone objective is to learn a mapping function $f\\in\\mathcal F$ such that for each image-text pair $(I,T)\\sim P(\\mathbf{I},\\mathbf{T})$ , the embeddings $f_{\\phi}(I)$ and $f_{\\theta}(T)$ are aligned in a shared space if they are a correct match, where $T$ is a text description of category $y$ . Let $\\ell(\\cdot)$ be the InfoNCE loss [48]. $\\begin{array}{r}{h(g(I,r))=\\sum_{i}\\!e_{i}\\cdot\\mathbb{1}(g(I,r)_{i}>\\tau)}\\end{array}$ extracts the image embedding of a given rationale. $\\mathcal{D}(\\cdot,\\cdot)$ is a distance met ric such as L2 distance. $\\tau,\\epsilon$ , and $\\delta$ are thresholding hyperparameters. For all $r,\\dot{r}^{\\prime}\\in\\{r_{k}^{y}\\}_{k=1}^{K}$ , we propose to develop double-correct predictions by optimizing: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathcal{R}(f):=\\mathbb{E}_{(I,T)\\sim P(\\mathbf{I},\\mathbf{T})}[\\ell(f_{\\phi}(I),f_{\\theta}(T))]}&{\\mathrm{~<~Correct~}\\mathbf{P}}\\\\ &{\\underbrace{\\mathcal{D}(h(g(I,r)),h(g(I,r^{\\prime})))\\geq\\epsilon}_{\\mathrm{Disentanglement}}\\;\\underbrace{\\mathcal{D}(\\sum_{r}h(g(I,r)),f_{\\theta}(y))\\leq\\delta.}_{\\mathrm{Reconstruction}}\\;}&{\\mathrm{~<~Correct~}\\mathbf{R}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Intuitively, the reconstruction term prevents the disentanglement from collapsing into trivial solutions, thereby ensuring localization. Solving Eq. 4 often leads to a non-convex problem, wherein methods such as stochastic gradient descent (SGD) cannot guarantee constraint satisfaction [49, 50]. To address this issue, we leverage Karush\u2013Kuhn\u2013Tucker (KKT) conditions [51, 52] and introduce Lagrange multipliers $\\lambda$ and $\\gamma$ to convert the constrained problem into its unconstrained counterpart: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{f\\in\\mathcal{F}}{\\operatorname*{min}}\\;\\{\\mathcal{R}(f):=\\mathbb{E}_{(I,T)\\sim P(\\mathbf{I},\\mathbf{T})}[\\ell(f(I,T))]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,\\lambda\\mathcal{D}(h(g(I,r)),h(g(I,r^{\\prime})))\\;+\\;\\gamma\\mathcal{D}(\\sum_{r}h(g(I,r)),f_{\\theta}(y))\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our method has the following merits: i) In contrast to existing works that rely on expensive pixel-wise annotations to localize objects [53, 54], the proposed rationale-informed optimization achieves a more fine-grained, attribute-level localization without manual annotations. ii) Our method can be integrated into vision-language model training without architectural changes and extra parameters. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we first evaluate the quality of our curated rationale dataset in Sec. 4.1. To best validate double-correct predictions, we then conduct a series of experiments to compare the proposed method with existing methods in Secs $4.2\\textrm{-}4.7$ . The experimental results prove that our model achieves superior prediction and rationale correctness on a wide range of benchmark datasets and tasks. ", "page_idx": 4}, {"type": "text", "text": "4.1 Evaluation of Rationale Quality ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Metrics: We focus on three essential aspects of the rationale quality. (1) Factual Consistency: whether the rationales are consistent with facts. (2) Comprehensiveness: whether the rationales provide sufficient information necessary to predict the category. (3) Visual Disentanglement: whether the rationales are visually disentanglable or non-overlap. We rate them on a 5-point Likert scale scoring system, where higher scores indicate better performance. For example, in Factual Consistency, score 5 means $100\\%$ of the generated rationales are consistent with facts, score 4 means $75\\%$ , score 3 means $50\\%$ , score 2 means $25\\%$ , and score 1 means completely wrong. ", "page_idx": 4}, {"type": "text", "text": "Evaluators: (1) Human Evaluators: We recruited four human evaluators, who are mostly graduate students. They are asked to conduct assessments based on commonsense knowledge and perform Internet searches for validation. On average, it takes them around one minute per sample. (2) Machine Evaluators: The latest GPT-4o and GPT-4v models (date accessed: Aug. 6th, 2024). For each evaluation, we perform three independent runs and calculate the average scores. Note that expanding human evaluations to the entire dataset is not scalable. To this end, we first prove the reliability of machine evaluations, then use it to automatically evaluation the entire dataset. ", "page_idx": 4}, {"type": "text", "text": "Human evaluations: We sample three independent groups of data from our rationale dataset, each consisting of 50 categories and their corresponding rationales. Specifically, categories were randomly selected from their superclasses: Animals (20), Objects & Artifacts (15), Natural Scenes (5), Plants (5), and Human Activities (5). This ensures that not only each superclass is represented but also that our results are robust [55]. As shown in Tab. 2, The dataset consistently achieves scores of 4.61 or higher on the average of evaluators for each metric, indicating that over $90.3\\%$ of the rationales for each category are highly factual, comprehensive, and visually disentanglable. ", "page_idx": 4}, {"type": "text", "text": "Machine evaluations: Note that the scores of all three metrics are almost identical between machines and humans. The Pearson Correlation coefficient of 0.82 reveals the strong positive correlation between machine and human evaluators. Based on this observation, we further conduct machine evaluations on the entire dataset efficiently. Our results indicate that 964 out of 1,000 categories have high-quality rationales $(\\ge\\!4.0)$ . See detailed results for the entire dataset in Appendix. B. ", "page_idx": 4}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/cdc7299bcacc67f150ed779235e35172c7874dd33686b07e5366bb23a4efb8e7.jpg", "table_caption": ["Table 2: Evaluation results of rationale quality. Both machine and human evaluators receive the same instructions about the metrics. The scores for all three metrics are nearly identical between machine and human evaluators, indicating that over $90.3\\%$ of our rationales are of high quality. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.2 Benchmark Datasets and Implementation Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Backbone model: Due to the computational cost of training large vision-language models (VLMs) from scratch, we focus on fine-tuning experiments. Specifically, we fine-tune the ViT-B/32 variant of CLIP on the ImageNet [18] dataset combined with our curated rationale dataset. To maintain simple and interpretable rationales, the ontology graph for each category is limited to a maximum depth of two, allowing for the extraction of five to six independent concepts on average. ", "page_idx": 5}, {"type": "text", "text": "Baseline models: We compare our model with state-of-the-art VLMs that use ViT-B/32 as their vision encoders, including large-scale pretrained models (CLIP [1], DeCLIP [56]), knowledge-augmented model (NegCLIP [11]), and fine-grained alignment models (FILIP [57], PyramidCLIP [53]). For fair comparisons, we also compare our model with ImageNet [18] fine-tuned models using the same CLIP initialization and augmented text descriptions as our model, including full model fine-tuning (-ft) and vision-encoder-only fine-tuning (-ft-vision). ", "page_idx": 5}, {"type": "text", "text": "Evaluation datasets: We validate the prediction correctness of the models on image classification and image-text retrieval tasks. For image classification (zero-shot, linear probe), experiments are carried out on nine benchmark datasets, including CUB [17], Caltech101 [58], OxfordPets [59], Food101 [60], SUN397 [61], StanfordCars [62], DTD [63], CIFAR-10 [64], and CIFAR-100 [64]. For retrieval, we conduct experiments on Flickr30K [65] and MSCOCO [66]. To evaluate the correctness of rationales, we evaluate the models\u2019 rationale localizability on CUB-Part [67] and PartImageNet [68] that provide ground truth segmentation masks of object parts, e.g., \u201chead\u201d and \u201cbody\u201d. Furthermore, we evaluate the rationale disentanglability on the aforementioned nine benchmark datasets. More details can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Implementation details: We follow the same architecture design as CLIP [1] for ViT-B/32. The input resolution of image encoder is $224\\!\\times\\!224$ and the maximum context length of text encoder is 77. We train our model using an AdamW [69] optimizer and the cosine learning rate scheduler with a linear warmup. Specifically, the learning rate linearly increases from 0 to the peak value within $10\\%$ of the total steps, and then decreases with a cosine anneal strategy. Our learning rate is set to 5e-7 and train the model for eight epochs. More details can be found in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "4.3 Evaluation Metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Prediction correctness: We use standard category prediction accuracy to evaluate the prediction correctness for zero-shot, linear probe, and fine-tuned settings. ", "page_idx": 5}, {"type": "text", "text": "Rationale correctness: We define two new metrics to measure rationale correctness. ", "page_idx": 5}, {"type": "text", "text": "i) Rationale localizability. We evaluate the correctness of rationales using ground truth segmentation masks of object parts [67, 68]. Following the standard evaluation protocol [70], we threshold the rationale explanation heatmaps to segmentation masks and calculate a mean Intersection over Union (mIoU $\\uparrow$ ) score with the ground truth masks across different object parts. Specifically, the dynamic threshold $\\tau=\\mu+\\sigma$ , where $\\mu$ and $\\sigma$ are the mean and standard deviation of importance values of all pixels in a heatmap. The pixel with an importance value larger than $\\tau$ is set to 1, otherwise 0. ", "page_idx": 5}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/bf8cffffe6031fd5e7303891f114b7528e238aa4f16c19fdf560790d9b21d7f0.jpg", "table_caption": ["Table 3: Comparison of prediction accuracy $(\\%)$ on nine benchmark datasets. Our results are on the average of three trials of experiments using different random seeds. We highlight the best results and the second best results. Surprisingly, different from most interpretability methods that compromise benchmark performance, our method also enhances prediction accuracy. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "ii) Rationale disentanglability. As shown in Fig. 4, for the CLIP model [1], the visual evidence of different rationales is entangled. Specifically, we treat the disentanglement between the visual evidence of different rationales as an important metric to evaluate whether the model can distinguish rationales. Specifically, we treat rationale explanation heatmaps $\\mathbf{m}$ and $\\mathbf{m}^{\\prime}$ as vectors and calculate $1-|\\langle{{\\bf{m}},{\\bf{m}}^{\\prime}}\\rangle|$ as an intuitive measure of disentanglability, the higher metric value the better. ", "page_idx": 6}, {"type": "text", "text": "4.4 Evaluation on Prediction Correctness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Zero-shot image classification: We compare our model against other state-of-the-art and fine-tuned VLMs on zero-shot image classification tasks. The results are shown in Tab. 3. On the average of nine datasets, our model outperforms the second-best result by $2.6\\%$ . The results indicate the strong transferability of our model to other vision datasets. ", "page_idx": 6}, {"type": "text", "text": "Linear probe: Following the common practice [1, 71], we conduct linear probe experiments on the nine image classification datasets. As shown in Tab. 3, our model outperforms the second-best result by $2.0\\%$ . These results demonstrate the superior vision representations learned by our model. ", "page_idx": 6}, {"type": "text", "text": "Fair comparison with fine-tuned models: As shown in Tab. 3, our model outperforms the best fine-tuned model by $10.1\\%$ and $5.3\\%$ on zero-shot and linear probe results. This suggests that the proposed Rationale-informed Optimization is essential in improving the model\u2019s performance. ", "page_idx": 6}, {"type": "text", "text": "4.5 Evaluation on Rationale Correctness ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Rationale localizability: We compare our model with state-of-the-art and fine-tuned VLMs. As shown in Tab. 4, our model significantly improves the localization accuracy of rationales by $7.5\\%$ and $6.0\\%$ on CUB-Part [67] and PartImageNet [68]. This suggests that even without using explicit region annotations, our method significantly enhances the model\u2019s localizability of rationales. ", "page_idx": 6}, {"type": "text", "text": "Rationale disentanglability: We compare the rationale disentanglement performance of our model with state-of-the-art and fine-tuned models. As shown in Tab. 5, on the average of nine image classification datasets, our model outperforms the second-best result by $36.5\\%$ . This significant improvement reveals that our model can distinguish between different rationales. ", "page_idx": 6}, {"type": "text", "text": "Fair comparison with fine-tuned models: To evaluate whether our model\u2019s performance gain can be obtained by solely introducing information from our rationale dataset, we conduct fair comparison experiments with fine-tuned CLIP models. We compare our model with baseline (CLIP-zs), full model fine-tuning (CLIP-ft), and vision-only fine-tuning (CLIP-ft-vision). All fine-tuned models use the same CLIP initialization and receive the same language supervision as our model. As shown in Tabs. 4& 3, our model outperforms the best fine-tuned model by $9.8\\%$ and $41.1\\%$ on rationale localizability and disentanglability. This indicates that naive fine-tuning using augmented information without constraints would deteriorate the rationale correctness of the model. ", "page_idx": 6}, {"type": "image", "img_path": "ADV0Pzi3Ol/tmp/bd0e82b33920f17f1163ac5a40e62b01cf4a3bcb2ab6c3035d9a35033e057df4.jpg", "img_caption": ["Figure 4: Qualitative results of rationale disentanglement and localization. The rationales\u2019 visual evidence of the CLIP model [1] typically highlights the entire object, lacking precise localization. In contrast, our model can correctly localize rationales, thereby enhancing trust in its predictions. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/8b9c50497f45c41f48a59b6bc9877263f2bcf4fba50b17c1703f2a530cefee85.jpg", "table_caption": ["Table 4: Comparison of rationale localizability on CUB-Part [67] and PartImageNet [68]. As detailed in Sec. 4.3, we threshold rationales\u2019 explanation heatmaps as segmentation masks and calculate their mIoU (\u2191) with ground truth masks of corresponding object parts. Our model significantly improves the localization accuracy of fine-grained object parts. Full table in Appendix C. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Qualitative results: In Fig. 4, we show the visualizations of our visual evidence of different rationales. As shown, the rationales\u2019 visual evidence of the CLIP model [1] are entangled and mislocalized. In contrast, the rationales\u2019 visual evidence of our model are visually distinct and correctly localized. ", "page_idx": 7}, {"type": "text", "text": "4.6 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Ablation on rationale disentanglement: The \u201cw/o disen.\u201d refers to a variant of our method without rationale disentanglement constraint. As shown in Tab. 7, the rationale localizability decreased by $10.4\\%$ , indicating the model might not learn to distinguish between rationales without constraints. ", "page_idx": 7}, {"type": "text", "text": "Ablation on reconstruction: The \u201cw/o recon.\u201d refers to a variant of our method without reconstruction constraint. As shown in Tab. 7, the rationale localizability and prediction accuracy drastically decreased by $13.3\\%$ and $30.2\\%$ . This reveals that recklessly optimizing the disentanglement between rationale can easily fall into trivial solutions. ", "page_idx": 7}, {"type": "text", "text": "Generalize to different rationale sets: According to DCLIP [9], using the text embeddings of concepts as a bottleneck layer to force the CLIP model [1] to predict based on them can improve prediction accuracy and interpretability. Specifically, the final prediction will be made by the average embedding similarity between the image and all concepts, namely $\\begin{array}{r}{\\hat{y}=\\operatorname*{argmax}_{y}\\frac{1}{K}{\\sum_{k=1}^{K}}\\langle f_{\\phi}(I),f_{\\theta}(c_{k}^{y})\\rangle}\\end{array}$ . We use the concept set provided in [9] rather than our training rationale dataset. As shown in Tab. 8, our model can generalize to an unseen concept set with improved prediction accuracy. ", "page_idx": 7}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/7aba23d33c499e074548356e0941e192cd7cbca4c6a6777f8c63709ead30b0a2.jpg", "table_caption": ["Table 5: Comparison of rationale disentanglement. We conduct experiments on nine image classification datasets. Our results are on the average of three trials using different random seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/edcbdb90b795ff142c5e4382a0be75987abda52d231545645ddd20f12c23b14d.jpg", "table_caption": ["Table 6: Comparison of zero-shot image-text retrieval accuracy $(\\%)$ . Double-correct prediction enhances the model\u2019s visual understanding. (Note that $\\mathrm{NegCLIP}$ is trained on MSCOCO [66]) "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/6c6b31feb7683d8af27bd63a0b7c92a4a4d8b8f58980b24b620f547d1af8b784.jpg", "table_caption": ["Table 7: Ablation study on proposed constraints using CUB-Part [67] and CUB [17]. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Ablation using random string: WaffleCLIP [72] shows that random concept strings as bottlenecks can achieve similar performance gains in DCLIP [9]. We conduct an ablation study using the random strings provided by [72]. As shown in Tab. 8, since our model can distinguish between different rationales, the random strings deteriorate the prediction accuracy of our model. ", "page_idx": 8}, {"type": "text", "text": "4.7 Evaluation on Retrieval Tasks ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Zero-shot image-text retrieval: We evaluate our model on zero-shot image-text retrieval tasks. As shown in Tab. 6, the improved rationale correctness also benefits retrieval tasks. ", "page_idx": 8}, {"type": "text", "text": "Rationale-based text-to-image retrieval: To better evaluate the rationale correctness of our model, we conduct a novel retrieval task: rationale-based text-to-image retrieval. The model should retrieve the image with a specified rationale presented. As shown in Fig. 5, in contrast to the CLIP model [1] that entangles rationales with specific categories, our model precisely understands the semantic meaning of rationales independent to categories. ", "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Vision model explainability. A widely adopted branch of explainability methods post hoc generates heatmaps to identify the image regions most crucial to the model\u2019s predictions, e.g., GradCAM [41], LIME [7], and SHAP [73]. Although useful for revealing the correlations between inputs and outputs, such explanations might be ambiguous, and fail to correspond to high-level concepts that humans easily understand [8]. Methods like TCAV [74] curate attribute datasets to explain vision models using concepts familiar to humans. However, such methods can fail when the models do not learn these concepts [75]. Another branch of methods attempts to design specific architecture to intrinsically interpret model predictions, e.g., CBM [76] and ProtoPNet [77]. However, they cannot guarantee the model learns the semantic meanings of the concepts correctly [14] and yield compromised prediction accuracy [76]. Different from existing works, our method incorporates explanations to guide the model training, achieving accurate predictions backed by correct rationales. ", "page_idx": 8}, {"type": "image", "img_path": "ADV0Pzi3Ol/tmp/11fa40db3021cbd20efe6f8163ccf00471a4a8e38b3c0f545e7bcd85f02b0709.jpg", "img_caption": ["", "Figure 5: Qualitative results of zero-shot text-to-image retrieval on MSCOCO [66]. The task is to retrieve the top-5 images with a given rationale presented. The CLIP results reveal a significant entangle of rationales with a specific category, such as \u201clong neck\u201d with giraffes and \u201cwings\u201d with airliners. In contrast, our model treats rationales independently from categories, thus offering diverse retrieval results. For example, the \u201clong neck\u201d found in birds, giraffes, dears, and bottles. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Knowledge augmentation for vision-language models. Visual models often learn spurious correlations that stem from data biases unrelated to the causal explanation of interest [78, 79], whereas external knowledge allows models to learn the right features [77, 80]. Existing attempts for injecting knowledge into the models are often from the language modality. K-LITE [81] enrich the image caption using knowledge from WordNet [20] and Wikitionary [82]. NegCLIP [11] and DANCE [12] improve the commonsense understanding of CLIP by generating hard negative captions, the latter uses knowledge from ConceptNet [19]. StructureCLIP [83] leverages scene-graphs [21] to incorporate knowledge into text embeddings. However, our results (Tabs. 4& 7) reveal that solely augmenting information in the language cannot guarantee the model learning correct features. In contrast to these works, our method offers supervision signals from both modalities to ensure double-correctness. ", "page_idx": 9}, {"type": "text", "text": "Contrastive vision-language alignment. Different from conventional multimodal learning that fuses different modalities [84, 85], large-scale vison-language pretrained models, such as CLIP [1] and ALIGN [71], exhibit promising zero-shot transferability to downstream tasks. However, their global alignment objective is coarse, which only learns the existence of objects like bag-of-word while ignoring their localizations [11]. Recent attempts like PyramidCLIP [53] and X-VLM [54] leverage object region annotations to align word phrases with image regions. DeCLIP [56] and FILIP [57] align text with image regions through self-supervised learning. However, their supervision is limited to a coarse, object-level granularity. Different from these works, our method offers fine-grained, concept-level supervisory signals of rationales without expensive manual annotations. ", "page_idx": 9}, {"type": "text", "text": "6 Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While our study advances the double-correctness of predictions, it is not without limitations. First, the absence of explicit ground truth for rationale localization in large-scale datasets remains a significant challenge. We mitigated this by leveraging a self-supervised rationale disentanglement and localization method, but this approach depends heavily on the quality of the structured rationale ontologies. Second, our methods, though effective, are computationally intensive, which may limit their applicability in resource-constrained scenarios. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduce a new concept of double-correct predictions aimed at training vision-language foundation models to make accurate predictions backed by correct rationales, thereby enhancing their safety for real-world deployment. To support this, we establish a solid foundation for the development of double-correct predictions. Specifically, we develop a unique dataset with structured rationales that clearly outline the reasoning processes necessary for visual recognition tasks. Furthermore, we propose a principled rationale-informed optimization method tailored for double-correct prediction. Our comprehensive empirical evaluations demonstrate that our method significantly enhances the double correctness of vision-language model predictions. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the NSF CAREER Award No. 2340074, the NSF SAFE Award No. 2416937, the NSF III CORE Award No. 2412675, and the DoD DEPSCoR Award AFOSR FA9550- 23-1-0494. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the supporting entities. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023.   \n[3] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.   \n[4] Xiwen Liang, Yangxin Wu, Jianhua Han, Hang Xu, Chunjing Xu, and Xiaodan Liang. Effective adaptation in multi-task co-training for unified autonomous driving. In Advances in Neural Information Processing Systems, 2022.   \n[5] Xiwen Liang, Minzhe Niu, Jianhua Han, Hang Xu, Chunjing Xu, and Xiaodan Liang. Visual exemplar driven task-prompting for unified perception in autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9611\u20139621, 2023.   \n[6] Randy L Teach and Edward H Shortliffe. An analysis of physician attitudes regarding computerbased clinical consultation systems. Computers and Biomedical Research, 14(6):542\u2013558, 1981.   \n[7] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144, 2016.   \n[8] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206\u2013215, 2019.   \n[9] Sachit Menon and Carl Vondrick. Visual classification via description from large language models. In The Eleventh International Conference on Learning Representations, 2022.   \n[10] Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. In The Eleventh International Conference on Learning Representations, 2022.   \n[11] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2022.   \n[12] Shuquan Ye, Yujia Xie, Dongdong Chen, Yichong Xu, Lu Yuan, Chenguang Zhu, and Jing Liao. Improving commonsense in vision-language models via knowledge graph riddles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2634\u20132645, 2023.   \n[13] Tang Li, Mengmeng Ma, and Xi Peng. Deal: Disentangle and localize concept-level explanations for vlms. In European Conference on Computer Vision, pages 383\u2013401. Springer, 2025.   \n[14] Andrei Margeloiu, Matthew Ashman, Umang Bhatt, Yanzhi Chen, Mateja Jamnik, and Adrian Weller. Do concept bottleneck models learn as intended? ICLR Workshop, 2021.   \n[15] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9568\u20139578, 2024.   \n[16] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594\u2013611, 2006.   \n[17] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.   \n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[19] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.   \n[20] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341, 1995.   \n[21] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32\u201373, 2017.   \n[22] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data, 5(1):1\u20139, 2018.   \n[23] Ribana Roscher, Bastian Bohn, Marco F Duarte, and Jochen Garcke. Explainable machine learning for scientific insights and discoveries. Ieee Access, 8:42200\u201342216, 2020.   \n[24] Lawrence W Barsalou. Grounded cognition. Annu. Rev. Psychol., 59:617\u2013645, 2008.   \n[25] Jeffrey Mark Siskind. Grounding language in perception. Artificial Intelligence Review, 8:371\u2013 391, 1994.   \n[26] Roxana Daneshjou, Mert Yuksekgonul, Zhuo Ran Cai, Roberto Novoa, and James Y Zou. Skincon: A skin disease dataset densely annotated by domain experts for fine-grained debugging and analysis. Advances in Neural Information Processing Systems, 35:18157\u201318167, 2022.   \n[27] Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen. Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12275\u201312284, 2020.   \n[28] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey of approaches and applications. IEEE transactions on knowledge and data engineering, 29(12):2724\u20132743, 2017.   \n[29] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494\u2013514, 2021.   \n[30] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arxiv. arXiv preprint arXiv:2303.12712, 2023.   \n[31] Zhengliang Liu, Hanqi Jiang, Tianyang Zhong, Zihao Wu, Chong Ma, Yiwei Li, Xiaowei Yu, Yutong Zhang, Yi Pan, Peng Shu, et al. Holistic evaluation of gpt-4v for biomedical imaging. arXiv preprint arXiv:2312.05256, 2023.   \n[32] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n[33] Timothy R McIntosh, Tong Liu, Teo Susnjak, Paul Watters, Alex $\\mathrm{Ng}.$ , and Malka N Halgamuge. A culturally sensitive test to evaluate nuanced gpt hallucination. IEEE Transactions on Artificial Intelligence, 2023.   \n[34] Mika\u00ebl Chelli, Jules Descamps, Vincent Lavou\u00e9, Christophe Trojani, Michel Azar, Marcel Deckert, Jean-Luc Raynier, Gilles Clowez, Pascal Boileau, and Caroline Ruetsch-Chelli. Hallucination rates and reference accuracy of chatgpt and bard for systematic reviews: Comparative analysis. Journal of Medical Internet Research, 26:e53164, 2024.   \n[35] Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19187\u201319197, 2023.   \n[36] Ziyuan Qin, Hua Hui Yi, Qicheng Lao, and Kang Li. Medical image understanding with pretrained vision language models: A comprehensive study. In The Eleventh International Conference on Learning Representations, 2022.   \n[37] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.   \n[38] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16816\u201316825, 2022.   \n[39] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928, 2020.   \n[40] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 782\u2013791, 2021.   \n[41] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.   \n[42] Yibing Liu, Haoliang Li, Yangyang Guo, Chenqi Kong, Jing Li, and Shiqi Wang. Rethinking attention-model explainability through faithfulness violation test. In International Conference on Machine Learning, pages 13807\u201313824. PMLR, 2022.   \n[43] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021.   \n[44] Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. Interpreting clip\u2019s image representation via text-based decomposition. In The Twelfth International Conference on Learning Representations, 2023.   \n[45] Matthieu Guillaumin, Daniel K\u00fcttel, and Vittorio Ferrari. Imagenet auto-annotation with segmentation propagation. International Journal of Computer Vision, 110:328\u2013348, 2014.   \n[46] Alexander Binder, Gr\u00e9goire Montavon, Sebastian Lapuschkin, Klaus-Robert M\u00fcller, and Wojciech Samek. Layer-wise relevance propagation for neural networks with local renormalization layers. In Artificial Neural Networks and Machine Learning\u2013ICANN 2016: 25th International Conference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings, Part II 25, pages 63\u201371. Springer, 2016.   \n[47] Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi Peng. Are data-driven explanations robust against out-of-distribution data? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3821\u20133831, 2023.   \n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[49] Alexander Robey, George J Pappas, and Hamed Hassani. Model-based domain generalization. Advances in Neural Information Processing Systems, 34:20210\u201320229, 2021.   \n[50] Fengchun Qiao and Xi Peng. Topology-aware robust optimization for out-of-distribution generalization. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.   \n[51] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[52] Mengmeng Ma, Tang Li, and Xi Peng. Beyond the federation: Topology-aware federated learning for generalization to unseen clients. In Proceedings of the International Conference on Machine Learning (ICML), 2024.   \n[53] Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, and Chunhua Shen. Pyramidclip: Hierarchical feature alignment for vision-language model pretraining. Advances in neural information processing systems, 35:35959\u201335970, 2022.   \n[54] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. In International Conference on Machine Learning, pages 25994\u2013 26009. PMLR, 2022.   \n[55] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521\u20131528. IEEE, 2011.   \n[56] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. In International Conference on Learning Representations, 2022.   \n[57] Y. L. et al. Filip: Fine-grained interactive language-image pre-training. ICLR, 2022.   \n[58] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178\u2013178. IEEE, 2004.   \n[59] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012.   \n[60] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014.   \n[61] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.   \n[62] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561, 2013.   \n[63] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613, 2014.   \n[64] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[65] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.   \n[66] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.   \n[67] Oindrila Saha, Zezhou Cheng, and Subhransu Maji. Improving few-shot part segmentation using coarse supervision. In European Conference on Computer Vision, pages 283\u2013299. Springer, 2022.   \n[68] Ju He, Shuo Yang, Shaokang Yang, Adam Kortylewski, Xiaoding Yuan, Jie-Neng Chen, Shuai Liu, Cheng Yang, Qihang Yu, and Alan Yuille. Partimagenet: A large, high-quality dataset of parts. In European Conference on Computer Vision, pages 128\u2013145. Springer, 2022.   \n[69] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.   \n[70] R. S. et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. ICCV, 2017.   \n[71] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.   \n[72] Karsten Roth, Jae Myung Kim, A Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata. Waffilng around for performance: Visual classification with random words and broad concepts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15746\u2013 15757, 2023.   \n[73] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.   \n[74] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pages 2668\u20132677. PMLR, 2018.   \n[75] Reduan Achtibat, Maximilian Dreyer, Ilona Eisenbraun, Sebastian Bosse, Thomas Wiegand, Wojciech Samek, and Sebastian Lapuschkin. From\" where\" to\" what\": Towards human-understandable explanations through concept relevance propagation. arXiv preprint arXiv:2206.03208, 2022.   \n[76] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International conference on machine learning, pages 5338\u20135348. PMLR, 2020.   \n[77] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks like that: deep learning for interpretable image recognition. Advances in neural information processing systems, 32, 2019.   \n[78] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[79] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12556\u201312565, 2020.   \n[80] Tang Li, Jing Gao, and Xi Peng. Deep learning for spatiotemporal modeling of urbanization. Advances in Neural Information Processing Systems Workshops (Best Paper Award), 2021.   \n[81] S. S. et al. K-lite: Learning transferable visual models with external knowledge. NeurIPS, 2022.   \n[82] Christian M Meyer and Iryna Gurevych. Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography. na, 2012.   \n[83] Yufeng Huang, Jiji Tang, Zhuo Chen, Rongsheng Zhang, Xinfeng Zhang, Weijie Chen, Zeng Zhao, Zhou Zhao, Tangjie Lv, Zhipeng Hu, et al. Structure-clip: Towards scene graph knowledge to enhance multi-modal structured representations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2417\u20132425, 2024.   \n[84] Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. Smil: Multimodal learning with severely missing modality. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 2302\u20132310, 2021.   \n[85] Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, and Xi Peng. Are multimodal transformers robust to missing modality? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18177\u201318186, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our Prompt to Obtain Structured Rationales ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "American Robin $=$ { \"nodes\": [ {\"id\": \"American Robin\", \"label\": \"American Robin\"}, {\"id\": \"Breast\", \"label\": \"Breast\"}, {\"id\": \"Tail\", \"label\": \"Tail\"}, {\"id\": \"Beak\", \"label\": \"Beak\"}, {\"id\": \"Eyes\", \"label\": \"Eyes\"}, {\"id\": \"Red\", \"label\": \"Red\"}, {\"id\": \"Gray\", \"label\": \"Gray\"}, {\"id\": \"Yellow\", \"label\": \"Yellow\"}, {\"id\": \"Round\", \"label\": \"Round\"}, {\"id\": \"Long\", \"label\": \"Long\"} ], \"edges\": [ {\"source\": \"American Robin\", \"target\": \"Breast\", \"relation\": \"has\"}, {\"source\": \"American Robin\", \"target\": \"Tail\", \"relation\": \"has\"}, {\"source\": \"American Robin\", \"target\": \"Beak\", \"relation\": \"has\"}, {\"source\": \"American Robin\", \"target\": \"Eyes\", \"relation\": \"has\"}, {\"source\": \"Breast\", \"target\": \"Red\", \"relation\": \"is\"}, {\"source\": \"Tail\", \"target\": \"Gray\", \"relation\": \"is\"}, {\"source\": \"Beak\", \"target\": \"Yellow\", \"relation\": \"is\"}, {\"source\": \"Eyes\", \"target\": \"Round\", \"relation\": \"are\"}, {\"source\": \"Tail\", \"target\": \"Long\", \"relation\": \"is\"} ]   \n}   \nAirliner $=$ { \"nodes\": [ {\"id\": \"Airliner\", \"label\": \"Airliner\"}, {\"id\": \"Wings\", \"label\": \"Wings\"}, {\"id\": \"Tail\", \"label\": \"Tail\"}, {\"id\": \"Fuselage\", \"label\": \"Fuselage\"}, {\"id\": \"Engines\", \"label\": \"Engines\"}, {\"id\": \"Windows\", \"label\": \"Windows\"}, {\"id\": \"Logo\", \"label\": \"Logo\"}, {\"id\": \"Large\", \"label\": \"Large\"}, {\"id\": \"Horizontal stabilizer\", \"label\": \"Horizontal stabilizer\"}, {\"id\": \"Cylindrical\", \"label\": \"Cylindrical\"}, {\"id\": \"Under wings\", \"label\": \"Under wings\"}, {\"id\": \"Rowed\", \"label\": \"Rowed\"}, {\"id\": \"Tail fin\", \"label\": \"Tail fin\"} ], \"edges\": [ {\"source\": \"Airliner\", \"target\": \"Wings\", \"relation\": \"has\"}, {\"source\": \"Airliner\", \"target\": \"Tail\", \"relation\": \"has\"}, {\"source\": \"Airliner\", \"target\": \"Fuselage\", \"relation\": \"has\"}, {\"source\": \"Airliner\", \"target\": \"Engines\", \"relation\": \"has\"}, {\"source\": \"Airliner\", \"target\": \"Windows\", \"relation\": \"has\"}, {\"source\": \"Airliner\", \"target\": \"Logo\", \"relation\": \"has\"}, {\"source\": \"Wings\", \"target\": \"Large\", \"relation\": \"are\"}, {\"source\": \"Tail\", \"target\": \"Horiz. stabilizer\", \"relation\": \"has\"}, ", "page_idx": 16}, {"type": "text", "text": "{\"source\": \"Fuselage\", \"target\": \"Cylindrical\", \"relation\": \"is\"}, {\"source\": \"Engines\", \"target\": \"Under wings\", \"relation\": \"are\"}, {\"source\": \"Windows\", \"target\": \"Rowed\", \"relation\": \"are\"}, {\"source\": \"Tail\", \"target\": \"Tail fin\", \"relation\": \"has\"} ] ", "page_idx": 17}, {"type": "text", "text": "What are useful visual concepts for distinguishing a {category_name} in a photo? These features should be visually distinctable and have limited overlap with each other. These features should include attributes and their relations. For each item, you should be concise and precise, and use no more than five words. No ambiguous answers. Show your answer using a tree structure in JSON format strictly following the examples shown above. Only contains two depths of nodes (depth 1: attributes, depth 2: subattributes). No connections between node with the same depth. Do not contain a node without an edge connected to it. No other explanations, only provide the graph. ", "page_idx": 17}, {"type": "text", "text": "B Machine Evaluation on Full Dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 9: The machine evaluation results on the quality of the full rationale dataset. ", "page_idx": 17}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/30fa3b5c0937dee5f6d6ceb78260dc1f516f3ee2d74a69e92f4e5d0eaf23978d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Full Table ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Table 10: Evaluation on PartImageNet [68] with ground truth region of parts using ViT-B/32 vision encoder. We summarize the annotated parts for different categories into 13 common parts. We apply thresholds to the explanation heatmaps and calculate their mIoU with ground truth masks. Our model improves the localization accuracy of each part, even though they appear significantly different across categories, such as \u201cwings\u201d for birds and airliners. ", "page_idx": 17}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/0e80cffef2a2fb2286b4bd3fd409c2f218b01e3a04c09ff52db9b668973461a1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "ADV0Pzi3Ol/tmp/2a3b8a95a8db0cc2e62112e5ea4375b89812a815e9f0754ce11743658ab82264.jpg", "table_caption": ["Table 11: Datasets for classification task. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The abstract and introduction clearly outline the main claims of the paper, delineating both the theoretical and experimental contributions, which are supported by the results presented. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper comprehensively discusses the limitations of the proposed methods, including robustness against violations of underlying assumptions and scalability concerns. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper provides a detailed presentation of full assumptions and definitions, as presented in Secs. 2& 3. Each equation and its definitions are clearly numbered and cross-referenced. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper fully discloses all necessary details for reproducing the main experimental results, including comprehensive descriptions of the methodologies, experimental setups, and parameter settings. In addition, the code and specific datasets are provided as well. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \nWhile NeurIPS does not require releasing code, the conference does require all submis  \nsions to provide some reasonable avenue for reproducibility, which may depend on the   \nnature of the contribution. For example   \n(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper provides an Anonymous GitHub link with open access to both the data and code used in the experiments, complete with detailed instructions in the supplemental material that enable faithful reproduction of the main experimental results. This includes exact commands, necessary environment details, and scripts for preprocessing data, ensuring that other researchers can replicate the study\u2019s findings without ambiguity. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper details all aspects of the experimental settings, including data splits, hyperparameter selection processes, and the types of optimizers used. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 21}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper provides statistical measures such as error bars and confidence intervals for all major experimental results, such as Tab. 7. These measures are correctly defined, and the paper details the variability factors they capture, including train/test splits and initialization randomness. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper adequately details the computational resources required for each experiment, including the types of compute workers (CPU or GPU), memory specifications, and execution times. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The research presented in the paper adheres fully to the NeurIPS Code of Ethics, ensuring ethical considerations are addressed and complied with throughout the study. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper effectively discusses both the potential positive and negative societal impacts of the research conducted. It acknowledges the beneftis of the proposed technology in enhancing data-driven decision-making processes while also addressing possible negative implications, such as unsafe predictions. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper outlines comprehensive safeguards for the responsible release of data and models, particularly those with potential for high misuse. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper properly credits the creators and original owners of all assets used, including datasets, models, and code. Each asset is clearly cited, with references to the original sources and explicit mention of licenses and terms of use. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The new assets introduced in the paper, including datasets and models, are well documented with comprehensive details provided in structured templates. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve any experiments or research activities that include crowdsourcing or direct interactions with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 24}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve any experiments or research activities that include crowdsourcing or direct interactions with human subjects. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]