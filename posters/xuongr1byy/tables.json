[{"figure_path": "xUoNgR1Byy/tables/tables_7_1.jpg", "caption": "Table 2: Eleven randomly sampled tokens and their predicted sentiment from GPT-Neo-125m compared with the sentiment values in the VADER lexicon that determined the reward during RLHF.", "description": "This table compares the sentiment scores predicted by the GPT-Neo-125m language model for eleven randomly selected tokens against their true sentiment scores from the VADER lexicon. The VADER lexicon scores were used to determine the reward during reinforcement learning from human feedback (RLHF) fine-tuning of the language model.  The comparison shows the model's accuracy in capturing the sentiment expressed in the tokens used for training.", "section": "4.1 Measuring the accuracy of LFPs"}, {"figure_path": "xUoNgR1Byy/tables/tables_7_2.jpg", "caption": "Table 3: The percentage accuracy of the logistic regression probes at predicting fine-tuning feedback from condensed LLM activations. LLMs tagged with 'HH' were trained to behave like helpful assistant using the Anthropic-HH dataset. LLMs tagged with 'toxic' were trained for toxicity using the dpo-toxic dataset.", "description": "This table presents the accuracy of logistic regression probes in predicting the feedback signal implicit in the activations of fine-tuned LLMs.  The probes were trained on condensed representations of LLM activations. The table is separated by two tasks:  'HH' representing helpfulness, and 'toxic' representing toxicity.  The accuracy is reported for four different LLMs (Pythia-70m, Pythia-160m, GPT-Neo-125m, Gemma-2b) for each task.", "section": "3.3 Probe training"}, {"figure_path": "xUoNgR1Byy/tables/tables_7_3.jpg", "caption": "Table 4: Kendall Tau correlation coefficient between the feedback signal implicit in LLM activations and the true feedback signal over many outputs. This comprises our measurement of the accuracy of LFPs for the controlled sentiment generation task, which we denote as 'VADER' in the table.", "description": "This table presents the Kendall Tau correlation coefficient and p-value, measuring the correlation between the feedback signal predicted by the probes and the actual feedback from the VADER lexicon for the controlled sentiment generation task.  The results show the accuracy of the Learned Feedback Patterns (LFPs) in predicting the fine-tuning feedback for different language models: Pythia-70m, Pythia-160m, GPT-Neo-125m. A baseline model (untrained linear regression model) is also included for comparison.", "section": "4.1 Measuring the accuracy of LFPs"}, {"figure_path": "xUoNgR1Byy/tables/tables_8_1.jpg", "caption": "Table 5: We measure how accurately the predictions of the VADER probes are the correct sign to the labels in the VADER lexicon. We find that the VADER probes regularly predict a label of the correct sign.", "description": "This table presents the accuracy of VADER probes in predicting the correct sign (positive or negative) of sentiment scores.  It shows the percentage of positive and negative words from the VADER lexicon for which the probes' predictions matched the correct sign. The results indicate a high degree of accuracy in predicting the direction of sentiment, even if the precise magnitude is not always perfectly matched.", "section": "4.1 Measuring the accuracy of LFPs"}, {"figure_path": "xUoNgR1Byy/tables/tables_9_1.jpg", "caption": "Table 6: Performance before and after the ablation of features identified to be related to the LFPs of a fine-tuned LLM as measured by the average reward of 1000 completions to thirty token prefixes for the base and fine-tuned models.", "description": "This table shows the average reward of 1000 completions to 30-token prefixes for three different LLMs (Pythia-70m, Pythia-160m, and GPT-Neo-125m) before and after ablating features identified as being related to their Learned Feedback Patterns (LFPs). The \"VADER\" task refers to the controlled sentiment generation task using the VADER lexicon for reward assignment.  The results indicate the impact of removing features associated with LFPs on model performance in sentiment generation.", "section": "4.2 Probe validation"}, {"figure_path": "xUoNgR1Byy/tables/tables_9_2.jpg", "caption": "Table 7: The frequency of activation for features in inputs predicted to have an activation delta of > 3 by our probes. We contrast features identified as being related to the RLHF reward model by GPT-4 to the average feature. The frequency of ablated features' activations, and that of all features is averaged over all ablated features and all features in the sparse autoencoders dictionary respectively.", "description": "This table presents the frequency of activation for features identified as related to the RLHF reward model by GPT-4 and those identified by the probes. It compares the frequency of activation for ablated features (those removed from the model) with the average frequency of activation for all features in the sparse autoencoder dictionary. This comparison helps validate the probes' ability to identify features associated with the LFPs.", "section": "4.2 Probe validation"}, {"figure_path": "xUoNgR1Byy/tables/tables_13_1.jpg", "caption": "Table 1: Five GPT-4 generated descriptions of features in a sparse autoencoder trained on an LLM for a task detailed in Appendix B sampled from Table 8. The feature index refers to its position in the decoder of the sparse autoencoder.", "description": "This table presents five examples of features identified by a sparse autoencoder trained on an LLM and the descriptions of those features generated by GPT-4.  Each row shows the layer the feature is in, the feature's index within that layer, and GPT-4's description of the patterns the feature represents.  These descriptions provide insights into the human-interpretable aspects of the learned patterns within the LLM's activations.", "section": "4.1 Measuring the accuracy of LFPs"}, {"figure_path": "xUoNgR1Byy/tables/tables_14_1.jpg", "caption": "Table 1: Five GPT-4 generated descriptions of features in a sparse autoencoder trained on an LLM for a task detailed in Appendix B sampled from Table 8. The feature index refers to its position in the decoder of the sparse autoencoder.", "description": "This table presents five examples of feature descriptions generated by GPT-4 for features in a sparse autoencoder trained on an LLM. Each description is associated with a layer and feature index within the autoencoder's decoder, providing context for understanding the feature's role in the model's processing of information related to a specific task (detailed in Appendix B). The table offers insight into the interpretability of the autoencoder's learned features and their relevance to the task's underlying concept.", "section": "4.1 Measuring the accuracy of LFPs"}, {"figure_path": "xUoNgR1Byy/tables/tables_15_1.jpg", "caption": "Table 1: Five GPT-4 generated descriptions of features in a sparse autoencoder trained on an LLM for a task detailed in Appendix B sampled from Table 8. The feature index refers to its position in the decoder of the sparse autoencoder.", "description": "This table presents five examples of feature descriptions generated by GPT-4 for features identified in a sparse autoencoder trained on an LLM. Each description explains a specific feature's role in the LLM's processing of the input text. The table also includes the layer and feature index of each described feature, providing context within the autoencoder's architecture.", "section": "4.1 Measuring the accuracy of LFPs"}, {"figure_path": "xUoNgR1Byy/tables/tables_17_1.jpg", "caption": "Table 9: Thirty tokens and their reconstructed sentiment values compared with their original sentiment values from GPT-Neo-125m.", "description": "This table presents a comparison of sentiment scores. For thirty tokens, it shows the sentiment value as reconstructed by a model and the true sentiment value from the GPT-Neo-125m model.  The purpose is to illustrate the accuracy of the model's reconstruction of sentiment.", "section": "D Reconstruction of the VADER lexicon from the fine-tuned model"}, {"figure_path": "xUoNgR1Byy/tables/tables_17_2.jpg", "caption": "Table 10: Kendall Tau correlation of our probes predictions and RLHF reward model for all tested LLMs and negative tokens only.", "description": "This table presents the results of calculating the Kendall Tau correlation between the predictions of the probes (which estimate the implicit feedback signal from LLM activations) and the true RLHF reward model, specifically focusing on negative tokens only. The Kendall Tau correlation measures the rank correlation between two ranked sets, indicating the strength of the monotonic relationship between the probe's predictions and the actual reward values. The p-values associated with each correlation coefficient show the statistical significance of the observed correlations.", "section": "4.2 Probe validation"}, {"figure_path": "xUoNgR1Byy/tables/tables_18_1.jpg", "caption": "Table 11: Normalized reconstruction and scaled true sparsity losses for Pythia-70m and Pythia-160m over 1 training epoch, over differing choices of whether to tie encoder and decoder weights. Both metrics are averaged over all highly divergent layers, and hyperparameter choices are otherwise as described in \u00a73.2.", "description": "This table presents the results of an experiment comparing the performance of sparse autoencoders with tied and untied encoder and decoder weights.  The experiment measures two metrics: normalized reconstruction loss and scaled true sparsity loss. The results are averaged across highly divergent layers in the models Pythia-70m and Pythia-160m, providing insights into the effects of weight tying on model performance and sparsity.", "section": "F Methodology for autoencoder training"}, {"figure_path": "xUoNgR1Byy/tables/tables_18_2.jpg", "caption": "Table 11: Normalized reconstruction and scaled true sparsity losses for Pythia-70m and Pythia-160m over 1 training epoch, over differing choices of whether to tie encoder and decoder weights. Both metrics are averaged over all highly divergent layers, and hyperparameter choices are otherwise as described in \u00a73.2.", "description": "This table presents the results of an experiment comparing the performance of sparse autoencoders with tied and untied encoder/decoder weights.  The metrics reported are normalized reconstruction loss and scaled true sparsity loss, averaged across all highly divergent layers in the models.  The experiment aimed to determine the optimal weight-tying strategy for these autoencoders, which were used in a larger study of learned feedback patterns in large language models.", "section": "F Methodology for autoencoder training"}]