[{"figure_path": "JO6T4rEJ32/figures/figures_3_1.jpg", "caption": "Figure 1: Schematic of proposed attractor model. In MEC, the gi are residue representations in grid modules, and c encodes a context label. Input of velocity estimate q(v) can produce path integration in grid modules via binding, denoted by \u22c5. In HC, p represents contextualized place. Binding serves two roles in the MEC/HC interaction (symbolized by bidirectional arrows): a) factorizing p into gi's, and b) generating an update of p from the gi's, for example, after path integration. In LEC, s represents sensory input, interacting with p through a learned heteroassociative projection.", "description": "This figure shows a schematic of the proposed attractor model for the hippocampal formation.  It illustrates the interaction between the medial entorhinal cortex (MEC), hippocampus (HC), and lateral entorhinal cortex (LEC).  MEC grid modules (Gi) represent individual residues (gi) of a spatial position, with a context label (c).  The hippocampus (HC) stores the contextualized spatial position (p).  Binding (\u22c5) combines these residues into the overall position.  The model shows how velocity (q(v)) influences path integration and sensory inputs (s) from LEC interact with the hippocampal representation.", "section": "2.3 Mapping the model to the HF"}, {"figure_path": "JO6T4rEJ32/figures/figures_4_1.jpg", "caption": "Figure 2: Residue number systems, combined with a modular attractor network (resonator network), result in a new kind of attractor neural network with favorable scaling for a large combinatorial range. A) Number of encoding states, M, grows rapidly in the number of modules, up to a maximum established by Landau's function (black dots). B) Coefficient of coding range, M, scales roughly as O(D\u03b1K), depending on the number of moduli, K, but with \u03b1K > 1. C) Estimation of scaling from slopes of linear regression (fit to log-log scale). Higher values of K require a higher dimension to achieve a particular coding range; empirical values are close to \u03b1K = K\u22121.", "description": "This figure demonstrates the favorable scaling properties of the proposed model for representing space. Panel A shows the exponential growth of the coding range (number of states) with the number of modules (K), reaching a maximum determined by Landau's function. Panel B illustrates the superlinear scaling relationship between the coding range and vector dimension (D) for different numbers of modules. Panel C shows the superlinear scaling coefficients (\u03b1K) for different numbers of modules, highlighting that these values are close to K\u22121.", "section": "Coding properties of the model"}, {"figure_path": "JO6T4rEJ32/figures/figures_5_1.jpg", "caption": "Figure 3: Recovery of encoded positions is robust to multiple kinds of noise. A) Visualization of the von Mises weight distribution. Note that the magnitude of the noise is inversely proportional to \u03ba, and that the variance of the phase perturbation is much larger than the distance between the discrete states of phasors. B-D) Visualizations of accuracy as a function of coding range and K for three separate cases: input noise (B), update noise (C), and codebook noise (D). Cases are shown in order of increasing difficulty. The resonator network maintains perfect accuracy up to a point, after which accuracy decays at an earlier point than the noiseless dynamics (black curve).", "description": "This figure demonstrates the robustness of the proposed attractor network model to various types of noise.  Panel A shows the von Mises distribution used to model the noise. Panels B, C, and D illustrate the accuracy of the model in recovering encoded positions across a range of coding ranges and noise levels (parameterized by \u03ba). Different noise types (input, update, and codebook noise) are evaluated, demonstrating that the model remains accurate despite the presence of noise, though performance degrades as the noise level increases.", "section": "3 Coding properties of the model"}, {"figure_path": "JO6T4rEJ32/figures/figures_6_1.jpg", "caption": "Figure 4: Smooth interpolation between integer states enables encoding and decoding of sub-integer values. A) Visualization of interpolation between two integer states. The position of the fractional value can be estimated by fitting a periodic sinc function (Appendix A.1) based on the inner products with integer codebooks (visualized in dots), then finding the location of the peak. B, C) Sub-integer states can be be decoded, up to a precision set by the noise level. Note that in both cases, sub-integer decoding can be just as accurate as integer decoding for the same range, even though the sub-integer decoding problem is strictly harder. Even k = 4 is sufficient to achieve accuracy within a precision of Ax = 0.07, but for higher noise (k = 2), the precision is worse. D) The best spatial precision (in bits) that can be decoded for a fixed noise level. Representations with less noise achieve both a higher coding range and higher information content per vector.", "description": "This figure demonstrates the ability of the model to represent and decode sub-integer values, showing that the network smoothly interpolates between integer states.  Panel A visualizes this interpolation using a sinc function fit to inner products. Panels B and C show how the accuracy of sub-integer decoding depends on both the coding range and the noise level (\u03ba). Panel D quantifies the information content (in bits) achievable at different noise levels and coding ranges.", "section": "Coding properties of the model"}, {"figure_path": "JO6T4rEJ32/figures/figures_6_2.jpg", "caption": "Figure 5: Hexagonal coding improves spatial resolution. A) Voronoi tessellation for m = 5. Each distinct color corresponds to a unique codeword in CD. Black arrows show the coordinate axes of the triangular \u2018Mercedes-Benz\u2019 frame in 2D. B) Hexagonal lattices have higher entropy than square lattices, allowing each state to carry higher resolution in its spatial output.", "description": "This figure shows that hexagonal lattices provide higher spatial resolution than square or oblique lattices. Panel A shows a Voronoi tessellation for m=5 where each color represents a unique codeword. Black arrows indicate the coordinate axes of the triangular frame in 2D. Panel B shows that hexagonal lattices achieve higher entropy than square or oblique lattices.", "section": "3 Coding properties of the model"}, {"figure_path": "JO6T4rEJ32/figures/figures_7_1.jpg", "caption": "Figure 6: Attractor dynamics facilitate robust path integration. A) Example of path integration of a 2D trajectory in the case of intrinsic input noise on the place cell representation. The grid cell modules correct the noise that would otherwise induce drift after a short period of time. B) Path integration results averaged over multiple trajectories in the case of intrinsic input noise on the place cell representation. Grid cell modules limit noise accumulation along the trajectory. Solid lines report the median error over 100 trials, with shaded intervals reporting 25th and 75th percentile. C) Simulated trajectory, along which colors represent the similarity between the gi of three different modules and vectors representing each position in the environment. We see hexagonal response fields, similar to those obtained from single unit recordings of MEC. D) Sensory patterns (symbolized by red dots), representing visual cues, are associated to positions in the environment. Presentation of visual cues helps correct drifted positions due to extrinsic noise.", "description": "This figure demonstrates the model's ability to perform path integration in the presence of noise. Panel A shows an example of path integration with intrinsic noise, highlighting how grid cell modules correct for the noise and prevent drift. Panel B shows the average path integration error over multiple trials with and without grid cell error correction, demonstrating the effectiveness of the correction. Panel C visualizes the hexagonal response fields of the grid cell modules, similar to what's observed experimentally. Finally, Panel D illustrates how sensory input helps to correct for path integration errors caused by extrinsic noise.", "section": "Testing functionalities of the model"}, {"figure_path": "JO6T4rEJ32/figures/figures_8_1.jpg", "caption": "Figure 7: Heteroassociation enables recovery of sensory patterns under corruption and superposition. A) Accuracy for denoising 60 different random binary patterns for different vector dimension D. The dotted line is the average similarity between the decoded and ground truth patterns. B) Same experiment as in panel A, but with 210 different possible binary patterns. The accuracy is lower on average. C) Accuracy for denoising multiple patterns from a single input. This task is especially challenging, because sums of patterns combined in this way interfere with each other in retrieval (a phenomenon known as cross-talk noise). However, the compositional structure of our modular attractor network enables multiple patterns to be decoded with high probability.", "description": "This figure demonstrates the robustness of the heteroassociative memory model to noise and pattern superposition.  Panel A shows the accuracy of denoising 60 random binary patterns with varying levels of corruption and different vector dimensions.  Panel B repeats the experiment with 210 patterns, showing a decrease in average accuracy. Panel C shows the accuracy when attempting to decode multiple patterns superimposed on a single input, highlighting the model's ability to handle 'cross-talk noise' due to its compositional structure.", "section": "Denoising sensory states via a heteroassociative memory"}, {"figure_path": "JO6T4rEJ32/figures/figures_19_1.jpg", "caption": "Figure S1: Response field visualization of 4 different gi in 3 different modules mi = 3, 5 and 7. For a fixed module, the response fields appear as translated versions of one another.", "description": "This figure visualizes the response fields of grid cells from different modules in the model.  It shows that for a given module, the response fields are translations of each other, meaning that the same pattern is repeated across the spatial field. This supports the model's representation of space as a modular system.", "section": "C.1 Further visualizations of grid cell modules"}, {"figure_path": "JO6T4rEJ32/figures/figures_20_1.jpg", "caption": "Figure S2: Remapping of place cells depending on context. The global remapping observed in the model response fields is similar to the findings of an experimental study of attractor network dynamics in hippocampus [81].", "description": "This figure shows the remapping of place cells in different contexts. The color intensity represents the firing rate of each place cell in each context.  The results are consistent with experimental studies of attractor network dynamics in the hippocampus, demonstrating the model's ability to simulate this complex phenomenon.", "section": "C Additional results"}, {"figure_path": "JO6T4rEJ32/figures/figures_21_1.jpg", "caption": "Figure 6: Attractor dynamics facilitate robust path integration. A) Example of path integration of a 2D trajectory in the case of intrinsic input noise on the place cell representation. The grid cell modules correct the noise that would otherwise induce drift after a short period of time. B) Path integration results averaged over multiple trajectories in the case of intrinsic input noise on the place cell representation. Grid cell modules limit noise accumulation along the trajectory. Solid lines report the median error over 100 trials, with shaded intervals reporting 25th and 75th percentile. C) Simulated trajectory, along which colors represent the similarity between the g\u2081 of three different modules and vectors representing each position in the environment. We see hexagonal response fields, similar to those obtained from single unit recordings of MEC. D) Sensory patterns (symbolized by red dots), representing visual cues, are associated to positions in the environment. Presentation of visual cues helps correct drifted positions due to extrinsic noise.", "description": "This figure demonstrates the model's ability to perform robust path integration in the presence of noise. Panel A shows an example of path integration with intrinsic noise, highlighting how grid cell modules correct for drift. Panel B presents averaged results over multiple trajectories, showing the effectiveness of grid cells in limiting noise accumulation. Panel C visualizes hexagonal response fields, similar to those observed in MEC recordings, along a simulated trajectory. Finally, Panel D illustrates how incorporating sensory cues (visual cues) helps correct for positional drift caused by extrinsic noise.", "section": "Testing functionalities of the model"}, {"figure_path": "JO6T4rEJ32/figures/figures_21_2.jpg", "caption": "Figure 2: Residue number systems, combined with a modular attractor network (resonator network), result in a new kind of attractor neural network with favorable scaling for a large combinatorial range. A) Number of encoding states, M, grows rapidly in the number of modules, up to a maximum established by Landau's function (black dots). B) Coefficient of coding range, M, scales roughly as O(D\u03b1K), depending on the number of moduli, K, but with \u03b1K > 1. C) Estimation of scaling from slopes of linear regression (fit to log-log scale). Higher values of K require a higher dimension to achieve a particular coding range; empirical values are close to \u03b1K = K\u22121.", "description": "This figure demonstrates the benefits of combining residue number systems (RNS) with a modular attractor network.  Panel A shows that the number of possible encoded states (M) increases exponentially with the number of modules (K), reaching a limit set by Landau's function.  Panel B illustrates that the coding range scales superlinearly with the dimension of the vectors representing each module (D), indicated by the exponent \u03b1K which is greater than 1. This superlinear scaling means that a larger coding range can be achieved with a relatively lower-dimensional representation. Panel C further supports the superlinear scaling by showing the relationship between dimension, number of modules, and coding range using a log-log plot.", "section": "Coding properties of the model"}, {"figure_path": "JO6T4rEJ32/figures/figures_22_1.jpg", "caption": "Figure 5: Flexible sequence retrieval via path integration in a conceptual space. A) An example of a hexagonal lattice with sensory observations associated with different states. Having knowledge of the underlying graph enables generalization to new trajectories in the space [6, 56]. B) Accuracy of random binary pattern retrieval as a function of position in the sequence for a fixed error rate and one context tag. The noiseless case achieves perfect accuracy, but errors accumulate after incorrect sequence predictions. C) Same as B), but with the additional task of inferring the context tag.", "description": "This figure demonstrates the model's ability to perform sequence retrieval via path integration in a conceptual space. Panel A shows an example of a hexagonal lattice where each node represents a state and sensory observations are associated with those states.  Panel B shows the accuracy of retrieving random binary patterns, demonstrating that accuracy decreases as the sequence position increases (with noise).  Panel C shows that the model can successfully infer the context tag when retrieving the sequence.", "section": "C.3 Storing and retracing sequences"}, {"figure_path": "JO6T4rEJ32/figures/figures_22_2.jpg", "caption": "Figure 6: Attractor dynamics facilitate robust path integration. A) Example of path integration of a 2D trajectory in the case of intrinsic input noise on the place cell representation. The grid cell modules correct the noise that would otherwise induce drift after a short period of time. B) Path integration results averaged over multiple trajectories in the case of intrinsic input noise on the place cell representation. Grid cell modules limit noise accumulation along the trajectory. Solid lines report the median error over 100 trials, with shaded intervals reporting 25th and 75th percentile. C) Simulated trajectory, along which colors represent the similarity between the g\u2081 of three different modules and vectors representing each position in the environment. We see hexagonal response fields, similar to those obtained from single unit recordings of MEC. D) Sensory patterns (symbolized by red dots), representing visual cues, are associated to positions in the environment. Presentation of visual cues helps correct drifted positions due to extrinsic noise.", "description": "This figure demonstrates the model's ability to perform robust path integration, even in the presence of noise. Panel A shows an example trajectory with and without the attractor dynamics, highlighting the noise correction provided by the grid cells. Panel B quantifies this effect by averaging over multiple trials and showing median error with 25th and 75th percentiles. Panel C visualizes the hexagonal response fields of grid cells, demonstrating that the model replicates experimental observations. Panel D illustrates how sensory cues can improve the accuracy of path integration.", "section": "4 Testing functionalities of the model"}]