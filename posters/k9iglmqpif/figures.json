[{"figure_path": "K9IGlMQpif/figures/figures_1_1.jpg", "caption": "Figure 1: Existing data selection methods depend heavily on the feature representations from a reference model, which makes their effectiveness vulnerable to the quality of training on the target domain [34]. For supervised fine-tuning (SFT), while pretrained models can effectively separate topics (shown in different colors) in natural language (Figure 1a), they struggle with fine-tuning data that deviates from the pretraining distribution (Figure 1b). Additionally, the cost of training a reference model escalates with model size (Figure 1c), making existing data selection methods for large models prohibitively expensive.", "description": "This figure illustrates the limitations of existing data selection methods.  Panel (a) shows that pre-trained models can effectively separate topics in natural language. However, panel (b) demonstrates that these methods struggle when fine-tuning data deviates from the pre-training distribution. Finally, panel (c) highlights the escalating computational cost of training a reference model as its size increases, making it impractical for large models.", "section": "Introduction"}, {"figure_path": "K9IGlMQpif/figures/figures_1_2.jpg", "caption": "Figure 1: Existing data selection methods depend heavily on the feature representations from a reference model, which makes their effectiveness vulnerable to the quality of training on the target domain [34]. For supervised fine-tuning (SFT), while pretrained models can effectively separate topics (shown in different colors) in natural language (Figure 1a), they struggle with fine-tuning data that deviates from the pretraining distribution (Figure 1b). Additionally, the cost of training a reference model escalates with model size (Figure 1c), making existing data selection methods for large models prohibitively expensive.", "description": "This figure illustrates the limitations of existing data selection methods.  Panel (a) shows that pre-trained models effectively separate topics in natural language. However, panel (b) demonstrates that these methods struggle when fine-tuning data deviates from the pre-training distribution.  Finally, panel (c) highlights the significant increase in training time as model size increases, rendering these methods impractical for large models.", "section": "1 Introduction"}, {"figure_path": "K9IGlMQpif/figures/figures_1_3.jpg", "caption": "Figure 1: Existing data selection methods depend heavily on the feature representations from a reference model, which makes their effectiveness vulnerable to the quality of training on the target domain [34]. For supervised fine-tuning (SFT), while pretrained models can effectively separate topics (shown in different colors) in natural language (Figure 1a), they struggle with fine-tuning data that deviates from the pretraining distribution (Figure 1b). Additionally, the cost of training a reference model escalates with model size (Figure 1c), making existing data selection methods for large models prohibitively expensive.", "description": "This figure illustrates the limitations of existing data selection methods for large language models (LLMs).  Panel (a) shows that pre-trained models can effectively separate topics in natural language. However, panel (b) demonstrates that these methods struggle when fine-tuning data deviates from the pre-training distribution. Finally, panel (c) highlights the high computational cost of training a reference model, especially for large LLMs, making existing data selection methods prohibitively expensive.", "section": "1 Introduction"}, {"figure_path": "K9IGlMQpif/figures/figures_4_1.jpg", "caption": "Figure 2: Examples in the same clusters have very similar loss trajectories (Figure 2a) while the loss trajectories of examples in different clusters are very different (Figure 2b).", "description": "This figure shows a comparison of loss trajectories for examples within the same cluster versus those in different clusters.  The left panel (2a) illustrates that examples within the same cluster exhibit highly similar loss trajectories during training, indicating similar gradient updates.  The right panel (2b), in contrast, shows examples from different clusters, demonstrating significant differences in their loss trajectories over training iterations. This visual representation supports the paper's claim that similar loss trajectories imply similar gradient behavior, which forms the basis for their data selection algorithm.", "section": "4 Methodology"}, {"figure_path": "K9IGlMQpif/figures/figures_4_2.jpg", "caption": "Figure 2: Examples in the same clusters have very similar loss trajectories (Figure 2a) while the loss trajectories of examples in different clusters are very different (Figure 2b).", "description": "This figure shows two plots visualizing the loss trajectories of examples during training.  (a) demonstrates examples clustered together, which exhibit almost identical loss trajectories across training iterations, highlighting the effectiveness of clustering similar examples together based on their loss. In contrast, (b) illustrates loss trajectories of examples in different clusters, demonstrating their dissimilarity and the rationale for selecting examples from a variety of clusters.", "section": "4 Methodology"}, {"figure_path": "K9IGlMQpif/figures/figures_5_1.jpg", "caption": "Figure 5: Wall-clock time required to train the reference model and select 100K data from MathInstruct for training Pythia-410M.", "description": "This figure compares the wall-clock time required for training the reference model and performing data selection using two different methods: Facility Location and S2L (the proposed method).  The x-axis represents the method used, while the y-axis indicates the time in hours. The figure shows that S2L is significantly faster than Facility Location, requiring substantially less time for both training the reference model and selecting the data.", "section": "5.2.1 Setting 1: Less Data for Better Models"}, {"figure_path": "K9IGlMQpif/figures/figures_6_1.jpg", "caption": "Figure 4: Data Scaling: Accuracies (\u2191) on in-domain and out-of-domain datasets using Pythia-410M. Data size refers to the total number of unique training examples used. All experiments in this table share the same total training steps and learning rate schedule (see Section 5.2). See breakdowns in Figure 14.", "description": "This figure compares the performance of S2L against several baseline data selection methods across various data sizes using the Pythia-410M model.  It shows that S2L consistently outperforms baselines, particularly with smaller datasets, demonstrating improved data efficiency. The results are shown as relative accuracy compared to training on the full dataset, highlighting S2L's superior performance even with a small fraction of the data.", "section": "5.2 Specialized Domain 1: Mathematical Reasoning"}, {"figure_path": "K9IGlMQpif/figures/figures_6_2.jpg", "caption": "Figure 6: Distribution of the coverage of top-1 topic in each cluster. Taller bars on the right end of the plot indicate clusters with a higher concentration of a single topic and therefore suggest a grouping of similar examples.", "description": "This figure compares the distribution of the most common topic within clusters generated using two different methods: Loss Trajectories (the proposed method S2L) and Fully-finetuned Embeddings (a standard method). The x-axis represents the fraction of the most common topic in each cluster, and the y-axis shows the number of clusters. The figure demonstrates that S2L produces clusters with a higher concentration of a single topic than the Fully-finetuned Embeddings method. This suggests that S2L is more effective at grouping similar examples together during training.", "section": "5.2.3 Why is S2L So Effective?"}, {"figure_path": "K9IGlMQpif/figures/figures_8_1.jpg", "caption": "Figure 7: Performance (\u2191) of models trained on (1) randomly selected 30K examples, (2) S2L selected 30K examples, and (3) full 61K examples (none) evaluated with 3 different metrics. The minimum value on the y-axis is the performance of the model before fine-tuning. S2L improves the data efficiency for the clinical text summarization task by outperforming training on the full dataset with only less than half of the data.", "description": "This figure compares the performance of models trained on randomly selected data, data selected using the SMALLTOLARGE (S2L) method, and the full dataset for clinical text summarization.  The results are evaluated using three metrics: BLEU, ROUGE-L, and BERTScore.  The figure shows that S2L achieves comparable or better performance than using the full dataset, while only using less than half the data, demonstrating the method's improved data efficiency.", "section": "5.3 Specialized Domain 2: Clinical Text Summarization"}, {"figure_path": "K9IGlMQpif/figures/figures_8_2.jpg", "caption": "Figure 8: S2L is robust to the length of training trajectories.", "description": "This figure shows the robustness of S2L to different lengths of training trajectories.  The results show that while longer trajectories generally perform slightly better, S2L remains effective even with shorter trajectories, demonstrating its resilience to variations in data preprocessing or computational constraints.", "section": "5.4 Ablation Studies"}, {"figure_path": "K9IGlMQpif/figures/figures_9_1.jpg", "caption": "Figure 14: Break-down accuracies (\u2191) on in-domain and out-of-domain datasets using Pythia-410M. Data size refers to the total number of unique training examples used. All experiments in this table share the same total training steps and learning rate schedule (see Section 5.2).", "description": "This figure displays the performance comparison across six datasets (three in-domain and three out-of-domain) using different data sizes (38% and 76% of the full dataset). It compares the performance of S2L against several baseline data selection methods.  The results show that S2L consistently outperforms the baselines, even when using less training data, demonstrating its efficiency.", "section": "5.2.1 Setting 1: Less Data for Better Models"}, {"figure_path": "K9IGlMQpif/figures/figures_9_2.jpg", "caption": "Figure 13: Per-dataset and average accuracy comparison between using different proxy models (Pythia-160M and GPT-2 (124M)) for data selection. Using both proxy models show comparable performance, demonstrating the effectiveness of different small models as reference models for S2L.", "description": "This figure compares the performance of S2L when using different small proxy models (Pythia-160M and GPT-2 (124M)).  The results for six different datasets (GSM8K, MATH, NumGLUE, SVAMP, Mathematics, SimulEq) are shown individually, along with an average across all six.  The comparison demonstrates the robustness of S2L to the choice of proxy model.", "section": "5.4 Ablation Studies"}, {"figure_path": "K9IGlMQpif/figures/figures_22_1.jpg", "caption": "Figure 4: Data Scaling: Accuracies (\u2191) on in-domain and out-of-domain datasets using Pythia-410M. Data size refers to the total number of unique training examples used. All experiments in this table share the same total training steps and learning rate schedule (see Section 5.2). See breakdowns in Figure 14.", "description": "This figure compares the performance of different data selection methods (Random Sampling, Least Confidence, Middle Perplexity, High Learnability, Facility Locations, Confidence Curriculum, and SMALLTOLARGE) against training on the full dataset when using Pythia-410M model. The y-axis represents the relative accuracy to the full dataset, and the x-axis represents the proportion of the data used for training (38%, 76%).  The results show that S2L significantly outperforms the other methods, especially with smaller datasets.", "section": "5.2 Specialized Domain 1: Mathematical Reasoning"}, {"figure_path": "K9IGlMQpif/figures/figures_24_1.jpg", "caption": "Figure 4: Data Scaling: Accuracies (\u2191) on in-domain and out-of-domain datasets using Pythia-410M. Data size refers to the total number of unique training examples used. All experiments in this table share the same total training steps and learning rate schedule (see Section 5.2). See breakdowns in Figure 14.", "description": "This figure shows the performance of different data selection methods on the Pythia-410M model for mathematical reasoning, comparing in-domain and out-of-domain datasets.  It demonstrates that the proposed S2L method significantly outperforms other methods, particularly when the training data is limited.  The y-axis represents relative accuracy compared to using the full dataset, and the x-axis shows the fraction of the training data used.", "section": "5.2 Specialized Domain 1: Mathematical Reasoning"}, {"figure_path": "K9IGlMQpif/figures/figures_24_2.jpg", "caption": "Figure 18: Compared to the original topic distribution, S2L prioritized easier topics (e.g., pre-algebra over intermediate algebra, algebra over other more advanced topics) while always ensuring complete and more balanced coverage of all topics.", "description": "This figure compares the topic distribution of the original MathInstruct dataset and the topic distributions of subsets of data selected by the SMALLTOLARGE (S2L) algorithm for different data sizes (30K, 50K, 100K).  It shows that S2L tends to prioritize simpler topics (like pre-algebra) while ensuring a more even representation of all topics compared to a random selection or the full dataset.", "section": "5.2.3 Why is S2L So Effective?"}, {"figure_path": "K9IGlMQpif/figures/figures_24_3.jpg", "caption": "Figure 1: Existing data selection methods depend heavily on the feature representations from a reference model, which makes their effectiveness vulnerable to the quality of training on the target domain [34]. For supervised fine-tuning (SFT), while pretrained models can effectively separate topics (shown in different colors) in natural language (Figure 1a), they struggle with fine-tuning data that deviates from the pretraining distribution (Figure 1b). Additionally, the cost of training a reference model escalates with model size (Figure 1c), making existing data selection methods for large models prohibitively expensive.", "description": "This figure illustrates the limitations of existing data selection methods.  Part (a) shows that pretrained models can effectively separate topics in natural language. However, part (b) demonstrates that these methods struggle when fine-tuning data deviates from the pretraining distribution.  Finally, part (c) highlights the high computational cost of training a reference model, especially for large models, making existing methods impractical.", "section": "1 Introduction"}, {"figure_path": "K9IGlMQpif/figures/figures_24_4.jpg", "caption": "Figure 18: Compared to the original topic distribution, S2L prioritized easier topics (e.g., pre-algebra over intermediate algebra, algebra over other more advanced topics) while always ensuring complete and more balanced coverage of all topics.", "description": "This figure shows a comparison of topic distributions in the MathInstruct dataset.  The bar chart (a) displays the original topic distribution within the full dataset. Charts (b) through (d) show the distributions of topics after data selection using the SMALLTOLARGE (S2L) method with different data budget sizes (30K, 50K, and 100K data points).  The S2L method is shown to shift the emphasis from more advanced topics (like calculus) towards easier topics (like pre-algebra), resulting in a more balanced distribution across topics.", "section": "5.2.3 Why is S2L So Effective?"}]