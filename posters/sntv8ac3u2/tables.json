[{"figure_path": "sntv8Ac3U2/tables/tables_7_1.jpg", "caption": "Table 1: User study on the qualitative preference of images/condition pairs generated by the FG-DM and SD+CEM, using 10 unique human evaluators. A. denotes (prompt) Adherence and Q. denotes Quality.", "description": "This table presents the results of a user study comparing the qualitative performance of the FG-DM and a combination of Stable Diffusion (SD) and a Condition Extraction Model (CEM).  Ten human evaluators assessed image and condition quality, as well as prompt adherence for both methods. The FG-DM significantly outperforms SD+CEM in all four categories.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_7_2.jpg", "caption": "Table 2: Ablation of attention distillation loss for T2I synthesis on COCO validation for FG-DM. FID reported for Images/Conditions.", "description": "This table presents the results of an ablation study evaluating the impact of the attention distillation loss on the quality of text-to-image (T2I) synthesis using the Factor Graph Diffusion Model (FG-DM). The experiment was performed on the COCO validation dataset.  The table shows FID scores (Frechet Inception Distance) for both the generated images and conditions, with and without the attention distillation loss.  Lower FID values generally indicate higher quality. The results demonstrate the effectiveness of the attention distillation loss in improving the fidelity of the generated conditions and images.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_7_3.jpg", "caption": "Table 3: Object recall statistics for sampling FG-DM with different seeds and timesteps on the ADE20K validation set prompts.", "description": "This table presents the results of an experiment evaluating the object recall performance of the Factor Graph Diffusion Model (FG-DM) using different sampling strategies. The experiment varies the number of timesteps (t) in the diffusion process and the number of different seeds (N) used for sampling. The results are reported as average minimum, maximum, and median recall values across 2000 prompts in the ADE20K validation set.  The table also shows the average number of images in a batch that satisfy object recall thresholds of 0.5, 0.75, and 0.9, as well as the average time taken per sampling batch.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_7_4.jpg", "caption": "Table 4: Quantitative comparison of Object Recall for different models and configurations on the ADE20K validation set prompts.", "description": "This table compares the object recall performance of different models and sampling strategies on the ADE20K validation set.  It contrasts Stable Diffusion (SD) with and without multiple sampling (N=10), and compares both with the Attend & Excite (A-E) method, and the proposed Factor Graph Diffusion Model (FG-DM) with different numbers of sampling timesteps (t). The table highlights FG-DM's superior recall and efficiency compared to SD and A-E, especially when using multiple samples to enhance prompt compliance.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_7_5.jpg", "caption": "Table 5: Quantitative comparison of Object Recall for different models and configurations on the ADE20K validation set prompts.", "description": "This table compares the object recall performance of different models and configurations on the ADE20K validation set prompts.  It contrasts Stable Diffusion (SD) with the proposed Factor Graph Diffusion Model (FG-DM), showing the improvements in recall achieved by FG-DM, especially when using multiple sampling seeds. The table also highlights the significant reduction in inference time achieved by FG-DM compared to SD, particularly when employing multiple seeds.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_19_1.jpg", "caption": "Table 6: Comparison of segmentation mask quality and throughput of FG-DM trained on ADE20K dataset against state-of-the-art conventional segmentation models of similar size. The FG-DM samples higher quality masks with only 10 DDIM steps, comparable to the throughput of Segformer-B5 but with superior quality as shown in the FID, Precision and Recall metrics.", "description": "This table compares the performance of FG-DM (trained on the ADE20K dataset) against the state-of-the-art SegFormer-B5 model for semantic segmentation.  The metrics used for comparison are FID (Frechet Inception Distance), LPIPS (Learned Perceptual Image Patch Similarity), Precision, Recall, and throughput (images per second).  The results show that FG-DM achieves comparable throughput to SegFormer-B5 while exhibiting superior quality (lower FID and higher LPIPS) using significantly fewer DDIM (Denoising Diffusion Implicit Models) steps.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_20_1.jpg", "caption": "Table 7: Ablation study on Image Synthesis by FG-DM trained separately and jointly with segmentation factor. FG-DM results presented as Images/Semantic maps.", "description": "This table presents the results of an ablation study comparing the performance of FG-DMs trained separately and jointly with a segmentation factor.  The study evaluates image synthesis quality and semantic segmentation performance across four datasets: MM-CelebA-HQ, ADE-20K, Cityscapes, and COCO-Stuff. The metrics reported include FID (Fr\u00e9chet Inception Distance), LPIPS (Learned Perceptual Image Patch Similarity), Precision, and Recall.  The comparison highlights the impact of joint training on the overall performance of the model. ", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_20_2.jpg", "caption": "Table 8: Ablation study on data augmentation with synthetic data generated by the FG-DM for facial part segmentation on MM-CelebA-HQ(22) dataset.", "description": "This table presents the results of an ablation study on the impact of data augmentation using synthetic data generated by the FG-DM for facial part segmentation.  It shows the performance of a model trained on the original MM-CelebA-HQ dataset compared to models trained with additional synthetic data generated by the FG-DM. The metrics used to evaluate performance are mIoU, frequency-weighted mIoU (F.W. mIoU), and F1-score. The results indicate whether adding synthetic data improves the performance of the model for facial part segmentation.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_20_3.jpg", "caption": "Table 9: Ablation study on data augmentation with synthetic data generated by the FG-DM for face landmark estimation on 300W(43) dataset.", "description": "This table presents the results of an ablation study evaluating the impact of data augmentation using synthetic data generated by the FG-DM on face landmark estimation performance.  The study used the 300W dataset. Three different scenarios are compared: using only the original dataset, augmenting with 1000 synthetic samples, and augmenting with 2000 synthetic samples. The performance is measured using the Normalized Mean Error (NME), a common metric for evaluating face landmark localization accuracy. The table breaks down the NME for three landmark difficulty levels: 'Common', 'Full', and 'Challenge'.  Lower NME values indicate better performance.", "section": "A.1.2 Models trained from scratch: Ablation Studies"}, {"figure_path": "sntv8Ac3U2/tables/tables_21_1.jpg", "caption": "Table 11: Ablation on the order of generated semantic map (S) and pose (P) conditions on CelebA-HQ (Top) and COCO (Bottom). I - Image.", "description": "This table presents an ablation study on the order of generating semantic maps (S) and pose (P) within the FG-DM framework.  It compares two different orders: (P\u2192S\u2192I) where pose is generated first, followed by semantics, and finally the image; and (S\u2192P\u2192I) where the order is reversed.  The results show that different orders significantly impact the FID and LPIPS scores, especially on the CelebA-HQ dataset, suggesting an optimal sequence for generating these conditions to maximize image quality and alignment with the prompts. The metrics used are FID (lower is better) and LPIPS (higher is better).  Precision (P) and Recall (R) are also given.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_21_2.jpg", "caption": "Table 10: Comparison of FG-DM conditioning vs concatenation approach for joint synthesis on CelebA-HQ. U-LDM reported for reference.", "description": "This table compares the performance of FG-DM against a joint synthesis approach using concatenation and U-LDM.  The FG-DM model demonstrates superior results (lower FID) while having a significantly smaller number of parameters.", "section": "4 Experimental Results"}, {"figure_path": "sntv8Ac3U2/tables/tables_23_1.jpg", "caption": "Table 13: Ablation study on Image Alignment with the segmentation masks by FG-DM trained separately and jointly.", "description": "This table presents the results of an ablation study comparing the image alignment performance of FG-DMs trained using separate and joint training methods.  The metric used is mIoU (mean Intersection over Union) and frequency weighted mIoU (f.w. IoU), which measure the degree of overlap between the generated images and their corresponding ground truth segmentation masks.  The results are shown for four different datasets: MM-CelebA, Cityscapes, ADE20K, and COCO.  The table helps assess the impact of joint training on the alignment of generated images with their segmentation masks across various datasets.", "section": "A.2.5 Evaluation Details"}, {"figure_path": "sntv8Ac3U2/tables/tables_24_1.jpg", "caption": "Table 14: Hyperparameter Settings for the FG-DMs trained from scratch on MM-CelebA-HQ, ADE-20K, Cityscapes and COCO datasets. We use an image resolution of 256 x 512 for Cityscapes and 256 x 256 for the others.", "description": "This table details the hyperparameters used for training the Factor Graph Diffusion Models (FG-DMs) from scratch on four different datasets: MM-CelebA-HQ, ADE20K, Cityscapes, and COCO.  It shows the hyperparameters for each dataset, highlighting differences in image resolution (256x512 for Cityscapes, 256x256 for others), network architecture details (f, z-shape, |Z|, channels, depth, channel multiplier, attention resolutions, head channels), training settings (optimizer, noise schedule, batch size, iterations, learning rate), and the total number of parameters (Nparams).", "section": "A.2.3 Hyperparameter Settings"}]