[{"figure_path": "xaqPAkJnAS/tables/tables_7_1.jpg", "caption": "Table 1: Quantitative results (%) on node clustering. The top 3 highest results are highlighted with red boldface, red color and boldface, respectively. The symbol \u201cOOM\u201d means out of memory.", "description": "This table presents the results of node clustering experiments performed on four real-world benchmark multiplex graph datasets (ACM, DBLP, Yelp, and MAG).  It compares the performance of InfoMGF (with two variants) against several other state-of-the-art unsupervised clustering methods (VGAE, DGI, O2MAC, MvAGC, MCGC, HDMI, MGDCR, DMG, and BTGF). The results are evaluated using four metrics: Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Accuracy (ACC), and F1-score. The top three highest results for each metric and dataset are highlighted.", "section": "4.2 Effectiveness Analysis (RQ1)"}, {"figure_path": "xaqPAkJnAS/tables/tables_7_2.jpg", "caption": "Table 2: Quantitative results with standard deviation (% \u00b1 \u03c3) on node classification. Available data for GSL during training is shown in the first column, supervised methods depend on Y for GSL. The symbol \"-\" indicates that the method is structure-fixed, which does not learn a new structure.", "description": "This table presents the results of node classification experiments using various methods.  It compares the performance of supervised and unsupervised methods, with and without the use of graph structure learning.  The \"Available Data for GSL\" column indicates whether the method used labeled data during training, highlighting the difference between supervised and unsupervised approaches.  The Macro-F1 and Micro-F1 scores are used as evaluation metrics.  The table shows that the proposed InfoMGF-LA method outperforms other methods across different datasets.", "section": "4.2 Effectiveness Analysis (RQ1)"}, {"figure_path": "xaqPAkJnAS/tables/tables_8_1.jpg", "caption": "Table 3: Performance (% \u00b1 \u03c3) of InfoMGF and its variants.", "description": "This table presents the ablation study results of the InfoMGF model. It shows the performance of InfoMGF and its variants (without maximizing shared task-relevant information, without maximizing unique task-relevant information, without graph augmentation, and without reconstruction loss) on three benchmark datasets (ACM, DBLP, and Yelp) in terms of Macro-F1 and Micro-F1 scores for node classification.  It helps to understand the contribution of each component in the InfoMGF framework.", "section": "4.4 Ablation Study (RQ3)"}, {"figure_path": "xaqPAkJnAS/tables/tables_15_1.jpg", "caption": "Table 1: Quantitative results (%) on node clustering. The top 3 highest results are highlighted with red boldface, red color and boldface, respectively. The symbol \u201cOOM\u201d means out of memory.", "description": "This table presents the quantitative results of node clustering experiments conducted on four benchmark datasets (ACM, DBLP, Yelp, and MAG).  The results are shown for various unsupervised node clustering methods.  The table displays performance metrics (NMI, ARI, ACC, F1) for each method on each dataset. The top three performing methods for each dataset are highlighted. 'OOM' indicates that the method ran out of memory.", "section": "4.2 Effectiveness Analysis (RQ1)"}, {"figure_path": "xaqPAkJnAS/tables/tables_21_1.jpg", "caption": "Table 1: Quantitative results (%) on node clustering. The top 3 highest results are highlighted with red boldface, red color and boldface, respectively. The symbol \u201cOOM\u201d means out of memory.", "description": "This table presents the quantitative results of node clustering experiments on four different datasets using various methods.  The results are expressed as percentages and are broken down by dataset and method.  The top 3 performing methods for each dataset are highlighted.  The abbreviation 'OOM' stands for 'out of memory', indicating that the experiment could not complete due to insufficient memory resources.", "section": "4.2 Effectiveness Analysis (RQ1)"}, {"figure_path": "xaqPAkJnAS/tables/tables_22_1.jpg", "caption": "Table 6: Details of the hyper-parameters settings.", "description": "This table shows the hyperparameter settings used for the InfoMGF model across four different datasets (ACM, DBLP, Yelp, and MAG).  It details the number of epochs (E), learning rate (lr), hidden dimension (dh), representation dimension (d), number of nearest neighbors (k), aggregation order (r), number of layers (L), random feature masking probability (\u03c1), temperature parameter (Tc), random edge dropping probability (Ps), generator learning rate (lrgen), Gumbel-Max temperature (T), and hyperparameter \u03bb for InfoMGF-LA. Note that for the MAG dataset, the random edge dropping probability (\u03c1) is 0 as the generative augmentation method is used. ", "section": "4.2 Effectiveness Analysis (RQ1)"}]