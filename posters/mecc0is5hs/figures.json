[{"figure_path": "MeCC0Is5hs/figures/figures_1_1.jpg", "caption": "Figure 1: The proposed architecture consists of three modules: a Denoising Network d\u0259 (xt, \u00f5t), an IR Network or (y) and a Fusion Network (x,x,t). A small version of MIRNet [81] is used as the Denoising Network, while a pre-trained SwinIR [42] or BSRT [50] or FFTFormer [34] is used as the IR Network, depending on the IR task. See section 3.3 for a detailed description.", "description": "This figure shows the proposed modular architecture for conditional diffusion-based image reconstruction. It consists of three main parts: an Image Restoration Network (pre-trained model), a Denoising Network (MIRNet), and a Fusion Network (a small newly-trained module).  The Image Restoration Network processes the low-quality input image (y). The Denoising Network takes a noisy version of the high-quality image (xt) as input and the Fusion network combines the outputs of these two networks to estimate the high-quality image (xo). The timestep (t) is also part of the process, influencing the noise level in the sampling.", "section": "3.3 Proposed Network Architecture"}, {"figure_path": "MeCC0Is5hs/figures/figures_5_1.jpg", "caption": "Figure 2: Forward and reverse diffusion process. Blue solid arrows: transitions at the forward pass with sampling distribution from eq. (1). Dashed arrow: cumulative transition probability from eq. (2). Black solid arrows: transitions at the backward pass with the sampling distribution from eq. (3). Red solid arrow: closed-form cumulative transition probability from eq. (8) representing our accelerated sampling.", "description": "This figure illustrates the forward and reverse diffusion processes used in the proposed conditional diffusion model.  The forward process adds noise to the original image (x0) over many timesteps, until a completely noisy image (xT) is reached. The reverse process then aims to reconstruct the original image from this noisy version, but with the addition of a conditioning step at time \u03c4 that uses an IR network estimate to improve efficiency and quality.  The figure highlights the accelerated sampling strategy using the closed-form cumulative transition probability (red arrow), which significantly reduces the number of steps required for image reconstruction.", "section": "3 Proposed Conditional Diffusion Model"}, {"figure_path": "MeCC0Is5hs/figures/figures_7_1.jpg", "caption": "Figure 3: Visual comparisons on the GoPro test set for the task of dynamic scene deblurring (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure shows visual comparisons of different dynamic scene deblurring methods on the GoPro test set.  Each output image is accompanied by its LPIPS (Learned Perceptual Image Patch Similarity) value, a metric quantifying the perceptual difference between the generated and ground truth images. Lower LPIPS values indicate higher perceptual similarity.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_8_1.jpg", "caption": "Figure 4: Visual comparisons on the DIV2K validation set for the task of 4\u00d7 bicubic super-resolution (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure compares visual results of different super-resolution (SR) methods on images from the DIV2K validation set.  Each row shows the input low-resolution image, followed by the results from several SR methods (SwinIR, HCFlow, ESRGAN, InDI, HAT, LDM, SRDiff), and finally the ground truth high-resolution image. The LPIPS (Learned Perceptual Image Patch Similarity) score, a metric that measures perceptual similarity, is provided below each output image.  The figure highlights the visual quality improvements achieved by the proposed method compared to existing methods.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_24_1.jpg", "caption": "Figure 1: The proposed architecture consists of three modules: a Denoising Network d\u0259 (xt, \u00f5t), an IR Network or (y) and a Fusion Network (x,x,t). A small version of MIRNet [81] is used as the Denoising Network, while a pre-trained SwinIR [42] or BSRT [50] or FFTFormer [34] is used as the IR Network, depending on the IR task. See section 3.3 for a detailed description.", "description": "This figure illustrates the modular architecture of the proposed conditional diffusion probabilistic image restoration framework (DP-IR).  It shows three main components:\n\n1.  **Denoising Network:** Uses a smaller version of MIRNet to estimate E[x0|xt] (the expected original image given the noisy image at timestep t).\n2. **IR Network:** Employs a pre-trained network (SwinIR, BSRT, or FFTformer depending on the specific image restoration task) to estimate E[x0|y] (the expected original image given the observed low-quality image y).\n3. **Fusion Network:** Combines the outputs of the Denoising and IR Networks, along with the timestep t, to produce the final conditional expectation E[x0|y, xt] which is used for the image reconstruction process.", "section": "3.3 Proposed Network Architecture"}, {"figure_path": "MeCC0Is5hs/figures/figures_26_1.jpg", "caption": "Figure 6: Visual comparison of our approach against competing methods on the Burst JDD-SR task (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure compares the visual results of different super-resolution methods on a sample image from the Burst JDD-SR dataset.  The leftmost image shows the ground truth (Target) and is followed by results from DBSR, DeepRep, BSRT-Small, BSRT-Large, BIPNet, EBSR, and finally the proposed method (Ours). Each result image is accompanied by its LPIPS (Learned Perceptual Image Patch Similarity) score which quantifies the perceptual difference between the reconstruction and the ground truth.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_26_2.jpg", "caption": "Figure 6: Visual comparison of our approach against competing methods on the Burst JDD-SR task (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure compares the results of the proposed method against several state-of-the-art methods for Burst Joint Demosaicking and Super-Resolution (JDD-SR).  It highlights perceptual differences by showing the LPIPS score of each reconstruction alongside a visual comparison of the reconstructed images against the ground truth. The lower LPIPS value indicates a higher perceptual similarity to the ground truth image.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_26_3.jpg", "caption": "Figure 1: The proposed architecture consists of three modules: a Denoising Network \u03d5D\u03b8D(x\u02dct, \u03c3\u02dct), an IR Network \u03d5IR\u03b8IR(y) and a Fusion Network \u03d5F\u03b8F(xIR0, xD0, t). A small version of MIRNet [81] is used as the Denoising Network, while a pre-trained SwinIR [42] or BSRT [50] or FFTFormer [34] is used as the IR Network, depending on the IR task. See section 3.3 for a detailed description.", "description": "This figure illustrates the modular architecture of the proposed conditional diffusion probabilistic image reconstruction framework (DP-IR).  It shows three main components: a Denoising Network (a modified MIRNet), an Image Restoration Network (a pre-trained SwinIR, BSRT, or FFTFormer, depending on the task), and a Fusion Network that combines the outputs of the first two networks. The Fusion Network is a relatively small module (0.7M parameters) trained for a specific image reconstruction task. The figure details the structure of the networks, showing convolutional, ReLU, and dense layers.", "section": "3 Proposed Conditional Diffusion Model"}, {"figure_path": "MeCC0Is5hs/figures/figures_27_1.jpg", "caption": "Figure 6: Visual comparison of our approach against competing methods on the Burst JDD-SR task (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure compares the visual quality of image reconstruction results produced by different methods on the Burst JDD-SR task.  The leftmost image shows the input low-quality burst image with a highlighted region of interest.  Subsequent images display the reconstruction results from DBSR, DeepRep, BSRT-Small, BSRT-Large, BIPNet, EBSR, and the proposed method. Each reconstructed image is accompanied by its Learned Perceptual Image Patch Similarity (LPIPS) score, a metric that measures perceptual similarity to the ground truth image. Lower LPIPS scores indicate better perceptual quality.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_27_2.jpg", "caption": "Figure 3: Visual comparisons on the GoPro test set for the task of dynamic scene deblurring (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure shows a visual comparison of different dynamic scene deblurring methods on the GoPro test set.  The input blurry image and ground truth are shown, along with results from several state-of-the-art methods (HINet, MPRNet, NAFNet, Restormer, DeblurGANv2, DvSR, InDI) and the proposed method (Ours).  Each result image includes its LPIPS (Learned Perceptual Image Patch Similarity) score, which quantifies perceptual differences between the generated and ground truth images. Lower LPIPS scores indicate better perceptual quality.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_27_3.jpg", "caption": "Figure 3: Visual comparisons on the GoPro test set for the task of dynamic scene deblurring (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure compares visual results of different dynamic scene deblurring methods on the GoPro test dataset.  Each image shows a deblurred result alongside its LPIPS score (a perceptual metric measuring the difference between two images). The goal is to demonstrate the visual quality of the proposed method compared to existing state-of-the-art techniques.  Lower LPIPS values indicate higher visual similarity to the ground truth.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_27_4.jpg", "caption": "Figure 3: Visual comparisons on the GoPro test set for the task of dynamic scene deblurring (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure provides a visual comparison of different image deblurring methods on the GoPro dataset.  The input image shows a blurry scene of a street, and the subsequent images show the results from different methods. Each result includes its LPIPS (Learned Perceptual Image Patch Similarity) score, a metric that quantifies perceptual differences between images.  Lower LPIPS scores indicate a better visual quality compared to the ground truth image.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_28_1.jpg", "caption": "Figure 3: Visual comparisons on the GoPro test set for the task of dynamic scene deblurring (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure shows visual comparison results of different dynamic scene deblurring methods on GoPro test dataset.  Each result image includes LPIPS score to quantify the perceptual quality of the deblurred image.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_28_2.jpg", "caption": "Figure 1: The proposed architecture consists of three modules: a Denoising Network d\u0259 (xt, \u00f5t), an IR Network or (y) and a Fusion Network (x,x,t). A small version of MIRNet [81] is used as the Denoising Network, while a pre-trained SwinIR [42] or BSRT [50] or FFTFormer [34] is used as the IR Network, depending on the IR task. See section 3.3 for a detailed description.", "description": "This figure shows the architecture of the proposed modular conditional diffusion framework for image reconstruction. It consists of three main modules: a Denoising Network, an Image Restoration Network, and a Fusion Network. The Denoising Network is based on MIRNet and aims to estimate E[x0|xt]. The Image Restoration Network uses pre-trained models such as SwinIR, BSRT, or FFTformer to estimate E[x0|y]. The Fusion Network combines the outputs of the two previous modules to estimate E[x0|y,xt], which is then used to generate the final reconstructed image. The choice of pre-trained Image Restoration Network depends on the specific image reconstruction task.", "section": "3 Proposed Conditional Diffusion Model"}, {"figure_path": "MeCC0Is5hs/figures/figures_28_3.jpg", "caption": "Figure 4: Visual comparisons on the DIV2K validation set for the task of 4\u00d7 bicubic super-resolution (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure compares the visual results of different super-resolution methods on a sample image from the DIV2K validation set.  The input is a low-resolution image, and the \"Target\" shows the ground truth high-resolution image.  The other images represent the output of various state-of-the-art super-resolution methods, including the authors' proposed approach.  Each output image is accompanied by its Learned Perceptual Image Patch Similarity (LPIPS) score, which quantifies perceptual differences between the output and the target. Lower LPIPS values indicate better perceptual quality.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_28_4.jpg", "caption": "Figure 4: Visual comparisons on the DIV2K validation set for the task of 4\u00d7 bicubic super-resolution (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure compares visual results of different super-resolution (SR) methods on images from the DIV2K validation set.  Each column shows the input low-resolution image, outputs from several SR methods (SwinIR, HCFlow, ESRGAN, InDI, HAT, LDM, SRDiff), and the ground truth high-resolution image.  The LPIPS (Learned Perceptual Image Patch Similarity) score, a perceptual quality metric, is provided for each output to quantify the visual similarity to the ground truth.", "section": "4.1 Results"}, {"figure_path": "MeCC0Is5hs/figures/figures_29_1.jpg", "caption": "Figure 17: Visual comparison of our approach against competing methods on the DIV2K validation set for the task of 4\u00d7 super-resolution (best viewed by zooming in). Every output image is accompanied by its LPIPS value.", "description": "This figure compares the visual results of the proposed method against several state-of-the-art super-resolution methods on the DIV2K validation dataset.  Each image is accompanied by its Learned Perceptual Image Patch Similarity (LPIPS) score, which measures perceptual similarity to the ground truth image. The LPIPS values indicate the visual quality of the reconstruction results.", "section": "4.1 Results"}]