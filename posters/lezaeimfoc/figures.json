[{"figure_path": "LezAEImfoc/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the STDChallenge, depicting the absent of targets and shotcut. STDChallenge is quite challenging, but CPDTrack can maintain robust tracking performance, demonstrating stronger visual search abilities compared to other trackers. (a) In the first column, most trackers fail; in the second column, they can recover; in the third column, a STDChallenge occurs; in the fourth column, all trackers except for CPDTrack fail; and in the fifth column, this remains the case, meaning trackers' limited recovery. (b) shows the status of the target within the sequence of \"095\" from VideoCube [4], and red dots means shotcut.", "description": "This figure illustrates the challenges of the STDChallenge benchmark for visual object trackers.  The top row shows examples of video frames from the benchmark highlighting different scenarios, including instances where trackers fail (absent targets and abrupt changes or shotcut). The bottom row shows a graph plotting the tracker status ('present' or 'absent') over time for a specific sequence, highlighting the superiority of the proposed CPDTrack (in maintaining consistent tracking even under challenging conditions).", "section": "1 Introduction"}, {"figure_path": "LezAEImfoc/figures/figures_2_1.jpg", "caption": "Figure 2: Comparison of tracking pipeline. Our CPDTrack differs from previous trackers in motion model. (a) The core components of mainstream tracking frameworks consist of a motion model, feature extraction, and a temporal module. (b-1) local tracker, which tracks targets in local areas and has difficulty recovering after failure; (b-2) global tracker, which tracks targets globally in the current frame, is susceptible to background interference and has lower efficiency; (b-3) local-global tracker, which can switch between the above two, depending on the performance of the local tracker; (b-4) our CPDTrack, which can model both central and peripheral information simultaneously.", "description": "This figure compares different tracking pipeline architectures, highlighting the differences in motion models.  It shows that mainstream trackers typically use either a local approach (tracking only a small area around the target), a global approach (tracking the entire image), or a hybrid approach that switches between these two.  The authors' proposed CPDTrack is presented as a superior alternative that combines central and peripheral vision for improved robustness and efficiency.", "section": "A new algorithm CPDTrack"}, {"figure_path": "LezAEImfoc/figures/figures_4_1.jpg", "caption": "Figure 3: The overall architecture of CPDTrack, referencing the latest one-stream trackers, models CPD. (a) The encoding-selection-decoding framework of CPD. Visual selection is the process of choosing the information to focus on, and visual decoding is the process of deeply understanding the selected features and information to make cognitive decisions. On the left is a mathematical model of visual selection, where we align human visual information processing with the resolution of devices and machines. (b) Architecture of the proposed CPDTrack: The original frame is treated as encoded visual data. We apply the acuity model described in (a) for visual selecting, which can be modulated by information query from temporal. A transformer is employed to replicate the decoding processes occurring in higher brain regions, facilitating complex cognitive tasks. The grey arrows running through both parts highlight the correspondence between the two parts.", "description": "This figure illustrates the architecture of CPDTrack, a new tracker inspired by the Central-Peripheral Dichotomy (CPD) theory of human vision.  The (a) part shows the encoding, selection, and decoding framework of CPD, highlighting how human vision processes information. The (b) part details the CPDTrack architecture, dividing the input frame into central and peripheral vision.  Central vision uses spatio-temporal information for precise localization, while peripheral vision enhances global awareness.  A transformer processes this information, mimicking higher-level cognitive functions.", "section": "A new algorithm CPDTrack"}, {"figure_path": "LezAEImfoc/figures/figures_6_1.jpg", "caption": "Figure 4: Quantitative indicators show that CPDTrack not only has higher accuracy in the STDChallenge but also behaves more similarly to humans. (a) represents the N-PRE score on STDChallenge-Turing. (b) represents the distribution of N-PRE scores of humans and machines on various sequences of STDChallenge-Turing. (c) represents the error consistency between machines and humans, with kappa representing the average error consistency.", "description": "This figure presents a quantitative comparison of CPDTrack's performance against other trackers and humans in the STDChallenge-Turing benchmark.  It uses three key metrics: (a) the N-PRE score (Normalized Precision), showing CPDTrack's superior performance; (b) boxplots illustrating the distribution of N-PRE scores across various sequences, highlighting CPDTrack's consistent performance and greater similarity to human performance; and (c) a scatter plot showcasing the error consistency (kappa coefficient) between different trackers and humans, further demonstrating CPDTrack's closer alignment with human visual search behavior.", "section": "5 Experiments"}, {"figure_path": "LezAEImfoc/figures/figures_6_2.jpg", "caption": "Figure 5: Human results do not necessarily mean correct, but humans can usually quickly re-locate the target after STDChallenge. In the upper row, humans can recognize environmental factors closely related to the target in the second image. In the lower row, even if the target is absent, humans are not distracted by the background in the second image. Humans are robust to occlusions in the fifth image.", "description": "This figure shows example results from humans and several trackers on the STDChallenge sequences.  The top row demonstrates that even when the target is temporarily absent (indicated by \"absent\") or there's a shotcut (indicated by \"shotcut\"), humans are able to successfully re-identify and track the target based on contextual understanding. They can leverage environmental cues to track the target even after loss or shotcut.  The bottom row highlights the robustness of human visual tracking in the face of occlusions; humans can successfully locate the target even when it is partially obscured.  The figure contrasts the human's performance to that of several state-of-the-art trackers, which show a higher failure rate in these challenging scenarios.", "section": "5 Experiments"}, {"figure_path": "LezAEImfoc/figures/figures_8_1.jpg", "caption": "Figure 6: We further emphasize the visual search capabilities of CPDTrack, especially in challenging STDChallenge scenarios. (a) performance of SUC as the STD changes. We have listed the SUC scores on the toughest 100 sequences. (b) represents robustness when facing the STDChallenge, where success rate refers to the percentage of frames in which the tracker successfully tracks the target.", "description": "This figure shows two graphs that evaluate the performance of CPDTrack and other trackers under the STDChallenge. The first graph (a) displays the SUC (Success rate) fluctuating with the STD (Spatio-temporal Discontinuity) across various challenging sequences. It illustrates the robustness of CPDTrack compared to other methods, especially in highly challenging scenarios with high STD values. The second graph (b) shows the recovery rate (percentage of frames where successful tracking was maintained after a STDChallenge) over the frame numbers needed to recover.  It showcases the swift recovery ability of CPDTrack in comparison to other trackers.", "section": "5 Experiments"}, {"figure_path": "LezAEImfoc/figures/figures_9_1.jpg", "caption": "Figure 7: Visualizations of CPDTrack and some local trackers tracking results. This demonstrates that CPDTrack, influenced by the global perspective, tends to frame the entire target, showing a similarity to human cognition of objects, such as \"a lion's tail\" or \"the tail rotor of a helicopter.\" This is not an error, but rather a bias introduced by the dataset's setup.", "description": "This figure shows the tracking results of CPDTrack and three other trackers (OSTrack, SeqTrack, MixViT) on two video sequences: one featuring a lion and another a helicopter.  The results highlight CPDTrack's tendency to encompass the entire target object within the bounding box, even parts that might be considered secondary or peripheral to the main body of the object (e.g., lion's tail or helicopter rotor).  The paper argues this is not a failure, but rather a reflection of CPDTrack's incorporation of global context and a more human-like visual search strategy, as opposed to the more focused local approach of other trackers. The difference in bounding boxes illustrates the distinctions in these approaches. ", "section": "Experiments"}, {"figure_path": "LezAEImfoc/figures/figures_15_1.jpg", "caption": "Figure 8: w<sub>t-1</sub> monotonically increases with w<sub>t-1</sub>. The green line represents w<sub>t-1</sub> = w<sub>t-1</sub>, while the black line represents the image width W.", "description": "This figure shows the relationship between w<sub>t-1</sub> (the width of the central region in the previous frame) and w<sub>t-1</sub> (the width of the central region in the current frame) for the CPDTrack algorithm.  The graph shows that w<sub>t-1</sub> increases monotonically with w<sub>t-1</sub>, meaning that the width of the central region in the current frame is always greater than or equal to the width of the central region in the previous frame.  The green line represents a linear relationship where the width of the central region in the current frame is equal to the width of the central region in the previous frame. The black line represents a horizontal line where the width of the central region is equal to the total image width (W = 1024). The orange line shows the actual calculated value of w<sub>t-1</sub> using the Gaussian model in the paper, which represents the calculated width of the central region after taking visual sensitivity into account. The function of the orange line shows that the central region's size is adjusted based on its sensitivity which is calculated based on the previous frame\u2019s tracking result. This demonstrates that the CPDTrack algorithm adaptively adjusts the size of the central region based on the visual sensitivity.", "section": "3.1 CPD Motion Model"}, {"figure_path": "LezAEImfoc/figures/figures_16_1.jpg", "caption": "Figure 9: The STDChallenge Benchmark is composed of high-difficulty sequences extracted from the LTT and GIT benchmarks. Its STD still exhibits a long-tail distribution.", "description": "This figure illustrates the composition and characteristics of the STDChallenge benchmark dataset.  Panel (a) shows a donut chart breaking down the proportion of sequences sourced from three different benchmark datasets: LaSOT, VOTLT2019, and VideoCube.  Panel (b) is a histogram displaying the distribution of the STD metric across all sequences in the benchmark.  Finally, Panel (c) presents a bar chart comparing the proportion of different attributes (e.g., 'corrcoef', 'fast motion', etc.) found in STDChallenge sequences versus those found in sequences from other datasets.  The long tail in the STD metric distribution highlights the challenge the benchmark presents, showing that many sequences contain fewer challenges while a smaller number contain a higher number of challenges.", "section": "4 The STDChallenge Benchmark"}, {"figure_path": "LezAEImfoc/figures/figures_17_1.jpg", "caption": "Figure 10: STDChallenge exhibits a long-tail distribution across most datasets. The horizontal axis represents the number of STDChallenge in each sequence, while the vertical axis represents the number of sequences.", "description": "This figure shows the distribution of the number of STDChallenge (spatio-temporal discontinuity) events within each sequence across four different datasets used in the STDChallenge benchmark.  The long tail indicates that many sequences contain few STDChallenge events, while a smaller number have a very large number. This highlights the challenge of ensuring adequate representation of real-world scenarios with high variability in the number of such events during dataset creation and highlights the non-uniform distribution of challenges in existing benchmarks.", "section": "4 The STDChallenge Benchmark"}, {"figure_path": "LezAEImfoc/figures/figures_18_1.jpg", "caption": "Figure 11: The STDChallenge Benchmark is composed of multiple datasets and has a more rational distribution of challenging attributes. Compared to VideoCube, it expands the range of STD and lengths of sequence, mitigating biases in single dataset and thus providing a more comprehensive assessment environment. It is recommended to view this in enlarged and color format for better clarity.", "description": "This figure shows a comparison of the STDChallenge Benchmark with other benchmarks, highlighting its improved distribution of challenging attributes and sequence lengths. It aims to address biases found in single-dataset benchmarks by incorporating data from multiple sources, creating a more realistic and comprehensive evaluation environment.", "section": "4 The STDChallenge Benchmark"}, {"figure_path": "LezAEImfoc/figures/figures_20_1.jpg", "caption": "Figure 12: Representative sequences used in Visual Turing Test: (T) TEST video example, which helps participants familiarize themselves with the operational process. (1) First group example from the STDChallenge Benchmark. Lower difficulty, containing fewer absent, almost no shotcut, and no changes in scenery. (2) Second group example from the STDChallenge Benchmark. Medium difficulty, containing some absent, fewer shotcut, and some changes in scenery. (3) Third group example from the STDChallenge Benchmark. Higher difficulty, containing many absent, more shotcut, and significant changes in scenery.", "description": "This figure shows example video sequences used in a visual Turing test to evaluate human performance on a visual tracking task.  The sequences are from the STDChallenge benchmark and are categorized by difficulty: low, medium, and high. The difficulty is determined by factors like the frequency of the target disappearing (absent), the frequency of video cuts (shotcut), and the amount of scene changes.  This helps demonstrate the range of challenges within the benchmark and how they compare to human ability.", "section": "E Experiment Organization"}, {"figure_path": "LezAEImfoc/figures/figures_23_1.jpg", "caption": "Figure 13: The error consistency differs among decision-makers (machines/humans) in STDChallenge. This represents the degree of behavioral similarity between different decision-makers. For each sequence in the STDChallenge-Turing Benchmark, we calculate the error consistency o each frame for different decision-makers, and then average these across all sequences to obtain the overall error consistency among the decision-makers.", "description": "This heatmap visualizes the error consistency between human and machine trackers across various sequences in the STDChallenge-Turing benchmark.  The color intensity represents the degree of similarity in error patterns, with darker colors indicating higher similarity.  The figure helps assess how closely different tracking algorithms mimic human behavior in challenging scenarios characterized by spatio-temporal discontinuities.", "section": "F.1 Error Consistency of Humans and Machines"}, {"figure_path": "LezAEImfoc/figures/figures_24_1.jpg", "caption": "Figure 14: Impact of different settings in tracker pipeline. We highlight the median in each setting. In (f), \"1\" means the training set contains GOT-10k, COCO, TrackingNet, and LaSOT, while \"2\" means the training set contains GOT-10k, COCO, TrackingNet, LaSOT, and VideoCube, \"3\" means the training set contains GOT-10k, TrackingNet, LaSOT, and VideoCube.", "description": "This figure presents the ablation study results, showing the impact of different modules and settings on tracker performance.  It includes subfigures demonstrating effects of the temporal module, architecture (customized, SNN+CF-SNN, one-stream), motion model (local crop, local-global, global, CPD), contextual ratio, model parameters (AlexNet, ResNet18, ResNet50, ViT-base, ViT-large), and training datasets (GOT-10k only, GOT-10k+COCO, GOT-10k+COCO+TrackingNet, GOT-10k+COCO+TrackingNet+LaSOT, GOT-10k+COCO+TrackingNet+LaSOT+VideoCube). Each subfigure displays the normalized precision score and error consistency, showing how changing these factors affects both tracking accuracy and the consistency of results compared to human performance.", "section": "5.3 Ablation and Analysis"}]