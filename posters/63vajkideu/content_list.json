[{"type": "text", "text": "Worst-Case Offline Reinforcement Learning with Arbitrary Data Support ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kohei Miyaguchi\u2217 IBM Research \u2013 Tokyo Tokyo, Japan koheimiyaguchi@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We propose a method of offline reinforcement learning (RL) featuring the performance guarantee without any assumptions on the data support. Under such conditions, estimating or optimizing the conventional performance metric is generally infeasible due to the distributional discrepancy between data and target policy distributions. To address this issue, we employ a worst-case policy value as a new metric and constructively show that the sample complexity bound of $O(\\epsilon^{-2})$ is attainable without any data-support conditions, where $\\epsilon>0$ is the policy suboptimality in the new metric. Moreover, as the new metric generalizes the conventional one, the algorithm can address standard offilne RL tasks without modification. In this context, our sample complexity bound can be seen as a strict improvement on the previous bounds under the single-policy concentrability and the single-policy realizability. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offline reinforcement learning (RL) (Levine et al., 2020; Prudencio et al., 2023) is a framework for learning decision-making policies while constrained to a fixed batch of data, preventing the learner from acquiring new information about the environment during training. ", "page_idx": 0}, {"type": "text", "text": "The primary challenges of offline RL are thus originated from the discrepancy between the stateaction distribution of the batch data $\\mu(s)\\beta(a|s)$ and the visitation distribution of the trained policy $d^{\\pi}(s)\\pi(a|s)$ . Most of the previous studies have avoided directly dealing with this discrepancy by posing the assumption known as concentrability (Munos and Szepesv\u00e1ri, 2008; Antos et al., 2008; Chen and Jiang, 2019; Xie et al., 2022). Roughly speaking, the condition asserts that the ratio between these two distributions $d^{\\pi}\\pi/\\mu\\beta$ is well-defined and uniformly bounded over the entire state-action space. This, in turn, constrains the trained policy $\\pi$ to strictly stay inside the state space covered by the data support. ", "page_idx": 0}, {"type": "text", "text": "However, concetrability may be impractical in real-world applications for several reasons. First, one often ends up with a poor coverage of the state-action space when exhaustive data collection is expensive or practically infeasible as in the domains of autonomous driving (Fang et al., 2022), healthcare (Yu et al., 2021) and public policy-making (Abe et al., 2010). Moreover, the precise shape of the partial coverage is unknown if the considerations making it partial are not well-documented or disclosed. On the other hand, it is generally difficult to accurately predict if a policy will visit a given state or not based only on the knowledge of the policy and the batch dataset. As a result, the set of concentrable policies in a hypothesis space may be too small to achieve reasonable performance or even empty.2Therefore, for applying offilne RL in such domains, we need a method that works well without concentrability or any coverage-related conditions. ", "page_idx": 0}, {"type": "table", "img_path": "63VajkIDEu/tmp/314eddc9c6867a0c2e5c68f3a9575d2c60de45c25a7035255bd70a4647bb0e10.jpg", "table_caption": ["Table 1: Assumptions and sample complexity bounds of related work. $\\pi^{*}$ and $\\tilde{\\pi}^{*}$ denote optimal policies in the conventional and worst-case offilne RL, respectively. $\\pi_{n}$ denotes a sequence of policies indexed with the sample size $n$ . The realizability of $\\pi$ means that $\\pi$ -associated model-free parameters (e.g., value functions, visitation weight functions and the policy itself) are realizable. $\\epsilon>0$ is the policy suboptimality given in Problem 4.1 (or equivalently in Problem 3.1, see Corollary 4.2 for the equivalence). $0<\\delta<1$ denotes the confidence parameter. $H$ denotes the time horizon and roughly comparable to $(1-\\gamma)^{-1}$ . $C_{\\mathrm{gap}}$ and $\\beta_{\\mathrm{gap}}$ denote the minimum and the lower-tail exponent of the action value gaps, respectively. $\\mathbf{\\bar{\\mathcal{N}}}$ denotes the cardinality of the function classes. The improvements made by our result are emphasized. See Appendix A for more details. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To tackle with this issue, we study offline RL with arbitrary data support. We present two major results in this paper. ", "page_idx": 1}, {"type": "text", "text": "i) We develop worst-case offline RL (Problem 4.1), a new offline RL framework for handling poor state-action coverage, which can be seen as a natural generalization of conventional offline RL (Corollary 4.2).   \nii) We develop worst-case minimax RL (WMRL, Section 6.3), a model-free algorithm addressing worst-case offilne RL (Corollary 6.3). The resulting sample complexity bound improves the previous state of the art in terms of both the weakness of the assumptions and the strength of the bound (Table 1). ", "page_idx": 1}, {"type": "text", "text": "The rest of the paper is organized as follows. In Section 2, we review the previous work in the literature of offline RL, centered around theoretical studies on the role of concentrability. In Section 3, we introduce some preliminaries around Markov decision process (MDP), offilne RL and concentrability. Then, in Section 4, with the observation that offline RL is ill-posed without concentrability, we introduce worst-case offline RL as a natural generalization and discuss some of its properties useful in our subsequent analysis. In Section 5, we establish the connection between worst-case offilne RL and the Lagrangians derived from the saddle-point formulation of offilne RL. In Section 6, exploiting the connection established earlier, we construct a method for solving worst-case offline RL with polynomial sample complexity. Finally, we discuss the limitation and the future work in Section 7. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The notion of concentrability is introduced by Munos (2003); Munos and Szepesv\u00e1ri (2008); Antos et al. (2008) to analyze the value/policy iteration algorithms, not necessarily in the context of offilne RL. Recently, it has been increasingly gaining traction as one of the key characteristics of the difficulty of offline RL (Chen and Jiang, 2019) due to the distribution mismatch. In its original definition, concentrability requires the norm of the density ratio $\\|d^{\\pi}\\pi/\\mu\\beta\\|_{\\infty}$ to be bounded uniformly for all the policies $\\pi$ . Liu et al. (2020) showed that this uniform boundedness can be relaxed to the single-policy boundedness with the principle of pessimism in the face of uncertainty $(P F U)$ . Considering the case where the single-policy concentrability is even slightly violated, Xie et al. (2021) further analyzed the performance degradation caused by the lack of concentrability. Finally, we completely remove the concentrability assumption by incorporating it into a new performance metric. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The removal of concentrability is useful not only for widening the applicability of offline RL, but also strengthening the sample complexity bound by streamlining the analysis. Previously, Zhan et al. (2022) established polynomial sample complexity bounds under the weakest known model-free assumptions, yet being unable to achieving the statistically reasonable rate $O(\\epsilon^{-2})$ . Also, Chen and Jiang (2022); Ozdaglar et al. (2023); Uehara et al. (2023) gave improved rates additionally assuming that the minimum action value gap is bounded away from zero. On the other hand, under a set of assumptions as weak as Zhan et al. (2022), our sample complexity bound achieves the rate of $O(\\epsilon^{-2})$ . See Appendix A for more detailed discussions. ", "page_idx": 2}, {"type": "text", "text": "Algorithmically, the PFU principle is often materialized as the pessimistic or behavioral regularization (Kumar et al., 2020; Fujimoto and Gu, 2021; Yu et al., 2020). Previous analyses are often sensitive to the hyperparameters controlling the degree of such regularization, such as the truncation threshold $b$ in Liu et al. (2020), the Bellman consistency threshold $\\varepsilon$ in Xie et al. (2021); Chen and Jiang (2022) and the regularization weight in Zhan et al. (2022); Uehara et al. (2023). On the other hand, our method has no hyperparameter other than the choice of the function approximators. One may see the root cause of this difference in that the PFU principle is built into our single new performance metric, whereas the previous studies adopt it as an additional objective, resulting in bi-objective optimizations. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We denote the set of nonnegative real numbers by $\\mathbb{R}_{+}=[0,\\infty)$ and the uniform norm of a function $g$ over its domain by $\\begin{array}{r}{\\left\\|g\\right\\|_{\\infty}:=\\operatorname*{sup}_{z\\in\\mathrm{dom}(g)}\\left|g(z)\\right|}\\end{array}$ . We also denote by $\\Delta(\\mathcal{X})$ the set of (generalized) probability density functions on $\\mathcal{X}$ relative to a suitable base measure,3 such as the counting measure and the Lebesgue measure. ", "page_idx": 2}, {"type": "text", "text": "Markov decision process (MDP) and RL. Let $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},R,T)$ be an MDP consisting of the state space $\\boldsymbol{S}$ , the action space $\\boldsymbol{\\mathcal{A}}$ , the reward function $R:S\\times A\\to\\Delta([0,1])$ and the transition probability $T:S\\times A\\to\\Delta(S)$ . We assume both $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ are finite sets for simplicity. The goal of RL in general is to optimizing policy $\\pi:S\\to\\Delta(A)$ in terms of the policy value, ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ(\\pi)=J(\\pi|\\mathcal{M}):=(1-\\gamma)\\mathbb{E}^{\\pi}\\left[\\sum_{t\\geq0}\\gamma^{t}r_{t}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with a discount factor $\\gamma\\in(0,1)$ . Here, the expectation $\\mathbb{E}^{\\pi}$ is taken with respect to the Markov chain generated with $a_{t}\\sim\\pi(\\cdot|s_{t})$ , $r_{t}\\sim R(s_{t},a_{t})$ and $s_{t+1}\\sim T(s_{t},a_{t}),\\,t\\,\\geq\\,0,$ , starting from a known initial-state distribution $s_{0}\\sim p_{0}(s)$ . ", "page_idx": 2}, {"type": "text", "text": "Offline constraint. In maximizing $J(\\pi)$ , the offline constraint prohibits us to access the environment $\\mathcal{M}$ except through the offline dataset $D\\,:=\\,\\{(s_{i},a_{i},r_{i},s_{i}^{\\prime})\\}_{i=1}^{n}$ . We assume the dataset is sampled from a fixed distribution $p_{\\mathtt{d a t a}}^{\\mathcal{M}}\\in\\Delta(S\\times\\mathcal{A}\\times[0,1]\\times\\mathcal{S})^{n}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\mathtt{d a t a}}^{\\mathcal{M}}(\\mathcal{D})=\\prod_{i=1}^{n}\\mu(s_{i})\\,\\beta(a_{i}|s_{i})\\,R(r_{i}|s_{i},a_{i})\\,T(s_{i}^{\\prime}|s_{i},a_{i}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mu\\,\\in\\,\\Delta(S)$ and $\\beta:{\\mathcal{S}}_{.}\\rightarrow\\Delta(A)$ are the behavior state distribution and the behavior policy, respectively. Typically, $p_{\\mathrm{data}}^{\\mathcal{M}}(\\mathcal{D})$ represents the distribution of the past observational data. The problem of offline RL is now formally given as follows. ", "page_idx": 2}, {"type": "text", "text": "Problem 3.1 (Offline RL). Given the offline dataset $\\mathcal{D}$ and a small number $\\epsilon>0$ , find a policy $\\pi$ achieving $J^{*}-J(\\pi)\\leq\\epsilon,$ , where $J^{*}:=\\operatorname*{max}_{\\pi:S\\to\\Delta(A)}J(\\pi)$ . ", "page_idx": 2}, {"type": "text", "text": "Value, visitation and weight functions. Let $r(s,a):=\\mathbb{E}_{y\\sim R(s,a)}\\left[y\\right]$ be the expected reward function and $\\begin{array}{r}{r^{\\pi}(s):=\\sum_{a}r(s,a)\\pi(a|s)}\\end{array}$ be its marginalization with respect to policy $\\pi$ . Let $\\tau,\\tau^{\\pi}$ and $\\tau_{*}^{\\pi}$ be the raw transition operator, the policy transition operator and its adjoint given by $\\tau_{v}(s,a)=$ $\\begin{array}{r}{\\sum_{s^{\\prime}}v(s^{\\prime})T(s^{\\prime}|s,a),\\mathcal{T}^{\\pi}v(s)=\\sum_{a}\\hat{T^{\\upsilon}}(s,\\dot{a})\\pi(a|s)}\\end{array}$ and $\\begin{array}{r}{\\mathcal{T}_{*}^{\\pi}d(s)=\\sum_{s^{\\prime},a^{\\prime}}d(s^{\\prime})\\pi(a^{\\prime}|\\dot{s}^{\\prime})T(s|\\dot{s}^{\\prime},\\stackrel{\\cdot}{a^{\\prime}})}\\end{array}$ , respectively. Then, the state value function $v^{\\pi}:S\\rightarrow\\mathbb{R}$ , the action value function $q^{\\pi}:S\\times A\\to\\mathbb{R}$ and the state visitation distribution $d^{\\pi}\\in\\Delta(S)$ are given by ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r l}{v^{\\pi}=(I-\\gamma T^{\\pi})^{-1}r^{\\pi},}&{}&&{q^{\\pi}=r+\\gamma T v^{\\pi},}&{}&&{d^{\\pi}=(1-\\gamma)(I-\\gamma T_{*}^{\\pi})^{-1}p_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "as well as the state weight function $w^{\\pi}\\;:\\;\\mathrm{supp}(\\mu)\\;\\to\\;\\mathbb{R}$ and the action weight function $f^{\\pi}$ : $\\operatorname{supp}(\\mu\\beta)\\to\\mathbb{R}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\nw^{\\pi}(s):={\\frac{d^{\\pi}(s)}{\\mu(s)}},\\qquad\\qquad\\qquad f^{\\pi}(s,a):=w^{\\pi}(s)\\,\\rho^{\\pi}(s,a),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\operatorname{supp}(g)\\,:=\\,\\{x\\,\\in\\,\\operatorname{dom}(g)\\,|\\,g(x)\\,\\neq\\,0\\}$ denotes the support of function $g$ and $\\rho^{\\pi}(s,a):=$ $\\frac{\\pi(a|s)}{\\beta(a|s)}$ is the density ratio of $\\pi$ to $\\beta$ . We also define the optimal value functions by $v^{*}(s)\\;:=\\;$ $\\operatorname*{max}_{\\pi}v^{\\pi}(s)$ and $q^{*}(s,a)~=~\\operatorname*{max}_{\\pi}q^{\\pi}(s,a)$ as well as the set of the optimal policies $\\Pi^{*}:=$ $\\{\\pi:S\\rightarrow\\Delta(A)\\,:\\,v^{\\pi}=v^{*}\\}$ , which by definition all attain $J^{*}$ . See Table 2 for the summary of the notation introduced above. ", "page_idx": 3}, {"type": "text", "text": "Concentrability. A policy $\\pi$ is said to be concentrable (or satisfying concentrability) if its stateaction visitation is contained in the data support, $\\operatorname{supp}(d^{\\pi}\\pi)\\subset\\operatorname{supp}(\\mu\\beta)$ . We denote the set of all the concentrable policies by $:=\\{\\pi:{\\mathcal{S}}\\rightarrow\\Delta(A):\\operatorname{supp}(d^{\\pi}\\pi)\\subset\\operatorname{supp}(\\mu\\beta)\\}.$ . ", "page_idx": 3}, {"type": "text", "text": "4 Worst-Case Offline Reinforcement Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In offline RL (Problem 3.1), the information on $\\mathcal{M}$ is restricted by the data support $\\operatorname{supp}(\\mu\\beta)$ . In such situations, one cannot know about the transition probability $T(s,a)$ and the reward probability $R(s,a)$ for $(s,a)\\not\\in\\mathrm{supp}(\\mu\\beta)$ . Consequently, the accurate estimation of $J(\\pi)$ is infeasible (even with $n=\\infty$ ) for unconcentrable policies, and thus previous analyses on Problem 3.1 often require that there exists at least one concentrable optimal policy, i.e., $\\Pi_{\\mathrm{CC}}\\cap\\mathrm{argmax}_{\\pi}\\,J(\\pi)\\neq\\emptyset$ . ", "page_idx": 3}, {"type": "text", "text": "To remove such dependency on concentrability, we introduce a performance metric alternative to $J(\\pi)$ . Let $\\mathfrak{U}:=\\overline{{\\{\\mathcal{M}^{\\prime}\\ :\\ \\mathfrak{p}_{\\mathrm{data}}^{M^{\\prime}}=p_{\\mathrm{data}}^{M}\\}}}$ be the set of the environments indistinguishable from the true environment M with respect to the resulting data distribution pdMata. Noting that U is the information-theoretic limit of the uncertainty on $\\mathcal{M}$ under the offline constraint, we follow the pessimism-in-the-face-of-uncertainty principle and consider the worst case within $\\mathfrak{U}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{J}(\\pi):=\\operatorname*{inf}_{\\mathcal{M}^{\\prime}\\in\\mathfrak{U}}J(\\pi|\\mathcal{M}^{\\prime}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which we refer to as worst-case policy value. Replacing $J(\\pi)$ with $\\tilde{J}(\\pi)$ in Problem 3.1, we arrive at the following problem. ", "page_idx": 3}, {"type": "text", "text": "Problem 4.1 (Worst-case offilne RL). Given the offilne dataset $\\mathcal{D}$ and a small number $\\epsilon>0$ , find $a$ policy $\\pi$ achieving $\\tilde{J}^{*}-\\tilde{J}(\\pi)\\leq\\epsilon$ , where $\\tilde{J}^{*}:=\\operatorname*{max}_{\\pi:S\\to\\Delta(A)}\\tilde{J}(\\pi)$ . ", "page_idx": 3}, {"type": "text", "text": "To facilitate the subsequent analysis on Problem 4.1, we next introduce the notion of truncated environment, which is similar to, yet different from those previously considered by Liu et al. (2020); Yin and Wang (2021) as it is based on the true and unknown data support rather than the empirical one. The truncation is useful for characterizing the worst-case policy value $\\tilde{J}(\\pi)$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 4.1 (Truncated environment). The truncation of $\\mathcal{M}$ with respect to $\\mu$ and $\\beta$ is given by $\\tilde{\\mathcal{M}}=(\\tilde{S},\\mathcal{A},\\tilde{T},\\tilde{R}),$ , where $\\tilde{S}=S\\cup\\{\\bot\\}$ with $\\perp$ being an absorbing state with reward zero and ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{R}(r|s,a):=\\chi_{\\mu,\\beta}(s,a)\\,R(r|s,a)+(1-\\chi_{\\mu,\\beta}(s,a))\\,\\delta_{0}(r),}\\\\ &{\\tilde{T}(s^{\\prime}|s,a):=\\chi_{\\mu,\\beta}(s,a)\\,T(s^{\\prime}|s,a)+(1-\\chi_{\\mu,\\beta}(s,a))\\,\\delta_{\\perp}(s^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $s\\in S$ and $a\\in{\\mathcal{A}}$ , where $\\chi_{\\mu,\\beta}(s,a):=\\mathbb{I}\\left\\{\\mu(s)>0\\right\\}\\mathbb{I}\\left\\{\\beta(a|s)>0\\right\\}$ is the indicator function of the support of $\\mu(s)\\,\\beta(a|s)$ and $\\delta_{x}\\in\\Delta(\\mathcal{X})$ is the Dirac\u2019s delta function located at $x\\in\\mathscr{X}$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 4.1. We have $\\tilde{J}(\\pi)=J(\\pi|\\tilde{\\mathcal{M}}).$ for all $\\pi:S\\to\\Delta(A)$ . ", "page_idx": 3}, {"type": "text", "text": "Proof (sketch). It suffices to show $J(\\pi|\\tilde{\\mathcal{M}})\\leq\\tilde{J}(\\pi)\\leq J(\\pi|\\tilde{\\mathcal{M}})$ , where the first inequalilty follows from $J(\\pi|\\tilde{\\mathcal{M}})\\leq J(\\pi|\\mathcal{M}^{\\prime})$ for all $\\mathcal{M}^{\\prime}\\in\\mathfrak{U}$ and the second inequality follows from $\\tilde{\\mathcal{M}}\\in\\mathfrak{U}$ . See Appendix D.1 for the complete proof. \u53e3 ", "page_idx": 4}, {"type": "text", "text": "In other words, worst-case offline RL is nothing but offline RL with the truncated environment $\\tilde{\\mathcal{M}}$ . Thus, in principle, one can exploit conventional offline RL methods to solve Problem 4.1. We hereafter refer to the truncated counterparts (those defined by replacing $\\mathcal{M}$ with M\u02dc) of $v^{\\pi}$ , $v^{*}$ , $q^{\\pi}$ , $q^{*}$ , $\\Pi^{*}$ , $d^{\\pi}$ , $w^{\\pi}$ and $f^{\\pi}$ as $\\tilde{v}^{\\pi},\\,\\tilde{v}^{*},\\,\\tilde{q}^{\\pi},\\,\\tilde{q}^{*},\\,\\tilde{\\Pi}^{*},\\,\\tilde{d}^{\\pi},\\,\\tilde{w}^{\\pi}$ and $\\tilde{f}^{\\bar{\\pi}}$ , respectively. Likewise, let $\\tilde{r},\\,\\tilde{r}^{\\pi},\\,\\tilde{\\mathcal{T}}$ , $\\tilde{\\mathcal{T}}^{\\pi}$ and $\\tilde{\\mathcal{T}}_{*}^{\\pi}$ be the truncated counterparts of r, $r^{\\pi}$ , $\\tau$ , $\\mathcal{T}^{\\pi}$ and $\\mathcal{T}_{*}^{\\pi}$ , respectively. ", "page_idx": 4}, {"type": "text", "text": "Let us remark several key implications of Theorem 4.1. First, since the unknown parameters $\\tilde{R}$ and $\\tilde{T}$ of the truncated environment M\u02dc are only nontrivial on the data support, it is intuitively obvious that $\\tilde{J}(\\pi)$ can be accurately estimated even without the concetrability, given sufficiently large $n$ . Thus, it is reasonable to expect that Problem 4.1 does not require any concetrabilities to be well-posed, unlike Problem 3.1. Second, the constructive existence of $\\tilde{\\mathcal{M}}$ makes the relationship between $J(\\pi)$ and $\\tilde{J}(\\pi)$ clearer, as stated in the following corollary. ", "page_idx": 4}, {"type": "text", "text": "Corollary 4.1. We have $\\tilde{J}(\\pi)=J(\\pi)$ if $\\pi\\in\\Pi_{C C}$ and $\\tilde{J}(\\pi)\\leq J(\\pi)$ otherwise. ", "page_idx": 4}, {"type": "text", "text": "Proof. See Appendix D.2. ", "page_idx": 4}, {"type": "text", "text": "According to Corollary 4.1, the pessimism introduced by the truncation is mild in the sense that it conserves the values of concentrable policies. Finally, it also clarifies the relationship between the suboptimality metrics of the conventional and the worst-case problems. ", "page_idx": 4}, {"type": "text", "text": "Corollary 4.2. For all $\\pi:S\\to\\Delta(A)$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ^{*}-J(\\pi)\\leq\\tilde{J}^{*}-\\tilde{J}(\\pi)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$i f\\Pi_{C C}\\cap\\mathrm{argmax}_{\\pi}\\,J(\\pi)\\neq\\emptyset.$ . Moreover, the equality is attained if in addition $\\pi\\in\\Pi_{C C}$ . ", "page_idx": 4}, {"type": "text", "text": "Proof. Trivial from Corollary 4.1. ", "page_idx": 4}, {"type": "text", "text": "In other words, solutions of the worst-case problem are also valid as solutions of the conventional problem under the standard assumption, while the two solution concepts are identical if only concentrable policies are concerned. In this sense, worst-case offline RL is a natural generalization of the conventional offline RL for handling arbitrary data distributions. ", "page_idx": 4}, {"type": "text", "text": "Finally, we conclude this section by showing a useful property of the worst-case optimal policies. Let $\\dot{\\Pi_{\\beta}}:=\\{\\pi:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})\\,|\\,\\operatorname{supp}(\\mu\\pi)\\subset\\operatorname{supp}(\\mu\\beta)\\}$ be the set of the on-support policies, i.e., the policies with the support covered by the behavior policy. The following lemma allows us to limit the scope of policy optimization to $\\Pi_{\\beta}$ without sacrificing the optimality in terms of M\u02dc. The proof is relegated to Appendix D.3. ", "page_idx": 4}, {"type": "text", "text": "Lemma 4.1. There is at least one worst-case optimal policy that is on-support, i.e., $\\tilde{\\Pi}^{*}\\cap\\Pi_{\\beta}\\neq\\emptyset$ . ", "page_idx": 4}, {"type": "text", "text": "5 Lagrangians for Worst-Case Offline Reinforcement Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we set up theoretical foundation of worst-case offline RL. Specifically, in Section 5.1, we show a connection between $\\tilde{J}(\\pi)$ and the Lagrangian of RL (Puterman, 2014). However, since the Lagrangian in its original form is unstable to the function approximation error (Section 5.2), we further introduce a regularized variant of it (Section 5.3). ", "page_idx": 4}, {"type": "text", "text": "5.1 Unregularized Lagrangian ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Consider the following functional of $v:S\\rightarrow\\mathbb{R}_{+}$ and $f:\\operatorname{supp}(\\mu\\beta)\\to\\mathbb{R}_{+}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nL(v,f):=(1-\\gamma)\\mathbb{E}_{p_{0}}\\left[v(s)\\right]+\\mathbb{E}_{\\mu,\\beta}\\left[f(s,a)\\,\\delta^{\\mathrm{TD}}v(s,a)\\right],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbb{E}_{p_{0}}$ and $\\mathbb{E}_{\\mu,\\beta}$ are the expectation operators with respect to $s\\sim p_{0}(s)$ and $(s,a)\\sim\\mu(s)\\beta(a|s)$ , respectively, and $\\mathrm{\\Dot{\\delta}^{T D}:}\\mathbb{R}^{S}\\rightarrow\\mathbb{R}^{S\\times A}$ is the time-difference error operator given by $\\delta^{\\mathrm{TD}}v(s,a)=$ $r(\\bar{s},a)+\\gamma\\mathcal{T}v(s,a)-v(s)$ . ", "page_idx": 5}, {"type": "text", "text": "We refer to $L(v,f)$ as the (unregularized) Lagrangian since it has been known as the Lagrangian of the linear-programming-based formulations of RL (Puterman, 2014; Chen and Wang, 2016; Nachum et al., 2019; Zhang et al., 2021; Zhan et al., 2022). The following theorem reveals that, perhaps surprisingly, it is also connected with worst-case offline RL. ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.1. For all $\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ , $(\\tilde{v}^{*},\\tilde{f}^{\\pi})$ is a saddle point of $L(v,f)$ in $\\mathbb{R}_{+}^{S}\\times\\mathbb{R}_{+}^{S\\times A}$ . ", "page_idx": 5}, {"type": "text", "text": "Proof (sketch). The key of the proof is the following identity of Lagrangian. ", "page_idx": 5}, {"type": "text", "text": "Lemma 5.1. For all $\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L(v,f)=\\tilde{J}^{*}-\\mathbb{E}_{\\mu,\\beta}\\left[(f-\\tilde{f}^{\\pi})(s,a)\\left(I-\\gamma\\tilde{\\mathcal{T}}\\right)(v-\\tilde{v}^{*})(s,a)\\right]+D_{\\mathrm{V}}^{\\pi}(v)-D_{\\mathrm{F}}^{*}(f),}\\\\ &{D_{\\mathrm{V}}^{\\pi}(v):=\\sum_{s\\notin\\mathrm{supp}(\\mu)}\\tilde{d}^{\\pi}(s)\\,v(s)\\,a n d\\,D_{\\mathrm{F}}^{*}(f):=\\mathbb{E}_{\\mu,\\beta}\\left[f(s,a)\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Now, observe that the third term $D_{\\mathrm{V}}^{\\pi}(v)$ and the fourth term $D_{\\mathrm{F}}^{*}(f)$ in Eq. (6) are nonnegative since all of $v,f,{\\tilde{d}}^{\\pi}$ and $\\tilde{v}^{*}-\\tilde{q}^{*}$ are nonnegative. Therefore, Lemma 5.1 implies that Lagrangian is bounded from below with $L(v,f)\\ge\\tilde{J}^{*}$ taking $f=\\tilde{f}^{\\pi}$ , while bounded from above with $L(v,f)\\le\\tilde{J}^{*}$ taking $v=\\tilde{v}^{*}$ . Combining these two inequalities, a class of the saddle points of Lagrangian is identified as desired. The full proof is found in Appendix E.2. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "iunn lai kdei fofeurr esnett tdinogm awiint,h Note that the previous studies on the LP-based formulation of RL typically consider the saddle points $\\mathbb{R}^{S}\\times\\mathbb{R}_{+}^{S\\times A}$ n,t .t hTeh idso cmoanisnt roaif ntth ies  tphrie mkaely  vtaor ieasbtlaeb $v$ sihs  tuhne ccoonnstnreacitnieodn, $v\\geq0$   \nwith the worst-case environment $\\tilde{\\mathcal{M}}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.1 superficially suggests that finding the saddle points of $L(v,f)$ is a reasonable way of finding the optimal policies with respect to M\u02dc. However, in the next section, we show that it is unstable and easily breaks down by the function approximation error. ", "page_idx": 5}, {"type": "text", "text": "5.2 Instability of Unregularized Lagrangian ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "When the state space $\\boldsymbol{S}$ is large, it is practically infeasible to find a saddle point of $L(v,f)$ na\u00efvely searching over the whole space $\\mathbb{R}_{+}^{S}\\times\\mathbb{R}_{+}^{S\\times A}$ . Therefore, one may introduce compact function classes $\\mathcal{V}\\subset\\mathbb{R}_{+}^{S}$ and $\\mathcal{F}\\subset\\mathbb{R}_{+}^{S\\times A}$ and limit the scope of the search to these classes. Since we do not know the saddle points (which motivates us to find one), such function classes likely incur the function approximation error. Thus, it is likely that the saddle points $(\\tilde{v}^{*},\\tilde{f}^{\\pi})$ may sit near the search space $\\nu\\times\\mathcal{F}$ , but not exactly included in the space, $(\\tilde{v}^{*},\\tilde{f}^{\\pi})\\not\\in\\mathcal{V}\\times\\mathcal{F}$ . ", "page_idx": 5}, {"type": "text", "text": "In this context, we show even a tiny function approximation error can completely disrupt the connection established in Theorem 5.1. Consider the function classes $\\mathcal{V}_{\\epsilon}=\\{v\\in\\mathbb{R}_{+}^{S}\\,\\vert\\,v\\geq\\tilde{v}^{\\ast}+\\epsilon\\}$ and $\\mathcal{F}_{\\epsilon}=\\{f\\in\\mathbb{R}_{+}^{S\\times A}|\\,\\exists\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ s.t. $f\\le\\tilde{f}^{\\pi}-\\epsilon\\}$ with a small constant $\\epsilon>0$ . Then, even though the function approximation error is small ( $\\epsilon$ for both and in terms of the $L^{\\infty}$ -norm), the saddle point is collapsed to zero under the approximations with $\\mathcal{V}_{\\epsilon}$ and $\\mathcal{F}_{\\epsilon}$ . The proof is relegated to Appendix E.3. ", "page_idx": 5}, {"type": "text", "text": "Corollary 5.1. Suppose $\\mathcal{V}_{\\epsilon}$ and $\\mathcal{F}_{\\epsilon}$ are nonempty. Then, the saddle points of $L(v,f)$ in $\\mathcal{V}_{\\epsilon}\\times\\mathbb{R}_{+}^{S\\times A}$ must satisfy $f=0$ . Moreover, the saddle points of $L(v,f)$ in $\\mathbb{R}_{+}^{S}\\times\\mathcal{F}_{\\epsilon}$ must satisfy $v=0$ . ", "page_idx": 5}, {"type": "text", "text": "5.3 Regularized Lagrangian ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Corollary 5.1 shows that the saddle points of Lagrangian cannot be used as a reliable way to find the optimal policies with the function approximation. As a workaround, we introduce a regularized Lagrangian, ", "page_idx": 5}, {"type": "equation", "text": "$$\nK(v,f):=(1-\\gamma)\\mathbb{E}_{\\mu}\\left[v(s)\\right]+\\mathbb{E}_{\\mu,\\beta}\\left[f(s,a)\\,\\delta^{\\mathrm{TD}}v(s,a)\\right]+\\frac{(1-\\gamma)^{2}}{2}\\left\\|v\\right\\|_{2,\\bar{\\mu}}^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\bar{\\mu}:=\\mu+\\gamma T_{*}^{\\beta}\\mu$ and $\\begin{array}{r}{\\|v\\|_{p,\\bar{\\mu}}:=\\{\\sum_{s}\\bar{\\mu}(s)v^{p}(s)\\}^{1/p}}\\end{array}$ denotes the $L^{p}(\\bar{\\mu})$ -norm of the functions over $\\boldsymbol{S}$ . Then, it is shown that the regularized Lagrangian is also connected with worst-case offline RL through its saddle points. To see this, let us define the regularized counterparts of $\\tilde{d}^{\\pi}$ , $\\tilde{w}^{\\pi}$ and $\\tilde{f}^{\\pi}$ with $\\breve{d}^{\\pi}:=(1-\\gamma)(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\pi})^{-1}\\breve{p}^{\\pi}$ , $\\breve{w}^{\\pi}:=\\breve{d}^{\\pi}/\\mu$ , $\\breve{f}^{\\pi}:=\\breve{w}^{\\pi}\\rho^{\\pi}$ , obtained by substituting the initial state distribution $p_{0}$ with $\\breve{p}^{\\pi}:=\\mu+(1-\\gamma)\\tilde{v}^{\\pi}\\bar{\\mu}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.2. For all $\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ , $(\\tilde{v}^{*},\\breve{f}^{\\pi})$ is a saddle point of $K(v,f)$ in $\\mathbb{R}_{+}^{S}\\times\\mathbb{R}_{+}^{S\\times A}$ . Moreover, the primal solution $\\tilde{v}^{*}$ is unique on $\\operatorname{supp}({\\bar{\\mu}})$ . ", "page_idx": 6}, {"type": "text", "text": "Proof (sketch). Similarly as the proof of Theorem 5.1, the key is the following identity. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.2. There exist $U^{*}\\in\\mathbb{R}$ such that, for all $\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K(v,f)=U^{*}-{\\mathbb E}_{\\mu,\\beta}\\Big[(f-\\check{f}^{\\pi})(s,a)\\left(I-\\gamma\\tilde{T}\\right)(v-\\tilde{v}^{*})(s,a)\\Big]+\\check{D}_{\\mathrm{V}}^{\\pi}(v)-D_{\\mathrm{F}}^{*}(f),}\\\\ &{\\check{D}_{\\mathrm{V}}^{\\pi}(v):=\\sum_{s\\notin\\mathrm{supp}(\\mu)}\\check{d}^{\\pi}(s)\\,v(s)+\\frac{(1-\\gamma)^{2}}{2}\\left\\lVert v-\\tilde{v}^{\\pi}\\right\\rVert_{2,\\bar{\\mu}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The first claim of Theorem 5.2 then follows from the fact $\\breve{D}_{\\mathrm{V}}^{\\pi}(v)$ is nonnegative, and the second claim follows from the strong convexity of $K(v,f)$ with respect to $v$ on $\\operatorname{supp}({\\bar{\\mu}})$ . The full proof is in Appendix E.5. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Comparing Theorems 5.1 and 5.2, it turns out that the regularization does not alter the primal part of the saddle points $\\tilde{v}^{*}$ . Moreover, since $K(v,f)$ is strongly convex in terms of $v$ , the regularized solution is more stable against the function approximation error as opposed to the unregularized solution. To see this, denote the regularized primal solution under the function approximation by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{v}_{\\sharp}^{*}\\in\\operatorname*{argmin}_{v\\in\\mathcal{V}}\\operatorname*{max}_{f\\in\\mathcal{F}}K(v,f),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathcal{V}\\subset\\mathbb{R}_{+}^{S}$ and $\\mathcal{F}\\subset\\mathbb{R}_{+}^{S\\times A}$ are compact function classes. Also denote the individual function approximation errors of $\\nu$ and $\\mathcal{F}$ by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathcal{V}}:=\\operatorname*{min}_{v\\in\\mathcal{V}}\\|v-\\tilde{v}^{*}\\|_{1,\\bar{\\mu}}\\,,\\quad\\epsilon_{\\mathcal{F}}:=\\operatorname*{min}_{f\\in\\mathcal{F},\\pi\\in\\Pi_{\\beta}}\\left\\{\\bar{B}_{\\mathcal{V}}\\|f-\\tilde{f}^{\\pi}\\|_{1,\\mu\\beta}+2(1-\\gamma)\\left\\|\\tilde{v}^{\\pi}-\\tilde{v}^{*}\\right\\|_{1,\\bar{\\mu}}\\right\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\bar{B}_{\\mathcal{V}}\\,:=\\,\\operatorname*{max}\\left\\{1+\\gamma B_{\\mathcal{V}},B_{\\mathcal{V}}\\right\\}$ and $B_{\\mathcal{V}}\\;:=\\;\\operatorname*{max}_{v\\in\\mathcal{V}}\\left\\|v\\right\\|_{\\infty}$ are scale factors of $\\mathcal{V}$ . Then, the following lemma shows the stability of the approximate solution $\\tilde{v}_{\\sharp}^{*}$ in terms of the aggregated function approximation error ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\varepsilon_{\\mathrm{app},\\mathrm{V}}({\\mathcal{V}},{\\mathcal{F}}):={\\frac{{\\sqrt{2\\left(2+B_{\\mathcal{F}}\\right)\\epsilon_{\\mathcal{V}}}}+4{\\sqrt{\\epsilon_{\\mathcal{F}}}}}{1-\\gamma}}=O\\left({\\frac{\\sqrt{B_{\\mathcal{F}}\\epsilon_{\\mathcal{V}}+\\epsilon_{\\mathcal{F}}}}{1-\\gamma}}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $B_{\\mathcal{F}}:=\\operatorname*{max}_{f\\in\\mathcal{F}}\\|f\\|_{\\infty}$ is the scale factor of $\\mathcal{F}$ . The proof is relegated to Appendix E.6. ", "page_idx": 6}, {"type": "text", "text": "Lemma 5.3 (Stability of the regularized primal solution). We have $\\|\\tilde{v}_{\\sharp}^{*}-\\tilde{v}^{*}\\|_{2,\\bar{\\mu}}\\leq\\varepsilon_{a p p,\\mathrm{V}}(\\mathcal{V},\\mathcal{F})$ . ", "page_idx": 6}, {"type": "text", "text": "Note that the approximation error of $\\mathcal{F}$ is trivially bounded by a simpler error term ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathcal{F}}\\leq\\bar{B}_{\\mathcal{V}}\\operatorname*{min}_{\\substack{f\\in\\mathcal{F},\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}}}\\|f-\\breve{f}^{\\pi}\\|_{1,\\mu\\beta},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "i.e., the $L^{1}$ -error with respect to the function $\\breve{f}^{\\pi}$ of the optimal on-support policy $\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ . Our definition of the error is weaker than that, measuring the error with respect to that of the possibly suboptimal on-support policy $\\pi\\in\\Pi_{\\beta}$ in exchange for the additional suboptimality cost $2(1-\\gamma)\\left\\|\\tilde{v}^{\\pi}-\\tilde{v}^{*}\\right\\|_{1,\\bar{\\mu}}$ . This is beneficial if the optimal policies are difficult to approximate, like deterministic policies in a continuous action space, yet some near-optimal policies such as the soft-optimal policies $\\pi(a|s)\\propto\\exp\\{-\\eta\\tilde{q}^{*}(s,a)\\}$ are easy to approximate. ", "page_idx": 6}, {"type": "text", "text": "We also note that the idea of stabilizing the saddle-point-based policy optimization via a strongly convex regularizer is not new (Nachum et al., 2019; Lee et al., 2021; Zhan et al., 2022; Uehara et al., 2023). The major difference here (other than the truncation) is that we regularize the value function $v$ (like Uehara et al. (2023)) but extract the information of the optimal policy from $f$ (like Nachum et al. (2019); Lee et al. (2021); Zhan et al. (2022)), which, combined with our worst-case framework, results in a striking improvement in the sample complexity. ", "page_idx": 6}, {"type": "text", "text": "6 Worst-Case Minimax Reinforcement Learning ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now, we present a method to solve worst-case offline RL with the saddle points of $K(v,f)$ . We first introduce a method of extracting policy from the dual variable $f$ (Section 6.1), then show the suboptimality bound of the extracted policy (Section 6.2), which is our main result, and finally show the sample complexity bound taking into account the finite sample approximation (Section 6.3). ", "page_idx": 7}, {"type": "text", "text": "6.1 Policy Extraction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Motivated by Theorem 5.2, we propose a method of extracting the worst-case optimal policy $\\pi^{*}$ from the saddle point of $K(v,f)$ . Specifically, we consider minimizing the loss function given by ", "page_idx": 7}, {"type": "equation", "text": "$$\nD_{\\Xi}({f};\\boldsymbol{w},\\pi):=\\operatorname*{max}_{\\boldsymbol{\\xi}\\in\\Xi}\\left\\{\\mathbb{E}_{\\boldsymbol{\\mu},\\boldsymbol{\\beta}}\\left[f(\\boldsymbol{s},\\boldsymbol{a})\\,\\boldsymbol{\\xi}(\\boldsymbol{s},\\boldsymbol{a})\\right]-\\mathbb{E}_{\\boldsymbol{\\mu},\\pi}\\left[\\underline{{w}}(\\boldsymbol{s})\\,\\boldsymbol{\\xi}(\\boldsymbol{s},\\boldsymbol{a})\\right]\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $w:S\\rightarrow\\mathbb{R}$ is an auxiliary weight function, $\\underline{w}(s):=\\operatorname*{max}\\left\\{1-\\gamma,w(s)\\right\\}$ is its lower clipping and $\\Xi\\subset\\mathbb{R}^{S\\times A}$ is a class of discriminator functions. Note that $D_{\\Xi}(f;w,\\pi)$ is the integral probability metric (IPM) (Sriperumbudur et al., 2009) between $f(s,a)\\,\\mu(s)\\,\\tilde{\\beta}(a|s)$ and $\\underline{{w(s)}}\\,\\mu(s)\\,\\bar{\\pi}(a|s)$ with respect to the discriminators $\\Xi$ . With a sufficiently rich $\\Xi$ , this implies $D_{\\Xi}(f;w,\\pi)$ attains its minimum value (i.e., zero) only if $\\pi=\\pi_{f}$ ,4thereby informally justifies the minimization of Eq. (11) as a way of policy extraction. ", "page_idx": 7}, {"type": "text", "text": "This approach introduces additional (functional) variables to be optimized, $w:S\\rightarrow\\mathbb{R}$ and $\\pi:{\\mathcal{S}}\\rightarrow$ $\\Delta(A)$ . To simplify the notation, consider a parameter space $\\Theta$ and suppose $f,\\,w$ and $\\pi$ share the same parameter space $\\Theta$ , i.e., there exists a mapping $\\Theta\\ \\overline{{\\ni}}\\ \\theta\\mapsto\\left(f_{\\theta},w_{\\theta},\\overline{{\\pi}}_{\\theta}\\right)$ , and redefine the dual space with $\\mathcal{F}=\\bar{\\mathcal{F}}(\\Theta):=\\{f_{\\theta}\\,|\\,\\theta\\in\\Theta\\}$ .5 We define the associated function approximation error with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\epsilon_{\\Theta}:=\\operatorname*{min}_{\\pi\\in\\Pi_{\\beta}}\\left\\{\\bar{B}_{\\mathcal{V},\\Xi}\\epsilon_{\\Theta}(\\pi)+2(1-\\gamma)\\left\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\right\\|_{1,\\bar{\\mu}}\\right\\},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{B}_{\\mathcal{V},\\Xi}:=\\operatorname*{max}\\{\\bar{B}_{\\mathcal{V}},B_{\\Xi},\\|\\tilde{\\xi}^{*}\\|_{\\infty}\\},B_{\\Xi}:=\\operatorname*{max}_{\\xi\\in\\Xi}\\|\\xi\\|_{\\infty}}\\end{array}$ and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\epsilon_{\\Theta}(\\pi):=\\operatorname*{min}_{\\theta\\in\\Theta}\\left\\{\\left\\|f_{\\theta}-\\check{f}^{\\pi}\\right\\|_{1,\\mu\\beta}+\\|w_{\\theta}-\\check{w}^{\\pi}\\|_{1,\\mu}+B_{\\underline{{\\mathcal{W}}}}\\left\\|\\pi_{\\theta}-\\pi\\right\\|_{\\mathrm{TV},\\mu}\\right\\}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "denotes the $\\pi$ -specific function approximation error of $\\Theta$ . Here, $B_{\\underline{{\\mathcal{W}}}}:=\\operatorname*{max}_{\\theta\\in\\Theta}\\left\\|\\underline{{w}}_{\\theta}\\right\\|_{\\infty}$ is the boundedness of ${\\underline{{w_{\\theta}}}}(s)$ and $\\Vert\\cdot\\Vert_{\\mathrm{TV},\\mu}$ is the mean total variation (TV) distance with respect to $\\mu$ , given by $\\begin{array}{r}{\\|\\pi-\\pi^{\\prime}\\|_{\\mathrm{TV},\\mu}:=\\mathbb{E}_{\\mu}\\sum_{a}|\\pi(a|\\dot{s})-\\pi^{\\prime}(a|s)|}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Finally, we conclude this section by introducing key quantities of the policy extraction for the subsequent analysis. Let $B_{\\Pi}:=\\mathrm{max}_{\\theta\\in\\Theta}\\left\\lVert\\pi_{\\theta}/\\pi_{0}\\right\\rVert_{\\infty}$ denote the size of the policy class with respect to some fixed base policy $\\pi_{0}$ . Also let $\\begin{array}{r}{\\epsilon_{\\Xi}:=\\operatorname*{min}_{\\xi\\in\\Xi}\\|\\xi\\!-\\!\\tilde{\\xi}^{*}\\|_{1,\\mu(\\beta+\\pi_{0})}}\\end{array}$ be the function approximation error of $\\Xi$ , where $\\tilde{\\xi}^{*}(s,a):=\\tilde{q}^{*}(s,a)-\\tilde{v}^{*}(s)$ is the optimal advantage function. ", "page_idx": 7}, {"type": "text", "text": "6.2 The Suboptimality Bound ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Unifying the saddle-point problem and the policy extraction problem, we arrive at the aggregated loss function ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\boldsymbol{\\theta}):=\\mathcal{L}_{\\mathrm{SP}}(\\boldsymbol{\\theta})+\\mathcal{L}_{\\mathrm{X}}(\\boldsymbol{\\theta}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{{\\mathcal{L}}_{\\mathrm{SP}}(\\theta):=-\\operatorname*{min}_{v\\in\\mathcal{V}}K(v,f_{\\theta})}\\end{array}$ is the loss of $f_{\\theta}$ as a dual solution (cf. Eq. (7)) and ${\\mathcal{L}}_{\\mathrm{X}}(\\theta):=$ $D_{\\Xi}(f_{\\theta};w_{\\theta},\\pi_{\\theta})$ is the loss of the policy extraction from $f_{\\theta}$ to $\\pi_{\\theta}$ (cf. Eq. (11)). Let us denote the corresponding estimation error by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathrm{est}}(\\theta):=\\mathcal{L}(\\theta)-\\operatorname*{min}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We also define the aggregated function approximation error with ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varepsilon_{\\mathrm{app},\\Pi}(\\mathcal{V},\\Theta,\\Xi):=\\left(2+3B_{\\mathcal{F}}\\right)\\varepsilon_{\\mathrm{app},\\mathrm{V}}(\\mathcal{V},\\mathcal{F}(\\Theta))+3\\epsilon_{\\Theta}+\\{B_{\\mathcal{F}}+(1-\\gamma)B_{\\Pi}\\}\\,\\epsilon_{\\Xi}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The following theorem establishes an upper bound on the policy suboptimality in terms of $\\epsilon_{\\mathrm{{est}}}(\\theta)$ and $\\varepsilon_{\\mathrm{app,II}}(\\gamma,\\Theta,\\Xi)$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 6.1. For all $\\theta\\in\\Theta$ , we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi_{\\theta})\\leq\\frac{\\|\\tilde{w}^{\\pi_{\\theta}}\\|_{\\infty}}{1-\\gamma}\\left\\{\\epsilon_{e s t}(\\theta)+\\varepsilon_{a p p,\\Pi}(\\mathcal{V},\\Theta,\\Xi)\\right\\}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Proof (sketch). At the heart of the proof is the following inequalities: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\Gamma(\\pi_{\\theta})\\leq\\frac{D_{\\mathrm{F}}^{*}(f_{\\theta})+\\mathcal{L}_{\\mathrm{X}}(\\theta)+B^{\\prime}\\epsilon_{\\Xi}}{1-\\gamma}\\leq\\frac{\\epsilon_{\\mathrm{est}}(\\theta)+\\varepsilon_{\\mathrm{app},\\Pi}(\\mathcal{V},\\Theta,\\Xi)}{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\Gamma(\\pi_{\\theta}):=\\mathbb{E}_{\\mu,\\pi_{\\theta}}\\left[\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right]$ denotes the average action value gap with policy $\\pi_{\\theta}$ and $B^{\\prime}:=B_{\\mathcal{F}}+(1-\\dot{\\gamma})B_{\\Pi}$ . Here, the lower clipping of $\\underline{w}$ (Eq. (11)) and the stability of the primal solution (Lemma 5.3) are essential for deriving the first and the second inequality, respectively. Then, the proof is completed by bounding $\\tilde{J}^{*}-\\tilde{J}(\\pi_{\\theta})$ in terms of $\\Gamma(\\pi_{\\theta})$ invoking the performance difference lemma in the worst-case environment. See Appendix F.1 for the full proof. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.1 suggests that one can minimize the policy suboptimality up to the function approximation error on two conditions, i.e., the weight factor $\\|\\tilde{w}^{\\pi_{\\theta}}\\|_{\\infty}$ is appropriately bounded and the loss function ${\\mathcal{L}}(\\theta)$ is minimized. In the following, we first discuss how to satisfy the first condition. ", "page_idx": 8}, {"type": "text", "text": "A trivial way of bounding $\\|\\tilde{w}^{\\pi_{\\theta}}\\|_{\\infty}$ is uniformly bounding it with respect to all $\\theta\\in\\Theta$ . Define $\\tilde{C}_{\\infty}:=$ $\\operatorname*{max}_{\\theta\\in\\Theta}\\left\\|\\tilde{w}^{\\pi_{\\theta}}\\right\\|_{\\infty}$ , which we refer to as the uniform truncated concentrability (UTC) coefficient. Since $\\|\\tilde{w}^{\\pi_{\\theta}}\\|_{\\infty}\\leq\\tilde{C}_{\\infty}$ , we get the following simple suboptimality bound. ", "page_idx": 8}, {"type": "text", "text": "Corollary 6.1. For all $\\theta\\in\\Theta$ , we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi_{\\theta})\\leq\\frac{\\tilde{C}_{\\infty}}{1-\\gamma}\\left\\{\\epsilon_{e s t}(\\theta)+\\varepsilon_{a p p,\\Pi}(\\mathcal{V},\\Theta,\\Xi)\\right\\}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that $\\tilde{C}_{\\infty}$ is always finite because of the compactness of the whole policy space $\\Delta(\\mathcal{A})^{S}$ and the continuity and well-definedness of $\\pi\\mapsto\\|\\tilde{w}^{\\pi}\\|_{\\infty}$ . Thus, Eq. (18) is non-vacuous for arbitrary data distributions as opposed to the conventional concentrability-based results. Moreover, if the conventional bounds are non-vacuous, $\\tilde{C}_{\\infty}$ recovers the conventional concentrability coefficient as $\\tilde{w}^{\\pi}=w^{\\pi}$ . ", "page_idx": 8}, {"type": "text", "text": "Eq. (18) can be further refined using the localized variants of the uniform coefficient $\\tilde{C}_{\\infty}$ . The results are presented as Corollaries F.2 and F.3 in Appendix F.3 due to space limitation. ", "page_idx": 8}, {"type": "text", "text": "6.3 Sample Complexity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Now that given Corollary 6.1 (and Corollaries F.2 and F.3 as well) bounds the weight factor $\\|\\tilde{w}^{\\pi_{\\theta}}\\|_{\\infty}$ with some milder variants of the concentrability coefficient, the remaining task, minimizing ${\\mathcal{L}}(\\theta)$ , is handled within the framework of the statistical learning, leading to a sample complexity bound. Let ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{L}}(\\boldsymbol{\\theta}):=\\operatorname*{max}_{\\boldsymbol{v}\\in\\mathcal{V}}\\operatorname*{max}_{\\boldsymbol{\\xi}\\in\\Xi}\\frac{1}{n}\\sum_{\\boldsymbol{z}\\in\\mathcal{D}}\\hat{\\mathcal{L}}_{\\boldsymbol{z}}(\\boldsymbol{\\theta};\\boldsymbol{v},\\boldsymbol{\\xi}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "be the empirical loss function where ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{\\mathcal{L}}_{z}(\\theta;v,\\xi):=-\\left\\{(1-\\gamma)\\,v(s)+f_{\\theta}(s,a)\\left\\{r+\\gamma v(s^{\\prime})-v(s)\\right\\}+\\frac{(1-\\gamma)^{2}\\,{\\left(v^{2}(s)+\\gamma v^{2}(s^{\\prime})\\right)}}{2}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad+\\,f_{\\theta}(s,a)\\xi(s,a)-\\underline{{w_{\\theta}(s)\\mathbb{E}_{a^{\\prime}\\sim\\pi v_{\\theta}(s)}\\left[\\xi(s,a^{\\prime})\\right]}}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(20\\pi^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "is the one-sample loss function, $z\\equiv(s,a,r,s^{\\prime})\\in\\mathcal{S}\\times\\mathcal{A}\\times[0,1]\\times\\mathcal{S}$ denotes a transition record. Note that Eq. (20) is an unbiased estimator of the objective function ${\\mathcal{L}}(\\theta)$ .6 Therefore, it is expected that the oracle loss ${\\mathcal{L}}(\\theta)$ can be approximated with the empirical loss $\\hat{\\mathcal{L}}(\\theta)$ and hence the oracle estimation error $\\epsilon_{\\mathrm{est}}(\\theta)$ can be approximated with the empirical estimation error ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\hat{\\epsilon}_{\\mathrm{est}}(\\theta):=\\hat{\\mathcal{L}}(\\theta)-\\operatorname*{min}_{\\theta\\in\\Theta}\\hat{\\mathcal{L}}(\\theta).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Formalizing such an intuition, the following corollary shows the empirical counterpart of Corollary 6.1.   \nSee Appendix F.5 for the proof. ", "page_idx": 8}, {"type": "text", "text": "Corollary 6.2. Let $\\mathcal{H}\\equiv\\mathcal{H}(\\mathcal{V},\\Theta,\\Xi):=\\{z\\mapsto\\hat{\\mathcal{L}}_{z}(\\theta;v,\\xi)\\,|\\,\\theta\\in\\Theta,v\\in\\mathcal{V},\\xi\\in\\Xi\\}$ be the class of the one-sample loss functions and $\\Re_{n}(\\mathcal{H})$ be its Rademacher complexity (Definition B.1). Then, for all $\\theta\\in\\Theta$ and $\\delta\\in(0,1)$ , with probability $1-\\delta$ , we have ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi_{\\theta})\\leq\\frac{\\tilde{C}_{\\infty}}{1-\\gamma}\\left\\{\\hat{\\epsilon}_{e s t}(\\theta)+\\varepsilon_{a p p,\\Pi}(\\mathcal{V},\\Theta,\\Xi)+4\\Re_{n}(\\mathcal{H})+B_{a l l}\\sqrt{\\frac{2\\ln(2/\\delta)}{n}}\\right\\},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $B_{a l l}:=(1-\\gamma)B_{\\nu}+B_{\\mathcal{F}}\\bar{B}_{\\mathcal{V}}+(1-\\gamma)^{2}B_{\\mathcal{V}}^{2}+(B_{\\mathcal{F}}+B_{\\mathcal{W}})B_{\\Xi}$ is the aggregated scale factor. ", "page_idx": 9}, {"type": "text", "text": "The corollary above implies that minimizing Eq. (19), which is possible with the minimax optimizers such as the one developed by Thekumparampil et al. (2019), gives a near-optimal policy in terms of the worst-case environment $\\tilde{\\mathcal{M}}$ , up to the error proportional to the sum of the optimization error $\\hat{\\epsilon}_{\\mathrm{est}}(\\theta)$ , the approximation error $\\varepsilon_{\\mathrm{app,\\Pi}}(\\gamma,\\Theta,\\Xi)$ and the statistical error $O(\\mathfrak{R}_{n}(\\mathcal{H})+n^{-1/2})$ . We refer to this method as worst-case minimax reinforcement learning (WMRL). ", "page_idx": 9}, {"type": "text", "text": "The next corollary gives the sample complexity of WMRL in the simplified case where the function approximators $\\nu$ , $\\Theta$ and $\\Xi$ are all finite sets. ", "page_idx": 9}, {"type": "text", "text": "Corollary 6.3. Suppose $\\nu$ , $\\mathcal{F}$ and $\\Xi$ are all finite sets and $\\hat{\\epsilon}_{e s t}(\\theta)=\\varepsilon_{a p p,\\Pi}(\\mathcal{V},\\Theta,\\Xi)=0$ . Take any $\\epsilon>0$ and $0<\\delta<1$ . Then, we have $\\tilde{J}^{*}-\\tilde{J}(\\pi_{\\theta})\\leq\\epsilon$ with probability $1-\\delta\\;i f$ ", "page_idx": 9}, {"type": "equation", "text": "$$\nn=\\Omega\\left(\\frac{B_{a l l}^{2}\\tilde{C}_{\\infty}^{2}}{\\epsilon^{2}(1-\\gamma)^{2}}\\ln\\frac{\\mathcal{N}}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\mathcal{N}:=\\left|\\mathcal{V}\\right|\\left|\\Theta\\right|\\left|\\Xi\\right|$ denote the product of the cardinalities of the function approximators. ", "page_idx": 9}, {"type": "text", "text": "Proof. It follows from Corollary 6.2 with Massart\u2019s lemma (Lemma B.2). ", "page_idx": 9}, {"type": "text", "text": "A few remarks follow. First, we can replace the UTC coefficient $\\tilde{C}_{\\infty}$ with the localized variants (Definitions F.1 and F.2) to obtain tighter bounds, by making the same argument starting from Corollaries F.2 and F.3 instead of Corollary 6.1, respectively. Second, there are implicit dependencies $B\\nu\\,\\geq\\,\\|\\tilde{v}^{*}\\|_{\\infty}$ and $B_{\\Xi}\\,\\geq\\,\\|\\tilde{\\xi}^{*}\\|_{\\infty}$ due to the realizability $\\tilde{v}^{*}\\in\\mathcal{V}$ and $\\tilde{\\xi}^{*}\\,\\in\\,\\Xi$ that follows from $\\varepsilon_{\\mathrm{app,}\\Pi}(\\mathcal{V},\\Theta,\\Xi)=0$ . Hence, Eq. (22) has an implicit $\\gamma$ -dependency through the scale factor $B_{\\mathrm{all}}$ , which brings an extra $\\Theta((1-\\gamma)^{-2})$ factor in the worst case. Table 1 adopts this form for the fairness of comparison. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "To develop an offilne RL method for challenging data distributions, we have introduced and studied a generalization of the conventional framework called worst-case offilne RL. As a result, we have shown it is possible to learn a worst-case optimal policy without any data-support conditions. Moreover, the presented sample complexity bound strictly improves the previous state of the art under the single-policy realizability and the single-policy concentrability, suggesting the utility of the proposed method even with non-challenging data distributions. ", "page_idx": 9}, {"type": "text", "text": "We anticipate the presented results are readily extendable to continuous state-action spaces, except that the truncated concentrability coefficients are not unconditionally finite anymore. The results in Appendix F.3 are particularly useful in this context, yet the complete picture on the conditions of their boundedness largely remains to be studied in future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The author is grateful to LY Corporation for providing travel funding. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Abe, N., Melville, P., Pendus, C., Reddy, C. K., Jensen, D. L., Thomas, V. P., Bennett, J. J., Anderson, G. F., Cooley, B. R., Kowalczyk, M., Domick, M., and Gardinier, T. (2010). Optimizing debt ", "page_idx": 9}, {"type": "text", "text": "collections using constrained reinforcement learning. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201910, page 75\u201384, New York, NY, USA. Association for Computing Machinery.   \nAntos, A., Szepesv\u00e1ri, C., and Munos, R. (2008). Learning near-optimal policies with bellmanresidual minimization based fitted policy iteration and a single sample path. Machine Learning, 71:89\u2013129.   \nChen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning. In International Conference on Machine Learning, pages 1042\u20131051. PMLR.   \nChen, J. and Jiang, N. (2022). Offilne reinforcement learning under value and density-ratio realizability: the power of gaps. In Uncertainty in Artificial Intelligence, pages 378\u2013388. PMLR.   \nChen, Y. and Wang, M. (2016). Stochastic primal-dual methods and sample complexity of reinforcement learning. arXiv preprint arXiv:1612.02516.   \nFang, X., Zhang, Q., Gao, Y., and Zhao, D. (2022). Offline reinforcement learning for autonomous driving with real world driving data. In 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC), pages 3417\u20133422.   \nFujimoto, S. and Gu, S. S. (2021). A minimalist approach to offilne reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145.   \nJiang, N. and Huang, J. (2020). Minimax value interval for off-policy evaluation and policy optimization. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red Hook, NY, USA. Curran Associates Inc.   \nKumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191.   \nLee, J., Jeon, W., Lee, B., Pineau, J., and Kim, K.-E. (2021). Optidice: Offline policy optimization via stationary distribution correction estimation. In International Conference on Machine Learning, pages 6120\u20136130. PMLR.   \nLevine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Offilne reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643.   \nLiu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. (2020). Provably good batch off-policy reinforcement learning without great exploration. Advances in neural information processing systems, 33:1264\u20131274.   \nMunos, R. (2003). Error bounds for approximate policy iteration. In Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML\u201903, page 560\u2013567. AAAI Press.   \nMunos, R. and Szepesv\u00e1ri, C. (2008). Finite-time bounds for fitted value iteration. Journal of Machine Learning Research, 9(5).   \nNachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D. (2019). Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074.   \nOzdaglar, A. E., Pattathil, S., Zhang, J., and Zhang, K. (2023). Revisiting the linear-programming framework for offline RL with general function approximation. In International Conference on Machine Learning, pages 26769\u201326791. PMLR.   \nPrudencio, R. F., Maximo, M. R., and Colombini, E. L. (2023). A survey on offline reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions on Neural Networks and Learning Systems.   \nPuterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons.   \nRashidinejad, P., Zhu, H., Yang, K., Russell, S., and Jiao, J. (2023). Optimal conservative offline RL with general function approximation via augmented lagrangian. In The Eleventh International Conference on Learning Representations.   \nShalev-Shwartz, S. and Ben-David, S. (2014). Understanding machine learning: From theory to algorithms. Cambridge university press.   \nSriperumbudur, B. K., Fukumizu, K., Gretton, A., Sch\u00f6lkopf, B., and Lanckriet, G. R. (2009). On integral probability metrics,\\phi-divergences and binary classification. arXiv preprint arXiv:0901.2698.   \nThekumparampil, K. K., Jain, P., Netrapalli, P., and Oh, S. (2019). Efficient algorithms for smooth minimax optimization. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.   \nUehara, M., Kallus, N., Lee, J. D., and Sun, W. (2023). Offline minimax soft-q-learning under realizability and partial coverage. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S., editors, Advances in Neural Information Processing Systems, volume 36, pages 12797\u201312809. Curran Associates, Inc.   \nUehara, M. and Sun, W. (2022). Pessimistic model-based offilne reinforcement learning under partial coverage. In International Conference on Learning Representations.   \nXie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021). Bellman-consistent pessimism for offilne reinforcement learning. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems.   \nXie, T., Foster, D. J., Bai, Y., Jiang, N., and Kakade, S. M. (2022). The role of coverage in online reinforcement learning. arXiv preprint arXiv:2210.04157.   \nYin, M. and Wang, Y.-X. (2021). Towards instance-optimal offline reinforcement learning with pessimism. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems, volume 34, pages 4065\u20134078. Curran Associates, Inc.   \nYu, C., Liu, J., Nemati, S., and Yin, G. (2021). Reinforcement learning in healthcare: A survey. ACM Comput. Surv., 55(1).   \nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., and Ma, T. (2020). MOPO: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142.   \nZanette, A. (2023). When is realizability sufficient for off-policy reinforcement learning? In International Conference on Machine Learning, pages 40637\u201340668. PMLR.   \nZhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J. (2022). Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730\u20132775. PMLR.   \nZhang, J., Hong, M., Wang, M., and Zhang, S. (2021). Generalization bounds for stochastic saddle point problems. In International Conference on Artificial Intelligence and Statistics, pages 568\u2013576. PMLR. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Additional Discussion on Table 1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "In Table 1, we compare our result with the previous results on the sample complexity of the conventional offilne RL under the weakest known assumptions. The weakest known means that it relies only on the assumptions of the single-policy realizability and the single-policy concentrability. Specifically, they do not assume the Bellman completeness (Munos and Szepesv\u00e1ri, 2008; Xie et al., 2021; Chen and Jiang, 2022) or its variants (Zanette, 2023; Rashidinejad et al., 2023), the model-based realizability (Uehara and Sun, 2022) or the all-policy realizability (Jiang and Huang, 2020), which are strictly stronger than the single-policy realizability and deemed to be rather stringent (Chen and Jiang, 2019). Below, we discuss each of the methods listed in the table. ", "page_idx": 11}, {"type": "table", "img_path": "63VajkIDEu/tmp/64f995d563b43349765133b4e32c52c2a4068fac478b6024b788f62140f98f10.jpg", "table_caption": [], "table_footnote": ["Table 2: Basic Notation "], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Leveraging the Lagrangian-based formulation of RL, Zhan et al. (2022) showed the polynomial sample complexity only with the single-policy realizability and the single-policy concentrability for the first time. However, the order of the sample complexity bound $O(\\epsilon^{-6})$ is significantly looser than the standard statistical rate $O(\\epsilon^{-2})$ . Moreover, their realizability assumption requires the function approximators to include the value/weight functions associated with a policy $\\pi_{n}$ depending on the sample size $n$ . In particular, the resulting realizability condition is difficult to interpret since there is no explicit characterization of $\\pi_{n}$ . ", "page_idx": 12}, {"type": "text", "text": "Chen and Jiang (2022) took a different approach, the pessimistic value learning, achieving the statistically reasonable rate $O(\\epsilon^{-2})$ . One of the main drawbacks of their result is, however, that the sample complexity bound blows up if the action value gap $C_{\\mathrm{gap}}$ is near zero. Here, the action value gap is defined as the minimum gap in the values of the best action and the second best action, $C_{\\mathrm{gap}}=\\mathrm{min}_{s\\in\\mathcal{S}}\\,\\mathrm{max}_{a\\in\\mathcal{A}}\\{q^{*}(s,a)-\\mathrm{max}_{a^{\\prime}\\ne a}\\,q^{*}(s,a^{\\prime})\\}$ , which becomes (near) zero if there exist two actions that are (near) optimal for even one state. In addition, it only competes with the optimal policy $\\pi^{*}$ and requires the data to cover the corresponding visitation distribution $d^{\\pi^{*}}$ . Finally, the time-horizon dependency $O(H^{5})$ is a bit worse than the other results in the table. ", "page_idx": 12}, {"type": "text", "text": "Ozdaglar et al. (2023) showed two distinct results: one requiring a completeness-type assumption and the other requiring realizability and action-value-gap assumptions, in addition to concentrability. We included the latter to the table. Roughly speaking, their result is similar to that of Chen and Jiang (2022) except with the difference in the infinite/finite time horizons. Consequently, it also requires the action gap to be bounded away from zero. We note that their algorithm relies on a constrained LP, where the number of the constraints is equal to the size of $\\boldsymbol{S}$ , making it possibly difficult to scale to practical problems. ", "page_idx": 12}, {"type": "text", "text": "Uehara et al. (2023) also proposed two distinct methods: one establishes slower $O(\\epsilon^{-8})$ rate with the entropy regularization method, and the other establishes a sample complexity depending on so-called the soft action value gap. We only shows the latter in Table 1. For the latter result, the new condition on the soft action value gap relaxes those imposed on the ordinary action value gap by Chen and Jiang (2022); Ozdaglar et al. (2023). The order of the resulting sample complexity bound is $O(\\epsilon^{-2+4/\\beta_{\\mathrm{gap}}})$ , where $\\beta_{\\mathrm{gap}}>0$ corresponds to the lower-tail exponent of the distribution of the state-wise action value gaps. We also note that these bounds are explicitly depending on the size of the action space $\\boldsymbol{\\mathcal{A}}$ , which could be a potential drawback of the entropy-based method. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Compared to these results, our sample complexity bound has the following advantages. First of all, it gives the performance guarantees even if there is no concentrable policies. Second, it only requires the model-free realizability with respect to a (fixed) worst-case optimal policy $\\tilde{\\pi}^{*}$ . Third, it achieves the statistically reasonable rate $O(\\dot{\\epsilon}^{-2})$ without any dependencies on the action value gap. Finally, it has no explicit dependency in the algorithm on the size of $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ , even in the policy extraction process. ", "page_idx": 13}, {"type": "text", "text": "B Rademacher Complexity and Uniform Convergence ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we introduce the Rademacher complexity and its properties as well as the celebrated uniform convergence theorem. Below, let $\\mathcal{Z}$ be a sample space, $p\\in\\Delta(\\mathcal{Z})$ be a probability distribution on it, and $\\mathcal{G}\\subset\\bar{\\mathbb{R}}^{\\mathcal{Z}}$ be a set of functions from $\\mathcal{Z}$ to $\\mathbb{R}$ . ", "page_idx": 13}, {"type": "text", "text": "Definition B.1 (Rademacher complexity). The Rademacher complexity of $\\mathcal{G}$ with the sample size $n\\geq1$ is given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Re_{n}(\\mathcal{G}):=\\mathbb{E}_{\\sigma^{n},z^{n}}\\left[\\operatorname*{sup}_{g\\in\\mathcal{G}}\\frac{1}{n}\\sum_{i=1}^{n}\\sigma_{i}g(z_{i})\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbb{E}_{\\sigma^{n},z^{n}}$ denotes the expectation with respect to samples $\\sigma^{n}=\\left\\{\\sigma_{i}\\right\\}_{i=1}^{n}$ and $z^{n}=\\left\\{z_{i}\\right\\}_{i=1}^{n}$ drawn from Uniform $^{n}(\\{-1,+1\\})$ and $p^{n}$ , respectively. ", "page_idx": 13}, {"type": "text", "text": "Lemma B.1 (Uniform convergence theorem). Suppose $\\|g\\|_{\\infty}\\leq c$ for all $g\\,\\in\\,{\\mathcal{G}}$ . Then, for all $\\delta\\in(0,1).$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{g\\in\\mathcal{G}}\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\left[g(z_{i})\\right]-\\mathbb{E}\\left[g(z_{1})\\right]\\right|\\le2\\Re_{n}(\\mathcal{G})+c\\sqrt{\\frac{2\\ln(2/\\delta)}{n}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with probability $1-\\delta$ on the draw of $z^{n}\\sim p^{n}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Refer to Claim 1, Theorem 26.5, Shalev-Shwartz and Ben-David (2014) for oneside high-probability bound and apply it to both of $\\begin{array}{r}{\\operatorname*{sup}_{g\\in\\mathcal{G}}\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\left[g(z_{i})\\right]-\\mathbb{E}\\left[g(z_{1})\\right]\\right\\}}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{sup}_{g\\in\\mathcal{G}}(-1)\\left\\{\\frac{1}{n}\\sum_{i=1}^{n}\\left[g(z_{i})\\right]-\\mathbb{E}\\left[g(z_{1})\\right]\\right\\}}\\end{array}$ setting the confidence parameter to $\\delta/2$ . The proof is completed by taking the union of the events that these high-probability bounds do not hold. \u518f\u53e3 ", "page_idx": 13}, {"type": "text", "text": "The following is another well-known result of the Rademacher complexity. ", "page_idx": 13}, {"type": "text", "text": "Lemma B.2 (Massart\u2019s lemma). For a finite set $\\mathcal{G}$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\Re_{n}({\\mathcal{G}})\\leq M({\\mathcal{G}}){\\sqrt{\\frac{2\\ln|{\\mathcal{G}}|}{n}}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 13}, {"type": "equation", "text": "$$\nM(\\mathcal{G}):=\\operatorname*{sup}_{g,g^{\\prime}\\in\\mathcal{G},\\;z\\in\\mathcal{Z}}\\left|g(z)-g^{\\prime}(z)\\right|.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Refer to Shalev-Shwartz and Ben-David (2014), Lemma 26.8. ", "page_idx": 13}, {"type": "text", "text": "C Basic Properties of Regularized Lagrangian ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we show basic property of regularized Lagrangian (Eq. (7)). ", "page_idx": 13}, {"type": "text", "text": "Lemma C.1 (Primal Lipschitz continuity). For all $v,v^{\\prime}:S\\times A\\rightarrow[0,(1-\\gamma)^{-1}]$ and $f\\in\\mathcal F$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n|K(v,f)-K(v^{\\prime},f)|\\le(2+B_{\\mathcal{F}})\\,\\|v-v^{\\prime}\\|_{1,\\bar{\\mu}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof. Observe that by Eq. (7) ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{K(v,f)-K(v^{\\prime},f)|\\leq(1-\\gamma)\\left\\|v-v^{\\prime}\\right\\|_{1,\\mu}+\\left\\|f\\cdot(\\delta^{\\mathrm{TD}}v-\\delta^{\\mathrm{TD}}v^{\\prime})\\right\\|_{1,\\mu\\beta}+(1-\\gamma)\\left\\|v-v^{\\prime}\\right\\|_{1,\\bar{\\mu}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The second term on the RHS is further bounded as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert f\\cdot(\\delta^{\\mathrm{TD}}v-\\delta^{\\mathrm{TD}}v^{\\prime})\\right\\rVert_{1,\\mu\\beta}=\\left\\lVert f\\cdot(I-\\gamma\\mathcal{T})(v-v^{\\prime})\\right\\rVert_{1,\\mu\\beta}}&{}\\\\ {\\stackrel{\\mathrm{(a)}}{\\leq}B_{\\mathcal{F}}\\left\\lVert(I-\\gamma\\mathcal{T})(v-v^{\\prime})\\right\\rVert_{1,\\mu\\beta}}&{}\\\\ {\\stackrel{\\mathrm{(b)}}{\\leq}B_{\\mathcal{F}}\\left\\lVert v-v^{\\prime}\\right\\rVert_{1,\\bar{\\mu}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (a) follows from H\u00f6lder\u2019s inequality and (b) follows from the triangle inequality. We obtain the desired result by summing up both sides of the two inequalities since $\\mu\\le\\bar{\\mu}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma C.2 (Stability of minimax value against dual error). For all $v\\geq0$ and $\\pi\\in\\Pi_{\\beta}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\nK(v,f)\\geq U^{*}-\\bar{B}\\nu\\left\\|f-\\check{f}^{\\pi}\\right\\|_{1,\\mu\\beta}-2(1-\\gamma)\\left\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\right\\|_{1,\\bar{\\mu}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\bar{B}_{\\mathcal{V}}:=\\operatorname*{max}\\big\\{1+\\gamma B_{\\mathcal{V}},B_{\\mathcal{V}}\\big\\}.$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K(v,f)\\overset{\\cong}{\\geq}K(v,f)-D_{\\nabla}^{\\pi}(v)}\\\\ &{\\overset{\\cong}{=}U(\\pi)-\\mathbb{E}_{\\mu,\\beta}\\Big[(f-\\tilde{f}^{\\pi})(s,a)\\,(I-\\gamma\\tilde{T})(v-\\tilde{v}^{\\pi})(s,a)\\Big]-D_{\\mathbf{F}}^{\\pi}(f)}\\\\ &{\\overset{\\cong}{=}U(\\pi)-\\mathbb{E}_{\\mu,\\beta}\\Big[(f-\\tilde{f}^{\\pi})(s,a)\\,(I-\\gamma\\tilde{T})(v-\\tilde{v}^{\\pi})(s,a)\\Big]}\\\\ &{\\quad\\quad\\quad-\\mathbb{E}_{\\mu,\\beta}\\,\\Big[(f-\\tilde{f}^{\\pi})(s,a)\\,\\{\\tilde{v}^{\\pi}(s)-\\tilde{q}^{\\pi}(s,a)\\}\\Big]}\\\\ &{\\overset{\\mathrm{(d)}}{=}U(\\pi)+\\mathbb{E}_{\\mu,\\beta}\\Big[(f-\\tilde{f}^{\\pi})(s,a)\\,\\Big\\{r(s,a)+\\gamma\\tilde{T}v(s,a)-v(s)\\Big\\}\\Big]}\\\\ &{\\overset{\\cong}{\\geq}U(\\pi)-\\bar{B}_{\\nu}\\,\\Big\\Vert f-\\tilde{f}^{\\pi}\\Big\\Vert_{1,\\mu\\beta}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where (a) follows from the nonegativity of $D_{\\mathrm{V}}^{\\pi}(v)$ , (b) from Lemma E.3, (c) from $D_{\\mathrm{F}}^{\\pi}(\\breve{f}^{\\pi})=0$ , (d) from $\\tilde{q}^{\\pi}(s,a)-\\gamma\\tilde{T}v(s,a)=r(s,a)$ and (e) from H\u00f6lder\u2019s inequality with $|r+\\gamma\\tilde{\\mathcal{T}}v-v|\\le\\bar{B}_{\\mathcal{V}}$ . The claim is then proved by the continuity of $U(\\pi)$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{U^{*}-U(\\pi)=(1-\\gamma)\\,\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\|_{1,\\mu}+\\displaystyle\\frac{(1-\\gamma)^{2}}{2}\\mathbb{E}_{\\bar{\\mu}}\\,[(\\tilde{v}^{*}+\\tilde{v}^{\\pi})(\\tilde{v}^{*}-\\tilde{v}^{\\pi})]}\\\\ &{\\qquad\\qquad\\leq(1-\\gamma)\\,\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\|_{1,\\mu}+(1-\\gamma)\\,\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\|_{1,\\bar{\\mu}}}\\\\ &{\\qquad\\qquad\\leq2(1-\\gamma)\\,\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\|_{1,\\bar{\\mu}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "D Proofs of Section 4 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. The claim $\\tilde{\\mathcal{M}}\\in\\mathfrak{U}$ is trivial from Definition 4.1. The other claim, $J(\\pi|\\tilde{\\mathcal{M}})\\,\\le\\,J(\\pi|\\mathcal{M}^{\\prime})$ , follows from that i) $\\tilde{r}(s,a)\\leq r(s,a)$ with $\\tilde{r}(s,a)=\\mathbb{E}_{y\\sim R(y|s,a)}\\left[y\\right]$ for all $s\\in S$ and $a\\in A$ and ii) the truncated transition probability $\\tilde{\\cal T}(\\cdot|s,a)$ , when in conflict with $T(\\cdot|s,a)$ , leads to the absorbing state $\\bot$ , which has the lowest possible cumulative discounted value, zero. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "D.2 Proof of Corollary 4.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. Since the inequality is trivial from Eq. (1), we show the equality. Observe that the state-action pair $\\left({{s}_{t}},{{a}_{t}}\\right)$ at time $t\\,\\geq\\,0$ stays inside $\\operatorname{supp}(\\mu\\beta)$ almost surely for all $t~\\geq~0$ if $\\operatorname{supp}(d^{\\pi}\\pi)\\;\\subset$ $\\operatorname{supp}(\\mu\\beta)$ . Hence, the law of the reward sequence $\\{r_{t}\\}_{t\\ge0}$ generated with $\\pi$ is the same under $\\mathcal{M}$ and $\\tilde{\\mathcal{M}}$ , leading to $J(\\pi|\\tilde{\\mathcal{M}})=J(\\pi|\\mathcal{M})$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "D.3 Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. We show denying the conclusion results in contradiction. Take a policy $\\pi\\;\\;\\in\\;\\;{\\tilde{\\Pi}}^{*}$ . Let $\\chi_{\\beta}(s,a)\\;:=\\;\\mathbb{I}\\,\\{(s,\\bar{a})\\in\\mathrm{supp}(\\beta)\\}$ be the indicator function of $\\operatorname{supp}(\\beta)$ . Let $z^{\\pi}(s)\\ :=$ $\\textstyle\\sum_{a}\\chi_{\\beta}(s,a)\\,\\pi(a|s)$ be the mass of $\\pi(\\cdot|s)$ inside the support of $\\beta$ , which satisfies $0\\leq z^{\\pi}(s)\\leq1$ for all $s\\in S$ . ", "page_idx": 15}, {"type": "text", "text": "Let $\\pi^{\\prime}\\,:\\,S\\,\\rightarrow\\,\\Delta(A)$ be a policy proportional to $\\pi$ with the support restricted to $\\operatorname{supp}(\\beta)$ , i.e., $z_{\\pi}(s)\\,\\pi^{\\prime}(a|s)=\\chi_{\\beta}(s,a)\\,\\pi(a|s)$ for all $s\\in S$ and $a\\in A$ . ", "page_idx": 15}, {"type": "text", "text": "By the definitions of ${\\tilde{r}}^{\\pi}$ and $\\tilde{T}^{\\pi}$ , we now have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\widetilde r}^{\\pi}(s)=\\sum_{a}\\chi_{\\beta}(s,a)\\,\\pi(a|s)\\,r(s,a)}}\\\\ {{\\displaystyle\\qquad=\\sum_{a}z^{\\pi}(s)\\,{\\pi^{\\prime}}(a|s)\\,r(s,a)}}\\\\ {{\\displaystyle\\qquad=z^{\\pi}(s)\\sum_{a}\\chi_{\\beta}(s,a)\\,{\\pi^{\\prime}}(a|s)\\,r(s,a)}}\\\\ {{\\displaystyle\\qquad=z^{\\pi}(s)\\,{\\widetilde r}^{\\pi^{\\prime}}(s)\\leq{\\widetilde r}^{\\pi^{\\prime}}(s)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{T}^{\\pi}v(s)=\\displaystyle\\sum_{a,s^{\\prime}}\\pi(a|s)\\,\\chi_{\\mu,\\beta}(s,a)\\,T(s^{\\prime}|s,a)\\,v(s^{\\prime})}\\\\ &{\\qquad\\qquad=\\displaystyle\\sum_{a,s^{\\prime}}z^{\\pi}(s)\\,\\pi^{\\prime}(a|s)\\,T(s^{\\prime}|s,a)\\,v(s^{\\prime})}\\\\ &{\\qquad=z^{\\pi}(s)\\displaystyle\\sum_{a,s^{\\prime}}\\pi^{\\prime}(a|s)\\,\\chi_{\\mu,\\beta}(s,a)\\,T(s^{\\prime}|s,a)\\,v(s^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad a\\,\\displaystyle\\int_{0}^{\\pi}\\pi^{\\prime}v(s)\\le\\widetilde{T}^{\\pi^{\\prime}}v(s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "for all $v:S\\rightarrow[0,\\infty)$ and $s\\in S$ . Therefore, we have for all $s\\in S$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{v}^{*}(s)=\\tilde{v}^{\\pi}(s)=(I-\\gamma\\tilde{T}^{\\pi})^{-1}r^{\\pi}(s)=\\sum_{t\\geq0}(\\gamma\\tilde{T}^{\\pi})^{t}r^{\\pi}(s)\\leq\\sum_{t\\geq0}(\\gamma\\tilde{T}^{\\pi^{\\prime}})^{t}r^{\\pi^{\\prime}}(s)=\\tilde{v}^{\\pi^{\\prime}}(s),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which leads to the strong optimality $\\tilde{v}^{\\pi^{\\prime}}=\\tilde{v}^{*}$ . However, we also have $\\pi^{\\prime}\\in\\Pi_{\\beta}$ by the definition, contradicting with the assumption $\\tilde{\\Pi}^{*}\\cap\\Pi_{\\beta}=\\emptyset$ . \u53e3 ", "page_idx": 15}, {"type": "text", "text": "E Proofs of Section 5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "E.1 Proof of Lemma 5.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proof relies on two lemmas (Lemmas E.1 and E.2), where the second one is built on top of the first one. Then, Lemma 5.1 is immediately proved as a special case of Lemma E.2 with $\\pi$ being restricted to $\\tilde{\\Pi}^{*}$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma E.1 gives a saddle-point decomposition of Lagrangian ignoring the offline constraint. ", "page_idx": 15}, {"type": "text", "text": "Lemma E.1 (Incomplete saddle-point decomposition of Lagrangian). For all $\\pi:S\\rightarrow\\Delta(A)$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\cal L}(v,f)={\\cal J}(\\pi)+\\sum_{s,a}\\left(f\\mu\\beta-d^{\\pi}\\pi\\right)(s,a)\\,\\delta^{\\mathrm{TD}}v(s,a).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Comparing the LHS and the RHS, it suffices to show ", "page_idx": 15}, {"type": "equation", "text": "$$\nJ(\\pi)-(1-\\gamma)\\mathbb{E}_{p_{0}}\\left[v(s)\\right]=\\sum_{s,a}d^{\\pi}(s)\\,\\pi(a|s)\\,\\delta^{\\mathrm{TD}}v(s,a).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is seen by simplifying the RHS of the above equation as follows, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle\\sum_{s,a}d^{\\pi}(s)\\,\\pi(a|s)\\,\\delta^{\\mathrm{TD}}v(s,a)\\stackrel{(a)}{=}\\sum_{s}d^{\\pi}(s)\\,(I-\\gamma T^{\\pi})(v^{\\pi}-v)}}}\\\\ {{{}}}\\\\ {{{\\displaystyle\\stackrel{(b)}{=}\\sum_{s}\\,(I-\\gamma T_{*}^{\\pi})d^{\\pi}(s)\\,(v^{\\pi}-v)}}}\\\\ {{{}}}\\\\ {{{\\displaystyle\\stackrel{(c)}{=}(1-\\gamma)\\sum_{s}p_{0}(s)\\,(v^{\\pi}-v)}}}\\\\ {{{}}}\\\\ {{{\\displaystyle\\stackrel{(d)}{=}J(\\pi)-(1-\\gamma){\\mathbb E}_{p_{0}}\\left[v(s)\\right].}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here, (a) follows from $\\begin{array}{r}{\\sum_{a}\\pi(a|s)\\,\\delta^{\\mathrm{TD}}v(s,a)=(r^{\\pi}+\\gamma T^{\\pi}v-v)(s)}\\end{array}$ and $\\tilde{r}^{\\pi}=(I-\\gamma T^{\\pi})v^{\\pi}$ , (b) from $(I-\\gamma T_{*}^{\\pi})$ being  the adjoint operator of $(I-\\gamma T^{\\pi})$ , (c) from $(1-\\gamma)p_{0}=(I-\\gamma T_{*}^{\\pi})d^{\\pi}$ , and (d) from the definition of $J(\\pi)$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Note that the second term of Eq. (24) may have no root with respect to $f$ if the data support does not cover the visitation distribution $d^{\\pi}$ , hence incomplete. Lemma E.2 gives a modification of Eq. (24) to fix this problem. ", "page_idx": 16}, {"type": "text", "text": "Lemma E.2 (Generalized saddle-point decomposition of Lagrangian). For all $\\pi\\in\\Pi_{\\beta}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(v,f)=\\tilde{J}(\\pi)-\\mathbb{E}_{\\mu,\\beta}\\left[(f-\\tilde{f}^{\\pi})(s,a)\\left(I-\\gamma\\tilde{\\mathcal{T}}\\right)(v-\\tilde{v}^{\\pi})(s,a)\\right]+D_{\\mathrm{V}}^{\\pi}(v)-D_{\\mathrm{F}}^{\\pi}(f),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{\\mathrm{V}}^{\\pi}(v):=\\sum_{s\\notin\\mathrm{supp}(\\mu)}\\tilde{d}^{\\pi}(s)\\,v(s)\\,a n d\\,D_{\\mathrm{F}}^{\\pi}(f):=\\mathbb{E}_{\\mu,\\beta}\\left[f(s,a)\\left\\{\\tilde{v}^{\\pi}(s)-\\tilde{q}^{\\pi}(s,a)\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Since Lagrangian is defined with $r(s,a)$ and $T(\\cdot|s,a)$ only on the support of the offilne data distribution $\\operatorname{supp}(\\mu\\beta)$ , Lemma E.1 together with the indistinguishability of M\u02dc and $\\mathcal{M}$ gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\cal L}(v,f)=\\tilde{J}(\\pi)+\\sum_{s,a}\\left(f\\mu\\beta-\\tilde{d}^{\\pi}\\pi\\right)(s,a)\\,\\tilde{\\delta}^{\\mathrm{TD}}v(s,a),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\tilde{\\delta}^{\\mathrm{TD}}v:=\\tilde{r}+\\gamma\\tilde{T}v-v$ . The second term of the RHS of the above equation further evaluated by separating the summation to the on-support and off-support terms ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s,a}(f\\mu\\beta-\\bar{d}^{n}\\pi)(s,a)\\,\\bar{\\delta}^{\\mathrm{TD}}v(s,a)}\\\\ &{=\\left(\\displaystyle\\sum_{(s,a)\\in\\mathrm{supp}(\\mu)}+\\displaystyle\\sum_{(s,a)\\in\\mathrm{supp}(\\mu),a)}\\left(f\\mu\\beta-\\bar{d}^{n}\\pi\\right)(s,a)\\,\\bar{\\delta}^{\\mathrm{TD}}v(s,a)}\\\\ &{\\overset{\\mathrm{(a)}}{=}\\displaystyle\\mathbb{E}_{\\mu,\\beta}\\left[(f-\\bar{f}^{n})(s,a)\\,\\bar{\\delta}^{\\mathrm{TD}}v(s,a)\\right]+\\sum_{(s,a)\\in\\mathrm{supp}(\\mu)}\\bar{\\delta}^{\\mathrm{T}}(s)\\,\\pi(a|s)\\,v(s)}\\\\ &{\\overset{\\mathrm{(b)}}{=}\\displaystyle-\\mathbb{E}_{\\mu,\\beta}\\left[(f-\\bar{f}^{n})(s,a)\\left(I-\\bar{\\gamma}\\bar{T}(v-\\bar{\\nu}^{\\circ})(s,a)\\right]-D_{\\mathbb{F}}^{\\circ}(f)\\right.}\\\\ &{\\qquad+\\left.\\left(\\displaystyle\\sum_{s\\neq a\\neq0}\\sum_{\\mu(a),a\\in\\mathrm{supp}(\\mu),a\\in\\mathrm{supp}(\\beta)(s)\\right)}\\bar{d}^{\\pi}(s)\\,\\pi(a|s)\\,v(s)}\\\\ &{\\overset{\\mathrm{(c)}}{=}\\displaystyle-\\mathbb{E}_{\\mu,\\beta}\\left[(f-\\bar{f}^{n})(s,a)\\left(I-\\bar{\\gamma}\\bar{T}(v-\\bar{\\nu}^{\\circ})(s,a)\\right]-D_{\\mathbb{F}}^{\\circ}(f)+D_{\\mathbb{V}}^{\\circ}(v)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where (a) follows from that $\\tilde{f}^{\\pi}(s,a)\\mu(s)\\beta(a|s)\\ =\\ \\tilde{d}^{\\pi}(s)\\pi(a|s)$ if $(s,a)\\ \\in\\ \\mathrm{supp}(\\mu\\beta)$ and $\\delta^{\\mathrm{TD}}v(s,a)=-v(s)$ if $(s,a)\\not\\in\\mathrm{supp}(\\mu\\beta)$ , (b) from transforming $\\tilde{r}$ in the first term, within $\\tilde{\\delta}^{\\mathrm{TD}}$ , with $\\tilde{r}=(I\\!-\\!\\gamma\\tilde{T})\\tilde{v}^{\\pi}\\!+\\!(\\tilde{q}^{\\pi}\\!-\\!\\tilde{v}^{\\pi})$ and simplify the resulting $\\left(\\tilde{\\boldsymbol{q}}^{\\pi}-\\tilde{\\boldsymbol{v}}^{\\pi}\\right)$ with $\\mathbb{E}_{a\\sim\\beta(a|s)}[\\tilde{f}^{\\pi}(s,a)(\\tilde{q}^{\\pi}(s,a)-$ $\\tilde{v}^{\\pi}(s))]\\ =\\ 0$ , and (c) from evaluating the last summation as zero with the fact $\\operatorname{supp}(\\mu\\pi)~\\subset$ $\\operatorname{supp}(\\mu\\beta)$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Finally, Lemma 5.1 is shown by taking $\\pi$ such that $\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ . ", "page_idx": 16}, {"type": "text", "text": "E.2 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. First, see Appendix E.1 for the proof of Lemma 5.1. Then, recall that $(\\tilde{v}^{*},\\tilde{f}^{\\pi})$ is a saddle point of $L(v,f)$ if $L(\\tilde{v}^{*},f)\\leq L(\\tilde{v}^{*},\\tilde{f}^{\\pi})\\leq L(v,\\tilde{f}^{\\pi})$ for all $v,f\\geq0$ . By Eq. (6) with the nonnegativity of $D_{\\mathrm{V}}^{\\pi}(v)$ and $D_{\\mathrm{F}}^{*}(f)$ , it suffices to show $D_{\\mathrm{V}}^{\\pi}(\\tilde{v}^{*})=0$ and $D_{\\mathrm{F}}^{*}(\\tilde{f}^{\\pi})=0$ . The former follows from $\\tilde{v}^{*}(s)=0$ for all $s\\not\\in\\operatorname{supp}(\\mu)$ and the latter follows from $\\mathbb{E}_{a\\sim\\beta(a|s)}[\\tilde{f}^{\\pi}(s,a)\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}]=$ $\\tilde{w}^{\\pi}(s)\\left\\{\\tilde{v}^{*}(s)-\\mathbb{E}_{a\\sim\\pi(a|s)}[\\tilde{q}^{*}(s,a)]\\right\\}=0$ for all $\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "E.3 Proof of Corollary 5.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. Note that the existence of the saddle points is always ensured since the (restricted) domains of $v$ and $f$ are all convex. Therefore, the first claim is reduced to $0=\\operatorname*{argmax}_{f\\geq0}\\operatorname*{min}_{v\\in\\mathcal{V}_{\\epsilon}}L(v,f)$ , which is shown by Lemma 5.1 and the fact $(I-\\gamma\\tilde{\\mathcal{T}})(v-\\tilde{v})\\geq(1-\\gamma)\\,\\epsilon>0$ for all $v\\in\\mathcal{V}_{\\epsilon}$ . Besides, the second claim is reduced to $0=\\operatorname{argmin}_{v\\geq0}\\operatorname*{max}_{f\\in\\mathcal{F}_{\\epsilon}}L(v,f)$ , which is also shown by Lemma 5.1 and the fact $\\mathbb{E}_{\\mu,\\beta}[(f-\\tilde{f}^{\\pi})(s,a)\\,(I-\\gamma\\tilde{\\mathcal{T}})(v-\\tilde{v}^{*})(s,a)]\\ge(1-\\gamma)\\,\\epsilon\\,\\mathbb{E}_{\\tilde{\\mathcal{T}}_{\\ast}^{\\pi}\\mu}\\,[(v-\\tilde{v}^{*})(s)]$ , which attains the minimum uniquely with $v=0$ . \u53e3 ", "page_idx": 17}, {"type": "text", "text": "E.4 Proof of Lemma 5.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Define ", "page_idx": 17}, {"type": "equation", "text": "$$\nU(\\pi):=(1-\\gamma)\\mathbb{E}_{\\mu}\\left[\\tilde{v}^{\\pi}(s)\\right]+\\frac{(1-\\gamma^{2})}{2}\\left\\|\\tilde{v}^{\\pi}\\right\\|_{2,\\bar{\\mu}}^{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, Lemma 5.2 is proved as a special case of the following lemma with the restriction $\\pi\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ and $\\begin{array}{r}{U^{*}:=(1-\\gamma)\\mathbb{E}_{\\mu}\\left[\\tilde{v}^{*}(s)\\right]+\\frac{(1-\\gamma^{2})}{2}\\left\\lVert\\tilde{v}^{*}\\right\\rVert_{2,\\bar{\\mu}}^{2}}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Lemma E.3 (Generalized saddle-point decomposition of regularized Lagrangian). For all $\\pi\\in\\Pi_{\\beta}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\nK(v,f)=U(\\pi)-\\mathbb{E}_{\\mu,\\beta}\\Big[(f-\\check{f}^{\\pi})(s,a)\\,(I-\\gamma\\tilde{\\mathcal{T}})(v-\\tilde{v}^{\\pi})(s,a)\\Big]+\\check{D}_{\\mathrm{V}}^{\\pi}(v)-D_{\\mathrm{F}}^{\\pi}(f).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v^{2}=(v-\\tilde{v}^{\\pi})^{2}+2\\tilde{v}^{\\pi}v-(\\tilde{v}^{\\pi})^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, multiplying both sides with $(1-\\gamma)^{2}/2$ and plugging to Eq. (7), we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K(v,f)=\\displaystyle\\frac{(1-\\gamma)^{2}}{2}\\left(\\|v-\\tilde{v}^{\\pi}\\|_{2,\\bar{\\mu}}^{2}-\\|\\tilde{v}^{\\pi}\\|_{2,\\bar{\\mu}}^{2}\\right)}\\\\ &{\\qquad\\qquad+\\left(1-\\gamma\\right)\\displaystyle\\sum_{s}\\left\\{\\mu(s)+(1-\\gamma)\\,\\tilde{v}^{\\pi}(s)\\,\\bar{\\mu}(s)\\right\\}v(s)+\\mathbb{E}_{\\mu,\\beta}\\left[f(s,a)\\,\\delta^{\\mathrm{TD}}v(s,a)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, applying Lemma E.2 on the last two terms of the RHS with the formal substitution $p_{0}\\gets\\breve{p}^{\\pi}$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle K(v,f)=\\frac{(1-\\gamma)^{2}}{2}\\left(\\|v-{\\tilde{v}}^{\\pi}\\|_{2,\\bar{\\mu}}^{2}-\\|{\\tilde{v}}^{\\pi}\\|_{2,\\bar{\\mu}}^{2}\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad+\\sum_{s}{\\tilde{d}}^{\\pi}(s)\\,{\\tilde{r}}^{\\pi}(s)-\\mathbb{E}_{\\mu,\\beta}\\left[(f-{\\tilde{f}}^{\\pi})(s,a)\\,(I-\\gamma{\\tilde{T}})(v-{\\tilde{v}}^{\\pi})(s,a)\\right]}}\\\\ {{\\displaystyle\\qquad+\\sum_{s\\in\\mathrm{supp}(\\mu)}{\\tilde{d}}^{\\pi}(s)\\,v(s)-D_{\\mathrm{F}}^{\\pi}(f)}}\\\\ {{\\displaystyle=\\sum_{s}{\\tilde{d}}^{\\pi}(s)\\,{\\tilde{r}}^{\\pi}(s)-\\frac{(1-\\gamma)^{2}}{2}\\,\\|{\\tilde{v}}^{\\pi}\\|_{2,\\bar{\\mu}}}}\\\\ {{\\displaystyle\\qquad-\\mathbb{E}_{\\mu,\\beta}\\left[(f-{\\tilde{f}}^{\\pi})(s,a)\\,(I-\\gamma{\\tilde{T}})(v-{\\tilde{v}}^{\\pi})(s,a)\\right]+\\breve{D}_{\\mathrm{F}}^{\\pi}(v)-D_{\\mathrm{F}}^{\\pi}(f),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\tilde{J}(\\pi),\\tilde{d}^{\\pi}(s)$ and $\\tilde{f}^{\\pi}(s,a)$ in Lemma 5.1 are replaced with $\\begin{array}{r}{\\sum_{s}\\breve{d}^{\\pi}(s)\\,r^{\\pi}(s),\\breve{d}^{\\pi}(s)}\\end{array}$ and $\\breve{f}^{\\pi}(s,a)$ , respectively, due to the substitution. Finally, the proof is concl uded by simplifying the first term on ", "page_idx": 17}, {"type": "text", "text": "the RHS", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{{\\displaystyle\\sum_{s}\\breve{d}^{\\pi}(s)\\,\\tilde{r}^{\\pi}(s)=(1-\\gamma)\\sum_{s}(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\pi})^{-1}\\left\\{\\mu+(1-\\gamma)\\tilde{v}^{\\pi}\\bar{\\mu}\\right\\}(s)\\,\\tilde{r}^{\\pi}(s)}}}\\\\ {{{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\triangleq(1-\\gamma)\\sum_{s}\\{\\mu+(1-\\gamma)\\tilde{v}^{\\pi}\\bar{\\mu}\\}\\,(I-\\gamma\\tilde{\\mathcal{T}}^{\\pi})^{-1}\\tilde{r}^{\\pi}(s)}}}\\\\ {{{\\displaystyle\\qquad\\qquad\\qquad\\qquad\\triangleq U(\\pi)+\\frac{(1-\\gamma)^{2}}{2}\\,\\|\\tilde{v}^{\\pi}\\|_{2,\\bar{\\mu}}\\,,}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where (a) follows from the fact $(I-\\gamma\\tilde{T}_{*}^{\\pi})^{-1}$ is the adjoint operator of $(I-\\gamma\\tilde{T}^{\\pi})^{-1}$ and (b) is owing to $\\tilde{v}^{\\pi}=(I-\\gamma\\tilde{T}^{\\pi})\\tilde{r}^{\\pi}$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E.5 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. First, see Appendix E.4 for the proof of Lemma 5.2. Then, note that the third term $\\breve{D}_{\\mathrm{V}}^{\\pi}(v)$ is nonegative for all $v~\\geq~0$ since $\\breve{d}^{\\pi}\\;\\geq\\;0$ . Thus, we have $K(v,f)\\;\\geq\\;U^{*}$ taking $f\\,=\\,{\\breve{f}}^{\\pi}$ and $K(v,f)\\leq U^{*}$ taking $v=\\tilde{v}^{*}$ . The first claim is then proved by combining these two inequalities, in the same manner as the proof of Theorem 5.1. The second claim, the uniqueness of $\\tilde{v}^{*}$ , follows from the strong convexity of $K(\\cdot,f)$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "E.6 Proof of Lemma 5.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Denote the primal excess risk function relative to $\\mathcal{F}$ by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varepsilon_{\\mathrm{PER}}(v):=\\operatorname*{max}_{f\\in\\mathcal{F}}K(v,f)-\\operatorname*{min}_{v\\geq0}\\operatorname*{max}_{f\\in\\mathcal{F}}K(v,f)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and its minimizer by $\\underline{{v}}^{*}:=\\,\\operatorname{argmin}_{v\\geq0}\\operatorname*{max}_{f\\in{\\mathcal{F}}}K(v,f)$ . Now, the strong convexity of $K(\\cdot,f)$ implies the strong convexity of $\\varepsilon_{\\mathrm{PER}}$ and thus, for all $v:S\\rightarrow\\mathbb{R}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{(1-\\gamma)^{2}}{2}\\left\\|v-\\underline{{v}}^{*}\\right\\|_{2,\\bar{\\mu}}^{2}\\leq\\varepsilon_{\\mathrm{PER}}(v).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We utilize this bound via the triangle inequality ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\Big\\|\\tilde{v}_{\\sharp}^{*}-\\tilde{v}^{*}\\Big\\|_{2,\\bar{\\mu}}\\leq\\Big\\|\\tilde{v}_{\\sharp}^{*}-{\\underline{{v}}^{*}}\\Big\\|_{2,\\bar{\\mu}}+\\|{\\underline{{v}}^{*}}-\\tilde{v}^{*}\\|_{2,\\bar{\\mu}}\\leq\\frac{\\sqrt{2\\varepsilon_{\\mathrm{PER}}(\\tilde{v}_{\\sharp}^{*})}+\\sqrt{2\\varepsilon_{\\mathrm{PER}}(\\tilde{v}^{*})}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, each term on the RHS is bounded by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\varepsilon_{\\mathrm{PER}}(\\tilde{v}^{*})\\leq2\\epsilon_{\\mathcal{F}},}\\\\ {\\varepsilon_{\\mathrm{PER}}(\\tilde{v}_{\\sharp}^{*})-\\varepsilon_{\\mathrm{PER}}({\\tilde{v}^{*}})\\leq(2+B_{\\mathcal{F}})\\epsilon_{\\mathcal{V}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "completing the proof. The proofs of Eqs. (27) and (28) are separately given below. ", "page_idx": 18}, {"type": "text", "text": "Proof of Eq. (27). Recall that by definition ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varepsilon_{\\mathrm{PER}}(\\tilde{v}^{*})=\\operatorname*{max}_{f\\in\\mathcal{F}}K(\\tilde{v}^{*},f)-\\operatornamewithlimits{a r g m i n}_{v\\geq0}\\operatorname*{max}_{f\\in\\mathcal{F}}K(v,f).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, on the RHS, the first term is upper bounded with $U^{*}$ by Theorem 5.2 and the second term (without the negative sign) is lower bounded by Lemma C.2, leading to the inequality ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\varepsilon_{\\mathrm{PER}}(\\tilde{v}^{*})\\leq\\operatorname*{min}_{f\\in\\mathcal{F}}\\left\\{\\bar{B}_{\\mathcal{V}}\\left\\|f-\\check{f}^{\\pi}\\right\\|_{1,\\mu\\beta}+2(1-\\gamma)\\left\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\right\\|_{1,\\bar{\\mu}}\\right\\}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $\\pi\\in\\Pi_{\\beta}$ . The proof is completed by taking the minimum with respect to $\\pi$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Eq. (28). It is immediately seen from ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\varepsilon_{\\mathrm{PER}}(\\tilde{v}_{\\sharp}^{*})-\\varepsilon_{\\mathrm{PER}}(\\tilde{v}^{*})=\\underset{v\\in\\mathcal{V}}{\\operatorname*{min}}\\underset{f\\in\\mathcal{F}}{\\operatorname*{max}}\\,K(v,f)-\\underset{f\\in\\mathcal{F}}{\\operatorname*{max}}\\,K(\\tilde{v}^{*},f)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underset{v\\in\\mathcal{V}}{\\operatorname*{min}}\\,\\underset{f\\in\\mathcal{F}}{\\operatorname*{max}}\\,\\{K(v,f)-K(\\tilde{v}^{*},f)\\}\\,.}\\\\ &{\\qquad\\qquad\\qquad\\leq(2+B_{\\mathcal{F}})\\underset{v\\in\\mathcal{V}}{\\operatorname*{min}}\\,\\Vert v-\\tilde{v}^{*}\\Vert_{1,\\bar{\\mu}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality follows from Lemma C.1. ", "page_idx": 18}, {"type": "text", "text": "F Proofs of Section 6 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.1 Proof of Theorem 6.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Recall that the average action value gap is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma(\\pi):=\\mathbb{E}_{\\mu,\\pi}\\left[\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The following lemma establishes the connection of $\\Gamma(\\pi_{\\theta})$ and $\\epsilon_{\\mathrm{est}}(\\theta)$ . ", "page_idx": 19}, {"type": "text", "text": "Lemma F.1. For all $\\theta\\in\\Theta$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma(\\pi_{\\theta})\\leq\\frac{\\epsilon_{e s t}(\\theta)+\\varepsilon_{a p p,\\Pi}(\\mathcal{V},\\Theta,\\Xi)}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Refer to Appendix F.2. ", "page_idx": 19}, {"type": "text", "text": "Then, Theorem 6.1 is proved by combining Lemma F.1 with the following lemma. ", "page_idx": 19}, {"type": "text", "text": "Lemma F.2. For all $\\pi:S\\rightarrow\\Delta(A)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi)\\leq\\|\\tilde{w}^{\\pi}\\|_{\\infty}\\,\\Gamma(\\pi).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. It follows directly from H\u00f6lder\u2019s inequality with the performance difference lemma for the truncated environment (Lemma F.3). \u53e3 ", "page_idx": 19}, {"type": "text", "text": "F.2 Proof of Lemma F.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proof relies on the following variant of the performance difference lemma adopted for the worst-case environment M\u02dc. ", "page_idx": 19}, {"type": "text", "text": "Lemma F.3 (Worst-case performance difference lemma). For all $\\pi:S\\rightarrow\\Delta(A)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi)=\\mathbb{E}_{\\mu,\\pi}\\left[\\tilde{w}^{\\pi}(s)\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Consequently, for all $\\pi\\in\\Pi_{\\beta}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\tilde{J}^{*}-\\tilde{J}(\\pi)=\\mathbb{E}_{\\mu,\\beta}\\left[\\tilde{f}^{\\pi}(s,a)\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}=(1-\\gamma)\\sum_{s}p_{0}(s)\\,\\tilde{v}^{*}(s)=\\sum_{s}\\tilde{d}^{\\prime\\prime}(s)\\,(I-\\gamma\\tilde{T}^{\\pi})\\tilde{v}^{*}(s)=\\mathbb{E}_{\\mu}\\left[\\tilde{w}^{\\pi}(s)\\,(I-\\gamma\\tilde{T}^{\\pi})\\tilde{v}^{*}(s)\\right],\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second equality follows from $(I-\\gamma\\tilde{T}_{*}^{\\pi})\\tilde{d}^{\\pi}\\,=\\,p_{0}$ and the third equality follows from $\\tilde{v}^{*}(s)=0$ and $\\tilde{\\mathcal{T}}^{\\pi}v(s)=0$ for all $s\\not\\in\\operatorname{supp}(\\mu)$ and $\\pi:S\\to\\Delta(A)$ . Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{J}^{*}-J(\\pi)=\\mathbb{E}_{\\mu}\\left[\\tilde{w}^{\\pi}(s)\\left(I-\\gamma\\tilde{T}^{\\pi})\\tilde{v}^{*}(s)\\right]-\\mathbb{E}_{\\mu}\\left[\\tilde{w}^{\\pi}(s)\\,\\tilde{r}^{\\pi}(s)\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\mu}\\left[\\tilde{w}^{\\pi}(s)\\left\\{\\tilde{v}^{*}(s)-\\mathcal{P}^{\\pi}\\tilde{q}^{*}(s)\\right\\}\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{\\mu,\\pi}\\left[\\tilde{w}^{\\pi}(s)\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second equality follows from $\\tilde{r}^{\\pi}+\\gamma\\tilde{T}^{\\pi}\\tilde{v}^{*}=\\mathcal{P}^{\\pi}\\tilde{q}^{*}$ . This proves the first claim. ", "page_idx": 19}, {"type": "text", "text": "The second claim follows from the definition of $\\tilde{f}^{\\pi}$ . ", "page_idx": 19}, {"type": "text", "text": "Let $\\breve{p}^{*}:=\\mu+(1-\\gamma)\\tilde{v}^{*}\\bar{\\mu}$ be an alternative (unnormalized) initial state distribution and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\breve{f}^{\\pi,*}(s,a):=(1-\\gamma)\\frac{(I-\\gamma{\\mathcal T}_{*}^{\\pi}){\\breve{p}}^{*}(s)}{\\mu(s)}\\rho^{\\pi}(s,a)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "be the corresponding action visitation weight function. Applying Lemma F.3 to this setting, we obtain the following corollary. ", "page_idx": 19}, {"type": "text", "text": "Corollary F.1. Define ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{U}(\\pi):=(1-\\gamma)\\sum_{s}\\breve{p}^{\\ast}(s)\\,\\tilde{v}^{\\pi}(s)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and let $\\begin{array}{r}{\\tilde{U}^{*}:=(1-\\gamma)\\sum_{s}\\breve{p}^{*}(s)\\,\\tilde{v}^{*}(s)}\\end{array}$ be its maximum. Then, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{U}^{*}-\\tilde{U}(\\pi)=\\mathbb{E}_{\\mu,\\beta}\\left[\\check{f}^{\\pi,*}(s,a)\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Now we are prepared to prove Lemma F.1. ", "page_idx": 20}, {"type": "text", "text": "Proof of Lemma $F.l$ . Let $\\begin{array}{r}{\\xi_{\\sharp}^{*}\\in\\mathrm{argmin}_{\\xi\\in\\Xi}\\|\\xi-\\tilde{\\xi}^{*}\\|_{1,\\mu\\beta_{0}}}\\end{array}$ . Now, observe that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Gamma(\\pi_{\\theta})=\\displaystyle\\mathbb{E}_{\\mu,\\pi_{\\theta}}\\left[-\\xi^{*}(s,a)\\right]}\\\\ {\\overset{\\mathrm{(i)}}{\\le}\\displaystyle\\mathbb{E}_{\\mu,\\pi_{\\theta}}\\left[-\\xi_{s}^{*}(s,a)\\right]+B_{\\Pi\\in\\Xi}}\\\\ {\\overset{\\mathrm{(ii)}}{\\le}\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{\\mu,\\pi_{\\theta}}\\left[\\varpi_{\\theta}(s)\\left\\{-\\xi_{s}^{*}(s,a)\\right\\}\\right]+B_{\\Pi\\in\\Xi}}\\\\ {\\overset{\\mathrm{(e)}}{\\le}\\displaystyle\\frac{1}{1-\\gamma}\\left\\{\\mathbb{E}_{\\mu,\\beta}\\left[f_{\\theta}(s,a)\\left\\{-\\xi_{s}^{*}(s,a)\\right\\}\\right]+D_{\\Xi}(f_{\\theta};w_{\\theta},\\pi_{\\theta})\\right\\}+B_{\\Pi\\in\\Xi}}\\\\ {\\overset{\\mathrm{(ii)}}{\\le}\\displaystyle\\frac{1}{1-\\gamma}\\left\\{\\mathbb{E}_{\\mu,\\beta}\\left[f_{\\theta}(s,a)\\left\\{-\\xi^{*}(s,a)\\right\\}\\right]+B_{\\mathcal{F}}\\epsilon_{\\Xi}+D_{\\Xi}(f_{\\theta};w_{\\theta},\\pi_{\\theta})\\right\\}+B_{\\Pi\\epsilon}\\le}\\\\ {\\overset{\\mathrm{(e)}}{=}\\displaystyle\\frac{1}{1-\\gamma}\\left\\{D_{\\Psi}^{*}(f_{\\theta})+\\mathcal{L}_{\\mathbf{X}}(\\theta)+B_{\\mathcal{F}}\\epsilon_{\\Xi}\\right\\}+B_{\\Pi\\epsilon}\\epsilon_{\\Xi}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (a) follows from $\\|\\xi_{\\sharp}^{*}-\\tilde{\\xi}^{*}\\|_{1,\\mu\\pi_{\\theta}}\\leq B_{\\Pi}\\epsilon_{\\Xi}$ , (b) from H\u00f6lder\u2019s inequality with $-\\tilde{\\xi}^{*}(s,a)\\geq0$ and $\\underline{{w}}_{\\theta}\\geq1-\\gamma$ , and (c) from Eq. (11), (d) from $\\|\\xi_{\\sharp}^{*}-\\tilde{\\xi}^{*}\\|_{1,\\mu\\beta}\\leq B_{\\Pi}\\epsilon_{\\Xi}$ , and (e) from the definition of $D_{\\mathrm{F}}^{*}(f)$ (Lemma 5.1) and ${\\mathcal{L}}_{\\mathrm{X}}(\\theta)$ . In the rest of the proof, we bound $D_{\\mathrm{F}}^{*}(f_{\\theta})+\\mathcal{L}_{\\mathrm{X}}(\\theta)$ with $\\epsilon_{\\mathrm{{est}}}(\\theta)$ . ", "page_idx": 20}, {"type": "text", "text": "Fix any $\\theta^{\\prime}\\quad\\in\\quad\\Theta$ and $\\pi\\mathrm{~{~\\small~\\mathscr~{~\\pi~}~}~}\\in\\mathrm{~{~\\prod~}~}_{\\beta}$ . Let $\\begin{array}{r l r}{\\tilde{v}_{\\sharp}^{*}}&{{}\\in}&{\\operatorname{argmin}_{v\\in\\mathcal{V}}\\operatorname*{max}_{f\\in\\mathcal{F}}K(v,f)}\\end{array}$ and $\\begin{array}{r l}{\\breve{f}_{\\sharp}^{*}}&{{}\\in}\\end{array}$ $\\mathrm{argmax}_{f\\in\\mathcal{F}}\\,K(\\tilde{v}_{\\sharp}^{*},f)$ . Also let $\\begin{array}{r}{\\bar{K}^{*}:=K(\\tilde{v}_{\\sharp}^{*},\\check{f}_{\\sharp}^{*})=\\operatorname*{min}_{v\\in\\mathcal{V}}\\operatorname*{max}_{f\\in\\mathcal{F}}K(v,f)}\\end{array}$ be the corresponding minimax value. Then, $\\bar{K}^{*}+\\mathcal{L}_{\\mathrm{SP}}(\\theta)$ is lower-bounded with $D_{\\mathrm{F}}^{*}(f_{\\theta})-D_{\\mathrm{F}}^{*}(f_{\\theta^{\\prime}})$ up to an error term, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{K}^{*}+\\mathcal{L}_{\\mathrm{SP}}(\\theta)=\\underset{v\\in\\mathcal{V}}{\\operatorname*{max}}\\left\\{K(\\tilde{v}_{\\xi}^{*},\\tilde{f}_{\\xi}^{*})-K(v,f_{\\theta})\\right\\}}\\\\ &{\\overset{(a)}{\\geq}K(\\tilde{v}_{\\xi}^{*},\\tilde{f}_{\\xi}^{*})-K(\\tilde{v}_{\\xi}^{*},f_{\\theta})}\\\\ &{\\overset{(b)}{\\geq}K(\\tilde{v}_{\\xi}^{*},f_{\\theta^{\\prime}})-K(\\tilde{v}_{\\xi}^{*},f_{\\theta})}\\\\ &{\\overset{(c)}{=}D_{\\mathbb{F}}^{*}(f_{\\theta})-D_{\\mathbb{F}}^{*}(f_{\\theta^{\\prime}})+\\mathbb{E}_{\\mu,\\beta}\\left[(f_{\\theta}-f_{\\theta^{\\prime}})(s,a)\\left(I-\\gamma\\tilde{T})(\\tilde{v}_{\\xi}^{*}-\\tilde{v}^{*})(s,a\\right)\\right]}\\\\ &{\\overset{(d)}{\\geq}D_{\\mathbb{F}}^{*}(f_{\\theta})-D_{\\mathbb{F}}^{*}(f_{\\theta^{\\prime}})-2B_{\\mathcal{F}}\\left\\|\\tilde{v}_{\\xi}^{*}-\\tilde{v}^{*}\\right\\|_{1,\\tilde{\\mu}}}\\\\ &{\\overset{(e)}{\\geq}D_{\\mathbb{F}}^{*}(f_{\\theta})-D_{\\mathbb{F}}^{*}(f_{\\theta^{\\prime}})-2B_{\\mathcal{F}}\\varepsilon_{\\mathrm{app},\\mathbb{V}}(\\mathcal{V},\\mathcal{F}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where (a) follows from compromising the maximum with $v=\\tilde{v}_{\\sharp}^{*}\\in\\mathcal{V}$ , (b) from the definition of $\\breve{f}_{\\sharp}^{*}$ above, (c) from Lemma 5.2, (d) from $\\|f_{\\theta}\\|_{\\infty}\\,,\\|f_{\\theta^{\\prime}}\\|_{\\infty}\\le B_{\\mathcal{F}}$ and (e) from Lemma 5.3. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Phi(\\theta^{\\prime}):=\\operatorname*{min}_{\\pi\\in\\Pi_{\\beta}}\\left\\{\\bar{B}_{\\mathcal{V},\\Xi}\\left\\|f_{\\theta^{\\prime}}-\\check{f}^{\\pi}\\right\\|_{1,\\mu\\beta}+2(1-\\gamma)\\left\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\right\\|_{1,\\bar{\\mu}}\\right\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "be the intrinsic error of $\\theta^{\\prime}$ and fix arbitrary $\\pi^{*}\\in\\Pi_{\\beta}\\cap\\tilde{\\Pi}^{*}$ . Then, $\\bar{K}^{*}+\\mathcal{L}_{\\mathrm{SP}}(\\theta^{\\prime})$ is upper-bounded with $\\Phi(\\theta^{\\prime})$ up to another error term, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{K}^{*}+\\mathcal{L}_{\\mathrm{SP}}(\\theta^{\\prime})=\\bar{K}^{*}-U^{*}+U^{*}-\\underset{v\\in\\mathcal{V}}{\\operatorname*{min}}\\,K(v,f_{\\theta^{\\prime}})}\\\\ &{\\overset{\\mathrm{(a)}}{=}K(\\tilde{v}_{\\sharp}^{*},\\tilde{f}_{\\sharp}^{*})-K(\\tilde{v}^{*},\\tilde{f}^{\\pi^{*}})+U^{*}-\\underset{v\\in\\mathcal{V}}{\\operatorname*{min}}\\,K(v,f_{\\theta^{\\prime}})}\\\\ &{\\overset{\\mathrm{(b)}}{\\leq}K(\\tilde{v}_{\\sharp}^{*},\\tilde{f}_{\\sharp}^{*})-K(\\tilde{v}^{*},\\tilde{f}_{\\sharp}^{*})+U^{*}-\\underset{v\\in\\mathcal{V}}{\\operatorname*{min}}\\,K(v,f_{\\theta^{\\prime}})}\\\\ &{\\overset{\\mathrm{(c)}}{\\leq}(2+B_{\\mathcal{F}})\\left\\lVert\\tilde{v}_{\\sharp}^{*}-\\tilde{v}^{*}\\right\\rVert_{1,\\bar{\\mu}}+\\Phi(\\theta^{\\prime})}\\\\ &{\\overset{\\mathrm{(d)}}{\\leq}(2+B_{\\mathcal{F}})\\varepsilon_{\\mathrm{ap},\\mathrm{V}}(\\mathcal{V},\\mathcal{F})+\\Phi(\\theta^{\\prime}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) follows from Lemma 5.2, (b) from Theorem 5.2, (c) from Lemma C.1 and Lemma C.2 and (d) from Lemma 5.3. ", "page_idx": 21}, {"type": "text", "text": "Subtracting both sides of Eq. (31) from those of Eq. (32), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{\\mathrm{F}}^{*}(f_{\\theta})\\leq\\mathcal{L}_{\\mathrm{SP}}(\\theta)-\\mathcal{L}_{\\mathrm{SP}}(\\theta^{\\prime})+D_{\\mathrm{F}}^{*}(f_{\\theta^{\\prime}})+\\Phi(\\theta^{\\prime})+(2+3B_{\\mathcal{F}})\\varepsilon_{\\mathrm{app,V}}(\\mathcal{V},\\mathcal{F}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which, summed with $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{X}}(\\theta)\\leq\\mathcal{L}_{\\mathrm{X}}(\\theta)+\\epsilon_{\\mathrm{est}}(\\theta^{\\prime})=\\epsilon_{\\mathrm{est}}(\\theta)-\\mathcal{L}_{\\mathrm{SP}}(\\theta)+\\mathcal{L}(\\theta^{\\prime})}\\end{array}$ on both sides, yields ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{\\mathrm{F}}^{*}(f_{\\theta})+\\mathcal{L}_{\\mathrm{X}}(\\theta)\\leq\\epsilon_{\\mathrm{est}}(\\theta)+D_{\\mathrm{F}}^{*}(f_{\\theta^{\\prime}})+\\mathcal{L}_{\\mathrm{X}}(\\theta^{\\prime})+\\Phi(\\theta^{\\prime})+(2+3B_{\\mathcal{F}})\\varepsilon_{\\mathrm{app},\\mathrm{V}}(\\mathcal{V},\\mathcal{F}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The remaining task is to choose $\\theta^{\\prime}\\in\\Theta$ such that $D_{\\mathrm{F}}^{*}(f_{\\theta^{\\prime}})+\\mathcal{L}_{\\mathrm{X}}(\\theta^{\\prime})+\\Phi(\\theta^{\\prime})$ is nicely bounded. Now, observe that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{J}_{\\mathrm{F}}^{*}(f_{\\theta^{\\prime}})=\\mathbb{E}_{\\mu,\\beta}\\left[f_{\\theta^{\\prime}}(s,a)\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right]}}\\\\ &{\\stackrel{\\mathrm{(a)}}{\\le}\\frac{\\operatorname*{min}}{\\pi\\epsilon\\Pi_{\\beta}}\\left\\{\\mathbb{\\mathbb{E}}_{\\mu,\\beta}\\left[\\tilde{f}^{\\pi,*}(s,a)\\left.\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right]+\\mathbb{E}_{\\mu,\\beta}\\left[(f_{\\theta^{\\prime}}-\\tilde{f}^{\\pi})(s,a)\\left.\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right]\\right.}\\\\ &{}&{\\stackrel{\\mathrm{(b)}}{=}\\frac{\\operatorname*{min}}{\\pi\\epsilon\\Pi_{\\beta}}\\left\\{\\tilde{U}^{*}-\\tilde{U}(\\pi)-\\mathbb{E}_{\\mu,\\beta}\\left[(f_{\\theta^{\\prime}}-\\tilde{f}^{\\pi})(s,a)\\left.\\tilde{\\xi}^{*}(s,a)\\right]\\right\\}}\\\\ &{\\stackrel{\\mathrm{(c)}}{\\le}\\frac{\\operatorname*{min}}{\\pi\\epsilon\\Pi_{\\beta}}\\left\\{(1-\\gamma)\\left.\\|\\tilde{v}^{*}-\\tilde{v}^{\\pi}\\|_{1,\\tilde{p}^{*}}+\\left\\|\\tilde{\\xi}^{*}\\right\\|_{\\infty}\\left\\|f_{\\theta^{\\prime}}-\\tilde{f}^{\\pi}\\right\\|_{1,\\mu\\beta}\\right\\}}\\\\ &{\\stackrel{\\mathrm{(d)}}{\\le}\\Phi(\\theta^{\\prime}),}&{(34)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) follows from $\\breve{f}^{\\pi,*}\\geq\\breve{f}^{\\pi}$ (see Eq. (29) for the definition of $\\breve{f}^{\\pi,*}$ ), (b) from Lemma F.3, (c) from H\u00f6lder\u2019s inequality and (d) from $\\breve{p}^{*}\\leq2\\bar{\\mu}$ and $\\|\\tilde{\\xi}^{*}\\|_{\\infty}\\leq\\bar{B}\\nu_{*}\\mathrm{\\bar{s}}$ . Moreover, for all $\\pi\\in\\Pi_{\\beta}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{\\mathrm{X}}(\\theta^{\\prime})=\\underset{\\xi\\in\\Xi}{\\operatorname*{max}}\\left\\{\\mathbb{E}_{\\mu,\\beta}\\left[f_{\\theta}(s,a)\\xi(s,a)\\right]-\\mathbb{E}_{\\mu,\\pi_{\\theta}}\\left[\\underline{{w}}_{\\theta}(s)\\xi(s,a)\\right]\\right\\}}\\\\ &{\\overset{\\mathrm{(a)}}{\\leq}\\underset{\\xi\\in\\Xi}{\\operatorname*{max}}\\mathbb{E}_{\\mu,\\beta}\\left[(f_{\\theta}-\\check{f}^{\\pi})(s,a)\\xi(s,a)\\right]+\\underset{\\xi\\in\\Xi}{\\operatorname*{max}}\\mathbb{E}_{\\mu,\\pi}\\left[(\\check{w}^{\\pi}-\\underline{{w}}_{\\theta})(s)\\xi(s,a)\\right]}\\\\ &{\\quad\\quad\\quad+\\underset{\\xi\\in\\Xi}{\\operatorname*{max}}\\left\\{\\mathbb{E}_{\\mu,\\pi}\\left[\\underline{{w}}_{\\theta}(s)\\xi(s,a)\\right]-\\mathbb{E}_{\\mu,\\pi_{\\theta}}\\left[\\underline{{w}}_{\\theta}(s)\\xi(s,a)\\right]\\right\\}}\\\\ &{\\overset{\\mathrm{(b)}}{\\leq}B_{\\Xi}\\left\\{\\left\\|f_{\\theta}-\\check{f}^{\\pi}\\right\\|_{1,\\mu\\beta}+\\|w_{\\theta}-\\check{w}^{\\pi}\\|_{1,\\mu}+B_{\\underline{{\\mathcal{W}}}}\\|\\pi_{\\theta}-\\pi\\|_{\\mathrm{TV},\\mu}\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where (a) follows from the telescoping and (b) from $\\|\\boldsymbol{\\xi}\\|_{\\infty}\\leq B_{\\Xi}$ for all $\\xi\\in\\Xi$ and $\\|\\breve{w}^{\\pi}-\\underline{{w}}_{\\theta}\\|_{1,\\mu}\\leq$ $\\|\\breve{w}^{\\pi}-w_{\\theta}\\|_{1,\\mu}$ since $\\check{w}^{\\pi}\\geq1-\\gamma$ . Adding both sides of Eqs. (34) and (35) and taking $\\pi$ and $\\theta^{\\prime}$ as the minimizers of Eqs. (12) and (13), respectively, we arrive at ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{\\mathrm{F}}^{*}(f_{\\theta^{\\prime}})+\\mathcal{L}_{\\mathrm{X}}(\\theta^{\\prime})+\\Phi(\\theta^{\\prime})\\leq2\\Phi(\\theta^{\\prime})+B\\_{\\Theta}\\epsilon_{\\Theta}(\\pi)\\leq3\\epsilon_{\\Theta}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality follows from $\\Phi(\\theta^{\\prime})\\leq\\epsilon_{\\Theta}$ (Eq. (12)). The proof is concluded by adding both sides of Eqs. (30), (33) and (36) and simplifying the error terms with Eq. (16). \u53e3 ", "page_idx": 21}, {"type": "text", "text": "F.3 Extensions of Corollary 6.1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The suboptimality bound established by Corollary 6.1 requires the uniform boundedness of $\\|\\tilde{w}^{\\pi_{\\theta}}\\|_{\\infty}$ With more careful analysis, however, we can obtain policy suboptimality bounds with milder conditions. We present two of such upper bounds below. ", "page_idx": 22}, {"type": "text", "text": "To this end, we introduce two types of local truncated concentrability. Let $\\Pi\\equiv\\Pi(\\Theta):=\\left\\{\\pi_{\\theta}\\,|\\,\\theta\\in\\Theta\\right\\}$ be the set of all the policy candidates. ", "page_idx": 22}, {"type": "text", "text": "Definition F.1. Let $\\tilde{C}_{\\epsilon}:=\\operatorname*{max}\\{||\\tilde{w}^{\\pi}||_{\\infty}\\mid\\tilde{J}^{*}-\\tilde{J}(\\pi)\\leq\\epsilon,\\pi\\in\\Pi\\}$ be the $\\epsilon$ -weakly local truncated concentrability (\u03f5-WLTC) coefficient for $\\epsilon\\mathrm{~>~0~}$ . We also define the limit WLTC coefficient as $\\begin{array}{r}{\\tilde{C}_{0}:=\\operatorname*{lim}_{\\epsilon\\to0+}\\tilde{C}_{\\epsilon}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Definition F.2. Let $\\tilde{c}_{\\epsilon}\\;:=\\;\\mathrm{max}\\{\\|\\tilde{w}^{\\pi}\\|_{\\infty}\\,\\mid\\mathrm{max}_{(s,a)\\in\\mathrm{supp}(\\mu\\pi)}\\,\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\}\\;\\le\\;\\epsilon,\\pi\\;:\\;\\mathcal{S}\\;\\rightarrow\\;\\infty\\}\\,.$ $\\Delta(\\mathcal{A})\\}$ be the $\\epsilon$ -strongly local truncated concentrability (\u03f5-SLTC) coefficient for $\\epsilon>0$ . We also define the limit SLTC coefficient as $\\tilde{c}_{0}:=\\mathrm{lim}_{\\epsilon\\rightarrow0+}\\,\\tilde{c}_{\\epsilon}$ . ", "page_idx": 22}, {"type": "text", "text": "Intuitively, both the WLTC and SLTC coefficients bound the norm of $\\tilde{w}^{\\pi}$ locally for near-optimal policies $\\pi$ . The difference is the ways they measure the locality: WLTC uses the policy suboptimality and SLTC uses the maximum action value gap. Note that WLTC dominates SLTC, $\\tilde{c}_{\\epsilon}\\leq\\tilde{C}_{\\epsilon}\\leq\\tilde{C}_{\\infty}$ , if $\\Pi(\\Theta)$ covers the entire policy space $\\Delta(\\mathcal{A})^{\\bar{s}}$ . In general, there is no particular order between WLTC and SLTC and we just have $\\tilde{C}_{\\epsilon}\\leq\\tilde{C}_{\\infty}$ . ", "page_idx": 22}, {"type": "text", "text": "The following lemma is the foundation of our local concentrability results. ", "page_idx": 22}, {"type": "text", "text": "Lemma F.4. For any $\\pi\\in\\Pi$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi)\\leq\\left(1+\\frac{\\tilde{c}_{\\epsilon_{0}(\\pi)}}{\\tilde{c}_{0}}\\right)\\epsilon_{0}(\\pi),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\epsilon_{0}(\\pi):=\\sqrt{\\tilde{c}_{0}\\Gamma(\\pi)/(1-\\gamma)}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Refer to Appendix F.4. ", "page_idx": 22}, {"type": "text", "text": "Combining it with Lemma F.1 and discarding the non-asymptotic term for the simplicity, we get the first bound as the following corollary. ", "page_idx": 22}, {"type": "text", "text": "Corollary F.2. For all $\\theta\\in\\Theta$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi_{\\theta})\\lesssim2\\sqrt{\\frac{\\tilde{c}_{0}}{1-\\gamma}\\left\\{\\epsilon_{e s t}(\\theta)+\\varepsilon_{a p p,\\Pi}(\\mathcal{V},\\Theta,\\Xi)\\right\\}},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $a\\lesssim b$ means lim $\\operatorname*{sup}_{b\\to0+}a/b\\leq1$ . ", "page_idx": 22}, {"type": "text", "text": "In words, Eq. (37) allows us to replace the uniform concentrability coefficient $\\tilde{C}_{\\infty}$ with the limit SLTC coefficient $\\tilde{c}_{0}$ at the cost of the slower convergence rate due to the square root. ", "page_idx": 22}, {"type": "text", "text": "Moreover, a faster bound can be obtained exploiting the limit WLTC coefficient ${\\tilde{C}}_{0}$ instead of $\\tilde{c}_{0}$ , leading to our second bound. ", "page_idx": 22}, {"type": "text", "text": "Corollary F.3. For all $\\theta\\in\\Theta$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi_{\\theta})\\lesssim\\frac{\\tilde{C}_{0}}{1-\\gamma}\\left\\{\\epsilon_{e s t}(\\theta)+\\varepsilon_{a p p,\\Pi}(\\mathcal{V},\\Theta,\\Xi)\\right\\}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. Observe that Corollary F.2 implies $\\|\\tilde{w}^{\\pi_{\\theta}}\\|_{\\infty}$ is bounded with ${\\tilde{C}}_{\\epsilon}$ , where $\\epsilon$ is taken as the RHS of Eq. (37). The claim thus follows from Theorem 6.1 with $\\|\\tilde{w}^{\\pi_{\\theta}}\\|_{\\infty}\\le\\tilde{C}_{\\epsilon}\\rightarrow\\tilde{C}_{0}$ as $\\epsilon_{\\mathrm{est}}(\\theta)+$ $\\varepsilon_{\\mathrm{app,}\\Pi}(\\mathcal{V},\\Theta,\\Xi)\\rightarrow0$ . ", "page_idx": 22}, {"type": "text", "text": "Similarly as Corollary F.2, in comparison to Theorem 6.1, the coefficient of the upper bound is improved from $\\tilde{C}_{\\infty}$ to ${\\tilde{C}}_{0}$ , meaning that asymptotically we only need the weakly local, not uniform, concentrability. ", "page_idx": 22}, {"type": "text", "text": "F.4 Proof of Lemma F.4 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Let us denote the set of the $\\epsilon$ -strongly near-optimal policies by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\Pi}_{\\epsilon}^{*}:=\\left\\{\\pi:\\mathcal{S}\\rightarrow\\Delta(\\mathcal{A})\\left|\\,\\mathrm{supp}(\\pi(s))\\subset\\mathrm{supp}(\\tilde{\\mathcal{A}}_{\\epsilon}^{*}(s)),\\,s\\in\\mathrm{supp}(\\mu)\\right.\\right\\},\\qquad\\epsilon\\geq0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\tilde{\\mathcal{A}}_{\\epsilon}^{*}(s):=\\{a\\in\\mathcal{A}\\,|\\,\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\leq\\epsilon\\}$ is the $\\epsilon$ -optimal action subset for $s\\in S$ .7 Note that by definition $\\tilde{\\Pi}_{0}^{*}=\\tilde{\\Pi}^{*}$ and $\\tilde{\\Pi}_{\\epsilon}^{*}$ is monotone nondecreasing with respect to $\\epsilon$ , reaching the set of the all policies with $\\epsilon\\geq1/(1-\\gamma)$ . Then, the SLTC coefficient can be written in terms of $\\tilde{\\Pi}_{\\epsilon}^{*}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{c}_{\\epsilon}=\\operatorname*{max}_{\\pi\\in\\tilde{\\Pi}_{\\epsilon}^{*}}\\|\\tilde{w}^{\\pi}\\|_{\\infty}\\,.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, to prove Lemma F.4, we show that there exists a strongly optimal policy $\\bar{\\pi}\\in\\tilde{\\Pi}_{\\epsilon}^{*}$ that approximates the target policy $\\pi$ if $\\Gamma(\\pi)$ is small. ", "page_idx": 23}, {"type": "text", "text": "Lemma F.5. For all $\\pi:S\\rightarrow\\Delta(A)$ and $\\epsilon>0$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\bar{\\pi}\\in\\tilde{\\Pi}_{\\epsilon}^{*}}\\Vert\\pi-\\bar{\\pi}\\Vert_{\\mathrm{TV},\\mu}\\leq\\frac{2\\Gamma(\\pi)}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Take $\\pi^{\\prime}\\in\\tilde{\\Pi}_{\\epsilon}^{*}$ as a projection of $\\pi$ onto $\\tilde{\\Pi}_{\\epsilon}^{*}$ , i.e., ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\pi^{\\prime}(a|s)=\\mathbf{1}\\{a\\in\\tilde{\\mathcal{A}}_{\\epsilon}^{*}(s)\\}\\pi(a|s)+c(s)\\,\\pi_{0}(a|s),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with arbitrary $\\pi_{0}\\in\\tilde{\\Pi}_{\\epsilon}^{*}$ and $\\begin{array}{r}{c(s):=1-\\sum_{a\\in\\tilde{\\mathcal{A}}_{\\epsilon}^{\\ast}(s)}\\pi(a|s)}\\end{array}$ . Observe that, by the triangle inequality, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\pi-\\pi^{\\prime}\\|_{\\mathrm{TV},\\mu}=\\mathbb{E}_{\\mu}\\displaystyle\\sum_{a}|\\pi(a|s)-\\pi^{\\prime}(a|s)|}\\\\ &{\\qquad\\qquad\\le\\mathbb{E}_{\\mu}\\displaystyle\\sum_{a}\\Big\\{(1-\\mathbf{1}\\{a\\in\\tilde{A}_{\\epsilon}^{*}(s)\\})\\pi(a|s)+c(s)\\,\\pi_{0}(a|s)\\Big\\}}\\\\ &{\\qquad\\qquad=2\\mathbb{E}_{\\mu}\\left[c(s)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then, we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\bar{\\pi}\\in\\tilde{\\Pi}_{\\epsilon}^{*}}\\|\\pi-\\bar{\\pi}\\|_{\\mathrm{TV},\\mu}\\leq2\\mathbb{E}_{\\mu}\\left[c(s)\\right]=2\\mathbb{E}_{\\mu,\\pi}\\left[{\\mathbf{1}}\\left\\{a\\notin\\tilde{\\mathcal{A}}_{\\epsilon}^{*}(s)\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now, plugging ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\bf1}\\left\\{a\\not\\in\\tilde{\\mathcal{A}}_{\\epsilon}^{*}(s)\\right\\}={\\bf1}\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)>\\epsilon\\right\\}\\leq\\frac{1}{\\epsilon}\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "on the RHS, we arrive at the desired inequality ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\bar{\\pi}\\in\\tilde{\\Pi}_{\\epsilon}^{*}}\\|\\pi-\\bar{\\pi}\\|_{\\mathrm{TV},\\mu}\\leq\\frac{2}{\\epsilon}\\mathbb{E}_{\\mu,\\pi}\\left[\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\}\\right]=\\frac{2}{\\epsilon}\\Gamma(\\pi).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "As a corollary, we can also bound the difference of their policy values based on the SLTC coefficient. Corollary F.4. For all $\\pi:S\\to\\Delta(A)$ and $\\epsilon>0$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\bar{\\pi}\\in\\tilde{\\Pi}_{\\epsilon}^{*}}\\Big|\\tilde{J}(\\pi)-\\tilde{J}(\\bar{\\pi})\\Big|\\leq\\frac{2\\tilde{c}_{\\epsilon}}{1-\\gamma}\\frac{\\Gamma(\\pi)}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Take arbitrary $\\bar{\\pi}\\in\\tilde{\\Pi}_{\\epsilon}^{*}$ and observe that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{d}^{\\pi}(s)-\\tilde{d}^{\\pi}(s)=(1-\\gamma)\\left\\{(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\pi})^{-1}-(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\bar{\\pi}})^{-1}\\right\\}p_{0}(s)}\\\\ &{\\qquad\\qquad\\qquad=(1-\\gamma)(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\pi})^{-1}\\left\\{(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\bar{\\pi}})-(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\pi})\\right\\}(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\bar{\\pi}})^{-1}p_{0}(s)}\\\\ &{\\qquad\\qquad\\quad=\\gamma(1-\\gamma)(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\pi})^{-1}\\left(\\tilde{\\mathcal{T}}_{*}^{\\pi}-\\tilde{\\mathcal{T}}_{*}^{\\bar{\\pi}}\\right)(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\bar{\\pi}})^{-1}p_{0}(s)}\\\\ &{\\qquad\\qquad\\quad=\\gamma(I-\\gamma\\tilde{\\mathcal{T}}_{*}^{\\pi})^{-1}\\tilde{\\mathcal{T}}_{*}\\left(\\mathcal{P}_{*}^{\\pi}-\\mathcal{P}_{*}^{\\bar{\\pi}}\\right)\\tilde{d}^{\\bar{\\pi}}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert\\bar{\\boldsymbol{w}}^{\\nu}-\\bar{\\boldsymbol{w}}^{\\nu}\\right\\rVert_{1,\\mu}=\\underset{s\\in\\mathrm{supp}(\\bar{\\mu})}{\\sum}\\Bigg|\\bar{\\mathcal{F}}(s)-\\bar{\\mathcal{A}}^{\\nu}(s)\\Bigg|}&{}\\\\ {=\\underset{s\\in\\mathrm{supp}(\\bar{\\mu})}{\\sum}\\Bigg|\\gamma(I-\\gamma\\frac{\\gamma}{\\mathcal{Z}_{s}^{\\nu}})^{-1}\\bar{\\mathcal{T}}_{s}\\left(\\mathcal{P}_{s}^{\\nu}-\\mathcal{P}_{s}^{\\nu}\\right)\\bar{d}^{\\nu}(s)\\Bigg|}\\\\ {\\overset{(a)}{\\leq}\\frac{\\gamma}{1-\\gamma}\\underset{(s,a)\\in\\mathrm{supp}(\\bar{\\mu})}{\\sum}\\Bigg|\\left(\\mathcal{P}_{s}^{\\nu}-\\mathcal{P}_{s}^{\\nu}\\right)\\bar{d}^{\\nu}(s,a)\\Bigg|}\\\\ {=\\underset{s=1}{\\overset{\\gamma}{\\sum}}\\frac{\\mathbb{E}_{\\mu}}{\\gamma}\\left[\\frac{\\bar{d}^{\\nu}(s)}{\\bar{\\mu}(s)}\\underset{s\\in\\mathrm{supp}(\\bar{\\mu})}{\\sum}|\\pi(a|s)-\\bar{\\pi}(a|s)|\\right]}\\\\ {\\overset{(b)}{\\leq}\\frac{\\gamma\\tilde{\\gamma}_{s}}{1-\\gamma}\\mathbb{E}_{\\mu}\\left[\\sum_{\\alpha}|\\pi(a|s)-\\bar{\\pi}(a|s)|\\right]}\\\\ {=\\frac{\\gamma\\tilde{\\gamma}_{s}}{1-\\gamma}\\left[\\pi^{\\alpha}\\pi^{\\alpha}|\\right]\\Gamma_{\\infty},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (a) follows from the fact $\\tilde{\\mathcal{T}}_{*}^{\\pi}$ and $\\tilde{\\mathcal{T}}_{*}$ can be identified as non-expansive mappings of types $L^{1}(\\operatorname{supp}(\\mu))\\to L^{1}(\\operatorname{supp}(\\mu))$ and $L^{1}(\\operatorname{supp}(\\mu\\beta))\\to L^{1}(\\operatorname{supp}(\\mu))$ , respectively, and (b) follows from $\\bar{\\pi}\\in\\tilde{\\Pi}_{\\epsilon}^{*}$ . Finally, observe that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\left|\\tilde{J}(\\pi)-\\tilde{J}(\\bar{\\pi})\\right|=\\left|\\mathbb{E}_{\\mu}\\left[(\\tilde{w}^{\\pi}\\hat{r}^{\\pi}-\\tilde{w}^{\\bar{\\pi}}\\hat{r}^{\\bar{\\pi}})(s)\\right]\\right|}}\\\\ &{}&{\\stackrel{\\mathrm{(a)}}{\\leq}\\mathbb{E}_{\\mu}\\left[\\big|(\\tilde{w}^{\\pi}-\\tilde{w}^{\\bar{\\pi}})\\tilde{r}^{\\pi}\\big|\\left(s\\right)+\\big|\\tilde{w}^{\\bar{\\pi}}(\\hat{r}^{\\pi}-\\tilde{r}^{\\bar{\\pi}})\\big|\\left(s\\right)\\right]}\\\\ &{}&{\\stackrel{\\mathrm{(b)}}{\\leq}\\left\\|\\tilde{w}^{\\pi}-\\tilde{w}^{\\bar{\\pi}}\\right\\|_{\\mu,1}+\\tilde{c}_{\\epsilon}\\left\\|\\pi-\\bar{\\pi}\\right\\|_{\\mathrm{TV},\\mu}}\\\\ &{}&{\\stackrel{\\mathrm{(c)}}{\\leq}\\frac{\\tilde{c}_{\\epsilon}}{1-\\gamma}\\left\\|\\pi-\\bar{\\pi}\\right\\|_{\\mathrm{TV},\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where (a) follows from the triangle inequality, (b) from the fact $|\\tilde{r}(s,a)|\\le1$ and (c) from Eq. (39). The proof is concluded by applying Lemma F.5 on the RHS. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "We now arrive at the following corollary, which immediately implies Lemma F.4 as a special case with the substitution $\\epsilon=\\epsilon_{0}(\\pi)$ . ", "page_idx": 24}, {"type": "text", "text": "Corollary F.5. For all $\\epsilon>0$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\pi)\\leq\\epsilon+\\frac{2\\tilde{c}_{\\epsilon}}{1-\\gamma}\\frac{\\Gamma(\\pi)}{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. By Lemma F.3, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\bar{\\pi})=\\mathbb{E}_{\\mu,\\pi}\\left[\\tilde{w}^{\\pi}(s)\\left\\{\\tilde{v}^{*}(s)-\\tilde{q}^{*}(s,a)\\right\\}\\right]\\leq\\epsilon\\mathbb{E}_{\\mu}\\left[\\tilde{w}^{\\pi}(s)\\right]\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $\\bar{\\pi}\\in\\tilde{\\Pi}_{\\epsilon}^{*}$ . Thus, decomposing the suboptimality as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\tilde{J}^{*}-\\tilde{J}(\\bar{\\pi})\\leq\\tilde{J}^{*}-\\tilde{J}(\\bar{\\pi})+\\Big|\\tilde{J}(\\pi)-\\tilde{J}(\\bar{\\pi})\\Big|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and applying Corollary F.4 results in the desired result. ", "page_idx": 24}, {"type": "text", "text": "F.5 Proof of Corollary 6.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Proof. The celebrated uniform convergence theorem (Lemma B.1) with the boundedness $\\left|\\hat{\\mathcal{L}}_{z}(\\theta;v,\\xi)\\right|\\leq B_{\\mathrm{all}}$ implies that, for all $\\delta\\in(0,1)$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\theta\\in\\Theta,\\,v\\in\\mathcal{V},\\xi\\in\\Xi}\\Big|\\hat{\\mathcal{L}}(\\theta;v,\\xi)-\\mathbb{E}\\left[\\mathcal{L}(\\theta;v,\\xi)\\right]\\Big|\\leq2\\Re_{n}(\\mathcal{H})+B_{\\mathrm{all}}\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with probability $1-\\delta$ . Now, by definition, $\\epsilon_{\\mathrm{est}}(\\theta)$ is uniformly approximated with $\\hat{\\epsilon}_{\\mathrm{est}}(\\theta)$ up to as twice as the statistical error given by Eq. (40). Plugging this into Eq. (18), we obtain the desired bound. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The worst-case policy value is introduced in Section 4 and the sample complexity bound is given by Corollary 6.3, wherein the realizability condition $(\\varepsilon_{\\mathrm{app,}\\Pi}(\\bar{\\mathcal{V}},\\Theta,\\Xi)\\,\\bar{=}\\,0)$ is the only major assumption. The improvements over the previous sample complexity bounds are summarized in Table 1 and discussed in Section 2 and Appendix A. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The limitations are discussed in Section 7. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: All the assumptions are explicit in the main text. The complete proof is presented in Appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: No experimental result is presented. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No experimental result is presented. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No experimental result is presented. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: No experimental result is presented. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No experimental result is presented. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper is purely theoretical. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: There is no immediate societal impact to be considered of the present work since the results are purely theoretical. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: No dataset or model is disclosed. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]