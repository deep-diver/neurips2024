[{"figure_path": "63VajkIDEu/tables/tables_1_1.jpg", "caption": "Table 1: Assumptions and sample complexity bounds of related work. \u03c0* and \u03c0* denote optimal policies in the conventional and worst-case offline RL, respectively. \u03c0\u03b7 denotes a sequence of policies indexed with the sample size n. The realizability of \u03c0 means that \u03c0-associated model-free parameters (e.g., value functions, visitation weight functions and the policy itself) are realizable. \u03b5 > 0 is the policy suboptimality given in Problem 4.1 (or equivalently in Problem 3.1, see Corollary 4.2 for the equivalence). 0 < \u03b4 < 1 denotes the confidence parameter. H denotes the time horizon and roughly comparable to (1 \u2013 \u03b3)\u2212\u00b9. Cgap and \u03b2gap denote the minimum and the lower-tail exponent of the action value gaps, respectively. N denotes the cardinality of the function classes. The improvements made by our result are emphasized. See Appendix A for more details.", "description": "This table compares the sample complexity bounds of different offline reinforcement learning methods.  It highlights the assumptions made by each method (concentrability and realizability) and shows how these assumptions affect the resulting sample complexity bound. The table emphasizes the improvement achieved by the proposed method in terms of both weaker assumptions and a tighter bound.", "section": "1 Introduction"}, {"figure_path": "63VajkIDEu/tables/tables_12_1.jpg", "caption": "Table 1: Assumptions and sample complexity bounds of related work. \u03c0* and \u03c0* denote optimal policies in the conventional and worst-case offline RL, respectively. \u03c0\u03b7 denotes a sequence of policies indexed with the sample size n. The realizability of \u03c0 means that \u03c0-associated model-free parameters (e.g., value functions, visitation weight functions and the policy itself) are realizable.  \u03b5 > 0 is the policy suboptimality given in Problem 4.1 (or equivalently in Problem 3.1, see Corollary 4.2 for the equivalence). 0 < \u03b4 < 1 denotes the confidence parameter. H denotes the time horizon and roughly comparable to (1 \u2212 \u03b3)\u207b\u00b9. Cgap and \u03b2gap denote the minimum and the lower-tail exponent of the action value gaps, respectively. N denotes the cardinality of the function classes. The improvements made by our result are emphasized. See Appendix A for more details.", "description": "This table compares the sample complexity bounds of several existing offline reinforcement learning methods with the proposed method. It shows that the proposed method achieves a better sample complexity bound under weaker assumptions.  The table highlights the assumptions (concentrability and realizability) made by each method and the resulting sample complexity bound. The assumptions are categorized as either 'Concentrability' or 'Realizability', while the sample complexity bound is provided as a mathematical expression.", "section": "1 Introduction"}]