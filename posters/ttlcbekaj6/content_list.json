[{"type": "text", "text": "AirSketch: Generative Motion to Sketch ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hui Xian Grace Lim \u2217 Xuanming Cui \u2217 Yogesh S Rawat Ser-Nam Lim hxgrace@ucf.edu xu979022@ucf.edu yogesh@crcv.ucf.edu sernam@ucf.edu ", "page_idx": 0}, {"type": "text", "text": "University of Central Florida ", "page_idx": 0}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/d1465bfa4cbab59d4bbaabfd655f7566f1bc6d40a0358d9db76b754a87a66eda.jpg", "img_caption": ["Figure 1: Given a hand drawing video, we first use a hand detection algorithm to extract the tracking image (Hand Tracking). We then take this noisy tracking image and generate a clean sketch (Generated), which faithfully and aesthetically resembles the intended sketch (Ground-truth). "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Illustration is a fundamental mode of human expression and communication. Certain types of motion that accompany speech can provide this illustrative mode of communication. While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additional digital markers, thereby limiting their accessibility and portability. Furthermore, air drawing demands considerable skill to achieve aesthetic results. To address these challenges, we introduce the concept of AirSketch, aimed at generating faithful and visually coherent sketches directly from hand motions, eliminating the need for complicated headsets or markers. We devise a simple augmentation-based self-supervised training procedure, enabling a controllable image diffusion model to learn to translate from highly noisy hand tracking images to clean, aesthetically pleasing sketches, while preserving the essential visual cues from the original tracking data. We present two air drawing datasets to study this problem. Our findings demonstrate that beyond producing photo-realistic images from precise spatial inputs, controllable image diffusion can effectively produce a refined, clear sketch from a noisy input. Our work serves as an initial step towards marker-less air drawing and reveals distinct applications of controllable diffusion models to AirSketch and AR/VR in general. Code and dataset are available under: https://github.com/hxgr4ce/DoodleFusion. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Hand gestures are an essential element in communication[43]. In particular, iconic hand motions (i.e. air drawing) can depict visual aspects of an object. This form of expression is frequently used to visually supplement verbal communication and is used in various practical applications, including conceptual discussions, overcoming language barriers, and aiding in visual design. ", "page_idx": 1}, {"type": "text", "text": "Popular AR/VR tools like Google\u2019s Tiltbrush [20] and HoloARt [5] create visuals via hand motions, but are inconvenient. These applications generally require head-mounted displays and digital hand markers, which are costly and hinder portability. Furthermore, their weight and temperature make them unsuitable for continuous use [60, 72, 13], and so are inconvenient for spontaneous usage. Yet, these devices provide accurate positioning, stabilization, and varied brush controls and are crucial to producing high-quality drawings. ", "page_idx": 1}, {"type": "text", "text": "Can we generate sketches from hand motions without additional sensors or markers? In order to enhance accessibility and convenience, we aim to generate sketches from hand motions videos captured using any standard camera embedded in devices like smartphones or smart glasses. ", "page_idx": 1}, {"type": "text", "text": "One could clearly deploy hand tracking algorithms [9, 74] to turn these hand motion videos into sketches. However, creating air drawings with a hand tracking algorithm alone presents several challenges. These include the user\u2019s drawing ability, physical fatigue, and inaccuracies in hand tracking. Noise in hand tracking can severely distort a sketch, rendering it almost unrecognizable. ", "page_idx": 1}, {"type": "text", "text": "The objective, therefore, is to generate clean sketches that faithfully represent the user\u2019s intent, from highly noisy and distorted hand motion input. This task requires the model to possess a deep understanding of shape and object priors, enabling it to discern and correct deformed motion cues while filtering out undesirable noise. We refer to this task as Generative Motion to Sketch. ", "page_idx": 1}, {"type": "text", "text": "There are many approaches to this task, with different architecture and data modalities. The input modality might include learned video representations, coordinate sequences from a hand tracking algorithm, or a rasterized image. Depending on the modality, the task may also be reformulated as video-to-sketch, sequence-to-sequence, image-to-image, or a combination thereof. This diversity introduces interesting opportunities for rich exploration of all these diverse approaches. ", "page_idx": 1}, {"type": "text", "text": "We explore the use of controllable image Diffusion Models (DM) in generating sketches from motion. Existing work such as ControlNet [76] and T2IAdapter [44] generate photo-realistic images given spatially-precise conditioning images. We explore a different use case by using controllable DMs to \u201creconstruct\u201d clean sketches from severely distorted and noisy input images obtained with a hand tracking algorithm. We propose a simple, augmentation-based, self-supervised training procedure and construct two air drawings datasets for evaluation purposes. ", "page_idx": 1}, {"type": "text", "text": "Our experiments show that with our augmentation-based training, controllable image DMs are able to recognize and interpret correct visual cues from noisy tracking images, some of which even appear to be nearly unrecognizable to the human eye, and generate sketches faithfully and aesthetically, while being robust to unseen objects. Moreover, we show that through simple augmentations, the model is capable of sketch-completion and text-instructed stroke styling. Finally, we conduct ablations to investigate 1) the effects of different augmentations, 2) the contribution of text prompts to sketch generation and 3) the effect of different levels of input \u2018chaos\u2019 on the quality of resulting generations. ", "page_idx": 1}, {"type": "text", "text": "In summary, in this paper: ", "page_idx": 1}, {"type": "text", "text": ". We conduct a pilot study of AirSketch, sketch generation from marker-less air drawing, and provide two air drawing datasets for evaluation. ", "page_idx": 1}, {"type": "text", "text": "2. We propose a controllable DM approach that generates faithful and aesthetic sketches from air-drawn tracking images with a self-supervised, augmentation-based training procedure, and conduct ablation studies to prove its effectiveness and robustness. ", "page_idx": 1}, {"type": "text", "text": "3. We explore a different way of using spatially-conditioned DMs and reveal some of their interesting properties in the context of sketch generation. We hope our experiments shed new light on understanding the properties of controllable DMs and facilitate future research. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Sketching in AR/VR ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "There are many methods for drawing in AR and VR. Applications such as Google\u2019s Tilt Brush [20], Mozilla\u2019s A-Painter [45], Oculus\u2019 Quill [48], and HoloARt [5] display user strokes as lines or tubes that can extend in all three dimensions. Many sketching applications such as these require a combination of VR/AR headsets and controllers to use. Since drawing freehand with six degrees of freedom makes it difficult to draw steady lines and surfaces, applications like AdaptiBrush [59] and Drawing on Air [28] use trajectory-based motion of ribbons to render strokes predictably and reliably. Just-a-Line [21] and ARCore Drawing [26] are smartphone-based AR drawing applications, where users draw lines by moving a smartphone in the air [21], or drawing on a smartphone screen [26]. ", "page_idx": 2}, {"type": "text", "text": "2.2 Sketch Applications ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Representing Sketches. A sketch can be represented in both vector and pixel space, and with varying levels of abstraction. A sketch can be represented as a rasterized image [40, 75, 50, 37, 32], a sequence of coordinates [23, 56, 38], a set of Bezier curves [67, 18], or a combination [8]. These representation methods are suited for different tasks. For example, the rasterized image representation is commonly used in Sketch-Based Image Retrieval (SBIR) in order to compare sketches with images, while the coordinate sequence representation is often used for sketch generation. On the other hand, a sketch can also depict the same object at varying abstraction levels, thereby imposing further challenges on downstream tasks, especially sketch-based retrieval in 2D [37, 32, 62] and 3D [69, 41, 14, 31]. ", "page_idx": 2}, {"type": "text", "text": "Sketch Generation. Most existing sketch generation methods adopt the vector representation [23, 56, 38], and view sketch generation as an auto-regressive problem on coordinate sequences. These sequences are typically represented by lists of tuple $(\\delta x,\\delta y,\\mathbf{p})$ , where $\\delta x$ and $\\delta y$ represent the offset distances of x and y from the previous point, and $\\mathbf{p}$ is a one-hot vector indicating the state of the pen. Sketch-RNN [23] uses a Variational Autoencoder (VAE) with a bi-directional RNN as the encoder and an auto-regressive RNN as the decoder. Sketch-Bert [38] follows the BERT [15] model design and training regime. Instead of auto-regressive sketch generation, SketchGAN [39] takes in a rasterized sketch image and uses a cascade GAN to iteratively complete the sketch. Sketch-Pix2Seq [12] adds a vision encoder on top of Sketch-RNN and thus allow the model to take in an image input and reconstruct the sketch using coordinate sequences. ", "page_idx": 2}, {"type": "text", "text": "Nonetheless, these methods generate sketches in the input sketch modality by taking in a sketch and reconstructing the exact same sketch, or by predicting endings given incomplete sketches. In contrast, our work considers the task for generating sketches from hand motions. Specifically, we adopt the image representation for sketch generation as opposed to using coordinate sequences. This offers several advantages: it 1) maintains constant computation complexity with regards to sketch length, 2) is drawing-order invariant, which is especially favorable in our case as we consider extremely noisy sketches as input, and 3) allows us to utilize large pretrained image generative models. ", "page_idx": 2}, {"type": "text", "text": "2.3 Image Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion Probabilistic Model. First introduced by Sohl-Dickstein et al. [64], diffusion probabilistic models have been widely applied in image generation [24, 65, 29]. To reduce computational cost, Latent Diffusion Models [57] project input images to lower dimension latent space, where diffusion steps are performed. Text-to-image diffusion models [47, 55, 54, 61, 51] achieve text control over image generation by fusing text embeddings obtained by pre-trained language models such as CLIP [52] and T5 [53] into diffusion UNet [58] via cross attention. ", "page_idx": 2}, {"type": "text", "text": "Controllable Image Generation. Beyond text, various recent works have focused on allowing more fine-grained controls over the diffusion process. This can be done via prompt engineering [73, 36, 78], manipulating cross-attention [6, 10, 27, 71], or training additional conditioning adapters [44, 76]. ControlNet [76] learns a trainable copy of the frozen UNet encoder and connects it to the original UNet via zero convolution. T2IAdapter [44] trains a lightweight adapter to predict residuals that are added to the UNet encoder at multiple scales. ", "page_idx": 2}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/c962b9c9eff20a7df537f2162261a15071aa9c7407531ece7b6870e097068d88.jpg", "img_caption": ["Figure 2: Examples of air drawing videos. Left: synthetic hand drawing; Right: real hand drawing. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "In particular, both ControlNet and T2IAdapter take spatially-precise conditional images such as canny edge, depth, and skeleton images extracted from the original image. Koley et al. [33] considers human sketches as a condition and observes that human sketches have shapes that are deformed compared to canny edge or depth images, resulting in generations that lack photo-realism. Therefore, they avoid using the spatial-conditioning approach by training a sketch adapter to transform the sketch into language tokens that replace the text embeddings as the input to the cross attention. ", "page_idx": 3}, {"type": "text", "text": "In our work, we show that the spatial-conditioning does not have to be a \u201chard\u201d constraint, at least not in the domain of sketch generation. Through proper augmentations, we discover that we can indeed teach ControlNet to map from a noisy input condition to a clean output. ", "page_idx": 3}, {"type": "text", "text": "2.4 Hand tracking and gesture recognition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Hand tracking and gesture recognition are used for several purposes, including communication and interaction in an AR environment [42] [34] and VR [1], for image-based pose estimation in sign language recognition [7] [3] [4], and many others [35] [11]. Many of these require depth-sensing hardware, such as work done by Oikonomidis et. al. using a Kinect sensor [49], or use deep learning for pose estimation [68] [19], making it difficult to integrate them into lightweight systems. ", "page_idx": 3}, {"type": "text", "text": "Hand pose estimation such as MediaPipe Hands [74] and OpenPose [9] take in RGB images or video and return the coordinates of 21 landmarks for each hand detected, and MediaPipe Hands is lightweight enough to integrate into even mobile devices. ", "page_idx": 3}, {"type": "text", "text": "3 Air-Drawing Dataset ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In order to thoroughly evaluate our model, we need datasets with sketch-video pairs. Popular sketch datasets include Sketchy [63], TUBerlin [17], and Quick, Draw! [22]. There are also hand motion datasets that associate hand motions with semantic meaning, such as Sign Language MNIST [66], How2Sign [16], and the American Sign Language Lexicon Video Dataset [46]. However, there are no datasets that associate sketches with air drawing hand motions, prompting us to create our own sketch-video pair datasets for evaluation purposes. ", "page_idx": 3}, {"type": "text", "text": "Synthetic Air-Drawing Dataset. We use samples from the Quick, Draw! dataset [22] as the intended ground-truth sketch; each stroke is represented as a sequence of timestamps and coordinates. A 3D human arm asset is animated in the Unity engine [30] using inverse kinematics and rotation constraints, see Figure 2 (left). While following stroke coordinates, the index finger is extended, and when the stroke ends, the hand is in a closed fist. The videos have an aspect ratio of 1920 by 1080 pixels and were recorded at 60 frames per second. We choose 50 common object categories from Quick, Draw! dataset, each with 100 sketches to form our synthetic dataset with a total of 5000 sketch-video pairs. This synthetic Air-Drawing dataset simulates the scenario where users have perfect drawing ability, and the errors are solely introduced by the tracking algorithm. ", "page_idx": 3}, {"type": "text", "text": "Real Air-Drawing Dataset. A human user attempts to replicate sketches from the Quick, Draw! dataset by moving their index finger through the air. The videos were recorded with aspect ratio of 1280 by 720 pixels and at 30 frames per second. We take 10 samples per category used in the synthetic dataset, resulting in 500 video-sketch pairs. ", "page_idx": 3}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/0f3313df9d1a2db08db5f7d6796387c1abeab899fe4d5374226c65d6c44b0346.jpg", "img_caption": ["Figure 3: The overall pipeline for training and inference. During training, we randomly apply augmentations to the original ground-truth sketch to form a distorted view. The distorted view is passed to ControlNet, which is asked to generate the original, undistorted sketch. During inference, a hand-tracking algorithm is used on a hand motion video to create the input. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4 Methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our method trains a controllable DM to recover the clean sketch from a noisy one, which we discuss in Sections 4.1 and 4.2. As input at training time, we simulate noisy sketches produced from the direct application of a hand tracking algorithm, discussed in Section 4.3. Finally, we evaluate our model using two different datasets: a dataset of 3D animated hand motion videos, and a small dataset of real hand motion videos. The creation of these two datasets is discussed in Section 3. ", "page_idx": 4}, {"type": "text", "text": "4.1 Preliminary: Controllable DMs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Diffusion models involve a forward and inverse diffusion process. Given an input image $\\pmb{x}_{0}$ , the forward process gradually adds noise to $\\scriptstyle x_{0}$ to form a noisy input $\\pmb{x}_{t}$ at time step $\\pmb{t}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pmb{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\pmb{x}_{0}+(\\sqrt{1-\\bar{\\alpha}_{t}})\\epsilon,\\epsilon\\sim\\mathcal{N}(0,I)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\bar{\\alpha}_{t}:=\\prod_{i=1}^{t}\\alpha_{i}}\\end{array}$ , and $\\alpha_{t}=1-\\beta_{t}$ is determined by a pre-defined noise scheduler [24]. ", "page_idx": 4}, {"type": "text", "text": "The reverse process trains a denoising UNet $\\epsilon_{\\theta}(\\cdot)$ to predict the noise $\\epsilon$ added to input $z_{\\mathrm{0}}$ . In the context of controllable generation, as in ControlNet [76] and T2IAdapter [44], with a set of conditions including a text prompt $c_{t}$ and an additional condition $c_{\\mathrm{f}}$ , the overall loss can be defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}_{\\boldsymbol{x}_{0},t,\\boldsymbol{c}_{t},\\boldsymbol{c}_{t},\\epsilon\\sim\\mathcal{N}(0,1)}\\Big[\\|\\epsilon-\\epsilon_{\\theta}(\\boldsymbol{x}_{t},t,\\boldsymbol{c}_{t},\\boldsymbol{c}_{\\mathrm{f}})\\|_{2}^{2}\\Big],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.2 Training Controllable DMs for Sketch Recovery ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We adopt ControlNet [76] as our primary controlling approach. Our training procedure is illustrated in Figure 3. Due to the lack of sketch-video pair datasets, we devise a self-supervised, augmentationbased training procedure. During training, for each sketch image, we randomly sample combinations of augmentations $A(\\cdot)$ and apply to $\\pmb{x}_{0}$ to get the distorted view $\\boldsymbol{\\mathcal{A}}(\\mathbf{\\boldsymbol{x}}_{0})$ . It is then used as the input to ControlNet\u2019s conditioning adapter. Hence, the loss function 2 can be re-written as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}=\\mathbb{E}_{\\mathbf{1}_{0},t,c_{t},c_{\\mathrm{f}},\\epsilon\\sim\\mathcal{N}(0,1)}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\mathbf{x}_{t},t,c_{t},\\mathcal{A}(\\mathbf{x}_{0}))\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Therefore, unlike regular controllable DMs where the conditioning adapter takes in edge-like maps and predicts spatial-conditioning signals to be injected to the UNet, our adapter learns both the spatial-conditioning signals and a mapping from the distorted to the clean input: $\\boldsymbol{A}(\\boldsymbol{x}_{0})\\rightarrow\\boldsymbol{x}_{0}$ . ", "page_idx": 4}, {"type": "text", "text": "4.3 Sketch Augmentations ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We categorize the prevalent errors from air drawings into three types: 1) user-induced artifacts such as hand jitters and stroke distortions, 2) hand tracking errors such as inaccurate hand landmark predictions, unintended strokes, and 3) aesthetic shortcomings related to the user\u2019s drawing proficiency. ", "page_idx": 4}, {"type": "text", "text": "In order to closely replicate these artifacts, we carefully examine typical noise found in real tracking samples and divide them into 3 categories: local, structural, and false strokes. For each category, we observe several types of artifacts, and apply augmentations to introduce each artifact to an input sketch. Visual examples for these augmentations are shown in Figure 4. ", "page_idx": 5}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/b0312215d8c6bc72ac3bcf9c2e5efe78b24152313f89409694fb18b1b5da8be9.jpg", "img_caption": ["Figure 4: Visual examples of different augmentations. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Local artifacts include jitters, stroke-wise distortion, and random spikes. Jitters are common and become especially obvious when drawing speed is low. Stroke-wise distortion happens when users fail to properly draw the desired shape primitive (e.g. drawing a rectangle but ending up with unwanted curvy edges), potentially due to lack of friction and visual landmarks. Random spikes can arise from an accidental jerk of a user\u2019s hand or an incorrect detection from the tracking algorithm. ", "page_idx": 5}, {"type": "text", "text": "Structural artifacts include sketch-level distortion, incorrect stroke size, and misplacement. Sketchlevel distortion refers to an overall distorted aspect ratio; incorrect stroke size and misplacement occur when the user unintentionally draws a stroke too small/large, and/or at the wrong position. ", "page_idx": 5}, {"type": "text", "text": "False strokes refer to unintentionally drawn strokes which commonly occur in three situations: entering or exiting the canvas, transitioning between strokes, and hesitating during drawing. ", "page_idx": 5}, {"type": "text", "text": "Unlike real images where shapes and scales are strictly defined, sketches are more versatile and thus do not have a clear boundary for correctness. For example, a slight change of scale during augmentation does not falsify the sketch. Therefore, we carefully tune the augmentation parameters such that the resulting augmented sketches are, in general, obviously incorrect. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Implementation Details. In training, we use a subset of 100 categories from the Quick, Draw! dataset [22]. Because a large portion of Quick, Draw! sketches are not well-drawn, we first calculate the CLIP Image-Text similarity between each sketch and their corresponding category and select the top $5\\%$ from each category, resulting in 60K sketches. Note that the sketches used for training are mutually exclusive with the sketches used for generating synthetic and real tracking images used during evaluation. To test for generalizability, we select 10 categories with similar statistics 2 from the rest and exclude them from training. ", "page_idx": 5}, {"type": "text", "text": "We primarily use Stable Diffusion XL [51] (SDXL) in our experiments, and adhere to the original ControlNet training and inference procedures. During both training and inference phases, we use text prompts in the format of \u201ca black and white sketch of a <category>\u201d to guide the model generation. We also finetune SDXL on the Quick, Draw! dataset with Low-Rank Adaptation [25] (LoRA) in order to \u201cequip\u201d the model with the basic ability to generate sketches in the appropriate style. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics. We primarily focus on using faithfulness, or the similarity between the generated sketch and the ground-truth sketch, as our model performance. Due to the versatility of sketches, we adopt multiple metrics to ensure comprehensive measurements. On the pixel-level, we use SSIM [70] to measure detailed local structural similarity, and Chamfer Distance [2] (CD) for global comparison, as CD is less sensitive to local density mismatch. Taking a perceptual perspective, we adopt LPIPS [77], CLIP [52] Image-to-Image similarity (I2I), and CLIP Image-to-Text similarity (I2T) between sketches and their corresponding text prompts to measure \u201crecognizability\u201d. ", "page_idx": 5}, {"type": "text", "text": "We benchmark our model on the similarity between the ground-truth sketch and the hand tracking image. We then train a ControlNet on sketches but without any augmentation as our second baseline. ", "page_idx": 5}, {"type": "table", "img_path": "ttLcbEkaj6/tmp/305fa5f14c1aeb8cf0718cfaf82ad0dc3f1f9f49a24df1431564eeae1f3652c7.jpg", "table_caption": ["Table 1: Results on the similarity between generated and ground-truth sketches from Quick, Draw! dataset. \u201cTracking\u201d refers to hand tracking images, and \u201cGen.\u201d refers to generated images. \u201cw/ Aug.\u201d refers to whether sketch augmentations have been applied. \u201cCLIP I2I/I2T\u201d refers to CLIP Image-to-Image/Image-to-Text similarity. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/758d48c09440954119d71f0294e30d21b49772092198bb229536ecebcfd9ebdd.jpg", "img_caption": ["Figure 5: Generations on TUBerlin dataset. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Results and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Faithfulness. In Figure 1 we can clearly observe the ControlNet trained with augmentations successfully identifies the visual cues from the noisy tracking image, removes artifacts, and generates the clean sketches, while being aesthetic and semantically coherent. Unsurprisingly, ControlNet trained without augmentations fails to make any improvement from the tracking. In Figure 5 we show additional results where the model is trained on TUBerlin [17] dataset. ", "page_idx": 6}, {"type": "text", "text": "Table 1 shows quantitative results for measuring the faithfulness of generated sketch images to ground-truth. For both synthetic and real datasets, we observe a noticeable performance gain. For example, with the real dataset and SDXL, SSIM increases by $10\\%$ in SSIM, LPIPS decreases by $6\\%$ , and CD decreases by $21\\%$ . ", "page_idx": 6}, {"type": "text", "text": "In Table 2 and Figure 6 we show comparison between ours and Sketch-Pix2Seq [12] (P2S). Since P2S performs best when the model is trained on only one category, we randomly pick 10 categories ", "page_idx": 6}, {"type": "text", "text": "Table 2: Quantitative comparison be- Figure 6: A comparison between generations from Sketch-Pix2Seq tween Sketch-Pix2Seq (P2S.) and ours (top) and ours (bottom).   \non a subset of 10 classes. ", "page_idx": 7}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/7004f15757c889631009a78701e0019e49b9fb01c415e15ff1f184505445325e.jpg", "img_caption": ["Figure 8: Generations on unseen categories from Quick, Draw! dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "to train $10\\,\\mathrm{P2S}$ models, and compare with ours using the selected categories. For each P2S model, following the original work, we first train 60M steps for reconstructing the exact same input sketch, and subsequently train 40M steps for constructing the clean sketch given the noisy tracking image. In Table 2 our SSIM and CD are noticeably higher than P2S, suggesting better faithfulness. This is validated in Figure 6, where we can observe while P2S is able to generate semantically correct sketches, it fails to follow the input tracking faithfully. Moreover, as P2S requires to train a separate model on each individual category and has no generalizability, it is hard to adapt to real-world usage. ", "page_idx": 7}, {"type": "text", "text": "In Figure 7 we show visualizations of the ControlNet hidden states across denoising steps for both the baseline model, trained with the original ControlNet recipe (top row), and our model with noise-augmented training (bottom row). The baseline ControlNet hidden states closely approximate the input edge structure early in the denoising process but fail to converge toward a clean sketch representation, resulting in largely static visualizations. ", "page_idx": 7}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/b0d16f69f5f81f10660c6c308bd75639e4e5694fdf09797ee1998d6bcfbe9eee.jpg", "img_caption": ["Figure 7: A comparison of visualization of ControlNet hidden states throughout denoising process from baseline approach without augmentation (top) and with augmentations (bottom). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "In contrast, the hidden states from our noise-augmented training progressively reveal a coherent and accurate sketch outline throughout the denoising process. ", "page_idx": 7}, {"type": "text", "text": "Generalizability. From Figure 8 we can see that our model generalizes well to categories which are not seen during training, suggesting that the trained ControlNet learns a robust category-agnostic mapping from the noisy to clean sketch. The generalizability is also verified in Table 1, where the resulting scores for unseen categories are close to scores obtained on the seen categories. ", "page_idx": 7}, {"type": "text", "text": "5.2 Ablations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Role of Augmentations. In Table 3 we provide similarity scores between ground-truth and generated sketches when different combinations of augmentations are applied in training. We observe that local augmentations plays an important role in detailed local similarity: with only local augmentation applied, SSIM increases more than other metrics, with a $10\\%$ improvement to the baseline, yet increases by only $4\\%$ and $0\\%$ when only false strokes and structural augmentations are applied. Conversely, false strokes and structural augmentation have larger effects on the global similarity of the generated sketch: CD decreases by $9\\%$ and $3\\%$ when only false strokes and structural augmentations are applied, respectively. ", "page_idx": 7}, {"type": "table", "img_path": "ttLcbEkaj6/tmp/d9e5e671fbabe3a5975be4a5170f92cc661b66162f924ebf7cd899476fa97901.jpg", "table_caption": ["Table 3: Model trained on seen categories with different combinations of augmentations. All the experiments are conducted using Stable Diffusion1.5 and trained for 5K steps. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/a650661fabfe7d437424ba4e6a5b1bcf71465fb1c3c05af4af487269f0bfb709.jpg", "img_caption": ["Figure 10: Examples of incorrect generation on unseen categories due to the absence of text prompt (w/o Prompt), comparing to correct generations when prompt is present (w/ Prompt). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "In Figure 9 we provide visual results when different combinations of sketch augmentations are applied during training. We observe that local augmentations are indeed crucial to removing jitters and correcting deformed lines, while false stroke augmentations ensure that the model does not falsely follow the spatial-conditions introduced by these false strokes. The structural augmentations are less significant, likely because structural artifacts are not as common as local artifacts and false strokes. ", "page_idx": 8}, {"type": "text", "text": "Effect of Text Prompts. In", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "put tracking image conditions are extremely noisy and may not even possess obvious visual cues about the nature of the intended sketch. It is then important to investigate the effect of text guidance on the generated output and examine if our augmentationbased training significantly contributes to the generation, or if it is guided purely by text prompts. ", "page_idx": 8}, {"type": "text", "text": "In Table 4 we show the similarity scores between the ground-truth and generated sketches with or without prompt on seen or unseen categories. When no augmentation is applied during training, there is little to no perfor", "page_idx": 8}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/8c17fcb6f62d619dc6340588811eff868b6c3ec6d74e13fa27790b581df3c11e.jpg", "img_caption": ["Figure 9: Qualitative results when different combinations of augmentations are applied during training. Each column represents the generated sketches when the model is trained with one combination of augmentations, e.g. Column 2 are the generated sketches when only structural augmentations are applied during training. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "mance gain, even with a text prompt. When augmentations are present, in most cases, there is noticeable improvement across all metrics. Such results verify the necessity of our augmentationbased training procedure. ", "page_idx": 8}, {"type": "text", "text": "We find that the model tends to rely more on text prompts when working with unseen categories. When prompts are not present, CLIP I2T and CD drop $24.7\\%$ and $21.4\\%$ respectively on the unseen categories, versus a decrease of $4.9\\%$ and $10.3\\%$ on the seen categories. In Figure 10, we show ", "page_idx": 8}, {"type": "text", "text": "Table 4: Quantitative results on real air drawing dataset with/without prompts. \u201cw/ Aug.\u201d refers to with/without augmentations. \u201cw/ P.\u201d refers to with/without prompts. \u201cI2I\u201d refers to CLIP Image-Image similarity between tracking/generated and ground-truth sketch, and \u201cI2T\u201d refers to CLIP Image-Text similarity between sketches and texts associated with their class labels. ", "page_idx": 9}, {"type": "text", "text": "Figure 11: The impact of level of chaos of the tracking on the faithfulness of the generated sketch, based on CD. We divide CD between tracking and groundtruth sketch equally into four bins. Within each bin, we plot the mean of CD between ground-truth sketch, tracking, and generated image with/without prompt. ", "page_idx": 9}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/b8fb21218ff05e4d83d900382d08a98bc8b8d9c7aba4a35c5df3ce05d335030d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "examples of the model failing to generate the correct sketch when no prompt is present, under unseen categories. Consider the bottom left example of Figure 10: the ground-truth sketch (Ground-truth) represents a snail. With the corresponding text prompt, the model successfully generates a sketch of a snail, following visual cues in the tracking image (Tracking) such as the spiral shape. When no prompt is given, the model falsely, yet reasonably, generates a fan (w/o Prompt) \u2013 a category seen during training. In fact, in the absence of a text prompt and given an input from an unseen category, we find that the failure cases tend to be generated sketches of the seen categories. ", "page_idx": 9}, {"type": "text", "text": "Effect of level of chaos on conditional input. In Figure 11, we investigate how the amount of chaos in the input conditioning tracking image, measured by the CD between the ground-truth and tracking images, affects generation. The improvements are more obvious when the CD between the ground-truth and tracking images is large. On the other hand, the CD between ground-truth and generated sketches are almost the same when the tracking image is already close to the ground-truth sketch, indicating that the generation is being faithful to the ground-truth sketch. We also observe that when the level of chaos is high for the unseen categories (bottom right), the performance gain with text guidance is most obvious, suggesting that the model relies more on text guidance for correct generation. Conversely, the model depends less on the text prompts when the input chaos level is low or under seen categories. Such results are also in line with Figure 10 and Table 4 as discussed above. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we tackle the problem of marker-less air drawing by leveraging a spatially-controlled diffusion model. We devise a simple augmentation-based data-free training procedure to learn a mapping from noisy to clean sketches. We collect two hand drawing datasets and verify that the trained model can effectively generate coherent and faithful sketch from an extremely noisy tracking image, and exhibits decent generalizability. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work. We provide a stepping stone for the proposed task by creating a framework that establishes a correspondence between hand motions and clean, pleasing, and coherent illustrations, and providing datasets to evaluate these tasks. However, this work does not explore the possibility of using hand gestures to create complex, full color images that image diffusers are known for. It also assumes that the desired output sketch is simple and often cartoon-like, as a majority of the Quick, Draw! sketches are drawn with few lines and simple shapes. ", "page_idx": 9}, {"type": "text", "text": "Societal Impact. Generative models for media are generally prone to misuse which can lead to the spread of inappropriate and harmful content. While this work is based in the sketch domain, there could still be adverse adoption for generating mocking content and bullying, especially by younger users. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Alex Adkins. The Importance of Hand Motions for Communication and Interaction in Virtual Reality. PhD thesis, Clemson University, 2022.   \n[2] M. Akmal Butt and P. Maragos. Optimum design of chamfer distance transforms. IEEE Transactions on Image Processing, 7(10):1477\u20131484, 1998. doi: 10.1109/83.718487. [3] Alaa Al-Obodi, Ameerh Al-Hanine, Khalda Al-Harbi, Maryam Al-Dawas, and Amal AlShargabi. A saudi sign language recognition system based on convolutional neural networks. Building Services Engineering Research and Technology, 13, 11 2020. doi: 10.37624/IJERT/13. 11.2020.3328-3334.   \n[4] Rahaf Alawwad, Ouiem Bchir, and Mohamed Maher Ben Ismail. Arabic sign language recognition using faster r-cnn. International Journal of Advanced Computer Science and Applications, 12:692\u2013700, 03 2021. doi: 10.14569/IJACSA.2021.0120380.   \n[5] Judith Amores Fernandez and Jaron Lanier. Holoart video showcase: Painting with holograms in mixed reality. pages 466\u2013466, 05 2017. doi: 10.1145/3027063.3049787.   \n[6] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, June 2022. doi: 10.1109/cvpr52688.2022.01767. URL http://dx.doi.org/ 10.1109/CVPR52688.2022.01767. [7] Qanita Bani Baker, Nour Alqudah, Tibra Alsmadi, and Rasha Awawdeh. Image-based arabic sign language recognition system using transfer deep learning models. Applied Computational Intelligence and Soft Computing, 2023:1\u201311, 12 2023. doi: 10.1155/2023/5195007.   \n[8] Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, and Yi-Zhe Song. Vectorization and rasterization: Self-supervised learning for sketch and handwriting, 2021.   \n[9] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. CoRR, abs/1812.08008, 2018. URL http://arxiv.org/abs/1812.08008.   \n[10] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance, 2023.   \n[11] Weiya Chen, Chenchen Yu, Chenyu Tu, Zehua Lyu, Jing Tang, Shiqi Ou, Yan Fu, and Zhidong Xue. A survey on hand pose estimation with wearable sensors and computer-vision-based methods. Sensors, 20:1074, 02 2020. doi: 10.3390/s20041074.   \n[12] Yajing Chen, Shikui Tu, Yuqi Yi, and Lei Xu. Sketch-pix2seq: a model to generate sketches of multiple categories, 2017. URL https://arxiv.org/abs/1709.04121.   \n[13] Takanori Chihara and Akihiko Seo. Evaluation of physical workload affected by mass and center of mass of head-mounted display. Applied ergonomics, 68:204\u2013212, 04 2018. doi: 10.1016/j.apergo.2017.11.016.   \n[14] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. Democratising 2d sketch to 3d shape retrieval through pivoting. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 23218\u201323229, 2023. doi: 10.1109/ICCV51070.2023.02127.   \n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.   \n[16] Amanda Cardoso Duarte, Shruti Palaskar, Deepti Ghadiyaram, Kenneth DeHaan, Florian Metze, Jordi Torres, and Xavier Gir\u00f3-i-Nieto. How2sign: A large-scale multimodal dataset for continuous american sign language. CoRR, abs/2008.08143, 2020. URL https://arxiv. org/abs/2008.08143.   \n[17] Mathias Eitz, James Hays, and Marc Alexa. How do humans sketch objects? ACM Trans. Graph. (Proc. SIGGRAPH), 31(4):44:1\u201344:10, 2012.   \n[18] Rinon Gal, Yael Vinker, Yuval Alaluf, Amit H. Bermano, Daniel Cohen-Or, Ariel Shamir, and Gal Chechik. Breathing life into sketches using text-to-video priors. 2023.   \n[19] Liuhao Ge, Hui Liang, Junsong Yuan, and Daniel Thalmann. Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns. 06 2016. doi: 10.1109/CVPR. 2016.391.   \n[20] Google. Tiltbrush, 2016. https://www.tiltbrush.com/.   \n[21] Google Creative Lab and Uncorked Studios. Just a line, March 2018. https://experiments.withgoogle.com/justaline.   \n[22] Google Research. Introducing the kaggle \u201cquick, draw!\u201d doodle recognition challenge, September 2018. https://blog.research.google/2018/09/introducing-kaggle-quick-draw-doodle.html.   \n[23] David Ha and Douglas Eck. A neural representation of sketch drawings, 2017.   \n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.   \n[25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.   \n[26] Jonas Jongejan. ARCore Drawing, August 2017. https://experiments.withgoogle.com/arcoredrawing.   \n[27] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: A native skeleton-guided diffusion model for human image generation, 2023.   \n[28] Daniel Keefe, Robert Zeleznik, and David Laidlaw. Drawing on air: Input techniques for controlled 3d line illustration. IEEE Transactions on Visualization and Computer Graphics, 13 (5):1067\u20131081, 2007. doi: 10.1109/TVCG.2007.1060.   \n[29] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models, 2023.   \n[30] Knife Entertainment. Realistic FPS hands, 2018. https://assetstore.unity.com/packages/3d/characters/realistic-fps-hands-107409.   \n[31] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. How to handle sketch-abstraction in sketch-based image retrieval?, 2024.   \n[32] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. You\u2019ll never walk alone: A sketch and text duet for fine-grained image retrieval, 2024.   \n[33] Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. It\u2019s all about your sketch: Democratising sketch control in diffusion models, 2024.   \n[34] Taehee Lee and Tobias H\u00f6llerer. Handy ar: Markerless inspection of augmented reality objects using fingertip tracking. pages 83\u201390, 11 2007. ISBN 978-1-4244-1452-9. doi: 10.1109/ISWC.2007.4373785.   \n[35] Rui Li, Zhenyu Liu, and Jianrong Tan. A survey on 3d hand pose estimation: Cameras, methods, and datasets. Pattern Recognition, 93, 04 2019. doi: 10.1016/j.patcog.2019.04.026.   \n[36] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation, 2023.   \n[37] Fengyin Lin, Mingkang Li, Da Li, Timothy Hospedales, Yi-Zhe Song, and Yonggang Qi. Zero-shot everything sketch-based image retrieval, and in explainable style, 2023.   \n[38] Hangyu Lin, Yanwei Fu, Yu-Gang Jiang, and Xiangyang Xue. Sketch-bert: Learning sketch bidirectional encoder representation from transformers by self-supervised learning of sketch gestalt, 2020.   \n[39] Fang Liu, Xiaoming Deng, Yu-Kun Lai, Yong-Jin Liu, Cuixia Ma, and Hongan Wang. Sketchgan: Joint sketch completion and recognition with generative adversarial network. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5823\u20135832, 2019. URL https://api.semanticscholar.org/CorpusID:182164433.   \n[40] Li Liu, Fumin Shen, Yuming Shen, Xianglong Liu, and Ling Shao. Deep sketch hashing: Fast free-hand sketch-based image retrieval, 2017.   \n[41] Ling Luo, Yulia Gryaditskaya, Tao Xiang, and Yi-Zhe Song. Structure-aware 3d vr sketch to 3d shape retrieval, 2022.   \n[42] Eric Marchand, Hideaki Uchiyama, and Fabien Spindler. Pose estimation for augmented reality: A hands-on survey. IEEE Transactions on Visualization and Computer Graphics, 22, 01 2016. doi: 10.1109/TVCG.2015.2513408.   \n[43] David McNeill. Hand and Mind: What Gestures Reveal about Thought. University of Chicago Press., Chicago, Illinois, USA, 1992.   \n[44] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.   \n[45] Mozilla. A-painter, 2016. https://blog.mozvr.com/a-painter/.   \n[46] Carol Neidle, Ashwin Thangali, and Stan Sclaroff. Challenges in the development of the american sign language lexicon video dataset (asllvd) corpus. 05 2012.   \n[47] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022.   \n[48] Oculus and Smoothstep. Quill, 2016. https://quill.art/.   \n[49] Iason Oikonomidis, Nikolaos Kyriazis, and Antonis Argyros. Efficient model-based 3d tracking of hand articulations using kinect. volume 1, 01 2011. doi: 10.5244/C.25.101.   \n[50] Kaiyue Pang, Ke Li, Yongxin Yang, Honggang Zhang, Timothy M. Hospedales, Tao Xiang, and Yi-Zhe Song. Generalising fine-grained sketch-based image retrieval. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 677\u2013686, 2019. doi: 10.1109/CVPR.2019.00077.   \n[51] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.   \n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.   \n[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.   \n[54] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021.   \n[55] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.   \n[56] Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, and Moacir Ponti. Sketchformer: Transformer-based representation for sketched structure, 2020.   \n[57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models, 2022.   \n[58] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015.   \n[59] Enrique Rosales, Chrystiano Ara\u00fajo, Jafet Rodriguez, Nicholas Vining, DongWook Yoon, and Alla Sheffer. Adaptibrush: adaptive general and predictable vr ribbon brush. ACM Trans. Graph., 40(6), 2021. doi: 10.1145/3478513.3480511. URL https://doi.org/10.1145/ 3478513.3480511.   \n[60] Michael Rupp. Is it getting hot in here? the effects of vr headset microclimate temperature on perceived thermal discomfort, vr sickness, and skin temperature. Applied Ergonomics, 114, 2024.   \n[61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.   \n[62] Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. Clip for all things zero-shot sketch-based image retrieval, fine-grained or not, 2023.   \n[63] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. The sketchy database: learning to retrieve badly drawn bunnies. ACM Transactions on Graphics, 35:1\u201312, 07 2016. doi: 10.1145/2897824.2925954.   \n[64] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/sohl-dickstein15.html.   \n[65] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id $\\cdot$ St1giarCHLP.   \n[66] tecperson. Sign language mnist: Drop-in replacement for mnist for hand gesture recognition tasks, 2016. https://www.kaggle.com/datasets/datamunge/sign-language-mnist.   \n[67] Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso: Semantically-aware object sketching, 2022.   \n[68] Chengde Wan, Thomas Probst, Luc Van Gool, and Angela Yao. Self-supervised 3d hand pose estimation through training by ftiting. pages 10845\u201310854, 06 2019. doi: 10.1109/CVPR.2019. 01111.   \n[69] Fang Wang, Le Kang, and Yi Li. Sketch-based 3d shape retrieval using convolutional neural networks, 2015.   \n[70] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004. doi: 10.1109/TIP.2003.819861.   \n[71] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion, 2023.   \n[72] Yan Yan, Ke Chen, Yu Xie, Song Yiming, and Yonghong Liu. The Effects of Weight on Comfort of Virtual Reality Devices, pages 239\u2013248. 01 2019. ISBN 978-3-319-94705-1. doi: 10.1007/978-3-319-94706-8_27.   \n[73] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Reco: Region-controlled text-to-image generation, 2022.   \n[74] Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George Sung, Chuo-Ling Chang, and Matthias Grundmann. Mediapipe hands: On-device real-time hand tracking. CoRR, abs/2006.10214, 2020. URL https://arxiv.org/abs/2006.10214.   \n[75] Jingyi Zhang, Fumin Shen, Li Liu, Fan Zhu, Mengyang Yu, Ling Shao, Heng Tao Shen, and Luc Van Gool. Generative domain-migration hashing for sketch-to-image retrieval. In Proceeding of European Conference on Computer Vision (ECCV), 2018.   \n[76] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023.   \n[77] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.   \n[78] Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and Xin Wang. Controllable text-to-image generation with gpt-4, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Implementation. Below we provide implementation details for each augmentation: ", "page_idx": 15}, {"type": "text", "text": "\u2022 Local Augmentation includes 3 types of sub-augmentations: stroke distortion, random spikes, and jitters. For stroke distortion, we generate a number of wave functions each with random frequency, shift, and amplitude. We aggregate them to form a random curve, whose value is used to distort the ground-truth sketch. Spikes are implemented in two modes sharp spikes and smooth spikes. Sharp spikes are determined by the width and height which are randomly sampled from a normal distribution; smooth spikes are implemented with Bezier curve, and the control points are randomly sampled from a uniform distribution within predefined range along the gradient of the ground-truth sketch. Jitters are simply small perturbations sampled from a pre-defined normal distribution and added to random locations along the ground-truth sketch. ", "page_idx": 15}, {"type": "text", "text": "\u2022 Structural Augmentation also include 3 types of sub-augmentations: sketch-wise distortion & relocation, stroke-wise misplacement, and stroke-wise resize. For sketch-wise distortion& relocation, we randomly shrink and change aspect ratio of the whole sketch and reposition the sketch in the canvas. For stroke-wise misplacement/resize, we randomly move/resize each stroke within a pre-defined range. ", "page_idx": 15}, {"type": "text", "text": "\u2022 False Strokes includes two types: transitional and random false strokes. For transitional false strokes, we simply draw lines between the transition of each stroke; for random false strokes, we randomly draw a number of extra lines on the canvas. ", "page_idx": 15}, {"type": "text", "text": "In the above augmentations, misplacement and stroke resize under structural augmentation are mutually-exclusively applied. This is because it is highly probable that the combination of the two will completely destroy the visual cues of the resulting sketch image. For all other augmentations, we set a $50\\%$ chance for each to be applied to each sketch sample during training. ", "page_idx": 15}, {"type": "text", "text": "A.2 Training Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Held-out categories. Because different object categories have different drawing difficulty, sketch and tracking samples belonging to different categories exhibit large variance in statistics (CLIP ImageText similarity between ground-truth sketch and corresponding text, CLIP Image-Image similarity, Chamfer Distance, or SSIM between ground-truth and tracking image). To allow for fair comparison of the performance between seen and held-out categories, we first partition categories into 10 clusters using K-Means Clustering, and randomly sample one category from each cluster as the held-out categories. The held-out categories are: ", "page_idx": 15}, {"type": "text", "text": "{car, face, cow, snail, diamond, candle, angel, cat, grapes, sun} ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Training configurations. All training is conducted on two Nvidia H100 GPUs. For finetuning the diffusion model with LORA, we set LORA rank to 4, per device batch size to 16, learning rate to $5\\mathrm{e}^{-5}$ , gradient accumulation steps to 4, and train for 6K steps on the Quick, Draw! dataset. For our augmentation-based ControlNet training, we set per device batch size to 8, learning rate to $2\\mathrm{e}^{-5}$ , gradient accumulation steps to 4, and the proportion of empty prompts to $25\\%$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Comparison on Egocentric Hand Tracking Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "An example of hand landmarking with MediaPipe, OpenPose, and NRSM is shown in Figure 12. We can see that only MediaPipe\u2019s hand landmarker comparatively accurate and consistent, while OpenPose often fails to detect landmarks, and NSRM is less accurate. However, even though MediaPipe seems to accurately predict hand landmarks, the resulting tracking image, as shown in Figure 1 and Figure 15, still contains large amount of noise. ", "page_idx": 15}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/6e63023f230d9bc6f70e1d79ae9227b6b987f59a87f2806d9ae8f915b5389aea.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 12: A comparison of hand landmarking done by MediaPipe, OpenPose, and NSRM respectively. MediaPipe in general provides most accurate hand landmarks, while OpenPose often struggling to detect the hand, and NSRM not able to provide accurate landmarks. ", "page_idx": 16}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/0958c389871441de8c2859b8d1142384d4b30b420de9e96176131e1fdd4bc5a8.jpg", "img_caption": ["Figure 13: Auto-completion given a partially-drawn sketch in two modes: line segment missing (Column 1&5) and whole strokes missing (Column 3&7). Bordered images are the generated results. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.4 Sketch Completion & Text-conditioned styling. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "With simple augmentations, we can also perform other sketch-based tasks such as sketch completion and text-conditioned styling. Unlike prior works [39, 38] on sketch completion that require sophisticated architectures or training procedures, we could achieve sketch completion by simply adding random erasure to the set of augmentations during training. In Figure 13, we show two types of sketch completion: completion with small missing line segments (column 1&5), and completion with whole strokes missing (column 3&7). In addition, as shown in Figure 14, we can also utilize the text-conditioning capacity of DMs to prompt for different stroke styles, such as different color and thickness, therefore alleviating the need for fine-grained style control on the user\u2019s part. ", "page_idx": 16}, {"type": "text", "text": "A.5 Visual Comparison between Controllable DMs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In Figure 15 we compare the generated sketches between ControlNet and T2IAdapter, when trained under our augmentation-based procedure. We can observe that both ControlNet and T2IAdapter generate visually coherent sketches and generally follow the input tracking image. Nonetheless, by looking into details, ControlNet\u2019s generations are more faithful to the visual cues from the tracking images. For example, the butterfly (bottom right) generated by ControlNet resembles the shapes of the wings from tracking image closely, while the generation from T2IAdapter looks much less similar. ", "page_idx": 16}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/c90a485cfb5b9c10b73876362ac6b64339f0b2e493db2c4457b7d580c2e5dd1a.jpg", "img_caption": ["Figure 14: Generation with text instructions for different line colors and brush thickness. Bordered images are the generated results. We use the prompt: \u201cA sketch of a <category>, <color> lines, <thickness> brush.\u201d "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "ttLcbEkaj6/tmp/d849606857ff8ebd3f7b981fa4c520a354a3897f3fadb5113721d0d80154325a.jpg", "img_caption": ["Figure 15: More inference results on ControlNet with our augmentation-based training procedure. We additionally show inference results by using T2IAdapter in our method. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: All claims made in the abstract and introduction are true and accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: See \"Limitations and Future Work\" in Section 6. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: There were no theoretical results ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: Paper details the model used, the data used in the training process, and the data used in inference. These are also all publicly available. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We release the full code and dataset. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper states that is follows ControlNet\u2019s provided training and inference methods, and states any time a hyperparameter is different. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: Error bars are not reported because it would be too computationally expensive; also, the improvement on quantitative results are obvious across multiple metrics, and are backed by qualitative results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: See Appendix A.2. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: Research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The work in this paper outputs sketch-like, oftentimes cartoon-like, images that might be misued for generating inappropriate memes, especially in the teenager category. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The data and model used in this paper have already been vetted to pose minimal risk for misuse by their creators. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Existing assets (i.e. Unity game engine and 3D assets used) were properly cited, see Section 3. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The creation and contents of the two new datasets are well documented in Section 3. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor resesarch with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor resesarch with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]