[{"figure_path": "ttLcbEkaj6/figures/figures_0_1.jpg", "caption": "Figure 1: Given a hand drawing video, we first use a hand detection algorithm to extract the tracking image (Hand Tracking). We then take this noisy tracking image and generate a clean sketch (Generated), which faithfully and aesthetically resembles the intended sketch (Ground-truth).", "description": "This figure shows the process of generating a clean sketch from a hand-drawn video.  The left column shows the hand drawing video. The middle column shows the noisy tracking image extracted from the video using a hand detection algorithm.  The right column displays the generated clean sketch, compared with the ground truth sketch. This demonstrates the ability of the model to translate from a noisy input to a clean and aesthetically pleasing output.", "section": "Abstract"}, {"figure_path": "ttLcbEkaj6/figures/figures_3_1.jpg", "caption": "Figure 2: Examples of air drawing videos. Left: synthetic hand drawing; Right: real hand drawing.", "description": "This figure shows two examples of air drawing videos. The left side displays a synthetic hand drawing video, which is computer-generated.  The right side displays a real hand drawing video captured with a camera. The images are included to help illustrate the difference between the synthetic dataset (used for training) and the real-world dataset (used for evaluation).  The videos showcase the hand movements that are used as input for generating sketches in the AirSketch model.", "section": "2.4 Hand tracking and gesture recognition"}, {"figure_path": "ttLcbEkaj6/figures/figures_4_1.jpg", "caption": "Figure 3: The overall pipeline for training and inference. During training, we randomly apply augmentations to the original ground-truth sketch to form a distorted view. The distorted view is passed to ControlNet, which is asked to generate the original, undistorted sketch. During inference, a hand-tracking algorithm is used on a hand motion video to create the input.", "description": "This figure illustrates the training and inference pipeline of the AirSketch model.  During training, the model learns to reconstruct clean sketches from noisy, augmented versions of sketches.  During inference, a hand-tracking algorithm processes a video of hand movements to create a noisy input sketch, which is then fed into the trained model to generate a clean output sketch. The process utilizes ControlNet, a controllable diffusion model, along with text prompts and augmentation techniques to achieve faithful and aesthetic sketch generation.", "section": "4 Methods"}, {"figure_path": "ttLcbEkaj6/figures/figures_5_1.jpg", "caption": "Figure 4: Visual examples of different augmentations.", "description": "This figure shows examples of different types of augmentations applied to a sketch of an angel. The augmentations are categorized into three groups: local, structural, and false strokes. Local augmentations include jitters, stroke-wise distortions, and random spikes. Structural augmentations include sketch-level distortions, incorrect stroke sizes, and misplacements. False strokes include transitional strokes and random false strokes. Each augmentation aims to simulate different types of errors that might occur during air drawing, such as hand jitters, tracking errors, and user-induced artifacts. These augmentations are used during training to make the model robust to noisy and distorted hand tracking data.", "section": "4.3 Sketch Augmentations"}, {"figure_path": "ttLcbEkaj6/figures/figures_6_1.jpg", "caption": "Figure 5: Generations on TUBerlin dataset.", "description": "This figure shows examples of sketches generated by the model on the TUBerlin dataset.  It visually demonstrates the model's ability to generate clean and aesthetically pleasing sketches from noisy hand-tracking data. The figure compares the hand-tracking input, the generated sketch, and the ground truth sketch for several different examples. This allows for a visual assessment of the model's faithfulness and aesthetic quality.", "section": "5.1 Results and Analysis"}, {"figure_path": "ttLcbEkaj6/figures/figures_7_1.jpg", "caption": "Figure 5: Generations on TUBerlin dataset.", "description": "This figure shows several examples of sketches generated by the model trained on the TUBerlin dataset. Each row displays the noisy hand tracking image (left), the generated sketch (middle), and the ground truth sketch (right). The figure visually demonstrates the model's ability to generate clean and aesthetically pleasing sketches from noisy input.", "section": "5.1 Results and Analysis"}, {"figure_path": "ttLcbEkaj6/figures/figures_7_2.jpg", "caption": "Figure 7: A comparison of visualization of ControlNet hidden states throughout denoising process from baseline approach without augmentation (top) and with augmentations (bottom).", "description": "This figure compares the visualization of ControlNet's hidden states during the denoising process. The top row shows the baseline approach without augmentations, while the bottom row shows the results with augmentations. It illustrates how the augmentations help the model converge towards a clean sketch representation more effectively.", "section": "5 Experiments"}, {"figure_path": "ttLcbEkaj6/figures/figures_8_1.jpg", "caption": "Figure 10: Examples of incorrect generation on unseen categories due to the absence of text prompt (w/o Prompt), comparing to correct generations when prompt is present (w/ Prompt).", "description": "This figure shows the impact of using text prompts in sketch generation.  The leftmost column displays the noisy hand-tracking input. The middle columns show sketch generations: one using a text prompt (w/ Prompt) and one without (w/o Prompt). The rightmost column shows the ground truth sketch. The results demonstrate that providing text prompts is crucial for the model to generate correct sketches for unseen categories, where the noisy hand-tracking input provides limited visual information. Without prompts, the model may produce plausible, but incorrect, sketches of seen categories.", "section": "5.2 Ablations"}, {"figure_path": "ttLcbEkaj6/figures/figures_8_2.jpg", "caption": "Figure 9: Qualitative results when different combinations of augmentations are applied during training. Each column represents the generated sketches when the model is trained with one combination of augmentations, e.g. Column 2 are the generated sketches when only structural augmentations are applied during training.", "description": "This figure shows the effect of different combinations of augmentations (local, structural, and false strokes) on the quality of generated sketches.  Each row represents a different input hand-tracking image, and each column shows the sketch generated with a different combination of augmentations applied during training. The ground truth sketch is shown in the last column for comparison. The results demonstrate the importance of different augmentation types in recovering the quality of the generated sketches.", "section": "5.2 Ablations"}, {"figure_path": "ttLcbEkaj6/figures/figures_9_1.jpg", "caption": "Figure 1: Given a hand drawing video, we first use a hand detection algorithm to extract the tracking image (Hand Tracking). We then take this noisy tracking image and generate a clean sketch (Generated), which faithfully and aesthetically resembles the intended sketch (Ground-truth).", "description": "This figure shows the process of generating clean sketches from hand-drawn videos.  It starts with a hand-drawn video, from which hand tracking is performed to extract a noisy tracking image. This image is then fed into an image generation model to produce a refined and aesthetically pleasing sketch that closely resembles the original intended sketch.", "section": "Abstract"}, {"figure_path": "ttLcbEkaj6/figures/figures_16_1.jpg", "caption": "Figure 12: A comparison of hand landmarking done by MediaPipe, OpenPose, and NSRM respectively. MediaPipe in general provides most accurate hand landmarks, while OpenPose often struggling to detect the hand, and NSRM not able to provide accurate landmarks.", "description": "This figure compares the performance of three different hand landmarking algorithms: MediaPipe, OpenPose, and NSRM.  It shows that MediaPipe generally provides the most accurate hand landmark detection, while OpenPose frequently struggles to detect the hand entirely, and NSRM's accuracy is significantly lower. The images show several frames from hand tracking videos, highlighting the differences in accuracy of the three algorithms.", "section": "A.3 Comparison on Egocentric Hand Tracking Algorithm"}, {"figure_path": "ttLcbEkaj6/figures/figures_16_2.jpg", "caption": "Figure 1: Given a hand drawing video, we first use a hand detection algorithm to extract the tracking image (Hand Tracking). We then take this noisy tracking image and generate a clean sketch (Generated), which faithfully and aesthetically resembles the intended sketch (Ground-truth).", "description": "This figure shows the process of generating sketches from hand-drawn videos. The first step involves using a hand detection algorithm to extract the tracking image from the video. This tracking image is then fed into a model that generates a clean sketch. The generated sketch is compared to the ground truth sketch to evaluate the model's performance. The figure showcases the effectiveness of the model by illustrating how it transforms the noisy tracking image into a clean and aesthetically pleasing sketch that closely resembles the intended sketch.", "section": "Abstract"}, {"figure_path": "ttLcbEkaj6/figures/figures_16_3.jpg", "caption": "Figure 1: Given a hand drawing video, we first use a hand detection algorithm to extract the tracking image (Hand Tracking). We then take this noisy tracking image and generate a clean sketch (Generated), which faithfully and aesthetically resembles the intended sketch (Ground-truth).", "description": "This figure shows the process of generating sketches from hand-drawn videos.  The left side displays a hand-drawn video's hand tracking image which is quite noisy.  The right side shows the generated, cleaned-up sketch that closely resembles the original, intended sketch.", "section": "Abstract"}, {"figure_path": "ttLcbEkaj6/figures/figures_17_1.jpg", "caption": "Figure 15: More inference results on ControlNet with our augmentation-based training procedure. We additionally show inference results by using T2IAdapter in our method.", "description": "This figure compares the sketch generation results of ControlNet and T2IAdapter, both trained with the proposed augmentation-based method.  The left and right columns show results from two different subsets of test images. Each row presents a hand tracking image as input (noisy and distorted), followed by the output of ControlNet, T2IAdapter, and the ground-truth sketch.  The comparison highlights the differences in the ability of the two models to reconstruct clean and faithful sketches from noisy hand tracking data. ControlNet generally produces sketches that more closely resemble the ground truth, indicating that it may be better at interpreting visual cues from noisy input data.", "section": "5 Experiments"}]