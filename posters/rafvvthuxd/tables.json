[{"figure_path": "rafVvthuxD/tables/tables_6_1.jpg", "caption": "Table 1: EMD-8 on ImageNet 64\u00d764, 100k steps of training", "description": "This table presents the results of the EM Distillation (EMD) method with 8 Langevin steps on ImageNet 64x64 after 100,000 training steps.  It compares the FID and IS scores achieved using different sampling strategies.  (x, z) indicates joint sampling of both x and z, while (\u03b5, z) represents reparametrized sampling using \u03b5 and z, and (x, )/(\u03b5, ) uses only one of them. The lower the FID, the better, and the higher the IS, the better, indicating superior image quality and diversity.", "section": "5.1 ImageNet"}, {"figure_path": "rafVvthuxD/tables/tables_7_1.jpg", "caption": "Table 2: Class-conditional generation on ImageNet 64\u00d764.", "description": "This table presents the results of class-conditional image generation on the ImageNet 64x64 dataset.  It compares various methods, both those involving multiple sampling steps and those using a single step. The metrics used for comparison are Number of Function Evaluations (NFE), Frechet Inception Distance (FID), and Recall. Lower NFE indicates higher efficiency, lower FID denotes higher image quality, and higher Recall represents better mode coverage.  The \"Teacher\" row shows the performance of the original diffusion model which is being distilled.", "section": "5.1 ImageNet"}, {"figure_path": "rafVvthuxD/tables/tables_7_2.jpg", "caption": "Table 3: Class-conditional generation on ImageNet 128\u00d7128.", "description": "This table presents the results of class-conditional image generation on the ImageNet 128x128 dataset. It compares different methods, including multi-step and single-step approaches, in terms of their performance measured by FID (Frechet Inception Distance), IS (Inception Score), and NFE (number of forward evaluations). The table shows that the proposed EMD-16 method achieves competitive FID and IS scores with a significantly reduced number of forward evaluations compared to other methods.", "section": "5.1 ImageNet"}, {"figure_path": "rafVvthuxD/tables/tables_7_3.jpg", "caption": "Table 4: FID-30k for text-to-image generation in MSCOCO. Results are evaluated by Yin et al. [23].", "description": "This table presents the FID scores for text-to-image generation on the MSCOCO dataset.  The FID (Fr\u00e9chet Inception Distance) is a metric used to evaluate the quality of generated images by comparing them to real images. Lower FID scores indicate better image quality. The table compares different methods, categorized into families (Unaccelerated, GANs, and Accelerated), showing their latency and FID scores. The results are evaluated using Yin et al. [23]'s method. The \"Teacher\" row shows the performance of the Stable Diffusion v1.5 model, which is used as the base for comparison.", "section": "5.2 Text-to-image generation"}, {"figure_path": "rafVvthuxD/tables/tables_7_4.jpg", "caption": "Table 5: CLIP Score in high CFG regime.", "description": "This table presents the CLIP scores achieved by various single-step and multi-step diffusion models on the text-to-image generation task.  It specifically focuses on the performance in a high CFG (classifier-free guidance) regime, comparing the proposed EMD-8 method against other state-of-the-art accelerated diffusion sampling techniques, such as LCM-LORA and DMD. The table highlights the trade-off between inference speed (latency) and the quality of generated images (CLIP score), demonstrating EMD-8's competitive performance in terms of both metrics.", "section": "5.2 Text-to-image generation"}, {"figure_path": "rafVvthuxD/tables/tables_9_1.jpg", "caption": "Table 6: Training steps per second in ablations for computation overhead in ImageNet 64\u00d764", "description": "This table shows the training time in seconds per step for different model variations.  It compares the baseline Diff-Instruct method with various versions of the proposed EMD method (EMD-1, EMD-2, EMD-4, EMD-8, EMD-16). The variations involve different numbers of MCMC steps in the E-step and whether only student score matching or both student score matching and generator updates are used. The purpose is to analyze the computational cost introduced by the more complex sampling scheme in EMD compared to a simpler baseline.", "section": "5.3 Computation overhead in training"}, {"figure_path": "rafVvthuxD/tables/tables_15_1.jpg", "caption": "Table 7: Hyperparameters for EMD on ImageNet 64\u00d764.", "description": "This table shows the hyperparameters used for the EM Distillation (EMD) method on the ImageNet 64x64 dataset.  It includes learning rates for the generator and score networks (lrg and lrs), batch size, Adam optimizer parameters (Adam b1 and b2), Langevin dynamics step sizes for epsilon and z (\u03b3e and \u03b3z), the number of MCMC steps (K), the target noise level (\u03bb*), and the weighting function for the noise levels (w(t)). These parameters are crucial for controlling the training process and achieving optimal performance.", "section": "C Implementation details"}, {"figure_path": "rafVvthuxD/tables/tables_16_1.jpg", "caption": "Table 8: Hyperparameters for EMD on ImageNet 128x128.", "description": "This table lists the hyperparameters used for training the EM distillation model on the ImageNet 128x128 dataset.  It includes learning rates for the generator and score networks (<i>lr<sub>g</sub></i> and <i>lr<sub>s</sub></i>), batch size, Adam optimizer parameters (<i>b<sub>1</sub></i> and <i>b<sub>2</sub></i>), step sizes for Langevin dynamics updates on epsilon and z (\u03b3<sub>\u03b5</sub> and \u03b3<sub>z</sub>), the number of Langevin steps (K), the target noise level (\u03bb*), and the weighting function w(t).", "section": "C.2 ImageNet 128x128"}, {"figure_path": "rafVvthuxD/tables/tables_16_2.jpg", "caption": "Table 9: Hyperparameters for EMD on Text-to-image generation.", "description": "This table lists the hyperparameters used for the EM Distillation (EMD) model when applied to text-to-image generation.  It includes learning rates for the generator and score networks (lrg, lrs), batch size, Adam optimizer parameters (b1, b2), step sizes for Langevin dynamics (\u03b3e, \u03b3z), the number of Langevin steps (K), the specific noise level used (t*), and the weighting function for the loss (w\u0303(t)). These settings were crucial for achieving optimal performance in the text-to-image generation task.", "section": "C.3 Text-to-image generation"}]