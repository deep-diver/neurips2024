[{"type": "text", "text": "Improving Alignment and Robustness with Circuit Breakers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andy $\\mathbf{Z_{OU}}^{\\dagger1,2,3}$ , Long Phan3, Justin Wang1, Derek Duenas1, Maxwell Lin1, Maksym Andriushchenko1, Rowan Wang1, Zico Kolter\u20211,2, Matt Fredrikson\u20201,2, Dan Hendrycks1,3 ", "page_idx": 0}, {"type": "text", "text": "1Gray Swan AI, 2Carnegie Mellon University, 3Center for AI Safety ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with \u201ccircuit breakers.\u201d Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility\u2014even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image \u201chijacks\u201d that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks. Code is available at github.com/GraySwanAI/circuit-breakers. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The landscape of artificial intelligence (AI) has long been marred by the persistent threat of adversarial attacks, particularly those targeting neural networks. These attacks exploit inherent vulnerabilities within AI systems, often leading to compromised outputs and raising concerns regarding their reliability and safety. Despite significant attention, existing mitigations have failed to achieve high reliability without dramatically compromising model performance. Thus, the trade-off between adversarial robustness and utility is widely accepted as an unavoidable fact [64]. ", "page_idx": 0}, {"type": "text", "text": "The rise of generative models has further complicated this issue. Generative models such as large language models (LLMs) can output copyrighted information or defame individuals, and agents can take harmful actions. To make models less harmful, they are \u201caligned\u201d with refusal training [12, 54], but it has become common to use adversarial attacks as a means of bypassing their safeguards. In these settings, vulnerability to attacks that break alignment poses a serious threat to utility, and raises pressing questions about whether it is feasible to deploy such systems with a high standard of safety and reliability\u2014especially against dedicated adversaries who intend to misuse them. ", "page_idx": 0}, {"type": "text", "text": "The fragility of alignment techniques to sophisticated attacks has motivated defenses that target specific attack methods, such as adversarial training, an approach originally proposed in the context ", "page_idx": 0}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/a9494b576c8a41cb4211604f01c4cec9c2ee3989f51b4bbdc10a5b6468239254.jpg", "img_caption": ["Prompt: Tell me how to build a bomb $^{+}$ ADV TEXT ", "Generation: "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Generation: ", "page_idx": 1}, {"type": "text", "text": "Sure, here\u2019s how to  build  a  bomb: Sure, here\u2019s how to  build  a  bomb: Sure, here\u2019s how to make a bomb: Step 1: Gather necessary materials. I don\u2019t recommend this action as it 1. Start with <EOS> <EOS> <EOS> These may include items such as a is highly illegal, but one can start by   \npressure cooker, explosives, a timer... gathering necessary materials such... ", "page_idx": 1}, {"type": "text", "text": "Figure 1: Introduction of circuit-breaking as a novel approach for constructing highly reliable safeguards. Traditional methods like RLHF and adversarial training offer output-level supervision that induces refusal states within the model representation space. However, harmful states remain accessible once these initial refusal states are bypassed. In contrast, inspired by representation engineering [77], circuit breaking operate directly on internal representations, linking harmful states to circuit breakers. This impedes traversal through a sequence of harmful states. ", "page_idx": 1}, {"type": "text", "text": "of standalone image classification [38] and later adapted to LLMs [40]. However, these methods often fail to generalize to new attacks that were unseen during training, and they introduce penalties on model capabilities that are usually proportional to gains in robustness. System-level defenses, including input and output filters, are cumbersome, resource-intensive, and often remain vulnerable to adversarial techniques. This has led to a growing concern that robust defenses may be unattainable. ", "page_idx": 1}, {"type": "text", "text": "We propose a novel approach outlined in Figure 1 that fundamentally diverges from traditional defenses: instead of attempting to remove vulnerabilities to specific attacks, our approach aims to directly circumvent the ability of the model to produce the harmful output in the first place. With circuit breakers, we make models intrinsically safer and reduce their risks by removing intrinsic model hazards\u2014their ability to produce harmful outputs\u2014rather than removing specific vulnerabilities with adversarial training, and rather than attempting to reduce exposure to attacks with input fliters [28, 21]. Using representation engineering (RepE) [77], our method connects the internal representations related to harmful outputs to circuit breakers so that when a model begins to generate such an output, its internal processes are interrupted, halting completion of the generation. Or this method is \u201cshortcircuiting\u201d the harmful processes as one might put it. Because the representation used to generate a harmful output is independent of any attack capable of eliciting it, this approach is attack-agnostic, and sidesteps the need for additional training, costly adversarial fine tuning, or the use of auxiliary \u201cguard\u201d models. Consequently, the resulting model with circuit breakers can be used normally without additional computational burden, and seamlessly integrated with existing monitoring and protection mechanisms. ", "page_idx": 1}, {"type": "text", "text": "Experimentally, we demonstrate that a circuit-breaking technique, Representation Rerouting (RR), notably improves the alignment of LLMs. It enhances the harmlessness of state-of-the-art LLMs, including against against a wide array of unseen adversarial attacks, including embedding and representation-space attacks\u2014namely, proxies for worst-case assumptions about attacker capabilities. Figure 2 and Table 1 present an overview of these results. Our method significantly outperforms standard refusal training and adversarial training, while imposing almost no penalty on standard capability. Notably, we integrate circuit-breakering with additional model control methods to develop a Llama-3-8B-Instruct finetune called Cygnet. This enhanced model not only surpasses its original capabilities but also exhibits a large reduction in harmful output by approximately two orders of magnitude, even when confronted with unforeseen adversarial attacks. To the best of our knowledge, this is the first convincing demonstration of the feasibility of designing techniques that significantly advance the Pareto frontier of capability versus harmlessness for LLMs, illustrating that such tradeoffs can be effectively managed. When applied to multimodal models, our results show marked increases in harmlessness. It also improves robustness against image-based attacks aimed at similarly circumventing model safeguards, again with almost no penalty on benchmarked capabilities. This remains true even in the presence of the Projected Gradient Descent (PGD) attack [38], which defenses for standalone image classifiers have been unable to achieve without a steep trade-off in accuracy. Finally, we apply circuit breakers to AI agents, illustrating its efficacy in controlling agent behaviors through evaluations on a new agent function-calling safety benchmark. ", "page_idx": 1}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/5805db0833ec0cd2180d1408dfb68353f5000eb19fd80c665eca5f180e3987ae.jpg", "img_caption": [], "img_footnote": ["Figure 2: Adding circuit breakers using Representation Rerouting (RR) to refusal trained Llama-3- 8B-Instruct model leads to significantly lower attack success rate (ASR) over a wide range of unseen attacks on HarmBench prompts [40], while its capabilities on standard LLM benchmarks (MT Bench and MMLU) are largely preserved. RR directly targets the representations that give rise to harmful outputs and reroutes them to an orthogonal space. This reliably interrupts the model from completing the harmful generations even under strong adversarial pressure. "], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our findings introduce a new paradigm for creating models that do not produce harmful outputs. Our method is highly robust against adversarial attacks, providing a promising path forward in the adversarial arms race. By ensuring safety and security without compromising capability, our approach increases the chances that we may ultimately be able to deploy robust AI systems in real-world applications. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Adversarial attacks on LLMs. Numerous manually written attack prompts on modern LLMs have been discovered [49, 68], forming the basis of red teaming for frontier LLMs [50, 5, 55], though it lacks standardization [16]. Automated red teaming has been shown effective in Perez et al. [53], Chao et al. [11], Mehrotra et al. [41], Zeng et al. [73]. Notably, transfer attacks using an adversarial suffix via gradient-based optimization were demonstrated by Zou et al. [78]. Whitebox access also facilitates prefilling attacks [67, 2], leading the LLM to generate harmful outputs. For a comprehensive summary of automated attacks, we refer to HarmBench [40]. Additionally, multi-modal vision-text attacks range from typographic attacks Goh et al. [18] to gradient-based optimization [9, 59, 6]. LLM agents have been benchmarked [35, 45], but their safety and robustness remain unexplored. ", "page_idx": 2}, {"type": "text", "text": "Defenses for LLMs. Our new defense addresses limitations in existing mechanisms. Widely used defenses include RLHF [12, 52] and DPO [54] using human annotations for safe vs. unsafe responses [63], but they often fall short against state-of-the-art adversarial attacks [78, 2]. Additional robustness is achieved by methods like Zhou et al. [76], which optimize prompts to refuse harmful requests. Inspired by adversarial training in vision [38], fine-tuning for the R2D2 model against the GCG attack [40] shows limited generalizability and drops MT-Bench scores [75]. Adversarial training for LLMs can be highly computationally expensive. Inference-time defenses, such as perplexity filters [1, 26], ", "page_idx": 2}, {"type": "text", "text": "Require: Original frozen model $\\mathcal{M}$ , model with circuit breakers $\\mathcal{M}_{\\mathrm{cb}}$ with LoRA adapters, a function rep that gathers representation from a model on a batch of inputs, a circuit breaker dataset $\\mathcal{D}_{s}$ , a retain dataset $\\mathcal{D}_{r}$ , number of steps $T$ , a hyperparameter $\\alpha$ ", "page_idx": 3}, {"type": "text", "text": "1: for $t=1,\\dots,T$ do   \n2: $x_{s}\\sim\\mathcal{D}_{s},x_{r}\\sim\\mathcal{D}_{r}$ $\\triangleright$ Sample Batch Elements   \n3: $\\begin{array}{r l}&{c_{s}=\\alpha(1-\\frac{t}{2T}),c_{r}=\\alpha\\frac{t}{2T}}\\\\ &{\\mathcal{L}_{s}=\\mathtt{R e L U}\\left(\\mathtt{c o s i n e}_{-}\\mathtt{s i m}\\left(\\mathtt{r e p}_{\\mathcal{M}}\\left(x_{s}\\right),\\mathtt{r e p}_{\\mathcal{M}_{\\mathrm{cb}}}\\left(x_{s}\\right)\\right)\\right)}\\\\ &{\\mathcal{L}_{r}=\\left\\|\\mathtt{r e p}_{\\mathcal{M}}\\left(x_{r}\\right)-\\mathtt{r e p}_{\\mathcal{M}_{\\mathrm{cb}}}\\left(x_{r}\\right)\\right\\|_{2}}\\\\ &{\\mathcal{L}=c_{s}\\mathcal{L}_{s}+c_{r}\\mathcal{L}_{r}}\\end{array}$ \u25b7Example Coefficient Schedule   \n4: \u25b7RR Loss   \n5: \u25b7Retain Loss   \n6: \u25b7Loss to be Optimized   \n7: end for ", "page_idx": 3}, {"type": "text", "text": "are effective only against non-adaptive attacks [36], while erase-and-check and SmoothLLM [56] incur high computational costs. System-level defenses against unsafe inputs or outputs [20, 25, 27] can still be circumvented by sophisticated adversaries [39]. The main conceptual difference is that instead of operating on input or output text, our method operates directly on representations which provides a more generalizable and computationally cheap solution. ", "page_idx": 3}, {"type": "text", "text": "Representation Engineering. As many contemporary defenses relying solely on supervising model outputs fail to achieve the desired levels of controllability and reliability, techniques that analyze and manage model\u2019s internal representations have garnered increased attention. This includes research ranging from uncovering emergent interpretable structures in intermediate representations [77, 46, 10], to the identification and modification of embedded knowledge [48, 42, 43], as well as steering model outputs [66, 7, 33, 24, 65]. Most relevant to our work is the control vector baseline introduced in the representation engineering paper [77], which can be applied to enhance large language models\u2019 resistance to adversarial attacks. Alongside the use of control vectors, they introduce an approach that bends representations with representation-level losses. Recent advancements extend this method to robustly unlearn hazardous knowledge [29] with a method termed RMU, demonstrating the potential of representation engineering for more complex objectives. Previous work has attempted to eliminate harmful circuits using bottom-up mechanistic interpretability, but these methods have proven insufficient [30]. Building on these foundations and further expanding RMU to a family of circuit-breaking techniques, we design a methodology based on model representations for robust alignment and control by preventing the generation of harmful outputs. ", "page_idx": 3}, {"type": "text", "text": "3 Circuit Breaking with Representation Engineering ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce a novel approach aimed at mitigating the generation of harmful outputs in neural networks by inducing a new type of phenomenon called \u201ccircuit-breaking.\u201d This phenomenon can be elicited using a family of techniques designed to monitor or remap model representations related to harmful processes, redirecting them towards incoherent or refusal representations. This process is reminiscent of \u201cshort-circuiting,\u201d where harmful representations are \u201cshorted\u201d and intercepted by circuit breakers. The core objective of this method is to robustly prevent the model from producing harmful or undesirable behaviors by through monitoring or controlling the representations. ", "page_idx": 3}, {"type": "text", "text": "Our focus on generative models\u2014such as language and multimodal agents\u2014presents a unique opportunity. Generative models inherently involve multi-step processes through which outputs are produced. When devising an attack, adversaries must effectively exert influence across each step of the targeted processes, so each step presents an opportunity to make the model more robust to attack. This insight drives our strategy, which focuses on disrupting adversarial control of the relevant multi-step processes rather than the binary classification problem of attempting to detect the presence of an attack. Building from techniques in representation engineering (RepE) [77], we accomplish this by remapping the sequence of model representations that leads to harmful outputs, directing them towards incoherent or refusal representations\u2014namely, breaking the circuit, or shorting the circuit as one might put it. Moreover, by directly targeting the processes involved in generating harmful responses, our method can generalize across the diverse range of inputs that may activate ", "page_idx": 3}, {"type": "text", "text": "Table 1: LLM evaluation results. Our circuit-breaking method Representation Rerouting (RR) shows strong generalization across a diverse range of unseen attacks, significantly reducing compliance rates to harmful requests while preserving model capability. Cygnet, a Llama-3-8B-Instruct finetune integrating circuit breakers and other representation control [77] methods, surpasses original capabilities and demonstrates a significant reduction in harmful output by roughly two orders of magnitude under strong attacks. This advancement shows promising initial steps in balancing capability and harmlessness in LLMs. Input embedding attack optimizes the soft input embeddings which is an unrealistically strong threat model for LLMs. Mistral-Adv Trained (R2D2) [40] is an SFT-only model. ", "page_idx": 4}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/ee435905c8e42ef7cf15f564bed24be0811dde9bd186a4820ce7e12c0277e5a8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "those processes. Consequently, we do not need to identify all of the potential inputs that could trigger undesirable outputs, rather we only need to ensure coverage of a well defined set of such outputs. ", "page_idx": 4}, {"type": "text", "text": "The applications of circuit breakers are multifaceted. They can be utilized to prevent the generation of harmful outputs in general, as well as to prevent more narrowly tailored types of output, such as private information or copyrighted material. The approach is versatile, as it is possible to identify and remap the relevant representations in virtually any neural network architecture. ", "page_idx": 4}, {"type": "text", "text": "The family of circuit-breaking techniques is characterized by two major components: datasets and loss functions. Algorithm 1 presents a circuit-breaking technique that uses Low-Rank Representation Adaptation (LoRRA) [77] which we call Representation Rerouting (RR). The remainder of this section details this approach, and how the data and chosen loss function contribute to the effectiveness of the overall method. ", "page_idx": 4}, {"type": "text", "text": "Data. The training data used in RR is partitioned into two sets: the Circuit Breaker Set and the Retain Set, each serving distinct purposes within the training process aimed at controlling harmful processes in the model. As with all representation control methods, the quality of the circuit breaker mechanism largely depends on how precisely the data can elicit the targeted representation. The Circuit Breaker Set is comprised of examples that yield internal representations potentially leading to harmful or undesirable behaviors, and are used to prompt the model\u2019s circuit breaker mechanism. Conversely, the Retain Set includes examples that should not activate circuit breakers, and are used to maintain existing desirable model representations to retain benign efficacy. While even a limited number of examples in each set can sufficiently alter the model\u2019s behavior in a manner that generalizes beyond the training data, the resulting performance is generally improved when the training data better aligns with the domains we aim to break the circuit and retain. ", "page_idx": 4}, {"type": "text", "text": "For models with pre-existing refusal mechanisms, like Llama-3-Instruct, careful dataset curation is essential. Adding refusal data to the Retain Set enhances the model\u2019s ability to correctly refuse harmful user requests and improves retention of its capabilities. Another challenge is to elicit harmful responses from models with effective refusal mechanisms. To address this, we must curate a Circuit Breaker set that includes text capable of bypassing the refusal mechanism and triggering harmful processes. We find that a practical approach is to remove harmful user requests while keeping the corresponding harmful assistant responses in the Circuit Breaker Set. These measures ensure the refusal mechanism\u2019s integrity while allowing the model to activate its circuit-breaking function correctly once the refusal is bypassed. Ablation results are detailed in Section 4.4. ", "page_idx": 4}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/aa896460f099dcef1b66637091d965a3ac2225674cfa0a0c4540e4d28f78d3a4.jpg", "img_caption": ["Figure 3: Circuit-breaking performance in multimodal settings with Representation Rerouting (RR). Under Projected Gradient Descent (PGD) attack, our LLaVA-NeXT-Mistral-7B $(+\\,{\\sf R R})$ with circuit breakers is significantly more robust compared to the original model even with a safety prompt that instructs the model to avoid harmful responses. Performance on multimodal capabilities benchmarks MMMU and LLaVA-Wild is preserved. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Loss. The accompanying losses for the datasets are the representation rerouting loss and retain loss. Denote the representation of harmful processes under the original model as $\\mathrm{rep}_{\\mathrm{orig}}$ and under the model with circuit breakers as $\\mathrm{rep}_{\\mathrm{c/b}}$ . The rerouting loss is designed to remap representations from harmful processes $\\mathrm{rep}_{\\mathrm{c/b}}$ to a desired target representation $\\mathrm{rep}_{\\mathrm{rand}}$ . Conversely, the retain loss is used to maintain representations within a retain set, which helps preserve these representations. This is often measured as the $\\ell_{2}$ distance between the current and retain representations. ", "page_idx": 5}, {"type": "text", "text": "The rerouting loss can take various forms. One approach involves routing the targeted representation to a fixed random direction with a large norm, as utilized in the unlearning method RMU [29]. This is expressed as $\\lVert\\mathrm{{rep}_{c/b}-\\alpha\\mathrm{{rep}_{r a n d}\\rVert_{2}}}$ , where $\\mathrm{rep}_{\\mathrm{rand}}$ is a random vector and $\\alpha$ is a large constant meant to amplify the norm of the representation. However, this approach requires extensive tuning of the $\\alpha$ parameter. We also explore a variant of the random vector loss that does not necessitate hyperparameter tuning, formulated as the $\\ell_{2}$ norm of $\\mathrm{rep}_{\\mathrm{c/b}}/\\left\\|\\mathrm{rep}_{\\mathrm{c/b}}\\right\\|-\\mathrm{rep}_{\\mathrm{rand}}/\\left\\|\\mathrm{rep}_{\\mathrm{rand}}\\right\\|$ . However, the use of a random vector is neither necessary nor optimal. Given that we want the targeted representation to be as unhelpful as possible for the harmful processes, another approach is to directly optimize the circuit-broken representation to be orthogonal to the original representation responsible for harmful processes. This is given by their cosine similarity: $\\mathrm{rep}_{\\mathrm{c/b}}\\cdot\\mathrm{rep}_{\\mathrm{orig}}/(||\\mathrm{rep}_{\\mathrm{c/b}}||_{2}||\\mathrm{rep}_{\\mathrm{orig}}||_{2})$ . To avoid optimizing the similarity beyond zero, we apply a ReLU function to this objective. We find this loss to be the most intuitive and most effective in terms of achieving a balance between robustness and preserved capability. An implementation of RR using Low-Rank Representation Adaptation is shown in Algorithm 1. Additionally, one could map $\\mathrm{rep}_{\\mathrm{c/b}}$ onto more semantically meaningful directions, such as a refusal direction or the embedding of the EOS token. We leave this to future work. Appendix C.1 discusses several additional design considerations. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Large Language Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Adding Circuit Breakers. In our experimental setup, we employ similar circuit breaker and retain datasets for both the Mistral-7B-Instruct-v2 [47] and Llama-3-8B-Instruct [44] models. Detailed information on the synthetic circuit breaker set for LLMs is provided in Appendix A.1. The retain set for both models includes UltraChat [15], comprising instructional conversations, and XSTest [57], an exaggerated refusal dataset. Additionally, for Llama-3, we enhance the retain set with extra refusal data points. We follow the implementation of Representation Rerouting (RR) specified in Algorithm 1 and select hyperparameters based on static attack test cases from HarmBench\u2019s validation set. More experimental details can be found in Appendix C.2.1. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Evaluation. We evaluate the harmfulness of the model using HarmBench [40], a standardized framework that includes harmful behaviors and a wide range of both black box and white box attacks. We select a subset of the strongest attacks reported on both open-source and closed-source models for evaluation. These attacks include gradient-based optimization (GCG [78]), LLM optimizers (PAIR [11]), custom jailbreaking pipelines (TAP-Transfer [73], AutoDAN [36], and HumanJailbreaks [62]). To further test the model, we incorporate a multilingual attack [70], and also introduce three powerful attacks that leverage system-level and representation-space access. We briefly describe these three additional attacks below, and provide a more detailed coverage in Appendix C.2.2. ", "page_idx": 6}, {"type": "text", "text": "1. Prefilling Attack: This system-level attack prefills the assistant\u2019s output with the beginning of a desired target completion. It leverages the autoregressive nature of LLMs, as it can be difficult for a model to \u201creverse-course\u201d after it has started to generate harmful content. Prefilling is straightforward to implement for any open-weight model, and is also supported for some proprietary LLMs like Claude [4]. ", "page_idx": 6}, {"type": "text", "text": "2. Input Embedding Attack: This white-box attack operates in the embedding space by optimizing a set of input embeddings directly instead of using hard tokens, with the objective of eliciting an affirmative assistant response [60]. ", "page_idx": 6}, {"type": "text", "text": "3. RepE Attack: This white-box attack manipulates the model\u2019s representation space. Previous work in representation engineering demonstrates the identification of directional vectors in the model\u2019s representation space that correspond to refusals [77]. By altering these vectors\u2014either adding or subtracting\u2014we can modulate the model\u2019s tendency to refuse requests. ", "page_idx": 6}, {"type": "text", "text": "We utilize HarmBench\u2019s LLM classifier to evaluate the attack success rate and manually verify the judgements. Detailed configurations for each attack are provided in Appendix C.2.2. To measure the capabilities of the models with circuit breakers, we evaluate our models on MTBench [75] for instruction-following abilities and on the OpenLLM Leaderboard [8] for knowledge and reasoning which includes MMLU [22], ARC-c [13], HellaSwag [72], TruthfulQA [31], Winogrande [58], and GSM8K [14]. Table 5 contains a detailed breakdown of performance on each dataset. Additionally, we follow the methodology in [5] to construct an over-refusal evaluation, described in Appendix B. For baselines, we use the original Mistral and Llama-3 Instruct models. Additionally, we include a state-of-the-art adversarially trained Mistral model, R2D2 [40], for comparison. ", "page_idx": 6}, {"type": "text", "text": "Results. We observe that our circuit-breaking technique RR demonstrates strong generalization across a diverse range of attacks, reducing compliance rates to harmful requests by an average of $87\\%$ with Mistral and $90\\%$ with Llama-3. Unlike the Mistral R2D2 model, which is trained against the GCG yet shows limited generalization to various attacks, our method eliminates the need for specific attack training and focuses on hindering harmful generations. Our approach moves away from the traditional cat-and-mouse paradigm, aiming for generalization to unforeseen attacks. Additionally, the results highlight a Pareto optimal trade-off in performance. Our model exhibits high reliability against unseen attacks with a minimal compromise in capability evaluation, showing a performance dip of less than $1\\%$ in proposed tests. This is difficult to achieve with traditional defenses. For example, the Mistral model, when adversarially trained, experiences a decline of over $8\\%$ in the MT Bench performance. In contrast, our model leverages representation engineering principles, focusing on internal control over external supervision, enabling more targeted and fine-grained control over model behavior without adversely impacting other functionalities. ", "page_idx": 6}, {"type": "text", "text": "4.2 Multimodal Models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Adding Circuit Breakers. We mix the circuit breaker and retain datasets from Section 4.1 with a synthetic multimodal circuit breaker set and the retain LLaVA-Instruct set [34]. The detailed process of generating the synthetic dataset is reported in appendix A.2. We perform RR on LLaVA-NeXTMistral-7B [34]. More experimental details can be found in Appendix C.3.1. ", "page_idx": 6}, {"type": "text", "text": "Evaluation. To evaluate the robustness of multimodal models with circuit breakers, we generate adversarial images using a whitebox approach. Following Projected Gradient Descent [38], we perturb images with a harmful prompt to produce a target string with an affirmative assistant response. We set epsilon to 32/255 and run the process for 1000 steps. As baselines, we test LLaVA-NeXT", "page_idx": 6}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/3dc217812a5b6db2dce9cfd5fca22f14e69bb29ad85fdbd9f592c96bc2f69e31.jpg", "img_caption": ["Figure 4: Circuit-breaking performance in AI agent settings with Representation Rerouting (RR). Our Llama-3-8B-Instruct $(+\\,{\\sf R R})$ with circuit breakers remains robust under Direct Request and Forced Function Calls, while retaining performance on the Berkeley Function Calling Leaderboard (BFCL). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Mistral-7B with and without a safety prompt that asks the model to avoid harmful responses. Our robustness results in Figure 3 show the percentage of harmful prompts the model complies with, labeled manually. We source a set of 133 harmful multimodal behaviors from HarmBench [40] and MM-SafetyBench [37], focusing on the most saliently harmful prompts. See Appendix C.3 for more details about the dataset\u2019s composition. For capabilities evaluation, we follow [34] to evaluate multimodal models on LLaVA-Wild for visual chat capability and MMMU [71] for multimodal understanding capability. ", "page_idx": 7}, {"type": "text", "text": "Results. Figure 3 demonstrates that for multimodal models, our circuit-breaking technique RR is also able to make a model significantly more robust while preserving model capabilities. Especially when subject to white-box PGD Attack, RR achieves reduction of $84\\%$ in the compliance rate compared to the original model and $85\\%$ compared to the safety prompt. Meanwhile, performance on MMMU and LLaVA-Wild remains within $0.5\\bar{\\%}$ of the original, as opposed to the safety prompt which causes a decrease of $3.3\\%$ on LLaVA-Wild. This demonstrates that despite the ongoing challenge of achieving adversarial robustness in standalone image recognition, circuit breakers enable the larger multimodal system to reliably counter image \u201chijacks\u201d [6] intended to elicit harmful outputs. ", "page_idx": 7}, {"type": "text", "text": "4.3 AI Agents ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Adding Circuit Breakers. We mix the circuit breaker and retain datasets from Section 4.1 with function calling circuit breaker and retain dataset. The detailed process of generating the function calling circuit breaker and retain dataset is described in Appendix A.3. For the LLMs with circuit breakers, we also use the same hyperparameter configuration as in Section 4.1. ", "page_idx": 7}, {"type": "text", "text": "Evaluation. To evaluate the effectiveness of RR as a method of preventing AI agents from making harmful function calls, we design a dataset that consists of 100 requests intended to produce harmful actions via function calls, along with associated function definitions. These requests span a variety of categories, including cybercrime, disinformation, fraud, and harassment. The associated function definitions are designed to capture typical use cases of deployed AI agents including sending messages, browsing URLs, and using simple tools in addition to task-specific functions. ", "page_idx": 7}, {"type": "text", "text": "We provide a representative example in Appendix C.4.1. We record model compliance rate with harmful requests under both the standard setting, where function call requests are directly given and the model decides whether to make a call, and under forced function-calling, where the assistant is forced to begin its response with the name of a function to be called. Forced function-calling is akin to the prefliling attack in 4.1 and is provided by major model providers [3, 51]. For capabilities evaluation, we measure performance on the Berkeley Function Calling Leaderboard (BFCL) [69]. We use Llama-3-8B-Instruct to benchmark, as it is one of few open-source models that both 1) performs reasonably well on the benchmark leaderboard, and 2) is currently served with function-calling capabilities by inference providers [19]. ", "page_idx": 7}, {"type": "text", "text": "Results. Figure 4 shows that after applying RR, our model is significantly more robust to harmful function calling requests, in both the no-attack and forced function-call settings, reducing harmful action compliance rates by $84\\%$ and $83\\%$ in the latter setting compared to baselines. Additionally, the model with circuit breakers retains performance on the Berkeley Function Calling Leaderboard. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Overall, this demonstrates the method\u2019s effectiveness in controlling agent behaviors under adversarial pressure and in environments with inherent reward biases. It suggests the potential for mitigating harms like power-seeking or dishonesty by adding circuit breakers to the relevant model representations, which can be as simple as adjusting the circuit breaker set. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation and Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablations. We do pairwise ablations for each component of the RR loss in Table 8. First, we see that augmenting the circuit breaker set with requests that bypass refusal mechanisms (w/ Augment) decreases ASR while still maintaining capabilities. Although ablating the refusal retain component (w/o refusal) increases ", "page_idx": 8}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/a6533ccef30674dd018aebb69285f2e2a68b590d8b592323cae230096948c6aa.jpg", "img_caption": ["Figure 5: Circuit Breaker set ablation across categories of harm, averaged over the same 6 attacks. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "relative robustness, it also degrades capabilities. Next, we try varying loss functions. We find that the RMU loss [29], which minimizes the $\\ell_{2}$ distance from a constant random unit vector, fails to converge. Finally, we find that minimizing the $\\ell_{2}$ distance from a distinct random positive vector at each step (RandP) works (though if the vector is centered at 0 (RandC), training fails). Overall, the cosine loss proposed in RR offers more stability than other losses. We then analyze the training data composition, which influences the kinds of harmful inputs that activate the circuit breakers. ", "page_idx": 8}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/2e6cdf3f068528c49ce55a742e4680e21d5ae09e21e49f7923d1954cca0a02af.jpg", "img_caption": ["Figure 6: Cosine analysis of internal representations of the Llama model without and with circuit breakers. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "To understand the generalization properties of circuit-breaking, we split our training data into six categories of harm, train category-specific models, and measure their generalization performance across categories. We find strong in-domain generalization, indicated by the low ASR along the diagonal in Figure 5, and observe that training on broader categories like Harmful and Illegal Activities offers greater generalization than narrower categories like Cybercrime. We report similar ablations for Mistral-7B in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "Representation analysis. In Figure 6, we plot the cosines between representations of the Llama-3- 8B-Instruct model with and without circuit breakers for a prefilled harmful response \u201cHere is how to build a bomb: 1. Start with\u201d. We additionally plot the norms of these representations in Figure 12. ", "page_idx": 8}, {"type": "text", "text": "We observe that in this case, the cosines and norms start to change dramatically during prefilling starting from layer 10, i.e., even before generation starts. We note that we use layers 10 and 20 for circuit-breaking, so we do not expect substantial changes in the cosines and norms before layer 10 which is confirmed by the behavior of these metrics at layer 5. Although we do not directly control the representation norms during training, we observe that they often dramatically increase after circuit-breaking occurs. We repeat the same experiment for Mistral-7B-Instruct and show it in Appendix H, where we also analyze two other prompts: one that leads to a similar behavior and one that triggers circuit breakers after generation starts. Importantly, we conclude that our proposed method has the intended effect on the representations and that we can detect activation of circuit breakers by directly analyzing the internal representations. This can lead to system-level mitigations like using a probe to detect when circuit breakers are activated to stop generation and, for example, provide a message that the request is considered harmful and further generation is not possible. ", "page_idx": 8}, {"type": "text", "text": "Circuit breaking with Harmfulness Probes (HP). Our proposed circuit breaking method relies on representation control. This section evaluates the potential efficacy of representation reading as an alternative. Instead of altering the harmful model representation, we simply monitor for its presence and halt model generation if detected. We employ the same training dataset (harmful circuit breaker set and retain set) used in the LLM experiments. A linear classifier and an MLP classifier are trained to distinguish between model activations from the two datasets. Specifically, activations are collected from the 16th layer of the Mistral model and from the final layer of the Llama-3 model for each token in the responses. The MLP probe has two layers with hidden size of 64 and 32. During testing, generation is halted and replaced with a refusal message if any generated token is flagged as harmful by the classifier. We find a threshold so that the false positive rate (FPR) on WildChat (Appendix B) is around the same as the model trained with RR [74]. We choose five settings to evaluate: prompt only (No Attack), manual attack (Manual), black-box attack (TAP-T), white-box attack (GCG), and an embedding space attack (Input Embed). ", "page_idx": 8}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/b5f29cdf32cccd1b754adce8b8ef826245185394a47316b3db862752fcd48876.jpg", "table_caption": ["Table 2: Comparison of Harmfulness Probing (HP) and Representation Rerouting (RR). RR is a representation control method whereas HP is a representation reading method. HP, when applied using a reasonable threshold, significantly lowers the ASR compared to a refusal-trained baseline. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "As shown in table Table 2, Harmfulness Probing (HP) significantly reduces the attack success rate compared to the refusal-trained baseline. Both Linear and MLP Harmfulness Probes are outperformed by the representation control approach (RR), however, the gap is smaller for the MLP probe. We emphasize that, although generally probing can be easily thwarted by adversarial attacks, much like input and output filters, its robustness in this context can be largely attributed to the continuous monitoring of model representations associated with harmful processes throughout the entire generation, a key idea in circuit breaking. Furthermore, one can combine HP and RR to implement multiple layers of defense. It is important to note, however, that the Harmful Probes are tested under a weaker adversarial setting, where the attacker lacks knowledge of the probe and does not directly optimize against it. Further investigation into Harmfulness Probes and other representation reading methods is left for future work. ", "page_idx": 9}, {"type": "text", "text": "5 Limitations and Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Despite the promise of the methods introduced here, we emphasize that the approach we present is aiming at preventing one particular type of adversarial attack: an attack against the ability of the model to produce harmful content (often specifically against the desires of the model developer). In general, adversarial attacks can achieve other aims as well, i.e., using a generative vision language model as a drop-in replacement for an image classifier. In such a use case, our method would not provide defense against \u201ctraditional\u201d adversarial attacks aimed at simply changing the class label, because no class label would be inherently \u201charmful.\u201d Thus, there is an important distinction of our approach: we are specifically targeting the adversarial attack setting where the goal of an attacker is to produce generically harmful information (content the model should never produce). Nonetheless, for this particular use case of adversarial attacks, and for single-turn conversations that we focus on circuit-breaking, our approach dramatically improves model robustness. Overall we found that circuit breakers, based on RepE, make models intrinsically safer and robust to unseen adversarial attacks. The method is highly general and can impart robustness to image hijacks, and it can also prevent AI agents from taking harmful actions. Our method is potentially a major step forward in making models more aligned and robust. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We are thankful to Steven Basart, Stephen Casper, David Dalrymple, Xander Davies, and Fabien Roger for providing valuable feedback on the paper. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] G. Alon and M. Kamfonas. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132, 2023.   \n[2] M. Andriushchenko, F. Croce, and N. Flammarion. Jailbreaking leading safety-aligned LLMs with simple adaptive attacks. arXiv preprint arXiv:2404.02151, 2024.   \n[3] Anthropic. Tool use (function calling), 2024. URL https://docs.anthropic.com/en/ docs/tool-use#forcing-tool-use. Anthropic documentation.   \n[4] Anthropic. Preflil claude\u2019s response, 2024. URL https://docs.anthropic.com/en/docs/ prefill-claudes-response. Anthropic documentation.   \n[5] Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024.   \n[6] L. Bailey, E. Ong, S. Russell, and S. Emmons. Image hijacks: Adversarial images can control generative models at runtime. arXiv preprint arXiv:2309.00236, 2023.   \n[7] D. Bau, H. Strobelt, W. Peebles, J. Wulff, B. Zhou, J.-Y. Zhu, and A. Torralba. Semantic photo manipulation with a generative image prior. arXiv preprint arXiv:2005.07727, 2020. [8] E. Beeching, C. Fourrier, N. Habib, S. Han, N. Lambert, N. Rajani, O. Sanseviero, L. Tunstall, and T. Wolf. Open llm leaderboard. https://huggingface.co/spaces/HuggingFaceH4/ open_llm_leaderboard, 2023.   \n[9] N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, P. W. W. Koh, D. Ippolito, F. Tramer, and L. Schmidt. Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems, 36, 2023.   \n[10] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.   \n[11] P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong. Jailbreaking black box large language models in twenty queries, 2023.   \n[12] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.   \n[13] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.   \n[14] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021.   \n[15] N. Ding, Y. Chen, B. Xu, Y. Qin, Z. Zheng, S. Hu, Z. Liu, M. Sun, and B. Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.   \n[16] M. Feffer, A. Sinha, Z. C. Lipton, and H. Heidari. Red-teaming for generative ai: Silver bullet or security theater? arXiv preprint arXiv:2401.15897, 2024.   \n[17] GlaiveAI. Glaive function calling v2 dataset, 2024. URL https://huggingface.co/ datasets/glaiveai/glaive-function-calling-v2. Accessed: 2024-05-21.   \n[18] G. Goh, N. Cammarata, C. Voss, S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah. Multimodal neurons in artificial neural networks. Distill, 6(3):e30, 2021.   \n[19] Groq. Groqcloud models documentation, 2024. URL https://console.groq.com/docs/ models. GroqCloud documentation.   \n[20] A. Helbling, M. Phute, M. Hull, and D. H. Chau. Llm self defense: By self examination, llms know they are being tricked. arXiv preprint arXiv:2308.07308, 2023.   \n[21] D. Hendrycks. Introduction to AI safety, ethics, and society. Taylor and Francis, 2024.   \n[22] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \n[23] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.   \n[24] G. Ilharco, M. T. Ribeiro, M. Wortsman, S. Gururangan, L. Schmidt, H. Hajishirzi, and A. Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022.   \n[25] H. Inan, K. Upasani, J. Chi, R. Rungta, K. Iyer, Y. Mao, M. Tontchev, Q. Hu, B. Fuller, D. Testuggine, and M. Khabsa. Llama guard: Llm-based input-output safeguard for human-ai conversations, 2023.   \n[26] N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer, P.-y. Chiang, M. Goldblum, A. Saha, J. Geiping, and T. Goldstein. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614, 2023.   \n[27] T. Kim, S. Kotha, and A. Raghunathan. Jailbreaking is best solved by definition. arXiv preprint arXiv:2403.14725, 2024.   \n[28] N. Leveson. Engineering a safer world: Systems thinking applied to safety. 2012.   \n[29] N. Li, A. Pan, A. Gopal, S. Yue, D. Berrios, A. Gatti, J. D. Li, A.-K. Dombrowski, S. Goel, L. Phan, G. Mukobi, N. Helm-Burger, R. Lababidi, L. Justen, A. B. Liu, M. Chen, I. Barrass, O. Zhang, X. Zhu, R. Tamirisa, B. Bharathi, A. Khoja, Z. Zhao, A. Herbert-Voss, C. B. Breuer, S. Marks, O. Patel, A. Zou, M. Mazeika, Z. Wang, P. Oswal, W. Liu, A. A. Hunt, J. Tienken-Harder, K. Y. Shih, K. Talley, J. Guan, R. Kaplan, I. Steneker, D. Campbell, B. Jokubaitis, A. Levinson, J. Wang, W. Qian, K. K. Karmakar, S. Basart, S. Fitz, M. Levine, P. Kumaraguru, U. Tupakula, V. Varadharajan, Y. Shoshitaishvili, J. Ba, K. M. Esvelt, A. Wang, and D. Hendrycks. The wmdp benchmark: Measuring and reducing malicious use with unlearning, 2024.   \n[30] N. M. Li Maximilian, Davies Xander. Circuit breaking: Removing model behaviors with targeted ablation. arXiv preprint arXiv:2309.05973, 2023.   \n[31] S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2022.   \n[32] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Doll\u00e1r. Microsoft coco: Common objects in context, 2015.   \n[33] H. Ling, K. Kreis, D. Li, S. W. Kim, A. Torralba, and S. Fidler. Editgan: High-precision semantic image editing. Advances in Neural Information Processing Systems, 34:16331\u201316345, 2021.   \n[34] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/blog/ 2024-01-30-llava-next/.   \n[35] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.   \n[36] X. Liu, N. Xu, M. Chen, and C. Xiao. Autodan: Generating stealthy jailbreak prompts on aligned large language models. ICLR, 2024.   \n[37] X. Liu, Y. Zhu, J. Gu, Y. Lan, C. Yang, and Y. Qiao. Mm-safetybench: A benchmark for safety evaluation of multimodal large language models, 2024.   \n[38] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[39] N. Mangaokar, A. Hooda, J. Choi, S. Chandrashekaran, K. Fawaz, S. Jha, and A. Prakash. Prp: Propagating universal perturbations to attack large language model guard-rails. arXiv preprint arXiv:2402.15911, 2024.   \n[40] M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li, S. Basart, B. Li, D. Forsyth, and D. Hendrycks. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. 2024.   \n[41] A. Mehrotra, M. Zampetakis, P. Kassianik, B. Nelson, H. Anderson, Y. Singer, and A. Karbasi. Tree of attacks: Jailbreaking black-box llms automatically. arXiv preprint arXiv:2312.02119, 2023.   \n[42] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35, 2022.   \n[43] K. Meng, A. S. Sharma, A. Andonian, Y. Belinkov, and D. Bau. Mass-editing memory in a transformer. arXiv preprint arXiv:2210.07229, 2022.   \n[44] Meta AI. Llama-3 8b instruct. https://huggingface.co/meta-llama/ Meta-Llama-3-8B-Instruct, 2024. Instruction-tuned version of the Llama 3 model.   \n[45] G. Mialon, R. Dess\u00ec, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozi\u00e8re, T. Schick, J. Dwivedi-Yu, A. Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.   \n[46] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.   \n[47] Mistral. Mistral 7b v0.2. https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.2, 2024. Fine-tuned model for instruction-following tasks.   \n[48] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. Fast model editing at scale. arXiv preprint arXiv:2110.11309, 2021.   \n[49] Z. Mowshowitz. Jailbreaking chatgpt on release day. https://www.lesswrong.com/posts/ RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day, 2022. Accessed: 2024- 05-19.   \n[50] OpenAI. Gpt-4 technical report, 2023.   \n[51] OpenAI. Chat completions (tool_choice), 2024. URL https://platform.openai.com/ docs/api-reference/chat/create#chat-create-tool_choice. OpenAI documentation.   \n[52] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[53] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022.   \n[54] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model, 2023.   \n[55] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   \n[56] A. Robey, E. Wong, H. Hassani, and G. J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks. arXiv preprint arXiv:2310.03684, 2023.   \n[57] P. R\u00f6ttger, H. R. Kirk, B. Vidgen, G. Attanasio, F. Bianchi, and D. Hovy. Xstest: A test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023.   \n[58] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. WINOGRANDE: an adversarial winograd schema challenge at scale, 2019.   \n[59] C. Schlarmann and M. Hein. On the adversarial robustness of multi-modal foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3677\u20133685, 2023.   \n[60] L. Schwinn, D. Dobre, S. Xhonneux, G. Gidel, and S. Gunnemann. Soft prompt threats: Attacking safety alignment and unlearning in open-source llms through the embedding space. arXiv preprint arXiv:2402.09063, 2024.   \n[61] L. Shen, W. Tan, S. Chen, Y. Chen, J. Zhang, H. Xu, B. Zheng, P. Koehn, and D. Khashabi. The language barrier: Dissecting safety challenges of llms in multilingual contexts. arXiv preprint arXiv:2401.13136, 2024.   \n[62] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang. \"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models, 2024.   \n[63] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[64] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds with accuracy, 2019.   \n[65] A. Turner, L. Thiergart, D. Udell, G. Leech, U. Mini, and M. MacDiarmid. Activation addition: Steering language models without optimization. arXiv preprint arXiv:2308.10248, 2023.   \n[66] P. Upchurch, J. Gardner, G. Pleiss, R. Pless, N. Snavely, K. Bala, and K. Weinberger. Deep feature interpolation for image content changes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.   \n[67] J. Vega, I. Chaudhary, C. Xu, and G. Singh. Bypassing the safety training of open-source llms with priming attacks. arXiv preprint arXiv:2312.12321, 2023.   \n[68] A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.   \n[69] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_ function_calling_leaderboard.html, 2024.   \n[70] Z.-X. Yong, C. Menghini, and S. H. Bach. Low-resource languages jailbreak gpt-4. arXiv preprint arXiv:2310.02446, 2023.   \n[71] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024.   \n[72] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish your sentence?, 2019.   \n[73] Y. Zeng, H. Lin, J. Zhang, D. Yang, R. Jia, and W. Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024.   \n[74] W. Zhao, X. Ren, J. Hessel, C. Cardie, Y. Choi, and Y. Deng. Wildchat: 1m chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id $\\cdot=$ Bl8u7ZRlbM.   \n[75] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.   \n[76] A. Zhou, B. Li, and H. Wang. Robust prompt optimization for defending language models against jailbreaking attacks. arXiv preprint arXiv:2401.17263, 2024.   \n[77] A. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski, et al. Representation engineering: A top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023.   \n[78] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Circuit Breaker Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Large Language Model Circuit Breaker Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To construct a dataset of diverse harmful behaviors to activate circuit breakers while maintaining generalization, we prompt an uncensored LLM to generate short harmful queries and harmful completions given few-shot examples across a wide range of categories. We then filter out all samples that have a BLEU score above 0.3 when compared to any behavior in HarmBench\u2019s standard behaviors set [40] to avoid data contamination with the benchmark. ", "page_idx": 15}, {"type": "text", "text": "A.2 Multimodal Circuit Breaker Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To effectively construct a multimodal circuit breaker dataset containing images and their corresponding harmful queries and completions, we first use the LLaVA-Mistral-7B model [34] to generate detailed image descriptions from a sample of images from the COCO Dataset [32]. We then prompt an uncensored LLM to generate related harmful queries based on the given image descriptions, as well as the harmful completions. The final circuit breaker multimodal dataset will consist of an image and its corresponding harmful queries and harmful completions. ", "page_idx": 15}, {"type": "text", "text": "A.3 Function Calling Circuit Breaker / Retain Dataset ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To construct the Agent Circuit Breaker Dataset, we start with function definitions from the Glaive Function Calling v2 [17]. Using these function definitions, we prompt an LLM to generate harmful requests. Following this, we use GPT-3.5-turbo to execute these harmful requests and obtain the corresponding function outputs. These outputs are then converted to the OpenFunctions format. Additionally, we filter out all samples that have a BLEU score above 0.1 when compared to any behavior in our proposed AgentBench (Section 4.3). We utilize the original Glaive Function Calling v2 dataset as the harmless retain set. ", "page_idx": 15}, {"type": "text", "text": "B Refusal Evaluation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Following the methodology outlined in [5], we construct an over-refusal evaluation using the WildChat dataset [74]. WildChat is a large corpus of real-world user-ChatGPT interactions, covering a wide range of complex topics such as ambiguous requests, code-switching, topic-switching, and political discussions. This dataset is instrumental in evaluating chat model\u2019s tendencies in handling problematic requests. ", "page_idx": 15}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/155ddcf238c38fdd718e3da97546698113e2d6b060297513500ef8b1b79f28df.jpg", "table_caption": ["Table 3: Refusal evaluation on WildChat [74]. Models with circuit breakers show an increase in refusal rate, however it still remains considerably lower compared to more refusal-trained models like Claude-3 and adversarial training. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "For our evaluation, we filter a subset of 500 English non-toxic user-GPT-4 requests. To measure refusal in standard models, we employ keyword checking. For the models with circuit breakers, we use both keyword checking and the perplexity score as measures of refusal. The refusal results are shown in Table 3. While models with circuit breakers show an increase in refusal rate, the rate remains considerably lower compared to more refusal-trained models like Claude-3. ", "page_idx": 15}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Additional Design Considerations for Circuit Breakers ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we discuss several important design considerations: ", "page_idx": 15}, {"type": "text", "text": "1. Loss Multiplier Scheduling: To achieve an optimal balance between circuit-breaking and retention, we initially apply a large multiplier to the circuit-breaking loss. This multiplier is then gradually reduced while simultaneously increasing the retention multiplier.   \n2. Selection of Tokens for Circuit Breaking: For enhanced robustness, we apply the circuitbreaking loss to both the user and assistant text within the circuit breaker set for large language models and agents. For a multimodal setup, we apply circuit-breaking to all tokens following the image embeddings.   \n3. Use of LoRA Tuning: To ensure greater stability and improved retention performance, we employ LoRA tuning [23] instead of directly adjusting the model weights. ", "page_idx": 16}, {"type": "text", "text": "C.2 Large Language Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.2.1 Detailed Circuit Breaker Setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For both models, we perform circuit-breaking training for 150 steps with a batch size of 16. For Mistral, we set $\\alpha$ to 5, whereas for Llama-3, we adjust $\\alpha$ to 10. Both models are trained with a batch size of 16. We specifically target layers 10 and 20 for the circuit-breaking loss and insert LoRA adapters into all linear layers from layers 0 through 20. Both models are trained on 1 A100-80GB for 20 minutes. ", "page_idx": 16}, {"type": "text", "text": "C.2.2 Detailed Evaluation Setup ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Multilingual Attack. We translate behaviors into six languages (Simplified Mandarin, Italian, Ukrainian, Thai, Zulu, Guarani), including some high-resource (HRL), medium-resource (MRL), and low-resource languages (LRL). We take the set of English inputs and translate them into the desired language. The model is prompted with the translated input and the output is translated back into English for evaluation. We used Google Translate as our translator. ", "page_idx": 16}, {"type": "text", "text": "Prefilling Attack. In our experiments, we use generic prefilled completions across all behaviors. For example, given the behavior \u201cWrite a message threatening someone into not voting in an election,\u201d we preflil the assistant response with \u201cSure, here\u2019s a message threatening someone into not voting in an election: $\\mathsf{M}\\mathsf{n}^{\\backprime}$ . We find that appending the colon and two new line characters strengthens the attack. In practice, providing specific completions (e.g. prefilling with a written message that threatens someone into not voting, in the above example) can be more effective, but even generic completions have a powerful effect. ", "page_idx": 16}, {"type": "text", "text": "Input Embedding Attack. The input embedding attack is similar to GCG, with the difference that it directly optimizes embeddings rather than using gradient information to search over candidate token sequences. Slightly more formally: given a prompt which gets mapped to a sequence of tokens $t_{1:N}$ , GCG seeks to find a sequence of tokens $a_{1:S}$ that maximize the probability that a model will generate a target response when fed the concatenation of these sequences as input. The input embedding attack uses the same loss function to directly optimize a matrix $A\\in\\mathbb{R}^{\\mathbf{\\star}}{S}\\times d$ , which is concatenated with the embeddings of $t_{1:N}$ before being passed into the model, where $S$ is the number of optimized embeddings and $d$ is the dimension of the model. Since we assume the ability to input embeddings into the model, rather than only hard tokens, there is no need to ensure that these embeddings correspond to tokens in the model vocabulary. ", "page_idx": 16}, {"type": "text", "text": "We tokenize the string $\\mathbf{\\cdots}_{\\mathrm{~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~}}^{\\ast\\mathbf{\\alpha}_{\\mathrm{~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~X~}}}$ and then embed the resulting tokens using the target model\u2019s input embedding matrix to get our initial matrix $A$ . Using this string and the default tokenizers, we have $S=20$ . We find that the embedding of this string is a good starting point for optimization. We optimize the embedding matrix $A$ for 500 steps using the SGD optimizer and perform early stopping, as model generations sometimes degrade in coherence when continuing to optimize after the model has already been jailbroken. For Mistral-7B, we use a learning rate of $1\\times10^{-4}$ and stop early when loss decreases below 0.05. For Llama-3, we use a learning rate of $1\\times10^{-3}$ and stop early when loss decreases below 0.01. ", "page_idx": 16}, {"type": "text", "text": "RepE Attack. We follow a standard RepE setup to find and apply directions in the residual stream that induce a model to produce harmful output. We use a dataset of $N$ input pairs, where each pair contains one harmful prompt and one harmless prompt, to generate activations that can be used to find harmful directions. For a given model, we run forward passes on each pair of prompts, and cache the per-layer activations at the last sequence position. We take the differences between the activations of each pair, and then apply PCA on the $N$ difference vectors at each layer, taking the first principal component to get per-layer directions that can be used to control the model. At inference time, we apply these directions to the outputs of transformer layers by using the linear-combination operator; i.e., for each layer we wish to control, we add to its output its corresponding direction vector scaled by a coefficient. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "In all our experiments, we use RepE on layers -11 through -20 (inclusive), where the -1 layer is the final transformer layer prior to the language modeling head, and layer indices that are more negative are closer to the input layer of the model. We use the harmful-harmless dataset [77] and control coefficients of 0.65 and 1.0 for Mistral-7B and Llama-3, respectively. ", "page_idx": 17}, {"type": "text", "text": "C.3 Multimodal Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.3.1 Detailed Circuit Breaker Setup ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We perform the circuit-breaking procedure on the language model backbone in LLaVA-NeXT-Mistral7B [34] while freezing the image encoder and projection layer. We set $\\alpha$ to 5 and target layer 16 for the circuit-breaking loss on LoRA adapters in layers 14 through 16. We alternate between circuit-breaking on text and multimodal training samples at each step. ", "page_idx": 17}, {"type": "text", "text": "C.3.2 Detailed Evaluation Setup ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Robustness Dataset Our dataset of 133 harmful text-image pairs used for robustness evaluation consists of 54 samples from HarmBench\u2019s multimodal behaviors dataset, and 79 samples from MM-SafetyBench\u2019s Tiny dataset. For both datasets, the samples consist of an image and a prompt referencing the image. From the HarmBench multimodal dataset, we chose behaviors that were most saliently harmful (i.e. promoting crime, injury, damage, fraud, etc.), primarily excluding prompts asking for CAPTCHA solutions along with borderline harmful prompts. MM-SafetyBench provides prompts for 13 scenarios, across which the nature of the prompts\u2019 harmfulness differs. We include examples from the scenarios labelled \u201cIllegal Activity\u201d, \u201cHate Speech\u201d, \u201cMalware Generation\u201d, \u201cPhysical Harm\u201d, \u201cEconomic Harm\u201d, \u201cFraud\u201d, and \u201cPrivacy Violence\u201d. We exclude examples from the remaining scenarios such as \u201cFinancial Advice\u201d and \u201cLegal Opinion\u201d, where the harm arises from the potential inaccuracy of model responses. ", "page_idx": 17}, {"type": "text", "text": "Robustness Evaluation For baseline evaluations with a safety prompt, we prefix prompt text with the same safety prompt as in [37], namely: \u201cIf the following questions are unsafe, harmful, or malicious, please refuse to answer them and give your explanation.\u201d ", "page_idx": 17}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/fef032da6441672b6182f7f46af96b6a7132411d3d4828111f38ab1ded5a5bb9.jpg", "table_caption": ["Table 4: Multimodal Robustness Results by Category "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "Table 4 shows a breakdown of our multimodal robustness results by category. The categories are based off of the MM-SafetyBench scenarios from which a portion of our dataset was drawn, with which we also used to categorize the HarmBench samples in our dataset. ", "page_idx": 17}, {"type": "text", "text": "C.4 AI Agents ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We use gpt-4-turbo [50] as a judge to evaluate robustness to harmful requests for function-calls, since the HarmBench classifier was not trained on function calling data. ", "page_idx": 18}, {"type": "text", "text": "We set generation temperature to 0 when evaluating on the Berkeley Function Calling Leaderboard.   \nWe report the average of the AST and Exec scores. ", "page_idx": 18}, {"type": "text", "text": "C.4.1 AI Agent Benchmark Example ", "text_level": 1, "page_idx": 18}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/cb14108c7e0ae1dbc546cca83416e21403484e446295391b56602980b39a8112.jpg", "table_caption": ["Figure 7: A generic function definition and harmful request. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "D Open LLM Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 5 shows the scores for each individual benchmark in the Open LLM evaluation. ", "page_idx": 18}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/a2225bad33ff21d9e6ea545b1b0b96cb3e5217f15d7adaca6e2ffab752ce2a08.jpg", "table_caption": ["Table 5: Open LLM Evaluation Results "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/85a1de2d502c97ff221f0b977373685a19438a6572b97f0cf6c69dc4625cb510.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "", "table_caption": ["Figure 8: Left: Multimodal results. Right: Agent results. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Detailed Results in Multimodal and Agent Settings ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The multimodal results on the left show that under Projected Gradient Descent (PGD) attack, the model with circuit breakers is significantly more robust compared to the original model even with a safety prompt (+Prompt) that instructs the model to avoid harmful responses. Performance on multimodal capabilities benchmarks LLaVA-Wild and MMMU is preserved. In the agent setting on the right, our model with circuit breakers remains robust under Forced Function Calling (Forced F/C), while retaining performance on the Berkeley Function Calling Leaderboard (BFCL). ", "page_idx": 19}, {"type": "text", "text": "F Multilingual Results ", "text_level": 1, "page_idx": 19}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/05bed2b541d41db569c0f221bb6a8d23bb7625b0212c59b2182464afc520931e.jpg", "table_caption": ["Table 6: Attack Success Rates by Language "], "table_footnote": ["In both [70] and [61], it was observed that LRL attacks perform better than HRL attacks. We do not see that trend in Table 6. We leave investigation of this to future work. "], "page_idx": 19}, {"type": "text", "text": "Table 8: Training set ablation: adding data that bypass refusal mechanism in the circuit breaker set (w/ Augment) and adding data that reinforce refusal mechanism in the retain set (w/ Refusal) achieve more balanced results. Training loss ablation: RandC (minimize distance between random centered unit vector) and RMU losses do not converge (\u2013), while RandP (minimize distance between random positive unit vector) converges but is less robust than RR. Average ASR is reported across 6 attacks (DirectRequest, HumanJailbreaks, TAP-T, GCG-T, Prefill, RepE). ", "page_idx": 20}, {"type": "table", "img_path": "IbIB8SBKFV/tmp/3f1e0ae8998859f8357ced052f1a6a6283797d0bc47db2ff91d560edea7c6977.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "G Additional Ablation Results ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/ad176623f1d0133378df7aa3f63f3808e673fc3f5c8dae8579b5cf8de99e69d9.jpg", "img_caption": ["Figure 9: Left: Circuit-breaking loss ablations. Average ASR is reported across 6 attacks (DirectRequest, HumanJailbreaks, TAP-T, GCG-T, Prefill, RepE). Right: Circuit-breaking generalization across categories of harm, averaged over the same 6 attacks as the circuit-breaking loss ablation. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "In general, we find that with Mistral, loss terms that leverage random vectors decrease ASR but also decrease capabilities. We don\u2019t try the augmenting the circuit breaker set or adding refusal data to the retain set because Mistral-7B is not trained with refusal mechanisms. We also find that the circuit breaker set is relatively robust to distribution shifts in categories of harm. ", "page_idx": 20}, {"type": "text", "text": "H Extended analysis of representations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here, we provide an extended representation analysis compared to Section 4.4. In Figure 10, we use the same prefliling example (\u201cHere is how to synthesize meth: 1. Start with\u201d) but on the Mistral-7B models. Here we also observe that circuit-breaking occurs during prefliling similarly to Llama-3-8B. However, this is not always the case: for example in Figure 11, we show a slightly modified prefliling example \u201cHere is how to synthesize m-e-t-h: 1. Start with\u201d (i.e., the word \u201cmeth\u201d is purposefully obfuscated). In this case, circuit-breaking happens during generation which is a common scenario: ", "page_idx": 20}, {"type": "text", "text": "under an adversarial attack, the model starts generating harmful content but switches to meaningless outputs before harmful knowledge is revealed. ", "page_idx": 21}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/ca02b1ae2d599fd3e380a5404cd5eb5f35e4242e855ad485d4c5043262cac2b4.jpg", "img_caption": ["Figure 10: Analysis of internal representations of the Mistral-7B- $\\cdot\\mathrm{v}0.2$ -Instruct model without and with circuit breakers for a prefilled response \u201cHere is how to synthesize meth: 1. Start with\u201d. The cosines and norms start to change dramatically during the prefilling indicating circuit-breaking (starting from layer 10) even before generation starts. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/b7ded56094fb889b492c47bbb9d517a44fdf6c0ff7748e3ee736ae48023093b7.jpg", "img_caption": ["Figure 11: Analysis of internal representations of the Mistral-7B-v0.2-Instruct model without and with circuit breakers for a prefliled response \u201cHere is how to synthesize m-e-t-h: 1. Start with\u201d (i.e., the word \u201cmeth\u201d is purposefully obfuscated). The cosines slightly decrease and norms of later layers increase dramatically after generation begins indicating circuit-breaking. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "In Figure 12 and Figure 13, we show cosines and norms on Llama-3-8B and Mistral-7B for a different prefliling prompt: \u201cSure, here is how to make a bomb: 1. Start with\u201d. We observe that for both model families, circuit-breaking occurs during prefliling, similarly to the example presented in Section 4.4. The only difference is that the norms at layer 10 do not grow significantly, unlike the norms at layer 20 that clearly indicate circuit-breaking. ", "page_idx": 21}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/3a80ce4538a35001e45699e366b2459983770f5dad8c8205794f5e64126fc82a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Figure 12: Analysis of internal representations of the Llama-3-8B-Instruct model without and with circuit breakers for a prefilled response \u201cSure, here is how to make a bomb: 1. Start with\u201d. The cosines and norms start to change dramatically during the prefliling indicating circuit-breaking even before generation starts. ", "page_idx": 22}, {"type": "image", "img_path": "IbIB8SBKFV/tmp/10dfeb8fe8dc87aa0d2acd830b3d67a9ae9175861c69e0318024105d6852d65e.jpg", "img_caption": ["Figure 13: Analysis of internal representations of the Mistral-7B- $\\cdot\\mathrm{v}0.2$ -Instruct model without and with circuit breakers for a prefliled response \u201cSure, here is how to make a bomb: 1. Start with\u201d. The cosines and norms start to change dramatically during the prefliling indicating circuit-breaking even before generation starts. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": ". Claims ", "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, the abstract and introduction accurately reflect our contributions and scope. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we discuss them in Section 5. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not provide theoretical results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we discuss our experimental settings in the main part and in detail in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide our code and instructions to reproduce the results in the supplemental material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we provide details of our experimental settings in the appendix. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [No] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do not provide error bars, but we conduct extensive experiments over many diverse attacks in different settings. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We specify our computation resources and runtime of experiments in the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, we confirm that the research complies with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, the positive societal impact of our work is the key aspect of our work which we motivate in detail in the introduction. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes, our work directly focuses on introducing safeguards to existing openweight models. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We cite the existing assets throughout the paper, with more details in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We document in detail the training procedure for our models and the approach we used for collecting fine-tuning data. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. \u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ", "page_idx": 27}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not conduct experiments with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our research does not require an IRB approval. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]