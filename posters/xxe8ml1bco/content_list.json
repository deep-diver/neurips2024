[{"type": "text", "text": "MultiPull: Detailing Signed Distance Functions by Pulling Multi-Level Queries at Multi-Step ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Takeshi Noda1\u2217 Chao Chen1\u2217 Weiqi Zhang1 Xinhai Liu2 Yu-Shen Liu1\u2020 Zhizhong Han3 ", "page_idx": 0}, {"type": "text", "text": "1School of Software, Tsinghua University, Beijing, China 2Tencent, Hunyuan, Beijing, China   \n3Department of Computer Science, Wayne State University, Detroit, USA   \nyeth21@mails.tsinghua.edu.cn chenchao19@tsinghua.org.cn zwq23@mails.tsinghua.edu.cn adlerxhliu@tencent.com liuyushen@tsinghua.edu.cn h312h@wayne.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reconstructing a continuous surface from a raw 3D point cloud is a challenging task. Recent methods usually train neural networks to overfit on single point clouds to infer signed distance functions (SDFs). However, neural networks tend to smooth local details due to the lack of ground truth signed distances or normals, which limits the performance of overftiting-based methods in reconstruction tasks. To resolve this issue, we propose a novel method, named MultiPull, to learn multi-scale implicit fields from raw point clouds by optimizing accurate SDFs from coarse to fine. We achieve this by mapping 3D query points into a set of frequency features, which makes it possible to leverage multi-level features during optimization. Meanwhile, we introduce optimization constraints from the perspective of spatial distance and normal consistency, which play a key role in point cloud reconstruction based on multi-scale optimization strategies. Our experiments on widely used object and scene benchmarks demonstrate that our method outperforms the state-of-the-art methods in surface reconstruction. Project page: https://takeshie.github.io/MultiPull ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reconstructing surfaces from 3D point clouds is an important task in computer vision. It is widely used in various real-world scenarios such as autonomous driving, 3D scanning and other downstream applications. Recently, using neural networks to learn signed distance functions from 3D point clouds has made huge progress [1, 2, 3, 4, 5, 6, 7, 8]. An SDF represents a surface as the zero-level set of a 3D continuous field, and the surface can be further extracted using the marching cubes algorithm [9]. In supervised methods [10, 11, 12, 13], a continuous field is learned using signed distance supervision. Some methods employ multi-level representations [14, 15], such as Fourier layers and level of detail (LOD) [16, 17], to learn detailed geometry. However, these methods require 3D supervision, including ground truth signed distances or point normals, calculated on a watertight manifold. To address this issue, several unsupervised methods [18, 19, 20, 21, 22] were proposed to directly infer an SDF by overftiting neural networks on a single point cloud without requiring ground truth signed distances and point normals. They usually need various strategies, such as geometric constraints [18, 19, 20] and consistency constraints [22, 23], for smoother and more completed signed distance field. However, the raw point cloud is a highly discrete approximation of the surface, learning ", "page_idx": 0}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/2be8b8ff94b2ac7bbfaf8d5d83ad8558e1f958663730336b6c8a2a6e0a0371c5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Visualization of the 3D shape reconstruction. In (a), (b) and (c), SDFs are learned from a point cloud by optimizing multi-level query points at multi-step. At each step, we optimize query points at one level with frequency features at this specific level as conditions. This enables the network to progressively recover coarse-to-fine geometry details. ", "page_idx": 1}, {"type": "text", "text": "SDFs directly from the point cloud is often inaccurate and highly ambiguous. This makes it hard for the network to learn accurate SDFs on local details, resulting in over-smooth reconstruction. ", "page_idx": 1}, {"type": "text", "text": "To address this issue, we propose MultiPull, to learn an accurate SDF with multi-scale frequency features. It enables network to predict SDF from coarse to fine, significantly enhancing the accuracy of the predictions. Furthermore, to optimize the SDF at different scales simultaneously, we introduce constraints on the pulling process. Specifically, given query points sampled around 3D space as input, we use a Fourier transform network to represent them as a set of Fourier features. Next, we design a network that can leverage multi-scale Fourier features to learn an SDF fields from coarse to fine. To optimize the signed distance fields with multi-scale features, we introduce a loss function based on gradient consistency and distance awareness. Compared with Level of Detail (LOD) methods [16, 17, 24], we can optimize the signed distance fields effectively without a need of signed distance supervision, recovering more accurate geometric details. Evaluations on widely used benchmarks show that our method outperforms the state-of-the-art methods. Our contribution can be summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose a novel framework that can directly learn SDFs with details from raw point clouds, progressing from coarse to fine. This provides a new perspective for recovering 3D geometry details.   \n\u2022 We introduce a multi-level loss function based on gradient consistency and distance awareness, enabling the network to geometry details.   \n\u2022 Our method outperforms state-of-the-art methods in surface reconstruction in terms of accuracy under widely used benchmarks. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Classic methods for geometric modeling [25, 26, 27, 28] have attempted to analyze the geometric modeling of objects, which do not require large-scale datasets. With the advent of extensive and intricate 3D datasets like ShapeNet [29] and ABC [30], learning-based methods have achieved significant advancements [18, 12, 31, 32, 22, 33, 34, 35, 36, 37, 38]. These approaches learn implicit representations from various inputs, including multi-view images[39, 40, 41, 42, 43, 44], point clouds [45, 46, 47], and voxels [48, 49, 50]. ", "page_idx": 1}, {"type": "text", "text": "Learning Implicit Functions with Supervision. Supervised methods have made significant progress in recent years. These methods leverage deep learning networks to learn priors from datasets or use real data for supervision [10, 11, 51, 52, 53, 54] to improve surface reconstruction performance. Some supervised approaches use signed distances and point normals as supervision, or leverage occupancy labels to guide the network\u2019s learning process. In order to improve the generalization ability of neural networks and learn more geometric details, some studies learn geometry prior of shapes through supervised learning. ", "page_idx": 1}, {"type": "text", "text": "Learning Implicit Functions with Conditions. To alleviate the dependence on supervised information, recent studies focus on unsupervised implicit reconstruction methods. These methods do not require pretrained priors during optimization. For example, NeuralPull (NP) [21] learns SDF by pulling query points in nearby space onto the underlying surface, which relies on the gradient field of the network. CAP [55] further complements this by forming a dense surface by additionally sampling dense query points. GridPull [23] generalizes this learning method to the grid, by pulling the query point using interpolated signed distances on the grid. In addition, some studies explore surface reconstruction more deeply and propose innovative methods, such as utilizing differentiable Poisson solutions [56], or learning signed [57, 58, 19, 51] or unsigned functions [59, 55] with priors. However, inferring implicit functions without 3D supervision requires a lengthy convergence process, which limits the performance of unsupervised methods on large-scale point cloud datasets. To address this, we propose a fitting-based frequency feature learning strategy that efficiently learns implicit fields without the need for additional supervision. ", "page_idx": 2}, {"type": "text", "text": "Learning Implicit Functions with LOD. Level-Of-Detail (LOD) models [16, 17, 24] are used to simplify code complexity and refine surface details through the architecture of multi-level outputs. Previous studies have explored multi-scale architectures in various reconstruction tasks. For example, NGLOD [16] uses octree-based feature volumes to represent implicit surfaces, which can adapt to shapes with multiple discrete levels of detail and enable continuous level-of-detail switching through SDF interpolation. MFLOD [17] applies Fourier layers to LOD, which can offer better feasibility in Fourier analysis. However, it is difficult to optimize multi-level features simultaneously to learn 3D shapes. To address this issue, we propose a novel strategy to optimize multi-level frequency features, allowing the network to progressively learn geometric details from coarse to fine. ", "page_idx": 2}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/de839927cce84e01c272bc4ed29bcb662b3d5e6e513b85f7b78ca5f5a14c7f3b.jpg", "img_caption": ["$\\odot$ Hadamard Product $\\bigcirc^{Q}$ \ud835\udc56:query points in $\\mathcal{Y}_{i}$ :frequency feature SDF predictions linear layers LSNN Linear Sequence i-th step pulling Neural Network "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Overview of our method: (a) Frequency Feature Transformation (FFT) module and (b) Multi-Step Pulling (MSP) module. In (a), we learn Fourier bases $h_{i}(Q)$ from query points $Q$ using the Fourier layer and obtain multi-level frequency features $y_{i}$ through Hadamard product. In (b), using multi-level frequency features from (a) and a linear network LSNN with shared parameters, we calculate the distance(D) of $Q_{i}$ to its corresponding surface target point $Q_{t}$ to predict a more accurate surface. We visualize the predicted SDF distribution map corresponding to the frequency features in (a) and the reconstruction from each step of SDF predictions on the right side of (b). ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Overview. The overview of MultiPull is shown in Fig. 2. We design a neural network to learn an implicit function $f$ from a single 3D point cloud by progressively pulling a set of query points $Q_{0}$ onto the underlying surface, where $Q_{0}$ is randomly sampled around the raw point cloud $S$ . Our network mainly consists of two parts as follows. ", "page_idx": 2}, {"type": "text", "text": "(1) The Frequency Feature Transformation (FFT) Module ( Fig. 2(a)) aims to convert the query points $Q_{0}$ into a set of multi-level frequency features $Y=\\{y_{i},i\\in[0,N_{L}-1]\\}$ . The key insight for introducing frequency features lies in a flexible control of the degree of details. (2) The Multi-Step Pulling (MSP) Module (Fig. 2(b)) is designed to predict $f$ with coarse-to-fine details under the guidance of frequency features $Y$ . At the $i$ -th step, we pull $Q_{i}$ to $Q_{i+1}$ using the predicted signed distances $s_{i}=f(Q_{i},y_{i})$ and the gradients at $Q_{i}$ , according to its feature $y_{i}$ . To this end, we constrain query points to be as close to their nearest neighbor point on $S$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Frequency Feature Transformation (FFT) Module ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We introduce a neural network to learn frequency features $Y$ from point clouds. The network manipulates input $Q_{0}$ through several linear layers to obtain an initial input $z_{0}$ and a set of Fourier basis $h_{i}(Q_{0}),i\\in[0,N_{L}-1]$ , formulated as follows. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\{{\\begin{array}{c}{h_{i}(Q_{0})=\\sin(\\omega_{i}Q_{0}+\\phi_{i}),}\\\\ {z_{0}=h_{0}(Q_{0}),}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\omega_{i}$ and $\\phi_{i}$ are the parameters of the network, and $N_{L}$ is the number of layers of the network. ", "page_idx": 3}, {"type": "text", "text": "To effectively represent the expression of the raw input in the frequency space, we choose the sine function as the activation function and employ the Hadamard product to compute the intermediate frequency feature output. Since the Hadamard product allows the representation of frequency components as the product of two feature inputs, denoted as $a$ and $b$ , which can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sin(a)\\sin(b)={\\frac{1}{2}}(\\sin(a+b-{\\frac{\\pi}{2}})+\\sin(a-b+{\\frac{\\pi}{2}})).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Through Eq. (2), we can calculate the frequency component $z_{i}$ of $h_{i}(Q_{0})$ , and then obtain the output $y_{i}$ of the $i$ -th layer, formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{z_{i}=h_{i}(Q_{0})\\odot(W_{i}z_{i-1}+b_{i}),i\\in[1,N_{L}-1],}\\\\ {y_{i}=W_{i}z_{i}+b_{i},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\odot$ indicates the Hadamard product, $W_{i},b_{i}$ are parameters of the network. ", "page_idx": 3}, {"type": "text", "text": "Frequency networks based on the Multiplication Filter Network (MFN) [14] typically employ uniform or fixed-weight initialization for network parameters in practice. This approach overlooks the issue of gradient vanishing in deep network layers during the training process, leading to underfitting and making the network overly sensitive to hyperparameter changes. To address this challenge, we propose a new initialization scheme that thoroughly considers the impact of network propagation, aiming at ensuring a uniform distribution of initial parameters. Specifically, we dynamically adjust initial weights, which can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Psi_{i}=\\sqrt{\\eta\\times\\sin(i\\pi/N_{L})},i\\in[1,N_{L}-1],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $N_{L}$ and $\\eta$ are the number of layers and the parameters of the network, respectively. We leverage the standard deviation as the initialization range to ensure that the parameters in Eq. (3) are within a reasonable range. As shown in Fig. 3, we compared the parameter distributions of different linear layers. The initialization scheme based on MFN results in gradient vanishing and small activations in deeper linear layers. In contrast, our initialization scheme ensures that the parameters of each linear layer follow a standard normal distribution. ", "page_idx": 3}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/d4ba900c98e6e6ea7abda1fd2b08b936074e02fc15b6edb559f14fa488e1f202.jpg", "img_caption": ["Figure 3: Comparison of parameter distributions of different linear layers especially in $(L_{2},L_{4},L_{6},L_{8})$ . We show the different initialization strategies on the results of the reconstruction task and the visualization effects in Appendix B. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Multi-Step Pulling (MSP) Module ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In Fig. 2(b), we demonstrate our idea of learning an accurate implicit function $f$ with multiple frequency features. Given a set of frequency feature $Y$ , we use frequency features $y_{i}$ in $Y$ as the input along with query points $Q_{i}$ for the MSP module. We follow NP[21] to construct initial query points and calculate the stride and direction of $Q_{i}$ at $i$ -th step for pulling it to the target surface point. Furthermore, we use the direction of the gradient as $\\nabla f(Q_{i},y_{i})$ and signed distance $f(Q_{i},y_{i})$ for the pulling, where $\\nabla f(Q_{i},y_{i})$ represents the fastest increase in signed distance in 3D space, pointing away from the surface direction. Therefore, $Q_{i}\\,=\\,Q_{i-1}\\,-\\,f(Q_{i-1},y_{i-1})\\cdot\\nabla f(\\bar{Q_{i-1}},\\bar{y_{i-1}})/\\left|\\right|$ $\\nabla f(Q_{i-1},y_{i-1})\\parallel_{2}$ . For each step of pulling the query points $Q_{i}$ , it corresponds to a nearest point $q_{i}$ on the surface, and the distance between query points and surface points can be described as $D_{i}=||Q_{i}-q_{i}||_{2}^{2}$ . Based on this, we initiate the optimization by pulling query points $Q_{i}$ the target points $q_{i}$ progressively. Therefore, we can obtain the combined loss $\\mathcal{L}_{\\mathrm{pull}}$ under optimal conditions: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{pull}}=\\sum_{i=1}^{I}D_{i},i\\in[1,I],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $I$ is the step of moving operation. However, optimizing all query points accurately through this equation alone is challenging when merely constraining surface points. This is because the query points $Q_{i}$ may be located across multiple spatial scales with inconsistent gradient directions, indicating that simultaneous optimization becomes challenging. Consequently, some outlier points may not be effectively optimized. Additionally, for sampling points near target points, some surface constraints are required to enable the network to accurately predict their corresponding zero levelset to avoid optimization errors. Therefore, we will further advance Eq. (5) from the perspectives of distance constraints, gradient consistency, surface constraints in Sec. 3.3 to enhance network performance. ", "page_idx": 4}, {"type": "text", "text": "3.3 Loss Function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Distance-Aware Constraint. Inspired by FOCAL [60], we design a novel constraint with distance-aware attention weights $\\alpha$ to ensure that the network pays more attention to the optimization of underfitting query points in space and optimizes the SDFs simultaneously. This allows query points at different distances from the surface to be optimized properly, and assigns higher attention weights for outlier and underlying points: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{\\alpha_{1},\\alpha_{2}=s o f t m a x(D_{1},D_{2}),}\\\\ {\\mathcal{L}_{\\mathrm{recon}}=\\alpha_{1}D_{1}+\\alpha_{2}^{\\gamma}D_{2}+D_{3},}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha_{1}$ and $\\alpha_{2}$ are calculated from $D_{1}$ , $D_{2}$ by the softmax activation, $\\gamma$ is a scaling coefficient we set to 2 by default. Here, we only consider 3 steps, which is a trade-off between performance and efficiency. ", "page_idx": 4}, {"type": "text", "text": "Consistent Gradients. We additionally introduce consistency constraints in the gradient direction. This loss encourages neighboring level sets to keep parallel, which reduces the artifacts off the surface and smooths the surface. We add a cosine gradient consistency loss function to encourage the gradient direction at the query points to keep consistent with the gradient direction at its target point on the surface, which aims to improve the continuity of the gradient during the multi-step pulling. We use $Q_{1},Q_{2}$ and $Q_{3}$ to represent the query points that have been continuously optimized by the multiple steps. We take the one with the lowest similarity score to measure the overall similarity. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Big\\{L\\nabla(Q_{i})=\\cos(\\nabla f(Q_{i},y_{i}),\\nabla f(Q_{0},y_{0})),\\qquad\\qquad}\\\\ {\\quad\\mathcal{L}_{\\mathrm{grad}}=1-\\operatorname*{min}\\{L_{\\nabla}(Q_{1}),L_{\\nabla}(Q_{2}),L_{\\nabla}(Q_{3})\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $L_{\\nabla}(Q_{i})$ represents the loss of cosine similarity between query points $Q$ and target surface points $q$ . ", "page_idx": 4}, {"type": "text", "text": "Surface Constraint. We introduce the surface constraint for the implicit function $f$ , aiming to assist the network in approaching the zero level-set on the surface at final step. Hence, we constrain the $f(Q_{I},y_{I})$ approaches zero at the final step: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{surf}}=\\parallel f(Q_{I},y_{I})\\parallel.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Joint Loss Function. Overall, we learn the SDFs by minimizing the following loss function $\\mathcal{L}$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{\\mathrm{recon}}+\\beta\\mathcal{L}_{\\mathrm{grad}}+\\delta\\mathcal{L}_{\\mathrm{surf}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\beta$ and $\\delta$ are balance weights. In the subsequent ablation experiments, we validated the effectiveness of different loss functions. ", "page_idx": 4}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we evaluate the performance of MultiPull in surface reconstruction by conducting numerical and visual comparisons with state-of-the-art methods on both synthetic and real-scan datasets. Specifically, in Sec. 4.1, we experiment on synthetic shape datasets with diverse topological structures. Furthermore, in Sec. 4.2, we report our results across various scales on real large-scale scene datasets. Meanwhile, we consider FAMOUS as the verification dataset in the ablation studies to compare the effectiveness of each module in MultiPull in Sec. 4.3. ", "page_idx": 5}, {"type": "text", "text": "4.1 Surface Reconstruction for Shapes ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets and Metrics. For the single shape surface reconstruction task, we perform evaluation on multiple datasets including ShapeNet [29], FAMOUS [10], Surface Reconstruction Benchmark (SRB) [45] Thingi10K [61] and D-FAUST [62]. We conduct validation experiments on 8 subcategories within the ShapeNet dataset, while the remaining datasets are experimented on the complete dataset. For metric comparison, we leverage L1 and L2 Chamfer Distance ${\\mathrm{CD}}_{L1}$ and $\\mathrm{CD}_{L2}$ , Normal Consistency (NC), and F-Score as evaluation metrics. ", "page_idx": 5}, {"type": "text", "text": "ShapeNet.We evaluate our approach on the ShapeNet[29] according to the experimental settings of GP [23] . We compared our methods with methods including ATLAS [63], DSDF [51], NP [21], PCP [64], GP [23], as shown in Tab. 1. We report $\\mathrm{CD}_{L2}$ , NC and F-Score metrics for ShapeNet, where we randomly sample 10,000 points on the reconstructed object surface for evaluation. MultiPull outperforms the state-of-the-art methods. Compare to previous gradient-based methods in Fig. 4, our method performs better by revealing more local details of these complex structures. We provide detailed results in Appendix C. ", "page_idx": 5}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/f21071cad2f029a1e2f4133f002ce2126a255f8c7733639cc91880d6f6316d52.jpg", "table_caption": ["Table 1: Reconstruction accuracy on ShapeNet in terms of $\\mathrm{CD}_{L2}$ , NC and F-Score with thresholds of 0.002 and 0.004. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/88a7987340a7ac2fc312c0d36fe6e35984241077ef6368721c4cfa2c2e7c61f1.jpg", "img_caption": ["Figure 4: Visual comparison of reconstructions on ShapeNet. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "FAMOUS. We evaluate the performance of our method on the FAMOUS dataset according to the experimental settings of PCP [64] and NP [21]. Our method demonstrates superiority over recent approaches, including GP [23], PCP [64], GenSDF [65], FGC [66], NP [21], and IGR [57]. As shown in Tab. 2, we compared the recent methods using $\\mathrm{CD}_{L2}$ and NC metrics, and our method exhibits outstanding performance. To demonstrate the effectiveness of our method in reconstruction accuracy, we visualize the error-map for comparison in Fig. 5. Compare to the the state-of-art methods, our method has better overall reconstruction accuracy (bluer). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/13f8ab82f2ab56def6648475123b54eb2cdce7741c38ffa2493e190ba7f10943.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/ee18a902f549450e6210217822bf148b6c7756337b43138b9a00075be3825fa5.jpg", "img_caption": ["Figure 5: Visual comparison of error maps on FAMOUS. "], "img_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/99da85ab64bb4f7f115f390cceab53dee5be5a1faaf0d093de7f2c522ad52676.jpg", "table_caption": ["Table 3: Reconstruction accuracy on SRB in terms of ${\\mathrm{CD}}_{L1}$ and F-Score with a threshold of $0.01$ . "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/a31b431c614637ef1b88815d407c019a39119b866437e1299cb870a495c579b5.jpg", "img_caption": ["Figure 6: Visual comparison on SRB. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "SRB. We validate our method on the real scanned dataset SRB, following the experimental settings of VisCo [67] and GP [23]. In Tab. 3, we compared our approach with recent methods including P2M [68], SAP [56], NP [21], BACON [15], CAP [55], GP [23]. We use $\\mathrm{CD}_{L1}$ and F-Score to evaluate performance , and we surpass all others in terms of these metrics. As depicted in Fig. 6, our method excels in reconstructing more complete and smoother surfaces. ", "page_idx": 6}, {"type": "text", "text": "D-FAUST. We evaluate our method on the D-FAUST dataset with SAP [56] settings. As indicated in Tab. 4, we compared our approach with recent methods including IGR [57], SAP [56], GP [23]. Our method excels in $\\mathrm{CD}_{L1}$ , F-Score and NC. As illustrated in Fig. 7, compared to other methods, our approach demonstrates superior accuracy in recovering human body shapes. ", "page_idx": 6}, {"type": "text", "text": "Thingi10K. We assess the performance of our approach on the Thingi10K dataset, following the experimental setup of SAP [56]. We compared our approach with recent methods including IGR [57], SAP [56], BACON [15], GP [23]. As indicated in Tab. 5, our method surpasses existing methods across in ${\\mathrm{CD}}_{L1}$ , F-Score and NC metrics. As illustrated in Fig. 8, our method can reconstruct surfaces with more accurate details. ", "page_idx": 6}, {"type": "text", "text": "Table 4: Reconstruction accuracy under DFAUST in terms of $\\mathrm{CD}_{L1}$ and F-Score with a threshold of 0.01. ", "page_idx": 7}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/137126f6a2657379da9b6536bde858b4611e8a888960412ba2b7b1eb3d398082.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 5: Reconstruction accuracy under Thingi 10K in terms of ${\\mathrm{CD}}_{L1}$ and F-Score with a threshold of 0.01. ", "page_idx": 7}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/7a0c1ba7018e3353fff8fc6c5cae5a0b0f0ff2387724b1b60d3eb173c3909aa7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/c8da3513b7b01bfda9f59c6d5478893164719e1265f53dc6b783b0759733eefc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/16223b19e5358aae4ea19a869a7d49bb42da518f787973b0145019630a9e1a98.jpg", "img_caption": ["Figure 8: Visual comparison on Thingi10K. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Surface Reconstruction for Real-Scan Scenes ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets and Metrics.For the scene reconstruction task, we validate our method on the 3DScene [69] and KITTI [70] datasets to assess the performance on large-scale datasets. We keep the same evaluation metrics as those used for shape reconstruction in Sec. 4.1. ", "page_idx": 7}, {"type": "text", "text": "3DScene. In accordance with the experimental settings of PCP [64], we compared our approach with recent methods including ConvOcc [39], NP [21], PCP [64] and GP [23]. We report the evaluation results of ${\\mathrm{CD}}_{L1}$ , $\\mathrm{CD}_{L2}$ and NC, and compared our method with the latest approaches listed in Tab. 6. As illustrated in Fig. 9, our method outperforms prior-based methods and overftiting based methods. ", "page_idx": 7}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/444daa4b1d2b927745c1bd08e5beac8da1ce2021f2791278eb1b087e990e7885.jpg", "img_caption": ["Figure 9: Visual comparison of CD error maps on 3DScene. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Table 6: Reconstruction accuracy on 3DScene in terms of $\\mathrm{CD}_{L1}$ , $\\mathrm{CD}_{L2}$ and NC. ", "page_idx": 8}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/b021dc4bf504e96cedf10bdffc2edc487c46ebed8af54ca11a6d3dca51cd5ae9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/f75b6f9bcc45c9f0136cf265890720c2ead3508bcabe6857ebd53224b29cb5f1.jpg", "img_caption": ["Figure 10: Visual comparison on KITTI. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "KITTI. We validate our method on the large-scale scanned point cloud dataset KITTI [70], which contains 13.8 million points. As shown in Fig. 10, our approach is capable of reconstructing more complete and accurate surfaces compared to the GP method [23]. GP struggles to reconstruct continuous surfaces such as walls and streets, whereas our method achieves a more detailed reconstruction of objects at various scales in real scanned scenes. It demonstrates that our method is robust when handling point cloud with various scales. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Effect of Frequency Layers. We denote the $j$ -th layer of the frequency network as $L_{j}$ , a specific combination of frequency feature layers can be formulated as $\\bar{\\{L_{i},L_{j},L_{k}\\}}$ , where $\\{i,j,k\\}\\ \\in$ $[1,N_{L}-1]$ . We evaluate the effectiveness of the frequency transformer layers in Tab. 7 with $\\mathrm{CD}_{L2}$ and NC, replacing the frequency network with linear layers results in a decrease in the performance of the $\\mathrm{CD}_{L2}$ and NC metrics. The performance of using only one layer $(L_{4})$ surpasses linear layers. With the increase of the frequency layers, $\\{L_{4},L_{6},L_{8}\\}$ produces best results. ", "page_idx": 8}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/09289b65e9cfed4620991bb2f79e2d9d9a755b75d849938a8a40751bc2fa8c4f.jpg", "table_caption": ["Table 7: Effect of frequency features. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Effect of MSP Module. We report comparisons with different features in Tab. 8. The \u2019Layer\u2019 column denotes the combination of frequency features obtained by the FFT module. For instance, $\\{L_{4},L_{6},L_{8}\\}$ represent the frequency features from the $4^{t h}$ , $6^{t h}$ , and $8^{t h}$ layers guiding the pulling of the query point in the MSP network, respectively. We find that the accuracy of the network increases with the number of steps. After considering both performance metrics and time efficiency, we have set Step $=\\!3$ by default. ", "page_idx": 9}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/f8ab29e94c59238271fd90a365bd1ba22f566e74b4c1b04d39055f01d7fde613.jpg", "table_caption": ["Table 8: Effect of MSP Module. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Effect of Loss Functions. We compared $\\mathrm{CD}_{L2}$ metric under different loss strategies in Tab. 9. As shown in the table, Weighting query points at different scales effectively enhances reconstruction accuracy and the reconstruction loss allows the network to obtain a complete shape with local details. Furthermore, The gradient loss improves the surface continuity of the object. And the surface supervision loss facilitates the learning of more precise zero-level sets, which also improves the accuracy. ", "page_idx": 9}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/7f7c80c1c1ac8afa0ad4ed53bd6b86bea5bb5bb22e59b0d07840bcc40ec23baa.jpg", "table_caption": ["Table 9: Effect of loss functions. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Effect of Different Levels of Noise.We evaluate the reconstruction performance of our method on the Famous dataset under two levels of noise: Mid-Level and Max-Level noise. As shown in Tab. 10, our method outperforms the majority of approaches even in the presence of noisy inputs. ", "page_idx": 9}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/4929639533b470632829c0b8cd25a99ab3debd5f84e739ba1de275c596315084.jpg", "table_caption": ["Table 10: Effect of different levels of noise. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose a novel method to learn detailed SDFs by pulling queries onto the surface at multi-step. We leverage the multi-level features to predict signed distances, which recovers high frequency details. Through optimization, our method is able to gradually restore the coarse-to-fine structure of reconstructed objects, thereby revealing more geometry details. Visual and numerical comparisons show that our approach demonstrates competitive performance over the state-of-the-art methods. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by National Key R&D Program of China (2022YFC3800600), and the National Natural Science Foundation of China (62272263, 62072268), and in part by TsinghuaKuaishou Institute of Future Media Data. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-NeRF: Scalable large scene neural view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8248\u20138258, 2022. [2] Michael Oechsle, Songyou Peng, and Andreas Geiger. UNISURF: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5589\u20135599, 2021. [3] Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias Zwicker. Drwr: A differentiable renderer without rendering for unsupervised 3D structure learning from silhouette images. In International Conference on Machine Learning, 2020. [4] Chao Chen, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Unsupervised learning of fine structure generation for 3D point clouds by 2D projections matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12466\u201312477, 2021.   \n[5] Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, and Zhizhong Han. Learning a more continuous zero level set in unsigned distance fields through level set projection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [6] Zixiong Wang, Yunxiao Zhang, Rui Xu, Fan Zhang, Peng-Shuai Wang, Shuangmin Chen, Shiqing Xin, Wenping Wang, and Changhe Tu. Neural-singular-hessian: Implicit neural representation of unoriented point clouds by enforcing singular hessian. ACM Transactions on Graphics (TOG), 42(6):1\u201314, 2023. [7] Fei Hou, Chiyu Wang, Wencheng Wang, Hong Qin, Chen Qian, and Ying He. Iterative poisson surface reconstruction (ipsr) for unoriented points. arXiv preprint arXiv:2209.09510, 2022. [8] Pengchong Hu and Zhizhong Han. Learning neural implicit through volume rendering with attentive depth fusion priors. In Advances in Neural Information Processing Systems, 2023. [9] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3D surface construction algorithm. ACM Siggraph Computer Graphics, 21(4):163\u2013169, 1987.   \n[10] Philipp Erler, Paul Guerrero, Stefan Ohrhallinger, Niloy J Mitra, and Michael Wimmer. Points2Surf learning implicit surfaces from point clouds. In European Conference on Computer Vision, pages 108\u2013124. Springer, 2020.   \n[11] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, Thomas Funkhouser, et al. Local implicit grid representations for 3D scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6001\u20136010, 2020.   \n[12] Julien NP Martel, David B Lindell, Connor Z Lin, Eric R Chan, Marco Monteiro, and Gordon Wetzstein. ACORN: adaptive coordinate networks for neural scene representation. ACM Transactions on Graphics (TOG), 40(4):1\u201313, 2021.   \n[13] Yueqi Duan, Haidong Zhu, He Wang, Li Yi, Ram Nevatia, and Leonidas J Guibas. Curriculum deepsdf. In European Conference on Computer Vision, pages 51\u201367. Springer, 2020.   \n[14] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico Kolter. Multiplicative filter networks. In International Conference on Learning Representations, 2020.   \n[15] David B Lindell, Dave Van Veen, Jeong Joon Park, and Gordon Wetzstein. BACON: Band-limited coordinate networks for multiscale scene representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16252\u201316262, 2022.   \n[16] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3d shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11358\u201311367, 2021.   \n[17] Yishun Dou, Zhong Zheng, Qiaoqiao Jin, and Bingbing Ni. Multiplicative fourier level of detail. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1808\u20131817, 2023.   \n[18] Matan Atzmon and Yaron Lipman. SAL: Sign agnostic learning of shapes from raw data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2565\u20132574, 2020.   \n[19] Matan Atzmon and Yaron Lipman. SALD: Sign agnostic learning with derivatives. In International Conference on Learning Representations, 2020.   \n[20] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for 3D shape. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4857\u20134866, 2020.   \n[21] Baorui Ma, Zhizhong Han, Yu-Shen Liu, and Matthias Zwicker. Neural-Pull: Learning signed distance function from point clouds by learning to pull space onto surface. In International Conference on Machine Learning, pages 7246\u20137257. PMLR, 2021.   \n[22] Baorui Ma, Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Towards better gradient consistency for neural signed distance functions via level set alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17724\u201317734, 2023.   \n[23] Chao Chen, Yu-Shen Liu, and Zhizhong Han. GridPull: Towards scalability in learning implicit representations from 3D point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18322\u201318334, 2023.   \n[24] Danhang Tang, Mingsong Dou, Peter Lincoln, Philip Davidson, Kaiwen Guo, Jonathan Taylor, Sean Fanello, Cem Keskin, Adarsh Kowdle, Sofien Bouaziz, et al. Real-time compression and streaming of 4d performances. ACM Transactions on Graphics (TOG), 37(6):1\u201311, 2018.   \n[25] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, Cl\u00e1udio Silva, and Gabriel Taubin. The ballpivoting algorithm for surface reconstruction. IEEE Transactions on Visualization and Computer Graphics, 5(4):349\u2013359, 1999.   \n[26] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics (TOG), 32(3):1\u201313, 2013.   \n[27] Yiyi Liao, Simon Donne, and Andreas Geiger. Deep marching cubes: Learning explicit surface representations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2916\u20132925, 2018.   \n[28] Yutaka Ohtake, Alexander Belyaev, Marc Alexa, Greg Turk, and Hans-Peter Seidel. Multi-level partition of unity implicits. ACM Transactions on Graphics, 22(3):463\u2013470, 2003.   \n[29] ShapeNet. https://www.shapenet.org/.   \n[30] Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. Abc: A big cad model dataset for geometric deep learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9601\u20139611, 2019.   \n[31] Konstantinos Rematas, Ricardo Martin-Brualla, and Vittorio Ferrari. Sharf: Shape-conditioned radiance fields from a single view. In International Conference on Machine Learning, 2021.   \n[32] Baorui Ma, Yu-Shen Liu, and Zhizhong Han. Learning signed distance functions from noisy 3d point clouds via noise to noise mapping. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \n[33] Shujuan Li, Junsheng Zhou, Baorui Ma, Yu-Shen Liu, and Zhizhong Han. Neaf: Learning neural angle fields for point normal estimation. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 1396\u20131404, 2023.   \n[34] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023.   \n[35] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. InstructPix2Pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vsion and Pattern Recognition, 2023.   \n[36] Zhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and ChenHsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition, 2023.   \n[37] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):1\u201316, 2023.   \n[38] Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, and Wenping Wang. Geoudf: Surface reconstruction from 3d point clouds via geometry-guided distance representation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14214\u201314224, 2023.   \n[39] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In European Conference on Computer Vision, pages 523\u2013540. Springer, 2020.   \n[40] Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li. Learning to infer implicit surfaces without 3D supervision. In Advances in Neural Information Processing Systems, 2019.   \n[41] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In Advances in Neural Information Processing Systems, 2021.   \n[42] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing Systems, 33, 2020.   \n[43] Liang Han, Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Binocular-guided 3D gaussian splatting with view consistency for sparse view synthesis. In Advances in Neural Information Processing Systems, 2024.   \n[44] Wenyuan Zhang, Yu-Shen Liu, and Zhizhong Han. Neural signed distance function inference through splatting 3d gaussians pulled on zero-level set. In Advances in Neural Information Processing Systems, 2024.   \n[45] Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan Bruna, and Daniele Panozzo. Deep geometric prior for surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10130\u201310139, 2019.   \n[46] Minghua Liu, Xiaoshuai Zhang, and Hao Su. Meshing point clouds with predicted intrinsic-extrinsic ratio guidance. In European Conference on Computer vision, 2020.   \n[47] Zhenxing Mi, Yiming Luo, and Wenbing Tao. SSRNet: Scalable 3D surface reconstruction network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 970\u2013979, 2020.   \n[48] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Improved direct voxel grid optimization for radiance fields reconstruction. arXiv preprint arXiv:2206.05085, 2022.   \n[49] N. Sedaghat, M. Zolfaghari, E. Amiri, and T. Brox. Orientation-boosted voxel nets for 3D object recognition. In British Machine Vision Conference, 2017.   \n[50] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. Vox-Surf: Voxel-based implicit surface representation. CoRR, abs/2208.10925, 2022.   \n[51] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 165\u2013174, 2019.   \n[52] Gal Metzer, Rana Hanocka, Denis Zorin, Raja Giryes, Daniele Panozzo, and Daniel Cohen-Or. Orienting point clouds with dipole propagation. ACM Transactions on Graphics (TOG), 40(4):1\u201314, 2021.   \n[53] Rui Xu, Zhiyang Dou, Ningna Wang, Shiqing Xin, Shuangmin Chen, Mingyan Jiang, Xiaohu Guo, Wenping Wang, and Changhe Tu. Globally consistent normal orientation for point clouds by regularizing the winding-number field. ACM Transactions on Graphics (TOG), 42(4):1\u201315, 2023.   \n[54] Qing Li, Huifang Feng, Kanle Shi, Yue Gao, Yi Fang, Yu-Shen Liu, and Zhizhong Han. Shs-net: Learning signed hyper surfaces for oriented normal estimation of point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13591\u201313600, 2023.   \n[55] Junsheng Zhou, Baorui Ma, Liu Yu-Shen, Fang Yi, and Han Zhizhong. Learning consistency-aware unsigned distance functions progressively from raw point clouds. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \n[56] Songyou Peng, Chiyu \u201cMax\" Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger. Shape as points: A differentiable poisson solver. In Advances in Neural Information Processing Systems, 2021.   \n[57] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. In International Conference on Machine Learning, pages 3789\u20133799. PMLR, 2020.   \n[58] Wenbin Zhao, Jiabao Lei, Yuxin Wen, Jianguo Zhang, and Kui Jia. Sign-agnostic implicit learning of surface self-similarities for shape modeling and reconstruction from raw point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10256\u201310265, 2021.   \n[59] Yu-Tao Liu, Li Wang, Jie Yang, Weikai Chen, Xiaoxu Meng, Bo Yang, and Lin Gao. Neudf: Leaning neural unsigned distance fields with volume rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 237\u2013247, 2023.   \n[60] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.   \n[61] Qingnan Zhou and Alec Jacobson. Thingi10k: A dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016.   \n[62] Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J Black. Dynamic FAUST: Registering human bodies in motion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6233\u20136242, 2017.   \n[63] Qian Yu, Chengzhuan Yang, and Hui Wei. Part-wise atlasnet for 3d point cloud reconstruction from a single image. Knowledge-Based Systems, 242:108395, 2022.   \n[64] Baorui Ma, Yu-Shen Liu, Matthias Zwicker, and Zhizhong Han. Surface reconstruction from point clouds by learning predictive context priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.   \n[65] Gene Chou, Ilya Chugunov, and Felix Heide. GenSDF: Two-stage learning of generalizable signed distance functions. In Proc. of Neural Information Processing Systems (NeurIPS), 2022.   \n[66] Mulin Yu and Florent Lafarge. Finding good configurations of planar primitives in unorganized point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6367\u20136376, June 2022.   \n[67] Albert Pumarola, Artsiom Sanakoyeu, Lior Yariv, Ali Thabet, and Yaron Lipman. Visco grids: Surface reconstruction with viscosity and coarea grids. In Advances in Neural Information Processing Systems, 2022.   \n[68] Rana Hanocka, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. Point2Mesh: a self-prior for deformable meshes. ACM Transactions on Graphics (TOG), 39(4):126\u20131, 2020.   \n[69] Qian-Yi Zhou and Vladlen Koltun. Dense scene reconstruction with points of interest. ACM Transactions on Graphics (TOG), 32(4):1\u20138, 2013.   \n[70] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3354\u20133361. IEEE, 2012.   \n[71] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1\u201315, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Implementation Details. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our network consists of two main parts: frequency feature transformation and multi-step pulling modules in Fig 2 (a) and (b), respectively. For frequency feature transformation, we transform the raw point cloud into frequency features $M$ , where $M$ is initialized to 256. Same for the multi-step pulling module, we train a linear sequence neural network (LSNN) with shared parameters and we fix intermediate layer output dimension to 512. In the construction of query points, we establish the corresponding pairs between query points and their nearest points on surfaces. Specifically, we follow NP [21] to construct 40 queries for each point of the point cloud, the construction of these query points follows a Gaussian distribution. During the reconstruction process, we use the Marching Cubes algorithm [27] to extract the mesh surface. ", "page_idx": 14}, {"type": "text", "text": "During the training process, we do optimization in 40,000 iterations, with an average time of 24 minutes for single-object reconstruction. We utilize a single NVIDIA RTX-3090 GPU for both training and testing. ", "page_idx": 14}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Effect of Frequency Features. To further validate the superiority of frequency features, we exclude multi-step pulling, and only use single-layer frequency feature for performance verification against linear layers. We note the frequency feature in the $i^{\\th}$ -th layer as $L_{i}$ . We compared the $\\mathrm{CD}_{L2}$ and NC of specific layers $(L_{2},L_{4},L_{6},L_{8})$ with the linear layers. As shown the Tab. 11 the performance of frequency features at different layers is superior to the linear layers, and with an increase in the number of layers, higher-level frequency conditions enhance the network\u2019s performance. ", "page_idx": 14}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/962b743fba35f8b7abd82ee2af5dc535d37dad97b3d8a36d8aa9688a2b72e18a.jpg", "table_caption": ["Table 11: Effect of frequency features. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Effect of Initialization Strategies. We compared our initialization strategy with random initialization and MFN-based method (BACON [15]) as example. We compared the metrics of these initialization methods in Tab. 12, which shows that combining random or BACON initialization with our approach does not yield satisfactory results.To further demonstrate the advantages of our initialization method, we visually compared SDF with random initialization and BACON initialization strategies. As shown in the Fig. 11, our method significantly outperforms other initialization methods in terms of convergence speed. In addition, our reconstruction results also indicate that a reasonable initialization method can enable the network to learn more accurate signed distance field. We compared the results with the same iterations and the final results under the default settings for different methods (Final) in Fig. 12, these failed reconstructions based on MFN demonstrate the instability of parameter initialization. ", "page_idx": 14}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/2b42962eccddf4e9477193584700768fde2b44e4b47405e70146a8799a982f12.jpg", "table_caption": ["Table 12: Effect of initialization strategies. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Effect of Parameters on Networks. We compared the parameter quantities of the methods listed in Tab. 13 below. It shows that the parameter number of PCP[64] is the largest among all the three methods, while NP[21] has the least parameters. To further investigate the performance of networks with the similar amount of parameters, we increase the parameters of NP and MultiPull to match PCP. The comparison in the Tab. 14 indicates that both NP and MultiPull show the improved performance. This demonstrates that more parameters are beneficial to improve the performance, but our performance comes from our novel methods rather than more parameters. Meanwhile,we further evaluate inference time with NP and PCP.As shown in Tb. 15 the local based method (PCP) has the longer inference time than the global based method, with NP having the fastest inference time. ", "page_idx": 14}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/f55bc05f27ddca214d56256cb7d27d94119f3f61d038319cdefe24a6230702a8.jpg", "img_caption": ["Figure 11: Comparison of signed distances in optimization "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/bfcc545c5742787b7277d298e55ca399afdef8eb38e10fb0327db71c31c54540.jpg", "img_caption": ["Figure 12: Comparison of the initialization strategies in optimization. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/5bd0e9b0733686fe5b37eeb26f24dd0bbdebbee1c390ec6a4633893222b89053.jpg", "table_caption": ["Table 13: Comparison at different parameter levels. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/c480350a5f1a194515d65f84ee6c528a34115e174f962ee49c58c4e388841a16.jpg", "table_caption": ["Table 14: Comparison at a uniform parameter levels. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/f3dae8f2b23baf8fe8100bdf04d849ea3a0f76b15dac8e37b483f43119a30e55.jpg", "table_caption": ["Table 15: Comparison of Inference Time. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Design of $\\mathcal{L}_{\\mathrm{grad}}$ .We further discuss the design of $\\mathcal{L}_{\\mathrm{grad}}$ and effectiveness on performance. We use minimum (min) as the baseline and compare it with using the average (avg). As shown in Tab. 16, using $L_{\\mathrm{grad}}(\\mathrm{avg})$ to calculate the similarity of query points at different time steps results in a slight increase in CD error. In contrast, $L_{\\mathrm{grad}}(\\mathrm{min})$ achieves a similar level of similarity but better performance in CD metrics. Therefore, we calculate minimum of the gradient similarities as a constraint to prevent significant deviations in the moving direction during training, making the network more sensitive to changes in gradient direction. ", "page_idx": 16}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/4d0155c5e58a619f4bab93d3c89ddaa652506302f96c9ff2941dab60ab99f9b2.jpg", "table_caption": ["Table 16: Comparison at a uniform parameter levels. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Feature Comparison. We combine MSP with the linear layers and the traditional feature learning encoder PointMLP to explore the effectiveness of MSP. We present the results of combining different feature learning networks with MSP methods in Tab. 17. We denote the single moving operation in MSP as Pull and use multiple feature learning networks as baselines. Due to the lack of an FFT module, the same features are used for multiple offsets in linear $+\\mathrm{MSP}$ and $\\mathrm{PointMLP+MSP}.$ As shown in Tab. 17, the combination of different feature extraction networks and MSP achieved better results in terms of both CD and NC metrics. We further demonstrate the effectiveness of MSP in Fig. 13 and Fig. 14 in the PDF. PointMLP / linear $+\\mathrm{MSP}$ can generate finer local details compared to PointMLP / linear $+\\mathrm{\\,Pull}$ . ", "page_idx": 16}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/eb326f905899f56e29801bb6a143f8ee540dfebebc64ab1362329fb0af51a3e1.jpg", "table_caption": ["Table 17: Comparison of Reconstruction Accuracy in $\\mathrm{CD}_{L2}\\times100$ "], "table_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/80997e008c15d4887840ce753e09ccead2ba39b5fa865ab5677d49c3e8369bae.jpg", "img_caption": ["Figure 13: Comparison of different feature encoders and MSP module on FAMOUS dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Default setting of Step.We set Step to 3 by default for two reasons: (1) MSP can bring partial accuracy improvement when learning more steps, but more network parameters are also needed. We present comparisons under different steps in Tab. 8. (2) Although deeper frequency features can represent more comprehensive information, combining frequency features from different layers can achieve better results. We denote the combination of the frequency. We conduct the experiment according to the default settings, and the results are shown in Tab. 18. ", "page_idx": 16}, {"type": "image", "img_path": "XxE8mL1bCO/tmp/8ef050b9c67df5d4dc3010e0998ce1f0f239f7376ed2dabcd045e608338c81dc.jpg", "img_caption": ["Figure 14: Comparison of different feature encoders and MSP module on FAMOUS dataset. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/1dc489f73f2a07a3590d27b101b189111caf671ede4b42a37423a2d35842fa3e.jpg", "table_caption": ["Table 18: Comparison of $\\mathrm{CD}_{L2}\\times100$ , NC under different feature combination. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Comparison Details for ShapeNet. The complete comparison under all the five scenes of the ShapeNet dataset. The results are shown in Tab. 19 to Tab .21. We use Chamfer Distance $(\\mathbf{CD}_{L1},\\mathbf{CD}_{L2})$ and NC as evaluation metrics. ", "page_idx": 17}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/14c67362513e2363fd8c28096d1092447293392662e8b836c5849520c3ab9c55.jpg", "table_caption": ["Table 19: Reconstruction accuracy on ShapeNet in terms of $\\mathrm{CD}_{L2}$ "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/e0f2031612a688c3a612c7d799db94e50b6f2b82be5ee7f6006e9b6cb9b33209.jpg", "table_caption": ["Table 20: Reconstruction accuracy on ShapeNet in terms of NC. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/1cc04b755c74eef84f2d1a5f6fd71834999195da853885d36f392ae4f452ab40.jpg", "table_caption": ["Table 21: Reconstruction accuracy on ShapeNet in terms of F- Score with thresholds of 0.002 and 0.004. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Comparison Details for 3DScene. We also provide detailed metrics for single scenes in 3DScene dataset. We evaluate it by ${\\mathrm{CD}}_{L1}$ , $\\mathrm{CD}_{L2}$ and NC. As shown in the Tab. 22, our approach achieves the best performance across all scenes. ", "page_idx": 18}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/a81ba1fa728ae6a30f5ea26d526b4798eb76b921a1fb29a4bbdb5dd707078cb6.jpg", "table_caption": ["Table 22: $\\mathrm{CD}_{L1}$ , $\\mathrm{CD}_{L2}\\times100$ and NC comparison under 3DScene. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Computational Complexity. We report our computational complexity in Tab. 23, we present numerical comparisons with the latest overfitting-based methods, including NP and PCP, using different point counts, such as 20K and 40K. The benchmark rounds for both NP and PCP are set at 40K. NP does not require learning priors, resulting in the highest operational efficiency. PCP needs to learn priors, which requires additional time. To achieve more refined results, we dedicate extra time to learning the frequency features of point clouds and computing the sampling point strides. Consequently, our speed is slower compared to NP. However, it is noteworthy that our method outperforms PCP and operates faster even without using local priors. ", "page_idx": 18}, {"type": "table", "img_path": "XxE8mL1bCO/tmp/72691c1cde82b3bc15c4412ff09ec669f67d3afcd6a45b96a546e1306399f670.jpg", "table_caption": ["Table 23: Comparison of computational complexity. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "D Limitation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We propose a method that approximates the accurate signed distance field through multi-step optimization, achieving more precise results. However, there is still room for further optimization in terms of time and computational efficiency as shown in Tab. 8 and Tab. 14. In future work, we will continue to explore how to integrate multi-resolution (such as NGLOD [16] and Instant-NGP [71]) features effectively to balance computational efficiency and accuracy. ", "page_idx": 19}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 20}, {"type": "text", "text": "Answer:[Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: Our main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 20}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: We analyze the limitations of our method in the Appendix. D ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 20}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not include theoretical results. method in the main paper. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 21}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide the detailed information in reproducing our methods in Sec. 3-4 of the main paper and the appendix. We also provide a demonstration code of our method in the supplementary materials. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 21}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide our demonstration code as a part of our supplementary materials.   \nWe will release the source code, data and instructions upon acceptance. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide the training and testing details in the experiment section (Sec.4) and the Appendix .A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [No] ", "page_idx": 22}, {"type": "text", "text": "Justification: We report the average performance as the experimental results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The computer resources needed to reproduce the experiments are provided in the Appendix .A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We discuss the applications and potential impacts of our method in the introduction and contribution sections. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 23}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [No] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We use the open-sourced datasets under their licenses. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 25}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 25}]