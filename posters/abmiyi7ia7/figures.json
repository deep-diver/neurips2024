[{"figure_path": "aBmiyi7iA7/figures/figures_7_1.jpg", "caption": "Figure 1: Acceptance rates of HMC with respect to the number of leapfrog steps L (left) and step size  \u03b5 (right) on BNNs with different activation functions. The decay in acceptance rates of sigmoid networks is much more moderate than those of ReLU-based networks.", "description": "This figure shows the results of a simulation to investigate the effects of the number of steps L and step size \u03b5 on the acceptance rate of the Hamiltonian Monte Carlo (HMC) algorithm for Bayesian neural networks (BNNs) with different activation functions (sigmoid, ReLU, and leaky ReLU). The left panel shows the acceptance rate as a function of the number of steps L for different values of \u03b5, while the right panel shows the acceptance rate as a function of \u03b5 for different values of L. The results show that the acceptance rate of sigmoid networks is much more stable and less sensitive to changes in L and \u03b5 compared to ReLU-based networks.", "section": "4.1 Synthetic Dataset"}, {"figure_path": "aBmiyi7iA7/figures/figures_8_1.jpg", "caption": "Figure 2: Efficiency of HMC with respect to acceptance rate on BNNs with different activation functions. On both synthetic and UTKFace datasets, HMC inference with sigmoid networks is more efficient than with ReLU-based networks.", "description": "This figure compares the efficiency of Hamiltonian Monte Carlo (HMC) using sigmoid and ReLU activation functions across different acceptance rates. The efficiency is measured by considering both the acceptance rate and computational cost.  Two datasets are used for the evaluation: a synthetic dataset and the UTKFace dataset. The results consistently show that HMC with sigmoid activation is far more efficient than that with ReLU activation for both datasets.", "section": "4 Experiments"}, {"figure_path": "aBmiyi7iA7/figures/figures_8_2.jpg", "caption": "Figure 3: Acceptance rate of HMC with respect to the number of model parameters on shallow and deep neural networks with different activation functions. HMC on shallow networks generally has lower acceptance rates than deep networks of the same size.", "description": "This figure shows how the acceptance rate of the Hamiltonian Monte Carlo (HMC) algorithm changes with the number of parameters in the model, for both shallow and deep neural networks.  Three different activation functions (Sigmoid, ReLU, Leaky ReLU) are compared.  The key observation is that shallower networks tend to have lower acceptance rates than deeper networks with the same number of parameters. The acceptance rate decreases significantly as the number of parameters increases, particularly for ReLU and Leaky ReLU activation functions.", "section": "4.1 Synthetic Dataset"}, {"figure_path": "aBmiyi7iA7/figures/figures_15_1.jpg", "caption": "Figure 4: Efficiency as functions of acceptance probability for symplectic integrator of second order (left, error rate O(\u03b5\u00b2)) and first order (right, error rate \u0398(\u03b5)). The y-axes of the graphs are presented up to unknown multiplicative constants and cannot be directly compared.", "description": "This figure shows the efficiency of symplectic integrators as a function of their acceptance probability.  The left panel shows the results for a second-order integrator (error rate proportional to \u03b5\u00b2), while the right panel shows results for a first-order integrator (error rate proportional to \u03b5). The y-axis represents efficiency, and the x-axis is the acceptance rate.  The vertical dashed lines mark the optimal acceptance rates.  Importantly, the y-axes are scaled arbitrarily, so direct comparison between the two plots is not possible; the key takeaway is the difference in the shapes of the curves and the location of the optimal acceptance rate.", "section": "3 Correctness and Efficiency of Leapfrog HMC on ReLU Neural Networks"}, {"figure_path": "aBmiyi7iA7/figures/figures_15_2.jpg", "caption": "Figure 4: Efficiency as functions of acceptance probability for symplectic integrator of second order (left, error rate O(\u03b5\u00b2)) and first order (right, error rate \u0398(\u03b5)). The y-axes of the graphs are presented up to unknown multiplicative constants and cannot be directly compared.", "description": "This figure shows the efficiency of symplectic integrators of the second order (left) and the first order (right) as a function of the acceptance probability. The y-axis represents efficiency, and the x-axis represents the acceptance probability. The graphs are shown up to unknown multiplicative constants, and cannot be directly compared. The red dotted lines in both graphs indicate the optimal acceptance probabilities.", "section": "Effects of dimensionality"}, {"figure_path": "aBmiyi7iA7/figures/figures_17_1.jpg", "caption": "Figure 6: The synthetic dataset used in our simulations. This dataset contains 100 data points where x ~ uniform(0, 4) and y ~ N(cos(2x), 0.12).", "description": "This figure shows a scatter plot of 100 data points generated from a cosine function with added Gaussian noise. The x-values are uniformly distributed between 0 and 4, and the y-values follow a normal distribution with a mean equal to the cosine of the corresponding x-value and a standard deviation of 0.1. This dataset is used in the paper's experiments to compare the performance of Hamiltonian Monte Carlo (HMC) on Bayesian neural networks with different activation functions.", "section": "4.1 Synthetic Dataset"}]