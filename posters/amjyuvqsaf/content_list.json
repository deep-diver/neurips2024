[{"type": "text", "text": "Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alberto Cabezas\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Louis Sharrock\u2217 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Department of Mathematics and Statistics Lancaster University, UK a.cabezasgonzalez@lancaster.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics and Statistics Lancaster University, UK l.sharrock@lancaster.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Christopher Nemeth Department of Mathematics and Statistics Lancaster University, UK c.nemeth@lancaster.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Continuous normalizing flows (CNFs) learn the probability path between a reference distribution and a target distribution by modeling the vector field generating said path using neural networks. Recently, Lipman et al. [45] introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM). In this paper, we repurpose this method for probabilistic inference by incorporating Markovian sampling methods in evaluating the FM objective, and using the learned CNF to improve Monte Carlo sampling. Specifically, we propose an adaptive Markov chain Monte Carlo (MCMC) algorithm, which combines a local Markov transition kernel with a non-local, flow-informed transition kernel, defined using a CNF. This CNF is adapted on-the-fly using samples from the Markov chain, which are used to specify the probability path for the FM objective. Our method also includes an adaptive tempering mechanism that allows the discovery of multiple modes in the target distribution. Under mild assumptions, we establish convergence of our method to a local optimum of the FM objective. We then benchmark our approach on several synthetic and real-world examples, achieving similar performance to other state-of-the-art methods, but often at a significantly lower computational cost. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The task of sampling from a probability distribution known only up to a normalization constant is a fundamental problem arising in a wide variety of fields, including statistical physics [51], Bayesian inference [25], and molecular dynamics [43]. In particular, let $\\pi(\\mathrm{d}x)$ be a target probability distribution on $\\mathbb{R}^{d}$ with density $\\pi(x)$ with respect to the Lebesgue measure of the form1 ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\pi(x)={\\frac{{\\hat{\\pi}}(x)}{Z}},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\hat{\\pi}:\\mathbb{R}^{d}\\to\\mathbb{R}_{+}$ is a continuously differentiable function which can be evaluated pointwise, and $\\textstyle Z=\\int_{\\mathbb{R}^{d}}{\\hat{\\pi}}(x)\\mathrm{d}x$ is an unknown normalizing constant. We are interested in generating samples from the target distribution $\\pi$ in order to approximate integrals of the form $\\pi[f]=\\mathbb{E}_{\\pi}[f(\\bar{x})]$ , where $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ . ", "page_idx": 0}, {"type": "text", "text": "A standard solution to this problem is Markov chain Monte Carlo (MCMC) [12, 64], which relies on the construction of a Markov process which admits the target $\\pi$ as its invariant distribution. One of the most broadly applicable and widely studied MCMC methods is the Metropolis-Hastings (MH) algorithm [32], which proceeds in two steps. First, given a current sample $x$ , a new sample $y$ riosb parboilpitoys .l  Tdihsitsr isbtruatiteogn $q(\\cdot|x)$ r.a tTehs ean ,M tahrisk osva cmhpalien  isw iatchc tehpet edde swiriethd $\\begin{array}{r}{\\alpha(x,y)=\\overset{\\smile}{\\operatorname*{min}}\\left\\{1,\\frac{\\pi(y)q(x|y)}{\\pi(x)q(y|x)}\\right\\}}\\end{array}$ stationary distribution and, under mild conditions on the proposal and the target, also ensures that the Markov chain is ergodic [65]. However, for high-dimensional, multi-modal settings, such methods can easily get stuck in local modes, and suffer from very slow mixing times [e.g., 48]. ", "page_idx": 1}, {"type": "text", "text": "Naturally, the choice of proposal distribution $q(\\cdot|x)$ is critical to ensuring that MH MCMC algorithms explore the target distribution within a reasonable number of iterations. A key goal is to obtain proposal distributions with fast mixing times, which can be applied generically to any target distribution. This is particularly challenging in the face of complex, multi-modal (or metastable) distributions, which commonly arise in applications such as genetics [38], protein folding [41], astrophysics [22], and sensor network localization [37]. On the one hand, local proposals, such as those employed in the Metropolis-Adjusted Langevin Algorithm (MALA) [66] or Hamiltonian Monte Carlo (HMC) [20, 56] struggle to transition between regions of high-probability, resulting in very long decorrelation times and few effective independent samples [e.g., 49]. On the other hand, global proposal distributions must be very carefully designed in order to avoid high rejection rates, particularly in high dimensions [17, 47]. ", "page_idx": 1}, {"type": "text", "text": "Another popular approach to sampling is variational inference (VI) [10, 34, 61, 79], which obtains a parametric approximation $\\pi_{\\theta^{*}}(\\bar{x})\\approx\\pi(x)$ to the target by minimising the Kullback-Leibler (KL) divergence to the target over a parameterized family of distributions $\\mathcal{D}_{\\theta}=\\{\\pi_{\\theta}:\\theta\\in\\Theta\\}$ . State-of-theart VI methods use normalizing flows (NFs), which consist of a sequence of invertible transformations between a reference and a target distribution, to define a flexible variational family [62]. There has also been growing interest in the use of continuous normalizing flows (CNFs), which define a path between distributions using ordinary differential equations [15, 27, 45]. CNFs avoid the need for strong constraints on the flow but, until recently, have been hampered by expensive maximum likelihood training. ", "page_idx": 1}, {"type": "text", "text": "In recent years, several works have sought hybrid methods which utilize NFs to enhance the performance of MCMC algorithms; see, e.g., [28] for a recent review. For example, NFs have been successfully used to precondition complex Bayesian posteriors, significantly improving the performance of existing MCMC methods [e.g., 33, 39, 59, 68]. The synergy between local MCMC proposals and global, flow-informed proposals has also been explored, leading to enhanced mixing rates and effective estimation of multimodal targets [e.g., 24, 67]. ", "page_idx": 1}, {"type": "text", "text": "Our contributions In this paper, we continue this promising line of work, introducing a new probabilistic inference scheme which integrates CNFs with MCMC sampling techniques. Our approach utilizes flow matching (FM), a scalable, simulation-free training objective for CNFs recently introduced by Lipman et al. [45]. This enables, for the first time, the incorporation of CNFs into an adaptive MCMC algorithm. Concretely, our approach augments a local, gradient-based Markov transition kernel with a non-local, flow-informed transition kernel, defined using a CNF. This CNF, and the corresponding transition kernel, are adapted on-the-fly using samples from the chain, which are used to define the probability path for the FM objective. Our scheme also includes an adaptive tempering mechanism, which is essential for discovering multiple modes in complex target distributions. Under mild assumptions, we establish that the flow-network parameters output by our method converge to a local optimum of the FM objective. We then demonstrate empirically the performance of our approach on several synthetic and real-world examples, illustrating comparable or superior performance to other state-of-the-art sampling methods. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Continuous Normalizing Flows A continuous normalizing flow (CNF) is a continuous-time generative model which is trained to map samples from a base distribution $p_{0}$ to a given target distribution [15]. Let $v_{t}$ be a time-dependent vector field that runs continuously in the unit interval. Under mild conditions, this vector field can be used to construct a time-dependent diffeomorphic map called a flow $\\phi:[0,1]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ , defined via the ordinary differential equation (ODE): ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\phi_{t}(x)=v_{t}(\\phi_{t}(x)),\\quad\\phi_{0}(x)=x.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Given a reference density $p_{0}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}$ , and the flow $\\phi$ , we can generate a probability density path $p:[0,1]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}_{+}$ as the pushforward of $p_{0}$ under $\\phi$ , viz $p_{t}:=\\overline{{[\\phi_{t}]_{\\sharp}}}p_{0}$ , for $t\\in[0,1]$ . This yields, via the instantaneous change-of-variables formula [e.g., 14] ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\log p_{t}(x_{t})=\\log p_{0}(x)-\\int_{0}^{t}\\nabla\\cdot\\boldsymbol{v}_{s}(x_{s})\\mathrm{d}s,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $x_{s}:=\\phi_{s}(x)$ , and where $\\nabla$ is the divergence operator, i.e. the trace of the Jacobian matrix. In modern applications, the vector field $v_{t}$ is often parameterized using a neural network $v_{t}^{\\theta}$ , in which case the ODE in (2) is referred to as a neural ODE [15]. In turn, this yields a deep parametric model $\\phi_{t}^{\\theta}$ for the flow $\\phi_{t}$ , known as a CNF [27]. ", "page_idx": 2}, {"type": "text", "text": "Flow Matching One would typically like to learn a CNF which maps between a given reference density $p_{0}$ and a target density $\\pi$ . Given samples from the target, one approach is to maximize the log-likelihood $\\mathbb{E}_{x\\sim\\pi}\\,\\big[\\log p_{1}^{\\theta}(\\bar{x})\\big]$ . In practice, however, maximum likelihood training is very slow as both sampling and likelihood evaluation require multiple network passes to solve the ODE in (2). ", "page_idx": 2}, {"type": "text", "text": "Flow Matching (FM) provides an alternative, simulation-free method for training CNFs [45]. Let $p_{t}(\\boldsymbol{x})$ be a target probability density path such that $p_{0}=p$ is a simple reference distribution, and $p_{1}\\approx\\pi$ is approximately equal to the target distribution. Let $\\boldsymbol{v}_{t}(\\boldsymbol{x})$ be a vector field which generates this $p_{t}(\\boldsymbol{x})$ . Then the FM objective for the CNF vector field $v_{t}^{\\theta}(\\stackrel{.}{x})$ is defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\theta;\\pi)=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1)}\\mathbb{E}_{x\\sim p_{t}}\\left[\\|v_{t}^{\\theta}(x)-v_{t}(x)\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "In practice, we do not have direct access to the target vector field, $v_{t}(x)$ , and so we cannot minimize (4) directly. However, as shown in Lipman et al. [45, Theorem 2], it is equivalent to minimize the conditional flow-matching (CFM) loss ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\theta;\\pi)=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1)}\\mathbb{E}_{x_{1}\\sim\\pi}\\mathbb{E}_{x\\sim p_{t}(\\cdot|x_{1})}\\left[||v_{t}^{\\theta}(x)-v_{t}(x|x_{1})||_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $p_{t}(\\cdot|x_{1})$ is a conditional probability density path satisfying $p_{0}(x|x_{1})=p_{0}$ and $p_{1}(x|x_{1})\\approx\\delta_{x_{1}}$ , and $v_{t}(\\cdot|x_{1}):\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ is a conditional vector field that generates $p_{t}(\\cdot|x_{1})$ . There are various choices for $p_{t}(\\cdot|x_{1})$ and $\\ v_{t}(\\cdot|x_{1})$ . For simplicity, we here assume that the conditional probability path is Gaussian, viz $p_{t}(x|x_{1})=\\mathcal{N}(x|m_{t}(x_{1}),s_{t}(x_{1})^{2}\\mathbb{I}_{d})$ , where $m:[0,1]\\times\\mathbb{R}^{d}\\to\\mathbb{R}^{d}$ denotes a timedependent mean, and $s:[0,1]\\times\\mathbb{R}\\to\\mathbb{R}_{+}$ a time-dependent scalar standard deviation. For our experiments, we further adopt the optimal transport conditional probability path introduced in [45], setting $m_{t}(x_{1})=t x_{1}$ and $s_{t}(x_{1})=1-(1-\\sigma_{\\operatorname*{min}})t$ for some $\\sigma_{\\mathrm{min}}\\ll1$ . In this case, the conditional vector field assumes the particularly simple form $\\begin{array}{r}{\\stackrel{\\prime}{v}_{t}(x|x_{1})=\\frac{x_{1}-(1-\\sigma_{\\operatorname*{min}})x}{1-(1-\\sigma_{\\operatorname*{min}})t}}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "3 Markovian Flow Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present our main contribution, an adaptive MCMC algorithm which combines a non-local, flow-informed transition kernel trained via FM; a local, gradient-based Markov transition kernel; and an adaptive annealing schedule. We begin by describing how CNFs can be used within a MH MCMC algorithm. ", "page_idx": 2}, {"type": "text", "text": "3.1 MCMC with Flow Matching ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Suppose, for now, that we have access to a CNF $(\\phi_{t}^{\\theta})_{t\\in[0,1]}$ , trained (e.g.) via flow-matching, with corresponding vector field $(v_{t}^{\\theta})_{t\\in[0,1]}$ , which generates a probability path $(p_{t}^{\\theta})_{t\\in[0,1]}$ between a reference density $p_{0}$ and an approximation of the target density $\\pi$ . Given a point $\\bar{x}_{0}\\in\\mathbb{R}^{d}$ on the reference space, we can evaluate the log-density of the pullback of the target distribution $\\pi$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\log[\\phi_{1}^{\\theta}]^{\\sharp}\\pi(x_{0})=\\log\\pi(\\phi_{1}^{\\theta}(x_{0}))-\\int_{1}^{0}\\nabla\\cdot v_{t}^{\\theta}(\\phi_{t}^{\\theta}(x_{0}))\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Under the assumption that the CNF approximately transports samples from $p_{0}$ to $\\pi$ , we expect that $[\\phi_{1}^{\\theta}]_{\\sharp}p_{0}\\approx\\pi$ in the target space, and that $[\\phi_{1}^{\\theta}]^{\\sharp}\\pi\\approx\\dot{p}_{0}$ in the reference space. Given that the reference distribution $p_{0}$ is chosen such that it is easy to sample from, this suggests the following strategy, sometimes referred to as neural trasport MCMC or neutraMCMC [28, 33, 44, 59]. First, transform initial positions $x_{1}$ from the target space to the reference space by solving ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\begin{array}{c c c}{\\displaystyle x_{0}}\\\\ {\\log p_{1}^{\\theta}(x_{1})-\\log p_{0}(x_{0})\\right]=\\left[\\begin{array}{c c c}{\\displaystyle x_{1}}\\\\ {0}\\end{array}\\right]+\\int_{1}^{0}\\left[\\begin{array}{c c c}{\\displaystyle v_{t}^{\\theta}(x_{t})}\\\\ {-\\nabla\\cdot v_{t}^{\\theta}(x_{t})\\right]\\mathrm{d}t,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which integrates the combined dynamics of $x_{t}$ and the log-density of the sample backwards in time. Then, generate MCMC proposals $y_{0}$ in the reference space using any standard MCMC scheme which targets the pullback of the target distribution, as defined in (6). Finally, transform accepted proposals back to target space using the forward dynamics, viz ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left[\\underset{\\log p_{1}^{\\theta}(y_{1})\\mathrm{~-~}\\log p_{0}(y_{0})}{y_{1}}\\right]=\\left[\\underset{0}{y_{0}}\\right]+\\int_{0}^{1}\\left[\\underset{-\\nabla\\cdot\\,v_{t}^{\\theta}(y_{t})}{v_{t}^{\\theta}(y_{t})}\\right]\\mathrm{d}t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This corresponds to using a transformation-informed proposal in a Markov transition step, an approach which has been successfully applied using (discrete) normalizing flows [28, 33, 44, 59]. ", "page_idx": 3}, {"type": "text", "text": "There are various possible choices for the proposal distribution on the reference space (see Appendix A). For example, [23] consider an independent MH (IMH) proposal, where i.i.d. samples are drawn from the reference distribution. Here we focus on a flow-informed random-walk, motivated largely by its superior empirical performance in numerical experiments. This proposal performs particularly well on high-dimensional problems, where overfitting of the CNF can be corrected with stochastic steps, while exacerbated by independent proposals [39]. Concretely, our flow-informed random-walk transition kernel, summarized in Algorithm 2 (see Appendix A), can be written as ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(x,\\mathrm{d}y;\\pi,\\theta)=\\alpha(x,y)\\rho_{\\theta}(\\mathrm{d}y\\vert x)+(1-b(x))\\delta_{x}(\\mathrm{d}y),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\rho_{\\theta}(\\mathrm{d}y|x)$ is the distribution defined by the transition ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{0}=x+\\int_{1}^{0}v_{t}^{\\theta}(\\phi_{t}^{\\theta}(x))\\mathrm{d}t,\\quad y_{0}\\sim\\mathcal{N}(x_{0},\\sigma_{\\mathrm{opt}}^{2}),\\quad y=y_{0}+\\int_{0}^{1}v_{t}^{\\theta}(\\phi_{t}^{\\theta}(y_{0}))\\mathrm{d}t,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Training the CNF Thus far, we have assumed that it is possible to train a CNF which maps samples from the reference distribution $p_{0}$ to (an approximation of) the target distribution $\\pi$ . Clearly, however, the CFM objective is not immediately applicable in our setting, since we do not have access to samples from the target $\\pi$ . ", "page_idx": 3}, {"type": "text", "text": "Tong et al. [72] propose two alternatives in this case: (i) use an importance sampling reweighted objective function, or (ii) use samples from a long-run MCMC algorithm (e.g., MALA) as approximate target samples. Both of these approaches, however, have limitations. The former is unlikely to succeed when the proposal distribution differs significantly from the target distribution, while the latter will only perform well when the chosen MCMC method mixes well. ", "page_idx": 3}, {"type": "text", "text": "In this paper, we adopt a different approach, updating the parameters of a CNF based on a dynamic estimate of the CFM objective obtained via an adaptive MCMC algorithm. This is similar in spirit to other recent flow-informed MCMC algorithms [24, 35, 67], and the Markovian score climbing algorithm in [54]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Adaptive MCMC with Flow Matching ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview Our adaptive MCMC scheme combines a non-local, flow-informed transition kernel (e.g., a flow informed random-walk) and a local transition kernel (e.g., MALA), which generate new samples from a sequence of annealed target distributions. These new samples are used to define a new estimate of the CFM objective in (5), which is optimized to define a new CNF. These steps are repeated until the samples converge in distribution to the target $\\pi$ , and the flow-network parameters converge to a local minima of the flow matching objective (see Proposition 3.1). This scheme, which we refer to as Markovian Flow Matching (MFM), is summarized in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "Sampling There is significant freedom regarding the choice of both the local and the non-local MCMC algorithms. In our experiments, we adopt the Metropolis-Adjusted Langevin Algorithm (MALA) as the local algorithm. Thus, the local Markov kernel $Q$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ(x,\\mathrm{d}y;\\pi)=\\alpha(x,y)q(\\mathrm{d}y|x)+(1-b(x))\\delta_{x}(\\mathrm{d}y),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $q(\\mathrm{d}y|x)$ is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(\\mathrm{d}y|x)\\propto\\exp\\left(-\\frac{1}{4\\tau}\\|y-x-\\tau\\nabla\\log\\pi(x)\\|^{2}\\right)\\mathrm{d}y,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and where, as elsewhere, $\\begin{array}{r}{\\alpha(x,y)=\\underset{\\mathbf{\\ell}}{\\mathrm{min}}\\left\\lbrace1,\\frac{\\pi(y)q(x|y)}{\\pi(x)q(y|x)}\\right\\rbrace}\\end{array}$ and $\\begin{array}{r}{b(x)=\\int_{\\mathbb{R}^{d}}\\alpha(x,y)q(\\mathrm{d}y|x)}\\end{array}$ . In principle, however, other choices such as HMC could also be used. ", "page_idx": 4}, {"type": "text", "text": "Meanwhile, for the non-local MCMC algorithm, we adopt the flow-informed random walk with nonlocal Markov kernel $P$ defined in (9). Together, assuming alternate local and non-local steps, these two kernels define a Markov chain with Markov transition kernel $R:=P\\circ Q$ , given explicitly by $\\begin{array}{r}{R(x,\\mathrm{d}y;\\pi,\\theta)=\\int_{\\mathcal{Z}}Q(x,\\mathrm{d}z;\\pi)P(z,\\mathrm{d}y;\\pi,\\theta)}\\end{array}$ . In practice, the balance between local and non-local moves is controlled by the hyperparameter $k_{Q}$ , which sets the number of local steps before a global step. ", "page_idx": 4}, {"type": "text", "text": "Training Following each MCMC step, the parameters of the flow-informed Markov transition kernel $P$ are updated based on a new estimate of ${\\mathcal{I}}(\\theta;\\pi)$ . To be precise, suppose we write $\\mu_{t}:=\\mu_{0}R^{k}(\\cdot,\\cdot;\\bar{\\pi},\\theta)$ for the distribution of the Markov chain with kernel $R(\\cdot,\\cdot;\\pi,\\theta)$ after $k\\in\\mathbb{N}$ steps, starting from initialization $\\mu_{0}$ , where $R^{k}=R\\circ R\\cdot\\cdot\\circ R$ . Our objective function is then given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\theta;\\mu_{k})=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1)}\\mathbb{E}_{x_{1}\\sim\\mu_{k}}\\mathbb{E}_{x\\sim p_{t}(\\cdot|x_{1})}\\left[||v_{t}^{\\theta}(x)-v_{t}(x|x_{1})||_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For our choice of conditional probability path (i.e., the optimal transport path), we can in fact rewrite this objective as [45, Section 4.1] ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}(\\theta;\\mu_{k},\\sigma_{\\operatorname*{min}})=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1)}\\mathbb{E}_{x_{1}\\sim\\mu_{k}}\\mathbb{E}_{x_{0}\\sim p_{0}}\\left[||v_{t}^{\\theta}(\\phi_{t}(x_{0}|x_{1}))-v_{t}(\\phi_{t}(x_{0}|x_{1})|x_{1})||_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where vt(x|x1) = x11\u2212\u2212((11\u2212\u2212\u03c3\u03c3min))tx and $\\phi_{t}(x|x_{1})\\,=\\,(1\\,-\\,(1\\,-\\,\\sigma_{\\mathrm{min}}t)x+t x_{1}$ . In practice, we will optimize a Monte Carlo estimate of this objective, namely, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\theta;\\{x^{i}(k)\\}_{i=1}^{N},\\sigma_{\\operatorname*{min}})=\\frac{1}{N}\\sum_{i=1}^{N}\\big|\\big|v_{t_{i}}^{\\theta}(\\phi_{t_{i}}(x_{0}^{i}|x^{i}(k)))-v_{t_{i}}(\\phi_{t_{i}}(x_{0}^{i}|x^{i}(k))|x^{i}(k))\\big|\\big|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "whie.ri.ed $\\{x^{i}(k)\\}_{i=1}^{N}$ are the samples from $N$ chains of our MCMC algorithm after $k\\in\\mathbb{N}$ iterations, $x_{0}^{i}\\overset{1.1.\\mathrm{d.}}{\\sim}p_{0}$ , and $\\overline{{t_{i}}}\\sim\\mathcal{U}(0,1)$ . The use of $N$ particles allow the state\u2019s mutation to $N$ computing cores running in parallel at each iteration. Sampling steps can be run in parallel using modern vectororiented libraries, before each particle is used to approximate the loss and update the parameters. Thus, the speedup gained by using more than one core scales linearly with the number of cores as long as there are as many cores as there are particles. ", "page_idx": 4}, {"type": "text", "text": "Annealing For complex (e.g., multimodal) target distributions, it can be challenging to learn a CNF that successfully maps between the reference $p_{0}$ and the target $\\pi$ . For example, if the locations of the modes of the target are not known a priori, and the MCMC chains are initialized far from one or more of the modes, it is unlikely that the local MCMC kernel, and therefore the trained flow, will ever discover these modes [e.g., 24, Section IV.C]. To alleviate this problem, one approach is to iteratively target a sequence of annealed densities $\\{\\pi_{k}(x)\\}_{k=0:K}$ , which smoothly interpolate between a simple base distribution $\\pi_{0}(x)$ (e.g., a standard Gaussian), and the target distribution $\\pi_{K}(x):=\\pi(x)$ . This idea is central to other Monte Carlo sampling methods such as Sequential Monte Carlo (SMC) [18] and Annealed Importance Sampling (AIS) [55], as well as sampling methods used in score-based generative modelling [e.g., 70]. In our case, the annealed targets act as intermediary steps within the flow-informed MCMC scheme. ", "page_idx": 4}, {"type": "text", "text": "A standard way in which to construct the sequence $\\{\\pi_{k}(x)\\}_{k=0:K}$ is to use a geometric interpolation, defining ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{k}(x)=\\pi_{K}(x)^{{\\beta}_{k}}\\pi_{0}(x)^{1-{\\beta}_{k}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "1: Input: target $\\pi$ , base $\\pi_{0}$ , vector field $v_{t}^{\\theta}$ , reference $p_{0}$ , concentration $\\sigma_{m i n}$ , initial parameters $\\theta_{0}$ ,   \ntarget ESS fraction $\\alpha$ , iterations $K$ , number of particles $N$ , local kernel $Q$ , flow-informed kernel   \n$P$ , MCMC steps per resampling step $k_{Q}$ , step sizes $\\varepsilon_{1:K}$ .   \n2: Output: flow-network parameters $\\theta_{K}$   \n3: Sample $x_{0}^{i}\\sim\\pi_{0}$ for $i=1,\\ldots,N$ (initialize samples)   \n4: for $k=1:K$ do   \n5: if $\\beta_{k-1}<1$ then   \n6: $\\beta_{k}=$ solve (17) for $\\beta_{k-1}<\\beta\\leq1$ (update annealing temperature)   \n7: $\\pi_{k}=\\mathrm{solve}$ (16) (update annealing density)   \n8: end if   \n9: if $k$ mod $k_{Q}+1=0$ then   \n10: $x_{k}^{i}\\sim P(x_{k-1}^{i},\\cdot;\\pi_{k},\\theta_{k-1})$ for $i=1,\\ldots,N$ (flow-informed Markov transition).   \n11: else   \n12: $x_{k}^{i}\\sim Q(x_{k-1}^{i},\\cdot;\\pi_{k})$ for $i=1,\\ldots,N$ (local Markov transition).   \n13: end if   \n14: $\\theta_{k}=\\theta_{k-1}+\\varepsilon_{k}\\nabla_{\\theta}\\mathcal{I}(\\theta_{k-1}|\\{x_{k}^{i}\\}_{i=1}^{N},\\sigma_{\\operatorname*{min}})$ (update flow-network parameters)   \n15: end for ", "page_idx": 5}, {"type": "text", "text": "where $\\beta_{0:K}$ is a sequence of temperatures which satisfies $0=\\beta_{0}<\\beta_{1}<\\cdot\\cdot<\\beta_{K}=1$ [e.g., 55]. In practice, it can be difficult to choose a good sequence of temperatures that provides a smooth transition between densities. One heuristic for adaptively setting this sequence is based on the effective sample size (ESS). In particular, by setting the ESS to a user-specified percentage $\\alpha$ of the number of particles $N$ , the next temperature $\\beta_{k}$ in the schedule can be determined by solving the recursive equation [9] ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\beta_{k}=\\operatorname*{inf}{\\biggl\\{}\\beta_{k-1}<\\beta\\leq1:{\\frac{\\left[{\\frac{1}{N}}\\sum_{i=1}^{N}w_{i}^{\\beta_{k-1}}(\\beta)\\right]^{2}}{{\\frac{1}{N}}\\sum_{i=1}^{N}w_{i}^{\\beta_{k-1}}(\\beta)^{2}}}=\\alpha{\\biggr\\}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $w_{i}^{\\beta_{k-1}}(\\beta)=\\left[\\pi_{K}(x^{i})^{\\beta}\\pi_{0}(x^{i})^{1-\\beta}\\right]/\\left[\\pi_{K}(x^{i})^{\\beta_{k-1}}\\pi_{0}(x^{i})^{1-\\beta_{k-1}}\\right]=[\\pi_{K}(x^{i})/\\pi_{0}(x^{i})]^{\\beta-\\beta_{k-1}}$ are new importance weights given the current temperature $\\beta_{k-1}$ . In practice, we find that the inclusion of this adaptive tempering scheme is essential in the presence of highly multimodal target distributions, enabling the discovery of modes which are not known a priori. ", "page_idx": 5}, {"type": "text", "text": "Convergence The output of Algorithm 1 is a vector of parameters $\\theta_{K}$ which defines a CNF $(\\phi_{t}^{\\theta_{K}})_{t\\in[0,1]}$ . Unde\u2217r the assumption that the parameter estimate converges, that is, $\\theta_{K}\\to\\theta_{\\mathrm{global}}^{*}$ as $K\\rightarrow\\infty$ , where \u03b8global = arg min\u03b8\u2208\u0398 J (\u03b8; \u03c0) is the global minimizer of the CFM objective J (\u03b8; \u03c0) in (5), this CNF is guaranteed to generate a probability path $(p_{t}^{\\theta})_{t\\in[0,1]}$ which transports samples from the reference $p_{0}$ to the true target $\\pi$ [e.g., 45]. ", "page_idx": 5}, {"type": "text", "text": "In practice, the objective ${\\mathcal{I}}(\\theta;\\pi)$ is highly non-convex, and thus it is not possible to establish a convergence result of this type without imposing unreasonably strong assumptions on the vector field $(v_{t}^{\\theta})_{t\\in[0,1]}$ . This being said, it is reasonable to ask whether $\\theta_{K}$ converges to a local optimum of the CFM objective. We now answer this question in the affirmative. In particular, under mild regularity conditions, Proposition 3.1 guarantees that $\\theta_{K}\\to\\theta^{*}$ almost surely as $K\\rightarrow\\infty$ , where $\\theta^{*}$ denotes a local minimum of the CFM objective. This proposition closely mirrors [54, Proposition 1]. Its proof, which relies on a classical result in [6, Theorem 3.17], is provided in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.1. Assume that Assumptions $B.I\\ -\\ B.6$ hold (see Appendix $B$ ). Assume also that $(\\theta_{K})_{K\\in\\mathbb{N}}$ is a bounded sequence, which almost surely visits a compact subset of the domain of attraction of $\\theta^{*}$ infinitely often. Then $\\theta_{K}\\to\\theta^{*}$ almost surely. ", "page_idx": 5}, {"type": "text", "text": "4 Related work ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In recent years, a number of works have proposed algorithms which combine MCMC techniques with NFs; see, e.g., [2, 28] for recent surveys. Broadly speaking, these algorithms fall into two distinct categories. NeutraMCMC methods leverage NFs as reparameterization maps which simplify the geometry of the target distribution, before running (local) MCMC samplers in the latent space. This technique was pioneered in the seminal paper [59], and since been investigated in a number of different works [e.g., 13, 33, 44, 54, 58, 68, 82, 84]. Flow MCMC methods, meanwhile, utilize the pushforward of the base distribution through the NF as an (independent) proposal within an MCMC scheme. This approach was first studied by [3], and further extended in [4, 31, 57]. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "More recently, [24, 67] have introduced adaptive MCMC schemes which combine local MCMC samplers (e.g., MALA or HMC), with a non-local, flow-informed proposal (IMH or i-SIR); see also [35]. Our algorithm combines aspects of both neutraMCMC and flow MCMC methods and, unlike any existing approach, make use of a CNF (as opposed to a discrete NF), by leveraging the conditional flow matching objective. The use of NFs within other Monte Carlo algorithms has also been the subject of recent interest. For example, [5, 50] consider augmenting SMC with NFs, while [19, 52] use NFs (or diffusion models) within AIS. ", "page_idx": 6}, {"type": "text", "text": "Although less directly comparable to our own approach, several other recent works have proposed to use (controlled) diffusion processes to sample from unnormalized probability distributions. Such works include Zhang and Chen [85], who introduce the path integral sampler, Vargas et al. [75], who propose the denoising diffusion sampler, and Zhang et al. [83], who introduce generative flow samplers. Some other relevant contributions in this direction include [1, 8, 16, 60, 63, 69, 73, 74, 75, 76, 77, 78]. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we evaluate the performance of MFM (Algorithm 1) on two synthetic and two real data examples. Our method is benchmarked against four relevant methods. The Denoising Diffusion Sampler [DDS; 75] is a VI method which approximates the reversed diffusion process from a reference distribution to an extended target distribution by minimizing the KL divergence. Adaptive Monte Carlo with Normalizing Flows [NF-MCMC; 24] is an augmented MCMC scheme which uses a mixture of MALA and adaptive transition kernels learned using discrete NFs. Flow Annealed Importance Sampling Bootstrap [FAB; 52] is an augmented AIS scheme minimizing the mass-covering $\\alpha$ -divergence with $\\alpha=2$ . Finally, Adaptive Tempered SMC (AT-SMC), i.e. the SMC algorithm described in [18] using a MALA transition kernel and a sequence of annealed distributions chosen adaptively by solving (17). ", "page_idx": 6}, {"type": "text", "text": "For each experiment, all MALA kernels use the same step size, targeting an acceptance rate of close to 1 since we estimate expectations, e.g. in (14), using the current ensemble of particles, rather than a single long chain. Following [85], we parameterize the vector field as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{NN}^{*}(t;\\theta_{3})v_{t}^{\\theta}(x)=\\mathbf{NN}(x,t;\\theta_{1})+\\mathbf{NN}(t;\\theta_{2})\\times\\nabla\\log\\pi(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where the neural networks are standard MLPs with 2 hidden layers, using a Fourier feature augmentation for $t$ [71], and where $\\mathbf{NN}^{*}$ outputs a real value that reweights the vector field output using the time component. This architecture is also used by DDS [75, Section 4]. Meanwhile, FAB and NF-MCMC use rational quadratic splines [21]. Flows are trained using Adam [40] with a linear decay schedule terminating at $\\varepsilon_{K}\\,=\\,0$ . We report results for all methods averaged over 10 independent runs with varying random seeds. Code to reproduce the experiments is provided at https://github.com/albcab/mfm. ", "page_idx": 6}, {"type": "text", "text": "5.1 4-mode Gaussian mixture ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our first example is a mixture of four Gaussians, evenly spaced and equally weighted, in twodimensional space. The four mixture components have means $(8,8)$ , $(-8,8)$ , $(8,-8)$ , $(-8,-8)$ , and all have identity covariance. This ensures that the modes are sufficiently separated to mean that jumping between modes requires trajectories over sets with close to null probability. Given the synthetic nature of the problem, we can measure approximation quality using the Maximum Mean Discrepancy (MMD) [e.g., 29]; see Appendix C.1 for details. We can also include, as a benchmark, the results for an approximation learned using FM with true target samples. Diagnostics for all models are presented in Table 1, and learned flow samples in Figure 1. Further algorithmic details and results are provided in Appendix C.2. ", "page_idx": 6}, {"type": "text", "text": "In this experiment, only our method (Figure 1a) and DDS (Figure 1c) learn the fully separated modes, reflecting the greater expressivity of CNFs in comparison to the discrete NFs used in, e.g., NF-MCMC (Figure 1d). It is worth noting that DDS provides a closer approximation to the real target than MFM and, notably, even FM trained using true target samples (top row). Given that both methods use the same network architecture but a different learning objective, this suggests a potential limitation with the FM objective, at least when using this network architecture. This being said, MFM is notably more efficient than DDS (as well as the other methods) in terms of total computation time. While this is not a critical consideration in this synthetic, low-dimensional setting, it is a significant advantage of MFM in higher-dimensional settings involving real data (e.g., Section 5.3 and Section 5.4). ", "page_idx": 6}, {"type": "table", "img_path": "amJyuVqSaf/tmp/c14e9055e7c166d96e25b0a43256f7fce4de94b9faf50754c57adc3e024f6b83.jpg", "table_caption": [], "table_footnote": ["Table 1: Diagnostics for the two synthetic examples. MMD is the Maximum Mean Discrepancy between real samples from the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "amJyuVqSaf/tmp/606e442dafe9e86cafd77cf38d9a2177756f7e90df7080bed99cac4646424078.jpg", "img_caption": ["Figure 1: Comparison between MFM, FAB, DDS, and NF-MCMC. Samples from the target density for the 4-mode Gaussian mixture example. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.2 16-mode Gaussian mixture ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The second experiment is a mixture of bivariate Gaussians with 16 mixture components. This is a modification of the 4-mode example, with contrasting qualities that illustrate other characteristics of each of the presented methods. In this case, the modes are evenly distributed on $[-16,16]^{2}$ , with random log-normal variances. The number of modes reduces the size of sets of (near) null probability between the modes, making jumping between them easier. To increase the difficulty of this model, all methods are initialized on a concentrated region of the sampling space. Diagnostics are presented in Table 1 and learned flow samples in Figure 2. Further details are provided in Appendix C.3. ", "page_idx": 7}, {"type": "text", "text": "In this example, DDS collapses to the modes closest to the initial positions while our method captures the whole target. Since the modes are no longer separated by areas of near-zero probability, the discrete NF methods are now able to accurately capture the target density. In this case, FAB marginally outperforms MFM as measured by the MMD, but this slight improvement in performance comes at the cost of a much higher run-time. ", "page_idx": 7}, {"type": "text", "text": "5.3 Field system ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our first real-world example considers the stochastic Allen\u2013Cahn model [7], used as a benchmark in [24], and described in Appendix C.5. This fundamental reaction-diffusion equation is central to the study of phase transitions in condensed matter systems. Incorporating random forcing terms or thermal fluctuations allows for a stochastic treatment of the dynamics, capturing the inherent randomness and uncertainties in physical systems. This model leads to a discretized target density ", "page_idx": 7}, {"type": "image", "img_path": "amJyuVqSaf/tmp/aa90c7d083a302994bff12adcdcc01efea71e1cb48cb4c8b76731d4db03174fc.jpg", "img_caption": ["Figure 2: Comparison between MFM, FAB, DDS, and NF-MCMC. Samples from the target density for the 16-mode Gaussian mixture example. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "which takes the form ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\log\\pi(x)=-\\beta\\bigg(\\frac{a}{2\\Delta s}\\sum_{i=1}^{d+1}(x_{i}-x_{i-1})^{2}+\\frac{b\\Delta s}{4}\\sum_{i=1}^{d}(1-x_{i}^{2})^{2}\\bigg),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with $\\Delta s\\ =\\ {\\textstyle{\\frac{1}{d}}}$ , and boundary conditions $x_{0}\\,=\\,x_{d+1}\\,=\\,0$ . In our experiments, we take $d\\,=\\,64$ Meanwhile, following [24], other parameter values are chosen to ensure bimodality at $x\\,=\\,\\pm1$ : $a=0.1$ , $b=1/a=10$ , and $\\beta=20$ . The bimodality induced by the two global minima complicates mixing when using traditional MCMC updates. Learning the global geometry of the target and using that information to propose transitions facilitates movement between modes. Unlike previous work [e.g., 24], we deliberately choose not to employ an informed base measure. Instead, we opt for a standard Gaussian with no additional information, making the problem significantly more challenging. This choice illustrates the robustness of our approach. ", "page_idx": 8}, {"type": "text", "text": "Numerical diagnostics for each method are presented in Table 2. In this case, we use the Kernelized Stein Discrepancy (KSD) as a measure of sample quality [e.g., 26, 46]; see Appendix C.1 for details. While this is not a perfect metric, it does allow us to qualitatively compare the different methods considered. ", "page_idx": 8}, {"type": "text", "text": "In this case, the tempering mechanism of our method is crucial for ensuring that the learned flow does not collapse on one of the modes and instead explores both global minima. This is confirmed when plotting the samples generated in the grid in Figure 3. This experiment demonstrates the ability of our method to capture complex multi-modal densities, even without an informed base measure, at a significantly lower computational cost (e.g., $10{-}25\\mathrm{x}$ faster) than competing methods. Indeed, while FAB was the best performing method in this experiment as measured by the KSD, it failed to capture both of the modes in the target distribution, and required a much greater total computation time (see Table 2). ", "page_idx": 8}, {"type": "text", "text": "It is worth noting that MFM (and the other two benchmarks, DDS and FAB) significantly outperformed NF-MCMC in this example, despite the similarities between MFM and NF-MCMC. While we tested various hyperparameter configurations for NF-MCMC, we were not able to find a setting that achieved comparable results in the absence of an informed base measure. ", "page_idx": 8}, {"type": "image", "img_path": "amJyuVqSaf/tmp/9f44a63f81be53694bff1ffd23ad83f26c449f4b30b72a998165a887a8474dec.jpg", "img_caption": ["Figure 3: Comparison between MFM, FAB, DDS, and NF-MCMC. Representative samples from the target density for the Field system example. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Bayesian inference for high-dimensional spatial models is known to be challenging. One such model is the log-Gaussian Cox process (LGCP) introduced in [53], which is used to model the locations of 126 Scots pine saplings in a natural forest in Finland. See Appendix C.6 for full details. The target space is discretized to a $M=40\\times40$ regular grid, rendering the target dimension $d=1600$ . In Table 2, we report diagnostics for each algorithm. ", "page_idx": 9}, {"type": "text", "text": "In this case, the lack of multimodality in the target makes it a good fit for non-tempered schemes. Similar to the previous example, NF-MCMC is unable to obtain an accurate approximation to the target distribution. We suspect that this may be a result of non-convergence: due to memory issues, it was not possible to run NF-MCMC (or FAB) for more than $K=10^{3}$ iterations. This also explains the (relatively) smaller run times of these algorithms in this example. By a small margin, DDS provides the best approximation of the target, slightly outperforming MFM and FAB. Meanwhile, MFM provides a good approximation to the target at a lower computational cost with respect to its competitors. ", "page_idx": 9}, {"type": "table", "img_path": "amJyuVqSaf/tmp/09ac6deef044460c128bb5fceaa889d5553162104ae7ef2feb196e808214ad82.jpg", "table_caption": [], "table_footnote": ["Table 2: Diagnostics for the two real data examples. KSD U-stat and V-stat are the Kernel Stein Discrepancy $\\mathrm{U}-$ and V-statistics between the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. "], "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary. In this paper, we introduced Markovian Flow Matching, a new approach to sampling from unnormalized probability distributions that augments MCMC with CNFs. Our method combines a local Markov kernel with a non-local, flow-informed Markov kernel, which is adaptively learned during sampling using FM. It also incorporates an adaptive tempering mechanism, which allows for the discovery of multiple target modes. Under mild assumptions, we established convergence of the flow network parameters output by our algorithm to a local optimum of the FM objective. We also benchmarked the performance of our algorithm on several examples, illustrating comparable performance to other state-of-the-art methods, often at a fraction of the computational cost. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Work. We highlight three limitations of our work. First, our theoretical result established convergence of the flow network parameters obtained via MFM to a local minimum of the FM objective. Further work is required to understand how well these local minima generalize, in order to accurately quantify how accurately the corresponding CNF captures the target posterior. Second, we did not establish non-asymptotic convergence rates for our method. Finally, since it was not the main focus of this work, we did not explore in great detail other choices of architecture for the flow network. We expect that, for certain targets, this could have a significant impact on the performance of MFM. Indeed, a promising avenue for further research lies in developing tailored CNFs designed for particular posterior distributions. This approach would go beyond the current practice of including the gradient of the log-posterior and instead exploit unique characteristics intrinsic to each model when constructing the flow. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "LS and CN were supported by the Engineering and Physical Sciences Research Council (EPSRC), grant number EP/V022636/1. CN acknowledges further support from the EPSRC, grant numbers EP/S00159X/1 and EP/Y028783/1. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] T. Akhound-Sadegh, J. Rector-Brooks, A. J. Bose, S. Mittal, P. Lemos, C.-H. Liu, M. Sendera, S. Ravanbakhsh, G. Gidel, Y. Bengio, et al. Iterated denoising energy matching for sampling from Boltzmann densities. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. 7 [2] M. S. Albergo and E. Vanden-Eijnden. Learning to sample better. arXiv preprint arXiv:2310.11232, 2023. 6 [3] M. S. Albergo, G. Kanwar, and P. E. Shanahan. Flow-based generative models for Markov chain Monte Carlo in lattice field theory. Physical Review D, 100(3):034515, 2019. 7 [4] M. S. Albergo, G. Kanwar, S. Racani\u00e8re, D. J. Rezende, J. M. Urban, D. Boyda, K. Cranmer, D. C. Hackett, and P. E. Shanahan. Flow-based sampling for fermionic lattice field theories. Physical Review D, 104:114507, 2021. doi: 10.1103/PhysRevD.104.114507. 7 [5] M. Arbel, A. Matthews, and A. Doucet. Annealed flow transport Monte Carlo. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. 7 [6] A. Benveniste, M. Metivier, and P. Priouret. Adaptive Algorithms and Stochastic Approximations. Springer-Verlag, Berlin, Heidelberg, 1990. doi: 10.1007/978-3-642-75894-2. 6 [7] N. Berglund, G. D. Ges\u00f9, and H. Weber. An Eyring\u2013Kramers law for the stochastic Allen\u2013Cahn equation in dimension two. Electronic Journal of Probability, 22:1 \u2013 27, 2017. doi: 10.1214/   \n17-EJP60. 8 [8] J. Berner, L. Richter, and K. Ullrich. An optimal control perspective on diffusion-based generative modeling. Transaction on Machine Learning Research (TMLR), 2024. 7 [9] A. Beskos, A. Jasra, N. Kantas, and A. Thiery. On the convergence of adaptive sequential Monte Carlo methods. Annals of Applied Probability, 26(2):1111\u20131146, 2016. doi: 10.1214/   \n15-AAP1113. 6 [10] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: a review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017. doi: 10.1080/   \n01621459.2017.1285773. 2 [11] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. 18 [12] S. Brooks, A. Gelman, G. Jones, and X.-L. Meng. Handbook of Markov Chain Monte Carlo. Chapman and Hall/CRC, 1st edition, 2011. doi: 10.1201/b10905. 2 [13] A. Cabezas and C. Nemeth. Transport elliptical slice sampling. In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics (AISTATS), 2023. 7 [14] R. Chen and Y. Lipman. Flow matching on general geometries. In Proceedings of the 12th International Conference on Learning Representations (ICLR), 2024. 3 [15] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In Proceedings of the 32nd Annual Conference on Neural Information Processing Systems (NeurIPS), 2018. 2, 3 [16] V. De Bortoli, M. Hutchinson, P. Wirnsberger, and A. Doucet. Target score matching. arXiv preprint arXiv:2402.08667, 2024. 7 [17] L. Del Debbio, J. Marsh Rossney, and M. Wilson. Efficient modeling of trivializing maps for lattice $\\phi\\,4$ theory using normalizing flows: a first look at scalability. Physical Review D, 104(9):   \n094507, 2021. doi: 10.1103/PhysRevD.104.094507. 2 [18] P. Del Moral, A. Doucet, and A. Jasra. Sequential Monte Carlo samplers. Journal of the Royal Statistical Society Series B: Statistical Methodology, 68(3):411\u2013436, 2006. 5, 7   \n[19] A. Doucet, W. Grathwohl, A. G. Matthews, and H. Strathmann. Score-based diffusion meets annealed importance sampling. In Proceedings of the 36th Annual Conference on Neural Information Processing Systems (NeurIPS), 2022. 7   \n[20] S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics letters B, 195(2):216\u2013222, 1987. doi: 10.1016/0370-2693(87)91197-X. 2   \n[21] C. Durkan, A. Bekasov, I. Murray, and G. Papamakarios. Neural spline flows. In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS), 2019. 7   \n[22] F. Feroz, M. Hobson, and M. Bridges. MultiNest: an efficient and robust Bayesian inference tool for cosmology and particle physics. Monthly Notices of the Royal Astronomical Society, 398(4):1601\u20131614, 2009. 2   \n[23] M. Gabri\u00e9, G. M. Rotskoff, and E. Vanden-Eijnden. Efficient Bayesian sampling using normalizing flows to assist Markov chain Monte Carlo methods. In Proceedings of the 38th International Conference on Machine Learning (ICML): 3rd Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021. 4   \n[24] M. Gabri\u00e9, G. M. Rotskoff, and E. Vanden-Eijnden. Adaptive Monte Carlo augmented with normalizing flows. Proceedings of the National Academy of Sciences, 119(10):e2109420119, 2022. doi: 10.1073/pnas.2109420119. 2, 4, 5, 7, 8, 9, 20   \n[25] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian data analysis. Chapman and Hall/CRC, 1995. 1   \n[26] J. Gorham and L. Mackey. Measuring sample quality with kernels. In Proceedings of the Proceedings of the 34th International Conference on Machine Learning (ICML), 2017. 9, 18   \n[27] W. Grathwohl, R. T. Chen, J. Bettencourt, I. Sutskever, and D. Duvenaud. FFJORD: Free-form continuous dynamics for scalable reversible generative models. In Proceedings of the 7th International Conference on Learning Representations (ICLR), 2018. 2, 3, 19, 21   \n[28] L. Grenioux, A. Durmus, \u00c9. Moulines, and M. Gabri\u00e9. On sampling with approximate transport maps. In Proceedings of the 40th International Conference on Machine Learning (ICML), 2023. 2, 4, 6   \n[29] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00f6olkopf, and A. Smola. A kernel two-sample test. Journal of Machine Learning Research (JMLR), 13:723\u2013773, 2012. 7, 18   \n[30] M. G. Gu and F. H. Kong. A stochastic approximation algorithm with Markov chain MonteCarlo method for incomplete data estimation problems. Proceedings of the National Academy of Sciences, 95(13):7270\u20137274, 1998. doi: 10.1073/pnas.95.13.7270. 17   \n[31] D. C. Hackett, C.-C. Hsieh, M. S. Albergo, D. Boyda, J.-W. Chen, K.-F. Chen, K. Cranmer, G. Kanwar, and P. E. Shanahan. Flow-based sampling for multimodal distributions in lattice field theory. arXiv preprint arXiv:2107.00734, 2021. 7   \n[32] W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1):97\u2013109, 1970. doi: 10.1093/biomet/57.1.97. 2   \n[33] M. Hoffman, P. Sountsov, J. V. Dillon, I. Langmore, D. Tran, and S. Vasudevan. Neutra-lizing bad geometry in Hamiltonian Monte Carlo using neural transport. In Proceedings of the 1st Symposium on Advances in Approximate Bayesian Inference (AABI), 2019. 2, 4, 7   \n[34] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):1303\u20131347, 2013. 2   \n[35] N. T. Hunt-Smith, W. Melnitchouk, F. Ringer, N. Sato, A. W. Thomas, and M. J. White. Accelerating Markov chain Monte Carlo sampling with diffusion models. Computer Physics Communications, 296:109059, 2024. doi: 10.1016/j.cpc.2023.109059. 4, 7   \n[36] M. F. Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059\u2013 1076, 1989. 19, 21   \n[37] A. Ihler, J. Fisher III, R. Moses, and A. Willsky. Nonparametric belief propagation for selflocalization of sensor networks. IEEE Journal on Selected Areas in Communications, 23(4), 2005. doi: 10.1109/JSAC.2005.843548. 2   \n[38] S. T. Jensen, X. S. Liu, Q. Zhou, and J. S. Liu. Computational discovery of gene regulatory binding motifs: a Bayesian perspective. Statistical Science, 19(1):188\u2013204, 2004. doi: 10.1214/ 088342304000000107. 2   \n[39] M. Karamanis, F. Beutler, J. A. Peacock, D. Nabergoj, and U. Seljak. Accelerating astronomical and cosmological inference with preconditioned Monte Carlo. Monthly Notices of the Royal Astronomical Society, 516(2):1644\u20131653, 2022. doi: 10.1093/mnras/stac2272. 2, 4   \n[40] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015. 7   \n[41] S. Kou, J. Oh, and W. H. Wong. A study of density of states and ground states in hydrophobichydrophilic protein folding models by equi-energy sampling. The Journal of Chemical Physics, 124(24), 2006. doi: 10.1063/1.2208607. 2   \n[42] A. Lee. U-Statistics: Theory and Practice. Taylor & Francis, 1st edition, 1990. doi: 10.1201/ 9780203734520. 18   \n[43] B. Leimkuhler and C. Matthews. Molecular dynamics. Interdisciplinary Applied Mathematics, 39:443, 2015. doi: 10.1007/978-3-319-16375-8. 1   \n[44] S.-H. Li and L. Wang. Neural network renormalization group. Physical Review Letters, 121: 260601, 2018. doi: 10.1103/PhysRevLett.121.260601. 4, 7   \n[45] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. In Proceedings of the 11th International Conference on Learning Representations (ICLR), 2023. 1, 2, 3, 5, 6   \n[46] Q. Liu, J. Lee, and M. Jordan. A kernelized Stein discrepancy for goodness-of-fit tests. In Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016. 9, 18   \n[47] A. H. Mahmoud, M. Masters, S. J. Lee, and M. A. Lill. Accurate sampling of macromolecular conformations using adaptive deep learning and coarse-grained representation. Journal of Chemical Information and Modeling, 62(7):1602\u20131617, 2022. doi: 10.1021/acs.jcim.1c01438. 2   \n[48] O. Mangoubi, N. S. Pillai, and A. Smith. Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal densities? arXiv preprint arXiv:1808.03230, 2018. 2   \n[49] O. Mangoubi, N. Pillai, and A. Smith. Simple conditions for metastability of continuous Markov chains. Journal of Applied Probability, 58(1):83\u2013105, 2021. doi: 10.1017/jpr.2020.83. 2   \n[50] A. Matthews, M. Arbel, D. J. Rezende, and A. Doucet. Continual repeated annealed flow transport Monte Carlo. In Proceedings of the 39th International Conference on Machine Learning (ICML), 2022. 7   \n[51] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6): 1087\u20131092, 1953. 1   \n[52] L. I. Midgley, V. Stimper, G. N. Simm, B. Sch\u00f6lkopf, and J. M. Hern\u00e1ndez-Lobato. Flow annealed importance sampling bootstrap. In Proceedings of the 11th International Conference on Learning Representations (ICLR), 2023. 7, 18, 20   \n[53] J. M\u00f8ller, A. R. Syversveen, and R. P. Waagepetersen. Log Gaussian Cox processes. Scandinavian Journal of Statistics, 25(3):451\u2013482, 1998. doi: 10.1111/1467-9469.00115. 10, 22   \n[54] C. Naesseth, F. Lindsten, and D. Blei. Markovian score climbing: variational inference with KL(p||q). Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), 2020. 4, 6, 7, 17   \n[55] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125\u2013139, 2001. doi: 10.1023/A:1008923215028. 5, 6   \n[56] R. M. Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov chain Monte Carlo, 2(11):2, 2011. 2   \n[57] K. A. Nicoli, S. Nakajima, N. Strodthoff, W. Samek, K.-R. M\u00fcller, and P. Kessel. Asymptotically unbiased estimation of physical observables with neural samplers. Physical Review E, 101(2): 023304, 2020. doi: 10.1103/PhysRevE.101.023304. 7   \n[58] F. No\u00e9, S. Olsson, J. K\u00f6hler, and H. Wu. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. Science, 365(6457), 2019. doi: 10.1126/science. aaw1147. 7, 20   \n[59] M. D. Parno and Y. M. Marzouk. Transport map accelerated Markov chain Monte Carlo. SIAM/ASA Journal on Uncertainty Quantification, 6(2):645\u2013682, 2018. doi: 10.1137/ 17M1134640. 2, 4, 7   \n[60] A. Phillips, H.-D. Dau, M. J. Hutchinson, V. D. Bortoli, G. Deligiannidis, and A. Doucet. Particle denoising diffusion sampler. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. 7   \n[61] R. Ranganath, S. Gerrish, and D. Blei. Black box variational inference. In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS), 2014. 2   \n[62] D. Rezende and S. Mohamed. Variational inference with normalizing flows. In Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015. 2   \n[63] L. Richter, J. Berner, and G.-H. Liu. Improved sampling via learned diffusions. In Proceedings of the 12th International Conference on Learning Representations (ICLR), 2024. 7   \n[64] C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer-Verlag, New York, 2nd edition, 2004. doi: 10.1007/978-1-4757-4145-22. 2   \n[65] G. O. Roberts and J. S. Rosenthal. General state space Markov chains and MCMC algorithms. Probability Surveys, 1:20 \u2013 71, 2004. doi: 10.1214/154957804100000024. 2   \n[66] G. O. Roberts and R. L. Tweedie. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, 2(4):341\u2013363, 1996. 2   \n[67] S. Samsonov, E. Lagutin, M. Gabri\u00e9, A. Durmus, A. Naumov, and E. Moulines. Local-global MCMC kernels: the best of both worlds. In Proceedings of the 36th Annual Conference on Neural Information Processing Systems (NeurIPS), 2022. 2, 4, 7   \n[68] C. Sch\u00f6nle and M. Gabri\u00e9. Optimizing Markov chain Monte Carlo convergence with normalizing flows and Gibbs sampling. In Proceedings of the 37th Annual Conference on Neural Information Processing Systems (NeurIPS): AI for Science Workshop, 2023. 2, 7   \n[69] M. Sendera, M. Kim, S. Mittal, P. Lemos, L. Scimeca, J. Rector-Brooks, A. Adam, Y. Bengio, and N. Malkin. Improved off-policy training of diffusion samplers. arXiv preprint arXiv:2402.05098, 2024. 7   \n[70] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems (NeurIPS), 2019. 5   \n[71] M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), 2020. 7   \n[72] A. Tong, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, K. Fatras, G. Wolf, and Y. Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research (TMLR), 2024. 4   \n[73] B. Tzen and M. Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In Proceedings of the 32nd Annual Conference on Learning Theory (COLT), 2019. 7   \n[74] F. Vargas and N. N\u00fcsken. Transport, variational inference and diffusions: with applications to annealed flows and Schr\u00f6dinger bridges. Proceedings of the 40th International Conference on Machine Learning (ICML): Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023. 7   \n[75] F. Vargas, W. Grathwohl, and A. Doucet. Denoising diffusion samplers. In Proceedings of the 11th International Conference on Learning Representations (ICLR), 2023. 7   \n[76] F. Vargas, A. Ovsianas, D. Fernandes, M. Girolami, N. D. Lawrence, and N. N\u00fcsken. Bayesian learning via neural Schr\u00f6dinger\u2013F\u00f6llmer flows. Statistics and Computing, 33(1):3, 2023. doi: 10.1007/s11222-022-10172-5. 7   \n[77] F. Vargas, T. Reu, and A. Kerekes. Expressiveness remarks for denoising diffusion models and samplers. In Proceedings of the 5th Symposium on Advances in Approximate Bayesian Inference (AABI), 2023. 7   \n[78] F. Vargas, S. Padhy, D. Blessing, and N. N\u00fcsken. Transport meets variational inference: controlled Monte Carlo diffusions. In Proceedings of the 12th International Conference on Learning Representations (ICLR), 2024. 7   \n[79] M. J. Wainwright, M. I. Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013305, 2008. doi: 10.1561/ 2200000001. 2   \n[80] K. W. K. Wong, M. Gabri\u00e9, and D. Foreman-Mackey. flowMC: Normalizing flow enhanced sampling package for probabilistic inference in JAX. Journal of Open Source Software, 8(83): 5021, 2023. 18   \n[81] H. Wu, J. K\u00f6hler, and F. No\u00e9. Stochastic normalizing flows. Advances in Neural Information Processing Systems, 33:5933\u20135944, 2020. 20   \n[82] B. J. Zhang, Y. M. Marzouk, and K. Spiliopoulos. Transport map unadjusted Langevin algorithms. arXiv preprint arXiv:2302.07227, 2023. 7   \n[83] D. Zhang, R. T. Q. Chen, C.-H. Liu, A. Courville, and Y. Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization. In Proceedings of the 12th International Conference on Learning Representations (ICLR), 2024. 7   \n[84] L. Zhang, D. M. Blei, and C. A. Naesseth. Transport score climbing: variational inference using forward kl and adaptive neural transport. Transactions on Machine Learning Research (TMLR), 2023. 7   \n[85] Q. Zhang and Y. Chen. Path integral sampler: a stochastic control approach for sampling. In Proceedings of the 10th International Conference on Learning Representations (ICLR), 2022. 7 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Flow-Informed Markov Chain Monte Carlo Methods ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Algorithm 2 Flow-informed Random Walk Metropolis Hastings ", "page_idx": 15}, {"type": "text", "text": "1: Input: initial $x$ , target $\\pi$ , vector field $v_{t}^{\\theta}$ , flow parameters   \n2: Output: $x^{\\prime}$   \n3: $\\sigma_{o p t}\\gets2.38/\\sqrt{d}$   \n4: $\\phi_{1}(x)=x_{t=1}\\leftarrow x$   \n5: $\\left[\\stackrel{x_{0}}{\\Delta\\log p(x_{0})}\\right]\\leftarrow\\left[\\stackrel{x}{0}\\right]+\\int_{1}^{0}\\left[\\stackrel{v_{t}^{\\theta}(\\phi_{t}(x))}{-\\nabla\\cdot v_{t}^{\\theta}(\\phi_{t}(x))}\\right]\\mathrm{d}t$   \n6: $y_{0}\\sim\\mathcal{N}(\\cdot|x_{0},\\sigma_{o p t}^{2})$   \n7: $\\left[\\underset{{\\Delta}\\log p(y_{1})}{\\overset{y_{1}}{\\log}}\\right]\\leftarrow\\left[\\b{0}_{0}^{y_{0}}\\right]+\\int_{0}^{1}\\left[\\underset{-\\nabla\\cdot\\b{\\cdot}\\b{v}_{t}^{\\theta}(\\phi_{t}(y))}{\\b{v}_{t}^{\\theta}(\\phi_{t}(y))}\\right]\\mathrm{d}t$   \n8: $\\begin{array}{r}{\\alpha\\gets\\operatorname*{min}\\left\\{1,\\frac{\\pi(y_{1})\\exp(-\\Delta\\log p(y_{1}))}{\\pi(x_{1})\\exp(\\Delta\\log p(x_{0}))}\\right\\}}\\end{array}$   \n9: With probability $\\alpha$ make $x^{\\prime}\\gets y_{1}$ else x\u2032 \u2190x ", "page_idx": 15}, {"type": "text", "text": "Algorithm 3 Flow-informed Independent Metropolis Hastings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Input: initial $x$ , target density $\\pi$ , vector field $v_{t}^{\\theta}$ , reference density $p_{0}$ , flow parameters $\\theta$ .   \n2: Output: $x^{\\prime}$   \n3: $\\phi_{1}(u)=u_{t=1}\\leftarrow x$   \n4: $\\begin{array}{r l}&{\\quad\\left[\\underset{{\\Delta}\\log p(u_{0})}{\\quad\\:u_{0}\\quad\\:}\\right]\\leftarrow\\left[\\underset{{0}}{u_{1}}\\right]+\\int_{1}^{0}\\left[\\underset{{-\\nabla\\cdot\\:\\eta_{t}^{\\theta}\\left(\\phi_{t}(u)\\right)}}{\\quad\\:v_{t}^{\\theta}\\left(\\phi_{t}(u)\\right)}\\right]d t}\\\\ &{\\phi_{0}(x)=x_{t=0}\\sim p_{0}}\\\\ &{\\left[\\underset{{\\Delta}\\log p(x_{1})}{\\quad\\:x_{1}}\\right]\\leftarrow\\left[\\underset{{\\log p_{0}\\left(x_{0}\\right)}}{\\quad\\:x_{0}\\quad\\:}\\right]+\\int_{0}^{1}\\left[\\underset{{-\\nabla\\cdot\\:\\eta_{t}^{\\theta}\\left(\\phi_{t}(x)\\right)}}{\\quad\\:v_{t}^{\\theta}\\left(\\phi_{t}(x)\\right)}\\right]d t}\\\\ &{\\alpha\\leftarrow\\operatorname*{min}\\left\\{1,\\frac{\\pi(x_{1})p_{0}(u_{0})\\exp(-\\Delta\\log p(u_{0}))}{\\exp(\\log p(x_{1}))\\pi(u_{1})}\\right\\}}\\\\ &{\\mathrm{{With~probability~}}\\pi\\mathtt{m a k e}\\times{r}^{\\prime}\\leftarrow x_{1}\\mathrm{~else~}x^{\\prime}\\leftarrow x}\\end{array}$   \n5:   \n6:   \n7:   \n8: With probability ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Algorithm 4 Flow-informed Conditional Importance Sampling ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: Input: initial $x$ , target density $\\pi$ , vector field $v_{t}^{\\theta}$ , reference density $q_{0}$ , flow parameters $\\theta$ , number of importance samples $K$ . ", "page_idx": 15}, {"type": "text", "text": "2: Output: $x^{\\prime}$   \n3: $\\phi_{1}(u)=u_{t=1}\\leftarrow x$   \n4: $\\left[\\stackrel{u_{0}}{\\Delta\\log p(u_{0})}\\right]\\leftarrow\\left[\\stackrel{u_{1}}{0}\\right]+\\int_{1}^{0}\\left[\\stackrel{v_{t}^{\\theta}\\left(\\phi_{t}(u)\\right)}{-\\nabla\\cdot v_{t}^{\\theta}(\\phi_{t}(u))}\\right]d t$   \n$\\begin{array}{r}{w_{0}\\leftarrow\\frac{\\pi\\left(u_{1}\\right)}{p_{0}\\left(u_{0}\\right)\\exp\\left(-\\Delta\\log p\\left(u_{0}\\right)\\right)}}\\end{array}$   \n6: $x^{(0)}\\gets x$   \n7: for $k=1:K$ do   \n8: $\\begin{array}{r l}&{\\phi_{0}(x)=x_{t=0}\\sim q_{0}}\\\\ &{\\left[\\log p(x_{1})\\right]\\leftarrow\\left[\\log p_{0}(x_{0})\\right]+\\int_{0}^{1}\\left[\\underset{-\\nabla\\cdot\\}v_{t}^{\\theta}(\\phi_{t}(x))\\right]d t}\\\\ &{w_{k}\\leftarrow\\frac{\\pi(x_{1})}{\\exp(\\log p(x_{1}))}}\\\\ &{x^{(k)}\\leftarrow x_{1}}\\end{array}$   \n9:   \n10:   \n11:   \n12: end for   \n13: Choose $k^{\\prime}$ with probability $P(k^{\\prime}=k)\\propto w_{k}$ , then make $x^{\\prime}\\gets x^{(k^{\\prime})}$ ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "B Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our proof follows closely the proof of [54, Proposition 1]. Let $\\theta^{*}$ be a minimizer of the CFM objective in (4), which we recall is given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\theta;\\pi)=\\mathbb{E}_{x_{1}\\sim\\pi}\\mathbb{E}_{t\\sim\\mathcal{U}(0,1)}\\mathbb{E}_{x\\sim p_{t}(\\cdot\\vert x_{1})}\\left[\\vert\\vert v_{t}^{\\theta}(x)-v_{t}(x\\vert x_{1})\\vert\\vert^{2}\\right]:=\\mathbb{E}_{x_{1}\\sim\\pi}\\left[j(\\theta,x_{1})\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have define $\\textrm{d}j(\\theta,x_{1})=\\mathbb{E}_{t\\sim\\mathcal{U}(0,1)}\\mathbb{E}_{x\\sim p_{t}(\\cdot|x_{1})}\\left[||v_{t}^{\\theta}(x)-v_{t}(x|x_{1})||^{2}\\right]$ . Now, consider the ordinary differential equation (ODE) given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\theta(t)=\\nabla\\mathcal{I}_{\\theta}(\\theta(t);\\pi),\\quad\\theta(0)=\\theta_{0},\\quad t\\geq0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We say that $\\hat{\\theta}$ is stability point of this ODE if, given the initial condition $\\theta(0)=\\hat{\\theta}$ , the ODE admits the unique solution $\\boldsymbol{\\theta}(t)\\dot{=}\\,\\hat{\\boldsymbol{\\theta}}$ for all $t\\geq0$ . Naturally, the minimizer $\\theta^{*}$ is a stability point of this ODE, since $\\nabla_{\\theta}\\mathcal{J}(\\theta;\\pi)|_{\\theta=\\theta^{*}}=0$ . Meanwhile, we call $\\Theta$ the domain of attraction of $\\theta^{*}$ if, given the initial condition $\\theta(0)\\in\\Theta$ , the solution $\\theta(t)\\in\\Theta$ for all $t\\geq0$ , and $\\theta(t)$ converges to $\\theta^{*}$ as $t\\to\\infty$ . ", "page_idx": 16}, {"type": "text", "text": "Let $\\boldsymbol{x}_{k}\\in\\mathbb{R}^{d_{x}}$ , and $\\mathcal{X}\\subseteq\\mathbb{R}^{d_{\\boldsymbol{x}}}$ be an open subset of $\\mathbb{R}^{d_{x}}$ . Let $\\Theta\\subseteq\\mathbb{R}^{d_{\\theta}}$ be an open set in $\\mathbb{R}^{d_{\\theta}}$ , and $\\Theta_{c}\\subseteq\\Theta$ be a compact subset of $\\Theta$ . Consider the Markov transition kernel $M:=P\\circ Q^{k}$ given by a cycle of $k_{Q}$ repeated transitions of a MALA transition kernel and a flow-informed RWMH transition kernel, viz ", "page_idx": 16}, {"type": "equation", "text": "$$\nM_{\\pi,\\theta}(x,\\mathrm{d}y)=\\int\\cdot\\cdot\\int Q(x,\\mathrm{d}x_{1};\\theta)Q(x_{1},\\mathrm{d}x_{2})\\cdot\\cdot\\cdot Q(x_{k_{Q}-1},\\mathrm{d}x_{k_{Q}};\\theta)P(x_{k_{Q}},\\mathrm{d}y;\\pi,\\theta).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This transition kernel is $\\pi$ -invariant since both $P$ and $Q$ are $\\pi$ -invariant. In addition, let $M_{\\pi,\\theta}^{k}(x,\\mathrm{d}y)$ be the repeated application of this Markov transition kernel, namely, ", "page_idx": 16}, {"type": "equation", "text": "$$\nM_{\\pi,\\theta}^{k}(x,\\mathrm{d}y)=\\int\\cdot\\cdot\\int M_{\\pi,\\theta}(x,\\mathrm{d}x_{1})M_{\\pi,\\theta}(x_{1},\\mathrm{d}x_{2})\\cdot\\cdot\\cdot M_{\\pi,\\theta}(x_{k-2},\\mathrm{d}x_{k-1})M_{\\pi,\\theta}(x_{k-1},\\mathrm{d}y).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Following [30, 54], we impose the following assumptions, for some sufficiently large positive real number $q>1$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption B.1 (Robbins-Monro Condition). The step size sequence $(\\varepsilon_{k})_{k=1}^{\\infty}$ satisfies the following requirements: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{\\infty}\\varepsilon_{k}=\\infty,\\quad\\sum_{k=1}^{\\infty}\\varepsilon_{k}^{2}<\\infty.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assumption B.2 (Integrability). There exists a constant $C_{1}>0$ such that for, any $\\theta\\in\\Theta,x\\in\\mathcal{X}$ and $k\\geq1$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\int(1+|y|^{q})M_{\\pi,\\theta}^{k}(x,\\mathrm{d}y)\\leq C_{1}(1+|x|^{q}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assumption B.3 (Convergence of the Markov Chain). For each $\\theta\\in\\Theta$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{k\\to\\infty}\\operatorname*{sup}_{x\\in\\mathcal{X}}\\frac{1}{1+|x|^{q}}\\int(1+|y|^{q})|M_{\\pi,\\theta}^{k}(x,\\mathrm{d}y)-\\pi(\\mathrm{d}y)|=0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assumption B.4 (Continuity in $\\theta$ ). There exists a constant $C_{2}$ such that for all $\\theta,\\theta^{\\prime}\\in\\Theta_{c}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\int(1+|y|^{q})(M_{\\pi,\\theta}^{k}(x,\\mathrm{d}y)-M_{\\pi,\\theta^{\\prime}}^{k}(x,\\mathrm{d}y))\\right|\\leq C_{2}|\\theta-\\theta^{\\prime}|(1+|x|^{q}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assumption B.5 (Continuity in $x$ ). There exists a constant $C_{3}$ such that for all $x_{1},x_{2}\\in\\mathcal{X}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\theta\\in\\Theta}\\left\\vert\\int(1+\\vert y\\vert^{q+1})(M_{\\pi,\\theta}^{k}(x_{1},\\mathrm{d}y)-M_{\\pi,\\theta}^{k}(x_{2},\\mathrm{d}y))\\right\\vert\\leq C_{3}\\vert x_{1}-x_{2}\\vert(1+\\vert x_{1}\\vert^{q}+\\vert x_{2}\\vert^{q}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assumption B.6 (Conditions on the Objective Function). For any compact subset $\\Theta_{c}\\subset\\Theta$ , there exist positive constants $p,K_{1},K_{2},K_{3}$ and $v>1/2$ such that for all $\\theta,\\bar{\\theta^{\\prime}}\\,\\bar{\\in}\\,\\Theta_{c}$ and $x,x_{1},x_{2}\\in\\mathcal{X}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{|\\nabla_{\\theta}j(\\theta,x_{1})|\\leq K_{1}(1+|x_{1}|^{p+1}),}\\\\ {|\\nabla_{\\theta}j(\\theta,x_{1})-\\nabla_{\\theta}j(\\theta,x_{1}^{\\prime})\\leq K_{2}|x_{1}-x_{1}^{\\prime}|(1+|x_{1}|^{p}+|x_{2}|^{p}),}\\\\ {|\\nabla_{\\theta}j(\\theta,x_{1})-\\nabla_{\\theta}j(\\theta^{\\prime},x_{1})\\leq K_{3}|\\theta-\\theta^{\\prime}|^{v}(1+|x_{1}|^{p+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With the above assumptions, the result follows from Theorem 1 of [30] by setting $x\\rightarrow x_{1},\\Pi_{\\theta}\\rightarrow$ $M_{\\pi,\\theta},H(\\theta,x)\\rightarrow\\nabla_{\\theta}\\bar{j}(\\theta,x_{1})$ . ", "page_idx": 16}, {"type": "text", "text": "C Additional Experimental Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Code for the numerical experiments is written in Python with array computations handled by JAX [11]. The implementation of relevant methods for comparison is sourced from open source repositories: DDS using franciscovargas/denoising_diffusion_samplers, NF-MCMC using kazewong/flowMC [80], and FAB using lollcat/fab-jax [52]. All experiments are run on an NVIDIA V100 GPU with 32GB of memory. In the following subsections, we will give more details on the modelling and hyperparameter choices for each experiment, along with additional results. ", "page_idx": 17}, {"type": "text", "text": "C.1 Diagnostics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let $\\pi$ and $\\nu$ be two probability measures. Let $\\mathcal{F}$ denote the unit ball in a reproducing kernel Hilbert space (RKHS) $\\mathcal{H}$ , associated with the positive definite kernel $k:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\stackrel{\\cdot}{\\rightarrow}\\mathbb{R}$ . Then the maximum mean discrepancy (MMD) between $\\pi$ and $\\nu$ is defined as [29, Section 2.2] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{{MMD}}_{k}^{2}(\\pi,\\nu)=\\|m_{\\pi}-m_{\\nu}\\|_{\\mathcal{F}}^{2},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $m_{\\pi}$ is the mean embedding of $\\pi$ , defined via $\\mathbb{E}_{\\pi}[f]=\\langle f,m_{\\pi}\\rangle_{\\mathcal{H}}$ for all $f\\in\\mathcal H$ . Using standard properties of the RKHS, the squared MMD can be written as [29, Lemma 6] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{MMD}_{k}^{2}(\\pi,\\nu)=\\mathbb{E}_{x,x^{\\prime}\\sim\\pi}\\left[k(x,x^{\\prime})\\right]-2\\mathbb{E}_{x\\sim\\pi,y\\sim\\nu}\\left[k(x,y)\\right]+\\mathbb{E}_{y\\sim\\nu,y^{\\prime}\\sim\\nu}\\left[k(y,y^{\\prime})\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, given samples $(x_{i})_{i=1}^{m}\\sim\\pi$ and $(y_{i})_{i=1}^{m}\\sim\\nu$ , an unbiased estimate of the squared MMD can be computed as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widehat{\\mathbf{MMD}}_{k}^{2}(\\pi,\\nu)=\\displaystyle\\frac{1}{m(m-1)}\\sum_{i=1}^{m}\\sum_{i\\neq j}^{m}k(x_{i},x_{j})-\\displaystyle\\frac{2}{m^{2}}\\sum_{i=1}^{m}\\sum_{j=1}^{m}k(x_{i},y_{j})}\\\\ {\\displaystyle\\qquad\\qquad+\\,\\frac{1}{m(m-1)}\\sum_{i=1}^{m}\\sum_{j\\neq i}^{m}k(y_{i},y_{j}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For a kernel $k$ , the kernel Stein discrepancy (KSD) between $\\pi$ and $\\nu$ is defined as the MMD between $\\pi$ and $\\nu$ , using the Stein kernel $k_{\\pi}$ associated with $k$ , which is defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{k_{\\pi}(x,x^{\\prime})=\\nabla_{x}\\cdot\\nabla_{x^{\\prime}}k(x,x^{\\prime})+\\nabla_{x}k(x,x^{\\prime})\\cdot\\nabla_{x^{\\prime}}\\log\\pi(x^{\\prime})}\\\\ &{\\qquad\\qquad+\\,\\nabla_{x^{\\prime}}k(x,x^{\\prime})\\cdot\\nabla_{x}\\log\\pi(x)+k(x,x^{\\prime})\\nabla_{x}\\log\\pi(x)\\cdot\\nabla_{x^{\\prime}}\\log\\pi(x),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and satisfies the Stein identity $\\mathbb{E}_{\\pi}\\left[k_{\\pi}(x,\\cdot)\\right]=0.$ . We thus have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KSD}_{k}^{2}(\\pi,\\nu)=\\mathrm{MMD}_{k_{\\pi}}^{2}(\\pi,\\nu)}\\\\ &{\\phantom{\\mathrm{KSD}_{k}^{2}(\\pi,\\nu)=}=\\mathbb{E}_{x,x^{\\prime}\\sim\\pi}\\left[k_{\\pi}(x,x^{\\prime})\\right]-2\\mathbb{E}_{x\\sim\\pi,y\\sim\\nu}\\left[k_{\\pi}(x,y)\\right]+\\mathbb{E}_{y\\sim\\nu,y^{\\prime}\\sim\\nu}\\left[k_{\\pi}(y,y^{\\prime})\\right]}\\\\ &{\\phantom{\\mathrm{KSD}_{k}^{2}(\\pi,\\nu)=}=\\mathbb{E}_{y\\sim\\nu,y^{\\prime}\\sim\\nu}\\left[k_{\\pi}(y,y^{\\prime})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can obtain estimates of the KSD by using U-statistics or ${\\mathrm{V}}.$ -statistics. In particular, an unbiased estimate of $\\mathrm{KSD}_{k}^{2}(\\pi,\\nu)$ is given by the U-statistic [42] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{KSD}}_{k,U}^{2}(\\pi,\\nu)=\\frac{1}{n(n-1)}\\sum_{i=1}^{n}\\sum_{i\\neq j}^{n}k_{\\pi}(y_{i},y_{i}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Alternatively, we can estimate $\\mathrm{KSD}_{k}^{2}(\\pi,\\nu)$ using a biased (but non-negative) ${\\mathrm{V}}.$ -statistic of the form [46, Section 4] ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{KSD}}_{k,V}^{2}(\\pi,\\nu)={\\frac{1}{n^{2}}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}k_{\\pi}(y_{i},y_{i}^{\\prime}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In all of our numerical experiments, we calculate the U- and $\\mathrm{V}_{\\mathrm{-}}$ statistics using the inverse multiquadratic kernel $k(x,x^{\\prime})\\stackrel{.}{=}(1+(x-x^{\\prime})^{T}(x-x^{\\prime}))^{\\beta}$ due to its favourable convergence properties [26, Theorem 8], setting $\\beta=-\\frac{1}{2}$ . ", "page_idx": 17}, {"type": "text", "text": "C.2 4-mode Gaussian mixture ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "For this experiment, all methods use $N=128$ parallel chains for training and 128 hidden dimensions for all neural networks. Methods with a MALA kernel use a step size of 0.2, and methods with splines use 4 coupling layers with 8 bins and range limited to $[-16,16]$ . ", "page_idx": 18}, {"type": "text", "text": "In Table 3, we present results for $K=10^{3}$ iterations. Since MFM is much more efficient than other methods, we also report results for a great number of total iterations. Table 4 contains results for $K=5\\cdot10^{3}$ iterations for MFM and AT-SMC. In the main text, we present results for $K=5\\cdot10^{3}$ learning iterations for MFM and AT-SMC, and $K=10^{3}$ iterations for the other algorithms, since this renders the total computational cost of all algorithms somewhat comparable. ", "page_idx": 18}, {"type": "text", "text": "For both choices of $K$ , we also present results using Hutchinson\u2019s trace estimator (HTE) [27, 36] to calculate the MH acceptance probability in the flow-informed Markov transition kernel. As expected, its effect on sample quality becomes more apparent as $k_{Q}$ increases. However, its effect on computation time is less significant than in larger dimensional examples. ", "page_idx": 18}, {"type": "table", "img_path": "amJyuVqSaf/tmp/3ce76ff42abae2b0617eb63ef46523fb2b46f54f2e95b15e6181c954ce04f2f1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 3: Diagnostics for the 4-mode Gaussian mixture with $K=10^{3}$ . $\\mathbb{E}_{[\\phi_{1}]_{\\#}p_{0}}\\log\\pi$ is the Monte Carlo approximation of the log-target density using the learned flow to generate samples; KSD U-stat and $\\mathrm{V}.$ -stat are the Kernel Stein Discrepancy U- and V-statistics between the target and samples generated from the learned flow; MMD is the Maximum Mean Discrepancy between real samples from the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. ", "page_idx": 18}, {"type": "table", "img_path": "amJyuVqSaf/tmp/2d375327b7c1e3814132c4e2d32a4dcb6dddac22ccd4e327f05c59202079fd84.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 4: Diagnostics for the 4-mode Gaussian mixture with $K=5\\cdot10^{3}$ . $\\mathbb{E}_{[\\phi_{1}]_{\\#}p_{0}}\\log\\pi$ is the Monte Carlo approximation of the log-target density using the learned flow to generate samples; KSD U-stat and V-stat are the Kernel Stein Discrepancy U- and ${\\mathrm{V}}.$ -statistics between the target and samples generated from the learned flow; MMD is the Maximum Mean Discrepancy between real samples from the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. ", "page_idx": 18}, {"type": "text", "text": "C.3 16-mode Gaussian Mixture ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Like the 4-mode example, all methods use $N=128$ parallel chains for training and 128 hidden dimensions for all neural networks. Methods with a MALA kernel use a step size of 0.2, and methods with splines use 4 coupling layers with 8 bins and range limited to $[-16,16]$ . In Table 5 we present results for $K=10^{3}$ iterations. In Table 6, we provide results for MFM and AT-SMC for $K=\\dot{5}\\times10^{3}$ learning iterations. In the main text, we present results for $K=5\\cdot10^{3}$ learning iterations for MFM and AT-SMC and $K=10^{3}$ iterations for all other algorithms, which yields a more comparable total computation time. ", "page_idx": 18}, {"type": "table", "img_path": "amJyuVqSaf/tmp/92f53dd46d634a3b87d5dd2a948897573bbd72aa1610fff2e1979956341cdf79.jpg", "table_caption": [], "table_footnote": ["Table 5: Diagnostics for the 16-mode Gaussian mixture with $K=10^{3}$ . $\\mathbb{E}_{[\\phi_{1}]\\neq p_{0}}$ log $\\pi$ is the Monte Carlo approximation of the log-target density using the learned flow to generate samples; KSD U-stat and V-stat are the Kernel Stein Discrepancy U- and V-statistics between the target and samples generated from the learned flow; MMD is the Maximum Mean Discrepancy between real samples from the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. "], "page_idx": 19}, {"type": "table", "img_path": "amJyuVqSaf/tmp/22f79aa7e65606d6c08b9350b6d4074aa8936aa23248eea455a593092f7b43bc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Table 6: Diagnostics for the 16-mode Gaussian mixture with $K\\,=\\,5\\cdot10^{3}$ . $\\mathbb{E}_{[\\phi_{1}]\\neq p_{0}}$ log \u03c0 is the Monte Carlo approximation of the log-target density using the learned flow to generate samples; KSD U-stat and V-stat are the Kernel Stein Discrepancy U- and V-statistics between the target and samples generated from the learned flow; MMD is the Maximum Mean Discrepancy between real samples from the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. ", "page_idx": 19}, {"type": "text", "text": "C.4 Many Well ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We also present a synthetic problem approximating the 32-dimensional Many Well distribution given by the product of 16 copies of the 2-dimensional Double Well distribution [e.g., 52, 58, 81], ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\log p(x_{1},x_{2})=-x_{1}^{4}+6x_{1}^{2}+\\frac{1}{2}x_{1}-\\frac{1}{2}x_{2}^{2}+\\mathrm{constant},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where each copy of the Double Well is evaluated on a different pair of the 32 inputs. The 32- dimensional Many Well has $2^{16}=65536$ modes, one for each possible choice of mode in each of the 16 copies of the double well. We can obtain exact samples from the Many Well by sampling each independent copy of the Double Well. ", "page_idx": 19}, {"type": "text", "text": "Like previous synthetic examples, all methods use $N=128$ parallel chains for training and 128 hidden dimensions for all neural networks. Methods with a MALA kernel use a step size of 0.1, and methods with splines use 4 coupling layers with 8 bins and a range limited to $[-16,16]$ . Table 7 presents results for $K=10^{3}$ iterations. In Table 8, we provide results for MFM and AT-SMC for $K=5\\times10^{3}$ learning iterations. In the main text, we present results for $K=5\\cdot10^{3}$ learning iterations for MFM and AT-SMC and $K=10^{3}$ iterations for all other algorithms, which yields a more comparable total computation time. ", "page_idx": 19}, {"type": "text", "text": "C.5 Field system ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The stochastic Allen\u2013Cahn equation is defined in terms of a random field $\\phi:[0,1]\\rightarrow\\mathbb{R}$ satisfying the following stochastic partial differential equation [e.g., 24, Section V]: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial\\phi}{\\partial t}=a\\frac{\\partial^{2}\\phi}{\\partial s^{2}}+a^{-1}(\\phi-\\phi^{3})+\\sqrt{2\\beta^{-1}}\\eta(t,s),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "table", "img_path": "amJyuVqSaf/tmp/a092f93db2a00be2c70452b808bc0d3b7de4eda83530dbbd55c0c1d31f18ef26.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 7: Diagnostics for the many well with $K=10^{3}$ . $\\mathbb{E}_{[\\phi_{1}]\\neq p_{0}}$ log $\\pi$ is the Monte Carlo approximation of the log-target density using the learned flow to generate samples; KSD U-stat and V-stat are the Kernel Stein Discrepancy U- and V-statistics between the target and samples generated from the learned flow; MMD is the Maximum Mean Discrepancy between real samples from the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. ", "page_idx": 20}, {"type": "table", "img_path": "amJyuVqSaf/tmp/34bf1cd2ee2d5403f367ae2ccf8fc83f0f711c8921d207c607a6b1620d881fdc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 8: Diagnostics for the many well with $K\\;=\\;5\\cdot10^{3}$ . $\\mathbb{E}_{[\\phi_{1}]_{\\#}p_{0}}\\log\\pi$ is the Monte Carlo approximation of the log-target density using the learned flow to generate samples; KSD U-stat and $\\mathrm{V}.$ -stat are the Kernel Stein Discrepancy U- and ${\\mathrm{V}}.$ -statistics between the target and samples generated from the learned flow; MMD is the Maximum Mean Discrepancy between real samples from the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. ", "page_idx": 20}, {"type": "text", "text": "where $a\\,>\\,0$ is a parameter, $\\beta$ is the inverse temperature, $s\\in[0,1]$ denotes the spatial variable, and $\\eta$ is spatiotemporal white noise. We impose Dirichlet boundary conditions throughout, so that $\\phi(s\\stackrel{.}{=}0)\\stackrel{.}{=}\\phi(s\\stackrel{.}{=}1)=0$ . ", "page_idx": 20}, {"type": "text", "text": "The associated Hamiltonian, reflecting a spatial coupling term penalizing changes in $\\phi$ , takes the form: ", "page_idx": 20}, {"type": "equation", "text": "$$\nU_{*}[\\phi]=\\beta\\int_{0}^{1}\\left[\\frac{a}{2}\\left(\\frac{\\partial\\phi}{\\partial s}\\right)^{2}+\\frac{1}{4a}\\left(1-\\phi^{2}(s)\\right)^{2}\\right]d s.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "At low temperatures, this coupling induces alignment of the field in either the positive or negative direction, leading to two global minima, $\\phi_{+}$ and $\\phi_{-}$ , with typical values of $\\pm1$ . ", "page_idx": 20}, {"type": "text", "text": "For this example, all methods use $N=1024$ parallel chains for training and 256 hidden dimensions for all neural networks. Methods with a MALA kernel use a step size of 0.0001, and methods with splines use 8 coupling layers with 8 bins and range limited to $[-5,5]$ . Results for $K=10^{4}$ learning iterations are presented in Table 9. In the main text, we present results for $k_{Q}=10^{2}$ for MFM. ", "page_idx": 20}, {"type": "text", "text": "Interestingly, in high-dimensional problems such as this and the following, the version of the algorithm using Hutchinson\u2019s trace estimator [27, 36] to calculate the MH acceptance probability has little apparent effect on the approximation quality. It does, however, have a significant impact on the computation time. ", "page_idx": 20}, {"type": "text", "text": "C.6 Log-Gaussian Cox process ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The original $10\\times10$ square meter plot is standardized to the unit square. We discretize the unit square $[0,1]^{2}$ into a $M=40\\times40$ regular grid. The latent intensity process $\\Lambda=\\{\\Lambda_{m}\\}_{m\\in M}$ is specified as $\\Lambda_{m}=\\exp(X_{m})$ , where $X=\\{X_{m}\\}_{m\\in M}$ is a Gaussian process with a constant mean $\\mu_{0}\\in\\mathbb{R}$ and exponential covariance function $\\Sigma_{0}(m,\\bar{n})={\\sigma}^{2}\\exp{(-{\\vert\\bar{m}-n\\vert}/{(40\\beta)})}$ for $m,n\\in M$ , i.e. $X\\sim\\mathcal{N}(\\mu_{0}1_{d},\\Sigma_{0})$ for $1_{d}=[1,\\dots,1]^{T}\\in\\mathbb{R}^{d}$ with dimension $d=1600$ . The chosen parameter values are $\\sigma^{2}=1.91$ , $\\beta=1/33$ , and $\\mu_{0}=\\log(126)\\!-\\!\\sigma^{2}/2$ , corresponding to the values estimated in [53]. The number of points in each grid cell $\\bar{Y_{=}}\\{\\dot{Y_{m}}\\}_{m\\in M}\\in\\mathbb{N}^{40\\dot{\\times}40}$ are modelled as conditionally independent and Poisson distributed with means $a\\Lambda_{m}$ , ", "page_idx": 20}, {"type": "table", "img_path": "amJyuVqSaf/tmp/1b6334cf28150c2da50e11673b81b83b5540f5bee4fa2e6e106c08cf424e6f10.jpg", "table_caption": [], "table_footnote": ["Table 9: Diagnostics for the field system with $K=10^{4}$ . $\\mathbb{E}_{[\\phi_{1}]\\neq p_{0}}$ log \u03c0 is the Monte Carlo approximation of the log-target density using the learned flow to generate samples; KSD U-stat and V-stat are the Kernel Stein Discrepancy U- and V-statistics between the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. "], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathscr{L}(Y|X)=\\prod_{m\\in[1:30]^{2}}\\exp(x_{m}y_{m}-a\\exp(x_{m})),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $a=1/40^{2}$ represents the area of each grid cell. For this example, all methods use $N=128$ parallel chains for training and 1024 hidden dimensions for all neural networks. Methods with a MALA kernel use a step size of 0.01, and methods with splines use 8 coupling layers with 8 bins and range limited to $[-10,10]$ . Results for $K=10^{4}$ learning iterations are presented in Table 10. In the main text, we present results for $k_{Q}=10^{3}$ for MFM. We were unable to run NF-MCMC and FAB for $K=10^{4}$ iterations because of memory issues; instead, we present results for $K=10^{3}$ iterations only for the models using discrete normalizing flows. ", "page_idx": 21}, {"type": "table", "img_path": "amJyuVqSaf/tmp/080057f70e63e9071d452c7c5511ae88c813640b5e330e25632b54d56fc74001.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "Table 10: Log Gaussian Cox point process diagnostics for $K=10^{4}$ where $\\mathbb{E}_{[\\phi_{1}]_{\\#}p_{0}}\\log\\pi$ is the Monte Carlo approximation of the log-target density using the learned flow to generate samples; KSD U-stat and V-stat are the Kernelized Stein discrepancy\u2019s U- and V-statistics between the target and samples generated from the learned flow. Results are averaged and empirical $95\\%$ confidence intervals over 10 independent runs. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The claims made in the abstract and the introduction provide an accurate reflection of the theoretical, methodological, and experiment contributions in the paper. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The final section includes a discussion of the limitations of this work, including the computational efficiency of the proposed method, situations in which the proposed method may be outperformed by other baselines, and hyperparameters which may strongly influence the performance of the proposed method. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Our theoretical result (Proposition 3.1 in Section 3) is accompanied by a full set of assumptions and a complete proof (in Appendix B). Any results upon which the proof relies are properly cited. In addition, all of the theorems, formulas, and proofs in the paper are numbered and cross-referenced. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper includes full details of the algorithm used to obtain the experimental results in the paper, including all of the hyper-parameter settings for each of the different numerical experiments. Our code is also made publicly available on GitHub (see Section 5). ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 23}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Our code is made publicly available on GitHub (see Section 5). Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We provide all of the training and test details, including hyperparameter settings, train-test splits, choice of optimizer, etc., necessary to understand and reproduce our experimental results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We report error bars for all of our numerical experiments (see Section 5). In the text, we provide full details regarding how these error bars are calculated, as well as the factors which vary between each experimental run. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We include full details of the type of compute workers used in the main text (see Appendix C). We also include details of the running time for each of the main numerical experiments, for both our proposed method and the relevant baselines. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The research conducted in the paper conforms with NeurIPS Code of Ethics in every respect, including with regards to Potential Harms Caused by the Research Process, Societal Impact and Potential Harmful Consequences, and Impact Mitigation Measures. All of the authors have read and reviewed the NeurIPS Code of Ethics to ensure that the research conducted in this paper conforms with the guidelines. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper contains foundational research on a new method for approximate statistical inference. It is not tied to any particular applications and, as such, these are not directly discussed in the main text. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not include any datasets or models associated with a high risk of misuse. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The creators and original owners of assets used in the paper (e.g., code for relevant baselines) are properly credited (see Section 5). We both cite the original papers that produced these assets, and include URLs to the code. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release any new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing experiments or any research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing experiments or any research with human subjects. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]