[{"figure_path": "WyQW4G57Zd/tables/tables_5_1.jpg", "caption": "Table 1: A Power(%) of a kernel two-sample test.", "description": "This table presents the results of a kernel two-sample test, a statistical test used to determine whether two samples are drawn from the same distribution.  The test was performed on three datasets (Quadratic, Melbourne, and Gridwatch) for 1D function generation, comparing the performance of the proposed DBFS method against two existing baselines, NDP [22] and SP-SGM [52]. The 'Power' values represent the probability that the test correctly rejects the null hypothesis (that the samples are from the same distribution) when the alternative hypothesis is true (the samples are from different distributions).  Lower power suggests lower performance.  The results show DBFS performs comparably or better than existing methods on these datasets.", "section": "5.1 Bridge Matching"}, {"figure_path": "WyQW4G57Zd/tables/tables_7_1.jpg", "caption": "Table 1: A Power(%) of a kernel two-sample test.", "description": "This table presents the results of a kernel two-sample test, comparing the performance of three different methods (NDP, SP-SGM, and DBFS) in generating 1D functions on three datasets (Quadratic, Melbourne, and Gridwatch). The power of the test represents the probability of correctly rejecting the null hypothesis that the generated samples and the real data come from the same distribution. Lower power indicates that the generated samples are more similar to the real data.  DBFS demonstrates comparable performance to the baselines.", "section": "5.1 Bridge Matching"}, {"figure_path": "WyQW4G57Zd/tables/tables_8_1.jpg", "caption": "Table 2: Test FID on unpaired image transfer task. (A) EMNIST \u2192 MNIST, (B) AFHQ-64 Wild \u2192 Cat.", "description": "This table shows the Fr\u00e9chet Inception Distance (FID) scores for unpaired image transfer tasks using different methods.  Lower FID scores indicate better performance.  The table compares the proposed DBFS model against two existing methods, IDBM and DSBM, on two distinct image transfer tasks: EMNIST to MNIST and AFHQ Wild to Cat. The results demonstrate the comparative performance of the DBFS model.", "section": "5.1 Bridge Matching"}, {"figure_path": "WyQW4G57Zd/tables/tables_9_1.jpg", "caption": "Table 4: Regression results. \"context\" and \"target\" refer to the log-likelihoods at O and T, respectively.", "description": "This table presents the results of a regression experiment comparing three different methods: CNP, NP, and DBFS.  The methods were evaluated on three different kernel types: RBF, Mat\u00e9rn 5/2, and Periodic. For each method and kernel type, the table shows the context and target log-likelihoods.  The context log-likelihood is a measure of how well the model predicts the observed data points (O), while the target log-likelihood measures how well the model predicts the unobserved data points (T). Higher log-likelihood values indicate better performance.", "section": "5.2 Bayesian Learning"}, {"figure_path": "WyQW4G57Zd/tables/tables_9_2.jpg", "caption": "Table 3: Test imputation RMSE on Physionet.", "description": "This table presents the root mean squared error (RMSE) for imputation tasks on the Physionet dataset.  Three methods, CSDI, DSDP-GP, and DBFS (the authors' method), are compared across three levels of data missingness (10%, 50%, and 90%). Lower RMSE values indicate better performance.", "section": "5.2 Bayesian Learning"}, {"figure_path": "WyQW4G57Zd/tables/tables_23_1.jpg", "caption": "Table A.1: Network Hyper-parameters", "description": "This table lists the hyperparameters used for the Transformer-based network architecture in the experiments.  It shows the latent dimension, position dimension, number of heads, number of encoder blocks, number of decoder blocks, number of self-attention layers per block, and the total number of parameters for both the MNIST and AFHQ datasets.", "section": "A.8 Experimental Details"}]