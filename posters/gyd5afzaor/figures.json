[{"figure_path": "GYd5AfZaor/figures/figures_2_1.jpg", "caption": "Figure 1: (a) An example of t-SNE illustration of contrastive fragment pairing. The data with label noise are grouped into six fragments (f \u2208 [1-6]) and formed into three contrastive pairs (f\u2208 [1, 4], [2, 5], [3, 6]). Contrastive fragment pairing transforms some of closed-set noise (whose ground truth is within the target label set) into open-set noise (whose ground truth is not within the label set). For example, in the [1,4] figure, label noise whose ground truth fragment is either 1 or 4 is closed-set noise, and the others are open-set noise. The t-SNE illustration shows that learned features of open-set noises tend to reside outside the feature clusters of the clean samples. (b) The open-set noise is less harmful with much lower errors (MRAE) in the downstream regression. (c) The contrastive pairing ([1, 4], [2, 5], [3, 6]) is more effective than using all-fragments together ([1-6]), resulting in much lower MRAE scores. All experiments are based on IMDB-Clean-B with more details in Appendix G.4-G.5.", "description": "This figure demonstrates the effect of contrastive fragment pairing using t-SNE visualizations.  Panel (a) shows how the method transforms some closed-set noisy samples into open-set noisy samples, which are less harmful.  Panel (b) shows that open-set noise leads to lower errors. Panel (c) compares the performance of contrastive fragment pairing against using all fragments together, showing that the contrastive approach is more effective.", "section": "2 ConFrag: Contrastive Fragmentation"}, {"figure_path": "GYd5AfZaor/figures/figures_2_2.jpg", "caption": "Figure 2: The contrastive fragment pairing algorithm.", "description": "The figure illustrates the algorithm for contrastive fragment pairing.  The dataset is first divided into fragments based on the continuous labels. Then, a graph is constructed where each fragment is a node, and the edge weight between nodes represents the distance between the closest samples of the fragments in the label space. Finally, the algorithm finds a perfect matching (a pairing of fragments) that maximizes the minimum edge weight, ensuring the selected pairs are maximally contrasting.", "section": "2 ConFrag: Contrastive Fragmentation"}, {"figure_path": "GYd5AfZaor/figures/figures_4_1.jpg", "caption": "Figure 3: Contrastive Fragmentation framework. (a) The overall sequential process of our framework. (b) Shows the fragmentation of the continuous label space to obtain contrasting fragment pairs (\u00a7 2.1) and train feature extractors on them. (c) Sample Selection by Mixture of Neighboring Fragments obtains the selection probability in both prediction and representation perspectives (\u00a7 2.3). (d) Illustration of Neighborhood Jittering (\u00a7 2.4).", "description": "This figure illustrates the ConFrag framework, showing the steps involved in its process.  (a) provides a high-level overview of the framework.  (b) details the fragmentation of the continuous label space into contrastive fragment pairs and the subsequent training of feature extractors. (c) explains how sample selection is performed using a mixture of neighboring fragments. (d) demonstrates the concept of neighborhood jittering to enhance the selection process.", "section": "2 ConFrag: Contrastive Fragmentation"}, {"figure_path": "GYd5AfZaor/figures/figures_5_1.jpg", "caption": "Figure 4: Jittering analysis. (a) When trained without jittering, feature extractors easily overfit the noisy training data (yellow-shaded region), while jittering-regularized feature extractors robustly learn from the noisy training data. (b) Overfitted feature extractors (yellow-shaded region) on noisy samples increase their likelihood, leading to a higher selection rate and ERR. It exhibits nearly twice higher ERR (a lower value is better). (c) Most importantly, jittering regularization improves performance in regression.", "description": "This figure shows the effects of neighborhood jittering regularization on the performance of feature extractors, sample selection, and regression.  Panel (a) demonstrates that without jittering, feature extractors trained on noisy data tend to overfit, while jittering helps them generalize better. Panel (b) shows that the overfitting leads to higher selection rates and higher Error Residual Ratios (ERR), indicating poorer sample selection quality.  Finally, panel (c) reveals that jittering regularization improves the final regression model's performance.", "section": "2.4 Neighborhood Jittering"}, {"figure_path": "GYd5AfZaor/figures/figures_8_1.jpg", "caption": "Figure 5: Selection/ERR/MRAE comparison between ConFrag and strong baselines of CNLCU-H, BMM, DY-S, AUX and Selfie on IMDB-Clean-B. We exclude the performance during the warm-up.", "description": "This figure compares the performance of ConFrag against five other strong baselines (CNLCU-H, BMM, DY-S, AUX, and Selfie) on the IMDB-Clean-B dataset. The comparison is based on three metrics: selection rate, error residual ratio (ERR), and mean relative absolute error (MRAE). The warm-up phase of training is excluded from the analysis.  The figure shows that ConFrag achieves a better balance between selection rate, ERR, and MRAE compared to the other methods. Specifically, ConFrag demonstrates a lower ERR (indicating cleaner selected samples) and maintains a reasonably high selection rate, ultimately resulting in better regression performance (lower MRAE).", "section": "4.3 Results and Discussion"}, {"figure_path": "GYd5AfZaor/figures/figures_9_1.jpg", "caption": "Figure 3: Contrastive Fragmentation framework. (a) The overall sequential process of our framework. (b) Shows the fragmentation of the continuous label space to obtain contrasting fragment pairs (\u00a7 2.1) and train feature extractors on them. (c) Sample Selection by Mixture of Neighboring Fragments obtains the selection probability in both prediction and representation perspectives (\u00a7 2.3). (d) Illustration of Neighborhood Jittering (\u00a7 2.4).", "description": "This figure illustrates the ConFrag framework, showing the four main steps involved: (a) an overview of the entire process; (b) the fragmentation of the continuous label space into contrasting fragment pairs to train feature extractors; (c) sample selection based on both predictive and representational aspects using a mixture of neighboring fragments; and (d) the application of neighborhood jittering for regularization.  Each step visually shows the data transformations and models used.", "section": "2 ConFrag: Contrastive Fragmentation"}, {"figure_path": "GYd5AfZaor/figures/figures_20_1.jpg", "caption": "Figure 1: (a) An example of t-SNE illustration of contrastive fragment pairing. The data with label noise are grouped into six fragments (f \u2208 [1-6]) and formed into three contrastive pairs (f\u2208 [1, 4], [2, 5], [3, 6]). Contrastive fragment pairing transforms some of closed-set noise (whose ground truth is within the target label set) into open-set noise (whose ground truth is not within the label set). For example, in the [1,4] figure, label noise whose ground truth fragment is either 1 or 4 is closed-set noise, and the others are open-set noise. The t-SNE illustration shows that learned features of open-set noises tend to reside outside the feature clusters of the clean samples. (b) The open-set noise is less harmful with much lower errors (MRAE) in the downstream regression. (c) The contrastive pairing ([1, 4], [2, 5], [3, 6]) is more effective than using all-fragments together ([1-6]), resulting in much lower MRAE scores. All experiments are based on IMDB-Clean-B with more details in Appendix G.4-G.5.", "description": "This figure shows the effect of contrastive fragment pairing using t-SNE visualization.  It demonstrates how the method transforms some closed-set noise into open-set noise, resulting in lower errors.  The contrastive pairing approach is shown to be more effective than using all fragments together.", "section": "2 ConFrag: Contrastive Fragmentation"}, {"figure_path": "GYd5AfZaor/figures/figures_21_1.jpg", "caption": "Figure 6: Analysis with 40% symmetric noise. (a) Comparison between the proposed contrastive pairing and other pairings on IMDB-Clean-B. (b) Comparison between fragment numbers on SHIFT15M-B and IMDB-Clean-B.", "description": "Figure 6 presents a comparison of the performance of contrastive fragment pairing against various alternative pairing strategies and different numbers of fragments.  The left panel (a) focuses on the IMDB-Clean-B dataset and illustrates the superior performance of contrastive fragment pairing, showcasing its effectiveness in minimizing errors compared to using all fragments or other pairing methods. The right panel (b) explores the impact of varying the number of fragments on two datasets, SHIFT15M-B and IMDB-Clean-B, showing relatively stable performance on SHIFT15M-B but a decline on IMDB-Clean-B when using more fragments, likely due to overfitting.", "section": "2.1 Contrastive Fragment Pairing"}, {"figure_path": "GYd5AfZaor/figures/figures_25_1.jpg", "caption": "Figure 9: Random Gaussian Noise. (a) Gaussian noise injected from the uniformly sampled random standard deviation between [1, 30]. (b) Gaussian noise injected from uniformly sampled random standard deviation between [1, 50].", "description": "The figure shows two heatmaps visualizing the effects of random Gaussian noise injection on the label space.  In (a), Gaussian noise is added with standard deviations randomly sampled from the range 1 to 30; in (b), the range is 1 to 50. The heatmaps depict the relationship between clean labels and their corresponding noisy labels, illustrating the variability and severity of the noise introduced. Darker colors represent a higher probability of a particular clean label being assigned a particular noisy label.", "section": "F.4 Random Gaussian Noise"}, {"figure_path": "GYd5AfZaor/figures/figures_27_1.jpg", "caption": "Figure 10: Fragment number analysis compares the Selection rate, ERR and MRAE on IMDB-Clean-B with symmetric 40% noise.", "description": "The figure shows the training curves of selection rate, error residual ratio (ERR), and mean relative absolute error (MRAE) on the IMDB-Clean-B dataset with 40% symmetric noise using different numbers of fragments (F). It compares the performance of ConFrag with different fragment numbers (F = 4, 6, 8, 10) against the vanilla model. It demonstrates that ConFrag maintains stable performance across different fragment numbers, while the vanilla model's performance degrades over time due to memorization of noisy samples. ", "section": "G.2 Fragment Number Analysis"}, {"figure_path": "GYd5AfZaor/figures/figures_28_1.jpg", "caption": "Figure 10: Fragment number analysis compares the Selection rate, ERR and MRAE on IMDB-Clean-B with symmetric 40% noise.", "description": "This figure shows the impact of varying the number of fragments (F) in the ConFrag model on three key metrics: Selection rate, Error Residual Ratio (ERR), and Mean Relative Absolute Error (MRAE).  The experiment is performed on the IMDB-Clean-B dataset with 40% symmetric label noise.  The results show how the performance changes across different values of F (4, 6, 8, 10), along with a comparison to a vanilla model (without ConFrag).  Each line represents the trends of the three metrics over training epochs.", "section": "G.2 Fragment Number Analysis"}, {"figure_path": "GYd5AfZaor/figures/figures_28_2.jpg", "caption": "Figure 12: Hyperparameter K analysis compares the Selection rate, ERR and MRAE on IMDB-Clean-B with symmetric 40% noise.", "description": "The figure shows the effects of changing the hyperparameter K on the performance of the ConFrag model.  The hyperparameter K controls the number of nearest neighbors considered when calculating neighborhood agreement for sample selection. The plots show selection rate, error residual ratio (ERR), and mean relative absolute error (MRAE) over training epochs for three different values of K (3, 5, and 7).  The results indicate that there is an optimal value of K for balancing the trade-off between selecting many samples (high selection rate) and selecting primarily clean samples (low ERR and MRAE).", "section": "2.4 Neighborhood Jittering"}, {"figure_path": "GYd5AfZaor/figures/figures_28_3.jpg", "caption": "Figure 12: Hyperparameter K analysis compares the Selection rate, ERR and MRAE on IMDB-Clean-B with symmetric 40% noise.", "description": "This figure shows the impact of varying the K-nearest neighbor parameter (K) on the performance of the ConFrag model. The three metrics (Selection Rate, ERR, and MRAE) are plotted against the training epoch for different values of K (3, 5, and 7).  The Selection Rate represents the percentage of data points selected by the model as clean. ERR (Error Residual Ratio) reflects the ratio of error in the selected samples to the total error, while MRAE (Mean Relative Absolute Error) is a metric for regression performance. The plot illustrates how different values of K affect the model's ability to select clean data and its overall performance.", "section": "2.4 Neighborhood Jittering"}, {"figure_path": "GYd5AfZaor/figures/figures_29_1.jpg", "caption": "Figure 4: Jittering analysis. (a) When trained without jittering, feature extractors easily overfit the noisy training data (yellow-shaded region), while jittering-regularized feature extractors robustly learn from the noisy training data. (b) Overfitted feature extractors (yellow-shaded region) on noisy samples increase their likelihood, leading to a higher selection rate and ERR. It exhibits nearly twice higher ERR (a lower value is better). (c) Most importantly, jittering regularization improves performance in regression.", "description": "This figure demonstrates the effect of neighborhood jittering on feature extraction, sample selection, and regression performance.  Panel (a) shows that without jittering, feature extractors overfit to noisy data, while jittering improves robustness. Panel (b) shows how overfitting leads to increased selection rates and error residual ratios (ERR). Finally, panel (c) highlights that jittering significantly improves regression performance.", "section": "2.4 Neighborhood Jittering"}, {"figure_path": "GYd5AfZaor/figures/figures_29_2.jpg", "caption": "Figure 10: Fragment number analysis compares the Selection rate, ERR and MRAE on IMDB-Clean-B with symmetric 40% noise.", "description": "The figure shows the comparison of selection rate, error residual ratio (ERR), and mean relative absolute error (MRAE) on the IMDB-Clean-B dataset with 40% symmetric noise, across different numbers of fragments (F) used in ConFrag.  The results illustrate how the performance metrics vary with different fragmentation schemes, providing insights into the optimal number of fragments for balancing model generalization and noise mitigation.", "section": "G.2 Fragment Number Analysis"}, {"figure_path": "GYd5AfZaor/figures/figures_30_1.jpg", "caption": "Figure 3: Contrastive Fragmentation framework. (a) The overall sequential process of our framework. (b) Shows the fragmentation of the continuous label space to obtain contrasting fragment pairs (\u00a7 2.1) and train feature extractors on them. (c) Sample Selection by Mixture of Neighboring Fragments obtains the selection probability in both prediction and representation perspectives (\u00a7 2.3). (d) Illustration of Neighborhood Jittering (\u00a7 2.4).", "description": "This figure illustrates the ConFrag framework, showing the process of fragmenting the continuous label space into contrasting fragment pairs and training separate feature extractors on them. It then depicts the sample selection process using a mixture of neighboring fragments and the use of neighborhood jittering regularization to enhance the model's performance and mitigate overfitting.", "section": "2 ConFrag: Contrastive Fragmentation"}, {"figure_path": "GYd5AfZaor/figures/figures_30_2.jpg", "caption": "Figure 10: Fragment number analysis compares the Selection rate, ERR and MRAE on IMDB-Clean-B with symmetric 40% noise.", "description": "This figure shows a comparison of the selection rate, error residual ratio (ERR), and mean relative absolute error (MRAE) for different numbers of fragments (F) in the ConFrag model.  The results are shown for the IMDB-Clean-B dataset with 40% symmetric noise.  The plot illustrates how the model's performance changes as the number of fragments is varied.  The figure allows for a visual assessment of the impact of the hyperparameter F on the overall performance of the ConFrag framework.", "section": "G.2 Fragment Number Analysis"}, {"figure_path": "GYd5AfZaor/figures/figures_31_1.jpg", "caption": "Figure 1: (a) An example of t-SNE illustration of contrastive fragment pairing. The data with label noise are grouped into six fragments (f \u2208 [1-6]) and formed into three contrastive pairs (f\u2208 [1, 4], [2, 5], [3, 6]). Contrastive fragment pairing transforms some of closed-set noise (whose ground truth is within the target label set) into open-set noise (whose ground truth is not within the label set). For example, in the [1,4] figure, label noise whose ground truth fragment is either 1 or 4 is closed-set noise, and the others are open-set noise. The t-SNE illustration shows that learned features of open-set noises tend to reside outside the feature clusters of the clean samples. (b) The open-set noise is less harmful with much lower errors (MRAE) in the downstream regression. (c) The contrastive pairing ([1, 4], [2, 5], [3, 6]) is more effective than using all-fragments together ([1-6]), resulting in much lower MRAE scores. All experiments are based on IMDB-Clean-B with more details in Appendix G.4-G.5.", "description": "This figure illustrates the concept of contrastive fragment pairing using t-SNE visualizations.  It shows how the proposed method transforms some closed-set noise (noise where the true label is within the selected fragments) into open-set noise (noise where the true label is outside the selected fragments). This transformation is beneficial because open-set noise is less detrimental to model performance.  The figure also compares the effectiveness of contrastive pairing versus using all fragments together, demonstrating that contrastive pairing leads to improved downstream regression performance.", "section": "2.1 Contrastive Fragment Pairing"}, {"figure_path": "GYd5AfZaor/figures/figures_32_1.jpg", "caption": "Figure 19: Closed-set/open-set noise analysis displays the selection, ERR and MRAE when closed-set or open-set noisy samples are injected into the clean dataset. The experiments are based on IMDB-Clean-B.", "description": "This figure displays the result of injecting both closed-set and open-set noisy samples into the clean dataset, and how that affects selection rate, ERR, and MRAE.  Closed-set noise is more harmful than open-set noise, because closed-set samples have labels that fall within the correct fragment's boundaries, but are not correctly labeled. Open-set samples have labels outside of any correct fragment. The figure shows that the approach used is beneficial for reducing the effects of closed-set noise.", "section": "G.5 Closed-Set versus Open-Set Noise"}, {"figure_path": "GYd5AfZaor/figures/figures_34_1.jpg", "caption": "Figure 4: Jittering analysis. (a) When trained without jittering, feature extractors easily overfit the noisy training data (yellow-shaded region), while jittering-regularized feature extractors robustly learn from the noisy training data. (b) Overfitted feature extractors (yellow-shaded region) on noisy samples increase their likelihood, leading to a higher selection rate and ERR. It exhibits nearly twice higher ERR (a lower value is better). (c) Most importantly, jittering regularization improves performance in regression.", "description": "This figure shows the effect of neighborhood jittering regularization on the performance of the ConFrag model.  Panel (a) compares the average accuracy of feature extractors trained with and without jittering, showing that jittering prevents overfitting to noisy data. Panel (b) shows that overfitting leads to a higher selection rate and a much larger ERR (Error Residual Ratio). A lower ERR value indicates better performance.  Finally, panel (c) demonstrates that jittering improves the regression performance of the model.", "section": "Neighborhood Jittering"}, {"figure_path": "GYd5AfZaor/figures/figures_35_1.jpg", "caption": "Figure 3: Contrastive Fragmentation framework. (a) The overall sequential process of our framework. (b) Shows the fragmentation of the continuous label space to obtain contrasting fragment pairs (\u00a7 2.1) and train feature extractors on them. (c) Sample Selection by Mixture of Neighboring Fragments obtains the selection probability in both prediction and representation perspectives (\u00a7 2.3). (d) Illustration of Neighborhood Jittering (\u00a7 2.4).", "description": "This figure illustrates the ConFrag framework, which is a novel method for noisy label regression. The framework has four main steps:\n\n1.  **Fragmentation and Contrastive Pairing:** The continuous label space is divided into fragments, and the most distant fragments are paired to form contrastive pairs.\n2.  **Training Feature Extractors:** Expert feature extractors are trained on the contrastive fragment pairs.\n3.  **Selection by Mixture of Neighboring Fragments:** A mixture model is used to select clean samples based on neighborhood agreement.\n4.  **Neighborhood Jittering:** Neighborhood jittering is used as a regularizer to enhance the selection process.\n\nThe figure shows how these steps work together to improve the performance of noisy label regression.", "section": "2 ConFrag: Contrastive Fragmentation"}, {"figure_path": "GYd5AfZaor/figures/figures_36_1.jpg", "caption": "Figure 5: Selection/ERR/MRAE comparison between ConFrag and strong baselines of CNLCU-H, BMM, DY-S, AUX and Selfie on IMDB-Clean-B. We exclude the performance during the warm-up.", "description": "This figure compares the performance of ConFrag against five other strong baselines (CNLCU-H, BMM, DY-S, AUX, and Selfie) on the IMDB-Clean-B dataset.  The comparison is made across three metrics: Selection rate, Error Residual Ratio (ERR), and Mean Relative Absolute Error (MRAE). The warm-up phase of training is excluded from the comparison.  The figure shows ConFrag's superior performance with the lowest ERR and MRAE while maintaining a reasonably high selection rate.", "section": "4.3 Results and Discussion"}, {"figure_path": "GYd5AfZaor/figures/figures_37_1.jpg", "caption": "Figure 5: Selection/ERR/MRAE comparison between ConFrag and strong baselines of CNLCU-H, BMM, DY-S, AUX and Selfie on IMDB-Clean-B. We exclude the performance during the warm-up.", "description": "The figure compares the performance of ConFrag against five strong baseline methods (CNLCU-H, BMM, DY-S, AUX, and Selfie) on the IMDB-Clean-B dataset with respect to selection rate, error residual ratio (ERR), and mean relative absolute error (MRAE).  It visualizes these metrics over training epochs, excluding the initial warm-up phase. The graph demonstrates ConFrag's superior performance in achieving a low ERR and MRAE while maintaining a competitive selection rate.", "section": "4.3 Results and Discussion"}]