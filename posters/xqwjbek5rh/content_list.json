[{"type": "text", "text": "Structural Inference of Dynamical Systems with Conjoined State Space Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aoran Wang 1 & Jun Pang 1,2 1 Faculty of Science, Technology and Medicine, University of Luxembourg 2 Institute for Advanced Studies, University of Luxembourg {aoran.wang, jun.pang}@uni.lu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces SICSM, a novel structural inference framework that integrates Selective State Space Models (selective SSMs) with Generative Flow Networks (GFNs) to handle the challenges posed by dynamical systems with irregularly sampled trajectories and partial observations. By utilizing the robust temporal modeling capabilities of selective SSMs, our approach learns input-dependent transition functions that adapt to non-uniform time intervals, thereby enhancing the accuracy of structural inference. By aggregating dynamics across diverse temporal dependencies and channeling them into the GFN, the SICSM adeptly approximates the posterior distribution of the system\u2019s structure. This process not only enables precise inference of complex interactions within partially observed systems but also ensures the seamless integration of prior knowledge, enhancing the model\u2019s accuracy and robustness. Extensive evaluations on sixteen diverse datasets demonstrate that SICSM outperforms existing methods, particularly in scenarios characterized by irregular sampling and incomplete observations, which highlight its potential as a reliable tool for scientific discovery and system diagnostics in disciplines that demand precise modeling of complex interactions. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the complex real-world phenomena, many dynamical systems manifest as networks of interacting entities. These systems are effectively modeled as graphs where nodes represent the agents, edges depict the interactions, and the adjacency matrix captures the structural essence of these interactions. Such representations are crucial across various domains, from intricate physical systems [32, 23, 59] and multi-agent systems [10, 34], to complex biological architectures [49, 44]. Unveiling the hidden structures within these networks is not only academically enriching but also essential for enhancing our understanding of the systems\u2019 intrinsic mechanisms and improving our ability to predict and manage their behaviors. However, this task becomes challenging when the observable data, often limited to features of agents within specific time frames, conceals the underlying structural dynamics. This limitation necessitates robust structural inference methodologies capable of discerning the latent structures from the trajectories\u2014the observable features of all agents over a given period. ", "page_idx": 0}, {"type": "text", "text": "As the field of scientific discovery advances, particularly with the integration of neural network technologies, structural inference has emerged as a key method for decoding the complex interactions within dynamical systems from trajectories [31, 2, 57, 12, 39, 51, 16, 54, 64]. This process is essential for understanding and predicting system behaviors but encounters significant challenges, particularly when addressing irregularly sampled data and partially observed some nodes of a dynamical system, which are typified by unequal sampling time intervals and the presence of unobserved nodes. Conventional methods, including those based on Variational Autoencoders (VAEs) [30], have pioneered some paths but often struggle with datasets characterized by nonuniform sampling rates and incomplete observations. These methods typically require uniform data and have difficulty managing indirect or obscured node interactions [31, 2, 57, 12, 39, 51, 54], highlighting a critical need for more adaptable and resilient inference models. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To overcome these limitations, this paper introduces a novel framework, Structural Inference with Conjoined State Space Models (SICSM), which conjoins Selective State Space Models (selective SSMs) [20] with a Generative Flow Network (GFN) [7, 8]. This innovative approach leverages the robust temporal modeling capabilities of selective SSMs alongside the flexible, data-driven structural inference provided by the GFN. SICSM is specifically designed to address the challenges of irregular sampling and partial observability, enhancing inference accuracy and robustness through the sophisticated integration of prior knowledge and adaptive learning mechanisms. Central to SICSM is its ability to learn input-dependent transition functions that dynamically adjust to the timing of data points, crucial for managing datasets with irregular intervals. Moreover, by aggregating outputs from multiple Residual Blocks containing an selective SSM in each, SICSM offers a rich representation of system dynamics, enabling more precise reconstruction of node interactions within partially observed systems. Our comprehensive evaluations across a variety of datasets\u2014from mechanical systems like spring simulations to biological networks depicting gene expressions\u2014demonstrate SICSM\u2019s superior performance over existing methods. Its robustness shines particularly in its ability to maintain high structural inference accuracy under diverse and challenging conditions, affirming its potential as an essential tool for scientific discovery and system diagnostics in disciplines that demand intricate, accurate modeling of complex systems. In essence, SICSM not only redefines approaches to structural inference in complex systems but also paves new research avenues previously limited by data sampling and observability constraints. With further refinement, this approach is poised to transform our understanding of interactions with dynamical systems across multiple scientific fields. Our contributions encompass the following aspects: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We develop a novel framework, SICSM, that integrates Selective State Space Models with a Generative Flow Network to enhance structural inference. \u2022 We introduce adaptive mechanisms within SICSM that effectively handle irregular sampling and partial observability, significantly enhancing the model\u2019s applicability to real-world datasets. \u2022 SICSM employs a novel approach by aggregating outputs from multiple Residual Blocks, which enables it to capture a deeper and more detailed representation of dynamic system interactions. \u2022 Extensive validation demonstrates its superiority over baselines in reconstructing complex structures, especially under challenging conditions of irregular sampling and incomplete observations. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Structural Inference. Structural inference aims to uncover the hidden structure of complex systems using observed trajectories. A pivotal contribution in this area is Neural Relational Inference (NRI) [31], which leverages a VAE within a fixed, fully connected graph framework. Building on NRI, subsequent research has expanded the domain of structural inference. Recent advancements include handling multi-interaction systems [57], integrating efficient message-passing [12], incorporating modular meta-learning [2], iteratively pruning indirect connections [51], developing structural inference with reservoir computing [54], and applying deep active learning to complex systems [52]. Other techniques involve reconstructing trajectories by minimizing relation potentials [16], computing partial correlation coefficients based on node embeddings [53], and with diffusion process [64]. Existing methods frequently employ the Variational Information Bottleneck principle [1], which often requires regularly sampled trajectories and complete observation of all nodes. Incorporating known interactions into VAEs necessitates complex adjustments, combining unsupervised and supervised learning in the latent space. This complexity complicates generalization to unlabeled edges. ", "page_idx": 1}, {"type": "text", "text": "State Space Models. State space models (SSMs) update sequences through recurrent hidden states. The selective state space architecture known as Mamba [20], recently emerged as an efficient and flexible design, using recurrent scans and a selection mechanism to control sequence flow into hidden states. Mamba shows promise across time-series tasks [56, 38, 43], video analysis [35, 61, 37] and healthcare applications [40, 45, 60]. In addition, several studies explore graph modeling with Mamba [55, 36, 6], but none of them apply it for structural inference from observational trajectories. In this work, we employ the fundamental operating mechanism, the selective SSM module, in the form of stacking blocks, to model observational trajectories and to deal with the challenge of irregular sampling as well as partial observation. ", "page_idx": 1}, {"type": "text", "text": "Generative Flow Networks. Generative flow networks (GFNs) excel in generating and sampling discrete states from high-dimensional distributions [7, 8]. Recent research has explored topics such as amortized inference [33], Bayesian structure learning [17], combinatorial optimization [63], biological sequence design [27], and broader scientific discovery [28]. Work in network inference [17, 18, 4] focuses on Bayesian inference to maintain system structure within state spaces. Our approach leverages Residual Blocks to learn one-dimensional embeddings from multi-dimensional features, enabling GFN to effectively learn while preserving dynamic complexity within the embeddings. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Notations and Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We model a dynamical system as a directed graph $G=(\\nu,E)$ , representing agents as nodes and interactions as directed edges. The graph consists of a node feature set $\\mathcal{V}$ with $n$ nodes and an edge set $E$ . Node features evolve over time, forming trajectories $\\mathcal{V}=\\left\\{V\\right\\}=\\left\\{V^{0},V^{1},\\ldots,V^{T-1}\\right\\}$ . , V T \u22121} across T time steps, where $V^{t}$ represents the feature set at time $t$ . Each node feature $v_{i}^{t}\\in\\mathbb{R}^{d}$ is $d$ -dimensional. For irregularly sampled trajectories, the $T$ time steps may have different intervals. And for partial observation of the dynamical system, we expect the count of observed nodes $n$ is smaller than the total count of nodes $n_{t o t}$ in the system. We observe a set of $M$ trajectories: $\\{V_{[1]},V_{[2]},\\ldots,V_{[M]}\\}$ , assuming a static edge set $E$ . An asymmetric adjacency matrix Adj is derived from $E$ , where $\\mathbf{Adj}_{i j}\\,\\in\\,\\{0,1\\}$ indicates the presence or absence of a directed edge. In dynamical systems, the dynamics of node $i$ at time $t+1$ are influenced by Adj as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nv_{i}^{t+1}=v_{i}^{t}+\\Delta\\cdot\\sum_{j\\in\\mathcal{U}_{i}}f(||v_{i},v_{j}||_{\\alpha}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\Delta$ is the time interval, $u_{i}$ denotes the set of nodes connected to node $i$ which is derived from Adj, $f(\\cdot)$ represents the state-transition function, and $||\\cdot,\\cdot||_{\\alpha}$ is the $\\alpha$ -distance. ", "page_idx": 2}, {"type": "text", "text": "For an illustrative example, we may consider a dynamical system comprising $n=10$ balls connected by springs, representing $n=10$ nodes $\\mathcal{V}$ and directed edges $E$ , respectively. Initially, we set the positions and velocities of each ball, so that each node feature $v_{i}^{t}\\in\\mathbb{R}^{d}$ is $d_{\\cdot}$ -dimensional where $d=4$ in this example. We then let them move under the influence of spring forces, which arise from the structural connections (edges) between the balls (nodes). Over the observation period, these balls change their positions and velocities. And we record the trajectories as the collection of the evolving features of all nodes: $\\mathcal{V}=\\{V\\}=\\{V^{0},V^{1},\\ldots,V^{T-1}\\}$ across $T$ time steps, where $V^{t}$ represents the feature set at time $t$ . In total we observe a set of $M$ trajectories: $\\{V_{[1]},V_{[2]},\\ldots,V_{[M]}\\}$ , assuming a static edge set $E$ . Suppose we initially lack knowledge of which balls are connected, i.e., $E$ is unknown; the task of structural inference in this scenario would involve deducing the connectivity between the balls based on their observed trajectories,represented by either the edge set $E$ or the adjacency matrix Adj. ", "page_idx": 2}, {"type": "text", "text": "In this work, we introduce SICSM, a novel structural inference method with conjoined state modeling, designed to handle both irregularly sampled trajectories and systems with partial observations. ", "page_idx": 2}, {"type": "text", "text": "3.2 State Space Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "SSMs capture the behavior of dynamical systems by modeling the internal state and relationships between latent states $h^{t}\\,\\in\\,\\mathbb{R}^{N}$ , input sequences $\\boldsymbol{x}^{t}\\in\\mathbb{R}^{D}$ and output sequences $\\boldsymbol{y}^{t}\\in\\mathbb{R}^{N}\\colon\\hat{\\boldsymbol{h}}^{t}=$ $\\mathbf{A}h^{t}+\\mathbf{B}x^{t}$ , $y^{t}=\\mathbf{C}h^{t}$ , where $\\dot{\\mathbf{A}}\\in\\mathbb{R}^{\\dot{N}\\times N}$ and ${\\bf B},{\\bf C}\\in\\mathbb{R}^{N\\times D}$ are learnable matrices. Due to the complexity of solving the above differential equation in deep learning settings, discrete state space models [21] discretize this system using a time-scale parameter $\\Delta$ : $h^{t}\\!=\\bar{\\mathbf{A}}h^{\\breve{t}-\\mathrm{i}}+\\bar{\\mathbf{B}}x^{t}$ , $y^{t}=$ $\\bar{\\mathbf{C}}h^{t}$ , where $\\bar{\\mathbf{A}}\\,=\\,\\exp\\Delta\\mathbf{A}$ , and $\\mathbf{\\bar{B}}=(\\mathbf{\\DeltaA}\\mathbf{\\bar{A}})^{-1}(\\exp{(\\mathbf{\\DeltaA}\\mathbf{\\dot{A}}-\\mathbf{I})})\\cdot\\mathbf{\\DeltaA}\\mathbf{B}$ . Note that $\\Delta$ in discrete SSMs operates similarly to that in Eqn. 1. Discrete-time SSMs are also shown to be equivalent to the following convolution: $y=x*{\\bar{\\mathbf{K}}}$ , where $\\bar{\\mathbf{K}}=(\\bar{\\mathbf{C}}\\bar{\\mathbf{B}},\\bar{\\mathbf{C}}\\bar{\\mathbf{A}}\\bar{\\mathbf{B}},...,\\bar{\\mathbf{C}}\\bar{\\mathbf{A}}^{L-1}\\bar{\\mathbf{B}})$ . Therefore, the continuous form $(\\Delta,{\\bf A},{\\bf B},{\\bf C})$ transitions to the discrete form $(\\bar{\\bf A},\\bar{\\bf B},{\\bf C})$ , allowing efficient computation using a linear recursive approach [22]. Furthermore, structured state space sequence models (S4) structures the state matrix $\\mathbf{A}$ based on HIPPO matrices, significantly improving efficiency and performance. ", "page_idx": 2}, {"type": "text", "text": "Recently, a selective structured state space architecture named Mamba [20] was introduced. It leverages recurrent scans and a selection mechanism to control which part of the sequence flows into the hidden states. Mamba\u2019s selection mechanism can be interpreted as using data-dependent state transition mechanisms, meaning $\\Delta$ , B and $\\mathbf{C}$ are functions of the input $x^{t}$ . In this work, we utilize Mamba\u2019s selection mechanism to handle irregularly sampled trajectories by learning $\\Delta$ from node feature set $V^{t}$ , enhancing the method\u2019s ability to model system dynamics. By stacking Residual Blocks containing selective SSM modules, we aggregate embedded dynamics from the outputs of different blocks, allowing us to reconstruct structures with dynamics from various temporal dependencies and address incomplete node observation. ", "page_idx": 3}, {"type": "text", "text": "3.3 Generative Flow Networks ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "GFNs [7, 8] are generative models operating over structured sample space $\\mathcal{X}$ , characterized by a directed acyclic graph $\\mathcal{G}$ with state space $\\boldsymbol{S}$ , where $\\mathcal{X}\\subseteq S$ . It is crucial to distinguish this from the interaction graph $G$ of the dynamical system, and the input sequence $x^{t}$ in SSMs. A sample $x\\in\\mathscr{X}$ is constructed by traversing $\\mathcal{G}$ from an initial state $s_{0}$ to a terminal state $s_{f}$ , the latter being a special state indicating the end of the sequence. Terminal states in $\\mathcal{X}$ are those connected by a directed edge $x\\rightarrow s_{f}$ , representing valid samples of the distribution induced by GFN. A terminal state trajectory in $\\mathcal{G}$ is represented by a path $s_{0}\\sim s_{f}$ , distinct from the observational trajectories used in structural inference. Each terminal state is associated with a reward $R(X)\\geq0$ , representing its unnormalized probability. The distribution over terminal states is proportional to the reward, which is governed by a flow function $F_{\\Omega}(s\\rightarrow s^{\\prime})\\geq0$ and satisfies the flow-matching conditions: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\sum_{s\\in\\mathrm{{Pa}}_{\\mathcal{G}}(s^{\\prime})}F_{\\Omega}(s\\to s^{\\prime})-\\sum_{s^{\\prime\\prime}\\in\\mathrm{{Ch}}_{\\mathcal{G}}(s^{\\prime})}F_{\\Omega}(s^{\\prime}\\to s^{\\prime\\prime})=R(s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$\\mathrm{Pa}_{\\mathcal{G}}(s^{\\prime})$ and $\\operatorname{Ch}_{\\mathcal{G}}(s^{\\prime})$ represent the preceding state and the subsequent state of $s^{\\prime}$ , respectively. The forward transition probability is $\\bar{P(}s_{k+1}|s_{k}\\bar{)}\\propto F_{\\Omega}(s_{k}\\to s_{k+1})$ , leading to a marginal probability for terminating in $x\\in\\mathscr{X}$ proportional to $R(x)$ . Consequently, there is also a backward transition probability $P_{B}(\\cdot)$ , but it is usually set to some fixed distribution (e.g. the uniform distribution over the parent states) to reduce the search space, making the forward transition probability the only quantity to learn [7, 8, 17]. Starting from the initial state $s_{0}$ , if we sample a terminal state trajectory $\\left(s_{0},s_{1},\\ldots,s_{K-1},x,s_{f}\\right)$ following the forward transition probability, defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nP(s_{k+1}|s_{k})\\propto F_{\\Omega}(s_{k}\\to s_{k+1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $s_{K}\\,=\\,x$ and $s_{K+1}\\,=\\,s_{f}$ , then the likelihood of a state trajectory ending in a state $x\\in\\mathscr{X}$ is directly proportional to the reward $R(x)$ . The flow function $\\bar{F_{\\Omega}}(s\\,\\rightarrow\\,s^{\\prime})$ , often parameterized by a neural network, is optimized to minimize discrepancies in the flow-matching conditions. This optimization results in a transition model capable of approximately sampling from the distribution over $\\mathcal{X}$ in proportion to $R$ . In the subsequent sections, we elucidate the construction of state spaces using graphs and detail the approximation of dynamical systems\u2019 structures through posterior estimation with a GFN. We demonstrate how GFN enhances SICSM by utilizing the embedded dynamics from Residual Blocks to reconstruct the structure of dynamical systems effectively. Moreover, SICSM facilitates the seamless integration of prior knowledge about existing connections into its training process, thereby improving the model\u2019s accuracy and efficacy in structural inference. ", "page_idx": 3}, {"type": "text", "text": "4 Structural Inference with Conjoined State Space Models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "SICSM utilizes selective SSM modules, embedding in Residual Blocks, to adaptively manage input-dependent time intervals $\\Delta$ , thus effectively handling irregularly sampled trajectories. This model architecture further aggregates dynamic embedding from multiple Residual Blocks, to obtain dynamics from various temporal dependencies, enhancing our ability to process partial observations. These dynamics are subsequently input into a GFN to approximate and sample the graph structures representing the system\u2019s structure. Figure 1 provides a schematic overview of the SICSM pipeline. ", "page_idx": 3}, {"type": "text", "text": "4.1 Aggregation of Learned Dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As illustrated in the upper row of Figure 1, the primary function of the upper branch of the SICSM is time-series forecasting. These models dynamically adapt the state space parameters $(\\Delta,{\\bf B},{\\bf C})$ ", "page_idx": 3}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/3e5bc783f204fc8226a9a855f137b7fc070d519eec5acb75590e32f65c8ffe69.jpg", "img_caption": ["Figure 1: (Upper) System architecture. (Lower Left) Detail of a Residual Block. (Lower Right) Structure of the Generative Flow Network for approximating the joint posterior distribution. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "based on the node-specific feature embeddings. Inspired by recent advancements in dimensionality reduction and noise reduction in mixed-node feature datasets [56], we introduce a feature-based embedding network designed to compress the feature dimension from $d$ to 1: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{h}_{i}^{t}=f_{e m b e d}(v_{i}^{t}),\\;\\mathrm{for}\\;t\\in\\{0,1,2,\\ldots,T-2\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{e m b e d}$ is a multi-layer perceptron. $\\mathbf{h}_{i}^{t}$ are sequentially organized per node to form $\\mathbf{H}_{i}=\\mathbf{\\partial}$ $[\\mathbf{h}_{i}^{0},\\mathbf{h}_{i}^{1},...,\\mathbf{h}_{i}^{T-2}]$ . The node dynamics are modeled by a series of Residual Blocks configured in an encoder-decoder structure. Each Residual Block incorporates a selective SSM module that dynamically learns the SSM parameters $(\\Delta,{\\bf B},{\\bf C})$ based on the embeddings $\\mathbf{H}_{i}$ for each node: ", "page_idx": 4}, {"type": "equation", "text": "$$\n(\\Delta_{i},\\mathbf{B}_{i},\\mathbf{C}_{i})=f_{S S M p r o j}(\\mathbf{H_{i}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{S S M p r o j}$ is a linear projection layer specific to each selective SSM, as described in [20]. This allows each node\u2019s input-dependent step-size $\\Delta$ to adjust dynamically, enhancing the model\u2019s ability to handle irregularly sampled trajectories and reflect flexible time intervals $\\Delta$ as specified in Eqn. 1. Furthermore, the matrices $\\mathbf{B}$ and $\\mathbf{C}$ are tailored for each node, updating node features over time and accommodating the unique dynamics of each node. More details on the selection SSM in this work can be found in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "To enhance the architectural sophistication of our model, we arrange $L$ Residual Blocks in a sequential configuration (to build a residual model), with the output of each block feeding directly into the next. This design significantly improves the model\u2019s ability to discern and interpret diverse features and aspects of the trajectory, facilitating the capture of a broad spectrum of temporal dependencies. The input to the first Residual Block is obtained by processing the concatenated node embeddings $\\mathbf{H}_{a l l}\\doteq[\\mathbf{H}_{i}$ , for all nodes]: ", "page_idx": 4}, {"type": "equation", "text": "$$\nU_{R B_{-}l}=f_{R B_{-}l}\\,({\\bf H}_{a l l}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $f_{R B_{-},l}$ denotes the first Residual Block function. The $l$ -th Residual Block has: $U_{R B\\_l}=$ $f_{R B\\_l}(U_{R B\\_l-1})$ . This setup is particularly vital in systems with partial observability, where direct connections may not be visible. In such scenarios, observable nodes may appear isolated but are frequently connected through hidden intermediaries, converting straightforward interactions into intricate multi-hop relationships. SICSM accommodates this complexity by integrating dynamics across a spectrum of temporal dependencies: shorter dependencies help reconstruct direct interactions, while longer dependencies are crucial for mapping multi-hop relationships. Unlike traditional methods that operate under fixed time intervals and predetermined direct interactions [31, 2, 57, 12, 39, 51, 54], which struggle with variable conditions, our SICSM\u2019s flexibility in adapting to different hop distances is essential. This adaptability enables it to accurately delineate potential indirect interactions and therefore to deal with incomplete observation. ", "page_idx": 4}, {"type": "text", "text": "Further enhancing our model, we implement an encoder-decoder structure composed of an additional $L^{\\prime}$ Residual Blocks. This configuration not only maintains the model\u2019s symmetry but also boosts its accuracy. The outputs from these blocks undergo transformation via a projection network, which restores the features to their original $d$ -dimensional state, preparing estimated node features for the subsequent time step, $\\hat{v}_{i}^{t+1}$ . Based on the system is fully observed or not, we have: ", "page_idx": 4}, {"type": "equation", "text": "$$\nU_{A l l}=\\left\\{U_{R B_{-}L}\\atop\\sum_{l=1}^{L}U_{R B_{-}l}\\quad\\mathrm{if~we~observe~all~nodes},\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$U_{R B\\_l}$ represents outputs from $l$ -th block. The aggregation of outputs from multiple blocks is crucial, especially in scenarios with partial observations typically caused by non-visible intermediate nodes. This aggregation ensures that dynamics of various temporal dependencies are comprehensively captured, enabling SICSM to deal with the partial observation. ", "page_idx": 5}, {"type": "text", "text": "4.2 Approximation of Posterior with a Generative Flow Network ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Upon acquiring the aggregated dynamics $U_{A l l}$ from either the output of the final Residual Block or the summation of outputs from all Residual Blocks, we feed these dynamics to the following GFN. We utilize a GFN to model the posterior distribution $P(\\mathbf{Adj}\\mid U_{A l l})$ , where Adj delineates the structure of the dynamical system under study. While any GFN capable of modeling the structure of graphs could be employed, we specifically choose the Joint Structure-Parameters GFN (JSP-GFN) [18] for its ability to capture the diverse dynamics of each node influenced by their interactions. This model effectively addresses the joint posterior distribution $P(\\mathbf{Adj},\\lambda\\mid U_{A l l})$ , where $\\lambda=\\{\\lambda_{1},\\ldots,\\lambda_{n}\\}$ represents the parameters for conditional probability distributions associated with each node $i$ , enhancing the accuracy and depth of structural inference by accommodating the unique characteristics and relationships of each node. ", "page_idx": 5}, {"type": "text", "text": "As depicted in the lower right of Fig. 1, the state construction process begins with an initial state containing a graph with an empty adjacency matrix $G_{0}=(U_{A l l},\\mathbf{Adj}_{0})$ , which progressively evolves by systematically adding edges to Adj based on the forward transition probability $\\bar{P}_{\\Omega}(G^{\\prime}|\\dot{G})$ , where $G^{\\prime}$ is the resultant graph state. This iterative addition continues until a \u2018stop\u2019 action is chosen, signifying the completion of the graph construction phase. Once the graph $G$ is established, we proceed to generate the parameter set $\\lambda$ , conditioned on $G$ , utilizing the forward transition probabilities $\\bar{P}_{\\Omega}(\\lambda|G)$ . Each terminal state $s=\\langle G,\\lambda\\rangle$ thus encapsulates a potential configuration of the system, with the construction process forming a tree structure rooted at $G_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "To approximate the joint posterior distribution $P(\\mathbf{Adj},\\lambda|U_{A l l})$ which is proportional to $P(\\mathbf{Adj},\\lambda,U_{A l l})$ [18], we define a reward function for each terminal state: ", "page_idx": 5}, {"type": "equation", "text": "$$\nR(\\langle G,\\lambda\\rangle)=P(U_{A l l}|\\lambda,\\mathbf{Adj})P(\\lambda|\\mathbf{Adj})P(\\mathbf{Adj}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $P(U_{A l l}|\\lambda,\\mathbf{Adj})$ represents the likelihood model implemented via a neural network that operates on each node, $P(\\lambda|\\mathbf{Adj})$ denotes the prior over the parameter set, and $P(\\mathbf{Adj})$ constitutes the general prior over structures. This reward function integrates the likelihood of the observational data with the parameter and graph priors, directing the learning towards accurate structural inference. To derive the adjacency matrix for the system, we approximate the marginal posterior $P(\\mathbf{Adj}|U_{A l l})$ by collecting samples $\\{\\mathbf{Adj}_{1},\\mathbf{Adj}_{2},\\dots,\\mathbf{Adj}_{B}\\}$ from the posterior distribution and estimate the marginal probability of an edge from node $i$ to node $j$ as: ", "page_idx": 5}, {"type": "equation", "text": "$$\nP_{\\Omega}(i\\rightarrow j|U_{A l l})\\approx\\frac{1}{B}\\sum_{b=1}^{B}{\\bf1}(i\\rightarrow j\\in{\\bf A d j}_{b}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\mathbf{1}(\\cdot)$ is the indicator function. The collected value $P_{\\Omega}$ is the approximation of structure of the dynamical system. This methodology ensures robust inference of the structure of the dynamical system. For details on the specific GFN in SICSM, please refer to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "This approach of modeling dynamical systems using a conjoined state space model framework integrates observational trajectory modeling via selective SSMs with posterior distribution modeling via a GFN, providing a comprehensive method for structural inference in complex systems. ", "page_idx": 5}, {"type": "text", "text": "4.3 Reward Function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The reward function in SICSM, as defined in Eqn. 8, comprises three key components: the likelihood model $P(U_{A l l}|\\lambda,G)$ , the prior over parameters $P(\\lambda|G)$ , and the prior over graphs $P(G)$ . Consistent with standard practices in GFNs [7, 8], we utilize a logarithmic transformation of the reward function: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\log R(\\langle G,\\lambda\\rangle)=\\log P(U_{A l l}|\\lambda,\\mathbf{Adj})+\\log P(\\lambda|\\mathbf{Adj})+\\log P(\\mathbf{Adj}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with component being implemented distinctly: ", "page_idx": 6}, {"type": "text", "text": "Likelihood Model. The first term of the reward function is a log-likelihood model that estimates prediction errors for future node features, considering the current graph structure ${\\tilde{A d j}}$ in each state. This approach aligns naturally with the evolution of dynamical systems: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\log P(U_{A l l}|\\lambda,\\mathbf{Adj})=\\sum_{t=0}^{T-2}\\sum_{n=1}^{N}\\log P(v_{i}^{t+1}|\\lambda,\\tilde{A d j},U_{i}^{t}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $U_{i}^{t}$ is the learned embeddings of node $i$ at time $t$ and is obtained from $U_{A l l}$ . $\\log P(v_{i}^{t+1}|\\lambda,\\tilde{A d j},U_{i}^{t})$ is modeled with a neural network to enable accurate predictions of future node features. This setup integrates both node features and graph structure into the computation, ensuring their collective influence on the reward function. ", "page_idx": 6}, {"type": "text", "text": "Parameter Prior. The second term of the reward function represents the prior over the parameters $\\lambda$ We utilize a unit Normal distribution for each parameter $\\lambda_{i j}$ , correlating with the sender and receiver nodes indexed by $i$ and $j$ : $P(\\lambda_{i j}|\\mathbf{Adj})=\\mathcal{N}(0,1)$ . This choice of prior contributes to a balanced modeling of node interactions within the graph. ", "page_idx": 6}, {"type": "text", "text": "Graph Prior. The component of the graph prior comprises a uniform prior alongside regularization terms designed to enhance the graph\u2019s structural smoothness. In reference to the current graph structure $\\bar{\\tilde{A d}}\\dot{j}$ , it is formulated as: ", "page_idx": 6}, {"type": "equation", "text": "$$\nP(\\mathbf{Adj})=P_{U}(\\mathbf{Adj})+\\exp\\Big(D(\\tilde{A d j},U_{A l l})+\\mathcal{L}_{d}(\\tilde{A d j})+\\mathcal{L}_{s}(\\tilde{A d j})\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $P_{U}(\\mathbf{Adj})$ is the uniform prior. The regularization terms include Dirichlet energy $D(\\Tilde{A d j},U_{A l l})$ to measure smoothness between adjacent node features, a connectivity term $\\mathscr{L}_{d}(\\tilde{A d j})$ to penalize unconnected structures, and a sparsity term $\\mathcal{L}_{s}(\\tilde{A d}\\!j)$ to regulate graph density: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D(\\tilde{A d j},U_{A l l})=-\\displaystyle\\frac{1}{n^{2}}\\sum_{i,j}\\tilde{A d j}_{i j}\\|U_{i}-U_{j}\\|^{2},}\\\\ &{\\mathcal{L}_{d}(\\tilde{A d j})=\\displaystyle\\frac{1}{n}\\mathbf{1}^{\\top}\\log(\\tilde{A d j}\\mathbf{1}),\\mathrm{~and~}\\mathcal{L}_{s}(\\tilde{A d j})=-\\displaystyle\\frac{1}{n^{2}}\\|\\tilde{A d j}\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "These terms, adapted for the reward function in SICSM, emphasize the influence of the graph\u2019s properties on the state space, enriching the model\u2019s structural inference capability. ", "page_idx": 6}, {"type": "text", "text": "4.4 Learning Objectives ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The learning objectives of SICSM are bifurcated into two main components: (1) time-series forecasting using Residual Blocks, and (2) modeling dynamics with the GFNs, focusing on the accuracy of transition probabilities. For the Residual Blocks, the primary learning objective is the minimization of the Mean Squared Error between predicted and actual node features across all time steps and nodes: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{R B}=\\frac{1}{T\\times n}\\sum_{t=0}^{T-2}\\sum_{i=1}^{n}\\|v_{i}^{t+1}-\\hat{v}_{i}^{t+1}\\|^{2},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\hat{v}_{i}^{t+1}$ and ${\\boldsymbol{v}}_{i}^{t+1}$ represent the predicted and actual features of node $i$ at time $t+1$ , respectively. This objective ensures that the Residual Blocks effectively capture and forecast the dynamics. Similar to [17, 18], the objective for the GFN involves optimizing the squared error of the logarithmic ratio between forward and backward transition probabilities to ensure accurate modeling of the graph structure: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{G F N}=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\left(\\log\\frac{R(\\langle G^{\\prime},\\wedge\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})P_{\\Omega}(\\wedge\\lambda|G)}{R(\\langle G,\\wedge\\lambda\\rangle)P_{\\Omega}(G^{\\prime}|G)P_{\\Omega}(\\wedge\\lambda^{\\prime}|G^{\\prime})}\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $P_{B}(\\cdot)$ indicates the backward transition probability, and $\\lambda^{\\prime}$ denotes the parameters generated conditional on graph $G^{\\prime}$ . The sampling distribution $\\pi$ covers pairs $\\langle G,\\lambda\\rangle$ and $\\langle{G^{\\prime},\\lambda^{\\prime}}\\rangle$ , with the \u2018stop-gradient\u2019 operation $(\\land)$ critical for halting backpropagation through the parameters $\\lambda$ and $\\lambda^{\\prime}$ , thus preventing potential feedback loops during training. Please refer to Appendix $\\mathbf{B}$ for the parameterization of these terms. The final learning objective of SICSM combines these terms: ", "page_idx": 6}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/bcceb0e91398eea51432a99df5d10db1721017d8cd5b95963f8e9febf1066061.jpg", "img_caption": ["Figure 2: AUROC values (expressed in percentage) for various methods as a function of the number of irregularly sampled time steps. Results are averaged across ten trials, with time steps varying from 49 to 10. All subplots share a common $\\mathbf{X}$ -axis and y-axis for uniform comparison. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}=\\mathcal{L}_{R B}+\\mathcal{L}_{G F N}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "This combined objective facilitates concurrent optimization of both time-series prediction accuracy and the fidelity of the inferred graph structure. ", "page_idx": 7}, {"type": "text", "text": "4.5 Integration of Prior Knowledge ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In SICSM, the integration of prior knowledge concerning existing network connections is executed more seamlessly and effectively compared to traditional VAE-based methods [31, 39, 51, 54, 53]. To incorporate this prior knowledge, we initialize the graph $G_{0}$ in the GFN\u2019s initial state with edges that represent the known connections based on prior knowledge: $G_{0,k}=(\\mathcal{V},\\mathbf{Adj_{0}}\\cup E_{k})$ , where $E_{k}$ contains the set of known edges based on prior knowledge. This setup ensures that the learning and sampling processes are continually influenced by this integrated knowledge, enhancing the model\u2019s accuracy and effectiveness in predicting and understanding the underlying dynamics of the system. ", "page_idx": 7}, {"type": "text", "text": "5 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This study systematically evaluates the performance of SICSM across an extensive array of datasets, which encompass both one-dimensional and multi-dimensional trajectories. The investigation specifically concentrates on challenging scenarios of irregularly sampled trajectories and partial observations. More results and methodological specifics are further elaborated in Appendix E. ", "page_idx": 7}, {"type": "text", "text": "5.1 General Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets. Our study first evaluates the SICSM model on two established structural inference datasets: the Spring Simulations dataset [31], which simulates dynamic interactions of balls connected by springs within a symmetric setting, and the NetSim dataset [47], which consists of simulated bloodoxygen-level-dependent imaging data from various brain regions in an asymmetric network. Both datasets include 10 nodes, with Spring Simulations offering four-dimensional features and NetSim one-dimensional features at each timestep, initially sampled at 49 regular intervals. ", "page_idx": 7}, {"type": "text", "text": "Additionally, we examined six directed synthetic biological networks (Linear, Linear Long, Cycle, Bifurcating, Trifurcating, and Bifurcating Converging) as outlined in [44], with abbreviations LI, LL, CY, BF, TF and BF-CV, respectively. These networks simulate developmental trajectories in differentiating cells using BoolODE [44], capturing one-dimensional mRNA expression levels over 49 timesteps with irregular intervals tailored to our experimental setups. ", "page_idx": 7}, {"type": "text", "text": "We also incorporated data from the StructInfer Benchmark [3], focusing on \u2018Vascular Networks\u2019 (VN) with node counts ranging from 15 to 100. These datasets, named under the categories Springs (SP) and NetSims (NS), were selected for their complex and varying underlying graph structures, providing a robust platform to validate the efficacy of the SICSM model. ", "page_idx": 7}, {"type": "text", "text": "Baselines and metrics. To evaluate the performance of SICSM, we compared it against a suite of state-of-the-art models: NRI [31], MPM [12], ACD [39], iSIDG [51], RCSI [54], JSP-GFN [18], CUTS [13], and SIDEC [53]. The comparative effectiveness of these methods was quantitatively assessed using the area under the receiver operating characteristic (AUROC) curve, focusing on the accuracy of the inferred adjacency matrix relative to the ground truth. ", "page_idx": 8}, {"type": "text", "text": "Experimental settings. All experiments were conducted on a single NVIDIA Ampere 40GB HBM graphics card, paired with 2 AMD Rome CPUs (32 cores $\\mathcal{Q}2.35\\,\\mathrm{GHz}$ ). Detailed configurations and additional results are elaborated in Appendix D and Appendix E. ", "page_idx": 8}, {"type": "text", "text": "5.2 Experimental Results with Irregularly Sampled Trajectories ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section examines the performance of the evaluated methods to irregular sampling of input trajectories. Detailed in Section 5.1, our datasets undergo randomized reduction in time steps to [40, 30, 20, 10] from an original count of 49. The baselines, alongside SICSM, are then trained and evaluated on these irregularly sampled trajectories, with the average AUROC results of 10 runs depicted in Figure 2. Besides, we report the AUPRC results, SHD values and F1-scores in Figues 7-9 in Appendix E.1. It should be noted that JSP-GFN, being limited to one-dimensional feature analysis, is not applicable to multi-dimensional datasets such as Springs Simulations and VN_SP. ", "page_idx": 8}, {"type": "text", "text": "SICSM exhibits exceptional consistency in its performance despite the decrease in time steps, which is a critical indicator of robustness within structural inference models. In datasets like Spring Simulations and NetSim, while the baseline models show significant declines in performance from 49 to 10 time steps, SICSM maintains AUROC scores above $85\\%$ . This underscores its potent capability to effectively leverage essential structural information, even when data availability is constrained. Moreover, when faced with irregular sampling, traditional VAE-based methods such as NRI, ACD, MPM, iSIDG, and RCSI struggle significantly, often performing no better than random guessing. This decline is primarily attributed to their dependency on fixed time intervals between observations\u2014an assumption not held in our experimental conditions. In contrast, in challenging synthetic biological networks like BF and TF, SICSM consistently surpasses baseline models by margins of $5.10\\%$ across all sampling levels, affirming its sophisticated understanding of complex, directional interactions which are crucial in genomics and systems biology. ", "page_idx": 8}, {"type": "text", "text": "SICSM\u2019s robustness to irregular sampling intervals is particularly notable in datasets such as VN_SP_30 and VN_SP_50. Unlike conventional models that falter under variable data availability, SICSM\u2019s architecture, equipped with adaptive time-interval handling, adeptly navigates these challenges, preserving its predictive accuracy. These findings validate the effectiveness of SICSM in managing complex, temporally variant structural inference challenges across a spectrum of demanding datasets. The model not only demonstrates resilience to data scarcity and irregular sampling, but also excels in capturing intricate systemic interactions. ", "page_idx": 8}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/2ba7bbc46d46ff0fac97865d917725d3cd61d639485b92341646e3386f538574.jpg", "img_caption": ["5.3 Experimental Results with Incomplete Observation of Systems "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: AUROC values (expressed in percentage) for various methods as a function of the proportion of observed nodes, averaged over 20 trials. Node sampling proportions are set at $[10\\bar{0}\\%$ , $90\\%$ , $80\\%$ , $70\\%$ , $60\\%]$ . ", "page_idx": 8}, {"type": "text", "text": "This section explores the resilience of evaluated methods to scenarios where only a subset of the system\u2019s nodes is observable. As outlined in Section 5.1, the datasets undergo a reduction in node count by sampling from all nodes, scaled to proportions of $[100\\%$ , $90\\%$ , $80\\%$ , $70\\bar{\\%}$ , $60\\%]$ , with rounding up to ensure integer counts. This experimental setup was applied particularly to datasets with more than 10 nodes\u2014specifically the LL and all VN datasets\u2014to facilitate a comprehensive investigation. The performance of the baselines, alongside our ", "page_idx": 8}, {"type": "text", "text": "SICSM model, is quantified using the average AUROC from 20 runs, as depicted in Figure 3. ", "page_idx": 8}, {"type": "text", "text": "SICSM demonstrates remarkable stability in AUROC scores across different levels of node sampling, particularly excelling in environments with partial observations. In the VN_SP_30 dataset, it maintains an AUROC above $80\\%$ , even with only $60\\%$ of nodes observed, significantly outperforming methods like NRI and ACD, whose performance dips below $75\\%$ . This highlights SICSM\u2019s ability to effectively utilize essential structural relationships under partial observations. In complex network structures like the VN_NS series, SICSM\u2019s strong performance underscores its proficiency in inferring critical interactions, despite considerable reductions in observable nodes. This robustness showcases the model\u2019s ability for managing data sparsity and leveraging available information effectively. ", "page_idx": 9}, {"type": "text", "text": "SIDEC also shows competitive performance, underscoring the beneftis of dynamics-encoding models in handling incomplete observations. However, without an adaptive transition dynamic function, SIDEC generally underperforms compared to SICSM, especially when fewer nodes are observed. In comparisons, SICSM consistently outshines JSP-GFN at lower node sampling percentages, illustrating its superior capability in managing partial observations and effectively using structural information even with limited data visibility. These results confirm SICSM\u2019s robustness in structural inference, particularly in scenarios with incomplete observations. Its resilience to node sparsity and ability to discern complex interactions make it a valuable tool for applications that require reliable, accurate structural predictions in data-constrained environments. ", "page_idx": 9}, {"type": "text", "text": "5.4 Why Do We Need All Residual Outputs? ", "text_level": 1, "page_idx": 9}, {"type": "table", "img_path": "xQWJBeK5rh/tmp/1469cd236ab285a12fb97e57accbf030cc41e02d684bf30e131e0f5604e90969.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "As discussed in Section 3.2, systems with partial observations can benefti from aggregating dynamics from multiple Residual Blocks for GFN input. We explored this by analyzing the use of only the dynamics from the final Residual Block of encoder in a case study using the VN_SP_15 dataset, where 12 nodes $80\\%$ of the total) are sampled. We evaluated two configurations: the comprehensive SICSM, integrating outputs from all Blocks in encoder, and SICSM-one, which relies solely on the final Block\u2019s output. Figure 4 shows that SICSM-one often inaccurately classified two-hop interactions as three-hop connections, leading to increased false positives due to its dependence on the output from the larger, final Residual Block, which tends to blur hop distinctions. In contrast, SICSM\u2019s approach of using outputs from multiple layers provided a rich dynamics representation that effectively managed both shorter and longer connections, reducing wrong results. Despite these strengths, some inaccuracies point to the potential need for an adaptive weighting mechanism that adjusts the influence of dynamics based on the graph\u2019s size and longest paths, potentially improving accuracy across different scenarios. These results, along with an additional case in Appendix E.4, highlight the effectiveness of SICSM\u2019s multi-layer dynamic integration in handling partial observations and its capability for precise structural inference in complex settings. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper presents SICSM, a novel structural inference approach that merges Selective State Space Models with Generative Flow Networks. By embedding dynamics with Residual Blocks, our method learns input-dependent transition parameters, effectively handling irregularly sampled trajectories. Aggregating outputs from multiple blocks enriches the dynamics captured, addressing the significant challenge of incomplete node observations. The downstream Generative Flow Network, leveraging these dynamics, achieves precise structural inference and seamlessly incorporates prior knowledge. Empirical evidence demonstrates SICSM\u2019s effectiveness, particularly in settings with irregular sampling and partial observations. Future research will explore specific adaptations of conjoined state space models for dynamic systems with mutable structural elements, such as evolving connections and emerging nodes. Additionally, we aim to explore the development of a comprehensive model capable of pioneering new paths in general scientific discovery. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy. Deep variational information bottleneck. In Proceedings of the 5th International Conference on Learning Representations (ICLR), 2017. [2] F. Alet, E. Weng, T. Lozano-P\u00e9rez, and L. P. Kaelbling. Neural relational inference with fast modular meta-learning. In Advances in Neural Information Processing Systems 32 (NeurIPS), 2019.   \n[3] A. Anonymous. Benchmarking structural inference methods for dynamical interacting systems. https://structinfer.github.io/, 2023. [4] L. Atanackovic, A. Tong, B. WANG, L. J. Lee, Y. Bengio, and J. Hartford. DynGFN: Towards bayesian inference of gene regulatory networks with GFlownets. In Advances in Neural Information Processing Systems 36 (NeurIPS), 2023. [5] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018.   \n[6] A. Behrouz and F. Hashemi. Graph mamba: Towards learning on graphs with state space models. arXiv preprint arXiv:2402.08678, 2024. [7] E. Bengio, M. Jain, M. Korablyov, D. Precup, and Y. Bengio. Flow network based generative models for non-iterative diverse candidate generation. In Advances in Neural Information Processing Systems 34 (NeurIPS), pages 27381\u201327394, 2021.   \n[8] Y. Bengio, S. Lahlou, T. Deleu, E. J. Hu, M. Tiwari, and E. Bengio. Gflownet foundations. Journal of Machine Learning Research, 24(210):1\u201355, 2023. [9] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Version 0.3.13.   \n[10] G. Bras\u00f3 and L. Leal-Taix\u00e9. Learning a neural solver for multiple object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6247\u2013 6257, 2020.   \n[11] C. Chen, K. Petty, A. Skabardonis, P. Varaiya, and Z. Jia. Freeway performance measurement system: mining loop detector data. Transportation Research Record, 1748(1):96\u2013102, 2001.   \n[12] S. Chen, J. Wang, and G. Li. Neural relational inference with efficient message passing mechanisms. In Proceedings of the 35th AAAI Conference on Artificial Intelligence (AAAI), pages 7055\u20137063, 2021.   \n[13] Y. Cheng, R. Yang, T. Xiao, Z. Li, J. Suo, K. He, and Q. Dai. CUTS: Neural causal discovery from irregular time-series data. In Proceedings of the 11th International Conference on Learning Representations (ICLR), 2023.   \n[14] W. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C. Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD, pages 257\u2013266. ACM, 2019.   \n[15] K. Cho. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.   \n[16] A. Comas, Y. Du, C. F. Lopez, S. Ghimire, M. Sznaier, J. B. Tenenbaum, and O. Camps. Inferring relational potentials in interacting systems. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 6364\u20136383. PMLR, 2023.   \n[17] T. Deleu, A. G\u00f3is, C. Emezue, M. Rankawat, S. Lacoste-Julien, S. Bauer, and Y. Bengio. Bayesian structure learning with generative flow networks. In Uncertainty in Artificial Intelligence, pages 518\u2013528. PMLR, 2022.   \n[18] T. Deleu, M. Nishikawa-Toomey, J. Subramanian, N. Malkin, L. Charlin, and Y. Bengio. Joint bayesian inference of graphical structure and parameters with a single generative flow network. In Advances in Neural Information Processing Systems 36 (NeurIPS), 2023.   \n[19] J. Godwin, T. Keck, P. Battaglia, V. Bapst, T. Kipf, Y. Li, K. Stachenfeld, P. Veli\u02c7ckovi\u00b4c, and A. Sanchez-Gonzalez. Jraph: A library for graph neural networks in jax., 2020. URL http://github.com/deepmind/jraph. Version 0.0.1.dev.   \n[20] A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[21] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems 33 (NeurIPS), pages 1474\u20131487, 2020.   \n[22] A. Gu, K. Goel, and C. R\u00e9. Efficiently modeling long sequences with structured state spaces. In Proceedings of the 10th International Conference on Learning Representations (ICLR), 2022.   \n[23] S. Ha and H. Jeong. Unraveling hidden interactions in complex systems with deep learning. Scientific Reports, 11(1):1\u201313, 2021.   \n[24] T. Hennigan, T. Cai, T. Norman, L. Martens, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku. Version 0.0.10.   \n[25] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8): 1735\u20131780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735.   \n[26] W. Huang, G. Wan, M. Ye, and B. Du. Federated graph semantic and structural learning. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI), pages 3830\u20133838, 2023.   \n[27] M. Jain, E. Bengio, A. Hern\u00e1ndez-Garc\u00eda, J. Rector-Brooks, B. F. P. Dossou, C. A. Ekbote, J. Fu, T. Zhang, M. Kilgour, D. Zhang, L. Simine, P. Das, and Y. Bengio. Biological sequence design with gflownets. In Proceedings of the 39th International Conference on Machine Learning (ICML), pages 9786\u20139801. PMLR, 2022.   \n[28] M. Jain, T. Deleu, J. Hartford, C.-H. Liu, A. Hernandez-Garcia, and Y. Bengio. Gflownets for ai-driven scientific discovery. Digital Discovery, 2(3):557\u2013577, 2023.   \n[29] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[30] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[31] T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel. Neural relational inference for interacting systems. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 2688\u20132697. PMLR, 2018.   \n[32] J. Kwapie\u00b4n and S. Dro\u02d9zd\u02d9z. Physical approach to complex systems. Physics Reports, 515(3): 115\u2013226, 2012.   \n[33] S. Lahlou, T. Deleu, P. Lemos, D. Zhang, A. Volokhova, A. Hern\u00e1ndez-Garc\u0131a, L. N. Ezzine, Y. Bengio, and N. Malkin. A theory of continuous generative flow networks. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 18269\u201318300. PMLR, 2023.   \n[34] J. Li, H. Ma, Z. Zhang, J. Li, and M. Tomizuka. Spatio-temporal graph dual-attention network for multi-agent prediction and tracking. IEEE Transactions on Intelligent Transportation Systems, 23(8):10556\u201310569, 2022.   \n[35] K. Li, X. Li, Y. Wang, Y. He, Y. Wang, L. Wang, and Y. Qiao. Videomamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977, 2024.   \n[36] L. Li, H. Wang, W. Zhang, and A. Coster. Stg-mamba: Spatial-temporal graph learning via selective state space model. arXiv preprint arXiv:2403.12418, 2024.   \n[37] W. Li, X. Hong, and X. Fan. Spikemba: Multi-modal spiking saliency mamba for temporal video grounding. arXiv preprint arXiv:2404.01174, 2024.   \n[38] A. Liang, X. Jiang, Y. Sun, and C. Lu. Bi-mamba4ts: Bidirectional mamba for time series forecasting. arXiv preprint arXiv:2404.15772, 2024.   \n[39] S. L\u00f6we, D. Madras, R. Z. Shilling, and M. Welling. Amortized causal discovery: Learning to infer causal graphs from time-series data. In Proceedings of the 1st Conference on Causal Learning and Reasoning (CLeaR), pages 509\u2013525. PMLR, 2022.   \n[40] J. Ma, F. Li, and B. Wang. U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722, 2024.   \n[41] K. Madan, J. Rector-Brooks, M. Korablyov, E. Bengio, M. Jain, A. C. Nica, T. Bosc, Y. Bengio, and N. Malkin. Learning gflownets from partial episodes for improved convergence and stability. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 23467\u201323483. PMLR, 2023.   \n[42] N. Malkin, M. Jain, E. Bengio, C. Sun, and Y. Bengio. Trajectory balance: Improved credit assignment in gflownets. In Advances in Neural Information Processing Systems 35 (NeurIPS), pages 5955\u20135967, 2022.   \n[43] B. N. Patro and V. S. Agneeswaran. Simba: Simplified mamba-based architecture for vision and multivariate time series. arXiv preprint arXiv:2403.15360, 2024.   \n[44] A. Pratapa, A. P. Jalihal, J. N. Law, A. Bharadwaj, and T. Murali. Benchmarking algorithms for gene regulatory network inference from single-cell transcriptomic data. Nature Methods, 17(2): 147\u2013154, 2020.   \n[45] J. Ruan and S. Xiang. Vm-unet: Vision mamba unet for medical image segmentation. arXiv preprint arXiv:2402.02491, 2024.   \n[46] S. M. Smith, K. L. Miller, G. Salimi-Khorshidi, M. Webster, C. F. Beckmann, T. E. Nichols, J. D. Ramsey, and M. W. Woolrich. Network modelling methods for FMRI. Neuroimage, 54(2): 875\u2013891, 2011.   \n[47] S. M. Smith, K. L. Miller, G. Salimi-Khorshidi, M. Webster, C. F. Beckmann, T. E. Nichols, J. D. Ramsey, and M. W. Woolrich. Network modelling methods for FMRI. Neuroimage, 54(2): 875\u2013891, 2011.   \n[48] C. Song, Y. Lin, S. Guo, and H. Wan. Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 914\u2013921, 2020.   \n[49] M. Tsubaki, K. Tomii, and J. Sese. Compound\u2013protein interaction prediction with end-to-end learning of neural networks for graphs and sequences. Bioinformatics, 35(2):309\u2013318, 2019.   \n[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems 30 (NIPS), pages 5998\u20136008, 2017.   \n[51] A. Wang and J. Pang. Iterative structural inference of directed graphs. In Advances in Neural Information Processing Systems 35 (NeurIPS), 2022.   \n[52] A. Wang and J. Pang. Active learning based structural inference. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 36224\u201336245. PMLR, 2023.   \n[53] A. Wang and J. Pang. Structural inference with dynamics encoding and partial correlation coefficients. In Proceedings of the 12th International Conference on Learning Representations (ICLR), 2024.   \n[54] A. Wang, T. P. Tong, and J. Pang. Effective and efficient structural inference with reservoir computing. In Proceedings of the 40th International Conference on Machine Learning (ICML), pages 36391\u201336410. PMLR, 2023.   \n[55] C. Wang, O. Tsepa, J. Ma, and B. Wang. Graph-mamba: Towards long-range graph sequence modeling with selective state spaces. arXiv preprint arXiv:2402.00789, 2024.   \n[56] Z. Wang, F. Kong, S. Feng, M. Wang, H. Zhao, D. Wang, and Y. Zhang. Is mamba effective for time series forecasting? arXiv preprint arXiv:2403.11144, 2024.   \n[57] E. Webb, B. Day, H. Andres-Terre, and P. Li\u00f3. Factorised neural relational inference for multi-interaction systems. arXiv preprints arXiv:1905.08721, 2019.   \n[58] E. Webb, B. Day, H. Andres-Terre, and P. Li\u00f3. Factorised neural relational inference for multi-interaction systems. arXiv preprint arXiv:1905.08721, 2019.   \n[59] H. Wu, Y. Liang, W. Xiong, Z. Zhou, W. Huang, S. Wang, and K. Wang. Earthfarsser: Versatile spatio-temporal dynamical systems modeling in one model. In Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI), pages 15906\u201315914, 2024.   \n[60] Z. Xing, T. Ye, Y. Yang, G. Liu, and L. Zhu. Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560, 2024.   \n[61] Y. Yang, Z. Xing, and L. Zhu. Vivim: a video vision mamba for medical video object segmentation. arXiv preprint arXiv:2401.14168, 2024.   \n[62] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. K. Prasanna. Graphsaint: Graph sampling based inductive learning method. In Proceedings of the 8th International Conference on Learning Representations (ICLR), 2020.   \n[63] D. W. Zhang, C. Rainone, M. Peschl, and R. Bondesan. Robust scheduling with gflownets. In Proceedings of the 11th International Conference on Learning Representations, (ICLR), 2023.   \n[64] S. Zheng, Z. Li, K. Fujiwara, and G. Tanaka. Diffusion model for relational inference. arXiv preprint arXiv:2401.16755, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Appendix of Structural Inference of Dynamical Systems with Conjoined State Space Models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A More Details on Selective SSM in SICSM ", "text_level": 1, "page_idx": 14}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/e0d43a1294e236cf56eb9ca241c7417be5138d0ade7500f5361061a1d2e56f09.jpg", "img_caption": ["Figure 5: The overview of Selective SSM in Residual Blocks. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "In this section, we delve deeper into the capabilities of the Selective SSM, which is central to our SICSM framework, enabling it to learn input-dependent time intervals, denoted as $\\Delta$ , with enhanced adaptability. As illustrated in Figure 5, the selective SSM processes time-series data for each node, represented by $x_{i}$ . For each node $i$ and at each time step $t$ , the projection layer of the selective SSM dynamically learns the transition parameters $\\mathbf{B}_{i}^{t}$ , $\\mathbf{C}_{i}^{t}$ , and $\\Delta_{i}^{t}$ . This capability not only provides the flexibility needed to adapt to varying time intervals but also significantly enriches the model\u2019s ability to capture the nuanced dynamics of each node over time. ", "page_idx": 14}, {"type": "text", "text": "Contrasting with previous models that often rely on static or less adaptable transition parameters [31, 58, 39, 51], our selective SSM design allows SICSM to adjust its learning mechanism based on the input data\u2019s temporal characteristics. This flexibility is crucial for effectively modeling the transitions in dynamics, particularly when dealing with irregularly sampled trajectories. By enabling the selective SSM to adapt its parameters dynamically, SICSM can more accurately reflect the evolving dynamics inherent in complex systems, thus providing a robust framework for predicting changes and interactions within these systems under varying observational conditions. ", "page_idx": 14}, {"type": "text", "text": "B More Details on GFN in SICSM ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The discussion in this section is an addition to the description of GFN used in SICSM (in Section 4.2), with an illustration of the GFN shown in Figure 6. ", "page_idx": 14}, {"type": "text", "text": "B.1 More Details on Flow-matching Conditions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first provide more introduction on general GFNs. GFNs, originally conceptualized through the flow-matching conditions as proposed by Bengio et al. [7], have seen the development of alternative ", "page_idx": 14}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/e0549c95f4722c3dda4333d0e107c1c90c4cb9d8ff24a06f29ff7ff18c2863b7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 6: Structure of GFN in SICSM. Each state $s$ consists of a graph structure $G$ of the underlying interaction graph of the dynamical system, and a generated parameter $\\lambda$ . The initial state $s_{0}$ is the completely disconnected graph. Each state $s$ is complete and connected to a terminal state $s_{f}$ and associated to a reward $\\bar{R}(\\bar{\\langle G,\\lambda\\rangle})$ . Transitioning from one state to another corresponds to adding a directed edge to the graph. ", "page_idx": 15}, {"type": "text", "text": "conditions that ensure equivalent guarantees. These conditions ensure that a GFN satisfying them would sample complete states in proportion to their associated rewards. ", "page_idx": 15}, {"type": "text", "text": "One such alternative is the detailed balance conditions (DB), derived from Markov chain theory, as discussed by Bengio et al. [8]. These conditions are defined for any transition $s\\rightarrow s^{\\prime}$ within the GFN as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nF(s)P_{F}(s^{\\prime}|s)=F(s^{\\prime})P_{B}(s|s^{\\prime}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $F(s)$ represents a flow function, which can be parameterized by a neural network. Bengio et al. [8] demonstrated that adherence to these detailed balance conditions across all transitions $s\\rightarrow s^{\\prime}$ in the GFN ensures that the resulting distribution is proportional to the reward $R(s)$ . In situations where all states in the GFN are complete, Deleu et al. [17] adapted these conditions to eliminate the need for a separate flow function. ", "page_idx": 15}, {"type": "text", "text": "An alternative set of conditions, known as trajectory balance conditions (TB), was introduced by Malkin et al. [42]. These conditions apply at the level of complete trajectories rather than individual transitions. For a complete trajectory $\\boldsymbol{\\tau}\\,=\\,\\left(s_{0},s_{1},...,s_{T},s_{f}\\right)$ , the trajectory balance condition is formulated as: ", "page_idx": 15}, {"type": "equation", "text": "$$\nZ\\prod_{t=1}^{T}P_{F}\\big(s_{t+1}|s_{t}\\big)=R(s_{T})\\prod_{t=1}^{T-1}P_{B}\\big(s_{t}|s_{t+1}\\big),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with the convention $s_{T+1}\\ =\\ s_{f}$ , and where $Z$ is the partition function of the distribution (i.e., $\\textstyle Z\\;=\\;\\sum_{x\\in{\\mathcal{X}}}R(x))$ ; in practice, $Z$ is a learnable parameter of the model that is being learned along side the forward and backward transition probabilities. Compliance with the trajectory balance conditions across all complete trajectories ensures that the induced distribution by the GFN is proportional to the reward $R(s)$ . ", "page_idx": 15}, {"type": "text", "text": "B.2 Learning Objective of GFN ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The GFN of the SICSM learning framework adopts the Subtrajectory Balance conditions (SubTB) [42], a critical concept for ensuring the balance of flow in GFNs, which is a more general and relaxed condition than DB. Instead of enforcing balance at each state transition, SubTB focuses on balancing the probability mass over entire subtrajectories of the generative process. A detailed overview of SubTB is provided in Appendix B.3. The GFN in SICSM is characterized by a dual structure, comprising both the interaction graph $G$ and the node-specific parameters $\\lambda$ . To accommodate this unique structure, we adopt a modified form of SubTB as proposed in [18]: ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nR(\\langle G^{\\prime},\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})P_{\\Omega}(\\lambda|G)=R(\\langle G,\\lambda\\rangle)P_{\\Omega}(G^{\\prime}|G)P_{\\Omega}(\\lambda^{\\prime}|G^{\\prime}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $P_{B}(\\cdot)$ represents the backward transition probability, and $\\lambda^{\\prime}$ denotes the parameters generated conditional on graph $G^{\\prime}$ . This reformulation ensures that both the graph $G$ (including the aggregated node dynamics embeddings $U_{A l l}$ as well as the structure Adj) and the parameters $\\lambda$ are integral to the reward function, thereby reinforcing their importance in structural inference tasks. We show in Appendix E.2 with experimental results that this set up greatly encourages the successful integration of prior knowledge on existing edges. Additional details on this reformulation are available in Appendix B.4. ", "page_idx": 16}, {"type": "text", "text": "For SICSM, we define the learning objective as the squared error of the log ratio between forward and backward transitions, in line with the approach in [18]. Specifically, the learning objective is given by: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}_{G F N}=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\left(\\log\\frac{R(\\langle G^{\\prime},\\wedge\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})P_{\\Omega}(\\wedge\\lambda|G)}{R(\\langle G,\\wedge\\lambda\\rangle)P_{\\Omega}(G^{\\prime}|G)P_{\\Omega}(\\wedge\\lambda^{\\prime}|G^{\\prime})}\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\pi$ is a sampling distribution over pairs $\\langle G,\\lambda\\rangle$ and $\\langle{G^{\\prime},\\lambda^{\\prime}}\\rangle$ , and $\\wedge$ denotes the \u2018stop-gradient\u2019 operation. This operation is crucial to prevent backpropagation through $\\lambda$ and $\\lambda^{\\prime}$ , thereby avoiding potential infinite loops. ", "page_idx": 16}, {"type": "text", "text": "B.3 More details about SubTB ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The concept of subtrajectory balance conditions, introduced by Malkin et al. [42], serves as a generalization of both detailed balance and trajectory balance conditions, extending their application to partial trajectories of varying lengths. These conditions are defined for a partial state trajectory $\\tau=(s_{m},s_{m+1},...,s_{n})$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nF(s_{m})\\prod_{t=m}^{n-1}P_{F}(s_{t+1}|s_{t})=F(s_{n})\\prod_{t=m}^{n-1}P_{B}(s_{t}|s_{t+1}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $F(s)$ denotes a flow function. This framework effectively encapsulates both conditions outlined in Appendix B.1, by accommodating for partial state trajectories of single-step transitions (as in Eqn. 18), and for complete trajectories (as in Eqn.19), with $F(s_{0})=Z$ as per Bengio et al. [8]). Furthermore, Madan et al. [41] proposed a novel objective, SubTB $(\\lambda)$ , which synergizes subtrajectory balance conditions for partial trajectories of differing lengths, drawing inspiration from the $\\mathrm{TD}(\\boldsymbol{\\lambda})$ approach in reinforcement learning. ", "page_idx": 16}, {"type": "text", "text": "These subtrajectory balance conditions are also adaptable to undirected paths, allowing for \u201cback and forth\u201d movements between states [42]. For an undirected path between $s_{m}$ and $s_{n}$ , this (generalized) subtrajectory balance condition can be written as ", "page_idx": 16}, {"type": "equation", "text": "$$\nF(s_{m})\\prod_{t=k}^{m-1}P_{B}(s_{t}|s_{t+1})\\prod_{t=k}^{n-1}P_{F}(s_{t+1}|s_{t})=F(s_{n})\\prod_{t=k}^{n-1}P_{B}(s_{t}|s_{t+1})\\prod_{t=k}^{m-1}P_{F}(s_{t+1}|s_{t}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $s_{k}$ is a common ancestor of both $s_{m}$ and $s_{n}$ . These conditions, whether generalized or specific, offer greater flexibility in their application. However, to guarantee that a GFN induces a distribution proportional to $R(s)$ , it is essential that these conditions are satisfied for all partial trajectories of any length. In this paper, we specifically focus on scenarios where these conditions are met for partial state trajectories of fixed length. Although this approach may deviate from the general guarantees, we follow the implementation discussed by Deleu et al. [18] and expound in Appendix B.4 how our GFN still induces a distribution $\\propto R(s)$ in our context. ", "page_idx": 16}, {"type": "text", "text": "B.4 More details about Reformulation of SubTB ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we strictly follow the implementation and proofs discussed in [18]. ", "page_idx": 16}, {"type": "text", "text": "Subtrajectory balance conditions for undirected paths of length 3. Consider an undirected path of length 3 denoted as $\\langle G,\\lambda\\rangle\\gets\\langle G,\\cdot\\rangle\\to\\langle G^{\\prime},\\cdot\\rangle\\to\\langle G^{\\prime},\\lambda^{\\prime}\\rangle$ , where $G^{\\prime}$ is derived from the directed acyclic graph (DAG) $\\mathcal{G}$ by the addition of a new edge.. Given that the state $\\langle G,\\cdot\\rangle$ is a ", "page_idx": 16}, {"type": "text", "text": "common ancestor to both complete states $\\langle G,\\lambda\\rangle$ and $\\langle{G^{\\prime},\\lambda^{\\prime}}\\rangle$ , we can apply the subtrajectory balance conditions as expressed in Eqn. 23. The conditions are reformulated as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(G,\\lambda)P_{B}(G|\\lambda)P_{F}(G^{\\prime}|G)P_{F}(\\lambda^{\\prime}|G^{\\prime})=F(G^{\\prime},\\lambda^{\\prime})P_{B}(G^{\\prime}|\\lambda^{\\prime})P_{B}(G|G^{\\prime})P_{F}(\\lambda|G),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $P_{B}(G|\\lambda)$ denotes $P_{B}(\\langle G,\\cdot\\rangle|\\langle G,\\lambda\\rangle)$ , a notation simplification for clarity. As $\\langle G,\\lambda\\rangle\\in\\mathcal{X}$ has a single parent state $\\langle G,\\cdot\\rangle$ , it follows that $P_{B}(G|\\lambda)=1$ , and a similar rationale applies to $\\langle{G^{\\prime},\\lambda^{\\prime}}\\rangle$ . Building upon the insights from Deleu et al. [17], the flow function $F(G,\\lambda)$ of a complete state $\\langle G,\\lambda\\rangle$ can be expressed as a function of its associated reward: ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(G,\\lambda)=\\frac{R(\\langle G,\\lambda\\rangle)}{P_{F}(s_{f}|\\langle G,\\lambda\\rangle)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In our GFN implementation, $s_{f}$ is the sole child of the terminal state $\\langle G,\\lambda\\rangle\\in{\\mathcal{X}}$ , indicating an (infinitely wide) tree structure rooted at $\\langle G,\\cdot\\rangle$ . Consequently, $P_{F}(s_{f}|\\langle G,\\lambda\\rangle)=1$ , leading to the simplification $F(G,\\lambda)=R(\\left\\langle{G,\\lambda}\\right\\rangle)$ . With these simplifications, Eqn. 24 becomes ", "page_idx": 17}, {"type": "equation", "text": "$$\nR(\\langle G,\\lambda\\rangle)P_{F}(G^{\\prime}|G)P_{F}(\\lambda^{\\prime}|G^{\\prime})=R(\\langle G^{\\prime},\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})P_{F}(\\lambda|G),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which is the subtrajectory balance condition in Eqn. 20. This formulation effectively captures the essence of the balance conditions, providing a clear and concise representation of the underlying principles in the GFN structure for structural inference. ", "page_idx": 17}, {"type": "text", "text": "Integrating undirected paths of length 2. Similar to the previous paragraph, we consider here an undirected path of length 2 of the form $\\langle G,\\lambda\\rangle\\,\\gets\\,\\langle G,\\cdot\\rangle\\,\\to\\,\\langle G,\\tilde{\\lambda}\\rangle$ . Since $\\langle G,\\cdot\\rangle$ is a common ancestor (a common parent in this case) of both terminal states $\\langle G,\\lambda\\rangle$ and $\\langle G,{\\tilde{\\lambda}}\\rangle$ , we can write the subtrajectory balance conditions (Eqn. 23) as: ", "page_idx": 17}, {"type": "equation", "text": "$$\nF(G,\\lambda)P_{B}(G|\\lambda)P_{F}(\\tilde{\\lambda}|G)=F(G,\\tilde{\\lambda})P_{B}(G|\\tilde{\\lambda})P_{F}(\\lambda|G).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using the same simplifications as in the previous paragraph $(P_{B}(G|\\lambda)=P_{B}(G|\\tilde{\\lambda})=1)$ ), we get the following subtrajectory balance conditions for the undirected paths of length 2: ", "page_idx": 17}, {"type": "equation", "text": "$$\nR(\\langle G,\\lambda\\rangle)P_{F}(\\tilde{\\lambda}|G)=R(\\langle G,\\tilde{\\lambda}\\rangle)P_{F}(\\lambda|G).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that these conditions are effectively redundant if the SubTB conditions over undirected paths of length 3 are satisfied for all possible pairs of terminal states $\\langle G,\\lambda\\rangle$ and $\\langle{G^{\\prime},\\lambda^{\\prime}}\\rangle$ . Indeed, if we write these conditions between $\\langle G,\\lambda\\rangle$ and $\\langle{G^{\\prime},\\lambda^{\\prime}}\\rangle$ on the one hand, and between $\\langle G,{\\tilde{\\lambda}}\\rangle$ and $\\langle{G^{\\prime},\\lambda^{\\prime}}\\rangle$ on the other hand (with a fixed $G^{\\prime}$ and $\\lambda^{\\prime}$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R(\\langle G^{\\prime},\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})P_{F}(\\lambda|G)=R(\\langle G,\\lambda\\rangle)P_{F}(G^{\\prime}|G)P_{F}(\\lambda^{\\prime}|G),}\\\\ &{R(\\langle G^{\\prime},\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})P_{F}(\\tilde{\\lambda}|G)=R(\\langle G,\\tilde{\\lambda}\\rangle)P_{F}(G^{\\prime}|G)P_{F}(\\lambda^{\\prime}|G),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we get the same subtrajectory balance conditions over undirected paths of length 2 as in Eqn. 28: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{R(\\langle G,\\lambda\\rangle)}{P_{F}(\\lambda|G)}=\\frac{R(\\langle G^{\\prime},\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})}{P_{F}(G^{\\prime}|G)P_{F}(\\lambda^{\\prime}|G^{\\prime})}=\\frac{R(\\langle G,\\lambda^{\\prime}\\rangle)}{P_{F}(\\tilde{\\lambda}|G)}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "However, since the SubTB conditions are only satisfied approximately in practice, it might be advantageous to also satisfy Eqn. 28. The equation above provides an alternative way to express Eqn. 28. Indeed, Eqn. 31 shows that the function ", "page_idx": 17}, {"type": "equation", "text": "$$\nf_{G}(\\lambda)\\triangleq\\log R(\\langle G,\\lambda\\rangle)-\\log P_{F}(\\lambda|G)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "is constant, albeit with a constant that depends on the graph $G$ . Since this function is differentiable, this is equivalent to $\\bigtriangledown\\lambda f_{G}(\\lambda)=0$ , and therefore we get the differential form of the subtrajectory balance conditions: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\nabla_{\\lambda}\\log P_{F}(\\lambda|G)=\\nabla_{\\lambda}\\log R(\\langle G,\\lambda\\rangle).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As shown by [18], one way to enforce the SubTB conditions over undirected paths of length 3 is to create a learning objective that encourages these conditions to be satisfied, and optimizing it using gradient methods. The learning objective has the form $\\mathcal{L}_{G F N}=\\mathbb{E}_{\\pi}[\\tilde{\\triangle}^{2}(\\Omega)]$ , where $\\tilde{\\triangle}(\\Omega)$ is a non-linear residual term ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\tilde{\\triangle}(\\Omega)=\\log\\frac{R(\\langle G^{\\prime},\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})P_{\\Omega}(\\lambda|G)}{R(\\langle G,\\lambda\\rangle)P_{\\Omega}(G^{\\prime}|G)P_{\\Omega}(\\lambda^{\\prime}|G^{\\prime})}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Suppose that the parameters $\\Omega$ of the GFN are such that the subtrajectory balance conditions in Eqn. 33 are satisfied for any $\\langle G,\\lambda\\rangle$ . Although this assumption is unlikely to be satisfied in practice, they will eventually be approximately satisfied over the course of optimization, given the discussion above about the relation between Eqn. 28 and Eqn. 26. Since $\\lambda$ and $\\lambda^{\\prime}$ depend on $\\Omega$ (via the reparametrization trick since they are sampled on-policy [18]), taking the derivative of $\\tilde{\\triangle}^{2}(\\Omega)$ , we get: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d\\Omega}\\tilde{\\triangle}^{2}(\\Omega)=\\tilde{\\triangle}(\\Omega)\\cdot\\frac{d}{d\\Omega}[\\log R(\\langle G^{\\prime},\\lambda^{\\prime}\\rangle)+\\log P_{\\Omega}(\\lambda|G)-\\log R(\\langle G,\\lambda\\rangle)-\\log P_{\\Omega}(G^{\\prime}|G)}\\\\ {-\\log P_{\\Omega}(\\lambda^{\\prime}|G^{\\prime})].\\hfill.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the law of total derivatives, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{d}{d\\Omega}[\\log P_{\\Omega}(\\lambda|G)-\\log R(\\langle G,\\lambda\\rangle)=\\underbrace{\\left[\\frac{\\partial}{\\partial\\lambda}\\log P_{\\Omega}(\\lambda|G)-\\frac{\\partial}{\\partial\\lambda}\\log R(\\langle G,\\lambda\\rangle)\\right]}_{=0}\\frac{d\\lambda}{d\\Omega}}}\\\\ &{\\quad\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +\\,\\frac{\\partial}{\\partial\\Omega}\\log P_{\\Omega}(\\lambda|G)}\\\\ &{=\\frac{\\partial}{\\partial\\Omega}\\log P_{\\Omega}(\\lambda|G)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and similarly for the terms in $\\langle{G^{\\prime},\\lambda^{\\prime}}\\rangle$ . The derivative of the objective then becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{d}{d\\Omega}\\tilde{\\triangle}^{2}(\\Omega)=\\tilde{\\triangle}(\\Omega)\\cdot\\left[\\frac{\\partial}{\\partial\\Omega}\\log P_{\\Omega}(\\lambda|G)-\\frac{\\partial}{\\partial\\Omega}\\log P_{\\Omega}(\\lambda^{\\prime}|G^{\\prime})-\\frac{d}{d\\Omega}\\log P_{\\Omega}(G^{\\prime}|G)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "An alternative way to obtain the same derivative in Eqn. 39 as the objective in Eqn. 33 is to take $d\\lambda/d\\Omega=0$ instead, meaning that we would not differentiate through $\\lambda$ (and $\\lambda^{\\prime}$ ). Using the stopgradient operation $\\wedge$ , this shows the following objective ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{L}_{G F N}=\\mathbb{E}_{\\boldsymbol\\pi}\\left[\\left(\\log\\frac{R(\\langle G^{\\prime},\\wedge\\lambda^{\\prime}\\rangle)P_{B}(G|G^{\\prime})P_{\\Omega}(\\wedge\\lambda|G)}{R(\\langle G,\\wedge\\lambda\\rangle)P_{\\Omega}(G^{\\prime}|G)P_{\\Omega}(\\wedge\\lambda^{\\prime}|G^{\\prime})}\\right)^{2}\\right],\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "takes the same value and has the same gradient (Eqn. 39) as the objective in Eqn. 34 when the subtrajectory balance conditions (in differential form) over undirected paths of length 2 are satisfied. ", "page_idx": 18}, {"type": "text", "text": "While optimizing Eqn. 40 alone leads to eventually satisfying the subtrajectory balance conditions over undirected paths of length 2, it may be advantageous to explicitly encourage this behavior, especially in cases for non-linear models. We can incorporate some penalty to the loss function, such as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\tilde{\\mathcal{L}}_{G F N}=\\mathcal{L}_{G F N}+\\frac{\\beta}{2}\\mathbb{E}_{\\pi}[\\|\\,\\bigtriangledown_{\\lambda}\\log P_{\\Omega}(\\lambda|G)-\\bigtriangledown_{\\lambda}\\log R(\\langle G,\\lambda\\rangle)\\|^{2}+\\|\\,\\bigtriangledown_{\\lambda^{\\prime}}\\log P_{\\Omega}(\\lambda^{\\prime}|G^{\\prime})}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad-\\,\\bigtriangledown_{\\lambda^{\\prime}}\\log R(\\langle G^{\\prime},\\lambda^{\\prime}\\rangle)\\|^{2}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.5 Forward Transition Probabilities ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "As delineated in Section 4.2, SICSM generates the pair $\\langle G,\\lambda\\rangle$ through a two-phase process: (a) constructing the graph $G=(U_{A l l},\\mathbf{Adj})$ by sequentially adding edges in Adj until a \u2018stop\u2019 action is triggered, followed by (b) sampling the parameters $\\lambda$ conditional on $G$ . These actions are governed by the forward transition probabilities $\\bar{P_{\\Omega}}(G^{\\prime}|G)$ in the first phase and $P_{\\Omega}(\\lambda|G)$ in the second phase. ", "page_idx": 18}, {"type": "text", "text": "To parameterize these terms, we adopt a hierarchical model strategy [18]. This model first determines whether to halt the first phase using the probability $P_{\\Omega}(\\operatorname{stop}|G)$ . Based on this decision, the process either continues by adding an edge to Adj, forming $G^{\\prime}=(U_{A l l},\\mathbf{Adj}^{\\prime})$ with probability $\\bar{P}_{\\Omega}(G^{\\prime}|G,\\mathrm{{\\negstop})}$ , or transitions to the second phase by sampling $\\lambda$ with probability $P_{\\Omega}(\\bar{\\lambda}|G,\\mathrm{stop})$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{\\Omega}(G^{\\prime}|G)=(1-P_{\\Omega}(\\mathrm{stop}|G))P_{\\Omega}(G^{\\prime}|G,\\mathrm{\\negstop}),}\\\\ &{P_{\\Omega}(\\lambda|G)=P_{\\Omega}(\\mathrm{stop}|G)P_{\\Omega}(\\lambda|G,\\mathrm{stop}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To accurately parameterize $P_{\\Omega}(\\operatorname{stop}|G)$ , $P_{\\Omega}(G^{\\prime}|G,\\mathrm{{\\negstop})}$ , and $P_{\\Omega}(\\lambda|G,\\mathrm{stop})$ , we utilize a combination of graph neural networks (GNNs) [5] and self-attention mechanisms [50]. This fusion of GNNs ", "page_idx": 18}, {"type": "text", "text": "and self-attention blocks ensures a robust and flexible modeling of the transitions between states in SICSM. Further details regarding this parameterization approach are elaborated in Section B.6. ", "page_idx": 19}, {"type": "text", "text": "B.6 Parameterization with Neural Networks ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In the GFN of SICSM, various components are parameterized using neural networks. Specifically, we focus on parameterizing the following: (a) $P_{\\Omega}(\\operatorname{stop}|G)$ , (b) $P_{\\Omega}(G^{\\prime}|G,\\mathrm{{\\negstop})}$ , (c) $P_{\\Omega}(\\lambda|G,\\mathrm{stop})$ , and (d) the log-likelihood term $\\log P(v_{i}^{t+1}|\\lambda,\\tilde{A d j},U_{i}^{t})$ . We combine GNN with self-attention mechanisms for parameterizing (a) $P_{\\Omega}(\\operatorname{stop}|G)$ and (b) $\\hat{P}_{\\Omega}(G^{\\prime}|G,\\mathrm{{\\negstop})}$ . This process generates a graph-level attribute $\\mathbf{g}$ and node-level attributes $\\left\\{\\mathbf{u}_{i},\\mathbf{v}_{i},\\mathbf{w}_{i}\\right\\}$ for each node $i$ in $G$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{g},\\left\\{\\mathbf{u}_{i},\\mathbf{v}_{i},\\mathbf{w}_{i}\\right\\}_{i=1}^{n}=\\mathbf{SelfAttention}_{\\Omega}\\left(\\mathbf{GNN}_{\\Omega}(G)\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The \u2018stop\u2019 action probability is computed as $P_{\\Omega}(\\mathrm{stop}|G)=f_{\\Omega}(\\mathbf{g})$ , where $f_{\\Omega}$ is a neural network with a sigmoid output layer. The probability of transitioning from $G$ to $G^{\\prime}$ in the absence of a \u2018stop\u2019 action is defined as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{\\Omega}(G^{\\prime}|G,\\mathrm{\\mathbf{\\vec{s}}t o p})\\propto\\mathbf{m}_{i j}\\exp(\\mathbf{u}_{i}^{\\top}\\mathbf{v}_{j}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{m}_{i j}$ is a binary mask that excludes already explored graph structures. (c) For sampling parameters $\\lambda_{i}$ for each node $i$ , we define: ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{\\Omega}(\\lambda_{i}|G,\\mathrm{stop})=\\mathcal{N}\\left(\\lambda_{i}|\\mu_{\\Omega}(\\mathbf{w}_{i}),\\sigma_{\\Omega}^{2}(\\mathbf{w}_{i})\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with $\\mu_{\\Omega}$ and $\\sigma_{\\Omega}^{2}$ being neural networks. This formulation effectively approximates the posterior distribution $P(\\bar{\\lambda}_{i}|G,U_{A l l})$ upon full training. To get the adjacency matrix for the dynamical system, we need to approximate the marginal posterior $P(\\mathbf{Adj}|U_{A l l})$ . We follow the phases to generate $G$ until a \u2018stop\u2019 action. By aggregating $\\{\\mathbf{Adj}_{1},\\mathbf{Adj}_{2},...,\\mathbf{Adj}_{B}\\}$ from the posterior approximation, we estimate the marginal probability of a directed edge from node $i$ to node $j$ as: ", "page_idx": 19}, {"type": "equation", "text": "$$\nP_{\\Omega}(i\\rightarrow j|U_{A l l})\\approx\\frac{1}{B}\\sum_{b=1}^{B}\\mathbf{1}(i\\rightarrow j\\in\\mathbf{A}\\mathbf{dj}_{b}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathbf{1}(\\cdot)$ is the indicator function. This process enables the inference of the underlying interaction graph\u2019s structure, which is critical for evaluating the accuracy of SICSM in structural inference tasks. (d) For the log-likelihood term, we employ a message-passing neural network to compute future node features: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\{\\mu_{i}^{t+1},\\sigma_{i}^{t+1}\\}=\\mathbf{MLP}(U_{A l l_{i}},\\tilde{A d j}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\mathrm{{MLP}(\\cdot)}$ is a neural network that outputs the parameters of the probability distribution for the future state ${\\boldsymbol{v}}_{i}^{t+1}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{MLP}(U_{A l l_{i}},\\tilde{A d j})=\\left\\{U_{i}^{t}+f_{e}\\left(\\sum_{j\\rightarrow i\\in\\tilde{A d j}}f_{a}(U_{i}^{t},U_{j}^{t})\\right)\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $f_{e}$ and $f_{a}$ are multilayer perceptions, and the operation is performed for all nodes. Then we have log-likelihood for each node $i$ at time $t$ can be computed as: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\log P(v_{i}^{t+1}|\\lambda,\\tilde{A d j},U_{i}^{t})=\\log\\mathcal{N}(v_{i}^{t+1}|\\mu_{i}^{t+1},\\sigma_{i}^{t+1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "If the node features are multi-dimensional, we set up multiple readout heads in Eqn. 48. This approach effectively handles multi-dimensional node features and incorporates both node features and graph structure, thereby reinforcing the model\u2019s predictive accuracy. ", "page_idx": 19}, {"type": "text", "text": "C More Details about Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we provide more details about the datasets used in this work apart the description in Section 5. ", "page_idx": 19}, {"type": "text", "text": "C.1 Springs Simulations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To generate these Springs Simulations datasets, we follow the description of the data in [31] but with fixed connections and with 10 nodes, in order to simulate spring-connected particles\u2019 motion in a 2D box using the Springs simulation. In this setup, nodes represent particles, and edges correspond to springs governed by Hooke\u2019s law. The Springs simulation\u2019s dynamics are described by a second-order ordinary differential equation: $\\begin{array}{r}{m_{i}\\cdot x_{i}^{\\hat{\\prime}}(t)\\overset{\\leftarrow}{=}\\sum_{j\\in\\mathcal{N}_{i}}-k\\cdot\\overset{\\cdot}{\\left(x_{i}(t)\\mathrm{~-~}x_{j}(t)\\right)}}\\end{array}$ . Here, $m_{i}$ represents particle mass (assumed as 1), $k$ is the fixed spring constant (set to 1), and ${\\mathcal{N}}_{i}$ is the set of neighboring nodes with directed connections to node $i$ , which is sub-sampled from the graphs generated in the StructInfer in previous steps. We integrate this equation to compute $x_{i}^{\\prime}(t)$ and subsequently $x_{i}(t)$ for each time step $t$ . The resulting values of $x_{i}^{\\prime}(t)$ and $x_{i}(t)$ create 4D node features at each time step. To be specific, at the beginning of the data generation for each springs dataset, we randomly generate a ground truth graph and then simulate 12000 trajectories on the same ground truth graph, but with different initial conditions. The rest settings are the same as that mentioned in [31]. We collect the trajectories and randomly group them into three sets for training, validation and testing with the ratio of 8: 2: 2, respectively. ", "page_idx": 20}, {"type": "text", "text": "C.2 NetSims ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "It is firstly mentioned in [46], which offers simulations of blood-oxygen-level-dependent (BOLD) imaging data in various human brain regions. Nodes in the dataset represent spatial regions of interest from brain atlases or functional tasks. Interaction graphs from the previous section determine connections between these regions. Dynamics are governed by a first-order ODE model: $x_{i}^{\\prime}(t)=$ $\\begin{array}{r}{\\sigma\\cdot\\sum_{j\\in\\mathcal{N}_{i}}x_{j}(t)-\\sigma\\cdot x_{i}(t)+C\\cdot u_{i}}\\end{array}$ , where $\\sigma$ controls temporal smoothing and neural lag (set to 0.1 based on [46], and $C$ regulates external input interactions (set to zero to minimize external input noise) [46]. 1D node features at each time step are obtained from the sampled $x_{i}(t)$ . ", "page_idx": 20}, {"type": "text", "text": "C.3 Synthetic Biological Networks ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The six directed Boolean networks (LI, LL, CY, BF, TF, BF-CV) are the most often observed fragments in many gene regulatory networks, each has 7, 18, 6, 7, 8 and 10 nodes, respectively. Thus by carrying out experiments on these networks, we can acknowledge the performance of the chosen methods on the structural inference of real-world biological networks. We collect the six groundtruth directed Boolean networks from [44] and simulate the single-cell evolving trajectories with BoolODE [44] (https://github.com/Murali-group/BoolODE) with default settings mentioned in that paper for every network. We first sample a total number of 12000 raw trajectories. We then sample different numbers of trajectories from raw trajectories and randomly group them into three datasets: for training, for validation, and for testing, with a ratio of $8:2:2$ . After that, we sample different numbers of snapshots according to the requirements of experiments in Section 5.1 with equal time intervals in every trajectory and save them as \u2018.npy\u2019 files for data loading. ", "page_idx": 20}, {"type": "text", "text": "C.4 StructInfer Benchmark ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The StructInfer benchmark [3] evaluated 12 structural inference methods in a comprehensive way on a synthetic dataset. The dataset covers 11 types of different underlying interaction graphs and two types of dynamical simulations. (https://structinfer.github.io/) As there are so many trajectories, we chose the ones under the name \u2018Vascular Networks\u2019, or in short \u2018VN\u2019, whose underlying interaction graphs approximate the real-world vascular networks in biology systems. As the data is already split into three sets: for training, for validation, and for testing, we keep this setting. In the following paragraphs, we describe more details about the Springs and NetSims simulations utilized by the StructInfer benchmark. ", "page_idx": 20}, {"type": "text", "text": "For Springs simulation, it follows the approach by Kipf et al. [31], to simulate spring-connected particles\u2019 motion in a 2D box using the Springs simulation. In this setup, nodes represent particles, and edges correspond to springs governed by Hooke\u2019s law. But different from Springs Simulations mentioned above, StructInfer generates ground-truth interaction graphs with the graph properties of the real-world graphs or network. The ground-truth interaction graphs are used to determine the connectivity between the nodes. The Springs simulation\u2019s dynamics are described by a second-order ordinary differential equation: $\\begin{array}{r}{m_{i}\\cdot\\dot{x_{i}^{\\prime\\prime}}(t)\\overset{\\ \u3001\\ \u3001}{=}\\sum_{j\\in\\mathcal{N}_{i}}-k\\overset{\\ \u3001\\ \u3001}{\\cdot}\\big(x_{i}(t)-x_{j}(t)\\big)}\\end{array}$ . Here, $m_{i}$ represents particle mass (assumed as 1), $k$ is the fixed spring constant (set to 1), and ${\\mathcal{N}}_{i}$ is the set of neighboring nodes with directed connections to node $i$ , which is sub-sampled from the graphs generated in the StructInfer in previous steps. We integrate this equation to compute $x_{i}^{\\prime}(t)$ and subsequently $x_{i}(t)$ for each time step $t$ . The resulting values of $x_{i}^{\\prime}(t)$ and $x_{i}(t)$ create 4D node features at each time step. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "For NetSims simulation, it is firstly mentioned in NetSim dataset [46], which offers simulations of blood-oxygen-level-dependent (BOLD) imaging data in various human brain regions. Nodes in the dataset represent spatial regions of interest from brain atlases or functional tasks. But different from NeiSim mentioned above, StructInfer generates ground-truth interaction graphs with the graph properties of the real-world graphs or network. The ground-truth interaction graphs are used to determine the connectivity between the nodes. Dynamics are governed by a first-order ODE model: $\\begin{array}{r}{x_{i}^{\\prime}(t)=\\sigma\\cdot\\sum_{j\\in\\mathcal{N}_{i}}{x_{j}(t)}\\overset{\\cdot}{-}\\sigma\\cdot x_{i}(t)+C\\cdot u_{i}}\\end{array}$ , where $\\sigma$ controls temporal smoothing and neural lag (set to 0.1 based on [46], and $C$ regulates external input interactions (set to zero to minimize external input noise) [46]. 1D node features at each time step are obtained from the sampled $x_{i}(t)$ . ", "page_idx": 21}, {"type": "text", "text": "C.5 PEMS Datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "These datasets, derived from the California Caltrans Performance Measurement System (PeMS) [11], comprise data aggregated into 5-minute intervals. The adjacency matrix of the nodes is constructed by road network distance with a thresholded Gaussian kernel [48]. Table 1 summarizes these datasets. ", "page_idx": 21}, {"type": "table", "img_path": "xQWJBeK5rh/tmp/a7a1f42be413701f23addc5d08511a709326a44f4e2e7b87f6e3d6cbe57ca6c5.jpg", "table_caption": ["Table 1: Statistics of PEMS datasets. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "We resampled the data such that constructing 49 time steps of points for each trajectory, and obtained 12000 trajectories for each with overlapping snapshots. It\u2019s important to note that these datasets\u2019 adjacency matrices only connect sensors on the same road, omitting alternative connecting paths, which could impact results. ", "page_idx": 21}, {"type": "text", "text": "D Implementation of Baselines ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the experiments without prior knowledge, we follow the official implementation of the baselines. As for the integrating of the prior knowledge, we leverage different strategies. For the methods based on VAEs, (e.g. NRI, MPM, ACD, iSIDG, RCSI), we directly perform supervised learning on the latent space with known edges, while keep the rest following the original implementation. For JSP-GFN, we set the graph structure in the initial state and reset states the same as prior knowledge. ", "page_idx": 21}, {"type": "text", "text": "D.1 NRI ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "NRI [31] is a VAE-based model for unsupervised relational inference. We use the official imple mentation code by the author from https://github.com/ethanfetaya/NRI with a customized data loader for our chosen datasets. We add our metric evaluation in the \u2018test\u2019 function, after the calculation of accuracy in the original code. ", "page_idx": 21}, {"type": "text", "text": "D.2 MPM ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "MPM [12] employs a VAE framework with a relational interaction mechanism and spatio-temporal message passing. We use the official implementation code by the author from https://github. com/hilbert9221/NRI-MPM with a customized data loader for our chosen datasets. We add our metric evaluation for AUROC in the \u2018evaluate()\u2019 function of class \u2018XNRIDECIns\u2019 in the original code. ", "page_idx": 21}, {"type": "text", "text": "D.3 ACD ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "ACD [39] utilizes shared dynamics to infer causal relations within datasets. We follow the official implementation code by the author as the framework for ACD (https://github.com/loeweX/ AmortizedCausalDiscovery). We run the code with a customized data loader for the datasets in this work. We implement the metric-calculation pipeline in the \u2018forward_pass_and_eval()\u2019 function. ", "page_idx": 22}, {"type": "text", "text": "D.4 ISIDG ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "iSIDG [51] iteratively refines adjacency matrices to enhance directional inference. We follow the official implementation code by the author as the framework for iSIDG (https://github.com/wang422003/ Benchmarking-Structural-Inference-Methods-for-Interacting-Dynamical-Systems/ tree/main/src/models/iSIDG). We disable the metric evaluations for the AUPRC and Jaccard index in the original implementation of iSIDG for faster computation. ", "page_idx": 22}, {"type": "text", "text": "D.5 RCSI ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "RCSI [54] integrates reservoir computing for efficient structural inference. We would like to thank the authors of RCSI for the code. Same as iSIDG, we disable the metric evaluations for AUPRC and Jaccard index in the original implementation of iSIDG for faster computation. ", "page_idx": 22}, {"type": "text", "text": "D.6 JSP-GFN ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "JSP-GFN [18] applies Generative Flow Networks for Bayesian inference of graphical structures. We follow the official implementation code by the author as the framework for JSP-GFN (https: //github.com/tristandeleu/jax-jsp-gfn). We run the code with a customized data loader for the datasets in this work. ", "page_idx": 22}, {"type": "text", "text": "D.7 SIDEC ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "SIDEC [53] encodes node dynamics to exploit partial correlations for structural inference. We follow the official implementation code by the author as the framework for SIDEC (https://github.com/ wang422003/SIDEC_torch). We run the code with a customized data loader for the datasets in this work. By incorporating the prior knowledge, we did not figure out a feasible way to do so. Thus we omit the implementation for integrating prior knowledge. ", "page_idx": 22}, {"type": "text", "text": "D.8 Implementation details of SICSM ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The general training pipeline of SICSM is presented in Algorithm 1. ", "page_idx": 22}, {"type": "text", "text": "SICSM is implemented with JAX [9], including following packages: dm-haiku [24] and jraph [19]. The implementation of SICSM model consists of two parts: (1) the implementation of Residual Blocks with Mamba, and (2) the implementation of GFN. The implementation of Residual Blocks follows the script of \u2018mamba-minimal-jax\u2019 (https://github.com/radarFudan/mamba-minimal-jax/ tree/main). We would like to thank the contributors of this repository for all the efforts they have done. And the implementation of GFN of SICSM follows the implementation of JSPGFN [18], and the authors\u2019 implementation can be found at https://github.com/tristandeleu/ jax-jsp-gfn. We would like to thank the authors for code and hints. The modifications were made to integrate various regularization terms in the graph prior, the log-likelihood, and the data-loading pipelines. Please refer to the link provided in the supplementary document for the exact implementation of SICSM. SICSM is trained with Adam [29] optimizer, with the learning rate as 0.00001 and for 1000 epochs. Implementation can be found at: https://github.com/wang422003/SICSM-JAX/. ", "page_idx": 22}, {"type": "text", "text": "Among all, the most important hyperparameter of SICSM would be the number of Residual Blocks in encoder and decoder, $L$ and $L^{\\prime}$ , respectively. As we expect a symmetric structure of both, so $L^{\\prime}$ is set as equal to $L$ . The exact number of layers actually depends on the number of nodes in the graph, and we report the values of $L$ in Table 2. ", "page_idx": 22}, {"type": "text", "text": "Algorithm 1 The training procedure of SICSM ", "page_idx": 23}, {"type": "text", "text": "1: Input: trajectory $\\nu$ of $n$ nodes 2: Parameters: number of steps of prefilling $\\xi$ , learning rate $\\alpha$ 3: Parameters: number of Residual Blocks in encoder $L$ , number of Residual Blocks in decoder $L^{\\prime}$ 4: Output: Structure of the dynamical system Adj 5: Decompose the input trajectory to form present feature set $V^{0:T-2}$ and forecasting feature set V 1:T \u22121 6: Initialize the State trajectory at $G_{0}$ as an unconnected graph with $n$ nodes 7: repeat 8: Get feature-based embedding $\\mathbf{h}_{i}^{t}$ for every node and for every time step: $\\mathbf{h}_{i}^{t}=f_{e m b e d}(v_{i}^{t})$ 9: Compose $\\mathbf{H}_{i}=[\\mathbf{h}_{i}^{0},\\mathbf{h}_{i}^{1},...,\\mathbf{h}_{i}^{T-2}]$ for every node 10: Compose $\\mathbf{H}_{a l l}=[\\mathbf{H}_{i}$ , for all nodes] 11: for $l\\le(L+L^{\\prime})$ do 12: if F thenirst Residual Block 13: Get the output of the block: $U_{0}=f_{R B_{I}}\\left(\\mathbf{H}_{a l l}\\right)$ 14: else 15: Get the output of the block: $U_{R B_{l}}=f_{R B_{l}}\\left(U_{R B_{l}-\\,l}\\right)$ 16: end if 17: end for 18: if C thenomplete observation of all nodes 19: Get the embeddings from the last block in encoder: $U_{A l l}=U_{R B_{L}}$ 20: else 21: Aggregate all embeddings from the blocks in encoder: $\\begin{array}{r}{U_{A l l}=\\sum_{l=1}^{L}U_{R B_{l}}}\\end{array}$ 22: end if 23: Project back to input dimension $\\hat{V}^{t+1}=f_{p r o j}(U_{R B_{(}L+L^{\\prime})})$ 24: for $\\xi$ steps do 25: Sample the stop action probability: $a\\sim P_{\\Omega}(\\mathrm{stop}|G_{k})$ 26: if $a$ is the \u2018stop\u2019 action then 27: Reset the State trajectory: $G_{k+1}=G_{0}$ 28: else 29: Sample $G_{k+1}\\sim P_{\\Omega}(G_{k+1}|G_{k},\\mathrm{-stop})$ 30: Store the transition Gk \u2192Gk+1 31: end if 32: end for 33: Sample $\\lambda\\sim P_{\\Omega}(\\lambda|G,\\mathrm{stop})$ 34: Sample $\\lambda^{\\prime}\\sim P_{\\Omega}(\\dot{\\lambda}^{\\prime}|G^{\\prime},\\mathrm{stop})$ 35: Evaluate the rewards $R(\\left\\langle{G,\\lambda}\\right\\rangle)$ and $R(\\left\\langle{G^{\\prime},\\lambda^{\\prime}}\\right\\rangle)$ 36: Evaluate the loss $\\mathcal{L}=\\mathcal{L}_{R B}+\\mathcal{L}_{G F N}$ 37: Update the parameters of the branch of Residual Blocks and GFN 38: until Convergence criterion 39: Sample the approximation of posteriors: $\\begin{array}{r}{P_{\\Omega}(i\\rightarrow j|U_{A l l})\\approx\\frac{1}{B}\\sum_{b=1}^{B}{\\bf1}(i\\rightarrow j\\in{\\bf A d j}_{b})}\\end{array}$ 40: Output the sampled as the structure of the investigated dynamical system ", "page_idx": 23}, {"type": "table", "img_path": "xQWJBeK5rh/tmp/3b7d6279a50f5eca7ff5ced29c0ddc5464e6b8d92bbd99bba5534005328e1fa1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "E More Experimental Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.1 Supplementary Experimental Results on Other Metrics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "E.2 Experimental Results with Prior Knowledge ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The experimental analysis focuses on the impact of integrating varying percentages of prior knowledge ${}\\!\\!\\!\\{0\\%}$ , $10\\%$ , $20\\%$ , and $30\\%$ ) into the training process of different structural inference models, including NRI, MPM, ACD, iSIDG, RCSI, JSP-GFN, and our proposed SICSM. The results, as depicted in the ", "page_idx": 23}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/3b302083b4d078a87c034abe90ad11d38987c8908f31bc4c6b941aa2411c562a.jpg", "img_caption": ["Figure 7: AUPRC values (expressed in percentage) for various methods as a function of the number of irregularly sampled time steps. Results are averaged across ten trials, with time steps varying from 49 to 10. The shadings show the standard deviation of each data point. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/b3fae66ba2c5c4a788cfa057c126d027f7179703ddd0144edb605f2eec6fa393.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 8: SHD values (expressed in percentage) for various methods as a function of the number of irregularly sampled time steps. Results are averaged across ten trials, with time steps varying from 49 to 10. The shadings show the standard deviation of each data point. ", "page_idx": 24}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/0301d24a5118f2610800ad207ad52c4c27107556a16a830cd2c06b98d36f25b6.jpg", "img_caption": ["Figure 9: F1 scores (expressed in percentage) for various methods as a function of the number of irregularly sampled time steps. Results are averaged across ten trials, with time steps varying from 49 to 10. The shadings show the standard deviation of each data point. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/fffcbd30046973638e78acf0fd6ef6dc0b1f9d43b0bdfaf2ac34bedcbbccd631.jpg", "img_caption": ["Figure 10: Average AUROC results (in $\\%$ ) of SICSM and baselines with different percentages of prior knowledge on VN datasets. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "provided line plots, are evaluated across four datasets: VN_SP_50, VN_SP_100, VN_NS_50, and VN_NS_100. As for SIDEC, we did not figure out a feasible way of integrating prior knowledge into it. The results are shown in Figure 10, and we can see that SICSM consistently outperforms other baseline models across all datasets and percentages of prior knowledge integrated. Notably, SICSM shows a significant improvement in AUROC as the percentage of prior knowledge increases, underscoring its capability to effectively utilize additional information to enhance structural inference accuracy. Models like NRI, MPM, and ACD show moderate improvements with increased prior knowledge but remain less effective compared to SICSM. This suggests that while these models benefit from prior knowledge, their overall adaptability and learning mechanisms might not fully capitalize on the information provided. JSP-GFN and RCSI display variable trends; for instance, JSP-GFN shows notable improvements in the VN_SP_50 and VN_SP_100 datasets but less so in VN_NS datasets, indicating potential dataset-specific sensitivities. ", "page_idx": 25}, {"type": "text", "text": "The enhancement in performance with increased prior knowledge is most pronounced in the VN_SP_100 and VN_SP_50 datasets for SICSM. This pattern illustrates the model\u2019s robustness in leveraging prior knowledge, particularly in scenarios with larger and possibly more complex network structures. In contrast, the increments in AUROC scores for baselines like iSIDG and RCSI are less steep, suggesting these models, while beneftiing from prior knowledge, do not adapt as effectively as SICSM. ", "page_idx": 25}, {"type": "text", "text": "To conclude, the integration of prior knowledge markedly beneftis the performance of structural inference models, with our SICSM model demonstrating superior capability to utilize such information to enhance prediction accuracy. These results validate the effectiveness of SICSM\u2019s design in adapting to additional contextual information, setting a benchmark for future developments in the field. Further investigations could explore optimizing the integration process of prior knowledge to maximize the performance benefits across diverse structural inference scenarios. ", "page_idx": 25}, {"type": "text", "text": "E.3 Experimental Results on PEMS ", "text_level": 1, "page_idx": 25}, {"type": "table", "img_path": "xQWJBeK5rh/tmp/c8b74b1588c7cc2cbe336aa05325dabe8a1ff6f29f21fcdc1959ff587dcb8e92.jpg", "table_caption": ["Table 3: Average AUROC results $(\\%)$ on PEMS datasets. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "This section presents the performance evaluation of two baseline methods, JSP-GFN and SIDEC, alongside our proposed SICSM on three real-world datasets: PEMS03, PEMS04, and PEMS07. These datasets are instrumental in assessing the robustness and effectiveness of structural inference methods in real-world scenarios. The results are summarized in Table 3. Other baselines fail to work on large graphs and encountered OOM errors on these datasets. ", "page_idx": 25}, {"type": "text", "text": "As we can see from the table, SICSM consistently exhibits the highest AUROC across all three datasets, with scores of $71.2\\%$ on PEMS03, $74.7\\%$ on PEMS04, and $71.2\\%$ on PEMS07. These results underscore SICSM\u2019s superior performance in capturing and predicting complex network dynamics in traffic systems, which is a real-world scenario. SIDEC also performs robustly, especially on the PEMS04 dataset where it achieves an AUROC of $73.5\\%$ . However, it slightly trails behind SICSM, particularly on the PEMS03 and PEMS07 datasets. JSP-GFN shows the lowest performance among the evaluated methods, with its highest AUROC at $61.2\\%$ on PEMS07, indicating a lesser adaptability to the dynamics of these specific traffic datasets. ", "page_idx": 25}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/26b4c76d8f1e87dfdac6744a897fcd573b8fd8e7c96f2e443f7674a17085a320.jpg", "img_caption": ["Figure 11: (1st Column) Ground truth structure with all nodes. (2nd Column) Two examples of 12-node sampling. (3rd, 4th Columns) Structural inference results from SICSM-one and SICSM. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "The standard deviations reported alongside the AUROC scores indicate the stability of each method\u2019s performance across different runs. SICSM demonstrates moderate stability with a standard deviation of approximately $0.44\\%$ to $0.47\\%$ , suggesting consistent performance despite the inherent variability in real-world data. SIDEC shows the highest stability, particularly on PEMS04, with a minimal standard deviation of $0.18\\%$ . This suggests that SIDEC is reliably effective in scenarios represented by this dataset. JSP-GFN, while the least effective in terms of AUROC, maintains a relatively consistent performance as indicated by its standard deviations, which range from $0.63\\%$ to $1.01\\%$ . ", "page_idx": 26}, {"type": "text", "text": "The evaluation on the PEMS datasets validates the effectiveness of SICSM, particularly in comparison to established baseline methods like JSP-GFN and SIDEC. SICSM\u2019s ability to consistently outperform other methods underlines its advanced structural inference capabilities, making it a promising solution for complex real-world applications in system dynamics and network analysis. ", "page_idx": 26}, {"type": "text", "text": "E.4 Why Do We Need All Residual Outputs? ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "As discussed in Section 3.2, in systems with partial observation, it is advantageous to combine learned dynamics from multiple Residual Blocks to enrich the dynamics available for the GFN. However, an intriguing question arises: What is the impact when only the dynamics from the final Residual Block in the encoder are used, similar to approaches used in fully observed systems? This section delves into this query through a detailed case study on a specific dataset. We selected the VN_SP_15 dataset for this examination, focusing on a scenario where 12 nodes $80\\%$ of the total) are sampled. We contrasted two configurations of our proposed structural inference method: the comprehensive SICSM, which integrates outputs from all Residual Blocks in the encoder, and a simplified version, SICSM-one, which relies solely on the output from the last Residual Block in the encoder. This comparison aimed to assess the impact of multi-layer output integration on the accuracy and robustness of the inferred network structures, with results illustrated in Figure 11. ", "page_idx": 26}, {"type": "text", "text": "Both configurations were evaluated on the same dataset, comprising a full graph and a 12-node sampled version, to appraise their performance across varying degrees of system completeness. Observations from the results indicated that SICSM-one frequently misinterpreted two-hop interactions as three-hop connections, leading to an increased incidence of false positives. This issue arises because SICSM-one relies solely on the output from the final, potentially larger, Residual Block, which can blur the distinctions between one-hop, two-hop, and three-hop dynamics. In contrast, SICSM leverages outputs from multiple layers, enabling a comprehensive representation of dynamics across ", "page_idx": 26}, {"type": "text", "text": "Table 4: Average counts of multi-hop negative edges and true positive edges reconstructed upon VN_SP_15 dataset with 12 nodes are sampled with different residual blocks. The average is performed based on 10 runs. For reference, $L=7$ . Each residual block is numbered as their closeness to the input side. For example, Residual Block [1] is the first one, $[1-3]$ refers to the integrating outputs from 1 to 3 Residual Blocks. ", "page_idx": 27}, {"type": "table", "img_path": "xQWJBeK5rh/tmp/6c9673d20c521554f42313812e51a8f2b830964a88cd8c183f8353165c0ad4d4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "different temporal dependencies. This multi-layer aggregation is particularly effective at emphasizing shorter connections while still accounting for longer pathways. The integration of shallow blocks plays a crucial role in this configuration, offering detailed insights into shorter dependencies and significantly reducing the likelihood of misidentifying longer connection paths as false positives. Despite these improvements, some inaccuracies remain, suggesting areas for further refinement. Future developments might include implementing an adaptive weighting mechanism that adjusts the influence of dynamics from different Residual Blocks. Such a mechanism would be tailored according to the size of the graph and the longest potential paths within the network, optimizing the model\u2019s accuracy in diverse operating conditions. ", "page_idx": 27}, {"type": "text", "text": "Moreover, as shown in Table 4, the occurrence of negative multi-hop edges is notably higher when only a single Residual Block is used. This number decreases to 6.5 when only the first block is used, but at the cost of reducing true positive predictions. The best configuration, as highlighted in the table, is the concatenation of outputs from all blocks. This approach not only reduces the occurrence of negative multi-hop edges but also increases the count of true positives, providing a more balanced and accurate representation of the underlying structure. ", "page_idx": 27}, {"type": "text", "text": "These findings underscore the effectiveness of multi-layer dynamic integration in SICSM, particularly in settings with partial node observability. They highlight the model\u2019s capacity to maintain structural integrity and provide accurate predictions, affirming its potential for broad application in complex, dynamically varying systems. ", "page_idx": 27}, {"type": "text", "text": "E.5 Ablation Study on the Choice of Neural Networks in the Blocks ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We conducted additional experiments comparing Transformer [50], LSTM [25], and GRU [15] models on irregularly sampled trajectories with 30 time steps and partial observations with 12 nodes in the VN_SP_15 dataset. The average results from 10 runs are presented in Table 5. For all models, we adjusted the parameters to accommodate the trajectory lengths and performed hyperparameter tuning using Bayesian optimization. As shown in the table, the Transformer model outperforms LSTM and GRU by a small margin, but all are notably inferior to SSSM, as they struggle to effectively handle multi-hop interactions. Additionally, these models perform poorly on irregularly sampled trajectories, as they lack the ability to learn adaptively. ", "page_idx": 27}, {"type": "text", "text": "E.6 Training Time Comparison ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The training time analysis for SICSM and baseline methods on the VN_NS datasets, as summarized in Table 6, provides valuable insights into the computational efficiency of these models. The reported times are averaged over ten runs and are presented in hours. ", "page_idx": 27}, {"type": "text", "text": "Table 5: Average AUROC results of SICSM with different neural networks in each block. The networks under consideration are Transformer, LSTM and GRU. The experiments are irregularly sampled time steps and partially observed nodes on VN_SP_15 dataset. ", "page_idx": 28}, {"type": "table", "img_path": "xQWJBeK5rh/tmp/b465709a029d1db452a73f7bcd2eb5661ef2e3852edb1651c422a68143195d65.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "table", "img_path": "xQWJBeK5rh/tmp/bf539f39466df085d44d00358eff59567e0e32f3a99cef31192fbc51e21da61a.jpg", "table_caption": ["Table 6: Training time (hours) of SICSM and baseline methods on VN_NS datasets. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "From the table, it\u2019s evident that SICSM, while providing advanced capabilities in structural inference as demonstrated in previous sections, exhibits longer training times compared to both traditional and other state-of-the-art baseline methods. SICSM consistently shows higher training times across all dataset sizes compared to other methods. For instance, at VN_NS_15, SICSM takes approximately 59.2 hours, which is about 35 hours longer than NRI and nearly 33 hours more than SIDEC, the method with the shortest training time for this dataset size. As the size of the dataset increases, the training time for SICSM also increases substantially, from 59.2 hours for VN_NS_15 to 120.5 hours for VN_NS_100. This scaling trend is consistent with other methods but more pronounced in SICSM, suggesting that its complexity scales significantly with larger networks. The reason is that we implement selective SSM with JAX, which lacks the cuda package to boost the selective process that is designed in [20], and the search over all possible state spaces in GFN is time-consuming. The data highlights a crucial area for future development in optimizing the computational efficiency of SICSM. Enhancements might focus on the implementation of JAX-suited cuda package for boosting selective SSM or integrating more efficient learning algorithms to reduce training times without compromising the model\u2019s performance. ", "page_idx": 28}, {"type": "text", "text": "E.7 How Good is the Approximation? ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this work, we approximating $P(A d j,\\lambda|U_{\\mathrm{all}})$ instead of the $A d j$ . Thus, it is necessary to evaluate how well $P(A d j,\\lambda|U_{\\mathrm{all}})$ is approximated with a distributional metric. ", "page_idx": 28}, {"type": "text", "text": "Similar to the experiments in JSP-GFN [18], we consider here models over $\\mathtt{d}=5$ variables, with linear Gaussian CPDs. We generate 20 different datasets of $\\Nu=100$ observations from randomly generated Bayesian Networks. The quality of the joint posterior approximations is evaluated separately for $A d j$ and $\\lambda$ . For $A d j$ , we compare the approximation and the exact posterior on different marginals of interest, also called features in JSP-GFN [18], e.g., the edge feature corresponds to the marginal probability of a specific edge being in the graph. Fig. 12 shows a comparison between the edge features computed with the exact posterior and with SICSM, proving that it can accurately approximate the edge features of the exact posterior. To evaluate the performance of the different methods as an approximation of the posterior over $\\lambda$ , we also estimate the cross-entropy between the sampling distribution of $\\lambda$ given $\\mathrm{G}$ and the exact posterior $P(\\lambda|A d j,U_{a l l})$ . The results are shown in Table 7. We observe that again SICSM samples parameters $\\lambda$ that are significantly more probable under the exact posterior compared to other methods. ", "page_idx": 28}, {"type": "image", "img_path": "xQWJBeK5rh/tmp/548642486d3a418c01c50f2603a9c23125b08bd4e0fb1ab00806ae5813a3073e.jpg", "img_caption": ["Figure 12: Comparison of the edge features computed with the exact posterior ( $\\bf\\Tilde{x}$ -axis) and the approximation given by GFN in SICSM. "], "img_footnote": [], "page_idx": 29}, {"type": "table", "img_path": "xQWJBeK5rh/tmp/3832bf6391655992c26366fc1cdc28addf9d6e43547003630cd090bb5e9eda0d.jpg", "table_caption": ["Table 7: Comparison with the exact posterior distribution, on small graphs with $n\\,=\\,5$ nodes. Quantitative evaluation of different methods for joint posterior approximation, both in terms of edge features and cross-entropy of sampling distribution and true posterior $P(\\lambda|A d j,U_{a l l})$ . All values correspond to the mean and $95\\%$ confidence interval across the 10 experiments. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "F Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "While SICSM marks a significant step forward in structural inference, it is imperative to acknowledge its potential limitations for a comprehensive understanding and to guide future research: ", "page_idx": 29}, {"type": "text", "text": "\u2022 Reliance on Prior Knowledge Accuracy: SICSM\u2019s enhanced performance through prior knowledge integration is contingent on the accuracy of this information. Misleading or incorrect prior knowledge could adversely impact the model\u2019s inference accuracy, leading to potentially flawed conclusions. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Prior Knowledge of Edge Existence: Currently, SICSM leverages prior knowledge about the existence of edges in the graph. However, it is not equipped to incorporate prior knowledge about the non-existence of specific edges, limiting its ability to exclude certain connections during the inference process. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Scalability to Very Large Graphs: The scalability of SIGFN to graphs with an extremely large number of nodes remains untested (e.g., with more than 1,000 nodes). Training and inference in such large-scale graphs may demand significant computational resources and time, which could be a practical constraint. We acknowledged this limitation of SICSM, and currently working on the variant with sub-graph ensemble methods inspired by Cluster-GCN [14] and GraphSAINT [62]. Some methods from federated graph learning may also solve the challenge of scalability of structural inference [26]. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Evaluation on Synthetic Data: Due to the challenges in obtaining reliable real-world datasets for structural inference, SICSM has primarily been evaluated on synthetic data in this study. We recognize the potential discrepancies between synthetic and real-world data and plan to address this limitation in future research by exploring real-world applications. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Dynamic Graphs Handling: Currently, SIGFN is formulated for static graphs. However, many real-world graphs are dynamic, with structures that evolve over time. Adapting SICSM to accommodate such dynamic graphs is an essential area for future development. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Long Run Time: As detailed in Appendix E.6, SICSM exhibits the longest running time among all evaluated methods. This extended duration primarily results from the selective SSM in JAX lacking optimized CUDA integration, which is critical for enhancing computational efficiency. Additionally, the time-intensive process of constructing all possible state spaces within the GFN significantly contributes to the overall duration. ", "page_idx": 30}, {"type": "text", "text": "Future enhancements to SICSM could involve strategies for validating and correcting prior knowledge, improving scalability and efficiency for handling larger graphs, extending the model\u2019s capabilities to dynamic graphs, and implementing efficient selection SSM with JAX as well as boosting the speed of GFN. These advancements will be vital in ensuring SICSM\u2019s applicability and reliability across various practical scenarios. ", "page_idx": 30}, {"type": "text", "text": "G Broader Impact ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Much like NRI, MPM, ACD, iSIDG, RCSI, SIDEC, and other structural inference methodologies, SICSM extends its utility to a diverse range of researchers across the realms of physics, chemistry, sociology, and biology, where the uncovering of underlying interaction graph structure is becoming more and more popular. In our investigations, we have demonstrated SISICSM\u2019s proficiency in reconstructing graph structures and display robustness to variations in the irregular samplings and incomplete observations, underscoring its versatility and broad applicability. There may be potential societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The abstract and introduction clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The discussion of limitations can be found in Appendix F. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All the formulas in the paper are numbered and cross-referenced. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We provide the link to our implementation in the supplementary document. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: While this paper does not include new datasets, we provide links to our implementation and the references to the data. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: See Appendix D.8. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: The plots in the main content come with shading showing the standard deviations at each data point. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: See Section 5.1. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: As we discussed in Appendix G, we did not recognize any direct path to negative applications or negative social impact. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 35}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]