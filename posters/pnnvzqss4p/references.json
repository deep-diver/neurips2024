{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, a fundamental building block of many modern LLMs, including those studied in this paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduced the LLaMA family of LLMs, several of which are used as the primary models for experimentation in this paper."}, {"fullname_first_author": "Coleman Hooper", "paper_title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization", "publication_date": "2024-01-18", "reason": "This paper introduced KVQuant, a key baseline method used for comparison in this paper's experimental evaluation of KV cache compression techniques."}, {"fullname_first_author": "Zirui Liu", "paper_title": "Kivi: A tuning-free asymmetric 2bit quantization for kv cache", "publication_date": "2024-02-02", "reason": "This paper introduced KIVI, another key baseline method for comparison in this paper's experimental evaluation of KV cache compression techniques."}, {"fullname_first_author": "Tianyi Zhang", "paper_title": "Nomad-attention: Efficient llm inference on cpus through multiply-add-free attention", "publication_date": "2024-03-01", "reason": "This paper introduced Nomad-attention, which is relevant to this paper's focus on efficient LLM inference techniques, although it focuses on CPU inference rather than GPU memory optimization."}]}