{"references": [{"fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023-00-00", "reason": "This paper provides the benchmark models used for comparison in the current study, establishing a crucial baseline for evaluating the performance of Mamba models."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This foundational work establishes the concept and importance of in-context learning (ICL) which is a central focus of this paper's downstream capabilities evaluation of Mamba models."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-00", "reason": "This paper introduces the Mamba model architecture, the primary subject of the current research."}, {"fullname_first_author": "Tri Dao", "paper_title": "Hungry hungry hippos: Towards language modeling with state space models", "publication_date": "2023-00-00", "reason": "This paper explores the potential of state-space models for language modeling, providing essential context for understanding Mamba's approach and its relation to existing techniques."}, {"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-00", "reason": "This paper introduces the LoRA technique, a crucial parameter-efficient fine-tuning method used in this paper's experiments to enhance Mamba's downstream learning performance."}]}