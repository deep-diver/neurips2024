[{"figure_path": "vjCFnYTg67/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of Bileve. (a) Embedding: The first m tokens from M form the message, which is signed using a secret key. Candidate tokens are selected via a rank-based strategy employing a Weighted Rank Addition (WRA) score, with a coarse-grained signal embedded. It then embeds the fine-grained signature by choosing the first candidate matching the designated signature bit. (b) Detection: We first extract the message-signature pair to conduct an integrity check using the public key. A statistical test is performed if necessary.", "description": "This figure illustrates the Bileve watermarking scheme.  Panel (a) shows the embedding process, where the first 'm' tokens of a message are hashed, signed with a secret key, and the resulting signature bits are embedded into subsequent tokens using a rank-based selection strategy guided by a weighted rank addition (WRA) score. A coarse-grained signal is also embedded. Panel (b) depicts the detection process, starting with an integrity check using a public key and then employing a statistical test if necessary to determine if a watermark exists.", "section": "4 Proposed Defense"}, {"figure_path": "vjCFnYTg67/figures/figures_7_1.jpg", "caption": "Figure 2: The alignment cost of human vs LLM.", "description": "This histogram visualizes the distribution of alignment costs for both human-generated and LLM-generated texts.  The alignment cost measures how well a generated text aligns with a specific key sequence. Lower alignment costs indicate a higher likelihood that the text was generated by the target LLM. The figure demonstrates the distinct separation between human-written text and text generated by the LLM, highlighting the effectiveness of the proposed approach in differentiating between the two.", "section": "5.2 Detectability"}, {"figure_path": "vjCFnYTg67/figures/figures_7_2.jpg", "caption": "Figure 8: The perplexity of applying different schemes to LLaMA-7B.", "description": "This figure shows a box plot comparing the perplexity scores achieved by human-written text, text generated using the Unigram method, text generated using the SLS method, and text generated using the Bileve method.  The perplexity is a measure of how well the generated text matches the distribution of human language, with lower scores indicating better quality. The results are shown for two datasets: LFQA and OpenGen.  The figure highlights the trade-off between watermark robustness and generation quality, with Bileve aiming to balance both aspects.", "section": "5.3 Generation Quality"}, {"figure_path": "vjCFnYTg67/figures/figures_8_1.jpg", "caption": "Figure 4: The p-value and alignment cost of each segment.", "description": "This figure visualizes the results of a signature preservation attack. It consists of three subfigures. The leftmost subfigure is a bar chart showing the p-value for each of the five segments of the text after the attack. The middle subfigure is a heatmap showing the alignment cost for each segment and shift before the attack. The rightmost subfigure is a heatmap showing the alignment cost for each segment and shift after the attack. The red box highlights the segment where the attack was performed. The figure demonstrates that the signature preservation attack successfully alters the text's integrity without significantly affecting the overall alignment cost, highlighting a vulnerability that needs to be addressed.", "section": "5.4 Security"}, {"figure_path": "vjCFnYTg67/figures/figures_13_1.jpg", "caption": "Figure 1: Overview of Bileve. (a) Embedding: The first m tokens from M form the message, which is signed using a secret key. Candidate tokens are selected via a rank-based strategy employing a Weighted Rank Addition (WRA) score, with a coarse-grained signal embedded. It then embeds the fine-grained signature by choosing the first candidate matching the designated signature bit. (b) Detection: We first extract the message-signature pair to conduct an integrity check using the public key. A statistical test is performed if necessary.", "description": "This figure illustrates the Bileve watermarking scheme.  Panel (a) shows the embedding process:  a message (the first *m* tokens) is created and signed using a secret key. A rank-based sampling strategy, using a weighted rank addition score, selects candidate tokens. The fine-grained signature bits are embedded by selecting a token whose hash matches the signature bit. Panel (b) details the detection process: the message and signature are extracted and verified using the public key. If the signature is invalid, a statistical test is performed to check for the watermark's presence. ", "section": "4 Proposed Defense"}, {"figure_path": "vjCFnYTg67/figures/figures_14_1.jpg", "caption": "Figure 3: The perplexity of applying different schemes to OPT-1.3B", "description": "This figure shows the perplexity scores achieved by different watermarking schemes (Human, Unigram, SLS, and Bileve) on two datasets: LFQA and OpenGen.  Perplexity is a measure of how well a language model predicts a sequence of words; lower perplexity suggests the generated text is closer to human-written text. The boxplots visually represent the distribution of perplexity scores, showing the median, quartiles, and outliers for each method on each dataset. This allows for a comparison of the impact of each watermarking scheme on the generation quality.", "section": "5.3 Generation Quality"}]