[{"figure_path": "08GbdALmEs/tables/tables_4_1.jpg", "caption": "Table 1: Skill prompting results. We report the zero-shot performance of models pretrained with different masking schemes. Results are averaged over 10 random seeds. The best and second results are bold and underlined, respectively.", "description": "This table presents the zero-shot performance results of various models trained using different masking schemes on skill prompting tasks.  The results are averaged across 10 independent runs (random seeds) for each model and masking scheme.  The best performing and second-best performing models for each task are highlighted.", "section": "5.1 Environment Setup"}, {"figure_path": "08GbdALmEs/tables/tables_6_1.jpg", "caption": "Table 1: Skill prompting results. We report the zero-shot performance of models pretrained with different masking schemes. Results are averaged over 10 random seeds. The best and second results are bold and underlined, respectively.", "description": "This table presents the zero-shot performance results for skill prompting across various tasks using different model training methods.  The performance metric is reward, and results are averages over ten random seeds.  The best-performing and second-best-performing methods for each task are highlighted.", "section": "5.1 Environment Setup"}, {"figure_path": "08GbdALmEs/tables/tables_6_2.jpg", "caption": "Table 2: Goal-conditioned planning results. We report the zero-shot performance of models pretrained with different masking schemes. Results are averaged over 20 random seeds. The best and second results are bold and underlined, respectively. The lower the better.", "description": "This table presents the results of goal-conditioned planning experiments.  The zero-shot performance of different models pretrained with various masking schemes is shown. The results are averaged over 20 trials to ensure statistical reliability, with the best and second-best results highlighted.  Lower values indicate better performance, as the metric is the average L2 distance between the achieved state and the target goal state.", "section": "5.3 Main Results"}, {"figure_path": "08GbdALmEs/tables/tables_13_1.jpg", "caption": "Table 1: Skill prompting results. We report the zero-shot performance of models pretrained with different masking schemes. Results are averaged over 10 random seeds. The best and second results are bold and underlined, respectively.", "description": "This table presents the zero-shot performance of various models on skill prompting tasks.  Models were pre-trained using different masking schemes (MaskDP, MTM, Mixed-inv, Mixed-prog, Mixed, GPT, CurrMask). The results, averaged over 10 random seeds, show the performance on several locomotion and robotic arm manipulation tasks. The best and second-best performing models for each task are highlighted.", "section": "5.1 Environment Setup"}, {"figure_path": "08GbdALmEs/tables/tables_13_2.jpg", "caption": "Table 3: Episodic return statistics of training datasets used for offline RL.", "description": "This table presents the minimum, maximum, and mean episodic returns observed during the training of offline reinforcement learning (RL) datasets for three locomotion tasks: stand, walk, and run.  These statistics summarize the performance of the agents used to generate the offline RL data, which is then used to train the model in the experiments. The variation in return values indicates the diversity and quality of the experiences within the training datasets.", "section": "B Experimental Details"}, {"figure_path": "08GbdALmEs/tables/tables_14_1.jpg", "caption": "Table 4: Hyperparameters used for model training and evaluation.", "description": "This table lists the hyperparameters used for training and evaluating the models in the paper.  It includes parameters related to the model architecture (number of layers, attention heads, hidden dimension), training process (optimizer, batch size, learning rate, number of gradient steps), and experimental settings for skill prompting, goal-conditioned planning, and offline RL.  The hyperparameters for the EXP3 algorithm, used for curriculum learning, are also specified.", "section": "5.1 Environment Setup"}, {"figure_path": "08GbdALmEs/tables/tables_16_1.jpg", "caption": "Table 5: Reward results. We report the performance across Token-wise masking and Block-wise masking with different block sizes. The best and second-best results are bold and underlined, respectively.", "description": "This table presents the average reward obtained across different tasks (walker_s, walker_w, walker_r, quad_w, quad_r, jaco_bl, jaco_br, jaco_tl, jaco_tr) using various masking schemes.  The \"Token\" row shows results using token-wise masking, while the other rows show results for block-wise masking with different block sizes (5, 10, 15, 20).  Higher reward values indicate better performance. The best and second-best results for each task are highlighted in bold and underlined, respectively. This allows for a comparison of the effectiveness of different masking strategies and block sizes on overall task performance.", "section": "5.3 Main Results"}]