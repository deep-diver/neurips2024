[{"figure_path": "08GbdALmEs/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of CurrMask. Based on the framework of masked prediction modeling in (a), the block-wise masking scheme in (b), we design a masking pool M in (c) consisting of masking schemes at different levels of complexity, which CurrMask evaluates the learning progress of the model \u03b8 and samples masking schemes from during pretraining in (d).", "description": "This figure illustrates the CurrMask framework.  Panel (a) shows the general masked sequence modeling framework using a bidirectional transformer.  (b) shows two masking strategies: random masking and block-wise masking.  (c) shows the concept of a masking pool containing various masking schemes with different complexity levels, represented as a set of different block sizes and mask ratios. (d) illustrates how CurrMask works during pretraining, where it evaluates the model's performance with different masking schemes and adjusts its sampling strategy accordingly to create a curriculum.", "section": "4 Curriculum Masked Prediction"}, {"figure_path": "08GbdALmEs/figures/figures_7_1.jpg", "caption": "Figure 2: Offline RL results. We report the finetuning performance of models pretrained with different masking schemes. Results are averaged over 10 random seeds.", "description": "This figure presents the results of offline reinforcement learning (RL) experiments.  Models were pretrained using different masking schemes (detailed in the paper), then fine-tuned on offline RL tasks.  The plots show the cumulative reward achieved over training steps for three different locomotion tasks: run, stand, and walk. Error bars represent the standard deviation across 10 runs.  The figure compares the performance of CurrMask (the proposed method) to several baselines, demonstrating its effectiveness in improving offline RL performance.", "section": "5.3 Main Results"}, {"figure_path": "08GbdALmEs/figures/figures_7_2.jpg", "caption": "Figure 2: Offline RL results. We report the finetuning performance of models pretrained with different masking schemes. Results are averaged over 10 random seeds.", "description": "This figure shows the cumulative reward achieved by different models during the finetuning phase of offline reinforcement learning.  The models were initially pretrained using various masking schemes.  The x-axis represents the training steps, and the y-axis shows the cumulative reward. The figure compares the performance of CurrMask against several baselines, demonstrating the superior performance of CurrMask in terms of both speed and final reward.", "section": "5.3 Main Results"}, {"figure_path": "08GbdALmEs/figures/figures_7_3.jpg", "caption": "Figure 3: Both block-wise masking and curriculum masking contribute to CurrMask's performance. Left: the performance of zero-shot skill prompting as a function of fixed block size. Right: the probabilities of choosing different block sizes and mask ratios during pretraining with CurrMask.", "description": "This figure shows the impact of block-wise masking and curriculum masking on the zero-shot skill prompting performance of the CurrMask model. The left panel shows that using larger block sizes for masking generally improves performance, indicating that block-wise masking helps to capture longer-term dependencies. The right panel shows the probabilities of selecting different block sizes and mask ratios during the curriculum learning process.  The probabilities change dynamically during training, reflecting the model's adaptation to different levels of complexity. This adaptive curriculum learning strategy is crucial for CurrMask's success.", "section": "5.4 Analysis"}, {"figure_path": "08GbdALmEs/figures/figures_8_1.jpg", "caption": "Figure 4: Analysis of long-term prediction capability. Left: We visualize the attention map (L2-normalized over different heads) of the first decoder layer, when the model is conducting skill prompting. The horizontal axis represents the keys, and the vertical axis represents the queries. Right: the performance of zero-shot skill prompting as a function of rollout length (tasks: walker run and jaco reach top left).", "description": "The figure demonstrates the attention maps and the performance of zero-shot skill prompting at different rollout lengths. The left panel shows the attention maps of the first decoder layer during skill prompting, highlighting the model's focus on different parts of the input sequence. The right panel shows how the performance of zero-shot skill prompting changes as the rollout length increases, comparing the performance of CurrMask with other baselines.", "section": "5.4 Analysis"}, {"figure_path": "08GbdALmEs/figures/figures_8_2.jpg", "caption": "Figure 2: Offline RL results. We report the finetuning performance of models pretrained with different masking schemes. Results are averaged over 10 random seeds.", "description": "This figure shows the cumulative reward achieved by different models during the finetuning phase of offline reinforcement learning.  The models were pretrained using various masking schemes (MaskDP, MTM, Mixed-inv, Mixed-prog, Mixed, GPT, and CurrMask). The x-axis represents the number of steps in the training process for three different tasks (run, stand, and walk) in the walker environment. The y-axis represents the cumulative reward obtained. The figure demonstrates the effectiveness of CurrMask in improving the performance of offline RL agents compared to other methods.", "section": "5.3 Main Results"}, {"figure_path": "08GbdALmEs/figures/figures_15_1.jpg", "caption": "Figure 5: Skill prompting performance on Walker run in the pretraining phase.", "description": "This figure shows the cumulative reward achieved by different methods for skill prompting on the Walker run task during the pretraining phase.  The x-axis represents the number of training steps, and the y-axis represents the cumulative reward. The area chart displays the performance broken down into segments of 30 steps (0-30, 30-60, 60-90, 90-120). The figure compares three different approaches: Random masking, the Mixed-prog curriculum, and CurrMask (the proposed method).  It visually demonstrates that CurrMask learns skills more efficiently and adapts its masking scheme dynamically, leading to higher cumulative rewards compared to the other methods.", "section": "5.4 Analysis"}, {"figure_path": "08GbdALmEs/figures/figures_17_1.jpg", "caption": "Figure 2: Offline RL results. We report the finetuning performance of models pretrained with different masking schemes. Results are averaged over 10 random seeds.", "description": "This figure displays the cumulative reward achieved during the finetuning phase of offline reinforcement learning for various models pretrained using different masking schemes.  The x-axis represents the number of training steps, and the y-axis represents the cumulative reward.  The plot shows that models pretrained with CurrMask generally outperform other methods, indicating that CurrMask's curriculum masking strategy leads to more effective skill learning and faster adaptation to new tasks.", "section": "5.3 Main Results"}, {"figure_path": "08GbdALmEs/figures/figures_18_1.jpg", "caption": "Figure 2: Offline RL results. We report the finetuning performance of models pretrained with different masking schemes. Results are averaged over 10 random seeds.", "description": "This figure shows the results of applying different pre-trained models (with different masking schemes) to offline reinforcement learning tasks.  The models were first pre-trained using masked prediction, then fine-tuned for specific offline RL tasks.  The graph likely displays metrics such as cumulative reward over training steps for each model. This allows for comparison of the models' performance after fine-tuning and demonstrates the effectiveness of CurrMask's pre-training strategy on downstream tasks.", "section": "5.3 Main Results"}]