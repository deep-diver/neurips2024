[{"type": "text", "text": "Transfer Learning for Latent Variable Network Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Akhil Jalan Department of Computer Science UT Austin akhiljalan@utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Arya Mazumdar Hal\u0131c\u0131og\u02d8lu Data Science Institute & Dept of CSE UC San Diego arya@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Soumendu Sundar Mukherjee Statistics and Mathematics Unit (SMU) Indian Statistical Institute, Kolkata ssmukherjee@isical.ac.in ", "page_idx": 0}, {"type": "text", "text": "Purnamrita Sarkar Department of Statistics and Data Sciences UT Austin purna.sarkar@austin.utexas.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study transfer learning for estimation in latent variable network models. In our setting, the conditional edge probability matrices given the latent variables are represented by $P$ for the source and $Q$ for the target. We wish to estimate $Q$ given two kinds of data: (1) edge data from a subgraph induced by an $o(1)$ fraction of the nodes of $Q$ , and (2) edge data from all of $P$ . If the source $P$ has no relation to the target $Q$ , the estimation error must be $\\Omega(1)$ . However, we show that if the latent variables are shared, then vanishing error is possible. We give an efficient algorithm that utilizes the ordering of a suitably defined graph distance. Our algorithm achieves $o(1)$ error and does not assume a parametric form on the source or target networks. Next, for the specific case of Stochastic Block Models we prove a minimax lower bound and show that a simple algorithm achieves this rate. Finally, we empirically demonstrate our algorithm\u2019s use on real-world and simulated network estimation problems. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Within machine learning and statistics, the paradigm of transfer learning describes a setup where data from a source distribution $P$ is exploited to improve estimation of a target distribution $Q$ for which a small amount of data is available. Transfer learning is quite well-studied in learning theory, starting with works such as Ben-David et al. (2006); Cortes et al. (2008); Crammer et al. (2008), and at the same time has found applications in areas such as computer vision (Tzeng et al., 2017a) and speech recognition (Huang et al., 2013). A fairly large body of work in transfer learning considers different types of relations that may exist between $P$ and $Q$ , for example, Mansour et al. (2009); Hanneke and Kpotufe (2019, 2022), with emphasis on model selection, multitask learning and domain adaptation. On the other hand, optimal nonparametric rates for transfer learning have very recently been studied, both for regression and classification problems (Cai and Wei, 2021; Cai and Pu, 2024). ", "page_idx": 0}, {"type": "text", "text": "In this paper, we study transfer learning in the context of random network/graph models. In our setting, we observe Bernoulli samples from the full $n\\times n$ edge probability matrix for the source $P$ and only a $n_{Q}\\times n_{Q}$ submatrix of $Q$ for $n_{Q}\\ll n$ . We would like to estimate the full $n\\times n$ probability matrix $Q$ , using the full source data and limited target data, i.e., we are interested in the task of estimating $Q$ in the partially observed target network, utilizing information from the fully observed source network. This is a natural extension of the transfer learning problem in classification/regression to a network context. However, it is to be noted that network transfer is a genuinely different problem owing to the presence of edge correlations. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While transfer learning in graphs seems to be a fundamental enough problem to warrant attention by itself, we are also motivated by potential applications. For example, metabolic networks model the chemical interactions related to the release and utilization of energy within an organism (Christensen and Nielsen, 2000). Existing algorithms for metabolic network estimation (Sen et al., 2018; Baranwal et al., 2020) and biological network estimation more broadly (Fan et al., 2019; Li et al., 2022) typically assume that some edges are observed for every node in the target network. One exception is Kshirsagar et al. (2013), who leverage side information for host-pathogen protein interaction networks. For the case of metabolic networks, determining interactions in vivo1requires metabolite balancing and labeling experiments, so only the edges whose endpoints are both incident to the experimentally chosen metabolites are observed (Christensen and Nielsen, 2000). For a non-model organism, the experimentally tested metabolites may be a small fraction of all metabolites believed to affect metabolism. However, data for a larger set of metabolites might be available for a model organism. ", "page_idx": 1}, {"type": "text", "text": "To study transfer learning on networks, one needs to fix a general enough class of networks that is appropriate for the applications (such as the biological networks mentioned above) and also suitable to capture the transfer phenomenon. The latent variable models defined below appear to be a natural candidate for that. ", "page_idx": 1}, {"type": "text", "text": "Latent Variable Models. Latent variable network models consist of a large class of models whose edge probabilities are governed by the latent positions of nodes. This includes latent distance models, stochastic block models, random dot product graphs and mixed membership block models (Hoff et al., 2002; Hoff, 2007; Handcock et al., 2007; Holland et al., 1983; Rubin-Delanchy et al., 2022; Airoldi et al., 2008). They can also be unified under graph limits or graphons (Lov\u00e1sz, 2012; Bickel and Chen, 2009), which provide a natural representation of vertex exchangeable graphs (Aldous, 1981; Hoover, 1979). In addition to their theoretical breadth and usefulness, latent variable models are relevant and applicable to real-world settings such as neuroscience Ren et al. (2023), ecology Trifonova et al. (2015), international relations Cao and Ward (2014), political pscyhology Barber\u00e1 et al. (2015), and education research Sweet et al. (2013). ", "page_idx": 1}, {"type": "text", "text": "For unseen latent variables $\\mathbf{\\boldsymbol{x}}_{1},\\ldots,\\mathbf{\\boldsymbol{x}}_{n}\\in\\mathcal{X}\\subset\\mathbb{R}^{d}$ and unknown function $f_{Q}:\\mathcal{X}\\times\\mathcal{X}\\to[0,1]$ where $\\mathcal{X}$ is a compact set and $d$ an arbitrary fixed dimension, the edge probabilities are: ", "page_idx": 1}, {"type": "equation", "text": "$$\nQ_{i j}=f_{Q}(\\pmb{x}_{i},\\pmb{x}_{j}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Typically, in network estimation, one observes adjacency matrix $\\{A_{i j}\\}$ distributed as $\\{\\mathbf{\\bar{Bernoulli}}(Q_{i j})\\}$ , and either has to learn $\\pmb{x}_{i}$ or directly estimate $f_{Q}$ . There has been much work in the statistics community on estimating $\\pmb{x}_{i}$ for specific models (usually up to rotation). For stochastic block models, see the excellent survey in Abbe (2017). ", "page_idx": 1}, {"type": "text", "text": "Estimating $f_{Q}$ can be done with some additional assumptions (Chatterjee, 2015). When $f_{Q}$ has appropriate smoothness properties, one can estimate it by a histogram approximation (Olhede and Wolfe, 2014; Chan and Airoldi, 2014). This setting has also been compared to nonparametric regression with an unknown design (Gao et al., 2015). Methods for network estimation include Universal Singular Value Thresholding (Chatterjee, 2015; Xu, 2018), combinatorial optimization (Gao et al., 2015; Klopp et al., 2017), and neighborhood smoothing (Zhang et al., 2017; Mukherjee and Chakrabarti, 2019). ", "page_idx": 1}, {"type": "text", "text": "Transfer Learning on Networks. We wish to estimate the target network $Q$ . However, we only observe $f_{Q}$ on $\\scriptstyle{\\binom{n_{Q}}{2}}$ pairs of nodes, for a uniformly random subset of variables $S\\subset\\{1,2,\\ldots,n\\}$ We assume $S$ is vanishingly small, so $n_{Q}:=|S|=o(n)$ . ", "page_idx": 1}, {"type": "text", "text": "Absent additional information, we cannot hope to achieve $o(1)$ mean-squared error. To see this, suppose $f_{Q}$ is a stochastic block model with 2 communities of equal size. For a node $i\\not\\in S$ , no edges incident to $i$ are observed, so its community cannot be learned. Since $n_{Q}\\ll n$ , we will attain $\\bar{\\Omega(1)}$ error overall. To attain error $o(1)$ , we hope to leverage transfer learning from a source $P$ if available. In fact, we give an efficient algorithm to achieve $o(1)$ error, formally stated in Section 2. ", "page_idx": 1}, {"type": "text", "text": "Theorem 1.1 (Theorem 2.3, Informal). There exists an efficient algorithm such that, if given source data $A_{P}\\in\\{0,1\\}^{n\\times n}$ and target data $A_{Q}\\in\\{0,1\\}^{n_{Q}\\times n_{Q}}$ coming from an appropriate pair $(f_{P},f_{Q})$ ", "page_idx": 1}, {"type": "text", "text": "of latent variable models, outputs $\\widehat{Q}\\in\\mathbb{R}^{n\\times n}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\frac{1}{n^{2}}\\|Q-\\widehat{Q}\\|_{F}^{2}\\leq o(1)\\right]\\geq1-o(1).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "There must be a relationship between $\\mathbf{P}$ and Q for them to be an appropriate pair for transfer learning.   \nWe formalize this relationship below. ", "page_idx": 2}, {"type": "text", "text": "Relationship Between Source and Target. It is natural to consider pairs $(f_{P},f_{Q})$ such that for all $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}\\in\\mathcal{X}$ , the difference $(f_{P}({\\pmb x},{\\pmb y})-f_{Q}({\\pmb x},{\\pmb y}))$ is small. For example, Cai and $\\mathrm{Pu}$ (2024) study transfer learning for nonparametric regression when $f_{P}-f_{Q}$ is close to a polynomial in $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}$ . But, requiring $f_{P}-f_{Q}$ to be pointwise small does not capture a broad class of pairs in the network setting. For example, if $f_{P}=\\alpha f_{Q}$ . Then $f_{P}-f_{Q}=(\\alpha-1)f_{Q}$ can be far from all polynomials if $f_{Q}$ is, e.g. a H\u00f6lder-smooth graphon.2 However, under the network model, this means $A_{P}$ and $A_{Q}$ are stochastically identical modulo one being $\\alpha$ times denser than the other. ", "page_idx": 2}, {"type": "text", "text": "We will therefore consider pairs $(f_{P},f_{Q})$ that are close in some measure of local graph structure. With this in mind, we use a graph distance introduced in Mao et al. (2021) for a different inference problem. ", "page_idx": 2}, {"type": "text", "text": "Definition 1.2 (Graph Distance). Let $P\\,\\in\\,[0,1]^{n\\times n}$ be the probability matrix of a graph. For $i,j\\in[n],i\\neq j$ , we define the graph distance between them as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd_{P}(i,j):=\\Vert(e_{i}-e_{j})^{T}P^{2}(I-e_{i}e_{i}^{T}-e_{j}e_{j}^{T})\\Vert_{2}^{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\boldsymbol{e}_{i},\\boldsymbol{e}_{j}\\in\\mathbb{R}^{n}$ are standard basis vectors. ", "page_idx": 2}, {"type": "text", "text": "Intuitively, this first computes the matrix $P^{2}$ of common neighbors, and then computes the distance between two rows of the same (ignoring the diagonal elements). We will require that $f_{P},f_{Q}$ satisfy a local similarity condition on the relative rankings of nodes with respect to this graph distance. Since we only estimate the probability matrix of $Q$ , the condition is on the latent variables $\\pmb{x}_{1},\\dots,\\pmb{x}_{n}$ of interest. The hope is that the proximity in graph distance reflects the proximity in latent positions. ", "page_idx": 2}, {"type": "text", "text": "Definition 1.3 (Rankings Assumption at Quantile $h_{n}$ ). Let $(P,Q)$ be a pair of graphs evaluated on $n$ latent positions. We say $(P,Q)$ satisfy the rankings assumption at quantile $h_{n}\\leq1$ if there exists constant $C>0$ such that for all $i\\,\\in\\,[n]$ and all $j\\neq i,$ if $j$ belongs to the bottom $h_{n}$ -quantile of $d_{P}(i,\\cdot)$ , then $j$ belongs to the bottom $C h_{n}$ -quantile of $d_{Q}(i,\\cdot)$ . ", "page_idx": 2}, {"type": "text", "text": "To further motivate Definition 1.3, recall our motivating example of biological network estimation. Previous works require some form of similarity between networks to enable transfer Sen et al. (2018); Fan et al. (2019); Baranwal et al. (2020). For example, Kshirsagar et al. (2013) require a commonality hypothesis: if pathogens A, B target the same neighborhoods in a protein interaction network, one can transfer from A to B. Our rankings assumption similarly posits that to transfer knowledge from A to B, A and B have similar 2-hop neighborhood structures. ", "page_idx": 2}, {"type": "text", "text": "Note that Definition 1.3 involves quantiles of graph distances; therefore it is a relative condition, because it depends on a rank-ordering within both graphs $P,Q$ before comparison. On the other hand, an absolute condition would require that for nodes $i,j\\,\\in\\,[n]$ , if e.g. $d_{P}(i,j)\\,<\\,100$ then $d_{Q}(i,j)<C\\cdot100$ . Our condition is more flexible and will hold for a larger set of graph pairs $(P,Q)$ , such as pairs where one graph is much more dense than the other. ", "page_idx": 2}, {"type": "text", "text": "Finally, to illustrate Definition 1.3, consider stochastic block models $f_{P},f_{Q}$ with $k_{P}\\geq k_{Q}$ communities respectively. If nodes $i,j$ are in the same communities then $P e_{i}=\\dot{P}e_{j}$ , so $d_{P}(i,j)=0$ . We require that $j$ minimizes $d_{Q}(i,\\cdot)$ . This occurs if and only if $d_{Q}(i,j)=0$ . Hence if $i,j$ belong to the same community in $P$ , they are in the same community in $Q$ . Note that the converse is not necessary; we could have $Q$ with 1 community and $P$ with arbitrarily many communities. ", "page_idx": 2}, {"type": "text", "text": "With the relationship between the source and target defined by the rankings assumption, our contributions are as follows. ", "page_idx": 2}, {"type": "text", "text": "(1) Algorithm for Latent Variable Models. We provide an efficient Algorithm 1 for latent variable models with H\u00f6lder-smooth $f_{P},f_{Q}$ . The benefit of this algorithm is that it does not assume a parametric form of $f_{P}$ and $f_{Q}$ . We prove a guarantee on its error in Theorem 2.3. ", "page_idx": 2}, {"type": "text", "text": "(2) Minimax Rates. We prove a minimax lower bound for Stochastic Block Models (SBMs) in Theorem 3.2. Moreover, we provide a simple Algorithm 2 that attains the minimax rate for this class (Proposition 3.4). ", "page_idx": 3}, {"type": "text", "text": "(3) Experimental Results on Real-World Data. We test both of our algorithms on real-world metabolic networks and dynamic email networks, as well as synthetic data (Section 4). ", "page_idx": 3}, {"type": "text", "text": "All proofs are deferred to the Appendix. ", "page_idx": 3}, {"type": "text", "text": "1.1 Other Related work ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Transfer learning has recently drawn a lot of interest both in applied and theoretical communities. The notion of transferring knowledge from one domain with a lot of data to another with less available data has seen applications in epidemiology Apostolopoulos and Bessiana (2020), computer vision Long et al. (2015); Tzeng et al. (2017b); Huh et al. (2016); Donahue et al. (2014); Neyshabur et al. (2020), natural language processing Daum\u00e9 (2007), etc. For a comprehensive survey see Zhuang et al. (2019); Weiss et al. (2016); Kim et al. (2022). Recently, there have also been advances in the theory of transfer learning Yang et al. (2013); Tripuraneni et al. (2020); Agarwal et al. (2023); Cai and Wei (2021); Cai and Pu (2024); Cody and Beling (2023). ", "page_idx": 3}, {"type": "text", "text": "In the context of networks, transfer learning is particularly useful since labeled data is typically hard to obtain. Tang et al. (2016) develop an algorithmic framework to transfer knowledge obtained using available labeled connections from a source network to do link prediction in a target network. Lee et al. (2017) proposes a deep learning framework for graph-structured data that incorporates transfer learning. They transfer geometric information from the source domain to enhance performance on related tasks in a target domain without the need for extensive new data or model training. The SGDA method Qiao et al. (2023) introduce adaptive shift parameters to mitigate domain shifts and propose pseudo-labeling of unlabeled nodes to alleviate label scarcity. Zou et al. (2021) proposes to transfer features from the previous network to the next one in the dynamic community detection problem. Simchowitz et al. (2023) work on combinatorial distribution shift for matrix completion, where only some rows and columns are given. A similar setting is used for link prediction in egocentrically sampled networks in Wu et al. (2018). Zhu et al. (2021) train a graph neural network for transfer based on an ego-graph-based loss function. Learning from observations of the full network and additional information from a game played on the network Leng et al. (2020); Rossi et al. (2022). Wu et al. (2024) study graph transfer learning for node regression in the Gaussian process setting, where the source and target networks are fully observed. ", "page_idx": 3}, {"type": "text", "text": "Levin et al. (2022) proposes an inference method from multiple networks all with the same mean but different variances. While our work is related, we do not assume $\\mathbb{E}[P_{i j}]=\\mathbb{E}[Q_{i j}]$ . Cao et al. (2010) do joint link prediction on a collection of networks with the same link function but different parameters. ", "page_idx": 3}, {"type": "text", "text": "Another line of related but different work deals with multiplex networks (Lee et al., 2014, 2015; Iacovacci and Bianconi, 2016; Cozzo et al., 2018) and dynamic networks Sarkar and Moore (2005); Kim et al. (2018); Sewell and Chen (2015); Sarkar et al. (2012); Chang et al. (2024); Wang et al. (2023). One can think of transfer learning in clustering as clustering with side information. Prior works consider stochastic block models with noisy label information (Mossel and Xu, 2016; Mazumdar and Saha, 2017a) or oracle access to the latent structure (Mazumdar and Saha, 2017b). ", "page_idx": 3}, {"type": "text", "text": "Notation. We use lowercase letters $a,b,c$ to denote (real) scalars, boldface $\\mathbf{\\Delta}x,y,z$ to denote vectors, and uppercase $A,B,C$ to denote matrices. Let $a\\vee b:=\\operatorname*{max}\\{a,b\\}$ and $a\\wedge b:=\\operatorname*{min}\\{a,b\\}$ . For integer $n>0$ , let $[n]:=\\{1,2,\\dots,n\\}$ . For a subset $S\\subset[n]$ and $A\\in\\mathbb{R}^{n\\times n}$ , let $A[S,S]\\in\\mathbb{R}^{|S|\\times|S|}$ be the principal submatrix with row and column indices in $S$ . We denote the $\\ell_{2}$ vector norm as $\\|\\pmb{x}\\|=\\|\\pmb{x}\\|_{2}$ , dot product as $\\langle\\pmb{x},\\pmb{y}\\rangle$ , and Frobenius norm as $\\|A\\|=\\|A\\|_{F}$ . For functions $f,g:\\mathbb{N}\\to\\mathbb{R}$ we let $f\\lesssim g$ denote $f=O(g)$ and $f\\gtrsim g$ denote $f=\\Omega(g)$ . All asymptotics ${\\cal O}(\\cdot),o(\\cdot),\\Omega(\\cdot),\\omega(\\cdot)$ are with respect to $n_{Q}$ unless specified otherwise. ", "page_idx": 3}, {"type": "text", "text": "2 Estimating Latent Variable Models with Rankings ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present a computationally efficient transfer learning algorithm for latent variable models. Algorithm 1 learns the local structure of $P$ based on graph distances (Definition 1.2). For ", "page_idx": 3}, {"type": "text", "text": "1: Input: $A_{P}\\in\\{0,1\\}^{n\\times n},A_{Q}\\in\\{0,1\\}^{n_{Q}\\times n_{Q}},S\\subset[n]$ s.t. $|S|=n_{Q}$ .   \n2: Initialize $\\widehat{Q}\\in\\mathbb{R}^{n\\times n}$ to be all zeroes.   \n3: For all $i$ , all $j\\neq i$ , compute graph distances:   \n$d_{A_{P}}(i,j):=\\lVert(e_{i}-e_{j})^{T}(A_{P})^{2}(I-e_{i}e_{i}^{T}-e_{j}e_{j}^{T})\\rVert_{2}^{2}.$   \n4: Fix a bandwidth h \u2208(0, 1) based on n, nQ.   \n5: for $i=1$ to $n$ do   \n6: Let $T_{i}^{A_{P}}(h)\\subset S$ be bottom $h$ -quantile of $S$ with respect to $d_{A_{P}}(i,\\cdot)$ .   \n7: if $i\\in S$ then   \n8: Update $T_{i}^{A_{P}}(h)\\gets T_{i}^{A_{P}}(h)\\cup\\{i\\}$ .   \n9: end if   \n10: end for   \n11: for $i=2$ to $n$ do   \n12: for $1\\leq j<i$ do   \n13: Compute $\\widehat{Q}_{i j}=\\widehat{Q}_{j i}$ by averaging: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat Q_{i j}:=\\frac{1}{|T_{i}^{A_{P}}(h)||T_{j}^{A_{P}}(h)|}\\sum_{r\\in T_{i}^{A_{P}}(h)}\\sum_{s\\in T_{j}^{A_{P}}(h)}A_{Q;r s}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "14: end for ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "15: end for   \n16: Return $\\widehat{Q}$ . ", "page_idx": 4}, {"type": "text", "text": "each node $i$ of $P$ , it ranks the nodes in $S$ with respect to the graph distance $d_{P}(i,\\cdot)$ . For most nodes $i,j\\in[n]$ , none of the edges incident to $i$ or $j$ are observed in $Q$ . Therefore, we estimate $\\widehat{Q}_{i j}$ by using the edge information about nodes $r,s\\in S$ such that $d_{P}(i,r)$ and $d_{P}(j,s)$ are small. ", "page_idx": 4}, {"type": "text", "text": "Formally, we consider a model as in Eq. (1) with a compact latent space $\\mathcal{X}\\subset\\mathbb{R}^{d}$ and latent variables sampled i.i.d. from the normalized Lebesgue measure on $\\mathcal{X}$ . We set $\\mathcal{X}=[0,1]^{d}$ without loss of generality and assume that functions $f:\\mathcal{X}\\times\\mathcal{X}\\to[0,1]$ are $\\alpha$ -H\u00f6lder-smooth. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.1. Let $f:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ and $\\alpha>0$ . We say $f$ is $\\alpha$ -H\u00f6lder-smooth if there exists $C_{\\alpha}>0$ such that for all $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime},\\mathbf{\\boldsymbol{y}}\\in\\mathcal{X}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{\\substack{\\kappa\\in\\mathbb{N}^{d}:\\sum_{i}\\kappa_{i}=\\lfloor\\alpha\\rfloor}}\\left\\vert\\frac{\\partial^{\\sum_{i}\\kappa_{i}}f}{\\partial_{x_{1}}^{\\kappa_{1}}\\cdot\\cdot\\cdot\\partial_{x_{d}}^{\\kappa_{d}}}(\\pmb{x},\\pmb{y})-\\frac{\\partial^{\\sum_{i}\\kappa_{i}}f}{\\partial_{x_{1}}^{\\kappa_{1}}\\cdot\\cdot\\cdot\\partial_{x_{d}}^{\\kappa_{d}}}(\\pmb{x}^{\\prime},\\pmb{y})\\right\\vert\\leq C_{\\alpha}\\|\\pmb{x}-\\pmb{x}^{\\prime}\\|_{2}^{\\alpha\\wedge1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To exclude degenerate cases where a node may not have enough neighbors in latent space, we require the following assumption. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2.2 (Assumption 3.2 of Mao et al. (2021)). Let $G$ be a graph on $\\pmb{x}_{1},\\dots,\\pmb{x}_{n}$ . There exist $c_{2}>c_{1}>0$ and $\\Delta_{n}=o(1)$ such that for all $\\mathbf{\\boldsymbol{x}}_{i},\\mathbf{\\boldsymbol{x}}_{j}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\nc_{1}\\|x_{i}-x_{j}\\|^{\\alpha\\wedge1}-\\Delta_{n}\\leq\\frac{1}{n^{3}}d_{G}(i,j)\\leq c_{2}\\|\\pmb{x}_{i}-\\pmb{x}_{j}\\|^{\\alpha\\wedge1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The second inequality follows directly from H\u00f6lder-smoothness, and the first is shown to hold for e.g.   \nGeneralized Random Dot Product Graphs, among others (Mao et al., 2021). ", "page_idx": 4}, {"type": "text", "text": "We establish the rate of estimation for Algorithm 1 below. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.3. Let $\\widehat{Q}$ be as in Algorithm 1. Let $f_{P}$ be $\\alpha$ -H\u00f6lder-smooth and $f_{Q}$ be $\\beta$ -H\u00f6ldersmooth for $\\beta\\geq\\alpha>0,$ , and let c be an absolute constant. Suppose $(P,Q)$ satisfy Definition 1.3 at $\\begin{array}{r}{h_{n}=c\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}}\\end{array}$ and $P$ satisfies Assumption 2.2 with $\\Delta_{n}=O((\\frac{\\log n}{n Q})^{\\frac{1}{2}\\vee\\frac{\\alpha\\wedge1}{d}})$ . Then there exists an absolute constant $C>0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\frac{1}{n^{2}}\\|\\widehat{Q}-Q\\|_{F}^{2}\\lesssim\\left(\\frac{d}{2}\\right)^{\\frac{\\beta\\wedge1}{2}}\\left(\\frac{\\log n}{n_{Q}}\\right)^{\\frac{\\beta\\wedge1}{2d}}\\right]\\geq1-n_{Q}^{-C}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "To parse Theorem 2.3, consider the effect of various parameter choices. First, observe that our upper bound scales quite slowly with n. Even if n is superpolynomial in nQ, e.g. n = nQ log nQ, then $\\log n=O((\\log n_{Q})^{2})=n_{Q}^{o(1)}$ , so the overall effect on the error is dominated by the $n_{Q}$ term. ", "page_idx": 5}, {"type": "text", "text": "Second, the bound is worse in large dimensions, and scales exponentially in $\\textstyle{\\frac{1}{d}}$ . This kind of scaling also occurs in minimax lower bounds for nonparametric regression (Tsybakov, 2009), and upper bounds for smooth graphon estimation (Xu, 2018). However, we caution that nonparametric regression can be quite different from network estimation; it would be very interesting to know the dependence of dimension on minimax lower bounds for network estimation, but to the best of our knowledge this is an open problem. Finally notice that a greater smoothness $\\beta$ results in a smaller error, up to $\\beta=1$ , exactly as in (Gao et al., 2015; Klopp et al., 2017; Xu, 2018). ", "page_idx": 5}, {"type": "text", "text": "3 Minimax Rates for Stochastic Block Models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we will show matching lower and upper bounds for a very structured class of latent variable models, namely, Stochastic Block Models (SBMs). ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1 (SBM). Let $P\\in[0,1]^{n\\times n}$ . We say $P$ is an $(n,k)$ -SBM if there exist $B\\in[0,1]^{k\\times k}$ and $z:[n]\\to[k]$ such that for all $i,j$ , $P_{i j}=B_{z(i)z(j)}$ . We refer to $z^{-1}(\\{j\\})$ as community $j\\in[k]$ . ", "page_idx": 5}, {"type": "text", "text": "We first state a minimax lower bound, proved via Fano\u2019s method. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (Minimax Lower Bound for SBMs). Let $k_{P}\\geq k_{Q}\\geq1$ with $k_{Q}$ dividing $k_{P}$ . Let $\\mathcal{F}$ be the family of pairs $(P,Q)$ where $P$ is an $(n,k_{P})$ -SBM, $Q$ is an $(n,k_{Q})$ -SBM, and $(P,Q)$ satisfy Definition 1.3 at $h_{n}=1/k_{P}$ . Moreover, suppose $S\\subset[n]$ is restricted to contain an equal number of nodes from communities $1,2,\\ldots,k_{P}$ of $P$ . Then the minimax rate of estimation is: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\widehat{Q}\\in[0,1]^{n\\times n}}\\operatorname*{sup}_{(P,Q)\\in\\mathcal{F}}\\mathbb{E}\\left[\\frac{1}{n^{2}}\\|\\widehat{Q}-Q\\|_{F}^{2}\\right]\\gtrsim\\frac{k_{Q}^{2}}{n_{Q}^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that Definition 1.3 at $h_{n}=1/k_{P}$ implies that the true community structure of $Q$ coarsens that of $P$ . The condition that $k_{Q}$ divides $k_{P}$ is merely a technical one that we assume for simplicity. ", "page_idx": 5}, {"type": "text", "text": "We remark that minimax lower bounds for smooth graphon estimation are established by first showing lower bounds for SBMs, and then constructing a graphon with the same block structure using smooth mollifiers (Gao et al., 2015). Therefore, we expect that Theorem 3.2 can also be extended to the graphon setting, using the same techniques. However, sharp lower bounds for other classes such as Random Dot Product Graphs will likely require different techniques (Xie and $\\mathrm{Xu}$ , 2020; Yan and Levin, 2023). ", "page_idx": 5}, {"type": "text", "text": "Remark 3.3 (Clustering Regime). In Appendix A.4 we also prove a minimax lower bound of $\\frac{\\log k_{Q}}{n_{Q}}$ in the regime where the error of recovering the true clustering $z$ dominates. This matches the rate of Gao et al. (2015), but for estimating all $n^{\\breve{2}}$ entries of $Q$ , rather than just the $n_{Q}^{2}$ observed entries. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 suggests that a very simple algorithm might achieve the minimax rate. Namely, use both $A_{P},A_{Q}$ to learn communities, and then use only $A_{Q}$ to learn inter-community edge probabilities. If $(P,Q)$ are in the nonparametric regime where regression error dominates clustering error (called the weak consistency or almost exact recovery regime), then the overall error will hopefully match the minimax rate. ", "page_idx": 5}, {"type": "text", "text": "We formalize this approach in Algorithm 2, and prove that it does achieve the minimax error rate in the weak consistency regime. To this end, we define the signal-to-noise ratio of an SBM with parameter $B\\in[0,1]^{k\\times\\bar{k}}$ as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\ns:={\\frac{p-q}{\\sqrt{p(1-q)}}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $p=\\operatorname*{min}_{i}B_{i i},q=\\operatorname*{max}_{i\\neq j}B_{i j}$ . ", "page_idx": 5}, {"type": "text", "text": "Proposition 3.4 (Error Rate of Algorithm 2). Suppose $P,Q\\ \\in\\ [0,1]^{n\\times n}$ are $(n,k_{P}),(n,k_{Q})$ - SBMs with minimum community sizes $n_{\\mathrm{min}}^{(P)},n_{\\mathrm{min}}^{(Q)}$ respectively. Suppose also that $(P,Q)$ satisfy ", "page_idx": 5}, {"type": "text", "text": "1: Input: $A_{P}\\in\\{0,1\\}^{n\\times n},A_{Q}\\in\\{0,1\\}^{n_{Q}\\times n_{Q}},S\\subset[n]$ s.t. $|S|=n_{Q}$ .   \n2: Estimate clusterings $\\widehat{Z}_{P}\\in\\{0,1\\}^{n\\times k_{P}},\\widehat{Z}_{Q}\\in\\{0,1\\}^{n_{Q}\\times k_{Q}}$ using Chen et al. (2014) on $A_{P},A_{Q}$ respectively.   \n3: Let $\\widehat{W}_{Q}\\in\\mathbb{R}^{k_{Q}\\times k_{Q}}$ be diagonal with ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{W}_{Q;i i}=(\\mathbf{1}^{T}\\widehat{Z}_{Q}e_{i})^{-1}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4: Initialize $\\widehat{\\Pi}\\in\\{0,1\\}^{k_{P}\\times k_{Q}}$ to be all zeroes. ", "page_idx": 6}, {"type": "text", "text": "6: Let $j_{P}\\in[k_{P}],j_{Q}\\in[k_{Q}]$ be the unique column indices at which row $i$ of $\\widehat{Z}_{P},\\widehat{Z}_{Q}$ respectively are nonzero.   \n7: Let $\\widehat{\\Pi}_{j_{P},j_{Q}}=1$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "9: Let $\\widehat{B}_{Q}\\in[0,1]^{k_{Q}\\times k_{Q}}$ be the block-average: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{B}_{Q}=\\widehat{W}_{Q}\\widehat{Z}_{Q}^{T}A_{Q}\\widehat{Z}_{Q}\\widehat{W}_{Q}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "10: return $\\widehat{Q}:=\\widehat{Z}_{P}\\widehat{\\Pi}\\widehat{B}_{Q}\\widehat{\\Pi}^{T}\\widehat{Z}_{P}^{T}$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 1.3 at $h_{n}=n_{\\operatorname*{min}}^{(P)}/n$ . Then if the signal-to-noise ratios are such that: $s_{P}\\geq C(\\frac{\\sqrt{n}}{n_{\\operatorname*{min}}^{(P)}}\\vee$ $\\frac{\\log^{2}(n)}{\\sqrt{n_{\\operatorname*{min}}^{(P)}}}\\Big)$ and $s_{Q}\\geq C(\\frac{\\sqrt{n_{Q}}}{n_{\\operatorname*{min}}(Q)}\\vee\\frac{\\log^{2}(n_{Q})}{\\sqrt{n_{\\operatorname*{min}}^{(Q)}}})$ lo g2((nQQ))) for large enough constant C > 0, Algorithm 2 returnsQ such that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\frac{1}{n^{2}}\\|\\widehat{Q}-Q\\|_{F}^{2}\\lesssim\\frac{k_{Q}^{2}\\log(n_{\\operatorname*{min}}^{(Q)})}{n_{Q}^{2}}\\right]\\geq1-O\\bigg(\\frac{1}{n_{Q}}\\bigg).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we test Algorithm 1 against several classes of simulated and real-world networks. We use quantile cutoff of hn = $h_{n}={\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}}$ logn QnQ for Algorithm 1 in all experiments. ", "page_idx": 6}, {"type": "text", "text": "Baselines. To the best of our knowledge, our exact transfer formulation has not been considered before in the literature. Therefore, we implement two algorithms as alternatives to Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "(1) Algorithm 2. Given $A_{P}\\;\\in\\;\\{0,1\\}^{n\\times n},A_{Q}\\;\\in\\;\\{0,1\\}^{n_{Q}\\times n_{Q}}$ , let $k_{P}\\,=\\,\\lceil{\\sqrt{n}}\\rceil,k_{Q}\\,=\\,\\lceil{\\sqrt{n_{Q}}}\\rceil$ . Compute spectral clusterings $\\widehat{Z}_{P},\\widehat{Z}_{Q}$ with $k_{P},k_{Q}$ clusters respectively. Let $J_{S}\\,\\in\\,\\{0,1\\}^{n_{Q}\\times n}$ is such that $J_{S;i j}=1$ if and only if $i=j$ and $i\\in S$ . The projection $\\widehat{\\Pi}\\in\\mathbb{R}^{k_{P}\\times k_{Q}}$ solves the leastsquares problem $\\mathrm{min}_{\\Pi\\in\\mathbb{R}^{k_{P}\\times k_{Q}}}\\parallel J_{S}\\widehat{Z}_{P}\\Pi-\\widehat{Z}_{Q}\\parallel_{F}^{2}$ . We compute the $\\widehat{\\Pi}$ differently from steps 4-7 in Algorithm 2 to account fRor cases whe re $Q$ is not a true coarsening of $P$ . When $Q$ is a true coarsening of $P$ , this reduces to the procedure in steps 4-7. Given $\\widehat{Z}_{P},\\widehat{\\Pi}$ we return $\\widehat{Q}$ as in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "(2) Oracle. Suppose that an oracle can access data for $Q$ on all $n\\gg n_{Q}$ nodes as follows. Fix an error probability $p\\in(0,1)$ . The oracle is given symmetric $A_{Q}^{\\prime}\\in\\{0,\\dot{1}\\}^{n\\times n}$ with independent entries following a mixture distribution. For all $i,j\\in[n]$ with $i<j$ let $X_{i j}\\sim\\mathrm{Bernoulli}(p)$ and $Y_{i j}\\sim$ Bernoulli $\\bar{(Q(x_{i},\\pmb{x}_{j}))}$ . Then: ", "page_idx": 6}, {"type": "equation", "text": "$$\nA_{Q;i j}^{\\prime}=\\mathbb{1}_{i\\in S,j\\in S}Y_{i j}+(1-\\mathbb{1}_{i\\in S,j\\in S})((1-X_{i j})Y_{i j}+X_{i j}(1-Y_{i j})).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Given $A_{Q}^{\\prime}$ , the oracle returns the estimate from Universal Singular Value Thresholding on $A_{Q}^{\\prime}$ Chatterjee (2015). As $p\\rightarrow0$ , the error will approach $O(n^{\\frac{-2\\beta}{2\\beta+d}})$ for a $\\beta$ -smooth network on on $d$ -dimensional latent variables (Xu, 2018), so the oracle will outperform any transfer algorithm. ", "page_idx": 6}, {"type": "table", "img_path": "PK8xOCBQRO/tmp/f51f43c103bd40040521454e36ba7bd1577cc5e3de4cde81d82d9fc26fe75839.jpg", "table_caption": [], "table_footnote": ["Table 1: Comparison of different algorithms on simulated networks. Each cell reports $\\widehat{\\mu}\\pm2\\widehat{\\sigma}$ of the mean-squared error over 50 independent trials. Error numbers are all scaled by $1e2$ for ease of reading. Bold: Best algorithm. Emphasis: Second-best algorithm. "], "page_idx": 7}, {"type": "text", "text": "Simulations. We first test on several classes of simulated networks. For $n_{Q}=50$ , $n=200$ , we run 50 independent trials for each setting. We report results for each setting in Table 1, and visualize estimates for stylized examples in Figure 1. ", "page_idx": 7}, {"type": "text", "text": "At a glance, Figure 1 shows that Algorithms 1 and 2 both work well on Stochastic Block Models (first row), that only Algorithm 1 works well on graphons (second and third rows), and that the Oracle performs well in all cases. ", "page_idx": 7}, {"type": "text", "text": "Smooth Graphons. The latent space is $\\mathcal{X}=[0,1]$ . We consider graphons of the form $f_{\\gamma}(x,y)=$ $\\textstyle{\\frac{x^{\\gamma}+y^{\\gamma}}{2}}$ where $P,Q$ have different $\\gamma$ . We denote this the $\\gamma$ -Smooth Graphon. ", "page_idx": 7}, {"type": "text", "text": "Mixed-Membership Stochastic Block Model. Set $k_{P}=\\lfloor{\\sqrt{n}}\\rfloor,k_{Q}=\\lfloor{\\sqrt{n_{Q}}}\\rfloor$ . The latent space $\\mathcal{X}$ is the probability simplex $\\begin{array}{r}{\\mathcal{X}=\\Delta_{k_{P}}:=\\{x\\in[0,1]^{k_{P}}:\\sum_{i}x_{i}=1\\}\\subset\\mathbb{R}^{k_{P}}}\\end{array}$ . The latent variables $\\pmb{x}_{1},\\dots,\\pmb{x}_{n}$ are iid-Dirichlet distributed with equal weigh ts $\\textstyle{\\frac{1}{k_{P}}},\\ldots,{\\frac{1}{k_{P}}}$ . Then $P_{i j}={\\pmb x}_{i}^{T}{\\cal B}_{P}{\\pmb x}_{j}$ and $Q_{i j}\\,=\\,\\Pi({\\pmb x}_{i})^{T}B_{Q}\\Pi({\\pmb x}_{j})$ , for connectivity matrices $B_{P}\\;\\in\\;[0,1]^{k_{P}\\times k_{P}},B_{Q}\\;\\in\\;[0,1]^{k_{Q}\\times k_{Q}}$ , and projection $\\Pi:\\Delta_{k_{P}}\\rightarrow\\Delta_{k_{Q}}$ for a fixed subset of $[k_{P}]$ . For parameters $a,b,\\epsilon\\in[0,1]$ we generate $B\\in[0,1]^{k\\times k}$ by sampling $\\stackrel{\\cdot}{E}\\in\\mathrm{Uniform}(-\\epsilon,\\epsilon)^{k\\times k}$ and set $B=\\mathrm{clip}((a-b)I+b\\mathbf{1}\\mathbf{1}^{T}+E,0,1)$ We call this Noisy-MMSB $(a,b,\\epsilon)$ . ", "page_idx": 7}, {"type": "text", "text": "Latent Distance Model. The latent space is the unit sphere $\\mathcal{X}=\\mathbb{S}^{d-1}\\subset\\mathbb{R}^{d}$ . For scale parameter $s>0$ , we call $f_{s}(\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}})=\\exp(-s\\|\\mathbf{\\boldsymbol{x}}-\\mathbf{\\boldsymbol{y}}\\|_{2})$ the $\\mathbb{R}^{d}$ -Latent $\\left(s\\right)$ model. ", "page_idx": 7}, {"type": "text", "text": "Discussion. When the latent dimension is larger than 1 (the Noisy MMSB and Latent Variable Models), our Algorithm 1 is better than both Algorithm 2 and the Oracle with $p=0.1$ . Note that Algorithms 1 and 2 use $\\begin{array}{r}{\\frac{n_{Q}^{2}}{n^{2}}\\approx0.06}\\end{array}$ unbiased edge observations from $Q$ , while the Oracle with p = 0.1 observes (1 \u2212p) n2 $\\begin{array}{r}{(1-p)\\frac{n^{2}-n_{Q}^{2}}{n^{2}}\\approx0.9}\\end{array}$ unbiased edge observations in expectation. ", "page_idx": 7}, {"type": "text", "text": "Real-World Data. Next, we test on two classes of real-world networks. We summarize our dataset characteristics in Table 2. See Appendix C for further details. ", "page_idx": 7}, {"type": "table", "img_path": "PK8xOCBQRO/tmp/504389edf90dad84bb67c5ada4bb46ecef71922076b7c140d98199885e751fbb.jpg", "table_caption": ["Table 2: Dataset Characteristics "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Transfer Across Species in Metabolic Networks. For a fixed organism, a metabolic network has a node for each metabolite, and an edge exists if and only if two metabolites co-occur in a metabolic reaction in that organism. We obtain the unweighted metabolic networks for multiple gram-negative bacteria from the BiGG genome-scale metabolic model dataset (King et al., 2016; Norsigian et al., 2020). In the left half of Figure 2, we compare two choices of source organism in estimating the network for BiGG model iJN1463 (Pseudomonas putida). For a good choice of source, Algorithm 1 is competitive with the Oracle at $p=0.1$ . ", "page_idx": 7}, {"type": "image", "img_path": "PK8xOCBQRO/tmp/7a512ae6ca56e2fab7f9e5ccf0af10c4b1eb9501037fb2943d1de360d90b71c5.jpg", "img_caption": ["Figure 1: Comparison of algorithms on three source-target pairs $n=2000$ , $n_{Q}=500\\$ ). Each row corresponds to a different source/target pair $(P,Q)$ . For a fixed row, the upper triangular part on columns 2, 3, 4 corresponds a $\\widehat{Q}$ for a different algorithm. The upper triangular part of column 1 shows the true $P$ . The lower triangular part of columns 1, 2, 3, and 4 is identical for a fixed row, and shows the true $Q$ . In each heatmap, the lower triangle is the target $Q$ . Algorithm 2 performs best when $(P,Q)$ are SBMs (top), while Algorithm 1 is better for smooth graphons (2nd and 3rd rows). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Transfer Across Time in the Email Interaction Networks. We use the EMAIL-EU interaction network between $n=1005$ members of a European research institution across 803 days Leskovec and Krevl (2014); Paranjape et al. (2017). The source graph $A_{P}$ is the network from day 1 to $\\approx80$ ([1, 80]). In Figure 2 we simulate transfer with targets [81, 160] (left) and [561, 640] (right). We visualize results for arbitrary target periods; similar results hold for other targets. Unlike metabolic networks, Algorithm 2 has comparable performance to both our Algorithm 1 and the oracle algorithm with $p\\in\\{0.01,0.05\\}$ . Compared to the metabolic networks, this indicates that the email interaction networks are relatively well-approximated by SBMs, although Algorithm 1 is still the best. ", "page_idx": 8}, {"type": "text", "text": "Additional Experiments and Baseline. In Appendix B.1, we present additional ablation experiments that test the dependence of Algorithms 1 and 2 on all relevant parameters. We compare their performance to the Oracle baseline with $p=0.0$ (the non-transfer setting), and an additional baseline adapted from Levin et al. (2022). We find that our Algorithms outperform this new baseline but are worse than the Oracle with $p=0.0$ , as expected. Further, in Appendix B.2, we test our Algorithms and original baselines on a link prediction task in the setting of Figure 2. We find that the relative accuracy of the methods for link prediction is qualitatively similar to that of Figure 2, and the Oracle performs even better with sparsity tuning. ", "page_idx": 8}, {"type": "image", "img_path": "PK8xOCBQRO/tmp/6b3d37471bd73b1791403a2b84b95d0b596adac105a2302c884573282b981680.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 2: Results of network estimation on real-world data. Shaded regions denote [1, 99] percentile outcomes from 50 trials. ", "page_idx": 9}, {"type": "text", "text": "Left half: Estimating metabolic network of iJN1463 (Pseudomonas putida) with source iWFL1372 (Escherichia coli W) leftmost, and source iPC815 (Yersinia pestis) second-left. ", "page_idx": 9}, {"type": "text", "text": "Right half: Using source data from days $1-80$ of EMAIL-EU to estimate target days $81-160$ (third-left) and target days $561-640$ (rightmost). Note that we use smaller values of $p$ for the Oracle in EMAIL-EU. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we study transfer learning for network estimation in latent variable models. We show that there exists an efficient Algorithm 1 that achieves vanishing error even when $n\\geq n_{Q}^{\\omega(1)}$ and a , simpler Algorithm 2 for SBMs that achieves the minimax rate. ", "page_idx": 9}, {"type": "text", "text": "There are several interesting directions for future work. ", "page_idx": 9}, {"type": "text", "text": "First, we believe that Algorithm 1 works for moderately sparse networks with population edge density $\\Omega\\big(\\frac{1}{\\sqrt{n}}\\big)$ . This is because the concentration of empirical graph distance (Algorithm 1 line 3) requires expected edge density $\\widetilde{\\Omega}(n^{-1/2})$ Mao et al. (2021). It would be interesting to see if a similar approach can work for edge density $\\Omega({\\frac{\\log n}{n}})$ . For example, in the aforementioned paper it is shown that a variation of the graph distance of Definition 1.2 concentrates at expected edge density $\\widetilde\\Omega(n^{-2/3})$ . While is this still far from the $\\Omega(\\frac{\\log n}{n})$ regime, it suggests that variations on the graph distance might ensure our Algorithm 1 works for sparser graphs. ", "page_idx": 9}, {"type": "text", "text": "Second, the case of multiple sources is also interesting. We have focused on the case of one source distribution, as in Cai and Wei (2021); Cai and Pu (2024), but expect that our algorithms can be extended to multiple sources as long as they satisfy Definition 1.3. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for their valuable feedback. ", "page_idx": 9}, {"type": "text", "text": "AJ and PS gratefully acknowledge NSF grants 2217069, 2019844, and DMS 2109155. ", "page_idx": 9}, {"type": "text", "text": "AM was supported by NSF awards 2217058 and 2133484. ", "page_idx": 9}, {"type": "text", "text": "SSM was partially supported by an INSPIRE research grant (DST/INSPIRE/04/2018/002193) from the Dept. of Science and Technology, Govt. of India and a Start-Up Grant from Indian Statistical Institute. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Abbe, E. (2017). Community detection and stochastic block models. arXiv preprint arXiv:1703.10146. ", "page_idx": 9}, {"type": "text", "text": "Agarwal, A., Song, Y., Sun, W., Wang, K., Wang, M., and Zhang, X. (2023). Provable benefits of representational transfer in reinforcement learning. In Neu, G. and Rosasco, L., editors, Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 2114\u20132187. PMLR. ", "page_idx": 9}, {"type": "text", "text": "Airoldi, E. M., Blei, D. M., Fienberg, S. E., and Xing, E. P. (2008). Mixed membership stochastic blockmodels. Journal of Machine Learning Research, 9:1981\u20132014.   \nAldous, D. J. (1981). Representations for partially exchangeable arrays of random variables. Journal of Multivariate Analysis, 11:581\u2013598.   \nApostolopoulos, I. D. and Bessiana, T. (2020). Covid-19: automatic detection from x-ray images utilizing transfer learning with convolutional neural networks. Physical and Engineering Sciences in Medicine, 43:635 \u2013 640.   \nBaranwal, M., Magner, A., Elvati, P., Saldinger, J., Violi, A., and Hero, A. O. (2020). A deep learning architecture for metabolic pathway prediction. Bioinformatics, 36(8):2547\u20132553.   \nBarber\u00e1, P., Jost, J. T., Nagler, J., Tucker, J. A., and Bonneau, R. (2015). Tweeting from left to right: Is online political communication more than an echo chamber? Psychological science, 26(10):1531\u20131542.   \nBen-David, S., Blitzer, J., Crammer, K., and Pereira, F. (2006). Analysis of representations for domain adaptation. Advances in neural information processing systems, 19.   \nBickel, P. J. and Chen, A. (2009). A nonparametric view of network models and newman\u2013girvan and other modularities. Proceedings of the National Academy of Sciences, 106(50):21068\u201321073.   \nCai, T. T. and Pu, H. (2024). Transfer learning for nonparametric regression: Non-asymptotic minimax analysis and adaptive procedure. arXiv preprint arXiv:2401.12272.   \nCai, T. T. and Wei, H. (2021). Transfer learning for nonparametric classification: Minimax rate and adaptive classifier. The Annals of Statistics, 49(1).   \nCao, B., Liu, N. N., and Yang, Q. (2010). Transfer learning for collective link prediction in multiple heterogenous domains. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 159\u2013166. Citeseer.   \nCao, X. and Ward, M. D. (2014). Do democracies attract portfolio investment? transnational portfolio investments modeled as dynamic network. International Interactions, 40(2):216\u2013245.   \nChan, S. and Airoldi, E. (2014). A consistent histogram estimator for exchangeable graph models. In International Conference on Machine Learning, pages 208\u2013216. PMLR.   \nChang, S., Koehler, F., Qu, Z., Leskovec, J., and Ugander, J. (2024). Inferring dynamic networks from marginals with iterative proportional fitting. arXiv preprint arXiv:2402.18697.   \nChatterjee, S. (2015). Matrix estimation by universal singular value thresholding. The Annals of Statistics, pages 177\u2013214.   \nChen, Y., Sanghavi, S., and Xu, H. (2014). Improved graph clustering. IEEE Transactions on Information Theory, 60(10):6440\u20136455.   \nChristensen, B. and Nielsen, J. (2000). Metabolic Network Analysis, pages 209\u2013231. Springer Berlin Heidelberg, Berlin, Heidelberg.   \nCody, T. and Beling, P. A. (2023). A systems theory of transfer learning. IEEE Systems Journal, 17(1):26\u201337.   \nCortes, C., Mohri, M., Riley, M., and Rostamizadeh, A. (2008). Sample selection bias correction theory. In International conference on algorithmic learning theory, pages 38\u201353. Springer.   \nCozzo, E., De Arruda, G. F., Rodrigues, F. A., and Moreno, Y. (2018). Multiplex networks: basic formalism and structural properties, volume 10. Springer.   \nCrammer, K., Kearns, M., and Wortman, J. (2008). Learning from multiple sources. Journal of Machine Learning Research, 9(8).   \nDaum\u00e9, H. (2007). Frustratingly easy domain adaptation. ArXiv, abs/0907.1815.   \nDonahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. In Xing, E. P. and Jebara, T., editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 647\u2013655, Bejing, China. PMLR.   \nFan, J., Cannistra, A., Fried, I., Lim, T., Schaffner, T., Crovella, M., Hescott, B., and Leiserson, M. D. (2019). Functional protein representations from biological networks enable diverse cross-species inference. Nucleic acids research, 47(9):e51\u2013e51.   \nGao, C., Lu, Y., and Zhou, H. H. (2015). Rate-optimal graphon estimation. The Annals of Statistics, pages 2624\u20132652.   \nGuruswami, V., Rudra, A., and Sudan, M. (2019). Essential coding theory.   \nHandcock, M. S., Raftery, A. E., and Tantrum, J. M. (2007). Model-based clustering for social networks. Journal of the Royal Statistical Society: Series A (Statistics in Society), 170(2):301\u2013354.   \nHanneke, S. and Kpotufe, S. (2019). On the value of target data in transfer learning. Advances in Neural Information Processing Systems, 32.   \nHanneke, S. and Kpotufe, S. (2022). A no-free-lunch theorem for multitask learning. The Annals of Statistics, 50(6):3119\u20133143.   \nHoeffding, W. (1994). Probability inequalities for sums of bounded random variables. The collected works of Wassily Hoeffding, pages 409\u2013426.   \nHoff, P. (2007). Modeling homophily and stochastic equivalence in symmetric relational data. Advances in neural information processing systems, 20.   \nHoff, P. D., Raftery, A. E., and Handcock, M. S. (2002). Latent Space Approaches to Social Network Analysis. Journal of the American Statistical Association, 97(460):1090\u20131098.   \nHolland, P. W., Laskey, K. B., and Leinhardt, S. (1983). Stochastic blockmodels: First steps. Social Networks, 5(2):109\u2013137.   \nHoover, D. N. (1979). Relations on probability spaces arrays of random variables. Institute for Advanced Study,RI.   \nHuang, J.-T., Li, J., Yu, D., Deng, L., and Gong, Y. (2013). Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 7304\u20137308. IEEE.   \nHuh, M., Agrawal, P., and Efros, A. A. (2016). What makes imagenet good for transfer learning? arXiv preprint arXiv:1608.08614.   \nIacovacci, J. and Bianconi, G. (2016). Extracting information from multiplex networks. Chaos: An Interdisciplinary Journal of Nonlinear Science, 26(6).   \nKim, B., Lee, K. H., Xue, L., and Niu, X. (2018). A review of dynamic network models with latent variables. Statistics Surveys, 12(none):105 \u2013 135.   \nKim, H., Cosa-Linan, A., Santhanam, N., and et al. (2022). Transfer learning for medical image classification: a literature review. BMC Medical Imaging, 22(1):69.   \nKing, Z. A., Lu, J., Dr\u00e4ger, A., Miller, P., Federowicz, S., Lerman, J. A., Ebrahim, A., Palsson, B. O., and Lewis, N. E. (2016). Bigg models: A platform for integrating, standardizing and sharing genome-scale models. Nucleic acids research, 44(D1):D515\u2013D522.   \nKlopp, O., Tsybakov, A. B., and Verzelen, N. (2017). Oracle inequalities for network models and sparse graphon estimation. Annals of Statistics, 45(1):316\u2013354.   \nKshirsagar, M., Carbonell, J., and Klein-Seetharaman, J. (2013). Multitask learning for host\u2013pathogen protein interactions. Bioinformatics, 29(13):i217\u2013i226.   \nLee, J., Kim, H., Lee, J., and Yoon, S. (2017). Transfer learning for deep learning on graph-structured data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.   \nLee, K.-M., Kim, J. Y., Lee, S., and Goh, K.-I. (2014). Multiplex networks. Networks of networks: The last frontier of complexity, pages 53\u201372.   \nLee, K.-M., Min, B., and Goh, K.-I. (2015). Towards real-world complexity: an introduction to multiplex networks. The European Physical Journal B, 88:1\u201320.   \nLeng, Y., Dong, X., Wu, J., and Pentland, A. (2020). Learning quadratic games on networks. In International Conference on Machine Learning, pages 5820\u20135830. PMLR.   \nLeskovec, J. and Krevl, A. (2014). SNAP Datasets: Stanford large network dataset collection. http://snap.stanford.edu/data.   \nLevin, K., Lodhia, A., and Levina, E. (2022). Recovering shared structure from multiple networks with unknown edge distributions. Journal of machine learning research, 23(3):1\u201348.   \nLi, L., Dannenfelser, R., Zhu, Y., Hejduk, N., Segarra, S., and Yao, V. (2022). Joint embedding of biological networks for cross-species functional alignment. bioRxiv, pages 2022\u201301.   \nLong, J., Shelhamer, E., and Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440.   \nLov\u00e1sz, L. (2012). Large Networks and Graph Limits., volume 60 of Colloquium Publications. American Mathematical Society.   \nMansour, Y., Mohri, M., and Rostamizadeh, A. (2009). Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430.   \nMao, X., Chakrabarti, D., and Sarkar, P. (2021). Consistent nonparametric methods for network assisted covariate estimation. In International Conference on Machine Learning, pages 7435\u20137446. PMLR.   \nMazumdar, A. and Saha, B. (2017a). Clustering with noisy queries. Advances in Neural Information Processing Systems, 30.   \nMazumdar, A. and Saha, B. (2017b). Query complexity of clustering with side information. Advances in Neural Information Processing Systems, 30.   \nMossel, E. and Xu, J. (2016). Local algorithms for block models with side information. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, pages 71\u201380.   \nMukherjee, S. S. and Chakrabarti, S. (2019). Graphon estimation from partially observed network data. arXiv preprint arXiv:1906.00494.   \nNeyshabur, B., Sedghi, H., and Zhang, C. (2020). What is being transferred in transfer learning? In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 512\u2013523. Curran Associates, Inc.   \nNorsigian, C. J., Pusarla, N., McConn, J. L., Yurkovich, J. T., Dr\u00e4ger, A., Palsson, B. O., and King, Z. (2020). Bigg models 2020: multi-strain genome-scale models and expansion across the phylogenetic tree. Nucleic acids research, 48(D1):D402\u2013D406.   \nOlhede, S. C. and Wolfe, P. J. (2014). Network histograms and universality of blockmodel approximation. Proceedings of the National Academy of Sciences, 111(41):14722\u201314727.   \nParanjape, A., Benson, A. R., and Leskovec, J. (2017). Motifs in temporal networks. In Proceedings of the tenth ACM international conference on web search and data mining, pages 601\u2013610.   \nQiao, Z., Luo, X., Xiao, M., Dong, H., Zhou, Y., and Xiong, H. (2023). Semi-supervised domain adaptation in graph transfer learning. ArXiv, abs/2309.10773.   \nRen, M., Zhang, S., and Wang, J. (2023). Consistent estimation of the number of communities via regularized network embedding. Biometrics, 79(3):2404\u20132416.   \nRossi, E., Monti, F., Leng, Y., Bronstein, M., and Dong, X. (2022). Learning to infer structures of network games. In International Conference on Machine Learning, pages 18809\u201318827. PMLR.   \nRubin-Delanchy, P., Cape, J., Tang, M., and Priebe, C. E. (2022). A statistical interpretation of spectral embedding: the generalised random dot product graph. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4):1446\u20131473.   \nSarkar, P., Chakrabarti, D., and Jordan, M. I. (2012). Nonparametric link prediction in dynamic networks. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML\u201912, page 1897\u20131904, Madison, WI, USA. Omnipress.   \nSarkar, P. and Moore, A. (2005). Dynamic social network analysis using latent space models. In Weiss, Y., Sch\u00f6lkopf, B., and Platt, J., editors, Advances in Neural Information Processing Systems, volume 18. MIT Press.   \nSen, R., Tagore, S., and De, R. K. (2018). Asapp: Architectural similarity-based automated pathway prediction system and its application in host-pathogen interactions. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 17(2):506\u2013515.   \nSewell, D. K. and Chen, Y. (2015). Latent space models for dynamic networks. Journal of the american statistical association, 110(512):1646\u20131657.   \nSimchowitz, M., Gupta, A., and Zhang, K. (2023). Tackling combinatorial distribution shift: A matrix completion perspective. In Neu, G. and Rosasco, L., editors, Proceedings of Thirty Sixth Conference on Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 3356\u20133468. PMLR.   \nSweet, T. M., Thomas, A. C., and Junker, B. W. (2013). Hierarchical network models for education research: Hierarchical latent space models. Journal of Educational and Behavioral Statistics, 38(3):295\u2013318.   \nTang, J., Lou, T., Kleinberg, J., and Wu, S. (2016). Transfer learning to infer social ties across heterogeneous networks. ACM Trans. Inf. Syst., 34(2).   \nTrifonova, N., Kenny, A., Maxwell, D., Duplisea, D., Fernandes, J., and Tucker, A. (2015). Spatiotemporal bayesian network models with latent variables for revealing trophic dynamics and functional networks in fisheries ecology. Ecological Informatics, 30:142\u2013158.   \nTripuraneni, N., Jordan, M. I., and Jin, C. (2020). On the theory of transfer learning: the importance of task diversity. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red Hook, NY, USA. Curran Associates Inc.   \nTsybakov, A. B. (2009). Introduction to Nonparametric Estimation. Springer series in statistics. Springer, Dordrecht.   \nTzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017a). Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167\u20137176.   \nTzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017b). Adversarial discriminative domain adaptation. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2962\u20132971.   \nVershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press.   \nWang, H., Mao, Y., Sun, J., Zhang, S., Fan, Y., and Zhou, D. (2023). Dynamic transfer learning across graphs. arXiv preprint arXiv:2305.00664.   \nWeiss, K. R., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. Journal of Big Data, 3:1\u201340.   \nWu, J., Ainsworth, L., Leakey, A., Wang, H., and He, J. (2024). Graph-structured gaussian processes for transferable graph learning. Advances in Neural Information Processing Systems, 36.   \nWu, Y.-J., Levina, E., and Zhu, J. (2018). Link prediction for egocentrically sampled networks. Journal of Computational and Graphical Statistics, 32:1296 \u2013 1319.   \nXie, F. and Xu, Y. (2020). Optimal bayesian estimation for random dot product graphs. Biometrika, 107(4):875\u2013889.   \nXu, J. (2018). Rates of convergence of spectral methods for graphon estimation. In International Conference on Machine Learning, pages 5433\u20135442. PMLR.   \nYan, H. and Levin, K. (2023). Minimax rates for latent position estimation in the generalized random dot product graph. arXiv preprint arXiv:2307.01942.   \nYang, L., Hanneke, S., and Carbonell, J. (2013). A theory of transfer learning with applications to active learning. Machine learning, 90:161\u2013189.   \nYu, B. (1997). Assouad, fano, and le cam. In Festschrift for Lucien Le Cam: research papers in probability and statistics, pages 423\u2013435. Springer.   \nZhang, Y., Levina, E., and Zhu, J. (2017). Estimating network edge probabilities by neighbourhood smoothing. Biometrika, 104(4):771\u2013783.   \nZhu, Q., Yang, C., Xu, Y., Wang, H., Zhang, C., and Han, J. (2021). Transfer learning of graph neural networks with ego-graph information maximization. Advances in Neural Information Processing Systems, 34:1766\u20131779.   \nZhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q. (2019). A comprehensive survey on transfer learning. Proceedings of the IEEE, 109:43\u201376.   \nZou, J., Lin, F., Gao, S., Deng, G., Zeng, W., and Alterovitz, G. (2021). Transfer learning based multiobjective genetic algorithm for dynamic community detection. arXiv preprint arXiv:2109.15136. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Recall Hoeffding\u2019s inequality. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.1 (Hoeffding (1994)). Let $X_{1},\\ldots,X_{n}$ be independent random variables such that $a_{i}\\leq X_{i}\\leq b_{i}$ almost surely for all $i\\in[n]$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{i=1}^{n}(X_{i}-\\mathbb{E}[X_{i}])\\right|\\geq t\\right]\\leq2\\exp\\left(\\frac{-2t^{2}}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We also need Bernstein\u2019s inequality. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.2 (Bernstein\u2019s Inequality). Let $X_{1},\\ldots,X_{n}$ be independent mean-zero random variables with $|X_{i}|\\leq1$ for all $i$ and $n\\geq5$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right|\\geq t\\right]\\leq2\\exp\\left(\\frac{-n t^{2}}{2(1+\\frac{t}{3})}\\right)\\leq2\\exp\\bigg(-\\frac{n t^{2}}{4}\\bigg).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Theorem 2.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Throughout this section, let $\\mathcal{X}=[0,1]^{d}$ and $\\mu:\\mathcal{X}\\to[0,1]$ be the normalized Lebesgue measure.   \nWe require the following Lemmata. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.3. Let $\\upsilon\\in(0,1)$ and $\\mu:\\mathcal{X}\\to[0,1]$ be the normalized Lebesgue measure. Then for all $\\pmb{x}\\in\\mathcal{X}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu(\\operatorname{Ball}(\\mathbf{x},2v)\\cap\\mathcal{X})\\geq\\mu(\\operatorname{Ball}(\\mathbf{0},v)\\cap\\mathcal{X}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Recall $\\mathcal{X}=[0,1]^{d}$ . Fix $\\pmb{x}\\in\\mathcal{X},v>0$ . Note that $\\mu(\\mathrm{Ball}(\\pmb{x},\\upsilon)\\cap\\mathcal{X})$ is smallest when $\\textbf{\\em x}$ is a vertex of the hypercube; therefore take $\\pmb{x}\\in\\{0,1\\}^{d}$ without loss of generality. Then, note that for each ${\\pmb z}\\in\\operatorname{Ball}({\\pmb x},{\\boldsymbol v})\\cap{\\boldsymbol\\chi}$ , we can find $(2^{d}-1)$ other points $z^{\\prime}\\in\\mathrm{Ball}({\\pmb x},v)\\setminus\\mathcal{X}$ by reflecting subsets of coordinates of $_{z}$ about $\\textbf{\\em x}$ . There are $2^{d}-1$ such nonempty subsets of coordinates. This shows that $\\mu(\\mathrm{Ball}(\\pmb{x},\\upsilon)\\cap\\mathcal{X})\\geq\\mu(\\mathrm{Ball}(\\pmb{x},\\upsilon))/2^{d}$ for all $\\textbf{\\em x}$ . Since $\\mu(\\mathrm{Ball}(\\pmb{x},v))\\asymp v^{d}$ , the conclusion follows. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "We will repeatedly make use of the concentration of latent positions. ", "page_idx": 15}, {"type": "text", "text": "Lemma A.4 (Latent Concentration). Let $\\mathcal{X}=[0,1]^{d}$ and $\\mu$ denote the normalized Lebesgue measure on $\\mathcal{X}$ . Suppose $x_{1},\\ldots,x_{n}\\sim\\mathcal{X}$ are sampled iid and uniformly at random from $\\mu$ . Fix some $T\\subset\\mathcal{X}$ such that $\\mu(T)=v$ . Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|v n-|\\{j\\in[n]:\\pmb{x}_{j}\\in T\\}|\\,\\right|\\ge10\\sqrt{\\frac{\\log n}{n}}\\right]\\le n^{-10}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. Let $X_{i}$ be an indicator variable that equals 1 if $\\pmb{x}_{i}\\in T$ and zero otherwise. Notice the $X_{i}$ are iid and bounded within $[0,1]$ . Moreover, $\\begin{array}{r}{\\sum_{i}\\mathbb{E}[X_{i}]=n\\mu(T)}\\end{array}$ . Therefore by Hoeffding\u2019s inequality, for any $t>0$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}[|v n-|\\{j\\in[n]:\\pmb{x}_{j}\\in T\\}||\\ge t]\\le2\\exp\\left(\\frac{-2t^{2}}{n}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Setting $t=10\\sqrt{\\frac{\\log n}{n}}$ gives the result. ", "page_idx": 15}, {"type": "text", "text": "Corollary A.5. Let $\\epsilon>0$ . For $i\\in[n]$ let $\\epsilon_{i}^{\\prime}>0$ be $\\epsilon_{i}^{\\prime}:=\\operatorname*{sup}\\{v>0:\\mu(\\mathrm{Ball}(\\pmb{x}_{i},v)\\cap\\mathcal{X})\\le\\epsilon.$ . Let $T_{i}:=\\mathrm{Ball}(\\pmb{x}_{i},\\epsilon_{i}^{\\prime})\\cap\\mathcal{X}$ . Let $u_{i}(S):=|\\{j\\in S:x_{j}\\in T_{i}\\}|$ denote the number of members of $S$ landing in $T_{i}$ . Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\forall i\\in[n]:|u_{i}(S)-n_{Q}\\epsilon|\\le10\\sqrt{\\frac{\\log n}{n_{Q}}}\\right]\\ge1-n^{-8}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Notice that each $T_{i}$ has Lebesgue measure $\\epsilon$ by definition. Therefore $\\mathbb{E}[u_{i}(S)]=n_{Q}\\epsilon$ . Since $S$ has $n_{Q}$ members, setting $\\begin{array}{r}{t=10\\sqrt{\\frac{\\log n}{n_{Q}}}}\\end{array}$ in the statement of Lemma A.4 and taking a union bound over all $i\\in[n]$ gives the conclusion. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "We will decompose the error of Algorithm 1 into two parts. ", "page_idx": 16}, {"type": "text", "text": "Proposition A.6. Let $\\widehat{Q}\\in[0,1]^{n\\times n}$ be the estimator from Algorithm 1. Then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\|Q-\\widehat{Q}\\|_{F}^{2}\\leq\\frac{2}{n^{2}}\\sum_{i,j\\in[n]}(J_{S}(i,j)+J_{B}(i,j)),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $J_{S},J_{B}$ are the smoothing and Bernoulli errors respectively: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{J_{S}(i,j):=\\frac{1}{\\left|T_{i}\\right|^{2}\\left|T_{j}\\right|^{2}}\\left(\\sum_{r\\in T_{i},s\\in T_{j}}Q_{i j}-Q_{r s}\\right)^{2}};}}\\\\ {{\\displaystyle{J_{B}(i,j):=\\frac{1}{\\left|T_{i}\\right|^{2}\\left|T_{j}\\right|^{2}}\\left(\\sum_{r\\in T_{i},s\\in T_{j}}Q_{r s}-A_{Q;r s}\\right)^{2}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Controlling the Bernoulli errors is relatively straightforward. ", "page_idx": 16}, {"type": "text", "text": "Proposition A.7. Let h be the bandwidth of Algorithm $^{\\,l}$ . The Bernoulli error is at most $O({\\frac{\\log n}{m}})$ with probability $\\geq1-n^{-8}$ , where $m=h^{2}n_{Q}^{2}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Fix $i,j\\in[n]$ . We will bound the maximum Bernoulli error $J_{S}(i,j)$ over $i,j$ , which suffices to bound the average. Let $m=\\left|T_{i}\\right|\\left|T_{j}\\right|$ . We want to bound: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\frac{1}{|T_{i}|\\,|T_{j}|}\\sum_{r\\in T_{i},s\\in T_{j}}(Q_{r s}-A_{Q;r s})\\right|^{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Notice each summand is bounded within $\\pm\\frac{1}{m}$ . Bernstein\u2019s inequality gives: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left(\\frac{1}{|T_{i}|\\,|T_{j}|}\\sum_{r\\in T_{i},\\,s\\in T_{j}}Q_{r s}-A_{Q;r s}\\right)^{2}\\geq t^{2}\\right]\\leq2\\exp(-0.5t^{2}m).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Setting $\\textstyle t=C{\\sqrt{\\frac{\\log n}{m}}}$ for large enough $C\\,=\\,O(1)$ , a union bound tells us that with probability $\\geq1-n^{-8}$ , the Bernoulli error is bounded by $t^{2}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Corollary A.8. The Bernoulli error is at most $O(\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}})$ with probability \u22651 \u2212n\u2212Q4 . ", "page_idx": 16}, {"type": "text", "text": "The rest of this section is devoted to bounded the smoothing errors $J_{S}(i,j)$ . ", "page_idx": 16}, {"type": "text", "text": "A.2.1 Latent Distance to Graph Distance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We claim that if nodes are close in the latent space then they are close in graph distance. ", "page_idx": 17}, {"type": "text", "text": "Proposition A.9. Suppose that $\\lVert\\pmb{x}_{i}-\\pmb{x}_{r}\\rVert\\leq\\epsilon$ and $Q$ is $\\beta$ -smooth. Then $d_{Q}(i,r)\\leq C_{\\beta}^{2}n^{3}\\epsilon^{2(\\beta\\wedge1)}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. We the use smoothness of $Q$ . By definition there exists $C_{\\beta}\\,>\\,0$ such that $Q_{k i}\\mathrm{~-~}Q_{k r}\\mathrm{~}\\leq$ $C_{\\beta}\\|\\pmb{x}_{i}-\\pmb{x}_{r}\\|^{\\beta\\wedge1}$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{Q}(i,r)=\\displaystyle\\sum_{\\ell\\neq i,r}\\left|(Q^{2})_{\\ell i}-(Q^{2})_{\\ell r}\\right|^{2}}\\\\ &{\\qquad=\\displaystyle\\sum_{\\ell\\neq i,r}\\left(\\sum_{k\\in[n]}Q_{\\ell k}(Q_{k i}-Q_{k r})\\right)^{2}}\\\\ &{\\qquad\\leq\\displaystyle\\sum_{\\ell\\neq i,r}\\sum_{k\\in[n]}Q_{\\ell k}^{2}C_{\\beta}^{2}\\epsilon^{2(\\beta\\wedge1)}}\\\\ &{\\qquad\\leq n^{3}C_{\\beta}^{2}\\epsilon^{2(\\beta\\wedge1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We can now bound the minimum sizes of the neighborhoods using the concentration of latent positions and the smoothness of the graphon. ", "page_idx": 17}, {"type": "text", "text": "Lemma A.10 (Vershynin (2018)). The volume of a ball of radius $r>0$ in $\\mathbb{R}^{d}\\,i s\\;\\frac{\\sqrt{\\pi}^{d}}{\\Gamma(d/2+1)}r^{d}$ , where $\\Gamma(\\cdot)$ is the \u0393 function. ", "page_idx": 17}, {"type": "text", "text": "Proposition A.11. Let Cd = (\u0393( d2 + 1))1/d. Let C0, C\u2032 be constants. If \u03c5n \u2265C \u00b7 Cd( longQ n ) for large enough constant $C>0$ , and $g_{n}=C_{0}C_{\\beta}^{2}n^{2}(v_{n})^{2(\\beta\\wedge1)}$ , then with probability $\\geq1-n^{-6}$ for all $i\\in[n]$ the neighborhood size is $\\begin{array}{r}{|\\{r:d_{Q}(i,r)\\le g_{n}\\}|\\ge C^{\\prime}n_{Q}\\sqrt{\\frac{\\log n}{n_{Q}}}}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Proof. Fix $i\\;\\in\\;[n]$ and $\\upsilon_{n}\\,>\\,0$ . Let $\\epsilon_{i}$ denot\u221ae the Lebesgu\u221ae measure of $\\mathrm{Ball}(\\pmb{x}_{i},\\pmb{\\upsilon}_{n})\\cap\\mathcal{X}$ . By Lemma A.3 and Lemma A.10, for all $i$ , $\\begin{array}{r}{\\epsilon_{i}\\geq(\\frac{\\sqrt{\\pi}v_{n}}{2C_{d}})^{d}=(\\frac{0.5\\sqrt{\\pi}v_{n}}{C_{d}})^{d}}\\end{array}$ ( 0.5C\u03c0\u03c5n)d. Let \u03f5 = mini\u2208[n] \u03f5i. ", "page_idx": 17}, {"type": "text", "text": "By Corollary A.5, with probability $\\geq1-n^{-8}$ , there are $n_{Q}\\epsilon-C\\sqrt{\\frac{\\log n}{n_{Q}}}$ members $j$ of $S$ such that $\\|{\\pmb x}_{i}-{\\pmb x}_{j}\\|\\leq v_{n}$ . A union bound over $i$ gives the result simultaneously for all $i$ with probability \u22651 \u2212n\u22126. ", "page_idx": 17}, {"type": "text", "text": "From Proposition A.9, it follows that for all $i\\in[n]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\{r\\in S:d_{Q}(i,r)\\leq C_{\\beta}^{2}n^{2}(2v_{n}^{\\prime})^{2(\\beta\\wedge1)}\\}\\right|\\geq n_{Q}\\epsilon-10\\sqrt{\\frac{\\log n}{n_{Q}}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Choosing $\\begin{array}{r}{v_{n}\\geq C\\cdot C_{d}(\\frac{\\log n}{n_{Q}})^{\\frac{1}{2d}}}\\end{array}$ for large enough $C>0$ gives the conclusion. ", "page_idx": 17}, {"type": "text", "text": "A.2.2 Graph Distance Concentration ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Next, we show that the empirical graph distance concentrates to the population distance. ", "page_idx": 17}, {"type": "text", "text": "Proposition A.12. For any arbitrary symmetric $P\\in[0,1]^{n\\times n}$ , we have, for all $i,j$ simultaneously with probability at least $\\geq1-O(n^{-8})$ , that ", "page_idx": 17}, {"type": "equation", "text": "$$\n|d_{A_{P}}(i,j)-d_{P}(i,j)|\\leq O(n^{2}\\log n)+O(n^{2.5}\\sqrt{\\log n}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Fix $i,j$ . Let $C_{i j}:=(A_{P}^{2})_{i j}$ . By Ma\u221ao et al. (2021) A.1, we have $C_{i j}=(P^{2})_{i j}+t_{i j}$ for an error term $t_{i j}$ such that $\\mathbb{P}[\\forall i,j:|t_{i j}|\\le10\\sqrt{n\\log n}]\\ge1-n^{-10}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left|d_{A_{P}}(i,j)-d_{P}(i,j)\\right|=\\left|\\displaystyle\\sum_{\\ell\\neq i,j}\\left((C_{i\\ell}-C_{j\\ell})^{2}-((P^{2})_{i\\ell}-(P^{2})_{j\\ell})^{2}\\right)\\right|}\\qquad}&{}\\\\ &{=\\displaystyle\\sum_{\\ell\\neq i,j}\\left|(t_{i\\ell}+t_{j\\ell})^{2}+2(t_{i\\ell}+t_{j\\ell})((P^{2})_{i\\ell}-(P^{2})_{j\\ell})\\right|}\\\\ &{\\leq O(n^{2}\\log n)+O\\left(\\sqrt{n\\log n}\\displaystyle\\sum_{\\ell\\neq i,j}\\left((P^{2})_{i\\ell}-(P^{2})_{j\\ell}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, notice that all entries of $P^{2}$ are of size $O(n)$ , so the conclusion follows. ", "page_idx": 18}, {"type": "text", "text": "Finally, we will show that taking the restriction of the graph distance $T_{i}^{P}$ to nodes in $S\\subset[n]$ does not incur too much error. ", "page_idx": 18}, {"type": "text", "text": "Proposition A.13. Suppose n = nOQ(1). Then there exists a constant C such that if h0 \u2265C l $\\begin{array}{r}{h_{0}\\geq C\\sqrt{\\frac{\\log n}{n_{Q}}}+}\\end{array}$ $\\Delta_{n}$ , then for all $i,r$ simultaneously, $r\\in T_{i}^{A_{P}}(h_{0})$ implies $r\\in T_{i}^{P}(h_{2})$ for some $h_{2}=O(h)$ with probability $\\geq1-O(n^{-5})$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Let us introduce the notation $T_{i}^{P,S}(h)$ to denote the bottom $h$ -quantile of $\\{d_{P}(i,j):j\\in$ $S\\}$ . In this notation, $T_{i}^{A_{P}}(h)\\,:=\\,T_{i}^{A\\bar{P},S}(h)$ since we restrict the quantile to nodes in $S$ . From Proposition A.12 and Assumption 2.2, we know that if $n\\geq n_{Q}$ then for $\\begin{array}{r}{h_{0}\\leq h_{1}-20\\sqrt{\\frac{\\log n}{n}}-\\Delta_{n}}\\end{array}$ we have $T_{i}^{A_{P}}(h_{0})\\subseteq T_{i}^{P,S}(h_{1})$ simultaneously for all $i\\,\\in\\,[n]$ with probability $\\geq1-O(n^{-8})$ . It remains to compare $T_{i}^{P,S}(h_{1})$ with $T_{i}^{P}(h_{2})$ for some $h_{2}$ . ", "page_idx": 18}, {"type": "text", "text": "We claim that if $h_{2}\\geq30\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}$ then $\\mathbb{P}[\\forall i\\,\\big|T_{i}^{P}\\cap S\\big|\\ge h_{2}n_{Q}-3\\sqrt{n_{Q}\\log n_{Q}}]\\ge1-O\\big(n_{Q}^{-2}\\big)$ . To see this, fix $i\\in[n]$ and consider $T_{i}^{P}(h_{2})$ . For $j\\in S$ , let $X_{j}$ be the indicator variable: ", "page_idx": 18}, {"type": "equation", "text": "$$\nX_{j}=\\left\\{{1\\atop0}\\ \\ \\mathrm{if}\\ j\\in T_{i}^{P}(h_{2}),\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that $\\begin{array}{r}{\\left|T_{i}^{P}(h_{2})\\cap S\\right|=\\sum_{j\\in S}X_{j}}\\end{array}$ . By Hoeffding\u2019s inequality, since $\\mathbb{E}[\\sum_{j\\in S}X_{j}]=h_{2}n_{Q}$ and $|X_{j}-h_{2}|\\le1$ for all $j$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\left|T_{i}^{P}(h_{2})\\cap S\\right|-h_{2}n_{Q}\\right|\\geq3\\sqrt{n_{Q}\\log n}\\right]\\leq2\\exp\\left(-\\,\\frac{6n_{Q}^{2}\\log n}{n_{Q}^{2}}\\right)\\leq2n^{-6}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking a union bound over all $i~\\in~[n]$ shows the claim holds with probability $\\geq\\,1-O(n^{-5})$ .   \nTherefore we set $\\begin{array}{r}{h_{1}\\leq h_{2}-3.1\\sqrt{\\frac{\\log n}{n_{Q}}}}\\end{array}$ then $j\\in T_{i}^{P,S}(h_{1})$ implies $j\\in T_{i}^{P}(h_{2})$ . ", "page_idx": 18}, {"type": "text", "text": "The conclusion follows with $\\begin{array}{r}{C=24\\sqrt{\\frac{\\log n}{\\log n_{Q}}}=O(1)}\\end{array}$ . ", "page_idx": 18}, {"type": "text", "text": "The ranking condition (Definition 1.3) then allows us to translate between graph distances in $A_{P}$ and $Q$ . ", "page_idx": 18}, {"type": "text", "text": "eCnooruogllh acroyn sAt.a1n4t. .p oSsuep tphoaste l. dTs hfeonr $(P,Q)$ $\\begin{array}{r}{h_{n}=c\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}+\\Delta_{n},}\\end{array}$ logn nQ+ \u2206n, for large llows that $c>0$ $n_{Q}\\leq n\\leq n_{Q}^{O(1)}$ $h>h_{n}$ $r\\in T_{i}^{A_{P}}(h)$ $r\\in T_{i}^{Q}(h_{3})$ for some $h_{3}=O(h)$ . The statement holds simultaneously for all $i,r$ with probability $\\geq1-\\dot{O}(n^{-5})$ . ", "page_idx": 18}, {"type": "text", "text": "A.2.3 Control of Smoothing Error ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We will decompose smoothing error into a sum of two terms called $E_{S,1}$ and $E_{S,2}$ . The control of $E_{S,1}$ is relatively straightforward. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.15. The total smoothing error can be bounded with two terms: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{2}{n^{2}}\\sum_{i,j\\in[n]}J_{S}(i,j)\\leq E_{S,1}+E_{S,2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{S,1}:=\\displaystyle\\frac{C}{n}\\operatorname*{max}_{j\\in[n],s\\in T_{j}}\\|Q(e_{j}-e_{s})\\|_{2}^{2};}\\\\ {E_{S,2}:=\\displaystyle\\frac{4}{n^{2}}\\sum_{i\\in[n]}\\frac{1}{|T_{i}|}\\,\\mathbb{E}\\left[\\displaystyle\\sum_{r\\in T_{i}}\\sum_{j\\in[n]}\\sum_{s\\in T_{j}}(Q_{r j}-Q_{r s})^{2}.\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. Note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2}{n^{2}}\\displaystyle\\sum_{i,j\\in[n]}J_{S}(i,j)=\\frac{2}{n^{2}}\\displaystyle\\sum_{i,j\\in[n]}\\frac{1}{|T_{i}|^{2}\\,|T_{j}|^{2}}\\,\\mathbb{E}\\left[\\left(\\displaystyle\\sum_{r\\in T_{i},\\,s\\in T_{j}}Q_{i j}-Q_{r s}\\right)^{2}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{2}{n}\\sum_{i\\in[n]}\\frac{1}{n\\,|T_{i}|}\\displaystyle\\sum_{j\\in[n]}\\frac{2}{|T_{j}|}\\,\\mathbb{E}\\left[\\sum_{r\\in T_{i},\\,s\\in T_{j}}(Q_{i j}-Q_{r j})^{2}+(Q_{r j}-Q_{r s})^{2}\\right]}\\\\ &{\\qquad\\qquad=\\displaystyle\\frac{4}{n}\\sum_{i\\in[n]}\\frac{1}{n\\,|T_{i}|}\\,\\mathbb{E}\\left[\\sum_{j}\\frac{1}{|T_{j}|}\\left(\\displaystyle\\sum_{r\\in T_{i}}(Q_{i j}-Q_{r j})^{2}+\\sum_{r\\in T_{i}}\\sum_{s\\in T_{j}}(Q_{r j}-Q_{r s})^{2}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second inner summand is precise $E_{S,2}$ . For $E_{S,1}$ , notice that $|T_{i}|\\,=\\,|T_{j}|\\,=\\,h(n_{Q}\\,-\\,1)$ by definition. Therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{j}\\frac{1}{|T_{j}|}\\sum_{r\\in T_{i}}(Q_{i j}-Q_{r j})^{2}=\\frac{1}{h(n_{Q}-1)}\\sum_{r\\in T_{i}}\\sum_{j}(Q_{i j}-Q_{r j})^{2}\\leq2\\operatorname*{max}_{r\\in T_{i}}\\|(e_{i}-e_{r})^{T}Q\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We can now bound $E_{S,1}$ in terms of graph distances. ", "page_idx": 19}, {"type": "text", "text": "Lemma A.16. The smoothing error term $E_{S,1}$ can be bounded as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\nE_{S,1}\\leq\\frac{2}{n}\\operatorname*{max}_{i\\in[n],r\\in T_{i}}\\sqrt{d_{Q}(i,r)}+\\frac{2c}{\\sqrt{n}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some constant $c>0$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Fix $i\\in[n]$ and $r\\in T_{i}$ . We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Q(e_{i}-e_{r})\\|_{2}^{2}\\leq\\|e_{i}-e_{r}\\|_{2}\\|Q^{T}Q(e_{i}-e_{r})\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\|Q^{2}(e_{i}-e_{r})\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now we will pass to graph distances. Let $e_{a b}:=((Q^{2})_{a a}-(Q^{2})_{a b})^{2}$ for $a,b\\in[n]$ . Notice that $\\|Q^{2}(e_{i}-e_{r})\\|_{2}=\\sqrt{d_{Q}(i,r)+e_{i r}+e_{r i}}$ . Moreover, ${\\sqrt{e_{i r}+e_{r i}}}\\leq2{\\sqrt{n}}$ since the entries of $Q^{2}$ are individually bounded by $O(n)$ . The conclusion follows. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Proposition A.17. Suppose $\\begin{array}{r}{\\Delta_{n}=O(\\sqrt{\\frac{\\log n}{n_{Q}}})}\\end{array}$ . Let $C_{d}$ be the constant of Proposition A.11. Then if the bandwidth of Algorithm $^{\\,l}$ is $\\begin{array}{r}{h_{n}=C\\sqrt{\\frac{\\log n}{n_{Q}}}}\\end{array}$ , for a constant $C=O(1)$ , then the smoothing error $E_{S,1}$ is at most ", "page_idx": 19}, {"type": "equation", "text": "$$\nE_{S,1}\\leq C_{2}C_{d}^{\\beta\\wedge1}\\left(\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}\\right)^{\\frac{\\beta\\wedge1}{d}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $C_{2}=O(1)$ , with probability $\\geq1-O(n^{-6})$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Fix i \u2208[n] and r \u2208T iAP(hn). By Corollary A.14, if hn \u2265C longQ n for a large enough constant $C>0$ , then there exists constant $C_{2}>0$ such that the following holds. With probability $\\geq1-O(n^{-5})$ , for all $i\\in[n]$ and $r\\in S$ , $r\\in T_{i}^{Q}(C_{2}h_{n})$ , ", "page_idx": 20}, {"type": "text", "text": "$\\begin{array}{r}{v_{n}=C C_{d}(\\sqrt{\\frac{\\log n}{n_{Q}}})^{1/d}}\\end{array}$ for $C_{d}$ as in Proposition A.11 and $C>0$ large enough constant. Then by Proposition A.11 the set of $s\\,\\in\\,S$ such that $d_{Q}(i,r)\\,\\leq\\,C_{0}C_{\\beta}^{2}n^{2}(v_{n})^{2(\\beta\\wedge1)}$ has size at least $C_{2}n_{Q}\\sqrt{\\frac{\\log n}{n_{Q}}}$ .The statement holds for all $i$ simultaneously with probability at least $1-O(n^{-6})$ . Therefore for all $i\\in[n]$ and $r\\in T_{i}^{A_{P}}(h_{n})$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nd_{Q}(i,r)\\leq C_{0}C_{\\beta}^{2}n^{2}(v_{n})^{2(\\beta\\wedge1)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some $C_{0},C_{\\beta}=O(1)$ , with probability $\\geq1-O(n^{-6})$ . By Lemma A.16 we conclude that $E_{S,1}$ is bounded by $\\textstyle2v_{n}^{\\beta\\wedge1}+{\\frac{2}{\\sqrt{n}}}$ with the same probability. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "A.2.4 Control of the Second Smoothing Error ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In this section, we show that the second smoothing error can be controlled in terms of $E_{S,1}$ . We will need to track the following quantity. ", "page_idx": 20}, {"type": "text", "text": "Definition A.18 (Membership Count). For $r\\,\\in\\,S$ and bandwidth $h$ , distance cutoff $\\epsilon$ , the $P$ - neighborhood count of $r$ is $\\psi\\dot{P(r)}:=\\left|\\{j\\in[n]:r\\in T_{j}^{P}(h,\\epsilon)\\}\\right|$ . ", "page_idx": 20}, {"type": "text", "text": "In words, $\\psi_{P}(r)$ counts the number of nodes $j\\in[n]$ such that $r$ lands in the neighborhood of $j$ in our algorithm. While we know that $\\left|T_{j}^{P}(h)\\right|\\leq h n_{Q}$ always, simply applying the pigeonhole principle gives too weak of a bound on membership counts. The base case is that there may be a \u201chub\u201d node $r$ lands in $T_{j}^{P}(h)$ for all $j$ . We will show that there can be no such hub node. ", "page_idx": 20}, {"type": "text", "text": "Supposing that we can control of the empirical count $\\psi_{A_{P}}$ , we show that the smoothing error can be bounded. ", "page_idx": 20}, {"type": "text", "text": "Proposition A.19. Let $h_{n}$ be the bandwidth. Then ", "page_idx": 20}, {"type": "equation", "text": "$$\nE_{S,2}\\leq O\\bigg(\\frac{E_{S,1}}{h_{n}n}\\bigg)\\cdot\\operatorname*{max}_{r\\in[n]}(\\psi_{A_{P}}(r)).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Rearranging terms, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{S,2}=\\frac{1}{n^{2}h^{2}n_{Q}^{2}}\\displaystyle\\sum_{i,j\\in[n],r\\in T_{i},s\\in T_{j}}(Q_{r j}-Q_{r s})^{2}}\\\\ &{\\qquad=\\frac{1}{n^{2}h^{2}n_{Q}^{2}}\\displaystyle\\sum_{r\\in S}\\psi_{A r}\\left(r\\right)\\sum_{j,s}(Q_{r j}-Q_{r s})^{2}}\\\\ &{\\qquad=\\frac{n Q}{n^{2}h^{2}n_{Q}^{2}}\\displaystyle\\sum_{r\\in S}\\mathbb{E}\\left[\\psi_{A r}(r)\\sum_{j,s}(Q_{r j}-Q_{r s})^{2}\\right]}\\\\ &{\\qquad=\\frac{n Q}{n^{2}h^{2}n_{Q}^{2}}\\displaystyle\\sum_{r\\in[n]}\\mathbb{E}_{\\mathit{a}_{P}}\\left[\\psi_{A r}(r)\\sum_{j,s}(Q_{r j}-Q_{r s})^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last step follows because $j,s$ do not depend on $i,r$ and because $S\\subset[n]$ is chosen uniformly at random. Now, we will control the expectation by passing to a row sum, which is handled by $E_{S,1}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{lig}_{r\\in[n]}\\bigg[\\psi_{A_{P}}(r)\\sum_{j,s}(Q_{r j}-Q_{r s})^{2}\\bigg]\\leq\\operatorname*{max}_{r\\in[n]}\\bigg(\\frac{\\psi_{A_{P}}(r)}{n}\\bigg)\\cdot\\sum_{j\\in[n]}\\sum_{s\\in T_{j}}\\|Q(e_{j}-e_{s})\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Recall that $n^{2}n_{Q}h_{n}E_{S,1}=\\Omega\\bigg(\\sum_{j\\in[n]}\\sum_{s\\in T_{j}}\\|Q(e_{j}-e_{s})\\|_{2}^{2}\\bigg).$ . Hence we conclude that ", "page_idx": 20}, {"type": "equation", "text": "$$\nE_{S,2}\\leq O\\bigg(\\frac{E_{S,1}}{h_{n}n}\\bigg)\\cdot\\operatorname*{max}_{r\\in[n]}(\\psi_{A_{P}}(r)).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We therefore must show that $\\operatorname*{max}_{r\\in S}\\psi_{A_{P}}(r)\\leq O(h n)$ with high probability. ", "page_idx": 21}, {"type": "text", "text": "Proposition A.20 (Population Version). Suppose Assumption 2.2 holds for $P$ with $c_{1}<c_{2}$ and $\\Delta_{n}\\,=\\,O\\big((\\frac{\\log n}{n_{Q}})^{\\frac{1}{2}\\bigvee\\frac{\\alpha\\wedge1}{d}}\\big)$ . Then if $\\begin{array}{r}{h\\leq C\\sqrt{\\frac{\\log n}{n_{Q}}}}\\end{array}$ for large enough constant $C>0$ , then we have $\\operatorname*{max}_{r\\in S}\\psi_{P}(r)\\leq O(h n)$ with probability at least $1-O(n_{Q}^{-8})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Fix $r\\in S$ . Let $C_{d}$ be as in Proposition A.11.Suppose that $\\begin{array}{r}{\\epsilon=C_{d}(C+10)\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}^{1/d}}\\end{array}$ and $\\begin{array}{r}{h=C\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}}\\end{array}$ . Now, we will claim that for large enough constant $c>0$ , that $\\psi_{P}(r)$ is at most the size of $\\operatorname{Ball}(\\pmb{x}_{r},c\\epsilon)\\cap\\{\\pmb{x}_{1},\\ldots,\\pmb{x}_{n}\\}$ . ", "page_idx": 21}, {"type": "text", "text": "Suppose that $c>0$ is a large enough constant. Now suppose that $\\pmb{x}_{j}$ is such that $\\|{\\pmb x}_{j}-{\\pmb x}_{r}\\|\\geq c\\epsilon$ . We can lower bound the graph distance using Assumption 2.2, as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{P}(r,j):=\\|(e_{r}-e_{j})^{T}P^{2}(I-e_{r}e_{r}^{T}-e_{j}e_{j}^{T})\\|_{2}^{2}\\geq c_{1}n^{3}(c\\epsilon)^{2(\\alpha\\wedge1)}-n^{3}\\Delta_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "On the other hand, suppose that $i\\in S$ is such that $\\|{\\pmb x}_{i}-{\\pmb x}_{j}\\|\\leq\\epsilon$ . Then $d_{P}(i,j)\\leq C_{\\alpha}^{2}n^{3}\\epsilon^{2(\\alpha\\wedge1)}$ by Proposition A.9. Therefore since $\\begin{array}{r}{\\epsilon=C_{d}(C+10)\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}^{1/d}}\\end{array}$ and $\\Delta_{n}=O\\big((\\frac{\\log n}{n_{Q}})^{\\frac{1}{2}\\bigvee\\frac{\\alpha\\wedge1}{d}}\\big)$ , for large enough $c_{1}>0$ we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nd_{P}(r,j):=\\|(e_{r}-e_{j})^{T}P^{2}(I-e_{r}e_{r}^{T}-e_{j}e_{j}^{T})\\|_{2}^{2}\\geq\\frac{c_{1}}{2}n^{3}(c\\epsilon)^{2(\\alpha\\wedge1)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then, if we choose $c>0$ such that $\\begin{array}{r}{c^{2(\\alpha\\wedge1)}>\\frac{2C_{\\alpha}^{2}}{c_{1}}}\\end{array}$ , then $d_{P}(i,j)<d_{P}(r,j)$ ", "page_idx": 21}, {"type": "text", "text": "Next, from our choices of $h,\\epsilon$ , by Corollary A.5, simultaneously for all $i\\in[n]$ there are at least $h n_{Q}$ nodes in $S$ that have distance $\\leq\\epsilon$ in latent space from $\\pmb{x}_{i}$ , with probablity $\\geq1-O(n_{Q}^{-6})$ . ", "page_idx": 21}, {"type": "text", "text": "Therefore, if $\\pmb{x}_{r}\\,\\,\\notin\\,\\mathrm{Ball}(\\pmb{x}_{j},c\\epsilon)\\cap\\{\\pmb{x}_{1},\\dots,\\pmb{x}_{n}\\}$ then $r\\,\\not\\in\\,T_{j}^{P}(h)$ . This implies that $\\psi_{P}(r)\\;\\leq$ $|\\{\\mathrm{Ball}(\\mathbf{x}_{r},2c\\epsilon)\\cap\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\}|$ . We can bound the size of this ball with Lemma A.4. Notice the Lebesgue measure of $\\mathrm{Ball}({\\pmb x}_{r},2c\\epsilon)\\cap[0,1]$ is at most $\\big(\\frac{4c\\epsilon}{C_{d}}\\big)^{d}$ . Therefore, since $\\pmb{x}_{i}$ are chosen iid from the Lebesgue measure on $\\mathcal{X}$ , with probability at least $\\geq\\hat{1}-O(n_{Q}^{-10})$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\left|\\mathrm{Ball}(\\pmb{x}_{r},2c\\epsilon)\\cap\\left\\{\\pmb{x}_{1},\\dots,\\pmb{x}_{n}\\right\\}\\right|\\leq2c\\epsilon+10\\sqrt{\\frac{\\log n}{n}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The right-hand side is bounded by $O(h)$ if $n\\geq n_{Q}$ . Taking a union bound over all $r\\in S$ gives the conclusion. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "We conclude with the desired upper bound. ", "page_idx": 21}, {"type": "text", "text": "Proposition A.21 (Bound on $\\psi_{A_{P}}(r))$ . Suppose Assumption 2.2 holds for $P$ with $c_{1}\\ <\\ c_{2}$ and \u2206n = O(( longQ n )21 \u2228\u03b1d\u22271 ). Then if h \u2264C0 logn QnQ for small enough constant $C_{0}$ , then we have $\\operatorname*{max}_{r\\in S}\\psi_{A_{P}}(r)\\leq O(h n)$ with probability at least $1-O(n_{Q}^{-8})$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. By Proposition A.12, with probability at least $1-O(n_{Q}^{-8})$ , we have for all $r\\in S,j\\in[n]$ simultaneously that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{A_{P}}(r,j)\\geq d_{P}(r,j)-O(n^{2.5}\\sqrt{\\log n})}\\\\ &{\\qquad\\qquad\\geq(1-O(\\frac{1}{\\sqrt{n}}))d_{P}(r,j).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, $\\begin{array}{r}{d_{A_{P}}(r,j)\\leq(1+O(\\frac{1}{\\sqrt{n}}))d_{P}(r,j)}\\end{array}$ . We conclude that $\\psi_{A_{P}}(r)\\leq2\\psi_{P}(r)=O(h n)$ with probability $\\geq1-O(n_{Q}^{-8})$ . \u53e3 ", "page_idx": 21}, {"type": "text", "text": "A.2.5 Overall Error", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We can bound $\\begin{array}{r}{C_{d}:=\\Gamma(\\frac{d}{2}+1)^{1/d}}\\end{array}$ with the elementary inequality. ", "page_idx": 22}, {"type": "text", "text": "Lemma A.22. Let $\\begin{array}{r}{C_{d}:=\\Gamma(\\frac{d}{2}+1)^{1/d}}\\end{array}$ . Then $C_{d}\\leq\\sqrt{d/2}$ . ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 2.3. By Proposition A.21 and Prop A.19, we have that $E_{S,1}\\,\\leq\\,O(E_{S,1})$ with probability at least $1-O(n_{Q}^{-8})$ . Therefore by Proposition A.17, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[E_{S,1}+E_{S,2}\\leq O\\bigg(C_{d}^{\\beta\\wedge1}\\bigg(\\frac{\\log n}{n_{Q}}\\bigg)^{\\frac{\\beta\\wedge1}{2d}}\\bigg)\\right]\\geq1-O(n_{Q}^{-6}).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Lemma A.22, $C_{d}\\;\\leq\\;\\sqrt{d/2}$ . Finally, by Corollary A.8, the Bernoulli error is bounded by $O(\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}})$ with probability $\\geq1-O(n_{Q}^{-4})$ . Applying a union bound over the two kinds of error and Lemma A.15 gives the result. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "A.3 Proof of Theorem 3.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Recall the Gilbert-Varshamov code (Guruswami et al., 2019). ", "page_idx": 22}, {"type": "text", "text": "Theorem A.23 (Gilbert-Varshamov). Let $q\\geq2$ be a prime power. For $\\begin{array}{r}{0<\\epsilon<\\frac{q-1}{q}}\\end{array}$ there exists an $\\epsilon$ -balanced code $C\\subset\\mathbb{F}_{q}^{n}$ with rate $\\Omega(\\epsilon^{2}n)$ . ", "page_idx": 22}, {"type": "text", "text": "We will use the following version of Fano\u2019s inequality. ", "page_idx": 22}, {"type": "text", "text": "Theorem A.24 (Generalized Fano Method, Yu (1997)). Let $\\mathcal{P}$ be a family of probability measures, $(\\mathcal{D},d)$ a pseudo-metric space, and $\\theta:\\mathcal{P}\\rightarrow\\mathcal{D}$ a map that extracts the parameters of interest. For $a$ distinguished $P\\in\\mathcal P$ , let $X\\sim P$ be the data and ${\\widehat{\\theta\\,}}:={\\widehat{\\theta\\,}}(X)$ be an estimator for $\\theta(P)$ . ", "page_idx": 22}, {"type": "text", "text": "Let $r\\geq2$ and $\\mathcal{P}_{r}\\subset\\mathcal{P}$ be a finite hypothesis class of size $r$ . Let $\\alpha_{r},\\beta_{r}>0$ be such that for all $i\\neq j$ , and all $P_{i},P_{j}\\in\\mathcal P_{r}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d(\\theta(P_{i}),\\theta(P_{j}))\\ge\\alpha_{r};}\\\\ {K L(P_{i},P_{j})\\le\\beta_{r}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{j\\in[r]}\\mathbb{E}_{P_{j}}[d(\\widehat{\\theta}(X),\\theta(P_{j}))]\\geq\\frac{\\alpha_{r}}{2}\\bigg(1-\\frac{\\beta_{r}+\\log2}{\\log r}\\bigg).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Definition A.25 (Relative Hamming Distance). For $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{y}}\\in\\{0,1\\}^{m}$ , we define their relative Hamming distance as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\nd_{H}(\\pmb{x},\\pmb{y}):=\\frac{1}{m}\\left|\\left\\{i\\in[m]:x_{i}\\neq y_{i}\\right\\}\\right|.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We will need the following construction of coupled codes. ", "page_idx": 22}, {"type": "text", "text": "Proposition A.26. Let $m_{P}$ $_{P},m_{Q}\\ge2$ and $m_{Q}$ divide $m_{P}$ . There exists a code $C\\subset\\{0,1\\}^{m_{P}}$ and a projection map $\\Pi:\\{0,1\\}^{m_{P}}\\rightarrow\\{0,1\\}^{m_{Q}}$ such that if $C^{\\prime}=\\{\\Pi(w):w\\in C\\}$ then $C^{\\prime}$ is a code with relative Hamming distance $\\Omega(1)$ . Moreover, $|C|=|C^{\\prime}|\\geq2^{0.1m_{Q}}$ ", "page_idx": 22}, {"type": "text", "text": "Throughout the proof, we will identify the community assignment function $z:[n]\\to[k]$ of an SBM (Definition 3.1) with the matrix $Z\\in\\{0,1\\}^{n\\times k}$ where $Z_{i j}=1$ if and only if $z(i)=j$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Begin with a Gilbert-Varshamov code $B\\subset\\{0,1\\}^{m_{Q}}$ as in Theorem A.23. We can \u201clift\u201d $B$ to a code on $\\{0,1\\}^{m_{P}}$ simply by concatenation. If $w\\in B$ , then the corresponding $w^{\\prime}\\in C$ is just $w^{\\prime}=(w,w,\\dots,w)\\in\\{0,1\\}^{m_{P}}$ . Let $\\Pi:\\{0,1\\}^{m_{P}}\\rightarrow\\{0,1\\}^{m_{Q}}$ simply select the first $m_{Q}$ bits of a word. It is clear that $B\\stackrel{}{=}\\{\\Pi(w):w\\in C\\}$ , so we are done. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Now we are ready to prove Theorem 3.2. ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 3.2. Let $m_{P}={\\binom{n}{2}}$ , $m_{Q}={\\binom{n_{Q}}{2}}$ , and $m=m_{P}$ . Let $C\\subset\\{0,1\\}^{m_{P}}$ be the code and $\\Pi:\\{0,1\\}^{m_{P}}\\rightarrow\\{0,1\\}^{m_{Q}}$ the projection map of Prop A.26. For each $w\\in C$ , we construct a pair of SBMs $P_{w},Q_{w}\\in\\mathbb{R}^{n\\times n}$ as follows. ", "page_idx": 23}, {"type": "text", "text": "community structure, namely the lexicographic assignment where nodes Each $P_{w},Q_{w}$ is a stochastic block model with $k_{P},k_{Q}$ classes respectively. All the $1,2,\\ldots,{\\frac{n}{k_{P}}}$ $P_{w}$ are assigned to share the same community 1, and so on. Similarly all the $Q_{w}$ share the same lexicographic community structure with nodes 1, 2, . . . , knQ assigned to community 1, and so on. Therefore, there are fixed $Z_{P}\\ \\in$ $\\{0,1\\}^{n\\times k_{P}},Z_{Q}\\in\\{0,\\underline{{{1}}}\\}^{n\\times k_{Q}}$ , such that for all $w\\in C$ , there exist $A_{w}\\in\\mathbb{R}^{k_{P}\\times k_{P}},B_{w}\\in\\mathbb{R}^{k_{Q}\\times k_{Q}}$ with ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{w}=Z_{P}A_{w}Z_{P}^{T},}\\\\ {Q_{w}=Z_{Q}B_{w}Z_{Q}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The $A_{w},B_{w}$ are defined as follows. Let $i,j\\in[k_{P}]$ and $i^{\\prime},j^{\\prime}\\in[k_{Q}]$ be such that $i<j$ and $i^{\\prime}<j^{\\prime}$ . Since $m_{P_{\\!\\_}}=\\binom{k_{P}}{2}$ and $m_{Q}\\,=\\,\\left({\\}_{\\!\\!\\begin{array}{c}{{2}}\\end{array}}^{\\not k_{Q}}\\right)$ , we can identify $(i,j)$ and $(i^{\\prime},j^{\\prime})$ with indices of $[m_{P}],[m_{Q}]$ respectively. Then for fixed $\\delta_{P},\\bar{\\delta_{Q}}>0$ , the edge connectivity probabilities are ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{A_{w}(i,j)=A_{w}(j,i):=\\binom{1/2}{1/2+\\delta_{P}}\\quad\\mathrm{if~}w_{i j}=0,}\\\\ {B_{w}(i^{\\prime},j^{\\prime})=B_{w}(j^{\\prime},i^{\\prime}):=\\binom{1/2}{1/2+\\delta_{Q}}\\quad\\mathrm{if~}\\Pi(w)_{i^{\\prime}j^{\\prime}}=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We can set the diagonals of $A_{w},B_{w}$ to be $1/2$ as well. ", "page_idx": 23}, {"type": "text", "text": "Next, let ${\\mathcal P}_{r}$ be a family of $r=|C|$ probability measures. For fixed $w\\in C$ , the corresponding measure is the distribution over data $(\\dot{A}_{P};\\dot{A}_{Q})\\in\\{0,1\\}^{n\\times n}\\times\\{0,1\\}^{n_{Q}\\times n_{Q}}$ sampled from $(P_{w},\\bar{Q}_{w}[S,S])$ . Note that we restrict $S$ to be a fixed subset of $[n]$ . ", "page_idx": 23}, {"type": "text", "text": "Next, let $\\theta((P_{w},Q_{w})):=Q_{w}.$ , and let $\\begin{array}{r}{d(\\theta((P_{w},Q_{w})),\\theta((P_{w^{\\prime}},Q_{w^{\\prime}}))):=\\frac{1}{n}\\|Q_{w}-Q_{w^{\\prime}}\\|_{F}}\\end{array}$ . We will show that for all $w,w^{\\prime}\\in C$ with $w\\ne w^{\\prime}$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K L((P_{w},Q_{w}),(P_{w^{\\prime}},Q_{w^{\\prime}}))\\leq K L(P_{w},P_{w^{\\prime}})+K L(Q_{w},Q_{w^{\\prime}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq O(n^{2}\\delta_{P}^{2}+n_{Q}^{2}\\delta_{Q}^{2})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=:\\beta,}\\\\ &{d((P_{w},Q_{w}),(P_{w^{\\prime}},Q_{w^{\\prime}})):=\\cfrac{1}{n}\\|Q_{w}-Q_{w^{\\prime}}\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\Omega(\\delta_{Q})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=:\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the $\\beta$ claim, by Proposition 4.2 of Gao et al. (2015), if $\\delta_{P},\\delta_{Q}\\in(0,1/4)$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K L((P_{w},Q_{w}),(P_{w^{\\prime}},Q_{w^{\\prime}}))\\le K L(P_{w},P_{w^{\\prime}})+K L(Q_{w},Q_{w^{\\prime}})}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\displaystyle\\sum_{i,j\\in[n]}(P_{w}(i,j)-P_{w^{\\prime}}(i,j))^{2}+(Q_{w}(i,j)-Q_{w^{\\prime}}(i,j))^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Next, notice that $A_{w}(i,j)\\neq A_{w^{\\prime}}(i,j)$ if and only if $w_{i j}\\neq w_{i j}^{\\prime}$ . Then for distinct $w,w^{\\prime}\\in C$ , we have $d_{H}(w,w^{\\prime})=\\Omega(m_{P})$ , so ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{i,j\\in[n]}(P_{w}(i,j)-P_{w^{\\prime}}(i,j))^{2}\\lesssim\\delta_{P}^{2}\\frac{n^{2}}{k_{P}^{2}}d_{H}(w,w^{\\prime})\\binom{k_{P}}{2}\\lesssim\\delta_{P}^{2}n^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The bound for $Q_{w}$ is similar, so this verifies the $\\beta$ claim. ", "page_idx": 23}, {"type": "text", "text": "Similarly, for the $\\alpha$ claim, notice that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\|Q_{w}-Q_{w^{\\prime}}\\|_{F}\\gtrsim\\frac{1}{k_{Q}}\\sqrt{\\delta_{Q}^{2}d_{H}(\\Pi(w),\\Pi(w^{\\prime}))}\\geq\\frac{\\delta_{Q}}{k_{Q}}\\sqrt{d_{H}(\\Pi(w),\\Pi(w^{\\prime}))}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Prop A.26, $d_{H}(\\Pi(w),\\Pi(w^{\\prime}))=\\Omega(m_{Q})=\\Omega(k_{Q}^{2})$ . Therefore $\\alpha\\leq\\Omega(\\delta_{Q})$ . ", "page_idx": 24}, {"type": "text", "text": "Next, by Prop A.27, the pair $(P_{w},Q_{w})$ satisfies Definition 1.3 for all $w\\in C$ . Moreover, $\\log\\left|C\\right|\\geq$ $0.1m_{Q}$ by Prop A.26. ", "page_idx": 24}, {"type": "text", "text": "Combining these results, by Theorem A.24 the overall lower bound is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{inf}_{\\hat{Q}}\\operatorname*{sup}_{w}\\frac{1}{n}\\|\\widehat{Q}-Q_{w}\\|_{F}\\gtrsim\\alpha\\bigg(1-\\frac{\\beta+\\log2}{0.1\\binom{k_{Q}}{2}}\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq\\delta_{Q}\\bigg(1-\\frac{30n^{2}\\delta_{P}^{2}}{k_{Q}^{2}}-\\frac{30n_{Q}^{2}\\delta_{Q}^{2}}{k_{Q}^{2}}-o(1)\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If we choose $\\begin{array}{r}{\\delta_{P}=0.01(\\frac{k_{Q}}{n})}\\end{array}$ and $\\begin{array}{r}{\\delta_{Q}=0.01\\frac{k_{Q}}{n_{Q}}}\\end{array}$ , then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\widehat{Q}}{\\operatorname*{inf}}\\operatorname*{sup}_{w}\\frac{1}{n^{2}}\\|\\widehat{Q}-Q_{w}\\|_{F}^{2}\\gtrsim\\delta_{Q}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\gtrsim\\frac{k_{Q}^{2}}{n_{Q}^{2}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Note that $k_{Q}\\leq n_{Q}\\leq n$ , so $\\delta_{P},\\delta_{Q}\\in(0,1/4)$ as desired. ", "page_idx": 24}, {"type": "text", "text": "Proposition A.27. If $\\begin{array}{r}{h_{n}=\\operatorname*{min}\\{\\frac{1}{k_{P}},\\frac{1}{k_{Q}}\\}}\\end{array}$ then for all $w\\in C$ , the pair $(P_{w},Q_{w})$ satisfies Defn 1.3 at $h_{n}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Consider $h=h_{n}$ and some node $i\\in[n]$ . Suppose that $j\\neq i$ is in the same $P_{w}$ -community as $i$ , and that $\\ell\\neq i$ is in a different community. Then notice that $d_{P_{w}}(i,\\ell)\\ge d_{P_{w}}(i,j)$ . Therefore $j\\in T_{i}^{P_{w}}(h)$ . Moreover, since $\\begin{array}{r}{h\\leq\\frac{1}{k_{P}}}\\end{array}$ and since the nodes of $S\\subset[n]$ are equidistributed among the communities $1,2,\\ldots,k_{P}$ , it follows that all members of $T_{i}^{P_{w}}(h)$ must belong to the same $P_{w}$ -community as $i$ . ", "page_idx": 24}, {"type": "text", "text": "Therefore, since the communities of $Q_{w}$ are a coarsening of the communities of $\\begin{array}{r}{P_{w},j\\in T_{i}^{Q_{w}}(\\frac{1}{k_{Q}})}\\end{array}$ . Since $\\begin{array}{r}{h\\leq\\frac{1}{k_{Q}}}\\end{array}$ , we are done. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "A.4 SBM Clustering Error ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we prove a minimax lower bound in the clustering regime for stochastic block models. ", "page_idx": 24}, {"type": "text", "text": "Theorem A.28. Let \u03a0 denote the parameter space of pairs of SBMs $(P,Q)$ on n nodes with $k_{P},k_{Q}$ communities respectively, such that the cluster structure of $Q$ is a coarsening the cluster structure of $P$ . Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{Q}}\\ \\operatorname*{sup}_{(P,Q)\\in\\Pi}\\mathbb{E}[\\frac{1}{n^{2}}\\|\\widehat{Q}-Q_{i}\\|_{F}^{2}]\\gtrsim\\frac{\\log k_{Q}}{n_{Q}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. Let $H_{m}\\,\\in\\,[0,1]^{m\\times m}$ be the Hadamard matrix of order $m$ modified to replace all entries $-1$ with 0. If $m$ is not a power of two, let $H_{m}$ be defined as follows. Let $\\ell=\\lfloor\\log_{2}m\\rfloor$ and let $H_{m^{\\prime}}\\in\\mathbb{R}^{m/2\\times m/2}$ contain $H_{2^{\\ell-1}}$ on its top left block and zeroes elsewhere. Let ", "page_idx": 24}, {"type": "equation", "text": "$$\nH_{m}=\\left[\\!\\!\\begin{array}{c c}{\\mathbf{00}^{T}}&{H_{m^{\\prime}}}\\\\ {H_{m^{\\prime}}^{T}}&{\\mathbf{00}^{T}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Notice that at most $\\frac{7}{8}$ fraction of the entries of $H_{m}$ are zero-padded, for any $m$ . Now, let $B_{P}=$ $\\scriptstyle{\\frac{1}{2}}\\mathbf{1}\\mathbf{1}^{T}\\,+\\,\\delta_{P}H_{k_{P}}$ and $\\begin{array}{r}{B_{Q}=\\frac{1}{2}\\mathbf{1}\\mathbf{1}^{T}+\\delta_{Q}H_{k_{Q}}}\\end{array}$ for some $\\delta_{P},\\delta_{Q}\\in(0,1/4)$ to be chosen later. ", "page_idx": 24}, {"type": "text", "text": "We will define two families of matrices indexed by a finite set $T$ . For $i\\in T$ , there are some $Z_{i}\\in\\{0,1\\}^{n\\times k_{P}}$ and $Y_{i}\\in\\{0,1\\}^{n\\times k_{Q}}$ to be specified later. Then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{i}=Z_{i}B_{P}Z_{i}^{T},}\\\\ {Q_{i}=Y_{i}B_{Q}Y_{i}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, we define $Y_{i}$ as follows. Let $Z_{n,k_{Q}}$ denote the set of balanced clusterings $z\\,:\\,[n]\\,\\rightarrow\\,[k_{Q}]$ such that for all $i,j\\in[k_{Q}]$ $_{Q}],\\,|z^{-1}(\\{i\\})|\\,\\,\\dot{=}\\,|z^{-1}(\\{j\\})|$ . Let $Z\\subset Z_{n,k_{Q}}$ select the $z$ such that for $\\begin{array}{r}{j\\le k_{Q}/2,z^{-1}(j)=\\big\\{\\left\\lfloor\\frac{n(j-1)}{k_{Q}}\\right\\rfloor,\\cdot\\cdot\\cdot,\\left\\lfloor\\frac{n j}{k_{Q}}\\right\\rfloor\\big\\}}\\end{array}$ . Define a distance on $Z$ as follows. For $y,y^{\\prime}\\in Z$ let $Y,Y^{\\prime}\\in\\{0,1\\}^{n\\times k_{Q}}$ be the corresponding cluster matrices and let $\\begin{array}{r}{d(y,y^{\\prime})\\,:=\\,\\frac{1}{n}\\|Y B_{Q}Y^{T}\\,-\\,}\\end{array}$ $Y^{\\prime}B_{Q}(Y^{\\prime})^{T}\\|_{F}$ . By Theorem 2.2 of Gao et al. (2015), there exists a packing $T_{0}\\subset Z$ with respect to $d$ such that for all $y,y^{\\prime}\\in T_{0}$ , we have $|\\{j:y^{\\prime}(j)\\neq y(j)\\}|\\geq n/6$ . Moreover, $\\begin{array}{r}{\\log|T_{0}|\\geq\\frac{1}{12}n\\log k_{Q}}\\end{array}$ . Set $T=T_{0}$ . For any $y_{i}\\in T_{0}$ , let $Y_{i}\\,\\in\\,\\{0,1\\}^{n\\times k_{Q}}$ be the corresponding cluster matrix and then $Q_{i}=Y_{i}B_{Q}Y_{i}^{T}$ . ", "page_idx": 25}, {"type": "text", "text": "Now, to define $Z_{i}$ , take $a\\in[k_{Q}]$ and partition $y_{i}^{-1}(\\{a\\})\\subset[n]$ into $k_{P}/k_{Q}$ equally sized communities in a uniformly random way. Number these 1, . . . , kkPQ . In this way, we split community 1 of yi into communities 1, . . . , kkPQ of $z_{i}$ , and so on. Define $Z_{i}$ to be the matrix corresponding to $z_{i}$ . Notice that $Z_{i},Y_{i}$ are both balanced clusterings and that the clustering $Y_{i}$ coarsens that of $Z_{i}$ . Therefore $(P_{i},Q_{i})$ are a pair of heterogeneous symmetric SBMs satisfying Definition 1.3 at $h=1/k_{Q}$ . ", "page_idx": 25}, {"type": "text", "text": "Next, we apply Fano\u2019s Inequality (Theorem A.24). Recall $\\begin{array}{r}{\\log|T|\\geq\\frac{1}{12}n\\log k_{Q}}\\end{array}$ . Now, for $i,j\\in T$ distinct, Prop 4.2 of Gao et al. (2015) gives ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{K L}\\bigl((P_{i},Q_{i}),(P_{j},Q_{j})\\bigr)\\leq D_{K L}\\bigl(P_{i},P_{j}\\bigr)+D_{K L}\\bigl(Q_{i},Q_{j}\\bigr)\\leq O\\bigl(n^{2}\\delta_{P}^{2}+n_{Q}^{2}\\delta_{Q}^{2}\\bigr)=:\\gamma_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, we can bound: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\quad}&{\\displaystyle\\frac{1}{n^{2}}\\|Q_{i}-Q_{i^{\\prime}}\\|_{F}^{2}\\geq\\frac{1}{n^{2}}\\sum_{n/2<j\\leq n}\\frac{n}{k_{Q}}\\|(e_{y_{i}(j)}-e_{y_{i}^{\\prime}(j)})B_{Q}\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\geq c_{0}\\delta_{Q}^{2}=:\\gamma_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $c_{0}>1$ is some constant. This follows because there are a constant fraction of $j>n/2$ such that $y_{i}(j)\\neq y_{i}^{\\prime}(j)$ , and any two rows of the Hadamard matrix differ on half their entries. ", "page_idx": 25}, {"type": "text", "text": "Now, set $\\begin{array}{r}{\\delta_{Q}^{2}=\\frac{n_{Q}\\log k_{Q}}{10n_{Q}^{2}}}\\end{array}$ Qand \u03b42P $\\delta_{P}^{2}={\\frac{\\log k_{Q}}{10n^{2}}}$ lo1g0 nk2Q . Since n \u2265nQ, we conclude that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{inf}_{\\hat{Q}}\\operatorname*{sup}_{i\\in T}\\mathbb{E}\\left[\\frac{1}{n^{2}}\\|\\widehat{Q}-Q_{i}\\|_{F}^{2}\\right]\\gtrsim\\gamma_{2}^{2}\\Bigg(1-\\frac{\\gamma_{1}+\\log2}{(1/12)n\\log k_{Q}}\\Bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\gtrsim\\frac{\\log k_{Q}}{n_{Q}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "A.5 Proof of Proposition 3.4 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We first argue that Algorithm 2 perfectly recovers $Z_{P},Z_{Q}$ with high probability. ", "page_idx": 25}, {"type": "text", "text": "Theorem A.29 (Implicit in Chen et al. (2014)). Let $M=Z B Z^{T}$ be an $(n,n_{m i n},s)$ -HSBM. Then there exists absolute constant $C>0$ such that the Algorithm of Chen et al. (2014) can recover $Z$ , up to permutation, with zero error with probability $\\geq1-O(n^{-\\tilde{8}})\\;i f$ ", "page_idx": 25}, {"type": "equation", "text": "$$\ns\\geq C{\\left({\\frac{\\sqrt{n}}{n_{\\operatorname*{min}}}}\\vee{\\frac{\\log^{2}(n)}{\\sqrt{n_{\\operatorname*{min}}}}}\\right)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The algorithm of Chen et al. (2014) returns a matrix $Y\\in\\{0,1\\}^{n\\times n}$ such that $Y_{i j}=1$ if and only if $i,j$ are in the same community, with probability $\\geq1-O(n^{-8})$ . Therefore, to construct a clustering from $Y$ , simply assign the cluster of node 1 to all $j\\in[n]$ such that $Y_{1j}=1$ , and so on. This returns the true $Z\\in\\{0,1\\}^{n\\times k}$ up to permutation with probability $\\geq1-O(n^{-8})$ . Note that $k$ is correctly chosen because $Y$ is equal to a block-diagonal matrix of ones up to permutation, with $k$ blocks. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Theorem A.29 implies the following. ", "page_idx": 25}, {"type": "text", "text": "Proposition A.30. Let $\\widehat{Z}_{P},\\widehat{Z}_{Q}$ be as in Algorithm 2. Let $s_{P},s_{Q}$ be the signal to noise ratios of $P,Q$ respectively. If $s_{P},s_{Q}$ satisfy the conditions of Theorem $A.29$ with respect to $(n,n_{\\mathrm{min}}^{(P)}$ and $(n_{Q},n_{\\operatorname*{min}}^{(Q)})$ respectively, then then with probability $\\geq1-O(n_{Q}^{-8})$ , there are permutation matrices $U_{P}\\in\\{0,1\\}^{k_{P}\\times k_{P}},U_{Q}\\in\\{0,1\\}^{k_{Q}\\times k_{Q}}$ such that $\\widehat{Z}_{P}=Z_{P}U_{P}$ and $\\widehat{Z}_{Q}=Z_{Q}U_{Q}$ . ", "page_idx": 26}, {"type": "text", "text": "Next, we want to recover the clustering of $Q$ on all $n$ nodes, not just the $n_{Q}$ nodes that we observe in $A_{Q}$ . This is given by the following. ", "page_idx": 26}, {"type": "text", "text": "Proposition A.31. 1. If $h_{n}=1/k_{P}$ and $k_{Q}\\leq k_{P}$ then there exists a unique $\\Pi\\in\\{0,1\\}^{k_{P}\\times k_{Q}}$ such that $Z_{P}\\Pi$ contains the $Q$ -clustering of all nodes in $[n]$ . Let $\\widetilde{Z}_{Q}:=Z_{P}\\Pi$ . ", "page_idx": 26}, {"type": "text", "text": "2. Let\u03a0  be as in Algorithm 2 and UP , UQ be as in Proposition A.30. Then with probability 1\u2212O( n1Q ), $Z_{P}U_{P}\\widehat{\\Pi}=\\widetilde{Z}_{Q}U_{Q}.$ . ", "page_idx": 26}, {"type": "text", "text": "Proof. Part (1) follows immediately from the SBM structure of $P,Q$ and definition of Definition 1.3. For Part (2), first notice that by Proposition A.30, with probability at least $\\textstyle1-O({\\frac{1}{n_{Q}}})$ , Algorithm 2 returns the true clusterings $\\widehat{Z}_{P}=Z_{P}\\in\\{0,1\\}^{n\\times k_{P}}$ and $\\widehat{Z}_{Q}=Z_{Q}\\in\\{0,1\\}^{n_{Q}\\times k_{Q}}$ , up to permutation. ", "page_idx": 26}, {"type": "text", "text": "Now, Algorithm 2 simply takes unions of the clusters of $Z_{P}$ to learn $\\widehat{\\Pi}$ . Therefore, let $V:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{n_{Q}}$ project onto coordinates in $S$ . Then $V\\widehat{Z}_{P}\\widehat{\\Pi}=\\widehat{Z}_{Q}$ . Moreover, by Proposition A.30, $\\widehat{Z}_{P}=Z_{P}U_{P}$ and $\\widehat{Z}_{Q}=Z_{Q}U_{Q}$ . Hence $V Z_{P}U_{P}\\widehat{\\Pi}=V\\widetilde{Z}_{Q}U_{Q}$ . To remove dependence on $V$ , we need to argue that  each $Q$ -cluster has a reprensen tatve in $S$ . ", "page_idx": 26}, {"type": "text", "text": "Let $E$ be the event that at least one $Q$ -cluster has no representative in $S$ . For a fixed $j\\in[k_{Q}]$ , cluster $\\leq\\left(1-\\frac{n_{\\operatorname*{min}}^{(Q)}}{n_{Q}}\\right)^{n_{Q}}$ 2 $j$ has no representative in $S$ with probability . A union bound implies that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{P}[E]\\leq k_{Q}\\bigg(1-\\frac{n_{\\operatorname*{min}}^{(Q)}}{n_{Q}}\\bigg)^{n_{Q}}\\leq k_{Q}\\exp(-n_{\\operatorname*{min}}^{(Q)})\\leq O\\big(n_{Q}^{-1}\\big).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The last inequality holds because the condition of Theorem A.29 implies that n(mQi)n $n_{\\mathrm{min}}^{(Q)}\\geq\\Omega(\\sqrt{n_{Q}})$ and $k_{Q}\\leq\\frac{n_{Q}}{n_{\\operatorname*{min}}^{(Q)}}$ . ", "page_idx": 26}, {"type": "text", "text": "Finally, we proceed by conditioning on $\\neg E$ . Since $\\widehat{Z}_{P}=Z_{P}U_{P}$ , we know that for all $i\\in S$ , the unique $j_{P}\\in[k_{P}]$ such that row $i$ , column $j_{P}$ of $Z_{P}$ is nonzero contains its true $P$ -community up to $U_{P}$ . Similarly since $\\widehat{Z}_{Q}=Z_{Q}U_{Q}$ , the the unique $j_{Q}\\in[k_{Q}]$ such that row $i$ , column $j_{Q}$ of $Z_{P}$ is nonzero contains its true $Q$ -community up to $U_{Q}$ . Therefore the nodes in community $j_{P}$ in $P$ are in community $j_{Q}$ in $Q$ . So, up to permutations $U_{P}$ and $U_{Q}$ , we have $\\Pi_{j_{P},j_{Q}}=1$ . Since we condition on $\\neg E$ , each cluster of $Q$ has at least one representative in $S$ , so each columns of $\\Pi$ is nonzero. We conclude that $Z_{P}U_{P}\\widehat{\\Pi}=\\widetilde{Z}_{Q}U_{Q}$ with probability at least $1-O(n_{Q}^{-1})$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "We are ready to give the overall error of Proposition 2. ", "page_idx": 26}, {"type": "text", "text": "Proposition A.32. Suppose that $\\widehat{Z}_{P}\\;=\\;Z_{P},\\widehat{\\Pi}\\;=\\;\\Pi$ in Algorithm 2. Then with probability $\\geq$ $\\textstyle1-O({\\frac{1}{n_{Q}}})$ , Algorithm 2 returns a $\\widehat{Q}\\in[0,1]^{n\\times n}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\|\\widehat{Q}-Q\\|_{F}^{2}\\lesssim\\frac{k_{Q}^{2}\\log(n_{\\operatorname*{min}}^{(Q)})}{n_{Q}^{2}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. By Proposition A.31, with probability $\\textstyle\\geq1-O({\\frac{1}{n_{Q}}})$ , we have $\\widehat{Z}_{P}=Z_{P}U_{P}$ , $\\widehat{Z}_{Q}=Z_{Q}U_{Q}$ , and $\\widetilde{Z}_{Q}U_{Q}=Z_{P}U_{P}\\widehat{\\Pi}$ . We proceed by conditioning on these events. ", "page_idx": 26}, {"type": "text", "text": "Next, let $W_{Q}\\in\\mathbb{R}^{k_{Q}\\times k_{Q}}$ be the population version of $\\widehat{W}_{Q}$ with $W_{Q;i i}=(\\mathbf{1}^{T}Z_{Q}\\pmb{e}_{i})^{-1}$ . Then since $\\widehat{Z}_{Q}=Z_{Q}U_{Q}$ we have $\\widehat{W}_{Q}=U_{Q}^{T}W_{Q}U_{Q}$ . Hence ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{Q}=(Z_{P}U_{P}\\widehat\\Pi)(U_{Q}^{T}W_{Q}U_{Q}^{T})(Z_{Q}U_{Q})^{T}A_{Q}(Z_{Q}U_{Q})(U_{Q}^{T}W_{Q}U_{Q})(Z_{P}U_{P}\\widehat\\Pi)^{T}}\\\\ &{\\quad=\\widetilde{Z}_{Q}(W_{Q}Z_{Q}^{T}A_{Q}Z_{Q}W_{Q})\\widetilde{Z}_{Q}^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, let $z_{Q}:[n]\\rightarrow[k_{Q}]$ be the ground truth clustering map given by $\\widetilde{Z}_{Q}\\in\\{0,1\\}^{n\\times k_{Q}}$ . Let $B_{Q}$ be defined analogously to $\\widehat{B}_{Q}$ in Algorithm 2, but using $W_{Q},Z_{Q},\\mathbb{E}[A_{Q}]$ in place of $\\widehat{W}_{Q},\\widehat{Z}_{Q},A_{Q}$ . Let $m_{i}:=W_{Q;i i}^{-1}$ be the the  number of nodes in $S$ belong to community $i$ , and let $n_{i}\\mathrm{be}$ the t he number of nodes in $[n]$ belonging to community of $Q$ . Then the error of Algorithm 2 is then ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{1}{n^{2}}\\|\\tilde{Z}_{Q}(\\widehat{B}_{Q}-B_{Q})\\tilde{Z}_{Q}^{T}\\|_{F}^{2}=\\frac{1}{n^{2}}\\bigg(\\sum_{i,j\\in[k_{Q}]}n_{i}n_{j}\\bigg(\\sum_{r\\in z_{Q}^{-1}(\\{i\\})\\cap S}\\frac{B_{Q;i j}-A_{Q;r s}}{m_{i}m_{j}}\\bigg)^{2}\\bigg)}}\\\\ &{}&{=\\frac{1}{n^{2}}\\sum_{i,j\\in[k_{Q}]}\\frac{n_{i}n_{j}}{m_{i}^{2}m_{j}^{2}}\\bigg(\\sum_{r\\in z_{Q}^{-1}(\\{i\\})\\cap S}B_{Q;i j}-A_{Q;r s}\\bigg)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, fix $i,j\\in[k_{Q}]$ and let ", "page_idx": 27}, {"type": "equation", "text": "$$\nX_{i j}=\\sum_{r\\in z_{Q}^{-1}(\\{i\\})\\cap S}B_{Q;i j}-A_{Q;r s}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "If we condition on the clusterings of $P,Q$ being correct then $\\mathbb{E}[B_{Q;i j}-A_{Q;r s}]=0$ . Therefore by Hoeffding\u2019s inequality, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}(X_{i j}\\geq t^{2})\\leq2\\exp\\bigg(-\\frac{2t^{2}}{m_{i}m_{j}}\\bigg).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Setting $t^{2}=10\\log(m_{i}m_{j})m_{i}m_{j}$ implies that with probability at least $1-k_{Q}^{2}\\operatorname*{min}_{i}(m_{i})^{-20}$ , that the overall error is ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\frac{1}{n^{2}}\\|\\widehat{Q}-Q\\|_{F}^{2}\\leq\\frac{1}{n^{2}}\\sum_{i,j\\in[k_{Q}]}\\frac{10\\log(m_{i}m_{j})n_{i}n_{j}}{m_{i}m_{j}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, n\u221aote that there exists a constant $c_{0}~>~0$ such that for all $i\\;\\in\\;[k_{Q}],\\;m_{i}\\;\\geq\\;c_{0}\\sqrt{n_{Q}}$ and $n_{i}\\,\\geq\\,c_{0}{\\sqrt{n}}$ , by assumption. Note that each $m_{i}$ is a random quantity depending on the choice of $S\\subset[n]$ such that $\\begin{array}{r}{\\mathbb{E}[m_{i}]=\\frac{n_{Q}}{n}n_{i}}\\end{array}$ . Hoeffding\u2019s inequality and a union bound over all $i\\in[k_{Q}]$ imply that that with probability at least $\\geq1-O(n_{Q}^{-8})$ that $m_{i}\\geq\\mathbb{E}[m_{i}]-10\\sqrt{\\log n_{Q}}\\geq\\Omega(\\mathbb{E}[m_{i}])$ . We conclude that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{n^{2}}\\|\\widehat{Q}-Q\\|_{F}^{2}\\leq O\\bigg(\\frac{1}{n_{Q}^{2}}\\displaystyle\\sum_{i,j\\in[k_{Q}]}10\\log(m_{i}m_{j})\\bigg)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq O\\bigg(\\frac{k_{Q}^{2}\\log(n_{\\operatorname*{min}}^{(Q)})}{n_{Q}}\\bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "B Additional Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "B.1 Ablation Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we discuss additional experiments that quantify the dependence of our algorithms on all relevant parameters. Our experiments also include a new baseline adapted from the estimator of Levin et al. (2022). ", "page_idx": 27}, {"type": "text", "text": "Description of New Baseline. Levin et al. (2022) assumes that full edge data from both $\\mathbf{P}$ and $\\mathrm{Q}$ are observed, and $P=Q$ . Since this is not true for us, we instead compute the following modified MLE based on their estimator from Section 3.3 of Levin et al. (2022). ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{Q}_{i j}=\\left\\{\\frac{w_{P}}{w_{P}+w_{Q}}A_{P;i j}+\\frac{w_{Q}}{w_{P}+w_{Q}}A_{Q;i j}\\right.\\mathrm{~if~}i,j\\in S,}\\\\ {A_{P;i j}\\qquad\\qquad\\qquad\\qquad\\left.\\mathrm{~otherwise.}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Here $w_{P},w_{Q}$ are computed as in their paper, based on estimated sub-gamma parameters of the noise for $A_{P},A_{Q}$ . Akin to their adjacency spectral embedding, which assumes known rank of $Q$ , we use Universal Singular Value Thresholding to obtain $\\widehat{Q}$ from $\\widetilde{Q}$ Chatterjee (2015). ", "page_idx": 28}, {"type": "text", "text": "Oracle with $p=0.0.$ . In addition to testing the new baseline from Levin et al. (2022), we also test the Oracle baseline with $p=0.0$ . As noted in Section 4, this corresponds to the non-transfer setting where all edges from the target graph $Q$ are observed. Note that in this case, the value of $n_{Q}$ does not matter because edges incident to nodes outside of $S$ never get filpped. The Oracle error for $\\beta$ -smooth graphons on $d$ -dimensional latent variables will therefore be $O\\big(n^{-\\frac{2\\beta}{2\\beta+d}}\\big)\\,\\mathrm{Xu}\\,(2018)$ , which is less than the error bound of Theorem 2.3. Indeed, we will find that the Oracle our transfer algorithms in the regimes where its theoretical upper bound is better than our theoretical upper bounds. ", "page_idx": 28}, {"type": "text", "text": "Next, we describe the experimental results. ", "page_idx": 28}, {"type": "text", "text": "Figure 3 tests Algorithm 1 for general latent variable models. The error (Theorem 2.3) depends on the smoothness $\\beta$ of the target graph, the number of observed target nodes $n_{Q}$ , and the dimension of the latent variables $d$ . ", "page_idx": 28}, {"type": "text", "text": "Figure 4 tests Algorithm 2 for Stochastic Block Models. The error (Proposition 3.4) depends on the number of communities $k_{Q}$ in the target graph, and the number of observed target nodes $n_{Q}$ . Note that Proposition 3.4 also depends logarithmically on the minimum community size of $Q$ , but this is less significant. ", "page_idx": 28}, {"type": "image", "img_path": "PK8xOCBQRO/tmp/8b2a737249aa18e68d55f189e6e795db3f80f5e9457085a7cc5c57a20f4adc96.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "Figure 3: Testing parameters of Algorithm 1 (Transfer for Latent Variable Models). For most parameter settings, our method is better than the baseline and worse than the Oracle. ", "page_idx": 28}, {"type": "text", "text": "Left: Testing H\u00f6lder-smoothness of $f_{Q}$ with $n=200,n_{Q}=25,d=1$ . All methods improve as \u03b2 \u21921. Here fP (x, y) = x\u03b12+y\u03b1, fQ(x, y) = x\u03b22+y\u03b2 with $\\alpha=0.01$ and $\\beta$ varying. ", "page_idx": 28}, {"type": "text", "text": "Middle: Testing number of observed target nodes $n_{Q}$ with $n\\,=\\,200,d\\,=\\,1$ . Here $f_{P}(x,y)\\,=$ $\\textstyle{\\frac{x^{\\alpha}+y^{\\alpha}}{2}}$ , $\\begin{array}{r}{f_{Q}(x,y)\\,=\\,\\frac{x^{\\beta}+y^{\\beta}}{2}}\\end{array}$ with $\\alpha=0.01,\\beta=0.1$ . Note that the oracle does not depend on $n_{Q}$ because it observes the full adjacency matrix $A_{Q}\\in\\{0,1\\}^{n\\times n}$ . ", "page_idx": 28}, {"type": "text", "text": "Right: Testing dimension $d$ of latent positions $x_{1},\\ldots,x_{n}\\,\\in\\,[0,1]^{d}$ (i.i.d. Lebesgue) with $n=$ 2 $)0,n_{Q}=25$ . Here $f_{P}(\\mathbf{x},\\pmb{y})=\\exp(-6\\|\\pmb{x}-\\pmb{y}\\|_{2})$ and $f_{Q}(\\pmb{x},\\pmb{\\dot{y}})=\\bar{\\exp}(-\\,|x_{1}-y_{1}|)$ . Points are the median MSE across 50 trials, with with [5, 95] percentile outcomes shaded. ", "page_idx": 28}, {"type": "image", "img_path": "PK8xOCBQRO/tmp/b3d80190293c2de7ea91dce36d701b32e23f0d44b6b1972a0255988da8574109.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 4: Testing parameters of Algorithm 2 (Transfer for SBMs). For most parameter settings, our method is better than the baseline and worse than the Oracle. ", "page_idx": 29}, {"type": "text", "text": "Left: $n=200,k_{P}=12,k_{Q}=6.$ Note that the oracle does not depend on $n_{Q}$ because it observes the full adjacency matrix $\\dot{A_{Q}}\\in\\{0,1\\}^{n\\times n}$ . ", "page_idx": 29}, {"type": "text", "text": "For both experiments, the intra-community edge probabilities are 0.2, 0.9 for $P,Q$ respectively, while the inter-community edge probabilities are $0.1,0.8$ respectively. Points are the median MSE across 50 trials, with with [5, 95] percentile outcomes shaded. ", "page_idx": 29}, {"type": "text", "text": "Note that while we can plot theoretical guarantees for the mean squared error $\\frac{1}{n^{2}}||\\widehat{Q}-Q||_{F}^{2}$ of both our algorithms\u2019 $\\widehat{Q}$ and the oracle\u2019s $\\widehat{Q}$ , Levin et al. (2022) only give theoretical guarantees on the spectral norm $||\\widehat{Q}-Q||_{2}$ for their estimator $\\widehat{Q}$ . Analyzing the stronger metric of mean-squared error would require different techniques than their paper. ", "page_idx": 29}, {"type": "text", "text": "B.2 Link Prediction Experiments ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we present additional link prediction experiments on the EMAIL-EU and BIGG MODELS datasets. Unlike Section 4, we tune the sparsity estimate ${\\widehat{\\rho}}\\in(0,1)$ used in the Universal Singular Value Thresholding step of the Oracle baseline. In part i cular, we set ${\\widehat{\\rho}}\\,\\in\\,(0,1)$ to be the mean of the entries of the ground truth target matrix $Q\\,\\in\\,[0,1]^{n\\times n}$ . Note  t hat this value is inaccessible to other algorithms since it requires knowing all the edges of $Q$ . ", "page_idx": 29}, {"type": "text", "text": "Figures 7 and 8 show the performance of our Algorithms on the EMAIL-EU dataset, and Figures 5 and 6 for the BIGG MODELS dataset. As in the mean-squared error setting (Figure 2), we find that Algorithm 1 outperforms Algorithm 2, and that the Oracle baseline outperforms both for small $p$ . Moreover, we find that the choice of source & target affects the performance of both of our algorithms. Hence Figure 7 shows better performance than Figure 8 for the same source but different targets, and Figure 5 shows better performance than Figure 6 for the same target but different sources. ", "page_idx": 29}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we give further details on the experiments of Section 4. ", "page_idx": 29}, {"type": "text", "text": "Compute Environment. We run all experiments on a personal Linux machine with 378GB of CPU/RAM. The total compute time across all results in the paper was less than 2 hours. ", "page_idx": 29}, {"type": "text", "text": "Functions for Figure 1. For the top row, the source is an $(n,4)$ -SBM with 0.8 on the diagonal and 0.2 on the off-diagonal of $B\\in\\mathbb{R}^{4\\times4}$ . The target is an $(n,2)$ -SBM with 0.9 on the diagonal and 0.1 on the off-diagonal of $B\\in\\mathbb{R}^{2\\times2}$ . ", "page_idx": 29}, {"type": "image", "img_path": "PK8xOCBQRO/tmp/09ac2788d0a45738c38fa6e466b0d417868faa853f87cd6b098a0d035204cadd.jpg", "img_caption": ["Metabolic Network Link Prediction (Source $=$ iWFL1372) "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 5: Link prediction results with the metabolic network of BiGG model iWFL1372 (Escherichia coli W) as the source and iJN1463 (Pseudomonas putida) the target. Shaded regions denote [5, 95] percentile outcomes from 50 independent trials. ", "page_idx": 30}, {"type": "image", "img_path": "PK8xOCBQRO/tmp/19245743658073142be1cd67c3de71f3aaf6f0279cb9fb6143f4a3550d7f7698.jpg", "img_caption": ["Metabolic Network Link Prediction (Source $=$ iPC815) "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 6: Link prediction results with the metabolic network of BiGG model iPC815 (Yersinia pestis) as the source and iJN1463 (Pseudomonas putida) the target. Shaded regions denote [5, 95] percentile outcomes from 50 independent trials. ", "page_idx": 30}, {"type": "image", "img_path": "PK8xOCBQRO/tmp/0f066eed60184fac4e3d52ccf70ee28346157c519187e1c54d518764fd214211.jpg", "img_caption": ["Email-EU Link Prediction (Target $=$ Days 81-160) "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "Figure 7: Link prediction results with Days 1-80 of EMAIL-EU as the source, and Days 81-160 as target. Shaded regions denote [5, 95] percentile outcomes from 50 independent trials. ", "page_idx": 30}, {"type": "text", "text": "For the second and third rows, the source function is $\\begin{array}{r}{Q(x,y)\\,=\\,\\frac{1+\\sin(\\pi(1+3(x+y-1)))}{2}}\\end{array}$ (modified from Zhang et al. (2017)). The sources are $P(x,y)=1-Q(x,y)$ and $P(x,y)^{2}{=}\\,Q(\\phi(x),\\stackrel{\\ldots}{y})$ , where $\\phi(x)=0.5+|x-0.5|$ if $x<0.5$ , and $0.5-|x-0.5|$ otherwise. ", "page_idx": 30}, {"type": "text", "text": "Metabolic Networks. We access metabolic models from King et al. (2016) at http://bigg.ucsd. edu. To construct a reasonable set of shared metabolites across the networks, we take the intersection of the node sets for the following BiGG models: iCHOv1, IJN1463, iMM1415, iPC815, iRC1080, ", "page_idx": 30}, {"type": "image", "img_path": "PK8xOCBQRO/tmp/a1d57b65f994ea2b236648e1a475b4d99405137c1f69dfc1075eef7b5425e541.jpg", "img_caption": ["Email-EU Link Prediction (Target $=$ Days 561-640) "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 8: Link prediction results with Days 1-80 of EMAIL-EU as the source, and Days 561-640 as target. Shaded regions denote [5, 95] percentile outcomes from 50 independent trials. ", "page_idx": 31}, {"type": "text", "text": "iSDY1059, iSFxv1172, iYL1228, iYS1720, and Recon3D. We obtain a set of $n=251$ metabolites that are present in all of the listed models. ", "page_idx": 31}, {"type": "text", "text": "The resulting networks are undirected, unweighted graphs on 251 nodes. We construct the matrix $A_{P}\\in\\{0,1\\}^{\\bar{n}\\times n}$ for species $P$ by setting $A_{P;u v}=1$ if and only if $u$ and $v$ co-occur in a metabolic reaction in the BiGG model for $P$ . ", "page_idx": 31}, {"type": "text", "text": "EMAIL-EU. We use the \u201cemail-EU-core-temporal\u201d dataset at https://snap.stanford.edu/ data/email-Eu-core-temporal.html, as introduced in Paranjape et al. (2017). Note that we do not perform any node preprocessing, so we use all $n=1005$ nodes present in the data, as opposed to Leskovec and Krevl (2014); Paranjape et al. (2017) who use only 986 nodes. ", "page_idx": 31}, {"type": "text", "text": "Data consist of triples $(u,v,t)$ where $u,v$ are anonymized individuals and $t>0$ is a timestamp. We split the data into 10 bins based on equally spaced timestamp percentiles. For simplicity we refer to these time periods as consisting of 80 days each in Section 4, but technically there are 803 days total. The network at time period $\\ell$ consists of an unweighted undirected graph with adjacency matrix entry $A_{u v}=1$ if and only if $(u,v,t)$ or $(v,t,u)$ occurred in the data for an appropriate timestamp $t$ . ", "page_idx": 31}, {"type": "text", "text": "Hyperparameters. We do not tune any hyperparameters. For Algorithm 1 we use the quantile cutoff $h_{n}={\\sqrt{\\frac{\\log n_{Q}}{n_{Q}}}}$ logn nQ in all experiments. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 32}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 32}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 32}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 32}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 32}, {"type": "text", "text": "IMPORTANT, please: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: We discuss our main contributions in the abstract and introduction, and then develop them in the main body of the paper. Specifically, we point out Algorithm 1 and Algorithm 2, the upper bounds from Theorem 2.3 and Proposition 3.4, and the lower bound Theorem 3.2. Finally, Section 4 shows experimental results. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the limitation of lacking a minimax lower bound for network estimation in $d$ -dimensions after Theorem 2.3, the need for a different graph distance for graphs with sparsity $O({\\frac{\\log n}{n}})$ in the Conclusion, and adapting to multiple source distributions in the Conclusion. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We give self-contained theoretical statements with definitions and assumptions in the main body, and full proofs for all claims in Appendix A. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We give experimental details in Section 4, and give further details on our exact data preparation methods in Appendix C. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: We submit our code as a supplementary zip flie in accordance with the NeurIPS code and data submission guidelines. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We describe our choices of source and target network, the network sizes and degrees, and the single hyperparameter (quantile cutoff) for Algorithm 1 in Section 4. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: For each experiment we run 50 independent trials. We report $\\pm2$ standard deviations for the mean-squared error in Table 1, and [1, 99] percentile errors in Figure 2. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: As described in Appendix C, we run all experiments on a personal Linux machine with 378GB of CPU/RAM. The total compute time across all results in the paper was less than 2 hours. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics with regards to potential harms caused by the research process, societal impact, and impact mitigation measures. Moreover, we have anonymized our code and manuscript. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper presents work whose goal is to advance the field of Machine Learning through foundational research. There are many possible societal consequences of our work, none which we feel must be specifically highlighted here. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not release data or models that have a high risk for misuse. We use only publicly available data from the academic literature. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We credit the authors of the BiGG metabolic models dataset (King et al., 2016; Norsigian et al., 2020) and the EMAIL-EU dataset (Leskovec and Krevl, 2014; Paranjape et al., 2017). Both datasets are publicly released for academic research. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: We do not release new assets. See Appendix C for the exact details regarding our use of existing datasets and assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not perform any crowdsourcing or research on human subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: We do not perform any research on living subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]