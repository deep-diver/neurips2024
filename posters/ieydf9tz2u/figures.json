[{"figure_path": "ieYdf9TZ2u/figures/figures_1_1.jpg", "caption": "Figure 1: As a foundational generative framework, we demonstrate Lumina-Next's capabilities to generate high-resolution images, multi-view images, general audio and music, and 16K point clouds.", "description": "This figure showcases the versatility of the Lumina-Next framework by demonstrating its ability to generate various types of media, including high-resolution images (2K resolution examples of an Inka warrior and a swan are shown), multi-view images (an example of a figurine is given), audio and music, and point clouds (a blue chair is shown as a point cloud example). This highlights the framework's capacity to operate as a foundational generative model across different data modalities.", "section": "Introduction"}, {"figure_path": "ieYdf9TZ2u/figures/figures_3_1.jpg", "caption": "Figure 2: Visualization of attention score using (a) 1D ROPE and (b) 2D ROPE on images. We set the central point in the image as the anchor query.", "description": "This figure compares the attention scores generated by 1D ROPE and 2D ROPE on images. The central point of the image is selected as the anchor query, and the attention scores are visualized for both methods. The comparison shows that 1D ROPE does not capture the spatial relationships between the different positions in the image, while 2D ROPE accurately reflects these relationships.", "section": "2.1 Architecture of Next-DiT"}, {"figure_path": "ieYdf9TZ2u/figures/figures_4_1.jpg", "caption": "Figure 3: Sandwich normalization effectively controls activation magnitudes over layers.", "description": "The figure shows the effectiveness of sandwich normalization in controlling the growth of activation magnitudes across layers in a neural network.  The plots illustrate how the mean and maximum activation values remain relatively stable across layers when sandwich normalization is used, preventing the uncontrolled growth observed without this technique. This stability is crucial for training large, deep networks and enhancing their overall performance.", "section": "2.1 Architecture of Next-DiT"}, {"figure_path": "ieYdf9TZ2u/figures/figures_5_1.jpg", "caption": "Figure 6: (a) Toy illustration of RoPE embeddings' wavelength with different extrapolation strategies. (b)-(g) Results of different resolution extrapolation strategies of 2K generation.", "description": "This figure demonstrates a comparison of different resolution extrapolation strategies for a 2K image generation task.  Subfigure (a) shows a toy example illustrating the wavelength of RoPE embeddings under different extrapolation methods: extrapolation, interpolation, NTK-aware, frequency-aware, and time-aware scaled ROPE. Subfigures (b) through (g) present visual results of these different strategies applied to a 1k image, demonstrating their respective impacts on image quality and the ability to extrapolate to higher resolutions.  Note the visual differences, particularly concerning repetition artifacts and detail preservation.", "section": "2.2 Improving NTK-Aware Scaled RoPE with Frequency- and Time-Awareness"}, {"figure_path": "ieYdf9TZ2u/figures/figures_6_1.jpg", "caption": "Figure 7: Comparison of few-step generation using different time schedules with Euler's method.", "description": "This figure compares the results of generating images using different time schedules (Uniform, Rational, Sigmoid) with Euler's method in a diffusion model.  Each row represents a different schedule. Each column shows the results for different image generation tasks (portraits, wreaths, phoenix, cityscape). The number of steps used (10 steps, 30 steps) is indicated above each column. The results showcase how different sampling schedules affect the generation quality and convergence speed.", "section": "2.3 Improving Sampling Efficiency with Fewer and Faster Steps"}, {"figure_path": "ieYdf9TZ2u/figures/figures_8_1.jpg", "caption": "Figure 8: Comparison of 4\u00d7 resolution extrapolation.", "description": "This figure compares the performance of several models (Lumina-Next (1K), SDXL, PixArt-a, MultiDiffusion, DemoFusion, ScaleCrafter, Lumina-T2I) on 4x resolution extrapolation.  It visually demonstrates the differences in image quality and artifact generation when extrapolating beyond the original training resolution.  Lumina-Next shows significantly less artifacts compared to the other models.", "section": "3 Lumina-Next by Composing Everything"}, {"figure_path": "ieYdf9TZ2u/figures/figures_9_1.jpg", "caption": "Figure 7: Comparison of few-step generation using different time schedules with Euler's method.", "description": "This figure compares the image generation results using different time schedules (Uniform, Rational, Sigmoid) with Euler's method for 10 and 30 steps.  The goal is to show how the choice of time schedule affects the quality of the generated image, particularly when using a reduced number of steps. The Sigmoid schedule demonstrates better performance, especially at lower step counts.", "section": "2.3 Improving Sampling Efficiency with Fewer and Faster Steps"}, {"figure_path": "ieYdf9TZ2u/figures/figures_16_1.jpg", "caption": "Figure 10: Architecture details of Flag-DiT and Next-DiT.", "description": "This figure illustrates the architectural differences between the original Flag-DiT and the improved Next-DiT.  Key improvements in Next-DiT are highlighted, including the replacement of 1D ROPE with 3D ROPE, the addition of sandwich normalization blocks, and the use of grouped-query attention. The figure shows the flow of information through both architectures, from noisy input to the final predicted velocity or noise, detailing the changes made to enhance stability and efficiency.", "section": "2.1 Architecture of Next-DiT"}, {"figure_path": "ieYdf9TZ2u/figures/figures_18_1.jpg", "caption": "Figure 11: Illustration of mixture-of-captioners.", "description": "This figure demonstrates the concept of using multiple captioning models to generate a more comprehensive and accurate description of an image.  Different models (BLIP2, ShareGPT4V, CogVLM, SPHINX-X, LLaVA-next, GPT-4V) are used to generate captions of the same image and the various captions provide different levels of detail and perspectives, demonstrating the value of combining outputs from multiple models to create a more robust image description.", "section": "Mixture-of-Captioners"}, {"figure_path": "ieYdf9TZ2u/figures/figures_18_2.jpg", "caption": "Figure 12: (a) Qualitative results of 2K images generated by Lumina-Next with and without context drop. (b) Comparison of inference speed between different settings using 50 Euler steps.", "description": "This figure shows the qualitative results of 2K images generated by Lumina-Next with and without using the context drop method.  The left panel (a) displays four example images generated with and without the method.  The right panel (b) presents a bar chart illustrating the inference time comparison for different settings (baseline, context drop, Flash Attention, and Flash Attention + Context Drop) at two resolutions (1024x1024 and 2048x2048).", "section": "Improving Sampling Efficiency with Fewer and Faster Steps"}, {"figure_path": "ieYdf9TZ2u/figures/figures_19_1.jpg", "caption": "Figure 13: Results of multilingual text-to-image generation by Lumina-Next, SDXL, and PixArt-a.", "description": "This figure displays the results of multilingual text-to-image generation using three different models: Lumina-Next, SDXL, and PixArt-a.  The same prompt was given in multiple languages (English, Japanese, Chinese, Russian, Ukrainian, Thai, Polish, Persian, Modern Standard Arabic, Korean, Vietnamese, Marathi, Kurdish, and Turkish).  The figure showcases the ability of each model to generate images reflecting the style and details described by the prompt, even across different languages.  The comparison allows a visual evaluation of each model's proficiency in multilingual understanding and image generation.", "section": "C More Text to Image Generation Results"}, {"figure_path": "ieYdf9TZ2u/figures/figures_20_1.jpg", "caption": "Figure 14: Results of text-to-image generation with emojis by Lumina-Next, SDXL, and PixArt-a.", "description": "This figure displays the results of text-to-image generation experiments using three different models: Lumina-Next, PixArt-a, and SDXL.  Each model was prompted with sentences containing emojis to test their ability to generate images that accurately reflect both the text and the emoji's meaning.  The results demonstrate the varying capabilities of each model in understanding and incorporating the emojis into their generated images, highlighting differences in style, detail, and accuracy.", "section": "C.2 Results of Zero-shot Multilingual Generation"}, {"figure_path": "ieYdf9TZ2u/figures/figures_21_1.jpg", "caption": "Figure 15: Results of multilingual text-to-image generation using different LLMs as text encoders.", "description": "This figure showcases the results of multilingual text-to-image generation using three different large language models (LLMs) as text encoders: Gemma-2B, InternLM-7B, and Qwen-1.8B.  Each LLM is used to generate images from the same set of prompts written in various languages. The purpose is to demonstrate the impact of different LLMs on the quality and cultural nuances of the generated images, highlighting the relationship between the LLM's capabilities and the resulting image generation.", "section": "C. More Text to Image Generation Results"}, {"figure_path": "ieYdf9TZ2u/figures/figures_21_2.jpg", "caption": "Figure 8: Comparison of 4\u00d7 resolution extrapolation.", "description": "This figure compares the performance of Lumina-Next, SDXL, PixArt-\u03b1, MultiDiffusion, and DemoFusion on 4x resolution extrapolation.  It showcases generated images from each method at a higher resolution than the models were originally trained on, demonstrating the relative strengths and weaknesses of each approach in handling high-resolution generation. Lumina-Next appears to show the highest quality and detail in the extrapolations.", "section": "3 Lumina-Next by Composing Everything"}, {"figure_path": "ieYdf9TZ2u/figures/figures_22_1.jpg", "caption": "Figure 17: Generated images of Lumina-Next with long prompts.", "description": "This figure showcases example images generated by Lumina-Next using long and detailed prompts.  The prompts are provided in multiple languages (English, Chinese) and demonstrate the model's ability to generate high-quality images that accurately reflect the specific details described in the prompts, even with complex scenes and descriptions.", "section": "C More Text to Image Generation Results"}, {"figure_path": "ieYdf9TZ2u/figures/figures_23_1.jpg", "caption": "Figure 10: Architecture details of Flag-DiT and Next-DiT.", "description": "This figure shows a detailed comparison of the architectures of Flag-DiT (the original Lumina-T2X architecture) and Next-DiT (the improved architecture in Lumina-Next).  It highlights key differences and improvements made in Next-DiT, such as the replacement of 1D ROPE with 3D ROPE, the addition of sandwich normalization blocks, and the use of grouped-query attention. The figure illustrates the flow of information through each architecture, detailing the processing steps involved in generating images.", "section": "2.1 Architecture of Next-DiT"}, {"figure_path": "ieYdf9TZ2u/figures/figures_24_1.jpg", "caption": "Figure 19: Illustration of our dynamic partitioning and padding scheme with masked attention for handling input images of arbitrary resolutions.", "description": "This figure illustrates the method used to handle images of various resolutions and aspect ratios.  The input images are dynamically partitioned into patches, and padding is applied to ensure consistent sequence lengths. Masked attention is then used to prevent unwanted interactions between padded tokens and actual image tokens. This dynamic approach is crucial for efficient training and inference with images of varying sizes.", "section": "D Any Resolution Recognition with Lumina-Next"}, {"figure_path": "ieYdf9TZ2u/figures/figures_25_1.jpg", "caption": "Figure 20: Performance of Next-DiT across different resolutions.", "description": "The figure shows the performance of the Next-DiT model on image classification across different resolutions (224, 384, 512, and 1024).  It demonstrates that Next-DiT generalizes better to larger image sizes compared to DeiT-base, even without fine-tuning, and significantly improves performance with fine-tuning, especially at higher resolutions. This highlights Next-DiT's ability to handle varied resolutions effectively.", "section": "D Any Resolution Recognition with Lumina-Next"}, {"figure_path": "ieYdf9TZ2u/figures/figures_26_1.jpg", "caption": "Figure 21: An illustration of multi-view images generation. Left: All view training paradigm of MV-Next-DiT. Right: Any view inference paradigm of MV-Next-DiT.", "description": "This figure illustrates the training and inference processes of the MV-Next-DiT model for multi-view image generation.  The left side shows the training process where all views are trained simultaneously. This involves feeding multiple noisy views into the model along with text and optional image condition information, leveraging relative pose embeddings to capture the relationships between the views. The right side depicts the inference process, where only a text condition and an optional image condition are inputted, and the model generates a specified number of views using the learned relationships between them. The architecture highlights the flexibility of the model to generate any number of views during inference based on the training from all views.", "section": "E.1 Image- and Text-conditional Multi-View Generation"}, {"figure_path": "ieYdf9TZ2u/figures/figures_27_1.jpg", "caption": "Figure 22: Results generated by MV-Next-DiT. The upper right is the result of text-to-multiview, and the rest is the result of text-&image-to-multiview, where the first column is the corresponding input image with background removed. In addition, every 3 rows from top to bottom are the generated results of 4 views, 6 views, and 8 views respectively.", "description": "This figure demonstrates the results of multi-view image generation using MV-Next-DiT. The top-right shows results from text-only input, while the rest show results from both image and text input. Each set of three rows shows generation results with 4, 6, and 8 views respectively. The first column in each set displays the input image used for the generation.", "section": "E.1 Image- and Text-conditional Multi-View Generation"}, {"figure_path": "ieYdf9TZ2u/figures/figures_29_1.jpg", "caption": "Figure 23: An illustration of text-guided music/audio generation. It consists of the following main components: 1) VAE to encode spectrogram into a latent and convert it back to spectrogram; 2) text encoder to derive high-level textual representation; 3) flow transformer to inject condition; and 4) separately-trained neural vocoder to convert mel-spectrograms to raw waveforms. In the following sections, we describe these components in detail.", "description": "This figure illustrates the architecture of the text-guided music/audio generation model.  The process involves encoding the audio spectrogram into a latent representation using a Variational Autoencoder (VAE). This latent representation, along with text embeddings (from CLAP and FLAN-T5 encoders) and time embeddings, is fed into Lumina-Next, which outputs the predicted velocity for the latent space. This velocity is then used by an ODE solver to iteratively refine the latent representation, ultimately producing a noise-free latent representation. Finally, a separate audio decoder reconstructs the audio waveform from the refined latent representation using a vocoder.", "section": "E.2 Text-Conditional Audio and Music Generation"}, {"figure_path": "ieYdf9TZ2u/figures/figures_31_1.jpg", "caption": "Figure 24: Examples of generated point clouds with different densities, sampled from the generator trained on 256 points.", "description": "This figure shows examples of point clouds generated by the model, demonstrating its ability to generate point clouds with varying densities (number of points).  The examples are for four different object categories: airplane, bench, lamp, and chair. Each row displays point clouds of the same object but with progressively increasing point density, illustrating how the model's output changes from a sparse representation to a detailed and higher-fidelity depiction of the object as the point count increases.", "section": "E.3 Label- and Text-Conditional Point Cloud Generation"}]