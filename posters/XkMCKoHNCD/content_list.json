[{"type": "text", "text": "A test of stochastic parroting in a generalisation task: predicting the characters in TV series ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 There are two broad, opposing views of the recent developments in large language   \n2 models (LLMs). The first of these uses the term \"stochastic parrots\" from Emily   \n3 Bender et al [3] to emphasise that because LLMs are simply a method for creating   \n4 a probability distribution over sequences of words, they can be viewed as simply   \n5 parroting information in the training data. The second view, \"Sparks of AGI\" from   \n6 Sebastien Bubeck et al [6], posits that the unprecedented scale of computation in   \n7 the newest generation of LLMs is leading to what its proponents call \"an early (yet   \n8 still incomplete) version of an artificial general intelligence (AGI) system\". In this   \n9 article, we propose a method for making predictions purely from the representation   \n10 of data inside the LLM. Specifically, we create a logistic regression model, using   \n11 the principal components of a LLM model embedding as features, in order to   \n12 predict an output variable. The task we use to illustrate our method is predicting the   \n13 characters in TV series, based on their lines in the show. We show that our method   \n14 can, for example, distinguish Penny and Sheldon in the Big Bang Theory with   \n15 an AUC performance of 0.79. Logistic regression models for other characters in   \n16 Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the   \n17 most significant distinguishing factors between characters relating to the number   \n18 and nature of comments they make about women. The characters in the TV-series   \n19 Friends are more difficult to distinguish using this method (AUCs range from 0.61   \n20 to 0.66). We find that the accuracy of our logistic regression on a linear feature   \n21 space is slightly lower than GPT-4, which is in turn at a level comparable to two   \n22 human experts. We discuss how the method we propose could be used to help   \n23 researchers be more specific in the claims they make about large language models. ", "page_idx": 0}, {"type": "text", "text": "24 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "25 Large language models (LLMs) are neural networks trained on a large text corpus to predict the next   \n26 word, phrase or paragraph in that dataset [25]. As the number of network parameters and the size of   \n27 the corpus increases, the ability of this network to write convincing-sounding texts improves [15]. As   \n28 a result, an increasing number of compelling LLM applications, from CHAT-GPT to Copilot, have   \n29 been developed. Recently, Bubek et al. argued that \"beyond its mastery of language, GPT-4 can solve   \n30 novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more,   \n31 without needing any special prompting\" [6]. For these authors, this ability to generalise revealed   \n32 \"Sparks of AGI\", going on to state that they believed \"that [GPT-4] could reasonably be viewed as an   \n33 early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"   \n34 The stochastic parrots paradigm critiques such claims by pointing out that large language models   \n35 simply predict the next word, sentence or paragraph, and it is humans who attribute understanding to   \n36 its output [3]. LLMs simply replicate examples (i.e. parrot text) from a massive corpus of data [7].   \n37 The stochastic parrots view provides an epistemic critique of claims, such as \"Sparks of AGI\", about   \n38 artificial general intelligence. For example, in the context of the benchmark tests (such as those later   \n39 carried out by [6]), Raj et al. (2021) write, \"the reality of [benchmark] development, use and adoption   \n40 indicates a construct validity issue, where the involved benchmarks \u2014 due to their instantiation   \n41 in particular data, metrics and practice \u2014 cannot possibly capture anything representative of the   \n42 claims to general applicability being made about them.\" In other words, the very notion of generality,   \n43 sought to be proven in \"Sparks of AGI\", cannot be captured by benchmark problems [6]. This   \n44 critique is fundamental: it doesn\u2019t matter how many specific tasks a model completes, there is no   \n45 convergence towards generality. Even setting these epistemic problems aside, the stochastic parrots   \n46 view also has practical implications for how we evaluate LLM performance. For example, Lewis   \n47 and Mitchell (2024) manipulate benchmark tasks to construct \u2019counterfactual\u2019 tasks, by for example   \n48 adding information that solves the task but LLM\u2019s neglect this information, because they are parroting   \n49 answers to similar, previously trained-on examples [18].   \n50 In spite of the limitation of benchmarks, the fact remains that LLMs do perform well over a wide   \n51 range of tasks, with little or no additional training data. It is the question of understanding how such   \n52 performance might arise which we address in this paper. Instead of proposing new benchmarks, we   \n53 focus on comparing how LLMs perform to simpler, well-understood statistical methods on a novel   \n54 task. An approach like ours has previously been persued medical imaging \u2014 where a systematic   \n55 review showed that logistic regression on selected features performed (on average) just as well as   \n56 complicated machine learning approaches [8] \u2014 and with respect to conflict prediction \u2014 logistic   \n57 regression perform just as well (as is easier to interpret) than more complex machine learning models   \n58 [16].   \n59 For many general tasks, a relatively straightforward method of making predictions is to use linear or   \n60 logistic regression on the leading principal components of a data set. One example is using principle   \n61 components of \u2019likes\u2019 of Facebook users to predict the answers people gave to big-five personality   \n62 tests [32, 19, 17]. Konsinski et al. (2016) first performed PCA or Latent Dirichlet Allocation (LDA)   \n63 on the matrix of likes and Facebook users, and then used the leading components of the PCA (or   \n64 clusters of LDA) in a regression model to predict the user\u2019s answers in personality tests [17]. This   \n65 allowed the authors to study how the accuracy of predictions increased with the number of dimensions   \n66 of the Facebook likes. The method is linear in the PCA space and has the advantage that the results   \n67 can be interpreted qualitatively. For example, young and female users could be predicted as liking   \n68 \"humorous and juvenile\" (author\u2019s choice of words) statements such as, \"I finally stop laughing . . .   \n69 look back over at you and start all over again\" [17].   \n70 The above method is potentially interesting in the context of stochastic parrots, because it allows us   \n71 to, so to speak, look inside the parrot\u2019s brain. Large language models encode information using vector   \n72 semantics: words and sentences are represented as vectors [14, 24, 20], referred to as embeddings.   \n73 Words that occur in similar contexts tend to have similar meanings, therefore, they will have a similar   \n74 vector [25]. The vectors are generally based on a co-occurrence matrix, a way of representing how   \n75 often words co-occur. An alternative to using the term-document matrix to represent words as vectors   \n76 of document counts, is to use the term-term matrix . If we then take every occurrence of each word   \n77 and count the context words around it, we get a word-word co-occurrence matrix [14]. Embeddings   \n78 can be obtained with transformers models [27, 9, 11, 13, 31, 30], which were initially developed for   \n79 machine translation in 2017 [27, 28].   \n80 We can use principal components of the embeddings of a language model, with respect to a specific   \n81 problem, in order to both understand what information is used in solving the task and to test the   \n82 degree to which performance on that task is achieved from the representation of the data or from   \n83 some other unknown mechanism. To make these statements concrete, we now outline what we do in   \n84 this article. We address the task of predicting which character said which specific lines of dialogue   \n85 in two US TV series: Big Bang Theory and Friends. This task is reminiscent of the personality   \n86 research discussed above in that the characters in the show have very stereotypical personalities:   \n87 can we predict character personalities from their line in the show? Such problems are of specific   \n88 interest for this article, for three reasons (1) an increasing number of applications of AI involve   \n89 supposed personality tests and analyses [10]; (2) such tests raise ethical issues about both reliability   \n90 and applications [29, 1]; (3) they are sometimes used to imply that machines can understand us better   \n91 than we understand ourselves [32]. The character personality test is an example of generalisation in   \n92 the sense that, while large language models might have been fed data from these series, they haven\u2019t   \n93 been trained to solve this specific task.   \n94 We proceed as follows. We first detail the method of and logistic regression on the principal   \n95 components of the embeddings. We then analyse which PCA components are most predictive of   \n96 statements by the characters, how the number components affects accuracy and differences between   \n97 the TV shows. Finally, we compare performance of our simpler model to GPT-4 [22] and one human   \n98 expert, with extensive experience of the two TV shows. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "99 2 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 2.1 Embeddings and PCA ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 The dataset is the transcript of the first 10 seasons of the TV-series The Big Bang Theory 1 and 10   \n102 seasons of the TV-serie Friends 2 in English. We cleaned the dataset, by only keeping the main   \n103 characters and their respective dialogue lines. This gives 44, 966 dialogue lines for the TV series The   \n104 Big Bang Theory and 51, 615 dialogue lines for the TV series Friends. We then transformed these   \n105 dialogue lines into a vector, i.e. we create embeddings using the python library SentenceTransformer   \n106 and the model \u2019all-MiniLM-L6-v2\u2019 [26]. Each dialogue line then has a specific embedding, a vector   \n107 of dimension 384. For comparison, the small text embedding of OpenAi, \u2019text-embedding-3-small\u2019,   \n108 gives 1,536 output dimension [21, 5].   \n109 We then performed a principal component analysis (PCA) on the embeddings (for more details of   \n110 the method we follow see [12]). Principal Component Analysis(PCA) determines the directions   \n111 that maximize the variation in the data. The PCA is a procedure that takes dataset with several   \n112 variables, to a smaller dataset with new variables (the principal components) that will be a linear   \n113 combination of the former variables. Each dimension in this space corresponds to a feature that will   \n114 be explicitly defined later. To ensure a representative view of the dataset, we need to standardize   \n115 it so that no single variable disproportionately influences the analysis, by removing the mean then   \n116 divide by the standard deviation. Then, we calculate the covariance matrix. A covariance matrix is a   \n117 square matrix that shows the covariance between pairs of variables in the dataset. The diagonal of the   \n118 matrix gives the variance of the variables and the other terms give the covariance between the pair of   \n119 variables. The covariance measures of how much two random variables vary together, by estimating   \n120 the linearity between them. From the covariance matrix we deduce the eigenvectors and eigenvalues,   \n121 by doing an eigenvalue decomposition of the covariance matrix $\\mathcal{C}$ . We find the eigenvector by solving   \n122 $(\\mathcal{C}-\\lambda I d)x=0$ , where $x$ is the eigenvector associated with the eigenvalue $\\lambda$ The eigenvalue gives   \n23 the magnitude (or accounted variance) of the data along the new feature dimension. The eigenvector   \n124 gives the direction of the data along the new feature dimension, and forms the linear combination   \n125 for a principal component. The eigenvalues are in descending order and as explained in [12], they   \n126 \u2019maximize the explained variances on each dimension\u2019. We refer to the the coefficients of the leading   \n127 eigenvector as the first principal component (PCA1), the second eigenvector as PCA2 and so on. We   \n128 reduce the 384 dimension of each embeddings to a dimension space of 300. All calculations were   \n129 performed in Sklearn [23] and full code is available here 3.   \n130 An important aspect of our approach is gaining a qualitative understanding of how the principal   \n131 components reflect the meaning of the dialogue lines. Each PCA corresponds to one eigenvector and   \n132 consequently to one dimension from which we are able investigate which kind of phrases tied to that   \n133 dimension. To help us make this analysis we used two-dimensional visualisation of the data. First we   \n134 implemented a 10-means cluster on two principal components at a time, starting with the leading   \n135 components (i.e PCA1 and PCA2). We colour each cluster and then assign the phrase nearest of the   \n136 center as the cluster name (see figure 1). We also looked at the most extreme dialogue line in each   \n137 PCA, by printing out the sentences with the highest values and the smallest values. From these we   \n138 assigned a qualitative interpretation of the \"meaning\" of the leading PCAs. In the annex, we report   \n139 the tenth highest values and the tenth smallest values. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "140 2.2 Character prediction ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "141 In order to predict which dialogue line comes from which character, we use a logistic regression on   \n142 the PCAs of the dialogue lines of the characters. We follow the notation from [12] and let $u_{j,i}$ be the   \n143 j-th the coefficient of the principal component of the i-th dialogue line. First, we normalise all the   \n144 coefficients $u_{j,i}$ of the principal components by taking away the mean and dividing by the standard   \n145 deviation, so each component has mean zero and standard deviation of one. We then performed a   \n146 binomial logistic regression \u2014 e.g. does the dialogue line belong to Penny or Sheldon ? \u2014 based on   \n147 a linear prediction of the dialogue line $i$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta_{0}+\\beta_{1}u_{1,i}+...+\\beta_{n}u_{n,i},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "148 allowing to measure (using regression coefficients $\\{\\beta_{0},...,\\beta_{n}\\})$ how the the explanatory variables   \n149 $u_{1,i},...,u_{n,i}$ , impact the prediction. The fitted logistic regression is model is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{P}\\left(\\mathrm{Sheldon}|\\mathrm{the~i.th~line~is~said~by~Sheldon~or~Penny}\\right)=\\frac{1}{1+e^{-\\left(\\beta_{0}+\\beta_{1}u_{1,i}+...+\\beta_{n}u_{n,i}\\right)}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "150 where $\\beta_{0}$ determines the intercept (i.e. it is the outcome when all the other predictors variables are   \n151 equal to zero). Each coefficient $\\beta_{i}$ estimates the additional effect of adding the corresponding variable   \n152 to the model prediction.   \n153 The sign of the coefficient indicates the influence of the specific principal component on the probability   \n154 it is a particular character. If the sign is positive then it is more likely to be that character (Penny   \n155 in the example above) if the dialogue line has larger and more positive values of that component.   \n156 Conversely, if the sign is negative that means it is less likely to be that character if the dialogue line   \n157 has larger and more positive values of that component. The larger the magnitude of the coefficient,   \n158 the more important the predictor variable is in making the prediction.   \n159 For each TV series, we proceed to a logistic regression with 300 first PCAs, for each possible pair   \n160 of characters. We obtain a predictor function and evaluate the absolute value of each regression   \n161 coefficient. We obtain the magnitude of each coefficient and therefore assess which coefficients have   \n162 the most importance in the logistic regression. Afterwards we take the ten regression coefficients with   \n163 the largest aboslute value and plot them (see figure 3 and 8). From this analyse, we deduce which   \n164 dimensions that have an impact on the character\u2019s prediction. To evaluate performance we calculate   \n165 the AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve to evaluate as a   \n166 function of the dimensions. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "167 2.3 Comparing to GPT4 and human expert ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "168 In order to test our method against a large language model we queried GPT4 with the following   \n169 system prompt: \"You are expert on the TV series The Big Bang Theory. You are now being challenged   \n170 to identify characters from the series. Try your best to do well. If you can beat another human expert   \n171 there is a prize.\" and a query that asked \"Tell me who was most likely out of Leonard and Sheldon   \n172 (from the series Big Bang Theory) to have said the following line of dialogue: [DIALOGUE LINE].   \n173 Now state the most likely character as a single word, either Leonard and Sheldon. Do not write   \n174 anything else.\" Character and TV series names were adjusted appropriately for each test. We tested   \n175 four pairs (Penny/Sheldon, Leonard/Sheldon, Phoebe/Ross, Phoebe/Chandler). We first repeated the   \n176 above procedure 100 times, 50 times for each character, to test the accuracy of the classification (i.e.   \n177 proportion of correct answers).   \n178 We also provided the same dialogue lines to two motivated human experts (who had watched both   \n179 series in their entirety two times, most recently within the last year) and expressed a determination to   \n180 beat GPT4. Both participants were relatives of the co-authors of this article. The same dialogue lines   \n181 on which GPT4 was tested, were presented in a random order in the spreadsheet file. The subjects   \n182 were asked to guess the name of the character for each dialogue line, and write it into the spreadsheet. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "183 3 Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "184 3.1 Qualitative analysis of the principal components ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "185 We started by plotting the embedded dialogue lines \u2019Big Bang Theory\u2019 in terms of the six most   \n186 important principal components, in order to visualise the most distinguishing features of the dialogue.   \n187 The first two of these (PCA1 and PCA2) are shown in figure 1aa. The nearest neighbour clustering   \n188 then allows us to see where different dialogue lines are found in these dimensions. We can see that   \n189 larger negative values of PCA1 corresponds to very short phrases (for example \u2019Uh\u2019 in the pink   \n190 cluster in the top left of the figure) and larger positive values of PCA1 correspond to phrases about   \n191 Sheldon (for example \u2019Sheldon, what do you expect us to do?\u2019 in the green cluster in the top right   \n192 of the figure). The qualitative analysis of PCA1 confirmed this pattern, with \u2019Yeah\u2019 being the most   \n193 extreme negative value and \u2019You know, I was thinking. Without Sheldon, most of us would have   \n194 never met, but Penny would still live across from him.\u2019 being the extreme positive value (see Annex   \n195 6 for a list of the ten most extreme positive and negative values of PCA1 and the other principal   \n196 components).   \n197 Following the same approach for PCA2, we found that the negative values are associated with long   \n198 phrases about a female characters an positive values with phrases about Sheldon. The most extreme   \n199 negative value is \u2019Well, there was the time I had my tonsils out, and I shared a room with a little   \n200 Vietnamese girl. She didn\u2019t make it through the night, but up till then, it was kind of fun.\u2019 and the   \n201 most extreme positive value is \u2019Leonard, Sheldon.\u2019(see annex 6). The cluster values in figure 1a also   \n202 show the same pattern: with \u2019Indian princess who befriends a monkey who was mocked by all other   \n203 monkeys because he was different. For some reason I related to it quite strongly\u2019 in the orange cluster   \n204 at the bottom of figure 1a and \u2019Sheldon, why are you doing this?\u2019 in the light green cluster at the top   \n205 of the same figure.   \n206 A similar approach can be used to interpret figure 1b and c. PCA 3 ranges from phrase that questions   \n207 a premise (\u2019Really? I didn\u2019t know that.\u2019) to phrases with a first person future action (\u2019Aw, sweetie,   \n208 I\u2019m comfortable around you, too.\u2019). PCA 4 ranges from a phrase about relationship (\u2019Really? That   \n209 seems rather short sighted, coming from someone who is generally considered altogether unlikable.   \n210 Why don\u2019t you take some time to reconsider?\u2019) to a phrase related to eating out (\u2019Excellent! What   \n211 are you planning to wear?\u2019). The fifth dimension is phrase with often a negation or counterargument   \n212 (like \u2019Oh no, no, no, crystals don\u2019t work\u2019, which is green in figure 1) to a short question about a   \n213 woman (like \u2019She knows you. She\u2019s tense. We all are. Buy a basket!\u2019, which is red in the same figure).   \n214 Finally, PCA 6 ranges from an apology (e.g. \u2019I wish you weren\u2019t wearing filp-flops. It\u2019s dangerous to   \n215 drive in filp-flops\u2019) to a phrase with affirmative statement( e.g. \u2019Still going to introduce him?\u2019). This   \n216 final interpretation is even clearer when we look at the extreme negative value ( \u2019Relax, it wasn\u2019t your   \n217 fault.\u2019) and extreme positive value (\u2019Sure. I\u2019d like to meet her.\u2019). Overall, in The Big Bang Theory   \n218 the distinguishing characteristics of the principal components often relate to the characters views of   \n219 women. For Friends, there are also clear semantic differences in the sentences, although these appear   \n220 to be less gender stereotyped. We give a full analysis of the leading six components in annex 5.3.   \n221 When we plot the average position of the characters in the space of the first two components, the   \n222 differences are very small in comparison to the variation (figure 5 in annex 6). For example, while   \n223 there is a distance of 0.33 between Leonard and Amy on the PCA 1 axis, the standard deviation of   \n224 the values for the Leonard and Amy on that axis are 3.62 and 3.58, respectively. This observations   \n225 indicates that it is impossible to distinguish the characters in terms of just a single dimension. We do   \n226 note, though, that Friends characters are even closer together than The Big Bang Theory characters   \n227 (the PCA1 distance between Chandler and Rachel is 0.15 and between Chandler and Joey is 0.11,   \n228 while the standard deviations of Chandler, Rachel and Joey are respectively 3.62, 4.03 and 3.78). The   \n229 biggest difference we observed is between Penny and Sheldon. ", "page_idx": 3}, {"type": "image", "img_path": "XkMCKoHNCD/tmp/936f49f3b03db4071342f4b325ed0c11722f0632fb47b64fb5db1e7bdee423fa.jpg", "img_caption": ["Figure 1: Projection of the first 6 PCAs. Each PCA has an interpretation from the qualitative analysis. Each plot has their respective cluster along with the average phrase of each cluster for The Big Bang Theory dialogue lines ", "(c) Projection of PCA5 and PCA6 "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "XkMCKoHNCD/tmp/712560f25b372946555abc0a4ff2a46366a6fe8fad2b1d35845f1bb5ba8582a0.jpg", "img_caption": ["Figure 2: AUC curves to assess the performance of the logistic regression, by increasing the number of dimensions, in the dialogue lines\u2019s prediction for two different couples for the two Tv serie "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "230 3.2 Character prediction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "231 While a small number of principal component dimensions is not sufficient to tell the characters apart,   \n232 can we use more of the dimensions to make the distinction? To test this we performed binomial logistic   \n233 regression on pairs of characters as a function of the number of principal components we included in   \n234 the model. The AUC values in figure 2a show a steady improvement in the predictions up to around   \n235 50 principal components for Big Bang Theory, after which only slight increases in performance are   \n236 obtained. Sheldon and Penny were easier to distinguish using this method than Sheldon and Leonard.   \n237 Figure 2b, shows that Friends characters were much more difficult to distinguish using this method.   \n238 If we view the principal component analysis as an attempt to capture the character\u2019s personality by   \n239 their dialogue lines (as in the analysis by [32]) then we can say that the TV characters personality   \n240 have a dimension of somewhere between 50 and 100. Each new dimension gives a small extra insight ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "XkMCKoHNCD/tmp/b0c5bbaab10cd07292fac6654854202af33ddcbd49a56f221067e60296038039.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: Regression coefficients for each possible character pairs for the TV series The Big Bang Theory. For each pair, we conduct a logistic regression to predict if the dialogue line is more likely to be said by a character1 such that $P$ (character1 $=1$ |the line is said by character1 or character2). We use the first 300 principal components in the logistic regression. Then, we assess the absolute value of each coefficient to determine their magnitude. Following this, we select the top ten coefficients for each linear predictor function. We report in this figure those coefficients, along with their corresponding dimensions. The coefficients are in decreasing order from left to right: the left side have the coefficient with the highest magnitude, the right side have the coefficients with the lowest magnitude. ", "page_idx": 6}, {"type": "text", "text": "241 into the character differences. Since Friends characters are more difficult to predict from what they   \n242 say, we can conclude that Friends characters are less stereotyped than characters in The Big Bang   \n243 Theory.   \n244 We can investigate which PCA dimensions best distinguish characters by looking at the coefficients   \n245 of the binary regression. Figure 3 shows the ten most important components (determined by the   \n246 magnitude of the absolute value of the coefficients in the regression) for distinguishing the characters   \n247 dialogue lines in The Big Bang Theory. Each row represents a character pair, with the PCAs ordered   \n248 from left to right according to the magnitude of the coefficients. The first column corresponds to the   \n249 coefficient with the largest magnitude in the linear predictor function, the second column corresponds   \n250 to the second coefficient with the second largest magnitude, and so on.   \n251 As an example, the first row is the character prediction for the couple \u2019Penny and Shel  \n252 don\u2019 should be read as considering the probability the dialogue line is by Penny, i.e.   \n253 $\\mathsf{P\\left(P e n n y=1|\\right.}$ the line is said by Penny or Sheldon). The first cell entry, PCA19, is the coefficient in   \n254 the linear predictor function with the largest absolute value. Performing a qualitative analysis on   \n255 PCA19 (see annex 6) we find that negative coefficients correspond to lines about food and positive   \n256 coefficients correspond to lines about comics. In this case, the coefficient of the PCA19 is negative,   \n257 implying that if a dialogue line is about meal or food, it is more likely to be spoken by Penny than   \n258 Sheldon.   \n259 The most common occurring component in figure 3 is exactly this PCA 19 (food vs. comics) which   \n260 has 12 occurrences. PCA 2, which is long phrases about a female character versus phrases with one   \n261 name has 11 occurrences. PCA 7 has 11 occurrences and ranges from phrases with yes/no to question   \n262 about the current situation. The next most common occurring components are PCA4 (10 occurrences)   \n263 which ranges from an apology to phrase with affirmative statement; PCA15 (10 occurrances) ranging   \n264 from long phrases about a woman to short phrases about houses; PCA17 (9 occurrences) range from   \n265 short phrases about travel to long food related phrases: PCA5 (9 occurrences) which range from long   \n266 dialogue lines that express an opinion to short questions about a female character.   \n267 Figure 4 shows the relationship between the characters in terms of PCA19 (which distinguishes   \n268 dialogue lines about meal/food related from those about comics). The graph shows the magnitude of   \n269 the coefficient and the direction of the arrow indicates that the coefficient is positive. For example, for   \n270 P(Penny|the line is said by Penny or Sheldon) the regression coefficient for the PCA19 is negative,   \n271 reflecting the fact that Penny talks more about food and Sheldon that talks more about comics, so   \n272 the arrow points from Sheldon to Penny. Similarly, we see that Bernadette talks more about food   \n273 than Raj, Howard Sheldon and Amy and thus the arrows point toward her. And Raj talks more about   \n274 comics than Penny, Bernadette and even Sheldon, so the arrows point out from him in the figure. In   \n275 the case of the TV series Friends, the magnitude of the regression coefficients are smaller than those   \n276 for The Big Bang Theory and a more varied number of components are represented (see annex 5.3).   \n277 While the method for constructing figure 4 can give an indication of how the components distinguish   \n278 the characters, we should bear in mind that in a regression of hundreds of variables (on which this   \n279 graph is based) the relationships established are not always straightforward. For example, in the   \n280 figure, we see that the respective models predict that Howard talks more about comics than Sheldon,   \n281 who talks more about comics than Penny, and Penny talks more about comics than Howard. This   \n282 inconsistency is likely due to other principal components distinguishing Penny and Howard better   \n283 than PCA19, and PCA19 acting as a counterbalance, to these additional components. A full analysis   \n284 of these relationships is beyond the scope of the current article. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "XkMCKoHNCD/tmp/02ae3910a0b1d10259d5d4de6b9492c41c58eeb4c20b9c5929f53521a8db6e1b.jpg", "img_caption": ["Figure 4: Relationship between characters in The Big Bang Theory in terms of PCA19 (that distinguishes lines about meal/food related form lines about comics). The value on the arrows show the magnitude of the coefficient. Only pairings where the absolute value of the regression coefficient is greater than 0.1 are included. The person at the start of the arrow talks about comics more than they talk about food compared to the person at the end of the arrow. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "285 3.3 Comparing to GPT4 and human expert ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "286 Initial prompting of GPT4 revealed that it has knowledge of the two TV series in its training data.   \n287 GPT4 replied that it \"can provide information about the show, its characters, plot points, cultural   \n288 impact, and more\". It was also able to provide motivation for its answers. For example, when we   \n289 asked if this dialogue line \u2019Okay, sweetie, I don\u2019t know if we\u2019re gonna have cookies, or he\u2019s just   \n290 gonna say hi, or really what\u2019s gonna happen, so just let me talk, and we\u2019ll. ..\u2019, it correctly answered   \n291 \u2019Penny\u2019. Then, when asked, it to explain why it draws conclusions about the characters, it cited   \n292 criteria \"Context of Character Behavior\", \"Speech Patterns\" and \"Interaction Dynamics\".   \n293 For the set of 100 dialogue lines, a direct prompt to GPT4 (see methods for details) was correct for   \n294 Penny versus Sheldon on 81 occasions, for Sheldon versus Leonard on 71 occasions, for Phoebe   \n295 versus Ross on 66 occasions, and for Phoebe versus Chandler on 65 occasions. For these same test   \n296 examples, the first human expert was correct on 71, 76, 67, and 59 occasions, respectively. The   \n297 second human expert was correct on 74, 72, 70, and 73 occasions. For comparison, the accuracy   \n298 (percentage correct over all sentences) for the 300 dimensional PCA model was $72.8\\%$ , $68.1\\%$ ,   \n299 $59.7\\%$ and $60.6\\%$ respectively. The standard error for a proportion of $70\\%$ is $0.7\\cdot0.3\\cdot100\\approx4.5\\%,$ ,   \n300 suggesting a comparable level of performance between the human experts and GPT4, and a slightly   \n301 lower level of performance for the 300 dimensional PCA model.   \n303 Our qualitative analysis highlights how, when interpreted by a human, the principal components of the   \n304 embeddings reflect the meaning of the dialogue lines of TV series. Many of dimensions contributing   \n305 to the prediction are related to female characters. This can be attributed to the fact that the TV series   \nportrays very stereotypical characters, with the main protagonists portrayed as geeks, embodying   \n307 various clich\u00e9s associated with them. A number of previous studies have identified gender and racial   \n308 stereotyping within the way models represent data [4, 2, 29], we have shown that these dimensions   \n309 are also important in the predictions these models make. Friends, in which the characters might be   \n310 considered to have smaller stereotyped (within-group) differences, was more difficult to predict using   \n311 this method.   \n312 We have shown that given the principal components of the dialogue in a TV series, we are able to   \n313 predict the characters personality using logistic regression, to a level of performance slightly below   \n314 that of GPT4. We needed 50-100 dimensions in the logistic regression to predict a dialogue line in the   \n315 TV series. This might be said to support the idea of a language model more like a stochastic parrot   \n316 than a spark of AI, in the sense that a large part of the predictive skill of the model can be obtained by   \n317 adding up the components of the word embeddings and providing an appropriate prediction. Indeed,   \nwe have used a much smaller embedding vector (384 dimensions) that GPT4 (several thousand   \n319 dimensions) to achieve somewhat comparable results.   \n320 That said, there remain two things which GPT4 does which our model does not. Firstly, our analysis   \n321 starts from the sentence embeddings. Taking these embeddings as given ignores the complex process   \n322 by which these are generated through training in the first place [9, 30]. Secondly, we had to specify   \n323 the problem we wanted to solve as a logistic regression problem and train on previous data. GPT4,   \n324 on the other hand, requires no additional training step and, from the given prompt, can identify the   \n325 requested character. In light of these limitations, we see our work as highlighting the need to be   \n326 more specific about claims related to sparks of AI [22]. We have shown that prediction part of the   \n327 question of identifying TV character personality is (to some degree) obtainable from linear models,   \n328 the question then is where the supposed spark lies? Is it in the creation of embeddings or is it in   \nGPT4\u2019s ability to identify the prediction problem from the input provided by the user? We would   \n330 suggest that further dissections of how these methods work, like we have done here for the prediction   \n331 stage, can shed more light on these questions.   \n332 Our study is limited to a qualitative study of two very specific datasets. The contribution is primarily   \n333 methodological. We propose an alternative to benchmark testing for understanding why a machine   \n334 learning method works in the way it does, by comparing it to a method based on linear predictions.   \n335 As such, it is a qualitative contribution to a larger debate around how to evaluate LLMs, rather than a   \n336 quantitative demonstration of model performance. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "337 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "338 [1] Rediet Abebe et al. \u201cRoles for computing in social change\u201d. In: Proceedings of the 2020   \n339 conference on fairness, accountability, and transparency. 2020, pp. 252\u2013260.   \n340 [2] Salter Anastasia and Blodgett Bridget. Toxic Geek Masculinity in Media. Springer, 2017.   \n341 [3] Emily M Bender et al. \u201cOn the dangers of stochastic parrots: Can language models be too big?\u201d   \n342 In: Proceedings of the 2021 ACM conference on fairness, accountability, and transparency.   \n343 2021, pp. 610\u2013623.   \n344 [4] Tolga Bolukbasi et al. \u201cMan is to computer programmer as woman is to homemaker? debiasing   \n345 word embeddings\u201d. In: Advances in neural information processing systems 29 (2016).   \n346 [5] Tom B. Brown et al. Language Models are Few-Shot Learners. 2020. arXiv: 2005.14165   \n347 [cs.CL].   \n348 [6] S\u00e9bastien Bubeck et al. \u201cSparks of artificial general intelligence: Early experiments with gpt-4\u201d.   \n349 In: arXiv preprint arXiv:2303.12712 (2023).   \n350 [7] Nicholas Carlini et al. Extracting Training Data from Large Language Models. 2021. arXiv:   \n351 2012.07805 [cs.CR].   \n352 [8] Evangelia Christodoulou et al. \u201cA systematic review shows no performance benefit of ma  \n353 chine learning over logistic regression for clinical prediction models\u201d. In: Journal of clinical   \n354 epidemiology 110 (2019), pp. 12\u201322.   \n355 [9] Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language   \n356 Understanding. 2019. arXiv: 1810.04805 [cs.CL].   \n357 [10] Jinyan Fan et al. \u201cHow well can an AI chatbot infer personality? Examining psychometric   \n358 properties of machine-inferred personality scores.\u201d In: Journal of Applied Psychology 108.8   \n359 (2023), p. 1277.   \n360 [11] William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion   \n361 Parameter Models with Simple and Efficient Sparsity. 2022. arXiv: 2101.03961 [cs.LG].   \n362 [12] Michael Greenacre et al. \u201cPrincipal component analysis\u201d. In: Nature Reviews Methods Primers   \n363 2.1 (2022), p. 100.   \n364 [13] Hongzhao Huang and Fuchun Peng. An Empirical Study of Efficient ASR Rescoring with   \n365 Transformers. 2019. arXiv: 1910.11450 [cs.CL].   \n366 [14] Dan Jurafsky and James H. Martin. Speech and Language Processing. URL: https://web.   \n367 stanford.edu/\\~jurafsky/slp3/.   \n368 [15] Jared Kaplan et al. \u201cScaling laws for neural language models\u201d. In: arXiv preprint   \n369 arXiv:2001.08361 (2020).   \n370 [16] Sayash Kapoor and Arvind Narayanan. \u201cLeakage and the reproducibility crisis in machine  \n371 learning-based science\u201d. In: Patterns 4.9 (2023).   \n372 [17] Michal Kosinski et al. \u201cMining big data to extract patterns and predict real-life outcomes.\u201d In:   \n373 Psychological methods 21.4 (2016), p. 493.   \n374 [18] Martha Lewis and Melanie Mitchell. \u201cUsing counterfactual tasks to evaluate the generality of   \n375 analogical reasoning in large language models\u201d. In: arXiv preprint arXiv:2402.08955 (2024).   \n376 [19] Dejan Markovikj et al. \u201cMining facebook data for predictive personality modeling\u201d. In: Pro  \n377 ceedings of the international AAAI conference on Web and social media. Vol. 7. 2. 2013,   \n378 pp. 23\u201326.   \n379 [20] Tomas Mikolov et al. Distributed Representations of Words and Phrases and their Composi  \n380 tionality. 2013. arXiv: 1310.4546 [cs.CL].   \n381 [21] OpenAI. Embeddings. URL: https : / / platform . openai . com / docs / guides /   \n382 embeddings/embedding-models.   \n383 [22] OpenAI et al. GPT-4 Technical Report. 2024. arXiv: 2303.08774 [cs.CL].   \n384 [23] F. Pedregosa et al. \u201cScikit-learn: Machine Learning in Python\u201d. In: Journal of Machine   \n385 Learning Research 12 (2011), pp. 2825\u20132830.   \n386 [24] Jeffrey Pennington, Richard Socher, and Christopher Manning. \u201cGloVe: Global Vectors for   \n387 Word Representation\u201d. In: Proceedings of the 2014 Conference on Empirical Methods in   \n388 Natural Language Processing (EMNLP). Ed. by Alessandro Moschitti, Bo Pang, and Walter   \n389 Daelemans. Doha, Qatar: Association for Computational Linguistics, Oct. 2014, pp. 1532\u2013   \n390 1543. DOI: 10.3115/v1/D14-1162. URL: https://aclanthology.org/D14-1162.   \n391 [25] Alec Radford et al. \u201cLanguage models are unsupervised multitask learners\u201d. In: OpenAI blog   \n392 1.8 (2019), p. 9.   \n393 [26] Nils Reimers and Iryna Gurevych. \u201cSentence-BERT: Sentence Embeddings using Siamese   \n394 BERT-Networks\u201d. In: Proceedings of the 2019 Conference on Empirical Methods in Natural   \n395 Language Processing. Association for Computational Linguistics, Nov. 2019. URL: https:   \n396 //arxiv.org/abs/1908.10084.   \n397 [27] Ashish Vaswani et al. \u201cAttention is All you Need\u201d. In: Advances in Neural Informa  \n398 tion Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017.   \n399 URL: https : / / proceedings . neurips . cc / paper _ files / paper / 2017 / file /   \n400 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.   \n401 [28] Ashish Vaswani et al. \u201cTensor2Tensor for Neural Machine Translation\u201d. In: Proceedings   \n402 of the 13th Conference of the Association for Machine Translation in the Americas (Vol  \n403 ume 1: Research Track). Ed. by Colin Cherry and Graham Neubig. Boston, MA: Asso  \n404 ciation for Machine Translation in the Americas, Mar. 2018, pp. 193\u2013199. URL: https:   \n405 //aclanthology.org/W18-1819.   \n406 [29] Laura Weidinger et al. \u201cTaxonomy of risks posed by language models\u201d. In: Proceedings of the   \n407 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022, pp. 214\u2013229.   \n408 [30] Thomas Wolf et al. HuggingFace\u2019s Transformers: State-of-the-art Natural Language Process  \n409 ing. 2020. arXiv: 1910.03771 [cs.CL].   \n410 [31] Linting Xue et al. mT5: A massively multilingual pre-trained text-to-text transformer. 2021.   \n411 arXiv: 2010.11934 [cs.CL].   \n412 [32] Wu Youyou, Michal Kosinski, and David Stillwell. \u201cComputer-based personality judgments   \n413 are more accurate than those made by humans\u201d. In: Proceedings of the National Academy of   \n414 Sciences 112.4 (2015), pp. 1036\u20131040. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "415 5 Annex1 : Supplementary material ", "text_level": 1, "page_idx": 11}, {"type": "image", "img_path": "XkMCKoHNCD/tmp/dc630bb2becc7b5638de2320e2b0de0715514f3f07d6034d35b2b5df8609a611.jpg", "img_caption": ["416 5.1 Average Position for each main character of the two Tv series "], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "Figure 5: Projection of the two first PCAs, and their respective interpretation, with the average position for each main character of the two Tv series ", "page_idx": 11}, {"type": "text", "text": "417 5.2 Accuracy curves the two Tv serie ", "page_idx": 11}, {"type": "image", "img_path": "XkMCKoHNCD/tmp/9550c2c576ab9bfce3fab014e774f1b0011316476c111723177667289251eea2.jpg", "img_caption": ["Figure 6: Accuracy curves to assess the performance of the logistic regression, by increasing the number of dimensions, in the dialogue lines\u2019s prediction for two different couples for the two Tv serie "], "img_footnote": [], "page_idx": 11}, {"type": "text", "text": "418 5.3 Friends Analysis ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "419 For Friends, we analyse similarly the 6 first dimensions as seen in Figure 7. The PCA1 is interpreted   \n420 as phrase that include a name to \u2019Hey\u2019. This is illustrate with the figure 7a by the dark blue cluster in   \n421 the left with the average phrase \u2019Ms. Monroe... Oh there you go\u2019, for the negative larger values of the   \n422 PCA1, and by the red cluster on the top right with average phrase \u2019Hey\u2019 for the positive larger values   \n423 of the PCA1. The qualitative analysis, in annex 7, gives as the most extreme negative value of the   \n424 PCA1 the phrase \u2019Yeah. It\u2019s just gonna be too hard. Y\u2019know? I mean, it\u2019s Ross. How can I watch   \n425 him get married? Y\u2019know it\u2019s just, it\u2019s for the best, y\u2019know it is, it\u2019s... Y\u2019know, plus, somebody\u2019s got   \n426 to stay here with Phoebe! Y\u2019know she\u2019s gonna be pretty big by then, and she needs someone to help   \n427 her tie her shoes; drive her to the hospital in case she goes into labour.\u2019. The most extreme positive   \n428 value of the PCA1 is \u2019Hey\u2019. The qualitative confirm our earlier statement about the interpretation of   \n429 the PCA1.   \n430 The PCA2 is phrase that include \u2019yeah\u2019 to \u2019Hi\u2019. The negative values of the PCA2 can be found on the   \n431 figure 7a, for example from the light green cluster of at the bottom left, with average phrase \u2019Um,   \n432 yeah.\u2019, and the positive value are on the top red cluster with average phrase \u2019Hey\u2019. It is confirm from   \n433 the qualitative analysis in annex 7, give the most extreme negative value \u2019Yeah, fair enough.\u2019 and the   \n434 most extreme positive value \u2019Hey! Hi!\u2019.   \n435 The PCA3 is phrase that express the willingness to help and support someone to phrase that include a   \n436 name.The projected values of the PCA3 are on the figure 7b, the negative values are on the left of the   \n437 graph, for example, the orange cluster with average phrase \u2019Did you like learn about her family?\u2019. In   \n438 regards of the positive values, they are on the right of the graph, for example the light green cluster   \n439 with average phrase \u2019Okay! Okay, you\u2019re yelling again! See that?\u2019. The qualitative analysis, see   \n440 annex 7, shows the most extreme negative value of the PCA3 is \u2019Phoebe?! Wait a-but-but she just,   \n441 she said that Joey was her backup.\u2019 and the most extreme positive value is \u2019 Hi! I\u2019m back. Yeah, that   \n442 sounds great. Okay. Well, we\u2019ll do it then. Okay, bye-bye.\u2019   \n443 The PCA4 interpretation is about phrase that include \u2019what\u2019 or \u2019oh my God\u2019, for example in the   \n444 figure $^{7\\mathrm{{b}}}$ in the dark purple cluster in the bottom left with average phrase \u2019What was that?\u2019, to phrase   \n445 about relationship and with the name, for example in the figure 7b with the dark green cluster with   \n446 average phrase \u2019Oh hey, I\u2019d shake your hand but uh: I\u2019m really into the game. Plus, I think it\u2019d be   \n447 better for my ego if we didn\u2019t stand rigt to each other.\u2019. The qualitative analysis, in annex 7, confirm   \n448 our statement with the following most extreme negative value \u2019What?! What is it?!. and the most   \n449 extreme positive value \u2019Well it\u2019s okay. Chandler is talking to her.\u2019   \n450 PCA5 is phrase about character relationship to phrase that include agreement. As seen in the figure   \n451 7c, the negative value of PCA5 are represented on the graph on the left, for example with the light   \n452 blue cluster with average phrase \u2019So, um, have you told your parents?\u2019. The positive value of PCA5   \n453 are on the right of the figure 7c, as we can pick out from the dark purple cluster with average phrase   \n454 \u2019No, but Ross. We are never gonna happen, OK. Accept that.\u2019. The qualitative analysis verify our   \n455 interpretation, in annex 7, we see that the most extreme negative value is the phrase \u2019But, also, what   \n456 happened between you and your Mom?. and the most extreme positive value is \u2019Yeah! That would be   \n457 great!\u2019.   \n458 We interpret the PCA6 as phrase that question a name to phrase about marriage and proposal. The   \n459 PCA6 projection is illustrate in the figure 7c, with negative values as the bottom, with for example   \n460 the cluster dark orange with average phrase \u2019Yeah, Chandler why don\u2019t you take a walk? This doesn\u2019t   \n461 concern you.\u2019. The positive value of the PCA6 are in the top of the graph, for example dark green   \n462 cluster with average phrase \u2019Okay, come on, I can\u2019t get married until I get something old, something   \n463 new, something borrowed, and something blue\u2019. Our statement confirmed by the qualitative analysis,   \n464 in annex 7, with the most extreme negative value is the phrase \u2019Wait a minute. What\u2019s his name?\u2019   \n465 and the most extreme positive value is the phrase \u2019Yes! We\u2019re getting married?!\u2019. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "image", "img_path": "XkMCKoHNCD/tmp/be2766699088bc5da3489c819b4246fa010a90783545b221c0bfdcd5274316c9.jpg", "img_caption": ["Figure 7: Projection of the first 6 PCAs. Each PCA has an interpretation from the qualitative analysis. Each plot has their respective cluster along with the average phrase of each cluster for Friends dialogue lines ", "(c) Projection of PCA5 and PCA6 "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "XkMCKoHNCD/tmp/c9c453cdf2f8b7a6c2de5e158be76423455ab081990da80313cd875bbe337e94.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 8: Regression coefficients for each possible character pairs for the TV series Friends. For each pair, we conduct a logistic regression to predict if the dialogue line is more likely to be said by a character1 such that $P$ (character1 $=1$ |the line is said by character1 or character2). We use the first 300 principal components in the logistic regression. Then, we assess the absolute value of each coefficient to determine their magnitude. Following this, we select the top ten coefficients for each linear predictor function. We report in this figure those coefficients, along with their corresponding dimensions. The coefficients are in decreasing order from left to right: the left side have the coefficient with the highest magnitude, the right side have the coefficients with the lowest magnitude. The rows are arrange such that the first row (the most significant coefficients) is in increasing order ", "page_idx": 14}, {"type": "text", "text": "466 In the case of the TV series Friends, in the figure 8, the first column are the most significant regression   \n467 coefficient for each pair. We can notice that the most extreme negative value is in the first row   \n468 and belongs to the regression coefficient of the character\u2019s dialogue lines prediction between Joey   \n469 and Monica. The probability is as follow, $P$ $^{\\ge}\\left(\\mathrm{Joey}=1|\\right.$ the line is said by Joey or Monica) $=p$ and   \n470 $P$ $^{\\circ}\\,({\\mathrm{Monica}}=0|)$ the line is said by Joey or Monica)) $)=1-p$ . The corresponding dimension of the   \n471 first coefficient is the PCA 18, it depicts phrase from \u2019Oh no\u2019 to phrase that include \u2019yeah\u2019 or   \n472 \u2019look\u2019 (see qualitative analysis in annex ??. In other words, a phrase that include \u2019Oh no\u2019 is more   \n473 likely from Joey. The most extreme positive value in this first column appears in the last row,   \n474 corresponding to the regression coefficients for predicting dialogue lines between the pair \u2019Rachel   \n475 and Monica\u2019. The probability is such that $P$ $\\mathrm{[Rachel=1|}$ the line is said by Rachel or Monica) $=p$   \n476 and $P$ (Monica $=0|$ the line is said by Rachel or Monica) $=1-p$ . The coefficient correspond to the   \n477 dimension PCA17: from phrase that include \u2019Joey\u2019 to phrase that include \u2019Ross\u2019. We can deduce that,   \n478 if a phrase include \u2019Ross\u2019 it is more likely from Rachel. ", "page_idx": 14}, {"type": "text", "text": "From phrase that include \u2019Oh\u2019,   \nPCA 9 8 occurrences to question about what the people has been doing From \u2019Oh no\u2019   \nPCA 18 8 occurrences to phrase that include \u2019yeah\u2019 or \u2019look\u2019   \nPCA 7 7 occurrences From phrase which is an answer a statement to \u2019What?\u2019 From phrase that include \u2019Joey\u2019   \nPCA 17 6 occurrences to phrase that include \u2019Ross\u2019 From phrase about a statement on a character   \nPCA 16 6 occurrences to question with \u2019What\u2019 ", "page_idx": 14}, {"type": "text", "text": "Table 1: Interpretation of the most important dimension in the dialogue lines prediction in Friends, with the number of time they occurs in the figure 8 ", "page_idx": 14}, {"type": "text", "text": "479 For Friends, we also count the occurrences of each PCA from the figure 8, and then interpret them.   \n480 We recapitulate the information in the table 1. Contrary to The Big Bang Theory the phrases in   \n481 Friends are much shorter, more exclamatory, and there are less obvious topic like food or comics.   \n482 In the TV series Friends, we note fewer instances of the main principal component analysis. For   \n483 instance, in The Big Bang Theory, PCA19 occurs most frequently, appearing 12 times. However, in   \n484 Friends, PCA9 and PCA18 are the most common dimensions, each occurring 8 times. If we count the   \n485 number of different PCA in figure 3 for The Big Bang Theory we obtain 59, and 56 different PCA for   \n486 Friends in the figure 8. The number of dimension is similar in both case, but we can pick out that the   \n487 magnitude of the coefficient is slightly higher in The Big Bang Theory than in Friends. Since the TV   \n488 serie Friends has less occurrences of the main PCAs, smaller magnitude in the regression coefficients   \n489 and less AUC accuracy, therefore more dimension are needed into the dialogue line predictions. This   \n490 is visible on the figure 5, where we can see that average position of the character in Friends are more   \n491 closer than the average position of the character in The Big Bang Theory.   \n492 In the figure 9, we show the relationship between the character of Friends for the PCA9, the   \n493 dimension that have the most occurrences, it is interpret as with phrase that include \u2019Oh\u2019 to question   \n494 about what the people has been doing. For example, in the dialogue lines prediction $P({\\tt R a c h e l}=$   \n495 1|the line is said by Rachel or Ross), the regression coefficient is positive, then if it is a question   \n496 about what the people has been doing, it is more likely from Rachel, and if it is a phrase that include   \n497 \u2019Oh\u2019, then it is more likely to be from Ross. Then the arrow goes from Rachel to Ross. If the   \n498 regression coefficient is negative, for example when we want to predict a dialogue line such that   \n499 $P({\\mathrm{Rachel}}=1|$ the line is said by Rachel or Joey), then if a phrase include \u2019Oh\u2019 it is more likely to   \n500 be said by Rachel, and if it is a question about what the people has been doing, it is more likely from   \n501 Joey. The arrows goes from Joey to Rachel. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "XkMCKoHNCD/tmp/39351d0edaea02b813395be3c233d7fc79cf10c2b4b6e93c859364d54d75f9bd.jpg", "img_caption": ["Figure 9: Relationship between characters in Friends for the dimension that occurs the most in Figure 9 (PCA9) with phrase that include \u2019Oh\u2019, to question about what the people has been doing. The person at the start of the arrow ask more about what the people has been doing more than they have phrase that include \u2019Oh\u2019 to the person at the end of the arrow. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "502 6 Annex 2: Dialogue example of The Big Bang Theory ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "503 6.1 PCA1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "504 6.1.1 Lowest coefficient ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "505 SHELDON : Yeah.   \n506 LEONARD : Yeah.   \n507 LEONARD : Yeah.   \n508 LEONARD : Yeah.   \n509 SHELDON : Yeah.   \n510 LEONARD : Yeah.   \n511 PENNY : Yeah.   \n512 PENNY : Yeah.   \n513 PENNY : Yeah.   \n514 SHELDON : Yeah. ", "page_idx": 16}, {"type": "text", "text": "515 6.1.2 Highest coefficient ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "516 BERNADETTE : You know, I was thinking. Without Sheldon, most of us would have   \n517 never met, but Penny would still live across from him.   \n518 AMY : Which couldn\u2019t have happened if you didn\u2019t live across the hall from her, which   \n519 couldn\u2019t have happened without Sheldon. Same goes with you guys. If Leonard   \n520 wasn\u2019t with Penny, she never would have set you up.   \n521 PENNY : Oh, my God, Sheldon the genius is jealous of Leonard.   \n522 HOWARD : Now, I never thought I\u2019d say this, but I\u2019m kind of excited to see Sheldon.   \n523 AMY : This isn\u2019t about me and Sheldon. This is about Rajesh moving in with Leonard   \n524 and Penny.   \n525 RAJ : It\u2019s a human emotion, Sheldon. Everyone gets jealous. I\u2019m jealous of Leonard   \n526 and Penny and Howard and Bernadette for being in such happy relationships.   \n527 LEONARD : Oh, come on. Sheldon, have you ever once heard me say that I don\u2019t trust   \n528 Penny? Sheldon? Where did he go?   \n529 PENNY : Well, yeah, he\u2019d been living with Sheldon.   \n530 LEONARD $:$ Really. Who do you think did that, Sheldon?   \n531 AMY : Well, I was hoping the next person I dated would be a little less like Sheldon. ", "page_idx": 16}, {"type": "text", "text": "532 6.2 PCA2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "533 6.2.1 Lowest coefficient ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "534   \n535   \n536   \n537   \n538   \n539   \n540 ", "page_idx": 16}, {"type": "text", "text": "AMY : Well, there was the time I had my tonsils out, and I shared a room with a little Vietnamese girl. She didn\u2019t make it through the night, but up till then, it was kind of fun.   \nBERNADETTE : Because it would make you seem like something she already thinks you are.   \nBERNADETTE : You don\u2019t think she\u2019d actually send you something gross or dangerous, do you? ", "page_idx": 16}, {"type": "text", "text": "541 LEONARD : Too expensive. You\u2019d think I\u2019d be used to women withholding their love. I   \n542 mean, my mother did. I mean, no matter how hard I tried, she just didn\u2019t have any   \n543 interest in me.   \n544 LEONARD : I mean, I know she\u2019s not my girlfriend or anything, but wouldn\u2019t you think   \n545 she\u2019d feel a little bad that I\u2019m going to be gone for the whole summer?   \n546 SHELDON : Or you might think she thinks you think it\u2019s a date even though she doesn\u2019t.   \n547 LEONARD : Yeah, yeah, that\u2019s the fun part. We\u2019re also getting new curtains for my   \n548 bedroom, and a dust ruffle, and a duvet, and I don\u2019t even know what a duvet is   \n549 but I\u2019m pretty sure if I did I wouldn\u2019t want one, but every time I talk to her about   \n550 moving out she cries and we have sex.   \n551 AMY : Parental pressure can be daunting. I remember the battle with my mother about   \n552 shaving my legs. Last year, I finally gave in and let her do it.   \n553 LEONARD : Don\u2019t you think if a woman was living with me I\u2019d be the first one to know   \n554 about it?   \n555 SHELDON $:$ I was hoping she might listen to you about the dangers of owning unhygienic   \n556 furniture. ", "page_idx": 17}, {"type": "text", "text": "557 6.2.2 Highest coefficient ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "558 LEONARD : Leonard, Sheldon.   \n559 LEONARD : Hi, I\u2019m Leonard, this is Sheldon.   \n560 HOWARD $:$ What about Sheldon?   \n561 LEONARD : Sheldon...   \n562 LEONARD : Sheldon...   \n563 LEONARD : Sheldon...   \n564 LEONARD : Sheldon...   \n565 HOWARD : Sheldon.   \n566 LEONARD : Sheldon.   \n567 HOWARD : Sheldon. ", "page_idx": 17}, {"type": "text", "text": "568 6.3 PCA3 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "569 6.3.1 Lowest coefficient ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "570 SHELDON : Really? I didn\u2019t know that.   \n571 PENNY : Did they make a movie about it?   \n572 RAJ : How did that even happen? Did they know that\u2019s what they were doing when they   \n573 were doing it?   \n574 HOWARD : Yeah, I saw it on Mythbusters.   \n575 BERNADETTE : Do they have that?   \n576 SHELDON : A more plausible explanation is that his work in robotics has made an   \n577 amazing leap forward.   \n578 SHELDON : Oh. Is it true they used scuba gear to create the sound of Darth Vader   \n579 breathing?   \n580 HOWARD $:$ Not exactly. They spent a ton of money developing this dandruff medication   \n581 that had the side effect of horrible anal leakage.   \n582 SHELDON $:$ It\u2019s been around for 25 years, and has been extensively corroborated by   \n583 other researchers. ", "page_idx": 17}, {"type": "text", "text": "585 6.3.2 Highest coefficient ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "586 PENNY : Aw, sweetie, I\u2019m comfortable around you, too.   \n587 LEONARD $:$ Great. Just relax and enjoy. Tonight is all about you.   \n588 SHELDON $:$ Thank you, but I\u2019ll be fine.   \n589 PENNY : Okay, well, we\u2019ll talk to you guys later. Bye. She said not to come. It\u2019s gonna   \n590 be a while.   \n591 SHELDON : Fine, let\u2019s go. Thank you for letting me sleep on your couch.   \n592 SHELDON $:$ Oh, well, you two sit down and get to know each other. I\u2019ll get your room   \n593 ready.   \n594 AMY : I will. I wish you were here.   \n595 LEONARD : Let\u2019s go. Okay, you two, just, have a nice... whatever this is.   \n596 PENNY : All right. Well, you guys have fun. I guess I\u2019ll see you Sunday night.   \n597 LEONARD : Yeah, no, I\u2019m fine. It\u2019s good, it\u2019s a good party, thanks for having us, it\u2019s just   \n598 getting a little late so.... ", "page_idx": 18}, {"type": "text", "text": "599 6.4 PCA4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "600 6.4.1 Lowest coefficient ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "601 SHELDON : Really? That seems rather short sighted, coming from someone who is   \n602 generally considered altogether unlikable. Why don\u2019t you take some time to   \n603 reconsider?   \n604 SHELDON : Yes, and she\u2019s not taking my feelings into account at all. Maybe it\u2019s time I   \n605 teach her a lesson.   \n606 AMY : No, we\u2019re sorry. We never should have been comparing relationships in the first   \n607 place.   \n608 HOWARD : Yeah, she was dating this guy, and I was kind of a jerk to her about it.   \n609 SHELDON $:$ Yeah, but to be fair, he only said the part about him getting sick of you.   \n610 SHELDON $:$ Oh, you\u2019re right. I could never be with a woman whose self-esteem was so   \n611 low she\u2019d be with Leonard.   \n612 SHELDON : Not true. No, look at me. I had an engagement ring to give a girl, and   \n613 instead, she rejected me. And am I emotional about that? No. No, I am sitting   \n614 here on a couch, talking about my favourite TV character like nothing happened.   \n615 \u2018Cause I am just like him, all logical, all the time.   \n616 SHELDON : It hurts that you would lie to me, Amy. I thought our relationship was based   \n617 on trust and a mutual admiration that skews in my favour.   \n618 PENNY : Okay, I have not tried to change Leonard. That\u2019s just what happens in relation  \n619 ships. Look how much Amy\u2019s changed you.   \n620 PENNY : I get that, okay? It\u2019s just, Leonard and I have been married for two years, and   \n621 we\u2019re no further along than when we were dating. ", "page_idx": 18}, {"type": "text", "text": "622 6.4.2 Highest coefficient ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "623 SHELDON : Excellent! What are you planning to wear?   \n624 HOWARD $:$ In our new minivan. Hey, what\u2019s for lunch?   \n625 BERNADETTE $:$ Where are you guys going to eat?   \n626 PENNY $:$ What beverage do you make for that?   \n627 SHELDON $:$ Oh, I have quite the evening planned. Our foetus-friendly festival of fun   \n628 begins with an in-depth look at the world of model trains, and then we\u2019ll kick   \n629 things up a notch and explore all the different ways that you can make toast.   \n630 LEONARD $:$ What are you drinking there? A little eggnog?   \n631 RAJ : Sounds great!   \n632 SHELDON : In here, you\u2019ll find emergency provisions. An eight-day supply of food and   \n633 water, a crossbow, season two of Star Trek: The Original Series on a high-density   \n634 flash drive.   \n635 AMY : I\u2019m going to the vending machine. Do you want anything?   \n636 SHELDON $:$ Greetings, gentlemen. How goes your little project? ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "637 6.5 PCA5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "638 6.5.1 Lowest coefficient ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "639 BERNADETTE : Absolutely. All we need to do is spend a little time and find something   \n640 you\u2019re passionate about.   \n641 PENNY : Okay, a simple yes will do.   \n642 BERNADETTE : Of course you can. But maybe a good rule would be to wait for people   \n643 to bring it up.   \n644 RAJ : No, no, it\u2019s a very promising area. In a perfect world I\u2019d spend several more   \n645 years on it. But I just couldn\u2019t pass up the opportunity to work with you on your   \n646 tremendously exciting and not yet conclusively disproved hypothesis.   \n647 LEONARD : Sheldon, I think this will work. Let\u2019s just try it my way.   \n648 LEONARD : If that\u2019s what you want to do, yes.   \n649 HOWARD : Yeah, this is a bad idea. We should go.   \n650 AMY : Of course. I get to be part of the first team to use radon markers to map the   \n651 structures that...   \n652 PENNY : Yeah. And there are a few things we need to stay on top of. So we thought it   \n653 would useful, and I can\u2019t believe I\u2019m about to say this, um.   \n654 LEONARD : No, I don\u2019t want to do it. You can do it. ", "page_idx": 19}, {"type": "text", "text": "655 6.5.2 Highest coefficient ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "656 HOWARD : How was she?   \n657 LEONARD : When was the last time you saw her?   \n658 LEONARD : How\u2019s your mom holding up?   \n659 AMY : Oh. What was her name?   \n660 LEONARD : How\u2019s she doing?   \n661 BERNADETTE : It was your mom.   \n662 LEONARD : Aw. What\u2019s wrong with her?   \n663 HOWARD : My mom died.   \n664 SHELDON $:$ What\u2019s her name?   \n665 HOWARD : So, what is she doing today? ", "page_idx": 19}, {"type": "text", "text": "666 6.6 PCA6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "667 6.6.1 Lowest coefficient ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "668 LEONARD : Relax, it wasn\u2019t your fault.   \n669 HOWARD $:$ I\u2019m sorry, too. It\u2019s all my fault.   \n670 AMY : Well, I didn\u2019t, and it\u2019s your fault.   \n671 PENNY $:$ I\u2019m sorry I yelled at you. It\u2019s not your fault.   \n672 LEONARD $:$ It\u2019s not your fault.   \n673 AMY $:$ It\u2019s not your fault.   \n674 SHELDON $:$ It\u2019s simple biology. There\u2019s nothing I can do about it.   \n675 HOWARD $:$ Look, I have felt terrible about this for years, and I\u2019m glad I have the   \n676 opportunity to tell you just how sorry I am.   \n677 LEONARD : This time, it\u2019s your fault.   \n678 LEONARD : Well, that\u2019s not your fault. ", "page_idx": 20}, {"type": "text", "text": "679 6.6.2 Highest coefficient ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "680 LEONARD : Sure. I\u2019d like to meet her.   \n681 LEONARD $:$ Will Amy be joining us for dinner?   \n682 BERNADETTE $:$ Maybe, if she asks.   \n683 HOWARD : Sure she would. Ma, do you mind if Bernadette stays here this weekend?   \n684 LEONARD : No, no, of course not. Just have your relationship someplace else.   \n685 SHELDON $:$ I\u2019m going to find her and ask her to marry me. And if she says yes, we can   \n686 put this behind us and resume our relationship. And if she says no, well, then she   \n687 can just ponfo miran.   \n688 HOWARD : Yes!   \n689 HOWARD : Yes!   \n690 HOWARD : Yes!   \n691 HOWARD : Yes! ", "page_idx": 20}, {"type": "text", "text": "692 6.7 PCA19 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "693 6.7.1 Highest coefficient ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "694 SHELDON : Yes. Oh, I\u2019m so excited. And I just can\u2019t hide it.   \n695 PENNY $:$ I do, it\u2019s just he wants to go to that party at the comic book store. A lot of the   \n696 guys that hang out there are kind of creepy.   \n697 LEONARD : Oh, I\u2019m just trying to find the stupid next of kin to this stupid video store   \n698 owner so I can return the DVD and see the look on Sheldon\u2019s stupid face when he   \n699 sees that I didn\u2019t let this get to me.   \n700 HOWARD : Ooh, I want to go to the comic book store. (He leaves.)   \n701 PENNY : Yeah, but those tickets only get him into Comic-Con. That dress gets me into   \n702 anywhere I want.   \n703 PENNY : No, come on, it\u2019s going to be fun, and you all look great, I mean, look at you,   \n704 Thor, and, oh, Peter Pan, that\u2019s so cute.   \n705 BERNADETTE $:$ Is it me, or is there something fun about watching him just float there?   \n706 HOWARD : Come on, Sheldon, there\u2019s so few places I can wear my jester costume.   \n707 RAJ : So, listen to what he wrote. Uh, I saw you play at the comic book store. You guys   \n708 rock. And then there\u2019s an animated smiley face raising the roof like this.   \n709 SHELDON $:$ Oh no! (He is also wearing a Flash costume.) ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "710 6.7.2 Lowest coefficient ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "711 SHELDON $:$ We can\u2019t have Thai food, we had Indian for lunch.   \n712 SHELDON $:$ It was a Monday afternoon. You joined us for Indian food.   \n713 SHELDON : Good morning, Friend Howard. Friend Raj. I see you gentlemen are   \n714 enjoying beverages. Perhaps they would taste better out of these.   \n715 RAJ : My stomach. Indian food doesn\u2019t agree with me. Ironic, isn\u2019t it?   \n716 LEONARD $:$ Well the only way we can play teams at this point is if we cut Raj in half.   \n717 LEONARD $:$ I\u2019ve always been a little confused about this. Why don\u2019t Hindus eat beef?   \n718 RAJ : Of course, but it\u2019s all Indian food. You can\u2019t find a bagel in Mumbai to save your   \n719 life. Schmear me.   \n720 SHELDON $:$ Yeah, I actually have information about Raj that would be helpful with this   \n721 discussion.   \n722 RAJ : We Indians invented them. You\u2019re welcome.   \n723 LEONARD : Here\u2019s an idea, why don\u2019t we just go out for Indian food. ", "page_idx": 21}, {"type": "text", "text": "724 6.8 PCA7 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "725 6.8.1 Highest coefficient ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "726 RAJ : He\u2019s gonna be here any second, what should we do?   \n727 PENNY $:$ What are you guys gonna do?   \n728 LEONARD $:$ What are we gonna do?   \n729 HOWARD : What are we gonna do?!   \n730 AMY : What\u2019s going on with him?   \n731 LEONARD : What are we going to do?   \n732 RAJ : So what are we going to do tonight?   \n733 LEONARD : What\u2019s with him?   \n734 HOWARD : What\u2019s with him?   \n735 PENNY $:$ What\u2019s with him? ", "page_idx": 21}, {"type": "text", "text": "736 6.8.2 Lowest coefficient ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "737 PENNY : Oh, Sheldon, are these letters from your grandmother?   \n738 PENNY $:$ I do, and you know, I don\u2019t think I\u2019ve ever thanked you properly for helping   \n739 me get it.   \n740 SHELDON $:$ Oh, yes. In fact, I improved upon it.   \n741 SHELDON : No, of course not. No, I used trickery and deceit.   \n742 LEONARD $:$ Yeah, no, I do, I use those... uh... just to polish up my... spear-fishing   \n743 equipment. I spear fish. When I\u2019m not crossbow hunting, I spear fish. Uh, Penny,   \n744 this is Sheldon\u2019s twin sister, Missy. Missy, this is our neighbour Penny.   \n745 LEONARD $:$ Yes, I\u2019ve always admired that about you.   \n746 PENNY : She was right, you know. The locus of my identity is totally exterior to me.   \n747 LEONARD : Oh, yes. Indeed, I did.   \n748 LEONARD : No, no, I\u2019m good. If my P.E. teachers had told me this is what I was training   \n749 for, I would have tried a lot harder.   \n750 RAJ : Do you kind of look like a shiny Sheldon? ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "751 6.9 PCA15 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "752 6.9.1 Highest coefficient ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "753 BERNADETTE : Yeah. You\u2019re inviting him into your home. It\u2019s intimate. It\u2019s where your   \n754 underpants live.   \n755 RAJ : It\u2019s a lease.   \n756 LEONARD $:$ What was I supposed to do? He needed a place to sleep it off.   \n757 LEONARD : Ask him for a napkin, I dare you. (There is a knock on the door.) I\u2019ll get it.   \n758 RAJ : He probably just goes to the bathroom.   \n759 HOWARD : Maybe the problem is he thinks you\u2019re available. Does he know you\u2019re   \n760 dating Sheldon?   \n761 LEONARD $:$ What if he lives in your garage?   \n762 HOWARD : How\u2019d you get him to come to your house?   \n763 BERNADETTE $:$ What are you going to do? Doesn\u2019t he know you have a boyfriend?   \n764 LEONARD : He\u2019s in his bedroom. ", "page_idx": 22}, {"type": "text", "text": "765 6.9.2 Lowest coefficient ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "766 LEONARD : Look, do I think that you are talented and that you are beautiful? Of course I   \n767 do. But isn\u2019t Los Angeles full of actresses who are just as talented, just as beautiful?   \n768 All right, look, we\u2019ll come back to that.   \n769 AMY : I do. Penny, Bernadette and I are sorry.   \n770 RAJ : Oh, yes, we\u2019ve got the moon and the trees and Elizabeth McNulty, who apparently   \n771 died when she was the same age I am.   \n772 SHELDON $:$ And on a different, but not unrelated topic, based on your current efforts to   \n773 buoy my spirits, do you truly believe that you were ever fit to be a cheer leader?   \n774 SHELDON : Hello, female children. Allow me to inspire you with a story about a great   \n775 female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of   \n776 radioactivity, she was a hero of science, until her hair fell out, her vomit and stool   \n777 became filled with blood, and she was poisoned to death by her own discovery.   \n778 With a little hard work, I see no reason why that can\u2019t happen to any of you. Are   \n779 we done? Can we go?   \n780 SHELDON : No, I don\u2019t think so. Those dolls represent three things I do not care for,   \n781 clowns, children and raggediness. I think it\u2019s a lost cause.   \n782 SHELDON : Yes. I think prolonged exposure to Penny has turned her into a bit of a   \n783 Gabby Gertie.   \n784 RAJ : Yes, isn\u2019t she an amazing actress.   \n785 SHELDON : Actually, I thought the first two renditions were far more compelling. Previ  \n786 ously I felt sympathy for the Leonard character, now I just find him to be whiny   \n787 and annoying.   \n788 HOWARD : She was just so sad all the time. I was the only person who could cheer her   \n789 up. Well, me and Ben and Jerry. ", "page_idx": 22}, {"type": "text", "text": "790 6.10 PCA17 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "791 6.10.1 Highest coefficient ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "792 SHELDON : Penny, a moment. We just had Thai food. In that culture, the last morsel   \n793 is called the krengjai piece, and it is reserved for the most important and valued   \n794 member of the group.   \n795 LEONARD : Yeah, it\u2019s delicious, the sarcasm\u2019s a little stale, though. Hey, how about   \n796 this? Until we figure out what to do with the ring, Penny holds on to it.   \n797 PENNY : Okay, sweetie, I don\u2019t know if we\u2019re gonna have cookies, or he\u2019s just gonna   \n798 say hi, or really what\u2019s gonna happen, so just let me talk, and we\u2019ll...   \n799 PENNY : Fine. What do you want?   \n800 HOWARD : Okay, this one is for a Cadbury Creme Egg.   \n801 LEONARD : Ah, well, what\u2019s this? A pot of oatmeal? Or, thanks to you, what I will now   \n802 call gloatmeal.   \n803 SHELDON $:$ I\u2019m sorry, but these are just ordinary foods with the names bent into tortured   \n804 puns. The dishes themselves are in no way Halloweenie.   \n805 LEONARD : Ah, that\u2019s a good question. Apparently someone was being awfully flirty   \n806 while not wearing their engagement ring, causing another someone to show up   \n807 here thinking the first someone might be available.   \n808 PENNY : Okay, well, I\u2019d offer you Halloween candy, but that\u2019s gone. So, what\u2019s up?   \n809 RAJ : Okay. Shall we? Oh, my God. It\u2019s light, it\u2019s flaky, it\u2019s buttery. You don\u2019t need to   \n810 have sex with him, just eat one of these. ", "page_idx": 23}, {"type": "text", "text": "811 6.10.2 Lowest coefficient ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "812 RAJ : Then she\u2019s going to have to convince your mother to let you go into space.   \n813 HOWARD : Then get out of my house.   \n814 BERNADETTE $:$ Yeah, if you want to go off the grid, you have to move out of your   \n815 mother\u2019s house.   \n816 SHELDON : I can\u2019t believe my own mother is abandoning me.   \n817 HOWARD : I will. I\u2019m obviously not going to live in my mother\u2019s house for the rest of   \n818 my life. I\u2019m not a child.   \n819 LEONARD $:$ With your career?   \n820 BERNADETTE : You\u2019re a real hero, Howard.   \n821 BERNADETTE $:$ I\u2019m proud of her. This is a great opportunity. It\u2019s nice to see her take it   \n822 seriously.   \n823 LEONARD : Also instead of just living in your mother\u2019s house, you could actually live   \n824 inside her body.   \n825 LEONARD $:$ And now you\u2019re also an astronaut. ", "page_idx": 23}, {"type": "text", "text": "826 7 Annex 3: Dialogue example of Friends ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "827 7.1 PCA1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "828 7.1.1 Highest coefficient ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "829 CHANDLER : Hey.   \n830 CHANDLER : Hey.   \n831 PHOEBE : Hey.   \n832 RACHEL : Hey.   \n833 ROSS : Hey.   \n834 MONICA : Hey.   \n835 RACHEL : Hey.   \n836 RACHEL : Hey.   \n837 CHANDLER : Hey.   \n838 ROSS : Hey. ", "page_idx": 24}, {"type": "text", "text": "839 7.1.2 Lowest coefficient ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "840 RACHEL : Yeah. It\u2019s just gonna be too hard. Y\u2019know? I mean, it\u2019s Ross. How can   \n841 I watch him get married? Y\u2019know it\u2019s just, it\u2019s for the best, y\u2019know it is, it\u2019s...   \n842 Y\u2019know, plus, somebody\u2019s got to stay here with Phoebe! Y\u2019know she\u2019s gonna be   \n843 pretty big by then, and she needs someone to help her tie her shoes; drive her to the   \n844 hospital in case she goes into labour.   \n845 RACHEL : Ross, you know what? She may need one..We\u2019re just going to have to make   \n846 our peace with that! Monica and Chandler\u2019s apartment.   \n847 JOEY : Look we\u2019ve got to find her. Phoebe just called!! Rachel\u2019s coming to tell Ross   \n848 she loves him!!   \n849 CHANDLER : Well, she\u2019s just so much fun with Joey, I just assumed, she\u2019d still be living   \n850 with him.   \n851 JOEY : Well, remember when they got in that big fight and broke up and we were all   \n852 stuck in her with no food or anything? Well, when Ross said Rachel at the wedding,   \n853 I figured it was gonna happen again, so I hid this in here.   \n854 MONICA : I can\u2019t believe this. Rachel and Joey?   \n855 RACHEL : Look Monica, getting cold feet is very common. Y\u2019know, it\u2019s-it\u2019s just because   \n856 of all the anticipation and you just have to remember that you love Chandler. And   \n857 also, I ran out on a wedding. You don\u2019t get to keep the gifts.   \n858 MONICA : No, look, she\u2019s obviously unstable, okay? I mean she\u2019s thinking about   \n859 running out on her wedding day. Okay, fine! But I mean, look at the position she\u2019s   \n860 putting him in! What\u2019s he gonna do? Ross is gonna run over there on the wedding   \n861 day and break up the marriage?! I mean, who would do that?! Okay, fine, all right,   \n862 but that\u2019s y\u2019know, it\u2019s different! Although it did involve a lot of the same people.   \n863 PHOEBE : Why do you think, she\u2019s having so much fun living with Joey?   \n864 PHOEBE $:$ It\u2019s so weird seeing Ross and Rachel with a baby. It\u2019s just so grown up. ", "page_idx": 24}, {"type": "text", "text": "865 7.2 PCA2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "866 7.2.1 Highest coefficient ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "867 RACHEL : Hey! Hi! ", "page_idx": 24}, {"type": "text", "text": "868 RACHEL : Hey! Hi!   \n869 ROSS : Hey! Hi!   \n870 PHOEBE : Hey! Hi!   \n871 RACHEL : Hey! We\u2019re here!   \n872 RACHEL : Hi!!   \n873 RACHEL : Hi!!   \n874 MONICA : Hi!   \n875 MONICA : Hi!   \n876 MONICA : Hi! ", "page_idx": 25}, {"type": "text", "text": "877 7.2.2 Lowest coefficient ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "878 RACHEL : Yeah, fair enough.   \n879 RACHEL : Really? You think so?   \n880 PHOEBE : Really? You think?   \n881 PHOEBE : Yeah, what\u2019s your point?   \n882 PHOEBE : Yeah, but not just that.   \n883 RACHEL : No, you\u2019re right, you are absolutely right. I mean that makes, that makes   \n884 everything different.   \n885 JOEY : No. Really?   \n886 ROSS : Really? Its not just frowned upon?   \n887 JOEY : Yeah, I wouldn\u2019t know about that.   \n888 CHANDLER : Yeah, you\u2019re right about that. ", "page_idx": 25}, {"type": "text", "text": "889 7.3 PCA3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "890 7.3.1 Highest coefficient ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "891 CHANDLER : Hi! I\u2019m back. Yeah, that sounds great. Okay. Well, we\u2019ll do it then. Okay,   \n892 bye-bye.   \n893 ROSS : I\u2019ll do it. Hey, whatever you need me to do, I\u2019m your man. Whoa-oh-whoa! Are   \n894 you, are you okay?   \n895 RACHEL : No, come on, I\u2019m totally ok. I don\u2019t need you to come! I can totally handle   \n896 this on my own.   \n897 ROSS : I\u2019ll help you. Yeah, I\u2019ll make up a schedule and make sure you stick to it. And   \n898 plus, it\u2019ll give me something to do.   \n899 JOEY : Alright, alright. I\u2019m around. Go ahead.   \n900 PHOEBE : Anyway, I should go. Okay, bye.   \n901 MONICA : Ok first of all...It would be great. But that\u2019s not what I\u2019m here to talk to you   \n902 about. I need to borrow some money.   \n903 MONICA : No, I\u2019ll do it. You just stick to your job.   \n904 ROSS : Oh, that\u2019d be great! Okay, but if you do, make sure it seems like you\u2019re there to   \n905 see him, okay, and you\u2019re not like doing it as a favour to me.   \n906 JOEY : Sure, yeah. I don\u2019t have time to say thank you because I really gotta go. ", "page_idx": 25}, {"type": "text", "text": "907 7.3.2 Lowest coefficient ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "908 RACHEL : Phoebe?! Wait a-but-but she just, she said that Joey was her backup.   \n909 MONICA $:$ They thought Joey was a child?   \n910 CHANDLER : And then Joey remembered something.   \n911 RACHEL : I thought it was Chandler!   \n912 MONICA $:$ Does it have to do with Joey?   \n913 RACHEL : Joey! Why did you tell Chandler that Monica was getting a boob job?   \n914 MONICA : And Rachel. And that\u2019s Chandler.   \n915 RACHEL : And that\u2019s Phoebe , and that\u2019s Joey.   \n916 RACHEL : And that\u2019s Phoebe , and that\u2019s Joey.   \n917 ROSS : Phoebe that\u2019s not true. ", "page_idx": 26}, {"type": "text", "text": "918 7.4 PCA4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "919 7.4.1 Highest coefficient ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "920 ROSS : Well it\u2019s okay. Chandler is talking to her.   \n921 JOEY : I said a little bit Ross. Now, how about you Chandler?   \n922 JOEY : Okay. I\u2019m Chandler   \n923 JOEY : Hey look Ross, you need to understand something okay? I uh...I am never gonna   \n924 act on this Rachel thing, okay? I-I would never do anything to jeopardize my   \n925 friendship with you.   \n926 JOEY : It\u2019s okay, Ross, alright? I totally understand. Of course you\u2019re not fine. You\u2019re..   \n927 You\u2019re Ross and Rachel.   \n928 JOEY : I\u2019m fine, I\u2019m fine, it\u2019s just, it\u2019s just weird what\u2019s happening with her and Ross.   \n929 You know, yesterday he asked me to fix him up with somebody.   \n930 RACHEL : All right. So you\u2019re telling me that there is nothing going on between you   \n931 and Chandler.   \n932 ROSS : Fine, fine, Rachel your with Monica, Joey you\u2019re with me.   \n933 PHOEBE : Okay. Oh umm, Chandler, Monica is looking for you.   \n934 ROSS : Umm, okay, yeah, sure. But wh-what\u2019s wrong with Monica and Chandler? ", "page_idx": 26}, {"type": "text", "text": "935 7.4.2 Lowest coefficient ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "936 MONICA : What?! What is it?!   \n937 MONICA $:$ Oh my God! I love that!   \n938 JOEY : What the hell is that?!!   \n939 JOEY : What the hell!   \n940 ROSS : What?! It is?!   \n941 RACHEL : Oh my God! That\u2019s the creepiest thing I\u2019ve ever heard!   \n942 RACHEL : Oh my God! Look at this!   \n943 MONICA $:$ What?! What is it?   \n944 ROSS : I can\u2019t believe this!!   \n945 JOEY $:$ What?! What?!! What is it?! ", "page_idx": 26}, {"type": "text", "text": "946 7.5 PCA5", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "947 7.5.1 Highest coefficient ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "948 RACHEL : Yeah! That would be great!   \n949 MONICA : Yeah, that\u2019d be great! Thank you!   \n950 JOEY : Yeah! Yeah! That would be very helpful! Yeah.   \n951 CHANDLER : All right, ready?   \n952 ROSS : All right, ready?   \n953 CHANDLER : All right, ready?   \n954 PHOEBE : All right, ready?   \n955 MONICA : All right, you ready?   \n956 PHOEBE : Sure, yeah!   \n957 JOEY : Sure. Yep. ", "page_idx": 27}, {"type": "text", "text": "958 7.5.2 Lowest coefficient ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "959 PHOEBE : But, also, what happened between you and your Mom?   \n960 JOEY : She was nothing compared to you.   \n961 JOEY : She was nothing compared to you.   \n962 CHANDLER : Hey that\u2019s what I tell girls about me.   \n963 JOEY : Me too. I mean I...haven\u2019t thought at all about how I put myself out there and   \n964 said all that stuff and how you didn\u2019t feel the same way about me and-and how it   \n965 was really awkward.   \n966 ROSS : Well, well I am married. Even though I haven\u2019t spoken to my wife since the   \n967 wedding.   \n968 PHOEBE : Oh, because, you know... they don\u2019t like you.   \n969 MONICA : Well, um, because mainly, um, they don\u2019t like you. I\u2019m sorry.   \n970 CHANDLER : Well it couldn\u2019t have been worse. A woman literally passed through me.   \n971 OK, so what is it, am I hideously unattractive?   \n972 ROSS : Hey, whatever it is, I am sure it has happened to me. Y\u2019know, actually once-once   \n973 I got dumped during sex. ", "page_idx": 27}, {"type": "text", "text": "974 7.6 PCA6 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "975 7.6.1 Highest coefficient ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "976 ROSS : Yes! We\u2019re getting married?!   \n977 JOEY : No! No, and I did not ask her to marry me!   \n978 ROSS : N-no! Okay? We\u2019ve been through this! We\u2019re not gonna get married just because   \n979 she\u2019s pregnant, okay?   \n980 JOEY : Well all right then, I guess I shouldn\u2019t get to excited about the fact that I just   \n981 kissed her!   \n982 CHANDLER : OH...MY...GAWD! I am so sorry sweetie, are you okay? You didn\u2019t tell   \n983 her we were getting married, did you?   \n984 ROSS : Hey! I offered to marry her!   \n985 CHANDLER : How can I not be upset? Okay? I finally fall in love with this fantastic   \n986 woman and it turns out that she wanted you first!   \n987 PHOEBE : You\u2019re still gonna go out with her?!   \n988 ROSS : Yeah? Oh-oh, she\u2019d be so excited!   \n989 ROSS : Okay. I did divert her and we ended up having a great time! Okay? ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "990 7.6.2 Lowest coefficient ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "991 PHOEBE : Wait a minute. What\u2019s his name?   \n992 MONICA $:$ Hey. It\u2019s him. Who is it?   \n993 MONICA : Nothing, I don\u2019t know.   \n994 JOEY : Seriously, who is this guy?   \n995 JOEY $:$ Who the hell is this guy?   \n996 RACHEL : Who are these men?   \n997 PHOEBE : Come on, give me something. What\u2019s his name?   \n998 CHANDLER : There\u2019s the man.   \n999 MONICA $:$ Who, who are they?   \n1000 ROSS : C\u2019mon, what\u2019s his name? ", "page_idx": 28}, {"type": "text", "text": "1001 7.7 PCA9 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1002 7.7.1 Highest coefficient ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1003 CHANDLER : What are you guys doing together?   \n1004 RACHEL : So what are you guys going to do?   \n1005 ROSS : What are you guys doing later?   \n1006 MONICA : So, what have you guys been doing?   \n1007 ROSS : Well, I\u2019m gonna go see her. I want to bring her something, what do you think   \n1008 she\u2019ll like?   \n1009 MONICA $:$ What are you guys gonna do?   \n1010 ROSS : So uh, any ideas for the bachelor party yet?   \n1011 RACHEL : What\u2019re you guys doing out here?   \n1012 ROSS : Hey, what have you guys been up to?   \n1013 RACHEL : Hey, what have you guys been up to? ", "page_idx": 28}, {"type": "text", "text": "1014 7.7.2 Lowest coefficient ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1015 PHOEBE : Oh, okay, oh.   \n1016 ROSS : Oh. Oh! Oh my God! Okay, I know this, give me-give me a second!   \n1017 PHOEBE : All right-Ooh! Oh dead God, save me!   \n1018 RACHEL : Oh-oh, sorry, it\u2019s this way, it\u2019s this way.   \n1019 RACHEL : Oh, okay!   \n1020 CHANDLER : Oh, okay!   \n1021 RACHEL : Oh, okay!   \n1022 MONICA : Oh, okay!   \n1023 ROSS : Oh, you\u2019re right, I\u2019m sorry.   \n1024 JOEY $:$ Oh, oh, oh, sorry. ", "page_idx": 28}, {"type": "text", "text": "1025 7.8 PCA18 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1026 7.8.1 Highest coefficient ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1027 JOEY : Yeah, he did, look... look, it\u2019s right there on the counter! Ha-ho-ho!   \n1028 CHANDLER : Okay, did you see that?! With the inappropriate and the pinching!!   \n1029 CHANDLER : Okay, did you see that?! With the inappropriate and the pinching!!   \n1030 JOEY : Hey! Handcuffs! And fur line, nice! I didn\u2019t know you guys had it in ya!   \n1031 JOEY : Look, it was a job all right?   \n1032 CHANDLER : Look! Look! Look what the... Look what... Look what the floating heads   \n1033 did!   \n1034 ROSS : Okay, there was some staring and pointing.   \n1035 MONICA : Yeah, yeah, it\u2019s interesting.. but y\u2019know what? Just for fun, let\u2019s see what it   \n1036 looked like in the old spot. Alright, just to compare. Let\u2019s see. Well, it looks good   \n1037 there too. Let\u2019s just leave it there for a while.   \n1038 JOEY : Uh, take a look at the guy\u2019s pants! I mean, I know you told us to show excitement,   \n1039 but don\u2019t you think he went a little overboard?   \n1040 RACHEL : Yeah, he did! Oh, see, this is what I\u2019m talking about! ", "page_idx": 29}, {"type": "text", "text": "1041 7.8.2 Lowest coefficient ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1042 RACHEL : Oh no.   \n1043 PHOEBE : Oh no.   \n1044 PHOEBE : Oh no.   \n1045 RACHEL : Oh no.   \n1046 CHANDLER : Oh no.   \n1047 PHOEBE : Oh no.   \n1048 ROSS : Oh no.   \n1049 PHOEBE : Oh no.   \n1050 PHOEBE : Oh no.   \n1051 ROSS : Oh no. ", "page_idx": 29}, {"type": "text", "text": "1052 7.9 PCA7 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1053 7.9.1 Highest coefficient ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1054 CHANDLER : What? What?   \n1055 CHANDLER : What? What?   \n1056 ROSS : What? What?   \n1057 ROSS : What? What?   \n1058 PHOEBE : What? What?   \n1059 ROSS : What? What?   \n1060 JOEY : What? What?   \n1061 ROSS : What? What?   \n1062 ROSS : What? What?   \n1063 MONICA $:$ What? ", "page_idx": 29}, {"type": "text", "text": "1064 7.9.2 Lowest coefficient ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1065 JOEY : Yeah, yeah, I met this woman.   \n1066 MONICA : Yes but my mom got me this job.   \n1067 PHOEBE : Yes, yes I do. God, oh it\u2019s just perfect! Wow! I bet it has a great story behind   \n1068 it too. Did they tell you anything? Like y\u2019know where it was from or...   \n1069 PHOEBE : No, not usually. But yeah, I could use one right now.   \n1070 PHOEBE : Yeah, kinda.   \n1071 MONICA $:$ Yeah, just like the one in the poem.   \n1072 CHANDLER : Yes, money well spent!   \n1073 PHOEBE : No! But it\u2019s the nicest kitchen, the refrigerator told me to have a great day.   \n1074 CHANDLER : Yeah, I remember.   \n1075 MONICA : No. But I remember people telling me about it. ", "page_idx": 30}, {"type": "text", "text": "1076 7.10 PCA17 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1077 7.10.1 Highest coefficient ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1078 RACHEL : And um, what-what is that Ross?   \n1079 RACHEL : Ross\u2019s what?   \n1080 RACHEL : Ok, Ross, Ross, ok listen, what we have is amazing.   \n1081 CHANDLER : Oh, that\u2019s Ross\u2019s.   \n1082 CHANDLER : Oh, that\u2019s Ross\u2019s.   \n1083 RACHEL : Ross, I...   \n1084 RACHEL : For Ross, Ross, Ross.   \n1085 RACHEL : Well-well, I don\u2019t know Ross-really?   \n1086 RACHEL : Well-well, I don\u2019t know Ross-really?   \n1087 RACHEL : Um... Ross? ", "page_idx": 30}, {"type": "text", "text": "1088 7.10.2 Lowest coefficient ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1089 MONICA : Hey, Joey, I don\u2019t think that you should leave Chandler alone. I mean it\u2019s   \n1090 only been two days since he broke up with Kathy. Maybe you can go fishing next   \n1091 week?   \n1092 JOEY : Chandler, you have to start getting over her. All right, if you play, you get some   \n1093 fresh air, maybe it\u2019ll take your mind off Janice, and if you don\u2019t play, everyone will   \n1094 be mad at you \u2019cause the teams won\u2019t be even. Come on.   \n1095 PHOEBE : Joey? How could you just let them leave?   \n1096 CHANDLER : Look, Joey, Kathy is clearly not fulfilling your emotional needs. But   \n1097 Casey, I mean granted I only saw the back of her head, but I got this sense that   \n1098 she\u2019s-she\u2019s smart, and funny, and gets you.   \n1099 MONICA $:$ Wait a minute...Joey. Joey you can\u2019t ask her out, she\u2019s your roommate. It-it\u2019ll   \n1100 be way too complicated.   \n1101 PHOEBE : Okay, but try and get Joey too.   \n1102 ROSS : No Joey! Look why don\u2019t, why don\u2019t we just let her decide? Okay? Hey-hey,   \n1103 we\u2019ll each go out with her one more time. And-and we\u2019ll see who she likes best.   \n1104 RACHEL : Yeah, Joey kinda disabled it when I moved in.   \n1105 MONICA $:$ Joey that is horriable.   \n1106 CHANDLER : No, see the thing is I want to get out of here before Joey gets all worked   \n1107 up and starts calling everybody bitch. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "1108 7.11 PCA16 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1109 7.11.1 Highest coefficient ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1110 RACHEL : Oh, oh. . What is this?   \n1111 PHOEBE : Oh, yeah. What\u2019s this?   \n1112 JOEY : I don\u2019t know. It\u2019s-it\u2019s just...lately, I\u2019ve been feeling... Okay, here\u2019s what it is...   \n1113 You know what? I feel a lot better, thanks!   \n1114 PHOEBE : Ohh. What is this?   \n1115 CHANDLER : Oh-oh, what are you doing?   \n1116 PHOEBE : Oh that\u2019s so great! Ohh, so what\u2019s going on now?   \n1117 PHOEBE : Oh my God, what\u2019s it doing here?   \n1118 JOEY : Yeah! Yeah, why? What\u2019s up?   \n1119 PHOEBE : Oh, why? What\u2019s up?   \n1120 PHOEBE : What-what\u2019s up? ", "page_idx": 31}, {"type": "text", "text": "1121 7.11.2 Lowest coefficient ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1122 CHANDLER : And then he did.   \n1123 PHOEBE : And we did.   \n1124 ROSS : No you didn\u2019t. You said you would, but you never did!   \n1125 CHANDLER : I sure did.   \n1126 RACHEL : No, you could\u2019ve lost your job.   \n1127 ROSS : Sure, Monica would have to give her up.   \n1128 CHANDLER : Yes he did.   \n1129 RACHEL : That is not true. She did! She forced me!   \n1130 RACHEL : That is not true. She did! She forced me!   \n1131 ROSS : Monica! Would it? ", "page_idx": 31}, {"type": "text", "text": "1132 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "133 1. Claims   \n1134 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n1135 paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] . ", "page_idx": 32}, {"type": "text", "text": "Justification: We outline the limitations clearly in the last paragraph of the conclusions. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 32}, {"type": "text", "text": "Justification: There are no theoretical results or proof in this paper. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 33}, {"type": "text", "text": "Justification: The code and data are provide. The datasets are public, and all the library used are also public. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "1183   \n1184   \n1185   \n1186   \n1187   \n1188   \n1189   \n1190   \n1191   \n1192   \n1193   \n1194   \n1195   \n1196   \n1197   \n1198   \n1199   \n1200   \n1201   \n1202   \n1203   \n1204   \n1205   \n1206   \n1207   \n1208   \n1209   \n1210   \n1211   \n1212   \n1213   \n1214   \n1215   \n1216   \n1217   \n1218   \n1219   \n1220   \n1221   \n1222   \n1223   \n1224   \n1225   \n1226   \n1227   \n1228   \n1229   \n1230   \n1231   \n1232   \n1233   \n1234   \n1235   \n1236   \n1237 ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The full code used are on gitub, that include all the information that we did (test split,...). ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer:[Yes] . ", "page_idx": 34}, {"type": "text", "text": "Justification: We report standard errors for our experimental results on GPT4 and human subjects. ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors). ", "page_idx": 34}, {"type": "text", "text": "1290 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n1291 of the mean.   \n1292 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n1293 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n1294 of Normality of errors is not verified.   \n1295 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n1296 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n1297 error rates).   \n1298 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n1299 they were calculated and reference the corresponding figures or tables in the text.   \n1300 8. Experiments Compute Resources   \n1301 Question: For each experiment, does the paper provide sufficient information on the com  \n1302 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n1303 the experiments?   \n1304 Answer: [Yes] .   \n1305 Justification: There are no information about time or resources, the calculus used takes 4   \n1306 seconds (to do the PCA) to a couple of minutes (to do the embeddings), and do not required   \n1307 lots of memory   \n1308 Guidelines:   \n1309 \u2022 The answer NA means that the paper does not include experiments.   \n1310 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n1311 or cloud provider, including relevant memory and storage.   \n1312 \u2022 The paper should provide the amount of compute required for each of the individual   \n1313 experimental runs as well as estimate the total compute.   \n1314 \u2022 The paper should disclose whether the full research project required more compute   \n1315 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n1316 didn\u2019t make it into the paper).   \n1317 9. Code Of Ethics   \n1318 Question: Does the research conducted in the paper conform, in every respect, with the   \n1319 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n1320 Answer: [Yes]   \n1321 Justification: The paper conforms to the NeurIPS code of Ethics   \n1322 Guidelines:   \n1323 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n1324 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n1325 deviation from the Code of Ethics.   \n1326 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n1327 eration due to laws or regulations in their jurisdiction).   \n1328 10. Broader Impacts   \n1329 Question: Does the paper discuss both potential positive societal impacts and negative   \n1330 societal impacts of the work performed?   \n1331 Answer: [Yes] .   \n1332 Justification: The qualitative analysis shows how gender stereotyping is a large part of how   \n1333 machine learning models make predictions. The article also critiques ideas around artificial   \n1334 general intelligence (AGI) in a way we think illuminates debate on these issues.   \n1335 Guidelines:   \n1336 \u2022 The answer NA means that there is no societal impact of the work performed.   \n1337 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n1338 impact or why the paper does not address societal impact.   \n1339 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n1340 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n1341 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n1342 groups), privacy considerations, and security considerations.   \n1343 \u2022 The conference expects that many papers will be foundational research and not tied   \n1344 to particular applications, let alone deployments. However, if there is a direct path to   \n1345 any negative applications, the authors should point it out. For example, it is legitimate   \n1346 to point out that an improvement in the quality of generative models could be used to   \n1347 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n1348 that a generic algorithm for optimizing neural networks could enable people to train   \n1349 models that generate Deepfakes faster.   \n1350 \u2022 The authors should consider possible harms that could arise when the technology is   \n1351 being used as intended and functioning correctly, harms that could arise when the   \n1352 technology is being used as intended but gives incorrect results, and harms following   \n1353 from (intentional or unintentional) misuse of the technology.   \n1354 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n1355 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n1356 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n1357 feedback over time, improving the efficiency and accessibility of ML).   \n1358 11. Safeguards   \n1359 Question: Does the paper describe safeguards that have been put in place for responsible   \n1360 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n1361 image generators, or scraped datasets)?   \n1362 Answer: [NA]   \n1363 Justification: No such risks   \n1364 Guidelines:   \n1365 \u2022 The answer NA means that the paper poses no such risks.   \n1366 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n1367 necessary safeguards to allow for controlled use of the model, for example by requiring   \n1368 that users adhere to usage guidelines or restrictions to access the model or implementing   \n1369 safety filters.   \n1370 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n1371 should describe how they avoided releasing unsafe images.   \n1372 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n1373 not require this, but we encourage authors to take this into account and make a best   \n1374 faith effort.   \n1375 12. Licenses for existing assets   \n1376 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1377 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1378 properly respected?   \n1379 Answer:[Yes]   \n1380 Justification: We credits the owners of the dataset and python libraries that we used   \n1381 Guidelines:   \n1382 \u2022 The answer NA means that the paper does not use existing assets.   \n1383 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1384 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1385 URL.   \n1386 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1387 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1388 service of that source should be provided.   \n1389 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1390 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1391 has curated licenses for some datasets. Their licensing guide can help determine the   \n1392 license of a dataset.   \n1393 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n1394 the derived asset (if it has changed) should be provided.   \n1395 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n1396 the asset\u2019s creators.   \n1397 13. New Assets   \n1398 Question: Are new assets introduced in the paper well documented and is the documentation   \n1399 provided alongside the assets?   \n1400 Answer: [Yes] .   \n1401 Justification: The details about training, limitations are in the article, and the code is on   \n1402 Github.   \n1403 Guidelines:   \n1404 \u2022 The answer NA means that the paper does not release new assets.   \n1405 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n1406 submissions via structured templates. This includes details about training, license,   \n1407 limitations, etc.   \n1408 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n1409 asset is used.   \n1410 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n1411 create an anonymized URL or include an anonymized zip file.   \n1412 14. Crowdsourcing and Research with Human Subjects   \n1413 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n1414 include the full text of instructions given to participants and screenshots, if applicable, as   \n1415 well as details about compensation (if any)?   \n1416 Answer: [Yes] .   \n1417 Justification: We recruit two relatives to do the test. We give them instructions and spread  \n1418 sheets with dialogue where they have to complete with the name of the characters   \n1419 Guidelines:   \n1420 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1421 human subjects.   \n1422 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n1423 tion of the paper involves human subjects, then as much detail as possible should be   \n1424 included in the main paper.   \n1425 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n1426 or other labor should be paid at least the minimum wage in the country of the data   \n1427 collector.   \n1428 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n1429 Subjects   \n1430 Question: Does the paper describe potential risks incurred by study participants, whether   \n1431 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n1432 approvals (or an equivalent approval/review based on the requirements of your country or   \n1433 institution) were obtained?   \n1434 Answer: [No] .   \n1435 Justification: The two subjects that we asked questions were relatives that like those TV   \n1436 series immensely. There were no risks for them in answering the questions, therefore we do   \n1437 not proceed for an Ethical approval for such a small study.   \n1438 Guidelines:   \n1439 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1440 human subjects.   \n1441 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n1442 may be required for any human subjects research. If you obtained IRB approval, you   \n1443 should clearly state this in the paper. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]