[{"type": "text", "text": "Strategic Linear Contextual Bandits ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thomas Kleine Buening The Alan Turing Institute ", "page_idx": 0}, {"type": "text", "text": "Aadirupa Saha Apple ML Research ", "page_idx": 0}, {"type": "text", "text": "Christos Dimitrakakis University of Neuchatel ", "page_idx": 0}, {"type": "text", "text": "Haifeng Xu University of Chicago ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Motivated by the phenomenon of strategic agents gaming a recommender system to maximize the number of times they are recommended to users, we study a strategic variant of the linear contextual bandit problem, where the arms can strategically misreport privately observed contexts to the learner. We treat the algorithm design problem as one of mechanism design under uncertainty and propose the Optimistic Grim Trigger Mechanism (OptGTM) that incentivizes the agents (i.e., arms) to report their contexts truthfully while simultaneously minimizing regret. We also show that failing to account for the strategic nature of the agents results in linear regret. However, a trade-off between mechanism design and regret minimization appears to be unavoidable. More broadly, this work aims to provide insight into the intersection of online learning and mechanism design. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recommendation algorithms that select the most relevant item for sequentially arriving users or queries have become vital for navigating the internet and its many online platforms. However, recommender systems are susceptible to manipulation by strategic agents seeking to artificially increase their frequency of recommendation [31, 33, 38]. These agents, ranging from sellers on platforms like Amazon to websites aiming for higher visibility in search results, employ tactics such as altering product attributes or engage in aggressive search engine optimization [29, 32]. By gaming the algorithms, agents attempt to appear more relevant than they actually are, often compromising the integrity and intended functionality of the recommender system. Here, the key issue lies in the agents\u2019 incentive to manipulate the learning algorithm to maximize their utility (i.e., profti). To address this challenge, we study and design algorithms in a strategic variant of the linear contextual bandit, where the agents (i.e., arms) have the ability to misreport privately observed contexts to the learner. Our main contribution is connecting online learning with approximate mechanism design to minimize regret while, at the same time, discouraging the arms from gaming our learning algorithm. ", "page_idx": 0}, {"type": "text", "text": "The contextual bandit [2, 24] is a generalization of the multi-armed bandit problem to the case where the learner observes relevant contextual information before pulling an arm. It has found application in various domains including healthcare [37] and online recommendation [25]. We here focus on the linearly realizable setting [1, 6], where each arm\u2019s reward is a linear function of the arm\u2019s context in the given round. In the standard linear contextual bandit, at the beginning of round $t$ , the learner observes the context $x_{t,i}^{*}\\in\\mathbb{R}^{d}$ of every arm $i\\in[K]$ , selects an arm $i_{t}$ , and receives a reward drawn from a distribution with mean $\\langle\\theta^{*},x_{t,i_{t}}^{*}\\rangle$ where $\\bar{\\theta}^{*}\\in\\mathbb{R}^{d}$ is an unknown parameter. In the strategic linear contextual bandit, we assume that each arm is a self-interested agent that wants to maximize the number of times it gets pulled by manipulating its contexts. ", "page_idx": 0}, {"type": "text", "text": "More precisely, we consider the situation where each arm $i$ privately observes its true context $\\boldsymbol{x}_{t,i}^{*}$ every round, e.g., its relevance to the user arriving in round $t$ , but reports a potentially gamed context vector $x_{t,i}$ to the learner. The learner does not observe the true contexts, but only the reported contexts $\\mathcal{X}_{t}=\\{x_{t,1},\\ldots,x_{t,K}\\}$ and chooses an action from this gamed action set $\\mathbf{\\mathcal{X}}_{t}$ . When the learner pulls arm $i_{t}$ , the learner then observes a reward $\\boldsymbol{r}_{t,i_{t}}$ drawn from a distribution with mean $\\langle\\theta^{*},x_{t,i_{t}}^{*}\\rangle$ . In other words, the arms can manipulate the contexts the learner observes, but cannot influence the underlying reward. This is often the case as superficially changing attributes or meta data has no effect on an item\u2019s true relevance to a user. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce a strategic variant of the linear contextual bandit problem, where each arm, in every round, can misreport its context to the learner to maximize its utility, defined as the total number of times the learner selects the arm over $T$ rounds (Section 3). We demonstrate that incentive-unaware algorithms, which do not explicitly consider the incentives they (implicitly) create for the arms, suffer linear regret in this strategic setting when the arms respond in Nash Equilibrium (NE) (Proposition 3.3). This highlights the necessity of integrating mechanism design with online learning techniques to minimize regret in the presence of strategic arms. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We begin with the case where $\\theta^{*}$ is known to the learner in advance (Section 4). This simplifies the problem setup, allowing us to establish fundamental concepts while highlighting the challenges of designing a sequential mechanism without payments. For this scenario, we propose the Greedy Grim Trigger Mechanism (GGTM), which incentivizes the arms to be a\u221approximately truthful while minimizing regret. We show that \u221a(a) Truthful reporting is an $\\bar{\\mathcal{O}}(\\sqrt{T})$ -NE for the arms (Theorem 4.1) and (b) GGTM has $\\tilde{\\mathcal{O}}(K^{2}\\sqrt{K T})$ regret under every NE of the arms (Theorem 4.2). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Next, we consider the case where $\\theta^{*}$ is unknown to the learner in advance (Section 5). Without access to the true contexts, estimating $\\theta^{*}$ accurately appears intractable, as the arms can manipulate the estimation process. Surprisingly, we show that learning $\\theta^{*}$ is not necessary for minimizing regret in the strategic linear contextual bandit. We construct confidence sets (which may not contain $\\theta^{*}$ ) to derive pessimistic and optimistic estimates of our expected reward. These estimates are used to construct the Optimistic Grim Trigger Mechanism (OptGTM). Despite possibly incorrect estimates of $\\theta^{*}$ , OptGTM bounds the impact of misrepor\u221ated contexts on both regret and the utility of all\u221a arms. We show that (a) Truthfulness is an $\\bar{\\mathcal{O}}(d\\sqrt{K T})$ -NE f\u221aor which OptGTM has regret $\\tilde{\\mathcal{O}}(d\\sqrt{K T})$ (Theorem 5.1) and (b) OptGTM incurs at most $\\tilde{\\mathcal{O}}(d K^{2}\\sqrt{K T})$ regret under every NE of the arms (Theorem 5.2). ", "page_idx": 1}, {"type": "text", "text": "\u2022 Finally, we support our theoretical findings with simulations of strategic gaming behavior in response to OptGTM and LinUCB (Section 6). We simulate how strategic arms adapt what contexts to report over time by equipping the arms with decentralized gradient ascent and letting the arms (e.g., vendors) and the learner (e.g., platform) repeatedly interact over several epochs. The experiments confirm the effectiveness of OptGTM and illustrate the shortcomings of incentiveunaware algorithms, such as LinUCB. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Linear Contextual Bandits. In related work on linear contextual bandits with adversarial reward corruptions [3, 20, 39, 45], an adversary corrupts the reward observation in round $t$ b\u221ay some amount $c_{t}$ but not the observed contexts. In this problem, the optimal regret is given by $\\Theta(d{\\dot{\\sqrt{T}}}+d C)$ , where $\\textstyle C:=\\sum_{t}\\left|c_{t}\\right|$ is the adversary\u2019s budget. To the best o\u221af our knowledge, adversarial context corruptions have  only been studied by [8], who achieve $\\tilde{\\mathcal{O}}(d\\tilde{C}\\sqrt{T})$ regret with $\\begin{array}{r}{\\tilde{C}:=\\sum_{t,i}\\lVert\\boldsymbol{x}_{t,i}^{*}-\\boldsymbol{x}_{t,i}\\rVert_{2}^{2}}\\end{array}$ , where $\\boldsymbol{x}_{t,i}^{*}$ and $x_{t,i}$ are the true and corrupted contexts, respectively. In contrast, we  do not assume a bounded corruption budget so that these regret guarantees become vacuous (cf. Proposition 3.3). Moreover, instead of taking the worst-case perspective of purely adversarial manipulation, we assume that each arm is a self-interested agent maximizing their own utility. ", "page_idx": 1}, {"type": "text", "text": "Strategic Multi-Armed Bandits. Braverman et al. [4] were the first to study a strategic variant of the multi-armed bandit problem and considered the case where the pulled arm privately receives the reward and shares only a fraction of it with the learner. An extension of this setting has recently been studied in [12]. In other lines of work, [9, 13] study the robustness of bandit learning against strategic manipulation, however, simply assume a bounded manipulation budget instead of performing mechanism design. [11, 35] consider multi-armed bandits with replicas where strategic agents can submit replicas of the same arm to increase the number of times one of their arms is pulled. Buening et al. [5] combine multi-armed bandits with mechanism design to discourage clickbait in online ", "page_idx": 1}, {"type": "text", "text": "Interaction Protocol 1: Strategic Linear Contextual Bandits ", "text_level": 1, "page_idx": 2}, {"type": "image", "img_path": "apPHMfE63y/tmp/08401369d81d992b4433c046c8cfb2f51692e58be211293ce1c7114367804fe6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "recommendation. In their model, each arm maximizes its total number of clicks and is characterized by a strategically chosen click-rate and a fixed post-click reward. However, all of these works substantially differ from our work in problem setup and/or methodology. ", "page_idx": 2}, {"type": "text", "text": "Modeling Incentives in Recommender Systems. A complementary line of work studies content creator incentives in recommender systems [16, 21, 22, 23, 27, 40, 41, 42] and how algorithms shape the behavior of agents more generally [7]. These works primarily focus on modeling content creator behavior and studying content creator incentives under existing algorithms. Instead, our goal is the design of incentive-aware learning algorithms which incentivize content creators (arms) to act in a desirable fashion (truthfully) while maximizing the recommender system\u2019s performance. ", "page_idx": 2}, {"type": "text", "text": "Strategic Learning. We also want to mention the extensive literature on strategic learning [14, 15, 18, 19, 26, 43, 44] and strategic classification [10, 17, 34, 36]. Similarly to the model we study in this paper, the premise is that rational agents strategically respond to the learner\u2019s algorithm (e.g., classifier) to obtain a desired outcome. However, the learner interacts with the agents only once and the agents are assumed to be myopic and to suffer a cost for, e.g., altering their features. Moreover, there is no competition among the agents like in the strategic linear contextual bandit. In contrast to these works, we wish to design a sequential (online learning) mechanism to incentivize truthfulness, which is only possible because we repeatedly interact with the same set of agents (i.e., arms). ", "page_idx": 2}, {"type": "text", "text": "3 Strategic Linear Contextual Bandits ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study a strategic-variant of the linear contextual bandit problem, where $K$ strategic agents (i.e., arms) aim to maximize their number of pulls by misreporting privately observed contexts to the learner. The learner follows the usual objective of minimizing regret, i.e., maximizing cumulative rewards, despite not observing the true contexts. We here focus on the case where the strategic arms respond in Nash equilibrium to the learning algorithm. The interaction between the environment, the learning algorithm, and the arms is specified in Interaction Protocol 1. ", "page_idx": 2}, {"type": "text", "text": "Notice that the arms can manipulate the contexts that the learner observes (and the learner only observes these gamed contexts and never the actual contexts), but the rewards are generated w.r.t. the true contexts. Moreover, if all arms are non-strategic and\u2014irrespective of the learning algorithm\u2014 report their features truthfully every round, i.e., $x_{t,i}=x_{t,i}^{*}$ for all $(t,i)\\in[T]\\times[K]$ , the problem reduces to the standard non-strategic linear contextual bandit. ", "page_idx": 2}, {"type": "text", "text": "3.1 Strategic Arms and Nash Equilibrium ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We assume that each arm $i$ reports a possibly gamed context $x_{t,i}$ to the learner after observing its true context $\\boldsymbol{x}_{t,i}^{*}$ and potentially other information. For example, the arms may have prior knowledge of $\\theta^{*}$ and observe the identity of the selected arm at the end of each round. However, we do not use any specific assumptions about the observational model of the arms. Our results can be viewed as a worst-case analysis over all such models. For concreteness, consider the case where the arms have prior knowledge of $\\theta^{*}$ and, at the end of every round, observe which arm was selected and the generated reward.1 ", "page_idx": 2}, {"type": "text", "text": "Let $\\sigma_{i}$ be a (mixed) strategy of arm $i$ that is history-dependent and in every round $t$ maps from observed true contexts $\\boldsymbol{x}_{t,i}^{*}$ to a distribution over reported contexts $x_{t,i}$ in $\\mathbb{R}^{d}$ . We define $\\sigma_{-i}$ as the strategies of all arms except $i$ and define a strategy proflie of the arms as $\\pmb{\\sigma}:=(\\sigma_{1},...,\\sigma_{K})$ . We call arm i truthful if it truthfully reports its privately context every round, i.e., $x_{t,i}=x_{t,i}^{*}$ for all $t\\in[T]$ . This truthful strategy is denoted $\\boldsymbol{\\sigma}_{i}^{*}$ and we let $\\pmb{\\sigma}^{*}=(\\sigma_{1}^{*},\\ldots,\\sigma_{K}^{*})$ . ", "page_idx": 3}, {"type": "text", "text": "We now formally define the objective of the arms. Let $\\begin{array}{r}{n_{T}(i):=\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i\\}}\\end{array}$ be the number of times arm $i$ is pulled by the learner\u2019s algorithm $M$ . The objective of every arm is to maximize the expected number of times it is pulled by the algorithm given by ", "page_idx": 3}, {"type": "equation", "text": "$$\nu_{i}(M,\\pmb\\sigma):=\\mathbb{E}_{M}\\left[n_{T}(i)\\mid\\pmb\\sigma\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we condition on the arm strategies $\\pmb{\\sigma}$ as these will (typically) impact the algorithm\u2019s decisions.   \nWe assume that the arms respond to the learning algorithm $M$ in Nash Equilibrium (NE). ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 (Nash Equilibrium). We say that $\\pmb{\\sigma}=(\\sigma_{1},\\ldots,\\sigma_{K})$ forms a NE under the learner\u2019s algorithm $M$ if for all $i\\in[K]$ and any deviating strategy $\\sigma_{i}^{\\prime}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{E}_{M}\\!\\left[n_{T}(i)\\mid\\sigma_{i},\\sigma_{-i}\\right]\\geq\\mathbb{E}_{M}\\!\\left[n_{T}(i)\\mid\\sigma_{i}^{\\prime},\\sigma_{-i}\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let ${\\mathrm{NE}}(M):=\\{\\pmb{\\sigma}\\colon\\pmb{\\sigma}$ is a NE under $M\\}$ be the set of NE under algorithm $M$ . We also consider $\\varepsilon$ -NE, which relax the requirement that no arm has an incentive to deviate. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 ( $\\varepsilon$ -Nash Equilibrium). We say that $\\pmb{\\sigma}=(\\sigma_{1},\\ldots,\\sigma_{K})$ forms a $\\varepsilon$ -NE under algorithm $M$ if for all $i\\in[K]$ and any deviating strategy $\\sigma^{\\prime}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{M}\\big[n_{T}(i)\\mid\\sigma_{i},\\sigma_{-i}\\big]\\ge\\mathbb{E}_{M}\\big[n_{T}(i)\\mid\\sigma_{i}^{\\prime},\\sigma_{-i}\\big]-\\varepsilon.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Strategic Regret ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the strategic linear contextual bandit, the performance of an algorithm depends on the arm strategies that it incentivizes. Naturally, minimizing regret when the arms always report their context truthfully is easier than when contexts are manipulated adversarially. We are interested in the strategic regret of an algorithm $M$ when the arms act according to a Nash equilibrium under $M$ . Formally, for $\\pmb{\\sigma}\\in\\mathrm{NE}(M)$ the strategic regret of $M$ is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\nR_{T}(M,\\pmb{\\sigma})=\\mathbb{E}_{M,\\pmb{\\sigma}}\\left[\\sum_{t=1}^{T}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle-\\langle\\theta^{*},x_{t,i_{t}}^{*}\\rangle\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $i_{t}^{*}\\,=\\,\\mathrm{argmax}_{i\\in[K]}\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ is the optimal arm in round $t$ . The regret guarantees of our algorithms hold uniformly over all NE that they induce, i.e., for $\\mathrm{max}_{{\\pmb\\sigma}\\in\\mathrm{NE}(M)}\\,R_{T}(M,{\\pmb\\sigma})$ . ", "page_idx": 3}, {"type": "text", "text": "Regularity Assumptions. We allow for the true context vectors $\\boldsymbol{x}_{t,i}^{*}$ to be chosen adversarially by nature, and make the following assumptions about the linear contextual bandit model. We assume that both the context vectors and the rewards are bounded, i.e., $\\begin{array}{r}{\\operatorname*{max}_{i,j\\in[K]}\\langle\\theta^{*},x_{\\underline{{t}},i}^{*}-x_{t,j}^{*}\\rangle\\leq1}\\end{array}$ and $\\|\\boldsymbol x_{t,i}^{*}\\|_{2}\\leq1$ for all $t\\,\\in\\,[T]$ . Moreover, we assume a constant optimality gap. That is, letting $\\Delta_{t,i}:=\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle$ , we assume that $\\Delta:=\\operatorname*{min}_{t,i:\\Delta_{t,i}>0}\\Delta_{t,i}$ is constant. ", "page_idx": 3}, {"type": "text", "text": "3.3 The Necessity of Mechanism Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The first question that arises in this strategic setup is whether mechanism design, i.e., actively aligning the arms\u2019 incentives, is necessary to minimize regret. As expected, we find that this is the case. Standard algorithms, which are oblivious to the incentives they create, implicitly incentivize the arms to heavily misreport their contexts which makes minimizing regret virtually impossible. ", "page_idx": 3}, {"type": "text", "text": "We call a problem instance trivial if the algorithm that selects an arm uniformly at random every round achieves sublinear regret. Conversely, we call a problem instance non-trivial if the uniform selection suffers linear expected regret. We show that being incentive-unaware generally leads to linear regret in non-trivial instances (even when the learner has prior knowledge of $\\theta^{*}$ ). ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.3. On any non-trivial problem instance, the incentive-unaware greedy algorithm that in round t plays $i_{t}=\\underset{\\smile}{\\operatorname{argmax}}_{i\\in[K]}\\langle\\theta^{*},x_{t,i}\\rangle$ (with ties broken uniformly) suffers linear regret $\\Omega(T)$ when the arms act according to any Nash equilibrium under the incentive-unaware greedy algorithm. Note that the incentive-unaware greedy algorithm has knowledge of $\\theta^{*}$ . ", "page_idx": 3}, {"type": "image", "img_path": "apPHMfE63y/tmp/f681d27631c96a03704b94585846aa80930d96d051d89ccd52ed9c353d2d120e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Similarly, algorithms for stochastic linear contextual bandits (LinUCB [1, 6]) and algorithms for linear contextual bandits with adversarial context corruptions (RobustBandit [8]) suffer linear regret when the arms act according to any Nash equilibrium that the algorithms incentivize. ", "page_idx": 4}, {"type": "text", "text": "Proof Sketch. We demonstrate that the only NE for the arms lies in strategies that myopically maximize the probability of being selected in every round, which results in linear regret for the learner, because all arms always appear similarly good. The proof can be found in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Another natural question to ask is whether exact incentive-compatibility is possible in the strategic linear contextual bandit. A learning algorithm is called incentive-compatible if truthfulness is a NE, i.e., reporting the true context $x_{t,i}=x_{t,i}^{*}$ every round is maximizing each arm\u2019s utility [14, 30]. For the interested reader, in Appendix A, we provide an incentive-compatible algorithm with constant regret in the fully deterministic case, where $\\theta^{*}$ is known a priori as well as the rewards of pulled arms directly observable. However, when $\\theta^{*}$ is unknown and/or the reward observations are subject to noise, we conjecture that exact incentive-compatibility (i.e., truthfulness is an exact NE, not $\\varepsilon$ -NE) is irreconcilable with regret minimization (cf. Appendix A). ", "page_idx": 4}, {"type": "text", "text": "4 Warm-Up: $\\theta^{*}$ is Known in Advance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "There are a number of challenges in the strategic linear contextual bandit. The most significant one is the need to incentivize the arms to be (approximately) truthful while simultaneously minimizing regret by learning about $\\theta^{*}$ and selecting the best arms, even when observing (potentially) manipulated contexts. Notably, Proposition 3.3 showed that if we fail to align the arms\u2019 incentives, minimizing regret becomes impossible. Therefore, in the strategic linear contextual bandit, we must combine mechanism design with online learning techniques. ", "page_idx": 4}, {"type": "text", "text": "The uncertainty about $\\theta^{*}$ poses a serious difficulty when trying to design such incentive-aware learning algorithms. As we only observe $x_{t,i}$ and $r_{t,i}=\\langle\\theta^{*},x_{t,i}^{*}\\rangle+\\eta_{t}$ , but do not observe the true context $\\boldsymbol{x}_{t,i}^{*}$ , accurate estimation of $\\theta^{*}$ is extremely challenging (and arguably intractable). We go into more depth in Section 5 when we introduce the Optimistic Grim Trigger Mechanism. For now, we consider the special case when $\\pmb{\\theta}^{*}$ is known to the learner in advance. This lets us highlight some of the challenges when connecting mechanism design with online learning in a less complex setting and introduce high-level ideas and concepts. When $\\theta^{*}$ is known in advance, it can be instructive to consider what we refer to as the reported (expected) reward $\\langle\\theta^{*},x_{t,i}\\rangle$ instead of the reported context vector $x_{t,i}$ itself. Taking this perspective, when arm $i$ reports a $d$ -dimensional vector $x_{t,i}$ , we simply think of arm $i$ reporting a scalar reward $\\langle\\theta^{*},x_{t,i}\\rangle$ . In what follows, it will prove useful to keep this abstraction in mind.2 ", "page_idx": 4}, {"type": "text", "text": "4.1 The Greedy Grim Trigger Mechanism ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "One idea for a mechanism is to use a grim trigger. In repeated social dilemmas, the grim trigger strategy ensures cooperation among self-interested players by threatening with defection for all remaining rounds if the grim trigger condition is satisfied [28]. Typically, the grim trigger condition is defined so that it is immediately satisfied if a player defected at least once. ", "page_idx": 4}, {"type": "text", "text": "In the strategic contextual bandit, from the perspective of the learner, an arm can be considered to \u2019cooperate\u2019 if it is reporting its context truthfully. In turn, an arm \u2019defects\u2019 when it is reporting a gamed context. However, when an arm is reporting some context $x_{t,i}$ we do not know whether this arm truthfully reported its context or not, because we do not have access to the true context $\\boldsymbol{x}_{t,i}^{*}$ . For this reason, we instead compare the expected reward $\\langle\\theta^{*},x_{t,i}\\rangle$ and the true reward $\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ . While we also cannot observe $\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ directly, we do observe $r_{t,i}:=\\langle\\theta^{*},x_{t,i}^{*}\\rangle+\\eta_{t}$ . ", "page_idx": 5}, {"type": "text", "text": "Grim Trigger Condition. Intuitively, if for any arm $i$ the total expected reward $\\textstyle\\sum_{\\ell\\leq t:\\;i_{\\ell}=i}\\langle\\theta^{*},x_{\\ell,i}\\rangle$ is larger than the total observed reward $\\begin{array}{r}{\\hat{r}_{t,i}:=\\sum_{\\ell\\le t:\\;i_{\\ell}=i}r_{\\ell,i}}\\end{array}$ , then arm $i$ must have been misreporting its contexts. However, $r_{\\ell,i}:=\\langle\\theta^{*},x_{\\ell,i}^{*}\\rangle+\\eta_{\\ell}$ is random so that we instead use the optimistic estimate of the observed reward given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{UCB}_{t}(\\hat{r}_{t,i}):=\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}r_{\\ell,i}+2\\sqrt{n_{t}(i)\\log(T)}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $2\\sqrt{n_{t}(i)\\log(T)}$ is the confidence width which can be derived from Hoeffding\u2019s inequality. To implement the grim trigger, we then eliminate arm $i$ in round $t$ if the total expected reward is larger than the optimistic estimate of the total observed reward, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{\\ell\\leq t:\\;i_{\\ell}=i}\\langle\\theta^{*},x_{\\ell,i}\\rangle>\\mathrm{UCB}_{t}(\\hat{r}_{t,i}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that using the optimistic estimate of the total observed reward ensures that elimination is justified with high probability. Conversely, we can guarantee with high probability that we do not erroneously eliminate a truthful arm. ", "page_idx": 5}, {"type": "text", "text": "Selection Rule. To complete the Greedy Grim Trigger Mechanism (GGTM, Mechanism 1), we then combine this with a greedy selection rule that pulls the arm with largest reported reward $\\langle\\theta^{*},x_{t,i}\\rangle$ in round $t$ from the set of arms that we believe have been truthful so far. Interestingly, even though we here assumed $\\theta^{*}$ to be known in advance, we see that GGTM still utilizes online learning techniques such as the optimistic estimate (1) to align the arms\u2019 incentives. ", "page_idx": 5}, {"type": "text", "text": "It is also worth noting that\u2014similar to its use in repeated social dilemmas\u2014our grim trigger mechanism is mutually destructive in the sense that eliminating an arm for all remaining rounds is inherently bad for the learner (and of course for the eliminated arm as well).3 Here lies the main challenge of the mechanism design and we must ensure that the arms are incentivized to \u201ccooperate\u201d (i.e., remain active) for a sufficiently long time. ", "page_idx": 5}, {"type": "text", "text": "4.2 Regret Analysis of GGTM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In what follows, we assume that each arm\u2019s strategy is restricted to reporting their \u2019reward\u2019 $\\langle\\theta^{*},x_{t,i}\\rangle$ not strictly lower than their true (mean) reward $\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ . It seems intuitive that no rational arm would ever under-report its value to the learner and make itself seem worse than it actually is. However, there are special cases, where under-reporting allows an arm to arbitrarily manipulate without detection. We discuss this later in Remark 4.3 and, more extensively, in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "Assumption 1. We assume that $\\langle\\theta^{*},x_{t,i}\\rangle\\geq\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ for all $(t,i)\\in[T]\\times[K]$ . ", "page_idx": 5}, {"type": "text", "text": "We now demonstrate that GGTM approximately incentivizes the arms to be truthful in\u221a the sense that the truthful strategy proflie $\\sigma^{*}$ such that $x_{t,i}=x_{t,i}^{*}$ for all $(t,i)\\in[T]\\times[K]$ is an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ -NE under GGTM. When the arms always report truthfully and no arm is erroneously eliminated, the greedy selection rule naturally selects the best arm every round so that GGTM\u2019s regret is constant. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1. Under the Greedy Grim Trigger Mechanism, being truthful is a $\\tilde{\\mathcal{O}}(\\sqrt{T})$ -NE for the arms. The strategic regret of GGTM when the arms act according to this equilibrium is at most ", "page_idx": 5}, {"type": "equation", "text": "$$\nR_{T}(\\mathrm{GGTM},\\pmb{\\sigma}^{*})\\leq\\sqrt[1]{T}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof Sketch. By design of the grim trigger, it is straightforward to show that the probability that a truthful arm gets eliminated is at most $^1\\!/\\!T^{2}$ . Moreove\u221ar, the grim trigger ensures that no arm can \u2018poach\u2019 selections from a truthful arm more than order $\\sqrt{T}$ times by misreporting its contexts. This achieves two things: (a) it protects truthful arms and guarantees that truthfulness is a good strategy, and (b) limits an arm\u2019s profit from being untruthful. The proof can be found in Appendix D.2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.1 tells us that truthfulne\u221ass is an approximate NE. We now also provide a more holistic strategic regret guarantee of $\\tilde{\\mathcal{O}}(K^{2}\\sqrt{K T})$ in every Nash equilibrium under GGTM. Proving this is more complicated as the arms can profti from exploiting our uncertainty about their truthfulness (i.e., the looseness of the grim trigger). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2. The Greedy Grim Trigger Mechanism has strategic regret ", "page_idx": 6}, {"type": "equation", "text": "$$\nR_{T}(\\mathbf{G}\\mathbf{G}\\mathbf{T}\\mathbf{M},\\sigma)=\\mathcal{O}\\left(\\underbrace{\\sqrt{K T\\log(T)}}_{c o s t\\,o f\\,m a n i p u l a t i o n}+\\underbrace{K^{2}\\sqrt{K T\\log(T)}}_{c o s t\\,o f\\,m e c h a n i s m\\,d e s i g n}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "for every $\\pmb{\\sigma}\\in\\mathrm{NE}(\\mathbf{GGTM})$ $\\begin{array}{r}{\\bar{\\Sigma}(\\mathrm{GGTM}).\\ H e n c e,\\operatorname*{max}_{\\sigma\\in\\mathrm{NE}(\\mathrm{GGTM})}R_{T}(\\mathrm{GGTM},\\sigma)=\\tilde{\\mathcal{O}}\\big(K^{2}\\sqrt{K T}\\big).}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Proof Sketch. The regret analysis is notably more complicated than the one in Theorem 4.1, as we must bound the regret due to the arms exploiting our uncertainty as well as the cost of committing to the grim trigger. Both of these quantities do not play a role when the arms always report truthfully (like in Theorem 4.1). A complete proof can be found in Appendix D. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "The regret bound (2) suggests that there are two sources of regret. The first term is due to our mechanism design being approximate (relying on estimates), which leaves room for the arms to exploit our uncertainty and misreport their contexts to obtain additional selections. The second part of (2) is the cost of the mechanism design, i.e., the cost of committing to the grim trigger. We suffer constant regret any round in which\u221a the round-optimal arm is no longer in the active set. In the worst-case, this quantity is of order $K^{2}{\\sqrt{K T}}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4.3. We want to briefly comment on Assumption 1. It appears intuitive that any rational arm would never under-report its value, i.e., make itself look worse than it actually is. However, in Appendix C, we provide a simple example where occasionally under-reporting its value allows an arm to simulate an environment where it is always optimal, even though it is in fact only optimal half of the time. We will explain in the example that without additional strong assumptions on the noise distribution the two environments are indistinguishable so that such manipulation by the arms appears unavoidable when trying to maximize rewards. ", "page_idx": 6}, {"type": "text", "text": "5 The Optimistic Grim Trigger Mechanism ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The problem of estimating the unknown parameter $\\theta^{*}$ appears daunting given that the arms can strategically alter their contexts to manipulate our estimate of $\\theta^{*}$ to their advantage. In fact, imagine an arm manipulating its contexts orthogonal to $\\theta^{*}$ so that $\\langle\\theta^{*},x_{t,i}\\,-\\,x_{t,i}^{*}\\rangle\\,=\\,0$ but $x_{t,i}\\neq x_{t,i}^{*}$ Observing only $x_{t,i}$ and $r_{t,i}:=\\langle\\theta^{*},x_{t,i}^{*}\\rangle+\\eta_{t}$ , our estimate of $\\theta^{*}$ becomes biased and could be arbitrarily far off the true parameter $\\theta^{*}$ even though the gamed context and true context have the same reward w.r.t. $\\theta^{*}$ . This is also the case more generally. Since we observe neither $\\theta^{*}$ nor $\\boldsymbol{x}_{t,i}^{*}$ , any observed combination of $x_{t,i}$ and $\\boldsymbol{r}_{t,i}$ will \u201cmake sense\u201d to us. But, how can we incentivize the arms to report truthfully and minimize regret despite incorrect estimates of $\\theta^{*}$ ? ", "page_idx": 6}, {"type": "text", "text": "Our key observation is that learning $\\theta^{*}$ is not necessary to incentivize the arms or minimize regret; it appears to be a hopeless endeavour after all. The idea of the Optimistic Grim Trigger Mechanism (OptGTM, Mechanism 2) is to construct pessimistic estimates of the total reward we expected from pulling an arm. Importantly, we can construct such pessimistic estimates of the expected (i.e., \u201creported\u201d) reward even when the contexts are manipulated. OptGTM then threatens arms with elimination if our pessimistic estimate of the expected reward exceeds the optimistic estimate of the observed reward. Interestingly, this does not relate to the amount of corruption in the feature space and, in fact, $\\textstyle\\sum_{t,i}\\|x_{t,i}-x_{t,i}^{*}\\|_{2}$ could become arbitrarily large. However, it does bound the effect of each arm\u2019s strategic manipulation on the decisions we make and thereby allows for effective incentive design and regret minimization. ", "page_idx": 6}, {"type": "text", "text": "To construct pessimistic (and optimistic) estimates of the expected reward, we use independent estimators $\\widehat{\\theta}_{t,i}^{\\,\\,\\,\\,\\star}$ and confidence sets $C_{t,i}$ around $\\widehat{\\theta}_{t,i}$ , which do not take into account that the contexts are potentially manipulated. That is, we have a separate estimator and confidence set for each arm $i\\in[K]$ . This will prevent one arm influencing the elimination of another. It also stops collusive arm behavior, where a majority group of the arms could dominate and steer our estimation process. ", "page_idx": 6}, {"type": "text", "text": "Mechanism 2: The Optimistic Grim Trigger Mechanism (OptGTM) ", "page_idx": 7}, {"type": "text", "text": "1 initialize: $A_{1}=[K]$   \n2 for $t=1,\\dots,T$ do   \n3 Observe reported contexts $\\mathcal{X}_{t}=\\{x_{t,1},\\ldots,x_{t,K}\\}$ .   \n4 Play the active arm with largest reported optimistic reward   \n$i_{t}=\\underset{i\\in A_{t}}{\\operatorname{argmax}}\\,\\mathrm{UCB}_{t,i}\\left(x_{t,i}\\right).$   \n5 Receive reward $\\boldsymbol{r}_{t,i_{t}}$ from playing arm $i_{t}$ .   \n6 if  \u2113 $<\\!t\\colon i_{\\ell}\\!=\\!i_{t}\\operatorname{LCB}_{\\ell,i_{t}}\\!\\left(x_{\\ell,i_{t}}\\right)>\\operatorname{UCB}_{t}\\!\\left(\\hat{r}_{t,i_{t}}\\right)$ then   \n7 Eliminate arm $i_{t}$ from the active set: $A_{t+1}\\leftarrow A_{t}\\setminus\\{i_{t}\\}$ .   \n8 if $A_{t+1}=\\emptyset$ then   \n9 Stop playing any arm and receive zero reward for all remaining rounds. ", "page_idx": 7}, {"type": "text", "text": "Confidence Sets. For every arm $i\\in[K]$ we define the least-squares estimator $\\widehat{\\theta}_{t,i}$ w.r.t. its reported contexts and observed rewards before round $t$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta}_{t,i}=\\underset{\\theta\\in\\mathbb{R}^{d}}{\\operatorname{argmin}}\\left(\\sum_{\\ell<t:\\;i_{\\ell}=i}\\left(\\langle\\theta,x_{\\ell,i}\\rangle-r_{\\ell,i}\\right)^{2}+\\lambda\\|\\theta\\|_{2}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\lambda>0$ . We then define the confidence set $C_{t,i}:=\\{\\theta\\in\\mathbb{R}^{d}\\colon\\|\\hat{\\theta}_{t,i}-\\theta\\|_{V_{t,i}}^{2}\\leq\\beta_{t,i}\\}$ where $\\beta_{t,i}:=\\mathcal{O}(d\\log(n_{t}(i)))$ is the confidence size. Here, $V_{t,i}$ is the covariance matrix of reported contexts of arm i given by V1,i ..= \u03bbI and Vt,i ..= \u03bbI + \u2113<t: i\u2113=i x\u2113,ix\u2113\u22a4,i.4 ", "page_idx": 7}, {"type": "text", "text": "It is well-known that if the contexts were always reported truthfully, i.e., $x_{t,i}=x_{t,i}^{*}$ , then with high probability $\\theta^{*}\\in C_{t,i}$ . But, if the sequence of reported contexts $x_{t,i}$ substantially differs from the true contexts $\\boldsymbol{x}_{t,i}^{*}$ there is no (high probability) guarantee that the true parameter $\\theta^{*}$ is element in $C_{t,i}$ . In the literature on learning with adversarial corruptions (in linear contextual bandits), the standard approach to deal with this is to widen the confidence set. However, for this to be effective we would need to assume a sufficiently small corruption budget for the arms and prior knowledge of the total amount of corruption, both of which we explicitly do not assume here. ", "page_idx": 7}, {"type": "text", "text": "Slightly overloading notation, we instead define the optimistic and pessimistic estimate of the expected reward of a context vector $x$ w.r.t. arm $i$ as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\textstyle\\operatorname{UCB}_{t,i}(x):=\\langle\\hat{\\theta}_{t,i},x\\rangle+\\sqrt{\\beta_{t,i}}\\|x\\|_{V_{t,i}^{-1}}\\quad\\mathrm{and}\\quad\\operatorname{LCB}_{t,i}(x):=\\langle\\hat{\\theta}_{t,i},x\\rangle-\\sqrt{\\beta_{t,i}}\\|x\\|_{V_{t,i}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We chose to state these estimates using additive bonuses. However, it can be convenient to think of them as $\\mathrm{UCB}_{t,i}(x)\\approx\\operatorname*{max}_{\\theta\\in C_{t,i}}\\langle\\theta,x\\rangle$ and $\\mathrm{LCB}_{t,i}(x)\\approx\\operatorname*{min}_{\\theta\\in C_{t,i}}\\langle\\theta,x\\rangle$ . ", "page_idx": 7}, {"type": "text", "text": "Grim Trigger Condition. In round $t\\in[T]$ , we eliminate arm $i$ from $A_{t}$ if the pessimistic estimate using the reports is larger than the optimistic estimate using the total observed reward, i.e., ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\Big(\\langle\\widehat\\theta_{\\ell,i},x_{\\ell,i}\\rangle-\\sqrt{\\beta_{\\ell}}\\|x_{\\ell,i}\\|_{V_{\\ell,i}^{-1}}\\Big)>\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}r_{\\ell,i}+2\\sqrt{n_{t}(i)\\log(T)}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In other words, $\\begin{array}{r}{\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\mathrm{LCB}_{\\ell,i}(x_{\\ell,i})>\\mathrm{UCB}_{t}(\\hat{r}_{t,i}).}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Examining the left side of (4), the careful reader may wonder why we do not simply use our latest and arguably best estimate $\\widehat{\\theta}_{t,i}$ , but instead the whole sequence of \u201cout-dated\u201d estimators $\\widehat{\\theta}_{\\ell,i}$ from previous rounds. In fact, this is crucial for the grim trigger. Using $\\widehat{\\theta}_{t,i}$ renders the grim trigger condition ineffective, because, by definition, $\\widehat{\\theta}_{t,i}$ is the (least-squares) minimizer (3) of the difference between \u2113\u2264t: i\u2113=i $\\textstyle\\sum_{\\ell\\leq t_{:}\\;i_{\\ell}=i}\\langle\\hat{\\theta}_{t,i},x_{\\ell,i}\\rangle$ and $\\textstyle\\sum_{\\ell\\leq t:i\\ell=i}r_{\\ell,i}$ . Hence, when using $\\widehat{\\theta}_{t,i}$ the grim trigger condition may not be satisfied even when the arms significantly and repeatedly misreport their contexts. ", "page_idx": 7}, {"type": "text", "text": "Selection Rule. We complete the OptGTM algorithm by selecting arms optimistically with respect to each arm\u2019s own estimator and confidence set. That is, OptGTM selects the active arm with maximal optimistic (expected) reward $\\mathrm{UCB}_{t,i}(x_{t,i}):=\\langle\\hat{\\theta}_{t,i},x_{t,i}\\rangle^{\\frac{1}{+}}\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}$ in round $t$ . We see that the grim trigger (4) incentivizes arms to ensure that over the course of all rounds $\\mathrm{LCB}_{t,i}(x_{t,i})$ is not much smaller than $r_{t,i}:=\\langle\\theta^{*},x_{t,i}^{*}\\rangle+\\eta_{t}$ . Hence, $\\mathrm{UCB}_{t,i}(x_{t,i})$ is also not substantially smaller than the true mean reward $\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ . This suggests that playing optimistically is a good strategy for the learner as long as the selected arm does not satisfy (4). ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.1 Regret Analysis of OptGTM ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "When $\\theta^{*}$ was known to the learner in advance, we assumed that the arms never report a value smaller than their true value, i.e., $\\langle\\theta^{*},x_{t,i}\\rangle\\geq\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ for all $(t,i)\\in[T]\\times[K]$ . Now, when $\\theta^{*}$ is unknown to the learner, we similarly assume that the arms do not report their optimistic value less than their true value. Again, it seems intuitive that in any given round, no arm would under-report its worth. ", "page_idx": 8}, {"type": "text", "text": "Assumption 2. We assume that $\\operatorname*{max}_{\\theta\\in C_{t,i}}\\langle\\theta,x_{t,i}\\rangle\\geq\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ for all $(t,i)\\in[T]\\times[K]$ . ", "page_idx": 8}, {"type": "text", "text": "We find that OptGTM approximately inc\u221aentivizes the arms to be truthful and, when the arms report truthfully, OptGTM suffers at most $\\bar{\\mathcal{O}}(d\\sqrt{K T})$ regret. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.1. Under the Optimistic Grim Trigger Mechanism, being truthful is a $\\tilde{\\mathcal{O}}(d\\sqrt{K T})$ -NE. When the arms report truthfully, the strategic regret of OptGTM under this approximate NE is at most ", "page_idx": 8}, {"type": "equation", "text": "$$\nR_{T}(\\mathrm{OptGTM},\\sigma^{*})=\\tilde{\\mathcal{O}}(d\\sqrt{K T}).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The optimal regret in stan\u221adard non-strategic linear contextual bandits is $\\Theta(d{\\sqrt{T}})$ so that OptGTM is optimal up\u221a to a factor of $\\sqrt{K}$ (and logarithmic factors) when the arms report truthfully. The additional factor of $\\sqrt{K}$ is caused by the fact that OptGTM maintains independent estimates for each arm. We now also provide a strategic regret bound for every NE of the arms under OptGTM. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.2. The Optimistic Grim Trigger Mechanism has strategic regret ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\ \\ \\ \\ R_{T}(\\mathrm{OptGTM},\\sigma)=\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}+d\\log(T)K^{2}\\sqrt{K T}\\right).}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,}\\\\ &{\\,\\,\\,\\,\\,\\,\\,\\,\\,\\sigma\\,\\,\\sigma\\,\\in\\mathrm{NE}(\\mathrm{OptGTM}).\\,\\,H e n c e,\\,\\mathrm{max}_{\\sigma\\in\\mathrm{NE}(\\mathrm{OptGTM})}\\,R_{T}(\\mathrm{OptGTM},\\sigma)=\\tilde{\\mathcal{O}}\\left(d K^{2}\\sqrt{K T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The proof ideas of Theorem 5.1 and Theorem 5.2 are similar to their counterparts in Section 4. The main difference lies in a more technically challenging analysis of the grim trigger condition (4). We also see that in contrast to non-strategic linear contextual bandits, where the regret typical\u221aly does not \u221adepend on the number of arms $K$ , Theorem 5.1 and Theorem 5.2 include a factor of $\\sqrt{K}$ and $K^{2}\\sqrt{K}$ , respectively. A dependence on $K$ is expected due to the strategic nature of the arms which forces us to explicitly incentivize each arm to be truthful. However, we conjecture that the regret bou\u221and in Theorem 5.2 is not tight in $K$ and expect the optimal dependence on the number of arms to be $\\sqrt{K}$ . The proofs of Theorem 5.1 and Theorem 5.2 can be found in Appendix E. ", "page_idx": 8}, {"type": "text", "text": "6 Experiments: Simulating Strategic Context Manipulation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We here experimentally analyze the efficacy of OptGTM when the arms strategically manipulate their contexts in response to our learning algorithm. We compare the performance of OptGTM with the traditional LinUCB algorithm [1, 6], which\u2014as shown in Proposition 3.3\u2014implicitly incentivizes the arms to manipulate their contexts and suffers large regret when the arms are strategic. ", "page_idx": 8}, {"type": "text", "text": "Contrary to the assumption of arms playing NE, we here model strategic arm behavior by letting the arms update their strategy (i.e., what contexts to report) based on past interactions with the algorithms. More precisely, we assume that the strategic arms interact with the deployed algorithm (i.e., OptGTM or LinUCB) over the course of 20 epochs, with each epoch consisting of $T=10k$ rounds. At the end of each epoch, every arm then updates its strategy using gradient ascent w.r.t. its utility. Importantly, this approach requires no prior knowledge from the arms, as they learn entirely through sequential interaction. This does not necessarily lead to equilibrium strategies, but serves as a natural model of strategic gaming behavior under which to study the algorithms. ", "page_idx": 8}, {"type": "text", "text": "Experimental Setup. We associate each arm with a true feature vector $\\boldsymbol{y}_{i}^{*}\\in\\mathbb{R}^{d_{1}}$ (e.g., product features) and randomly sample a sequence of user vectors $c_{t}\\in\\mathbb{R}^{d_{2}}$ (i.e., customer features). We assume that every arm can alter its feature vector $y_{i}^{*}$ by reporting some other vector $y_{i}$ , but cannot alter the user contexts $c_{t}$ . We use a feature mapping $\\varphi(c_{t},y_{i})=x_{t,i}$ to map $y_{i}\\in\\mathbb{R}^{d_{1}}$ and $c_{t}\\in\\mathbb{R}^{d_{2}}$ to an arm-specific context $x_{t,i}\\in\\mathbb{R}^{d}$ that the algorithm observes. At the end of every epoch, each arm then performs an approximated gradient step on $y_{i}$ w.r.t. its utility, i.e., the number of times it is selected. We let $K=5$ and $d=d_{1}=d_{2}=5$ and average the results over 10 runs. ", "page_idx": 8}, {"type": "image", "img_path": "apPHMfE63y/tmp/665077fec213c24170c5d101be58cf65c9c81828a3f21d531a5855645c1c0ce5.jpg", "img_caption": ["(a) Total strategic regret $R_{T}$ as the (b) Epoch 0 (Truthful Arms): Regret (c) Epoch 20 (Strategic Arms): Rearms adapt their strategies to the de- as a function of $t$ before the arms gret as a function of $t$ after the arms ployed algorithm over the course of have interacted with the deployed have interacted with the deployed 20 epochs. algorithm. algorithm. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 1: Comparison of the strategic regret of OptGTM and LinUCB. The strategic arms adapt their strategies gradually over the course of 20 epochs. OptGTM performs similarly across all epochs, whereas LinUCB performs increasingly worse as the arms adapt to the algorithm (Figure 1a). Figure 1b and 1c provide a closer look at the regret of the algorithms across the $T$ rounds in the initial epoch, where the arms are truthful, and the final epoch after the arms have adapted to the algorithms. ", "page_idx": 9}, {"type": "text", "text": "Results. In Figure 1a, we observe that OptGTM performs similarly well across all epochs, which suggests that OptGTM successfully discourages the emergence of harmful gaming behavior. In contrast, as the arms adapt their strategies (i.e., what features to report), LinUCB suffers increasingly more regret and almost performs as badly as uniform sampling in the final epoch. In epoch 0, when the all arms are ", "page_idx": 9}, {"type": "image", "img_path": "apPHMfE63y/tmp/e71ca74b595c6640ed5bb2996eda8ed35d292491e010cb78e4223d58c6d292df.jpg", "img_caption": ["Figure 2: Context manipu- Figure 3: Utility of the arms lation $\\textstyle\\sum_{t,i}\\|x_{t,i}^{*}-x_{t,i}\\|_{2}$ . for each of the 10 runs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "truthful, i.e., are non-strategic, LinUCB performs better than OptGTM (Figure 1b). This is expected as OptGTM suffers additional regret due to maintaining independent estimates of $\\theta^{*}$ for each arm (as a mechanism to incentivize truthfulness). However, OptGTM significantly outperforms LinUCB as the arms strategically adapt, which is most evident in the final epoch (Figure 1c). Interestingly, as already suggested in Section 5, OptGTM cannot prevent manipulation in the feature space (see Figure 2). However, OptGTM does manage to bound the effect of the manipulation on the regret (Figure 1a) and, most importantly, the effect on the utility of the arms as well (Figure 3). As a result, the arms are discouraged from heavily gaming their contexts and the context manipulation has only a minor effect on the actions taken by OptGTM. ", "page_idx": 9}, {"type": "text", "text": "7 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We study a strategic variant of the linear contextual bandit problem, where the arms strategically misreport privately observed contexts to maximize their selection frequency. To address this, we design two online learning mechanisms: the Greedy and the Optimistic Grim Trigger Mechanism, for the scenario where $\\theta^{*}$ is known and unknown, respectively. We demonstrate that our mechanisms incentivize the arms to be approximately truthful and, in doing so, effectively minimize regret. More generally, with this work, we aim to advance our understanding of problems at the intersection of online learning and mechanism design. As the digital landscape, including online platforms and marketplaces, becomes increasingly agentic and dominated by self-interested agents, it will be crucial to understand the incentives created by learning algorithms and to align these incentives while optimizing for performance. ", "page_idx": 9}, {"type": "text", "text": "Limitations. One limitation is the otherwise intuitive assumption that the arms do not under-report their value to the learner (Assumption 1 and Assumption 2). Secondly, we believe that the factor of $K^{2}$ in the universal regret guarantees of Theorem 4.2 and Theore\u221am 5.2 is suboptimal and we conjecture that the optimal worst-case strategic regret is given by $\\mathcal{O}(d\\sqrt{K T})$ . We leave this investigation for future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Thomas Kleine Buening is supported by the UKRI Prosperity Partnership Scheme (Project FAIR). Haifeng Xu is supported in part by the Army Research Office Award W911NF-23-1-0030, the ONR Award N00014-23-1-2802 and the NSF Award CCF-2303372. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011.   \n[2] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3(Nov):397\u2013422, 2002. [3] Ilija Bogunovic, Arpan Losalka, Andreas Krause, and Jonathan Scarlett. Stochastic linear bandits robust to adversarial attacks. In International Conference on Artificial Intelligence and Statistics, pages 991\u2013999. PMLR, 2021. [4] Mark Braverman, Jieming Mao, Jon Schneider, and S Matthew Weinberg. Multi-armed bandit problems with strategic arms. In Conference on Learning Theory, pages 383\u2013416. PMLR, 2019. [5] Thomas Kleine Buening, Aadirupa Saha, Christos Dimitrakakis, and Haifeng Xu. Bandits meet mechanism design to combat clickbait in online recommendation. In The Twelfth International Conference on Learning Representations, 2023.   \n[6] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 208\u2013214. JMLR Workshop and Conference Proceedings, 2011. [7] Sarah Dean, Evan Dong, Meena Jagadeesan, and Liu Leqi. Accounting for ai and users shaping one another: The role of mathematical models. arXiv preprint arXiv:2404.12366, 2024. [8] Qin Ding, Cho-Jui Hsieh, and James Sharpnack. Robust stochastic linear contextual bandits under adversarial attacks. In International Conference on Artificial Intelligence and Statistics, pages 7111\u20137123. PMLR, 2022.   \n[9] Jing Dong, Ke Li, Shuai Li, and Baoxiang Wang. Combinatorial bandits under strategic manipulations. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pages 219\u2013229, 2022.   \n[10] Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic classification from revealed preferences. In Proceedings of the 2018 ACM Conference on Economics and Computation, pages 55\u201370, 2018.   \n[11] Seyed Esmaeili, MohammadTaghi Hajiaghayi, and Suho Shin. Replication-proof bandit mechanism design. arXiv preprint arXiv:2312.16896, 2023.   \n[12] Seyed A Esmaeili, Suho Shin, and Aleksandrs Slivkins. Robust and performance incentivizing algorithms for multi-armed bandits with strategic agents. arXiv preprint arXiv:2312.07929, 2023.   \n[13] Zhe Feng, David Parkes, and Haifeng Xu. The intrinsic robustness of stochastic bandits to strategic manipulation. In International Conference on Machine Learning, pages 3092\u20133101. PMLR, 2020.   \n[14] Rupert Freeman, David Pennock, Chara Podimata, and Jennifer Wortman Vaughan. No-regret and incentive-compatible online learning. In International Conference on Machine Learning, pages 3270\u20133279. PMLR, 2020.   \n[15] Nicolas Gast, Stratis Ioannidis, Patrick Loiseau, and Benjamin Roussillon. Linear regression from strategic data sources. ACM Transactions on Economics and Computation (TEAC), 8(2): 1\u201324, 2020.   \n[16] Arpita Ghosh and Patrick Hummel. Learning and incentives in user-generated content: Multiarmed bandits with endogenous arms. In Proceedings of the 4th conference on Innovations in Theoretical Computer Science, pages 233\u2013246, 2013.   \n[17] Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science, pages 111\u2013122, 2016.   \n[18] Keegan Harris, Dung Daniel T Ngo, Logan Stapleton, Hoda Heidari, and Steven Wu. Strategic instrumental variable regression: Recovering causal relationships from strategic responses. In International Conference on Machine Learning, pages 8502\u20138522. PMLR, 2022.   \n[19] Keegan Harris, Chara Podimata, and Steven Z Wu. Strategic apple tasting. Advances in Neural Information Processing Systems, 36:79918\u201379945, 2023.   \n[20] Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Nearly optimal algorithms for linear contextual bandits with adversarial corruptions. arXiv preprint arXiv:2205.06811, 2022.   \n[21] Jiri Hron, Karl Krauth, Michael I Jordan, Niki Kilbertus, and Sarah Dean. Modeling content creator incentives on algorithm-curated platforms. arXiv preprint arXiv:2206.13102, 2022.   \n[22] Xinyan Hu, Meena Jagadeesan, Michael I Jordan, and Jacob Steinhard. Incentivizing highquality content in online recommender systems. arXiv preprint arXiv:2306.07479, 2023.   \n[23] Meena Jagadeesan, Nikhil Garg, and Jacob Steinhardt. Supply-side equilibria in recommender systems. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Tor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. Cambridge University Press, 2020.   \n[25] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661\u2013670, 2010.   \n[26] Yang Liu and Yiling Chen. A bandit framework for strategic regression. Advances in Neural Information Processing Systems, 29, 2016.   \n[27] Yang Liu and Chien-Ju Ho. Incentivizing high quality user contributions: New arm generation in bandit learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.   \n[28] Michael W Macy and Andreas Flache. Learning dynamics in social dilemmas. Proceedings of the National Academy of Sciences, 99(suppl_3):7229\u20137236, 2002.   \n[29] Ross A Malaga. Worst practices in search engine optimization. Communications of the ACM, 51(12):147\u2013150, 2008.   \n[30] Yishay Mansour, Aleksandrs Slivkins, and Vasilis Syrgkanis. Bayesian incentive-compatible bandit exploration. In Proceedings of the Sixteenth ACM Conference on Economics and Computation, pages 565\u2013582, 2015.   \n[31] Nicol\u00f2 Pagan, Joachim Baumann, Ezzat Elokda, Giulia De Pasquale, Saverio Bolognani, and Anik\u00f3 Hann\u00e1k. A classification of feedback loops and their relation to biases in automated decision-making systems. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, pages 1\u201314, 2023.   \n[32] Shankar Prawesh and Balaji Padmanabhan. The \u201cmost popular news\u201d recommender: Count amplification and manipulation resistance. Information Systems Research, 25(3):569\u2013589, 2014.   \n[33] Paul Resnick and Rahul Sami. The influence limiter: provably manipulation-resistant recommender systems. In Proceedings of the 2007 ACM conference on Recommender systems, pages 25\u201332, 2007.   \n[34] Elan Rosenfeld and Nir Rosenfeld. One-shot strategic classification under unknown costs. arXiv preprint arXiv:2311.02761, 2023.   \n[35] Suho Shin, Seungjoon Lee, and Jungseul Ok. Multi-armed bandit algorithm against strategic replication. In International Conference on Artificial Intelligence and Statistics, pages 403\u2013431. PMLR, 2022.   \n[36] Ravi Sundaram, Anil Vullikanti, Haifeng Xu, and Fan Yao. Pac-learning for strategic classification. Journal of Machine Learning Research, 24(192):1\u201338, 2023.   \n[37] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. Mobile health: sensors, analytic methods, and applications, pages 495\u2013517, 2017.   \n[38] Zongwei Wang, Min Gao, Junliang Yu, Hao Ma, Hongzhi Yin, and Shazia Sadiq. Poisoning attacks against recommender systems: A survey. arXiv preprint arXiv:2401.01527, 2024.   \n[39] Chen-Yu Wei, Christoph Dann, and Julian Zimmert. A model selection approach for corruption robust reinforcement learning. In International Conference on Algorithmic Learning Theory, pages 1043\u20131096. PMLR, 2022.   \n[40] Fan Yao, Chuanhao Li, Denis Nekipelov, Hongning Wang, and Haifeng Xu. How bad is top- $k$ recommendation under competing content creators? In International Conference on Machine Learning, pages 39674\u201339701. PMLR, 2023.   \n[41] Fan Yao, Chuanhao Li, Karthik Abinav Sankararaman, Yiming Liao, Yan Zhu, Qifan Wang, Hongning Wang, and Haifeng Xu. Rethinking incentives in recommender systems: Are monotone rewards always beneficial? Advances in Neural Information Processing Systems, 36, 2024.   \n[42] Fan Yao, Yiming Liao, Mingzhe Wu, Chuanhao Li, Yan Zhu, James Yang, Qifan Wang, Haifeng Xu, and Hongning Wang. User welfare optimization in recommender systems with competing content creators. arXiv preprint arXiv:2404.18319, 2024.   \n[43] Mengxin Yu, Zhuoran Yang, and Jianqing Fan. Strategic decision-making in the presence of information asymmetry: Provably efficient rl with algorithmic instruments. arXiv preprint arXiv:2208.11040, 2022.   \n[44] Hanrui Zhang and Vincent Conitzer. Incentive-aware pac learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 5797\u20135804, 2021.   \n[45] Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Linear contextual bandits with adversarial corruptions. arXiv preprint arXiv:2110.12615, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Remarks on Incentive-Compatible No-Regret Algorithms ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Section 3, we conjectured that there exists no incentive-compatible no-regret algorithm when the reward observations are subject to noise and $\\theta^{*}$ unknown. For the interested reader, we here consider the fully deterministic case where $\\theta^{*}$ is known a priori and reward observations are directly observable, i.e., subject to no noise so that $\\eta_{t}\\equiv0$ . We can design the following provably incentive-compatible no-regret algorithm. In fact, we show that this mechanism is strategyproof, i.e., incentive-compatible in weakly dominant strategies. ", "page_idx": 13}, {"type": "text", "text": "Mechanism 3: Incentive-Compatible No-Regret Algorithm in the Fully Deterministic Case   \n1 initialize: $A_{1}=[K]$   \n2 for $t<T-(K+1)$ do   \n3 Play $i_{t}\\in\\arg\\operatorname*{max}_{i\\in A_{t}}\\langle\\theta^{*},x_{t,i}\\rangle$   \n4 Observe reward $r_{t,i_{t}}^{*}:=\\langle\\theta^{*},x_{t,i_{t}}^{*}\\rangle$ (i.e., rewards of chosen arms are directly observable)   \n5 if $\\langle\\theta^{*},x_{t,i_{t}}\\rangle\\neq r_{t,i_{t}}^{*}$ then   \n6 Eliminate arm it: $A_{t+1}\\leftarrow A_{t}\\setminus\\{i_{t}\\}$ .   \n7 for $t\\geq T-(K+1)$ do   \n8 Play $i_{t}\\sim\\mathrm{Uniform}(A_{t})$   \n9 Observe reward $r_{t,i_{t}}^{*}:=\\langle\\theta^{*},x_{t,i_{t}}^{*}\\rangle$   \n10 if $\\langle\\theta^{*},x_{t,i_{t}}\\rangle\\neq r_{t,i_{t}}^{*}$ then   \n11 Eliminate arm $i_{t}$ : $A_{t+1}\\leftarrow A_{t}\\setminus\\{i_{t}\\}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. Incentive-Compatibility in Weakly Dominant Strategies. It is easy to see that for the last $K+1$ rounds, reporting truthfully, i.e., reporting $\\boldsymbol{x}_{t,i}^{*}$ , is a weakly dominant strategy, since the the set of active arms is played uniformly and nothing can be gained from misreporting (an arm can only miss out on being selected by misreporting in the last $K+1$ steps). Hence, conditional on any history, reporting truthfully is the best continuation for any arm. In particular, when an arm plays truthfully in these rounds the obtained utility in the last $K+1$ steps is at least KK+ 1, since |At| \u2264K. ", "page_idx": 13}, {"type": "text", "text": "Now, for the time steps $t<T-(K+1)$ note that any untruthful strategy can obtain at most one more allocation than the truthful strategy, because if $i_{t}=i$ and $\\langle\\theta^{*},x_{t,i}\\rangle>\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ , then arm $i$ is eliminated immediately. Hence, at most utility 1 can be gained from receiving an allocation by misreporting. However, in this case the arm gets eliminated and receives utility 0 in the last $K+1$ rounds. As seen before the minimum utility the truthful strategy receives in the last $K+1$ receives is $\\textstyle{\\frac{K+1}{K}}>1$ . Consequently, irrespective of the other arms strategies, the truthful strategy is (weakly) optimal for arm . ", "page_idx": 13}, {"type": "text", "text": "One may wonder why the truthful strategy is not strictly dominant. To see this note that reporting any $x_{t,i}\\neq x_{t,i}^{*}$ such that the difference $x_{t,i}-x_{t,i}^{*}$ is orthogonal to $\\theta^{*}$ , i.e., $\\langle\\theta^{*},x_{t,i}-x_{t,i}^{*}\\rangle=\\overline{{0}}$ , does not cause elimination and is equivalent under Mechanism 3. In other words, such untruthful strategies, which however have no effect on the selection, are equally good. ", "page_idx": 13}, {"type": "text", "text": "Regret. The regret in the last $K+1$ rounds is trivially bounded by $K+1$ . When showing that the algorithm is strategyproof we showed that any untruthful strategy such that there exists $i_{t}=i$ with $\\bar{\\langle\\theta^{*},x_{t,i}\\rangle}\\,>\\,\\langle\\theta^{*},\\dot{x_{t,i}^{*}}\\rangle$ is worse than the truthful strategy independently from what the other arms are playing. Hence, in any Nash equilibrium arm $i$ chooses strategies such that if $i_{t}=i$ then $\\bar{\\langle\\theta^{*},x_{t,i}\\rangle}\\,\\bar{=}\\,\\bar{\\langle\\theta^{*},x_{t,i}^{*}\\rangle}$ . In other words, since the selection is greedy, Mechanism 3 selected the best arm in the given round. Mechanism 3 therefore suffers zero regret in the first $T-(K+1)$ regret in any Nash equilibrium of the arms. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "As discussed in Section 3, we conjecture that there exists no incentive-compatible no-regret algorithm for the strategic linear contextual bandits when the reward observations are subject to noise. The intuition for this conjecture is as follows. Suppose there exists a learning algorithm $M$ that is incentive-compatible and no-regret, that is, the strategy profile where every arm is always truthful is a NE. Since $M$ is also no-regret, the selection policy of $M$ must depend on the reported contexts in some way. In particular, in some round $t$ in which $M$ does not select arm $i$ \u2014but $M$ maps from reported contexts to an action in $[K]$ \u2014there must exist a context $\\tilde{x}_{t}$ that arm $i$ could report that increases its probability of being selected. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Suppose arm $i$ changes its strategy from $\\boldsymbol{\\sigma}_{i}^{*}$ (i.e., being truthful) to the strategy that is always truthful except for round $t$ where it reports $\\tilde{x}_{t}$ instead of $\\boldsymbol{x}_{t,i}^{*}$ . The algorithm $M$ then observes a reward drawn from a distribution with mean $\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ , but might have expected a reward drawn from a distribution with mean $\\left\\langle\\theta^{*},\\tilde{x}_{t}\\right\\rangle$ . We believe that the difference in observed and expected reward is statistically insignificant when arm $i$ only misreports a single or constant number of times. However, due to the intricate relationship between the learning algorithm and the induced NE strategies for the $K$ arms, providing a rigorous argument for this is challenging. ", "page_idx": 14}, {"type": "text", "text": "B Proof of Proposition 3.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of Proposition 3.3. We begin with the incentive-unaware greedy algorithm that in round $t$ pulls arm $i_{t}=\\operatorname{argmax}_{i\\in[K]}\\langle\\theta^{*},x_{t,i}\\rangle$ . Let $\\tilde{x}:=\\mathrm{argmax}_{\\|x\\|\\leq1}\\langle\\theta^{*},x\\rangle$ and w.l.o.g. we assume that $\\tilde{x}$ is unique. We show that the strategy profile, where every arm always reports $\\tilde{x}$ is the only Nash equilibrium under the incentive-unaware greedy algorithm. Let $\\pmb{\\sigma}$ be any strategy profile which is such that there exists a round $t$ and arm $i$ such that $x_{t,i}\\neq\\tilde{x}$ . We distinguish between two cases. ", "page_idx": 14}, {"type": "text", "text": "Case 1: There exists a round $t$ and arm $i$ such that $\\begin{array}{r}{\\langle\\theta^{*},x_{t,i}\\rangle<\\operatorname*{max}_{j\\in[K]}\\langle\\theta^{*},x_{t,j}\\rangle.}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Note that this implies that arm $i$ is not selected by the learner. However, by reporting $\\tilde{x}$ instead of $x_{t,i}$ , arm $i$ is guaranteed to be selected with probability at least $1/K$ . Hence, reporting $x_{t,i}$ is strictly worse than reporting $\\tilde{x}$ so that $\\pmb{\\sigma}$ cannot be a NE. ", "page_idx": 14}, {"type": "text", "text": "Case 2: $\\begin{array}{r}{\\langle\\theta^{*},x_{t,i}\\rangle=\\operatorname*{max}_{j\\in[K]}\\langle\\theta^{*},x_{t,j}\\rangle}\\end{array}$ for all rounds $t$ and arms $i$ ", "page_idx": 14}, {"type": "text", "text": "Note that this implies that each arm $i$ is selected with probability $1/K$ every round.6 Suppose that for any of these rounds $t$ we have $\\operatorname*{max}_{j\\in[K]}\\langle\\theta^{*},x_{t,j}\\rangle<\\langle\\theta^{*},\\tilde{x}\\rangle$ . Then, by reporting $\\tilde{x}$ instead of $x_{t,i}$ arm $i$ could ensure to be selected with probability one. Hence, the strategy where arm $i$ in round $t$ reports $\\tilde{x}$ instead of $x_{t,i}$ is a strictly better response. Therefore, $\\pmb{\\sigma}$ cannot be a NE. The other case is when $\\operatorname*{max}_{j\\in[K]}\\langle\\theta^{*},x_{t,j}\\rangle=\\langle\\theta^{*},\\tilde{x}\\rangle$ , but this cannot be because $\\pmb{\\sigma}$ is supposed to be different to always reporting $\\tilde{x}$ . ", "page_idx": 14}, {"type": "text", "text": "Consequently, the strategy proflie where every arm always reports $\\tilde{x}$ is the only NE under the incentiveunaware greedy algorithm. Under this strategy profile, the incentive-unaware greedy algorithm will play uniformly and therefore suffer linear regret. ", "page_idx": 14}, {"type": "text", "text": "Insufficiency of Non-Strategic Linear Contextual Bandit Algorithms. It is not really surprising that algorithms for non-strategic linear contextual bandits fail in the strategic linear contextual bandit, since such algorithms implicitly incentivize the arms to \u201ccompete\u201d in every round by misreporting their context as the best possible one. Nothing prevents the arms to not myopically optimize their probability of being selected every round. As an example of a standard algorithm for non-strategic linear contextual b\u221aandits we consider LinUCB that in the non-strategic problem setup enjoys a regret guarantee of $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ . ", "page_idx": 14}, {"type": "text", "text": "The reasons for LinUCB\u2019s failure in this strategic problem are the same as for the incentive-unaware greedy algorithm from before. It will be the strictly dominant strategy for the arms to maximize their selection probability in the given round by misreporting their context. Recall that LinUCB maintains a least-squares estimator given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{\\theta}_{t}=\\operatorname*{argmin}_{\\theta\\in\\mathbb{R}^{d}}\\sum_{\\ell=1}^{t-1}(\\langle\\theta,x_{\\ell,i_{\\ell}}\\rangle-r_{\\ell,i_{\\ell}})^{2}+\\lambda\\|\\theta\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and in round $t$ selects arm (ties broken uniformly at random) ", "page_idx": 15}, {"type": "equation", "text": "$$\ni_{t}=\\underset{i\\in[K]}{\\mathrm{argmax}}\\langle\\hat{\\theta}_{t},x_{t,i}\\rangle+\\sqrt{\\beta_{t}}\\|x_{t,i}\\|_{V_{t}^{-1}},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The argument for LinUCB will be the same as for the incentive-unaware greedy algorithm. Let $\\tilde{x}_{t}:=\\mathrm{argmax}_{\\|x\\|_{2}\\leq1}\\operatorname{UCB}_{t}(x)$ and w.l.o.g. assume that $\\tilde{x}_{t}$ is unique. ", "page_idx": 15}, {"type": "text", "text": "Importantly, in what follows, keep in mind that it will not matter how the reports of arm $i$ influenced ${\\tilde{\\theta}}_{t}$ or $V_{t}$ in previous rounds. Once again, suppose $\\pmb{\\sigma}$ is a strategy proflie such that there exists a round $t$ and arm $i$ such that conditioned on the history $x_{t,i}\\neq\\tilde{x}_{t}$ . Once again we distinguish between the following two cases: ", "page_idx": 15}, {"type": "text", "text": "Case 1: There exists a round $t$ and arm $i$ such that $\\begin{array}{r}{\\mathrm{UCB}_{t}(x_{t,i})<\\operatorname*{max}_{j\\in[K]}\\mathrm{UCB}_{t}(x_{t,j}).}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "This implies that arm $i$ was not selected by the learner in round $t$ . However, by reporting $\\tilde{x}_{t}$ and in all future rounds report $\\tilde{x}_{\\ell}$ for $\\ell>t$ , arm $i$ can guarantee to be selected with probability at least $1/K$ in round $t$ and at least as many selections as under $\\sigma_{i}$ . Hence, $\\sigma_{i}$ cannot be a best response to $\\sigma_{-i}$ . ", "page_idx": 15}, {"type": "text", "text": "Case 2: $\\begin{array}{r}{\\mathrm{UCB}_{t}(x_{t,i})=\\operatorname*{max}_{j\\in[K]}\\mathrm{UCB}_{t}(x_{t,j})}\\end{array}$ for all rounds $t$ and arms $i$ . ", "page_idx": 15}, {"type": "text", "text": "Note that this implies that arm $i$ is selected with probability $1/K$ every round. Suppose that for any round $t$ it is the case that $\\mathrm{max}_{j\\in[K]}\\,\\mathrm{UCB}_{t}(x_{t,j})\\leq\\mathrm{UCB}_{t}(\\bar{x})$ . Then, by choosing strategy $\\tilde{x}_{t}$ in round $t$ and $\\tilde{x}_{\\ell}$ adaptively for all future rounds $\\ell>t$ , arm $i$ obtains more selections than when reporting $x_{t,i}$ . Hence, $\\pmb{\\sigma}$ cannot be a NE. ", "page_idx": 15}, {"type": "text", "text": "As a result, the strategy proflie where all arms report $\\tilde{x}_{t}$ in round $t$ is the only NE and LinUCB suffers linear regret, as it pulls arms uniformly at random. In exactly the same way, we can also show that the algorithms for linear contextual bandits with adversarial context corruptions in [8] suffer linear regret. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "C Assumption 1 and Remark 4.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Example 1. We here give a simple example where a strategic arm can simulate a situation where it is always optimal even though it is only optimal half of the time. ", "page_idx": 15}, {"type": "text", "text": "Let $\\theta^{*}=1$ and consider the following problem instance with two arms 1 and 2, where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x_{t,1}^{*}=\\left\\{0,\\;t\\;\\mathrm{is~even}\\right.\\qquad\\mathrm{and~}\\quad x_{t,2}^{*}=1/4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, suppose that arm 1 always reports $x_{t,1}=1/2$ and arm 2 reports truthfully (or approximately so). Then, arm 1 appears optimal every round $t$ . In particular, on average arm when we pull arm 1 it has reward $1/2$ , which is consistent with its report of $x_{t,1}=1/2$ . ", "page_idx": 15}, {"type": "text", "text": "Now, consider a second problem instance, where ", "page_idx": 15}, {"type": "equation", "text": "$$\nx_{t,1}^{*}=1/2\\quad\\mathrm{~and~}\\quad x_{t,2}^{*}=1/4.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recall that we assume that, like almost always in the literature, the noise is sub-Gaussian. As an example, let\u2019s consider Bernoulli-noise such that $\\mathbb{P}(r_{t,i}=1)=x_{t,i}^{*}=1-\\mathbb{P}(r_{t,i}=0)$ . Then, the first environment when arm 1 manipulates as suggested is identical to the second environment when the arms are truthful. In the second environment, to suffer sublinear regret we must select arm 1 order $T$ many times. However, in the first environment, we must select arm 1 only order $o(T)$ many times. ", "page_idx": 15}, {"type": "text", "text": "Discussion. Based on these observations, we expect that we would have to make additional (strong) assumptions about the distribution of the noise and the prior knowledge of the learner in order to drop Assumption 1. As an example, let\u2019s assume standard normal noise $\\mathbf{\\bar{\\mathcal{N}}}(0,1)$ and that the learner knows that the variance is always 1 a priori. Then, one potentially effective approach would be to extend our current grim trigger to additionally threaten arms that misreport their variance. Of course, the variance is unknown, however, we could estimate the variance of each arm\u2019s reports separately and use confidence intervals around the estimated variance. We could then threaten an arm with elimination if the arm\u2019s estimated variance falls out of the confidence interval. However, we expect there to be several technical subtleties in analyzing such variance-aware mechanisms. ", "page_idx": 15}, {"type": "text", "text": "D Proof of Theorem 4.1 and Theorem 4.2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "D.1 Preliminaries ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We begin with some preliminaries that we use in the proofs of both Theorem 4.1 and Theorem 4.2. Note that $\\mathbb E[r_{t,i}]=\\langle\\bar{\\theta}^{*},x_{t,i}^{*}\\rangle$ and we denote the total true mean reward by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\hat{r}_{t,i}^{*}:=\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\langle\\theta^{*},x_{\\ell,i}^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Let us also recall the definition of the total observed reward as $\\begin{array}{r}{\\hat{r}_{t,i}:=\\sum_{\\ell\\le t:\\;i_{\\ell}=i}r_{\\ell,i}}\\end{array}$ and recall its upper confidence bound $\\mathrm{UCB}_{t}(\\hat{r}_{t,i}):=\\hat{r}_{t,i}+2\\sqrt{n_{t}(i)\\log(T)}$ and define the lower confidence bound $\\mathrm{LCB}_{t}(\\hat{r}_{t,i}):=\\hat{r}_{t,i}-2\\sqrt{n_{t}(i)\\log(T)}$ . ", "page_idx": 16}, {"type": "text", "text": "We now analyze the basic properties of the grim trigger condition of GGTM, which eliminates arm $i$ in round $t$ if ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\langle\\theta^{*},x_{\\ell,i}\\rangle>\\mathrm{UCB}_{t}(\\hat{r}_{t,i}),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "or equivalently ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}\\rangle-r_{\\ell,i}\\right)>2\\sqrt{n_{t}(i)\\log(T)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Define the good event $\\mathcal{G}$ as the event that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{G}:=\\left\\{\\mathrm{LCB}_{t}(\\hat{r}_{t,i})\\leq\\hat{r}_{t,i}^{*}\\leq\\mathrm{UCB}_{t}(\\hat{r}_{t,i})\\,\\forall t\\in[T],i\\in[K]\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Hoeffding\u2019s inequality, we know that the good event occurs with probability at least $\\mathbb{P}(\\mathcal{G})\\geq1\\!-\\!\\frac{1}{T^{2}}$ . Next, let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tau_{i}:=\\operatorname*{min}\\{t\\in[T]\\colon i\\notin A_{t}\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "denote the first round in which $i$ is no longer active and, by convention, let $\\tau_{i}=T$ if $i\\in A_{T}$ . By design of the grim trigger condition, note that $\\tau_{i}=T$ for all $i\\in[K]$ on the good event $\\mathcal{G}$ if all arms always report truthfully. ", "page_idx": 16}, {"type": "text", "text": "We now provide a general result bounding the maximal amount of manipulation any arm can exercise before being eliminated by GGTM. ", "page_idx": 16}, {"type": "text", "text": "Lemma D.1. On the good event $\\mathcal{G}$ , for any round $t\\in[T]$ and any arm $i\\in A_{t}$ it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}\\rangle-x_{\\ell,i}^{*}\\right)\\leq4\\sqrt{n_{t}(i)\\log(T)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From the definition of $\\tau_{i}$ this entails that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq\\tau_{i}:\\ i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}\\rangle-x_{\\ell,i}^{*}\\right)\\leq4\\sqrt{n_{\\tau_{i}}(i)\\log(T)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. On the good event $\\mathcal{G}$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}^{*}\\rangle-r_{\\ell,i}\\right)\\in\\big[-2\\sqrt{n_{t}(i)\\log(T)},+2\\sqrt{n_{t}(i)\\log(T)}\\big],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which implies that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}\\rangle-r_{\\ell,i}\\right)\\geq\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\langle\\theta^{*},x_{\\ell,i}-x_{\\ell,i}^{*}\\rangle-2\\sqrt{n_{t}(i)\\log(T)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, if $\\begin{array}{r}{\\mathrel{\\phantom{=}}\\sum_{\\ell\\leq t:\\;i_{\\ell}=i}\\langle\\theta^{*},x_{\\ell,i}-x_{\\ell,i}^{*}\\rangle>4\\sqrt{n_{t}(i)\\log(T)}}\\end{array}$ , then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}\\rangle-r_{\\ell,i}\\right)>2\\sqrt{n_{t}(i)\\log(T)},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which means that arm $i$ is eliminated from $A_{t}$ . Finally, $\\tau_{i}$ is defined as the first round such that $i\\not\\in A_{t}$ so that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}\\rangle-x_{\\ell,i}^{*}\\right)\\leq4{\\sqrt{n_{t}(i)\\log(T)}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $t\\leq\\tau_{i}$ and $\\begin{array}{r}{\\sum_{\\ell\\leq\\tau_{i}+1:\\ i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}\\rangle-x_{\\ell,i}^{*}\\right)>4\\sqrt{n_{\\tau_{i}}(i)\\log(T)}.}\\end{array}$ ", "page_idx": 16}, {"type": "text", "text": "For completeness, we also formally state the fact that on the good event $\\mathcal{G}$ any truthful arm is not eliminated with high probability. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.2. If arm $i$ reports truthfully every round, i.e., plays strategy $\\boldsymbol{\\sigma}_{i}^{*}$ with $x_{t,i}=x_{t,i}^{*}$ for all round $t\\in[T]$ , then on the good event $\\mathcal{G}$ arm $i$ stays active for all rounds. ", "page_idx": 17}, {"type": "text", "text": "Proof. When arm $i$ is truthful, then $\\begin{array}{r l r}{\\sum_{\\ell\\le t:\\ i_{\\ell}=i}\\langle\\theta^{*},x_{\\ell,i}\\rangle\\!\\!}&{{}=}&{\\!\\!\\hat{r}_{t,i}^{*}}\\end{array}$ . On the good event, $\\hat{r}_{t,i}^{*}~\\leq$ $\\mathrm{UCB}_{t}\\big(\\hat{r}_{t,i}\\big)$ for all $t\\in[T]$ . Hence, the grim trigger condition is never satisfied and arm $i$ remains active throughout all $T$ rounds. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "D.2 Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Theorem 4.1. We have to show that the strategy profile $\\sigma^{*}$ , where ev\u221aery arm always truthfully reports their context, i.e., $x_{t,i}=x_{t,i}^{*}$ for all $(t,i)\\in[T]\\times[K]$ , forms a $\\tilde{\\mathcal{O}}(\\sqrt{T})$ -Nash equilibrium for the arms under G\u221aGTM. We do this by showing that any deviating strategy $\\sigma_{i}$ for arm $i$ cannot gain more than this $\\sqrt{T}$ clicks. Recall that $i_{t}^{*}$ is the optimal arm in round $t$ and $i_{t}$ the arm the learner selects. ", "page_idx": 17}, {"type": "text", "text": "We begin by deriving the minimum utility of every arm when everyone is truthful. To this end, let $\\begin{array}{r}{n_{T}^{*}(i):=\\dot{\\sum_{t=1}^{T}}\\mathbb{1}\\{i_{t}^{*}=i\\}}\\end{array}$ nboe  tahrem n ugemtsb eerl iomf itinmateesd a (rLme $i$ m ism tah eD o.2p)ti amnadl $i$ rfuotrh faulll, $\\mathcal{G}$ $\\langle\\theta^{*},x_{t,i}\\rangle\\,=\\,\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ $(t,i)\\in[T]\\times[K]$ . As a result, GGTM pulls the optimal arm $i_{t}^{*}$ in every round $t$ . First, note that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{\\sigma}^{*}}[n_{T}(i)]\\ge n_{T}^{*}(i)-\\frac{1}{T},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "because on the good event $\\mathcal{G}$ (when everyone is truthful), we have $n_{T}(i)\\;\\geq\\;n_{T}^{*}(i)$ . Since by construction $\\mathbb{P}(\\mathcal{G})\\geq1-\\mathbb{1}/T^{2}$ , the lower bound follows. ", "page_idx": 17}, {"type": "text", "text": "Next, we bound the utility of a deviating strategy $\\sigma_{i}$ in response to GGTM and the other arms\u2019 truthful strategies $\\boldsymbol{\\sigma}_{-i}^{*}$ . On the good event $\\mathcal{G}$ , when the arms play strategies $(\\sigma_{i},\\sigma_{-i}^{*})$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle n_{T}(i)=\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i,i_{t}^{*}=i\\}+\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i,i_{t}^{*}\\neq i\\}}}\\\\ {{\\displaystyle\\leq\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i\\}+\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i,i_{t}^{*}\\neq i\\}}}\\\\ {{\\displaystyle=n_{T}^{*}(i)+\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i,i_{t}^{*}\\neq i\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We will now bound the sum on the right hand side from above. ", "page_idx": 17}, {"type": "text", "text": "Every arm $j\\neq i$ is truthful and therefore, on the good event, $j\\in A_{t}$ for all $t$ . If the optimal arm is not $i$ , i.e., $i_{t}^{*}\\neq i$ , it means that $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle>0$ . Next, since GGTM selects the arms greedily according to the reported reward, the event $i_{t}=i$ implies that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle\\theta^{*},x_{t,i}\\rangle\\geq\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used that any arm $i_{t}^{*}\\neq i$ is truthful so that $\\langle\\theta^{*},x_{t,i_{t}^{*}}\\rangle=\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle$ . As a result, we can apply Lemma D.1 to obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\neq i,i_{t}=i\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle\\leq\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i,i_{t}^{*}\\neq i\\}\\langle\\theta^{*},x_{t,i}-x_{t,i}^{*}\\rangle}}\\\\ &{}&{\\leq\\displaystyle\\sum_{t=1}^{\\tau_{i}}\\mathbb{1}\\{i_{t}=i\\}\\langle\\theta^{*},x_{t,i}-x_{t,i}^{*}\\rangle}\\\\ &{}&{\\leq4\\sqrt{n_{\\tau_{i}}(i)\\log(T)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since for $i_{t}^{*}~\\neq~i$ the gap $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle$ is positive and assumed to be constant, we get that $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\neq i,i_{t}=i\\}\\le\\mathcal{O}(\\sqrt{n_{\\tau_{i}}(i)\\log(T)})}\\end{array}$ . We coarsely upper bound $n_{\\tau_{i}}(i)$ by $T$ and using that the good event $\\mathcal{G}$ has probability at least $\\mathrm{i}\\stackrel{}{-}\\mathrm{}{}^{1/}T^{2}$ , we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma_{i},\\sigma_{-i}^{*}}[n_{T}(i)]\\le n_{T}^{*}(i)+\\mathcal{O}\\left(\\sqrt{T\\log(T)}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We have thus shown that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma^{*}}[n_{T}(i)]\\ge\\mathbb{E}_{\\sigma_{i},\\sigma_{-i}^{*}}[n_{T}(i)]+\\mathcal{O}\\left(\\sqrt{T\\log(T)}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any deviating (dishonest) strategy $\\sigma_{i}$ . This means that $\\sigma^{*}$ is a $\\tilde{\\mathcal{O}}(\\sqrt{T})$ -Nash equilibrium for the arms. ", "page_idx": 18}, {"type": "text", "text": "Finally, the regret of GGTM when the arms are truthful is quickly bounded by $^1\\!/\\!T$ by using the fact that on the good event no arm gets eliminated and, therefore, GGTM picks the round-optimal arm every round. The event that $\\mathcal{G}$ does not hold has probability at most $^1\\!/T^{2}$ which implies expected regret $^1\\!/\\!T$ , i.e., $R_{T}(\\mathrm{GGTM},\\pmb{\\sigma}^{*})\\leq1/T$ . ", "page_idx": 18}, {"type": "text", "text": "D.3 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 4.2. The proof of Theorem 4.2 is notably more involved than that of Theorem 4.1, even though the general proof idea remains similar. ", "page_idx": 18}, {"type": "text", "text": "We begin by decomposing of GGTM into the rounds where the optimal arm is active and the rounds in which it is being ignored. To this end, recall the definition of the arm that is optimal in round $t$ as $i_{t}^{*}:=\\operatorname*{argmax}_{i\\in[K]}\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ . We have ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{T}=\\underbrace{\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\in A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]}_{I_{1}}+\\underbrace{\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]}_{I_{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We now bound $I_{1}$ and $I_{2}$ separately as follows. ", "page_idx": 18}, {"type": "text", "text": "Lemma D.3 (Bounding $I_{1}$ ). ", "text_level": 1, "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\in A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]\\leq\\mathcal{O}\\left(\\sqrt{K T\\log(T)}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $i_{t}$ denote the selection of GGTM in round $t$ . Recall that GGTM greedily selects the arm in $A_{t}$ with highest reported value in round $t$ , that is, $\\langle\\theta^{*},x_{t,i_{t}}\\rangle=\\operatorname*{max}_{i\\in A_{t}}\\langle\\theta^{*},x_{t,i}\\rangle$ . Consequently, on event $\\{i_{t}^{*}\\in A_{t}\\}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\bigl\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\bigr\\rangle\\leq\\bigl\\langle\\theta^{*},x_{t,i_{t}^{*}}\\bigr\\rangle\\leq\\operatorname*{max}_{i\\in A_{t}}\\bigl\\langle\\theta^{*},x_{t,i}\\bigr\\rangle=\\bigl\\langle\\theta^{*},x_{t,i_{t}}\\bigr\\rangle,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality holds by the assumption the optimal arm $i_{t}^{*}$ reports their value at least as high as their true value. As a consequence, it holds that $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\leq\\langle\\theta^{*},x_{t,i_{t}}-x_{t,i_{t}}^{*}\\rangle$ which implies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\in A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]\\leq\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\in A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}}-x_{t,i_{t}}^{*}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When $i_{t}^{*}\\in A_{t}$ , a necessary condition for arm $i$ to be selected in round $t$ (i.e., $i_{t}=i$ ) is that $t\\leq\\tau_{i}$ Finally, we split the sum into each arm\u2019s contribution and apply Lemma D.1 to obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\n(5)\\leq\\mathbb{E}\\left[\\sum_{i=1}^{K}\\sum_{t=1}^{\\tau_{i}}\\mathbb{1}\\{i_{t}=i\\}\\langle\\theta^{*},x_{t,i_{t}}-x_{t,i_{t}}^{*}\\rangle\\right]\\leq\\mathbb{E}\\left[\\sum_{i=1}^{K}4\\sqrt{n_{\\tau_{i}}(i)\\log(T)}\\right]\\leq4\\sqrt{K T\\log(T)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last step follows from Jensen\u2019s inequality by bounding $n_{\\tau_{i}}(i)$ by $n_{T}(i)$ and using that $\\begin{array}{r}{\\sum_{i=1}^{K}n_{T}(i)\\leq T}\\end{array}$ by definition of $n_{T}(i)$ . \u53e3 ", "page_idx": 18}, {"type": "text", "text": "While bounding $I_{1}$ is fairly straightforward and we did not have to rely on the fact that the arms respond in Nash equilibrium, bounding $I_{2}$ becomes more challenging as we must argue that it is in each arms\u2019 interest to maintain active for a sufficiently long time. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.4 (Bounding $I_{2}$ ). ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\big(\\mu_{t,i_{t}^{*}}^{*}-\\mu_{t,i_{t}}^{*}\\big)\\right]\\le5K^{2}\\sqrt{K T\\log(T)}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. To bound $I_{2}$ we argue via the best response property of the Nash equilibrium. This requires some intermediate steps. We begin with a lower bound on the expected number of selections any arm must receive when the arms act according to a Nash equilibrium under GGTM. ", "page_idx": 19}, {"type": "text", "text": "Recall the definition $\\begin{array}{r}{n_{\\tau}^{*}(i):=\\sum_{t=1}^{\\tau}\\mathbb{1}\\{i_{t}^{*}=i\\}}\\end{array}$ and that the indicator variables $\\mathbb{1}\\{i_{t}^{*}=i\\}$ are not random, since we work under an adversarially chosen sequence of true contexts. In contrast, the indicator $\\mathbb{1}\\{i_{t}=i\\}$ is a random variable as it generally depends on the random reward observations and any randomization of the algorithm. ", "page_idx": 19}, {"type": "text", "text": "The following lemma provides a lower bound on the number of allocations any arm must receive in equilibrium. To p\u221arove the lemma, we show that we are able to protect any truthful arm from losing more than order $\\sqrt{K T}$ allocations to manipulating arms. This is crucial as it would be impossible to incentivize approximately truthful arm behavior if an arm would lose too many allocations, e.g., order $T$ many, by doing so. ", "page_idx": 19}, {"type": "text", "text": "A key challenge here is that under two different strategies $\\sigma_{i}$ and $\\sigma_{i}^{\\prime}$ , the set of active arms can be quite different. This is the case since even though we estimate each arm\u2019s expected reward independently, arm $i$ can still slightly influence the elimination of some other arm $j$ by poaching selections from them. As a result, we must content ourselves with a more conservative bound than one may originally expect. ", "page_idx": 19}, {"type": "text", "text": "Lemma D.5. Let $\\pmb{\\sigma}\\in\\mathrm{NE}(\\mathbf{GGTM})$ . Then, ", "page_idx": 19}, {"type": "text", "text": "In particular, it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\sigma}[n_{T}(i)]\\geq n_{T}^{*}(i)-{\\mathcal O}\\big(\\sqrt{K T\\log(T)}\\big).}\\\\ &{\\mathbb{E}_{\\sigma}[n_{t}(i)]\\geq n_{t}^{*}(i)-{\\mathcal O}\\big(\\sqrt{K T\\log(T)}\\big)\\,f o r\\,a n y\\,t\\in[T].}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We use the fact that if $\\pmb{\\sigma}\\,=\\,(\\sigma_{1},\\ldots,\\sigma_{K})$ is a NE under GGTM, then $\\sigma_{i}$ must be a best response to $\\sigma_{-i}$ , i.e., $\\mathbb{E}_{\\sigma_{i},\\sigma_{-i}}[n_{T}(i)]\\ge\\mathbb{E}_{\\sigma_{i}^{\\prime},\\sigma_{-i}}[n_{T}(i)]$ for all strategies $\\sigma_{i}^{\\prime}$ . In particular, it must hold for the truthful strategy $\\boldsymbol{\\sigma}_{i}^{*}$ that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma_{i},\\sigma_{-i}}[n_{T}(i)]\\ge\\mathbb{E}_{\\sigma_{i}^{*},\\sigma_{-i}}[n_{T}(i)].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We focus on the good event $\\mathcal{G}$ so that $i\\in A_{t}$ for all $t$ given that arm $i$ is truthful. We are interested in the number of rounds such that $i_{t}^{*}=i$ and $i_{t}\\neq i$ . Given strategies $(\\sigma_{i}^{*},\\sigma_{-i})$ so that $i\\in A_{T}$ on the good event, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}\\neq i\\}=\\sum_{j\\neq i}\\sum_{t=1}^{\\tau_{j}}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}=j\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note that $i_{t}=j$ with $i_{t}^{*}=i\\in A_{t}$ implies that $\\langle\\theta^{*},x_{t,j}\\rangle\\geq\\langle\\theta^{*},x_{t,i}\\rangle$ . Moreover, because $i$ is truthful and $i_{t}^{*}=i$ , we have $\\langle\\bar{\\theta^{*}},x_{t,i}\\rangle=\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle$ so that $\\langle\\theta^{\\bar{*}},x_{t,j}\\rangle>\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle$ . As a result, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,j}^{*}\\rangle<\\langle\\theta^{*},x_{t,j}-x_{t,j}^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It then follows from Lemma D.1 that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t=1}^{\\tau_{j}}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}=j\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,j}^{*}\\rangle<\\sum_{t=1}^{\\tau_{j}}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}=j\\}\\langle\\theta^{*},x_{t,j}-x_{t,j}^{*}\\rangle}}\\\\ &{}&{\\leq\\displaystyle\\sum_{t=1}^{\\tau_{j}}\\mathbb{1}\\{i_{t}=j\\}\\langle\\theta^{*},x_{t,j}-x_{t,j}^{*}\\rangle\\ \\ \\ }\\\\ &{}&{\\leq4\\sqrt{n_{\\tau_{j}}(j)\\log(T)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,j}^{*}\\rangle$ is constant for $i_{t}^{*}=i,i_{t}=j$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{j\\neq i}\\sum_{t=1}^{T}\\mathbb{1}\\big\\{i_{t}^{*}=i,i_{t}\\neq i\\big\\}\\leq\\sum_{j\\neq i}\\mathcal{O}\\big(\\sqrt{n_{\\tau_{j}}(j)\\log(T)}\\big)\\leq\\mathcal{O}\\big(\\sqrt{K T\\log(T)}\\big),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows from Jensen\u2019s inequality. Recalling that $\\mathbb{P}(\\mathcal{G})\\geq1-1/T^{2}$ , this provides us with the following lower bound on the utility of the truthful strategy ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\sigma_{i}^{*},\\sigma_{-i}}[n_{T}(i)]=\\mathbb{E}_{\\sigma_{i}^{*},\\sigma_{-i}}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i\\}\\right]}\\\\ &{\\phantom{\\sum_{\\sigma_{i}^{*},\\sigma_{-i}}\\bigl[n_{T}(i)\\bigr]}\\geq\\mathbb{E}_{\\sigma_{i}^{*},\\sigma_{-i}}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i\\}-\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}\\neq i\\}\\right]}\\\\ &{\\geq n_{T}^{*}(i)-\\mathcal{O}\\bigl(\\sqrt{K T\\log(T)}\\bigr)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that we, like before, we account for the event $\\mathcal{G}^{c}$ by increasing the constant factor by one, since $(1-1/T^{2})n_{T}(i)\\geq n_{T}(i)-1/T$ as $n_{T}(i)\\le T$ . ", "page_idx": 20}, {"type": "text", "text": "Since $\\sigma_{i}$ has to be a best response to $\\sigma_{-i}$ , it must be as least as good as $\\boldsymbol{\\sigma}_{i}^{*}$ sot that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}[n_{T}(i)]\\ge\\mathbb{E}_{\\sigma_{i}^{*},\\sigma_{-i}}[n_{T}(i)]\\ge n_{T}^{*}(i)-\\mathcal{O}\\big(\\sqrt{K T\\log(T)}\\big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To get the result for any $t\\,\\in\\,[T]$ , suppose that on the good event $\\mathcal{G}$ it holds that $n_{t}(i)\\,<\\,n_{t}^{*}(i)\\,-$ $\\omega(\\sqrt{K T\\log(T)})$ . Now, recall from equation (7) that the number of rounds such that $i_{t}^{*}=\\textit{i}$ and $i_{t}\\neq i$ is bounded by $\\mathcal{O}(\\sqrt{K t\\log(T)})$ on event $\\mathcal{G}$ . Hence, since we assumed that $n_{t}(i)\\,<$ $n_{t}^{*}(i)-\\omega(\\sqrt{K T\\log(T)})$ and $\\langle\\theta^{*},x_{t,i}-x_{t,i}^{*}\\rangle\\geq0$ , it must hold that $\\tau_{i}<t$ . Consequently, on the good event $\\mathcal{G}$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\nn_{\\tau_{i}}(i)\\leq n_{t}(i)<n_{t}^{*}(i)-\\omega\\big(\\sqrt{K T\\log(T)}\\big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This implies that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}(i)]<\\left(1-1/T^{2}\\right)\\Big(n_{t}^{*}(i)-\\omega\\big(\\sqrt{K T\\log(T)}\\big)\\Big)+1/T\\leq n_{t}^{*}(i)-\\omega\\big(\\sqrt{K T\\log(T)}\\big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This stands in contradiction to the earlier lower bound of $\\mathbb{E}_{\\sigma}[n_{T}(i)]\\,=\\,\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}(i)]\\,\\ge\\,n_{T}^{*}(i)\\,-$ $\\mathcal{O}\\big(\\sqrt{K T\\log(T)}\\big)$ . ", "page_idx": 20}, {"type": "text", "text": "Next, we provide an upper bound on the number of times an arm is pulled in any Nash equilibrium.   \nIn other words, we bound the profit any arm can make under GGTM from misreporting contexts. ", "page_idx": 20}, {"type": "text", "text": "Lemma D.6. Let $\\pmb{\\sigma}$ be any NE under GGTM. Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}[n_{T}(i)]\\le\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}^{*}(i)]+{\\mathcal O}\\big((K-1)\\sqrt{K T\\log(T)}\\big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Note that $\\begin{array}{r}{\\sum_{i=1}^{K}n_{\\tau}^{*}(i)=\\sum_{i=1}^{K}\\sum_{t=1}^{\\tau}\\mathbb{1}\\{i_{t}^{*}=i\\}=\\tau}\\end{array}$ for any $\\tau\\in[T]$ . Using Lemma D.5, we then obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\sigma}[n_{T}(i)]=\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}(i)]}\\\\ &{=\\mathbb{E}_{\\sigma}\\left[\\underset{t=1}{\\overset{7n}{\\sum}}1\\{i_{t}=i\\}\\right]}\\\\ &{=\\mathbb{E}_{\\sigma}\\left[\\underset{t=1}{\\overset{7n}{\\sum}}(1-1\\{i_{t}\\neq i\\})\\right]}\\\\ &{=\\mathbb{E}_{\\sigma}\\left[\\tau_{i}\\right]-\\underset{j\\neq i}{\\sum}[\\tau_{n_{i}}(j)]}\\\\ &{\\leq\\mathbb{E}_{\\sigma}[\\tau_{i}]-\\underset{j\\neq i}{\\sum}\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}^{*}(j)]+{\\mathcal{O}}\\big((K-1)\\sqrt{K T\\log(T)}\\big)}\\\\ &{=\\mathbb{E}_{\\sigma}[n_{\\tau}^{*}(i)]+{\\mathcal{O}}\\big((K-1)\\sqrt{K T\\log(T)}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining Lemma D.5 and Lemma D.6 we get for any Nash equilibrium $\\pmb{\\sigma}\\in\\mathrm{NE}(\\mathbf{GGTM})$ that $\\begin{array}{r}{\\mathbb{E}_{\\sigma}[n_{T}^{*}(i)]-\\mathcal{O}\\big(\\sqrt{K T\\log(T)}\\big)\\leq\\mathbb{E}_{\\sigma}[n_{T}(i)]\\leq\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}^{*}(i)]-\\mathcal{O}\\big((K-1)\\sqrt{K T\\log(T)}\\big)}\\end{array}$ , which implies that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pmb{\\sigma}}[n_{T}^{*}(i)-n_{\\tau_{i}}^{*}(i)]\\le\\mathcal{O}\\big(K\\sqrt{K T\\log(T)}\\big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The expression $n_{T}^{*}(i)\\!-\\!n_{\\tau_{i}}^{*}(i)$ is the number of rounds where arm $i$ was optimal but already eliminated by the grim trigger. As a result, we can express the total number of rounds where the round-optimal arm $i_{t}^{*}$ was no longer active as follows. ", "page_idx": 21}, {"type": "text", "text": "Lemma D.7. For any $\\pmb{\\sigma}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\right]=\\sum_{i=1}^{K}\\mathbb{E}_{\\sigma}[n_{T}^{*}(i)-n_{\\tau_{i}}^{*}(i)].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Rewriting $\\{i_{t}^{*}\\notin A_{t}\\}$ yields ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\sigma}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\right]=\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}_{\\sigma}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i\\notin A_{t}\\}\\right]}&{}\\\\ {=\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}_{\\sigma}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i\\}-\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i\\in A_{t}\\}\\right]}&{}\\\\ &{=\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}_{\\sigma}\\left[n_{T}^{*}(i)-n_{\\tau_{i}}^{*}(i)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, note that $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\leq1$ so that from equation (10) it follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]\\le\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\right]}&{}&\\\\ {=\\displaystyle\\sum_{i=1}^{K}\\mathbb{E}\\left[n_{T}^{*}(i)-n_{\\tau_{i}}^{*}(i)\\right]}&{}&\\\\ {\\le\\displaystyle\\sum_{i=1}^{K}\\mathcal{O}\\big(K\\sqrt{K T\\log(T)}\\big)=\\mathcal{O}\\big(K^{2}\\sqrt{K T\\log(T)}\\big).}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Connecting the bound on $I_{1}$ and $I_{2}$ , we then obtain the final regret bound of Theorem 4.2 ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{T}(\\mathrm{GGTM},\\sigma)\\leq\\mathcal{O}\\left(\\underbrace{\\sqrt{K T\\log(T)}}_{\\mathrm{Lemma}\\,\\mathrm{D}.3}+\\underbrace{K^{2}\\sqrt{K T\\log(T)}}_{\\mathrm{Lemma}\\,\\mathrm{D}.4}\\right)\\leq\\tilde{\\mathcal{O}}\\left(K^{2}\\sqrt{K T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Remark D.8. We want to briefly comment on the existence of a Nash equilibrium. Since each arm\u2019s strategy space, given by $\\{x\\in\\mathrm{{\\bar{R}}}^{d}\\colon\\|x\\|_{2}\\leq1\\}$ in every round, is continuous, it is not obvious that a Nash equilibrium for the arms exists under every algorithm. However, Glickberg\u2019s theorem shows that the continuity of the arms\u2019 utility in the arms\u2019 strategies is a sufficient condition for the existence of a NE, since the strategy space is compact. We can then ensure the continuity by, e.g., choosing arms proportionally to $\\exp(T\\langle\\theta^{*},x_{t,i}\\rangle)$ in GGTM and $\\exp(T\\mathrm{UCB}_{t}(x_{t,i}))$ in OptGTM, and remarking that the probability of eliminating arm $i$ in round $t$ is continuous in $x_{t,i}$ conditional on any history. Due to the exponential scaling with $T$ the effect of such slight randomization is negligible in the regret analysis. ", "page_idx": 21}, {"type": "text", "text": "E Proof of Theorem 5.1 and Theorem 5.2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The following preliminaries are fundamental to the proofs of Theorem 5.1 and Theorem 5.2 so that we derive them jointly here. ", "page_idx": 22}, {"type": "text", "text": "E.1 Preliminaries ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We begin by recalling the definition of the least-squares estimator w.r.t. arm $i$ \u2019s reported contexts and the corresponding confidence ellipsoid $C_{t,i}$ . Note that since the arms are manipulating their contexts, the least-squares estimator may not accurately estimate $\\theta^{*}$ and $\\theta^{*}$ may not be contained in $C_{t,i}$ w.h.p. As discussed in the main text, since accurate estimation of $\\theta^{*}$ appears hopeless, the main idea idea is to incentivize arms to report contexts such that our expected reward does not differ substantially from the observed reward. ", "page_idx": 22}, {"type": "text", "text": "The least-squares estimator w.r.t. arm $i$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\theta}_{t,i}=\\underset{\\theta\\in\\mathbb{R}^{d}}{\\operatorname{argmin}}\\left(\\sum_{\\ell<t:\\;i_{\\ell}=i}\\left(\\langle\\theta,x_{\\ell,i}\\rangle-r_{\\ell,i}\\right)^{2}+\\lambda\\|\\theta\\|_{2}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\lambda>0$ . In the algorithm, we set the penalty factor to $\\lambda=1$ . The closed form solution is then given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{\\theta}_{t,i}=V_{t,i}^{-1}\\sum_{\\ell<t:\\;i_{\\ell}=i}x_{\\ell,i}r_{\\ell,i}\\quad\\mathrm{with}\\quad V_{t,i}=\\lambda I+\\sum_{\\ell<t:\\;i_{\\ell}=i}x_{\\ell,i}x_{\\ell,i}^{\\top}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The confidence set $C_{t,i}$ is defined as ", "page_idx": 22}, {"type": "equation", "text": "$$\nC_{t,i}=\\left\\{\\theta\\in\\mathbb{R}^{d}\\colon\\|\\hat{\\theta}_{t,i}-\\theta\\|_{V_{t}}^{2}\\leq\\beta_{t,i}\\right\\},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where we let $\\begin{array}{r}{\\beta_{t,i}=\\sqrt{d\\log\\left(\\frac{1+n_{t}(i)/\\lambda}{\\delta}\\right)}+\\sqrt{\\lambda S}}\\end{array}$ with $\\|\\theta^{*}\\|_{2}\\leq S$ and $\\delta={^1\\!/}T^{2}$ . ", "page_idx": 22}, {"type": "text", "text": "We now translate the standard result used to assert the validity of the confidence set to our situation. Clearly, when the sequence of $x_{t,i}$ differs significantly from the true contexts $\\boldsymbol{x}_{t,i}^{*}$ , the true parameter $\\theta^{*}$ will not be contained in $C_{t,i}$ . Instead, we will formulate the concentration result as follows. ", "page_idx": 22}, {"type": "text", "text": "Lemma E.1. Suppose there exists $\\theta_{i}^{*}\\in\\mathbb{R}^{d}$ such that for all $t$ with $i_{t}=i$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\langle\\theta^{*},x_{t,i}^{*}\\rangle=\\langle\\theta_{i}^{*},x_{t,i}\\rangle.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In other words, the reported features $x_{t,i}$ are linearly realizable by some parameter $\\theta_{i}^{*}$ . ", "page_idx": 22}, {"type": "text", "text": "For any $\\delta\\in(0,1)$ let the confidence size be ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\beta_{t,i}=\\sqrt{d\\log\\left(\\frac{1+n_{t}(i)/\\lambda}{\\delta}\\right)}+\\sqrt{\\lambda}S,\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\|\\theta_{i}^{*}\\|_{2}\\,\\leq\\,S$ . Note that the typical expression also includes some constant $L$ such that $\\|x_{t,i}\\|_{2}\\leq L_{:}$ , which we here simply set to 1. With probability at least $1-\\delta$ it then holds that $\\theta_{i}^{*}\\in C_{t,i}$ . In what follows, we choose $\\delta={^1\\!/}T^{2}$ . ", "page_idx": 22}, {"type": "text", "text": "As a special case, when arm $i$ is always truthful so that $x_{t,i}=x_{t,i}^{*},$ , the true parameter $\\theta^{*}$ trivially satisfies (11) and the result reduces to the standard confidence bound statement $[I,24J$ restricted to observations from arm $i$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Let $\\theta_{i}^{*}$ satisfy (11). Then, note that $r_{t,i}:=\\,\\langle\\theta^{*},x_{t,i}^{*}\\rangle+\\eta_{t}=\\,\\langle\\theta_{i}^{*},x_{t,i}\\rangle+\\eta_{t}$ . Hence, the sequence of reported features $x_{t,i}$ and observed reward $\\boldsymbol{r}_{t,i}$ yield a standard linear contextual bandit structure with unknown parameter $\\theta_{i}^{*}$ (instead of $\\theta^{*}$ ). Then, to obtain the confidence bound follow the arguments from [1, 24], where we remark that we choose the confidence radius $\\beta_{t,i}$ arm specific. However, we could also choose a larger confidence radius such as $\\beta_{t}\\approx d\\log(t)$ or even constant $\\beta\\approx d\\log(T)$ . This will only have a negligible effect on the final regret. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "The grim trigger condition (4) of OptGTM stated that arm $i$ gets eliminated in round $t$ if ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\Big(\\langle\\widehat\\theta_{\\ell,i},x_{\\ell,i}\\rangle-\\sqrt{\\beta_{\\ell}}\\|x_{\\ell,i}\\|_{V_{\\ell,i}^{-1}}\\Big)>\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}r_{\\ell,i}+2\\sqrt{n_{t}(i)\\log(T)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Equivalently, $\\begin{array}{r}{\\sum_{\\ell\\leq t:\\;i_{\\ell}=i}\\mathrm{LCB}_{\\ell,i}(x_{\\ell,i})>\\mathrm{UCB}_{t}(\\hat{r}_{t,i}).}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "As a sanity check, we show that when an arm always reports truthfully, i.e., $x_{t,i}=x_{t,i}^{*}$ for all $t$ , it doesn\u2019t get eliminated with probability at least $1-1/T^{2}$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma E.2. When arm i always reports truthfully it does not get eliminated with high probability, that is, $i\\in A_{T}$ with probability at least $1-1/T^{2}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. We consider the event that the true parameter $\\theta^{*}$ is contained in $C_{t,i}$ , i.e., $\\mathcal{G}_{i}^{\\prime}\\;:=\\;\\{\\theta^{*}\\;\\in\\;\\mathfrak{~}$ $C_{t,i}\\,\\dot{\\forall}t\\in[T]\\}$ . The event $\\mathcal{G}_{i}^{\\prime}$ has probability at least $1-1/T^{2}$ according to Lemma E.1 when arm $i$ is truthful. Moreover, suppose that the reward observations concentrate as well, i.e., we assume the good event $\\mathcal{G}:=\\left\\{\\mathrm{LCB}_{t}\\bar{(}\\bar{\\hat{r}}_{t,i})\\leq\\hat{r}_{t,i}^{*}\\leq\\mathrm{UCB}_{t}(\\hat{r}_{t,i})\\,\\forall t\\in[T],i\\in[K]\\right\\}$ . Recall that $\\mathcal{G}$ has probability at least $1-1/T^{2}$ according to Hoeffding\u2019s inequality. A union bound then shows that the intersection of the two events has probability at least $1-{^2}/{T^{2}}$ . ", "page_idx": 23}, {"type": "text", "text": "Now, since arm $i$ is truthful, we have $x_{t,i}\\,=\\,x_{t,i}^{*}$ and $\\langle\\theta,x_{t,i}\\rangle\\,=\\,\\langle\\theta,x_{t,i}^{*}\\rangle$ for all $\\theta\\in\\mathbb{R}^{d}$ . Using Cauchy-Schwarz inequality and the fact that $\\theta^{*}\\in C_{t,i}$ , we get that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\langle\\hat{\\theta}_{t,i},x_{t,i}\\rangle-\\langle\\theta^{*},x_{t,i}^{*}\\rangle=\\langle\\hat{\\theta}_{t,i}-\\theta^{*},x_{t,i}^{*}\\rangle\\leq\\Vert\\hat{\\theta}_{t,i}-\\theta^{*}\\Vert_{V_{t,i}}\\Vert x_{t,i}^{*}\\Vert_{V_{t,i}^{-1}}\\leq\\sqrt{\\beta_{t,i}}\\Vert x_{t,i}\\Vert_{V_{t,i}^{-1}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Moreover, as we work on the good event $\\mathcal{G}$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\;i_{\\ell}=i}\\left(\\langle\\theta^{*},x_{\\ell,i}^{*}\\rangle-r_{\\ell,i}\\right)\\in[-2\\sqrt{n_{t}(i)\\log(T)},+2\\sqrt{n_{t}(i)\\log(T)}].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining these two statements yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\left(\\langle\\hat{\\theta}_{\\ell,i},x_{t,i}\\rangle-r_{\\ell,i}\\right)\\leq\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}\\sqrt{\\beta_{\\ell,i}}\\|x_{\\ell,i}\\|_{V_{\\ell,i}^{-1}}+2\\sqrt{n_{t}(i)\\log(T)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all $t\\in[T]$ . In other words, the grim trigger condition is never satisfied so that $i\\in A_{T}$ on event $\\mathcal{G}\\cap\\mathcal{G}_{i}^{\\prime}$ , which, as we saw, occurs with probability at least $1-1/T^{2}$ . \u53e3 ", "page_idx": 23}, {"type": "text", "text": "We now analyze the grim trigger of the OptGTM algorithm. As before, let $\\tau_{i}:=\\operatorname*{min}\\{t\\colon i\\notin A_{t}\\}$ with the convention that $\\tau_{i}=T$ if $i\\in A_{T}$ . The following lemma upper bounds the total amount of manipulation that an arm can exert before being eliminated by OptGTM\u2019s grim trigger elimination rule. ", "page_idx": 23}, {"type": "text", "text": "Lemma E.3. On the good event $\\mathcal{G}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t\\leq\\tau_{i}:\\ t_{t}=i}\\left(\\langle\\hat{\\theta}_{t,i},x_{t,i}\\rangle-\\langle\\theta^{*},x_{t,i}^{*}\\rangle\\right)\\leq\\sum_{\\ t\\leq\\tau_{i}:\\ t_{t}=i}\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}+4\\sqrt{n_{\\tau_{i}}(i)\\log(T)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Or equivalently, since $\\mathrm{UCB}_{t,i}(x_{t,i})=\\langle\\hat{\\theta}_{t,i},x_{t,i}\\rangle+\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t\\leq\\tau_{i}:\\ i_{t}=i}\\left(\\mathrm{UCB}_{t,i}\\left(x_{t,i}\\right)-\\langle\\theta^{*},x_{t,i}^{*}\\rangle\\right)\\leq\\sum_{\\substack{t\\leq\\tau_{i}:\\ i_{t}=i}}2\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}+4\\sqrt{n_{\\tau_{i}}(i)\\log(T)}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $t\\in[T]$ . On the good event $\\mathcal{G}:=\\left\\{\\mathrm{LCB}_{t}(\\hat{r}_{t,i})\\leq\\hat{r}_{t,i}^{*}\\leq\\mathrm{UCB}_{t}(\\hat{r}_{t,i})\\;\\forall t\\in[T],i\\in[K]\\right\\}$ and by definition of $\\mathrm{UCB}_{\\ell,i}(x_{\\ell,i}$ , it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\substack{\\xi\\leq t\\,:\\;i_{\\ell}=i}}\\big(\\langle\\widehat\\theta_{\\ell,i},x_{\\ell,i}\\rangle-r_{\\ell,i}\\big)}&{}\\\\ {\\displaystyle\\geq\\sum_{\\substack{\\ell\\leq t\\,:\\;i_{\\ell}=i}}\\big(\\mathrm{UCB}_{\\ell,i}(x_{\\ell,i})-\\langle\\theta^{*},x_{\\ell,i}^{*}\\rangle\\big)-\\displaystyle\\sum_{\\substack{\\ell\\leq t\\,:\\;i_{\\ell}=i}}\\sqrt{\\beta_{\\ell,i}}\\|x_{\\ell,i}\\|_{V_{\\ell-1,i}^{-1}}-2\\sqrt{n_{t}(i)\\log(T)}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence, if $\\begin{array}{r}{\\sum_{\\ell\\leq t:\\;i_{\\ell}=i}\\left(\\mathrm{UCB}_{\\ell,i}(x_{\\ell,i})-\\langle\\theta^{*},x_{\\ell,i}^{*}\\rangle\\right)>2R}\\end{array}$ , then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{\\ell\\leq t:\\;i_{\\ell}=i}\\left(\\langle\\hat{\\theta}_{\\ell,i},x_{\\ell,i}\\rangle-r_{\\ell,i}\\right)>\\sum_{\\ell\\leq t:\\;i_{t}=i}\\sqrt{\\beta_{\\ell,i}}\\|x_{\\ell,i}\\|_{V_{\\ell,i}^{-1}}+2\\sqrt{n_{t}(i)\\log(T)},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which means that arm $i$ must have been eliminated in a previous round or in round $t$ , i.e., $\\tau_{i}>t$ .   \nHence, for any $t\\leq\\tau_{i}$ , the the left hand side must be smaller or equal to the right hand side. ", "page_idx": 24}, {"type": "text", "text": "Interestingly, notice that we worked on the good event $\\mathcal{G}$ that only concerns the realization of the rewards and not the validity of the confidence set. This is important, since it is generally not true that the true parameter $\\theta^{*}$ is contained in the confidence set $C_{t,i}$ . ", "page_idx": 24}, {"type": "text", "text": "Lastly, before we begin with the proof Theorem 5.1 and Theorem 5.2, we establish a bound on the total exploration bonus, which after some additional work follows from the well-known elliptical potential lemma [1, 24]. ", "page_idx": 24}, {"type": "text", "text": "Lemma E.4. It holds that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t\\leq\\tau_{i}:\\;i_{t}=i}\\sqrt{\\beta_{t}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{n_{\\tau_{i}}(i)}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The constant on the right hand side can be derived from the choice of $\\beta_{t,i}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Before we can apply the elliptical potential lemma[1], we need to make sure that the exploration bonus does not blow up in early rounds. To this end, recall the definition of ", "page_idx": 24}, {"type": "equation", "text": "$$\nV_{t,i}:=\\lambda I+\\sum_{\\ell<t:\\ i_{\\ell}=i}x_{\\ell,i}x_{\\ell,i}^{\\top}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $\\begin{array}{r}{A=\\sum_{\\ell\\leq t:\\ i_{\\ell}=i}x_{\\ell,i}x_{\\ell,i}^{\\top}}\\end{array}$ . Note that $A$ is positive semi-definite so that $\\lambda I+A$ is positive definite and its inverse $(\\lambda I+A)^{-1}$ as well. The matrix inversion lemma let\u2019s us express this inverse as ", "page_idx": 24}, {"type": "equation", "text": "$$\n(\\lambda I+A)^{-1}=\\lambda I-(\\lambda I+A)^{-1}A.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, the eigenvalues of $B\\,=\\,(\\lambda I+A)^{-1}A$ are given by $\\lambda_{i}/(1+\\lambda_{i})$ , where $\\lambda_{i}~\\geq~0$ are the eigenvalues of $A$ , which means that $B$ is positive semi-definite. Consequently, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|x_{t,i}\\|_{V_{t,i}^{-1}}^{2}=x_{t,i}^{\\top}(\\lambda I+A)^{-1}x_{t,i}=x_{t,i}^{\\top}\\lambda x_{t,i}-x_{t,i}^{\\top}B x_{t,i}\\leq\\lambda\\|x_{t,i}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We assumed that $\\|{\\boldsymbol x}_{t,i}\\|_{2}^{2}\\leq1$ (similarly we could assume an upper bound $L$ ) so that $\\|x_{t,i}\\|_{V_{t,i}^{-1}}\\leq\\sqrt\\lambda$ . For convenience, we set the penalty factor to $\\lambda=1$ . We then apply Cauchy-Schwarz to get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t\\leq\\tau_{i}:\\ i_{t}=i}\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}\\leq\\sqrt{n_{\\tau_{i}}(i)\\beta_{T}\\sum_{t\\leq\\tau_{i}:\\ i_{t}=i}\\operatorname*{min}\\{1,\\|x_{t,i}\\|_{V_{t,i}^{-1}}^{2}\\}}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The elliptical potential lemma [1, 24] bounds the sum on the right hand side as ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{t\\leq\\tau_{i}:\\ t_{t}=i}\\operatorname*{min}\\{1,\\|x_{t,i}\\|_{V_{t,i}^{-1}}^{2}\\}\\leq2d\\log\\left(\\frac{d+n_{\\tau_{i}}(i)}{d}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, recall that we chose $\\beta_{t,i}=\\mathcal{O}\\left(d\\log\\left(n_{t}(i)\\right)\\right)$ , which then yields the claimed bound. ", "page_idx": 24}, {"type": "text", "text": "E.2 Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof of Theorem 5.1. We begin by proving that being truthful is an approximate Nash equilibrium under OptGTM. ", "page_idx": 24}, {"type": "text", "text": "Truthfulness is a $\\tilde{\\mathcal{O}}(d\\sqrt{K T})$ -NE\u221a. In a fist step, we show that if every arm is truthful, every arm is guaranteed at least $n_{T}^{*}(i)-\\tilde{\\mathcal{O}}(d\\sqrt{T})$ utility, where $\\begin{array}{r}{n_{T}^{*}(i):=\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i\\}}\\end{array}$ . To this end, we write ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{n_{T}(i)=\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i,i_{t}^{*}=i\\}+\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i,i_{t}^{*}\\neq i\\}}}\\\\ &{}&{\\geq\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i\\}-\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}\\neq i\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and our task will be bounding the sum on the right hand side. We focus on the event that $\\theta^{*}\\in C_{t,i}$ for all $(t,i)\\in[T]\\times[K]$ , which according to Lemma E.1 occurs with probability at least $1-1/T^{2}$ . Since $\\theta^{*}\\in C_{t,i}$ , we have $\\mathrm{UCB}_{t,i}(x_{t,i_{t}}^{*})\\geq\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle$ for every $i\\in[K]$ . Let $i_{t}^{*}=i$ but $i_{t}=j$ with $j\\ne i$ . Keeping in mind that $x_{t,i}=x_{t,i}^{*}$ for all $(t,\\dot{i})\\in[T]\\times[K]$ , since all arms are truthful, this implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{UCB}_{t,i}(x_{t,j}^{*})\\geq\\mathrm{UCB}_{t,i}(x_{t,i}^{*})\\geq\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "As a result, it holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{UCB}_{t,i}(x_{t,j}^{*})-\\langle\\theta^{*},x_{t,j}^{*}\\rangle\\geq\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,j}^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, since $\\theta^{*}\\in C_{t,i}$ and $\\widehat{\\theta}_{t,i}\\in C_{t,i}$ , we find that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{UCB}_{t,j}(x_{t,j}^{*})-\\langle\\theta^{*},x_{t,j}^{*}\\rangle\\leq\\langle\\hat{\\theta}_{t,j},x_{t,j}^{*}\\rangle-\\langle\\theta^{*},x_{t,j}^{*}\\rangle+\\sqrt{\\beta_{t,j}}\\|x_{t,j}\\|_{V_{t,j}^{-1}}}\\\\ &{\\phantom{\\mathrm{UCB}_{t,j}(x_{t,j}^{*})}\\leq\\|\\hat{\\theta}_{t,j}-\\theta^{*}\\|_{V_{t,j}}\\|x_{t,j}\\|_{V_{t,j}^{-1}}}\\\\ &{\\phantom{\\mathrm{UCB}_{t,j}(x_{t,j}^{*})}\\leq\\sqrt{\\beta_{t,j}}\\|x_{t,j}\\|_{V_{t,j}^{-1}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the second line follows from Cauchy-Schwarz inequality. Using Lemma E.4, this implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{t\\leq T:\\ i_{t}=j}\\mathrm{UCB}_{t,j}(x_{t,j}^{*})-\\langle\\theta^{*},x_{t,j}^{*}\\rangle\\leq\\sum_{t\\leq T:\\ i_{t}=j}\\sqrt{\\beta_{t,j}}\\|x_{t,j}\\|_{V_{t,j}^{-1}}}}\\\\ &{}&{\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{n_{\\tau_{j}}(j)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,j}^{*}\\rangle$ is constant for $i_{t}^{*}\\neq j$ , this means that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}=j\\}\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{n_{\\tau_{j}}(j)}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "so that by Jensen\u2019s inequality ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}\\neq i\\}=\\sum_{j\\neq i}\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}=j\\}\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In a second step, we show that for a\u221any deviating strategy $\\sigma_{i}$ that is not truthful, the utility of arm $i$ is upper bounded by $n_{T}^{*}(i)+\\tilde{\\mathcal{O}}(d\\sqrt{T})$ . In what follows, we work on the event that $j\\in A_{t}$ and $\\theta^{*}\\in C_{t,j}$ for all $t\\in[T]$ and $j\\neq i$ and recall that this event has probability at least $1-1/T^{2}$ since the arms are reporting truthfully (see Lemma E.2). Since $j\\in A_{T}$ for all $j\\neq i$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nn_{T}(i)\\leq\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i\\}+\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}=i,i_{t}^{*}\\neq i\\},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and we are tasked with bounding the sum on the right hand side appropriately. Now, similarly to before if $i_{t}=i$ and $i_{t}^{*}\\neq i$ (given $i_{t}^{*}\\in A_{t}$ ), then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{UCB}_{t,i}(x_{t,i})\\geq\\mathrm{UCB}_{t,i_{t}^{*}}(x_{t,i_{t}^{*}})\\geq\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used that $\\theta^{*}\\in C_{t,i_{t}^{*}}$ since the arm $i_{t}^{*}\\neq i$ is truthful. Consequently, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{UCB}_{t,i}(x_{t,i})-\\langle\\theta^{*},x_{t,i}^{*}\\rangle\\geq\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining Lemma E.3 and Lemma E.4 tells us that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t\\leq\\tau_{i}:\\ i_{t}=i}\\mathrm{UCB}_{t,i}(x_{t,i})-\\langle\\theta^{*},x_{t,i}^{*}\\rangle\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{T}\\right),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we coarsely upper bounded $n_{\\tau_{i}}(i)$ by $T$ . Since $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle$ for $i_{t}^{*}\\neq i$ is positive and constant, it follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{1}\\big\\{i_{t}=i,i_{t}^{*}\\neq i\\big\\}\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{T}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "In summary, we have shown that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\sigma^{*}}[n_{T}(i)]\\ge n_{T}^{*}(i)-\\tilde{\\mathcal{O}}\\left(d\\sqrt{K T}\\right)\\quad\\mathrm{~and~}\\quad\\mathbb{E}_{\\sigma_{i},\\sigma_{-i}^{*}}[n_{T}(i)]\\le n_{T}^{*}(i)+\\tilde{\\mathcal{O}}\\left(d\\sqrt{T}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for any deviating strategy $\\sigma_{i}$ . Hence, $\\sigma^{*}$ is a $\\tilde{\\mathcal{O}}(d\\sqrt{K T})$ -Nash equilibrium under OptGTM. ", "page_idx": 25}, {"type": "text", "text": "Regret analysis. Since we maintain estimates and confidence sets for each arm independently, it is natural to decompose the regret as ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle=\\sum_{i=1}^{K}\\sum_{t\\leq T:\\ i_{t}=i}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Note that w.h.p. no truthful arm gets eliminated and $\\theta^{*}\\in C_{t,i}$ for all $(t,i)\\in[T]\\times[K]$ . The regret analysis then proceeds similarly to that of LinUCB. ", "page_idx": 26}, {"type": "text", "text": "Since $\\theta^{*}\\in C_{t,i}$ , we know that for any round such that $i_{t}=i$ that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\right\\rangle\\leq\\mathrm{UCB}_{t,i_{t}^{*}}(x_{t,i_{t}^{*}}^{*})=\\mathrm{UCB}_{t,i_{t}^{*}}(x_{t,i_{t}^{*}})\\leq\\mathrm{UCB}_{t,i}(x_{t,i})=\\mathrm{UCB}_{t,i}(x_{t,i}^{*}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, again for any round with $i_{t}=i$ , applying Cauchy-Schwarz inequality yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle\\leq\\mathrm{UCB}_{t,i}(x_{t,i}^{*})-\\langle\\theta^{*},x_{t,i}^{*}\\rangle\\leq2\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Next, first using Cauchy-Schwarz inequality and the elliptical potential lemma (Lemma E.4), and then Jensen\u2019s inequality, it follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n2\\sum_{i=1}^{K}\\sum_{t\\leq T:\\ i_{t}=i}\\sqrt{\\beta_{t,i}}\\Vert x_{t,i}\\Vert_{V_{t,i}^{-1}}\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hence, connecting equations (13)-(15), we obtain $R_{T}(\\mathrm{OptGTM},\\sigma^{*})\\le\\tilde{\\mathcal{O}}(d\\sqrt{K T})$ . We see that the additional $\\sqrt{K}$ factor emerges due to OptGTM maintaining independent estimates for each arm. Usually a dependence on the action set size can be prevented since observations from one arm can be used for another arm as well. However, to prevent collusion in the strategic linear contextual bandit it is important to limit the influence an arm has on the selection (and elimination) of other arms. ", "page_idx": 26}, {"type": "text", "text": "E.3 Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof of Theorem 5.2. We begin the proof of Theorem 5.2 by decomposing the regret into two expression, which we then separately bound. ", "page_idx": 26}, {"type": "text", "text": "Decomposing strategic regret. We now decompose the regret of OptGTM into the rounds $t$ where the optimal arm in round $t$ is still active and the rounds where it is not. Like before, let $i_{t}^{*}:=\\operatorname*{argmax}_{i\\in[K]}\\langle\\theta^{*},x_{t,i}^{*}\\rangle$ be the optimal arm in round $t$ . For any $\\pmb{\\sigma}\\in\\mathrm{NE}(\\mathrm{OptGTM})$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\nR_{T}(\\sigma)=\\mathbb{E}_{\\sigma}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\in A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]+\\mathbb{E}_{\\sigma}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "With the help of Lemma E.3 and Lemma E.4 we now bound $J_{1}$ . ", "page_idx": 26}, {"type": "text", "text": "Lemma E.5 (Bounding $J_{1}$ ). ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\in A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]\\leq\\mathcal{O}\\left(d\\sqrt{K T}\\log(T)\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. By design of OptGTM, we we have $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle\\leq\\mathrm{UCB}_{t,i_{t}^{*}}(x_{t,i_{t}^{*}})\\leq\\mathrm{UCB}_{t,i_{t}}(x_{t,i_{t}})$ . Then, on the good event $\\mathcal{G}$ , Lemma E.3 yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\leq\\tau_{i}:\\ t_{t}=i}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle\\leq\\displaystyle\\sum_{t\\leq\\tau_{i}:\\ t_{t}=i}\\left(\\mathrm{UCB}_{t,i_{t}}(x_{t,i_{t}})-\\langle\\theta^{*},x_{t,i}^{*}\\rangle\\right)}\\\\ &{\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2\\left(2\\sqrt{n_{\\tau_{i}}(i)\\log(T)}+\\displaystyle\\sum_{t\\leq\\tau_{i}:\\ t_{t}=i}\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, on the good event, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\in A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle=\\displaystyle\\sum_{i=1}^{K}\\sum_{t=1}^{\\tau_{i}}\\mathbb{1}\\{i_{t}=i\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i}^{*}\\rangle}\\\\ &{\\displaystyle\\leq\\sum_{i=1}^{K}\\sum_{\\substack{t\\leq\\tau_{i}:\\,i_{t}=i}}2\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}+\\sum_{i=1}^{K}2\\sqrt{n_{\\tau_{i}}(i)\\log(T)}}\\\\ &{\\displaystyle\\leq\\sum_{i=1}^{K}\\mathcal{O}\\left(d\\log(T)\\sqrt{n_{\\tau_{i}}(i)}\\right)+\\sum_{i=1}^{K}2\\sqrt{n_{\\tau_{i}}(i)\\log(T)}}\\\\ &{\\displaystyle\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we applied Jensen\u2019s inequality in the last inequality and used that $\\begin{array}{r}{\\sum_{i=1}^{K}n_{\\tau_{i}}(i)\\le T}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma E.6 (Bounding $J_{2}$ ). ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]\\leq\\mathcal{O}\\left(d K^{2}\\sqrt{K T}\\log(T)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The proof idea is the same as the one for Lemma D.4, which was used to show the regret upper bound of the Greedy Grim Trigger Mechanism (Theorem 4.2, Appendix D). Recall that by assumption $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\leq1$ . We reuse Lemma D.7 from the proof of Theorem 4.2 to get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\right]\\leq\\mathbb{E}_{\\sigma}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}\\notin A_{t}\\}\\right]=\\sum_{i=1}^{K}\\mathbb{E}_{\\sigma}\\left[n_{T}^{*}(i)-n_{\\tau_{i}}^{*}(i)\\right],\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r}{n_{t}^{*}(i):=\\sum_{s=1}^{t}\\mathbb{1}\\{i_{s}^{*}=i\\}}\\end{array}$ is the number of rounds up to round $t$ that $i$ is the optimal. To bound the right hand s ide, we first prove a lower bound on $\\mathbb{E}_{\\pmb{\\sigma}}[n_{T}(i)]$ for any NE $\\pmb{\\sigma}\\in\\mathrm{NE}(\\mathrm{OptGTM})$ . ", "page_idx": 27}, {"type": "text", "text": "Lemma E.7. Let $\\pmb{\\sigma}\\in\\mathrm{NE}(\\mathrm{OptGTM})$ . It holds that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pmb{\\sigma}}\\left[n_{T}(i)\\right]\\ge n_{T}^{*}(i)-\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In particular, it holds that $\\mathbb{E}_{\\sigma}\\left[n_{t}(i)\\right]\\ge n_{t}^{*}(i)-\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right)f o r\\,t\\in[T].$ ", "page_idx": 27}, {"type": "text", "text": "Since $\\mathcal{G}$ occurs with probability $1-1/T^{2}$ and $n_{T}(i)\\le T$ by definition, on event $\\mathcal{G}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nn_{T}(i)\\ge n_{T}^{*}(i)-\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right)\\qquad n_{t}(i)\\ge n_{t}^{*}(i)-\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Given that arm $i$ always reports truthfully, i.e., $x_{t,i}=x_{t,i}^{*}$ for all $t$ , consider the event that $\\theta^{*}\\in C_{t,i}$ for all $t$ . Recall that this event has probability at least $1-1/T^{2}$ according to Lemma E.1. ", "page_idx": 27}, {"type": "text", "text": "We use the best response property of the Nash equilibrium by comparing against the truthful strategy. To this end, consider the strategy proflie $(\\sigma_{i}^{*},\\sigma_{-i})$ and the event that $\\theta^{*}\\in C_{t,i}$ for all $t$ as well as $\\mathcal{G}$ We then have that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{n_{T}(i)\\geq\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i\\}-\\sum_{t=1}^{T}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}\\neq i\\}}}\\\\ &{}&{=n_{T}^{*}(i)-{\\displaystyle\\sum_{j\\neq i}\\sum_{t=1}^{\\tau_{i}}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}=j\\}},\\ \\ \\ }\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the sum on the right hand side is the number of rounds where $i$ is optimal but OptGTM pulls another arm (because it has reported a larger optimistic value). ", "page_idx": 27}, {"type": "text", "text": "Next, recall that $\\theta^{*}\\in C_{t,i}$ so that for $i_{t}^{*}=i$ it follows that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{UCB}_{t,i}(x_{t,i})=\\mathrm{UCB}_{t,i}(x_{t,i}^{*})\\geq\\langle\\theta^{*},x_{t,i}^{*}\\rangle=\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Now, $i_{t}=j$ implies $\\mathrm{UCB}_{t,j}(x_{t,j})\\geq\\mathrm{UCB}_{t,i}(x_{t,i})\\geq\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}\\rangle$ , since $i\\in A_{t}$ for all $t$ and OptGTM selects the arm with maximal optimistic value. As a result, when $i_{t}^{*}=i$ and $i_{t}=j$ , we obtain that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{UCB}_{t,j}(x_{t,j})-\\langle\\theta^{*},x_{t,j}^{*}\\rangle\\geq\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,j}^{*}\\rangle.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Importantly, we have shown in Lemma E.3 that the total difference on the left hand side is bounded before elimination, i.e., before round $\\tau_{j}$ . As a consequence, we get ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{\\tau_{i}}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}=j\\}\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,j}^{*}\\rangle\\leq\\displaystyle\\sum_{t=1}^{\\tau_{i}}\\mathbb{1}\\{i_{t}^{*}=i,i_{t}=j\\}\\left(\\mathrm{UCB}_{t,j}(x_{t,j})-\\langle\\theta^{*},x_{t,j}^{*}\\rangle\\right)}\\\\ &{\\le\\displaystyle\\sum_{t=1}^{\\tau_{i}}\\mathbb{1}\\{i_{t}=j\\}\\left(\\mathrm{UCB}_{t,j}(x_{t,j})-\\langle\\theta^{*},x_{t,j}^{*}\\rangle\\right)}\\\\ &{\\leq2\\displaystyle\\sum_{t=1}^{\\tau_{j}}\\mathbb{1}\\{i_{t}=j\\}\\sqrt{\\beta_{t,j}}\\|x_{t,j}\\|_{V_{t,j}^{-1}}+4\\sqrt{n_{\\tau_{j}}(i)\\log(T)}}\\\\ &{\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{n_{\\tau_{j}}(j)}\\right)+4\\sqrt{n_{\\tau_{j}}(i)\\log(T)}}\\\\ &{\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{n_{\\tau_{j}}(j)}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we first used Lemma E.3 and then Lemma E.4. Recalling that $\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,j}^{*}\\rangle>0$ is constant for $j\\neq i_{t}^{*}$ by assumption of a constant optimality gap, it then follows from Jensen\u2019s inequality that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{j\\neq i}\\sum_{t=1}^{\\tau_{i}}\\mathbb{1}\\left\\{i_{t}^{*}=i,i_{t}=j\\right\\}\\leq\\sum_{j\\neq i}\\mathcal{O}\\left(d\\log(T)\\sqrt{n_{\\tau_{j}}(j)}\\right)\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used that $\\begin{array}{r}{\\sum_{j\\neq i}n_{\\tau_{j}}(j)\\le T}\\end{array}$ . Hence, equation (17) yields ", "page_idx": 28}, {"type": "equation", "text": "$$\nn_{T}(i)\\geq n_{T}^{*}(i)-\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $\\sigma_{i}$ must be a best response to $\\sigma_{-i}$ , we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\sigma}[n_{T}(i)]\\ge\\mathbb{E}_{\\sigma_{i}^{*},\\sigma_{-i}}[n_{T}(i)]\\ge n_{T}^{*}(i)-{\\mathcal O}\\left(d\\log(T)\\sqrt{K T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "To obtain the result for $t\\,\\in\\,[T]1$ , suppose that on the good event $\\mathcal{G}$ the contrary is true so that $n_{t}(i)<n_{t}^{*}(i)\\!-\\!\\omega(d\\log(T)\\sqrt{K T})$ . However, similarly to before, Lemma E.3 tells us that the number of rounds th\u221aat are \u201cpoached\u201d from arm $i$ , i.e., $i_{t}^{*}=i$ and $i_{t}\\neq i$ , is upper bounded from ab\u221aove by $\\mathcal{O}(d\\log(T)\\sqrt{K t})$ . Hence, since $\\mathrm{UCB}_{t,i}(x_{t,i})\\geq\\langle\\theta^{*},x_{t,i_{t}}^{*}\\rangle$ and $n_{t}(i)\\leq n_{t}^{*}(i)-\\omega(d\\log(T)\\sqrt{K T})$ , it must hold that $\\tau_{i}~<t$ . Then, on the good event $\\mathcal{G}$ , it follows that $n_{\\tau_{i}}(i)\\,\\leq\\,n_{t}(i)\\,<\\,n_{t}^{*}(i)\\,-$ $\\omega(d\\log(T)\\sqrt{K T})$ . Since event $\\mathcal{G}$ has probability at least $1-1/T^{2}$ , this implies ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}[n_{T}(i)]=\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}(i)]\\le n_{t}^{*}(i)-\\omega\\big(d\\log(T)\\sqrt{K T}\\big).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "This contradicts the lower bound of $\\mathbb{E}_{\\sigma}[n_{T}(i)]\\ge n_{T}^{*}(i)-{\\mathcal O}(d\\log(T)\\sqrt{K T}).$ ", "page_idx": 28}, {"type": "text", "text": "Lemma E.8. Let $\\pmb{\\sigma}\\in\\mathrm{NE}(\\mathrm{OptGTM})$ . Then, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\sigma}[n_{T}(i)]\\le\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}^{*}(i)]-\\mathcal{O}\\left(d\\log(T)K\\sqrt{K T}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the expectation on the right hand side is taken w.r.t. $\\tau_{i}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. We have $\\begin{array}{r}{\\sum_{i=1}^{K}n_{\\tau}^{*}(i)=\\sum_{i=1}^{K}\\sum_{t=1}^{\\tau}\\mathbb{1}\\{i_{t}^{*}=i\\}=\\tau}\\end{array}$ for any $\\tau\\in[T]$ . Using Lemma E.7, we obtain ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}_{\\sigma}[n_{T}(i)]=\\mathbb{E}_{\\sigma}[n_{\\tau}(i)]}\\\\ &{\\quad=\\mathbb{E}_{\\sigma}\\left[\\sum_{t=1}^{\\tau_{t}}\\mathbb{I}\\{i_{t}=i\\}\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\sigma}\\left[\\frac{\\sum_{t=1}^{\\tau_{t}}(1-1)\\,[i_{t}\\,\\neq\\,i])}{\\sum_{t=1}^{\\tau_{t}}(1-1)\\,\\cdot\\,[i_{t}\\,\\neq\\,i])}\\right]}\\\\ &{\\quad=\\mathbb{E}_{\\sigma}\\left[\\tau_{i}\\right]-\\sum_{j\\neq i}\\mathbb{E}_{\\sigma}\\left[n_{\\tau_{i}}(j)\\right]}\\\\ &{\\quad\\le\\mathbb{E}_{\\sigma}[\\tau_{i}]-\\sum_{j\\neq i}\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}^{*}(j)]+{\\mathcal O}\\left(d\\log(T)K\\sqrt{K T}\\right)}\\\\ &{\\quad=\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}^{*}(i)]+{\\mathcal O}\\left(d\\log(T)K\\sqrt{K T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Combing the lower and upper bounds on each arm\u2019s utility of Lemma E.7 and Lemma E.8, we get ", "page_idx": 29}, {"type": "equation", "text": "$$\nn_{T}^{*}(i)-\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right)\\leq\\mathbb{E}_{\\sigma}[n_{T}(i)]\\leq\\mathbb{E}_{\\sigma}[n_{\\tau_{i}}^{*}(i)]-\\mathcal{O}\\left(d\\log(T)K\\sqrt{K T}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "It then follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\sigma}[n_{T}^{*}(i)-n_{\\tau_{i}}^{*}(i)]\\le\\mathcal{O}\\left(d\\log(T)K\\sqrt{K T}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "so that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{k=1}^{K}\\mathbb{E}_{\\pmb{\\sigma}}\\left[n_{T}^{*}(i)-n_{\\tau_{i}}^{*}(i)\\right]\\leq\\mathcal{O}\\left(d\\log(T)K^{2}\\sqrt{K T}\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This concludes the proof of Lemma E.6. ", "page_idx": 29}, {"type": "text", "text": "Finally, recalling the regret decomposition from the beginning of the proof and using Lemma E.5 and Lemma E.6, we obtain for any ${\\pmb\\sigma}\\in\\mathrm{NE}({\\pmb\\sigma})$ that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{R_{T}(\\mathrm{GGTM},\\sigma)\\leq\\mathcal{O}\\left(\\underbrace{d\\log(T)\\sqrt{K T}}_{\\mathrm{Lemma~E.}5}+\\underbrace{d\\log(T)K^{2}\\sqrt{K T}}_{\\mathrm{Lemma~E.}6}\\right)\\leq\\tilde{\\mathcal{O}}\\left(d K^{2}\\sqrt{K T}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "E.4 Linear Realizability of Reported Contexts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In the following, we comment on an interesting observation in the strategic linear contextual bandit that may also provide some insight into the effectiveness of OptGTM. Suppose that each arm reports its contexts in a linearly realizable fashion (without us explicitly incentivizing them to do so). Formally, we can express this as the following assumption ", "page_idx": 29}, {"type": "text", "text": "Assumption 3 (Linear Realizability of Reported Contexts). Every arm reports so that its reports follow some linear reward model. That is, for every arm $i\\in[K]$ , there exists a vector $\\theta_{i}^{*}\\in\\mathbb{R}^{d}$ such that for all $t\\in[T]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\langle\\theta^{*},x_{t,i}^{*}\\rangle=\\langle\\theta_{i}^{*},x_{t,i}\\rangle.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Perhaps surprisingly, the regret analysis of OptGTM becomes straightforward when \u221athe arms\u2019 strategies satisfy Assumption 3. Moreover, we can prove that OptGTM suffers $\\tilde{\\mathcal{O}}(d\\sqrt{K T})$ strategic regret in every Nash equilibrium of the arms. That is, the regret guarantee is better than that of Theorem 5.2. ", "page_idx": 29}, {"type": "text", "text": "A quick regret analysis. Let $\\pmb{\\sigma}$ be any NE under OptGTM. When we observe a reward $\\boldsymbol{r}_{t,i}$ after pulling arm $i$ in round $t$ , we can interpret the reward as $r_{t,i}:=\\,\\langle\\theta^{*},x_{t,i}^{*}\\rangle+\\eta_{t}=\\,\\langle\\theta_{i}^{*},x_{t,i}\\rangle+\\eta_{t}$ . Hence, isolating arm $i$ , the learner is essentially playing a linear contextual bandit with true unknown parameter $\\theta_{i}^{*}$ , contexts $x_{t,i}$ , and rewards $r_{t,i}=\\langle\\theta_{i}^{*},x_{t,i}\\rangle+\\eta_{t}$ . As a result, the independent estimators $\\widehat{\\theta}_{t,i}$ for every arm $i$ , are in fact estimating $\\theta_{i}^{*}$ and, according to Lemma E.1, with high probability $\\theta_{i}^{*}\\in C_{t,i}$ . It is then also easy to see that OptGTM will never eliminate any of the arms with high probability. Now, since $\\theta_{i}^{*}\\in C_{t,i}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\langle\\theta^{*},x_{t,i}^{*}\\rangle=\\langle\\theta_{i}^{*},x_{t,i}\\rangle\\leq\\mathrm{UCB}_{t,i}(x_{t,i}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "As a result, using Cauchy Schwarz inequality, we obtain ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{UCB}_{t,i}(x_{t,i})-\\langle\\theta^{*},x_{t,i}^{*}\\rangle\\leq\\langle\\hat{\\theta}_{t,i}-\\theta_{i}^{*},x_{t,i}\\rangle+\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\|\\hat{\\theta}_{t,i}-\\theta_{i}^{*}\\|_{V_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}+\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}\\leq2\\sqrt{\\beta_{t,i}}\\|x_{t,i}\\|_{V_{t,i}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In every round $t$ , OptGTM selects the arm with maximal optimistic reported value $\\mathrm{UCB}_{t,i}(x_{t,i})$ so that $\\mathrm{UCB}_{t,i_{t}^{*}}(\\boldsymbol{x}_{t,i_{t}^{*}})-\\mathrm{UCB}_{t,i_{t}}(\\boldsymbol{x}_{t,i_{t}})\\le0$ . We can then bound the instantaneous regret in round $t$ as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\theta^{*},x_{t,i_{t}^{*}}^{*}-x_{t,i_{t}}^{*}\\rangle\\leq\\mathrm{UCB}_{t,i_{t}^{*}}(x_{t,i_{t}^{*}})-\\langle\\theta^{*},x_{t,i_{t}}^{*}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\mathrm{UCB}_{t,i_{t}^{*}}(x_{t,i_{t}^{*}})-\\mathrm{UCB}_{t,i_{t}}(x_{t,i_{t}})+2\\sqrt{\\beta_{t,i_{t}}}\\|x_{t,i_{t}}\\|_{V_{t,i_{t}}^{-1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\sqrt{\\beta_{t,i_{t}}}\\|x_{t,i_{t}}\\|_{V_{t,i_{t}}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Using the elliptical potential lemma and Jensen\u2019s inequality (Lemma E.4, [1, 24]), the total regret of OptGTM is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\nR_{T}(\\mathrm{OptGTM},\\sigma)\\leq\\sum_{i=1}^{K}\\sum_{\\substack{t\\leq T\\colon i_{t}=i}}2\\sqrt{\\beta_{t,i}}\\Vert x_{t,i}\\Vert_{V_{t,i}^{-1}}\\leq\\mathcal{O}\\left(d\\log(T)\\sqrt{K T}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We have thus shown the following guarantee. ", "page_idx": 30}, {"type": "text", "text": "Corollary E.9. Suppose that Assumption 3 holds. Then, ", "page_idx": 30}, {"type": "equation", "text": "$$\nR_{T}(\\mathrm{OptGTM},\\sigma)=\\tilde{\\mathcal{O}}\\left(d\\sqrt{K T}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for every Nash equilibrium $\\pmb{\\sigma}\\in\\mathrm{NE}(\\mathrm{OptGTM})$ . ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: In the abstract, we list the main claims of this paper in a general fashion. Then, in the introduction we state them in more detail. They accurately reflect the paper\u2019s contribution and scope. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We discuss the limitations and assumptions of our work throughout the paper.   \nAdditional limitations are highlighted in the discussion. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 31}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The assumptions can be found in Section 3 and in the paragraphs before the theorems. We provide proof sketches in the main text and complete proofs in the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We do not see any potential negative social impact of this work. On the contrary, this work aims to improve the fairness and equity of online platforms by discouraging harmful gaming behavior in response to recommendation algorithms. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This work addresses the problem of aligning agent incentives with that of learning algorithms. While there are some potential positive societal impacts of this theoretical work, we do not believe that it is necessary to highlight them here. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 35}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]