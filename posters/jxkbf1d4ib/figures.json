[{"figure_path": "JXKbf1d4ib/figures/figures_2_1.jpg", "caption": "Figure 1: (a) The density of a distribution v (grey), and its categorical projection \u03a0m\u03bd \u2208 P({z1,..., zm}) (blue). (b) A categorical distribution (blue); its update after being scaled by \u03b3 and shifted by r by the distributional Bellman operator T, moving its support off the grid {z1,..., zm} (pink); the resulting realigned distribution supported on the grid {z1,..., zm} after projection via \u03a0m (green). (c) Hat functions hi (solid) and hm (dashed).", "description": "This figure illustrates the categorical dynamic programming approach to distributional reinforcement learning.  Panel (a) shows how a continuous distribution is projected onto a discrete set of points, representing a categorical distribution. Panel (b) demonstrates the distributional Bellman update, which shifts and scales the categorical distribution, causing its support to fall outside the original grid.  A projection step is then required to map the updated distribution back onto the grid, as shown in the figure. Panel (c) shows the hat functions used to perform this projection.", "section": "2.3 Categorical dynamic programming"}, {"figure_path": "JXKbf1d4ib/figures/figures_8_1.jpg", "caption": "Figure 2: Left: Example MRP with r(x0) = 1,r(x1) = 0, y = 0.9. Right: Categorical fixed point F* (x0) with m = 15, and 5 independent samples from the random CDF \u03a6*(x0).", "description": "The left panel shows a simple Markov Reward Process with two states (x0 and x1) and a discount factor of 0.9. The reward is 1 for state x0 and 0 for state x1. The right panel shows the CDF of the categorical fixed point (blue line), generated by solving the equation: F = TpF (Equation (10) in the paper), and 5 independent samples from the stochastic categorical CDF Bellman equation (Equation (16) in the paper; grey dashed lines).  This figure illustrates the concept of the stochastic categorical CDF Bellman equation, showing that the CDF generated from the fixed-point equation represents the expected value of the stochastic equation.", "section": "5.2 The stochastic categorical CDF Bellman equation"}, {"figure_path": "JXKbf1d4ib/figures/figures_9_1.jpg", "caption": "Figure 3: Approximation error/wallclock time for a variety of distributional RL methods, discount factors, numbers of atoms, and numbers of environment samples.", "description": "This figure compares the performance of several distributional reinforcement learning methods (DCFP, QDP, CDP) across various settings.  It shows the approximation error (Wasserstein distance) and wall-clock time required to achieve this error as a function of the number of samples per state.  The results are presented for different discount factors (\u03b3),  numbers of atoms (m) used to approximate the distribution, and number of environment samples. The figure helps to understand the trade-offs between accuracy, computational cost, and sample efficiency for each algorithm under different conditions.", "section": "6 Empirical evaluation"}, {"figure_path": "JXKbf1d4ib/figures/figures_35_1.jpg", "caption": "Figure 4: Monte Carlo approximations of return distributions in each of the four environments tested.", "description": "This figure shows the Monte Carlo approximations of the return distributions for the four environments in the paper: Chain, Low random, High random, and Two-state. Each subplot represents a different environment and shows the distribution of returns for each state in that environment.  The distributions are estimated using Monte Carlo sampling, and show the variability of returns in each environment.", "section": "6 Empirical evaluation"}, {"figure_path": "JXKbf1d4ib/figures/figures_37_1.jpg", "caption": "Figure 1: (a) The density of a distribution v (grey), and its categorical projection Im\u03bd \u2208 P({z1,..., zm}) (blue). (b) A categorical distribution (blue); its update after being scaled by \u03b3 and shifted by r by the distributional Bellman operator T, moving its support off the grid {z1,..., zm} (pink); the resulting realigned distribution supported on the grid {z1,..., zm} after projection via Im (green). (c) Hat functions hi (solid) and hm (dashed).", "description": "This figure illustrates the categorical dynamic programming approach to distributional reinforcement learning. Panel (a) shows how a continuous distribution is projected onto a discrete set of atoms. Panel (b) shows the effect of the Bellman operator on a categorical distribution and how it is subsequently projected back onto the discrete set of atoms. Finally, Panel (c) shows the hat functions used to perform the projection.", "section": "2.3 Categorical dynamic programming"}, {"figure_path": "JXKbf1d4ib/figures/figures_38_1.jpg", "caption": "Figure 3: Approximation error/wallclock time for a variety of distributional RL methods, discount factors, numbers of atoms, and numbers of environment samples.", "description": "This figure compares the performance of various distributional reinforcement learning algorithms (DCFP, QDP, CDP) across different experimental conditions.  The x-axis represents either the number of samples used or the wall-clock time.  The y-axis represents the approximation error (Wasserstein distance).  The plots show how the algorithms perform under different discount factors (\u03b3), numbers of atoms (m\u2014used in categorical representations), and numbers of environment samples. This allows for analysis of the trade-off between accuracy and computational cost for each algorithm.", "section": "6 Empirical evaluation"}, {"figure_path": "JXKbf1d4ib/figures/figures_39_1.jpg", "caption": "Figure 3: Approximation error/wallclock time for a variety of distributional RL methods, discount factors, numbers of atoms, and numbers of environment samples.", "description": "This figure compares the performance of several distributional reinforcement learning algorithms (DCFP, QDP, CDP) in terms of approximation error and computation time.  The results are shown for different discount factors (\u03b3), numbers of atoms (m), and numbers of environment samples (N).  The figure helps illustrate the trade-off between accuracy and computational cost for different algorithms and parameter settings, offering insights into the practical considerations for choosing an appropriate approach and configuration for specific use cases.", "section": "6 Empirical evaluation"}, {"figure_path": "JXKbf1d4ib/figures/figures_40_1.jpg", "caption": "Figure 3: Approximation error/wallclock time for a variety of distributional RL methods, discount factors, numbers of atoms, and numbers of environment samples.", "description": "This figure compares the performance of several distributional reinforcement learning algorithms (DCFP, QDP, CDP) under different experimental conditions.  The x-axis represents either the number of samples used to estimate the transition probabilities or the wall-clock time, while the y-axis shows the maximum Wasserstein-1 distance between the estimated return distribution and the true distribution (estimated via Monte Carlo).  Different lines represent different algorithms and numbers of atoms used for approximation. The results demonstrate that DCFP and QDP show better performance at high discount factors than CDP, with DCFP demonstrating faster execution than QDP.  The impact of environmental stochasticity on algorithm performance is also shown.", "section": "6 Empirical evaluation"}, {"figure_path": "JXKbf1d4ib/figures/figures_41_1.jpg", "caption": "Figure 3: Approximation error/wallclock time for a variety of distributional RL methods, discount factors, numbers of atoms, and numbers of environment samples.", "description": "This figure compares the performance of several distributional reinforcement learning algorithms in terms of approximation error and wall-clock time.  The algorithms include DCFP, QDP, and CDP, with variations in the number of atoms (representing the granularity of the distribution approximation) used. The experiment varies discount factors and numbers of environment samples to assess performance under different conditions.  The results demonstrate a trade-off between accuracy and computational cost.  Different algorithms perform better under different conditions, highlighting the importance of algorithm choice based on the specifics of the environment and task.", "section": "6 Empirical evaluation"}, {"figure_path": "JXKbf1d4ib/figures/figures_42_1.jpg", "caption": "Figure 10: Supremum-Wasserstein distance on convergence. Error envelope indicates 95% confidence interval by bootstrapping. The return range of the two-state environment coincides with the global return range.", "description": "This figure compares the performance of DCFP and QDP algorithms across various settings, such as different discount factors (\u03b3) and number of atoms (m).  It shows how the supremum Wasserstein distance between the estimated return distribution and the true return distribution changes as the number of samples used to estimate the transition matrix (N) increases. The results are presented for four different environments: Chain, Low random, High random, and Two-state, highlighting the impact of environment stochasticity on algorithm performance. The figure also shows results for two atom support settings: using the global return range and using environment-specific return range.", "section": "6 Empirical evaluation"}]