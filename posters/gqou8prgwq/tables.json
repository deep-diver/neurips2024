[{"figure_path": "Gqou8PRgWq/tables/tables_3_1.jpg", "caption": "Table 1: We apply DSIR [50] to compile a high-quality dataset (10k instances), a random dataset (10k instances) from MMLU, and a mixed dataset samples 5k instances from each of the high-quality and random datasets. We fine-tune the LLaMA-7B model [3] on the curated dataset and evaluate them using the MMLU test set.", "description": "This table presents the performance of the LLaMA-7B model fine-tuned on three different datasets created from the MMLU dataset: a high-quality dataset selected using DSIR, a random dataset, and a mixed dataset combining samples from the high-quality and random datasets. The performance is measured using the MMLU test set, showing the accuracy achieved by each dataset.", "section": "3.1 Preliminary"}, {"figure_path": "Gqou8PRgWq/tables/tables_5_1.jpg", "caption": "Table 2: Performance comparison of curated datasets of the same size by SHED and baseline methods.", "description": "This table presents a comparison of the performance of datasets curated using different methods, including SHED (with Quality-Ordered Cluster Sampling (QOCS) and Quality-Weighted Cluster Sampling (QWCS)), Random Sampling (RS), Dataset Quantization (DQ), and Data Selection with Importance Resampling (DSIR).  The performance is evaluated on two tasks: MMLU and ARC-challenge, using two original datasets: MMLU and WizardLM.  The goal is to demonstrate SHED's superiority in creating high-quality, smaller datasets compared to existing methods.  The table shows the performance achieved when using the same number of samples from the various curated datasets for fine-tuning.", "section": "4.2 Experiment Results"}, {"figure_path": "Gqou8PRgWq/tables/tables_5_2.jpg", "caption": "Table 3: Performance of the best-selected datasets of SHED and baseline methods on the MMLU task.", "description": "This table compares the performance of the best-performing dataset selected by SHED and several baseline methods (RS, DQ, DSIR, and using the full dataset) on the MMLU benchmark.  The best-selected dataset for each method is the one that achieved the highest performance across various sample sizes tested for that particular method. The table shows the accuracy achieved by each method's best-performing dataset on the MMLU task, along with the size of that dataset in parentheses.", "section": "4.2 Experiment Results"}, {"figure_path": "Gqou8PRgWq/tables/tables_6_1.jpg", "caption": "Table 4: Performance of the best-selected datasets of SHED and baselines on the ARC-challenge task.", "description": "This table presents a comparison of the performance achieved by SHED (with QOCS and QWCS sampling methods), and three baseline methods (RS, DQ, DSIR) on the ARC-challenge task.  The performance metric is not explicitly defined but is presumed to be accuracy, given the context. The results are shown for both the MMLU and WizardLM datasets. For each method, the size of the best-performing curated dataset (in number of instances) is indicated in parentheses.  The \"Full\" row shows the performance achieved when training on the full original dataset, providing a baseline for comparison.", "section": "4.2 Experiment Results"}, {"figure_path": "Gqou8PRgWq/tables/tables_6_2.jpg", "caption": "Table 5: MT-Bench evaluation of the best-selected datasets of SHED and baselines.", "description": "This table presents the results of evaluating the best-performing datasets selected by SHED and baseline methods using MT-Bench. MT-Bench assesses the human preference for the model's generated responses, evaluating aspects such as quality and alignment with human expectations.  The table shows the scores achieved by different models (LLaMA-7B) and datasets (full dataset, randomly sampled datasets, and datasets curated by SHED's QOCS and QWCS methods) on both MMLU and WizardLM tasks.  Lower scores indicate better alignment with human preference.", "section": "4.2 Experiment Results"}, {"figure_path": "Gqou8PRgWq/tables/tables_7_1.jpg", "caption": "Table 6: Transferability evaluation using the best-selected datasets across different models on MMLU task.", "description": "This table presents the performance of the best-performing datasets selected by SHED and baseline methods (RS, QOCS, QWCS) across three different LLMs (LLaMA-13B, VICUNA-7B, and GPT-2) on the MMLU task.  It demonstrates the transferability of the datasets curated by SHED, showing consistent performance improvements across various model sizes and families.  The numbers represent the accuracy achieved by each method on the MMLU task, and the dataset size is indicated in parentheses.", "section": "4.2 Experiment Results"}, {"figure_path": "Gqou8PRgWq/tables/tables_7_2.jpg", "caption": "Table 6: Transferability evaluation using the best-selected datasets across different models on MMLU task.", "description": "This table presents the performance of the best-selected datasets (curated by SHED and baseline methods) when fine-tuned on different LLMs (LLaMA-13B, VICUNA-7B, and GPT-2) for the MMLU task.  It demonstrates the transferability of the datasets curated by SHED, showing consistent performance across various LLMs. The table shows the performance of the full dataset and datasets with various sizes selected by different methods. The best performance is indicated in bold.", "section": "4.2 Experiment Results"}, {"figure_path": "Gqou8PRgWq/tables/tables_14_1.jpg", "caption": "Table 8: Performance comparison between SHED and LIMA.", "description": "This table compares the performance of SHED's selected dataset (with 1k samples) against LIMA's selected dataset (also with 1k samples) on the MMLU and ARC-challenge tasks using LLaMA-7B as the base model.  It highlights that SHED, despite being fully automated, achieves comparable or better performance than the manually curated LIMA dataset.", "section": "D Comparison with Other Methods"}, {"figure_path": "Gqou8PRgWq/tables/tables_14_2.jpg", "caption": "Table 4: Performance of the best-selected datasets of SHED and baselines on the ARC-challenge task.", "description": "This table presents a comparison of the performance of the best-performing datasets selected by SHED and baseline methods on the ARC-challenge task.  It shows the performance (likely accuracy scores) achieved by SHED-QOCS, SHED-QWCS, RS (Random Sampling), DQ (Dataset Quantization), DSIR (Data Selection with Importance Resampling), and the original full dataset, across different dataset sizes for both the MMLU and WizardLM datasets.", "section": "4.2 Experiment Results"}]