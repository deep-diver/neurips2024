[{"type": "text", "text": "Fundamental Limits of Prompt Compression: A RateDistortion Framework for Black-Box Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alliot Nagle\u2217 Adway Girish\u2217 Marco Bondaschi Michael Gastpar UT Austin EPFL EPFL EPFL ", "page_idx": 0}, {"type": "text", "text": "Ashok Vardhan Makkuva\u2020 Hyeji Kim\u2020 EPFL UT Austin ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We formalize the problem of prompt compression for large language models (LLMs) and present a framework to unify token-level prompt compression methods which create hard prompts for black-box models. We derive the distortion-rate function for this setup as a linear program, and provide an efficient algorithm to compute this fundamental limit via the dual of the linear program. Using the distortion-rate function as the baseline, we study the performance of existing compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain, natural language queries, and their respective answers. Our empirical analysis demonstrates the criticality of query-aware prompt compression, where the compressor has knowledge of the downstream task/query for the blackbox LLM. We show that there is a large gap between the performance of current prompt compression methods and the optimal strategy, and propose Adaptive QuerySelect, a query-aware, variable-rate adaptation of a prior work to close the gap. We extend our experiments to a small natural language dataset to further confirm our findings on our synthetic dataset. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In spite of the recent success of transformer-based [1] large language models (LLMs) in language modeling tasks, inference calls to a transformer can be costly in both time and memory usage. Although significant progress has been made to improve the memory usage and runtime efficiency via implementation-level optimizations [2, 3, 4] and architecture-level optimizations and alternatives [5, 6, 7], a third type of optimization that compresses the input (an input-level optimization) has the benefit that it directly reduces the resource usage of an LLM inference call, and it can be used in conjunction with the other two types of optimizations for further efficiency gains. In this work, we offer a framework and analysis for a recent body of literature in this direction, known as prompt compression [8, 9, 10]. ", "page_idx": 0}, {"type": "text", "text": "The goal of a prompt compression method is to transform a sequence of input tokens $x$ into a shorter sequence of tokens $m$ such that the response generated by a target LLM will semantically mean the same thing regardless of whether $x$ or $m$ is given as input. Using $m$ as the input directly decreases the memory and runtime requirements necessary for an LLM inference call. Moreover, the additional benefits to this approach are: (1) redundant or superfluous tokens are removed, making room to fit more pertinent information in the target LLM\u2019s limited-size context window, (2) it can be used in addition to implementation and architecture-level optimizations to get further efficiency gains, and (3) it is the only technique available when seeking to lower costs for black-box API calls to closed-source ", "page_idx": 0}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/6332a1e36ddd1cdccd7a89b4a90dc5705cbd886ac44da3515b4c83d856a3d153.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: The distortion-rate trade-off of all prompt compression methods compared to the queryaware and query-agnostic theoretical limits on a synthetic dataset with binary prompts. All distortions are computed with the log loss (left) and 0/1 loss (right) distortion metrics formally defined in (1). We observe that (1) most existing methods are far from the theoretical limit, suggesting that there is still room for improvement in this field, (2) conditioning on the query allows for a significant improvement, as seen by the performance of the query-aware method QuerySelect against the queryagnostic LLMLingua-2 [14], and (3) our proposed method Adaptive QuerySelect, a query-aware and variable-rate adaptation of LLMLingua-2, achieves the best performance among all methods considered, and is the only method to outperform the optimal query-agnostic strategy. ", "page_idx": 1}, {"type": "text", "text": "models. This third point is particularly important, since the associated cost for a black-box API model inference call, from the perspective of the caller, is determined by the runtime and the number of input tokens, both which can be reduced with prompt compression. In our framework and analysis, we focus on the prompt compression for black-box models setting, where the output of a prompt compression method is a set of tokens (\u201chard prompts\u201d) [11, 12, 13, 14], and exclude methods which output embedding vectors (\u201csoft prompts\u201d) [15, 16, 17] as those are not transferable to black-box models. ", "page_idx": 1}, {"type": "text", "text": "Despite the progress in the prompt compression literature, there is a lack of proper formalization of this problem and there is no clear framework to unify these works. Most works propose methods that work well but offer no insight into key questions, such as \u201cHow far are we from the theoretical limit of the rate-distortion trade-off?\u201d, \u201cHow essential is the conditioning on the query when compressing the prompt?\u201d, and \u201cHow does tokenization impact the performance of prompt compression methods?\u201d We offer a unifying framework for the problem of prompt compression and seek to answer these questions with theory and experiments. Our main contributions can be summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "1. Theoretical analysis: We formalize the problem of prompt compression and formulate it as a rate-distortion problem (Sec. 3.1). We characterize the optimal trade-off between the rate of compression and the distortion incurred, i.e., the distortion-rate function, via a dual linear program, and provide a geometric algorithm to compute this optimal trade-off (Sec. 3.2, Sec. 3.3).   \n2. Evaluation: We introduce a synthetic dataset with binary prompts and natural language queries, for which we can compute the distortion-rate function (Sec. 4.1), and compare and obtain insights on existing prompt compression algorithms as in Fig. 1 (Sec. 4.2). We further confirm our findings by extending our experiments to a small natural language dataset and NarrativeQA [18].   \n3. Algorithm design: Our novel method, \u201cAdaptive QuerySelect,\u201d a query-aware, variable-rate adaptation of LLMLingua-2 [14], outperforms all prompt compression methods on our datasets and has a rate-distortion curve that significantly reduces the gap with the theoretical limit (Sec. 4). ", "page_idx": 1}, {"type": "text", "text": "2 Background and related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Long prompts slow the inference process due to the increase in the number of tokens for the LLM to process. It is also known that very long prompts can cause LLMs to \u201cforget\u201d parts of the input and produce erroneous answers [19]. Therefore, studying how these prompts can be compressed is essential. As shown in Fig. 2, we wish to design a compressor that, upon receiving the prompt, produces a \u201ccompressed\u201d version (which has fewer tokens than the prompt) called the compressed ", "page_idx": 1}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/a6d2daa762120020251936106ce32562ee2ebde91f144a3fdf0cce5d3adf55a7.jpg", "img_caption": ["(a) Black-box LLM without prompt compression. "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/9288131b55e2e77e481690f130e97d13d18011f67a12c796ffce5301e7e5da05.jpg", "img_caption": ["(c) Query-aware prompt compression "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: Model for prompt compression in LLMs. (a): Without prompt compression, the LLM takes a long Prompt and Query as input, and produces an Output distribution. (b) and (c): The prompt is passed through a compressor to obtain a shorter Compressed prompt and the LLM takes this compressed prompt and query as input instead. (b) The compressor does not have access to the query, and preserves all highlighted tokens. (c) The compressor has access to the query, and preserves only the tokens highlighted in orange. ", "page_idx": 2}, {"type": "text", "text": "prompt, such that a target LLM is able to give answers that are \u201cclose enough,\u201d per some appropriately chosen metric, to the ground truth. Though similar in spirit to text summarization, prompt compression has the advantage that the compressed prompt is not required to be human-readable. ", "page_idx": 2}, {"type": "text", "text": "All prompt compression methods belong to one of two groups: those that compress the prompt into soft prompts and those that compress the prompt into hard prompts. In soft-prompt compression, the compressor is trained to transform the input prompt into a set of embedding vectors (sometimes referred to as \u201csoft tokens\u201d) that do not map back into the token space. These methods, including Gist Tokens [15], AutoCompressor [17], and In-Context Auto-Encoder [16] are trained end-to-end and require specialized fine-tuning of the target LLM to interpret the soft prompt inputs. ", "page_idx": 2}, {"type": "text", "text": "In this work, we focus instead on methods that compress the prompt into hard prompts, where the compressor\u2019s output is a set of tokens. While it is technically feasible to fine-tune the target LLM in this setting, it is unnecessary and often avoided because the utility of this setting is compressing prompts for black-box models that are not fine-tuned. These methods often use either the target LLM, or a smaller and faster LLM, to compress the prompt. The basic idea behind all these methods is to identify the tokens that are \u201cmost relevant,\u201d per an appropriate metric, and retain as many of them in the compressed prompt as possible. These methods include Selective Context [11], LLMLingua [12], LLMLingua-2 [14], and LongLLMLingua [13]. More details on these works can be found in Sec. 4.2. Precursors to the prompt compression works include text compression methods, which have the added constraint that the compressed text is human-readable [20, 21, 22]. Prompt compression methods are different from these in that the text only needs to be interpretable by the target LLM, not by a human. ", "page_idx": 2}, {"type": "text", "text": "We offer a framework for hard-prompt compression methods where we assume that a query is provided in addition to the compressed prompt during the target LLM inference call. Functionally, this is the most useful interpretation of prompt compression since it clarifies that the goal is to compress the prompt for a given query/task. This setting is also used in the LLMLingua and LongLLMLingua works, and is more general than the setting where no query is used (the query can then be empty). ", "page_idx": 2}, {"type": "text", "text": "3 Distortion-rate function for prompt compression ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We first formalize the problem of prompt compression, and then develop a rate-distortion framework to study its fundamental limits. In particular, we define and characterize the distortion-rate function, which describes the optimal trade-off between how much and how well the prompt is compressed. A complete overview of the notation can be found in App. A. ", "page_idx": 2}, {"type": "text", "text": "Black-box LLM. As depicted in Fig. 2a, we assume that we have a pretrained LLM which takes a pair of the prompt $x\\in\\mathcal{V}^{n_{x}}$ and the query $q\\in\\mathcal{V}^{n_{q}}$ , $(x,q)\\in\\mathcal{V}^{n_{x}+n_{q}}$ as inputs, where $\\mathcal{V}$ refers to the vocabulary of the LLM (i.e., the set of all tokens), and $n_{x}$ and $n_{q}$ are the lengths of the prompt and query respectively. The output of the LLM is given by $\\mathsf{P}_{\\hat{Y}}=\\phi_{\\mathsf{L L M}}(x,q)$ , where $\\phi_{\\mathsf{L L M}}:\\mathcal{V}^{*}\\to\\mathcal{P}(\\mathcal{V}^{*})$ is a deterministic function which maps a sequence of tokens to a probability distribution on sequences of tokens. We denote the set of all prompts $x$ by $\\mathcal{X}$ and the set of all queries $q$ by $\\mathcal{Q}$ . Clearly, they are both equal to $\\mathcal{V}^{*}$ , but this notation is useful in the subsequent discussion. We model prompt-query pairs $(X,Q)$ as random variables drawn according to the joint distribution $\\mathsf{P}_{X Q}\\in\\mathcal{P}(\\mathcal{X}\\times\\mathcal{Q})$ . ", "page_idx": 3}, {"type": "text", "text": "In cases where we have a correct answer $y\\in\\mathcal{Y}=\\mathcal{V}^{*}$ corresponding to the pair $(x,q)$ , we characterize the \u201ccloseness\u201d of the LLM output $\\mathsf{P}_{\\hat{Y}}\\,=\\,\\phi_{\\mathsf{L L M}}(x,q)$ to the answer $y$ using a distortion measure ${\\mathsf{d}}:{\\mathcal{V}}\\!\\times{\\mathcal{P}}({\\mathcal{V}})\\to[0,\\infty]$ . Two possible choices of d are the log loss ${\\sf d}_{\\mathrm{log}}$ and the $0/1$ loss ${\\sf d}_{0/1}$ , given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathsf{d}_{\\log}(y,\\mathsf{P}_{\\hat{Y}})=\\log\\frac{1}{\\mathsf{P}_{\\hat{Y}}(y)}\\quad\\mathrm{~and~}\\quad\\mathsf{d}_{0/1}(y,\\mathsf{P}_{\\hat{Y}})=\\mathbb{1}\\Big\\{y\\neq\\underset{\\hat{y}}{\\operatorname{argmax}}\\,\\mathsf{P}_{\\hat{Y}}(\\hat{y})\\Big\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "These are respectively the cross-entropy loss between the distributions $\\delta_{y}$ and $\\mathsf{P}_{\\hat{Y}}$ , and the prediction error. When dealing with natural language queries, a semantic distortion metric such as RougeL [23] or BertScore [24] is more appropriate. Additionally, there is no single answer that is uniquely correct. To account for this variability in what qualifies as a correct answer, we model the answer as a random variable $Y$ drawn from the distribution $\\mathsf{P}_{Y|X Q}(\\cdot|x,q)$ , which depends on the prompt $x$ and query $q$ . This induces the joint distribution $\\mathsf{P}_{X Q Y}=\\mathsf{P}_{X Q}\\mathsf{P}_{Y|X Q}$ . We characterize the \u201ccloseness\u201d between the correct answer and the LLM output by the average distortion, given by $\\mathbb{E}_{Y\\sim\\mathsf{P}_{Y|X Q}(\\cdot|x,q)}\\left[\\mathsf{d}\\!\\left(Y,\\phi_{\\mathsf{L L M}}(x,q)\\right)\\right]$ . With ${\\mathsf{d}}={\\mathsf{d}}_{\\mathrm{log}}$ , this is the cross-entropy loss between $\\mathsf{P}_{Y|X Q}(\\cdot|x,q)$ and $\\mathsf{P}_{\\hat{Y}}=\\phi_{\\mathsf{L L M}}(x,q)$ , and with ${\\mathsf{d}}={\\mathsf{d}}_{0/1}$ , this is the prediction error probability. ", "page_idx": 3}, {"type": "text", "text": "Prompt compression. As described in Sec. 2, we consider two types of prompt compression: query-agnostic and query-aware. Fig. 2b depicts the query-agnostic version, where the goal is to design a compressor denoted by comp as a possibly random function from $\\mathcal{X}$ to $\\mathcal{M}$ , i.e., the set of all compressed prompts. The compressor takes in the prompt $X\\sim\\mathsf{P}_{X}$ and produces a compressed prompt $M=\\mathsf{c o m p}(X)$ with $\\mathrm{len}(M)\\leq\\mathrm{len}(X)$ . Then, the user replaces $X$ with the compressed prompt $M$ and provides the LLM with the input $(M,Q)$ , resulting in the output distribution $\\mathsf{P}_{\\hat{Y}}=$ $\\phi_{\\mathsf{L L M}}(M,Q)$ . To quantify the performance of this compressor comp, two quantities are of interest: ", "page_idx": 3}, {"type": "text", "text": "(1) the rate $\\mathbb{E}\\left[{\\frac{\\log(M)}{\\log(X)}}\\right]$ , to measure how much the prompt is compressed, and (2) the distortion $\\mathbb{E}\\left[\\mathsf{d}\\big(Y,\\phi_{\\mathsf{L L M}}(M,Q)\\big)\\right]$ , to measure how well the prompt is compressed, with both expectations taken with respect to (w.r.t.) $\\mathsf{P}_{M X Q Y}$ . If we compress $x$ to a low rate, the compressed prompt $m$ may not retain the information in $x$ that is necessary for the query $q$ , leading to an output $\\phi_{\\mathsf{L L M}}(m,q)$ that is different from $\\mathsf{P}_{Y|X Q}(\\cdot|x,q)$ and hence, a high distortion. Thus, there is a trade-off between these quantities, which we formalize as the distortion-rate function in Sec. 3.2. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "We can also model query-aware prompt compression similarly, with the difference being that the compressor also has access to the query $q\\in\\mathcal{Q}$ , as shown in Fig. 2c. In addition to the average rate and distortion computed over all queries, it is also interesting to consider the rate and distortion for each query. To simplify the presentation, we restrict our discussion here to the query-agnostic setting, and only briefly mention the analogous definitions and results for the query-aware setting. A complete development of the query-aware setting can be found in App. B. ", "page_idx": 3}, {"type": "text", "text": "3.2 Rate-distortion formulation for prompt compression ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Distortion-rate function $D^{*}(R)$ . The distortion-rate function for any compression problem characterizes the fundamental trade-off between the distortion and the rate [25, 24, 26, 27]. We say that the pair $(R,D)$ is achievable if there exists a compressor with rate at most $R$ and distortion at most $D$ . For a given rate $R$ , the distortion-rate function $D^{*}(R)$ is the smallest distortion that can be achieved by a compressor with rate at most $R$ . Formally, it is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D^{*}(R)\\triangleq\\operatorname*{inf}\\{D\\ge0\\mid(R,D)\\mathrm{~is~achievable}\\}}\\\\ &{\\qquad\\quad=\\operatorname*{inf}\\{D\\ge0\\mid\\operatorname{there~exists~a~compressor~with~rate}\\le R\\mathrm{~and~distortion}\\le D\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$D^{*}(R)$ for query-agnostic prompt compression. Recall that our quantities of interest are the rate $\\mathbb{E}\\left[{\\frac{\\log(M)}{\\log(X)}}\\right]$ , and the distortion $\\mathbb{E}\\left[\\mathsf{d}\\big(Y,\\phi_{\\mathsf{L L M}}(M,Q)\\big)\\right]$ , with both expectations taken w.r.t. $\\mathsf{P}_{M X Q Y}$ , where $\\bar{M}\\,\\bar{=}\\,{\\mathsf{c o m p}}(X)$ for a random function comp. By the functional representation lemma [27, 28], a random function from $\\mathcal{X}$ to $\\mathcal{M}$ is equivalent to a conditional distribution $\\mathsf{P}_{M|X}$ . Thus, we can equivalently model the compressor as a conditional distribution $\\mathsf{P}_{M|X}$ , and (2) is explicitly written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D^{*}(R)=}&{{}\\underset{\\mathsf{P}_{M|X}}{\\operatorname*{inf}}\\quad\\mathbb{E}\\left[\\mathsf{d}\\!\\left(Y,\\phi_{\\mathsf{L L M}}(M,Q)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\frac{\\log(M)}{\\log(X)}}\\right]\\leq R,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with both expectations taken w.r.t. the joint distribution $\\mathsf{P}_{M X Q Y}=\\mathsf{P}_{M|X}\\mathsf{P}_{X Q Y}$ induced by the compressor $\\mathsf{P}_{M|X}$ . The constraint ${}^{\\bullet\\bullet}\\mathsf{P}_{M|X}$ is a compressor\u201d is short for the following requirements: (1) it is a conditional distribution, i.e., for each $\\begin{array}{r}{x\\in\\mathcal{X},\\sum_{m\\in\\mathcal{M}}\\mathsf{P}_{M|X}(m|x)=1}\\end{array}$ , (2) if $\\mathrm{len}(m)>\\mathrm{len}(x)$ , then $\\mathsf{P}_{M|X}(m|x)=0$ , and (3) if $\\mathrm{len}(m)\\,=\\,\\mathrm{len}(x)$ , then $\\mathsf{P}_{M|X}\\big(m|x\\big)=0$ unless $m\\,=\\,x$ . This means that the compressor either strictly reduces the length of the prompt or does no compression and retains the original prompt. ", "page_idx": 4}, {"type": "text", "text": "Note that all of the expressions in the objective and the constraints in (3) are linear in $\\mathsf{P}_{M|X}$ . Hence, the optimization problem is simply a linear program (LP), which is simple from an optimization perspective [29, 30]. However, the dimensions of this problem are still large and solving the LP directly quickly becomes infeasible as the lengths of the prompts increase. In Sec. 3.3, we deal with this optimization problem directly, and show that the dual of the LP provides an exact, practically realizable solution. ", "page_idx": 4}, {"type": "text", "text": "The extension to the query-aware setting is straightforward; we then have query-dependent (or conditional) distortion-rate functions $D_{q}^{*}(R)$ for each $q\\in\\mathcal{Q}$ , and an average distortion-rate function, denoted by $\\bar{D}^{*}(R)$ . Refer to App. B for an explicit characterization of $D_{q}^{*}(R)$ and $\\bar{D}^{*}(R)$ . ", "page_idx": 4}, {"type": "text", "text": "Connections to information-theoretic setups. We provide a brief overview of rate-distortion theory from the information theory literature in App. D and describe how our model compares. In particular, we note that our model for prompt compression closely resembles the setup of compression with side-information for function computation [31, 32, 33, 34], where both the encoder and the decoder are part of the system design. More recently, there has also been a growing interest in computing the distortion-rate functions of these classical setups for real-world datasets [35, 36, 37]. However, in our model for prompt compression, only the encoder (which is the compressor) can be designed, hence our model is one of compression for a fixed decoder. Such a model has not been actively studied in the information theory literature before, but in the next subsection, we show that the distortion-rate function can be written as an explicit LP in terms of this fixed decoder. ", "page_idx": 4}, {"type": "text", "text": "3.3 Linear program formulation of the distortion-rate function ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Having expressed the distortion-rate function for prompt compression as an LP, we now look to solve this LP. We first rewrite (3) as an explicit LP using optimization-theoretic notation, and hide the probabilistic notation involving expectations and conditional probabilities in the parameters of the LP. Refer to App. A for an overview of the notation. ", "page_idx": 4}, {"type": "text", "text": "Proposition 1 (Primal LP). The distortion-rate function for query-agnostic prompt compression (3) is given by the solution to the linear program ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D^{*}(R)=}&{\\ \\underset{\\left(z_{x}\\in\\mathbb{R}_{+}^{M_{x}}\\right)_{x\\in\\mathcal{X}}}{\\operatorname*{inf}}\\ \\sum_{x\\in\\mathcal{X}}D_{x}^{\\top}z_{x}}\\\\ &{\\quad\\quad\\quad\\ s.t.\\quad\\quad\\displaystyle\\sum_{x\\in\\mathcal{X}}R_{x}^{\\top}z_{x}\\leq R,\\quad\\mathbf{1}^{\\top}z_{x}=1,\\quad\\forall\\,x\\in\\mathcal{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where for each $x\\in\\mathscr{X}$ , $\\mathcal{M}_{x}$ denotes the set of compressed prompts associated to $x$ , i.e., the set of all possible token sequences of length smaller than $\\operatorname{len}(x)$ , the vectors $z_{x}\\in\\mathbb{R}_{+}^{M_{x}}$ are the optimization ", "page_idx": 4}, {"type": "text", "text": "variables and the constants $D_{x},R_{x}\\in\\mathbb{R}_{+}^{\\mathcal{M}_{x}}$ with components indexed by $m\\in\\mathcal{M}_{x}$ are given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{x,m}\\triangleq\\mathsf{P}_{X}(x)\\,\\mathbb{E}\\left[\\mathsf{d}(Y,\\phi_{\\mathrm{LLM}}(m,Q))\\right]\\quad a n d\\quad R_{x,m}\\triangleq\\mathsf{P}_{X}(x)\\,\\displaystyle\\frac{\\mathrm{len}(m)}{\\mathrm{len}(x)},\\qquad m\\in\\mathcal{M}_{x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with the expectation taken with respect to $\\mathsf{P}_{Q Y|M X}(\\cdot,\\cdot|m,x)$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. This follows immediately from (3) by defining the constants $D_{x},R_{x}\\ \\in\\ \\mathbb{R}_{+}^{\\mathcal{M}_{x}}$ for each $x\\in\\mathscr{X}$ as given in (4), and taking $z_{x}$ to be $\\mathsf{P}_{M|X}(\\cdot|x)$ . We use the fact that $\\mathsf{P}_{M|X}(m|\\dot{x})=0$ when $\\mathrm{len}(m)>\\mathrm{len}(x)$ to reduce the dimension of $z_{x}$ from $\\mathcal{M}$ to $\\mathcal{M}_{x}$ to obtain (LP). \u53e3 ", "page_idx": 5}, {"type": "text", "text": "For our experimental setup in Sec. 4.2, we see that dimension of the LP is too large to solve (LP) directly using off-the-shelf solvers. Fortunately, the dual of the LP can be written more concisely, and can also be solved using a relatively simple algorithm. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Dual LP). The distortion-rate function for query-agnostic prompt compression (3) is given by the solution to the dual of the linear program (LP), i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\nD^{*}(R)=\\operatorname*{sup}_{\\lambda\\geq0}\\left\\{-\\lambda R+\\sum_{x\\in\\mathcal{X}}\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}\\left[D_{x,m}+\\lambda R_{x,m}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof sketch. This follows by taking the dual [29] of the LP (LP) and simplifying the resulting expression. For a complete proof, refer to App. C.1. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Algorithm to solve (dual-LP). While the optimization problem in (dual-LP) seems difficult to solve, with its max-min structure and the supremum over a continuous variable, it provides a neat geometric interpretation which allows for a computationally simple algorithm given in Algorithm 1. It takes as input $R$ , $(D_{x})_{x\\in\\mathcal{X}}$ , $(R_{x})_{x\\in\\mathcal{X}}$ , and returns as output the distortion-rate function at $R$ , i.e., $D^{*}(R)$ . In App. C.2, we prove that the output is indeed $D^{*}(R)$ , and provide a step-bystep illustration of the algorithm on an artificial example. A high-level description of the algorithm is given below. ", "page_idx": 5}, {"type": "text", "text": "Before presenting the algorithm, it is useful to define the following geometric object. The lower-left convex envelope of a set of points in $\\mathbb{R}_{+}^{2}$ is the largest convex function that lies below and to the left of the points, as shown in Fig. 3 for the points $\\left\\{(R_{x,m},D_{x,m})\\right\\}_{m\\in\\mathring{\\mathcal{M}}_{x}}$ for a fixed $x\\in\\mathscr{X}$ . Let $k_{x}$ be the number of points on this envelope, then these $k_{x}$ points are exactly the minimizers of $\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}$ $[D_{x,m}+\\lambda R_{x,m}]$ for some $\\lambda\\geq0$ . Solving this inner minimization problem of (dual-LP) is thus easy, and amounts to simply finding the points labelled as $\\cdot_{m}(x)\\cdot$ on the lower-left convex envelope, ordered from left to right, as done in Lines 3\u20134 of Algorithm 1. Let the magnitudes of slopes of the line segments on the envelope be given by the \u201c $\\cdot\\chi(x)^{,}$ \u201d terms in decreasing order (Lines 5\u20136). Also letting $\\bar{\\lambda}_{0}^{(x)}=+\\infty$ and $\\bar{\\lambda_{k_{x}}^{(x)}}^{-}=0$ , observe that for $i=1,\\ldots,k_{x}$ , $m_{i}^{(x)}$ minimizes $D_{x,m}+\\lambda R_{x,m}$ over $m$ for $\\lambda\\in\\left[\\lambda_{i}^{(x)},\\lambda_{i-1}^{(x)}\\right]$ . Importantly, it is enough to consider just these sequences $\\cdot\\vert m^{(x)}$ \u201d and \u201c $\\lambda^{(x)}{}^{,}$ \u201d sequences computed for all $x\\in\\mathscr{X}$ (Lines 2\u20136) to solve (dual-LP), instead of the entire set $\\mathcal{M}_{x}$ and the continuum of all positive real numbers $\\lambda$ respectively. This makes the problem tractable even for large values of $|\\mathcal{M}_{x}|$ . ", "page_idx": 5}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/e79fe6db08dd19942cb1ff040a6f4d0e1756956e7da53ea781ac17c384722fe8.jpg", "img_caption": ["Figure 3: Lower-left convex envelope for an example with $\\left|\\mathcal{M}_{x}\\right|=$ 11, $k_{x}=3$ . "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "The rest of the algorithm computes the outer supremum. Lines 7\u20139 prepare for this by introducing new notation $\\;^{\\ldots}\\widetilde{m}^{(x)}$ \u201d and \u201c $\\widetilde{\\lambda}^{,}$ such that for $\\lambda\\in[\\widetilde{\\lambda}_{j},\\widetilde{\\lambda}_{j-1})$ , $\\widetilde{m}_{j}^{(x)}$ minimizes $D_{x,m}+\\lambda R_{x,m}$ over $m\\in\\mathcal{M}_{x}$ . Ther  e is no calcu lation involved in this s te p ; what  w e gain is that the range of $\\lambda$ on which the minimizer ism(jx) no longer depends on $x$ . This gives us everything we need to compute the distortion-rate fu n ction, which is obtained by lines 10\u201313. Observe that the input $R$ is only used for lines 10\u201313, so for a given dataset, lines 1\u20139 can be run once and the results $\\;^{\\ldots}\\widetilde m^{(x)}\\!\\}$ \u201d and \u201c $\\widetilde{\\lambda}^{,,}$ stored, with only lines 10\u201313 run for each value of $R$ . ", "page_idx": 5}, {"type": "text", "text": "We derive a similar dual LP formulation of the query-aware distortion-rate functions in App. B. In fact, we see that both the conditional and average distortion-rate functions are of the same form as (dual-LP), with different parameters. Hence, Algorithm 1 can compute all of the distortion-rate functions that we have defined, namely the query-agnostic and query-aware (conditional and average) distortion-rate functions. ", "page_idx": 5}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/ebb43839304bbf708b38c3a916a2e1ef089981a41385a56ead6d1b52cde48ff9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The distortion-rate function defined in Sec. 3 describes the best possible trade-off between the achievable values of rate and distortion in the query-aware and query-agnostic cases. In this section, we compare the performance of existing prompt compression methods (that are compatible with the black-box model setting we consider here) with the optimal curve for a synthetic dataset. We observe that there is a sizeable gap between the performance of existing methods and the optimal curve. We propose Adaptive QuerySelect, a query-aware and variable-rate adaptation of LLMLingua-2 [14], that outperforms the existing methods on this synthetic dataset. We also consider a query-aware version of LLMLingua-2 called QuerySelect and observe that it outperforms the query-agnostic version, which highlights the importance of conditioning on the query. ", "page_idx": 6}, {"type": "text", "text": "We include an ablation study on the impact of tokenization of the prompt compression problem, as tokenization is lossy since it groups together multiple symbols into a single symbol before passing it to an LLM. We study the effect of tokenization on the prompt compression problem by forcing the tokenizer on the encoder and decoder side to tokenize the bits of the binary string prompts in our dataset individually, which we refer to as \u201cforced tokenization.\u201d We run experiments in this setting and with the regular \u201cstandard tokenization.\u201d Additional details on our experiments can be found in App. F. Our code is made available for reproducibility purposes.2 ", "page_idx": 6}, {"type": "text", "text": "4.1 Experimental setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset. In order to run experiments that are computationally tractable but still meaningful to the prompt compression problem, we construct a synthetic dataset $\\{(x_{i},q_{i},y_{i})\\}_{i=1}^{N}$ with (1) prompts $x_{i}$ being sequences from $\\mathcal{V}=\\{0,1\\}$ , i.e., binary prompts, (2) natural language queries $q_{i}$ , such as \u201cCount the number of 1s,\u201d \u201cCompute the parity,\u201d and \u201cIs the binary string a palindrome?\u201d and (3) their associated answers $y_{i}$ . In total, we construct a dataset of seven queries; a complete specification of the dataset, including a few examples is available in App. F.2.1. The binary prompts are generated from a first-order Markov chain on $\\{0,1\\}$ with a 0.1 probability of transitioning and a 0.9 probability of remaining in the same state, and the minimum and maximum possible lengths for each prompt are four and ten, respectively. All methods are evaluated on a validation set of 1400 examples in total (7 queries, 200 examples per query). The optimal distortion-rate function is computed using Algorithm 1, taking $\\mathsf{P}_{X Q Y}$ to be the empirical distribution on the dataset, i.e., $\\begin{array}{r}{\\mathsf{P}_{X Q Y}=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{(x_{i},q_{i},y_{i})}}\\end{array}$ . This is the natural choice when the true distribution is unknown. Another choice is a parametric model with parameters learned from a dataset, but it is unclear what is an appropriate model in this case. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "We also run experiments on a small natural language dataset curated with GPT-4 [38] and NarrativeQA [18] for a large-scale experiment. The small dataset consists of ten prompts with four queries each, and a few examples are provided in Table 3 in App. F.2.1. More details on the considerations made in constructing our datasets are provided in App. E. ", "page_idx": 7}, {"type": "text", "text": "Baseline methods. We compare the rate-distortion trade-off of the optimal strategy (both queryaware and query-agnostic) with prompt compression methods that can be used to compress prompts for a black-box target LLM: Selective Context [11], LLMLingua [12], LLMLingua Query [13], LLMLingua-2 [14]. As such, we do not consider methods like Gist Tokens [15], In-Context Autoencoder [16], and AutoCompressor [17] since they require special training methods generally not compatible with black-box target LLMs. Selective Context uses $-\\log\\mathsf{P}(x_{i}^{\\bar{\\ i}}\\mid x_{0},x_{1},\\bar{\\ldots},x_{i-1})$ to score the $i$ -th token, and retains the tokens whose score is larger than the $p$ -percentile, where $p\\in[0,1]$ is the ratio parameter. LLMLingua uses a similar method, but they first partition the input prompt into segments and condition on previously compressed segments to compress the current segment. They later extended their method to perform query-aware compression, which is what we use for LLMLingua Query. While these methods use a decoder-style (causal) transformer LLM to do prompt compression, this approach makes an independence assumption on the influence of future tokens have on the $i$ -th token. LLMLingua-2 instead uses an encoder-style (bidirectional) LLM to perform a token classification task, where their model predicts whether a given token should be kept or removed. ", "page_idx": 7}, {"type": "text", "text": "Our proposed methods. We add two novel contributions over the LLMLingua-2 work: (1) we adapt LLMLingua-2 to the query-aware setting, whereas the original work only proposed the queryagnostic approach, which we call \u201cQuerySelect,\u201d and (2) we further adapt this query-aware approach into a variable-rate approach we refer to as \u201cAdaptive QuerySelect.\u201d This approach lets the encoder model decide which tokens to keep based on the confidence over a specified threshold. In other words, LLMLingua-2 and QuerySelect accept a rate parameter to determine the compression ratio, but Adaptive QuerySelect replaces the rate parameter with a threshold parameter. The encoder model predicts the probability of keeping a particular token, and the token is kept if the predicted probability is above the threshold, resulting in a variable-rate compression of the prompt. Variablerate compression is important as some prompts are more compressible than others, and vice versa. ", "page_idx": 7}, {"type": "text", "text": "Models. We use Mistral 7B Instruct $\\mathrm{v}0.2$ [39] as our black-box target LLM, which is fine-tuned on the training set partition of our synthetic dataset. This model is fixed after fine-tuning and no prompt compression methods have access to any part of it. All prompt compression methods use an LLM as part of their compression algorithm; we use deduplicated Pythia 1B [40] for Selective Context, LLMLingua, and LLMLingua Query and RoBERTa Base [41] for LLMLingua-2-based methods. For each method, we finetune on the training set partition to enable the best performance possible for that method. More information on how we trained these methods and the data we used is in App. F. For all models, including the target LLM, we fine-tune with LoRA [42] and conduct a hyperparameter grid search. We choose the configuration with the best performance on a test set that is different from the validation set. More details on the hyperparameter search are provided in App. F.3. ", "page_idx": 7}, {"type": "text", "text": "For the natural language dataset, no fine-tuning is necessary on the decoder side. On the encoder side, Selective Context, LLMLingua, and LLMLingua Query use the same model as on the decoder side, and LLMLingua-2 uses a specially fine-tuned version of XLM RoBERTa Large [43, 14]. We use a custom fine-tuned XLM RoBERTa Large model as the encoder for the QuerySelect and Adaptive QuerySelect methods. The training dataset of (prompt, query, answer) tuples used to train this custom model is filtered from the Databricks Dolly $15\\mathbf{k}$ [44] dataset to only include examples with prompt lengths between a specified minimum and maximum length (see Sec. F.3.2 for details). ", "page_idx": 7}, {"type": "text", "text": "4.2 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Fig. 1 summarizes our experimental contributions on the synthetic dataset. We observe a large gap between the optimal curve and existing prompt compression methods. Thus, we propose QuerySelect as a query-aware and Adaptive QuerySelect as a query-aware, variable-rate modification of LLMLingua-2 to close this gap. Our results show that Adaptive QuerySelect achieves the best performance and, in fact, is the only method to outperform the optimal query-agnostic strategy. We also note that the optimal distortion-rate curves eventually fall below the baseline performance of using the full prompt (no compression). This observation is especially interesting because it shows that compressing prompts can improve performance on downstream tasks, as observed on natural language datasets in previous prompt compression works [12, 13, 14]. We accredit the performance of Adaptive QuerySelect to variable-rate compression, where we allow the compressor to choose how much it should compress based on the query and prompt as input (see App. B, Remark 1 for a formal explanation of variable-rate compression). Even though this approach relinquishes explicit control over the rate, our experiments show that variable-rate compression is the closest to optimality. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/0c16131e69cbe3ebeda33a2677e598653f5b16e4f6c53bb419db8332d36d5774.jpg", "img_caption": ["Figure 4: We highlight the distortion-rate curves for two of the seven queries in the validation partition of our synthetic dataset. Our method, Adaptive QuerySelect, is able to match the performance of the optimal query-aware strategy (left). Some queries naturally incur less distortion than others with the target LLM, even with a query-agnostic approach, if the query is aligned well with the data generation process for the prompt (right). Note that QuerySelect covers the line of LLMLingua-2 as their performance is identical for this query. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Gap from optimality depends on the query. In Fig. 4, we highlight the distortion-rate curves for two out of seven of the queries in our synthetic dataset. Despite the fact that Fig. 1 shows a gap in average performance between the query-aware optimal strategy and Adaptive QuerySelect, Fig. 4 (left) shows that Adaptive QuerySelect can match the performance of the optimal query-aware compression scheme. Comparing Fig. 4 (left) and (right), we see that the prompt compression problem is easier (methods are closer to optimality) for certain tasks or queries depending on how the prompts were generated. For our synthetic dataset, all prompts are generated from a Markov chain with a transition probability of 0.1 and a probability of 0.9 for remaining in the same state. This means the tokens with the highest entropy are those that are part of a transition, and those tokens are the most important for answering this query. As a result, we see that methods that use the negative log-likelihood as a means for compression (Selective Context, LLMLingua, and LLMLingua Query) perform well, even without conditioning on the query. An exception here is the performance of LLMLingua Query, which we find has mixed performance compared to vanilla LLMLingua for tokenlevel prompt compression on our dataset. Please refer to Fig. 11 in App. F for results on all queries. ", "page_idx": 8}, {"type": "text", "text": "Effect of tokenization. Finally, the results of our ablation study on the effects of tokenization are provided in Fig. 10 in App. F. Interestingly, the optimal curves are nearly identical, suggesting that tokenization does not play a role in attaining the best possible trade-off. Furthermore, we see that, for a fixed rate, the standard tokenization performance often matches or exceeds the performance of forced tokenization. However, the standard tokenization approach does not allow for average rates below 0.6 due to the limited size of the prompts in our synthetic dataset, so the comparison is somewhat limited. In particular, standard tokenization allows for compression of at most four tokens (but usually only two or three tokens), whereas forced tokenization allows for compression of at most ten tokens. ", "page_idx": 8}, {"type": "text", "text": "Extension to natural language prompts. We have thus far shown the gap between current tokenlevel prompt compression algorithms and their optimal strategies for both the query-aware and queryagnostic encoder on a synthetic dataset. Here, we extend our results to a small natural language dataset curated with GPT-4 [38] (details in App. F.2.1). For natural language prompts, the number of possible combinations of tokens grows too quickly, either by increasing the number of tokens in the prompt or increasing the vocabulary size, to compute the full $D_{x}$ . Instead, we rely on the observation that current token-level prompt compression strategies simply remove tokens in place. With this observation, the number of prompts to consider has the same growth rate as a prompt with a binary alphabet. Please refer to App. E for more details. Although we cannot compute the true optimal curves where every possible combination of tokens is considered, we can compute the optimal curves for current prompt compression algorithms that do not generate new tokens. Fig. 8 shows that the gap for this approximation is negligible on binary prompts. ", "page_idx": 8}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/112a64827e25cb9dd8af191b8f78b60ab8ff814df84c4d0d2b395c802c2b1c6c.jpg", "img_caption": ["Figure 5: Comparison among all prompt compression methods on our natural language dataset. We show the rate-distortion trade-off for RougeL [23] (left) and BertScore [45] (right). Since a higher RougeL and BertScore metric is better, we plot $^{\\bullet}1-$ the computed average distortion\u201d so that a higher rate should yield a lower loss. We discuss the choice of our metrics in App. F.2.2. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "The results of our extension to natural language prompts, presented in Fig. 5, show that both of our proposed methods achieve the lowest distortion among all other prompt compression algorithms for low rates. However, the gap between all algorithms and the optimal strategies is significant. We posit the quality of the training data for LLMLingua-2-based methods accounts for the discrepancy in how far away the best method is from the optimal strategies between binary (Fig. 1) and natural language (Fig. 5) prompts. More specifically, the labels used for the binary synthetic dataset can be determined algorithmically and are optimal, but GPT-4 is used to determine the labels for the natural language dataset, which generally does not have a set of optimal \u201cground truth\u201d labels. Remarkably, the gap between feeding the prompt directly to the black-box LLM (no compression) and either optimal prompt compression strategy is also large, and Fig. 5 shows that much lower distortion can be achieved in roughly $70\\%$ and $40\\%$ fewer tokens for the query-aware and query-agnostic cases, respectively. LLMLingua-2 methods are the only methods that achieve lower distortion than the no compression result, albeit for higher rates. Finally, we present a few histograms of the rates for QuerySelect and Adaptive QuerySelect in Fig. 15, which shows the greater range of rates that Adaptive QuerySelect may choose from over QuerySelect. ", "page_idx": 9}, {"type": "text", "text": "Although we cannot compute the optimal rate-distortion curves on a dataset as large as NarrativeQA, we did compute the curve for all existing methods to compare them on a larger-scale dataset. Those results are provided in Fig. 16; they confirm that our proposed methods outperform all other methods for rates below 0.5. We also display the average time to compress a prompt for each method in Table 6. Since our methods are adapted from LLMLingua-2, they share the same runtime. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have proposed a framework for understanding the prompt compression problem for black-box target LLMs. With this framework, we defined and formulated the optimal distortion-rate trade-off as a linear program, and devised an algorithm to solve this efficiently via its dual, for both query-agnostic and query-aware settings. We compared the optimal curves with prompt compression methods in the existing literature and adapt one of them, LLMLingua-2, to be query-aware and variable-rate; this modified method, Adaptive QuerySelect, exhibits superior performance, sometimes even matching the performance of the optimal query-aware strategy, on our synthetic dataset. As future work, it is important to exhaustively study our proposed method on natural language datasets. Additionally, it is worthwhile to pursue an approximation to the optimal curves for large-scale datasets to observe the fundamental limit in that regime. We share preliminary results in that direction in App. G.3. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was partly supported by ARO Award W911NF2310062, ONR Award N000142412542, and the 6G@UT center within WNCG at UT Austin. The work was also supported in part by the Swiss National Science Foundation under Grant 200364. The authors would like to thank Ananda Theertha Suresh for introducing them to the problem of prompt compression. AG would like to thank Emre Telatar for helpful discussions on the problem formulation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page 6000\u20136010, Red Hook, NY, USA, 2017. Curran Associates Inc.   \n[2] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP \u201923, page 611\u2013626, New York, NY, USA, 2023. Association for Computing Machinery.   \n[3] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.   \n[4] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.   \n[5] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity, 2020.   \n[6] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart\u0142omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanis\u0142aw Wo\u00b4zniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023.   \n[7] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.   \n[8] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment, 2021. [9] Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context, 2022.   \n[10] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5621\u20135634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.   \n[11] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference efficiency of large language models, 2023.   \n[12] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13358\u201313376. Association for Computational Linguistics, December 2023.   \n[13] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression, 2023.   \n[14] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. ArXiv preprint, abs/2403.12968, 2024.   \n[15] Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. 2023.   \n[16] Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model. In The Twelfth International Conference on Learning Representations, 2024.   \n[17] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3829\u20133846, Singapore, December 2023. Association for Computational Linguistics.   \n[18] Tom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge, 2017.   \n[19] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. arXiv:2307.03172.   \n[20] Tong Niu, Caiming Xiong, and Richard Socher. Deleter: Leveraging bert to perform unsupervised successive text compression, 2019.   \n[21] Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, and Katja Markert. Discrete optimization for unsupervised sentence summarization with word-level extraction. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5032\u20135042, Online, July 2020. Association for Computational Linguistics.   \n[22] Demian Ghalandari, Chris Hokamp, and Georgiana Ifrim. Efficient unsupervised sentence compression by fine-tuning transformers with reinforcement learning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1267\u20131280, Dublin, Ireland, May 2022. Association for Computational Linguistics.   \n[23] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics.   \n[24] T. Berger. Rate Distortion Theory: A Mathematical Basis For Data Compression. Prentice-Hall, Inc., Englewood Cliffs, NJ, 1971.   \n[25] Claude E Shannon. Coding theorems for a discrete source with a fidelity criterion. IRE Nat. Conv. Rec, 4(142-163):1, 1959.   \n[26] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, USA, 2006.   \n[27] Abbas El Gamal and Young-Han Kim. Network Information Theory. Cambridge University Press, 2011.   \n[28] Bruce Hajek and Michael B Pursley. Evaluation of an achievable rate region for the broadcast channel. IEEE Transactions on Information Theory, 25(1):36\u201346, 1979.   \n[29] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[31] Aaron D Wyner and Jacob Ziv. The rate-distortion function for source coding with side information at the decoder. IEEE Transactions on Information Theory, 22(1):1\u201310, 1976.   \n[32] Aaron D Wyner. The rate-distortion function for source coding with side information at the decoder-II: General sources. Information and Control, 38(1):60\u201380, 1978.   \n[33] Tsachy Weissman and Abbas El Gamal. Source coding with limited-look-ahead side information at the decoder. IEEE Transactions on Information Theory, 52(12):5218\u20135239, 2006.   \n[34] Hirosuke Yamamoto. Wyner-Ziv theory for a general function of the correlated sources (corresp.). IEEE Transactions on Information Theory, 28(5):803\u2013807, 1982.   \n[35] Eric Lei, Hamed Hassani, and Shirin Saeedi Bidokhti. Neural estimation of the rate-distortion function with applications to operational source coding. IEEE Journal on Selected Areas in Information Theory, 3(4):674\u2013686, 2022.   \n[36] Yibo Yang, Stephan Eckstein, Marcel Nutz, and Stephan Mandt. Estimating the rate-distortion function by wasserstein gradient descent. In ICML 2023 Workshop Neural Compression: From Information Theory to Applications, 2023.   \n[37] Heasung Kim, Hyeji Kim, and Gustavo De Veciana. Estimation of rate-distortion function for computing with decoder side information. In First \u2019Learn to Compress\u2019 Workshop $@$ ISIT 2024, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[38] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, ", "page_idx": 12}, {"type": "text", "text": "David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00f3n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. ", "page_idx": 13}, {"type": "text", "text": "[39] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b, 2023. ", "page_idx": 13}, {"type": "text", "text": "[40] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. Pythia: a suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023.   \n[41] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2020.   \n[42] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[43] Sebastian Ruder, Anders Sgaard, and Ivan Vuli. Unsupervised cross-lingual representation learning. In Preslav Nakov and Alexis Palmer, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 31\u201338, Florence, Italy, July 2019. Association for Computational Linguistics.   \n[44] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm, 2023.   \n[45] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert, 2020.   \n[46] John von Neumann. Zur Theorie der Gesellschaftsspiele. Math. Ann., 100(1):295\u2013320, 1928.   \n[47] Stephen Simons. Minimax Theorems and Their Proofs, pages 1\u201323. Springer US, Boston, MA, 1995.   \n[48] R. Gray. Conditional rate-distortion theory. Technical report, Stanford University, 1972.   \n[49] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The appendix is organized as follows: ", "page_idx": 14}, {"type": "text", "text": "1. In App. A, we provide a complete summary of the notation used throughout the paper.   \n2. In App. B, we formally characterize the distortion-rate functions for the query-aware prompt compression setting, analogous to the query-agnostic setting in Sec. 3.   \n3. In App. C, we prove the main result of the paper (Thm. 1) and give a detailed description of the working of Algorithm 1.   \n4. In App. D, we provide an overview of the information theory literature on rate-distortion theory, and explain how our model compares.   \n5. In App. E, we describe how Algorithm 1 can be used to approximately compute the distortionrate function for small natural language datasets.   \n6. In App. F, we provide details on the setup used for the experiments in Sec. 4.   \n7. In App. G, we have additional experimental results, including preliminary results on approximating the optimal distortion-rate curve for large-scale natural language datasets in G.3. ", "page_idx": 14}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "General. We use $\\triangleq$ to signify a definition. For a set $\\mathcal{X}$ , with $x_{i}\\ \\in\\ \\mathcal X$ for $i\\,=\\,1,\\ldots,n$ , we represent the sequence $(x_{1},\\ldots,x_{n})$ by $x^{n}\\in\\mathcal{X}^{n}$ , which is short for $\\mathcal X\\times\\cdot\\cdot\\times\\mathcal X$ . We use $\\varkappa^{*}$ to denote $\\textstyle\\bigcup_{n\\geq1}X^{\\bar{n}}$ , the set of all nonempty finite-length sequences on $\\mathcal{X}$ . In general, for $y\\,\\in\\,\\mathcal{X}^{n}$ , we den ote the length of $y$ by ${\\mathrm{len}}(y)\\,=\\,n$ . We denote the cardinality of a set $\\mathcal{X}$ by $|{\\mathcal{X}}|$ . We use $\\mathbb{R}_{+}$ to denote the set of nonnegative real numbers. For a set ${\\mathcal{X}}=\\{x_{1},\\ldots,x_{k}\\}$ , we use the boldface $(A_{x})_{x\\in\\mathcal{X}}$ to denote the vector $(A_{x_{1}},\\ldots,A_{x_{k}})$ indexed by elements of $\\mathcal{X}$ . We also write $\\mathbb{R}_{+}^{\\mathcal{X}}=\\{(\\pmb{v}_{x})_{x\\in\\mathcal{X}}:\\pmb{v}_{x}\\in\\mathbb{R}_{+}$ for each $x\\in\\mathcal{X}\\}$ . We use $\\mathbb{1}$ to denote the indicator function, which takes the value 1 when its argument is true and 0 otherwise otherwise. The infimum and supremum of a set of values is denoted using inf and sup respectively. We use 0 and 1 to denote the vector of appropriate dimension with all elements equal to 0 and 1 respectively. ", "page_idx": 14}, {"type": "text", "text": "Probability. We deal with discrete probability distributions on finite sets, for which we use calligraphic letters to denote the set (e.g. $\\mathcal{X}$ ), uppercase letters to denote the random variable (r.v., e.g. $X$ ) and lowercase letters to denote samples of the r.v. (e.g. $x$ ). The set of all probability distributions on the set $\\mathcal{X}$ is denoted by $\\mathcal{P}(\\mathcal{X})$ . The probability distribution of the r.v. $X$ on $\\mathcal{X}$ is denoted by $\\mathsf{P}_{X}\\in\\mathcal{P}(\\mathcal{X})$ , and we say $X\\sim\\mathsf{P}_{X}$ . For a (measurable) function $f$ , the expectation of the r.v. $f(X)$ is denoted by $\\mathbb{E}_{X\\sim\\mathsf{P}_{X}}\\left[f(X)\\right]$ , or $\\mathbb{E}\\left[f(X)\\right]$ , with the subscripts dropped when the distribution and/or r.v.\u2019s are clear from context. The degenerate probability distribution with mass 1 at $x\\in\\mathscr{X}$ is represented by $\\delta_{x}\\,\\in\\,\\mathcal{P}(\\mathcal{X})$ . Conditional probabilities from $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ are denoted as $\\mathsf{P}_{Y\\mid X}$ , and for each $x\\in\\mathscr{X}$ , we denote the distribution on $Y$ as $\\mathsf{P}_{Y|X}(\\cdot|x)$ . ", "page_idx": 14}, {"type": "text", "text": "Problem-setup-specific notation. In our model, we use $\\mathcal{V}$ to refer to the vocabulary of the prompt. We use the uppercase letters $X$ to refer to the prompt, $M$ to refer to the compressed prompt, $Q$ to refer to the query and $Y$ to refer to the answer as random variables (and corresponding calligraphic and lowercase letters to denote the set and samples of the r.v. respectively). We use $\\mathsf{P}_{\\hat{Y}}$ to refer to the output distribution of the LLM, which is modeled as the function $\\phi_{\\mathsf{L L M}}$ . For a given prompt $x$ , we use $\\mathcal{M}_{x}$ to refer to the set of possible compressed prompts, which is the set of all sequences of length smaller than $\\operatorname{len}(x)$ . To denote the distortion measure, we use ${\\sf d}$ , which can be either the log loss $\\mathsf{d}_{\\mathrm{log}}$ or the $0/1$ loss ${\\sf d}_{0/1}$ . We denote the query-agnostic distortion-rate function at rate $R$ by $D^{*}(R)$ . The average query-aware distortion-rate function is denoted by $\\bar{D}^{*}(R)$ , and the conditional query-aware distortion-rate function for query $q\\in\\mathcal{Q}$ is given by $D_{q}^{*}(R)$ . ", "page_idx": 14}, {"type": "text", "text": "B Extensions to query-aware prompt compression ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As mentioned in Sec. 3.2 and Sec. 3.3, analogous definitions and results can be obtained for the query-aware setting as well. The difference is that the compressor has access to the query in addition to the prompt. Thus, the compressor comp is a possibly random function from $\\mathcal{X}\\times\\mathcal{Q}$ to $\\mathcal{M}$ . For the query $Q$ , the compressor maps the prompt $X$ to the compressed prompt $M=\\mathsf{c o m p}(X,Q)$ with $\\mathrm{len}(M)\\,\\leq\\,\\mathrm{len}(X)$ . The user provides the input $[M,Q]$ to the LLM, which produces the output distribution $\\mathsf{P}_{\\hat{Y}}=\\phi_{\\mathsf{L L M}}(M,Q)$ . Just as in the query-agnostic setting, two quantities of interest are ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(1)\\;\\mathrm{the~(average)~rate~}\\mathbb{E}\\left[\\frac{\\mathrm{len}(M)}{\\mathrm{len}(X)}\\right],\\quad\\mathrm{and~}\\quad(2)\\;\\mathrm{the~distortion}\\;\\mathbb{E}\\left[\\mathsf{d}_{\\log}(Y,\\phi_{\\mathsf{L L M}}(M,Q))\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with both expectations taken with respect to the joint distribution $\\mathsf{P}_{M X Q Y}$ . Since different queries may require different amounts of information to be preserved during compression, it is also of interest to define the (conditional) rate and distortion for the specific query $q$ as $\\mathbb{E}\\left[{\\frac{\\ln(M)}{\\ln(X)}}\\right]$ and $\\mathbb{E}\\left[\\mathsf{d}_{\\log}(Y,\\phi_{\\mathsf{L L M}}(M,q))\\right]$ respectively, with both expectations taken with respect to the joint distribution $\\mathsf{P}_{M X Y|Q}(\\cdot|q)$ . ", "page_idx": 15}, {"type": "text", "text": "The rate-distortion problem for query-aware prompt compression can be also formulated similarly to (3). We model comp as a random mapping $\\mathsf{P}_{M|X Q}$ from $\\mathcal{X}\\times\\mathcal{Q}$ to $\\mathcal{M}$ . Then, the (average) distortionrate function at rate $R$ is the smallest distortion that can be achieved by a query-aware compressor with rate at most $R$ , given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bar{D}^{*}(R)=}&{\\underset{\\mathsf{P}_{M|X Q}}{\\operatorname*{inf}}\\quad\\mathbb{E}\\left[\\mathsf{d}_{\\log}\\!\\left(Y,\\phi_{\\mathsf{L L M}}(M,Q)\\right)\\right]}\\\\ &{\\quad\\mathrm{~s.t.~}\\;\\;\\mathsf{P}_{M|X Q}\\;\\mathrm{is~a~compressor,~and}}\\\\ &{\\quad\\quad\\;\\;\\mathbb{E}\\left[\\frac{\\mathrm{len}(M)}{\\mathrm{len}(X)}\\right]\\leq R,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with both expectations taken with respect to the joint distribution $\\mathsf{P}_{M X Q Y}=\\mathsf{P}_{M|X Q}\\mathsf{P}_{X Q Y}$ induced by the compressor. The condition ${}^{\\bullet\\bullet}\\mathsf{P}_{M|X Q}$ is a compressor\u201d is short for (1) for each $\\dot{x}\\in\\mathcal{X}$ and $q\\ \\in\\ \\mathcal{Q}$ $,\\,\\textstyle\\sum_{m\\in{\\mathcal{M}}}\\mathsf{P}_{M|X Q}(m|x,q)\\,=\\,1$ , (2) $\\mathsf{P}_{M|X Q}(m|x,q)\\,=\\,0$ if $\\mathrm{len}(m)\\,>\\,\\mathrm{len}(x)$ , and (3) if $\\mathrm{len}(m)=\\mathrm{len}(x)$ , then $\\mathsf{P}_{M|X Q}(m|x,q)=0$ unless $m=x$ . Similarly, the (conditional) distortionrate function at rate $R$ is the smallest distortion that can be achieved by a query-aware compressor for query $q$ at rate at most $R$ , given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{q}^{*}(R)=}&{\\underset{\\mathsf{P}_{M|X Q}(\\cdot|\\cdot,q)}{\\operatorname*{inf}}~}&{\\mathbb{E}\\left[\\mathsf{d}_{\\log}\\big(Y,\\phi_{\\mathsf{L L M}}(M,Q)\\big)\\right]}\\\\ {\\mathrm{s.t.}}&{\\mathsf{P}_{M|X Q}(\\cdot|\\cdot,q)~\\mathrm{is~a~compressor,~and}}\\\\ &{\\mathbb{E}\\left[\\frac{\\mathrm{len}(M)}{\\mathrm{len}(X)}\\right]\\leq R,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with both expectations taken with respect to $\\mathsf{P}_{M X Y|Q}(\\cdot,\\cdot,\\cdot|q)$ . ", "page_idx": 15}, {"type": "text", "text": "Just like the query-agnostic setting, note that both (5) and (6) are linear programs, as the objective and constraints are all linear in $\\mathsf{P}_{M|X Q}$ and $\\mathsf{P}_{M|X Q}(\\cdot|\\cdot,q)$ respectively. We obtain explicit linear programs analogous to (LP) by defining constants $\\Bar{D}_{x}^{q}$ and $\\bar{R}_{x}^{q}\\in\\mathbb{R}_{+}^{\\mathcal{M}_{x}}$ for the average distortion-rate function and $D_{x}^{q}$ and $R_{x}^{q}\\in\\mathbb{R}_{+}^{\\mathcal{M}_{x}}$ for the conditional distortion-rate functions, for each $x\\in\\mathscr{X}$ and $q\\in\\mathcal{Q}$ , similarly to (4). ", "page_idx": 15}, {"type": "text", "text": "Proposition 2 (Query-aware primal LPs). The (average) distortion-rate function for query-aware prompt compression (5) is given by the solution to ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\bar{D}^{*}(R)=}&{\\ \\underset{\\big(z_{x,q}\\in\\mathbb{R}_{+}^{M_{x}}\\big)_{x\\in\\mathcal{X},q\\in\\mathcal{Q}}}{\\operatorname*{inf}}\\quad\\displaystyle\\sum_{x\\in\\mathcal{X},q\\in\\mathcal{Q}}\\bar{D}_{x}^{q^{\\top}}z_{x,q}}\\\\ {\\quad}&{\\quad\\quad\\quad\\quad\\quad s.t.\\quad\\displaystyle\\sum_{x\\in\\mathcal{X},q\\in\\mathcal{Q}}\\bar{R}_{x}^{q^{\\top}}z_{x,q}\\leq R,}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mathbf{1}^{\\top}z_{x,q}=1,\\quad\\forall\\,x\\in\\mathcal{X},\\,q\\in\\mathcal{Q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The (conditional) distortion-rate function for query-aware prompt compression for query $q$ (6) is given by the solution to ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{q}^{*}(R)=}&{\\ \\underset{\\left(z_{x}\\in\\mathbb{R}_{+}^{M_{x}}\\right)_{x\\in\\mathcal{X}}}{\\operatorname*{inf}}\\ \\underset{x\\in\\mathcal{X}}{\\sum}D_{x}^{q^{\\top}}z_{x}}\\\\ &{\\qquad\\qquad s.t.\\quad\\;\\;\\;\\displaystyle\\sum_{x\\in\\mathcal{X}}R_{x}^{q^{\\top}}z_{x}\\leq R,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad1^{\\top}z_{x}=1,\\quad\\forall x\\in\\mathcal{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For each $x\\in\\mathscr{X}$ , $\\mathcal{M}_{x}$ denotes the set of all possible compressed prompts associated to $x$ , i.e., the set of all possible token sequences of length at most $\\operatorname{len}(x)$ , the vectors $z_{x,q}\\in\\mathbb{R}_{+}^{\\mathcal{M}_{x}}$ for $q\\in\\mathcal{Q}$ and $z_{x}\\in\\mathbb{R}_{+}^{\\mathcal{M}_{x}}$ are the optimization variables respectively and the constants $\\bar{D}_{x}^{q},\\bar{R}_{x}^{q},D_{x}^{q},R_{x}^{q}\\in\\mathbb{R}_{+}^{\\mathcal{M}_{x}}$ are given by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{D}_{x,m}^{q}\\triangleq\\mathsf{P}_{X Q}(x,q)\\,\\mathbb{E}\\,[\\mathsf{d}_{\\log}(Y,\\phi_{\\mathrm{LU}}(m,q))]\\quad a n d\\ \\bar{R}_{x,m}^{q}\\triangleq\\mathsf{P}_{X Q}(x,q)\\,\\frac{\\log(m)}{\\log(x)},\\quad m\\in\\mathcal{M}_{x},}\\\\ &{\\boldsymbol{D}_{x,m}^{q}\\triangleq\\mathsf{P}_{X|Q}(x|q)\\,\\mathbb{E}\\,[\\mathsf{d}_{\\log}(Y,\\phi_{\\mathrm{LU}}(m,q))]\\quad a n d\\ \\boldsymbol{R}_{x,m}^{q}\\triangleq\\mathsf{P}_{X|Q}(x|q)\\,\\frac{\\log(m)}{\\log(x)},\\quad m\\in\\mathcal{M}_{x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with the expectation taken with respect to $\\mathsf{P}_{Y|M X Q}(\\cdot|m,x,q)$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. This follows immediately from (5) and (6) by defining the constants $\\bar{D}_{x}^{q},\\bar{R}_{x}^{q},D_{x}^{q},R_{x}^{q}\\in\\mathbb{R}_{+}^{\\mathcal{M}_{x}}$ for each $x\\,\\in\\,{\\mathcal{X}}$ and $q\\,\\in\\,\\mathcal{Q}$ as given in (7) and (8) and taking $z_{x,q}$ and $z_{x}$ to be $\\mathsf{P}_{M|X Q}(\\cdot|x,q)$ respectively,. We use the fact that $\\mathsf{P}_{M|X Q}(m|x,q)\\,=\\,0$ when $\\operatorname{len}(m)\\,>\\,\\log(x)$ to reduce the dimension of $z_{x,q}$ and $z_{x}$ from $\\mathcal{M}$ to $\\mathcal{M}_{x}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Remark 1. An interesting phenomenon here that does not occur in the query-agnostic setting is the comparison between the average and conditional distortion-rate functions, i.e., $\\bar{D}^{*}(R)$ and $D_{q}^{*}(R)$ for $q\\in\\mathcal{Q}$ . One possible way to \u201caverage\u201d the conditional distortion-rate functions would be to simply compute $\\mathbb{E}_{Q\\sim\\mathsf{P}_{Q}}\\left[D_{Q}^{*}(\\bar{R})\\right]$ , but we always have $\\bar{D}^{*}(R)\\leq\\mathbb{E}_{Q\\sim\\mathsf{P}_{Q}}\\left[D_{Q}^{*}(R)\\right]$ . This is because the latter averages the distortion-rate functions over $\\mathsf{P}_{Q}$ at a fixed value of the rate, i.e., the prompt for each query is forced to be compressed to the same rate $R$ . For $\\bar{D}^{*}(R)$ , on the other hand, only the average rate over the queries is required to be $R$ . This allows the compressor to set a higher rate for \u201cdifficult queries\u201d that have higher distortion values, and use a lower rate for queries that have lower distortion values in general. This is exactly the phenomenon we exploit in designing the variable-rate compression scheme Adaptive QuerySelect in Sec. 4.1, which outperforms other existing schemes in our experiments. ", "page_idx": 16}, {"type": "text", "text": "Just as in the query-agnostic setting, it is useful to compute and solve the dual linear programs instead of directly solving the linear programs above. ", "page_idx": 16}, {"type": "text", "text": "Theorem 2 (Query-aware dual LPs). The (average) distortion-rate function for query-aware prompt compression (5) is given by the solution to the dual of the linear program (avg-cond-LP), i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\bar{D}^{*}(R)=\\operatorname*{sup}_{\\lambda\\geq0}\\left\\{-\\lambda R+\\sum_{x\\in\\mathcal{X},q\\in\\mathcal{Q}}\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}\\left[\\bar{D}_{x,m}^{q}+\\lambda\\bar{R}_{x,m}^{q}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The (conditional) distortion-rate function for query-aware prompt compression for query $q$ (6) is given by the solution to the dual of the linear program (cond-LP), i.e., ", "page_idx": 16}, {"type": "equation", "text": "$$\nD_{q}^{*}(R)=\\operatorname*{sup}_{\\lambda\\geq0}\\left\\{-\\lambda R+\\sum_{x\\in\\mathcal{X}}\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}\\left[D_{x,m}^{q}+\\lambda R_{x,m}^{q}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. (Conditional) This follows trivially by simply observing that the linear program in (cond-LP) is identical to that in (LP), except that $D_{x}$ and $R_{x}$ are replaced by the (conditional) query-aware versions $D_{x}^{q}$ and $R_{x}^{q}$ respectively. Henc, by Thm. 1 the solution to (cond-LP) is given by (dual-LP) with $D_{x}$ and $R_{x}$ replaced by the (conditional) query-aware versions $D_{x}^{q}$ and $R_{x}^{q}$ respectively, which gives (cond-dual-LP). ", "page_idx": 16}, {"type": "text", "text": "(Average) In addition to replacing $D_{x}$ and $R_{x}$ from (LP) by the (average) query-aware versions $\\Bar{D}_{x}^{q}$ and $\\bar{R}_{x}^{q}$ respectively, we also have that the optimization variables are given by $z_{x,q}$ for each pair $(x,q)\\in\\mathcal{X}\\times\\mathcal{Q}$ as opposed to simply $z_{x}$ for each $x\\in\\mathscr{X}$ . Hence, by Thm. 1 the solution to (avg-cond-LP) is given by (dual-LP) with $D_{x}$ and $R_{x}$ are replaced by the (average) query-aware versions $\\Bar{D}_{x}^{q}$ and $\\Bar{R}_{x}^{q}$ respectively and $\\mathcal{X}$ replaced by $\\mathcal{X}\\times\\mathcal{Q}$ . This gives (avg-cond-dual-LP) exactly, and we are done. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Note that both (avg-cond-dual-LP) and (cond-dual-LP) are of the same form as (dual-LP), with some minor differences. For a given $q\\in\\mathcal{Q}$ , the conditional distortion-rate function $D_{q}^{*}(R)$ is identical to the query-unaware distortion-rate function $D_{q}^{*}(R)$ with $(D_{x},R_{x})$ replaced by $(D_{x}^{q},R_{x}^{q})$ , and hence can be solved by running Algorithm 1 with the input $\\left\\{R,(D_{x}^{q})_{x\\in\\mathcal{X}}\\,,(R_{x}^{q})_{x\\in\\mathcal{X}}\\right\\}$ . For the average distortion-rate function $\\bar{D}^{*}(R)$ , in addition to replacing $D_{x}$ and $R_{x}$ by $\\Bar{D}_{x}^{q}$ and $\\bar{R}_{x}^{q}$ , we also have that $\\mathcal{X}$ is replaced by $\\mathcal{X}\\times\\mathcal{Q}$ , hence $\\bar{D}^{*}(R)$ is obtained by running 1 with the input $\\left\\{R,\\left(D_{x^{\\prime}}^{q}\\right)_{x^{\\prime}\\in\\mathcal{X}^{\\prime}},\\left(R_{x^{\\prime}}^{q}\\right)_{x^{\\prime}\\in\\mathcal{X}^{\\prime}}\\right\\}$ , where $\\mathcal{X}^{\\prime}\\triangleq\\mathcal{X}\\times\\mathcal{Q}$ and $x^{\\prime}$ runs over all pairs $(x,q)$ . ", "page_idx": 17}, {"type": "text", "text": "C The dual linear program: proof and solution ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Derivation of the dual linear program ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Thm. 1. We start from the linear program (LP) and construct its dual. Recall that (LP) is given by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D^{*}(R)=}&{\\ \\underset{\\left(z_{x}\\in\\mathbb{R}_{+}^{M_{x}}\\right)_{x\\in\\mathcal{X}}}{\\operatorname*{inf}}\\ \\ \\sum_{x\\in\\mathcal{X}}D_{x}^{\\top}z_{x}}\\\\ &{\\qquad\\mathrm{s.t.}\\quad\\ \\displaystyle\\sum_{x\\in\\mathcal{X}}R_{x}^{\\top}z_{x}\\leq R,}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\quad1^{\\top}z_{x}=1,\\quad\\forall x\\in\\mathcal{X}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Introduce the Lagrange multipliers $\\lambda\\geq0$ to handle the inequality constraint and $\\mu_{x}\\in\\mathbb{R}$ for each $x\\in\\mathscr{X}$ to handle the equality constraints. Then, the above equation is equivalent to ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\b{\\gamma}}^{*}(R)=\\operatorname*{inf}_{\\substack{\\left(z_{x}\\in\\mathbb{R}_{+}^{M_{x}}\\right)_{x\\in\\mathcal{X}}}}\\left\\{\\sum_{x\\in\\mathcal{X}}D_{x}^{\\top}z_{x}+\\operatorname*{sup}_{\\lambda\\geq0}\\lambda\\left(\\sum_{x\\in\\mathcal{X}}R_{x}^{\\top}z_{x}-R\\right)+\\sum_{x\\in\\mathcal{X}}\\left[\\operatorname*{sup}_{\\mu_{x}\\in\\mathbb{R}}\\mu_{x}\\left({\\bf1}^{\\top}z_{x}-1\\right)\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To see why this equivalence holds, observe that the terms $\\begin{array}{r}{\\operatorname*{sup}_{\\lambda\\geq0}\\lambda\\left(\\sum_{x\\in\\mathcal{X}}R_{x}^{\\top}z_{x}-R\\right)}\\end{array}$ and $\\operatorname*{sup}_{\\mu_{x}\\in\\mathbb{R}}\\mu_{x}\\left(\\mathbf{1}^{\\top}z_{x}-1\\right)$ are both 0 when $(z_{x})_{x\\in\\mathcal{X}}$ is in the feasible set of (LP) and $+\\infty$ otherwise. Let $\\mu\\triangleq(\\mu_{x})_{x\\in\\mathcal{X}}\\in\\mathbb{R}^{\\chi}$ , then we can simplify the above expression by rearranging terms, to obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\nD^{*}(R)=\\operatorname*{inf}_{\\substack{\\left(z_{x}\\in\\mathbb{R}_{+}^{M_{x}}\\right)_{x\\in\\mathcal{X}}}}\\operatorname*{sup}_{\\mu\\in\\mathbb{R}^{X}}\\,\\left\\{\\sum_{x\\in\\mathcal{X}}\\left(D_{x}+\\lambda R_{x}+\\mu_{x}\\mathbf{1}\\right)^{\\top}z_{x}-\\lambda R-\\sum_{x\\in\\mathcal{X}}\\mu_{x}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that the objective $\\begin{array}{r}{\\sum_{x\\in\\mathcal{X}}\\left(D_{x}+\\lambda\\pmb{R}_{x}+\\mu_{x}\\pmb{1}\\right)^{\\top}z_{x}-\\lambda\\pmb{R}-\\sum_{x\\in\\mathcal{X}}\\mu_{x}}\\end{array}$ is linear in $(z_{x})_{x\\in\\mathcal{X}}$ and in $(\\mu,\\lambda)$ , and the mini mization and maximization are both over convex sets. Hence, by the minmax theorem [46, 47], we can switch their order without affecting the equality, i.e., ", "page_idx": 17}, {"type": "equation", "text": "$$\nD^{*}(R)=\\operatorname*{sup}_{\\mu\\in\\mathbb{R}^{\\mathcal{X}},\\atop\\lambda\\geq0}\\operatorname*{inf}_{\\substack{\\left(z_{x}\\in\\mathbb{R}_{+}^{M_{x}}\\right)_{x\\in\\mathcal{X}}}}\\left\\{\\sum_{x\\in\\mathcal{X}}\\left(D_{x}+\\lambda R_{x}+\\mu_{x}\\mathbf{1}\\right)^{\\top}z_{x}-\\lambda R-\\sum_{x\\in\\mathcal{X}}\\mu_{x}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If, for some $x$ , there is a component of the vector $D_{x}+\\lambda R_{x}+\\mu_{x}\\mathbf{1}\\in\\mathbb{R}^{\\mathcal{M}_{x}}$ that is negative, then letting that component of $z_{x}$ go to infinity, we have that the inner infimum is $-\\infty$ . On the other hand, if every component of $D_{x}+\\lambda R_{x}+\\mu_{x}\\mathbf{1}$ is nonnegative for every $x$ , then the infimum is simply 0, attained by setting $z_{x}=\\mathbf{0}$ . Hence, the above equation reduces to ", "page_idx": 17}, {"type": "equation", "text": "$$\nD^{*}(R)=\\operatorname*{sup}_{\\begin{array}{c}{\\mu\\in\\mathbb{R}^{\\mathcal{X}},}\\\\ {\\lambda\\geq0}\\end{array}}\\quad-\\lambda R-\\sum_{x\\in\\mathcal{X}}\\mu_{x}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\nD_{x,m}+\\lambda R_{x,m}+\\mu_{x}\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For a given $x$ , the constraint $D_{x,m}+\\lambda R_{x,m}+\\mu_{x}\\ge0$ for all $m\\in\\mathcal{M}_{x}$ is equivalent to $-\\mu_{x}\\leq$ $\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}$ $(D_{x,m}+\\lambda R_{x,m})$ . Letting $\\begin{array}{r}{\\nu_{x}\\triangleq\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}\\left(D_{x,m}+\\lambda R_{x,m}\\right)+\\mu_{x}}\\end{array}$ and $\\nu\\triangleq(\\nu_{x})_{x\\in\\mathcal{X}}$ , the constraint is simply that $\\nu_{x}\\ge0$ for all $x$ , or equivalently, $\\nu\\in\\mathbb{R}_{+}^{\\mathcal{X}}$ . Hence, the above equation can be written as ", "page_idx": 18}, {"type": "equation", "text": "$$\nD^{\\ast}(R)=\\operatorname*{sup}_{\\nu\\in\\mathbb{R}_{+}^{x}\\!,0}\\ -\\lambda R+\\sum_{x\\in\\mathcal{X}}\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}\\left(D_{x,m}+\\lambda R_{x,m}\\right)-\\sum_{x\\in\\mathcal{X}}\\nu_{x}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Observe that only the first two terms depend on $\\lambda$ , and only the last term depends on $\\nu$ . This lets us optimize over $\\lambda$ and $\\nu$ separately, to give ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D^{*}(R)=\\underset{\\lambda\\geq0}{\\operatorname*{sup}}\\left\\lbrace-\\lambda R+\\displaystyle\\sum_{x\\in\\mathcal{X}}\\underset{m\\in\\mathcal{M}_{x}}{\\operatorname*{min}}\\left(D_{x,m}+\\lambda R_{x,m}\\right)\\right\\rbrace+\\underset{\\nu\\in\\mathbb{R}_{+}^{\\chi}}{\\operatorname*{sup}}\\left(-\\displaystyle\\sum_{x\\in\\mathcal{X}}\\nu_{x}\\right)}\\\\ &{\\qquad=\\underset{\\lambda\\geq0}{\\operatorname*{sup}}\\left\\lbrace-\\lambda R+\\displaystyle\\sum_{x\\in\\mathcal{X}}\\underset{m\\in\\mathcal{M}_{x}}{\\operatorname*{min}}\\left(D_{x,m}+\\lambda R_{x,m}\\right)\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "since $\\begin{array}{r}{\\operatorname*{sup}_{\\nu\\in\\mathbb{R}_{+}^{\\chi}}\\left(-\\sum_{x\\in\\mathcal{X}}\\nu_{x}\\right)\\;=\\;-\\operatorname*{inf}_{\\nu\\in\\mathbb{R}_{+}^{\\chi}}\\sum_{x\\in\\mathcal{X}}\\nu_{x}\\;=\\;-\\sum_{x\\in\\mathcal{X}}\\operatorname*{inf}_{\\nu_{x}\\geq0}\\nu_{x}\\;=\\;0}\\end{array}$ , and we are done. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "C.2 Proof and illustration of Algorithm 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we explain each step of Algorithm 1 in detail. In doing so, we prove that the algorithm does indeed solve (dual-LP), i.e., computes ", "page_idx": 18}, {"type": "equation", "text": "$$\nD^{*}(R)=\\operatorname*{sup}_{\\lambda\\geq0}\\left\\{-\\lambda R+\\sum_{x\\in\\mathcal{X}}\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}\\left[D_{x,m}+\\lambda R_{x,m}\\right]\\right\\}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We also use an artificial example as described below to show the working of the algorithm, in particular lines 1\u20139. For convenience, the algorithm is repeated verbatim below, without comments: ", "page_idx": 18}, {"type": "text", "text": "Algorithm 1: To compute the distortion-rate function via the dual linear program (dual-LP) ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1 Input: R, $(D_{x})_{x\\in\\mathcal{X}}$ , $(R_{x})_{x\\in\\mathcal{X}}$ ; Output: $D^{*}(R)$ , the distortion-rate function at rate $R$ ;   \n2 for $x\\in\\mathscr{X}$ do   \n3 Find $\\mathcal{M}_{\\mathrm{env}}^{(x)}\\subseteq\\mathcal{M}_{x}$ such that $\\{(R_{x,m},D_{x,m})\\}_{m\\in\\mathcal{M}_{\\mathrm{env}}^{(x)}}$ are on the lower-left convex boundary of {(Rx,m, Dx,m)}m\u2208Mx; $\\left\\{m_{1}^{(x)},m_{2}^{(x)},\\ldots,m_{k_{x}}^{(x)}\\right\\}\\gets\\mathcal{M}_{\\mathrm{env}}^{(x)}$ D ordere\u2212d Dsuch that $\\pmb{R}_{x,m_{k_{x}}^{(x)}}>\\cdot\\cdot\\cdot>\\pmb{R}_{x,m_{1}^{(x)}}$ ;   \n5 for i = 1, . . . , kx \u22121 do \u03bbi(x)\u2190 Rxx,,mmii((xx+))1\u2212   \n6 $\\lambda_{0}^{(x)}\\gets+\\infty;\\lambda_{k_{x}}^{(x)}\\gets0;\\Lambda^{(x)}\\gets\\Big\\{\\lambda_{0}^{(x)},\\lambda_{1}^{(x)},\\cdot\\cdot\\cdot,\\lambda_{k_{x}-1}^{(x)},\\lambda_{k_{x}}^{(x)}\\Big\\};$ $\\begin{array}{r}{\\left\\{{\\widetilde{\\lambda}_{0}},\\ldots,{\\widetilde{\\lambda}_{k}}\\right\\}\\gets\\bigcup_{x\\in\\mathcal{X}}\\Lambda^{(x)}}\\end{array}$ with $+\\infty=\\widetilde{\\lambda}_{0}>\\widetilde{\\lambda}_{1}>\\dots>\\widetilde{\\lambda}_{k-1}>\\widetilde{\\lambda}_{k}=0$ ;   \n8 fo r $x\\in\\mathscr{X}$ do   \n9 for j = 1, . . . , k do Find $i\\in\\{1,\\ldots,k_{x}\\}:\\left(\\lambda_{i}^{(x)},\\lambda_{i-1}^{(x)}\\right)\\supseteq\\left(\\widetilde{\\lambda}_{j},\\widetilde{\\lambda}_{j-1}\\right)$ ; set $\\widetilde{m}_{j}^{(x)}\\gets m_{i}^{(x)}$   \n10 for $j=1,\\dots,k$ do   \n11 $\\mathbf{f}\\sum_{x\\in\\mathcal{X}}R_{x,\\tilde{m}_{j}^{(x)}}>R$ then $\\lambda_{j}\\leftarrow\\widetilde{\\lambda}_{j-1}$ else $\\lambda_{j}\\leftarrow\\widetilde{\\lambda}_{j}$ ;   \n12 $\\begin{array}{r}{D_{j}\\gets-\\lambda_{j}R+\\sum_{x\\in\\mathcal{X}}\\left[D_{x,\\widetilde{m}_{j}^{(x)}}+\\lambda_{j}R_{x,\\widetilde{m}_{j}^{(x)}}\\right]}\\end{array}$ ;   \n13 Return $\\operatorname*{max}_{j=1,\\dots,k}D_{j}$ ; ", "page_idx": 18}, {"type": "text", "text": "Consider the following artificial example with $\\mathcal{X}\\,=\\,\\left\\{\\alpha,\\beta\\right\\}$ . Let $(R_{\\alpha},D_{\\alpha})$ and $(R_{\\beta},D_{\\beta})$ be as given by the blue points in the scatter plots over $m\\in\\mathcal{M}_{\\alpha}$ and $m\\in\\mathcal{M}_{\\beta}$ respectively in Fig. 6. In our example, we have $|\\mathcal{M}_{\\alpha}|=11$ and $|\\mathcal{M}_{\\beta}|=8$ . The following observation is crucial: recall the definitions of $R_{x}$ and $D_{x}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\boldsymbol D_{x,m}\\triangleq\\mathsf{P}_{X}(x)\\mathbb{E}\\left[\\mathsf{d}_{\\log}(Y,\\phi_{\\mathsf{L L M}}(m,Q))\\right]\\quad\\mathrm{and}\\quad\\boldsymbol R_{x,m}\\triangleq\\mathsf{P}_{X}(x)\\,\\frac{\\ln(m)}{\\log(x)},\\qquad m\\in\\mathcal{M}_{x},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with the expectation computed with respect to $\\mathsf{P}_{Q Y|M X}(\\cdot,\\cdot|m,x)$ . For a fixed value of $x$ , the positive real numbers $D_{x,m}$ can be arbitrary, but $\\boldsymbol{R}_{x,m}$ must be an integral multiple of the constant $\\frac{\\mathsf{P}_{X}(x)}{\\mathrm{len}(x)}$ Hence, for a given $x$ , $\\scriptstyle R_{x,m}$ takes at most $\\operatorname{len}(x)$ possible values. This turns out to be extremely beneficial in the first step, namely identifying the points on the lower-left convex boundary. ", "page_idx": 19}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/af3b7ae0161ba2851b2f6ffc45c5e966412dc3f2810316356e6edb68db305f60.jpg", "img_caption": ["Figure 6: Scatter plots showing the points $\\{(R_{\\alpha,m},D_{\\alpha,m})\\}_{m\\in\\mathcal{M}_{\\alpha}}$ and $\\{(\\pmb{R}_{\\beta,m},\\pmb{D}_{\\beta,m})\\}_{m\\in\\mathcal{M}_{\\beta}}$ in blue. The associated lower-left convex boundaries $\\mathcal{M}_{\\mathrm{bd}}^{(\\alpha)}\\,=\\,\\{m_{1}^{(\\alpha)},m_{2}^{(\\alpha)},m_{3}^{(\\alpha)}\\}$ and $\\mathcal{M}_{\\mathrm{bd}}^{(\\beta)}=\\begin{array}{r l}\\end{array}$ $\\{m_{1}^{(\\beta)},m_{2}^{(\\beta)}\\}$ are in red; $\\lambda_{1}^{(\\alpha)}$ , $\\lambda_{2}^{(\\alpha)}$ and $\\lambda_{1}^{(\\beta)}$ are the magnitudes of the slopes of the associated line segments. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "For x \u2208X, lines 2\u20133 of the algorithm identify the kx points m(1x ), . . . , m (kxx) that lie on the lowerleft convex boundary of $\\{(R_{x,m},D_{x,m})\\}_{m\\in\\mathcal{M}_{x}}$ . The lower-left convex boundaries are given by the red lines and the points lying on the boundary are outlined in red. Observe that $k_{\\alpha}=3$ and $k_{\\beta}=2$ . The quantities computed in line 5 are simply the magnitudes of the slopes of the line segments on the boundary. A simple computation gives the result of line 6 of the algorithm in our example to be $\\Lambda^{(\\alpha)}=\\dot{\\{}\\mathrm{+}\\infty,\\boldsymbol{1.5,0.5,0}\\}$ and $\\bar{\\Lambda^{(\\beta)}}\\,\\bar{=}\\,\\{\\neg\\infty,1,0\\}$ . Clearly, for a given value of $x\\in\\mathscr{X}$ and $\\lambda\\in$ $[\\lambda_{j}^{(x)},\\lambda_{j-1}^{(x)})$ , we have that $m_{i}^{(x)}$ minimizes $D_{x,m}+\\lambda R_{x,m}$ over all $m\\in\\mathcal{M}_{x}$ , by virtue of the fact that these points come from the lower-left convex boundary. Hence, for $\\lambda\\in\\left[\\lambda_{j}^{(x)},\\lambda_{j-1}^{(x)}\\right)$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{x\\in\\mathcal{X}}\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}\\left[D_{x,m}+\\lambda R_{x,m}\\right]=\\sum_{x\\in\\mathcal{X}}\\left[D_{x,m_{j}^{(x)}}+\\lambda R_{x,m_{j}^{(x)}}\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We cannot simplify this further in its current state since $\\lambda$ in the above expression depends on $x$ . Hence, we must remove the dependence of the range $\\left[\\lambda_{j}^{(x)},\\lambda_{j-1}^{(x)}\\right)$ on $x$ . To do so, observe that each $\\Lambda^{(x)}$ is a partition of $\\mathbb{R}_{+}$ on which $m_{i}^{(x)}$ is the minimizer of $D_{x,m}+\\lambda R_{x,m}$ . Line 7 of the algorithm simply constructs the union of all these partitions, with $k$ elements, denoted by the $\\widetilde{\\lambda}$ variables; here we have $k=4$ and the union is $\\{+\\infty,1.5,1,0.5,0\\}$ . For each $x$ , the minimizer on   each interval $[\\widetilde{\\lambda}_{j},\\widetilde{\\lambda}_{j-1})$ of the finer partition is known exactly to be one of the mi(x)\u2019s; lines 8\u20139 associate to eac h i n terval the corresponding minimizer, given by m(jx ). There is no computation involved in these steps, only notational rewriting. The corresponding   values obtained for our example are given in the table below (with\u03bb0 = +\u221e); observe thatm(jx minimizes $D_{x,m}+\\lambda R_{x,m}$ over $m\\in\\mathcal{M}_{x}$ for $\\lambda\\in\\big[\\widetilde{\\lambda}_{j},\\widetilde{\\lambda}_{j-1}\\big)$ . ", "page_idx": 19}, {"type": "text", "text": "At this point, we have for $\\lambda\\in\\big[\\widetilde{\\lambda}_{j},\\widetilde{\\lambda}_{j-1}\\big)$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{x\\in\\mathcal{X}}\\operatorname*{min}_{m\\in\\mathcal{M}_{x}}\\left[D_{x,m}+\\lambda R_{x,m}\\right]=\\sum_{x\\in\\mathcal{X}}\\left[D_{x,\\tilde{m}_{j}^{(x)}}+\\lambda R_{x,\\tilde{m}_{j}^{(x)}}\\right]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Table 1: The outputs produced by lines 7\u20139 of Algorithm 1 with $(R_{\\alpha},D_{\\alpha})$ and $(R_{\\beta},D_{\\beta})$ as given in Fig. 6. ", "page_idx": 20}, {"type": "table", "img_path": "TeBKVfhP2M/tmp/83742cbfab9cca3e1a6210019245668d70098aec170ba4ed75f61aaf73bcbee2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{\\Sigma}=\\left(\\sum_{x\\in\\mathcal{X}}D_{x,\\widetilde{m}_{j}^{(x)}}\\right)+\\lambda\\left(\\sum_{x\\in\\mathcal{X}}R_{x,\\widetilde{m}_{j}^{(x)}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, the right-hand side of (dual-LP) is simply ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j=1,\\dots,k}{\\operatorname*{max}}\\underset{\\lambda\\in[\\tilde{\\lambda}_{j},\\tilde{\\lambda}_{j-1}]}{\\operatorname*{sup}}\\left\\lbrace\\left(\\sum_{x\\in\\mathcal{X}}D_{x,\\tilde{m}_{j}^{(x)}}\\right)+\\lambda\\left(\\sum_{x\\in\\mathcal{X}}R_{x,\\tilde{m}_{j}^{(x)}}-R\\right)\\right\\rbrace}\\\\ &{=\\underset{j=1,\\dots,k}{\\operatorname*{max}}\\left\\lbrace\\left(\\sum_{x\\in\\mathcal{X}}D_{x,\\tilde{m}_{j}^{(x)}}\\right)+\\underset{\\lambda\\in[\\tilde{\\lambda}_{j},\\tilde{\\lambda}_{j-1}]}{\\operatorname*{sup}}\\lambda\\left(\\sum_{x\\in\\mathcal{X}}R_{x,\\tilde{m}_{j}^{(x)}}-R\\right)\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first equality follows since $\\left\\{{\\widetilde\\lambda}_{j}\\right\\}_{j=0}^{k}$ is a partition of $\\mathbb{R}_{+}$ . Consider the term $\\begin{array}{r}{\\operatorname*{sup}_{\\lambda\\in[\\widetilde{\\lambda}_{j},\\widetilde{\\lambda}_{j-1})}\\lambda\\left(\\sum_{x\\in\\mathcal{X}}R_{x,\\widetilde{m}_{j}^{(x)}}-R\\right)}\\end{array}$ . If Rx,m(x) > R, this supremum occurs in the limit as $\\begin{array}{r l r}{\\lambda}&{{}\\to}&{\\widetilde{\\lambda}_{j-1}}\\end{array}$ , otherwise it is achieved at $\\begin{array}{r l r}{\\lambda}&{{}=}&{\\widetilde{\\lambda}_{j}}\\end{array}$ . Hence, defining $\\lambda_{j}$ to be $\\widetilde{\\lambda}_{j-1}$ or $\\widetilde{\\lambda}_{j}$ ac cordingly as in line 11, we have that the  above expression is simply $\\begin{array}{r}{\\operatorname*{max}_{j=1,\\dots,k}\\left\\{\\left(\\sum_{x\\in\\mathcal{X}}D_{x,\\widetilde{m}_{j}^{(x)}}\\right)+\\lambda_{j}\\left(\\sum_{x\\in\\mathcal{X}}R_{x,\\widetilde{m}_{j}^{(x)}}-R\\right)\\right\\}}\\end{array}$ , which is exactly what line 13 returns. Hence, we have that the algorithm correctly computes the distortion-rate function $D^{*}(R)$ . ", "page_idx": 20}, {"type": "text", "text": "D Connections to information theory literature ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Rate-distortion theory is an area of information theory introduced by Shannon [25] to study the fundamental limits of source compression. The simplest rate-distortion setup is shown in Fig. 7a: We are given a source which generates samples $X_{1},\\ldots,X_{n}$ independently and identically distributed (i.i.d.) according to the distribution $\\mathsf{P}_{X}$ on the set $\\mathcal{X}$ . We are also given a reconstruction alphabet $\\hat{\\chi}$ , which may or may not be equal to $\\mathcal{X}$ . The goal is to compress $X^{n}$ to a sequence of $k$ elements from an alphabet $\\mathcal{V}$ , such that a reconstruction onto ${\\hat{X}}^{n}$ is as \u201cfaithful\u201d as possible, while keeping $k$ as small as possible (in the information theory literature, $\\mathcal{V}=\\{0,1\\}$ typically). The fidelity of representation is quantified by a distortion function $\\mathsf{d}:\\mathcal{X}\\times\\hat{\\mathcal{X}}\\to[0,\\infty]$ . For example, the problem of compressing images with $p$ real-valued pixels into bit sequences can be cast in this formulation by taking $\\mathcal{X}=\\hat{\\mathcal{X}}=\\mathbb{R}^{p}$ , ${\\mathcal{V}}=\\{0,1\\}$ , and the squared-loss distortion function ${\\mathsf{d}}(x,{\\hat{x}})=\\|x-{\\hat{x}}\\|_{2}^{2}$ . ", "page_idx": 20}, {"type": "text", "text": "Formally, the goal is to construct an encoder enc : $\\mathcal{X}^{n}\\to\\mathcal{V}^{k}$ and a decoder $\\mathrm{dec}:\\mathcal{V}^{k}\\to\\hat{\\mathcal{X}}^{n}$ such that: (1) the rate $k/n$ , and (2) the (average) distortion $\\mathbb{E}\\left[{\\mathsf{d}}(X^{n},{\\mathsf{d e c}}(\\operatorname{enc}(X^{n}))\\right]$ , are both as small as possible. We say that the rate-distortion pair $(R,D)$ is achievable for the source $\\mathsf{P}_{X}$ under the distortion function ${\\sf d}$ if there exists an (enc, dec) pair with rate at most $R$ and average distortion at most $D$ . If the pair $(R,D)$ is achievable, then clearly, for $\\widetilde{R}\\geq R$ and $\\widetilde D\\ge D$ , the pair $(\\widetilde{R},\\widetilde{D})$ is also achievable. Thus, the quantity of interest to us is the low er  boundary  o f the set of achie v ab l e $(R,D)$ pairs. This is given by the distortion-rate function $D^{*}$ , which is defined as follows: the distortion-rate function at rate $R$ is the smallest distortion $D$ such that the pair $(R,D)$ is achievable, or equivalently, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D^{*}(R)\\triangleq\\operatorname*{inf}\\{D\\ge0\\mid(R,D)\\mathrm{~is~achievable}\\}}\\\\ &{\\qquad\\qquad=\\operatorname*{inf}\\{D\\ge0\\mid\\mathrm{there~exists~(enc,dec)~with~rate}\\le R\\mathrm{~and~distortion}\\le D\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the distortion-rate function depends on the source $\\mathsf{P}_{X}$ and the choice of distortion measure. Closed form expressions are known in some cases, the reader is encouraged to refer to classical texts on information theory [24, 26, 27] for examples. It is important to note that the distortion-rate function is a fundamental limit; no choice of encoder and decoder can give a lower rate and a lower distortion. Thus, the distortion-rate function characterizes the Pareto-optimal front of the trade-off between rate and distortion. It is more common in the information theory literature to define the ratedistortion function $R^{*}(D)$ , which is the smallest rate $R$ such that the pair $(R,D)$ is achievable. The two functions trace the same curve when plotted on the same two-dimensional plane. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Several variants of this problem can be defined by introducing the notion of side-information, where we have i.i.d. samples $(X_{1},Q_{1}),\\ldots,(X_{n},{\\dot{Q}}_{n})$ of a pair of correlated random variables $(X,Q)\\sim\\mathsf{P}_{X Q}\\in\\mathcal{P}(\\mathcal{X}\\times\\bar{\\mathcal{Q}})$ . A natural question to ask is what improvement is possible in terms of the rate-distortion trade-off for $X^{n}$ when either the encoder or decoder or both have access to this side-information $Q^{n}$ , which is correlated with $X$ . If only the encoder has access to $Q^{n}$ , then no improvement can be obtained. If both the encoder and decoder have access to $Q^{n}$ as shown in Fig. $\\mathrm{7c}$ and studied by Gray [48], then, clearly, an improvement is possible. Surprisingly, we can obtain nontrivial improvements when the decoder has access to $Q^{n}$ as shown in Fig. 7b and studied by Wyner and Ziv [31, 32]. ", "page_idx": 21}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/d4b36bdb73c0eef42daf214ac27d855ba9e856c413aa231f932bfa14f46337c4.jpg", "img_caption": ["(c) Side-information at the encoder and the decoder [48] (d) For function computation, $Z=f(X,Q)$ [34] ", "Figure 7: Rate-distortion models of compression. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "These models resemble our setups for query-aware and query-agnostic prompt compression respectively, with a key difference being that the decoder in our problem is the pretrained LLM, which is fixed. An rate-distortion setup that is closer to our problem in this sense is that of compression for function computation, introduced by [34]. Here, the goal is to recover an estimate ${\\hat{Z}}^{n}$ that is close to $Z^{n}$ , with $Z_{i}=f(X_{i},Q_{i})$ for some desired function $f$ . At first glance, it might appear that this setup is exactly our model for prompt compression, but this turns out to be false \u2014 the desired output is an estimate of $f(X,Q)$ , but the decoder can be designed to compute any arbitrary function of $M$ and $Q$ . In prompt compression, we have the constraint that the function computed by the decoder is fixed to be $\\phi_{\\mathsf{L L M}}$ , in addition to requiring that the output be close to some function of $X$ and $Q$ . Thus, our model for prompt compression actually corresponds to a rate-distortion problem for function computation with side-information with a fixed decoder, which has not been studied before, to the best of our knowledge. The distortion-rate function $D^{*}(R)$ for this setup is given by (LP) and (dual-LP). A closed form expression for $D^{*}(R)$ cannot be obtained without making further assumptions on $\\phi_{\\mathsf{L L M}}$ ; nonetheless, $D^{*}(R)$ can be computed for any $\\phi_{\\mathsf{L L M}}$ by solving Algorithm 1. ", "page_idx": 21}, {"type": "text", "text": "E Extension to natural language datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "As discussed in Sec. 4.2, we also use Algorithm 1 to compute the distortion-rate function for a small natural language dataset. The decisive bottleneck in running Algorithm 1 turns out to be obtaining $D_{x}$ for each $x\\in\\mathscr{X}$ , i.e., the input to the algorithm. We require one inference call for each possible   \nbweh earlle qisu tehnec evso coaf b $m\\in\\mathcal{M}_{x}$ h ael leLrL tMha. nT $D_{x}$ d, ewl eu sseede  itnh aotu rt heaxt ptehrei msi $x$ t so, f ails $\\mathcal{M}_{x}$ , $\\operatorname{len}(x)$ $\\mathcal{M}_{x}$ $\\textstyle\\sum_{i=1}^{\\log(x)-1}|\\gamma|^{i}$ $\\nu$ [39], has $|\\mathcal{V}|=32,000$ . Clearly, it is then virtually impossible to consider prompts with more than   \n2 tokens, and in fact, makes it difficult to consider even medium length prompts (50 tokens) for a vocabulary of size more than 2. ", "page_idx": 21}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/49e2b9e5f2dbf72192c09017a14f6ad0d87c4a431a44f8bfb487d065db38c312.jpg", "img_caption": ["Figure 8: Query-agnostic distortion-rate curves plotted for log loss and 0/1 loss distortion measures. The curves marked with a \u2018diamond\u2019 are computed using all possible shorter sequences, while those marked with an $\\mathbf{\\check{\\rho}}\\times\\mathbf{\\check{\\rho}}$ are computed using only pruned versions of the original prompt. They are nearly identical, which suggests that a good approximation to the optimal distortion-rate curve can be obtained by considering pruned prompts only. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "A key first step towards extending our algorithm for natural language prompts is the observation that all prompt compression methods in the literature work by pruning tokens, i.e., (1) they are non-generative, i.e., work by removing tokens from the original prompt and therefore do not generate any new tokens, and (2) they preserve the order of the tokens as they appear in the input sequence. Hence, to compute the fundamental limit for the schemes that compress the prompt by pruning, it is enough to consider $\\mathcal{M}_{x}$ to be the sequences that are obtained from $x$ by removing some number of (not necessarily contiguous) tokens. Then, we have $|\\mathcal{M}_{x}|=2^{\\log(x)}$ , irrespective of the vocabulary size $\\vert\\mathcal{V}\\vert$ . ", "page_idx": 22}, {"type": "text", "text": "In Fig. 8, we observe that the distortion-rate curves obtained by restricting $\\mathcal{M}_{x}$ to be only those sequences obtained from $x$ from via pruning, are nearly identical to the original curves, where we take $\\mathcal{M}_{x}$ to be all shorter sequences. This suggests two things: (1) there is no fundamental drawback to considering compression schemes are not generative, i.e., work by pruning the original prompt, and (2) we can approximate the optimal distortion-rate function reasonably well by considering only pruned versions of the prompt as possible compressed prompts. Thus, in principle, we can replicate experiments with natural language prompts of the same lengths (4 to 10) as the binary prompts in our experiments above, with the same computational cost. However, it is difficult to identify sufficiently rich natural language prompts of such short lengths for which compression is a reasonable problem, and hence, use binary prompts (generated from a Markov chain, to model the dependence between tokens) with natural language queries (since there is no computational restriction on the vocabulary of the queries) to run experiments such as those in Sec. 4.2 and App. F at scale. ", "page_idx": 22}, {"type": "text", "text": "Nonetheless, to illustrate that we can get meaningful results by considering such \u201cpruned\u201d compressed prompts, we generate a small natural language dataset using GPT-4 [38], as described in App. F.2.1. ", "page_idx": 22}, {"type": "text", "text": "To make sense of the computational complexity involved in computing the optimal distortion-rate curve, consider the following toy example: suppose that the vocabulary size is 10, and that the lengths of all prompts are 100. Then, the number of possible compressed prompts is of the order of $1\\bar{0^{100}}$ . When restricted to compressed prompts obtained by simply \u201cpruning\u201d the input, this number reduces to nearly $2^{100}$ , which is still large. However, since the length of the compressed prompt is at most 100, the number of points on the lower-left convex envelope in Algorithm 1 is at most 100. Hence, the major bottleneck is in computing the $D_{x}$ quantities, which require $10^{100}$ inference calls. Once these quantities are known, the complexity of Algorithm 1 itself is negligible, even for large vocabulary sizes and prompt lengths. One option is to approximate the optimal curve as best as possible while limiting the number of LLM inference calls made; we provide preliminary results in this direction in App. G.3. To exactly compute the optimal curve while avoiding the $10^{\\bar{1}00}$ inference calls requires some assumptions to be made about $\\phi_{\\mathsf{L L M}}$ and some structure on the generated outputs; we leave this for future work. ", "page_idx": 22}, {"type": "text", "text": "F Experiment details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "F.1 Synthetic data experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide additional details regarding our synthetic dataset and how we fine-tuned all models. Experiments were run on three different machines, two of which are identical machines with an AMD Ryzen Threadripper PRO 5975WX CPU (32 cores), 256 GB of system RAM, and $2\\mathbf{x}$ Nvidia RTX 4090 GPUs with 24 GB each. We also ran experiments on a DGX machine with an AMD EPYC 7742 64-Core Processor, 512 GB of system RAM, and 4x 80GB SMX4 A100 GPUs. The duration of LLM fine-tuning on the synthetic dataset varies, depending on the model being fine-tuned. In general, it takes 10 to 30 minutes for a single fine-tuning run. Running the code necessary to reproduce all plots takes several hours. ", "page_idx": 23}, {"type": "text", "text": "We use code from the LLMLingua and Selective Context GitHub repos, which are released under the MIT license. In our experiments, we use the following models: Mistral 7B Instruct v0.2 (Apache2.0), RoBERTa Base (MIT), and Pythia 1B deduped (Apache-2.0). ", "page_idx": 23}, {"type": "text", "text": "Each method requires a rate or threshold parameter $r$ , for which we use $r\\quad\\in$ $\\{0.04,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.96,0.99,1.0\\}$ in our experiments. However, the length of the returned compressed prompt might not be faithful to this rate parameter, so for each $r$ , we report the average rate and average distortion on the examples in our validation dataset. LLMLingua and LLMLingua Query have one additional parameter controlling the size of each segment the prompt is broken into before compressing each segment. We found that using a segment size of 2 works best for our synthetic dataset. ", "page_idx": 23}, {"type": "text", "text": "F.2 Natural language experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We train our models on the same DGX system used in the synthetic dataset experiments. Unlike the synthetic dataset experiments, however, we train these XLM RoBERTa Large (MIT license) models with the single precision (float32) data format and use full fine-tuning rather than LoRA. We observed non-negligible performance improvements with this configuration over bfloat16 with LoRA. As a result, training took one to two hours and $30\\,\\mathrm{GB}$ of VRAM on a single A100. We used the same set or rate and threshold parameters as done in the synthetic data experiments. ", "page_idx": 23}, {"type": "text", "text": "F.2.1 Small natural language dataset ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Solving for the optimal rate-distortion curve requires a substantial amount of compute as mentioned in App. E. To overcome this, we construct a synthetic dataset consisting of binary string prompts, natural language queries, and numerical and yes/no answers. We show a few examples of the validation partition of our synthetic dataset in Table 2. ", "page_idx": 23}, {"type": "table", "img_path": "TeBKVfhP2M/tmp/b72f73738de9d1a32cd18ed4b71d86e03c1459122a40c1db9ed4e6716a01a4cc.jpg", "table_caption": ["Table 2: One example of each query from the validation set of our synthetic dataset "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "We curated a small natural language dataset to generate results shown in Fig. 5 by prompting GPT-4 [38] to provide short natural language prompts of 15 tokens or less, provide four questions about each prompt, and give the answer. Afterward, we modified some of the questions and prompts slightly when the generated prompt by GPT-4 was too long or the questions and answers contained too much overlap with each other for a given prompt. In total, our dataset consists of ten prompts with four questions each. A few examples of our dataset are shown in Table 3. ", "page_idx": 23}, {"type": "table", "img_path": "TeBKVfhP2M/tmp/ace582968e1ff83dda68fd4fe2a452c15efa72696b82b80e75c6926c79886a56.jpg", "table_caption": ["Table 3: One example of each prompt from our natural language dataset "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "F.2.2 Choice of distortion metric ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The proper choice of distortion function is important to meaningfully measure the change in performance (distortion) of the LLM as the rate is varied. Ideally, a distortion metric where two texts with the \u201csame meaning,\u201d as would be determined by humans, achieve a \u201clow distortion\u201d is the gold standard, but this metric is unknown. This is a crucial open problem not just for our work but also for the fair benchmarking of LLMs in general. ", "page_idx": 24}, {"type": "text", "text": "For our results on natural language, we report our results using the following distortion metrics (or rather, \u201c1\u2212\u201d these, since these are similarity metrics and we want a low distortion to mean high similarity). rougeL [23] is a standard metric used to evaluate summaries, which does so by computing the precision and recall between the tokens in the generated text and the reference texts. In contrast, BertScore [45] computes contextualized embeddings for the generated tokens and reference tokens and uses the pairwise cosine similarity among them to produce the final score. The authors of the BertScore work highlight that their metric correlates better with human judgements than all other metrics (rougeL included). Regardless, our results with these two metrics are in agreement with each other, suggesting that a \u201cgood enough\u201d metric may be sufficient. A popular approach in current literature is to ask GPT4 to give a score on the similarity between generated and reference texts. Although it has been shown that humans agree with GPT4\u2019s evaluation more than the evaluation of other humans [49], we are skeptical of this metric because it is not reproducible, and GPT4 has biases that may result in unfair or inaccurate evaluations. Additionally, our theoretical framework is general and does not assume any specific distortion function. In particular, it can also be used with new distortion functions that better capture semantic notions when they are discovered. ", "page_idx": 24}, {"type": "text", "text": "F.3 LLM fine-tuning ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "F.3.1 Synthetic dataset ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Given that our synthetic dataset of binary prompts is not naturally in the distribution of training data of LLMs, we use Mistral 7B Instruct v0.2 [39] as our black-box model, and fine-tune it on tuples of (prompt, query, answer). This is also known as \u201cinstruction fine-tuning;\u201d we only compute the loss over the answer. ", "page_idx": 24}, {"type": "text", "text": "Each prompt compression method requires an LLM as part of its compression algorithm; we fine-tune Pythia 1B deduplicated [40] for Selective Context and LLMLingua-based methods. Selective Context and LLMLingua only use negative log-likelihood scores over the prompt, so for these methods we fine-tune with the next-word prediction over the prompts. For LLMLingua Query, we place the (query, prompt, answer) tuple into context and then perform next token prediction over the entire context. We place only the query and prompt into the context for the prompt compression step (inference time). ", "page_idx": 24}, {"type": "text", "text": "LLMLingua-2 methods require an additional label set for every prompt as ground-truth answers to teach the model to predict which tokens should be kept. For our dataset, gathering the labels for each prompt is deterministic if the query is known, so it is easy to assemble the label set for query-aware LLMLingua-2 methods. For example, for the query \u201cIs the binary string a palindrome?\u201d we can easily choose the shortest sequence of tokens from the input that is also a palindrome (if the answer is \u201cyes\u201d) as the ground-truth compressed prompt. For QuerySelect and Adaptive QuerySelect, both of which are query-aware, we put the query and prompt into context and then train the LLM to predict which tokens to keep using the constructed label set. This process is less straightforward for queryagnostic LLMLingua-2 since it is not clear how to assign the labels without the query. In this case, we choose the ground-truth compressed prompt to consist of the highest entropy tokens. Given the Markov chain from which our prompts were generated, these tokens contain the transitions between bits. For all LLMLingua-2 methods, we fine-tune RoBERTa Base [41]. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "We conduct a grid search over a set of hyperparameters before fine-tuning the final model used for each method. Specifically, we use the training set to fine-tune a model with all combinations of hyperparameters, evaluate the final performance on each model with a test set, and choose the combination of hyperparameters leading to the best performance. We then merge the train and test set and train with the chosen hyperparameters and do a final evaluation on the validation, which is the dataset used in the results of this paper. ", "page_idx": 25}, {"type": "text", "text": "All models are searched over the same learning rate $\\{5\\mathrm{e}{-}6,1\\mathrm{e}{-}5,5\\mathrm{e}{-}5,1\\mathrm{e}{-}4\\}$ , batch size {16, 32}, LoRA rank {16, 32, 64, 128}, and LoRA alpha {16, 32, 64, 128} hyperparameters. For the number of training epochs, we search over {1, 2, 4} for Mistral 7B Instruct v0.2 and Pythia 1B deduplicated, and {8, 12} for RoBERTa Base. ", "page_idx": 25}, {"type": "text", "text": "We report our final set of hyperparameters used to fine-tune the LLM used for each prompt compression method in Table 4. ", "page_idx": 25}, {"type": "table", "img_path": "TeBKVfhP2M/tmp/97af66061d873869b0a1769ec78beefaa0beba753a8833333136b389364cc607.jpg", "table_caption": ["Table 4: Final set of hyperparameters used to train the LLM used in each prompt compression method. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "F.3.2 Natural language dataset ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We use fine-tuned models available on Hugging Face for the Selective Context, LLMLingua, LLMLingua Query, and LLMLingua-2 prompt compression methods; only QuerySelect and Adaptive QuerySelect, our novel methods, require specialized fine-tuning starting from a pretrained XLM RoBERTa Large [41] model. To fine-tune these models, we modify the LLMLingua-2 training code available in the LLMLingua GitHub repository to accept (prompt, query) pairs as input and classify which tokens of the prompt should be kept. Since the query uses additional context, and since the max context window of XLM RoBERTA Large is 512 tokens, we shrink the window size dedicated to the prompt to 384 tokens. We used the same hyperparameters used in the LLMLingua-2 paper [14] for fine-tuning, i.e., learning rate $_{1\\mathrm{e-5}}$ , batch size 10, epochs 10, and the Adam optimizer with $\\beta_{1}=0.9,\\beta_{2}=0.999$ , $\\epsilon=\\scriptstyle1\\mathbf{e}-8$ , and weight decay 0. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "The dataset used to train our models is a filtered version of Databricks Dolly 15k [44]. Our final filtered dataset contains samples that meet the following conditions: (1) the context is non-empty, (2) the context is between 50 and 5000 characters in length, and (3) the instruction has fewer than 360 characters. After filtering, the dataset contains $4.35\\mathbf{k}$ samples. The contexts of this dataset are chunked into windows of 384 tokens, and we prompt GPT-4 to compress the context by asking it only to keep the tokens necessary for responding to the provided instruction. This allows us to construct a label set for the token classification problem of choosing which tokens to keep or remove. Our dataset used for training consists of the instructions (queries), chunked contexts (prompts), and the set of labels for the contexts. $95\\%$ of the samples in this dataset are used in the training dataset, and the remaining $5\\%$ form the validation set. Table 5 shows the prompt we used for GPT-4, which is a modified version of the prompt used in the LLMLingua-2 paper. All of our code used for constructing the training dataset and training the models are modified from the LLMLingua GitHub repo. Our modified code is available in the code release. ", "page_idx": 26}, {"type": "text", "text": "Table 5: Our prompt for GPT-4 to determine which tokens to remove. This prompt was used to construct the dataset for training QuerySelect and Adaptive QuerySelect on natural language. This is a slight modification from the prompt used in the LLMLingua-2 work [14]. ", "page_idx": 26}, {"type": "table", "img_path": "TeBKVfhP2M/tmp/eab9a395f6dbf6eba622f18ec045fba98fa8b184cbf38fea5a3f7fd1fd3eef12.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "G Additional experimental results ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "G.1 Small-scale datasets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We provide the remainder of our empirical results below. Fig. 9 is similar to Fig. 1, but shows the result when the prompt compression method uses standard tokenization rather than forced tokenization. Fig. 10 shows a direct comparison between the trade-off for methods using standard and forced tokenization. Fig. 11, Fig. 12, Fig. 13, and Fig. 14 show the rate-distortion trade-off curves for each of the seven queries in our synthetic dataset. Fig. 11 shows forced tokenization with 0/1 loss, Fig. 12 shows forced tokenization with log loss, Fig. 13 shows standard tokenization with 0/1 loss, and Fig. 14 shows standard tokenization with log loss. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/4187f03e55a87e8d2511a73649d0d2aaa1b3462eabb5832a3c6a358e9b4aeb1e.jpg", "img_caption": ["Figure 9: The distortion-rate curves of all prompt compression methods and the optimal strategy attained by solving our dual LP formulation when standard tokenization is used for the prompt. All methods are compared with the log loss (left) and 0/1 loss (right) metrics. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/e05cc778977857573ee5ae22608937b679d3a1a18be9534309c85b6ee3385e14.jpg", "img_caption": ["Figure 10: Performance comparison when standard tokenization (left) and forced tokenization (right) is used on our synthetic dataset. Interestingly, the optimal performance is nearly equivalent between the two, and, for a given rate, methods with standard tokenization match or improve upon the performance of a method that forced separate tokenization of every bit. However, standard tokenization results in compression of 1 to 4 tokens on our dataset, whereas forced tokenization compresses up to 10 tokens, allowing for a greater range of rates. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "G.2 NarrativeQA ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We compare existing methods on the NarrativeQA [18] dataset. Specifically, we use the summaries from the dataset as the prompt, and use the query and answer as given. Since the sizes of the prompts are hundreds of tokens and approximately 3,500 samples, it is not feasible to compute the optimal ratedistortion curve for this dataset. Instead, Fig. 16 showcases the performance of our proposed methods over existing methods. In particular, Fig. 16 shows the same conclusion as Fig. 5: our proposed methods are better for rates less than 0.5 and remain competitive for rates about 0.5. Table 6 shows the average time required to compress a prompt on the NarrativeQA dataset and our curated small-scale NLP dataset. Since our methods are adapted from LLMLingua-2, our methods share the same timings. ", "page_idx": 27}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/e250e7d2f1807fcb7a26a484a3a79b20810bed3c53c4759c17b421b6074e3a0e.jpg", "img_caption": ["Figure 11: The rate-distortion trade-off of all methods on each individual query for forced tokenization and $0/1$ loss. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/d5b3ac028975fede434419805e0aeb7d9d5c39563e6feae2677a65de3db15cb0.jpg", "img_caption": ["Figure 12: The rate-distortion trade-off of all methods on each individual query for forced tokenization and log loss. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/c68891ac0f5390aac7abe0bd8294b3238667ede781a2162f557a81ca306bc13b.jpg", "img_caption": ["Figure 13: The rate-distortion trade-off of all methods on each individual query for standard tokenization and $0/1$ loss. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/7f1118b4e98d1c6f36188d855dbf918eb026bb6170bf7612bef2a0ebda6f681e.jpg", "img_caption": ["Figure 14: The rate-distortion trade-off of all methods on each individual query for standard tokenization and log loss. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/3801659430e3a43ec73cb971c4464d6b5d09fa990001959ed62d00a166b002d7.jpg", "img_caption": ["Figure 15: Histograms of the rates for QuerySelect and Adaptive QuerySelect for a given average rate from the left-side figure of Fig. 5. These figures show that Adaptive QuerySelect has a larger spread of rates across the samples of the natural language dataset. In particular, Adaptive QuerySelect has greater flexibility in choosing an appropriate rate for a given (prompt, query) pair. "], "img_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "TeBKVfhP2M/tmp/05729222be79799087774d58730bd7ab2016af3264159eb04bc6528f38922167.jpg", "table_caption": ["Table 6: Average time required to compress a single prompt (seconds). "], "table_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/46bd055c69170c8ae971f2d4bf15484af312c8348fa84dd3e056c375613fb3b1.jpg", "img_caption": ["Figure 16: Comparison among all prompt compression methods on the NarrativeQA [18] summaries dataset. We show the rate-distortion trade-off for RougeL [23] (left) and BertScore [45] (right). Since a higher RougeL and BertScore metric is better, we plot $^{\\bullet}1-$ the computed average distortion\u201d so that a higher rate should yield a lower loss. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "G.3 NarrativeQA Beam Search ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "As discussed in the Conclusion and App. E, computing the optimal rate-distortion curves for largescale datasets, where the prompts consist of hundreds or thousands of tokens, is intractable. The difficulty lies in computing $D_{x}$ , which requires an inference call for every possible compressed prompt for a given prompt. Even for our curated small-scale dataset, whose largest prompt consists of 15 tokens, it is not feasible to compute the \u201ctrue\u201d optimal curve outright. Instead, we considered the set of compressed prompts constructed from in-place token removal from the original prompt, which significantly reduced the number of inference calls in constructing $D_{x}$ from len(x)\u22121|V|i to $2^{\\mathrm{len}(x)}$ , where $\\vert\\protect\\nu\\vert$ is the size of vocabulary of the tokenizer and $x\\in\\mathscr{X}$ is the pro mpt. While this approach does not yield the \u201ctrue\u201d optimal curve, it does provide an optimal curve to all existing prompt compression methods since they all strategically remove tokens in place. Thus, we can still establish the fundamental limit of existing methods! ", "page_idx": 33}, {"type": "text", "text": "However, when $\\operatorname{len}(x)$ is a few hundred or thousand, as is the case for large-scale datasets, $2^{\\mathrm{len}(x)}$ is far too many inference calls to make $D_{x}$ to be practical. Ideally, one can approximate the optimal curve by finding an upper bound; to do this, we use beam search. Our search space is over all ${\\bar{2}}^{\\mathrm{len}(x)}$ binary masks (a binary mask, when applied to the prompt, forms a compressed prompt). Each mask is assigned a distortion value by computing the distortion between the ground truth answer and the generated answer when the black-box LLM is given the mask\u2019s associated compressed prompt and the query. Thus, we can construct the search tree over the binary masks and use beam search to find masks with low distortion. ", "page_idx": 33}, {"type": "text", "text": "The search tree begins with the \u201call ones\u201d mask at the root node $(l=0)$ ); each of the root node\u2019s children $\\left.\\left.l\\right.=1\\right.$ ) contains a bit filpped to 0 in precisely one of the $\\operatorname{len}(x)$ positions. At the third level $(l=2)$ ), each node inherits a 0 at the same position as its parent and then filps a 1 to a 0 so that each mask at $l=2$ has precisely two 0s. This pattern continues throughout the rest of the tree, resulting in masks with ${l}~0\\mathrm{s}$ at level $l$ . Furthermore, the branching factor $F_{l}$ of our beam search is the number of children nodes a given node has at level $l$ , is $N-l,$ At level $l$ , accounting for all nodes that are shared among the parent nodes in level $l-1$ , there are $\\binom{N}{l}$ nodes, where $N=\\operatorname{len}(x)$ . As a sanity check, this tree contains kN=0 Nk = 2len(x) nodes in this tree, so all binary masks of length N are in the tree. We can sear ch every node in the tree as long as the beam width $B$ is large enough to contain every node at the broadest level of the tree. To achieve this, we need to set $B=\\overline{{\\operatorname*{max}_{l}\\binom{\\check{N}}{l}}}=\\binom{N}{\\lfloor N/2\\rfloor}$ ", "page_idx": 33}, {"type": "text", "text": "Recall that our purpose for using beam search is to drastically reduce the number of LLM inference calls required for constructing $D_{x}$ while retaining a competitive upper bound to the optimal curve. The cost $C$ associated with the beam search, which is the number of nodes visited in the tree (i.e., the number of LLM inference calls) is ", "page_idx": 33}, {"type": "image", "img_path": "TeBKVfhP2M/tmp/a06341124261861803d3dcee97cffa5eb0f845b2cb0f9536f6752bf01a7b7504.jpg", "img_caption": ["Figure 17: Comparison between existing prompt compression methods (replicated here from Fig. 16) and our approximation to the optimal rate-distortion curves via beam search on NarrativeQA. "], "img_footnote": [], "page_idx": 34}, {"type": "equation", "text": "$$\nC=B\\sum_{l=0}^{N}F_{l}=B\\sum_{l=0}^{N}(N-l)=B\\sum_{k=0}^{N}k={\\frac{B N(N+1)}{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that, in this case, we search over $C<|\\mathcal{M}_{x}|$ in (LP) compressed prompts, so beam search will provide an upper bound. When $N$ is large, checking $\\mathcal{O}(B N^{2})$ nodes is a drastic reduction over checking $2^{N}$ nodes, but the growth rate is still too large. For example, running beam search on a single prompt of just 100 tokens will require on the order of $10^{6}$ LLM inference calls, or roughly 11.5 days if assuming a single inference call takes one second. To further reduce the number of checked nodes, we can chunk the binary masks into spans of bits, and flip entire spans from 1 to 0 at each level. With this approach, we can effectively control the length of the binary masks we search over, which we call $N_{\\mathrm{eff}}$ , for a given budget/cost $C$ . Solving (10) for $N$ , we arrive at ", "page_idx": 34}, {"type": "equation", "text": "$$\nN_{\\mathrm{eff}}=\\frac{-1+\\sqrt{1+\\frac{8C}{B}}}{2}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "This approach allows us to choose the total number of LLM inference calls $C$ per prompt that we will spend on beam search for a particular beam width $B$ . Since we are chunking the binary mask into spans and then masking the spans as we search, we have reduced the search space and the resulting tree will be smaller than the $N_{\\mathrm{eff}}=N$ case. ", "page_idx": 34}, {"type": "text", "text": "Fig. 17 compares existing methods to our beam search upper bounds for $C=4000$ and $B\\,=\\,5$ . Although LLMLingua-2-based methods can outperform the upper bound in the query-agnostic case for most rates, the query-agnostic beam search approach shows room for improvement from existing methods. Impressively, the gap between existing methods and the query-aware case is quite large, even for the methods that use the query to compress the prompt. ", "page_idx": 34}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We clearly mention the assumptions under which we present our results. ", "page_idx": 35}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide a section detailing the limitations of our approach and potential approaches to remedy them in App. E. ", "page_idx": 35}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our theoretical results in the main paper are Proposition 1 (proved in the main paper) and Thm. 1 (proved in C.1, with proof sketch in the main paper); both are stated with all required assumptions. We also provide an informal proof in the main paper that Algorithm 1 computes the distortion-rate function; this is made formal in App. C.2. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We provide our code with details on how to reproduce our results as supplementary material, which will be made publicly available upon acceptance. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our code is available at https://github.com/acnagle/fundamentallimits, with instructions to reproduce results as in the paper. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: All details of our experimental setup can be found in Sec. 4.2 and App. F. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 35}, {"type": "text", "text": "Justification: Our experiments are not statistical in nature. All our plots are deterministic and can be reproduced using the code provided in the supplementary material. ", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Information on compute resources can be found in App. F. ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not use human subjects or private data, our work has no direct societal impact, and we appropriately reference all code that we have used for our experiments in Sec. 4.2. ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work is foundational research, and though it has applications to the practical problem of prompt compression, our focus is on understanding its fundamental limits. There are no direct societal impacts of our work. ", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: We do not release any new data or models. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: All assets for which we are not the creators, including some code and models, are properly cited and available under either the Apache-2.0 or MIT licenses. This information is provided in App. F. ", "page_idx": 36}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No new assets are introduced. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No human subjects were involved. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: No human subjects were involved. ", "page_idx": 36}]