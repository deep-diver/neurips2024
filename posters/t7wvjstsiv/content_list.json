[{"type": "text", "text": "SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jianyi Zhang1, Da-Cheng Juan2, Cyrus Rashtchian2, Chun-Sung Ferng2, Heinrich Jiang2, Yiran Chen1 ", "page_idx": 0}, {"type": "text", "text": "1 Duke University, 2 Google Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to $20\\%$ compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) have achieved remarkable breakthroughs in recent years, demonstrating exceptional performance across various domains [1, 2, 31, 32, 39, 42, 43]. However, a significant challenge associated with LLMs is their tendency to hallucinate or distort the truth, resulting in outputs that are not factual [15, 16, 62]. This issue of hallucination undermines the reliability and trustworthiness of LLMs in practical applications. A popular strategy for improving the LLM factuality involves refining the decoding process [38, 48]. ", "page_idx": 0}, {"type": "text", "text": "Decoding focuses on how the model selects the next token during the generation process, which can significantly influence the factual accuracy of the output. The decoding methods can be cost-effective since (a) they do not rely on external knowledge and (b) no additional training is required. Furthermore, decoding methods can be synergistically combined with other techniques aimed at improving the LLM factuality, such as retrieving information from external knowledge bases [21, 22], various fine-tuning strategies for better alignment [41, 43], or ensemble learning methods [9]. ", "page_idx": 0}, {"type": "image", "img_path": "t7wvJstsiV/tmp/74aa5804b35d85ece4008ae47bdaf0de8b07ed7a87c652856741156d84e1ed0f.jpg", "img_caption": ["Figure 1: Factuality decoding overview. "], "img_footnote": [], "page_idx": 0}, {"type": "image", "img_path": "t7wvJstsiV/tmp/9226902dae7ac4f7857ad67fec709462333d4e413ad5445fde26ef4dedd87787.jpg", "img_caption": ["Figure 2: Illustration of our Self Logits-Evolution Decoding (SLED) workflow. ", "The capital of British Columbia province is "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Recent studies [19, 23, 37, 45] suggest that LLMs sometimes have learned the factual content based on extensive pretraining or fine-tuning, although they fail to produce the correct answer when a user queries the model. This has inspired the development of several factuality decoding methods [6, 23, 24, 61] to reveal what the model implicitly \"knows.\" Figure 1 summarizes the underlying mechanism of these factuality decoding methods. The LLMs\u2019 output distribution is derived by applying the softmax function to the output logits from the final layer. During the training phase, this distribution is optimized based on the real-world factuality distribution represented by the training dataset. However, during the inference phase, \"what the LLM tells\" might still contain factual errors, which implies a discrepancy between the output distribution and the real-world factuality distribution. While the real-world distribution remains inaccessible during the inference phase, the model\u2019s latent knowledge (\"what the model knows\") may have implicitly learned some factual content correctly during the training phase [19, 45]. Therefore, a key challenge for factuality decoding strategies lies in effectively harnessing the latent knowledge embedded within LLMs to refine the output distribution (logits) during inference. ", "page_idx": 1}, {"type": "text", "text": "To address this challenge, we propose Self Logits Evolution Decoding (SLED), a novel factuality decoding approach that leverages the latent knowledge within LLMs by contrasting the final layer\u2019s logits with early layers\u2019 logits. During the decoding process, as LLMs progress from early to final layers, they progressively incorporate factual information stored in each layer into the output. SLED tracks this evolution process to unearth the latent knowledge within LLMs, and enables the \u201cselfevolution\u201d of the output distribution further to align it more closely with real-world facts. Furthermore, our approach recognizes that the latent knowledge within LLMs, while valuable, may not always be perfect. Therefore, instead of simply replacing the original outputs with this latent knowledge, SLED integrates it into the original logits through an operation similar to \u201csingle-step gradient descent\u201d over the output logits during the inference time. This operation minimizes the Kullback-Leibler (KL) divergence between the latent knowledge distribution and the output distribution, effectively balancing the two and mitigating potential drawbacks such as overfitting or biased outputs. Figure 2 illustrates the SLED workflow, highlighting how SLED optimizes the output logits, leading to a more factual output distribution. We evaluate SLED on various LLMs (e.g., LLaMA 2 [43], LLaMA 3 [1], Gemma [28]) and benchmarks to demonstrate its state-of-the-art performance in layer-wise contrastive decoding methods. In summary, our main contributions are: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose SLED, a novel decoding method that aligns LLMs outputs with factual knowledge without requiring an external knowledge base or fine-tuning data.   \n\u2022 We conduct extensive experiments across a range of LLMs, with varying configurations and scales. The results demonstrate that SLED consistently improves factual accuracy on various tasks and benchmarks, including multiple-choice, open-ended generation, and chain-of-thought reasoning tasks.   \n\u2022 SLED can be flexibly integrated with other factuality decoding methods to enhance their effectiveness further.   \n\u2022 We provide a new interpretable perspective for understanding layer-wise contrastive decoding methods, paving the way for further developments in factuality decoding. ", "page_idx": 1}, {"type": "image", "img_path": "t7wvJstsiV/tmp/aa4ab680ec0097312b15a556a98dad40efe85c0bb2d54e8e5124df2e398d596e.jpg", "img_caption": ["Figure 3: We analyze the next-token predictions of three LLaMA-2-base models using the logits from each layer individually. This analysis is performed on 200 true claims from the FACTOR dataset. The results verify that the logits distribution at the final layer is closer to the real-world distribution than all the early layers in terms of KL divergence. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "2 Self Logits Evolution Decoding ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A large language model, equipped with $N$ layers and a vocabulary $\\mathcal{V}=[v_{1},v_{2},...,v_{d}]$ , typically generates text in the next-token prediction fashion. For each given prefix, the model computes the logits at the final ( $N$ -th) layer, $l o g i t s_{N}\\triangleq(\\ell_{(1,N)},\\ell_{(2,N)},\\ldots,\\ell_{(d,N)})$ , which are obtained by applying a linear transformation to the hidden states of the final layer, projecting the high-dimensional hidden state vectors onto the space of the vocabulary size. Subsequently, the output distribution $\\mathcal{P}_{l o g i t s_{N}}$ at the final ( $N$ -th) layer for the next token is derived by applying softmax function on the logits, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}_{l o g i t s_{N}}\\triangleq\\left(p_{\\left(1,N\\right)},\\ldots,p_{\\left(d,N\\right)}\\right)=s o f t m a x\\left(l o g i t s_{N}/\\tau\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\tau$ is the temperature parameter. Therefore, for each $p_{(i,N)}$ $(1\\leq i\\leq d)$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{(i,N)}=\\exp(\\ell_{(i,N)}/\\tau)/S,\\mathrm{~where~}\\,S={\\sum}_{j=1}^{d}\\exp(\\ell_{(j,N)}/\\tau).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Similarly, we can also derive the logits from early layers by applying the same linear transformation mentioned above to their hidden states. For any early layer $n$ $(n<N)$ , we denote its logits as $l o g i t s_{n}\\triangleq(\\ell_{(1,n)},\\ldots,\\ell_{(d,n)})$ and the corresponding distribution as $\\mathcal{P}_{l o g i t s_{n}}\\triangleq(p_{(1,n)},...\\,,p_{(d,n)})$ . ", "page_idx": 2}, {"type": "text", "text": "2.1 Logits Evolution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To improve factual accuracy, it is crucial that the correct token $v_{i}$ receives a higher value of logits $N$ to ensure a higher probability value $p_{(i,N)}$ in the output distribution $\\mathcal{P}_{l o g i t s_{N}}$ . From a mathematical perspective, this means aligning the model\u2019s output distribution $\\mathcal{P}_{l o g i t s_{N}}$ closely with the real-world factuality distribution $\\mathcal{P}_{r e a l}$ . Specifically, we can formulate this goal as optimizing the following loss function $\\mathcal{L}$ regarding the logits: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(l o g i t s)\\triangleq K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s}),\\,\\mathrm{where}\\ l o g i t s=(\\ell_{1},...,\\ell_{d}),\\,\\mathcal{P}_{l o g i t s}=s o f t m a x(l o g i t s/\\tau)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We describe the above optimization as Logits Evolution. Interestingly, the training of LLMs also aims at minimizing the divergence (typically the $K L$ divergence, as the training loss function is often the cross-entropy loss) between the ground truth $\\mathcal{P}_{r e a l}$ and the output distribution $\\mathcal{P}_{l o g i t s_{N}}$ . During the training phase, the logits evolution is driven externally by the real-world distribution $\\mathcal{P}_{r e a l}$ presented in the training dataset, and the corresponding solution is $l o g i t s\\,=\\,l o g i t s_{N}$ . However, $\\mathcal{P}_{r e a l}$ is not accessible during the inference phase. To address this challenge, SLED utilizes the model\u2019s latent knowledge to estimate $\\mathcal{P}_{r e a l}$ and enables \"self-evolution\" of the logits. We denote the estimation as $\\mathcal{P}_{l a t e n t}$ and the self logits evolution can be achieved by the following gradient-descent operation: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widetilde{l o g i t s}_{N}=l o g i t s_{N}-\\alpha\\cdot\\nabla_{l o g i t s_{N}}K L(\\mathcal{P}_{l a t e n t},\\mathcal{P}_{l o g i t s_{N}}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The parameter $\\alpha$ , termed the Evolution Rate, governs the magnitude of adjustments applied to logits $N$ in the direction of the gradient $\\nabla_{l o g i t s_{N}}K L(\\mathcal{P}_{l a t e n t},\\mathcal{P}_{l o g i t s_{N}})$ . In the following Section 2.2 and 2.3, we discuss how we derive the $\\mathcal{P}_{l a t e n t}$ as the estimation of the real-world distribution $\\mathcal{P}_{r e a l}$ . ", "page_idx": 2}, {"type": "text", "text": "The core principle of our method involves leveraging the difference between each early layer\u2019s logits and the final layer\u2019s logit, $\\it{l o g i t s}_{n}-\\it{l o g i t s}_{N}$ to approximate the gradient of $K L(\\mathcal{P}_{r e a l}^{\\'},\\mathcal{P}_{l o g i t s})$ at $l o g i t s=l o g i t s_{n}$ . Then we estimate $\\mathcal{P}_{r e a l}$ based on this approximation. ", "page_idx": 3}, {"type": "text", "text": "This is inspired by a new perspective of interpreting the training phase of LLMs as the evolution of logits described in Problem 1. As mentioned above, the solution derived by the training phase is the final layer\u2019s logits $l o g i t s=l o g i t s_{N}$ , since the final layer\u2019s logits $N$ directly engage with the real-world distribution $\\mathcal{P}_{r e a l}$ through the loss function in training. This implies that we can generally consider the final logits logits $^{\\prime}N$ to be a better solution than the logits from an early layer logitsn, with $K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s_{N}})<K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s_{n}})$ . We present some examples in Figure 3 to demonstrate this. Based on this discussion, if we contrast the final layer\u2019s logits with the early layer\u2019s logits, we can consider the direction (orientation) of l $\\slash{o g i t s}_{n}-l o g i t s_{N}$ can approximately align with the direction of the gradient $\\nabla_{l o g i t s}K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s})|_{l o g i t s=l o g i t s_{n}}$ . To further verify this motivation, we calculate the cosine similarity between log $\\d_{i}{t}\\d_{s}_{n}\\mathrm{~-~}\\d_{l o g i t s}_{N}$ and $\\nabla_{l o g i t s_{n}}K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s_{n}})$ for thousands of tokens across different models in Figure 7. We find that the majority of these values are positive, which means that the directions of these two vectors are close. ", "page_idx": 3}, {"type": "text", "text": "aHnedn cdee,r ifvoer  tehaec ltao yeesrt $n$ , awtee  tphreo .to maximize the following function of cosine similarity $\\bar{\\mathcal{P}_{l a t e n t}^{(n)}}$ $\\mathcal{P}_{r e a l}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{P}_{l a t e n t}^{(n)}=\\arg\\operatorname*{max}_{\\mathcal{P}}\\left(\\mathrm{CosSim}(l o g i t s_{n}-l o g i t s_{N},\\nabla_{l o g i t s_{n}}K L(\\mathcal{P},\\mathcal{P}_{l o g i t s_{n}})\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "2.3 Achieving the Self Logits Evolution in Three Phases ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Based on the above analysis, we can introduce the procedures of SLED: First we estimate $\\mathcal{P}_{l a t e r}^{(n)}$ t for each early layer using the gradient approximation in Section 2.2. Subsequently, we apply a weighted average on {Pl(ant)en across all early layers $n<N$ to derive $\\mathcal{P}_{l a t e n t}$ , which serves as the final estimation of the real-world distribution. Finally, we apply $\\mathcal{P}_{l a t e n t}$ in Equation 2 to facilitate the self-evolution of logits $N$ , thereby derive the updated logits, $\\overleftarrow{l o g i t s}_{N}$ . ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{l o g i t s_{n}-l o g i t s_{N}\\stackrel{\\mathrm{in\\,direction}}{\\approx}\\nabla_{l o g i t s_{n}}K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s_{n}})\\qquad\\qquad\\qquad\\qquad}\\\\ {\\frac{\\mathrm{Phase\\,1}}{\\mathrm{Estimate}}\\,\\stackrel{\\mathrm{P}(n)}{\\sim}\\,\\stackrel{\\mathrm{Phase\\,2}}{\\mathrm{P}_{l a t e n t}}\\,\\stackrel{\\mathrm{Phase\\,2}}{\\mathrm{Ensemble}}\\,\\mathcal{P}_{l a t e n t}\\,\\xrightarrow[\\mathrm{Self-evolution\\,in\\,Eq\\,2}]{\\mathrm{Phase\\,3}}\\widehat{l o g i t s}_{N}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Phase 1: An exhaustive search for an exact solution to the complex optimization problem (Equation 3) is computationally impractical. We can reduce the solution space by the following. Suppose the real-world factuality distribution dictates that the next word to be generated is the $i$ -th token $v_{i}$ from the vocabulary $\\nu$ . Thus $\\mathcal{P}_{r e a l}=\\mathcal{P}_{e_{i}}$ , where $\\mathcal{P}_{e_{i}}$ represents a standard basis vector (one-hot vector) with the $i$ -th component set to 1 and all other components set to 0. Then, we can simplify the aforementioned optimization problem by limiting the solution space to $\\{\\mathcal{P}_{e_{i}}\\}_{i=0}^{d}$ and decide which token should be selected. The corresponding gradient when $\\mathcal{P}=\\mathcal{P}_{e_{i}}$ has the following formulation. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. The gradient of $K L(\\mathcal{P}_{e_{i}},\\mathcal{P}_{l o g i t s})$ at l $o g i t s=l o g i t s_{n}$ is: ", "page_idx": 3}, {"type": "equation", "text": "$\\begin{array}{r}{\\nabla_{l o g i t s_{n}}K L(\\mathcal{P}_{e_{i}},\\mathcal{P}_{l o g i t s_{n}})=(\\mathcal{P}_{l o g i t s_{n}}-\\mathcal{P}_{e_{i}})/\\tau=\\left(p_{(1,n)},\\ldots,p_{(i,n)}-1,\\ldots,p_{(d,n)}\\right)/\\tau}\\end{array}$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We calculate the cosine similarity between the gradient $\\nabla_{l o g i t s_{n}}K L(\\mathcal{P}_{e_{i}},\\mathcal{P}_{l o g i t s_{n}})$ and the difference $l o g i t s_{n}-l o g i t s_{N}$ for each token in the vocabulary $\\nu$ . Then we select the $\\boldsymbol{\\mathcal{P}}_{e_{i^{*}}}$ of which the gradient is closest to logitsn \u2212logitsN as the estimation Pl(ant)en . Mathematically, this involves selecting $i^{*}$ according to the following criterion ", "page_idx": 3}, {"type": "equation", "text": "$\\arg\\operatorname*{max}_{1\\leqslant i<d}m_{i}^{(n)},\\mathrm{~where~}m_{i}^{(n)}=\\operatorname*{max}\\left(\\mathrm{CosSim}(l o g i t s_{n}-l o g i t s_{N},\\mathcal{P}_{l o g i t s_{n}}-\\mathcal{P}_{e_{i}}),0\\right),$ ", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and adopting Pl(ant)en $\\mathcal{P}_{l a t e n t}^{(n)}=\\mathcal{P}_{e_{i^{*}}}$ as the \"hard estimation\" of $\\mathcal{P}_{r e a l}$ . Drawing from the concept of hard and soft targets in label smoothing and knowledge distillation, we further extend it to the \"soft estimation\", $\\mathcal{P}_{l a t e n t}^{(n)}=(m_{1}^{(n)},...\\,,m_{i}^{(n)},...\\,,m_{d}^{(n)})/m^{(n)}$ , where $m^{(n)}=\\sum_{i=1}^{d}m_{i}^{(n)}$ is a normalization factor. Prior studies have shown that soft targets usually offer stronger generalization capabilities, carry more information, and are more robust to noise than hard targets [13, 30, 35, 40, 54, 58]. Hence, we also adopt the soft estimation in lieu of the hard estimation. ", "page_idx": 3}, {"type": "image", "img_path": "t7wvJstsiV/tmp/429c85922e4652ee365f0096a3cd3e6b86dffc39356efbdf3db11baa44fd0dc8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 4: An example from GSM8K demonstrating SLED\u2019s mechanism. SLED derives the estimations P(n) by contrasting final layer\u2019s logits logits $N$ with early layers\u2019 logits $\\{l o g i t s_{n}\\}$ . We list the token with the highest probability value from the Pl(ant)en for different early layers. As shown, SLED downplays incorrect tokens by assigning lower weights s(n) to the corresponding Pl(ant)en t. Conversely, if the estimation is correct, the weights are relatively larger. The parameter evaluation scale is set to 2. ", "page_idx": 4}, {"type": "text", "text": "Phase 2: We ensemble Pl(ant)en t across all layers by computing a weighted average of the set $\\{\\mathcal{P}_{l a t e n t}^{(n)}\\}$ and adopt it as the final estimation of the $\\mathcal{P}_{l a t e n t}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{P}_{l a t e n t}=\\sum_{n=0}^{N}s^{(n)}\\mathcal{P}_{l a t e n t}^{(n)},\\mathrm{~where~}s^{(n)}=m^{(n)}/(\\sum_{n=0}^{N}m^{(n)})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This estimation suggests that the weight $s^{(n)}$ of certain layer $n$ will be larger if the corresponding gradient approximation $l o g i t s_{n}\\,-\\,l o g i t s_{N}$ is more closely aligned with the gradients $\\{\\mathbf{\\dot{V}}_{l o g i t s_{n}}\\}\\tilde{K}L(\\mathbf{\\mathcal{P}}_{e_{i}},\\mathbf{\\mathcal{\\dot{P}}}_{l o g i t s_{n}}^{\\texttt{c}})\\}$ for the tokens in the vocabulary. This in turn amplifies the influence of layer $n$ on the final estimation, which is a desirable effect in our method. Figure 4 demonstrates that SLED can downplay incorrect tokens based on the gradient alignment. One can further validate that for each component $m_{i}$ in the final estimation $\\mathcal{P}_{l a t e n t}\\;\\triangleq\\;\\left(m_{1},m_{2},...\\,,m_{d}\\right)$ , the following relationship holds: $\\begin{array}{r}{m_{i}=\\sum_{n=0}^{N}m_{i}^{(n)}/(\\sum_{n=0}^{N}\\sum_{j=1}^{d}m_{j}^{(n)})}\\end{array}$ . This property simplifies the description in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Phase 3: Applying $\\mathcal{P}_{l a t e n t}$ in Equation 2 enables us to derive the gradient necessary for steering the self-evolution on the final layer\u2019s logits logits $N$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 2. The gradient of $K L(\\mathcal{P}_{l a t e n t},\\mathcal{P}_{l o g i t s})$ at log $i t s=l o g i t s_{N}$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{l o g i t s_{N}}K L(\\mathcal{P}_{l a t e n t},\\mathcal{P}_{l o g i t s_{N}})=(\\mathcal{P}_{l o g i t s_{N}}-\\mathcal{P}_{l a t e n t})/\\tau=\\left(p_{(1,N)}-m_{1},\\ldots,p_{(d,N)}-m_{d}\\right)/\\tau}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then we can derive the self-evolved logits $\\widetilde{l o g i t s}_{N}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{l o g i t s}_{N}\\triangleq(\\widetilde{\\ell}_{(1,N)},\\ldots,\\widetilde{\\ell}_{(i,N)},\\ldots,\\widetilde{\\ell}_{(d,N)}),\\mathrm{~where~}\\widetilde{\\ell}_{(i,N)}=\\ell_{(i,N)}-\\alpha(p_{(i,N)}-m_{i})/\\tau.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "2.4 Computational Complexity and Design Decisions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For each layer, computing $\\mathrm{CosSim}(l o g i t s_{n}\\textrm{--}l o g i t s_{N},\\mathcal{P}_{l o g i t s_{n}}\\textrm{--}\\mathcal{P}_{e_{i}})$ for every token $v_{i}$ in the vocabulary $\\nu$ needs $O(d^{2})$ operations. To reduce the computational complexity, we select only a subset $\\mathcal{V}_{I_{k}}$ , where the token $v_{i}\\in\\mathcal{V}_{I_{k}}$ has the top- $k$ highest logits in the final layer. In this scenario, we only initiate the self-evolution in Equation 2 of the logits corresponding to these top- $\\cdot k$ tokens. For the remaining tokens, which have lower probabilities, their logits are adjusted to a very lower numerical value, e.g., $-1000$ . This strategy significantly reduces the computational complexity to ${\\mathcal{O}}(k^{2})$ where $k\\ll d$ , while maintaining focus on the most relevant tokens. We name the parameter $k$ , as Evolution Scale, since it determines the number of top-probability tokens active for self-evolution. ", "page_idx": 4}, {"type": "text", "text": "$Q\\,2.1$ : Why SLED contrast the final layer with all the early layers, instead of picking one premature layer to contrast based on JSD? ", "page_idx": 4}, {"type": "text", "text": "DoLa selects a subset of early layers to form a candidate set. Then it calculates the Jensen-Shannon Divergence (JSD) between the final layer and each layer in this set. Their strategy is to choose the ", "page_idx": 4}, {"type": "text", "text": "1: Initialization: LLM with $N$ layers, inputs, evolution rate $\\alpha$ , evolution scale $k>0,\\,\\eta\\ll0.$ ,   \ntemperature parameter $\\tau$ , and the one-hoc vectors $\\{\\mathcal{P}_{e_{i}}\\}$ defined in Section 2.3.   \n2: Feed the inputs into the LLM to obtain the logits logit $\\dot{\\mathfrak{s_{n}}}=(\\ell_{(1,n)},\\dots,\\ell_{(d,n)})$ and probabilities   \nP $\\prime_{l o g i t s_{n}}=\\left(p_{\\left(1,n\\right)},\\ldots,p_{\\left(d,n\\right)}\\right)=s o f t m a x(l o g i t s_{n}/\\tau)$ at each layer $n$ , where $n\\leq N$ .   \n3: Identify the tokens with the top- $k$ largest values in logits $N$ and denote their indices by $I_{k}$ .   \n4: for each early layer $n$ , $(n<N)$ do   \n5: Compute differences for top- $\\cdot k$ logits logitsn,Ik \u2212logitsN,Ik.   \n6: Calculat $\\begin{array}{r l}&{:m_{i}^{(n)}=\\operatorname*{max}\\left(\\mathrm{CosSim}(l o g i t s_{n,I_{k}}-l o g i t s_{N,I_{k}},\\mathcal{P}_{l o g i t s_{n,I_{k}}}-\\mathcal{P}_{e_{i}}),0\\right),i\\in I_{k}.}\\end{array}$   \n7: end for   \n8: Compute weighted average $\\begin{array}{r}{m_{i}=\\frac{\\sum_{n=1}^{N}m_{i}^{(n)}}{\\sum_{n=1}^{N}\\sum_{j=1}^{d}m_{j}^{(n)}}}\\end{array}$ across different layers for each $i\\in I_{k}$ .   \n9: for each $i$ from 1 to $d$ do   \n10: Set $\\begin{array}{r}{\\tilde{\\ell}_{(i,N)}=\\ell_{(i,N)}-\\frac{\\alpha}{\\tau}(p_{(i,N)}-m_{i})}\\end{array}$ if $i\\in I_{k}$ else Set $\\tilde{\\ell}_{(i,N)}=\\eta\\ll0$ .   \n11: end for   \n12: Output: The self-evolved logits are $\\widetilde{l o g i t s}_{N}=(\\widetilde{\\ell}_{(1,N)},\\ldots,\\widetilde{\\ell}_{(i,N)},\\ldots,\\widetilde{\\ell}_{(d,N)}).$ ", "page_idx": 5}, {"type": "text", "text": "layer with the highest JSD as the premature layer, and the chosen layer will be contrasted with the final layer to update probabilities. Obviously, if this strategy is reasonable, a larger candidate set should lead to a better choice of the premature layer and, consequently, enhanced overall performance. However, a paradoxical finding from their experimental results, which our tests also confirm in the discussion in Section 3.5, is that a larger candidate set for DoLa leads to decreased performance. Specifically, when the candidate set for DoLa ranged from 0 to 32 layers for LLaMA-2-7B-Base, the performance was inferior compared to a smaller set of 0 to 16 layers. This fundamental flaw indicates that selecting a good candidate set remains a challenge when applying DoLa. In contrast, our method does not face this concern as it applies an ensemble approach to all early layers. It is also important to note that our method works well even when only contrasting the final layer with part of the early layers, as demonstrated in Section 3.5 and A, proving the robustness of our approach. ", "page_idx": 5}, {"type": "text", "text": "$Q\\,2.2$ : Why not use $\\mathcal{P}_{l a t e n t}$ directly as the model\u2019s output distribution? ", "page_idx": 5}, {"type": "text", "text": "It is crucial to understand that $\\mathcal{P}_{l a t e n t}$ is merely an estimation of the real-world distribution based on the model\u2019s latent knowledge instead of the exact $\\mathcal{P}_{r e a l}$ . Consequently, relying solely on $\\mathcal{P}_{l a t e n t}$ , similar to DoLa, might lead to inaccuracies, as the latent knowledge can be imperfect. The original logits logits $N$ are still important as they are refined directly by real-world data during training. The evolution rate $\\alpha$ in Equation 2, serves to balance this trade-off, enabling a reciprocal enhancement between $\\mathcal{P}_{l a t e n t}$ and the original logits $N$ . More ablation studies are provided in Section 3.5 and A. ", "page_idx": 5}, {"type": "text", "text": "$Q\\,2.2$ : Considering that SLED adopts logits $\\mathrm{~s~}_{n}-l o g i t s_{N}$ as the estimation of the gradient, why not directly apply it in Equation 2? ", "page_idx": 5}, {"type": "text", "text": "It is important to note that while $l o g i t s_{n}\\,-\\,l o g i t s_{N}$ is unconstrained, the gradients estimated in Equation 2 (e.g., $p_{(1,N)}-m_{1},\\ldots,p_{(d,N)}-m_{d})$ are constrained within $[-1,1]$ . Thus, direct substitution could lead to a mismatch in magnitudes and might also introduce unexpected noise. Proper normalization and subsequent aggregation of estimations from different layers are precisely what our method addresses in Section 2.2 and 2.3. Further analysis is provided in Section A. ", "page_idx": 5}, {"type": "text", "text": "3 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As a novel layer-wise contrastive decoding approach, we first benchmark SLED against the stateof-the-art approach DoLa [6] across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and model scales (from 2B to 70B), including the more advanced mixture of experts (MoE) architecture, as detailed in Section 3.2 and 3.3. The results showcase notable factuality improvements across a variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. Then, in Section 3.4, we integrate our method with other established factuality decoding techniques, illustrating that SLED can further enhance their performance. In Section 3.5, we further conduct in-depth studies on mitigating the repetition issue, layer selection, various parameter settings, and latency overhead to gain more comprehensive insights into SLED\u2019s performance. We ", "page_idx": 5}, {"type": "table", "img_path": "t7wvJstsiV/tmp/f06f40283e9aa91e35fb71a2b1954a8c3ef56da2d9919eb3fae5dd044a2e4dc5.jpg", "table_caption": ["Table 1: Comparison on LLaMA 2 model family. The best results are in bold for each dataset/metric. SLED outperforms DoLa and the vanilla greedy decoding. "], "table_footnote": ["also extend our analysis with additional ablation studies and results across more benchmarks in Section A and C in the Appendix, and provide several examples of generated text as the qualitative study in Section B. "], "page_idx": 6}, {"type": "text", "text": "3.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Benchmarks We compare our method with baselines on several multiple-choice and open-ended generation tasks. For multiple-choice question tasks, we use the TruthfulQA [26] and FACTOR (Wiki) [29] datasets to assess the LLMs\u2019 factuality in short-answer/long-paragraph scenario, respectively. For open-ended generation tasks, we adopt TruthfulQA [26] and tasks involving chain-of-thought reasoning [47]: StrategyQA [12] and GSM8K [7]. ", "page_idx": 6}, {"type": "text", "text": "Models & Baselines We evaluate the performance of SLED on six LLaMA-2 models [43] ({7B,13B,70B}-Base, {7B,13B,70B}-Chat), four LLaMA-3 family models [1] ({8B,70B}-Base, {8B,70B}-IT), two Gemma models (2B,7B), two MoE models (Mixtral- $8\\!\\times\\!7\\mathrm{B}$ , Mixtral- $\\mathbf{\\nabla}\\cdot8\\times7\\mathbf{B}$ - IT) [17]. We adopt the following baselines: 1) standard decoding (greedy decoding or sampling depending on the tasks), 2) DoLa [6], 3) Inference Time Intervention (ITI) [23], 4) Activation Decoding (AD) [4], 5) Contrastive Decoding (CD) [24], and 6) Induce-then-Contrast Decoding (ICD) [61]. ", "page_idx": 6}, {"type": "text", "text": "Metrics We adopt the factual accuracy implemented in DoLa for multiple-choice tasks and chain-ofthought reasoning tasks. For the open-ended generation task on TruthfulQA, we follow the evaluation procedure in [6, 26], using \u201cfinetuned-GPT3-judge\u201ds to measure the truthfulness, informativeness, and rejection rate of generated outputs respectively. ", "page_idx": 6}, {"type": "text", "text": "3.2 Evaluation on a Broad Range of LLM Benchmarks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Multiple-Choices Tasks The objective of these tasks is to employ decoding methods that enable LLMs to assign higher probabilities to correct completions/answers over incorrect alternatives. We demonstrate the effectiveness of SLED for both Short-Answer Factuality on the TruthfulQA and Long-Paragraph Factuality on the FACTOR dataset. For both DoLa and our SLED, we contrast the results from the final layer against all preceding layers. We randomly sample approximately $5\\%$ of the data for validation regarding parameter selection. The results, as shown in Table 1, indicate that SLED achieves superior outcomes in almost all metrics across six LLaMA-2 models. Notably, SLED ", "page_idx": 6}, {"type": "table", "img_path": "t7wvJstsiV/tmp/b3dff901b7c773fbe67302f963d4d917357ef92908382906713afdda2e4d2d73.jpg", "table_caption": ["Table 2: Using SLED with other LLM families also improves the factuality. "], "table_footnote": ["achieves better performance under the MC1/MC3 metrics on TruthfulQA, which are more sensitive to fluctuations and pose a greater challenge. For long sentences in FACTOR, our method shows improvements over baselines by $5.13\\%$ . These results not only underscore the beneftis of our method for factuality but also demonstrate its robustness across different lengths of text. "], "page_idx": 7}, {"type": "text", "text": "Open-Ended Generation Tasks In open-ended settings, we prompt the model to generate answers for the same questions from TruthfulQA, following the settings outlined in [26, 6, 24]. In Table 1, we compare the performance of six LLaMA-2 models using standard greedy decoding, (greedy) DoLa, and (greedy) SLED. All the generated answers are then evaluated by a fine-tuned GPT-3 model for both truthfulness and informativeness scores. Considering that a $100\\%$ truthful score can be easily achieved by simply responding with \u2019I have no comment,\u2019 which would result in a $0\\%$ informative score and thus is not very useful, we have introduced additional metrics\u2014%Truth $\\times$ Info and the rejection ratio $\\%$ Reject \u2014to demonstrate that SLED is a mutual-gains approach to achieve better both truthful and informative scores. We have improved the overall $\\%$ Truth x Info scores by $3{-}20\\%$ across different models and reduced the rejection ratio by up to $95\\%$ . These enhancements demonstrate that our method effectively avoids the \u2019rejection pitfall,\u2019 making it more helpful. ", "page_idx": 7}, {"type": "text", "text": "Adaptation to Chain-of-thought Reasoning Tasks Although the StrategyQA and GSM8K tasks are also open-ended and require factual accuracy, the primary focus here is to evaluate how different decoding methods adapt to the Chain-of-Thought (COT) approach for handling complex reasoning tasks. We maintain a repetition penalty of 1, as we will discuss the repetition flaws associated with DoLa in Section 3.5. StrategyQA demands multi-hop reasoning, and as shown in Table 1, our method boosts accuracy by up to $3\\%$ across six models, whereas DoLa generally worsens it without a repetition penalty. GSM8K, a benchmark for math word problems that require arithmetic reasoning, shows a $1\\%$ accuracy improvement with SLED in the 7B models, with smaller yet consistent improvements observed in the 13B and 70B models. ", "page_idx": 7}, {"type": "text", "text": "3.3 Evaluation Across Diverse LLM Configurations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As discussed above and shown in Table 1, our method, SLED, demonstrates strong generalization capabilities across the LLaMA-2 model family, proving robust from 7B to 70B model sizes. In Table 2, we further showcase SLED\u2019s impressive performance on the more recent LLaMA-3 family models, both at 8B and 70B sizes, in terms of long paragraph factuality and short answer factuality. Interestingly, SLED is also applicable to different pre-trained models, such as Gemma at both 2B and 7B sizes, and can even be adapted to the increasingly popular Mixture of Experts (MoE) architectures. These results confirm the exceptional adaptability of our method across various LLM configurations. ", "page_idx": 7}, {"type": "text", "text": "3.4 Evaluation on Integrating SLED with Other LLM Factuality Decoding Methods ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "SLED exclusively focuses on contrasting differences between layers without altering other parts of the model. Thus, it remains compatible with other techniques that incorporate additional strategies or utilize auxiliary models. This compatibility allows SLED to be seamlessly integrated into existing methods, enhancing the factuality further without the need for modifications on SLED. We integrate SLED with the following approaches: ITI, AD, CD and ICD. Table 3 shows that SLED leads to accuracy improvements from $1\\%$ to $12\\%$ across four LLaMA-2 models. ", "page_idx": 7}, {"type": "table", "img_path": "t7wvJstsiV/tmp/47f4e761b138ddfe955ef9800426e5ddf59a0b1bcf36e90ecbc5bc1a0e687159.jpg", "table_caption": ["Table 3: Comparison of decoding strategies on TruthfulQA datasets. SLED can also be seamlessly combined with other decoding strategies to improve performance further. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "t7wvJstsiV/tmp/7ccf184c38113e374889b2f501912dbc11cb0c68a26a0331cd374a4ee17e459e.jpg", "table_caption": ["Table 4: Accuracy of LLaMA 2 13B Base on StrategyQA with Varying Repetition Penalties "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "3.5 Ablation Studies and Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Mitigating Repetition Issues Table 4 demonstrates that our method, SLED, effectively addresses a significant issue in DoLa: repetitive content in open-ended generation tasks. Our approach outperforms DoLa without the need for excessive repetition penalty. While a slight increase in the repetition penalty further enhances the performance of our method, excessive penalties, such as 1.1, tend to degrade it. This suggests that SLED does not inherently require heavy adjustments for repetition issues. In contrast, DoLa\u2019s performance improves with higher penalties (e.g., 1.1, 1.2, 2), indicating a more critical need for addressing repetitive content. We also employ two intuitive metrics, Repetition-4 and Repetition-Sen, to gauge the severity of repetition issues, following prior research [50]. Regardless of the repetition penalty imposed, our method consistently exhibits lower repetition rates. Table 7 includes some examples of generated text to illustrate this further. ", "page_idx": 8}, {"type": "text", "text": "Layer Selection As discussed in Section 2.4, how to choose a good candidate set is still a paradoxically difficult task when applying DoLa. Our method does not exhibit this issue. Instead of selecting a single premature layer from the candidate set like DoLa, SLED contrasts the final layer with all layers in the candidate set and then ensembles all the results. Figure 5 shows that setting a larger candidate set, such as all the 32 layers for LLaMA-2-7B-Base, yields better performance than focusing solely on either the first [0, 16) or second half [16, 32). This implies that our layer-wise contrast approach captures more useful information in a more scientific manner. Furthermore, our tests confirm the robustness of our method even when the candidate set is minimal, such as a single layer, consistently demonstrating strong performance. Our settings mirror those of DoLa. ", "page_idx": 8}, {"type": "text", "text": "Parameter Analysis We next investigate the impact of parameters \u2014 evolution rate $\\alpha$ and evolution scale $k$ \u2014 on the performance of SLED using a subset of the FACTOR dataset. We test evolution ", "page_idx": 8}, {"type": "text", "text": "Figure 5: Evaluating using different premature layers for SLED and DoLa on a $10\\%$ subset of GSM8K dataset. Contrasting all layers for SLED is better than using only the first half [0, 16) or second half [16, 32). Hence, there are no improvements for SLED from strategic layer subset selection. ", "page_idx": 9}, {"type": "image", "img_path": "t7wvJstsiV/tmp/34bd73e10c080fd916acfd6542711c2713c902984bd0090e11cca33fb5b390a5.jpg", "img_caption": ["Figure 6: WE explore the impact of evolution scale and rate based on the factual accuracy of a subset of the FACTOR dataset. (G: Greedy, D: DoLa) "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "t7wvJstsiV/tmp/1c002d21e6d7720b9eb2d1e9d3a806fabdb6a58b56e305128f1dcb778dec2fc3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "rates from $\\{0.01,0.1,1,2,5,10\\}$ and evolution scale values from $\\{5,10,20,50\\}$ . Without extreme evolution rates (e.g., 10), our method performs well, confirming its robustness. As analyzed in our methodology and Eq. 2, the evolution rate balances the logit distribution $(\\mathcal{P}_{\\mathcal{N}})$ with the latent knowledge distribution $(\\mathcal{P}_{l a t e n t})$ . A lower evolution rate works better for larger models (13B) and chat models as their logits already better represent real-world distributions. ", "page_idx": 9}, {"type": "text", "text": "Latency Our method, SLED, does not incur significant latency overhead. The latencies presented in Table 5 demonstrate that our method, SLED, increases the decoding time of DoLa by factors ranging from $0.1\\%$ to $10\\%$ . Notably, even with an atypical setting such as $\\mathrm{top}_{k}=100$ , which is seldom used, the increase remains around $10\\%$ . The latency for DoLa is much higher compared to the baseline, which can be attributed to our methodological choice aimed at ensuring a fair comparison. Specifically, we set all early layers as candidate layers set for both DoLa and SLED. ", "page_idx": 9}, {"type": "table", "img_path": "t7wvJstsiV/tmp/1a98b3ee5e7b5b9842cbebadfcb0907b88fbad2513f120d0287a31dca1844b68.jpg", "table_caption": ["Table 5: Latency (ms/token) comparison across different configurations on different sizes of models. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We introduced Self Logits Evolution Decoding (SLED), which is a new method to improve accuracy and factuality without requiring external knowledge (e.g., RAG) or fine-tuning (e.g., SFT). The key idea is to optimize the output logits based on the LLMs\u2019 latent knowledge to improve factuality during inference. On several datasets, SLED achieved the SOTA results, improving over the vanilla decoding and DoLa. We also show that SLED does not increase the inference time significantly and that it can be combined with other factuality decoding methods. For future work, it would be interesting to combine SLED with supervised fine-tuning methods, e.g., to adapt to other domains. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was done when Jianyi Zhang was an intern at Google Research. In addition, Jianyi Zhang and Yiran Chen disclose the support from grants NSF CNS-2112562 and ARO W911NF-23-2-0224. We thank area chair and reviewers for their valuable comments. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md.   \n[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.   \n[3] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17754\u201317762, 2024.   \n[4] Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, and Junxian He. In-context sharpness as alerts: An inner representation perspective for hallucination mitigation. arXiv preprint arXiv:2403.01548, 2024.   \n[5] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up: Retrieval-augmented text generation with self-memory. Advances in Neural Information Processing Systems, 36, 2024.   \n[6] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=Th6NyL07na.   \n[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[8] Yujuan Ding, Wenqi Fan, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A survey on rag meets llms: Towards retrieval-augmented large language models. arXiv preprint arXiv:2405.06211, 2024.   \n[9] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate, 2024. URL https: //openreview.net/forum?id=QAwaaLJNCk.   \n[10] Peter I. Frazier. A tutorial on bayesian optimization, 2018. URL https://arxiv.org/abs/ 1807.02811.   \n[11] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. RARR: Researching and revising what language models say, using language models. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16477\u201316508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.910. URL https://aclanthology.org/2023.acl-long.910.   \n[12] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346\u2013361, 2021.   \n[13] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. ", "page_idx": 10}, {"type": "text", "text": "[14] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. ", "page_idx": 11}, {"type": "text", "text": "[15] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023.   \n[16] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338, 2023.   \n[17] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024. URL https://arxiv.org/abs/2401.04088.   \n[18] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017.   \n[19] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.   \n[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.   \n[21] Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et al. Chain of natural language inference for reducing large language model ungrounded hallucinations. arXiv preprint arXiv:2310.03951, 2023.   \n[22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459\u20139474, 2020.   \n[23] Kenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inferencetime intervention: Eliciting truthful answers from a language model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 41451\u201341530. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 81b8390039b7302c909cb769f8b6cd93-Paper-Conference.pdf.   \n[24] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022.   \n[25] Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.   \n[26] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https: //aclanthology.org/2022.acl-long.229. ", "page_idx": 11}, {"type": "text", "text": "[27] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference algorithm, 2019. ", "page_idx": 12}, {"type": "text", "text": "[28] Gemma Team Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, L. Sifre, Morgane Riviere, Mihir Kale, J Christopher Love, Pouya Dehghani Tafti, L\u2019eonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am\u2019elie H\u2019eliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl\u2019ement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vladimir Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Brian Warkentin, Ludovic Peran, Minh Giang, Cl\u2019ement Farabet, Oriol Vinyals, Jeffrey Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology. ArXiv, abs/2403.08295, 2024. URL https://api.semanticscholar.org/CorpusID:268379206. ", "page_idx": 12}, {"type": "text", "text": "[29] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. Generating benchmarks for factuality evaluation of language models. arXiv preprint arXiv:2307.06908, 2023.   \n[30] Rafael M\u00fcller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural information processing systems, 32, 2019.   \n[31] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt/, November 2022.   \n[32] OpenAI. GPT-4 Technical Report. arXiv e-prints, art. arXiv:2303.08774, March 2023. doi: 10.48550/arXiv.2303.08774.   \n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[34] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. Fine-tuning or retrieval? comparing knowledge injection in llms. arXiv preprint arXiv:2312.05934, 2023.   \n[35] Joshua C Peterson, Ruairidh M Battleday, Thomas L Grifftihs, and Olga Russakovsky. Human uncertainty makes classification more robust. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9617\u20139626, 2019.   \n[36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   \n[37] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.   \n[38] Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. A thorough examination of decoding methods in the era of llms. arXiv preprint arXiv:2402.06925, 2024.   \n[39] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.   \n[40] Christian Thiel. Classification on soft labels is robust against label noise. In Ignac Lovrek, Robert J. Howlett, and Lakhmi C. Jain, editors, Knowledge-Based Intelligent Information and Engineering Systems, pages 65\u201373, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg. ISBN 978-3-540-85563-7.   \n[41] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea Finn. Finetuning language models for factuality. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=WPZ2yPag4K.   \n[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[44] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. doi: 10. 1609/aaai.v32i1.12340. URL https://ojs.aaai.org/index.php/AAAI/article/view/ 12340.   \n[45] Chenguang Wang, Xiao Liu, and Dawn Song. Language models are open knowledge graphs. arXiv preprint arXiv:2010.11967, 2020.   \n[46] Qinsi Wang, Saeed Vahidian, Hancheng Ye, Jianyang Gu, Jianyi Zhang, and Yiran Chen. Coreinfer: Accelerating large language model inference with semantics-inspired adaptive sparse activation, 2024. URL https://arxiv.org/abs/2410.18311.   \n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J.   \n[48] Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024.   \n[49] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681\u2013688, 2011.   \n[50] Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. Advances in Neural Information Processing Systems, 35:3082\u20133095, 2022.   \n[51] Haoran Yang, Deng Cai, Huayang Li, Wei Bi, Wai Lam, and Shuming Shi. A frustratingly simple decoding method for neural text generation. arXiv preprint arXiv:2305.12675, 2023.   \n[52] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600.   \n[53] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.   \n[54] Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Han, Zhen Li, and Ming-Ming Cheng. Delving deep into label smoothing. IEEE Transactions on Image Processing, 30: 5984\u20135996, 2021.   \n[55] Jianyi Zhang, Ruiyi Zhang, Lawrence Carin, and Changyou Chen. Stochastic particleoptimization sampling and the non-asymptotic convergence theory. In Silvia Chiappa and Roberto Calandra, editors, Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pages 1877\u20131887. PMLR, 26\u201328 Aug 2020. URL https://proceedings.mlr.press/ v108/zhang20d.html.   \n[56] Jianyi Zhang, Yang Zhao, and Changyou Chen. Variance reduction in stochastic particleoptimization sampling. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11307\u201311316. PMLR, 13\u201318 Jul 2020. URL https: //proceedings.mlr.press/v119/zhang20ac.html.   \n[57] Jianyi Zhang, Ang Li, Minxue Tang, Jingwei Sun, Xiang Chen, Fan Zhang, Changyou Chen, Yiran Chen, and Hai Li. Fed-cbs: A heterogeneity-aware client sampling mechanism for federated learning via class-imbalance reduction. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[58] Jianyi Zhang, Aashiq Muhamed, Aditya Anantharaman, Guoyin Wang, Changyou Chen, Kai Zhong, Qingjun Cui, Yi Xu, Belinda Zeng, Trishul Chilimbi, and Yiran Chen. ReAugKD: Retrieval-augmented knowledge distillation for pre-trained language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1128\u20131136, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-short.97. URL https://aclanthology.org/2023.acl-short.97.   \n[59] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Yufan Zhou, Guoyin Wang, and Yiran Chen. Towards building the federated gpt: Federated instruction tuning, 2024. URL https://arxiv.org/abs/2305.05644.   \n[60] Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cyclical stochastic gradient mcmc for bayesian deep learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkeS1RVtPS.   \n[61] Yue Zhang, Leyang Cui, Wei Bi, and Shuming Shi. Alleviating hallucinations of large language models through induced hallucinations. arXiv preprint arXiv:2312.15710, 2023.   \n[62] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren\u2019s song in the ai ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.   \n[63] Yang Zhao, Jianyi Zhang, and Changyou Chen. Self-adversarially learned bayesian sampling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5893\u20135900, 2019. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Additional Analysis and Ablation Studies ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification on the Gradients Approximation of SLED in Section 2.2 To further support our method\u2019s mechanism, which utilizes $\\it{l o g i t s}_{n}\\mathrm{~-~}\\it{l o g i t s}_{N}$ to approximate the gradient of $K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s})$ at $l o g i t s\\,=\\,l o g i t s_{n}$ , we manually calculate the Cosine_similarity(logitsn \u2212 logit $\\mathbf{\\Psi}_{N},\\nabla_{l o g i t s}K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s})|_{l o g i t s=l o g i t s_{n}})$ among thousands of tokens and layers. We plot the density function for different models. We find that the majority of these values are positive, demonstrating that the directions of these two vectors are very close. Hence our gradient approximation strategy in Section 2.2 is reasonable. ", "page_idx": 15}, {"type": "image", "img_path": "t7wvJstsiV/tmp/8c71fdfb6781b362985f245636cc19fbd817bd31353a47f0950d91df3fec69a4.jpg", "img_caption": ["Figure 7: We collect 10k pairs of $(l o g i t s_{n}-l o g i t s_{N},\\nabla_{l o g i t s_{n}}K L(\\mathcal{P}_{r e a l},\\mathcal{P}_{l o g i t s_{n}}))$ on different tokens in FACTOR and different early layers. We calculate their cosine similarity values and draw the density function for each LLM. Most of the pairs have positive Cosine similarity values. "], "img_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "t7wvJstsiV/tmp/695b3090ac397351c9f061ec536bf8fec414889e33156f5a88886e03c500c0ed.jpg", "table_caption": ["Further Ablation Studies for Section 2.4 We design the following two ablation studies to support our claims in Section 2.4. The first study, referred to as \u2019Ablation 1\u2019, directly employs $\\mathcal{P}_{l a t e n t}$ as the output distribution as discussed in $Q\\,2.2$ . The second study, denoted as \u2019Ablation $\\acute{2}$ , involves directly scaling the differences, $\\{l o g i t s_{n}-l o g i t s_{N}\\}$ , to constrain their magnitudes within $[-1,1]$ . Then, we simply average these scaled differences across different layers and apply them to Equation 2 as mentioned in $Q\\,2.3$ . The results presented in Table 6 demonstrate that the design of our SLED is reasonable. ", "Table 6: Performance comparison of ablation studies and SLED on FACTOR and TruthfulQA. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Limitations. As we continue to refine our approach, several aspects of our method can be further developed and enhanced. Our method, SED, achieves better factuality at the cost of operating slightly slower. Ideally, we could improve the output logits without incurring any computational cost compared to performing inference on the base LLM model. Another aspect is that currently, our experimental results support the superiority of SLED on multiple datasets. Parameter optimization using Bayesian methods [10, 49, 27, 60, 55, 56, 57, 63] might also lead to more robust performance. ", "page_idx": 15}, {"type": "text", "text": "It would also be ideal to back up our results with more analysis of SLED, which could illuminate why SLED leads to more factual outputs. ", "page_idx": 16}, {"type": "text", "text": "B Qualitative Studies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present some examples from the StrategyQA dataset in Table 7 to illustrate that our method addresses the repetition issue of DoLa. ", "page_idx": 16}, {"type": "table", "img_path": "t7wvJstsiV/tmp/3e98166ca8bccd51bc61d0f3d27040b675781081d6142dea55c428825544e78e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 7: We present three examples of the generated text from LLaMA-2-13B-Base on StrategyQA dataset. SLED method can mitigate the repetition issue. ", "page_idx": 16}, {"type": "text", "text": "C Further Results from Open-ended Generation Task Benchmarks ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We have conducted additional experiments on more realistic open-ended generations datasets, HotPotQA [52], Natural Question (NQ) [20], TriviaQA [18]. We adopt Exact Match(EM) and the F1 score. Different from the setting in the Section 3, we adopt $[0,2,4,6,8,10,12,14]$ as candidate ", "page_idx": 16}, {"type": "text", "text": "layers for LLaMA 2 7B Chat model and $[0,2,4,6,8,10,12,14,18]$ as candidate layers for LLaMA 2 13B Chat model for both DoLa and SLED. Our method still has robust performance across different datasets and metrics. ", "page_idx": 17}, {"type": "table", "img_path": "t7wvJstsiV/tmp/6652283949e7141250ce9aa1c390777571eb692bcc24ae6bb5cdf22759883960.jpg", "table_caption": ["Table 8: Performance comparison on HotPotQA, Natural Question (NQ) and TriviaQA. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conducted all experiments utilizing NVIDIA A100 GPUs and implemented our method based on the following repositories: $\\mathrm{\\boldsymbol{DoLa}^{1}}$ , $\\bar{\\mathbf{A}}\\mathbf{D}^{2}$ , and $\\mathrm{ICD}^{3}$ . For decoding responses from the LLMs on TruthfulQA, StrategyQA, and GSM8K, we employed greedy decoding. The models were operated with 16-bit floating-point precision and a batch size of 1. For the LLaMA 2 models sized 7B, 13B, and 70B, we utilized 1, 1, and 3 GPUs respectively. Cross-GPU inference, involving model weight sharding, was facilitated by the Hugging Face Accelerate package4. ", "page_idx": 17}, {"type": "text", "text": "Regarding the details in Section 3.4, we evaluate the 7B-chat model for ITI, as the checkpoint is publicly available. Combining ITI with SLED results in better performance compared to using ITI alone. AD employs an entropy-based metric to measure the \u2018sharpness\u2019 of in-context hidden states and incorporates it into the decoding process. Combining AD with SLED surpasses both the original AD and its combination with DoLa across four model types. For CD, we have conducted experiments in two distinct configurations: (i) the LLaMA 2 13B base model is contrasted with that of the Llama 2 7B base model, and (ii) the LLaMA 2 13B chat model and the LLaMA 2 7B chat model are compared. Applying SLED to the larger models (13B) boosts performance beyond vanilla CD. ICD contrasts a trustworthy 7B model with a fine-tuned, untrustworthy 7B model, and again, applying SLED on the trustworthy 7B model improves factual accuracy further. ", "page_idx": 17}, {"type": "text", "text": "E Related Work ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "There have been many advances in improving training and inference to develop better out-of-the-box LLMs [42, 43, 1, 39, 32, 25, 59, 46]. Unfortunately, LLMs still suffer from hallucinations and producing non-factual text. This has led researchers to develop many methods to improve factuality. ", "page_idx": 17}, {"type": "text", "text": "Retrieval, Fine-tuning, and Preferences. Many techniques use additional knowledge graphs or fine-tuning data to increase factuality by updating the model parameters for this goal. One method is Retrieval-Augmented Generation (RAG) to use external knowledge to improve generation [3, 5, 8, 22]. Another option is to use post-generation retrieval and editing for improving attribution [11]. Other directions that use additional training or preference data are supervised fine-tuning (SFT) [34, 41], RLHF [33], DPO [36] or self-rewarding [53]. Complementary to these approaches, we wish to improve the LLM output distribution directly without needing any additional data. ", "page_idx": 17}, {"type": "text", "text": "Decoding and Factuality Decoding For each prefix, the LLM generates a probability distribution for the next token on a fixed vocabulary list, and a decoding method determines how the next token is derived based on the estimated distribution. Decoding methods were initially developed to enhance the fluency and coherence of text generation, such as Beam Search (BS), which maintains the $k$ most probable sequences at each time step. Common decoding methods also include Diverse Beam Search (DBS) [44], Contrastive Decoding [24], Top-p Sampling [14] and so on. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Recently, the potential of decoding has extended beyond merely improving text readability, with some factuality decoding methods being proposed. These methods modify the generation process to focus on truthful statements rather than unsupported claims during the inference phase, aiming to reduce hallucinations. Notable recent works include Inference-Time Intervention (ITI) [23], InducedContrastive Decoding [61], Decoding by Contrasting Layers (DoLa) [6] and so on. ITI adjusts model activations during inference by following learned directions across a limited number of attention heads to improve truthfulness. Some researchers have extended previous Contrastive Decoding [24] methods to improve factual accuracy, such as Frustratingly Easy Model Decoding [51] and InducedContrastive Decoding [61], leveraging differences between expert and amateur models. Most closely related to our work is DoLa, which also employs contrasting logits from different layers. However, significant distinctions exist: Firstly, our method diverges in how to utilize those differences between logits to extract latent knowledge. Secondly, whereas DoLa directly substitutes the original output distribution with the latent knowledge distribution, our approach recognizes potential inaccuracies in this estimated distribution and adopts gradient descent within an optimization framework to integrate the model\u2019s latent knowledge with the original output. ", "page_idx": 18}, {"type": "text", "text": "F Additional Results of DoLa ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Table 9 presents some additional results of DoLa across various benchmarks. 5 Specifically, DoLa in Table 9 selects a subset of early layers as candidates for calculating the Jensen-Shannon Divergence (JSD) instead of using all layers. For example, for the LLaMA 2 7B Chat model, layers $\\mathrm{[0,2,4,6,8,10,12,14]}$ ] are designated as candidate layers. Notably, a specific trick implemented in DoLa is omitting the post-softmax step on logits for the TruthfulQA multiple-choice task to enhance accuracy. This trick is not applied to the vanilla greedy decoding in Table 9. In contrast, for the results presented in our Tables 1, 2, and 3, this technique is also been applied to vanilla greedy decoding to ensure a fair comparison. ", "page_idx": 18}, {"type": "table", "img_path": "t7wvJstsiV/tmp/1651783c7b9decc5a896981015931e84dfa0157be598ab642b7cf686649da687.jpg", "table_caption": ["Table 9: The Performance of DoLa Across Various Benchmarks "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper introduces Self Logits Evolution Decoding and shows that it improves the factual accuracy on benchmark datasets. This is the main contribution and is reflected in the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The limitations are discussed in Section A. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper introduces a new decoding algorithm for large language models, and validates its performance empirically on benchmark datasets. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The experiment settings are described in Section 3 and the parameters are listed in Section 3. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We use open source LLMs for all experiments, as well as baselines that have publicly available implementations. We do not use any confidential data or libraries for our experiments. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The experiment settings are described in Section 3. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 19}, {"type": "text", "text": "Justification: We also follow the settings of existing work in this area for our experiments for a more consistent comparison. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We use publicly available models and datasets, and hence, the inference time is dominated by their computational costs and implementations, which are well documented. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The experiments in this paper are conducted on public benchmark datasets. The algorithm proposed in this paper does not pose new safety, security, or other societal risks. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: This paper is about the factuality of LLMs and the experiments show that clearly these LLMs are not ready for critical applications requiring high accuracy answers. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The new decoding algorithm introduced by this paper does not pose new risks as long as it is used on a reasonably safeguarded model. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We cite all papers and creators used in our studies. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: We do not release new assets. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: We do not use crowdsourcing. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing, nor research with human subjects. ", "page_idx": 21}]