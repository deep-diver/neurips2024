{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces Llama, a foundational language model used extensively in the experiments, influencing the results and analysis."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "Llama 2, another significant language model family used in the experiments, is described in this paper, which is crucial to understanding the experimental setup and results."}, {"fullname_first_author": "Yung-Sung Chuang", "paper_title": "DoLa: Decoding by contrasting layers improves factuality in large language models", "publication_date": "2024-00-00", "reason": "DoLa is a key comparative method in this paper, serving as a baseline and highlighting the novelty and improvements offered by the proposed SLED approach."}, {"fullname_first_author": "Katherine Tian", "paper_title": "Fine-tuning language models for factuality", "publication_date": "2024-00-00", "reason": "This paper addresses a related problem of improving factuality in LLMs and provides context for the techniques used and challenges addressed in the current research."}, {"fullname_first_author": "Stephanie Lin", "paper_title": "TruthfulQA: Measuring how models mimic human falsehoods", "publication_date": "2022-05-29", "reason": "TruthfulQA is a primary benchmark dataset used for evaluating the factuality of LLMs, making this paper foundational to the experiments and results presented."}]}