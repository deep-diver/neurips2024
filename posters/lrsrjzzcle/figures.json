[{"figure_path": "lrSrJZZCle/figures/figures_1_1.jpg", "caption": "Figure 1: (a) ~ (c): Same bird embedded in different backgrounds. A robust model is expected to predict consistently well on objects in all images. (d) ~ (f): The feature extraction and exchange process of COD. Z represents spurious correlations, and T represents causal correlations.", "description": "This figure illustrates the concept of Correlation-Oriented Disentanglement (COD).  The first three subfigures (a-c) show the same bird in different backgrounds, highlighting the challenge of distinguishing true object features from spurious correlations in the background. Subfigures (d-f) visually explain the COD process: (d) shows initialization, (e) demonstrates how spurious correlations (Z) are \"trapped\", and (f) shows how causal features (T) are disentangled from these spurious correlations. This disentanglement is crucial for creating a robust model less susceptible to subpopulation shifts.", "section": "3 Correlation-Oriented Disentanglement and Augmentation (CODA)"}, {"figure_path": "lrSrJZZCle/figures/figures_1_2.jpg", "caption": "Figure 2: Overview of the CODA Framework on ColoredMNIST dataset. In ColoredMNIST, samples with digit less than 5 are negative samples (Y = 0), and the rest are positive samples (Y = 1). Each sample is either painted red (A = 1) or green (A = 0), with color spuriously correlated with the label in training set. In stage one, CODA first learns to encode and disentangle causal correlations from spurious ones. The decoder reconstructs the input data from the two encodings, while the decoy classifier is employed to lure the spurious information flowing towards the variance encoder. In stage two, CODA further enhances robustness by creating synthesized samples through the recombination of encoded features from different inputs, ensuring that the resultant samples maintain original class information but vary in spurious attributes. Finally, a robust classifier is trained with a novel reweighted consistency loss to deliver consistent performance across both original and synthetic samples, thereby reinforcing its resilience to spurious correlations.", "description": "This figure illustrates the CODA (Correlation-Oriented Disentanglement and Augmentation) framework applied to the ColoredMNIST dataset.  It details the two-stage process: (1) disentanglement of causal and spurious correlations using a bi-branch encoder (variance and invariance encoders), a decoder, and a decoy classifier; and (2) sample augmentation with reweighted consistency loss to improve model robustness.  The framework aims to create a classifier resilient to spurious correlations by learning to separate causal features from irrelevant ones.", "section": "Correlation-Oriented Disentanglement and Augmentation (CODA)"}, {"figure_path": "lrSrJZZCle/figures/figures_6_1.jpg", "caption": "Figure 3: Visualization of the synthesized samples. Images from the top-row and the leftmost column are real samples, while the remaining images are reconstructions. Each reconstructed image is generated by combining t extracted from the corresponding leftmost sample and z from the corresponding top-row sample. The main diagonal images represent same sample reconstructions.", "description": "This figure visualizes the synthesized samples generated by the CODA model. The top row and leftmost column show real samples from the ColoredMNIST and CelebA datasets. The remaining images are reconstructions created by combining latent features (z and t) from different real samples.  The diagonal shows reconstructions of the same sample, highlighting the model's ability to generate variations while preserving the original sample's identity. This demonstrates CODA's disentanglement and synthesis capabilities, showing its ability to generate samples that vary in spurious attributes while preserving the core information. ", "section": "4.2 Disentangling and synthesis abilities of CODA"}, {"figure_path": "lrSrJZZCle/figures/figures_8_1.jpg", "caption": "Figure 4: Sensitivity analysis on the weight of the reweighted consistency loss. When \u03bb = 0, the methods degrade to vanilla ERM, RWG, and GDRO.", "description": "The figure shows the sensitivity analysis of the reweighted consistency loss (\u03bb) on the worst-group accuracy.  It demonstrates how different values of \u03bb impact the performance of three methods: CODA+ERM, CODA+RWG, and CODA+GDRO.  When \u03bb is 0, the methods revert to their standard (non-CODA) counterparts. The graph shows that an optimal value for \u03bb exists that maximizes performance; values too high or too low reduce performance.", "section": "4.3 Benchmarking studies and analysis"}, {"figure_path": "lrSrJZZCle/figures/figures_17_1.jpg", "caption": "Figure 3: Visualization of the synthesized samples. Images from the top-row and the leftmost column are real samples, while the remaining images are reconstructions. Each reconstructed image is generated by combining t extracted from the corresponding leftmost sample and z from the corresponding top-row sample. The main diagonal images represent same sample reconstructions.", "description": "This figure visualizes the ability of CODA to generate synthesized samples by combining latent features from different samples. The top row shows real samples, while the leftmost column are also real samples. The remaining images are reconstructed samples generated by CODA using the latent representations (z and t) extracted from the corresponding top-row and leftmost column samples.  The main diagonal shows reconstructions of the same sample, illustrating CODA's ability to reconstruct samples from their latent representations.", "section": "4.2 Disentangling and synthesis abilities of CODA"}]