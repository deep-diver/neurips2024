{"references": [{"fullname_first_author": "Been Kim", "paper_title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV)", "publication_date": "2018-07-01", "reason": "This paper introduces Concept Activation Vectors (CAVs), the core methodology that the current research builds upon and improves."}, {"fullname_first_author": "Andrew Bai", "paper_title": "Concept gradient: Concept-based interpretation without linear assumption", "publication_date": "2023-00-00", "reason": "This paper is a key related work that extends the original CAV method to non-linear concept functions, addressing limitations that LG-CAV also tackles."}, {"fullname_first_author": "Amirata Ghorbani", "paper_title": "Towards automatic concept-based explanations", "publication_date": "2019-12-01", "reason": "This paper is cited as a significant prior work in concept-based explanation methods, making it important for contextualizing the current research's contribution."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, a foundational vision-language model crucial to LG-CAV's approach of leveraging pre-trained models for concept knowledge transfer."}, {"fullname_first_author": "Tuomas P. Oikarinen", "paper_title": "Clip-dissect: Automatic description of neuron representations in deep vision networks", "publication_date": "2023-00-00", "reason": "This paper presents CLIP-Dissect, another method using CLIP for interpretability, providing a relevant comparison and highlighting LG-CAV's unique contributions."}]}