[{"type": "text", "text": "LG-CAV: Train Any Concept Activation Vector with Language Guidance ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Qihan Huang1, Jie Song1, \u2020, Mengqi Xue2, Haofei Zhang1, Bingde $\\mathbf{H}\\mathbf{u}^{\\mathbf{1}}$ , Huiqiong Wang3, Hao Jiang4, Xingen Wang1, 5, Mingli Song1 ", "page_idx": 0}, {"type": "text", "text": "1 Zhejiang University, 2 Hangzhou City University 3 Ningbo Innovation Center, Zhejiang University 4 Alibaba Group, 5 Bangsheng Technology Co., Ltd. {qh.huang,sjie,haofeizhang,tonyhu,huiqiong_wang,newroot,brooksong}@zju.edu.cn mqxue@zucc.edu.cn, aoshu.jh@alibaba-inc.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Concept activation vector (CAV) has attracted broad research interest in explainable AI, by elegantly attributing model predictions to specific concepts. However, the training of CAV often necessitates a large number of high-quality images, which are expensive to curate and thus limited to a predefined set of concepts. To address this issue, we propose Language-Guided CAV (LG-CAV) to harness the abundant concept knowledge within the certain pre-trained vision-language models (e.g., CLIP). This method allows training any CAV without labeled data, by utilizing the corresponding concept descriptions as guidance. To bridge the gap between visionlanguage model and the target model, we calculate the activation values of concept descriptions on a common pool of images (probe images) with vision-language model and utilize them as language guidance to train the LG-CAV. Furthermore, after training high-quality LG-CAVs related to all the predicted classes in the target model, we propose the activation sample reweighting (ASR), serving as a model correction technique, to improve the performance of the target model in return. Experiments on four datasets across nine architectures demonstrate that LG-CAV achieves significantly superior quality to previous CAV methods given any concept, and our model correction method achieves state-of-the-art performance compared to existing concept-based methods. Our code is available at https://github.com/hqhQAQ/LG-CAV. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Concept activation vector (CAV) [16] interprets the pre-trained black-box classification models (target models) by quantifying the significance of a concept to the model predictions. CAV provides intuitive insights to comprehend the intrinsic behavior of black-box models, elucidating the patterns behind their decision-making processes. Owing to its simplicity and effectiveness, it has been followed by numerous studies [10, 36, 11, 1, 40] and extended to diverse domains, such as recommender system [45], 3D shape generation [8], abusive language detection [25], etc. ", "page_idx": 0}, {"type": "text", "text": "However, the training of CAV usually necessitates an ample amount of high-quality images that accurately depict the corresponding concept. Unfortunately, in practical contexts, gathering an adequate number of training images is challenging especially when the number of concepts is extensive, thereby significantly impacting the quality (estimated using the proposed concept accuracy and concept-to-class accuracy) of the trained CAVs. Figure 1 delineates the correlation between the number of training images for each concept and the quality of the trained CAVs on the Broden dataset [2]. It can be concluded that when the number of training images is small, the quality of the trained CAVs is low, hindering CAVs from properly interpreting the model. ", "page_idx": 0}, {"type": "image", "img_path": "MjD9Y05Q6i/tmp/8cd3e72f2eea9d5ef2f136be6c458af04f65ad4ca6d172d6df49bd928c4a4375.jpg", "img_caption": ["Figure 1: The quality of CAV is significantly affected by the number of training images. Here concept accuracy estimates whether the CAV faithfully represents its corresponding concept. Concept-to-class accuracy measures the similarity between the CAV and its strongly semantic-related class. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "MjD9Y05Q6i/tmp/97ab8d36fad3c7a5429ed3c6c7effe15f753a1ae58434fb7559426a1b841ae9f.jpg", "img_caption": ["Figure 2: (A) LG-CAV is trained guided by activations of concept descriptions on the probe images from VL model. (B) The distribution of activation values on a concept named \u201cSkyscraper\u201d (from the Broden dataset [2]) in the target model (ResNet18) and VL model (CLIP) differs a lot. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In recent years, the advent of foundational vision-language models (referred to as VL models, such as CLIP [30]) establishes connections between images and text by mapping image features and text features into a shared feature space. These VL models undergo pre-training on extensive image-text datasets, equipping them with the ability to grasp a multitude of concepts. Inspired by this, to address the data-scarcity problem of CAV training, in this work we propose LG-CAV to utilize the abundant concept knowledge from VL models for more cheaply getting CAV for any concept given the concept descriptions, without being confined to specific pre-defined concepts. ", "page_idx": 1}, {"type": "text", "text": "The concept features extracted by VL model cannot be directly used for training the LG-CAV, as VL model and the target model operate within distinct feature spaces. To bridge the gap, our work ingeniously trains the LG-CAV by calculating its activation values on a common pool of images (probe images), and making them mimic the activation values of the corresponding concept from VL model on these images, as shown in Figure 2 (A). Therefore, LG-CAV learns its corresponding concept according to the concept\u2019s existence degree (activation value) on the probe images from VL model. ", "page_idx": 1}, {"type": "text", "text": "However, directly applying the above framework is not guaranteed to improve the quality of LGCAV (see experiments in subsubsection 4.1.2), because the calculated activation values from the target model and VL model are in different distributions (see Figure 2 (B)). To tackle this problem, our work proposes a Gaussian alignment (GA) module to align the activation values from the target and VL models. Besides, we propose a concept ensemble (CE) module and a deviation sample reweighting (DSR) module into this framework to further improve the quality of LG-CAV. Detailedly, CE module strengthens the completeness of concept descriptions by employing data augmentations on the concept texts. DSR module optimizes the selection of probe images by allocating higher training weights to the probe images with a more stable concept representation. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, after training numerous high-quality LG-CAVs that can describe all classes in the dataset, our work makes a considerable improvement on previous CAV methods by applying LGCAVs to model correction on generic datasets like ImageNet. To this end, we fine-tune the target model to align the prediction of each class with its strongly-related concept, with a proposed activation sample reweighting (ASR) module that allocates higher training weights to the samples activated more highly by the corresponding LG-CAVs. ", "page_idx": 1}, {"type": "text", "text": "We perform extensive experiments to validate the performance of our proposed method. Experiments demonstrate that LG-CAV achieves significantly higher CAV quality (concept accuracy & concept-toclass accuracy) than previous CAV methods on the Broden & ImageNet datasets over nine backbones. Besides, we conduct model correction on the ImageNet & CUB-200-2011 & CIFAR-100 datasets over nine backbones. Experiments present that our method achieves significantly superior performance to other concept-based methods. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "To sum up, the key contributions of this work can be listed as follows: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We propose LG-CAV to tackle the data-scarcity problem of CAV training, which is trained guided by the corresponding concept descriptions from VL model. \u2022 We propose a Gaussian alignment (GA) module, a concept ensemble (CE) module, and a deviation sample reweighting (DSR) module to further enhance the quality of LG-CAV. \u2022 Beyond providing explanations, we apply LG-CAV to model correction, by proposing an activation sample reweighting (ASR) module. \u2022 Experiment results verify that LG-CAV achieves significantly higher CAV quality, and our model correction method outperforms existing concept-based methods remarkably. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Concept Activation Vector (CAV). With the development and widespread application of deep learning [12, 31, 3, 46], it has become increasingly important to explain the internal mechanisms of deep neural networks (e.g., using concept activation vector (CAV)). Each CAV [16] is trained for a specific concept in the target model, and is used to quantify the importance of this concept to model predictions. Most existing CAV methods only utilize CAVs to interpret the target model. Concept_Gradient [1] extends the original linear CAV to non-linear concept functions, which improves the interpretability of CAV without the linear separability assumption of CAV. OA-TCAV [40] proposes an adversarial training approach to improve the quality of CAV. Differently, our method achieves significantly superior CAV quality to these methods, by transferring the abundant concept knowledge from VL model. ", "page_idx": 2}, {"type": "text", "text": "Vision-Language Models for Interpretability. CLIP-Dissect [27] and DISCOVER [28] utilize CLIP model to describe the neurons inside the target model. Label-free CBM [26] and PCBM [47] utilize CLIP model to generate additional concept annotations for concept bottleneck models [17]. These methods are limited to solely interpreting the target model and lacking the ability to improve the model performance using the explanation results. ", "page_idx": 2}, {"type": "text", "text": "Model Correction. Model correction methods aim to improve the target model by introducing corrective information into the model. Most existing methods [32, 22, 24, 19, 41] are limited to customized tasks with narrow scope (e.g., debias the color bias of model representations on the ColorMNIST dataset [22]). Some methods [11, 4] improve the accuracy of generic classification models, but they are limited to small-sized datasets. Differently, our method trains high-quality LG-CAVs that can describe all classes in the dataset, thus facilitating the task of model correction on generic datasets like ImageNet. ", "page_idx": 2}, {"type": "text", "text": "We provide more detailed comparisons with the related methods in Appendix C.3. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The target model is a pre-trained classification model that receives image $\\textbf{\\em x}$ as input and outputs $K$ classification logits, with a backbone $f$ and a final layer $h$ . Detailedly, $f$ extracts the image features $f(\\pmb{x})\\in\\mathbb{R}^{\\breve{D_{f}}}$ of $\\textbf{\\em x}$ $\\boldsymbol{D}_{f}$ is dimension size), and $h$ is a linear layer that projects $f({\\pmb x})$ into $K$ classification logits. Note $h(f(\\pmb{x}))\\in\\mathbb{R}^{K}$ , and $h_{k}(f(\\pmb{x}))\\in\\mathbb{R}$ is the classification logit for class $k$ . ", "page_idx": 2}, {"type": "text", "text": "Concept activation vector (CAV) [16] represents a concept for the target model. Specifically, given positive images $(\\mathcal{P}_{c})$ and negative images $(\\mathcal{N}_{c})$ for the concept $c$ , a binary linear classifier is trained on internal features $\\{f(\\pmb{x}):\\pmb{x}\\in\\mathcal{P}_{c}\\}$ and $\\{f(\\pmb{x}):\\pmb{x}\\in\\mathcal{N}_{c}\\}$ to discriminate $c$ , with a classification loss $\\mathcal{L}_{\\mathrm{cls}}$ . Finally, the CAV $\\pmb{v}_{c}\\in\\mathbb{R}^{D_{f}}$ for $c$ is defined as the weight vector for $c$ in the classifier. ", "page_idx": 2}, {"type": "text", "text": "VL model [30] consists of an image encoder $g_{\\mathrm{img}}$ that projects input image $\\textbf{\\em x}$ into image features $g_{\\mathrm{img}}(\\pmb{x})\\in\\mathbb{R}^{D_{\\mathrm{VL}}}$ , and a text encoder $g_{\\mathrm{text}}$ that projects input texts $\\pmb{t}$ into text features $g_{\\mathrm{text}}(t)\\in$ $\\mathbb{R}^{D_{\\mathrm{VL}}}$ . After trained on a large-scale image-text dataset, $g_{\\mathrm{img}}(x)$ and $\\begin{array}{r}{g_{\\mathrm{text}}(t)}\\end{array}$ are projected into the same feature space and can be directly compared. ", "page_idx": 3}, {"type": "text", "text": "3.2 Evaluation of CAV Quality ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We propose two metrics (concept accuracy and concept-to-class accuracy) to evaluate the CAV quality, based on the definition that CAV quantifies the importance of a concept to the class prediction. ", "page_idx": 3}, {"type": "text", "text": "Concept accuracy. Concept accuracy estimates whether the CAV faithfully represents its corresponding concept. To this end, the accuracy $\\operatorname{Acc}({\\pmb v}_{c})$ for CAV $v_{c}$ is calculated as the test accuracy of the binary classification model. Specifically, let $\\mathcal{C}$ denote the set of all concepts, concept accuracy $S_{\\mathrm{concept}}$ is finally calculated averagely over all concepts (note that $\\|\\cdot\\|$ denotes cardinality of a set): ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{\\mathrm{concept}}={\\frac{1}{\\|C\\|}}\\sum_{c\\in C}\\operatorname{Acc}({\\boldsymbol{v}}_{c}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Concept-to-class accuracy. The original CAV simply determines whether the trained CAV $v_{c}$ has a positive relation to a class $k$ in the target model, by simply determining whether the angle between $v_{c}$ and $\\nabla h_{k}(f({\\pmb x}))$ (the gradients of classification logit for class $k$ on $f({\\boldsymbol{x}}))$ is acute. However, this metric is too simplified to reflect the degree of connection between CAVs and classes. Therefore, we propose concept-to-class accuracy to estimate the extent to which the CAV $v_{c}$ relates to class $k$ according to the cosine similarity between $v_{c}$ and $\\nabla h_{k}(f({\\pmb x}))$ . We construct the ground-truth set $(\\mathcal{D})$ of positively-related concept-class pairs by calculating the similarity between the concepts and the class names with a language model (like all-mpnet-base-v2 [39] as used in CLIP-Dissect [27]) and selecting the concept-class pairs with the similarity exceeding a threshold $\\epsilon$ . Finally, concept-to-class accuracy $S_{\\mathrm{concept\\_to\\_class}}$ is calculated averagely over all ground-truth concept-class pairs $\\mathcal{D}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{\\mathrm{concept.to_{-}c l a s s}}=\\frac{1}{\\|\\mathcal{D}\\|}\\sum_{(c,k)\\in\\mathcal{D}}\\frac{\\pmb{v}_{c}\\cdot\\nabla h_{k}(\\pmb{f}(\\pmb{x}))}{\\|\\pmb{v}_{c}\\|\\|\\nabla h_{k}(\\pmb{f}(\\pmb{x}))\\|}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.3 LG-CAV ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we first propose a framework on how to transfer the concept knowledge from VL model to the LG-CAV, then propose three modules into this framework to further improve the quality of LG-CAV: a Gaussian alignment (GA) module, a concept ensemble (CE) module, and a deviation sample reweighting (DSR) module. ", "page_idx": 3}, {"type": "text", "text": "3.3.1 Framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The features of concept descriptions extracted by VL model cannot be directly used to supervise the training of CAVs, because VL model and the target model have different feature spaces. Therefore, we propose an ingenious method that transforms the concept knowledge of VL model into activation values on a common pool of images (also named probe images, denoted as $\\mathcal{R}$ ) and trains the LG-CAV from these activation values, inspired by previous concept-based method [7] that adopts probe images to recognize common units of different models. ", "page_idx": 3}, {"type": "text", "text": "Specifically, this method consists of three steps to train LG-CAV $v_{c}$ : (1) Calculate the activation values $\\{\\operatorname{Act}_{v_{c}}(f(\\pmb{x}))~:~\\pmb{x}~\\in~\\mathcal{R}\\}$ of $v_{c}$ on the image features $\\{f(\\mathbf{x})\\ :\\ \\mathbf{x}\\ \\in\\ \\mathcal{R}\\}$ extracted by the target model. (2) Calculate the activation values $\\{\\operatorname{Act}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x})):\\pmb{x}\\in\\mathcal{R}\\}$ of $g_{\\mathrm{text}}(c)$ on $\\{g_{\\mathrm{img}}(\\pmb{x})\\,:\\,\\pmb{x}\\,\\in\\,\\mathcal{R}\\}$ using VL model. (3) Train $\\pmb{v}_{c}$ by aligning $\\{\\operatorname{Act}_{\\pmb{v}_{c}}(f(\\pmb{x}))\\,:\\,\\pmb{x}\\,\\in\\,\\mathcal{R}\\}$ with $\\{\\operatorname{A}\\!{\\bar{\\operatorname{ct}}}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x})):\\pmb{x}\\in\\mathcal{R}\\}$ , and the corresponding loss function $\\mathcal{L}_{\\mathrm{LG-CAV}}$ is shown in Equation 3 (note that $\\Vert\\cdot\\Vert_{2}$ denotes the L2 norm, $f,g_{\\mathrm{text}},g_{\\mathrm{img}}$ are freezed, and only $v_{c}$ is trainable). ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{LG-CAV}}=\\frac{1}{\\Vert\\mathcal{R}\\Vert}\\sum_{\\boldsymbol{x}\\in\\mathcal{R}}\\left(\\mathrm{Act}_{\\boldsymbol{v}_{c}}\\big(f(\\pmb{x})\\big)-\\mathrm{Act}_{\\boldsymbol{g}_{\\mathrm{text}}(c)}\\big(g_{\\mathrm{img}}(\\pmb{x})\\big)\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We calculate activation value as cosine similarity between two vectors (e.g., $\\operatorname{Act}_{{\\pmb v}_{c}}(f({\\pmb x}))\\ =$ $\\frac{{\\pmb v}_{c}\\cdot f({\\pmb x})}{||{\\pmb v}_{c}||||f({\\pmb x})||})$ , because cosine similarity is invariant to the norms of feature vectors which differ a lot in different models. Therefore, the LG-CAV learns to recognize images with the corresponding ", "page_idx": 3}, {"type": "image", "img_path": "MjD9Y05Q6i/tmp/2f2c26670454642d0d6943510e7444f9683b862cc4bbe175e378a372281c45af.jpg", "img_caption": ["", "Figure 3: Top: The original CAV is defined as the weight vector for its represented concept in the binary linear classifier. Bottom: The LG-CAV is learned by mimicking the activation values of its represented concept on the probe images $\\mathcal{R}$ using VL model. Besides, three modules (GA module, CE module, and DSR module) are proposed to enhance the quality of LG-CAV. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "concept and the images without the corresponding concept. Besides, compared with the original binary classification task for CAV training, the activation values encompass richer information about the extent to which the concepts exist in the images, thus facilitating the training of LG-CAV. ", "page_idx": 4}, {"type": "text", "text": "3.3.2 Gaussian Alignment Module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "However, directly utilizing the above $\\mathcal{L}_{\\mathrm{LG-CAV}}$ is not guaranteed to improve the quality of CAVs (see experiments in subsubsection 4.1.2), because the activation values calculated from VL model and the target model have significantly different distributions (due to the huge difference of feature space in the two models). To address this problem, Gaussian alignment (GA) module aligns the distribution of activation values for VL model with that for the target model, based on the observation that the distribution of activation values resembles a Gaussian distribution (Figure 2 (B)). GA module consists of three steps: (1) Calculate the cosine similarity for each pair of features in $\\{f(\\pmb{x}):\\pmb{x}\\in\\mathcal{R}\\}$ to simulate the activation values from the target model, which will be $\\begin{array}{r}{\\pmb{\\mathcal{A}}\\;=\\;\\{\\frac{f(\\pmb{x}^{\\prime})\\cdot f(\\pmb{x}^{\\prime\\prime}))}{\\|f(\\pmb{x}^{\\prime})\\|\\|f(\\pmb{x}^{\\prime\\prime})\\|}}\\end{array}$ $\\mathbf{\\boldsymbol{x}}^{\\prime},\\mathbf{\\boldsymbol{x}}^{\\prime\\prime}\\in\\mathcal{R}\\}$ . (2) Estimate the parameters (mean & standard deviation) of Gaussian distribution $X\\sim{\\mathcal{N}}(\\mu_{\\mathrm{target}},\\,\\sigma_{\\mathrm{target}}^{2})$ for $\\boldsymbol{\\mathcal{A}}$ (activation values from the target model), and $X\\sim\\mathcal{N}(\\mu_{\\mathrm{VL}},\\,\\sigma_{\\mathrm{VL}}^{2})$ for $\\{\\operatorname{Act}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x})):\\pmb{x}\\in\\mathcal{R}\\}$ (activation values from VL model). (3) Calculate the transformation function for these two Gaussian distributions, then use it to transform each $\\mathrm{Act}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\mathbf{\\boldsymbol{x}}))$ to be $\\tilde{\\mathrm{Act}}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x}))$ , as shown in Equation 4. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{\\mathrm{Act}}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x}))=\\frac{\\mathrm{Act}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x}))-\\mu_{\\mathrm{VL}}}{\\sigma_{\\mathrm{VL}}}\\cdot\\sigma_{\\mathrm{target}}+\\mu_{\\mathrm{target}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Detailedly, this transformation first transforms $X~\\sim~{\\mathcal N}(\\mu_{\\mathrm{VL}},\\,\\sigma_{\\mathrm{VL}}^{2})$ into a standard Gaussian distribution $(X\\ \\sim\\ {\\mathcal{N}}(0,\\,1))$ , then transforms the standard Gaussian distribution into $\\textit{X}\\sim$ $\\mathcal{N}(\\mu_{\\mathrm{target}},\\,\\sigma_{\\mathrm{target}}^{2})$ , as shown in Appendix A. ", "page_idx": 4}, {"type": "text", "text": "3.3.3 Concept Ensemble Module ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Concept ensemble (CE) module employs data augmentations on the concept descriptions, thus enhancing the comprehensiveness of the concept. Specifically, instead of using a single prompt like \u201ca photo of the concept $c^{\\circ}$ (that will be fed into $g_{\\mathrm{text.}}$ ), CE module uses multiple prompts (e.g., \u201ca bright photo of the concept $c^{\\circ}$ , \u201ca cropped photo of the concept $c^{\\circ}$ ) to describe $c$ . These concept prompts follow the class prompts in the original CLIP model, as demonstrated in Appendix C.2. Next, $g_{\\mathrm{text}}$ will encode these augmented prompts into text features, and generate the augmented text features $\\tilde{g}_{\\mathrm{text}}(c)$ by averaging them. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.3.4 Deviation Sample Reweighting Module ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Deviation sample reweighting (DSR) module optimizes the selection of probe images, by allocating higher training weights to the probe images that can more stably represent the concept. To this end, DSR module estimates the weight of the probe image $\\textbf{\\em x}$ according to the standard deviation of its similarities with the ground-truth positive images $\\mathcal{P}_{c}$ , using three steps: (1) Calculate $\\cos_{f}(\\pmb{x},\\mathcal{P}_{c})=$ {\u2225ff((xx))\u2225\u00b7\u2225ff((xx\u2032\u2032))\u2225 : x\u2032 \u2208Pc}. (2) Calculate the standard deviation stdf(x, Pc) of cosf(x, Pc). Note that $\\mathrm{std}_{f}(\\pmb{x},\\mathcal{P}_{c})\\in\\mathbb{R}$ , and lower $\\mathrm{std}_{f}(\\pmb{x},\\mathcal{P}_{c})$ indicates more stable concept representation of $\\textbf{\\em x}$ . (3) The weight $\\omega_{f}(\\mathbf{\\boldsymbol{x}},\\mathcal{P}_{c})$ is finally calculated by normalizing the opposite of $\\bar{\\mathrm{std}}_{f}(\\pmb{x},\\mathcal{P}_{c})$ with a softmax operation, as shown in Equation 5. Note that the averaged value of all sample weights equals 1, and the softmax function can be replaced by other normalization functions. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\omega_{f}(\\mathbf{x},\\mathcal{P}_{c})=\\|\\mathcal{R}\\|\\cdot\\frac{\\exp\\big(-\\mathrm{std}_{f}(\\mathbf{x},\\mathcal{P}_{c})\\big)}{\\sum_{\\mathbf{x^{\\prime}}\\in\\mathcal{R}}\\exp\\big(-\\mathrm{std}_{f}(\\mathbf{x^{\\prime}},\\mathcal{P}_{c})\\big)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3.5 Loss Function ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "With the above three modules, the updated LG-CAV loss $\\tilde{\\mathcal{L}}_{\\mathrm{LG-CAV}}$ is calculated as in Equation 6. Besides, when positive images $\\mathcal{P}_{c}$ and negative images $\\mathcal{N}_{c}$ are provided, $\\tilde{\\mathcal{L}}_{\\mathrm{LG-CAV}}$ can be added into the training framework of original CAV to enhance the CAV quality, and the total loss function $\\mathcal{L}_{\\mathrm{total}}$ will be $\\bar{\\mathcal{L}}_{\\mathrm{total}}=\\bar{\\mathcal{L}}_{\\mathrm{cls}}+\\bar{\\mathcal{L}}_{\\mathrm{LG-CAV}}$ (note that $\\mathcal{L}_{\\mathrm{cls}}$ is the classification loss for the original CAV). ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{L}}_{\\mathrm{LG-CAV}}\\!=\\!\\frac{1}{\\Vert\\mathcal{R}\\Vert}\\sum_{x\\in\\mathcal{R}}\\omega_{f}(x,\\mathcal{P}_{c})\\cdot\\big(\\mathrm{Act}_{v_{c}}(f(x))-\\mathrm{A\\tilde{c}t}_{\\tilde{g}_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(x))\\big)^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.4 Model Correction ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Due to the lack of high-quality CAVs that can sufficiently relate to all classes in the dataset, most existing CAV methods are confined to explaining the local behavior of target model using a very limited number and variety of CAVs. Different from these methods, our proposed method can train a sufficient quantity of high-quality LG-CAVs that relate to all classes in the dataset, thus having great potential to improve the performance of target model in an interpretable manner. ", "page_idx": 5}, {"type": "text", "text": "Specifically, our model correction method alleviates spurious correlation in the target model (i.e., incorrect dependence of a class on unrelated concepts) to improve the model performance. To this end, we fine-tune the target model to align the prediction of each class with its strongly-related LG-CAV. However, directly aligning the gradients for each class with the LG-CAV would easily interfere with other correct concepts and hurt the performance. To align them in a more soft manner, activation sample reweighting (ASR) module allocates different training weights to the images of each class, according to the activation values of the corresponding LG-CAV on them. Assume concept $c$ is strongly related to class $k$ , and let $\\mathcal{T}_{k}$ denote the training images of class $k$ , then ASR module reweights image $\\textbf{\\em x}$ of $\\mathcal{T}_{k}$ in two steps (similar to DSR module): (1) Calculate $\\operatorname{Act}_{{\\pmb v}_{c}}(f({\\pmb x}))$ (the activation value of LG-CAV $v_{c}$ on $\\textbf{\\em x}$ ). (2) Calculate the weight $\\omega_{f}^{\\mathrm{fine-tune}}(x)$ by normalizing $\\operatorname{Act}_{{\\pmb v}_{c}}(f({\\pmb x}))$ with a softmax operation, as shown in Equation 7. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\omega_{f}^{\\mathrm{fine\\,tune}}(x)=\\|\\mathcal{Z}_{k}\\|\\cdot\\frac{\\exp\\left(\\mathrm{Act}_{v_{c}}(f(x))\\right)}{\\sum_{x^{\\prime}\\in\\mathcal{Z}_{k}}\\exp\\left(\\mathrm{Act}_{v_{c}}(f(x^{\\prime}))\\right)}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Next , \u03c9ffine-tune(x) will be used as the weight of image x in the classification loss during fine-tuning. In this manner, the target model learns to predict class $k$ from the samples activated more highly by the LG-CAV $\\pmb{v}_{c}$ , thus better aligning the prediction of class $k$ with its strongly-related concept $c$ . Besides, this method requires no further training on the backbone $f$ (only uses $f$ to extract image features and trains the subsequent layers), leading to minimal training cost. ", "page_idx": 5}, {"type": "text", "text": "Table 1: The comprehensive evaluation of concept accuracy $(\\%)$ for different CAVs on the Broden dataset. The results are on nine backbones pre-trained on ImageNet (Note that Res denotes ResNet, Dense denotes DenseNet) averaged over 4 runs with different seeds. Bold font denotes the best result. ", "page_idx": 6}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/c28fbf8efe3be6a678afcbdddf1aca1f909f4e6e85432f04e47136a159162286.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/f5c5746a8ecac51923378f9ffc9c26a3ec13b653c930b4de4e8e583680cf5f50.jpg", "table_caption": ["Table 2: The comprehensive evaluation of concept-to-class accuracy for different CAVs on the Broden dataset averaged over 4 runs with different seeds. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 The Quality of LG-CAV ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1.1 Experiment Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We estimate the quality of LG-CAV on the Broden dataset [2] (a popular concept-based dataset with 63,305 images for 1197 visual concepts). In the Broden dataset, each image may contain multiple concepts. Therefore, we collect positive samples for each concept by selecting the images containing only this concept, and randomly select the same number of images from other concepts as negative samples. Finally, the simplified Broden dataset consists of 17,746 images for 468 visual concepts. The probe images $({\\mathcal{R}})$ for each LG-CAV are from ImageNet and the images of other concepts in the Broden dataset. Specifically, we select the most activated and the same number of most least activated images by VL model. ", "page_idx": 6}, {"type": "text", "text": "Backbones. We follow the original CAV work [16] to train CAVs for the target models pre-trained on ImageNet (from the open-sourced PyTorch package [29]). These backbones include ResNet [12], DenseNet [14], VGG [38], and Vision Transformer [6]. ", "page_idx": 6}, {"type": "text", "text": "Parameters. To simulate the absence of images for training CAVs in reality, we set the number of positive samples $(\\mathcal{P}_{c})$ and negative samples $(\\mathcal{N}_{c})$ to be 10, and the remaining images will be used as the test set. The threshold $\\epsilon$ for determining positively-related concept-class pair is 0.6. For each CAV method, we use SGD optimizer [34] to train the CAV for 10 epochs with a learning rate of 1e-3. $\\lVert\\mathcal{R}\\rVert$ (the number of probe images) is set to be 1000. The loss function adopted here is $\\mathcal{L}_{\\mathrm{total}}$ since $\\mathcal{P}_{c}$ and $\\mathcal{N}_{c}$ are available. ", "page_idx": 6}, {"type": "text", "text": "4.1.2 Experiment Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Concept accuracy. Table 1 demonstrates that without sufficient data, the accuracy of original CAV is insufficient to accurately represent a concept. The first version of LG-CAV (Ours) has a lower accuracy than the original CAV when no other modules are added, due to the large difference in the distribution of activation values. The added GA module aligns the activation values from target model and VL model, and improves the concept accuracy by 5.83 points averagely. The added CE and DSR modules both effectively improve the concept accuracy, and the final LG-CAV outperforms Text-to-Concept and OA-TCAV (see Appendix C.3 for the analysis of them) by a large margin. ", "page_idx": 6}, {"type": "text", "text": "Concept-to-class accuracy. Table 2 demonstrates that our proposed modules also enhance conceptto-class accuracy, because the CAV that better represents a concept can more accurately correspond ", "page_idx": 6}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/6218856687ad0ea89471a51c921ae25147581f92888cf5d1e95347bc6f193825.jpg", "table_caption": ["Table 3: The comprehensive evaluation of accuracy $(\\%)$ on selected classes (40 classes) of ImageNet averaged over 4 runs with different seeds. "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/d39b2c10e359986ac4dfe1c8119f372f6d0b1d2b16eca5f860d0c0a1ced09a45.jpg", "table_caption": ["Table 4: The comprehensive evaluation of accuracy $(\\%)$ for different methods on ImageNet (note that KD denotes knowledge distillation) averaged over 4 runs with different seeds. "], "table_footnote": ["to its strongly-related class. The final LG-CAV also has much higher concept-to-class accuracy than Text-to-Concept and OA-TCAV. "], "page_idx": 7}, {"type": "text", "text": "4.2 Model Correction ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.2.1 Experiment Settings ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Datasets. We employ our model correction method on three representative datasets: ImageNet [5] (large-scale dataset), CUB-200-2011 [43] (a popular dataset used by many concept-based methods), and CIFAR-100 [18] (small-scale dataset). The probe images $({\\mathcal{R}})$ for each LG-CAV are also the most highly and least activated images by VL model from their respective datasets. ", "page_idx": 7}, {"type": "text", "text": "Backbones. The target models pre-trained on ImageNet are from PyTorch, and the target models pre-trained on CIFAR-100 and CUB-200-2011 are from another open-sourced PyTorchCV package, following PCBM [47]. The VL model adopted here is CLIP model with ViT-L/14 as backbone. ", "page_idx": 7}, {"type": "text", "text": "Parameters. We use SGD optimizer to train the final classification layer for 20 epochs with a learning rate of 1e-3. Note that different from the experiments in subsection 4.1, the training of LG-CAVs for these concepts does not require the original classification loss $\\mathcal{L}_{\\mathrm{cls}}$ and DSR module, due to the lack of ground-truth positive samples $\\mathcal{P}_{c}$ . ", "page_idx": 7}, {"type": "text", "text": "4.2.2 Experiment Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We adopt two methods to find the strongly-related concept of each class in the target model, corresponding to two types of datasets: datasets with few classes & datasets with many classes. ", "page_idx": 7}, {"type": "text", "text": "Datasets with few classes. We manually collect the concept descriptions of each class from Wikipedia for these datasets (e.g., the randomly selected subset of ImageNet with 40 classes (ImageNet-40)). The selected classes and their corresponding concept descriptions can be referred to in Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "Appendix B.1 demonstrates that the trained LG-CAVs have ability to distinguish whether images contain their respective concepts. Next, we utilize these LG-CAVs for model correction with the ASR module. As shown in Table 3, our model correction method effectively improves the performance of original pre-trained model (converted from the pre-trained 1000-classes model by removing other 960 classes in the final classification layer), by an improvement of up to 0.91 points. ", "page_idx": 7}, {"type": "text", "text": "Datasets with many classes. Collecting sufficient high-quality concept descriptions for datasets with many classes is a challenging task. Therefore, we instead acquire the concept descriptions of each class based on its comparison with its confused class, inspired by relative CAV proposed in the original CAV work. Specifically, for the class $k$ , we first find the confused class $k^{\\prime}$ to which images from class $k$ are most likely to be mispredicted by the pre-trained model, then define the concept descriptions as \u201ca photo of class $k$ , not $k^{\\prime\\bullet}$ . This approach is applied to the whole ImageNet, CUB-200-2011, and CIFAR-100. ", "page_idx": 7}, {"type": "image", "img_path": "MjD9Y05Q6i/tmp/4f7fe34e30fe1ca12c933be9b28987abd2138b7c0d4a606624606437f243ba19.jpg", "img_caption": ["Figure 5: (A) Activation values of the LG-CAV & (B) Model correction example. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "As shown in Table 4, our method improves model performance on the whole ImageNet in an interpretable manner based on LG-CAV, surpassing original model (by up to 0.68 points), Concept_Distillation, knowledge distillation, and Label-free CBM (see Appendix C.3 for the analysis of them). Besides, the results on CUB-200-2011 (Appendix B.2) & CIFAR-100 (Appendix B.3) also verify the effectiveness of our method. ", "page_idx": 8}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Selection of probe images. In the above experiments, we select the most activated and least activated images as probe images $\\mathcal{R}$ . We compare this selection strategy with random selection in this subsection. As shown in the first two figures of Figure 4, the LG-CAVs trained with this selection strategy have higher quality, because the probe images selected by this strategy contain richer information to represent the corresponding concepts. ", "page_idx": 8}, {"type": "text", "text": "Number of probe images. This subsection investigates how the number of probe images affects the quality of LG-CAV. As shown in the last two figures of Figure 4, when the number of probe images is small, increasing the quantity of probe images can improve the quality of LG-CAV. However, when the number of probe images reaches a certain level (saturation), further increasing the quantity of probe images does not improve the quality of LG-CAV. ", "page_idx": 8}, {"type": "text", "text": "Additionally, we provide more ablation experiments on the choice of VL model, the coefficient of LG-CAV loss, and the depth of extracted image features in the target model in Appendix B.5. ", "page_idx": 8}, {"type": "text", "text": "4.4 Visualization Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Activation values of the trained LG-CAV. Figure 5 (A) demonstrates the activation values of a trained LG-CAV on its highly-activated and lowly-activated images, indicating that the trained LG-CAV can accurately activate images that contain the corresponding concepts. ", "page_idx": 8}, {"type": "text", "text": "Examples of model correction. As shown in Figure 5 (B), an image of \u201cTiger Cat\u201d is misclassified as \u201cTabby Cat\u201d by the target model (with ResNet18 as backbone) before model correction. During model correction, ASR module mitigates spurious correlation of the target model by aligning the prediction of \u201cTiger Cat\u201d with its strongly-related concept \u201ca cat animal with orange stripes\u201d. This image is activated by the LG-CAV of this concept with a high activation value (0.7488), thus the classification logit for \u201cTiger Cat\u201d increases after model correction. Besides, we utilize Grad-CAM [37] to attribute the prediction of \u201cTiger Cat\u201d in the target model, and it shows that the attribution map focuses more accurately on the cat\u2019s body after model correction. ", "page_idx": 8}, {"type": "text", "text": "Furthermore, we provide more visualization results in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we propose LG-CAV to address the data-scarcity problem of original CAV by transferring the extensive concept knowledge from VL model. Specifically, LG-CAV mimics the activation values from VL model on the probe images to learn these concept knowledge. Besides, we propose a Gaussian alignment (GA) module, a concept ensemble (CE) module, and a deviation sample reweighting (DSR) module to further enhance the quality of LG-CAV. Furthermore, we go beyond previous CAV methods by generalizing LG-CAV to model correction, with a human-understandable method that aligns the class predictions with the strongly-related concepts. Experiment results demonstrate that LG-CAV significantly improves the CAV quality, and our model correction method outperforms existing concept-based methods by a large margin. We hope our work can provide inspiration for future interpretable methods based on vision-language models. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by National Natural Science Foundation of China (62106220, U20B2066), Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, and Zhejiang Province \u201cPioneering Soldier\u201d and \u201cLeading Goose\u201d R&D Project under Grant 2023C01027. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Andrew Bai, Chih-Kuan Yeh, Neil Y. C. Lin, Pradeep Kumar Ravikumar, and Cho-Jui Hsieh. Concept gradient: Concept-based interpretation without linear assumption. In ICLR 2023. OpenReview.net, 2023. [2] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In CVPR 2017, pages 3319\u20133327. IEEE Computer Society, 2017. [3] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. Bias and debias in recommender system: A survey and future directions. ACM Trans. Inf. Syst., 41(3):67:1\u201367:39, 2023.   \n[4] Muxi Chen, Yu Li, and Qiang Xu. Hibug: On human-interpretable model debug. In NeurIPS 2023, 2023.   \n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR 2009, pages 248\u2013255. Ieee, 2009.   \n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR 2021. OpenReview.net, 2021.   \n[7] Amil Dravid, Yossi Gandelsman, Alexei A. Efros, and Assaf Shocher. Rosetta neurons: Mining the common units in a model zoo. In ICCV 2023, pages 1934\u20131943. IEEE, 2023.   \n[8] Stefan Druc, Aditya Balu, Peter Wooldridge, Adarsh Krishnamurthy, and Soumik Sarkar. Concept activation vectors for generating user-defined 3d shapes. In CVPR Workshops 2022, pages 2992\u20132999. IEEE, 2022. [9] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving CLIP training with language rewrites. In NeurIPS 2023, 2023.   \n[10] Amirata Ghorbani, James Wexler, James Y. Zou, and Been Kim. Towards automatic conceptbased explanations. In NeurIPS 2019, pages 9273\u20139282, 2019.   \n[11] Avani Gupta, Saurabh Saini, and P. J. Narayanan. Concept distillation: Leveraging humancentered explanations for model improvement. In NeurIPS 2023, 2023.   \n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR 2016, pages 770\u2013778. IEEE Computer Society, 2016.   \n[13] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.   \n[14] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR 2017, pages 4700\u20134708. IEEE Computer Society, 2017.   \n[15] Qihan Huang, Jie Song, Jingwen Hu, Haofei Zhang, Yong Wang, and Mingli Song. On the concept trustworthiness in concept bottleneck models. In AAAI 2024, pages 21161\u201321168. AAAI Press, 2024.   \n[16] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai, James Wexler, Fernanda B. Vi\u00e9gas, and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In ICML 2018, volume 80 of Proceedings of Machine Learning Research, pages 2673\u20132682. PMLR, 2018.   \n[17] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In ICML 2020, volume 119 of Proceedings of Machine Learning Research, pages 5338\u20135348. PMLR, 2020.   \n[18] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[19] Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and Jaegul Choo. Learning debiased representation via disentangled feature augmentation. In NeurIPS 2021, pages 25123\u201325133, 2021.   \n[20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping languageimage pre-training for unified vision-language understanding and generation. In ICML 2022, volume 162 of Proceedings of Machine Learning Research, pages 12888\u201312900. PMLR, 2022.   \n[21] Xianhang Li, Zeyu Wang, and Cihang Xie. An inverse scaling law for CLIP training. In NeurIPS 2023, 2023.   \n[22] Yi Li and Nuno Vasconcelos. REPAIR: removing representation bias by dataset resampling. In CVPR 2019, pages 9572\u20139581. IEEE Computer Society, 2019.   \n[23] Mazda Moayeri, Keivan Rezaei, Maziar Sanjabi, and Soheil Feizi. Text-to-concept (and back) via cross-model alignment. In ICML 2023, volume 202 of Proceedings of Machine Learning Research, pages 25037\u201325060. PMLR, 2023.   \n[24] Jun Hyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: De-biasing classifier from biased classifier. In NeurIPS 2020, 2020.   \n[25] Isar Nejadgholi, Kathleen C. Fraser, and Svetlana Kiritchenko. Improving generalizability in implicitly abusive language detection with concept activation vectors. In ACL 2022, pages 5517\u20135529. Association for Computational Linguistics, 2022.   \n[26] Tuomas P. Oikarinen, Subhro Das, Lam M. Nguyen, and Tsui-Wei Weng. Label-free concept bottleneck models. In ICLR 2023. OpenReview.net, 2023.   \n[27] Tuomas P. Oikarinen and Tsui-Wei Weng. Clip-dissect: Automatic description of neuron representations in deep vision networks. In ICLR 2023. OpenReview.net, 2023.   \n[28] Konstantinos P. Panousis and Sotirios Chatzis. DISCOVER: making vision networks interpretable via competition and dissection. In NeurIPS 2023, 2023.   \n[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS 2019, pages 8024\u20138035, 2019.   \n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML 2021, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 2021.   \n[31] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. In NeurIPS 2015, pages 91\u201399, 2015.   \n[32] Laura Rieger, Chandan Singh, W. James Murdoch, and Bin Yu. Interpretations are useful: Penalizing explanations to align neural networks with prior knowledge. In ICML 2020, volume 119 of Proceedings of Machine Learning Research, pages 8116\u20138126. PMLR, 2020.   \n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR 2022, pages 10674\u201310685. IEEE Computer Society, 2022.   \n[34] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.   \n[35] Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. arXiv preprint arXiv:2008.04859, 2020.   \n[36] Jessica Schrouff, Sebastien Baur, Shaobo Hou, Diana Mincu, Eric Loreaux, Ralph Blanes, James Wexler, Alan Karthikesalingam, and Been Kim. Best of both worlds: local and global explanations with human-understandable concepts. arXiv preprint arXiv:2106.08641, 2021.   \n[37] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV 2017, pages 618\u2013626, 2017.   \n[38] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR 2015. OpenReview.net, 2015.   \n[39] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. In NeurIPS 2020, 2020.   \n[40] Rahul Soni, Naresh Shah, Chua Tat Seng, and Jimmy D Moore. Adversarial tcav\u2013robust and effective interpretation of intermediate layers in neural networks. arXiv preprint arXiv:2002.03549, 2020.   \n[41] Wolfgang Stammer, Patrick Schramowski, and Kristian Kersting. Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations. In CVPR 2021, pages 3619\u20133629. IEEE Computer Society, 2021.   \n[42] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023.   \n[43] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.   \n[44] Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. In ICLR 2021. OpenReview.net, 2021.   \n[45] Hojin Yang, Tianshu Shen, and Scott Sanner. Bayesian critiquing with keyphrase activation vectors for vae-based recommender systems. In SIGIR 2021, pages 2111\u20132115. ACM, 2021.   \n[46] Jingwen Ye, Zunlei Feng, and Xinchao Wang. Flocking birds of a feather together: Dual-step GAN distillation via realer-fake samples. In VCIP 2022, pages 1\u20135. IEEE, 2022.   \n[47] Mert Y\u00fcksekg\u00f6n\u00fcl, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. In ICLR 2023. OpenReview.net, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proof ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To address the problem that activation values calculated from $\\mathrm{VL}$ model and the target model have significantly different distributions (due to the huge difference of feature space in the two models), our proposed Gaussian alignment (GA) module first estimates the Gaussian distribution of activation values for the target model and VL model $(X_{\\mathrm{VL}}\\,\\sim\\,{\\mathcal N}(\\mu_{\\mathrm{VL}},\\,\\sigma_{\\mathrm{VL}}^{2})$ and $X_{\\mathrm{target}}\\,\\sim$ $\\mathcal{N}(\\mu_{\\mathrm{target}},\\,\\sigma_{\\mathrm{target}}^{2}))$ , then transforms each $\\mathrm{Act}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x}))$ to be $\\mathrm{Act}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x}))$ according to the transformation between these two Gaussian distributions: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\tilde{\\mathrm{Act}}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x}))=\\frac{\\mathrm{Act}_{g_{\\mathrm{text}}(c)}(g_{\\mathrm{img}}(\\pmb{x}))-\\mu_{\\mathrm{VL}}}{\\sigma_{\\mathrm{VL}}}\\cdot\\sigma_{\\mathrm{target}}+\\mu_{\\mathrm{target}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Specifically, this transformation first transforms $X\\,\\sim\\,{\\mathcal N}(\\mu_{\\mathrm{VL}},\\,\\sigma_{\\mathrm{VL}}^{2})$ into a standard Gaussian distribution $(X\\ \\sim\\ {\\mathcal{N}}(0,\\,1))$ , then transforms the standard Gaussian distribution into $\\textit{X}\\sim$ $\\mathcal{N}(\\mu_{\\mathrm{target}},\\,\\sigma_{\\mathrm{target}}^{2})$ , which can be proved in the following theorem. ", "page_idx": 13}, {"type": "text", "text": "Theorem: The Gaussian distribution $X_{\\mathrm{VL}}\\sim\\mathcal{N}(\\mu_{\\mathrm{VL}},\\,\\sigma_{\\mathrm{VL}}^{2})$ of activation values for VL model can be converted into Gaussian distribution $X_{\\mathrm{target}}\\sim{\\mathcal{N}}(\\mu_{\\mathrm{target}},\\,\\sigma_{\\mathrm{target}}^{2})$ of activation values for the target model with a linear transformation: $\\begin{array}{r}{X_{\\mathrm{target}}=\\frac{X_{\\mathrm{VL}}-\\mu_{\\mathrm{VL}}}{\\sigma_{\\mathrm{VL}}}\\cdot\\sigma_{\\mathrm{target}}+\\mu_{\\mathrm{target}}.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Proof. We first prove that $X_{\\mathrm{VL}}\\,\\sim\\,{\\mathcal{N}}(\\mu_{\\mathrm{VL}},\\,\\sigma_{\\mathrm{VL}}^{2})$ can be converted into the standard Gaussian distribution $X_{\\mathrm{standard}}\\sim{\\mathcal{N}}(0,\\,1)$ with a linear transformation: $\\frac{X_{\\mathrm{VL}}-\\mu_{\\mathrm{VL}}}{\\sigma_{\\mathrm{VL}}}$ . ", "page_idx": 13}, {"type": "text", "text": "As Gaussian distributions, the cumulative distribution function of $X_{\\mathrm{VL}}$ is $F_{X_{\\mathrm{VL}}}(k)=\\operatorname{P}(X_{\\mathrm{VL}}\\leq$ $\\begin{array}{r}{k)=\\int_{-\\infty}^{k}\\frac{1}{\\sqrt{2\\pi}\\sigma_{\\mathrm{VL}}}\\exp\\big(-\\frac{(x-\\mu_{\\mathrm{VL}})^{2}}{2\\sigma_{\\mathrm{VL}}^{2}}\\big)\\mathrm{d}x}\\end{array}$ , and the cumulative distribution function of $X_{\\mathrm{standard}}$ is $\\begin{array}{r}{F_{X_{\\mathrm{standard}}}(k)=\\overbrace{\\mathrm{P}(X_{\\mathrm{standard}}\\leq k)}^{\\cdot\\;\\cdot}=\\int_{-\\infty}^{k}{\\frac{1}{\\sqrt{2\\pi}}\\exp\\big(-\\frac{x^{2}}{2}\\big)}\\mathrm{d}x.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "Let $\\begin{array}{r}{X=\\frac{X_{\\mathrm{VL}}-\\mu_{\\mathrm{VL}}}{\\sigma_{\\mathrm{VL}}}}\\end{array}$ , then the cumulative distribution function of $X$ will be $F_{X}(k)=\\operatorname{P}(X\\leq k)=$ $\\begin{array}{r}{\\mathrm{P}(\\frac{X_{\\mathrm{VL}}-\\mu_{\\mathrm{VL}}}{\\sigma_{\\mathrm{VL}}}\\leq\\bar{k})=\\mathrm{P}(X_{\\mathrm{VL}}\\leq k\\cdot\\sigma_{\\mathrm{VL}}\\!+\\!\\mu_{\\mathrm{VL}})=\\int_{-\\infty}^{k\\cdot\\sigma_{\\mathrm{VL}}+\\mu_{\\mathrm{VL}}}\\frac{1}{\\sqrt{2\\pi}\\sigma_{\\mathrm{VL}}}\\exp\\big(-\\frac{(x-\\mu_{\\mathrm{VL}})^{2}}{2\\sigma_{\\mathrm{VL}}^{2}}\\big)\\mathrm{d}x.}\\end{array}$ Next, let x = z \u00b7 \u03c3VL + \u00b5VL, then dx = \u03c3VLdz, and FX(k) = \u2212k\u221e\u221a2\u03c01\u03c3VL $\\begin{array}{r}{F_{X}(k)=\\int_{-\\infty}^{k}\\frac{1}{\\sqrt{2\\pi}\\sigma_{\\mathrm{VL}}}\\exp\\big(-\\frac{(z\\cdot\\sigma_{\\mathrm{VL}})^{2}}{2\\sigma_{\\mathrm{VL}}^{2}}\\big)\\sigma_{\\mathrm{VL}}\\mathrm{d}z=}\\end{array}$ $\\begin{array}{r}{\\int_{-\\infty}^{k}\\frac{1}{\\sqrt{2\\pi}}\\exp\\big(-\\frac{z^{2}}{2}\\big)\\mathrm{d}z}\\end{array}$ . Therefore, $F_{X}(k)$ (the cumulative distribution function of $X$ ) is equal to $F_{X_{\\mathrm{standard}}}(k)$ (the cumulative distribution function of $X_{\\mathrm{standard}})$ , proving that $X$ is identical with Xstandard. ", "page_idx": 13}, {"type": "text", "text": "Likewise, $X_{\\mathrm{target}}\\sim\\mathcal N(\\mu_{\\mathrm{target}},\\,\\sigma_{\\mathrm{target}}^{2})$ can be converted into the standard Gaussian distribution $X_{\\mathrm{standard}}\\sim{\\mathcal{N}}(0,\\,1)$ with a linear transformation: $\\frac{X_{\\mathrm{target}}-\\mu_{\\mathrm{target}}}{\\sigma_{\\mathrm{target}}}$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\left\\{{\\begin{array}{l l}{X_{\\mathrm{standard}}=\\frac{X_{\\mathrm{VL}}-\\mu_{\\mathrm{VL}}}{\\sigma_{\\mathrm{VL}}}.}\\\\ {X_{\\mathrm{standard}}=\\frac{X_{\\mathrm{target}}-\\mu_{\\mathrm{target}}}{\\sigma_{\\mathrm{target}}}.}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Therefore, by combining these two equations, $X_{\\mathrm{target}}$ can be converted from $X_{\\mathrm{VL}}$ with a linear transformation, as shown in Equation 10. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{X_{\\mathrm{target}}=X_{\\mathrm{standard}}\\cdot\\sigma_{\\mathrm{target}}+\\mu_{\\mathrm{target}}}\\\\ &{~~~~~~~~=\\frac{X_{\\mathrm{VL}}-\\,\\mu_{\\mathrm{VL}}}{\\sigma_{\\mathrm{VL}}}\\cdot\\sigma_{\\mathrm{target}}+\\mu_{\\mathrm{target}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "B Experiments ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 LG-CAV Quality ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For the datasets with few classes, we manually collect the strongly-related concept descriptions of each class from Wikipedia. After training the LG-CAVs for these concept descriptions, we first verify the quality of LG-CAVs by calculating the Recall $@100$ performance of them. Specifically, given an LG-CAV, we use CLIP model (with ViT-L/14 as backbone) to find 50 test images that best match its corresponding concept as ground-truth, then calculate the Recall $@100$ by comparing them with the 100 most activated test images calculated from LG-CAV. Table 5 illustrates the averaged Recall $@100$ performance over all LG-CAVs, indicating that the trained LG-CAVs have the ability to distinguish images containing their respective concepts. ", "page_idx": 13}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/97add8560e36daf6315459bfa248aac81dcb8006e252ee4224744da87ee0a03f.jpg", "table_caption": ["Table 5: The averaged Recall $@$ 100 $(\\%)$ of LG-CAVs on ImageNet-40. Random CAV denotes the randomly initialized CAV. Bold font denotes the best result. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/6180735ca26cd211217244f749a9e2a4189e108236afcbb06e528c27e33060e5.jpg", "table_caption": ["Table 6: The evaluation of accuracy $(\\%)$ for different concept-based methods on the CUB-200-2011 dataset over five backbones. The results of PCBM & Trustworthy CBM & Label-free CBM are from their original paper. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "B.2 Model Correction on CUB-200-2011 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 6 demonstrates the experiment results of model correction on CUB-200-2011 over five backbones (ResNet10, ResNet12, ResNet14, ResNet16, and ResNet20), indicating that our model correction method shows superior performance to the original model. Besides, our method naturally exceeds other concept-based interpretability methods (PCBM [47], Trustworthy CBM [15], and Label-free CBM [26]) that sacrifice performance for the sake of interpretability. ", "page_idx": 14}, {"type": "text", "text": "B.3 Model Correction on CIFAR-100 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 7 also verifies the effectiveness of our method on small-scale dataset (CIFAR-100) over five backbones (ResNet20, DenseNet40, PreResNet20, SEResNet20, and SEPreResNet20). ", "page_idx": 14}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/8913acb44d31fc8ec03b323a7eecc20e890d257d6a2ee42ea29a7e86a4f3df54.jpg", "table_caption": ["Table 7: The evaluation of accuracy $(\\%)$ for different methods on the CIFAR-100 dataset over five backbones. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.4 TCAV Score ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In the main paper, we use concept-to-class accuracy to estimate the similarity between the CAV and its strongly semantic-related class. The original CAV adopts a simpler but incomplete metric named TCAV score for this purpose, which estimates whether the trained CAV $\\pmb{v}_{c}$ has a positive relation to its positively-related class $k$ by simply determining whether the angle between $\\pmb{v}_{c}$ and $\\nabla h_{k}(f({\\pmb x}))$ (the gradients of classification logit for class $k$ ) is acute. Table 8 demonstrates that our proposed modules also effectively improve the TCAV score. ", "page_idx": 14}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/f01f7ecf66aaa3dc0654db971b88bb2d94e55401aa814b2cd0cbfc2aa3386ed7.jpg", "table_caption": ["Table 8: The comprehensive evaluation of TCAV score $(\\%)$ for different CAVs on the Broden dataset. Bold font denotes the best result. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B.5 Additional Ablation Study ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.5.1 Different VL Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We conduct ablation experiments on how the choice of CLIP model affects the quality of LG-CAV. As shown in Table 9 and Table 10, the LG-CAVs trained from CLIP models with ViTs (ViTL/14, ViT-B/16, and ViT-B/32) as backbone have higher quality than those trained from CLIP models with CNNs $({\\bf R}{\\bf N}50{\\times}16)$ as backbone, because the former CLIP models have much higher performance (zero-shot accuracy) than the latter ones. Besides, Table 9 and Table 10 demonstrates that the quality of LG-CAVs increases in the order of Vi $\\mathrm{T}\\mathrm{-}\\mathrm{L}/14\\rightarrow\\mathrm{ViT-B}/16\\rightarrow\\mathrm{ViT-B}/32,$ , indicating that the ViTs with larger patch sizes lead to LG-CAVs with higher quality. This is because ViTs with larger patch size focus more on the overall concepts in the images rather than specific local details, making the learned concept features easier to transfer to the target model. ", "page_idx": 15}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/89d693ae4272e9a93395bc064bdafa2c16b8691e020f45b02d27bc21d0be1e8c.jpg", "table_caption": ["Table 9: The comprehensive evaluation of concept accuracy $(\\%)$ with different CLIP models in four target backbones (ResNet18, DenseNet121, VGG13, and ViT-B/16) on Broden. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/28bef98786dee4b94dddd099b6a7ad08c1a7dfb9f2638286ca1ff799c7fa6c71.jpg", "table_caption": ["Table 10: The comprehensive evaluation of concept-to-class accuracy with different CLIP models in four target backbones (ResNet18, DenseNet121, VGG13, and ViT-B/16) on Broden. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Besides, we adopt other VL models (EVA-CLIP [42], LaCLIP [9], CLIPA [21]) to train the LGCAVs. These VL models are advanced variants of the original CLIP model with more accurate vision-language alignment, and the LG-CAVs trained with these VL models have higher concept accuracy and concept-to-class accuracy, as shown in Table 11 and Table 12. ", "page_idx": 15}, {"type": "text", "text": "B.5.2 Coefficient of LG-CAV Loss ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Figure 6 demonstrates how the coefficient of LG-CAV loss affects the quality of LG-CAV. Initially, increasing the loss coefficient will increase the quality of LG-CAV (in both concept accuracy and concept-to-class accuracy). However, when it exceeds 3.0, further increasing it will decrease the quality of LG-CAV. ", "page_idx": 15}, {"type": "text", "text": "B.5.3 CAV Quality on Intermediate Features ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the experiments, we utilize the image features extracted from the last layer of the backbone to train CAVs, because deep features better capture high-level concepts. Besides, we also conduct experiments on the intermediate features of the target model over three backbones (ResNet18, DenseNet121, and ViT-B). Specifically, the depth of layer for extracting the intermediate features of these backbones is 13, 88, 11, respectively. ", "page_idx": 15}, {"type": "text", "text": "Table 11: The comprehensive evaluation of concept accuracy $(\\%)$ with different VL models in four target backbones (ResNet18, DenseNet121, VGG13, and ViT-B/16) on Broden. These VL models all adopt ViT-L/14 as backbones. ", "page_idx": 16}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/4bf2fd04582f30f3a60a5df754b4eb4f6f11dd7a3943594f943174c7a524acd0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/7e842b4af2552d748cd30dfa9d3df67b3b45ace085963969ddb9bd98d74782b8.jpg", "table_caption": ["Table 12: The comprehensive evaluation of concept-to-class accuracy $(\\%)$ with different VL models in four target backbones (ResNet18, DenseNet121, VGG13, and ViT-B/16) on Broden. These VL models all adopt ViT-L/14 as backbones. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "As shown in Table 13 and Table 14, our proposed modules can still effectively improve the quality of LG-CAV (in both concept accuracy and concept-to-class accuracy) on the intermediate features. However, compared with the features extracted from the last layer, the LG-CAVs trained from intermediate features have much lower quality. Besides, intermediate features have a much larger dimension size than the features extracted from the last layer, resulting in enormous training costs. Therefore, these two factors hinder the CAVs trained from intermediate features for broader applications. ", "page_idx": 16}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/837aa441e76a2d352288e7cc15e6c25d7f916e70dcaca31b3b3f44736380be31.jpg", "table_caption": ["Table 13: The comprehensive evaluation of concept accuracy $(\\%)$ for intermediate features in three backbones (ResNet18, DenseNet121, and ViT-B/16) of the target model on the Broden dataset. Bold font denotes the best result. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.6 Standard Deviation ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The experiment results of our main experiments are averaged over 4 runs with different seeds, but the standard deviations of experiment results are omitted in the main paper due to space limit. Table 15, Table 16, and Table 17 demonstrate the standard deviation of experiment results, indicating that the results are relatively stable in the experiments of evaluation of CAV quality and model correction. The results of four representative backbones from ResNet, DenseNet, VGG, and ViT are demonstrated, and the results of other backbones are similar. ", "page_idx": 16}, {"type": "text", "text": "C More Experiment Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "C.1 Concept Descriptions of 40 Classes in ImageNet ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the datasets with few classes (e.g., the randomly selected subset of ImageNet with 40 classes (ImageNet-40)), we manually collect the concept descriptions of each class from Wikipedia for these datasets, as shown in Table 18. ", "page_idx": 16}, {"type": "image", "img_path": "MjD9Y05Q6i/tmp/45ec3d374bf7f827c07d537598ef62b419ea27936666671637d959e290af64fa.jpg", "img_caption": ["Figure 6: Ablation experiments on coefficient of LG-CAV loss. $\\dagger$ denotes the loss coefficient used in the experiments of the main paper. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/c037a29afdc0a50490c70e6d1e2925a06b77a9b8748e684019a7ec772c65f0c3.jpg", "table_caption": ["Table 14: The comprehensive evaluation of concept-to-class accuracy for intermediate features in three backbones (ResNet18, DenseNet121, and ViT-B/16) on Broden. Bold font denotes the best result. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "C.2 Data Augmentation Templates in the CE Module ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Concept ensemble (CE) module employs data augmentations on the concept descriptions. Specifically, instead of using a single prompt like \u201ca photo of the concept $c^{\\circ}$ (that will be fed into $\\begin{array}{r}{g_{\\mathrm{text}},}\\end{array}$ ), CE module uses multiple prompts (e.g., \u201ca bright photo of the concept $c^{\\circ}$ , \u201ca cropped photo of the concept $c^{\\infty},$ ) to describe $c$ . The data augmentation templates are demonstrated in Table 19. ", "page_idx": 17}, {"type": "text", "text": "C.3 Comparisons with Baselines ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.3.1 The Quality of CAVs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We compare LG-CAV with four CAV methods: original CAV [16], Concept Gradient [1], OATCAV [40], and Text-to-Concept [23]. ", "page_idx": 17}, {"type": "text", "text": "Original CAV [16]. The original CAV is defined as the weight vector for the corresponding concept in the binary linear classifier that classifies the positive images and negative images for this concept. The original CAV has poor quality when the training images for the concept are insufficient. ", "page_idx": 17}, {"type": "text", "text": "Concept Gradient [1]. Concept Gradient extends the original linear CAV to non-linear concept functions, improving the quality of CAV trained on the features extracted from intermediate layers of the target model. In particular, when the features are extracted from the final layer of target model (linearly separable), Concept Gradient is identical to the original CAV. ", "page_idx": 17}, {"type": "text", "text": "OA-TCAV [40]. OA-TCAV proposes an adversarial training approach to improve the quality of CAV.   \nHowever, it still suffers from the data-scarcity problem, and thus is inferior to our method. ", "page_idx": 17}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/bd2f5f17d9083ebb605a69aec2a6f8b9695045acee81504aa8572bdd78198fdd.jpg", "table_caption": ["Table 15: STD of concept accuracy $(\\%)$ for different CAVs on the Broden dataset. The results are on nine backbones pre-trained on ImageNet. "], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/0ae8bce9876d8cc2d0a328c66c9955be7fefafb755388350fea5da97816c6ea5.jpg", "table_caption": ["Table 16: STD of concept-to-class accuracy for different CAVs on the Broden dataset averaged over 4 runs with different seeds. "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/a3a878ed74c32a49b4134375cca1d4ddb172d1bb9e74fab94d4a01dcee731c91.jpg", "table_caption": ["Table 17: STD of accuracy $(\\%)$ for different methods on ImageNet averaged over 4 runs with different seeds. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Text-to-Concept [23]. Similar to our proposed LG-CAV, Text-to-Concept leverages VL model to generate CAVs, by directly mapping the features of VL model into the feature space of target model. However, it roughly conducts feature mapping with a linear projection matrix, without specialized optimization for each individual CAV like our LG-CAV. Therefore, the quality of CAV trained with Text-to-Concept is inferior to LG-CAV. Besides, Text-to-Concept can only be applied to small-sized datasets with few classes (e.g., IN9 dataset [44], Living17 dataset [35]), without generalization ability to large datasets like ImageNet. ", "page_idx": 18}, {"type": "text", "text": "C.3.2 Model Correction ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We compare our model correction method with four baselines: Concept_Distillation [11], Knowledge Distillation [13], Label-free CBM [26], and HiBug [4]. ", "page_idx": 18}, {"type": "text", "text": "Concept_Distillation [11]. Concept_Distillation mitigates spurious correlation in the target model by directly aligning the gradients for each class with the LG-CAV using cosine similarity. However, this approach is not applicable to generic datasets like ImageNet, because it would easily interfere with other correct concepts for each class and hurt the performance, as shown in Table 17. ", "page_idx": 18}, {"type": "text", "text": "Knowledge Distillation [13]. Knowledge distillation transfers the knowledge from VL model to the target model by transferring the probabilistic predictions from VL model. For comparison with our method, we freeze the backbone of target model and only train the final classification layer using knowledge distillation. ", "page_idx": 18}, {"type": "text", "text": "Label-free CBM [26]. Label-free CBM incorporates an intermediate concept layer into the target model, and makes class predictions based on the prior concept predictions. As a concept-based interpretable model, Label-free CBM sacrifices more performance to achieve higher model transparency, and thus naturally performs worse than our method. ", "page_idx": 18}, {"type": "text", "text": "HiBug [4]. HiBug leverages pre-trained large language models like ChatGPT and pre-trained visionlanguage models like BLIP [20] to interpret the target model, and repair the model by training it on the generated data from stable diffusion model [33]. HiBug is limited to small-sized datasets (ImageNet40) because the data generation cost is too large for large datasets. ", "page_idx": 18}, {"type": "text", "text": "D More Visualization Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide more visualization results generated by our method. Specifically, Figure 7 demonstrates the highly activated images (and the activation values) of LG-CAVs for eight concepts on the ResNet18 backbone. Figure 8 demonstrates eight model correction examples for the target model on the ResNet18 backbone. ", "page_idx": 18}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/4f2cff4e262ff0605b1a1f9fff8747a4ba02b0abc4027a661addee9d25c408dc.jpg", "table_caption": ["Table 18: Concept descriptions of 40 classes from ImageNet. "], "table_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "MjD9Y05Q6i/tmp/3e8405ac37a7d6b69bf4ae273cc02944714097cc3b8c3ed4502359cc33b9555d.jpg", "table_caption": ["Table 19: The data augmentation templates for the concept descriptions in the CE module. "], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "MjD9Y05Q6i/tmp/f3356106a0884969358f512ce187e19d981aa972c4a4e40c1852190f16e2c5eb.jpg", "img_caption": ["Figure 7: Highly activated images (and the activation values) of LG-CAVs. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "MjD9Y05Q6i/tmp/3247142d4abe4156b0a6a088b5010ebf07dbbf499b7f47cbc1bb8746af822d7a.jpg", "img_caption": ["Figure 8: Model correction examples. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 23}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 23}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 23}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 23}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 23}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have discussed the limitations of the work in the appendix. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We have provided the full set of assumptions and a complete (and correct) proof for each theoretical result. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided the details of the experiments in the paper. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: We have provided the source codes of this paper, and we use the public datasets in this paper. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 25}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have specified all the training and test details for the experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have reported the error bars about the statistical significance of the experiments in the appendix. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have provided the computer resources for the experiments in the appendix. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have discussed the societal impacts of this work in the appendix. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper has no such risks. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The work uses the publicly available datasets and pre-trained models. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided a detailed readme file for the source codes. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work hasn\u2019t conducted experiments with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This work hasn\u2019t conducted experiments with human subjects. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]