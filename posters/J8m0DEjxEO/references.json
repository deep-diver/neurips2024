{"references": [{"fullname_first_author": "Andy Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper introduces the Greedy Coordinate Gradient (GCG) method, a foundational technique for jailbreaking LLMs that is further enhanced in the current work."}, {"fullname_first_author": "Xiaogeng Liu", "paper_title": "AutoDAN: Generating stealthy jailbreak prompts on aligned large language models", "publication_date": "2024-00-00", "reason": "This paper presents AutoDAN, another significant jailbreaking method that serves as a baseline and is augmented by the current paper's approach."}, {"fullname_first_author": "Patrick Chao", "paper_title": "Jailbreaking black box large language models in twenty queries", "publication_date": "2023-10-08", "reason": "This paper offers a different perspective on LLM jailbreaking that complements the optimization-based approach of the current work by introducing a black-box attack technique."}, {"fullname_first_author": "Meta AI", "paper_title": "Introducing meta llama 3: The most capable openly available llm to date", "publication_date": "2024-05-20", "reason": "This paper introduces LLaMa 3, a significant large language model used for experimental evaluations in the current paper."}, {"fullname_first_author": "Google Gemini Team", "paper_title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", "publication_date": "2024-03-05", "reason": "This paper introduces Gemini 1.5, another notable large language model used for experimental evaluations in the current paper."}]}