[{"figure_path": "4jXaca2NYa/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of our proposed ZOPP. The core of ZOPP is a complete pipeline to achieve offboard panoptic perception of AD scenes, including multi-view mask track generation (Red), 3D semantic and instance segmentation (Orange), point cloud completion (Green), 3D detection (Blue), and 4D occupancy reconstruction (Purple).", "description": "This figure illustrates the overall framework of the Zero-shot Offboard Panoptic Perception (ZOPP) system.  It shows a pipeline that processes multi-view images and point clouds to generate 2D and 3D panoptic segmentations, 3D object detections, and 4D occupancy reconstruction.  Each stage is color-coded to illustrate the flow of information and the different modules involved.", "section": "3 Methodology"}, {"figure_path": "4jXaca2NYa/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of our object association across multiple views. Multi-view images are concatenated in a panoramic order. The visual features and horizontal pixel coordinates of each object are drawn at the top and bottom of the images, respectively. Visual features v1 and v5 are very similar, so the location differences d1 and d5 contribute to the matching determination. The visual features of traffic lights are almost the same (v8, v9, v10), so we can associate them with location similarities (d6, d7).", "description": "This figure illustrates the multi-view object association method used in the ZOPP framework.  It shows how the system matches objects across multiple camera views by combining visual appearance similarity (visual features v1-v10) and spatial proximity (distances d1-d7). Objects with similar visual features and close spatial locations are linked, ensuring consistent tracking even when objects appear in multiple views. The example highlights how traffic lights, despite being spatially separated, are correctly associated due to similar visual features.", "section": "3.1 Multi-view Mask Track Generation"}, {"figure_path": "4jXaca2NYa/figures/figures_4_2.jpg", "caption": "Figure 3: Point clouds are projected into the image plane, and visualized in a color map based on the depth values (Near to Far). On the right, we compare the effect before (top) and after (bottom) our proposed parallax occlusion. Please zoom in the highlighted pink boxes to see the filtering points.", "description": "This figure shows the effect of parallax occlusion filtering on point cloud projection. The top row displays point clouds projected onto the image plane before filtering, showing how background points are incorrectly projected onto foreground objects due to the LiDAR's higher vantage point. The bottom row demonstrates the result after filtering, where background points are removed, resulting in cleaner and more accurate foreground object point clouds. The color map represents the depth values, with nearer points in lighter colors and farther points in darker colors. The pink boxes highlight areas where background points were filtered out.", "section": "3.2 Point Cloud Segmentation"}, {"figure_path": "4jXaca2NYa/figures/figures_5_1.jpg", "caption": "Figure 1: Overview of our proposed ZOPP. The core of ZOPP is a complete pipeline to achieve offboard panoptic perception of AD scenes, including multi-view mask track generation (Red), 3D semantic and instance segmentation (Orange), point cloud completion (Green), 3D detection (Blue), and 4D occupancy reconstruction (Purple).", "description": "This figure provides a high-level overview of the ZOPP framework, illustrating the various stages involved in achieving offboard panoptic perception.  It highlights the key components and their sequential processing, from multi-view mask tracking to final 4D occupancy reconstruction.", "section": "3 Methodology"}, {"figure_path": "4jXaca2NYa/figures/figures_5_2.jpg", "caption": "Figure 4: Two cases of constructing the local rectangle regions in our proposed algorithm. Projected points with large depth values pfar (orange) inside the local rectangle region will be filtered out.", "description": "This figure illustrates two scenarios for constructing local rectangular regions used in the parallax occlusion filtering algorithm.  The algorithm aims to identify and remove background points that are incorrectly projected onto foreground objects due to the parallax effect of LiDAR's higher vantage point compared to the cameras.  In (a), multiple foreground points (green) have projected points (orange) inside the rectangle region, indicating background points that must be removed. In (b), only one foreground point has this issue, and a pseudo-coordinate is used to generate the rectangular region. The rectangle region encompasses the points to be filtered out.", "section": "3.2 Point Cloud Segmentation"}, {"figure_path": "4jXaca2NYa/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative results of our proposed ZOPP on various perception tasks in AD scenes, including 2D segmentation, 3D detection, 3D semantic segmentation, 3D panoptic segmentation, and occupancy prediction.", "description": "This figure showcases the qualitative results of the ZOPP model on various perception tasks in autonomous driving scenarios. It presents results for 2D segmentation, 3D detection (with bounding boxes), 3D semantic segmentation (showing the different semantic classes), 3D panoptic segmentation (combining semantic and instance information), and occupancy prediction (representing the 3D structure of the scene). Each column represents a different scene from the Waymo Open Dataset, illustrating the capabilities of ZOPP in handling diverse and complex driving scenarios.", "section": "4 Experiments"}, {"figure_path": "4jXaca2NYa/figures/figures_16_1.jpg", "caption": "Figure 6: (a) Before the parallax occlusion and noise filtering, our box interpretation would produce inaccurate box dimensions based on the wrong object points. (b) After filtering, we will output 3D boxes with precise dimensions.", "description": "This figure shows the effectiveness of the parallax occlusion and noise filtering module by comparing the box interpretation results before and after the filtering operation.  The left image (a) demonstrates inaccurate box dimensions due to incorrect object points before filtering. The right image (b) shows the accurate 3D boxes obtained after filtering out background points and noise, leading to more precise dimensions.", "section": "D.3 Parallax Occlusion and Noise Filtering"}, {"figure_path": "4jXaca2NYa/figures/figures_17_1.jpg", "caption": "Figure 3: Point clouds are projected into the image plane, and visualized in a color map based on the depth values (Near to Far). On the right, we compare the effect before (top) and after (bottom) our proposed parallax occlusion. Please zoom in the highlighted pink boxes to see the filtering points.", "description": "This figure shows the effect of the parallax occlusion and noise filtering module. Before filtering (top), the 3D points belonging to the background are incorrectly projected into the pixel regions of the foreground object (car). After applying the proposed filtering method, the background points are removed, resulting in a more accurate segmentation of the foreground object.", "section": "3.2 Point Cloud Segmentation"}, {"figure_path": "4jXaca2NYa/figures/figures_17_2.jpg", "caption": "Figure 8: Visual comparisons of point cloud completion. Compared with the sparse inputs (Top), we can produce fine-grained geometric structures of dense point clouds (Bottom).", "description": "This figure shows the effectiveness of the point cloud completion module. The top row displays the sparse and incomplete input point clouds for various objects. The bottom row presents the same objects but with dense and complete point clouds generated by the point completion module.  This demonstrates the module's ability to recover detailed geometric structures from sparse data, which is crucial for accurate 3D bounding box prediction.", "section": "3.3 Box Interpretation"}, {"figure_path": "4jXaca2NYa/figures/figures_17_3.jpg", "caption": "Figure 3: Point clouds are projected into the image plane, and visualized in a color map based on the depth values (Near to Far). On the right, we compare the effect before (top) and after (bottom) our proposed parallax occlusion. Please zoom in the highlighted pink boxes to see the filtering points.", "description": "This figure shows the effect of the parallax occlusion and noise filtering module.  The left side shows point clouds projected onto the image plane, color-coded by depth. The right side compares the results before and after applying the filtering, highlighting how background points that are incorrectly assigned to foreground objects due to parallax are removed.", "section": "3.2 Point Cloud Segmentation"}, {"figure_path": "4jXaca2NYa/figures/figures_18_1.jpg", "caption": "Figure 5: Qualitative results of our proposed ZOPP on various perception tasks in AD scenes, including 2D segmentation, 3D detection, 3D semantic segmentation, 3D panoptic segmentation, and occupancy prediction.", "description": "This figure shows several qualitative results of the proposed ZOPP model on various perception tasks for autonomous driving.  It presents visualizations for different tasks, such as 2D and 3D semantic segmentation, object detection, panoptic segmentation, and occupancy prediction. The results showcase the model's ability to generate high-quality perception outputs across different modalities.", "section": "4 Experiments"}, {"figure_path": "4jXaca2NYa/figures/figures_19_1.jpg", "caption": "Figure 11: The illustration of the failure cases. It indicates that the image data are influenced by the lighting conditions at night (a), rainy weather conditions (a), and the camera's overexposure condition (c). Then we could not generate accurate detection and segmentation results (b), and reconstruction with lower quality (d).", "description": "This figure shows four examples of failure cases for the ZOPP model. The failures are caused by challenging weather conditions (night, rain), and overexposed images.  The resulting outputs show the negative impact of these conditions on detection, segmentation, and reconstruction quality.", "section": "D.6 Failure Pattern Analysis"}]