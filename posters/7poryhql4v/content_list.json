[{"type": "text", "text": "Great Minds Think Alike: The Universal Convergence Trend of Input Salience ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yipei Wang, Jeffrey Mark Siskind, Xiaoqian Wang Elmore Family School of Electrical and Computer Engineering Purdue University West Lafayette, IN 47907 wang4865,qobi,joywang@purdue.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Uncertainty is introduced in optimized DNNs through stochastic algorithms, forming specific distributions. Training models can be seen as random sampling from this distribution of optimized models. In this work, we study the distribution of optimized DNNs as a family of functions by leveraging a pointwise approach. We focus on the input saliency maps, as the input gradient field is decisive to the models\u2019 mathematical essence. Our investigation of saliency maps reveals a counter-intuitive trend: two stochastically optimized models tend to resemble each other more as either of their capacities increases. Therefore, we hypothesize several properties of these distributions, suggesting that (1) Within the same model architecture (e.g., CNNs, ResNets), different family variants (e.g., varying capacities) tend to align in terms of their population mean directions of the input salience. And (2) the distributions of optimized models follow a convergence trend to their shared population mean as the capacity increases. Furthermore, we also propose semi-parametric distributions based on the Saw distribution to model the convergence trend, satisfying all the counter-intuitive observations. Our experiments shed light on the significant implications of our hypotheses in various application domains, including black-box attacks, deep ensembles, etc. These findings not only enhance our understanding of DNN behaviors but also offer valuable insights for their practical application in diverse areas of deep learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The advancement in computational power has significantly enhanced the capabilities of Deep Neural Networks (DNNs), leading to their unparalleled expressiveness and success in a multitude of applications across various fields (Krizhevsky et al., 2012; He et al., 2016; Rajkomar et al., 2018; Berner et al., 2019; Rombach et al., 2022; Padmaja et al., 2023; Thirunavukarasu et al., 2023). Despite these achievements, DNNs remain enigmatic, not only to end-users but also to researchers and practitioners (Ribeiro et al., 2016; Rudin, 2018; Preece et al., 2018). Due to the over-parameterization nature of modern DNNs, they are capable of reaching zero loss in the training distribution (Goodfellow et al., 2014b; Allen-Zhu et al., 2019; Du et al., 2019). Furthermore, the inherent stochastic nature of training algorithms means that even when using the same training data, DNNs tend to converge to various minima (Huang et al., 2017; Liu et al., 2020). Thus even though these models may exhibit comparable performance in terms of metrics like testing loss or accuracy, their underlying mechanisms can still differ significantly. Because of the stochastic nature of the training procedure, optimized DNNs collectively form a distribution over the functional space $\\mathcal{C}^{1}(\\mathcal{X})$ , and training DNNs from scratch is thereby equivalent to randomly sampling from such a distribution without any guarantee. This inherent opacity, combined with the high dimensionality and nonlinearity, limits our understanding of the internal mechanisms of DNNs. ", "page_idx": 0}, {"type": "image", "img_path": "7PORYhql4V/tmp/74560312805b65a6758d92ff77db883b4d5ab02ecc9bd11dbca36aff5a831b35.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: A synthetic illustration of the distribution of the directional gradients of stochastically optimized models of the same input data. The subfigures demonstrate (a) an intuitive, stochastic scenario, where the distributions of different model families are not closely dependent. and (b) the converging distribution trend introduced by our hypothesis. Different colors represent different model families, and points represent different optimized models. ", "page_idx": 1}, {"type": "text", "text": "In response to these challenges, we study the aforementioned distributions. By adopting a pointwise approach, our focus is on the distribution of input salience (Simonyan et al., 2013) from the context of eXplainable Artificial Intelligence (XAI), which aims to demystify the inner workings of these complex models (Gunning and Aha, 2019; Arrieta et al., 2020; Van der Velden et al., 2022). Saliency maps, particularly in the form of input gradients, represent the data points within the gradient fields of DNNs. Thus the study of gradients can offer a deterministic view of the landscape of model predictions. This approach allows us to examine the intricate nuances of DNNs in a more structured and analytical manner. ", "page_idx": 1}, {"type": "text", "text": "For clarity, in the following context, we distinguish between the term model architecture (e.g. skip/direct connections) from the term model family. The latter refers to a specific collection of models $\\mathcal{F}$ , that differ only in capacity as determined by width and depth. Two models are said to be in the same family if they differ only in parameter values. Given an input, varying model families result in distinct distributions. A synthetic visualization of such distributions is shown in Figure 1(a). Different models are depicted by the points. However, the relationship between different model families, represented by various colors, remains elusive. In this work, we introduce and verify several hypotheses to uncover a striking pattern. (1) Within the same model architecture (e.g., CNNs, ResNets), different family variants (e.g., varying capacities) tend to align in terms of their population mean direction. (2) As the model capacities increase, the variance within the distribution of the same family diminishes. This leads to a converging trend of the distributions. Both hypotheses are illustrated in Figure 1(b). Additionally, we introduce a semi-parametric approach to model these distributions, providing detailed quantification of the convergence. ", "page_idx": 1}, {"type": "text", "text": "The similarities observed in input salience have direct implications for understanding the important vulnerability of DNNs regarding gradient attacks (Szegedy et al., 2013; Goodfellow et al., 2014a). In particular, in black-box attack settings, the gradients of the target model are not directly accessible. A higher degree of salience similarity naturally enhances transferability (Chen et al., 2023). Our findings elucidate why models with larger capacity consistently exhibit superiority in terms of adversarial robustness compared to smaller models (Madry et al., 2017; Gustafsson et al., 2020; Li et al., 2020; Bubeck and Sellke, 2021). Moreover, given that the mean direction is aligned across different models, it is possible to approximate this mean direction by randomly sampling from a set of independently optimized models. We demonstrate that these estimated mean directions can attain a near-perfect cosine similarity of almost 1.0, even between completely independent models or ensembles, in a high-dimensional space. Moreover, note that deep ensembles essentially calculate this population mean direction (Lee et al., 2015; Lakshminarayanan et al., 2017; Fort et al., 2019; Kondratyuk et al., 2020), where the mean of a group of independently trained models can improve the performance. As a consequence, the insights of our hypotheses also shed light on this phenomenon which, although empirically successful, has been somewhat enigmatic in terms of their source of capability (Lobacheva et al., 2020; Deng and Shi, 2021; Abe et al., 2022; Theisen et al., 2023). Furthermore, since deep ensembles approximate the aligned mean directions much faster than scaling up single models, this also demystifies the significant black-box attack transferability of deep ensembles (Yang et al., 2021; Chen et al., 2023). Our research thus not only advances the understanding of model behavior in practical applications but also contributes to the broader field of AI trustworthiness and efficiency. Our main contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "\u2022 We reveal an appealing phenomenon where the mean distribution directions of input salience across different model families have extremely high resemblance.   \n\u2022 We empirically demonstrate the distribution converges towards the mean direction as model capacity increases.   \n\u2022 Incorporating both empirical observations and theoretical analysis, we hypothesize distributional properties of optimized models, quantifying the aforementioned phenomena.   \n\u2022 The hypotheses effectively explain many hitherto unclear phenomena such as black-box attack transferability, the efficacy of deep ensemble methods, etc. ", "page_idx": 2}, {"type": "text", "text": "2 The Convergence of Input Saliency ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Salience Similarities ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. Let $\\mathcal{X}\\times\\mathcal{Y}=\\mathcal{D}$ denote the dataset, where $\\mathcal{X}\\subset\\mathbb{R}^{d}$ is the input set and $y=[c]$ is the set of labels and $c\\in\\mathbb{N}_{+}$ is the number of classes. Following the benign overfitting phenomenon (Bartlett et al., 2020; Papyan et al., 2020; Cao et al., 2022), we let $\\bar{\\mathcal{F}}=\\{f\\vert\\mathcal{L}(f;\\mathcal{X}_{\\mathtt{t r a i n}}^{\\overline{{\\mathtt{~\\mu~}}}},\\mathcal{V}_{\\mathtt{t r a i n}})<$ $\\mathrm{i0^{-3}}\\}$ denote a family of optimized models, distinguished by different architectures, such as vanilla sequential CNNs, skipping blocks, etc. $\\mathcal{L}$ denotes the expected cross-entropy loss for the training distribution. For simplicity, we focus on $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ , which predicts the logit specifically for the targeted class. This is to stay consistent with XAI methods. We demonstrate in Appendix A that the difference between logit and probability (Wang and Wang, 2022) does not affect the observed phenomena. Unless otherwise indicated, experiments are carried out over the test set $\\mathcal{X}=\\mathcal{X}_{t e s t},\\mathcal{Y}=$ $\\gamma_{t e s t}$ . Within the same architecture, model capacity is determined by both the width and the depth. Since it is more difficult to model depth as a single variable, we model varying depth as different families $\\mathcal{F}$ but model varying width $k$ as a parameter of the family, $\\mathcal{F}(k)$ . ", "page_idx": 2}, {"type": "image", "img_path": "7PORYhql4V/tmp/d4766957edee947311df4d10c09d7b783c08c41a512c01f25afa63f2608e1ff3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: The individual similarity $\\rho_{i n d}\\bigl(f^{(1)},f^{(2)}\\bigr)\\,=\\,\\mathbb{E}_{\\mathbf{\\boldsymbol{x}}\\in\\mathcal{X}}\\bigl[\\mathsf{C o s S i m}(\\nabla_{\\mathbf{\\boldsymbol{x}}}f^{(1)}(\\mathbf{\\boldsymbol{x}}),\\nabla_{\\mathbf{\\boldsymbol{x}}}f^{(2)}(\\mathbf{\\boldsymbol{x}}))\\bigr],$ where $f^{(1)}\\in\\mathcal{F}(k_{1}),f^{(2)}\\in\\mathcal{F}(k_{2})$ . CIFAR-10/100 and CNN & ResNets are tested here. ", "page_idx": 2}, {"type": "image", "img_path": "7PORYhql4V/tmp/949f4b711396b889c3aab7a8888f4575bdae75a95784b738276941a25f3dfdc1.jpg", "img_caption": ["Figure 3: The expected similarity $\\rho(k_{1},k_{2})$ between model families of varying capacities $k_{1},k_{2}$ . Here the datasets are CIFAR-10/100, and the models are CNN and ResNets. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "The Increasing Similarity. Let $\\mathtt{C o s S i m}\\,:\\,\\mathbb{R}^{d}\\,\\times\\,\\mathbb{R}^{d}\\;\\to\\;[-1,1]$ denote the cosine similarity metric, then the individual similarity between the input salience of two given models $f^{(1)}~\\in$ $\\mathcal{F}(k_{1}),f^{(2)}\\in\\mathcal{F}(k_{2})$ given input $\\textbf{\\em x}$ is $\\rho_{i n d}(f^{(1)},f^{(2)};\\bar{\\pmb{x}})=\\mathsf{C o s S i m}(\\nabla_{\\pmb{x}}f^{(1)}(\\pmb{x}),\\nabla_{\\pmb{x}}f^{(2)}(\\pmb{x})).$ Taking the entire testing set into consideration, denote $\\rho_{i n d}(f^{(1)},f^{(2)})=\\mathbb{E}_{\\pmb{x}\\in\\mathscr{X}}[\\rho_{i n d}(f^{(1)},f^{(2)};\\pmb{x})]$ . In Figure 2, the expectations over the testing set Ex\u2208 $;\\chi\\mathsf{C o s S i m}\\big(\\nabla_{\\pmb{x}}f^{(1)}({\\pmb x}),\\nabla_{\\pmb{x}}f^{(2)}({\\pmb x})\\big)$ with varying $k_{1},k_{2}~\\in~K$ are illustrated. Here, we define $K\\;=\\;\\{j2^{i}\\;:\\;4\\;\\leq\\;j\\;\\leq\\;7,1\\;\\leq\\;i\\;\\leq\\;6\\}\\;=$ $\\{8,10,12,14,16,20,\\cdots\\,,384,448\\}$ to balance between finer linear scaling and coarser exponential scaling. It can be observed that the similarity between two stochastically optimized models $f^{(1)},f^{(2)}$ has an increasing trend with respect to both $k_{1},k_{2}$ . Two different architectures CNN and ResNet are included. To rule out the influence of any single model, we define the similarity between families $\\mathcal{F}(k_{1}),\\mathcal{F}(k_{2})$ for a given input $\\textbf{\\em x}$ by taking the expectation of the two models: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\rho(k_{1},k_{2};\\pmb{x}):=\\!\\mathbb{E}_{f^{(1)}\\in\\mathcal{F}(k_{1}),f^{(2)}\\in\\mathcal{F}(k_{2})}\\!\\mathsf{C o s S i m}(\\nabla_{\\pmb{x}}f^{(1)}(\\pmb{x}),\\nabla_{\\pmb{x}}f^{(2)}(\\pmb{x}))\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The global similarity between models of widths $k_{1},k_{2}$ is then denoted by $\\begin{array}{r l}{\\rho(k_{1},k_{2})}&{{}=}\\end{array}$ $\\mathbb{E}_{{\\mathbf{x}}\\in{\\mathcal{X}}}\\rho(k_{1},k_{2};{\\mathbf{x}})$ . Note that estimating this value requires training numerous $f\\in\\mathcal{F}(k)$ for each $k\\ \\in\\ K$ . Therefore, we carry out the experiments over $K^{\\prime}\\,=\\,\\{j\\bar{2}^{i}\\,:\\,j\\,=\\,5i\\,=\\,1,2,3,4,5\\}\\,=$ $\\{10,20,40,80,160\\}\\,\\subset\\,K$ . For each $\\overline{{k_{1},k_{2}}}\\,\\in\\,K^{\\prime}$ , 100 models are sampled respectively to empirically estimate the expectation over $\\mathcal{F}(k_{1}),\\mathcal{F}(k_{2})$ . As observed in Figure 3, $\\rho(k_{1},k_{2})$ has an increasing trend w.r.t. both $k_{1},k_{2}$ . Compared with Figure 3, the average similarity over the model families is similar to the individual cosine similarity for the same $k$ values. As a result, studying the similarity of two randomly sampled models instead of the expectation over $\\mathcal{F}\\mathrm{s}$ can significantly alleviate the computational burden. This is further discussed in detail in Section 3.2. Besides, It trivially follows that $\\forall k_{1}>k_{2},\\rho(k_{1},k_{1})>\\rho(k_{1},k_{2})>\\rho(k_{2},k_{2})$ , which suggests that larger models tend to resemble smaller models even more than smaller models themselves. \u2013 Even if the two smaller models only differ in the random seeds during training. Please refer to Appendix A for the results of more datasets, where such increasing trends still exist. ", "page_idx": 3}, {"type": "text", "text": "2.2 The Spherical Distribution of the Salience ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Since the cosine similarity can be written as the inner product between $\\frac{\\nabla_{\\mathbf{\\alpha}}f^{(1)}(\\mathbf{\\alpha}\\mathbf{\\alpha})}{||\\nabla_{\\mathbf{\\alpha}}f^{(1)}(\\mathbf{\\alpha}\\mathbf{\\alpha})||}$ and $\\frac{\\nabla_{\\mathbf{\\alpha}}f^{(2)}(\\mathbf{\\alpha}\\mathbf{x})}{||\\nabla_{\\mathbf{\\alpha}}f^{(2)}(\\mathbf{\\alpha}\\mathbf{x})||}$ , which are high-dimensional unit vectors, we explore the properties and potential distributions of $\\mathcal{F}$ through the perspective of spherical statistics. Given an input $x\\in\\mathcal{X}$ , we denote by $\\mathcal{G}_{k}({\\boldsymbol{x}})$ the set of all possible gradient directions of input $\\textbf{\\em x}$ regarding the models $f$ in $\\mathcal{F}(k)$ . Formally, let ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{G}_{k}(\\pmb{x})=\\!\\left\\{\\pmb{u}=\\nabla_{\\pmb{x}}f(\\pmb{x})/\\|\\nabla_{\\pmb{x}}f(\\pmb{x})\\|\\left|\\forall\\pmb{f}\\in\\mathcal{F}(k)\\right\\}\\!,\\forall\\pmb{x}\\in\\mathcal{X}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Then the similarity is re-written as the inner product $\\rho(k_{1},k_{2};\\mathbf{x})=\\mathbb{E}_{\\mathbf{u}_{1}\\in\\mathcal{G}_{k_{1}}(\\mathbf{x}),\\mathbf{u}_{2}\\in\\mathcal{G}_{k_{2}}(\\mathbf{x})}[\\mathbf{u}_{1}^{T}\\mathbf{u}_{2}].$ . ", "page_idx": 3}, {"type": "text", "text": "The Intra-Family vs. Cross-Family Paradox. An interesting paradox is raised as $\\rho(\\cdot,\\cdot)$ increases with both inputs. Naturally, one would reasonably deduce that two models $f^{(1)},f^{(2)}\\in\\mathcal{F}(k_{1})$ should resemble each other since they are from the same family (i.e. having the exact same structure and only differ in training seeds). However, since $\\rho(k_{2},k_{1})>\\rho(k_{1},k_{1})$ , the cross-model family similarity becomes greater than the intra-model family similarity. To uncover the mystery of the observations, we present the intuitive understanding and the rigorously analyzed hypotheses as follows. ", "page_idx": 3}, {"type": "text", "text": "The Intra-Family Hypothesis. Note that for intra-family scenario, ${\\pmb u},{\\pmb v}\\in{\\mathcal G}_{k}({\\pmb x})$ are i.i.d., the similarity can be written as $\\rho(k,k;\\pmb{x})=(\\mathbb{E}_{\\mathcal{G}_{k}(\\pmb{x})}[\\pmb{u}])^{T}(\\mathbb{E}_{\\mathcal{G}_{k}(\\pmb{x})}[\\pmb{v}])=\\|\\mathbb{E}_{\\mathcal{G}_{k}(\\pmb{x})}[\\pmb{u}]\\|_{2}^{2}$ , which denotes the square of the population mean resultant length (Mardia et al., 2000) of $\\mathcal{G}_{k}({\\boldsymbol{x}})$ . The population mean resultant length $\\sqrt{\\rho(k,k;x)}$ quantifies the degree of dispersion of $\\mathcal{G}_{k}({\\boldsymbol{x}})$ , where a larger length suggests a more concentrated distribution. In directional statistics, the degree of dispersion is usually quantified by the spherical variance $2(1-\\sqrt{\\rho(k,k;x)})$ or the total variation $1-\\rho(k,k;x)$ . Since $\\rho(k,k;{\\boldsymbol{x}})$ also increases w.r.t. $k$ , this suggests an increasing concentration of input salience of models as the width $k$ of the model increases. In conclusion, the larger the models are, the smaller the spherical variance of the salience is. Formally, we propose the following hypothesis. ", "page_idx": 3}, {"type": "text", "text": "$k$ edneonteo tteh teh es etw iodft hi n(pcautp agcriatydi) eonft  tdhier emctoiodnels  arnedg $\\mathcal{G}_{k}({\\pmb x})=\\{{\\pmb u}=}$ $\\frac{\\nabla_{\\mathbf{x}}f(\\mathbf{x})}{\\|\\nabla_{\\mathbf{x}}f(\\mathbf{x})\\|}\\left|\\forall f\\;\\in\\;\\mathcal{F}(k)\\right\\}$ $\\textbf{\\em x}$ ", "page_idx": 3}, {"type": "text", "text": "Note that $H l$ also holds for the change of model depths, which is positively related to the dispersion degree of the distribution. However, the change in model depth inevitably affects model width. Thus, we only provide empirical verification in Section 3 but do not include it as a part of $_{\\mathrm{H}2}$ . ", "page_idx": 4}, {"type": "text", "text": "The Cross-Family Hypothesis. Unlike the intra-family similarity, the increasing cross-family similarity is where the phenomenon becomes counter-intuitive. Then due to the increasing intra-family similarities, when $k_{1},k_{2}$ increase, ${\\pmb u}_{1}\\in{\\mathcal G}_{k_{1}}({\\pmb x})$ becomes closer to $\\pmb{\\mu}(k_{1};\\pmb{x})$ , ${\\pmb u}_{2}\\in\\mathcal G_{k_{2}}\\bar{(}{\\pmb x})$ becomes closer to $\\pmb{\\mu}(k_{2};\\mathbf{x})$ . However, the cross-familty similarities suggest that as $\\mathbf{\\boldsymbol{u}}_{1},\\mathbf{\\boldsymbol{u}}_{2}$ approach their respective mean directions, their similarity increases as well. This indicates that the mean directions ${\\pmb{\\mu}}(\\bar{k}_{1};{\\pmb x}),{\\pmb\\mu}(k_{2};{\\pmb x})$ are similar, too. Formally, this intuition is considered as follows. For $k_{1}>k_{2}$ , when $\\pmb{\\mu}(k_{1};\\pmb{x})$ and $\\pmb{\\mu}(k_{2};\\mathbf{x})$ are sufficiently similar, $\\pmb{\\mu}(k_{1};\\pmb{x})^{T}\\pmb{\\mu}(k_{2};\\pmb{x})\\approx\\|\\pmb{\\mu}(k_{1};\\pmb{x})\\|\\|\\pmb{\\mu}(k_{1};\\pmb{x})\\|$ Thus we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\rho(k_{1},k_{2};x)=\\!\\!\\mathbb{E}_{{\\mathbf{u}_{1}}\\in{\\mathcal{G}_{k_{1}}}({\\mathbf{x}}),{\\mathbf{u}_{2}}\\in{\\mathcal{G}_{k_{2}}}({\\mathbf{x}})}[{\\mathbf{u}_{1}^{T}}{\\mathbf{u}_{2}}]=\\!\\!\\mathbb{E}_{{\\mathbf{u}_{1}}\\in{\\mathcal{G}_{k_{1}}}({\\mathbf{x}})}[{\\mathbf{u}_{1}}]^{T}\\mathbb{E}_{{\\mathbf{u}_{2}}\\in{\\mathcal{G}_{k_{1}}}({\\mathbf{x}})}[{\\mathbf{u}_{2}}]}\\\\ &{}&{\\approx\\!\\!\\|{\\mathbb{E}_{{\\mathbf{u}_{1}}\\in{\\mathcal{G}_{k_{1}}}({\\mathbf{x}})}[{\\mathbf{u}_{1}}]}\\|\\|{\\mathbb{E}_{{\\mathbf{u}_{2}}\\in{\\mathcal{G}_{k_{1}}}({\\mathbf{x}})}}[{\\mathbf{u}_{2}}]\\|=\\sqrt{\\rho(k_{1},k_{1};x)\\rho(k_{2},k_{2};x)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is monotonic w.r.t. both $k_{1},k_{2}$ . Formally, this is summarized as ", "page_idx": 4}, {"type": "text", "text": "\u2022 Hypothesis II $\\left(\\mathbf{H}2\\right)$ : Let $\\mathcal{G}_{k_{1}}(\\pmb{x}),\\mathcal{G}_{k_{2}}(\\pmb{x})$ denote the input gradient directions of two model families where $k_{1}\\neq k_{2}$ . Then $\\pmb{\\mu}(k_{1};\\pmb{x})\\approx\\pmb{\\mu}(k_{2};\\pmb{x})$ regardless of $k_{1},k_{2}$ . ", "page_idx": 4}, {"type": "text", "text": "The two hypotheses are both empirically verified. For a smoother flow of the presentation, we defer the detailed experiments to Section 3. The basic ideas of H1 and H2 are illustrated in Figure 4 (a). ", "page_idx": 4}, {"type": "text", "text": "2.3 The Directional Distribution of Gradients ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given the analysis and hypothesis above, one can have an overview of the models\u2019 internal mechanisms. As the model capacity increases, models are distributed in a more concentrated manner, while the mean direction stays almost invariant. To better understand the models\u2019 behavior with the stochasticity, we delve into the distribution of $\\mathcal{G}_{k}(x)$ and present a semi-parametric analysis with experimental verification. A general form of centralized symmetric distribution over hypersphere is known as the Saw distribution (Fisher et al., 1993) $\\begin{array}{r}{p(\\boldsymbol{u};\\boldsymbol{\\mu})\\,=\\,\\frac{\\psi(\\boldsymbol{u}^{T}\\boldsymbol{\\mu})}{Z}}\\end{array}$ , where $\\pmb{\\mu}$ is the mean direction with $\\|\\pmb{\\mu}\\|=1$ , $\\psi\\in\\mathcal{C}([-1,1])$ , and $\\begin{array}{r}{Z=\\int_{S^{d-1}}\\psi(\\boldsymbol{u}^{T}\\boldsymbol{\\mu})\\mathrm{d}\\boldsymbol{u}}\\end{array}$ is the normalization term for distributions. Due to the symmetry assumption, the shape of the distribution is solely determined by $\\psi$ . For example, a monotonically increasing $\\psi$ suggests that $\\textbf{\\em u}$ is distributed more densely near the mean direction and sparsely distant from the mean direction. Considering the concentration trend of gradients, we hypothesize that $\\psi_{k}(\\cdot)$ of $\\mathcal{G}_{k}({\\boldsymbol{x}})$ not only monotonically increases with the input, but also increases faster with greater $k$ values. ", "page_idx": 4}, {"type": "text", "text": "Marginalization. For $\\forall\\pmb{u}\\in\\mathcal{G}_{k}(\\pmb{x})$ , it can be decomposed to ${\\pmb u}=t\\cdot{\\pmb\\mu}({\\pmb x})+\\sqrt{1-t^{2}}\\cdot{\\pmb\\mu}({\\pmb x})^{\\perp}$ , where $\\pmb{\\mu}(\\pmb{x})^{\\perp}$ is a unit tangent to $S^{d-1}$ at $\\pmb{\\mu}$ . Then $t=u^{T}\\pmb{\\mu}(\\pmb{x})$ . This is shown in Figure 4 (b). Note that $\\pmb{\\mu}(\\pmb{x})^{\\bot}$ is independent from $t$ , then the distribution of $t$ is the marginal distribution over the intersection between $S^{d-1}$ and the hyperplane spanned by $\\pmb{\\mu}(\\pmb{x})^{\\perp}$ , which is a $(d-2)$ -dimensional hypersphere. According to the symmetry assumption of Saw distribution, conditioned on a fixed similarity $t$ , the distribution of $\\pmb{u}|t$ over the dashed $S^{d-2}$ does not affect $\\psi$ . Therefore, we focus on the marginalized distribution of $t$ . Note that the radius of the intersection $S^{d-2}$ is $\\sqrt{1-t^{2}}$ , we thus have du = 2\u03c0(d\u22121)/2(1\u2212t2)(d\u22123)/2d , where the density of $t$ is observed by the integral over the corresponding $(d-2)$ -hypersphere. As a result, the marginal distribution of $t$ has the PDF ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{k}(t;{\\pmb x})=\\psi_{k}(t;{\\pmb x})(1-t^{2})^{(d-3)/2}/Z^{\\prime}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $Z^{\\prime}$ is a constant normalization term. Note that $(1-t^{2})^{(d-3)/2}$ is a symmetric bell curve centered at $t=0$ . Equation (4) can thus be viewed as using $\\psi_{k}(t;{\\mathbf{x}})$ to reweight its PDF $p_{\\mathrm{origin}}(t)=$ $\\begin{array}{r l r}{\\lefteqn{(1-t^{2})^{(d-3)/2}\\frac{\\Gamma(d/2)}{\\sqrt{\\pi}\\Gamma((d-1)/2)}}}\\end{array}$ 2\u221a\u03c0\u0393\u0393(((dd/\u221221))/2). Note that here pk(t; x) is the distribution of t = uT \u00b5(k; x), which can be empirically estimated, the shape of the function $\\psi_{k}$ becomes empirically accessible with varying $k$ values. The empirical studies and verification are provided in Section 3.3. ", "page_idx": 4}, {"type": "image", "img_path": "7PORYhql4V/tmp/3824f54db3188cc2b97f70924cfc6729570484990a515c149dbe93ad623aab44.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 4: (a) presents an illustration of H1 and H2. Blue and green caps represent ${\\pmb u}_{1}\\,\\in\\,\\mathcal{G}_{k_{1}}({\\pmb x})$ (green) and ${\\pmb u}_{2}\\in\\mathcal G_{k_{2}}({\\pmb x})$ (blue) regions 2. H1: larger ks lead to smaller spherical variances; H2: the mean directions are extremely similar. (b) illustrates (left) the decomposition of $\\textbf{\\em u}$ to the mean direction and the orthogonal direction; and (right) the marginalization of the distribution from $\\textbf{\\em u}$ to $t$ ", "page_idx": 5}, {"type": "text", "text": "3 Empirical Verification of Hypotheses ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we provide comprehensive experimental results to verify the aforementioned hypotheses. First, we introduce the detailed setups of our experiments. They are carried out on Intel(R) Core(TM) i9-9960X CPU $@$ 3.10GHz with Quadro RTX 6000 GPUs. ", "page_idx": 5}, {"type": "text", "text": "Datasets & Models. Due to the massive size of experiments, here we mainly follow the setups of the benign overftiting (Nakkiran et al., 2021), which also present a comprehensive study of optimized DNNs through CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009). Besides, we also include TinyImagenet-200 (Le and Yang, 2015) as a compromise between the computational efficiency and the dataset complexity. As for models, we include CNNs and ResNets as in (Nakkiran et al., 2021). These two models represent the two typical architectures \u2013 the direction connection and the skip connection. We also notice a striking capacity gap between them in the original implementation. Therefore, we term them CNNSmall (CS) and ResNetLarge (RL), respectively, and include CNNLarge (CL), and ResNetSmall (RS) to fill the gap. The comparison between the small and large families also shows the influence of depths. As for the training process, following Nakkiran et al. (2021), we use stochastic gradient descent (SGD) as the solver, with a batch size of 128. The input data are normalize\u221ad, but not augmented. We start with the initial learning rate $\\gamma_{0}=0.1$ and update it with $\\gamma_{t}=\\gamma_{0}/\\sqrt{1+t}$ , where $t$ is the epoch. Please refer to Appendix $\\mathbf{B}$ for more experimental details. ", "page_idx": 5}, {"type": "text", "text": "3.1 The Mean Direction Similarity $\\left(\\mathbf{H}2\\right)$ ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "H2 can be verified independently from H1 and can provide simplifications and insights to verifying H1. We hence focus on H2 first. As stated in H2, the mean directions of different model families are consistently aligned, i.e., $\\pmb{\\mu}(k_{1};\\pmb{x})\\approx\\pmb{\\mu}(k_{2};\\pmb{x})$ . For each family $\\mathcal{F}(k)$ where $k\\in K^{\\prime}$ , $M=100$ models with different random seeds are trained. The population mean is then estimated through ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\tilde{\\mu}(k;x)=\\Big(\\frac{1}{M}\\sum_{i=1}^{M}\\frac{\\nabla_{x}f_{i}(x)}{\\|\\nabla_{x}f_{i}(x)\\|}\\Big)\\Big/\\Big\\|\\frac{1}{M}\\sum_{i=1}^{M}\\frac{\\nabla_{x}f_{i}(x)}{\\|\\nabla_{x}f_{i}(x)\\|}\\Big\\|\\approx\\mu(k;x),f_{i}\\in\\mathcal{F}(k).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then the similarity of mean directions are naturally $(\\tilde{\\pmb{\\mu}}(k_{1};\\pmb{x}))^{T}\\tilde{\\pmb{\\mu}}(k_{2};\\pmb{x})$ . Note that when $k_{1}=k_{2}$ , the 100 models are partitioned by the seeds to avoid overlapping. The results of the expectation over $\\mathcal{X}$ are visualized in Figure 5. It can be found that the mean directions have extremely high resemblance within each architecture, as proved by the high cosine similarities. It should be noted that with high dimensionality (e.g. $d\\,=\\,3072$ for CIFAR), a cosine similarity close to 1 is an extremely significant result. We demonstrate this with the uniform distribution on the hypersphere in Appendix D. The observations verify the hypothesis all $\\mathcal{G}_{k}({\\boldsymbol{x}})$ almost share the same mean direction within the model architecture. This not only hold across different widths determined by $k$ , but also holds across different depths (i.e. CS vs. CL, RS vs. RL). Therefore, the mean direction is mostly related to the certain model architecture instead of any single model $f\\in\\mathcal F$ , making it an intrinsic property of the model architecture. With this property, how different model architectures differ in mechanisms can be studied by looking deeper into the population mean direction of saliency maps. For instance, it can be observed that the ResNet architecture admits a closer relation between models of different depths, compared with the CNN architecture. ", "page_idx": 5}, {"type": "image", "img_path": "7PORYhql4V/tmp/db2bf8ee015df1b2a024716140c00b4f340966fbd96f57ca393bc1eb7a2dd961.jpg", "img_caption": ["Figure 5: The heatmap visualization between the estimated population mean directions from different model families. Each entry is computed by $\\mathbb{E}_{\\pmb{x}\\in\\mathcal{X}}\\mathrm{CosSim}(\\tilde{\\pmb{\\mu}}_{j}(\\pmb{x};\\mathcal{F}),\\tilde{\\pmb{\\mu}}_{j^{\\prime}}(\\pmb{x};\\mathcal{F}^{\\prime}))$ The results are generated from CIFAR-10/100 and TinyImagenet datasets. $\\mathcal{F}\\in\\{\\mathrm{CS},\\mathrm{CL}$ , RS, RL}. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "7PORYhql4V/tmp/33c468031fa0568428ba6ddf3e124f910c64e38d919a1e83040ec2c1d235e225.jpg", "img_caption": ["Figure 6: Illustration of (red) $\\rho_{i n d}(f^{(1)},f^{(2)}),f^{(1)},f^{(2)}\\in\\mathcal{F}(k)$ and (blue) $\\rho(k,k)$ on CIFAR-10. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "3.2 The Decreasing Spherical Variance with $k$ (H1) ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Expectation over $\\mathcal{F}$ vs. Conditioned on $f\\in\\mathcal F$ . As previously discussed, the spherical variance of distributions over the hypersphere can be measured by the population mean resultant length $\\sqrt{\\rho(k,k;x)}$ , which, unfortunately, requires an estimation of the mean directions. This can be expensive to study for a comprehensive set $K$ of $k$ values. The experiments on a subset $K^{\\prime}\\,=$ $\\{10,20,40,80,160\\}$ are already carried out in Figure 3, shown as the diagonal elements. As $k$ increases, the resultant length increases monotonically, indicating a decreasing spherical variance and a more concentrated distribution around the mean directions. ", "page_idx": 6}, {"type": "text", "text": "The computational burden of taking the expectation over $\\mathcal{F}$ can be alleviated by considering randomly picked $f$ . In order to compare $\\rho$ and $\\rho_{i n d}$ , we consider the model-dependent set $S(f)\\;=\\;\\bar{\\{\\rho}_{i n d}(f,f;x)\\;:\\;x\\;\\in\\;\\chi\\}$ for each $f\\ \\in\\ {\\mathcal{F}}$ . Here we compute the expected Wasserstein distance $\\mathbb{E}_{f^{(1)},f^{(2)}\\in\\mathcal{F}(k)}\\mathsf{W D}\\big(S\\big(f^{(1)}\\big),S\\big(f^{(2)}\\big)\\big)$ . This is estimated by the $\\binom{M}{2}$ distinct pairs of models. The distances of all 60 (dataset, model family) pairs lie below 0.035. Such observation suggests that after taking the expectation over $\\mathcal{X}$ , the differences across individual models can be mitigated. Please refer to Table 1 for comprehensive results on all model families and datasets. As a consequence, it suffices to use $\\rho_{i n d}(f,f)$ for some $f\\,\\in\\,{\\mathcal{F}}(k)$ to approximate $\\rho(k,k)$ . This is in fact the diagonal elements of Figure 2. A comparison between the diagonal elements $\\rho_{k,k}$ and $\\rho_{i n d}(f^{(1)},f^{(2)}),f^{(1)},f^{(2)}\\in\\mathcal{F}(k)$ over CIFAR-10 is presented in Figure 6. Please refer to the appendix for other datasets. $\\rho_{i n d}$ is evaluated over $k\\in K$ , while $\\rho$ is evaluated over $k\\in K^{\\prime}\\subset K$ . It can be found that after taking the average over $\\mathcal{X}$ , even though $\\rho$ is a little smoother than $\\rho_{i n d}$ , they are very consistent. This verifies that the resultant length increases with $k\\in K$ in a much more comprehensive set of models. Thus H1 is empirically verified. ", "page_idx": 6}, {"type": "image", "img_path": "7PORYhql4V/tmp/fbc037a38a5c7db8f7aabb3d95584436090d41addd63dcda377103ebbacaf5bb.jpg", "img_caption": ["Figure 8: (left) The illustration of the frequency of Figure 9: The illustration of the relation bethe mixture Tk, where k \u2208{10, 20, 40, 80, 160}. tween the expected testing loss EX [L] and the Specifically, the black histogram represents the marginal expectation $\\mathbb{E}_{\\mathcal{X}}[t]$ , where models are distribution $p_{o r i g i n}$ . The dashed curves are the from (a) single models with varying capacities (b) approximated PDF pk obtained by KDE. The re- deep ensembles with varying member #. Each 10. (right) The illustration of logpoprikgin  in (b), different marker shapes indicate different sults are generated using CNNSmall and CIFAR- color represents a model family. In particular, , which is linearly related to $\\log{\\psi_{k}}$ . $k\\in[10,20,40,80]$ of the ensembles. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "3.3 The Shape of the Saw Distribution ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given $\\mathcal{G}_{k}({\\boldsymbol{x}})$ , the marginalized distribution $p_{k}(t;{\\mathbf{x}})$ can be approximated by ${\\cal{T}}_{k}({\\pmb x})~=~\\{\\tilde{t}~=~$ $\\pmb{u}^{T}\\tilde{\\pmb{\\mu}}(k;\\pmb{x})|\\pmb{u}\\,\\in\\,\\mathcal{G}_{k}(\\pmb{x})\\bar{\\}$ . In order to obtain the global results over test dataset $\\mathcal{X}$ , consider the unions of different input samples $\\begin{array}{r}{\\mathcal{T}_{k}=\\cup_{{\\mathbf x}\\in\\mathcal{X}}\\mathcal{T}_{k}({\\boldsymbol x})}\\end{array}$ . This is an approximation to the mixture distributions $\\begin{array}{r}{p_{k}(t)=\\frac{1}{|\\mathcal{X}|}\\,\\dot{\\sum}_{\\pmb{x}\\in\\mathcal{X}}\\,\\dot{p_{k}}(t;\\pmb{x})}\\end{array}$ . We plot the histogram of $\\mathcal{T}_{k}$ with $k\\in K^{\\prime}$ for CNNSmall and CIFAR-10 in Figure 8. The left figure shows $p_{o r i g i n}$ and the estimated $p_{k}$ , visualized by different colors. Qualitatively, $p_{k}(t)$ has higher means with greater $k$ values. The reason why the density of $p_{k}$ is not centered at $t=1$ (i.e. $\\pmb{u}=\\pmb{\\mu}$ ) is because as $t$ increases, the size of the $(d-2)$ -hypershpere decreases with $\\begin{array}{r}{p_{\\mathrm{origin}}(t)=(1-t^{2})^{(d-3)/2}\\frac{\\Gamma(d/2)}{\\sqrt{\\pi}\\Gamma((d-1)/2)}}\\end{array}$ \u221a\u03c0\u0393\u0393(((dd/\u221221))/2), which is shown as black histograms. This is much faster than the increase of $\\psi_{k}$ . The shape of $\\psi_{k}$ is determined by $p_{k}/p_{\\mathrm{origin}}$ with normalization. From the right figure, by comparing $p_{k}$ with $p_{\\mathrm{PDF}}$ , it can be empirically verified that $\\psi_{k}(t)$ is increasing vastly. It is also observed that larger $k\\mathbf{s}$ lead to a faster increase of $\\psi_{k}$ and higher $\\mathbb{E}[t]$ . This also provides a quantitative understanding of $_{\\mathrm{H}1}$ and $_{\\mathrm{H}2}$ . The results of other model architectures and datasets can be found in the appendix. ", "page_idx": 7}, {"type": "text", "text": "Verification of the Symmetry. Saw distributions study the marginalized value $t\\,=\\,u^{T}\\pmb{\\mu}$ to directly focus on the degree of concentration of the gradients. This naturally leads to rotationally symmetric distributions since the distribution on the intersection between $S^{d-1}$ and the hyperplane does not affect the distribution of $t$ . We thus carry out an empirical study of the distribution on the intersection (i.e. conditioned on $t$ ). Specifically, we train 1000 CNN models with $K\\,=\\,40$ and seeds 1-1000 on CIFAR-10 and compute $t$ regarding each test sample. The distribution of the first sample is visualized in Figure 7. We partition the range of $t$ into 10 intervals by every 10 percent of the frequency, and inspect the direction of the mean of the gradients in each interval, each direction is estimated by 100 models. If these conditional mean directions are consistent with the population mean direction, then the gradients are symmetrically distributed on each $S^{d-2}$ hypersphere (R7(right)), thus verifying the rotational symmetry. We investigate the cosine similarities between the conditional and unconditional mean directions on the first 1000 samples. The $10\\times1000$ similarity values have a mean and std at approximately 0.970 and 0.013 respectively. Thus the rotational symmetry is empirically verified. ", "page_idx": 7}, {"type": "image", "img_path": "7PORYhql4V/tmp/77111cd92c96c6126e56e071a48d30c6a466d68bfc5abbf6db6a6f2fbb533b8a.jpg", "img_caption": ["Figure 7: The marginal distribution of $t$ of the first test sample of CIFAR-10. Red dashed lines partition the range of $t$ every 10 percent of the frequency. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "7PORYhql4V/tmp/7c9d203e95a1df8534bbb77d371f85c22da815de63e6f3e65c2425b0cc6026ac.jpg", "img_caption": ["Figure 10: The results of single model black-box attack. The value of each entry is $\\alpha(k_{1},k_{2})$ for different model capacities, where $k_{1}$ is the width parameter of the source model and $k_{2}$ is the width parameter of the target model. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "7PORYhql4V/tmp/a6bf6c5ba50a4341340d25729ce4352735a8469e50c1e4e53696ce7bd69020e5.jpg", "img_caption": ["Figure 11: The comparison between the single-model attack from the largest model (red), the singlemodel attack from the very same capacity (green) and the attack by the mean direction (blue). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4 Applications of Hypotheses ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "4.1 Deep Ensemble: Why Does It Work? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "After verifying the hypotheses, we explore possible applications and implications of the discovered phenomena. The deep ensemble method makes use of the stochasticity to of models by incorporating the predictions from $m$ members. While deep ensembles have been verified to be effective in improving performance, the source of such capability remains mysterious. Note that ensemble members are i.i.d. optimized models with SGD, which correspond to the population mean of our hypothesis. We thus provide another perspective in understanding the capability of deep ensembles. ", "page_idx": 8}, {"type": "text", "text": "For single models, as the model capacity increases, benign overfitting suggests that the testing loss decreases, too. We deduce that this is because the distribution of larger models becomes more concentrated, and combined with H2, the closeness to the aligned population mean is highly related to the models\u2019 testing performance. As shown in Figure 9(a), it can be observed that the expected loss $\\mathbb{E}_{\\mathcal{X}}[\\mathcal{L}]$ and the marginal expectation $\\mathbb{E}_{\\mathcal{X}}[t]$ are highly correlated. Similarly, deep ensemble approximates the population mean much more effectively by increasing the number $m$ of members than scaling up a single model by $k$ . We thus scale up the deep ensemble by changing the number of ensemble members. The results are shown in Figure 9(b). It can be found that (1) the correlation between $\\mathbb{E}_{\\mathcal{X}}[\\mathcal{L}]$ and $\\mathbb{E}_{\\mathcal{X}}[t]$ is not only significant, and (2) the correlation pattern is shared between two completely different scaling mechanisms, single model scaling and model ensembles. ", "page_idx": 8}, {"type": "text", "text": "4.2 Black-Box Attack via Saliency Similarity ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The understanding of adversarial attacks can benefit from the behaviors of the input salience of optimized models given their close relation to input gradients. We verify the aforementioned similarities through the black-box attacks, where the adversarial samples are generated from the gradients of source models while the gradients of the target models are not available. Let $f^{(1)}\\in\\mathcal{F}(k_{1})$ denote the source model and $f^{(2)}\\in\\mathcal{F}(k_{2})$ denote the target model. We define the attack rate from ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\alpha(f^{(1)},f^{(2)})=\\underset{x\\in\\mathcal{X}}{\\mathbb{E}}\\Big[f^{(2)}\\big(\\mathbf{\\boldsymbol{x}}-\\boldsymbol{\\epsilon}\\cdot\\mathbf{\\delta}\\mathbf{sign}(\\nabla_{x}f^{(1)}(\\pmb{x}))\\big)\\big/f^{(2)}(\\pmb{x})\\Big]\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "which is the performance decay of $f^{(2)}$ when attacked by model $f^{(1)}$ . Small $\\alpha(k_{1},k_{2})$ values suggest successful attack from $f^{(1)}$ to $f^{(2)}$ . The results are shown in Figure 10, where the attack step is set to $\\epsilon=0.05$ . In each heatmap figure, the $y$ -axis represents the width of the source models, while the $x$ -axis represents the width of the target models. It can be observed that larger models succeed in attacking smaller models, but the opposite is not true. To attack a large model, the gradient needs to be generated from a model of a comparable level in terms of capacity. ", "page_idx": 9}, {"type": "text", "text": "Mean Direction Attack. According to the verified hypothesis H2, for any two individual models $f^{(1)}\\in\\mathcal{F}(k_{1})$ , $f^{(2)}\\in\\mathcal{F}(k_{2})$ the mean directions $\\pmb{\\mu}(k;\\pmb{x})$ is closer to both of them than themselves, regardless of $k,k_{1},k_{2}$ . It is then suggested that using the mean gradient can perform more successful black-box attacks. We employ the mean direction $\\tilde{\\pmb{\\mu}}(160,\\pmb{x})$ to attack models of different capacities, and compare the results with the attack from the largest single model (red) and the attack from the models of the identical structure (green). The results are shown in Figure 11, where it can be observed that the mean direction of salience transfers much more successfully than single models. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce hypotheses to explain the observations on the input salience convergence w.r.t. the model capacities. Under the same model architecture, stochastic algorithms such as SGD, result in certain distributions of optimized models. We hypothesize and use pointwise methods to verify that such distribution follows a Saw distribution with aligned population means, which is invariant from the model families. Besides, the variance of the distribution decreases as the model capacity increases, suggesting a convergence trend of the models\u2019 internal mechanism \u2013 the larger the models are, the less variant they tend to be affected by the randomness from the stochastic algorithm during the training phase. Furthermore, since the distributions converge towards the aligned population mean direction, the limiting points can be estimated by the population mean of models. Based on this, we present comprehensive experiments on the properties of the limiting model and demonstrate its capability in various domains, such as the black-box attack transferability, and the explanation of the effectiveness of deep ensembles. However, it is admitted that, due to the high computational burden, although improved from CIFAR-10/100 to TinyImagenet compared to (Nakkiran et al., 2021), our experiments are limited to rather small datasets. ", "page_idx": 9}, {"type": "text", "text": "Our introduced hypotheses also lead to various interesting topics. Note that the aligned mean direction stays invariant to the model families, which indicates such population mean is more related to the essence of the dataset itself rather than any single model. Leveraging this property can bring a deeper and more comprehensive understanding of the relation between data distributions and models. ", "page_idx": 9}, {"type": "text", "text": "6 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In terms of the convergence trend of DNNs, existing works focus on the convergence of single models throughout the training process. The parameters of DNNs have been demonstrated to converge to global minima throughout the training progress (Goodfellow et al., 2014b; Li et al., 2018; Allen-Zhu et al., 2019; Liu et al., 2020; Damian et al., 2021; Refinetti et al., 2023; Suh and Cheng, 2024). Recent years, the studies of benign overfitting also suggest that increasing model capacities can improve the performance instead of exacerbating the overfitting issue (Bartlett et al., 2020; Nakkiran et al., 2021; Cao et al., 2022). While the studies of input gradients span into an abundant but extremely complicated spectrum. Among them, the area that is the most related to our work is the XAI domain, where the input gradient and its variants are crucial in revealing the models\u2019 internal mechanisms (Simonyan et al., 2013; Springenberg et al., 2014; Selvaraju et al., 2017; Sundararajan et al., 2017; Adebayo et al., 2018; Shah et al., 2021). On the other hand, the studies of the distribution of optimized models have received little attention. Such topics are slightly dipped in the efforts to demystify the source of capability of deep ensembles (Lee et al., 2015; Fort et al., 2019; Allen-Zhu and Li, 2020; Kobayashi et al., 2021; Abe et al., 2022; Ganaie et al., 2022; Theisen et al., 2023) and their implications (Lakshminarayanan et al., 2017; Geiger et al., 2020; Yang et al., 2021; Chen et al., 2023). Thus, to our knowledge, the studies on the distribution of optimized models remain a novel topic. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported, in part, by the Defense Advance Research Projects Agency (Prime contract award number: HR0011222003, subcontract award number: 2103299-01, grant number: 13001129), the EMBRIO Institute, contract #2120200, a National Science Foundation (NSF) Biology Integration Institute, and NSF IIS #1955890, IIS #2146091, IIS #2345235. The content of the information does not necessarily reflect the position of the US Government. No official endorsement should be inferred. Approved for public release; distribution is unlimited. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abe, T., Buchanan, E. K., Pleiss, G., Zemel, R., and Cunningham, J. P. (2022). Deep ensembles work, but are they necessary? Advances in Neural Information Processing Systems, 35:33646\u201333660. ", "page_idx": 10}, {"type": "text", "text": "Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B. (2018). Sanity checks for saliency maps. Advances in neural information processing systems, 31.   \nAllen-Zhu, Z. and Li, Y. (2020). Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816.   \nAllen-Zhu, Z., Li, Y., and Song, Z. (2019). A convergence theory for deep learning via overparameterization. In International conference on machine learning, pages 242\u2013252. PMLR.   \nArrieta, A. B., D\u00edaz-Rodr\u00edguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garc\u00eda, S., Gil-L\u00f3pez, S., Molina, D., Benjamins, R., et al. (2020). Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information fusion, 58:82\u2013115.   \nBartlett, P. L., Long, P. M., Lugosi, G., and Tsigler, A. (2020). Benign overftiting in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063\u201330070.   \nBerner, C., Brockman, G., Chan, B., Cheung, V., D\u02dbebiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. (2019). Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680.   \nBubeck, S. and Sellke, M. (2021). A universal law of robustness via isoperimetry. Advances in Neural Information Processing Systems, 34:28811\u201328822.   \nCao, Y., Chen, Z., Belkin, M., and Gu, Q. (2022). Benign overfitting in two-layer convolutional neural networks. Advances in neural information processing systems, 35:25237\u201325250.   \nChen, H., Zhang, Y., Dong, Y., and Zhu, J. (2023). Rethinking model ensemble in transfer-based adversarial attacks. arXiv preprint arXiv:2303.09105.   \nDamian, A., Ma, T., and Lee, J. D. (2021). Label noise sgd provably prefers flat global minimizers. Advances in Neural Information Processing Systems, 34:27449\u201327461.   \nDeng, D. and Shi, E. B. (2021). Ensembling with a fixed parameter budget: When does it help and why? In Asian Conference on Machine Learning, pages 1176\u20131191. PMLR.   \nDu, S., Lee, J., Li, H., Wang, L., and Zhai, X. (2019). Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pages 1675\u20131685. PMLR.   \nFisher, N. I., Lewis, T., and Embleton, B. J. (1993). Statistical analysis of spherical data. Cambridge university press.   \nFort, S., Hu, H., and Lakshminarayanan, B. (2019). Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757.   \nGanaie, M. A., Hu, M., Malik, A., Tanveer, M., and Suganthan, P. (2022). Ensemble deep learning: A review. Engineering Applications of Artificial Intelligence, 115:105151.   \nGeiger, M., Jacot, A., Spigler, S., Gabriel, F., Sagun, L., d\u2019Ascoli, S., Biroli, G., Hongler, C., and Wyart, M. (2020). Scaling description of generalization with number of parameters in deep learning. Journal of Statistical Mechanics: Theory and Experiment, 2020(2):023401.   \nGoodfellow, I. J., Shlens, J., and Szegedy, C. (2014a). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.   \nGoodfellow, I. J., Vinyals, O., and Saxe, A. M. (2014b). Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544.   \nGunning, D. and Aha, D. (2019). Darpa\u2019s explainable artificial intelligence (xai) program. AI magazine, 40(2):44\u201358.   \nGustafsson, F. K., Danelljan, M., and Schon, T. B. (2020). Evaluating scalable bayesian deep learning methods for robust computer vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 318\u2013319.   \nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778.   \nHuang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., and Weinberger, K. Q. (2017). Snapshot ensembles: Train 1, get m for free. arXiv preprint arXiv:1704.00109.   \nKobayashi, S., von Oswald, J., and Grewe, B. F. (2021). On the reversed bias-variance tradeoff in deep ensembles. ICML.   \nKondratyuk, D., Tan, M., Brown, M., and Gong, B. (2020). When ensembling smaller models is more efficient than single large models. arXiv preprint arXiv:2005.00570.   \nKrizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.   \nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25.   \nLakshminarayanan, B., Pritzel, A., and Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30.   \nLe, Y. and Yang, X. (2015). Tiny imagenet visual recognition challenge. CS 231N, 7(7):3.   \nLee, S., Purushwalkam, S., Cogswell, M., Crandall, D., and Batra, D. (2015). Why m heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint arXiv:1511.06314.   \nLi, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. (2018). Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31.   \nLi, Y., Bai, S., Zhou, Y., Xie, C., Zhang, Z., and Yuille, A. (2020). Learning transferable adversarial examples via ghost networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11458\u201311465.   \nLiu, S., Papailiopoulos, D., and Achlioptas, D. (2020). Bad global minima exist and sgd can reach them. Advances in Neural Information Processing Systems, 33:8543\u20138552.   \nLobacheva, E., Chirkova, N., Kodryan, M., and Vetrov, D. P. (2020). On power laws in deep ensembles. Advances In Neural Information Processing Systems, 33:2375\u20132385.   \nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.   \nMardia, K. V., Jupp, P. E., and Mardia, K. (2000). Directional statistics, volume 2. Wiley Online Library.   \nMuller, M. E. (1959). A note on a method for generating points uniformly on n-dimensional spheres. Communications of the ACM, 2(4):19\u201320.   \nNakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I. (2021). Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12):124003.   \nPadmaja, B., Moorthy, C. V., Venkateswarulu, N., and Bala, M. M. (2023). Exploration of issues, challenges and latest developments in autonomous cars. Journal of Big Data, 10(1):61.   \nPapyan, V., Han, X., and Donoho, D. L. (2020). Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652\u2013 24663.   \nPreece, A., Harborne, D., Braines, D., Tomsett, R., and Chakraborty, S. (2018). Stakeholders in explainable ai. arXiv preprint arXiv:1810.00184.   \nRajkomar, A., Oren, E., Chen, K., Dai, A. M., Hajaj, N., Hardt, M., Liu, P. J., Liu, X., Marcus, J., Sun, M., et al. (2018). Scalable and accurate deep learning with electronic health records. NPJ digital medicine, 1(1):18.   \nRefinetti, M., Ingrosso, A., and Goldt, S. (2023). Neural networks trained with sgd learn distributions of increasing complexity. In International Conference on Machine Learning, pages 28843\u201328863. PMLR.   \nRibeiro, M. T., Singh, S., and Guestrin, C. (2016). \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144.   \nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695.   \nRudin, C. (2018). Please stop explaining black box models for high stakes decisions. Stat, 1050:26.   \nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626.   \nShah, H., Jain, P., and Netrapalli, P. (2021). Do input gradients highlight discriminative features? Advances in Neural Information Processing Systems, 34:2046\u20132059.   \nSimonyan, K., Vedaldi, A., and Zisserman, A. (2013). Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034.   \nSmith, I., Ortmann, J., Abbas-Aghababazadeh, F., Smirnov, P., and Haibe-Kains, B. (2023). On the distribution of cosine similarity with application to biology. arXiv preprint arXiv:2310.13994.   \nSpringenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. (2014). Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806.   \nSuh, N. and Cheng, G. (2024). A survey on statistical theory of deep learning: Approximation, training dynamics, and generative models. arXiv preprint arXiv:2401.07187.   \nSundararajan, M., Taly, A., and Yan, Q. (2017). Axiomatic attribution for deep networks. In International conference on machine learning, pages 3319\u20133328. PMLR.   \nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. (2013). Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.   \nTheisen, R., Kim, H., Yang, Y., Hodgkinson, L., and Mahoney, M. W. (2023). When are ensembles really effective? arXiv preprint arXiv:2305.12313.   \nThirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F., and Ting, D. S. W. (2023). Large language models in medicine. Nature medicine, 29(8):1930\u20131940. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Van der Velden, B. H., Kuijf, H. J., Gilhuijs, K. G., and Viergever, M. A. (2022). Explainable artificial intelligence (xai) in deep learning-based medical image analysis. Medical Image Analysis, 79:102470. Wang, Y. and Wang, X. (2022). \u201cwhy not other classes?\u201d: Towards class-contrastive back-propagation explanations. Advances in Neural Information Processing Systems, 35:9085\u20139097. Yang, Z., Li, L., Xu, X., Zuo, S., Chen, Q., Zhou, P., Rubinstein, B., Zhang, C., and Li, B. (2021). Trs: Transferability reduced ensemble via promoting gradient diversity and model smoothness. Advances in Neural Information Processing Systems, 34:17642\u201317655. ", "page_idx": 13}, {"type": "text", "text": "A Additional Results of the Similarities ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The illustration of CIFAR-100 & TinyImagenet. In addition to the cosine similarity $\\rho_{i n d}(k_{1},k_{2})$ shown in Figure 2, we include more sophisticated datasets such as CIFAR-100 and TinyImagenet in Figure 12. It is observed that the observed phenomena, where $\\rho_{i n d}(k_{1},k_{2})$ tend to increase w.r.t. both $k_{1},k_{2}$ , hold across different datasets. It is also worth noticing that compared with the results of CIFAR-10 shown in Figure 2, the resulting $\\rho_{i n d}(k_{1},k_{2})$ of CIFAR-100 and TinyImagnet (Figure 12) shows another peak at around $k\\approx10$ (see Figure 12(e)). This is related to the deep double descent phenomenon (Nakkiran et al., 2021), where when the complexities of the model and the dataset are comparable, the overfitting issue is at peak. For smaller such as CIFAR-10 or larger models such as ResNet, this issue becomes much less significant, as a very small $k$ value is already sufficient for the data distribution. Also, as the complexity of the data distribution increases, the cosine similarity decreases inevitably for the same model complexity. This suggests a less concentrated distribution of the optimized models of the same capacity compared with less complex dataset. Correspondingly, the results of the expectation over the model sets for $k\\in K^{\\prime}=[1\\bar{0},20,40,80,160]$ are shown in Figure 13. ", "page_idx": 14}, {"type": "image", "img_path": "7PORYhql4V/tmp/ac68573c8de4ebbd1a48ab54f377c7987fe9f2b3a573d19a9174bf6439602233.jpg", "img_caption": ["Figure 12: The expected similarity $\\rho(k_{1},k_{2})$ between models of varying widths $k_{1},k_{2}$ . Here we include CNNSmall, CNNLarge, ResNetSmall, and ResNetLarge as $\\mathcal{F}$ . The values of $k_{1},k_{2}$ determine the widths in each layer. Here the datasets are CIFAR-10 (top), CIFAR-100 (middle) and tinyImagenet (bottom). "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "The Ablation of the Training Process. To verify that the observed increasing trends of $\\rho_{i n d}(k_{1},k_{2})$ with model capacities are caused by the training process of DNNs instead of some normalization issue, we compare the similarity for models with initialized parameters. The results are shown in Figure 14. All three datasets and four model families are included. It can be clearly observed that, when the model parameters are initialized, the similarity between input saliency maps of different models are distributed randomly. The cosine similarity values are very concentrated around 0, which is the mean of random distribution. This verifies that the aforementioned increasing trends are caused by the optimization of models instead of normalization process. ", "page_idx": 14}, {"type": "image", "img_path": "7PORYhql4V/tmp/3fcdd174d3ce3f18c4130b2a7b15ca7deb00237eed4824b78510a45ced6270d3.jpg", "img_caption": ["Figure 13: The expected similarity $\\rho(k_{1},k_{2})$ between models of varying widths $k_{1},k_{2}$ . Here we include CNNSmall, CNNLarge, ResNetSmall, and ResNetLarge as $\\mathcal{F}$ . The values of $k_{1},k_{2}\\ \\in$ [10, 20, 40, 80, 160] determine the widths in each layer. Here the datasets are CIFAR-10 (top), CIFAR-100 (middle), and tinyImagenet (bottom). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Softmax Activations. Apart from the normalization concern, recent work in (Wang and Wang, 2022) demonstrate the difference between the input salience of the predicted logits and probabilities. As a result, we clarify that, although we define $f$ as the predicted logit (before softmax activations), this choice does not affect the observed increasing trend, no matter when the input salience is generated concerning the logit, probability, or the loss. The results of $\\rho_{i n d}(k_{1},k_{2})$ , generated from the saliency maps w.r.t. the predicted probability (after softmax activations), are shown in Figure 15. It can be found that $\\rho_{i n d}(k_{1},k_{2})$ still increases with both $k_{1}$ and $k_{2}$ . ", "page_idx": 15}, {"type": "text", "text": "B Experiment Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Model Details Throughout the experiments, we use CNNSmall, CNNLarge, ResNetSmall, and ResNetLarge as model families. Within each family, model width is controlled by the parameter $k$ . And the model depths are controlled by the \u201cSmall\" vs. \u201cLarge\" suffixes. For CNNs, CNNSmall consists of convolutional layers with channels $[k,2k,4k,8k]$ , while CNNLarge repeats each layer twice: $[k,k,2k,2k,4k,4k,8k,8k]$ . The details of CNNs are shown in Table 2. Additionally, for TinyImagenet, since the input data is of size $64\\times64$ , we increase the stride of the second MaxPool2d layer (Layer 10) to 4. As for ResNets, we modify the width of ResNet-10 for ResNetSmall and ResNet-18 for ResNetLarge. Note that $k=64$ ResNetSmall corresponds to ResNet-10, while $k=64$ ResNetLarge corresponds to ResNet-18. The sizes of models are illustrated in Figure 16 as the # of trainable parameters. ", "page_idx": 15}, {"type": "text", "text": "It should also be noted that, ideally, CNNSmall and CNNLarge, ResNetSmall and ResNetLarge are considered as the same families due to the same architecture. However, since widths can be adjusted independently of the depth, while the adjustment of depth inevitable affects the width, we split them. ", "page_idx": 15}, {"type": "image", "img_path": "7PORYhql4V/tmp/f559ed8b4e4552a1707222fb6d61be38cc623ff30bbabbc8b9f3318432cf32aa.jpg", "img_caption": ["Figure 14: The expected similarity $\\rho(k_{1},k_{2})$ between models of varying widths $k_{1},k_{2}$ . Here we include CNNSmall (CS), CNNLarge (CL), ResNetSmall (RS), and ResNetLarge (RL) as $\\mathcal{F}$ . The values of $k_{1},k_{2}$ determine the widths in each layer. All the models are initialized to random values without any optimizations. Here the datasets are CIFAR-10 (top), CIFAR-100 (middle), and TinyImagenet (bottom). "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Table 1: The average Wasserstein distance $\\mathbb{E}_{f^{(1)},f^{(2)}\\in\\mathcal{F}(k)}\\mathsf{W D}\\big(S\\big(f^{(1)}\\big),S\\big(f^{(2)}\\big)\\big)$ with the standard deivation for all model families $\\{\\mathrm{CS},\\mathrm{CL},\\mathrm{RS},\\mathrm{RL}\\}\\times\\{10,20,40,80,160\\}$ over CIFAR-10/100 and TinyImagenet datasets. Note that here the baseline should be 2 since the cosine similarity lies in $[-1,1]$ . It is observed that deeper models usually have larger distances (CS vs. CL, RS vs. RL). We deduce that this is because of the training for deeper models is more difficult. ", "page_idx": 16}, {"type": "table", "img_path": "7PORYhql4V/tmp/11717c8ad7b662e9acbb23a8e4905c8778de12dca6db357f3999d4374dc89d9e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "But our experiments in Section 3.1 verify that the depths do not affect the population mean of model distributions. In experiments, we set $\\mathcal{X}$ as the first 1000 samples of the unshuffled testing set of each dataset (CIFAR-10/100, TinyImagenet). ", "page_idx": 16}, {"type": "image", "img_path": "7PORYhql4V/tmp/f462451a9817587bd4550bd837d0413f32acffa1908363303d0d84aabe81c287.jpg", "img_caption": ["Figure 15: The expected similarity $\\rho(k_{1},k_{2})$ between models of varying widths $k_{1},k_{2}$ . Here we include CNNSmall, CNNLarge, ResNetSmall, and ResNetLarge as $\\mathcal{F}$ . The values of $k_{1},k_{2}$ determine the widths in each layer. In particular, all the cosine similarities are between the input saliency maps of the predicted probabilities instead of the predicted logits. Here the datasets are CIFAR-10 (top), CIFAR-100 (middle) and tinyImagenet (bottom). "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "7PORYhql4V/tmp/94afa92c3929e6add34f291144eae61d240a3384e7ee716b9677ad2ab5ac2af1.jpg", "img_caption": ["Figure 16: The # of trainable parameters of models vs. the width parameter $k$ for each architecture. "], "img_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "7PORYhql4V/tmp/bce2395ee28a03635d2dc01445b7d78719c54a5083b5d213ef9e09e80661b64d.jpg", "table_caption": ["Table 2: CNNSmall Model Details "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "C Additional Settings ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This work aims to reveal the convergence trend of the distribution of model behaviors under the stochasticity of the training criterion. This does not limit the conclusion to the specific criterion described above. Distinct training criteria can lead to different distributions of trained models. However, these different distributions of trained models all satisfy the revealed trend. To verify this, we present additional experiments to investigate possible variants such as (1) depths and widths; (2) learning rates; (3) batch sizes; (4) solvers (5) initializations, and (6) other model architectures. Note that as studied in Figure 6 the manuscript, $\\rho_{i n d}$ can be a computationally efficient compromise of $\\rho$ . Therefore, we studied $\\rho_{i n d}$ in these additional experiments. Besides, there exists enumerous possible combinations of different variants of different aspects. As a result, here we only vary these settings partially since enumerating the entire grid is infeasible. The results as detailed as follows. ", "page_idx": 18}, {"type": "text", "text": "Depths and widths. The scale of depths is not as straightforward as the width since modifying depths may change widths as well. Therefore, in the manuscript we study the influence of depth by setting -small and -large variations. Here we present additional results that study the influence of depths continuously, with 1-5 layers, each of which is followed by a max-pooling layer with stride 2. Finally, an adaptive pooling layer is appended at the end. To rule out the influence of widths (channels), all layers have the same width, determined by $k$ . e.g., For the 4-layer scenario, the intermediate layers have widths $[k,k,k,k]$ instead of $[k,2k,4k,8k]$ in the manuscript. The results are shown in Figure 17. It can be found that (1) Given a fixed depth or width, the influence of the ", "page_idx": 18}, {"type": "image", "img_path": "7PORYhql4V/tmp/fc44f6457df69475c03fc46d98e172f8d08ff106b1c8fbb6f55a1125ef2c3ec5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 18: (a) and (b) illustrate the cosine similarity between (a) CNNs and (b) ResNets with different batch sizes in $\\{64,128,256,512\\}$ . (c) shows the loss and (d) shows the accuracy of trained models. ", "page_idx": 19}, {"type": "text", "text": "other factor is similar when scaled up. (2) Depths are slightly different from widths. Larger widths lead to higher similarities, while closer structures in depths have higher similarities. For widths (left), the similarity always increases left-to-right and top-to-bottom. But for depths (right), pairs near the diagonal have higher similarities. ", "page_idx": 19}, {"type": "text", "text": "Batch Sizes. We investigate the influence of batch sizes, varying in $\\{64,128,256,512\\}$ The results are shown in Figure 18. It can be observed that although different batch sizes lead to different performance (e.g. testing accuracy), the convergence trend holds in all scenarios. ", "page_idx": 19}, {"type": "text", "text": "Learning Rates. We test different learning rates on how they affect the results. We include 1e-1, 1e-2, default, where \u201cdefault\" refers to the criterion used in the manuscript. As shown in Figure 19, the revealed trend is preserved in all learning rates. It is also worth noticing that learning rates affect ResNets more than CNNs. ", "page_idx": 19}, {"type": "text", "text": "Solvers. Apart from SGD, we include Adam, AdamW, and SGD w/ momentum. For Adam and AdamW we set the learning rate to 1e-3, while SGD w/ momentum uses a learning rate of 1e-1 with a momentum of 0.9. The results are shown in Figure 20. Although different solvers lead to models of different performances, they all preserve the same convergence trend. ", "page_idx": 19}, {"type": "table", "img_path": "7PORYhql4V/tmp/3aad38a0f9085d831cdb51333dd99b704fca69a069256cccb8e8fd125ff426fb.jpg", "table_caption": ["Table 3: The comparison between the similarities between single models of different criteria. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "Initializations. Given a training scheme and model family $\\mathcal{F}(k)$ , the training procedure leads to a distribution of trained models $p(f)$ . When the initialization is fixed to $\\theta$ , the training procedure is essentially sampling from the conditional distribution $p(f|\\theta)$ instead of the unconditional distribution $p(f)$ . We then studied the difference between the unconditional distribution $p(f)$ and the conditional distribution $p(f|\\theta_{0})$ . We focus on two conditional distributions $p(f|\\theta_{0})$ and $p(f|\\theta_{1})$ , where $\\theta_{1}$ ", "page_idx": 19}, {"type": "image", "img_path": "7PORYhql4V/tmp/af0f21663ce6ceebbab89286e2d196b51c898f0f7fbc3fe8b2304cfc259892af.jpg", "img_caption": ["(a) The cosine similarity between CNNs "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "7PORYhql4V/tmp/a2bec59c4a31181bf652a64b42451b1ce36dbfe42167425629f8b3ddc68e474a.jpg", "img_caption": ["(b) The cosine similarity between ResNets "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "7PORYhql4V/tmp/a6364c91ab2086346cd776c20927ba75a5fd5989d37626671db5aa29e04dc999.jpg", "img_caption": ["(c) The Loss of Models ", "(d) The Accuracy of Models "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 19: (a) and (b) illustrate the cosine similarity between (a) CNNs and (b) ResNets with different batch sizes in $\\{64,128,256,512\\}$ . (c) shows the loss and (d) shows the accuracy of trained models. ", "page_idx": 20}, {"type": "image", "img_path": "7PORYhql4V/tmp/4cc6268f38f257789151e1baddd9a777c68303b9eeef8eb007de70f6b9713dc4.jpg", "img_caption": ["(b) Performance "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 20: (a) The cosine similarity between CNN models with different solvers in {SGD, Adam, AdamW, SGD w/ Momentum}. (b): The accuracy and loss of trained models. ", "page_idx": 20}, {"type": "image", "img_path": "7PORYhql4V/tmp/9daae820695f858f7df0d26cb320bf132b61905ec4f25135fb074eae6710b569.jpg", "img_caption": ["Figure 21: The cosine similarity between Vision Transformers (ViTs) on CIFAR-10. The capacity is controlled by $k\\in\\{10,20,40,\\dot{8}0\\}$ , where the embedding dimension is $4k$ , separated to $k/2$ heads. The left subfigure shows the mean of the similarity. The right subfigure shows the similarity of the population mean. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "represents the initializations under seed $=\\!1$ and $\\theta_{0}$ represents those under seed ${}=0$ . Other settings are identical. We thus have $f_{1}^{0},\\cdot\\cdot\\cdot\\cdot,f_{100}^{0}\\sim p(f|\\theta_{0})$ and $f_{1}^{1},\\cdot\\cdot\\cdot,f_{100}^{1}\\sim p(f|\\theta_{1})$ . The superscript indicates the initialization seeds and the subscript indicates the training seeds. ", "page_idx": 21}, {"type": "text", "text": "First, we notice immediately that the training seeds for both $\\theta_{0}$ and $\\theta_{1}$ are 1 100. This means that $\\forall i,f_{i}^{0},f_{i}^{1}$ differ only in initializations. We inspect (a) $\\rho_{i n d}(f_{i}^{0},f_{i}^{1})$ (100 pairs) to see if they have exceptional similarity compared with (b) $\\rho_{i n d}(f_{i}^{0},f_{j}^{1}),i\\neq j\\,(\\binom{100}{2}=4950$ pairs). Besides, within the same condition, all models only differ in terms of the orders of the training batch. We thus also inspect the similarity of all models of the same condition: (c) $\\rho_{i n d}(f_{i}^{0},f_{j}^{\\overline{{0}}}),i\\,\\neq\\,j$ and (d) $\\rho_{i n d}(f_{i}^{1},f_{j}^{1}),i\\neq j$ . Each of them has $\\textstyle{\\binom{100}{2}}=4950$ pairs. ", "page_idx": 21}, {"type": "text", "text": "As demonstrated in Table 3, (i) the comparison between (a) and (b) indicates that with different initializations, the same order of batches in the training procedure does not contribute to higher similarities. (ii) The comparison among (b)(c)(d) indicates that the same initialization indeed leads to higher similarity even though the order of batches is distinct. It should be noted that the contributions of batch orders and initializations are also affected by the number of epochs. Intuitively, more training epochs should lead to smaller contributions from the initializations but greater contributions from the batch orders. ", "page_idx": 21}, {"type": "text", "text": "Vision Transformers. Vision Transformers (ViT) have risen in recent years as another powerful architecture for CV tasks. Here we include a brief study of ViTs to demonstrate that the discovered phenomenon holds across different architectures. ", "page_idx": 21}, {"type": "text", "text": "Specifically, we train vision transformer (ViT) models on CIFAR-10 with varying capacities controlled by $k\\in\\{10,20,40,80\\}$ . CIFAR-10 has an input size of $32\\times32$ pixels, thus the patch size is set as $4\\times4$ , resulting in $8\\times8$ patches. The embedding size is set to $4k$ , divided by $k/2$ heads, and we set the depth to 8. The seeds vary in 1-100 and results in 100 trained models of each $k$ . We study the mean of the similarity $\\rho(k_{1},k_{2})$ (i.e. the same experiments as Figure 3 in the manuscript) and the similarity of the population mean (i.e. the same as Figure 5 in the manuscript). The results are shown in Figure 21. It can be observed that although distinct from convolutional layers, the transformer structure also has the discovered convergence trend. It can also be noted that the degree of dispersion of ViTs is much higher than CNN-based models. ", "page_idx": 21}, {"type": "text", "text": "In conclusion, although training schemes can affect the resulting distributions of models, the influence of the model capacity stays invariant across different criteria. ", "page_idx": 21}, {"type": "text", "text": "D Uniform Distributions on the Hypersphere ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "According to (Muller, 1959), due to the spherical symmetry property of the zero-mean Gaussian distribution, the cosine similarity between two Gaussian variables are actually uniformly distributed over the hypersphere $S^{d-1}$ in $\\mathbb{R}^{d}$ .3 Therefore, the cosine similarity between two i.i.d. multivariate Gaussian tensors is essentially the inner product between two i.i.d. uniform tensors over the hypersphere. Formally, suppose $X,Y$ to be high-dimensional i.i.d. random variables of dimension $d$ that follow the Gaussian distribution $\\mathcal{N}(\\mathbf{0},I_{d\\times d})$ . The cosine similarity is $\\begin{array}{r}{Z\\,=\\,\\frac{X^{T}Y}{\\|X\\|\\cdot\\|Y\\|}}\\end{array}$ . WLOG, it suffices to consider the scenario where $\\begin{array}{r}{\\frac{Y}{\\|Y\\|}=e_{1}=(1,0,\\cdots,0)}\\end{array}$ . And it is written as $\\begin{array}{r}{Z=e_{i}^{T}\\frac{X}{\\|X\\|}=\\frac{X_{1}}{\\|X\\|}=\\frac{X_{1}}{\\sqrt{\\sum_{i=1}^{d}X_{i}^{2}}}=\\sqrt{\\frac{X_{1}^{2}}{X_{1}^{2}\\sum_{i=1}^{d}X_{i}^{2}}}}\\end{array}$ . Note that $X_{1}^{2}\\sim\\chi^{2}(1),\\textstyle\\sum_{i=2}^{d}X_{i}^{2}\\sim\\chi^{2}(d{-}1).$ As a result, Z2 = X12+ Xid1=2 Xi2 follows a beta distribution $\\begin{array}{r}{Z^{2}\\sim\\mathit{B e t a}(\\frac{1}{2},\\frac{d-1}{2})}\\end{array}$ . The pdf is thus $\\begin{array}{r}{f_{Z^{2}}(x)=\\frac{x^{-1/2}(1-x)^{\\stackrel{}{(}d-3)/2}}{B(1/2,(d-1)/2)}}\\end{array}$ xB(1/(21,\u2212(dx\u2212)1)/2) . And then when Z > 0, ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{Z}(x)=\\!f_{Z^{2}}(x^{2})|2x|}}\\\\ {{\\quad\\quad\\ =\\!\\!\\frac{(x^{2})^{-1/2}(1-x^{2})^{(d-3)/2}}{B(1/2,(d-1)/2)}|2x|}}\\\\ {{\\quad\\quad\\ =\\!\\!\\frac{2}{B(\\frac{1}{2},\\frac{d-1}{2})}(1-x^{2})^{(d-3)/2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "According to (Smith et al., 2023), let $u=(1\\!+x)/2$ ,then $x^{2}=(2u-1)^{2}$ . Then this can be simplified to ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{U}(u)\\!=\\!\\frac{2}{B\\left(\\frac{1}{2},\\frac{d-1}{2}\\right)}\\!\\left(1-(2u-1)^{2}\\right)^{(d-3)/2}\\cdot\\frac{\\mathrm{d}x}{\\mathrm{d}u}}}\\\\ {{\\!=\\!\\frac{1}{B\\left(\\frac{1}{2},\\frac{d-1}{2}\\right)^{2}}2^{d-2}u^{(d-1)/2-1}(1-u)^{(d-1)/2-1}}}\\\\ {{\\!=\\!\\frac{1}{B\\left(\\frac{1}{2},\\frac{d-1}{2}\\right)2^{2-d}}u^{(d-1)/2-1}(1-u)^{(d-1)/2-1}}}\\\\ {{\\!=\\!B e t a(\\frac{d-1}{2},\\frac{d-1}{2})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "This is because ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B(\\frac{d-1}{2},\\frac{d-1}{2})=B(\\frac{d-1}{2},\\frac{d+1}{2})\\cdot2}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{\\Gamma(\\frac{d-1}{2})\\Gamma(\\frac{d+1}{2})}{\\Gamma(d)}\\cdot2}\\\\ &{\\qquad\\qquad\\qquad=\\frac{\\Gamma(\\frac{d-1}{2})\\Gamma(\\frac{d+1}{2})}{2^{d-1}\\pi^{-1/2}\\Gamma(\\frac{d-1}{2})\\Gamma(\\frac{d+1}{2})}\\cdot2}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2^{2-d}\\frac{\\Gamma(\\frac{d-1}{2})\\Gamma(\\frac{1}{2})}{\\Gamma(\\frac{d}{2})}=B(\\frac{1}{2},\\frac{d-1}{2})2^{2-d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, we are able to conclude that $\\textstyle{\\frac{1+Z}{2}}\\,\\sim\\,B e t a({\\frac{d-1}{2}},{\\frac{d-1}{2}})$ , where $Z$ is the cosine similarity between two i.i.d. $d$ -dimensional Gaussian vectors. This result suggests that the distribution of $Z$ will become very concentrated around 0. And this concentration exacerbates exponentially with the dimension $d$ . Given a cosine similarity level $\\rho$ , the probability $\\mathbb{P}(Z>\\rho)$ can be extremely small, and also deceases exponentially with $\\rho$ , too. We visualize the relation between the probability $\\mathbb{P}(Z>\\rho)$ and $\\rho$ with varying dimensions $d$ in Figure 22. In the low-dimensional space such as $d=3$ , the distribution of the cosine similarity is close to uniform as humans\u2019 intuition. However, as the dimension increases, the cosine similarity is very unlikely to maintain high, as demonstrated by the curves of $d=3\\times8\\times8=48$ (orange) and $d=3\\times32\\times32=3072$ (green). ", "page_idx": 22}, {"type": "image", "img_path": "7PORYhql4V/tmp/7676abf34f8f0bbe71ab48c3e8b4dd701a37d01effc9d0f19cc12406e19a6e16.jpg", "img_caption": ["Figure 22: The relation between the probability $\\mathbb{P}(Z>\\rho)$ and the cosine similarity value $\\rho$ . "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "E Supplementary Experiment Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Note that all the experiments are carried across three datasets CIFAR-10/100 and TinyImagenet, along with four model families CNNSmall, CNNLarge, ResNetSmall, and ResNetLarge. Due to the space limit, only part of the selected results can be put in the manuscript. Therefore, we defer the results with different models/datasets into this section for the audience\u2019 reference. All the conclusions drawn from the experiment results shown in the manuscript hold for the results demonstrated here. ", "page_idx": 23}, {"type": "text", "text": "In Figure 23, we present the complementary results of Figure 6 on CIFAR-100 and TinyImagenet. It can be clearly observed that $\\rho_{i n d}$ and $\\rho$ have very similar values, after taking the expectation over $\\mathcal{X}$ . ", "page_idx": 23}, {"type": "text", "text": "Figure 25 shows the results of black-box attack results between different models. All other settings are identical to the results shown in Figure 10, but with CIFAR-100 and TinyImagenet instead of CIFAR-10. It is observed that the capacity of models has significant influence to the models\u2019 robustness and black-box attack transferability. And this trend is highly correlated to the similarity \u03c1ind, $\\rho$ as demonstrated in Figures 2, 3, 12, 13 and 15. ", "page_idx": 23}, {"type": "text", "text": "Similarly, Figure 26 shows supplementary results of Figure 11. All settings are identical except for the datasets. CIFAR-100 and TinyImagenet are tested instead of CIFAR-10. It can be observed that using the estimated mean gradient direction (blue), the performance drops much more significantly than the attack from single models of either the exact same family as the target model (green) or the largest single model tested $k=448$ , red). Note that due to the complexity of TinyImagenet, in the overfitting phase (i.e. when the target models\u2019 capacities are comparable to the dataset (Nakkiran et al., 2021)), the single-model black-box attack results in opposite effect \u2013 the prediction actually increases. ", "page_idx": 23}, {"type": "text", "text": "In Figure 24, we present CIFAR-100 and TinyImagenet results as supplementary of Figure 9. It can be observed that the testing loss is highly correlated to the expectation of $\\pmb{t}=\\pmb{u}^{T}\\pmb{\\mu}(\\pmb{x})$ . Such phenomena are also consistent across different model families and datasets. For both single models and ensembles, the closer they are to the convergent limiting point (i.e. larger $\\mathbb{E}[t];$ , the higher testing performance they have. Note that here for the sake of consistency, we approximate ${\\pmb\\mu}({\\pmb x})$ through $\\tilde{\\pmb{\\mu}}(160;\\pmb{x})$ . Therefore, in the ensemble experiments (left of each subfigure), $k=160$ is omitted. ", "page_idx": 23}, {"type": "image", "img_path": "7PORYhql4V/tmp/bb4be30eb465d6be0beff6c3a1e4604debc81a3bc5a2743f02576a57102e5248.jpg", "img_caption": ["(b) TinyImagenet "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 23: Illustration of (red) $\\rho_{i n d}(f^{(1)},f^{(2)}),f^{(1)},f^{(2)}\\in\\mathcal{F}(k)$ and (blue) $\\rho(k,k)$ on (a) CIFAR100 and (b) TinyImagenet. ", "page_idx": 24}, {"type": "image", "img_path": "7PORYhql4V/tmp/f6d2df5cd4e1c02cd11a6aa725b60f03447194aa33665f353f3fc3a02ff2516f.jpg", "img_caption": ["Figure 24: The illustration of the relation between the expected testing loss $\\mathbb{E}_{\\mathcal{X}}[\\mathcal{L}]$ and the marginal expectation $\\mathbb{E}_{\\mathcal{X}}[t]$ . Both (a) CIFAR-100 and (b) TinyImagenet results are shown as supplementary to Figure 9. Models are from (i) single models with varying structure; and (ii) deep ensembles with varying members. Each color represents a model family. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "7PORYhql4V/tmp/717339893ccd8599e929f42e118dc47d6ca38d8b0a27d11252d3bd0bd503d9ed.jpg", "img_caption": ["Figure 25: The results of single model black-box attack. The value of each entry is $\\alpha(f^{(1)},f^{(2)})$ , $f^{(1)}\\in\\mathcal{F}(k_{1})$ , $f^{(2)}\\in\\mathcal{F}(k_{2})$ for different model capacities. Here $k_{1}$ is the width parameter of the source model and $k_{2}$ is the width parameter of the target model. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "7PORYhql4V/tmp/c080d1be355d9a90d98797031524a3c623eca6ec5034fa8c2e5b7a4f99942926.jpg", "img_caption": ["Figure 26: The comparison between the single-model attack from the largest model (red), the singlemodel attack from the very same structure (green), and the attack by the mean direction (blue). The top row shows the results of CIFAR-100, and the bottom row shows the results of TinyImagenet. Some figures\u2019 $y$ -axis are set to logarithm for clarity. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The abstract mainly points out the hypotheses made in this work. They are all properly discussed and empirically verified in the manuscript. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Please refer to the conclusion section for the limitations. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The theoretical analysis in this paper is rigorous. Hypothesis are both theoretically explained and empirically verified. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in the appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The information needed to reproduce the experiments is discussed in both the manuscript and the appendix. The code is also provided in the supplementary materials. The repository will be publicized upon acceptance. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The code is provided as the supplementary material in the submission. The repository will be publicized upon acceptance. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Details of the experiments are discussed in both the manuscript and the appendix. The code is also provided in the supplementary materials. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: The potential statistical significance lies in the variance among different ensemble members. This is studied in Table 1. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Please refer to the first paragraph of Section 3 ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The authors have reviewed the code of ethics and this work conforms with the code of ethics of NeurIPS. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Focusing on fundamental, theoretical and general topics, this works does not have societal impact. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There is no such risk of this work. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: This work only uses general packages such as Numpy, PyTorch, torchvision, etc. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This work does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}]